nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.705, Test loss: 2.272, Test accuracy: 20.22
Round   1, Train loss: 1.137, Test loss: 1.803, Test accuracy: 31.72
Round   2, Train loss: 0.937, Test loss: 1.542, Test accuracy: 46.22
Round   3, Train loss: 0.944, Test loss: 1.304, Test accuracy: 49.82
Round   4, Train loss: 0.907, Test loss: 1.044, Test accuracy: 54.17
Round   5, Train loss: 0.873, Test loss: 1.064, Test accuracy: 56.02
Round   6, Train loss: 0.819, Test loss: 0.914, Test accuracy: 56.82
Round   7, Train loss: 0.800, Test loss: 0.954, Test accuracy: 58.80
Round   8, Train loss: 0.821, Test loss: 0.817, Test accuracy: 64.05
Round   9, Train loss: 0.721, Test loss: 0.824, Test accuracy: 63.02
Round  10, Train loss: 0.633, Test loss: 0.792, Test accuracy: 64.43
Round  11, Train loss: 0.768, Test loss: 0.799, Test accuracy: 63.88
Round  12, Train loss: 0.726, Test loss: 0.736, Test accuracy: 67.48
Round  13, Train loss: 0.736, Test loss: 0.709, Test accuracy: 68.80
Round  14, Train loss: 0.697, Test loss: 0.804, Test accuracy: 67.30
Round  15, Train loss: 0.586, Test loss: 0.698, Test accuracy: 69.40
Round  16, Train loss: 0.637, Test loss: 0.632, Test accuracy: 72.98
Round  17, Train loss: 0.559, Test loss: 0.614, Test accuracy: 73.15
Round  18, Train loss: 0.712, Test loss: 0.626, Test accuracy: 73.83
Round  19, Train loss: 0.670, Test loss: 0.619, Test accuracy: 73.42
Round  20, Train loss: 0.605, Test loss: 0.602, Test accuracy: 74.02
Round  21, Train loss: 0.659, Test loss: 0.589, Test accuracy: 75.23
Round  22, Train loss: 0.620, Test loss: 0.592, Test accuracy: 75.15
Round  23, Train loss: 0.557, Test loss: 0.591, Test accuracy: 75.45
Round  24, Train loss: 0.588, Test loss: 0.570, Test accuracy: 76.25
Round  25, Train loss: 0.587, Test loss: 0.565, Test accuracy: 76.52
Round  26, Train loss: 0.612, Test loss: 0.559, Test accuracy: 76.92
Round  27, Train loss: 0.521, Test loss: 0.553, Test accuracy: 76.92
Round  28, Train loss: 0.546, Test loss: 0.550, Test accuracy: 77.50
Round  29, Train loss: 0.615, Test loss: 0.542, Test accuracy: 77.67
Round  30, Train loss: 0.514, Test loss: 0.530, Test accuracy: 78.05
Round  31, Train loss: 0.490, Test loss: 0.527, Test accuracy: 78.43
Round  32, Train loss: 0.488, Test loss: 0.527, Test accuracy: 78.53
Round  33, Train loss: 0.556, Test loss: 0.533, Test accuracy: 77.80
Round  34, Train loss: 0.538, Test loss: 0.541, Test accuracy: 77.17
Round  35, Train loss: 0.567, Test loss: 0.524, Test accuracy: 78.23
Round  36, Train loss: 0.537, Test loss: 0.524, Test accuracy: 78.17
Round  37, Train loss: 0.491, Test loss: 0.505, Test accuracy: 79.15
Round  38, Train loss: 0.445, Test loss: 0.497, Test accuracy: 79.27
Round  39, Train loss: 0.508, Test loss: 0.498, Test accuracy: 78.87
Round  40, Train loss: 0.470, Test loss: 0.496, Test accuracy: 79.52
Round  41, Train loss: 0.508, Test loss: 0.491, Test accuracy: 79.42
Round  42, Train loss: 0.441, Test loss: 0.489, Test accuracy: 79.92
Round  43, Train loss: 0.500, Test loss: 0.494, Test accuracy: 80.03
Round  44, Train loss: 0.438, Test loss: 0.485, Test accuracy: 80.32
Round  45, Train loss: 0.509, Test loss: 0.492, Test accuracy: 79.53
Round  46, Train loss: 0.372, Test loss: 0.486, Test accuracy: 79.98
Round  47, Train loss: 0.466, Test loss: 0.478, Test accuracy: 80.30
Round  48, Train loss: 0.448, Test loss: 0.479, Test accuracy: 80.28
Round  49, Train loss: 0.387, Test loss: 0.472, Test accuracy: 80.50
Round  50, Train loss: 0.453, Test loss: 0.470, Test accuracy: 80.47
Round  51, Train loss: 0.486, Test loss: 0.474, Test accuracy: 80.48
Round  52, Train loss: 0.391, Test loss: 0.483, Test accuracy: 80.02
Round  53, Train loss: 0.479, Test loss: 0.468, Test accuracy: 80.48
Round  54, Train loss: 0.516, Test loss: 0.459, Test accuracy: 81.48
Round  55, Train loss: 0.442, Test loss: 0.459, Test accuracy: 81.50
Round  56, Train loss: 0.435, Test loss: 0.461, Test accuracy: 81.07
Round  57, Train loss: 0.517, Test loss: 0.456, Test accuracy: 81.53
Round  58, Train loss: 0.345, Test loss: 0.452, Test accuracy: 82.27
Round  59, Train loss: 0.418, Test loss: 0.460, Test accuracy: 81.50
Round  60, Train loss: 0.338, Test loss: 0.457, Test accuracy: 81.47
Round  61, Train loss: 0.470, Test loss: 0.435, Test accuracy: 82.53
Round  62, Train loss: 0.352, Test loss: 0.446, Test accuracy: 81.95
Round  63, Train loss: 0.421, Test loss: 0.443, Test accuracy: 82.15
Round  64, Train loss: 0.350, Test loss: 0.444, Test accuracy: 82.17
Round  65, Train loss: 0.447, Test loss: 0.438, Test accuracy: 82.73
Round  66, Train loss: 0.404, Test loss: 0.446, Test accuracy: 82.45
Round  67, Train loss: 0.319, Test loss: 0.442, Test accuracy: 82.02
Round  68, Train loss: 0.356, Test loss: 0.439, Test accuracy: 82.08
Round  69, Train loss: 0.415, Test loss: 0.440, Test accuracy: 82.60
Round  70, Train loss: 0.333, Test loss: 0.439, Test accuracy: 82.50
Round  71, Train loss: 0.320, Test loss: 0.444, Test accuracy: 81.63
Round  72, Train loss: 0.372, Test loss: 0.441, Test accuracy: 82.42
Round  73, Train loss: 0.333, Test loss: 0.435, Test accuracy: 82.37
Round  74, Train loss: 0.348, Test loss: 0.435, Test accuracy: 82.68
Round  75, Train loss: 0.364, Test loss: 0.441, Test accuracy: 82.62
Round  76, Train loss: 0.304, Test loss: 0.432, Test accuracy: 82.52
Round  77, Train loss: 0.342, Test loss: 0.426, Test accuracy: 83.05
Round  78, Train loss: 0.319, Test loss: 0.427, Test accuracy: 82.53
Round  79, Train loss: 0.382, Test loss: 0.424, Test accuracy: 82.77
Round  80, Train loss: 0.372, Test loss: 0.426, Test accuracy: 82.95
Round  81, Train loss: 0.241, Test loss: 0.417, Test accuracy: 83.27
Round  82, Train loss: 0.342, Test loss: 0.425, Test accuracy: 82.68
Round  83, Train loss: 0.311, Test loss: 0.430, Test accuracy: 82.72
Round  84, Train loss: 0.398, Test loss: 0.425, Test accuracy: 83.20
Round  85, Train loss: 0.309, Test loss: 0.425, Test accuracy: 83.00
Round  86, Train loss: 0.424, Test loss: 0.448, Test accuracy: 82.13
Round  87, Train loss: 0.280, Test loss: 0.444, Test accuracy: 82.27
Round  88, Train loss: 0.300, Test loss: 0.433, Test accuracy: 82.43
Round  89, Train loss: 0.333, Test loss: 0.431, Test accuracy: 83.05
Round  90, Train loss: 0.285, Test loss: 0.433, Test accuracy: 82.95
Round  91, Train loss: 0.320, Test loss: 0.426, Test accuracy: 83.22
Round  92, Train loss: 0.323, Test loss: 0.422, Test accuracy: 83.55
Round  93, Train loss: 0.262, Test loss: 0.424, Test accuracy: 82.98
Round  94, Train loss: 0.282, Test loss: 0.413, Test accuracy: 83.52
Round  95, Train loss: 0.287, Test loss: 0.423, Test accuracy: 83.05
Round  96, Train loss: 0.337, Test loss: 0.419, Test accuracy: 83.17
Round  97, Train loss: 0.230, Test loss: 0.417, Test accuracy: 83.43
Round  98, Train loss: 0.249, Test loss: 0.430, Test accuracy: 82.75
Round  99, Train loss: 0.349, Test loss: 0.428, Test accuracy: 83.00
Final Round, Train loss: 0.250, Test loss: 0.427, Test accuracy: 83.18
Average accuracy final 10 rounds: 83.16166666666668
855.4520611763
[3.0708041191101074, 4.168212175369263, 5.294707536697388, 6.40197229385376, 7.593447685241699, 8.693101167678833, 9.781639575958252, 10.876776218414307, 11.965669870376587, 13.053395986557007, 14.161326885223389, 15.252087116241455, 16.337754249572754, 17.420461177825928, 18.518187999725342, 19.60824179649353, 20.69040846824646, 21.791423797607422, 22.92512536048889, 24.040849685668945, 25.122618436813354, 26.207356452941895, 27.3034029006958, 28.42728352546692, 29.499194145202637, 30.571244716644287, 31.638532638549805, 32.70531392097473, 33.80549931526184, 34.88925242424011, 35.97963762283325, 37.069583892822266, 38.164610385894775, 39.24691414833069, 40.324190855026245, 41.40978407859802, 42.49455976486206, 43.58005332946777, 44.64177942276001, 45.73452377319336, 46.82109308242798, 47.911447525024414, 49.0035605430603, 50.095444202423096, 51.17899298667908, 52.26662254333496, 53.36164379119873, 54.451361656188965, 55.53810214996338, 56.626463174819946, 57.724231481552124, 58.8142569065094, 59.90235114097595, 60.98592138290405, 62.08018207550049, 63.16171312332153, 64.24608516693115, 65.32646918296814, 66.41674661636353, 67.5023205280304, 68.5824568271637, 69.66103410720825, 70.74392914772034, 71.82410097122192, 72.91475749015808, 74.01816153526306, 75.09447193145752, 76.17064166069031, 77.24870204925537, 78.33232641220093, 79.44132256507874, 80.55767130851746, 81.66702699661255, 82.74363613128662, 83.82429671287537, 84.92898535728455, 86.01251077651978, 87.09867644309998, 88.18693542480469, 89.26971650123596, 90.36335515975952, 91.43624806404114, 92.58231139183044, 93.67926359176636, 94.7708535194397, 95.92955923080444, 97.01495862007141, 98.093923330307, 99.16796684265137, 100.24180006980896, 101.31626987457275, 102.39060163497925, 103.47038102149963, 104.54912829399109, 105.63155698776245, 106.71443462371826, 107.7945282459259, 108.87238621711731, 109.94554305076599, 111.0197594165802, 112.5794849395752]
[20.216666666666665, 31.716666666666665, 46.21666666666667, 49.81666666666667, 54.166666666666664, 56.016666666666666, 56.81666666666667, 58.8, 64.05, 63.016666666666666, 64.43333333333334, 63.88333333333333, 67.48333333333333, 68.8, 67.3, 69.4, 72.98333333333333, 73.15, 73.83333333333333, 73.41666666666667, 74.01666666666667, 75.23333333333333, 75.15, 75.45, 76.25, 76.51666666666667, 76.91666666666667, 76.91666666666667, 77.5, 77.66666666666667, 78.05, 78.43333333333334, 78.53333333333333, 77.8, 77.16666666666667, 78.23333333333333, 78.16666666666667, 79.15, 79.26666666666667, 78.86666666666666, 79.51666666666667, 79.41666666666667, 79.91666666666667, 80.03333333333333, 80.31666666666666, 79.53333333333333, 79.98333333333333, 80.3, 80.28333333333333, 80.5, 80.46666666666667, 80.48333333333333, 80.01666666666667, 80.48333333333333, 81.48333333333333, 81.5, 81.06666666666666, 81.53333333333333, 82.26666666666667, 81.5, 81.46666666666667, 82.53333333333333, 81.95, 82.15, 82.16666666666667, 82.73333333333333, 82.45, 82.01666666666667, 82.08333333333333, 82.6, 82.5, 81.63333333333334, 82.41666666666667, 82.36666666666666, 82.68333333333334, 82.61666666666666, 82.51666666666667, 83.05, 82.53333333333333, 82.76666666666667, 82.95, 83.26666666666667, 82.68333333333334, 82.71666666666667, 83.2, 83.0, 82.13333333333334, 82.26666666666667, 82.43333333333334, 83.05, 82.95, 83.21666666666667, 83.55, 82.98333333333333, 83.51666666666667, 83.05, 83.16666666666667, 83.43333333333334, 82.75, 83.0, 83.18333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.687, Test loss: 2.131, Test accuracy: 25.40
Round   1, Train loss: 1.115, Test loss: 1.883, Test accuracy: 29.03
Round   2, Train loss: 1.090, Test loss: 1.381, Test accuracy: 42.30
Round   3, Train loss: 1.005, Test loss: 1.294, Test accuracy: 43.85
Round   4, Train loss: 0.907, Test loss: 1.084, Test accuracy: 49.92
Round   5, Train loss: 0.886, Test loss: 1.035, Test accuracy: 54.03
Round   6, Train loss: 0.835, Test loss: 0.985, Test accuracy: 56.90
Round   7, Train loss: 0.803, Test loss: 0.861, Test accuracy: 60.70
Round   8, Train loss: 0.770, Test loss: 0.847, Test accuracy: 60.58
Round   9, Train loss: 0.819, Test loss: 0.801, Test accuracy: 63.23
Round  10, Train loss: 0.664, Test loss: 0.764, Test accuracy: 64.90
Round  11, Train loss: 0.786, Test loss: 0.733, Test accuracy: 66.25
Round  12, Train loss: 0.640, Test loss: 0.730, Test accuracy: 66.23
Round  13, Train loss: 0.731, Test loss: 0.706, Test accuracy: 68.07
Round  14, Train loss: 0.657, Test loss: 0.694, Test accuracy: 68.37
Round  15, Train loss: 0.608, Test loss: 0.689, Test accuracy: 68.68
Round  16, Train loss: 0.691, Test loss: 0.682, Test accuracy: 68.97
Round  17, Train loss: 0.673, Test loss: 0.673, Test accuracy: 70.82
Round  18, Train loss: 0.586, Test loss: 0.659, Test accuracy: 71.00
Round  19, Train loss: 0.612, Test loss: 0.641, Test accuracy: 72.23
Round  20, Train loss: 0.586, Test loss: 0.634, Test accuracy: 71.83
Round  21, Train loss: 0.536, Test loss: 0.635, Test accuracy: 71.68
Round  22, Train loss: 0.539, Test loss: 0.647, Test accuracy: 71.15
Round  23, Train loss: 0.585, Test loss: 0.621, Test accuracy: 72.37
Round  24, Train loss: 0.551, Test loss: 0.616, Test accuracy: 72.62
Round  25, Train loss: 0.546, Test loss: 0.624, Test accuracy: 72.20
Round  26, Train loss: 0.595, Test loss: 0.605, Test accuracy: 73.23
Round  27, Train loss: 0.589, Test loss: 0.594, Test accuracy: 74.20
Round  28, Train loss: 0.568, Test loss: 0.603, Test accuracy: 73.43
Round  29, Train loss: 0.571, Test loss: 0.599, Test accuracy: 73.98
Round  30, Train loss: 0.549, Test loss: 0.603, Test accuracy: 73.18
Round  31, Train loss: 0.537, Test loss: 0.590, Test accuracy: 74.60
Round  32, Train loss: 0.608, Test loss: 0.565, Test accuracy: 75.33
Round  33, Train loss: 0.453, Test loss: 0.574, Test accuracy: 74.62
Round  34, Train loss: 0.512, Test loss: 0.575, Test accuracy: 74.97
Round  35, Train loss: 0.387, Test loss: 0.554, Test accuracy: 75.93
Round  36, Train loss: 0.592, Test loss: 0.558, Test accuracy: 76.32
Round  37, Train loss: 0.538, Test loss: 0.552, Test accuracy: 76.75
Round  38, Train loss: 0.602, Test loss: 0.540, Test accuracy: 77.32
Round  39, Train loss: 0.502, Test loss: 0.543, Test accuracy: 77.18
Round  40, Train loss: 0.569, Test loss: 0.553, Test accuracy: 77.00
Round  41, Train loss: 0.482, Test loss: 0.553, Test accuracy: 76.65
Round  42, Train loss: 0.546, Test loss: 0.545, Test accuracy: 76.85
Round  43, Train loss: 0.393, Test loss: 0.542, Test accuracy: 76.67
Round  44, Train loss: 0.431, Test loss: 0.529, Test accuracy: 77.27
Round  45, Train loss: 0.419, Test loss: 0.534, Test accuracy: 77.12
Round  46, Train loss: 0.506, Test loss: 0.543, Test accuracy: 76.82
Round  47, Train loss: 0.458, Test loss: 0.535, Test accuracy: 77.38
Round  48, Train loss: 0.453, Test loss: 0.536, Test accuracy: 77.50
Round  49, Train loss: 0.394, Test loss: 0.537, Test accuracy: 77.20
Round  50, Train loss: 0.412, Test loss: 0.520, Test accuracy: 77.93
Round  51, Train loss: 0.433, Test loss: 0.513, Test accuracy: 78.18
Round  52, Train loss: 0.521, Test loss: 0.510, Test accuracy: 78.82
Round  53, Train loss: 0.374, Test loss: 0.523, Test accuracy: 78.33
Round  54, Train loss: 0.396, Test loss: 0.527, Test accuracy: 78.27
Round  55, Train loss: 0.391, Test loss: 0.513, Test accuracy: 78.52
Round  56, Train loss: 0.427, Test loss: 0.510, Test accuracy: 79.20
Round  57, Train loss: 0.293, Test loss: 0.509, Test accuracy: 78.90
Round  58, Train loss: 0.518, Test loss: 0.506, Test accuracy: 78.42
Round  59, Train loss: 0.444, Test loss: 0.510, Test accuracy: 78.48
Round  60, Train loss: 0.407, Test loss: 0.514, Test accuracy: 78.85
Round  61, Train loss: 0.411, Test loss: 0.509, Test accuracy: 78.82
Round  62, Train loss: 0.399, Test loss: 0.508, Test accuracy: 79.08
Round  63, Train loss: 0.434, Test loss: 0.500, Test accuracy: 80.03
Round  64, Train loss: 0.416, Test loss: 0.506, Test accuracy: 78.98
Round  65, Train loss: 0.478, Test loss: 0.496, Test accuracy: 79.32
Round  66, Train loss: 0.399, Test loss: 0.492, Test accuracy: 79.57
Round  67, Train loss: 0.330, Test loss: 0.480, Test accuracy: 80.07
Round  68, Train loss: 0.390, Test loss: 0.487, Test accuracy: 79.70
Round  69, Train loss: 0.465, Test loss: 0.494, Test accuracy: 79.47
Round  70, Train loss: 0.490, Test loss: 0.506, Test accuracy: 79.07
Round  71, Train loss: 0.403, Test loss: 0.491, Test accuracy: 79.40
Round  72, Train loss: 0.400, Test loss: 0.482, Test accuracy: 80.23
Round  73, Train loss: 0.350, Test loss: 0.475, Test accuracy: 80.78
Round  74, Train loss: 0.370, Test loss: 0.468, Test accuracy: 80.55
Round  75, Train loss: 0.310, Test loss: 0.466, Test accuracy: 80.82
Round  76, Train loss: 0.394, Test loss: 0.476, Test accuracy: 80.88
Round  77, Train loss: 0.357, Test loss: 0.487, Test accuracy: 80.48
Round  78, Train loss: 0.359, Test loss: 0.470, Test accuracy: 80.98
Round  79, Train loss: 0.466, Test loss: 0.483, Test accuracy: 80.37
Round  80, Train loss: 0.373, Test loss: 0.482, Test accuracy: 80.52
Round  81, Train loss: 0.316, Test loss: 0.474, Test accuracy: 80.85
Round  82, Train loss: 0.437, Test loss: 0.471, Test accuracy: 80.62
Round  83, Train loss: 0.329, Test loss: 0.473, Test accuracy: 80.48
Round  84, Train loss: 0.341, Test loss: 0.481, Test accuracy: 80.62
Round  85, Train loss: 0.319, Test loss: 0.476, Test accuracy: 80.30
Round  86, Train loss: 0.319, Test loss: 0.466, Test accuracy: 81.00
Round  87, Train loss: 0.271, Test loss: 0.484, Test accuracy: 80.40
Round  88, Train loss: 0.315, Test loss: 0.463, Test accuracy: 81.42
Round  89, Train loss: 0.360, Test loss: 0.454, Test accuracy: 81.60
Round  90, Train loss: 0.286, Test loss: 0.460, Test accuracy: 81.55
Round  91, Train loss: 0.370, Test loss: 0.452, Test accuracy: 81.97
Round  92, Train loss: 0.359, Test loss: 0.453, Test accuracy: 81.70
Round  93, Train loss: 0.252, Test loss: 0.454, Test accuracy: 81.73
Round  94, Train loss: 0.271, Test loss: 0.456, Test accuracy: 81.55
Round  95, Train loss: 0.348, Test loss: 0.451, Test accuracy: 82.28
Round  96, Train loss: 0.298, Test loss: 0.462, Test accuracy: 81.68
Round  97, Train loss: 0.248, Test loss: 0.465, Test accuracy: 81.77
Round  98, Train loss: 0.264, Test loss: 0.464, Test accuracy: 81.60
Round  99, Train loss: 0.265, Test loss: 0.465, Test accuracy: 81.77
Final Round, Train loss: 0.272, Test loss: 0.457, Test accuracy: 82.07
Average accuracy final 10 rounds: 81.75999999999999
1604.6303761005402
[4.558927536010742, 6.8474647998809814, 9.137402296066284, 11.447227954864502, 13.753238201141357, 16.05060601234436, 18.34324598312378, 20.633548736572266, 22.935317039489746, 25.21898365020752, 27.50617742538452, 29.79889678955078, 32.16309928894043, 34.466135025024414, 36.74184608459473, 39.02434849739075, 41.29950189590454, 43.58080315589905, 45.8619818687439, 48.173253297805786, 50.461931228637695, 52.752460956573486, 55.03702354431152, 57.37702441215515, 59.654545307159424, 61.932666063308716, 64.21404004096985, 66.49398493766785, 68.7649245262146, 71.04238390922546, 73.32009506225586, 75.59456944465637, 77.88078236579895, 80.16657757759094, 82.4499442577362, 84.73832821846008, 87.01841163635254, 89.30276441574097, 91.57688927650452, 93.86599707603455, 96.29825615882874, 98.61930131912231, 100.92097306251526, 103.54270219802856, 105.83303833007812, 108.12015438079834, 110.40814018249512, 112.69866895675659, 114.98905491828918, 117.28252577781677, 119.60111021995544, 121.87165689468384, 124.14788031578064, 126.42127513885498, 128.69533395767212, 130.9703197479248, 133.24178743362427, 135.51841282844543, 137.7962191104889, 140.06251311302185, 142.3351755142212, 144.6004102230072, 146.8737232685089, 149.14532494544983, 151.41346740722656, 153.68222546577454, 155.95725870132446, 158.25968146324158, 160.57085609436035, 162.88167929649353, 165.16334652900696, 167.4522864818573, 169.7286913394928, 172.0102355480194, 174.2915177345276, 176.5708487033844, 178.88255882263184, 181.14921641349792, 183.42964792251587, 185.70795607566833, 187.98844599723816, 190.26572370529175, 192.53825163841248, 194.81167340278625, 197.12486386299133, 199.40559554100037, 201.67515182495117, 203.94479036331177, 206.24133014678955, 208.52275943756104, 210.80284929275513, 213.08997678756714, 215.37111830711365, 217.6480894088745, 219.92480039596558, 222.20532131195068, 224.4844274520874, 226.76838564872742, 229.05261874198914, 231.33643913269043, 234.50966882705688]
[25.4, 29.033333333333335, 42.3, 43.85, 49.916666666666664, 54.03333333333333, 56.9, 60.7, 60.583333333333336, 63.233333333333334, 64.9, 66.25, 66.23333333333333, 68.06666666666666, 68.36666666666666, 68.68333333333334, 68.96666666666667, 70.81666666666666, 71.0, 72.23333333333333, 71.83333333333333, 71.68333333333334, 71.15, 72.36666666666666, 72.61666666666666, 72.2, 73.23333333333333, 74.2, 73.43333333333334, 73.98333333333333, 73.18333333333334, 74.6, 75.33333333333333, 74.61666666666666, 74.96666666666667, 75.93333333333334, 76.31666666666666, 76.75, 77.31666666666666, 77.18333333333334, 77.0, 76.65, 76.85, 76.66666666666667, 77.26666666666667, 77.11666666666666, 76.81666666666666, 77.38333333333334, 77.5, 77.2, 77.93333333333334, 78.18333333333334, 78.81666666666666, 78.33333333333333, 78.26666666666667, 78.51666666666667, 79.2, 78.9, 78.41666666666667, 78.48333333333333, 78.85, 78.81666666666666, 79.08333333333333, 80.03333333333333, 78.98333333333333, 79.31666666666666, 79.56666666666666, 80.06666666666666, 79.7, 79.46666666666667, 79.06666666666666, 79.4, 80.23333333333333, 80.78333333333333, 80.55, 80.81666666666666, 80.88333333333334, 80.48333333333333, 80.98333333333333, 80.36666666666666, 80.51666666666667, 80.85, 80.61666666666666, 80.48333333333333, 80.61666666666666, 80.3, 81.0, 80.4, 81.41666666666667, 81.6, 81.55, 81.96666666666667, 81.7, 81.73333333333333, 81.55, 82.28333333333333, 81.68333333333334, 81.76666666666667, 81.6, 81.76666666666667, 82.06666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 0.915, Test loss: 2.051, Test accuracy: 22.58
Round   1, Train loss: 0.627, Test loss: 1.717, Test accuracy: 34.47
Round   2, Train loss: 0.531, Test loss: 1.674, Test accuracy: 36.08
Round   3, Train loss: 0.459, Test loss: 1.549, Test accuracy: 43.80
Round   4, Train loss: 0.497, Test loss: 1.335, Test accuracy: 51.82
Round   5, Train loss: 0.400, Test loss: 1.256, Test accuracy: 55.87
Round   6, Train loss: 0.457, Test loss: 1.347, Test accuracy: 53.00
Round   7, Train loss: 0.486, Test loss: 1.105, Test accuracy: 56.50
Round   8, Train loss: 0.369, Test loss: 1.089, Test accuracy: 56.17
Round   9, Train loss: 0.410, Test loss: 1.128, Test accuracy: 57.13
Round  10, Train loss: 0.325, Test loss: 0.988, Test accuracy: 61.27
Round  11, Train loss: 0.372, Test loss: 0.971, Test accuracy: 59.95
Round  12, Train loss: 0.333, Test loss: 1.135, Test accuracy: 52.77
Round  13, Train loss: 0.358, Test loss: 1.024, Test accuracy: 56.78
Round  14, Train loss: 0.295, Test loss: 0.996, Test accuracy: 60.30
Round  15, Train loss: 0.323, Test loss: 0.993, Test accuracy: 60.32
Round  16, Train loss: 0.283, Test loss: 0.807, Test accuracy: 65.90
Round  17, Train loss: 0.315, Test loss: 0.822, Test accuracy: 65.77
Round  18, Train loss: 0.361, Test loss: 0.833, Test accuracy: 65.82
Round  19, Train loss: 0.286, Test loss: 0.765, Test accuracy: 67.88
Round  20, Train loss: 0.264, Test loss: 0.767, Test accuracy: 67.22
Round  21, Train loss: 0.328, Test loss: 0.750, Test accuracy: 67.83
Round  22, Train loss: 0.284, Test loss: 0.738, Test accuracy: 68.87
Round  23, Train loss: 0.295, Test loss: 0.716, Test accuracy: 69.75
Round  24, Train loss: 0.298, Test loss: 0.733, Test accuracy: 69.47
Round  25, Train loss: 0.295, Test loss: 0.755, Test accuracy: 69.77
Round  26, Train loss: 0.276, Test loss: 0.748, Test accuracy: 71.13
Round  27, Train loss: 0.282, Test loss: 0.809, Test accuracy: 68.90
Round  28, Train loss: 0.279, Test loss: 0.900, Test accuracy: 66.82
Round  29, Train loss: 0.319, Test loss: 0.917, Test accuracy: 66.35
Round  30, Train loss: 0.263, Test loss: 0.794, Test accuracy: 70.97
Round  31, Train loss: 0.287, Test loss: 0.752, Test accuracy: 71.53
Round  32, Train loss: 0.273, Test loss: 0.729, Test accuracy: 72.38
Round  33, Train loss: 0.209, Test loss: 0.636, Test accuracy: 75.20
Round  34, Train loss: 0.245, Test loss: 0.616, Test accuracy: 75.88
Round  35, Train loss: 0.226, Test loss: 0.723, Test accuracy: 72.45
Round  36, Train loss: 0.199, Test loss: 0.704, Test accuracy: 73.07
Round  37, Train loss: 0.199, Test loss: 0.771, Test accuracy: 72.45
Round  38, Train loss: 0.161, Test loss: 0.748, Test accuracy: 73.48
Round  39, Train loss: 0.206, Test loss: 0.791, Test accuracy: 70.30
Round  40, Train loss: 0.212, Test loss: 0.745, Test accuracy: 72.63
Round  41, Train loss: 0.230, Test loss: 0.757, Test accuracy: 72.25
Round  42, Train loss: 0.191, Test loss: 0.724, Test accuracy: 73.57
Round  43, Train loss: 0.198, Test loss: 0.742, Test accuracy: 72.52
Round  44, Train loss: 0.202, Test loss: 0.788, Test accuracy: 70.97
Round  45, Train loss: 0.173, Test loss: 0.771, Test accuracy: 72.85
Round  46, Train loss: 0.191, Test loss: 0.766, Test accuracy: 72.50
Round  47, Train loss: 0.180, Test loss: 0.729, Test accuracy: 74.35
Round  48, Train loss: 0.150, Test loss: 0.728, Test accuracy: 74.87
Round  49, Train loss: 0.160, Test loss: 0.778, Test accuracy: 73.30
Round  50, Train loss: 0.163, Test loss: 0.814, Test accuracy: 72.20
Round  51, Train loss: 0.204, Test loss: 0.785, Test accuracy: 71.78
Round  52, Train loss: 0.198, Test loss: 0.776, Test accuracy: 71.92
Round  53, Train loss: 0.240, Test loss: 0.732, Test accuracy: 73.30
Round  54, Train loss: 0.157, Test loss: 0.673, Test accuracy: 75.18
Round  55, Train loss: 0.154, Test loss: 0.741, Test accuracy: 72.65
Round  56, Train loss: 0.169, Test loss: 0.735, Test accuracy: 73.35
Round  57, Train loss: 0.149, Test loss: 0.780, Test accuracy: 72.22
Round  58, Train loss: 0.154, Test loss: 0.694, Test accuracy: 74.37
Round  59, Train loss: 0.152, Test loss: 0.703, Test accuracy: 74.43
Round  60, Train loss: 0.145, Test loss: 0.706, Test accuracy: 74.92
Round  61, Train loss: 0.120, Test loss: 0.640, Test accuracy: 76.20
Round  62, Train loss: 0.125, Test loss: 0.589, Test accuracy: 77.77
Round  63, Train loss: 0.120, Test loss: 0.666, Test accuracy: 76.70
Round  64, Train loss: 0.137, Test loss: 0.643, Test accuracy: 76.70
Round  65, Train loss: 0.098, Test loss: 0.667, Test accuracy: 75.27
Round  66, Train loss: 0.174, Test loss: 0.630, Test accuracy: 76.82
Round  67, Train loss: 0.208, Test loss: 0.733, Test accuracy: 73.93
Round  68, Train loss: 0.180, Test loss: 0.688, Test accuracy: 75.22
Round  69, Train loss: 0.186, Test loss: 0.678, Test accuracy: 75.75
Round  70, Train loss: 0.129, Test loss: 0.619, Test accuracy: 77.78
Round  71, Train loss: 0.170, Test loss: 0.638, Test accuracy: 76.75
Round  72, Train loss: 0.173, Test loss: 0.748, Test accuracy: 74.28
Round  73, Train loss: 0.106, Test loss: 0.749, Test accuracy: 75.13
Round  74, Train loss: 0.111, Test loss: 0.728, Test accuracy: 75.25
Round  75, Train loss: 0.167, Test loss: 0.681, Test accuracy: 76.70
Round  76, Train loss: 0.093, Test loss: 0.655, Test accuracy: 77.88
Round  77, Train loss: 0.118, Test loss: 0.635, Test accuracy: 77.18
Round  78, Train loss: 0.117, Test loss: 0.609, Test accuracy: 78.23
Round  79, Train loss: 0.105, Test loss: 0.610, Test accuracy: 77.92
Round  80, Train loss: 0.134, Test loss: 0.680, Test accuracy: 77.22
Round  81, Train loss: 0.131, Test loss: 0.736, Test accuracy: 76.10
Round  82, Train loss: 0.130, Test loss: 0.687, Test accuracy: 77.17
Round  83, Train loss: 0.120, Test loss: 0.685, Test accuracy: 76.57
Round  84, Train loss: 0.108, Test loss: 0.618, Test accuracy: 78.43
Round  85, Train loss: 0.094, Test loss: 0.625, Test accuracy: 78.15
Round  86, Train loss: 0.084, Test loss: 0.654, Test accuracy: 78.20
Round  87, Train loss: 0.107, Test loss: 0.705, Test accuracy: 77.35
Round  88, Train loss: 0.146, Test loss: 0.658, Test accuracy: 77.78
Round  89, Train loss: 0.136, Test loss: 0.664, Test accuracy: 77.18
Round  90, Train loss: 0.089, Test loss: 0.584, Test accuracy: 79.92
Round  91, Train loss: 0.104, Test loss: 0.634, Test accuracy: 78.65
Round  92, Train loss: 0.107, Test loss: 0.677, Test accuracy: 77.43
Round  93, Train loss: 0.105, Test loss: 0.646, Test accuracy: 78.12
Round  94, Train loss: 0.085, Test loss: 0.691, Test accuracy: 77.42
Round  95, Train loss: 0.142, Test loss: 0.734, Test accuracy: 76.77
Round  96, Train loss: 0.092, Test loss: 0.673, Test accuracy: 78.80
Round  97, Train loss: 0.103, Test loss: 0.618, Test accuracy: 79.28
Round  98, Train loss: 0.124, Test loss: 0.670, Test accuracy: 77.42
Round  99, Train loss: 0.111, Test loss: 0.702, Test accuracy: 76.97
Final Round, Train loss: 0.082, Test loss: 0.498, Test accuracy: 83.30
Average accuracy final 10 rounds: 78.07666666666668
3107.4151644706726
[7.057954788208008, 12.166991949081421, 17.26768708229065, 21.692875146865845, 26.143198013305664, 30.57465434074402, 35.70102524757385, 40.844483375549316, 45.977033615112305, 51.09961485862732, 56.204877853393555, 61.33073091506958, 65.76253318786621, 70.38929080963135, 74.82822775840759, 79.26542091369629, 83.71213507652283, 88.1401720046997, 92.58867955207825, 97.03012585639954, 101.46484971046448, 105.97630786895752, 110.43296241760254, 114.86008954048157, 119.28200769424438, 123.71149921417236, 128.15619730949402, 132.57949709892273, 137.0112509727478, 141.4430432319641, 145.96904826164246, 150.4947474002838, 155.02314472198486, 159.5589542388916, 164.10182118415833, 168.53726410865784, 172.97372150421143, 177.4818663597107, 181.90048265457153, 186.3240303993225, 190.85812401771545, 195.32378244400024, 199.7983763217926, 204.93175792694092, 210.0779664516449, 215.23944568634033, 220.4005982875824, 224.86018085479736, 229.29870104789734, 233.73229026794434, 238.20516896247864, 242.81988286972046, 247.38364505767822, 251.9610185623169, 256.5248165130615, 261.0837697982788, 265.5544707775116, 270.0175898075104, 275.0946989059448, 279.8194224834442, 284.36096930503845, 288.8288359642029, 293.2828571796417, 297.7112731933594, 302.13512802124023, 306.5667164325714, 311.04064750671387, 315.5094745159149, 319.98232078552246, 324.4472620487213, 328.9190835952759, 333.38506269454956, 337.8553512096405, 342.31976652145386, 346.7846052646637, 351.24953842163086, 355.6918547153473, 360.11061358451843, 364.59808444976807, 369.06608867645264, 373.5371880531311, 377.9999918937683, 382.47313475608826, 387.37131810188293, 391.8372015953064, 396.31063985824585, 400.81499576568604, 405.29755783081055, 409.72336053848267, 414.1754002571106, 418.6009359359741, 423.0226571559906, 427.47355818748474, 431.8989336490631, 436.34938764572144, 440.7816560268402, 445.23817253112793, 449.7174735069275, 454.19242811203003, 458.6575701236725, 470.1004319190979]
[22.583333333333332, 34.46666666666667, 36.083333333333336, 43.8, 51.81666666666667, 55.86666666666667, 53.0, 56.5, 56.166666666666664, 57.13333333333333, 61.266666666666666, 59.95, 52.766666666666666, 56.78333333333333, 60.3, 60.31666666666667, 65.9, 65.76666666666667, 65.81666666666666, 67.88333333333334, 67.21666666666667, 67.83333333333333, 68.86666666666666, 69.75, 69.46666666666667, 69.76666666666667, 71.13333333333334, 68.9, 66.81666666666666, 66.35, 70.96666666666667, 71.53333333333333, 72.38333333333334, 75.2, 75.88333333333334, 72.45, 73.06666666666666, 72.45, 73.48333333333333, 70.3, 72.63333333333334, 72.25, 73.56666666666666, 72.51666666666667, 70.96666666666667, 72.85, 72.5, 74.35, 74.86666666666666, 73.3, 72.2, 71.78333333333333, 71.91666666666667, 73.3, 75.18333333333334, 72.65, 73.35, 72.21666666666667, 74.36666666666666, 74.43333333333334, 74.91666666666667, 76.2, 77.76666666666667, 76.7, 76.7, 75.26666666666667, 76.81666666666666, 73.93333333333334, 75.21666666666667, 75.75, 77.78333333333333, 76.75, 74.28333333333333, 75.13333333333334, 75.25, 76.7, 77.88333333333334, 77.18333333333334, 78.23333333333333, 77.91666666666667, 77.21666666666667, 76.1, 77.16666666666667, 76.56666666666666, 78.43333333333334, 78.15, 78.2, 77.35, 77.78333333333333, 77.18333333333334, 79.91666666666667, 78.65, 77.43333333333334, 78.11666666666666, 77.41666666666667, 76.76666666666667, 78.8, 79.28333333333333, 77.41666666666667, 76.96666666666667, 83.3]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: center_psl, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.358, Test loss: 2.162, Test accuracy: 25.45
Round   1, Train loss: 0.959, Test loss: 1.799, Test accuracy: 33.63
Round   2, Train loss: 0.801, Test loss: 1.838, Test accuracy: 33.03
Round   3, Train loss: 0.924, Test loss: 1.782, Test accuracy: 38.63
Round   4, Train loss: 0.697, Test loss: 1.467, Test accuracy: 44.68
Round   5, Train loss: 0.747, Test loss: 1.307, Test accuracy: 49.57
Round   6, Train loss: 0.665, Test loss: 1.234, Test accuracy: 50.35
Round   7, Train loss: 0.568, Test loss: 1.120, Test accuracy: 56.68
Round   8, Train loss: 0.539, Test loss: 1.051, Test accuracy: 57.90
Round   9, Train loss: 0.585, Test loss: 1.087, Test accuracy: 57.35
Round  10, Train loss: 0.488, Test loss: 1.103, Test accuracy: 58.00
Round  11, Train loss: 0.554, Test loss: 0.954, Test accuracy: 60.68
Round  12, Train loss: 0.461, Test loss: 0.987, Test accuracy: 62.52
Round  13, Train loss: 0.482, Test loss: 1.033, Test accuracy: 60.00
Round  14, Train loss: 0.504, Test loss: 1.033, Test accuracy: 59.77
Round  15, Train loss: 0.429, Test loss: 0.898, Test accuracy: 64.62
Round  16, Train loss: 0.444, Test loss: 0.888, Test accuracy: 66.22
Round  17, Train loss: 0.477, Test loss: 0.732, Test accuracy: 71.73
Round  18, Train loss: 0.410, Test loss: 0.819, Test accuracy: 68.03
Round  19, Train loss: 0.408, Test loss: 0.919, Test accuracy: 66.17
Round  20, Train loss: 0.389, Test loss: 0.924, Test accuracy: 66.55
Round  21, Train loss: 0.349, Test loss: 0.856, Test accuracy: 68.25
Round  22, Train loss: 0.395, Test loss: 0.903, Test accuracy: 67.68
Round  23, Train loss: 0.318, Test loss: 0.944, Test accuracy: 67.05
Round  24, Train loss: 0.326, Test loss: 0.841, Test accuracy: 70.05
Round  25, Train loss: 0.359, Test loss: 0.827, Test accuracy: 69.55
Round  26, Train loss: 0.334, Test loss: 0.856, Test accuracy: 69.32
Round  27, Train loss: 0.302, Test loss: 0.810, Test accuracy: 70.35
Round  28, Train loss: 0.303, Test loss: 0.797, Test accuracy: 70.72
Round  29, Train loss: 0.289, Test loss: 0.820, Test accuracy: 69.93
Round  30, Train loss: 0.347, Test loss: 0.894, Test accuracy: 68.52
Round  31, Train loss: 0.260, Test loss: 0.841, Test accuracy: 70.68
Round  32, Train loss: 0.257, Test loss: 0.925, Test accuracy: 69.10
Round  33, Train loss: 0.274, Test loss: 0.951, Test accuracy: 68.35
Round  34, Train loss: 0.263, Test loss: 0.866, Test accuracy: 70.27
Round  35, Train loss: 0.270, Test loss: 0.863, Test accuracy: 70.13
Round  36, Train loss: 0.272, Test loss: 0.841, Test accuracy: 71.07
Round  37, Train loss: 0.261, Test loss: 0.761, Test accuracy: 73.33
Round  38, Train loss: 0.218, Test loss: 0.692, Test accuracy: 75.37
Round  39, Train loss: 0.290, Test loss: 0.648, Test accuracy: 75.83
Round  40, Train loss: 0.241, Test loss: 0.703, Test accuracy: 74.63
Round  41, Train loss: 0.253, Test loss: 0.681, Test accuracy: 74.82
Round  42, Train loss: 0.207, Test loss: 0.676, Test accuracy: 75.80
Round  43, Train loss: 0.224, Test loss: 0.670, Test accuracy: 74.92
Round  44, Train loss: 0.223, Test loss: 0.655, Test accuracy: 76.28
Round  45, Train loss: 0.202, Test loss: 0.655, Test accuracy: 75.88
Round  46, Train loss: 0.171, Test loss: 0.692, Test accuracy: 74.75
Round  47, Train loss: 0.207, Test loss: 0.705, Test accuracy: 75.13
Round  48, Train loss: 0.219, Test loss: 0.686, Test accuracy: 75.08
Round  49, Train loss: 0.235, Test loss: 0.774, Test accuracy: 72.43
Round  50, Train loss: 0.171, Test loss: 0.670, Test accuracy: 75.35
Round  51, Train loss: 0.208, Test loss: 0.677, Test accuracy: 74.82
Round  52, Train loss: 0.154, Test loss: 0.619, Test accuracy: 76.77
Round  53, Train loss: 0.228, Test loss: 0.612, Test accuracy: 76.98
Round  54, Train loss: 0.167, Test loss: 0.629, Test accuracy: 76.37
Round  55, Train loss: 0.202, Test loss: 0.630, Test accuracy: 76.53
Round  56, Train loss: 0.143, Test loss: 0.596, Test accuracy: 77.72
Round  57, Train loss: 0.159, Test loss: 0.638, Test accuracy: 77.37
Round  58, Train loss: 0.180, Test loss: 0.711, Test accuracy: 75.22
Round  59, Train loss: 0.175, Test loss: 0.686, Test accuracy: 76.25
Round  60, Train loss: 0.161, Test loss: 0.604, Test accuracy: 78.23
Round  61, Train loss: 0.128, Test loss: 0.639, Test accuracy: 77.70
Round  62, Train loss: 0.160, Test loss: 0.596, Test accuracy: 78.55
Round  63, Train loss: 0.145, Test loss: 0.583, Test accuracy: 78.88
Round  64, Train loss: 0.174, Test loss: 0.582, Test accuracy: 78.30
Round  65, Train loss: 0.136, Test loss: 0.574, Test accuracy: 78.37
Round  66, Train loss: 0.154, Test loss: 0.589, Test accuracy: 78.48
Round  67, Train loss: 0.156, Test loss: 0.688, Test accuracy: 77.02
Round  68, Train loss: 0.124, Test loss: 0.718, Test accuracy: 76.60
Round  69, Train loss: 0.183, Test loss: 0.739, Test accuracy: 76.10
Round  70, Train loss: 0.120, Test loss: 0.634, Test accuracy: 78.20
Round  71, Train loss: 0.121, Test loss: 0.617, Test accuracy: 78.87
Round  72, Train loss: 0.117, Test loss: 0.578, Test accuracy: 79.57
Round  73, Train loss: 0.142, Test loss: 0.599, Test accuracy: 79.02
Round  74, Train loss: 0.134, Test loss: 0.634, Test accuracy: 78.15
Round  75, Train loss: 0.093, Test loss: 0.609, Test accuracy: 78.45
Round  76, Train loss: 0.130, Test loss: 0.591, Test accuracy: 78.88
Round  77, Train loss: 0.199, Test loss: 0.600, Test accuracy: 78.70
Round  78, Train loss: 0.112, Test loss: 0.612, Test accuracy: 79.17
Round  79, Train loss: 0.110, Test loss: 0.620, Test accuracy: 79.27
Round  80, Train loss: 0.121, Test loss: 0.598, Test accuracy: 79.82
Round  81, Train loss: 0.132, Test loss: 0.554, Test accuracy: 80.80
Round  82, Train loss: 0.146, Test loss: 0.542, Test accuracy: 80.93
Round  83, Train loss: 0.109, Test loss: 0.564, Test accuracy: 80.23
Round  84, Train loss: 0.098, Test loss: 0.612, Test accuracy: 79.38
Round  85, Train loss: 0.130, Test loss: 0.633, Test accuracy: 78.75
Round  86, Train loss: 0.090, Test loss: 0.627, Test accuracy: 79.18
Round  87, Train loss: 0.117, Test loss: 0.596, Test accuracy: 79.52
Round  88, Train loss: 0.122, Test loss: 0.559, Test accuracy: 79.92
Round  89, Train loss: 0.089, Test loss: 0.553, Test accuracy: 80.82
Round  90, Train loss: 0.080, Test loss: 0.574, Test accuracy: 80.02
Round  91, Train loss: 0.098, Test loss: 0.536, Test accuracy: 80.95
Round  92, Train loss: 0.108, Test loss: 0.546, Test accuracy: 80.27
Round  93, Train loss: 0.100, Test loss: 0.549, Test accuracy: 80.43
Round  94, Train loss: 0.094, Test loss: 0.541, Test accuracy: 80.77
Round  95, Train loss: 0.082, Test loss: 0.573, Test accuracy: 80.33
Round  96, Train loss: 0.070, Test loss: 0.561, Test accuracy: 80.18
Round  97, Train loss: 0.093, Test loss: 0.603, Test accuracy: 80.17
Round  98, Train loss: 0.077, Test loss: 0.574, Test accuracy: 80.62
Round  99, Train loss: 0.075, Test loss: 0.572, Test accuracy: 80.98
Final Round, Train loss: 0.086, Test loss: 0.511, Test accuracy: 82.88
Average accuracy final 10 rounds: 80.47166666666666
3295.718645334244
[7.406842231750488, 12.86659049987793, 18.311745405197144, 23.764950037002563, 29.21331262588501, 34.667311668395996, 40.12564134597778, 45.58468317985535, 51.037123680114746, 56.521952867507935, 61.97553062438965, 67.41681957244873, 72.30308961868286, 77.1764235496521, 82.04769706726074, 86.83620262145996, 91.66960620880127, 96.4979956150055, 101.30292868614197, 106.13362264633179, 110.48746418952942, 114.7969617843628, 119.11496806144714, 123.41855573654175, 127.72287726402283, 132.02730441093445, 136.34759163856506, 140.6501853466034, 144.9552206993103, 149.7698404788971, 154.57797813415527, 159.37465858459473, 164.1676607131958, 168.97080254554749, 173.74054145812988, 178.51294779777527, 183.3003809452057, 188.0704050064087, 192.8140058517456, 197.7662320137024, 202.69526076316833, 207.49988198280334, 212.27759051322937, 217.0975263118744, 221.90201139450073, 226.67478251457214, 231.48670983314514, 236.32592582702637, 241.1295096874237, 245.87579703330994, 250.6548662185669, 255.38705158233643, 260.1563444137573, 265.0764961242676, 269.800518989563, 274.56254386901855, 279.3325972557068, 284.1079170703888, 288.93461418151855, 293.75657844543457, 298.47878336906433, 303.19182801246643, 307.9232933521271, 312.65885829925537, 317.4238374233246, 322.29669523239136, 327.16864252090454, 332.0268042087555, 336.83070039749146, 342.2566068172455, 347.6619825363159, 352.42921257019043, 357.1892259120941, 361.9517002105713, 366.7102880477905, 371.4899687767029, 376.23584508895874, 380.97298860549927, 385.72156262397766, 390.4834032058716, 395.23921298980713, 399.9908699989319, 404.7195146083832, 409.43877124786377, 414.2075879573822, 418.9001955986023, 423.6108407974243, 428.3637752532959, 433.1758975982666, 437.919606924057, 442.65087699890137, 447.43342566490173, 452.2397379875183, 457.05236411094666, 461.78035616874695, 466.5020408630371, 471.2975769042969, 476.16870403289795, 480.8886008262634, 485.62440729141235, 497.5448534488678]
[25.45, 33.63333333333333, 33.03333333333333, 38.63333333333333, 44.68333333333333, 49.56666666666667, 50.35, 56.68333333333333, 57.9, 57.35, 58.0, 60.68333333333333, 62.516666666666666, 60.0, 59.766666666666666, 64.61666666666666, 66.21666666666667, 71.73333333333333, 68.03333333333333, 66.16666666666667, 66.55, 68.25, 67.68333333333334, 67.05, 70.05, 69.55, 69.31666666666666, 70.35, 70.71666666666667, 69.93333333333334, 68.51666666666667, 70.68333333333334, 69.1, 68.35, 70.26666666666667, 70.13333333333334, 71.06666666666666, 73.33333333333333, 75.36666666666666, 75.83333333333333, 74.63333333333334, 74.81666666666666, 75.8, 74.91666666666667, 76.28333333333333, 75.88333333333334, 74.75, 75.13333333333334, 75.08333333333333, 72.43333333333334, 75.35, 74.81666666666666, 76.76666666666667, 76.98333333333333, 76.36666666666666, 76.53333333333333, 77.71666666666667, 77.36666666666666, 75.21666666666667, 76.25, 78.23333333333333, 77.7, 78.55, 78.88333333333334, 78.3, 78.36666666666666, 78.48333333333333, 77.01666666666667, 76.6, 76.1, 78.2, 78.86666666666666, 79.56666666666666, 79.01666666666667, 78.15, 78.45, 78.88333333333334, 78.7, 79.16666666666667, 79.26666666666667, 79.81666666666666, 80.8, 80.93333333333334, 80.23333333333333, 79.38333333333334, 78.75, 79.18333333333334, 79.51666666666667, 79.91666666666667, 80.81666666666666, 80.01666666666667, 80.95, 80.26666666666667, 80.43333333333334, 80.76666666666667, 80.33333333333333, 80.18333333333334, 80.16666666666667, 80.61666666666666, 80.98333333333333, 82.88333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.7000
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.1833
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.6367
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0267
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4367
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5767
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0067
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.5067
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.3933
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6167
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.719, Test loss: 2.109, Test accuracy: 24.80
Round   1, Train loss: 1.178, Test loss: 1.935, Test accuracy: 28.52
Round   2, Train loss: 1.070, Test loss: 1.734, Test accuracy: 35.15
Round   3, Train loss: 1.035, Test loss: 1.559, Test accuracy: 39.35
Round   4, Train loss: 1.036, Test loss: 1.418, Test accuracy: 38.75
Round   5, Train loss: 0.913, Test loss: 1.236, Test accuracy: 48.52
Round   6, Train loss: 1.053, Test loss: 1.150, Test accuracy: 48.55
Round   7, Train loss: 0.938, Test loss: 1.032, Test accuracy: 55.32
Round   8, Train loss: 0.969, Test loss: 0.896, Test accuracy: 58.10
Round   9, Train loss: 0.935, Test loss: 0.868, Test accuracy: 59.35
Round  10, Train loss: 1.023, Test loss: 0.899, Test accuracy: 56.63
Round  11, Train loss: 0.893, Test loss: 0.878, Test accuracy: 58.10
Round  12, Train loss: 0.986, Test loss: 0.894, Test accuracy: 57.22
Round  13, Train loss: 1.053, Test loss: 0.893, Test accuracy: 58.48
Round  14, Train loss: 0.883, Test loss: 0.880, Test accuracy: 58.07
Round  15, Train loss: 0.894, Test loss: 0.865, Test accuracy: 57.88
Round  16, Train loss: 0.822, Test loss: 0.855, Test accuracy: 59.37
Round  17, Train loss: 0.790, Test loss: 0.825, Test accuracy: 62.70
Round  18, Train loss: 0.764, Test loss: 0.823, Test accuracy: 63.65
Round  19, Train loss: 0.930, Test loss: 0.840, Test accuracy: 62.00
Round  20, Train loss: 0.944, Test loss: 0.829, Test accuracy: 62.22
Round  21, Train loss: 0.882, Test loss: 0.829, Test accuracy: 62.95
Round  22, Train loss: 0.682, Test loss: 0.804, Test accuracy: 63.43
Round  23, Train loss: 0.693, Test loss: 0.793, Test accuracy: 63.93
Round  24, Train loss: 0.986, Test loss: 0.788, Test accuracy: 64.03
Round  25, Train loss: 0.812, Test loss: 0.793, Test accuracy: 63.15
Round  26, Train loss: 0.932, Test loss: 0.793, Test accuracy: 63.57
Round  27, Train loss: 0.780, Test loss: 0.788, Test accuracy: 64.07
Round  28, Train loss: 0.895, Test loss: 0.783, Test accuracy: 64.43
Round  29, Train loss: 0.881, Test loss: 0.777, Test accuracy: 64.10
Round  30, Train loss: 0.819, Test loss: 0.771, Test accuracy: 65.08
Round  31, Train loss: 0.799, Test loss: 0.762, Test accuracy: 65.08
Round  32, Train loss: 0.970, Test loss: 0.772, Test accuracy: 65.02
Round  33, Train loss: 0.855, Test loss: 0.766, Test accuracy: 64.67
Round  34, Train loss: 0.673, Test loss: 0.761, Test accuracy: 64.48
Round  35, Train loss: 0.684, Test loss: 0.765, Test accuracy: 64.25
Round  36, Train loss: 0.717, Test loss: 0.762, Test accuracy: 64.42
Round  37, Train loss: 0.789, Test loss: 0.762, Test accuracy: 65.22
Round  38, Train loss: 0.696, Test loss: 0.767, Test accuracy: 65.12
Round  39, Train loss: 0.772, Test loss: 0.755, Test accuracy: 65.38
Round  40, Train loss: 0.775, Test loss: 0.757, Test accuracy: 65.70
Round  41, Train loss: 0.818, Test loss: 0.761, Test accuracy: 66.37
Round  42, Train loss: 0.813, Test loss: 0.763, Test accuracy: 66.20
Round  43, Train loss: 0.779, Test loss: 0.751, Test accuracy: 65.70
Round  44, Train loss: 0.856, Test loss: 0.753, Test accuracy: 65.85
Round  45, Train loss: 0.790, Test loss: 0.728, Test accuracy: 67.10
Round  46, Train loss: 0.658, Test loss: 0.736, Test accuracy: 67.15
Round  47, Train loss: 0.729, Test loss: 0.739, Test accuracy: 66.97
Round  48, Train loss: 0.760, Test loss: 0.736, Test accuracy: 67.85
Round  49, Train loss: 0.830, Test loss: 0.742, Test accuracy: 66.48
Round  50, Train loss: 0.850, Test loss: 0.751, Test accuracy: 66.62
Round  51, Train loss: 0.801, Test loss: 0.734, Test accuracy: 66.98
Round  52, Train loss: 0.534, Test loss: 0.716, Test accuracy: 68.18
Round  53, Train loss: 0.808, Test loss: 0.724, Test accuracy: 68.25
Round  54, Train loss: 0.582, Test loss: 0.724, Test accuracy: 68.42
Round  55, Train loss: 0.700, Test loss: 0.713, Test accuracy: 68.70
Round  56, Train loss: 0.623, Test loss: 0.719, Test accuracy: 68.28
Round  57, Train loss: 0.576, Test loss: 0.713, Test accuracy: 68.22
Round  58, Train loss: 0.589, Test loss: 0.726, Test accuracy: 67.28
Round  59, Train loss: 0.759, Test loss: 0.724, Test accuracy: 67.38
Round  60, Train loss: 0.639, Test loss: 0.722, Test accuracy: 67.37
Round  61, Train loss: 0.749, Test loss: 0.719, Test accuracy: 68.92
Round  62, Train loss: 0.736, Test loss: 0.732, Test accuracy: 67.93
Round  63, Train loss: 0.676, Test loss: 0.723, Test accuracy: 67.77
Round  64, Train loss: 0.696, Test loss: 0.728, Test accuracy: 67.83
Round  65, Train loss: 0.679, Test loss: 0.713, Test accuracy: 69.25
Round  66, Train loss: 0.657, Test loss: 0.711, Test accuracy: 68.58
Round  67, Train loss: 0.711, Test loss: 0.715, Test accuracy: 68.58
Round  68, Train loss: 0.721, Test loss: 0.710, Test accuracy: 68.48
Round  69, Train loss: 0.656, Test loss: 0.715, Test accuracy: 68.05
Round  70, Train loss: 0.449, Test loss: 0.702, Test accuracy: 68.73
Round  71, Train loss: 0.684, Test loss: 0.706, Test accuracy: 68.97
Round  72, Train loss: 0.691, Test loss: 0.708, Test accuracy: 68.70
Round  73, Train loss: 0.767, Test loss: 0.711, Test accuracy: 68.78
Round  74, Train loss: 0.644, Test loss: 0.696, Test accuracy: 68.57
Round  75, Train loss: 0.726, Test loss: 0.714, Test accuracy: 68.08
Round  76, Train loss: 0.562, Test loss: 0.706, Test accuracy: 68.85
Round  77, Train loss: 0.688, Test loss: 0.710, Test accuracy: 69.12
Round  78, Train loss: 0.728, Test loss: 0.700, Test accuracy: 69.42
Round  79, Train loss: 0.525, Test loss: 0.696, Test accuracy: 69.37
Round  80, Train loss: 0.609, Test loss: 0.700, Test accuracy: 69.13
Round  81, Train loss: 0.695, Test loss: 0.697, Test accuracy: 69.00
Round  82, Train loss: 0.607, Test loss: 0.705, Test accuracy: 68.93
Round  83, Train loss: 0.498, Test loss: 0.696, Test accuracy: 69.58
Round  84, Train loss: 0.739, Test loss: 0.707, Test accuracy: 68.72
Round  85, Train loss: 0.516, Test loss: 0.705, Test accuracy: 68.63
Round  86, Train loss: 0.585, Test loss: 0.698, Test accuracy: 69.72
Round  87, Train loss: 0.670, Test loss: 0.700, Test accuracy: 69.73
Round  88, Train loss: 0.840, Test loss: 0.706, Test accuracy: 68.48
Round  89, Train loss: 0.538, Test loss: 0.705, Test accuracy: 68.98
Round  90, Train loss: 0.612, Test loss: 0.704, Test accuracy: 68.85
Round  91, Train loss: 0.617, Test loss: 0.702, Test accuracy: 68.90
Round  92, Train loss: 0.619, Test loss: 0.706, Test accuracy: 68.53
Round  93, Train loss: 0.489, Test loss: 0.699, Test accuracy: 68.85
Round  94, Train loss: 0.489, Test loss: 0.704, Test accuracy: 68.82
Round  95, Train loss: 0.729, Test loss: 0.704, Test accuracy: 69.18
Round  96, Train loss: 0.523, Test loss: 0.699, Test accuracy: 69.22
Round  97, Train loss: 0.449, Test loss: 0.704, Test accuracy: 69.27
Round  98, Train loss: 0.406, Test loss: 0.709, Test accuracy: 68.73
Round  99, Train loss: 0.710, Test loss: 0.714, Test accuracy: 68.73
Final Round, Train loss: 0.562, Test loss: 0.716, Test accuracy: 68.77
Average accuracy final 10 rounds: 68.90833333333335
870.8122951984406
[3.1557207107543945, 4.384332895278931, 5.601434707641602, 6.837395668029785, 8.057374477386475, 9.28067421913147, 10.507956743240356, 11.731587409973145, 12.95441722869873, 14.172685623168945, 15.395612239837646, 16.62119221687317, 17.84689497947693, 18.950619220733643, 20.035151481628418, 21.108426570892334, 22.187058210372925, 23.25916600227356, 24.33418893814087, 25.40332055091858, 26.483097553253174, 27.555952787399292, 28.625378847122192, 29.698089122772217, 30.77522611618042, 31.85030770301819, 32.93099594116211, 34.01125240325928, 35.091400384902954, 36.168118715286255, 37.24333882331848, 38.32172608375549, 39.405585289001465, 40.49945068359375, 41.59223794937134, 42.677451372146606, 43.762903451919556, 44.851688623428345, 45.939685344696045, 47.0214786529541, 48.126384973526, 49.20311522483826, 50.28048777580261, 51.40081739425659, 52.52293348312378, 53.63962006568909, 54.76254677772522, 55.878228187561035, 56.99096369743347, 58.10750102996826, 59.22862792015076, 60.34873580932617, 61.46879863739014, 62.587469816207886, 63.70517539978027, 64.78497290611267, 65.86713290214539, 66.95210528373718, 68.06015658378601, 69.14151930809021, 70.22000765800476, 71.29884791374207, 72.37979412078857, 73.45787930488586, 74.57218718528748, 75.6462914943695, 76.72119164466858, 77.79291677474976, 78.87881398200989, 79.96139717102051, 81.04462027549744, 82.12777543067932, 83.20790076255798, 84.29286026954651, 85.37287878990173, 86.46153974533081, 87.54683804512024, 88.6400887966156, 89.72600436210632, 90.81191897392273, 91.89560079574585, 92.98246312141418, 94.06255030632019, 95.14670705795288, 96.23205041885376, 97.31802177429199, 98.40722894668579, 99.50206112861633, 100.5823073387146, 101.66966819763184, 102.75200748443604, 103.83390951156616, 104.91487979888916, 105.99848008155823, 107.08247518539429, 108.17160058021545, 109.25582098960876, 110.3406765460968, 111.42430639266968, 112.50881385803223, 114.1170346736908]
[24.8, 28.516666666666666, 35.15, 39.35, 38.75, 48.516666666666666, 48.55, 55.31666666666667, 58.1, 59.35, 56.63333333333333, 58.1, 57.21666666666667, 58.483333333333334, 58.06666666666667, 57.88333333333333, 59.36666666666667, 62.7, 63.65, 62.0, 62.21666666666667, 62.95, 63.43333333333333, 63.93333333333333, 64.03333333333333, 63.15, 63.56666666666667, 64.06666666666666, 64.43333333333334, 64.1, 65.08333333333333, 65.08333333333333, 65.01666666666667, 64.66666666666667, 64.48333333333333, 64.25, 64.41666666666667, 65.21666666666667, 65.11666666666666, 65.38333333333334, 65.7, 66.36666666666666, 66.2, 65.7, 65.85, 67.1, 67.15, 66.96666666666667, 67.85, 66.48333333333333, 66.61666666666666, 66.98333333333333, 68.18333333333334, 68.25, 68.41666666666667, 68.7, 68.28333333333333, 68.21666666666667, 67.28333333333333, 67.38333333333334, 67.36666666666666, 68.91666666666667, 67.93333333333334, 67.76666666666667, 67.83333333333333, 69.25, 68.58333333333333, 68.58333333333333, 68.48333333333333, 68.05, 68.73333333333333, 68.96666666666667, 68.7, 68.78333333333333, 68.56666666666666, 68.08333333333333, 68.85, 69.11666666666666, 69.41666666666667, 69.36666666666666, 69.13333333333334, 69.0, 68.93333333333334, 69.58333333333333, 68.71666666666667, 68.63333333333334, 69.71666666666667, 69.73333333333333, 68.48333333333333, 68.98333333333333, 68.85, 68.9, 68.53333333333333, 68.85, 68.81666666666666, 69.18333333333334, 69.21666666666667, 69.26666666666667, 68.73333333333333, 68.73333333333333, 68.76666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.7000
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.1833
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.6367
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0267
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4367
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.6067
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0067
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.5067
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.3933
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6167
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.752, Test loss: 2.108, Test accuracy: 24.47
Round   1, Train loss: 1.162, Test loss: 1.936, Test accuracy: 26.95
Round   2, Train loss: 1.033, Test loss: 1.698, Test accuracy: 34.53
Round   3, Train loss: 1.057, Test loss: 1.584, Test accuracy: 36.77
Round   4, Train loss: 1.003, Test loss: 1.449, Test accuracy: 38.47
Round   5, Train loss: 0.932, Test loss: 1.264, Test accuracy: 45.35
Round   6, Train loss: 1.015, Test loss: 1.181, Test accuracy: 46.58
Round   7, Train loss: 0.956, Test loss: 1.079, Test accuracy: 52.62
Round   8, Train loss: 0.984, Test loss: 0.934, Test accuracy: 56.60
Round   9, Train loss: 0.941, Test loss: 0.910, Test accuracy: 57.45
Round  10, Train loss: 0.995, Test loss: 0.917, Test accuracy: 56.40
Round  11, Train loss: 0.885, Test loss: 0.917, Test accuracy: 56.98
Round  12, Train loss: 0.953, Test loss: 0.939, Test accuracy: 53.87
Round  13, Train loss: 0.996, Test loss: 0.947, Test accuracy: 54.48
Round  14, Train loss: 0.846, Test loss: 0.927, Test accuracy: 56.03
Round  15, Train loss: 0.854, Test loss: 0.882, Test accuracy: 57.62
Round  16, Train loss: 0.806, Test loss: 0.858, Test accuracy: 58.15
Round  17, Train loss: 0.802, Test loss: 0.864, Test accuracy: 59.00
Round  18, Train loss: 0.767, Test loss: 0.837, Test accuracy: 60.50
Round  19, Train loss: 0.918, Test loss: 0.874, Test accuracy: 59.33
Round  20, Train loss: 0.922, Test loss: 0.863, Test accuracy: 59.97
Round  21, Train loss: 0.858, Test loss: 0.862, Test accuracy: 60.53
Round  22, Train loss: 0.656, Test loss: 0.844, Test accuracy: 60.93
Round  23, Train loss: 0.662, Test loss: 0.857, Test accuracy: 60.75
Round  24, Train loss: 0.948, Test loss: 0.873, Test accuracy: 59.47
Round  25, Train loss: 0.778, Test loss: 0.863, Test accuracy: 59.50
Round  26, Train loss: 0.901, Test loss: 0.842, Test accuracy: 61.45
Round  27, Train loss: 0.759, Test loss: 0.832, Test accuracy: 62.65
Round  28, Train loss: 0.861, Test loss: 0.855, Test accuracy: 62.00
Round  29, Train loss: 0.839, Test loss: 0.860, Test accuracy: 61.87
Round  30, Train loss: 0.786, Test loss: 0.860, Test accuracy: 61.03
Round  31, Train loss: 0.784, Test loss: 0.849, Test accuracy: 60.28
Round  32, Train loss: 0.925, Test loss: 0.848, Test accuracy: 60.52
Round  33, Train loss: 0.831, Test loss: 0.862, Test accuracy: 59.92
Round  34, Train loss: 0.645, Test loss: 0.851, Test accuracy: 59.65
Round  35, Train loss: 0.642, Test loss: 0.855, Test accuracy: 59.60
Round  36, Train loss: 0.703, Test loss: 0.832, Test accuracy: 61.28
Round  37, Train loss: 0.771, Test loss: 0.826, Test accuracy: 62.08
Round  38, Train loss: 0.661, Test loss: 0.823, Test accuracy: 62.57
Round  39, Train loss: 0.751, Test loss: 0.809, Test accuracy: 63.68
Round  40, Train loss: 0.725, Test loss: 0.815, Test accuracy: 63.70
Round  41, Train loss: 0.804, Test loss: 0.840, Test accuracy: 61.38
Round  42, Train loss: 0.771, Test loss: 0.838, Test accuracy: 62.43
Round  43, Train loss: 0.768, Test loss: 0.849, Test accuracy: 60.38
Round  44, Train loss: 0.826, Test loss: 0.862, Test accuracy: 59.33
Round  45, Train loss: 0.753, Test loss: 0.853, Test accuracy: 62.12
Round  46, Train loss: 0.613, Test loss: 0.808, Test accuracy: 63.95
Round  47, Train loss: 0.707, Test loss: 0.834, Test accuracy: 63.97
Round  48, Train loss: 0.736, Test loss: 0.815, Test accuracy: 64.77
Round  49, Train loss: 0.789, Test loss: 0.815, Test accuracy: 64.70
Round  50, Train loss: 0.816, Test loss: 0.815, Test accuracy: 64.32
Round  51, Train loss: 0.767, Test loss: 0.849, Test accuracy: 62.58
Round  52, Train loss: 0.532, Test loss: 0.824, Test accuracy: 63.35
Round  53, Train loss: 0.767, Test loss: 0.806, Test accuracy: 63.47
Round  54, Train loss: 0.569, Test loss: 0.803, Test accuracy: 64.45
Round  55, Train loss: 0.672, Test loss: 0.824, Test accuracy: 63.92
Round  56, Train loss: 0.597, Test loss: 0.816, Test accuracy: 63.65
Round  57, Train loss: 0.561, Test loss: 0.810, Test accuracy: 63.70
Round  58, Train loss: 0.567, Test loss: 0.802, Test accuracy: 64.63
Round  59, Train loss: 0.745, Test loss: 0.806, Test accuracy: 64.38
Round  60, Train loss: 0.635, Test loss: 0.794, Test accuracy: 65.25
Round  61, Train loss: 0.738, Test loss: 0.789, Test accuracy: 65.63
Round  62, Train loss: 0.730, Test loss: 0.796, Test accuracy: 64.82
Round  63, Train loss: 0.658, Test loss: 0.791, Test accuracy: 65.27
Round  64, Train loss: 0.688, Test loss: 0.786, Test accuracy: 64.75
Round  65, Train loss: 0.657, Test loss: 0.789, Test accuracy: 64.77
Round  66, Train loss: 0.633, Test loss: 0.795, Test accuracy: 64.23
Round  67, Train loss: 0.707, Test loss: 0.784, Test accuracy: 65.07
Round  68, Train loss: 0.700, Test loss: 0.785, Test accuracy: 65.22
Round  69, Train loss: 0.641, Test loss: 0.789, Test accuracy: 65.25
Round  70, Train loss: 0.460, Test loss: 0.779, Test accuracy: 66.07
Round  71, Train loss: 0.668, Test loss: 0.759, Test accuracy: 66.38
Round  72, Train loss: 0.686, Test loss: 0.757, Test accuracy: 67.13
Round  73, Train loss: 0.735, Test loss: 0.775, Test accuracy: 66.73
Round  74, Train loss: 0.613, Test loss: 0.772, Test accuracy: 66.48
Round  75, Train loss: 0.695, Test loss: 0.776, Test accuracy: 66.90
Round  76, Train loss: 0.556, Test loss: 0.757, Test accuracy: 67.20
Round  77, Train loss: 0.664, Test loss: 0.750, Test accuracy: 67.35
Round  78, Train loss: 0.711, Test loss: 0.763, Test accuracy: 66.43
Round  79, Train loss: 0.536, Test loss: 0.756, Test accuracy: 66.05
Round  80, Train loss: 0.611, Test loss: 0.750, Test accuracy: 66.53
Round  81, Train loss: 0.673, Test loss: 0.770, Test accuracy: 65.73
Round  82, Train loss: 0.615, Test loss: 0.747, Test accuracy: 66.80
Round  83, Train loss: 0.523, Test loss: 0.749, Test accuracy: 66.40
Round  84, Train loss: 0.744, Test loss: 0.782, Test accuracy: 64.85
Round  85, Train loss: 0.508, Test loss: 0.747, Test accuracy: 66.42
Round  86, Train loss: 0.578, Test loss: 0.753, Test accuracy: 66.23
Round  87, Train loss: 0.653, Test loss: 0.773, Test accuracy: 65.95
Round  88, Train loss: 0.813, Test loss: 0.773, Test accuracy: 66.80
Round  89, Train loss: 0.531, Test loss: 0.754, Test accuracy: 67.05
Round  90, Train loss: 0.592, Test loss: 0.760, Test accuracy: 67.20
Round  91, Train loss: 0.591, Test loss: 0.753, Test accuracy: 67.62
Round  92, Train loss: 0.612, Test loss: 0.753, Test accuracy: 67.78
Round  93, Train loss: 0.492, Test loss: 0.754, Test accuracy: 67.57
Round  94, Train loss: 0.495, Test loss: 0.760, Test accuracy: 67.43
Round  95, Train loss: 0.719, Test loss: 0.776, Test accuracy: 66.75
Round  96, Train loss: 0.514, Test loss: 0.774, Test accuracy: 66.17
Round  97, Train loss: 0.447, Test loss: 0.747, Test accuracy: 67.38
Round  98, Train loss: 0.418, Test loss: 0.746, Test accuracy: 67.22
Round  99, Train loss: 0.679, Test loss: 0.757, Test accuracy: 66.45
Final Round, Train loss: 0.565, Test loss: 0.761, Test accuracy: 66.17
Average accuracy final 10 rounds: 67.15666666666667
1618.848355293274
[4.546787738800049, 7.169114351272583, 9.804441213607788, 12.443203449249268, 15.070227146148682, 17.687846183776855, 20.00191044807434, 22.352338075637817, 24.876277208328247, 27.149641036987305, 29.414570331573486, 31.721870183944702, 34.0136444568634, 36.297179222106934, 38.58244037628174, 40.8680055141449, 43.157835245132446, 45.441139698028564, 47.72220492362976, 50.00563406944275, 52.29321765899658, 54.58253073692322, 56.891273975372314, 59.18327188491821, 61.47595167160034, 63.8191237449646, 66.141348361969, 68.43285703659058, 70.73208284378052, 73.0270254611969, 75.32075119018555, 77.62094855308533, 79.91507005691528, 82.21232652664185, 84.5021722316742, 86.79117727279663, 89.08185148239136, 91.37941217422485, 93.67346620559692, 95.9702296257019, 98.26364493370056, 100.56735849380493, 102.86776947975159, 105.22040247917175, 107.5191342830658, 109.817697763443, 112.10361981391907, 114.40261459350586, 116.69380903244019, 118.98271894454956, 121.26915717124939, 123.56979060173035, 125.90165162086487, 128.23555707931519, 130.50726556777954, 132.78521609306335, 135.0521628856659, 137.60235381126404, 139.86746549606323, 142.12866234779358, 144.3891842365265, 146.64915227890015, 148.92924237251282, 151.2052948474884, 153.4854166507721, 155.7678689956665, 158.04216814041138, 160.30608415603638, 162.56781888008118, 164.83321118354797, 167.0951316356659, 169.35746097564697, 171.63136315345764, 173.90470266342163, 176.18882083892822, 178.51474404335022, 180.8440806865692, 183.10404586791992, 185.37126302719116, 187.64631056785583, 189.92872405052185, 192.19889426231384, 194.46987080574036, 196.7457857131958, 199.00267052650452, 201.2648663520813, 203.54193425178528, 205.81886672973633, 208.12269973754883, 210.3938925266266, 212.66392874717712, 214.92985439300537, 217.29549098014832, 219.6173858642578, 221.9232976436615, 224.20493006706238, 226.53162503242493, 228.85891318321228, 231.18068885803223, 233.5334312915802, 237.096937417984]
[24.466666666666665, 26.95, 34.53333333333333, 36.766666666666666, 38.46666666666667, 45.35, 46.583333333333336, 52.61666666666667, 56.6, 57.45, 56.4, 56.983333333333334, 53.86666666666667, 54.483333333333334, 56.03333333333333, 57.61666666666667, 58.15, 59.0, 60.5, 59.333333333333336, 59.96666666666667, 60.53333333333333, 60.93333333333333, 60.75, 59.46666666666667, 59.5, 61.45, 62.65, 62.0, 61.86666666666667, 61.03333333333333, 60.28333333333333, 60.516666666666666, 59.916666666666664, 59.65, 59.6, 61.28333333333333, 62.083333333333336, 62.56666666666667, 63.68333333333333, 63.7, 61.38333333333333, 62.43333333333333, 60.38333333333333, 59.333333333333336, 62.11666666666667, 63.95, 63.96666666666667, 64.76666666666667, 64.7, 64.31666666666666, 62.583333333333336, 63.35, 63.46666666666667, 64.45, 63.916666666666664, 63.65, 63.7, 64.63333333333334, 64.38333333333334, 65.25, 65.63333333333334, 64.81666666666666, 65.26666666666667, 64.75, 64.76666666666667, 64.23333333333333, 65.06666666666666, 65.21666666666667, 65.25, 66.06666666666666, 66.38333333333334, 67.13333333333334, 66.73333333333333, 66.48333333333333, 66.9, 67.2, 67.35, 66.43333333333334, 66.05, 66.53333333333333, 65.73333333333333, 66.8, 66.4, 64.85, 66.41666666666667, 66.23333333333333, 65.95, 66.8, 67.05, 67.2, 67.61666666666666, 67.78333333333333, 67.56666666666666, 67.43333333333334, 66.75, 66.16666666666667, 67.38333333333334, 67.21666666666667, 66.45, 66.16666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.7000
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.1833
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.6367
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0267
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4367
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5767
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0067
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.5067
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.3933
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6167
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 0.979, Test loss: 2.139, Test accuracy: 15.12
Round   1, Train loss: 0.711, Test loss: 1.940, Test accuracy: 23.03
Round   2, Train loss: 0.586, Test loss: 1.814, Test accuracy: 30.10
Round   3, Train loss: 0.606, Test loss: 1.782, Test accuracy: 33.02
Round   4, Train loss: 0.614, Test loss: 1.641, Test accuracy: 34.83
Round   5, Train loss: 0.502, Test loss: 1.432, Test accuracy: 41.47
Round   6, Train loss: 0.631, Test loss: 1.433, Test accuracy: 38.80
Round   7, Train loss: 0.493, Test loss: 1.267, Test accuracy: 46.33
Round   8, Train loss: 0.562, Test loss: 1.177, Test accuracy: 47.42
Round   9, Train loss: 0.514, Test loss: 1.165, Test accuracy: 50.78
Round  10, Train loss: 0.607, Test loss: 1.197, Test accuracy: 47.20
Round  11, Train loss: 0.518, Test loss: 1.126, Test accuracy: 48.13
Round  12, Train loss: 0.524, Test loss: 1.108, Test accuracy: 48.35
Round  13, Train loss: 0.605, Test loss: 1.202, Test accuracy: 46.62
Round  14, Train loss: 0.438, Test loss: 1.106, Test accuracy: 50.82
Round  15, Train loss: 0.463, Test loss: 1.130, Test accuracy: 52.75
Round  16, Train loss: 0.454, Test loss: 1.064, Test accuracy: 55.27
Round  17, Train loss: 0.403, Test loss: 1.123, Test accuracy: 55.07
Round  18, Train loss: 0.378, Test loss: 1.112, Test accuracy: 55.70
Round  19, Train loss: 0.500, Test loss: 1.167, Test accuracy: 49.98
Round  20, Train loss: 0.472, Test loss: 1.181, Test accuracy: 51.10
Round  21, Train loss: 0.448, Test loss: 1.055, Test accuracy: 52.67
Round  22, Train loss: 0.356, Test loss: 1.016, Test accuracy: 54.87
Round  23, Train loss: 0.336, Test loss: 1.006, Test accuracy: 55.92
Round  24, Train loss: 0.511, Test loss: 1.210, Test accuracy: 49.90
Round  25, Train loss: 0.413, Test loss: 1.196, Test accuracy: 51.50
Round  26, Train loss: 0.476, Test loss: 1.215, Test accuracy: 51.25
Round  27, Train loss: 0.393, Test loss: 1.034, Test accuracy: 53.45
Round  28, Train loss: 0.477, Test loss: 1.103, Test accuracy: 52.17
Round  29, Train loss: 0.467, Test loss: 1.111, Test accuracy: 52.78
Round  30, Train loss: 0.466, Test loss: 1.061, Test accuracy: 53.58
Round  31, Train loss: 0.405, Test loss: 1.107, Test accuracy: 53.22
Round  32, Train loss: 0.496, Test loss: 1.131, Test accuracy: 50.73
Round  33, Train loss: 0.432, Test loss: 1.111, Test accuracy: 52.78
Round  34, Train loss: 0.316, Test loss: 1.037, Test accuracy: 55.43
Round  35, Train loss: 0.376, Test loss: 1.012, Test accuracy: 55.68
Round  36, Train loss: 0.360, Test loss: 0.995, Test accuracy: 55.47
Round  37, Train loss: 0.385, Test loss: 1.010, Test accuracy: 57.07
Round  38, Train loss: 0.395, Test loss: 1.015, Test accuracy: 56.03
Round  39, Train loss: 0.460, Test loss: 1.072, Test accuracy: 56.30
Round  40, Train loss: 0.365, Test loss: 1.048, Test accuracy: 56.75
Round  41, Train loss: 0.426, Test loss: 1.062, Test accuracy: 56.62
Round  42, Train loss: 0.370, Test loss: 1.165, Test accuracy: 53.58
Round  43, Train loss: 0.406, Test loss: 1.157, Test accuracy: 52.70
Round  44, Train loss: 0.391, Test loss: 1.101, Test accuracy: 53.83
Round  45, Train loss: 0.417, Test loss: 1.019, Test accuracy: 57.42
Round  46, Train loss: 0.272, Test loss: 0.942, Test accuracy: 59.70
Round  47, Train loss: 0.321, Test loss: 0.999, Test accuracy: 55.30
Round  48, Train loss: 0.337, Test loss: 1.063, Test accuracy: 55.90
Round  49, Train loss: 0.401, Test loss: 1.069, Test accuracy: 54.70
Round  50, Train loss: 0.388, Test loss: 1.111, Test accuracy: 54.72
Round  51, Train loss: 0.353, Test loss: 1.076, Test accuracy: 54.92
Round  52, Train loss: 0.265, Test loss: 1.190, Test accuracy: 53.28
Round  53, Train loss: 0.369, Test loss: 1.175, Test accuracy: 53.42
Round  54, Train loss: 0.284, Test loss: 1.120, Test accuracy: 55.58
Round  55, Train loss: 0.339, Test loss: 1.090, Test accuracy: 54.45
Round  56, Train loss: 0.302, Test loss: 1.086, Test accuracy: 55.65
Round  57, Train loss: 0.256, Test loss: 1.102, Test accuracy: 55.93
Round  58, Train loss: 0.254, Test loss: 1.083, Test accuracy: 56.67
Round  59, Train loss: 0.357, Test loss: 1.030, Test accuracy: 57.80
Round  60, Train loss: 0.311, Test loss: 0.962, Test accuracy: 61.95
Round  61, Train loss: 0.347, Test loss: 1.034, Test accuracy: 60.27
Round  62, Train loss: 0.330, Test loss: 1.079, Test accuracy: 57.85
Round  63, Train loss: 0.274, Test loss: 1.103, Test accuracy: 57.25
Round  64, Train loss: 0.300, Test loss: 1.063, Test accuracy: 57.28
Round  65, Train loss: 0.335, Test loss: 0.974, Test accuracy: 59.97
Round  66, Train loss: 0.286, Test loss: 0.987, Test accuracy: 60.93
Round  67, Train loss: 0.301, Test loss: 1.011, Test accuracy: 60.20
Round  68, Train loss: 0.343, Test loss: 1.047, Test accuracy: 59.65
Round  69, Train loss: 0.335, Test loss: 1.033, Test accuracy: 60.45
Round  70, Train loss: 0.198, Test loss: 1.009, Test accuracy: 61.72
Round  71, Train loss: 0.282, Test loss: 0.946, Test accuracy: 61.48
Round  72, Train loss: 0.336, Test loss: 1.058, Test accuracy: 58.90
Round  73, Train loss: 0.362, Test loss: 1.121, Test accuracy: 58.18
Round  74, Train loss: 0.300, Test loss: 1.055, Test accuracy: 60.60
Round  75, Train loss: 0.326, Test loss: 1.125, Test accuracy: 57.67
Round  76, Train loss: 0.237, Test loss: 1.102, Test accuracy: 60.03
Round  77, Train loss: 0.269, Test loss: 1.012, Test accuracy: 61.20
Round  78, Train loss: 0.302, Test loss: 1.034, Test accuracy: 60.58
Round  79, Train loss: 0.212, Test loss: 1.025, Test accuracy: 61.98
Round  80, Train loss: 0.250, Test loss: 0.985, Test accuracy: 62.18
Round  81, Train loss: 0.312, Test loss: 1.041, Test accuracy: 60.63
Round  82, Train loss: 0.265, Test loss: 1.067, Test accuracy: 58.67
Round  83, Train loss: 0.205, Test loss: 1.097, Test accuracy: 59.25
Round  84, Train loss: 0.285, Test loss: 1.116, Test accuracy: 58.03
Round  85, Train loss: 0.201, Test loss: 1.013, Test accuracy: 60.30
Round  86, Train loss: 0.251, Test loss: 1.027, Test accuracy: 60.38
Round  87, Train loss: 0.260, Test loss: 1.113, Test accuracy: 58.85
Round  88, Train loss: 0.336, Test loss: 1.190, Test accuracy: 57.13
Round  89, Train loss: 0.244, Test loss: 1.163, Test accuracy: 57.12
Round  90, Train loss: 0.224, Test loss: 1.198, Test accuracy: 56.75
Round  91, Train loss: 0.295, Test loss: 1.109, Test accuracy: 58.78
Round  92, Train loss: 0.247, Test loss: 1.183, Test accuracy: 57.87
Round  93, Train loss: 0.210, Test loss: 1.082, Test accuracy: 60.85
Round  94, Train loss: 0.205, Test loss: 1.051, Test accuracy: 60.68
Round  95, Train loss: 0.288, Test loss: 1.062, Test accuracy: 61.92
Round  96, Train loss: 0.235, Test loss: 1.042, Test accuracy: 62.93
Round  97, Train loss: 0.183, Test loss: 1.080, Test accuracy: 60.55
Round  98, Train loss: 0.178, Test loss: 1.107, Test accuracy: 60.02
Round  99, Train loss: 0.272, Test loss: 1.113, Test accuracy: 60.50
Final Round, Train loss: 0.222, Test loss: 0.963, Test accuracy: 64.73
Average accuracy final 10 rounds: 60.085
3037.4742481708527
[6.311060428619385, 10.703003644943237, 15.197582721710205, 19.63784646987915, 24.066016674041748, 28.55429768562317, 33.62340068817139, 38.71249794960022, 43.774608850479126, 48.84112763404846, 53.91748905181885, 59.00626492500305, 64.07537603378296, 68.44889426231384, 72.8197865486145, 77.21988677978516, 81.59873247146606, 86.11308932304382, 90.5688796043396, 95.01091194152832, 99.45270299911499, 103.89276432991028, 108.33611583709717, 112.72370409965515, 117.11255526542664, 121.54972100257874, 126.38768482208252, 131.44485926628113, 135.31819033622742, 139.21347665786743, 143.1020770072937, 146.9890742301941, 150.88264632225037, 154.78107023239136, 158.66840839385986, 162.54549646377563, 166.43008494377136, 170.86419916152954, 175.3620674610138, 180.0374162197113, 184.3796398639679, 188.71420168876648, 193.05565071105957, 197.5037612915039, 201.8299081325531, 206.1498100757599, 210.5095386505127, 214.86046051979065, 219.2281050682068, 223.623957157135, 228.0152235031128, 232.42481017112732, 236.81308913230896, 241.19750952720642, 245.59541082382202, 249.99694871902466, 254.3968677520752, 258.7990539073944, 263.2265429496765, 267.6835997104645, 272.07933354377747, 276.48943972587585, 280.90126872062683, 285.3046326637268, 289.6968824863434, 294.1065311431885, 299.1072061061859, 304.140900850296, 309.1688320636749, 314.22081089019775, 319.2331910133362, 324.27772974967957, 329.0222706794739, 333.3905928134918, 337.75096249580383, 342.1191475391388, 346.49909496307373, 350.86739468574524, 355.2429344654083, 359.6379256248474, 364.0581474304199, 368.4150035381317, 372.77741837501526, 377.12742471694946, 381.5402102470398, 385.91048097610474, 390.3908107280731, 394.789514541626, 399.2370936870575, 403.6040804386139, 408.01259660720825, 412.6002604961395, 417.05880069732666, 421.50639820098877, 425.9520752429962, 430.3433184623718, 434.7960629463196, 439.2289412021637, 443.5839698314667, 448.1915428638458, 458.64497780799866]
[15.116666666666667, 23.033333333333335, 30.1, 33.016666666666666, 34.833333333333336, 41.46666666666667, 38.8, 46.333333333333336, 47.416666666666664, 50.78333333333333, 47.2, 48.13333333333333, 48.35, 46.61666666666667, 50.81666666666667, 52.75, 55.266666666666666, 55.06666666666667, 55.7, 49.983333333333334, 51.1, 52.666666666666664, 54.86666666666667, 55.916666666666664, 49.9, 51.5, 51.25, 53.45, 52.166666666666664, 52.78333333333333, 53.583333333333336, 53.21666666666667, 50.733333333333334, 52.78333333333333, 55.43333333333333, 55.68333333333333, 55.46666666666667, 57.06666666666667, 56.03333333333333, 56.3, 56.75, 56.61666666666667, 53.583333333333336, 52.7, 53.833333333333336, 57.416666666666664, 59.7, 55.3, 55.9, 54.7, 54.71666666666667, 54.916666666666664, 53.28333333333333, 53.416666666666664, 55.583333333333336, 54.45, 55.65, 55.93333333333333, 56.666666666666664, 57.8, 61.95, 60.266666666666666, 57.85, 57.25, 57.28333333333333, 59.96666666666667, 60.93333333333333, 60.2, 59.65, 60.45, 61.71666666666667, 61.483333333333334, 58.9, 58.18333333333333, 60.6, 57.666666666666664, 60.03333333333333, 61.2, 60.583333333333336, 61.983333333333334, 62.18333333333333, 60.63333333333333, 58.666666666666664, 59.25, 58.03333333333333, 60.3, 60.38333333333333, 58.85, 57.13333333333333, 57.11666666666667, 56.75, 58.78333333333333, 57.86666666666667, 60.85, 60.68333333333333, 61.916666666666664, 62.93333333333333, 60.55, 60.016666666666666, 60.5, 64.73333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: center_psl, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.7000
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.1833
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.6500
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0267
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4367
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5767
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0067
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.5067
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.3933
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6167
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.422, Test loss: 2.035, Test accuracy: 25.47
Round   1, Train loss: 1.110, Test loss: 1.841, Test accuracy: 34.22
Round   2, Train loss: 0.963, Test loss: 1.799, Test accuracy: 30.98
Round   3, Train loss: 0.940, Test loss: 1.883, Test accuracy: 37.62
Round   4, Train loss: 0.960, Test loss: 1.572, Test accuracy: 38.77
Round   5, Train loss: 0.761, Test loss: 1.538, Test accuracy: 40.58
Round   6, Train loss: 0.982, Test loss: 1.613, Test accuracy: 40.00
Round   7, Train loss: 0.805, Test loss: 1.315, Test accuracy: 47.32
Round   8, Train loss: 0.806, Test loss: 1.135, Test accuracy: 52.12
Round   9, Train loss: 0.764, Test loss: 1.192, Test accuracy: 51.87
Round  10, Train loss: 0.946, Test loss: 1.196, Test accuracy: 52.35
Round  11, Train loss: 0.751, Test loss: 1.113, Test accuracy: 51.13
Round  12, Train loss: 0.872, Test loss: 1.185, Test accuracy: 49.62
Round  13, Train loss: 0.946, Test loss: 1.284, Test accuracy: 45.58
Round  14, Train loss: 0.712, Test loss: 1.122, Test accuracy: 50.93
Round  15, Train loss: 0.735, Test loss: 1.075, Test accuracy: 52.82
Round  16, Train loss: 0.633, Test loss: 1.097, Test accuracy: 54.37
Round  17, Train loss: 0.572, Test loss: 1.056, Test accuracy: 56.57
Round  18, Train loss: 0.580, Test loss: 1.131, Test accuracy: 54.73
Round  19, Train loss: 0.750, Test loss: 1.202, Test accuracy: 53.07
Round  20, Train loss: 0.731, Test loss: 1.173, Test accuracy: 54.12
Round  21, Train loss: 0.622, Test loss: 1.188, Test accuracy: 51.58
Round  22, Train loss: 0.471, Test loss: 1.154, Test accuracy: 56.32
Round  23, Train loss: 0.508, Test loss: 1.093, Test accuracy: 58.23
Round  24, Train loss: 0.705, Test loss: 1.034, Test accuracy: 58.08
Round  25, Train loss: 0.569, Test loss: 1.054, Test accuracy: 58.57
Round  26, Train loss: 0.623, Test loss: 1.032, Test accuracy: 58.52
Round  27, Train loss: 0.489, Test loss: 0.969, Test accuracy: 61.07
Round  28, Train loss: 0.557, Test loss: 0.979, Test accuracy: 60.78
Round  29, Train loss: 0.633, Test loss: 1.021, Test accuracy: 58.05
Round  30, Train loss: 0.555, Test loss: 0.945, Test accuracy: 59.08
Round  31, Train loss: 0.515, Test loss: 0.980, Test accuracy: 57.45
Round  32, Train loss: 0.641, Test loss: 1.058, Test accuracy: 54.82
Round  33, Train loss: 0.540, Test loss: 1.035, Test accuracy: 55.97
Round  34, Train loss: 0.390, Test loss: 0.979, Test accuracy: 58.10
Round  35, Train loss: 0.371, Test loss: 0.973, Test accuracy: 58.53
Round  36, Train loss: 0.387, Test loss: 1.127, Test accuracy: 56.38
Round  37, Train loss: 0.406, Test loss: 1.116, Test accuracy: 56.97
Round  38, Train loss: 0.375, Test loss: 1.134, Test accuracy: 57.97
Round  39, Train loss: 0.407, Test loss: 1.093, Test accuracy: 57.23
Round  40, Train loss: 0.418, Test loss: 1.157, Test accuracy: 57.23
Round  41, Train loss: 0.448, Test loss: 1.184, Test accuracy: 55.55
Round  42, Train loss: 0.374, Test loss: 1.146, Test accuracy: 56.67
Round  43, Train loss: 0.386, Test loss: 1.191, Test accuracy: 55.40
Round  44, Train loss: 0.385, Test loss: 1.242, Test accuracy: 53.12
Round  45, Train loss: 0.492, Test loss: 1.245, Test accuracy: 52.70
Round  46, Train loss: 0.302, Test loss: 1.133, Test accuracy: 57.33
Round  47, Train loss: 0.336, Test loss: 1.138, Test accuracy: 55.98
Round  48, Train loss: 0.334, Test loss: 1.137, Test accuracy: 56.52
Round  49, Train loss: 0.354, Test loss: 1.040, Test accuracy: 58.00
Round  50, Train loss: 0.399, Test loss: 1.026, Test accuracy: 58.23
Round  51, Train loss: 0.407, Test loss: 1.011, Test accuracy: 58.73
Round  52, Train loss: 0.242, Test loss: 0.950, Test accuracy: 61.38
Round  53, Train loss: 0.395, Test loss: 1.029, Test accuracy: 58.57
Round  54, Train loss: 0.285, Test loss: 0.974, Test accuracy: 62.03
Round  55, Train loss: 0.318, Test loss: 1.005, Test accuracy: 60.52
Round  56, Train loss: 0.285, Test loss: 1.006, Test accuracy: 60.32
Round  57, Train loss: 0.249, Test loss: 1.037, Test accuracy: 59.33
Round  58, Train loss: 0.242, Test loss: 1.011, Test accuracy: 61.15
Round  59, Train loss: 0.326, Test loss: 1.070, Test accuracy: 58.25
Round  60, Train loss: 0.291, Test loss: 0.964, Test accuracy: 61.73
Round  61, Train loss: 0.310, Test loss: 1.069, Test accuracy: 59.38
Round  62, Train loss: 0.275, Test loss: 1.096, Test accuracy: 58.07
Round  63, Train loss: 0.254, Test loss: 1.047, Test accuracy: 59.07
Round  64, Train loss: 0.260, Test loss: 1.101, Test accuracy: 59.33
Round  65, Train loss: 0.260, Test loss: 1.116, Test accuracy: 58.25
Round  66, Train loss: 0.252, Test loss: 1.075, Test accuracy: 58.95
Round  67, Train loss: 0.246, Test loss: 1.082, Test accuracy: 58.53
Round  68, Train loss: 0.329, Test loss: 1.099, Test accuracy: 59.03
Round  69, Train loss: 0.258, Test loss: 1.157, Test accuracy: 57.28
Round  70, Train loss: 0.175, Test loss: 1.089, Test accuracy: 59.92
Round  71, Train loss: 0.248, Test loss: 1.051, Test accuracy: 59.80
Round  72, Train loss: 0.300, Test loss: 1.063, Test accuracy: 59.53
Round  73, Train loss: 0.297, Test loss: 1.051, Test accuracy: 59.78
Round  74, Train loss: 0.274, Test loss: 1.034, Test accuracy: 60.88
Round  75, Train loss: 0.275, Test loss: 1.050, Test accuracy: 60.08
Round  76, Train loss: 0.187, Test loss: 1.092, Test accuracy: 59.52
Round  77, Train loss: 0.237, Test loss: 1.132, Test accuracy: 58.53
Round  78, Train loss: 0.246, Test loss: 1.134, Test accuracy: 57.87
Round  79, Train loss: 0.178, Test loss: 1.141, Test accuracy: 57.48
Round  80, Train loss: 0.227, Test loss: 1.165, Test accuracy: 56.67
Round  81, Train loss: 0.268, Test loss: 1.131, Test accuracy: 58.13
Round  82, Train loss: 0.225, Test loss: 1.176, Test accuracy: 57.40
Round  83, Train loss: 0.170, Test loss: 1.134, Test accuracy: 58.15
Round  84, Train loss: 0.228, Test loss: 1.215, Test accuracy: 56.58
Round  85, Train loss: 0.164, Test loss: 1.167, Test accuracy: 58.35
Round  86, Train loss: 0.224, Test loss: 1.158, Test accuracy: 59.43
Round  87, Train loss: 0.200, Test loss: 1.188, Test accuracy: 58.05
Round  88, Train loss: 0.259, Test loss: 1.246, Test accuracy: 56.25
Round  89, Train loss: 0.201, Test loss: 1.122, Test accuracy: 59.88
Round  90, Train loss: 0.186, Test loss: 1.065, Test accuracy: 60.83
Round  91, Train loss: 0.205, Test loss: 1.086, Test accuracy: 60.87
Round  92, Train loss: 0.222, Test loss: 1.074, Test accuracy: 60.53
Round  93, Train loss: 0.145, Test loss: 1.103, Test accuracy: 60.77
Round  94, Train loss: 0.152, Test loss: 1.092, Test accuracy: 61.23
Round  95, Train loss: 0.220, Test loss: 1.149, Test accuracy: 60.02
Round  96, Train loss: 0.191, Test loss: 1.114, Test accuracy: 61.15
Round  97, Train loss: 0.145, Test loss: 1.078, Test accuracy: 62.13
Round  98, Train loss: 0.142, Test loss: 1.052, Test accuracy: 62.18
Round  99, Train loss: 0.212, Test loss: 1.124, Test accuracy: 59.45
Final Round, Train loss: 0.223, Test loss: 1.045, Test accuracy: 63.55
Average accuracy final 10 rounds: 60.91666666666667
3294.9312241077423
[7.385079860687256, 12.805887460708618, 18.23405647277832, 23.009617805480957, 27.86058497428894, 32.631651401519775, 37.48708176612854, 42.253719329833984, 47.00618553161621, 51.753514528274536, 56.62372088432312, 61.3834810256958, 66.13812685012817, 71.0292158126831, 75.85009837150574, 80.68259263038635, 85.53045201301575, 90.27111434936523, 94.99621963500977, 99.72517561912537, 104.51704812049866, 109.25020289421082, 114.0606210231781, 118.7685489654541, 124.1693799495697, 129.57456135749817, 134.98217177391052, 140.39572548866272, 145.18179416656494, 149.9314250946045, 154.75724244117737, 159.48646306991577, 164.20368719100952, 169.0486662387848, 173.3808536529541, 178.27557587623596, 183.06979203224182, 187.76564359664917, 192.45556259155273, 197.26190543174744, 202.61345505714417, 208.07291293144226, 213.4533052444458, 218.8611545562744, 224.34764790534973, 229.70021557807922, 234.48888969421387, 239.1865348815918, 243.85582780838013, 248.55505514144897, 253.2855944633484, 257.9638526439667, 262.6642849445343, 267.34063744544983, 272.0501489639282, 276.76322317123413, 281.5939953327179, 286.43098402023315, 291.2794270515442, 296.06904888153076, 300.96831154823303, 305.76493310928345, 310.55954098701477, 315.26481461524963, 320.07286643981934, 324.85585927963257, 329.6725175380707, 334.37394070625305, 339.0628318786621, 344.0079724788666, 348.94954657554626, 353.7208969593048, 358.51640605926514, 363.1991424560547, 367.9081060886383, 372.6092231273651, 377.3138768672943, 382.0660002231598, 387.00697112083435, 391.70532155036926, 396.3886013031006, 401.16805267333984, 405.8937990665436, 410.67584586143494, 415.38010263442993, 420.0913288593292, 424.78889298439026, 429.50252079963684, 434.18016719818115, 438.8403146266937, 443.49514174461365, 448.13440465927124, 452.7836847305298, 457.56834268569946, 462.27166652679443, 466.9170026779175, 471.61142110824585, 476.36103439331055, 481.00109219551086, 485.6896514892578, 499.1883451938629]
[25.466666666666665, 34.21666666666667, 30.983333333333334, 37.61666666666667, 38.766666666666666, 40.583333333333336, 40.0, 47.31666666666667, 52.11666666666667, 51.86666666666667, 52.35, 51.13333333333333, 49.61666666666667, 45.583333333333336, 50.93333333333333, 52.81666666666667, 54.36666666666667, 56.56666666666667, 54.733333333333334, 53.06666666666667, 54.11666666666667, 51.583333333333336, 56.31666666666667, 58.233333333333334, 58.083333333333336, 58.56666666666667, 58.516666666666666, 61.06666666666667, 60.78333333333333, 58.05, 59.083333333333336, 57.45, 54.81666666666667, 55.96666666666667, 58.1, 58.53333333333333, 56.38333333333333, 56.96666666666667, 57.96666666666667, 57.233333333333334, 57.233333333333334, 55.55, 56.666666666666664, 55.4, 53.11666666666667, 52.7, 57.333333333333336, 55.983333333333334, 56.516666666666666, 58.0, 58.233333333333334, 58.733333333333334, 61.38333333333333, 58.56666666666667, 62.03333333333333, 60.516666666666666, 60.31666666666667, 59.333333333333336, 61.15, 58.25, 61.733333333333334, 59.38333333333333, 58.06666666666667, 59.06666666666667, 59.333333333333336, 58.25, 58.95, 58.53333333333333, 59.03333333333333, 57.28333333333333, 59.916666666666664, 59.8, 59.53333333333333, 59.78333333333333, 60.88333333333333, 60.083333333333336, 59.516666666666666, 58.53333333333333, 57.86666666666667, 57.483333333333334, 56.666666666666664, 58.13333333333333, 57.4, 58.15, 56.583333333333336, 58.35, 59.43333333333333, 58.05, 56.25, 59.88333333333333, 60.833333333333336, 60.86666666666667, 60.53333333333333, 60.766666666666666, 61.233333333333334, 60.016666666666666, 61.15, 62.13333333333333, 62.18333333333333, 59.45, 63.55]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.7067
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.4367
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6333
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3400
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5300
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.5900
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3567
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.6367
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.5467
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6767
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.739, Test loss: 2.119, Test accuracy: 25.20
Round   1, Train loss: 1.226, Test loss: 2.127, Test accuracy: 22.60
Round   2, Train loss: 1.124, Test loss: 1.732, Test accuracy: 27.08
Round   3, Train loss: 1.208, Test loss: 1.587, Test accuracy: 37.20
Round   4, Train loss: 1.105, Test loss: 1.337, Test accuracy: 41.07
Round   5, Train loss: 1.028, Test loss: 1.347, Test accuracy: 40.62
Round   6, Train loss: 1.157, Test loss: 1.182, Test accuracy: 43.23
Round   7, Train loss: 1.083, Test loss: 1.053, Test accuracy: 47.15
Round   8, Train loss: 0.993, Test loss: 1.040, Test accuracy: 49.33
Round   9, Train loss: 1.074, Test loss: 1.024, Test accuracy: 50.78
Round  10, Train loss: 0.926, Test loss: 1.006, Test accuracy: 50.83
Round  11, Train loss: 1.090, Test loss: 1.012, Test accuracy: 49.17
Round  12, Train loss: 0.907, Test loss: 0.990, Test accuracy: 52.32
Round  13, Train loss: 1.028, Test loss: 0.987, Test accuracy: 52.75
Round  14, Train loss: 0.945, Test loss: 0.977, Test accuracy: 52.95
Round  15, Train loss: 0.960, Test loss: 0.989, Test accuracy: 52.18
Round  16, Train loss: 1.093, Test loss: 0.967, Test accuracy: 53.68
Round  17, Train loss: 0.943, Test loss: 0.962, Test accuracy: 54.18
Round  18, Train loss: 1.036, Test loss: 0.945, Test accuracy: 56.32
Round  19, Train loss: 0.946, Test loss: 0.962, Test accuracy: 54.48
Round  20, Train loss: 1.060, Test loss: 0.962, Test accuracy: 54.48
Round  21, Train loss: 1.010, Test loss: 0.948, Test accuracy: 55.17
Round  22, Train loss: 0.931, Test loss: 0.945, Test accuracy: 55.63
Round  23, Train loss: 0.935, Test loss: 0.931, Test accuracy: 56.12
Round  24, Train loss: 1.082, Test loss: 0.938, Test accuracy: 54.68
Round  25, Train loss: 0.901, Test loss: 0.927, Test accuracy: 55.47
Round  26, Train loss: 1.077, Test loss: 0.919, Test accuracy: 56.87
Round  27, Train loss: 0.943, Test loss: 0.916, Test accuracy: 55.75
Round  28, Train loss: 0.938, Test loss: 0.917, Test accuracy: 58.20
Round  29, Train loss: 0.915, Test loss: 0.894, Test accuracy: 58.95
Round  30, Train loss: 0.932, Test loss: 0.890, Test accuracy: 59.95
Round  31, Train loss: 0.973, Test loss: 0.905, Test accuracy: 59.70
Round  32, Train loss: 0.916, Test loss: 0.900, Test accuracy: 58.58
Round  33, Train loss: 1.007, Test loss: 0.901, Test accuracy: 58.50
Round  34, Train loss: 1.075, Test loss: 0.894, Test accuracy: 58.70
Round  35, Train loss: 0.842, Test loss: 0.885, Test accuracy: 58.88
Round  36, Train loss: 0.964, Test loss: 0.879, Test accuracy: 59.57
Round  37, Train loss: 0.880, Test loss: 0.875, Test accuracy: 59.68
Round  38, Train loss: 0.863, Test loss: 0.882, Test accuracy: 60.05
Round  39, Train loss: 0.937, Test loss: 0.876, Test accuracy: 59.80
Round  40, Train loss: 0.927, Test loss: 0.878, Test accuracy: 58.38
Round  41, Train loss: 0.805, Test loss: 0.879, Test accuracy: 58.52
Round  42, Train loss: 0.885, Test loss: 0.878, Test accuracy: 59.87
Round  43, Train loss: 0.836, Test loss: 0.879, Test accuracy: 59.17
Round  44, Train loss: 0.958, Test loss: 0.868, Test accuracy: 60.15
Round  45, Train loss: 0.922, Test loss: 0.864, Test accuracy: 61.00
Round  46, Train loss: 0.948, Test loss: 0.862, Test accuracy: 60.27
Round  47, Train loss: 0.812, Test loss: 0.858, Test accuracy: 60.45
Round  48, Train loss: 0.893, Test loss: 0.856, Test accuracy: 60.27
Round  49, Train loss: 0.861, Test loss: 0.854, Test accuracy: 60.95
Round  50, Train loss: 0.777, Test loss: 0.853, Test accuracy: 61.47
Round  51, Train loss: 0.835, Test loss: 0.857, Test accuracy: 60.72
Round  52, Train loss: 0.900, Test loss: 0.850, Test accuracy: 60.80
Round  53, Train loss: 0.772, Test loss: 0.843, Test accuracy: 61.45
Round  54, Train loss: 0.750, Test loss: 0.836, Test accuracy: 61.90
Round  55, Train loss: 0.926, Test loss: 0.834, Test accuracy: 62.72
Round  56, Train loss: 0.726, Test loss: 0.838, Test accuracy: 61.97
Round  57, Train loss: 0.815, Test loss: 0.850, Test accuracy: 60.68
Round  58, Train loss: 0.821, Test loss: 0.844, Test accuracy: 61.63
Round  59, Train loss: 0.979, Test loss: 0.848, Test accuracy: 61.40
Round  60, Train loss: 0.846, Test loss: 0.845, Test accuracy: 61.62
Round  61, Train loss: 0.758, Test loss: 0.844, Test accuracy: 61.22
Round  62, Train loss: 0.861, Test loss: 0.833, Test accuracy: 61.67
Round  63, Train loss: 0.694, Test loss: 0.840, Test accuracy: 61.53
Round  64, Train loss: 0.737, Test loss: 0.840, Test accuracy: 60.97
Round  65, Train loss: 0.810, Test loss: 0.839, Test accuracy: 60.93
Round  66, Train loss: 0.892, Test loss: 0.841, Test accuracy: 60.77
Round  67, Train loss: 0.860, Test loss: 0.836, Test accuracy: 61.97
Round  68, Train loss: 0.685, Test loss: 0.834, Test accuracy: 62.33
Round  69, Train loss: 0.801, Test loss: 0.839, Test accuracy: 61.90
Round  70, Train loss: 0.929, Test loss: 0.837, Test accuracy: 62.38
Round  71, Train loss: 0.781, Test loss: 0.834, Test accuracy: 62.03
Round  72, Train loss: 0.617, Test loss: 0.830, Test accuracy: 62.05
Round  73, Train loss: 0.752, Test loss: 0.831, Test accuracy: 62.45
Round  74, Train loss: 0.684, Test loss: 0.836, Test accuracy: 61.87
Round  75, Train loss: 0.663, Test loss: 0.840, Test accuracy: 61.38
Round  76, Train loss: 0.764, Test loss: 0.843, Test accuracy: 61.00
Round  77, Train loss: 0.911, Test loss: 0.833, Test accuracy: 62.28
Round  78, Train loss: 0.864, Test loss: 0.841, Test accuracy: 61.37
Round  79, Train loss: 1.016, Test loss: 0.843, Test accuracy: 60.58
Round  80, Train loss: 0.794, Test loss: 0.845, Test accuracy: 60.78
Round  81, Train loss: 0.766, Test loss: 0.857, Test accuracy: 60.30
Round  82, Train loss: 0.614, Test loss: 0.847, Test accuracy: 60.98
Round  83, Train loss: 0.486, Test loss: 0.847, Test accuracy: 60.77
Round  84, Train loss: 0.662, Test loss: 0.834, Test accuracy: 61.62
Round  85, Train loss: 0.767, Test loss: 0.830, Test accuracy: 61.47
Round  86, Train loss: 0.779, Test loss: 0.838, Test accuracy: 61.13
Round  87, Train loss: 0.789, Test loss: 0.831, Test accuracy: 62.03
Round  88, Train loss: 0.721, Test loss: 0.834, Test accuracy: 61.82
Round  89, Train loss: 0.748, Test loss: 0.838, Test accuracy: 61.55
Round  90, Train loss: 0.724, Test loss: 0.840, Test accuracy: 61.40
Round  91, Train loss: 0.709, Test loss: 0.834, Test accuracy: 62.15
Round  92, Train loss: 0.544, Test loss: 0.829, Test accuracy: 62.52
Round  93, Train loss: 0.651, Test loss: 0.827, Test accuracy: 62.75
Round  94, Train loss: 0.779, Test loss: 0.832, Test accuracy: 62.03
Round  95, Train loss: 0.846, Test loss: 0.835, Test accuracy: 61.83
Round  96, Train loss: 0.882, Test loss: 0.841, Test accuracy: 61.40
Round  97, Train loss: 0.680, Test loss: 0.840, Test accuracy: 61.33
Round  98, Train loss: 0.681, Test loss: 0.836, Test accuracy: 61.95
Round  99, Train loss: 0.844, Test loss: 0.842, Test accuracy: 61.37
Final Round, Train loss: 0.684, Test loss: 0.850, Test accuracy: 60.83
Average accuracy final 10 rounds: 61.873333333333335
870.0818150043488
[3.0489063262939453, 4.138705492019653, 5.235168933868408, 6.3332743644714355, 7.427468299865723, 8.52506709098816, 9.623497009277344, 10.72300672531128, 11.822778701782227, 12.918161630630493, 14.012478828430176, 15.106249570846558, 16.201454639434814, 17.298892974853516, 18.410053491592407, 19.499351739883423, 20.59760808944702, 21.689951181411743, 22.802600622177124, 23.89319658279419, 24.983320236206055, 26.14377737045288, 27.22946786880493, 28.31959891319275, 29.439481019973755, 30.526506185531616, 31.618217706680298, 32.71502494812012, 33.836612939834595, 34.93147540092468, 36.017831563949585, 37.108208417892456, 38.199809074401855, 39.292797327041626, 40.387133836746216, 41.484620809555054, 42.57599234580994, 43.670138120651245, 44.764211893081665, 45.85510730743408, 46.947293519973755, 48.03971600532532, 49.136996269226074, 50.23344612121582, 51.334028482437134, 52.426841497421265, 53.519755363464355, 54.80922031402588, 55.89424800872803, 56.9913444519043, 58.08199763298035, 59.16808581352234, 60.41067147254944, 61.50230574607849, 62.59083080291748, 63.697213649749756, 64.78334069252014, 65.86806607246399, 66.95766758918762, 68.03481578826904, 69.1171875, 70.2024712562561, 71.29871582984924, 72.41979050636292, 73.53951454162598, 74.62421751022339, 75.71569299697876, 76.80128026008606, 77.88876748085022, 78.97764992713928, 80.06400275230408, 81.15232133865356, 82.23517537117004, 83.32456302642822, 84.41366863250732, 85.49803137779236, 86.5805242061615, 87.66473865509033, 88.75109124183655, 89.83786153793335, 90.91927814483643, 92.01062965393066, 93.10012078285217, 94.1908643245697, 95.27737259864807, 96.36077284812927, 97.50238704681396, 98.59061431884766, 99.67514824867249, 100.75775957107544, 101.84464240074158, 102.92839193344116, 104.0117609500885, 105.1307384967804, 106.24648427963257, 107.33179235458374, 108.5432596206665, 109.7642343044281, 110.99174785614014, 112.21406245231628, 114.01353907585144]
[25.2, 22.6, 27.083333333333332, 37.2, 41.06666666666667, 40.61666666666667, 43.233333333333334, 47.15, 49.333333333333336, 50.78333333333333, 50.833333333333336, 49.166666666666664, 52.31666666666667, 52.75, 52.95, 52.18333333333333, 53.68333333333333, 54.18333333333333, 56.31666666666667, 54.483333333333334, 54.483333333333334, 55.166666666666664, 55.63333333333333, 56.11666666666667, 54.68333333333333, 55.46666666666667, 56.86666666666667, 55.75, 58.2, 58.95, 59.95, 59.7, 58.583333333333336, 58.5, 58.7, 58.88333333333333, 59.56666666666667, 59.68333333333333, 60.05, 59.8, 58.38333333333333, 58.516666666666666, 59.86666666666667, 59.166666666666664, 60.15, 61.0, 60.266666666666666, 60.45, 60.266666666666666, 60.95, 61.46666666666667, 60.71666666666667, 60.8, 61.45, 61.9, 62.71666666666667, 61.96666666666667, 60.68333333333333, 61.63333333333333, 61.4, 61.61666666666667, 61.21666666666667, 61.666666666666664, 61.53333333333333, 60.96666666666667, 60.93333333333333, 60.766666666666666, 61.96666666666667, 62.333333333333336, 61.9, 62.38333333333333, 62.03333333333333, 62.05, 62.45, 61.86666666666667, 61.38333333333333, 61.0, 62.28333333333333, 61.36666666666667, 60.583333333333336, 60.78333333333333, 60.3, 60.983333333333334, 60.766666666666666, 61.61666666666667, 61.46666666666667, 61.13333333333333, 62.03333333333333, 61.81666666666667, 61.55, 61.4, 62.15, 62.516666666666666, 62.75, 62.03333333333333, 61.833333333333336, 61.4, 61.333333333333336, 61.95, 61.36666666666667, 60.833333333333336]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.7067
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.4367
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6333
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3400
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5300
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.5900
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3567
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.6367
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.5467
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6767
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.757, Test loss: 2.187, Test accuracy: 25.07
Round   1, Train loss: 1.233, Test loss: 2.206, Test accuracy: 21.80
Round   2, Train loss: 1.111, Test loss: 1.773, Test accuracy: 25.98
Round   3, Train loss: 1.201, Test loss: 1.668, Test accuracy: 34.87
Round   4, Train loss: 1.123, Test loss: 1.403, Test accuracy: 36.57
Round   5, Train loss: 1.038, Test loss: 1.390, Test accuracy: 36.03
Round   6, Train loss: 1.142, Test loss: 1.239, Test accuracy: 38.60
Round   7, Train loss: 1.077, Test loss: 1.127, Test accuracy: 43.18
Round   8, Train loss: 0.987, Test loss: 1.115, Test accuracy: 44.20
Round   9, Train loss: 1.061, Test loss: 1.104, Test accuracy: 45.53
Round  10, Train loss: 0.935, Test loss: 1.079, Test accuracy: 46.65
Round  11, Train loss: 1.055, Test loss: 1.071, Test accuracy: 48.48
Round  12, Train loss: 0.929, Test loss: 1.040, Test accuracy: 49.93
Round  13, Train loss: 0.986, Test loss: 1.053, Test accuracy: 49.28
Round  14, Train loss: 0.924, Test loss: 1.026, Test accuracy: 49.80
Round  15, Train loss: 0.922, Test loss: 1.030, Test accuracy: 50.63
Round  16, Train loss: 1.069, Test loss: 1.013, Test accuracy: 52.45
Round  17, Train loss: 0.928, Test loss: 1.001, Test accuracy: 52.83
Round  18, Train loss: 0.993, Test loss: 0.980, Test accuracy: 54.20
Round  19, Train loss: 0.922, Test loss: 0.985, Test accuracy: 54.13
Round  20, Train loss: 1.011, Test loss: 1.016, Test accuracy: 53.68
Round  21, Train loss: 0.980, Test loss: 0.983, Test accuracy: 55.48
Round  22, Train loss: 0.911, Test loss: 0.996, Test accuracy: 55.58
Round  23, Train loss: 0.906, Test loss: 0.970, Test accuracy: 56.13
Round  24, Train loss: 1.050, Test loss: 1.008, Test accuracy: 53.05
Round  25, Train loss: 0.862, Test loss: 0.991, Test accuracy: 53.30
Round  26, Train loss: 1.031, Test loss: 0.978, Test accuracy: 53.58
Round  27, Train loss: 0.924, Test loss: 0.974, Test accuracy: 53.07
Round  28, Train loss: 0.903, Test loss: 0.981, Test accuracy: 53.48
Round  29, Train loss: 0.840, Test loss: 0.973, Test accuracy: 53.52
Round  30, Train loss: 0.896, Test loss: 0.973, Test accuracy: 54.37
Round  31, Train loss: 0.945, Test loss: 0.974, Test accuracy: 54.17
Round  32, Train loss: 0.856, Test loss: 0.979, Test accuracy: 54.22
Round  33, Train loss: 0.933, Test loss: 0.991, Test accuracy: 53.77
Round  34, Train loss: 1.016, Test loss: 0.983, Test accuracy: 54.37
Round  35, Train loss: 0.794, Test loss: 0.977, Test accuracy: 54.65
Round  36, Train loss: 0.911, Test loss: 0.978, Test accuracy: 53.85
Round  37, Train loss: 0.838, Test loss: 0.971, Test accuracy: 54.58
Round  38, Train loss: 0.820, Test loss: 0.963, Test accuracy: 55.23
Round  39, Train loss: 0.882, Test loss: 0.950, Test accuracy: 56.60
Round  40, Train loss: 0.859, Test loss: 0.947, Test accuracy: 56.52
Round  41, Train loss: 0.760, Test loss: 0.955, Test accuracy: 56.83
Round  42, Train loss: 0.833, Test loss: 0.950, Test accuracy: 57.33
Round  43, Train loss: 0.800, Test loss: 0.958, Test accuracy: 56.45
Round  44, Train loss: 0.912, Test loss: 0.939, Test accuracy: 56.62
Round  45, Train loss: 0.868, Test loss: 0.951, Test accuracy: 57.47
Round  46, Train loss: 0.893, Test loss: 0.966, Test accuracy: 57.80
Round  47, Train loss: 0.776, Test loss: 0.953, Test accuracy: 57.48
Round  48, Train loss: 0.835, Test loss: 0.959, Test accuracy: 57.60
Round  49, Train loss: 0.832, Test loss: 0.946, Test accuracy: 56.57
Round  50, Train loss: 0.732, Test loss: 0.941, Test accuracy: 56.10
Round  51, Train loss: 0.816, Test loss: 0.932, Test accuracy: 56.52
Round  52, Train loss: 0.853, Test loss: 0.923, Test accuracy: 57.35
Round  53, Train loss: 0.738, Test loss: 0.916, Test accuracy: 57.68
Round  54, Train loss: 0.719, Test loss: 0.919, Test accuracy: 57.80
Round  55, Train loss: 0.883, Test loss: 0.933, Test accuracy: 58.00
Round  56, Train loss: 0.697, Test loss: 0.915, Test accuracy: 58.20
Round  57, Train loss: 0.782, Test loss: 0.939, Test accuracy: 57.33
Round  58, Train loss: 0.776, Test loss: 0.925, Test accuracy: 58.48
Round  59, Train loss: 0.932, Test loss: 0.917, Test accuracy: 59.87
Round  60, Train loss: 0.809, Test loss: 0.918, Test accuracy: 59.68
Round  61, Train loss: 0.734, Test loss: 0.932, Test accuracy: 58.57
Round  62, Train loss: 0.819, Test loss: 0.929, Test accuracy: 58.92
Round  63, Train loss: 0.646, Test loss: 0.923, Test accuracy: 59.57
Round  64, Train loss: 0.711, Test loss: 0.901, Test accuracy: 59.78
Round  65, Train loss: 0.770, Test loss: 0.939, Test accuracy: 58.10
Round  66, Train loss: 0.843, Test loss: 0.939, Test accuracy: 58.30
Round  67, Train loss: 0.824, Test loss: 0.945, Test accuracy: 58.20
Round  68, Train loss: 0.648, Test loss: 0.935, Test accuracy: 57.42
Round  69, Train loss: 0.769, Test loss: 0.939, Test accuracy: 57.15
Round  70, Train loss: 0.881, Test loss: 0.951, Test accuracy: 57.63
Round  71, Train loss: 0.772, Test loss: 0.942, Test accuracy: 58.02
Round  72, Train loss: 0.594, Test loss: 0.952, Test accuracy: 57.70
Round  73, Train loss: 0.697, Test loss: 0.955, Test accuracy: 57.28
Round  74, Train loss: 0.655, Test loss: 0.935, Test accuracy: 58.30
Round  75, Train loss: 0.653, Test loss: 0.911, Test accuracy: 59.23
Round  76, Train loss: 0.731, Test loss: 0.899, Test accuracy: 59.00
Round  77, Train loss: 0.860, Test loss: 0.902, Test accuracy: 59.67
Round  78, Train loss: 0.832, Test loss: 0.920, Test accuracy: 57.93
Round  79, Train loss: 0.974, Test loss: 0.950, Test accuracy: 57.00
Round  80, Train loss: 0.759, Test loss: 0.953, Test accuracy: 57.63
Round  81, Train loss: 0.715, Test loss: 0.957, Test accuracy: 57.82
Round  82, Train loss: 0.590, Test loss: 0.902, Test accuracy: 59.25
Round  83, Train loss: 0.488, Test loss: 0.901, Test accuracy: 59.15
Round  84, Train loss: 0.657, Test loss: 0.901, Test accuracy: 58.98
Round  85, Train loss: 0.759, Test loss: 0.909, Test accuracy: 59.15
Round  86, Train loss: 0.744, Test loss: 0.942, Test accuracy: 58.58
Round  87, Train loss: 0.768, Test loss: 0.911, Test accuracy: 59.28
Round  88, Train loss: 0.699, Test loss: 0.923, Test accuracy: 58.68
Round  89, Train loss: 0.709, Test loss: 0.912, Test accuracy: 58.52
Round  90, Train loss: 0.724, Test loss: 0.912, Test accuracy: 59.30
Round  91, Train loss: 0.717, Test loss: 0.898, Test accuracy: 59.87
Round  92, Train loss: 0.558, Test loss: 0.912, Test accuracy: 59.60
Round  93, Train loss: 0.625, Test loss: 0.921, Test accuracy: 59.47
Round  94, Train loss: 0.754, Test loss: 0.926, Test accuracy: 59.28
Round  95, Train loss: 0.818, Test loss: 0.923, Test accuracy: 60.05
Round  96, Train loss: 0.835, Test loss: 0.950, Test accuracy: 58.47
Round  97, Train loss: 0.662, Test loss: 0.926, Test accuracy: 58.90
Round  98, Train loss: 0.653, Test loss: 0.901, Test accuracy: 59.83
Round  99, Train loss: 0.833, Test loss: 0.904, Test accuracy: 59.68
Final Round, Train loss: 0.678, Test loss: 0.925, Test accuracy: 59.45
Average accuracy final 10 rounds: 59.445
1666.8084111213684
[4.3100292682647705, 6.638807058334351, 8.96434998512268, 11.291280508041382, 13.630380868911743, 15.954140901565552, 18.27125096321106, 20.593167781829834, 22.911989212036133, 25.2274112701416, 27.551525592803955, 29.865813970565796, 32.18613791465759, 34.44877576828003, 36.74975919723511, 39.01435613632202, 41.31424880027771, 43.681910037994385, 46.019704818725586, 48.33427143096924, 50.710426568984985, 53.09723401069641, 55.48971772193909, 57.88071966171265, 60.267398834228516, 62.581695795059204, 64.89677715301514, 67.21120357513428, 69.85465216636658, 72.4937973022461, 75.12662506103516, 77.76322913169861, 80.39596509933472, 83.03379154205322, 85.66962933540344, 88.29530835151672, 90.93464994430542, 93.56894969940186, 96.20697617530823, 98.8468849658966, 101.4910876750946, 104.12374663352966, 106.76799821853638, 109.40220785140991, 112.04459571838379, 114.71878409385681, 117.33348393440247, 119.9484350681305, 122.5659852027893, 125.19131064414978, 127.68738579750061, 130.02442860603333, 132.5566909313202, 134.82391238212585, 137.08022046089172, 139.33860754966736, 141.59465384483337, 143.84962487220764, 146.1102147102356, 148.3785843849182, 150.63779020309448, 152.90454959869385, 155.16242480278015, 157.42910933494568, 159.69797658920288, 161.97948575019836, 164.27889275550842, 166.58284449577332, 168.8633120059967, 171.14559149742126, 173.4348087310791, 175.7246699333191, 178.0137505531311, 180.31335544586182, 182.59006094932556, 184.87234449386597, 187.14287900924683, 189.4849455356598, 191.7625904083252, 194.04841232299805, 196.32622909545898, 198.61231398582458, 200.87918663024902, 203.1806845664978, 205.4478669166565, 207.7778627872467, 210.07122945785522, 212.36022663116455, 214.64837408065796, 216.92956042289734, 219.21147990226746, 221.49433994293213, 223.78154969215393, 226.06344318389893, 228.34916353225708, 230.64027380943298, 232.94818997383118, 235.22435116767883, 237.5102343559265, 239.844220161438, 243.04734301567078]
[25.066666666666666, 21.8, 25.983333333333334, 34.86666666666667, 36.56666666666667, 36.03333333333333, 38.6, 43.18333333333333, 44.2, 45.53333333333333, 46.65, 48.483333333333334, 49.93333333333333, 49.28333333333333, 49.8, 50.63333333333333, 52.45, 52.833333333333336, 54.2, 54.13333333333333, 53.68333333333333, 55.483333333333334, 55.583333333333336, 56.13333333333333, 53.05, 53.3, 53.583333333333336, 53.06666666666667, 53.483333333333334, 53.516666666666666, 54.36666666666667, 54.166666666666664, 54.21666666666667, 53.766666666666666, 54.36666666666667, 54.65, 53.85, 54.583333333333336, 55.233333333333334, 56.6, 56.516666666666666, 56.833333333333336, 57.333333333333336, 56.45, 56.61666666666667, 57.46666666666667, 57.8, 57.483333333333334, 57.6, 56.56666666666667, 56.1, 56.516666666666666, 57.35, 57.68333333333333, 57.8, 58.0, 58.2, 57.333333333333336, 58.483333333333334, 59.86666666666667, 59.68333333333333, 58.56666666666667, 58.916666666666664, 59.56666666666667, 59.78333333333333, 58.1, 58.3, 58.2, 57.416666666666664, 57.15, 57.63333333333333, 58.016666666666666, 57.7, 57.28333333333333, 58.3, 59.233333333333334, 59.0, 59.666666666666664, 57.93333333333333, 57.0, 57.63333333333333, 57.81666666666667, 59.25, 59.15, 58.983333333333334, 59.15, 58.583333333333336, 59.28333333333333, 58.68333333333333, 58.516666666666666, 59.3, 59.86666666666667, 59.6, 59.46666666666667, 59.28333333333333, 60.05, 58.46666666666667, 58.9, 59.833333333333336, 59.68333333333333, 59.45]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.7067
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.4367
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6333
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3400
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5300
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.5900
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3567
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.6367
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.5467
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6767
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.028, Test loss: 2.109, Test accuracy: 25.05
Round   1, Train loss: 0.734, Test loss: 2.170, Test accuracy: 23.48
Round   2, Train loss: 0.593, Test loss: 1.726, Test accuracy: 26.60
Round   3, Train loss: 0.675, Test loss: 1.806, Test accuracy: 35.00
Round   4, Train loss: 0.615, Test loss: 1.549, Test accuracy: 35.95
Round   5, Train loss: 0.572, Test loss: 1.550, Test accuracy: 34.50
Round   6, Train loss: 0.596, Test loss: 1.403, Test accuracy: 39.88
Round   7, Train loss: 0.593, Test loss: 1.323, Test accuracy: 39.48
Round   8, Train loss: 0.539, Test loss: 1.321, Test accuracy: 39.97
Round   9, Train loss: 0.615, Test loss: 1.412, Test accuracy: 39.80
Round  10, Train loss: 0.458, Test loss: 1.237, Test accuracy: 43.80
Round  11, Train loss: 0.626, Test loss: 1.347, Test accuracy: 40.27
Round  12, Train loss: 0.470, Test loss: 1.320, Test accuracy: 40.50
Round  13, Train loss: 0.564, Test loss: 1.290, Test accuracy: 42.48
Round  14, Train loss: 0.481, Test loss: 1.313, Test accuracy: 44.75
Round  15, Train loss: 0.504, Test loss: 1.346, Test accuracy: 43.27
Round  16, Train loss: 0.588, Test loss: 1.257, Test accuracy: 44.95
Round  17, Train loss: 0.476, Test loss: 1.161, Test accuracy: 47.03
Round  18, Train loss: 0.571, Test loss: 1.145, Test accuracy: 47.97
Round  19, Train loss: 0.479, Test loss: 1.196, Test accuracy: 45.82
Round  20, Train loss: 0.544, Test loss: 1.204, Test accuracy: 45.47
Round  21, Train loss: 0.546, Test loss: 1.217, Test accuracy: 47.33
Round  22, Train loss: 0.511, Test loss: 1.227, Test accuracy: 45.28
Round  23, Train loss: 0.478, Test loss: 1.212, Test accuracy: 46.70
Round  24, Train loss: 0.605, Test loss: 1.271, Test accuracy: 45.70
Round  25, Train loss: 0.418, Test loss: 1.242, Test accuracy: 46.65
Round  26, Train loss: 0.604, Test loss: 1.240, Test accuracy: 45.12
Round  27, Train loss: 0.488, Test loss: 1.245, Test accuracy: 46.08
Round  28, Train loss: 0.502, Test loss: 1.338, Test accuracy: 45.32
Round  29, Train loss: 0.417, Test loss: 1.211, Test accuracy: 48.88
Round  30, Train loss: 0.487, Test loss: 1.262, Test accuracy: 47.72
Round  31, Train loss: 0.496, Test loss: 1.292, Test accuracy: 47.82
Round  32, Train loss: 0.409, Test loss: 1.295, Test accuracy: 48.62
Round  33, Train loss: 0.482, Test loss: 1.376, Test accuracy: 45.65
Round  34, Train loss: 0.479, Test loss: 1.285, Test accuracy: 46.57
Round  35, Train loss: 0.420, Test loss: 1.096, Test accuracy: 50.28
Round  36, Train loss: 0.445, Test loss: 1.212, Test accuracy: 48.83
Round  37, Train loss: 0.416, Test loss: 1.295, Test accuracy: 47.55
Round  38, Train loss: 0.388, Test loss: 1.324, Test accuracy: 46.57
Round  39, Train loss: 0.457, Test loss: 1.169, Test accuracy: 49.38
Round  40, Train loss: 0.398, Test loss: 1.141, Test accuracy: 48.40
Round  41, Train loss: 0.394, Test loss: 1.184, Test accuracy: 49.62
Round  42, Train loss: 0.440, Test loss: 1.246, Test accuracy: 48.58
Round  43, Train loss: 0.420, Test loss: 1.274, Test accuracy: 47.45
Round  44, Train loss: 0.458, Test loss: 1.275, Test accuracy: 48.15
Round  45, Train loss: 0.429, Test loss: 1.283, Test accuracy: 47.48
Round  46, Train loss: 0.470, Test loss: 1.305, Test accuracy: 47.95
Round  47, Train loss: 0.355, Test loss: 1.337, Test accuracy: 47.45
Round  48, Train loss: 0.412, Test loss: 1.267, Test accuracy: 48.27
Round  49, Train loss: 0.392, Test loss: 1.318, Test accuracy: 47.05
Round  50, Train loss: 0.408, Test loss: 1.261, Test accuracy: 49.75
Round  51, Train loss: 0.403, Test loss: 1.185, Test accuracy: 51.05
Round  52, Train loss: 0.442, Test loss: 1.151, Test accuracy: 52.00
Round  53, Train loss: 0.377, Test loss: 1.144, Test accuracy: 52.18
Round  54, Train loss: 0.322, Test loss: 1.128, Test accuracy: 53.13
Round  55, Train loss: 0.449, Test loss: 1.230, Test accuracy: 50.27
Round  56, Train loss: 0.310, Test loss: 1.171, Test accuracy: 53.23
Round  57, Train loss: 0.386, Test loss: 1.141, Test accuracy: 53.62
Round  58, Train loss: 0.368, Test loss: 1.204, Test accuracy: 50.92
Round  59, Train loss: 0.485, Test loss: 1.273, Test accuracy: 48.87
Round  60, Train loss: 0.349, Test loss: 1.242, Test accuracy: 49.47
Round  61, Train loss: 0.357, Test loss: 1.318, Test accuracy: 48.52
Round  62, Train loss: 0.414, Test loss: 1.212, Test accuracy: 51.87
Round  63, Train loss: 0.309, Test loss: 1.318, Test accuracy: 51.70
Round  64, Train loss: 0.294, Test loss: 1.209, Test accuracy: 52.75
Round  65, Train loss: 0.318, Test loss: 1.248, Test accuracy: 52.65
Round  66, Train loss: 0.412, Test loss: 1.245, Test accuracy: 51.47
Round  67, Train loss: 0.345, Test loss: 1.292, Test accuracy: 50.55
Round  68, Train loss: 0.246, Test loss: 1.269, Test accuracy: 51.03
Round  69, Train loss: 0.311, Test loss: 1.297, Test accuracy: 50.78
Round  70, Train loss: 0.390, Test loss: 1.379, Test accuracy: 48.07
Round  71, Train loss: 0.320, Test loss: 1.303, Test accuracy: 49.90
Round  72, Train loss: 0.251, Test loss: 1.192, Test accuracy: 52.42
Round  73, Train loss: 0.277, Test loss: 1.233, Test accuracy: 52.33
Round  74, Train loss: 0.281, Test loss: 1.153, Test accuracy: 53.08
Round  75, Train loss: 0.297, Test loss: 1.235, Test accuracy: 52.50
Round  76, Train loss: 0.306, Test loss: 1.268, Test accuracy: 51.38
Round  77, Train loss: 0.427, Test loss: 1.321, Test accuracy: 50.33
Round  78, Train loss: 0.356, Test loss: 1.346, Test accuracy: 49.28
Round  79, Train loss: 0.443, Test loss: 1.343, Test accuracy: 48.28
Round  80, Train loss: 0.331, Test loss: 1.261, Test accuracy: 49.63
Round  81, Train loss: 0.304, Test loss: 1.215, Test accuracy: 50.92
Round  82, Train loss: 0.213, Test loss: 1.191, Test accuracy: 52.98
Round  83, Train loss: 0.185, Test loss: 1.195, Test accuracy: 52.37
Round  84, Train loss: 0.277, Test loss: 1.256, Test accuracy: 50.58
Round  85, Train loss: 0.339, Test loss: 1.251, Test accuracy: 50.45
Round  86, Train loss: 0.303, Test loss: 1.319, Test accuracy: 50.38
Round  87, Train loss: 0.306, Test loss: 1.268, Test accuracy: 51.55
Round  88, Train loss: 0.273, Test loss: 1.242, Test accuracy: 52.23
Round  89, Train loss: 0.290, Test loss: 1.170, Test accuracy: 53.40
Round  90, Train loss: 0.287, Test loss: 1.191, Test accuracy: 53.53
Round  91, Train loss: 0.258, Test loss: 1.183, Test accuracy: 52.87
Round  92, Train loss: 0.232, Test loss: 1.169, Test accuracy: 53.95
Round  93, Train loss: 0.257, Test loss: 1.258, Test accuracy: 52.53
Round  94, Train loss: 0.341, Test loss: 1.402, Test accuracy: 50.85
Round  95, Train loss: 0.333, Test loss: 1.378, Test accuracy: 50.00
Round  96, Train loss: 0.382, Test loss: 1.429, Test accuracy: 48.70
Round  97, Train loss: 0.286, Test loss: 1.334, Test accuracy: 51.57
Round  98, Train loss: 0.245, Test loss: 1.239, Test accuracy: 52.90
Round  99, Train loss: 0.340, Test loss: 1.361, Test accuracy: 51.77
Final Round, Train loss: 0.264, Test loss: 1.221, Test accuracy: 56.17
Average accuracy final 10 rounds: 51.86666666666667
3046.81410241127
[6.424724817276001, 10.905793905258179, 15.404042959213257, 19.91449546813965, 24.420732021331787, 28.994887590408325, 33.554535150527954, 38.06311249732971, 42.53895282745361, 47.125155210494995, 51.72221040725708, 56.31072187423706, 60.89376902580261, 65.39447522163391, 69.87831568717957, 74.39548945426941, 78.94776582717896, 83.38495826721191, 87.84626698493958, 92.38717555999756, 96.94841504096985, 101.51239943504333, 105.99107694625854, 110.92438316345215, 115.373051404953, 119.86368823051453, 124.3588490486145, 128.84393048286438, 133.30208587646484, 137.79902911186218, 142.28487038612366, 146.77830719947815, 151.2521688938141, 155.81292867660522, 160.2772138118744, 164.71977925300598, 169.46289587020874, 173.8830645084381, 178.30722451210022, 182.7231466770172, 187.13309860229492, 191.54030561447144, 195.96541547775269, 200.36163878440857, 204.77144646644592, 209.16644382476807, 213.5802822113037, 217.9954810142517, 222.43839049339294, 226.90374326705933, 231.36770343780518, 235.83764266967773, 240.30393195152283, 244.762775182724, 249.21730995178223, 253.66873216629028, 258.1301076412201, 262.5901560783386, 267.0397834777832, 271.48874378204346, 275.9508762359619, 280.40171241760254, 284.8481137752533, 289.2731192111969, 293.6370565891266, 298.0373921394348, 302.4312880039215, 306.87325263023376, 311.31439089775085, 315.7625982761383, 320.2068176269531, 324.67233753204346, 329.0995194911957, 333.51813101768494, 337.9660007953644, 342.3771376609802, 346.78682255744934, 351.1929998397827, 355.741418838501, 360.1704170703888, 364.60060024261475, 369.1381194591522, 373.581707239151, 378.1934530735016, 382.56863951683044, 386.9443166255951, 391.3207743167877, 395.7015628814697, 400.0713140964508, 404.46081376075745, 408.84975838661194, 413.22948956489563, 417.5922212600708, 421.95649790763855, 426.3314518928528, 430.6977951526642, 435.0637674331665, 439.44433975219727, 443.8044900894165, 448.1676368713379, 458.634863615036]
[25.05, 23.483333333333334, 26.6, 35.0, 35.95, 34.5, 39.88333333333333, 39.483333333333334, 39.96666666666667, 39.8, 43.8, 40.266666666666666, 40.5, 42.483333333333334, 44.75, 43.266666666666666, 44.95, 47.03333333333333, 47.96666666666667, 45.81666666666667, 45.46666666666667, 47.333333333333336, 45.28333333333333, 46.7, 45.7, 46.65, 45.11666666666667, 46.083333333333336, 45.31666666666667, 48.88333333333333, 47.71666666666667, 47.81666666666667, 48.61666666666667, 45.65, 46.56666666666667, 50.28333333333333, 48.833333333333336, 47.55, 46.56666666666667, 49.38333333333333, 48.4, 49.61666666666667, 48.583333333333336, 47.45, 48.15, 47.483333333333334, 47.95, 47.45, 48.266666666666666, 47.05, 49.75, 51.05, 52.0, 52.18333333333333, 53.13333333333333, 50.266666666666666, 53.233333333333334, 53.61666666666667, 50.916666666666664, 48.86666666666667, 49.46666666666667, 48.516666666666666, 51.86666666666667, 51.7, 52.75, 52.65, 51.46666666666667, 50.55, 51.03333333333333, 50.78333333333333, 48.06666666666667, 49.9, 52.416666666666664, 52.333333333333336, 53.083333333333336, 52.5, 51.38333333333333, 50.333333333333336, 49.28333333333333, 48.28333333333333, 49.63333333333333, 50.916666666666664, 52.983333333333334, 52.36666666666667, 50.583333333333336, 50.45, 50.38333333333333, 51.55, 52.233333333333334, 53.4, 53.53333333333333, 52.86666666666667, 53.95, 52.53333333333333, 50.85, 50.0, 48.7, 51.56666666666667, 52.9, 51.766666666666666, 56.166666666666664]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: center_psl, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.7067
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.4367
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6333
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3400
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5867
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.5900
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3567
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.6367
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.5467
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6767
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.462, Test loss: 2.111, Test accuracy: 25.48
Round   1, Train loss: 1.192, Test loss: 1.987, Test accuracy: 28.57
Round   2, Train loss: 1.018, Test loss: 1.778, Test accuracy: 30.25
Round   3, Train loss: 1.195, Test loss: 1.730, Test accuracy: 36.82
Round   4, Train loss: 0.993, Test loss: 1.458, Test accuracy: 38.32
Round   5, Train loss: 0.890, Test loss: 1.475, Test accuracy: 39.82
Round   6, Train loss: 1.047, Test loss: 1.366, Test accuracy: 41.55
Round   7, Train loss: 0.995, Test loss: 1.262, Test accuracy: 46.10
Round   8, Train loss: 0.872, Test loss: 1.265, Test accuracy: 45.73
Round   9, Train loss: 0.925, Test loss: 1.345, Test accuracy: 45.02
Round  10, Train loss: 0.739, Test loss: 1.244, Test accuracy: 47.68
Round  11, Train loss: 0.952, Test loss: 1.307, Test accuracy: 42.40
Round  12, Train loss: 0.762, Test loss: 1.145, Test accuracy: 49.92
Round  13, Train loss: 0.824, Test loss: 1.149, Test accuracy: 51.52
Round  14, Train loss: 0.718, Test loss: 1.130, Test accuracy: 51.40
Round  15, Train loss: 0.767, Test loss: 1.118, Test accuracy: 50.38
Round  16, Train loss: 0.960, Test loss: 1.144, Test accuracy: 49.73
Round  17, Train loss: 0.703, Test loss: 1.099, Test accuracy: 51.32
Round  18, Train loss: 0.792, Test loss: 1.168, Test accuracy: 50.80
Round  19, Train loss: 0.735, Test loss: 1.154, Test accuracy: 49.07
Round  20, Train loss: 0.794, Test loss: 1.293, Test accuracy: 44.40
Round  21, Train loss: 0.799, Test loss: 1.243, Test accuracy: 46.25
Round  22, Train loss: 0.697, Test loss: 1.181, Test accuracy: 50.32
Round  23, Train loss: 0.694, Test loss: 1.212, Test accuracy: 50.52
Round  24, Train loss: 0.810, Test loss: 1.160, Test accuracy: 51.28
Round  25, Train loss: 0.608, Test loss: 1.139, Test accuracy: 53.08
Round  26, Train loss: 0.762, Test loss: 1.150, Test accuracy: 52.12
Round  27, Train loss: 0.642, Test loss: 1.140, Test accuracy: 53.15
Round  28, Train loss: 0.574, Test loss: 1.192, Test accuracy: 53.12
Round  29, Train loss: 0.538, Test loss: 1.080, Test accuracy: 57.00
Round  30, Train loss: 0.655, Test loss: 1.176, Test accuracy: 52.70
Round  31, Train loss: 0.632, Test loss: 1.194, Test accuracy: 51.52
Round  32, Train loss: 0.544, Test loss: 1.199, Test accuracy: 52.22
Round  33, Train loss: 0.617, Test loss: 1.197, Test accuracy: 52.08
Round  34, Train loss: 0.583, Test loss: 1.205, Test accuracy: 52.67
Round  35, Train loss: 0.451, Test loss: 1.137, Test accuracy: 54.87
Round  36, Train loss: 0.522, Test loss: 1.133, Test accuracy: 54.20
Round  37, Train loss: 0.449, Test loss: 1.175, Test accuracy: 51.12
Round  38, Train loss: 0.441, Test loss: 1.227, Test accuracy: 51.70
Round  39, Train loss: 0.538, Test loss: 1.293, Test accuracy: 49.50
Round  40, Train loss: 0.428, Test loss: 1.382, Test accuracy: 50.05
Round  41, Train loss: 0.382, Test loss: 1.336, Test accuracy: 51.32
Round  42, Train loss: 0.432, Test loss: 1.294, Test accuracy: 52.72
Round  43, Train loss: 0.468, Test loss: 1.291, Test accuracy: 53.38
Round  44, Train loss: 0.507, Test loss: 1.190, Test accuracy: 53.67
Round  45, Train loss: 0.432, Test loss: 1.217, Test accuracy: 52.70
Round  46, Train loss: 0.494, Test loss: 1.238, Test accuracy: 51.98
Round  47, Train loss: 0.356, Test loss: 1.166, Test accuracy: 53.65
Round  48, Train loss: 0.377, Test loss: 1.294, Test accuracy: 51.23
Round  49, Train loss: 0.397, Test loss: 1.250, Test accuracy: 52.07
Round  50, Train loss: 0.329, Test loss: 1.240, Test accuracy: 54.47
Round  51, Train loss: 0.412, Test loss: 1.296, Test accuracy: 51.82
Round  52, Train loss: 0.451, Test loss: 1.303, Test accuracy: 51.43
Round  53, Train loss: 0.371, Test loss: 1.288, Test accuracy: 53.25
Round  54, Train loss: 0.307, Test loss: 1.287, Test accuracy: 53.08
Round  55, Train loss: 0.413, Test loss: 1.420, Test accuracy: 50.25
Round  56, Train loss: 0.260, Test loss: 1.263, Test accuracy: 54.37
Round  57, Train loss: 0.388, Test loss: 1.202, Test accuracy: 54.67
Round  58, Train loss: 0.295, Test loss: 1.307, Test accuracy: 53.55
Round  59, Train loss: 0.421, Test loss: 1.291, Test accuracy: 51.53
Round  60, Train loss: 0.311, Test loss: 1.285, Test accuracy: 54.47
Round  61, Train loss: 0.262, Test loss: 1.252, Test accuracy: 54.15
Round  62, Train loss: 0.357, Test loss: 1.253, Test accuracy: 54.82
Round  63, Train loss: 0.264, Test loss: 1.267, Test accuracy: 54.95
Round  64, Train loss: 0.259, Test loss: 1.227, Test accuracy: 56.02
Round  65, Train loss: 0.264, Test loss: 1.337, Test accuracy: 54.10
Round  66, Train loss: 0.358, Test loss: 1.296, Test accuracy: 54.23
Round  67, Train loss: 0.347, Test loss: 1.293, Test accuracy: 53.88
Round  68, Train loss: 0.236, Test loss: 1.291, Test accuracy: 54.83
Round  69, Train loss: 0.288, Test loss: 1.342, Test accuracy: 53.88
Round  70, Train loss: 0.331, Test loss: 1.321, Test accuracy: 52.07
Round  71, Train loss: 0.291, Test loss: 1.305, Test accuracy: 52.58
Round  72, Train loss: 0.224, Test loss: 1.307, Test accuracy: 54.70
Round  73, Train loss: 0.235, Test loss: 1.323, Test accuracy: 53.95
Round  74, Train loss: 0.302, Test loss: 1.302, Test accuracy: 55.53
Round  75, Train loss: 0.205, Test loss: 1.321, Test accuracy: 54.33
Round  76, Train loss: 0.291, Test loss: 1.373, Test accuracy: 53.10
Round  77, Train loss: 0.342, Test loss: 1.375, Test accuracy: 53.13
Round  78, Train loss: 0.303, Test loss: 1.435, Test accuracy: 51.80
Round  79, Train loss: 0.384, Test loss: 1.498, Test accuracy: 49.05
Round  80, Train loss: 0.269, Test loss: 1.529, Test accuracy: 50.20
Round  81, Train loss: 0.232, Test loss: 1.497, Test accuracy: 52.02
Round  82, Train loss: 0.224, Test loss: 1.429, Test accuracy: 54.17
Round  83, Train loss: 0.126, Test loss: 1.411, Test accuracy: 54.93
Round  84, Train loss: 0.210, Test loss: 1.406, Test accuracy: 54.55
Round  85, Train loss: 0.301, Test loss: 1.341, Test accuracy: 53.88
Round  86, Train loss: 0.242, Test loss: 1.381, Test accuracy: 54.00
Round  87, Train loss: 0.240, Test loss: 1.295, Test accuracy: 54.63
Round  88, Train loss: 0.211, Test loss: 1.333, Test accuracy: 54.25
Round  89, Train loss: 0.238, Test loss: 1.309, Test accuracy: 53.67
Round  90, Train loss: 0.253, Test loss: 1.338, Test accuracy: 54.70
Round  91, Train loss: 0.198, Test loss: 1.302, Test accuracy: 55.12
Round  92, Train loss: 0.187, Test loss: 1.270, Test accuracy: 55.30
Round  93, Train loss: 0.190, Test loss: 1.317, Test accuracy: 54.58
Round  94, Train loss: 0.268, Test loss: 1.366, Test accuracy: 53.77
Round  95, Train loss: 0.283, Test loss: 1.478, Test accuracy: 51.32
Round  96, Train loss: 0.294, Test loss: 1.407, Test accuracy: 51.57
Round  97, Train loss: 0.244, Test loss: 1.344, Test accuracy: 53.32
Round  98, Train loss: 0.174, Test loss: 1.358, Test accuracy: 54.73
Round  99, Train loss: 0.272, Test loss: 1.352, Test accuracy: 54.22
Final Round, Train loss: 0.267, Test loss: 1.333, Test accuracy: 55.82
Average accuracy final 10 rounds: 53.86166666666667
3317.459691286087
[6.725890398025513, 11.541306495666504, 16.28482460975647, 21.54783320426941, 26.904441356658936, 31.795984983444214, 37.1373336315155, 42.464035987854004, 47.21445941925049, 52.647146463394165, 57.493895053863525, 62.84579682350159, 67.61523413658142, 72.9632420539856, 78.3308093547821, 83.16879940032959, 88.40899896621704, 93.77519369125366, 99.30542039871216, 104.30541181564331, 109.57140517234802, 114.79773688316345, 119.52218461036682, 124.76873087882996, 130.03965497016907, 135.29671788215637, 140.6131992340088, 145.33523106575012, 150.50185680389404, 155.86097931861877, 160.7488694190979, 166.10797095298767, 171.57682728767395, 177.03524947166443, 182.52297520637512, 188.01407146453857, 193.4844889640808, 198.80097246170044, 204.0755386352539, 209.39524912834167, 214.8045530319214, 220.14269304275513, 224.92911982536316, 229.7094111442566, 235.07122492790222, 240.30697321891785, 245.65384125709534, 250.98521399497986, 256.33521842956543, 261.6744866371155, 266.9928410053253, 271.8340184688568, 276.65190410614014, 281.41284227371216, 286.1860432624817, 290.92623591423035, 296.23793721199036, 301.0873761177063, 306.4461965560913, 311.23594760894775, 316.59718585014343, 321.95653915405273, 327.3340377807617, 332.6598868370056, 337.9442982673645, 343.23628664016724, 348.03940534591675, 353.4321668148041, 358.82548213005066, 363.97707319259644, 369.2614641189575, 374.4438798427582, 379.1867198944092, 384.4679250717163, 389.728768825531, 395.00020599365234, 400.17347860336304, 404.8993308544159, 409.6638596057892, 414.96270394325256, 420.1570382118225, 424.9897804260254, 430.408474445343, 435.70236444473267, 440.4252254962921, 445.162082195282, 449.88889336586, 455.1849913597107, 460.4715943336487, 465.75597167015076, 470.4778370857239, 475.74301171302795, 480.48102140426636, 485.7796137332916, 491.0836057662964, 495.81161427497864, 500.5320177078247, 505.82879400253296, 511.11536955833435, 516.377875328064, 529.8677792549133]
[25.483333333333334, 28.566666666666666, 30.25, 36.81666666666667, 38.31666666666667, 39.81666666666667, 41.55, 46.1, 45.733333333333334, 45.016666666666666, 47.68333333333333, 42.4, 49.916666666666664, 51.516666666666666, 51.4, 50.38333333333333, 49.733333333333334, 51.31666666666667, 50.8, 49.06666666666667, 44.4, 46.25, 50.31666666666667, 50.516666666666666, 51.28333333333333, 53.083333333333336, 52.11666666666667, 53.15, 53.11666666666667, 57.0, 52.7, 51.516666666666666, 52.21666666666667, 52.083333333333336, 52.666666666666664, 54.86666666666667, 54.2, 51.11666666666667, 51.7, 49.5, 50.05, 51.31666666666667, 52.71666666666667, 53.38333333333333, 53.666666666666664, 52.7, 51.983333333333334, 53.65, 51.233333333333334, 52.06666666666667, 54.46666666666667, 51.81666666666667, 51.43333333333333, 53.25, 53.083333333333336, 50.25, 54.36666666666667, 54.666666666666664, 53.55, 51.53333333333333, 54.46666666666667, 54.15, 54.81666666666667, 54.95, 56.016666666666666, 54.1, 54.233333333333334, 53.88333333333333, 54.833333333333336, 53.88333333333333, 52.06666666666667, 52.583333333333336, 54.7, 53.95, 55.53333333333333, 54.333333333333336, 53.1, 53.13333333333333, 51.8, 49.05, 50.2, 52.016666666666666, 54.166666666666664, 54.93333333333333, 54.55, 53.88333333333333, 54.0, 54.63333333333333, 54.25, 53.666666666666664, 54.7, 55.11666666666667, 55.3, 54.583333333333336, 53.766666666666666, 51.31666666666667, 51.56666666666667, 53.31666666666667, 54.733333333333334, 54.21666666666667, 55.81666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2033
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0433
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5533
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0433
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4567
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.2933
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1467
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4967
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3400
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.734, Test loss: 2.141, Test accuracy: 22.68
Round   1, Train loss: 1.210, Test loss: 2.129, Test accuracy: 32.02
Round   2, Train loss: 1.124, Test loss: 1.479, Test accuracy: 40.37
Round   3, Train loss: 1.053, Test loss: 1.340, Test accuracy: 43.35
Round   4, Train loss: 1.120, Test loss: 1.192, Test accuracy: 49.60
Round   5, Train loss: 0.958, Test loss: 1.142, Test accuracy: 51.67
Round   6, Train loss: 0.985, Test loss: 0.916, Test accuracy: 56.95
Round   7, Train loss: 0.952, Test loss: 0.888, Test accuracy: 60.45
Round   8, Train loss: 0.853, Test loss: 0.842, Test accuracy: 62.58
Round   9, Train loss: 0.932, Test loss: 0.851, Test accuracy: 61.52
Round  10, Train loss: 0.899, Test loss: 0.837, Test accuracy: 62.53
Round  11, Train loss: 1.045, Test loss: 0.812, Test accuracy: 63.72
Round  12, Train loss: 0.716, Test loss: 0.790, Test accuracy: 65.82
Round  13, Train loss: 0.724, Test loss: 0.781, Test accuracy: 65.67
Round  14, Train loss: 0.913, Test loss: 0.785, Test accuracy: 64.50
Round  15, Train loss: 0.831, Test loss: 0.778, Test accuracy: 65.35
Round  16, Train loss: 0.801, Test loss: 0.764, Test accuracy: 67.18
Round  17, Train loss: 0.840, Test loss: 0.773, Test accuracy: 66.85
Round  18, Train loss: 0.797, Test loss: 0.740, Test accuracy: 68.15
Round  19, Train loss: 0.713, Test loss: 0.727, Test accuracy: 69.50
Round  20, Train loss: 0.820, Test loss: 0.716, Test accuracy: 70.63
Round  21, Train loss: 0.873, Test loss: 0.708, Test accuracy: 69.33
Round  22, Train loss: 0.853, Test loss: 0.700, Test accuracy: 69.57
Round  23, Train loss: 0.722, Test loss: 0.701, Test accuracy: 68.67
Round  24, Train loss: 0.776, Test loss: 0.706, Test accuracy: 68.78
Round  25, Train loss: 0.861, Test loss: 0.710, Test accuracy: 69.12
Round  26, Train loss: 0.632, Test loss: 0.700, Test accuracy: 70.07
Round  27, Train loss: 0.818, Test loss: 0.686, Test accuracy: 71.32
Round  28, Train loss: 0.831, Test loss: 0.697, Test accuracy: 70.37
Round  29, Train loss: 0.628, Test loss: 0.700, Test accuracy: 70.65
Round  30, Train loss: 0.668, Test loss: 0.700, Test accuracy: 69.90
Round  31, Train loss: 0.743, Test loss: 0.680, Test accuracy: 71.40
Round  32, Train loss: 0.698, Test loss: 0.678, Test accuracy: 71.32
Round  33, Train loss: 0.718, Test loss: 0.668, Test accuracy: 72.00
Round  34, Train loss: 0.637, Test loss: 0.668, Test accuracy: 71.62
Round  35, Train loss: 0.831, Test loss: 0.666, Test accuracy: 72.38
Round  36, Train loss: 0.687, Test loss: 0.663, Test accuracy: 72.98
Round  37, Train loss: 0.739, Test loss: 0.657, Test accuracy: 73.30
Round  38, Train loss: 0.771, Test loss: 0.657, Test accuracy: 74.60
Round  39, Train loss: 0.689, Test loss: 0.651, Test accuracy: 73.82
Round  40, Train loss: 0.632, Test loss: 0.654, Test accuracy: 73.67
Round  41, Train loss: 0.712, Test loss: 0.649, Test accuracy: 73.35
Round  42, Train loss: 0.796, Test loss: 0.653, Test accuracy: 73.77
Round  43, Train loss: 0.646, Test loss: 0.650, Test accuracy: 72.83
Round  44, Train loss: 0.676, Test loss: 0.647, Test accuracy: 72.85
Round  45, Train loss: 0.508, Test loss: 0.650, Test accuracy: 72.77
Round  46, Train loss: 0.633, Test loss: 0.646, Test accuracy: 72.65
Round  47, Train loss: 0.727, Test loss: 0.636, Test accuracy: 72.97
Round  48, Train loss: 0.623, Test loss: 0.642, Test accuracy: 73.12
Round  49, Train loss: 0.774, Test loss: 0.644, Test accuracy: 72.40
Round  50, Train loss: 0.608, Test loss: 0.634, Test accuracy: 73.68
Round  51, Train loss: 0.473, Test loss: 0.632, Test accuracy: 74.02
Round  52, Train loss: 0.494, Test loss: 0.630, Test accuracy: 73.52
Round  53, Train loss: 0.781, Test loss: 0.641, Test accuracy: 72.42
Round  54, Train loss: 0.593, Test loss: 0.636, Test accuracy: 73.25
Round  55, Train loss: 0.820, Test loss: 0.628, Test accuracy: 73.32
Round  56, Train loss: 0.554, Test loss: 0.629, Test accuracy: 73.10
Round  57, Train loss: 0.554, Test loss: 0.622, Test accuracy: 72.75
Round  58, Train loss: 0.739, Test loss: 0.621, Test accuracy: 74.75
Round  59, Train loss: 0.468, Test loss: 0.628, Test accuracy: 74.28
Round  60, Train loss: 0.555, Test loss: 0.622, Test accuracy: 73.93
Round  61, Train loss: 0.629, Test loss: 0.623, Test accuracy: 73.85
Round  62, Train loss: 0.542, Test loss: 0.618, Test accuracy: 74.48
Round  63, Train loss: 0.706, Test loss: 0.607, Test accuracy: 75.03
Round  64, Train loss: 0.710, Test loss: 0.616, Test accuracy: 74.18
Round  65, Train loss: 0.473, Test loss: 0.616, Test accuracy: 74.03
Round  66, Train loss: 0.779, Test loss: 0.617, Test accuracy: 73.93
Round  67, Train loss: 0.644, Test loss: 0.619, Test accuracy: 73.60
Round  68, Train loss: 0.543, Test loss: 0.605, Test accuracy: 75.00
Round  69, Train loss: 0.476, Test loss: 0.607, Test accuracy: 75.00
Round  70, Train loss: 0.483, Test loss: 0.611, Test accuracy: 74.52
Round  71, Train loss: 0.659, Test loss: 0.602, Test accuracy: 75.67
Round  72, Train loss: 0.704, Test loss: 0.617, Test accuracy: 74.30
Round  73, Train loss: 0.611, Test loss: 0.610, Test accuracy: 74.97
Round  74, Train loss: 0.813, Test loss: 0.615, Test accuracy: 74.87
Round  75, Train loss: 0.627, Test loss: 0.607, Test accuracy: 74.80
Round  76, Train loss: 0.648, Test loss: 0.599, Test accuracy: 75.17
Round  77, Train loss: 0.550, Test loss: 0.610, Test accuracy: 74.48
Round  78, Train loss: 0.576, Test loss: 0.600, Test accuracy: 74.42
Round  79, Train loss: 0.796, Test loss: 0.604, Test accuracy: 74.53
Round  80, Train loss: 0.594, Test loss: 0.612, Test accuracy: 74.22
Round  81, Train loss: 0.454, Test loss: 0.604, Test accuracy: 74.53
Round  82, Train loss: 0.605, Test loss: 0.615, Test accuracy: 73.92
Round  83, Train loss: 0.553, Test loss: 0.606, Test accuracy: 74.35
Round  84, Train loss: 0.588, Test loss: 0.610, Test accuracy: 74.57
Round  85, Train loss: 0.593, Test loss: 0.601, Test accuracy: 74.60
Round  86, Train loss: 0.619, Test loss: 0.603, Test accuracy: 74.43
Round  87, Train loss: 0.503, Test loss: 0.600, Test accuracy: 74.62
Round  88, Train loss: 0.705, Test loss: 0.603, Test accuracy: 74.62
Round  89, Train loss: 0.576, Test loss: 0.601, Test accuracy: 74.43
Round  90, Train loss: 0.594, Test loss: 0.598, Test accuracy: 74.72
Round  91, Train loss: 0.573, Test loss: 0.609, Test accuracy: 74.13
Round  92, Train loss: 0.558, Test loss: 0.620, Test accuracy: 74.27
Round  93, Train loss: 0.589, Test loss: 0.602, Test accuracy: 74.77
Round  94, Train loss: 0.555, Test loss: 0.598, Test accuracy: 75.05
Round  95, Train loss: 0.392, Test loss: 0.604, Test accuracy: 75.00
Round  96, Train loss: 0.472, Test loss: 0.597, Test accuracy: 75.20
Round  97, Train loss: 0.621, Test loss: 0.595, Test accuracy: 75.22
Round  98, Train loss: 0.600, Test loss: 0.606, Test accuracy: 74.75
Round  99, Train loss: 0.447, Test loss: 0.598, Test accuracy: 74.63
Final Round, Train loss: 0.507, Test loss: 0.602, Test accuracy: 74.65
Average accuracy final 10 rounds: 74.77333333333334
840.4375801086426
[2.998061418533325, 4.063218355178833, 5.136121034622192, 6.210486173629761, 7.27497935295105, 8.340925455093384, 9.393773794174194, 10.460854768753052, 11.512280702590942, 12.558666467666626, 13.620769500732422, 14.673339128494263, 15.73215937614441, 16.80559229850769, 17.866648197174072, 18.955766677856445, 20.013651371002197, 21.074137687683105, 22.12992024421692, 23.199734926223755, 24.26684880256653, 25.326719284057617, 26.394197702407837, 27.46254563331604, 28.52953815460205, 29.59369659423828, 30.657846689224243, 31.72347378730774, 32.78982424736023, 33.83998775482178, 34.90905046463013, 36.003657579422, 37.065268754959106, 38.12747240066528, 39.19311809539795, 40.28380751609802, 41.35253596305847, 42.42095327377319, 43.49614477157593, 44.56168985366821, 45.63027262687683, 46.69869136810303, 47.76772618293762, 48.83797740936279, 49.908124685287476, 50.97612428665161, 52.04935145378113, 53.1177921295166, 54.1933650970459, 55.26053214073181, 56.33632254600525, 57.4051251411438, 58.483436584472656, 59.550983905792236, 60.61785292625427, 61.69024085998535, 62.76259136199951, 63.838048458099365, 64.92041087150574, 65.99154686927795, 67.06234097480774, 68.13504362106323, 69.21265244483948, 70.28606605529785, 71.35953283309937, 72.43549728393555, 73.51060366630554, 74.58846592903137, 75.69521379470825, 76.76217770576477, 77.81368970870972, 78.88547372817993, 79.9659194946289, 81.03267168998718, 82.08883023262024, 83.15038752555847, 84.20703101158142, 85.2701027393341, 86.3314368724823, 87.38549184799194, 88.42960405349731, 89.49525547027588, 90.5468122959137, 91.60620331764221, 92.66459012031555, 93.73897790908813, 94.79501795768738, 95.84864664077759, 96.91520524024963, 97.96719670295715, 99.03177690505981, 100.10612368583679, 101.17810249328613, 102.24077868461609, 103.30327844619751, 104.35439872741699, 105.42133665084839, 106.48803997039795, 107.5559434890747, 108.60721015930176, 110.17926359176636]
[22.683333333333334, 32.016666666666666, 40.36666666666667, 43.35, 49.6, 51.666666666666664, 56.95, 60.45, 62.583333333333336, 61.516666666666666, 62.53333333333333, 63.71666666666667, 65.81666666666666, 65.66666666666667, 64.5, 65.35, 67.18333333333334, 66.85, 68.15, 69.5, 70.63333333333334, 69.33333333333333, 69.56666666666666, 68.66666666666667, 68.78333333333333, 69.11666666666666, 70.06666666666666, 71.31666666666666, 70.36666666666666, 70.65, 69.9, 71.4, 71.31666666666666, 72.0, 71.61666666666666, 72.38333333333334, 72.98333333333333, 73.3, 74.6, 73.81666666666666, 73.66666666666667, 73.35, 73.76666666666667, 72.83333333333333, 72.85, 72.76666666666667, 72.65, 72.96666666666667, 73.11666666666666, 72.4, 73.68333333333334, 74.01666666666667, 73.51666666666667, 72.41666666666667, 73.25, 73.31666666666666, 73.1, 72.75, 74.75, 74.28333333333333, 73.93333333333334, 73.85, 74.48333333333333, 75.03333333333333, 74.18333333333334, 74.03333333333333, 73.93333333333334, 73.6, 75.0, 75.0, 74.51666666666667, 75.66666666666667, 74.3, 74.96666666666667, 74.86666666666666, 74.8, 75.16666666666667, 74.48333333333333, 74.41666666666667, 74.53333333333333, 74.21666666666667, 74.53333333333333, 73.91666666666667, 74.35, 74.56666666666666, 74.6, 74.43333333333334, 74.61666666666666, 74.61666666666666, 74.43333333333334, 74.71666666666667, 74.13333333333334, 74.26666666666667, 74.76666666666667, 75.05, 75.0, 75.2, 75.21666666666667, 74.75, 74.63333333333334, 74.65]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2033
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0433
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5533
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0433
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4567
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.2933
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2267
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4967
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3500
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.709, Test loss: 2.154, Test accuracy: 18.52
Round   1, Train loss: 1.214, Test loss: 2.182, Test accuracy: 28.82
Round   2, Train loss: 1.100, Test loss: 1.472, Test accuracy: 41.32
Round   3, Train loss: 1.013, Test loss: 1.337, Test accuracy: 44.15
Round   4, Train loss: 1.060, Test loss: 1.199, Test accuracy: 50.02
Round   5, Train loss: 0.968, Test loss: 1.158, Test accuracy: 48.77
Round   6, Train loss: 0.956, Test loss: 0.934, Test accuracy: 53.98
Round   7, Train loss: 0.936, Test loss: 0.900, Test accuracy: 58.22
Round   8, Train loss: 0.848, Test loss: 0.832, Test accuracy: 61.68
Round   9, Train loss: 0.917, Test loss: 0.842, Test accuracy: 60.20
Round  10, Train loss: 0.871, Test loss: 0.829, Test accuracy: 61.72
Round  11, Train loss: 0.991, Test loss: 0.832, Test accuracy: 61.80
Round  12, Train loss: 0.724, Test loss: 0.792, Test accuracy: 64.90
Round  13, Train loss: 0.725, Test loss: 0.780, Test accuracy: 66.10
Round  14, Train loss: 0.901, Test loss: 0.794, Test accuracy: 65.27
Round  15, Train loss: 0.812, Test loss: 0.785, Test accuracy: 66.68
Round  16, Train loss: 0.779, Test loss: 0.779, Test accuracy: 66.45
Round  17, Train loss: 0.824, Test loss: 0.784, Test accuracy: 65.67
Round  18, Train loss: 0.781, Test loss: 0.742, Test accuracy: 68.82
Round  19, Train loss: 0.696, Test loss: 0.751, Test accuracy: 68.30
Round  20, Train loss: 0.780, Test loss: 0.716, Test accuracy: 69.75
Round  21, Train loss: 0.811, Test loss: 0.735, Test accuracy: 68.37
Round  22, Train loss: 0.785, Test loss: 0.725, Test accuracy: 68.95
Round  23, Train loss: 0.685, Test loss: 0.727, Test accuracy: 68.60
Round  24, Train loss: 0.740, Test loss: 0.721, Test accuracy: 69.37
Round  25, Train loss: 0.810, Test loss: 0.730, Test accuracy: 69.82
Round  26, Train loss: 0.615, Test loss: 0.720, Test accuracy: 69.78
Round  27, Train loss: 0.766, Test loss: 0.756, Test accuracy: 68.35
Round  28, Train loss: 0.790, Test loss: 0.720, Test accuracy: 69.35
Round  29, Train loss: 0.606, Test loss: 0.695, Test accuracy: 70.35
Round  30, Train loss: 0.652, Test loss: 0.691, Test accuracy: 70.65
Round  31, Train loss: 0.689, Test loss: 0.703, Test accuracy: 70.12
Round  32, Train loss: 0.649, Test loss: 0.704, Test accuracy: 70.37
Round  33, Train loss: 0.674, Test loss: 0.693, Test accuracy: 70.65
Round  34, Train loss: 0.618, Test loss: 0.686, Test accuracy: 70.65
Round  35, Train loss: 0.788, Test loss: 0.682, Test accuracy: 71.07
Round  36, Train loss: 0.650, Test loss: 0.681, Test accuracy: 71.10
Round  37, Train loss: 0.687, Test loss: 0.688, Test accuracy: 70.32
Round  38, Train loss: 0.743, Test loss: 0.686, Test accuracy: 70.85
Round  39, Train loss: 0.648, Test loss: 0.688, Test accuracy: 69.72
Round  40, Train loss: 0.621, Test loss: 0.686, Test accuracy: 70.53
Round  41, Train loss: 0.685, Test loss: 0.669, Test accuracy: 71.72
Round  42, Train loss: 0.773, Test loss: 0.676, Test accuracy: 70.80
Round  43, Train loss: 0.621, Test loss: 0.662, Test accuracy: 72.13
Round  44, Train loss: 0.632, Test loss: 0.660, Test accuracy: 71.98
Round  45, Train loss: 0.496, Test loss: 0.647, Test accuracy: 72.83
Round  46, Train loss: 0.616, Test loss: 0.656, Test accuracy: 72.52
Round  47, Train loss: 0.695, Test loss: 0.653, Test accuracy: 71.93
Round  48, Train loss: 0.592, Test loss: 0.638, Test accuracy: 73.38
Round  49, Train loss: 0.740, Test loss: 0.649, Test accuracy: 72.90
Round  50, Train loss: 0.593, Test loss: 0.643, Test accuracy: 73.33
Round  51, Train loss: 0.467, Test loss: 0.641, Test accuracy: 73.22
Round  52, Train loss: 0.496, Test loss: 0.657, Test accuracy: 72.58
Round  53, Train loss: 0.741, Test loss: 0.669, Test accuracy: 72.10
Round  54, Train loss: 0.576, Test loss: 0.651, Test accuracy: 72.98
Round  55, Train loss: 0.772, Test loss: 0.646, Test accuracy: 73.58
Round  56, Train loss: 0.537, Test loss: 0.645, Test accuracy: 73.53
Round  57, Train loss: 0.546, Test loss: 0.657, Test accuracy: 73.40
Round  58, Train loss: 0.703, Test loss: 0.641, Test accuracy: 73.87
Round  59, Train loss: 0.449, Test loss: 0.633, Test accuracy: 74.13
Round  60, Train loss: 0.540, Test loss: 0.631, Test accuracy: 74.78
Round  61, Train loss: 0.585, Test loss: 0.626, Test accuracy: 74.82
Round  62, Train loss: 0.532, Test loss: 0.620, Test accuracy: 74.85
Round  63, Train loss: 0.650, Test loss: 0.622, Test accuracy: 73.78
Round  64, Train loss: 0.663, Test loss: 0.616, Test accuracy: 74.32
Round  65, Train loss: 0.473, Test loss: 0.618, Test accuracy: 74.53
Round  66, Train loss: 0.748, Test loss: 0.624, Test accuracy: 74.42
Round  67, Train loss: 0.613, Test loss: 0.612, Test accuracy: 74.40
Round  68, Train loss: 0.540, Test loss: 0.621, Test accuracy: 74.25
Round  69, Train loss: 0.457, Test loss: 0.604, Test accuracy: 74.67
Round  70, Train loss: 0.461, Test loss: 0.621, Test accuracy: 74.37
Round  71, Train loss: 0.635, Test loss: 0.625, Test accuracy: 74.38
Round  72, Train loss: 0.674, Test loss: 0.626, Test accuracy: 73.18
Round  73, Train loss: 0.575, Test loss: 0.629, Test accuracy: 72.60
Round  74, Train loss: 0.778, Test loss: 0.639, Test accuracy: 72.98
Round  75, Train loss: 0.612, Test loss: 0.618, Test accuracy: 74.10
Round  76, Train loss: 0.639, Test loss: 0.621, Test accuracy: 73.90
Round  77, Train loss: 0.531, Test loss: 0.620, Test accuracy: 74.30
Round  78, Train loss: 0.557, Test loss: 0.619, Test accuracy: 74.22
Round  79, Train loss: 0.756, Test loss: 0.622, Test accuracy: 74.57
Round  80, Train loss: 0.577, Test loss: 0.638, Test accuracy: 73.27
Round  81, Train loss: 0.444, Test loss: 0.620, Test accuracy: 74.52
Round  82, Train loss: 0.569, Test loss: 0.622, Test accuracy: 73.87
Round  83, Train loss: 0.516, Test loss: 0.612, Test accuracy: 74.65
Round  84, Train loss: 0.595, Test loss: 0.622, Test accuracy: 73.87
Round  85, Train loss: 0.573, Test loss: 0.609, Test accuracy: 74.07
Round  86, Train loss: 0.587, Test loss: 0.609, Test accuracy: 73.98
Round  87, Train loss: 0.503, Test loss: 0.609, Test accuracy: 74.15
Round  88, Train loss: 0.671, Test loss: 0.619, Test accuracy: 73.80
Round  89, Train loss: 0.546, Test loss: 0.609, Test accuracy: 74.05
Round  90, Train loss: 0.574, Test loss: 0.618, Test accuracy: 73.42
Round  91, Train loss: 0.558, Test loss: 0.624, Test accuracy: 73.48
Round  92, Train loss: 0.551, Test loss: 0.618, Test accuracy: 73.35
Round  93, Train loss: 0.541, Test loss: 0.625, Test accuracy: 73.07
Round  94, Train loss: 0.534, Test loss: 0.624, Test accuracy: 73.23
Round  95, Train loss: 0.374, Test loss: 0.619, Test accuracy: 73.62
Round  96, Train loss: 0.430, Test loss: 0.617, Test accuracy: 73.50
Round  97, Train loss: 0.612, Test loss: 0.630, Test accuracy: 73.17
Round  98, Train loss: 0.596, Test loss: 0.612, Test accuracy: 73.50
Round  99, Train loss: 0.430, Test loss: 0.608, Test accuracy: 74.08
Final Round, Train loss: 0.485, Test loss: 0.614, Test accuracy: 74.42
Average accuracy final 10 rounds: 73.44166666666666
1617.6821575164795
[4.255764961242676, 6.580777406692505, 8.89677357673645, 11.219891786575317, 13.538181781768799, 15.855870246887207, 18.173288106918335, 20.497838735580444, 22.813065767288208, 25.132976293563843, 27.44990086555481, 29.777114152908325, 32.090240716934204, 34.39777231216431, 36.706241607666016, 39.02205991744995, 41.32390785217285, 43.63590335845947, 45.945878744125366, 48.2641384601593, 50.568403482437134, 52.868829011917114, 55.178447008132935, 57.472434520721436, 59.76189041137695, 62.19165253639221, 64.74546408653259, 67.05763173103333, 69.41137981414795, 71.75276637077332, 74.09319281578064, 76.40477800369263, 78.74467849731445, 81.05577063560486, 83.47292423248291, 86.09598231315613, 88.68758010864258, 90.99938344955444, 93.31438446044922, 95.62714457511902, 97.93118953704834, 100.23597002029419, 102.55149221420288, 104.92281484603882, 107.29693841934204, 109.67267632484436, 111.99331831932068, 114.31192326545715, 116.63338899612427, 119.23065304756165, 121.52528953552246, 123.81691360473633, 126.1253731250763, 128.41662096977234, 130.7105839252472, 132.99588704109192, 135.27820491790771, 137.5751235485077, 139.86490392684937, 142.16455078125, 144.4674677848816, 146.76038718223572, 149.05043053627014, 151.3470902442932, 153.64016246795654, 155.9353370666504, 158.2287769317627, 160.5389974117279, 162.81068396568298, 165.09412169456482, 167.38123726844788, 169.66397261619568, 171.95238327980042, 174.23024654388428, 176.5273642539978, 178.81434655189514, 181.09608364105225, 183.39592242240906, 185.69072222709656, 187.9872269630432, 190.2917947769165, 192.5925178527832, 194.89125633239746, 197.20619654655457, 199.49659395217896, 201.78846096992493, 204.0705840587616, 206.3478274345398, 208.6400763988495, 210.91951370239258, 213.20860600471497, 215.48608374595642, 217.75896048545837, 220.03511905670166, 222.30633521080017, 224.578688621521, 226.85285305976868, 229.1259765625, 231.3983497619629, 233.67635869979858, 236.8273651599884]
[18.516666666666666, 28.816666666666666, 41.31666666666667, 44.15, 50.016666666666666, 48.766666666666666, 53.983333333333334, 58.21666666666667, 61.68333333333333, 60.2, 61.71666666666667, 61.8, 64.9, 66.1, 65.26666666666667, 66.68333333333334, 66.45, 65.66666666666667, 68.81666666666666, 68.3, 69.75, 68.36666666666666, 68.95, 68.6, 69.36666666666666, 69.81666666666666, 69.78333333333333, 68.35, 69.35, 70.35, 70.65, 70.11666666666666, 70.36666666666666, 70.65, 70.65, 71.06666666666666, 71.1, 70.31666666666666, 70.85, 69.71666666666667, 70.53333333333333, 71.71666666666667, 70.8, 72.13333333333334, 71.98333333333333, 72.83333333333333, 72.51666666666667, 71.93333333333334, 73.38333333333334, 72.9, 73.33333333333333, 73.21666666666667, 72.58333333333333, 72.1, 72.98333333333333, 73.58333333333333, 73.53333333333333, 73.4, 73.86666666666666, 74.13333333333334, 74.78333333333333, 74.81666666666666, 74.85, 73.78333333333333, 74.31666666666666, 74.53333333333333, 74.41666666666667, 74.4, 74.25, 74.66666666666667, 74.36666666666666, 74.38333333333334, 73.18333333333334, 72.6, 72.98333333333333, 74.1, 73.9, 74.3, 74.21666666666667, 74.56666666666666, 73.26666666666667, 74.51666666666667, 73.86666666666666, 74.65, 73.86666666666666, 74.06666666666666, 73.98333333333333, 74.15, 73.8, 74.05, 73.41666666666667, 73.48333333333333, 73.35, 73.06666666666666, 73.23333333333333, 73.61666666666666, 73.5, 73.16666666666667, 73.5, 74.08333333333333, 74.41666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2033
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0433
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5533
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0433
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4733
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.2933
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1467
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4967
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.1933
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 0.985, Test loss: 2.022, Test accuracy: 21.28
Round   1, Train loss: 0.789, Test loss: 2.053, Test accuracy: 31.85
Round   2, Train loss: 0.593, Test loss: 1.559, Test accuracy: 37.78
Round   3, Train loss: 0.597, Test loss: 1.389, Test accuracy: 39.68
Round   4, Train loss: 0.647, Test loss: 1.357, Test accuracy: 44.82
Round   5, Train loss: 0.549, Test loss: 1.274, Test accuracy: 47.37
Round   6, Train loss: 0.526, Test loss: 1.026, Test accuracy: 53.18
Round   7, Train loss: 0.506, Test loss: 1.066, Test accuracy: 52.32
Round   8, Train loss: 0.437, Test loss: 1.042, Test accuracy: 55.38
Round   9, Train loss: 0.499, Test loss: 1.032, Test accuracy: 56.35
Round  10, Train loss: 0.500, Test loss: 0.985, Test accuracy: 57.93
Round  11, Train loss: 0.540, Test loss: 1.016, Test accuracy: 56.78
Round  12, Train loss: 0.366, Test loss: 1.005, Test accuracy: 55.90
Round  13, Train loss: 0.410, Test loss: 1.028, Test accuracy: 56.38
Round  14, Train loss: 0.536, Test loss: 1.049, Test accuracy: 55.50
Round  15, Train loss: 0.425, Test loss: 1.025, Test accuracy: 53.70
Round  16, Train loss: 0.433, Test loss: 0.929, Test accuracy: 58.83
Round  17, Train loss: 0.461, Test loss: 1.021, Test accuracy: 53.83
Round  18, Train loss: 0.426, Test loss: 0.896, Test accuracy: 57.75
Round  19, Train loss: 0.379, Test loss: 0.864, Test accuracy: 60.40
Round  20, Train loss: 0.456, Test loss: 0.875, Test accuracy: 60.63
Round  21, Train loss: 0.447, Test loss: 0.882, Test accuracy: 62.08
Round  22, Train loss: 0.445, Test loss: 0.874, Test accuracy: 62.78
Round  23, Train loss: 0.344, Test loss: 0.875, Test accuracy: 62.35
Round  24, Train loss: 0.367, Test loss: 0.906, Test accuracy: 60.23
Round  25, Train loss: 0.412, Test loss: 0.894, Test accuracy: 61.37
Round  26, Train loss: 0.288, Test loss: 0.886, Test accuracy: 64.62
Round  27, Train loss: 0.419, Test loss: 0.923, Test accuracy: 61.37
Round  28, Train loss: 0.437, Test loss: 0.895, Test accuracy: 63.05
Round  29, Train loss: 0.291, Test loss: 0.849, Test accuracy: 64.53
Round  30, Train loss: 0.328, Test loss: 0.875, Test accuracy: 61.40
Round  31, Train loss: 0.367, Test loss: 0.877, Test accuracy: 62.75
Round  32, Train loss: 0.347, Test loss: 0.850, Test accuracy: 61.65
Round  33, Train loss: 0.362, Test loss: 0.833, Test accuracy: 61.47
Round  34, Train loss: 0.335, Test loss: 0.843, Test accuracy: 62.92
Round  35, Train loss: 0.446, Test loss: 0.953, Test accuracy: 58.72
Round  36, Train loss: 0.341, Test loss: 0.945, Test accuracy: 59.68
Round  37, Train loss: 0.399, Test loss: 0.962, Test accuracy: 60.55
Round  38, Train loss: 0.401, Test loss: 0.955, Test accuracy: 61.12
Round  39, Train loss: 0.314, Test loss: 0.870, Test accuracy: 64.35
Round  40, Train loss: 0.292, Test loss: 0.855, Test accuracy: 64.70
Round  41, Train loss: 0.350, Test loss: 0.902, Test accuracy: 63.55
Round  42, Train loss: 0.415, Test loss: 0.915, Test accuracy: 63.07
Round  43, Train loss: 0.322, Test loss: 0.851, Test accuracy: 64.43
Round  44, Train loss: 0.316, Test loss: 0.860, Test accuracy: 63.35
Round  45, Train loss: 0.247, Test loss: 0.813, Test accuracy: 65.90
Round  46, Train loss: 0.321, Test loss: 0.877, Test accuracy: 64.73
Round  47, Train loss: 0.384, Test loss: 0.802, Test accuracy: 66.28
Round  48, Train loss: 0.304, Test loss: 0.833, Test accuracy: 65.60
Round  49, Train loss: 0.376, Test loss: 0.897, Test accuracy: 63.95
Round  50, Train loss: 0.286, Test loss: 0.899, Test accuracy: 64.33
Round  51, Train loss: 0.189, Test loss: 0.781, Test accuracy: 67.12
Round  52, Train loss: 0.238, Test loss: 0.856, Test accuracy: 65.27
Round  53, Train loss: 0.367, Test loss: 0.820, Test accuracy: 66.95
Round  54, Train loss: 0.328, Test loss: 0.867, Test accuracy: 64.63
Round  55, Train loss: 0.434, Test loss: 1.017, Test accuracy: 60.77
Round  56, Train loss: 0.256, Test loss: 1.001, Test accuracy: 61.83
Round  57, Train loss: 0.245, Test loss: 0.910, Test accuracy: 64.78
Round  58, Train loss: 0.322, Test loss: 0.883, Test accuracy: 65.97
Round  59, Train loss: 0.210, Test loss: 0.883, Test accuracy: 66.25
Round  60, Train loss: 0.254, Test loss: 0.904, Test accuracy: 65.10
Round  61, Train loss: 0.265, Test loss: 0.846, Test accuracy: 65.87
Round  62, Train loss: 0.223, Test loss: 0.809, Test accuracy: 66.73
Round  63, Train loss: 0.306, Test loss: 0.806, Test accuracy: 66.77
Round  64, Train loss: 0.338, Test loss: 0.862, Test accuracy: 65.13
Round  65, Train loss: 0.210, Test loss: 0.953, Test accuracy: 63.88
Round  66, Train loss: 0.374, Test loss: 0.935, Test accuracy: 62.47
Round  67, Train loss: 0.314, Test loss: 0.970, Test accuracy: 62.62
Round  68, Train loss: 0.261, Test loss: 0.919, Test accuracy: 63.95
Round  69, Train loss: 0.205, Test loss: 0.901, Test accuracy: 64.47
Round  70, Train loss: 0.191, Test loss: 0.960, Test accuracy: 64.10
Round  71, Train loss: 0.273, Test loss: 1.003, Test accuracy: 63.22
Round  72, Train loss: 0.273, Test loss: 1.017, Test accuracy: 61.60
Round  73, Train loss: 0.231, Test loss: 0.971, Test accuracy: 63.93
Round  74, Train loss: 0.385, Test loss: 1.044, Test accuracy: 61.83
Round  75, Train loss: 0.263, Test loss: 1.058, Test accuracy: 61.03
Round  76, Train loss: 0.256, Test loss: 0.994, Test accuracy: 64.83
Round  77, Train loss: 0.200, Test loss: 0.975, Test accuracy: 66.27
Round  78, Train loss: 0.223, Test loss: 0.938, Test accuracy: 65.10
Round  79, Train loss: 0.317, Test loss: 1.030, Test accuracy: 62.82
Round  80, Train loss: 0.241, Test loss: 0.928, Test accuracy: 65.65
Round  81, Train loss: 0.179, Test loss: 0.871, Test accuracy: 68.20
Round  82, Train loss: 0.247, Test loss: 0.987, Test accuracy: 63.92
Round  83, Train loss: 0.229, Test loss: 0.959, Test accuracy: 64.48
Round  84, Train loss: 0.237, Test loss: 0.939, Test accuracy: 65.27
Round  85, Train loss: 0.227, Test loss: 0.990, Test accuracy: 65.80
Round  86, Train loss: 0.241, Test loss: 0.987, Test accuracy: 65.93
Round  87, Train loss: 0.208, Test loss: 0.925, Test accuracy: 66.52
Round  88, Train loss: 0.267, Test loss: 0.977, Test accuracy: 65.92
Round  89, Train loss: 0.175, Test loss: 0.958, Test accuracy: 66.08
Round  90, Train loss: 0.191, Test loss: 0.973, Test accuracy: 65.40
Round  91, Train loss: 0.234, Test loss: 0.945, Test accuracy: 65.38
Round  92, Train loss: 0.186, Test loss: 0.885, Test accuracy: 66.17
Round  93, Train loss: 0.196, Test loss: 0.909, Test accuracy: 66.35
Round  94, Train loss: 0.212, Test loss: 0.865, Test accuracy: 67.48
Round  95, Train loss: 0.129, Test loss: 0.843, Test accuracy: 67.82
Round  96, Train loss: 0.187, Test loss: 0.885, Test accuracy: 66.87
Round  97, Train loss: 0.223, Test loss: 0.917, Test accuracy: 65.80
Round  98, Train loss: 0.212, Test loss: 0.915, Test accuracy: 65.08
Round  99, Train loss: 0.161, Test loss: 0.905, Test accuracy: 66.35
Final Round, Train loss: 0.168, Test loss: 0.763, Test accuracy: 71.48
Average accuracy final 10 rounds: 66.27
3034.112191915512
[6.4818174839019775, 11.046849250793457, 15.772047281265259, 20.370070457458496, 24.876972913742065, 29.408855438232422, 33.892284870147705, 38.35663676261902, 42.77511811256409, 47.1977972984314, 51.628111600875854, 56.05402660369873, 60.49610233306885, 64.91271924972534, 69.4626202583313, 73.92156672477722, 78.49139785766602, 82.89737725257874, 87.33409929275513, 91.73744606971741, 96.1493706703186, 100.53592872619629, 104.89750528335571, 109.3252158164978, 113.73285126686096, 118.14826965332031, 122.55861735343933, 126.95744824409485, 131.3661870956421, 135.7560350894928, 140.15968799591064, 144.65174460411072, 149.1385350227356, 153.54574370384216, 157.94170761108398, 162.3542730808258, 166.76221203804016, 171.17247986793518, 175.58396553993225, 179.99697947502136, 184.47715973854065, 188.97572779655457, 193.47186493873596, 197.99239206314087, 202.46099543571472, 206.91011500358582, 211.36169338226318, 215.7876696586609, 220.21231174468994, 224.67526817321777, 229.11612844467163, 233.56639194488525, 238.00123262405396, 242.44812870025635, 246.90533900260925, 251.35444688796997, 255.8007743358612, 260.2446904182434, 264.6922297477722, 269.12818336486816, 273.5776274204254, 278.0949499607086, 282.5143268108368, 287.1861340999603, 291.61934757232666, 296.04786682128906, 300.473806142807, 304.88958764076233, 309.4066631793976, 313.80199003219604, 318.18855810165405, 322.57242822647095, 326.96093463897705, 331.3860921859741, 335.78372073173523, 340.17407274246216, 344.5673248767853, 348.96803402900696, 353.41667318344116, 357.7858214378357, 362.1587190628052, 366.5338656902313, 370.90941524505615, 375.27948665618896, 379.65743470191956, 384.0390074253082, 388.4492735862732, 392.88179421424866, 397.2453348636627, 401.604877948761, 406.1255760192871, 410.5030047893524, 414.88015151023865, 419.25705790519714, 423.62836027145386, 427.99688148498535, 432.3518702983856, 436.72456097602844, 441.0958151817322, 445.46003913879395, 456.06810116767883]
[21.283333333333335, 31.85, 37.78333333333333, 39.68333333333333, 44.81666666666667, 47.36666666666667, 53.18333333333333, 52.31666666666667, 55.38333333333333, 56.35, 57.93333333333333, 56.78333333333333, 55.9, 56.38333333333333, 55.5, 53.7, 58.833333333333336, 53.833333333333336, 57.75, 60.4, 60.63333333333333, 62.083333333333336, 62.78333333333333, 62.35, 60.233333333333334, 61.36666666666667, 64.61666666666666, 61.36666666666667, 63.05, 64.53333333333333, 61.4, 62.75, 61.65, 61.46666666666667, 62.916666666666664, 58.71666666666667, 59.68333333333333, 60.55, 61.11666666666667, 64.35, 64.7, 63.55, 63.06666666666667, 64.43333333333334, 63.35, 65.9, 64.73333333333333, 66.28333333333333, 65.6, 63.95, 64.33333333333333, 67.11666666666666, 65.26666666666667, 66.95, 64.63333333333334, 60.766666666666666, 61.833333333333336, 64.78333333333333, 65.96666666666667, 66.25, 65.1, 65.86666666666666, 66.73333333333333, 66.76666666666667, 65.13333333333334, 63.88333333333333, 62.46666666666667, 62.61666666666667, 63.95, 64.46666666666667, 64.1, 63.21666666666667, 61.6, 63.93333333333333, 61.833333333333336, 61.03333333333333, 64.83333333333333, 66.26666666666667, 65.1, 62.81666666666667, 65.65, 68.2, 63.916666666666664, 64.48333333333333, 65.26666666666667, 65.8, 65.93333333333334, 66.51666666666667, 65.91666666666667, 66.08333333333333, 65.4, 65.38333333333334, 66.16666666666667, 66.35, 67.48333333333333, 67.81666666666666, 66.86666666666666, 65.8, 65.08333333333333, 66.35, 71.48333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: center_psl, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2033
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0433
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5533
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0433
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4567
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.2933
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1467
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4967
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.1933
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.304, Test loss: 2.030, Test accuracy: 29.72
Round   1, Train loss: 1.109, Test loss: 2.085, Test accuracy: 32.25
Round   2, Train loss: 1.047, Test loss: 1.519, Test accuracy: 41.12
Round   3, Train loss: 0.969, Test loss: 1.420, Test accuracy: 41.80
Round   4, Train loss: 0.986, Test loss: 1.357, Test accuracy: 47.93
Round   5, Train loss: 0.800, Test loss: 1.377, Test accuracy: 47.62
Round   6, Train loss: 0.823, Test loss: 1.143, Test accuracy: 54.00
Round   7, Train loss: 0.791, Test loss: 1.090, Test accuracy: 53.85
Round   8, Train loss: 0.711, Test loss: 1.044, Test accuracy: 53.57
Round   9, Train loss: 0.818, Test loss: 1.110, Test accuracy: 51.42
Round  10, Train loss: 0.759, Test loss: 1.020, Test accuracy: 55.23
Round  11, Train loss: 0.902, Test loss: 0.974, Test accuracy: 56.10
Round  12, Train loss: 0.513, Test loss: 0.964, Test accuracy: 58.42
Round  13, Train loss: 0.562, Test loss: 1.020, Test accuracy: 57.03
Round  14, Train loss: 0.765, Test loss: 0.975, Test accuracy: 59.37
Round  15, Train loss: 0.671, Test loss: 1.017, Test accuracy: 56.13
Round  16, Train loss: 0.616, Test loss: 1.021, Test accuracy: 57.97
Round  17, Train loss: 0.661, Test loss: 1.042, Test accuracy: 56.83
Round  18, Train loss: 0.586, Test loss: 0.989, Test accuracy: 58.68
Round  19, Train loss: 0.526, Test loss: 0.902, Test accuracy: 61.92
Round  20, Train loss: 0.613, Test loss: 0.895, Test accuracy: 61.57
Round  21, Train loss: 0.663, Test loss: 0.921, Test accuracy: 60.88
Round  22, Train loss: 0.628, Test loss: 0.934, Test accuracy: 61.77
Round  23, Train loss: 0.506, Test loss: 0.923, Test accuracy: 62.82
Round  24, Train loss: 0.534, Test loss: 0.977, Test accuracy: 60.17
Round  25, Train loss: 0.570, Test loss: 0.979, Test accuracy: 59.27
Round  26, Train loss: 0.410, Test loss: 0.940, Test accuracy: 61.10
Round  27, Train loss: 0.532, Test loss: 0.911, Test accuracy: 61.67
Round  28, Train loss: 0.512, Test loss: 0.969, Test accuracy: 60.57
Round  29, Train loss: 0.384, Test loss: 0.940, Test accuracy: 61.33
Round  30, Train loss: 0.395, Test loss: 0.962, Test accuracy: 61.72
Round  31, Train loss: 0.492, Test loss: 0.942, Test accuracy: 63.43
Round  32, Train loss: 0.396, Test loss: 0.939, Test accuracy: 63.57
Round  33, Train loss: 0.388, Test loss: 0.867, Test accuracy: 65.38
Round  34, Train loss: 0.371, Test loss: 0.874, Test accuracy: 65.05
Round  35, Train loss: 0.497, Test loss: 0.885, Test accuracy: 63.47
Round  36, Train loss: 0.389, Test loss: 0.912, Test accuracy: 63.05
Round  37, Train loss: 0.381, Test loss: 0.942, Test accuracy: 64.05
Round  38, Train loss: 0.401, Test loss: 0.941, Test accuracy: 64.53
Round  39, Train loss: 0.354, Test loss: 0.874, Test accuracy: 66.03
Round  40, Train loss: 0.315, Test loss: 0.876, Test accuracy: 65.08
Round  41, Train loss: 0.358, Test loss: 0.890, Test accuracy: 65.23
Round  42, Train loss: 0.429, Test loss: 0.921, Test accuracy: 64.72
Round  43, Train loss: 0.343, Test loss: 0.866, Test accuracy: 66.35
Round  44, Train loss: 0.332, Test loss: 0.922, Test accuracy: 64.12
Round  45, Train loss: 0.239, Test loss: 0.943, Test accuracy: 64.40
Round  46, Train loss: 0.290, Test loss: 0.932, Test accuracy: 64.70
Round  47, Train loss: 0.343, Test loss: 0.926, Test accuracy: 65.03
Round  48, Train loss: 0.272, Test loss: 0.924, Test accuracy: 65.83
Round  49, Train loss: 0.344, Test loss: 0.939, Test accuracy: 64.88
Round  50, Train loss: 0.284, Test loss: 0.867, Test accuracy: 66.08
Round  51, Train loss: 0.203, Test loss: 0.846, Test accuracy: 68.20
Round  52, Train loss: 0.217, Test loss: 0.835, Test accuracy: 68.73
Round  53, Train loss: 0.378, Test loss: 0.895, Test accuracy: 65.78
Round  54, Train loss: 0.258, Test loss: 0.895, Test accuracy: 65.85
Round  55, Train loss: 0.352, Test loss: 0.945, Test accuracy: 65.47
Round  56, Train loss: 0.250, Test loss: 0.904, Test accuracy: 66.60
Round  57, Train loss: 0.226, Test loss: 0.887, Test accuracy: 67.63
Round  58, Train loss: 0.353, Test loss: 0.884, Test accuracy: 66.20
Round  59, Train loss: 0.187, Test loss: 0.875, Test accuracy: 66.83
Round  60, Train loss: 0.226, Test loss: 0.835, Test accuracy: 68.38
Round  61, Train loss: 0.238, Test loss: 0.822, Test accuracy: 69.23
Round  62, Train loss: 0.216, Test loss: 0.771, Test accuracy: 70.45
Round  63, Train loss: 0.273, Test loss: 0.822, Test accuracy: 68.67
Round  64, Train loss: 0.282, Test loss: 0.886, Test accuracy: 67.25
Round  65, Train loss: 0.206, Test loss: 0.874, Test accuracy: 67.70
Round  66, Train loss: 0.298, Test loss: 0.983, Test accuracy: 65.15
Round  67, Train loss: 0.235, Test loss: 0.905, Test accuracy: 66.40
Round  68, Train loss: 0.242, Test loss: 0.904, Test accuracy: 65.97
Round  69, Train loss: 0.191, Test loss: 0.857, Test accuracy: 66.67
Round  70, Train loss: 0.185, Test loss: 0.876, Test accuracy: 66.50
Round  71, Train loss: 0.250, Test loss: 0.910, Test accuracy: 65.65
Round  72, Train loss: 0.227, Test loss: 0.950, Test accuracy: 65.18
Round  73, Train loss: 0.208, Test loss: 0.898, Test accuracy: 66.90
Round  74, Train loss: 0.290, Test loss: 0.998, Test accuracy: 64.15
Round  75, Train loss: 0.218, Test loss: 0.956, Test accuracy: 64.67
Round  76, Train loss: 0.208, Test loss: 1.014, Test accuracy: 64.87
Round  77, Train loss: 0.167, Test loss: 0.954, Test accuracy: 66.00
Round  78, Train loss: 0.255, Test loss: 0.885, Test accuracy: 68.45
Round  79, Train loss: 0.240, Test loss: 0.905, Test accuracy: 66.25
Round  80, Train loss: 0.210, Test loss: 0.885, Test accuracy: 66.58
Round  81, Train loss: 0.159, Test loss: 0.822, Test accuracy: 69.33
Round  82, Train loss: 0.209, Test loss: 0.881, Test accuracy: 67.52
Round  83, Train loss: 0.190, Test loss: 0.849, Test accuracy: 68.37
Round  84, Train loss: 0.201, Test loss: 0.880, Test accuracy: 67.18
Round  85, Train loss: 0.204, Test loss: 0.918, Test accuracy: 66.18
Round  86, Train loss: 0.196, Test loss: 0.932, Test accuracy: 66.22
Round  87, Train loss: 0.193, Test loss: 0.956, Test accuracy: 66.22
Round  88, Train loss: 0.216, Test loss: 0.955, Test accuracy: 66.02
Round  89, Train loss: 0.145, Test loss: 0.956, Test accuracy: 66.28
Round  90, Train loss: 0.169, Test loss: 0.968, Test accuracy: 66.07
Round  91, Train loss: 0.170, Test loss: 0.951, Test accuracy: 66.27
Round  92, Train loss: 0.203, Test loss: 0.892, Test accuracy: 67.98
Round  93, Train loss: 0.162, Test loss: 0.907, Test accuracy: 68.02
Round  94, Train loss: 0.155, Test loss: 0.893, Test accuracy: 67.63
Round  95, Train loss: 0.116, Test loss: 0.884, Test accuracy: 69.32
Round  96, Train loss: 0.161, Test loss: 0.898, Test accuracy: 68.23
Round  97, Train loss: 0.169, Test loss: 0.913, Test accuracy: 68.55
Round  98, Train loss: 0.167, Test loss: 0.890, Test accuracy: 68.58
Round  99, Train loss: 0.134, Test loss: 0.871, Test accuracy: 69.25
Final Round, Train loss: 0.182, Test loss: 0.873, Test accuracy: 70.22
Average accuracy final 10 rounds: 67.99
3288.0931713581085
[6.685522079467773, 12.022695541381836, 17.046496868133545, 22.454634189605713, 27.719124794006348, 32.41273236274719, 37.3728129863739, 42.44435453414917, 47.47904562950134, 52.17388916015625, 57.41991949081421, 62.67574596405029, 67.4300172328949, 72.29315376281738, 77.61436200141907, 82.40306425094604, 87.20442795753479, 91.99336433410645, 96.86681461334229, 101.74405288696289, 107.00670123100281, 112.24728465080261, 117.47663068771362, 122.23396348953247, 127.01730394363403, 132.30767107009888, 137.1701741218567, 142.41846013069153, 147.73270988464355, 152.62769389152527, 157.5043065547943, 162.57327151298523, 167.5863025188446, 172.6382884979248, 177.38680338859558, 182.68128609657288, 187.4352684020996, 192.70660400390625, 197.54219889640808, 202.8788766860962, 207.647155046463, 212.40050506591797, 217.16095638275146, 222.42809915542603, 227.3097789287567, 232.37887334823608, 237.66164445877075, 242.92317032814026, 248.18138360977173, 253.4372055530548, 258.1708505153656, 262.8962371349335, 267.64692854881287, 272.67693424224854, 277.4226939678192, 282.6777033805847, 287.42630553245544, 292.15567445755005, 297.1549983024597, 301.8619246482849, 306.5896580219269, 311.5843961238861, 316.29199981689453, 321.5247611999512, 326.529039144516, 331.28271675109863, 336.61938428878784, 341.8374619483948, 346.54431200027466, 351.2574315071106, 355.9828631877899, 360.69852590560913, 365.92013669013977, 371.1351225376129, 376.3637852668762, 381.0938673019409, 386.3415274620056, 391.5554313659668, 396.56908535957336, 401.8174946308136, 406.5348675251007, 411.25862741470337, 416.2635362148285, 421.26713395118713, 426.0219871997833, 430.75078105926514, 435.7306373119354, 441.13785314559937, 446.50413823127747, 451.7866179943085, 457.0849277973175, 462.3509337902069, 467.12780714035034, 472.36113357543945, 477.14718747138977, 481.89710760116577, 486.6307735443115, 491.88743710517883, 497.2112491130829, 501.9925103187561, 515.2800748348236]
[29.716666666666665, 32.25, 41.11666666666667, 41.8, 47.93333333333333, 47.61666666666667, 54.0, 53.85, 53.56666666666667, 51.416666666666664, 55.233333333333334, 56.1, 58.416666666666664, 57.03333333333333, 59.36666666666667, 56.13333333333333, 57.96666666666667, 56.833333333333336, 58.68333333333333, 61.916666666666664, 61.56666666666667, 60.88333333333333, 61.766666666666666, 62.81666666666667, 60.166666666666664, 59.266666666666666, 61.1, 61.666666666666664, 60.56666666666667, 61.333333333333336, 61.71666666666667, 63.43333333333333, 63.56666666666667, 65.38333333333334, 65.05, 63.46666666666667, 63.05, 64.05, 64.53333333333333, 66.03333333333333, 65.08333333333333, 65.23333333333333, 64.71666666666667, 66.35, 64.11666666666666, 64.4, 64.7, 65.03333333333333, 65.83333333333333, 64.88333333333334, 66.08333333333333, 68.2, 68.73333333333333, 65.78333333333333, 65.85, 65.46666666666667, 66.6, 67.63333333333334, 66.2, 66.83333333333333, 68.38333333333334, 69.23333333333333, 70.45, 68.66666666666667, 67.25, 67.7, 65.15, 66.4, 65.96666666666667, 66.66666666666667, 66.5, 65.65, 65.18333333333334, 66.9, 64.15, 64.66666666666667, 64.86666666666666, 66.0, 68.45, 66.25, 66.58333333333333, 69.33333333333333, 67.51666666666667, 68.36666666666666, 67.18333333333334, 66.18333333333334, 66.21666666666667, 66.21666666666667, 66.01666666666667, 66.28333333333333, 66.06666666666666, 66.26666666666667, 67.98333333333333, 68.01666666666667, 67.63333333333334, 69.31666666666666, 68.23333333333333, 68.55, 68.58333333333333, 69.25, 70.21666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4367
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3567
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.6800
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.3367
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5300
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.5200
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.2967
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4400
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5667
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4967
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.709, Test loss: 2.162, Test accuracy: 17.62
Round   1, Train loss: 1.266, Test loss: 2.024, Test accuracy: 22.22
Round   2, Train loss: 1.198, Test loss: 1.648, Test accuracy: 34.78
Round   3, Train loss: 1.179, Test loss: 1.535, Test accuracy: 37.37
Round   4, Train loss: 1.147, Test loss: 1.218, Test accuracy: 44.93
Round   5, Train loss: 1.067, Test loss: 1.278, Test accuracy: 45.47
Round   6, Train loss: 1.221, Test loss: 1.254, Test accuracy: 42.63
Round   7, Train loss: 1.207, Test loss: 1.219, Test accuracy: 42.05
Round   8, Train loss: 1.020, Test loss: 1.047, Test accuracy: 49.28
Round   9, Train loss: 0.976, Test loss: 1.067, Test accuracy: 47.90
Round  10, Train loss: 1.022, Test loss: 1.066, Test accuracy: 49.55
Round  11, Train loss: 1.026, Test loss: 1.026, Test accuracy: 53.28
Round  12, Train loss: 0.932, Test loss: 0.961, Test accuracy: 53.62
Round  13, Train loss: 1.050, Test loss: 0.952, Test accuracy: 53.17
Round  14, Train loss: 1.039, Test loss: 0.944, Test accuracy: 53.85
Round  15, Train loss: 1.095, Test loss: 0.946, Test accuracy: 53.22
Round  16, Train loss: 1.006, Test loss: 0.948, Test accuracy: 54.43
Round  17, Train loss: 0.972, Test loss: 0.932, Test accuracy: 56.75
Round  18, Train loss: 0.995, Test loss: 0.909, Test accuracy: 57.65
Round  19, Train loss: 0.857, Test loss: 0.902, Test accuracy: 57.12
Round  20, Train loss: 1.009, Test loss: 0.898, Test accuracy: 58.02
Round  21, Train loss: 0.980, Test loss: 0.898, Test accuracy: 57.10
Round  22, Train loss: 0.984, Test loss: 0.888, Test accuracy: 57.37
Round  23, Train loss: 0.956, Test loss: 0.873, Test accuracy: 59.62
Round  24, Train loss: 1.065, Test loss: 0.883, Test accuracy: 60.15
Round  25, Train loss: 1.027, Test loss: 0.867, Test accuracy: 60.90
Round  26, Train loss: 0.916, Test loss: 0.863, Test accuracy: 62.17
Round  27, Train loss: 0.804, Test loss: 0.851, Test accuracy: 62.02
Round  28, Train loss: 0.909, Test loss: 0.856, Test accuracy: 61.73
Round  29, Train loss: 0.908, Test loss: 0.844, Test accuracy: 63.30
Round  30, Train loss: 1.040, Test loss: 0.836, Test accuracy: 64.25
Round  31, Train loss: 0.725, Test loss: 0.835, Test accuracy: 63.28
Round  32, Train loss: 1.019, Test loss: 0.840, Test accuracy: 63.07
Round  33, Train loss: 0.987, Test loss: 0.843, Test accuracy: 62.82
Round  34, Train loss: 0.982, Test loss: 0.842, Test accuracy: 62.12
Round  35, Train loss: 0.885, Test loss: 0.827, Test accuracy: 64.33
Round  36, Train loss: 0.930, Test loss: 0.826, Test accuracy: 63.25
Round  37, Train loss: 0.881, Test loss: 0.833, Test accuracy: 62.85
Round  38, Train loss: 0.950, Test loss: 0.842, Test accuracy: 62.63
Round  39, Train loss: 0.875, Test loss: 0.831, Test accuracy: 64.13
Round  40, Train loss: 0.990, Test loss: 0.832, Test accuracy: 64.05
Round  41, Train loss: 0.971, Test loss: 0.822, Test accuracy: 65.17
Round  42, Train loss: 0.858, Test loss: 0.805, Test accuracy: 65.73
Round  43, Train loss: 0.887, Test loss: 0.800, Test accuracy: 65.28
Round  44, Train loss: 0.713, Test loss: 0.794, Test accuracy: 66.65
Round  45, Train loss: 0.959, Test loss: 0.790, Test accuracy: 66.53
Round  46, Train loss: 0.863, Test loss: 0.784, Test accuracy: 66.77
Round  47, Train loss: 0.693, Test loss: 0.776, Test accuracy: 67.28
Round  48, Train loss: 0.845, Test loss: 0.773, Test accuracy: 66.58
Round  49, Train loss: 0.985, Test loss: 0.791, Test accuracy: 65.25
Round  50, Train loss: 0.892, Test loss: 0.797, Test accuracy: 65.05
Round  51, Train loss: 0.845, Test loss: 0.782, Test accuracy: 66.65
Round  52, Train loss: 0.768, Test loss: 0.780, Test accuracy: 66.80
Round  53, Train loss: 0.708, Test loss: 0.774, Test accuracy: 66.97
Round  54, Train loss: 0.929, Test loss: 0.774, Test accuracy: 67.55
Round  55, Train loss: 0.954, Test loss: 0.780, Test accuracy: 67.42
Round  56, Train loss: 0.894, Test loss: 0.778, Test accuracy: 66.97
Round  57, Train loss: 0.728, Test loss: 0.774, Test accuracy: 67.28
Round  58, Train loss: 0.942, Test loss: 0.777, Test accuracy: 67.92
Round  59, Train loss: 0.842, Test loss: 0.780, Test accuracy: 67.15
Round  60, Train loss: 0.734, Test loss: 0.766, Test accuracy: 67.85
Round  61, Train loss: 0.848, Test loss: 0.753, Test accuracy: 68.10
Round  62, Train loss: 0.644, Test loss: 0.753, Test accuracy: 67.20
Round  63, Train loss: 0.705, Test loss: 0.755, Test accuracy: 67.27
Round  64, Train loss: 0.819, Test loss: 0.759, Test accuracy: 67.08
Round  65, Train loss: 0.725, Test loss: 0.759, Test accuracy: 66.60
Round  66, Train loss: 0.657, Test loss: 0.756, Test accuracy: 67.02
Round  67, Train loss: 0.859, Test loss: 0.747, Test accuracy: 67.62
Round  68, Train loss: 0.743, Test loss: 0.755, Test accuracy: 66.85
Round  69, Train loss: 0.790, Test loss: 0.752, Test accuracy: 67.70
Round  70, Train loss: 0.856, Test loss: 0.750, Test accuracy: 67.72
Round  71, Train loss: 0.789, Test loss: 0.756, Test accuracy: 66.50
Round  72, Train loss: 0.706, Test loss: 0.755, Test accuracy: 67.02
Round  73, Train loss: 0.883, Test loss: 0.754, Test accuracy: 66.78
Round  74, Train loss: 0.755, Test loss: 0.756, Test accuracy: 66.72
Round  75, Train loss: 0.644, Test loss: 0.749, Test accuracy: 68.03
Round  76, Train loss: 0.884, Test loss: 0.744, Test accuracy: 68.25
Round  77, Train loss: 0.716, Test loss: 0.735, Test accuracy: 68.85
Round  78, Train loss: 0.693, Test loss: 0.743, Test accuracy: 69.02
Round  79, Train loss: 0.639, Test loss: 0.744, Test accuracy: 68.07
Round  80, Train loss: 0.895, Test loss: 0.743, Test accuracy: 68.30
Round  81, Train loss: 0.938, Test loss: 0.750, Test accuracy: 68.42
Round  82, Train loss: 0.800, Test loss: 0.750, Test accuracy: 68.30
Round  83, Train loss: 0.892, Test loss: 0.761, Test accuracy: 67.57
Round  84, Train loss: 0.837, Test loss: 0.766, Test accuracy: 67.27
Round  85, Train loss: 0.711, Test loss: 0.755, Test accuracy: 68.05
Round  86, Train loss: 0.646, Test loss: 0.743, Test accuracy: 68.10
Round  87, Train loss: 0.625, Test loss: 0.745, Test accuracy: 67.67
Round  88, Train loss: 0.725, Test loss: 0.741, Test accuracy: 67.75
Round  89, Train loss: 0.829, Test loss: 0.746, Test accuracy: 67.97
Round  90, Train loss: 0.587, Test loss: 0.743, Test accuracy: 67.58
Round  91, Train loss: 0.685, Test loss: 0.751, Test accuracy: 67.38
Round  92, Train loss: 0.794, Test loss: 0.758, Test accuracy: 66.97
Round  93, Train loss: 0.654, Test loss: 0.751, Test accuracy: 67.53
Round  94, Train loss: 0.881, Test loss: 0.744, Test accuracy: 67.82
Round  95, Train loss: 0.792, Test loss: 0.746, Test accuracy: 67.20
Round  96, Train loss: 0.777, Test loss: 0.756, Test accuracy: 66.20
Round  97, Train loss: 0.658, Test loss: 0.747, Test accuracy: 66.68
Round  98, Train loss: 0.584, Test loss: 0.738, Test accuracy: 67.98
Round  99, Train loss: 0.497, Test loss: 0.745, Test accuracy: 68.00
Final Round, Train loss: 0.675, Test loss: 0.747, Test accuracy: 68.03
Average accuracy final 10 rounds: 67.33500000000001
875.9073119163513
[3.049468517303467, 4.158004522323608, 5.259705543518066, 6.374602556228638, 7.480669736862183, 8.582704305648804, 9.68998670578003, 10.796297788619995, 11.898882627487183, 13.001901149749756, 14.108996391296387, 15.21651554107666, 16.323283672332764, 17.43269395828247, 18.544782400131226, 19.65047550201416, 20.752782821655273, 21.85668659210205, 22.961668014526367, 24.06406569480896, 25.17434024810791, 26.283191919326782, 27.42906904220581, 28.557456254959106, 29.668447732925415, 30.776652336120605, 31.88562250137329, 33.126904249191284, 34.369025468826294, 35.471673011779785, 36.58161997795105, 37.68706154823303, 38.7995445728302, 39.90157175064087, 41.00998282432556, 42.11386179924011, 43.22064971923828, 44.32192301750183, 45.4239706993103, 46.520275831222534, 47.62035632133484, 48.72288513183594, 49.827860832214355, 50.92596197128296, 52.02415323257446, 53.12819242477417, 54.23191571235657, 55.33371424674988, 56.43761134147644, 57.537487506866455, 58.65982437133789, 59.75599408149719, 60.855008602142334, 61.955249309539795, 63.04845094680786, 64.14829587936401, 65.24604415893555, 66.34075045585632, 67.4443428516388, 68.55262875556946, 69.65507960319519, 70.75705409049988, 71.86037254333496, 72.96329379081726, 74.06795144081116, 75.16948890686035, 76.26455879211426, 77.36152839660645, 78.4570791721344, 79.56092238426208, 80.70162081718445, 81.83491659164429, 82.93673205375671, 84.03875017166138, 85.13576745986938, 86.23357462882996, 87.34631371498108, 88.4380841255188, 89.53643608093262, 90.64108419418335, 91.74080729484558, 92.8373155593872, 93.93435454368591, 95.03202795982361, 96.13381505012512, 97.23357772827148, 98.33247423171997, 99.43525314331055, 100.53649258613586, 101.64123916625977, 102.73970150947571, 103.83389902114868, 104.93420314788818, 106.0331518650055, 107.12597870826721, 108.22227048873901, 109.32216620445251, 110.41892647743225, 111.52134037017822, 112.61970734596252, 114.21750378608704]
[17.616666666666667, 22.216666666666665, 34.78333333333333, 37.36666666666667, 44.93333333333333, 45.46666666666667, 42.63333333333333, 42.05, 49.28333333333333, 47.9, 49.55, 53.28333333333333, 53.61666666666667, 53.166666666666664, 53.85, 53.21666666666667, 54.43333333333333, 56.75, 57.65, 57.11666666666667, 58.016666666666666, 57.1, 57.36666666666667, 59.61666666666667, 60.15, 60.9, 62.166666666666664, 62.016666666666666, 61.733333333333334, 63.3, 64.25, 63.28333333333333, 63.06666666666667, 62.81666666666667, 62.11666666666667, 64.33333333333333, 63.25, 62.85, 62.63333333333333, 64.13333333333334, 64.05, 65.16666666666667, 65.73333333333333, 65.28333333333333, 66.65, 66.53333333333333, 66.76666666666667, 67.28333333333333, 66.58333333333333, 65.25, 65.05, 66.65, 66.8, 66.96666666666667, 67.55, 67.41666666666667, 66.96666666666667, 67.28333333333333, 67.91666666666667, 67.15, 67.85, 68.1, 67.2, 67.26666666666667, 67.08333333333333, 66.6, 67.01666666666667, 67.61666666666666, 66.85, 67.7, 67.71666666666667, 66.5, 67.01666666666667, 66.78333333333333, 66.71666666666667, 68.03333333333333, 68.25, 68.85, 69.01666666666667, 68.06666666666666, 68.3, 68.41666666666667, 68.3, 67.56666666666666, 67.26666666666667, 68.05, 68.1, 67.66666666666667, 67.75, 67.96666666666667, 67.58333333333333, 67.38333333333334, 66.96666666666667, 67.53333333333333, 67.81666666666666, 67.2, 66.2, 66.68333333333334, 67.98333333333333, 68.0, 68.03333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4367
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3567
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.6800
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.3367
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5300
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.5200
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.2967
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4400
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5567
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4267
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.709, Test loss: 2.191, Test accuracy: 16.80
Round   1, Train loss: 1.246, Test loss: 2.100, Test accuracy: 20.88
Round   2, Train loss: 1.185, Test loss: 1.675, Test accuracy: 32.87
Round   3, Train loss: 1.134, Test loss: 1.551, Test accuracy: 33.63
Round   4, Train loss: 1.106, Test loss: 1.230, Test accuracy: 45.32
Round   5, Train loss: 0.995, Test loss: 1.319, Test accuracy: 42.07
Round   6, Train loss: 1.137, Test loss: 1.274, Test accuracy: 40.10
Round   7, Train loss: 1.116, Test loss: 1.234, Test accuracy: 43.67
Round   8, Train loss: 0.986, Test loss: 1.048, Test accuracy: 50.57
Round   9, Train loss: 0.939, Test loss: 1.066, Test accuracy: 49.27
Round  10, Train loss: 0.987, Test loss: 1.079, Test accuracy: 49.08
Round  11, Train loss: 1.002, Test loss: 1.034, Test accuracy: 52.55
Round  12, Train loss: 0.936, Test loss: 0.955, Test accuracy: 53.73
Round  13, Train loss: 0.977, Test loss: 0.960, Test accuracy: 52.22
Round  14, Train loss: 0.986, Test loss: 0.963, Test accuracy: 52.08
Round  15, Train loss: 1.040, Test loss: 0.987, Test accuracy: 52.68
Round  16, Train loss: 0.949, Test loss: 0.959, Test accuracy: 54.45
Round  17, Train loss: 0.924, Test loss: 0.937, Test accuracy: 55.25
Round  18, Train loss: 0.932, Test loss: 0.919, Test accuracy: 54.92
Round  19, Train loss: 0.820, Test loss: 0.905, Test accuracy: 55.35
Round  20, Train loss: 0.962, Test loss: 0.903, Test accuracy: 55.55
Round  21, Train loss: 0.938, Test loss: 0.914, Test accuracy: 55.83
Round  22, Train loss: 0.929, Test loss: 0.902, Test accuracy: 56.50
Round  23, Train loss: 0.898, Test loss: 0.897, Test accuracy: 56.67
Round  24, Train loss: 0.994, Test loss: 0.908, Test accuracy: 56.08
Round  25, Train loss: 0.929, Test loss: 0.913, Test accuracy: 55.37
Round  26, Train loss: 0.854, Test loss: 0.906, Test accuracy: 56.48
Round  27, Train loss: 0.785, Test loss: 0.904, Test accuracy: 57.00
Round  28, Train loss: 0.858, Test loss: 0.892, Test accuracy: 57.70
Round  29, Train loss: 0.871, Test loss: 0.891, Test accuracy: 56.45
Round  30, Train loss: 0.979, Test loss: 0.891, Test accuracy: 57.10
Round  31, Train loss: 0.683, Test loss: 0.861, Test accuracy: 58.43
Round  32, Train loss: 0.966, Test loss: 0.869, Test accuracy: 58.37
Round  33, Train loss: 0.904, Test loss: 0.892, Test accuracy: 55.83
Round  34, Train loss: 0.919, Test loss: 0.886, Test accuracy: 57.17
Round  35, Train loss: 0.818, Test loss: 0.881, Test accuracy: 58.07
Round  36, Train loss: 0.866, Test loss: 0.856, Test accuracy: 59.27
Round  37, Train loss: 0.831, Test loss: 0.870, Test accuracy: 59.38
Round  38, Train loss: 0.877, Test loss: 0.873, Test accuracy: 59.08
Round  39, Train loss: 0.824, Test loss: 0.860, Test accuracy: 59.80
Round  40, Train loss: 0.907, Test loss: 0.863, Test accuracy: 60.13
Round  41, Train loss: 0.880, Test loss: 0.867, Test accuracy: 59.92
Round  42, Train loss: 0.828, Test loss: 0.845, Test accuracy: 61.47
Round  43, Train loss: 0.832, Test loss: 0.845, Test accuracy: 62.50
Round  44, Train loss: 0.699, Test loss: 0.837, Test accuracy: 63.25
Round  45, Train loss: 0.894, Test loss: 0.833, Test accuracy: 62.97
Round  46, Train loss: 0.824, Test loss: 0.839, Test accuracy: 62.35
Round  47, Train loss: 0.683, Test loss: 0.850, Test accuracy: 62.55
Round  48, Train loss: 0.798, Test loss: 0.835, Test accuracy: 62.70
Round  49, Train loss: 0.915, Test loss: 0.844, Test accuracy: 61.63
Round  50, Train loss: 0.815, Test loss: 0.847, Test accuracy: 61.93
Round  51, Train loss: 0.767, Test loss: 0.841, Test accuracy: 63.02
Round  52, Train loss: 0.737, Test loss: 0.851, Test accuracy: 62.70
Round  53, Train loss: 0.673, Test loss: 0.819, Test accuracy: 63.70
Round  54, Train loss: 0.851, Test loss: 0.836, Test accuracy: 63.67
Round  55, Train loss: 0.911, Test loss: 0.840, Test accuracy: 62.73
Round  56, Train loss: 0.819, Test loss: 0.828, Test accuracy: 64.40
Round  57, Train loss: 0.706, Test loss: 0.826, Test accuracy: 64.63
Round  58, Train loss: 0.894, Test loss: 0.863, Test accuracy: 62.60
Round  59, Train loss: 0.774, Test loss: 0.840, Test accuracy: 63.42
Round  60, Train loss: 0.712, Test loss: 0.828, Test accuracy: 63.80
Round  61, Train loss: 0.815, Test loss: 0.860, Test accuracy: 62.55
Round  62, Train loss: 0.618, Test loss: 0.845, Test accuracy: 62.17
Round  63, Train loss: 0.668, Test loss: 0.827, Test accuracy: 62.22
Round  64, Train loss: 0.763, Test loss: 0.827, Test accuracy: 63.77
Round  65, Train loss: 0.658, Test loss: 0.820, Test accuracy: 63.77
Round  66, Train loss: 0.648, Test loss: 0.826, Test accuracy: 63.93
Round  67, Train loss: 0.831, Test loss: 0.819, Test accuracy: 64.32
Round  68, Train loss: 0.699, Test loss: 0.810, Test accuracy: 64.42
Round  69, Train loss: 0.755, Test loss: 0.807, Test accuracy: 64.92
Round  70, Train loss: 0.820, Test loss: 0.803, Test accuracy: 65.53
Round  71, Train loss: 0.746, Test loss: 0.809, Test accuracy: 65.47
Round  72, Train loss: 0.652, Test loss: 0.805, Test accuracy: 65.52
Round  73, Train loss: 0.830, Test loss: 0.828, Test accuracy: 64.72
Round  74, Train loss: 0.723, Test loss: 0.808, Test accuracy: 64.48
Round  75, Train loss: 0.612, Test loss: 0.813, Test accuracy: 64.97
Round  76, Train loss: 0.840, Test loss: 0.844, Test accuracy: 63.82
Round  77, Train loss: 0.723, Test loss: 0.838, Test accuracy: 64.10
Round  78, Train loss: 0.670, Test loss: 0.840, Test accuracy: 64.05
Round  79, Train loss: 0.611, Test loss: 0.817, Test accuracy: 64.58
Round  80, Train loss: 0.833, Test loss: 0.836, Test accuracy: 64.47
Round  81, Train loss: 0.864, Test loss: 0.846, Test accuracy: 64.35
Round  82, Train loss: 0.769, Test loss: 0.836, Test accuracy: 65.00
Round  83, Train loss: 0.846, Test loss: 0.855, Test accuracy: 64.48
Round  84, Train loss: 0.802, Test loss: 0.831, Test accuracy: 65.65
Round  85, Train loss: 0.674, Test loss: 0.835, Test accuracy: 65.23
Round  86, Train loss: 0.620, Test loss: 0.821, Test accuracy: 65.55
Round  87, Train loss: 0.609, Test loss: 0.826, Test accuracy: 65.20
Round  88, Train loss: 0.703, Test loss: 0.837, Test accuracy: 64.33
Round  89, Train loss: 0.764, Test loss: 0.847, Test accuracy: 63.57
Round  90, Train loss: 0.577, Test loss: 0.839, Test accuracy: 64.32
Round  91, Train loss: 0.665, Test loss: 0.813, Test accuracy: 64.93
Round  92, Train loss: 0.764, Test loss: 0.832, Test accuracy: 64.08
Round  93, Train loss: 0.640, Test loss: 0.813, Test accuracy: 64.33
Round  94, Train loss: 0.844, Test loss: 0.819, Test accuracy: 64.38
Round  95, Train loss: 0.761, Test loss: 0.820, Test accuracy: 64.38
Round  96, Train loss: 0.733, Test loss: 0.818, Test accuracy: 65.40
Round  97, Train loss: 0.621, Test loss: 0.796, Test accuracy: 66.45
Round  98, Train loss: 0.563, Test loss: 0.785, Test accuracy: 66.35
Round  99, Train loss: 0.482, Test loss: 0.783, Test accuracy: 65.68
Final Round, Train loss: 0.646, Test loss: 0.803, Test accuracy: 65.77
Average accuracy final 10 rounds: 65.03166666666667
1638.5236659049988
[4.602602481842041, 7.233553409576416, 9.85696029663086, 12.474320650100708, 15.10202670097351, 17.745148420333862, 20.37651491165161, 23.00550127029419, 25.631541967391968, 28.26404356956482, 30.884987115859985, 33.503360986709595, 36.15731191635132, 38.55742621421814, 40.8723418712616, 43.39232039451599, 45.66766691207886, 47.95371174812317, 50.29737591743469, 52.6284818649292, 54.927302837371826, 57.22442626953125, 59.52123236656189, 61.82444357872009, 64.12841963768005, 66.43313360214233, 68.73458909988403, 71.03924131393433, 73.48658490180969, 75.7770562171936, 78.0845320224762, 80.37613010406494, 82.6614465713501, 84.94362187385559, 87.23355150222778, 89.52431321144104, 91.80389261245728, 94.08173155784607, 96.36969757080078, 98.65993547439575, 100.94533514976501, 103.2423050403595, 105.5328311920166, 107.82472443580627, 110.11817669868469, 112.38545560836792, 114.64990305900574, 116.92414331436157, 119.25214552879333, 121.59383535385132, 123.92364978790283, 126.19432067871094, 128.46033835411072, 130.735595703125, 133.01150226593018, 135.28262090682983, 137.55964279174805, 139.830162525177, 142.1597466468811, 144.4248251914978, 146.69299960136414, 148.97913336753845, 151.2603542804718, 153.54113054275513, 155.8076889514923, 158.0767216682434, 160.3447937965393, 162.61312675476074, 164.8874135017395, 167.1628234386444, 169.43371272087097, 171.70500993728638, 173.98408150672913, 176.25684475898743, 178.52491927146912, 180.79828119277954, 183.07383751869202, 185.35673570632935, 187.62190055847168, 189.88938879966736, 192.15419268608093, 194.41744565963745, 196.68813848495483, 199.04761624336243, 201.3293478488922, 203.61944770812988, 205.9029586315155, 208.1878228187561, 210.47368097305298, 212.7551782131195, 215.04532480239868, 217.31981563568115, 219.5821497440338, 221.84301614761353, 224.10831832885742, 226.37512111663818, 228.630619764328, 230.8920283317566, 233.45360040664673, 235.73964977264404, 239.16389417648315]
[16.8, 20.883333333333333, 32.86666666666667, 33.63333333333333, 45.31666666666667, 42.06666666666667, 40.1, 43.666666666666664, 50.56666666666667, 49.266666666666666, 49.083333333333336, 52.55, 53.733333333333334, 52.21666666666667, 52.083333333333336, 52.68333333333333, 54.45, 55.25, 54.916666666666664, 55.35, 55.55, 55.833333333333336, 56.5, 56.666666666666664, 56.083333333333336, 55.36666666666667, 56.483333333333334, 57.0, 57.7, 56.45, 57.1, 58.43333333333333, 58.36666666666667, 55.833333333333336, 57.166666666666664, 58.06666666666667, 59.266666666666666, 59.38333333333333, 59.083333333333336, 59.8, 60.13333333333333, 59.916666666666664, 61.46666666666667, 62.5, 63.25, 62.96666666666667, 62.35, 62.55, 62.7, 61.63333333333333, 61.93333333333333, 63.016666666666666, 62.7, 63.7, 63.666666666666664, 62.733333333333334, 64.4, 64.63333333333334, 62.6, 63.416666666666664, 63.8, 62.55, 62.166666666666664, 62.21666666666667, 63.766666666666666, 63.766666666666666, 63.93333333333333, 64.31666666666666, 64.41666666666667, 64.91666666666667, 65.53333333333333, 65.46666666666667, 65.51666666666667, 64.71666666666667, 64.48333333333333, 64.96666666666667, 63.81666666666667, 64.1, 64.05, 64.58333333333333, 64.46666666666667, 64.35, 65.0, 64.48333333333333, 65.65, 65.23333333333333, 65.55, 65.2, 64.33333333333333, 63.56666666666667, 64.31666666666666, 64.93333333333334, 64.08333333333333, 64.33333333333333, 64.38333333333334, 64.38333333333334, 65.4, 66.45, 66.35, 65.68333333333334, 65.76666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4367
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3567
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.6800
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.3367
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5300
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.5200
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.2967
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4400
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5567
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4267
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.040, Test loss: 2.170, Test accuracy: 17.42
Round   1, Train loss: 0.759, Test loss: 2.109, Test accuracy: 21.03
Round   2, Train loss: 0.687, Test loss: 1.710, Test accuracy: 31.97
Round   3, Train loss: 0.646, Test loss: 1.732, Test accuracy: 29.97
Round   4, Train loss: 0.698, Test loss: 1.521, Test accuracy: 36.87
Round   5, Train loss: 0.543, Test loss: 1.610, Test accuracy: 39.07
Round   6, Train loss: 0.589, Test loss: 1.651, Test accuracy: 37.17
Round   7, Train loss: 0.619, Test loss: 1.595, Test accuracy: 35.27
Round   8, Train loss: 0.474, Test loss: 1.306, Test accuracy: 46.30
Round   9, Train loss: 0.495, Test loss: 1.233, Test accuracy: 47.07
Round  10, Train loss: 0.526, Test loss: 1.241, Test accuracy: 47.03
Round  11, Train loss: 0.556, Test loss: 1.153, Test accuracy: 49.03
Round  12, Train loss: 0.519, Test loss: 1.206, Test accuracy: 44.10
Round  13, Train loss: 0.558, Test loss: 1.126, Test accuracy: 47.80
Round  14, Train loss: 0.501, Test loss: 1.135, Test accuracy: 48.78
Round  15, Train loss: 0.577, Test loss: 1.127, Test accuracy: 47.75
Round  16, Train loss: 0.498, Test loss: 1.134, Test accuracy: 47.88
Round  17, Train loss: 0.502, Test loss: 1.097, Test accuracy: 49.08
Round  18, Train loss: 0.506, Test loss: 1.103, Test accuracy: 51.48
Round  19, Train loss: 0.442, Test loss: 1.024, Test accuracy: 55.60
Round  20, Train loss: 0.545, Test loss: 1.039, Test accuracy: 53.20
Round  21, Train loss: 0.501, Test loss: 1.108, Test accuracy: 52.08
Round  22, Train loss: 0.502, Test loss: 1.083, Test accuracy: 48.73
Round  23, Train loss: 0.479, Test loss: 1.045, Test accuracy: 51.05
Round  24, Train loss: 0.510, Test loss: 1.082, Test accuracy: 50.93
Round  25, Train loss: 0.518, Test loss: 1.098, Test accuracy: 52.77
Round  26, Train loss: 0.485, Test loss: 1.185, Test accuracy: 51.77
Round  27, Train loss: 0.422, Test loss: 1.140, Test accuracy: 53.47
Round  28, Train loss: 0.417, Test loss: 1.101, Test accuracy: 53.17
Round  29, Train loss: 0.476, Test loss: 1.047, Test accuracy: 56.42
Round  30, Train loss: 0.527, Test loss: 1.140, Test accuracy: 50.32
Round  31, Train loss: 0.370, Test loss: 1.077, Test accuracy: 53.72
Round  32, Train loss: 0.531, Test loss: 1.099, Test accuracy: 54.30
Round  33, Train loss: 0.500, Test loss: 1.037, Test accuracy: 56.63
Round  34, Train loss: 0.534, Test loss: 1.008, Test accuracy: 59.13
Round  35, Train loss: 0.406, Test loss: 1.062, Test accuracy: 56.98
Round  36, Train loss: 0.497, Test loss: 1.041, Test accuracy: 54.93
Round  37, Train loss: 0.472, Test loss: 1.089, Test accuracy: 52.33
Round  38, Train loss: 0.496, Test loss: 1.096, Test accuracy: 51.92
Round  39, Train loss: 0.439, Test loss: 1.134, Test accuracy: 51.32
Round  40, Train loss: 0.517, Test loss: 1.030, Test accuracy: 54.30
Round  41, Train loss: 0.489, Test loss: 1.013, Test accuracy: 54.80
Round  42, Train loss: 0.438, Test loss: 1.136, Test accuracy: 53.25
Round  43, Train loss: 0.411, Test loss: 1.119, Test accuracy: 53.02
Round  44, Train loss: 0.382, Test loss: 1.027, Test accuracy: 55.33
Round  45, Train loss: 0.522, Test loss: 1.043, Test accuracy: 56.88
Round  46, Train loss: 0.445, Test loss: 1.025, Test accuracy: 55.22
Round  47, Train loss: 0.362, Test loss: 0.935, Test accuracy: 59.08
Round  48, Train loss: 0.405, Test loss: 0.941, Test accuracy: 57.62
Round  49, Train loss: 0.484, Test loss: 0.992, Test accuracy: 55.98
Round  50, Train loss: 0.392, Test loss: 1.066, Test accuracy: 54.93
Round  51, Train loss: 0.425, Test loss: 1.040, Test accuracy: 56.83
Round  52, Train loss: 0.363, Test loss: 1.011, Test accuracy: 58.33
Round  53, Train loss: 0.339, Test loss: 1.069, Test accuracy: 56.23
Round  54, Train loss: 0.412, Test loss: 1.174, Test accuracy: 53.80
Round  55, Train loss: 0.443, Test loss: 1.205, Test accuracy: 52.95
Round  56, Train loss: 0.414, Test loss: 1.047, Test accuracy: 57.85
Round  57, Train loss: 0.320, Test loss: 0.980, Test accuracy: 57.55
Round  58, Train loss: 0.467, Test loss: 0.953, Test accuracy: 56.82
Round  59, Train loss: 0.381, Test loss: 0.970, Test accuracy: 57.57
Round  60, Train loss: 0.329, Test loss: 0.971, Test accuracy: 56.98
Round  61, Train loss: 0.397, Test loss: 1.075, Test accuracy: 55.13
Round  62, Train loss: 0.288, Test loss: 1.101, Test accuracy: 55.80
Round  63, Train loss: 0.317, Test loss: 1.121, Test accuracy: 55.08
Round  64, Train loss: 0.375, Test loss: 1.142, Test accuracy: 53.05
Round  65, Train loss: 0.295, Test loss: 1.090, Test accuracy: 55.40
Round  66, Train loss: 0.323, Test loss: 1.130, Test accuracy: 54.37
Round  67, Train loss: 0.350, Test loss: 1.083, Test accuracy: 56.07
Round  68, Train loss: 0.305, Test loss: 1.094, Test accuracy: 57.82
Round  69, Train loss: 0.370, Test loss: 1.153, Test accuracy: 55.87
Round  70, Train loss: 0.414, Test loss: 1.136, Test accuracy: 54.72
Round  71, Train loss: 0.298, Test loss: 1.204, Test accuracy: 53.52
Round  72, Train loss: 0.325, Test loss: 1.279, Test accuracy: 52.13
Round  73, Train loss: 0.411, Test loss: 1.312, Test accuracy: 51.55
Round  74, Train loss: 0.372, Test loss: 1.313, Test accuracy: 51.57
Round  75, Train loss: 0.247, Test loss: 1.278, Test accuracy: 52.40
Round  76, Train loss: 0.343, Test loss: 1.228, Test accuracy: 51.75
Round  77, Train loss: 0.346, Test loss: 1.155, Test accuracy: 53.12
Round  78, Train loss: 0.276, Test loss: 1.170, Test accuracy: 53.52
Round  79, Train loss: 0.233, Test loss: 1.147, Test accuracy: 55.28
Round  80, Train loss: 0.362, Test loss: 1.069, Test accuracy: 56.15
Round  81, Train loss: 0.385, Test loss: 1.131, Test accuracy: 54.78
Round  82, Train loss: 0.331, Test loss: 1.137, Test accuracy: 55.40
Round  83, Train loss: 0.358, Test loss: 1.181, Test accuracy: 54.73
Round  84, Train loss: 0.326, Test loss: 1.209, Test accuracy: 53.83
Round  85, Train loss: 0.270, Test loss: 1.252, Test accuracy: 54.52
Round  86, Train loss: 0.241, Test loss: 1.243, Test accuracy: 54.77
Round  87, Train loss: 0.230, Test loss: 1.152, Test accuracy: 56.08
Round  88, Train loss: 0.279, Test loss: 1.210, Test accuracy: 54.83
Round  89, Train loss: 0.320, Test loss: 1.158, Test accuracy: 55.72
Round  90, Train loss: 0.250, Test loss: 1.099, Test accuracy: 58.08
Round  91, Train loss: 0.275, Test loss: 1.107, Test accuracy: 57.37
Round  92, Train loss: 0.302, Test loss: 1.151, Test accuracy: 56.43
Round  93, Train loss: 0.276, Test loss: 1.139, Test accuracy: 57.02
Round  94, Train loss: 0.332, Test loss: 1.121, Test accuracy: 57.40
Round  95, Train loss: 0.335, Test loss: 1.172, Test accuracy: 57.22
Round  96, Train loss: 0.288, Test loss: 1.173, Test accuracy: 58.30
Round  97, Train loss: 0.246, Test loss: 1.179, Test accuracy: 58.08
Round  98, Train loss: 0.224, Test loss: 1.223, Test accuracy: 57.62
Round  99, Train loss: 0.191, Test loss: 1.122, Test accuracy: 59.33
Final Round, Train loss: 0.239, Test loss: 0.999, Test accuracy: 62.02
Average accuracy final 10 rounds: 57.685
3039.732007265091
[6.942893743515015, 11.328052997589111, 15.72028374671936, 20.12387442588806, 24.541412591934204, 28.94370198249817, 33.323314905166626, 37.724802017211914, 42.12193822860718, 46.54957580566406, 50.9394896030426, 55.32376980781555, 59.76201248168945, 64.11645770072937, 68.6362419128418, 73.05820083618164, 77.43933653831482, 81.81171226501465, 86.16717910766602, 90.58300137519836, 94.95887851715088, 99.34514904022217, 103.72304368019104, 108.10606575012207, 112.48806929588318, 116.84270906448364, 121.23030924797058, 125.59999322891235, 129.95759272575378, 134.3280873298645, 138.67094540596008, 143.0674169063568, 147.52270460128784, 151.87872171401978, 156.2158019542694, 160.58125138282776, 164.9462378025055, 169.33963179588318, 173.68836760520935, 178.0316960811615, 182.4170641899109, 186.80650186538696, 191.151784658432, 195.65421342849731, 200.0979073047638, 204.3960828781128, 208.6805717945099, 212.98127007484436, 217.27915859222412, 221.57694745063782, 225.916166305542, 230.23215913772583, 234.65246725082397, 238.98367953300476, 243.2915940284729, 247.60678791999817, 251.91065573692322, 256.2213695049286, 260.52546095848083, 264.84786915779114, 269.2506561279297, 273.5528841018677, 278.01181721687317, 282.44440627098083, 286.8492941856384, 291.1844024658203, 295.53823590278625, 299.8678123950958, 304.21537470817566, 308.5896644592285, 312.9638822078705, 317.2783365249634, 321.5993504524231, 325.90102195739746, 330.26258659362793, 334.60404324531555, 338.959242105484, 343.9455487728119, 348.24395656585693, 352.55272698402405, 356.8905334472656, 361.1918077468872, 365.4852714538574, 369.7817370891571, 374.0857677459717, 378.4019629955292, 382.8052341938019, 387.104120016098, 391.4241073131561, 395.84826707839966, 400.1565246582031, 404.9351453781128, 409.4744327068329, 414.3622627258301, 419.28857254981995, 424.25668835639954, 429.22965121269226, 434.2024202346802, 439.18320631980896, 444.1673483848572, 456.00564885139465]
[17.416666666666668, 21.033333333333335, 31.966666666666665, 29.966666666666665, 36.86666666666667, 39.06666666666667, 37.166666666666664, 35.266666666666666, 46.3, 47.06666666666667, 47.03333333333333, 49.03333333333333, 44.1, 47.8, 48.78333333333333, 47.75, 47.88333333333333, 49.083333333333336, 51.483333333333334, 55.6, 53.2, 52.083333333333336, 48.733333333333334, 51.05, 50.93333333333333, 52.766666666666666, 51.766666666666666, 53.46666666666667, 53.166666666666664, 56.416666666666664, 50.31666666666667, 53.71666666666667, 54.3, 56.63333333333333, 59.13333333333333, 56.983333333333334, 54.93333333333333, 52.333333333333336, 51.916666666666664, 51.31666666666667, 54.3, 54.8, 53.25, 53.016666666666666, 55.333333333333336, 56.88333333333333, 55.21666666666667, 59.083333333333336, 57.61666666666667, 55.983333333333334, 54.93333333333333, 56.833333333333336, 58.333333333333336, 56.233333333333334, 53.8, 52.95, 57.85, 57.55, 56.81666666666667, 57.56666666666667, 56.983333333333334, 55.13333333333333, 55.8, 55.083333333333336, 53.05, 55.4, 54.36666666666667, 56.06666666666667, 57.81666666666667, 55.86666666666667, 54.71666666666667, 53.516666666666666, 52.13333333333333, 51.55, 51.56666666666667, 52.4, 51.75, 53.11666666666667, 53.516666666666666, 55.28333333333333, 56.15, 54.78333333333333, 55.4, 54.733333333333334, 53.833333333333336, 54.516666666666666, 54.766666666666666, 56.083333333333336, 54.833333333333336, 55.71666666666667, 58.083333333333336, 57.36666666666667, 56.43333333333333, 57.016666666666666, 57.4, 57.21666666666667, 58.3, 58.083333333333336, 57.61666666666667, 59.333333333333336, 62.016666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: center_psl, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4367
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3567
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.6800
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.3367
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5300
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.5200
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.2967
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4400
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5567
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4267
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.449, Test loss: 2.120, Test accuracy: 17.05
Round   1, Train loss: 1.256, Test loss: 2.061, Test accuracy: 24.17
Round   2, Train loss: 1.088, Test loss: 1.774, Test accuracy: 34.52
Round   3, Train loss: 1.099, Test loss: 1.724, Test accuracy: 33.02
Round   4, Train loss: 1.105, Test loss: 1.528, Test accuracy: 40.00
Round   5, Train loss: 0.975, Test loss: 1.509, Test accuracy: 41.80
Round   6, Train loss: 1.204, Test loss: 1.534, Test accuracy: 39.25
Round   7, Train loss: 1.141, Test loss: 1.547, Test accuracy: 36.77
Round   8, Train loss: 0.861, Test loss: 1.376, Test accuracy: 44.33
Round   9, Train loss: 0.792, Test loss: 1.272, Test accuracy: 48.00
Round  10, Train loss: 0.909, Test loss: 1.234, Test accuracy: 46.35
Round  11, Train loss: 0.878, Test loss: 1.215, Test accuracy: 47.25
Round  12, Train loss: 0.812, Test loss: 1.153, Test accuracy: 49.13
Round  13, Train loss: 0.880, Test loss: 1.058, Test accuracy: 51.87
Round  14, Train loss: 0.890, Test loss: 1.120, Test accuracy: 50.13
Round  15, Train loss: 0.909, Test loss: 1.170, Test accuracy: 49.98
Round  16, Train loss: 0.862, Test loss: 1.134, Test accuracy: 52.77
Round  17, Train loss: 0.778, Test loss: 1.202, Test accuracy: 51.45
Round  18, Train loss: 0.788, Test loss: 1.098, Test accuracy: 55.70
Round  19, Train loss: 0.641, Test loss: 1.119, Test accuracy: 54.38
Round  20, Train loss: 0.818, Test loss: 1.064, Test accuracy: 55.58
Round  21, Train loss: 0.750, Test loss: 1.114, Test accuracy: 53.32
Round  22, Train loss: 0.756, Test loss: 1.179, Test accuracy: 50.38
Round  23, Train loss: 0.697, Test loss: 1.217, Test accuracy: 49.73
Round  24, Train loss: 0.799, Test loss: 1.248, Test accuracy: 48.27
Round  25, Train loss: 0.784, Test loss: 1.145, Test accuracy: 51.67
Round  26, Train loss: 0.627, Test loss: 1.104, Test accuracy: 53.38
Round  27, Train loss: 0.578, Test loss: 1.119, Test accuracy: 54.70
Round  28, Train loss: 0.600, Test loss: 1.194, Test accuracy: 52.53
Round  29, Train loss: 0.586, Test loss: 1.132, Test accuracy: 55.73
Round  30, Train loss: 0.674, Test loss: 1.241, Test accuracy: 53.95
Round  31, Train loss: 0.453, Test loss: 1.105, Test accuracy: 55.97
Round  32, Train loss: 0.683, Test loss: 1.166, Test accuracy: 52.38
Round  33, Train loss: 0.576, Test loss: 1.109, Test accuracy: 55.13
Round  34, Train loss: 0.606, Test loss: 1.119, Test accuracy: 53.72
Round  35, Train loss: 0.453, Test loss: 1.099, Test accuracy: 55.38
Round  36, Train loss: 0.628, Test loss: 1.122, Test accuracy: 55.03
Round  37, Train loss: 0.541, Test loss: 1.131, Test accuracy: 54.30
Round  38, Train loss: 0.544, Test loss: 1.148, Test accuracy: 52.73
Round  39, Train loss: 0.468, Test loss: 1.167, Test accuracy: 53.80
Round  40, Train loss: 0.564, Test loss: 1.151, Test accuracy: 54.47
Round  41, Train loss: 0.535, Test loss: 1.216, Test accuracy: 53.25
Round  42, Train loss: 0.469, Test loss: 1.170, Test accuracy: 55.68
Round  43, Train loss: 0.477, Test loss: 1.079, Test accuracy: 56.25
Round  44, Train loss: 0.411, Test loss: 1.058, Test accuracy: 56.85
Round  45, Train loss: 0.552, Test loss: 1.041, Test accuracy: 57.82
Round  46, Train loss: 0.440, Test loss: 1.067, Test accuracy: 57.67
Round  47, Train loss: 0.371, Test loss: 1.074, Test accuracy: 59.13
Round  48, Train loss: 0.396, Test loss: 1.088, Test accuracy: 58.65
Round  49, Train loss: 0.511, Test loss: 1.128, Test accuracy: 57.38
Round  50, Train loss: 0.405, Test loss: 1.137, Test accuracy: 57.63
Round  51, Train loss: 0.404, Test loss: 1.134, Test accuracy: 57.45
Round  52, Train loss: 0.362, Test loss: 1.084, Test accuracy: 59.37
Round  53, Train loss: 0.305, Test loss: 1.072, Test accuracy: 61.10
Round  54, Train loss: 0.431, Test loss: 1.117, Test accuracy: 56.53
Round  55, Train loss: 0.407, Test loss: 1.165, Test accuracy: 55.82
Round  56, Train loss: 0.379, Test loss: 1.147, Test accuracy: 56.12
Round  57, Train loss: 0.323, Test loss: 1.119, Test accuracy: 57.12
Round  58, Train loss: 0.373, Test loss: 1.105, Test accuracy: 57.48
Round  59, Train loss: 0.424, Test loss: 1.107, Test accuracy: 58.10
Round  60, Train loss: 0.355, Test loss: 1.068, Test accuracy: 59.32
Round  61, Train loss: 0.363, Test loss: 1.104, Test accuracy: 57.98
Round  62, Train loss: 0.224, Test loss: 1.116, Test accuracy: 57.90
Round  63, Train loss: 0.315, Test loss: 1.135, Test accuracy: 57.33
Round  64, Train loss: 0.345, Test loss: 1.158, Test accuracy: 56.78
Round  65, Train loss: 0.310, Test loss: 1.098, Test accuracy: 58.30
Round  66, Train loss: 0.296, Test loss: 1.087, Test accuracy: 58.22
Round  67, Train loss: 0.334, Test loss: 1.082, Test accuracy: 58.05
Round  68, Train loss: 0.258, Test loss: 1.069, Test accuracy: 59.95
Round  69, Train loss: 0.310, Test loss: 1.122, Test accuracy: 59.12
Round  70, Train loss: 0.319, Test loss: 1.175, Test accuracy: 57.25
Round  71, Train loss: 0.287, Test loss: 1.128, Test accuracy: 57.70
Round  72, Train loss: 0.285, Test loss: 1.223, Test accuracy: 57.98
Round  73, Train loss: 0.357, Test loss: 1.247, Test accuracy: 56.78
Round  74, Train loss: 0.250, Test loss: 1.215, Test accuracy: 58.40
Round  75, Train loss: 0.206, Test loss: 1.195, Test accuracy: 59.25
Round  76, Train loss: 0.293, Test loss: 1.273, Test accuracy: 56.82
Round  77, Train loss: 0.308, Test loss: 1.128, Test accuracy: 58.97
Round  78, Train loss: 0.237, Test loss: 1.142, Test accuracy: 58.53
Round  79, Train loss: 0.198, Test loss: 1.129, Test accuracy: 59.87
Round  80, Train loss: 0.330, Test loss: 1.199, Test accuracy: 57.07
Round  81, Train loss: 0.261, Test loss: 1.191, Test accuracy: 56.80
Round  82, Train loss: 0.298, Test loss: 1.223, Test accuracy: 57.82
Round  83, Train loss: 0.302, Test loss: 1.208, Test accuracy: 57.72
Round  84, Train loss: 0.257, Test loss: 1.217, Test accuracy: 56.22
Round  85, Train loss: 0.201, Test loss: 1.188, Test accuracy: 57.68
Round  86, Train loss: 0.255, Test loss: 1.120, Test accuracy: 60.03
Round  87, Train loss: 0.181, Test loss: 1.131, Test accuracy: 59.83
Round  88, Train loss: 0.229, Test loss: 1.193, Test accuracy: 58.32
Round  89, Train loss: 0.266, Test loss: 1.204, Test accuracy: 57.73
Round  90, Train loss: 0.205, Test loss: 1.110, Test accuracy: 59.78
Round  91, Train loss: 0.214, Test loss: 1.110, Test accuracy: 60.48
Round  92, Train loss: 0.261, Test loss: 1.198, Test accuracy: 58.53
Round  93, Train loss: 0.224, Test loss: 1.202, Test accuracy: 58.78
Round  94, Train loss: 0.313, Test loss: 1.300, Test accuracy: 56.55
Round  95, Train loss: 0.268, Test loss: 1.222, Test accuracy: 56.38
Round  96, Train loss: 0.259, Test loss: 1.217, Test accuracy: 57.60
Round  97, Train loss: 0.211, Test loss: 1.213, Test accuracy: 58.82
Round  98, Train loss: 0.157, Test loss: 1.244, Test accuracy: 59.42
Round  99, Train loss: 0.155, Test loss: 1.269, Test accuracy: 60.27
Final Round, Train loss: 0.263, Test loss: 1.168, Test accuracy: 61.05
Average accuracy final 10 rounds: 58.66166666666667
3313.554866552353
[6.951660394668579, 11.964015007019043, 17.523182153701782, 23.084429502487183, 28.417353630065918, 33.968467712402344, 39.276479721069336, 44.58349323272705, 49.88872575759888, 54.88624835014343, 59.762765884399414, 65.33639907836914, 70.12863349914551, 75.69596982002258, 81.05536127090454, 87.36766862869263, 92.77057123184204, 97.70880460739136, 103.20030999183655, 108.51785254478455, 113.93133115768433, 119.3885235786438, 124.64055871963501, 129.55772876739502, 135.38132071495056, 140.91931676864624, 146.381831407547, 151.11400938034058, 155.90476989746094, 161.3402111530304, 166.9281919002533, 171.9299340248108, 176.80849242210388, 182.27183198928833, 187.14638590812683, 191.97116088867188, 197.19438982009888, 202.09822821617126, 206.95384550094604, 212.17940473556519, 217.61161613464355, 222.85394287109375, 228.260192155838, 233.21055269241333, 238.0315945148468, 243.57565116882324, 248.55588388442993, 253.5287220478058, 259.07743644714355, 264.6503174304962, 270.192866563797, 275.1614451408386, 280.5659086704254, 285.97238540649414, 290.86012840270996, 296.30378556251526, 301.1846203804016, 305.91927790641785, 311.3403923511505, 316.23248505592346, 320.9585499763489, 326.3879714012146, 331.8073410987854, 336.9999816417694, 341.79173517227173, 347.0183353424072, 351.7474398612976, 357.1955075263977, 362.63101744651794, 367.4860649108887, 372.8936471939087, 378.05041694641113, 383.2756931781769, 388.73472571372986, 394.1909523010254, 398.9913589954376, 404.46750712394714, 409.21113562583923, 413.9186599254608, 419.3593544960022, 424.2372660636902, 429.66604828834534, 434.5494966506958, 439.99694561958313, 445.43470191955566, 450.73183131217957, 455.5170774459839, 461.00802397727966, 466.4644567966461, 471.84376287460327, 476.6244328022003, 482.0182008743286, 486.85032200813293, 491.6942889690399, 496.88501739501953, 501.588356256485, 506.9753839969635, 512.1390342712402, 517.3445074558258, 522.0854599475861, 536.1001331806183]
[17.05, 24.166666666666668, 34.516666666666666, 33.016666666666666, 40.0, 41.8, 39.25, 36.766666666666666, 44.333333333333336, 48.0, 46.35, 47.25, 49.13333333333333, 51.86666666666667, 50.13333333333333, 49.983333333333334, 52.766666666666666, 51.45, 55.7, 54.38333333333333, 55.583333333333336, 53.31666666666667, 50.38333333333333, 49.733333333333334, 48.266666666666666, 51.666666666666664, 53.38333333333333, 54.7, 52.53333333333333, 55.733333333333334, 53.95, 55.96666666666667, 52.38333333333333, 55.13333333333333, 53.71666666666667, 55.38333333333333, 55.03333333333333, 54.3, 52.733333333333334, 53.8, 54.46666666666667, 53.25, 55.68333333333333, 56.25, 56.85, 57.81666666666667, 57.666666666666664, 59.13333333333333, 58.65, 57.38333333333333, 57.63333333333333, 57.45, 59.36666666666667, 61.1, 56.53333333333333, 55.81666666666667, 56.11666666666667, 57.11666666666667, 57.483333333333334, 58.1, 59.31666666666667, 57.983333333333334, 57.9, 57.333333333333336, 56.78333333333333, 58.3, 58.21666666666667, 58.05, 59.95, 59.11666666666667, 57.25, 57.7, 57.983333333333334, 56.78333333333333, 58.4, 59.25, 56.81666666666667, 58.96666666666667, 58.53333333333333, 59.86666666666667, 57.06666666666667, 56.8, 57.81666666666667, 57.71666666666667, 56.21666666666667, 57.68333333333333, 60.03333333333333, 59.833333333333336, 58.31666666666667, 57.733333333333334, 59.78333333333333, 60.483333333333334, 58.53333333333333, 58.78333333333333, 56.55, 56.38333333333333, 57.6, 58.81666666666667, 59.416666666666664, 60.266666666666666, 61.05]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.7000
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1367
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0233
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4167
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5733
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0033
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.5000
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.4733
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0267
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.3867
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3400
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6600
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1267
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3200
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4800
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2033
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.714, Test loss: 2.219, Test accuracy: 21.17
Round   1, Train loss: 1.305, Test loss: 2.017, Test accuracy: 20.87
Round   2, Train loss: 1.192, Test loss: 1.731, Test accuracy: 34.58
Round   3, Train loss: 1.154, Test loss: 1.259, Test accuracy: 40.80
Round   4, Train loss: 1.105, Test loss: 1.283, Test accuracy: 42.77
Round   5, Train loss: 1.065, Test loss: 1.259, Test accuracy: 41.10
Round   6, Train loss: 1.064, Test loss: 1.036, Test accuracy: 48.45
Round   7, Train loss: 1.162, Test loss: 1.048, Test accuracy: 47.80
Round   8, Train loss: 1.114, Test loss: 1.033, Test accuracy: 49.15
Round   9, Train loss: 1.046, Test loss: 1.034, Test accuracy: 49.88
Round  10, Train loss: 1.174, Test loss: 1.011, Test accuracy: 51.08
Round  11, Train loss: 1.093, Test loss: 0.981, Test accuracy: 53.72
Round  12, Train loss: 0.960, Test loss: 0.945, Test accuracy: 56.10
Round  13, Train loss: 0.934, Test loss: 0.931, Test accuracy: 56.35
Round  14, Train loss: 0.894, Test loss: 0.919, Test accuracy: 57.37
Round  15, Train loss: 0.976, Test loss: 0.922, Test accuracy: 56.53
Round  16, Train loss: 1.011, Test loss: 0.935, Test accuracy: 56.17
Round  17, Train loss: 0.968, Test loss: 0.933, Test accuracy: 57.10
Round  18, Train loss: 0.985, Test loss: 0.918, Test accuracy: 56.75
Round  19, Train loss: 1.055, Test loss: 0.918, Test accuracy: 57.15
Round  20, Train loss: 0.902, Test loss: 0.905, Test accuracy: 59.30
Round  21, Train loss: 0.865, Test loss: 0.885, Test accuracy: 60.63
Round  22, Train loss: 0.923, Test loss: 0.893, Test accuracy: 60.98
Round  23, Train loss: 0.912, Test loss: 0.885, Test accuracy: 61.20
Round  24, Train loss: 0.813, Test loss: 0.873, Test accuracy: 61.18
Round  25, Train loss: 0.967, Test loss: 0.870, Test accuracy: 61.78
Round  26, Train loss: 1.056, Test loss: 0.865, Test accuracy: 61.28
Round  27, Train loss: 0.909, Test loss: 0.859, Test accuracy: 60.68
Round  28, Train loss: 0.921, Test loss: 0.861, Test accuracy: 61.12
Round  29, Train loss: 0.948, Test loss: 0.869, Test accuracy: 61.30
Round  30, Train loss: 0.928, Test loss: 0.859, Test accuracy: 61.90
Round  31, Train loss: 0.974, Test loss: 0.856, Test accuracy: 61.30
Round  32, Train loss: 0.865, Test loss: 0.857, Test accuracy: 61.38
Round  33, Train loss: 0.990, Test loss: 0.852, Test accuracy: 62.63
Round  34, Train loss: 0.865, Test loss: 0.851, Test accuracy: 62.93
Round  35, Train loss: 0.917, Test loss: 0.859, Test accuracy: 61.17
Round  36, Train loss: 0.920, Test loss: 0.849, Test accuracy: 62.42
Round  37, Train loss: 0.833, Test loss: 0.840, Test accuracy: 63.10
Round  38, Train loss: 0.921, Test loss: 0.845, Test accuracy: 63.30
Round  39, Train loss: 0.812, Test loss: 0.841, Test accuracy: 63.90
Round  40, Train loss: 0.668, Test loss: 0.832, Test accuracy: 64.07
Round  41, Train loss: 0.910, Test loss: 0.834, Test accuracy: 63.32
Round  42, Train loss: 0.844, Test loss: 0.828, Test accuracy: 64.00
Round  43, Train loss: 0.999, Test loss: 0.836, Test accuracy: 63.37
Round  44, Train loss: 0.869, Test loss: 0.834, Test accuracy: 63.78
Round  45, Train loss: 0.832, Test loss: 0.832, Test accuracy: 64.03
Round  46, Train loss: 0.920, Test loss: 0.834, Test accuracy: 63.20
Round  47, Train loss: 0.950, Test loss: 0.833, Test accuracy: 63.10
Round  48, Train loss: 0.981, Test loss: 0.843, Test accuracy: 63.10
Round  49, Train loss: 0.903, Test loss: 0.850, Test accuracy: 62.13
Round  50, Train loss: 0.844, Test loss: 0.826, Test accuracy: 63.95
Round  51, Train loss: 0.914, Test loss: 0.818, Test accuracy: 64.38
Round  52, Train loss: 0.928, Test loss: 0.824, Test accuracy: 63.25
Round  53, Train loss: 0.909, Test loss: 0.818, Test accuracy: 64.47
Round  54, Train loss: 0.889, Test loss: 0.821, Test accuracy: 63.98
Round  55, Train loss: 0.981, Test loss: 0.830, Test accuracy: 63.85
Round  56, Train loss: 0.931, Test loss: 0.835, Test accuracy: 63.40
Round  57, Train loss: 0.856, Test loss: 0.820, Test accuracy: 64.67
Round  58, Train loss: 0.662, Test loss: 0.805, Test accuracy: 65.17
Round  59, Train loss: 0.860, Test loss: 0.808, Test accuracy: 65.02
Round  60, Train loss: 0.950, Test loss: 0.820, Test accuracy: 63.43
Round  61, Train loss: 0.791, Test loss: 0.822, Test accuracy: 63.13
Round  62, Train loss: 0.935, Test loss: 0.824, Test accuracy: 62.70
Round  63, Train loss: 0.808, Test loss: 0.824, Test accuracy: 62.80
Round  64, Train loss: 0.875, Test loss: 0.812, Test accuracy: 63.42
Round  65, Train loss: 0.834, Test loss: 0.818, Test accuracy: 62.63
Round  66, Train loss: 0.765, Test loss: 0.816, Test accuracy: 63.15
Round  67, Train loss: 0.771, Test loss: 0.807, Test accuracy: 64.18
Round  68, Train loss: 0.784, Test loss: 0.806, Test accuracy: 64.50
Round  69, Train loss: 0.911, Test loss: 0.807, Test accuracy: 64.23
Round  70, Train loss: 0.840, Test loss: 0.804, Test accuracy: 64.68
Round  71, Train loss: 0.735, Test loss: 0.805, Test accuracy: 63.92
Round  72, Train loss: 0.798, Test loss: 0.794, Test accuracy: 64.92
Round  73, Train loss: 0.698, Test loss: 0.800, Test accuracy: 64.62
Round  74, Train loss: 0.894, Test loss: 0.804, Test accuracy: 63.88
Round  75, Train loss: 0.719, Test loss: 0.803, Test accuracy: 63.75
Round  76, Train loss: 0.871, Test loss: 0.801, Test accuracy: 63.03
Round  77, Train loss: 0.767, Test loss: 0.798, Test accuracy: 63.32
Round  78, Train loss: 0.731, Test loss: 0.806, Test accuracy: 62.93
Round  79, Train loss: 0.820, Test loss: 0.808, Test accuracy: 63.27
Round  80, Train loss: 0.849, Test loss: 0.810, Test accuracy: 63.47
Round  81, Train loss: 0.782, Test loss: 0.816, Test accuracy: 63.73
Round  82, Train loss: 0.653, Test loss: 0.797, Test accuracy: 64.97
Round  83, Train loss: 0.779, Test loss: 0.794, Test accuracy: 64.85
Round  84, Train loss: 0.815, Test loss: 0.797, Test accuracy: 64.67
Round  85, Train loss: 0.745, Test loss: 0.799, Test accuracy: 65.23
Round  86, Train loss: 0.709, Test loss: 0.796, Test accuracy: 64.92
Round  87, Train loss: 0.866, Test loss: 0.806, Test accuracy: 64.38
Round  88, Train loss: 0.833, Test loss: 0.809, Test accuracy: 64.65
Round  89, Train loss: 0.897, Test loss: 0.824, Test accuracy: 63.33
Round  90, Train loss: 0.673, Test loss: 0.805, Test accuracy: 64.45
Round  91, Train loss: 0.671, Test loss: 0.799, Test accuracy: 65.18
Round  92, Train loss: 0.554, Test loss: 0.792, Test accuracy: 64.60
Round  93, Train loss: 0.735, Test loss: 0.800, Test accuracy: 64.42
Round  94, Train loss: 0.756, Test loss: 0.796, Test accuracy: 64.87
Round  95, Train loss: 0.802, Test loss: 0.812, Test accuracy: 63.60
Round  96, Train loss: 0.753, Test loss: 0.804, Test accuracy: 64.05
Round  97, Train loss: 0.792, Test loss: 0.801, Test accuracy: 64.00
Round  98, Train loss: 0.822, Test loss: 0.812, Test accuracy: 63.15
Round  99, Train loss: 0.696, Test loss: 0.805, Test accuracy: 63.52
Final Round, Train loss: 0.705, Test loss: 0.807, Test accuracy: 64.32
Average accuracy final 10 rounds: 64.18333333333334
873.5588209629059
[3.178717613220215, 4.4140894412994385, 5.64672589302063, 6.876366376876831, 8.092816352844238, 9.317532539367676, 10.545409679412842, 11.777461051940918, 12.99923324584961, 14.227904319763184, 15.456921100616455, 16.53980255126953, 17.625109672546387, 18.70233702659607, 19.785160303115845, 20.8695228099823, 21.955368518829346, 23.180441856384277, 24.2488431930542, 25.328127145767212, 26.406179428100586, 27.486985683441162, 28.570424795150757, 29.646011352539062, 30.72230052947998, 31.802932500839233, 32.884570598602295, 33.965906381607056, 35.05320477485657, 36.134653091430664, 37.2122745513916, 38.29367518424988, 39.371862173080444, 40.451382637023926, 41.53290367126465, 42.616288900375366, 43.71916127204895, 44.794952154159546, 45.86927890777588, 46.948691606521606, 48.030808210372925, 49.11116313934326, 50.18941521644592, 51.270132303237915, 52.346662759780884, 53.42141532897949, 54.49786138534546, 55.571572065353394, 56.64694356918335, 57.73310875892639, 58.814313650131226, 59.89099407196045, 61.0551118850708, 62.177990674972534, 63.26534819602966, 64.34735250473022, 65.4302875995636, 66.51151466369629, 67.59231448173523, 68.67796850204468, 69.75813746452332, 70.83878588676453, 71.92312335968018, 73.00837135314941, 74.08608555793762, 75.16146063804626, 76.24051904678345, 77.31240272521973, 78.4184091091156, 79.51036381721497, 80.58210253715515, 81.67091536521912, 82.74945402145386, 83.81789541244507, 84.89318799972534, 85.97014451026917, 87.04706716537476, 88.12057518959045, 89.19494938850403, 90.27410650253296, 91.35342335700989, 92.42695116996765, 93.52420592308044, 94.61286282539368, 95.69673418998718, 96.77900195121765, 97.86364841461182, 98.96017694473267, 100.042897939682, 101.12309265136719, 102.21108269691467, 103.2980101108551, 104.37931776046753, 105.46290493011475, 106.56066107749939, 107.64090371131897, 108.71535086631775, 109.79632902145386, 110.87764525413513, 111.95669865608215, 113.54416418075562]
[21.166666666666668, 20.866666666666667, 34.583333333333336, 40.8, 42.766666666666666, 41.1, 48.45, 47.8, 49.15, 49.88333333333333, 51.083333333333336, 53.71666666666667, 56.1, 56.35, 57.36666666666667, 56.53333333333333, 56.166666666666664, 57.1, 56.75, 57.15, 59.3, 60.63333333333333, 60.983333333333334, 61.2, 61.18333333333333, 61.78333333333333, 61.28333333333333, 60.68333333333333, 61.11666666666667, 61.3, 61.9, 61.3, 61.38333333333333, 62.63333333333333, 62.93333333333333, 61.166666666666664, 62.416666666666664, 63.1, 63.3, 63.9, 64.06666666666666, 63.31666666666667, 64.0, 63.36666666666667, 63.78333333333333, 64.03333333333333, 63.2, 63.1, 63.1, 62.13333333333333, 63.95, 64.38333333333334, 63.25, 64.46666666666667, 63.983333333333334, 63.85, 63.4, 64.66666666666667, 65.16666666666667, 65.01666666666667, 63.43333333333333, 63.13333333333333, 62.7, 62.8, 63.416666666666664, 62.63333333333333, 63.15, 64.18333333333334, 64.5, 64.23333333333333, 64.68333333333334, 63.916666666666664, 64.91666666666667, 64.61666666666666, 63.88333333333333, 63.75, 63.03333333333333, 63.31666666666667, 62.93333333333333, 63.266666666666666, 63.46666666666667, 63.733333333333334, 64.96666666666667, 64.85, 64.66666666666667, 65.23333333333333, 64.91666666666667, 64.38333333333334, 64.65, 63.333333333333336, 64.45, 65.18333333333334, 64.6, 64.41666666666667, 64.86666666666666, 63.6, 64.05, 64.0, 63.15, 63.516666666666666, 64.31666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.7000
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1367
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0233
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4167
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5733
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0033
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.5367
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.4733
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.2300
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.3867
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3400
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6600
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1567
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4667
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2033
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.731, Test loss: 2.311, Test accuracy: 15.07
Round   1, Train loss: 1.320, Test loss: 2.085, Test accuracy: 21.03
Round   2, Train loss: 1.183, Test loss: 1.757, Test accuracy: 32.12
Round   3, Train loss: 1.145, Test loss: 1.275, Test accuracy: 40.78
Round   4, Train loss: 1.081, Test loss: 1.307, Test accuracy: 41.08
Round   5, Train loss: 1.030, Test loss: 1.265, Test accuracy: 40.12
Round   6, Train loss: 1.060, Test loss: 1.046, Test accuracy: 48.62
Round   7, Train loss: 1.108, Test loss: 1.033, Test accuracy: 47.20
Round   8, Train loss: 1.081, Test loss: 1.039, Test accuracy: 45.98
Round   9, Train loss: 1.031, Test loss: 1.029, Test accuracy: 46.75
Round  10, Train loss: 1.074, Test loss: 1.026, Test accuracy: 47.47
Round  11, Train loss: 1.044, Test loss: 0.988, Test accuracy: 50.87
Round  12, Train loss: 0.938, Test loss: 0.945, Test accuracy: 53.93
Round  13, Train loss: 0.920, Test loss: 0.933, Test accuracy: 54.25
Round  14, Train loss: 0.916, Test loss: 0.932, Test accuracy: 55.57
Round  15, Train loss: 0.970, Test loss: 0.929, Test accuracy: 57.32
Round  16, Train loss: 0.963, Test loss: 0.923, Test accuracy: 57.67
Round  17, Train loss: 0.953, Test loss: 0.933, Test accuracy: 57.27
Round  18, Train loss: 0.944, Test loss: 0.930, Test accuracy: 57.65
Round  19, Train loss: 0.984, Test loss: 0.916, Test accuracy: 57.32
Round  20, Train loss: 0.894, Test loss: 0.909, Test accuracy: 57.98
Round  21, Train loss: 0.855, Test loss: 0.888, Test accuracy: 58.62
Round  22, Train loss: 0.889, Test loss: 0.895, Test accuracy: 58.50
Round  23, Train loss: 0.875, Test loss: 0.899, Test accuracy: 57.27
Round  24, Train loss: 0.798, Test loss: 0.882, Test accuracy: 59.23
Round  25, Train loss: 0.955, Test loss: 0.895, Test accuracy: 58.90
Round  26, Train loss: 1.005, Test loss: 0.903, Test accuracy: 58.42
Round  27, Train loss: 0.895, Test loss: 0.885, Test accuracy: 59.18
Round  28, Train loss: 0.901, Test loss: 0.885, Test accuracy: 59.52
Round  29, Train loss: 0.913, Test loss: 0.882, Test accuracy: 60.10
Round  30, Train loss: 0.877, Test loss: 0.871, Test accuracy: 59.83
Round  31, Train loss: 0.942, Test loss: 0.877, Test accuracy: 58.52
Round  32, Train loss: 0.851, Test loss: 0.888, Test accuracy: 57.90
Round  33, Train loss: 0.968, Test loss: 0.885, Test accuracy: 57.02
Round  34, Train loss: 0.866, Test loss: 0.890, Test accuracy: 58.68
Round  35, Train loss: 0.909, Test loss: 0.881, Test accuracy: 58.07
Round  36, Train loss: 0.908, Test loss: 0.868, Test accuracy: 59.72
Round  37, Train loss: 0.827, Test loss: 0.858, Test accuracy: 60.50
Round  38, Train loss: 0.899, Test loss: 0.862, Test accuracy: 59.87
Round  39, Train loss: 0.798, Test loss: 0.851, Test accuracy: 59.77
Round  40, Train loss: 0.677, Test loss: 0.843, Test accuracy: 60.02
Round  41, Train loss: 0.877, Test loss: 0.856, Test accuracy: 59.20
Round  42, Train loss: 0.831, Test loss: 0.847, Test accuracy: 59.60
Round  43, Train loss: 0.976, Test loss: 0.855, Test accuracy: 60.48
Round  44, Train loss: 0.877, Test loss: 0.851, Test accuracy: 59.67
Round  45, Train loss: 0.819, Test loss: 0.842, Test accuracy: 59.78
Round  46, Train loss: 0.896, Test loss: 0.847, Test accuracy: 59.92
Round  47, Train loss: 0.928, Test loss: 0.845, Test accuracy: 60.87
Round  48, Train loss: 0.954, Test loss: 0.845, Test accuracy: 61.42
Round  49, Train loss: 0.886, Test loss: 0.843, Test accuracy: 60.53
Round  50, Train loss: 0.824, Test loss: 0.842, Test accuracy: 60.83
Round  51, Train loss: 0.867, Test loss: 0.850, Test accuracy: 60.82
Round  52, Train loss: 0.869, Test loss: 0.844, Test accuracy: 61.20
Round  53, Train loss: 0.860, Test loss: 0.843, Test accuracy: 61.22
Round  54, Train loss: 0.841, Test loss: 0.830, Test accuracy: 62.35
Round  55, Train loss: 0.943, Test loss: 0.850, Test accuracy: 60.20
Round  56, Train loss: 0.884, Test loss: 0.851, Test accuracy: 60.70
Round  57, Train loss: 0.802, Test loss: 0.831, Test accuracy: 62.43
Round  58, Train loss: 0.645, Test loss: 0.824, Test accuracy: 62.83
Round  59, Train loss: 0.820, Test loss: 0.827, Test accuracy: 63.12
Round  60, Train loss: 0.898, Test loss: 0.835, Test accuracy: 61.27
Round  61, Train loss: 0.761, Test loss: 0.829, Test accuracy: 61.90
Round  62, Train loss: 0.886, Test loss: 0.843, Test accuracy: 60.92
Round  63, Train loss: 0.791, Test loss: 0.844, Test accuracy: 61.50
Round  64, Train loss: 0.832, Test loss: 0.825, Test accuracy: 62.32
Round  65, Train loss: 0.783, Test loss: 0.822, Test accuracy: 62.32
Round  66, Train loss: 0.746, Test loss: 0.823, Test accuracy: 62.35
Round  67, Train loss: 0.742, Test loss: 0.831, Test accuracy: 62.10
Round  68, Train loss: 0.741, Test loss: 0.821, Test accuracy: 62.65
Round  69, Train loss: 0.863, Test loss: 0.827, Test accuracy: 62.22
Round  70, Train loss: 0.831, Test loss: 0.831, Test accuracy: 62.18
Round  71, Train loss: 0.726, Test loss: 0.825, Test accuracy: 62.45
Round  72, Train loss: 0.770, Test loss: 0.821, Test accuracy: 63.00
Round  73, Train loss: 0.680, Test loss: 0.813, Test accuracy: 62.97
Round  74, Train loss: 0.863, Test loss: 0.818, Test accuracy: 62.48
Round  75, Train loss: 0.709, Test loss: 0.821, Test accuracy: 61.98
Round  76, Train loss: 0.834, Test loss: 0.823, Test accuracy: 62.00
Round  77, Train loss: 0.742, Test loss: 0.822, Test accuracy: 61.82
Round  78, Train loss: 0.707, Test loss: 0.822, Test accuracy: 62.50
Round  79, Train loss: 0.780, Test loss: 0.821, Test accuracy: 62.52
Round  80, Train loss: 0.810, Test loss: 0.833, Test accuracy: 61.60
Round  81, Train loss: 0.771, Test loss: 0.814, Test accuracy: 63.27
Round  82, Train loss: 0.659, Test loss: 0.819, Test accuracy: 63.12
Round  83, Train loss: 0.761, Test loss: 0.817, Test accuracy: 63.12
Round  84, Train loss: 0.783, Test loss: 0.820, Test accuracy: 63.27
Round  85, Train loss: 0.731, Test loss: 0.823, Test accuracy: 63.52
Round  86, Train loss: 0.683, Test loss: 0.808, Test accuracy: 64.10
Round  87, Train loss: 0.848, Test loss: 0.815, Test accuracy: 63.87
Round  88, Train loss: 0.797, Test loss: 0.826, Test accuracy: 62.87
Round  89, Train loss: 0.864, Test loss: 0.832, Test accuracy: 62.82
Round  90, Train loss: 0.652, Test loss: 0.825, Test accuracy: 62.93
Round  91, Train loss: 0.665, Test loss: 0.824, Test accuracy: 63.20
Round  92, Train loss: 0.550, Test loss: 0.824, Test accuracy: 63.12
Round  93, Train loss: 0.728, Test loss: 0.818, Test accuracy: 63.05
Round  94, Train loss: 0.734, Test loss: 0.827, Test accuracy: 62.10
Round  95, Train loss: 0.781, Test loss: 0.834, Test accuracy: 62.35
Round  96, Train loss: 0.733, Test loss: 0.815, Test accuracy: 62.83
Round  97, Train loss: 0.770, Test loss: 0.818, Test accuracy: 63.12
Round  98, Train loss: 0.784, Test loss: 0.814, Test accuracy: 63.40
Round  99, Train loss: 0.685, Test loss: 0.835, Test accuracy: 62.17
Final Round, Train loss: 0.696, Test loss: 0.835, Test accuracy: 62.67
Average accuracy final 10 rounds: 62.82666666666667
1628.0431549549103
[4.555771589279175, 6.935022592544556, 9.2528715133667, 11.551374435424805, 13.849528312683105, 16.139230966567993, 18.43594789505005, 20.732763528823853, 23.029181003570557, 25.329536199569702, 27.624876260757446, 29.91904902458191, 32.20814847946167, 34.50269937515259, 36.7907133102417, 39.07686901092529, 41.356048822402954, 43.63798213005066, 45.93577003479004, 48.23437547683716, 50.54806470870972, 52.85517382621765, 55.16615152359009, 57.47064924240112, 59.78243041038513, 62.095839977264404, 64.39945793151855, 66.70942854881287, 69.02084589004517, 71.33057808876038, 73.63731527328491, 75.94479322433472, 78.25719285011292, 80.57284498214722, 82.88478446006775, 85.3156669139862, 87.60757493972778, 89.88917207717896, 92.2376856803894, 94.59309220314026, 96.92112517356873, 99.19917464256287, 101.48699045181274, 103.77525854110718, 106.06082534790039, 108.34186935424805, 110.62151265144348, 112.90965986251831, 115.19906973838806, 117.47652626037598, 119.77639722824097, 122.05300331115723, 124.32869458198547, 126.60721015930176, 128.8769772052765, 131.47519731521606, 133.77341985702515, 136.06918287277222, 138.37801551818848, 140.66992616653442, 142.96425437927246, 145.26558446884155, 147.59248614311218, 149.8752408027649, 152.168395280838, 154.4578914642334, 156.74297952651978, 159.03066968917847, 161.3176589012146, 163.60282135009766, 165.88407278060913, 168.17113256454468, 170.45064401626587, 172.73065876960754, 175.00513219833374, 177.2960443496704, 179.58445477485657, 181.8809015750885, 184.1646032333374, 186.44296073913574, 188.72116255760193, 190.99588871002197, 193.2847626209259, 195.5668215751648, 197.84909081459045, 200.13891768455505, 202.43205332756042, 204.82797741889954, 207.42547035217285, 209.70291948318481, 211.98183822631836, 214.29741978645325, 216.63250946998596, 218.9660804271698, 221.25240635871887, 223.53952527046204, 225.812673330307, 228.08993315696716, 230.37276220321655, 232.73204731941223, 235.8980975151062]
[15.066666666666666, 21.033333333333335, 32.11666666666667, 40.78333333333333, 41.083333333333336, 40.11666666666667, 48.61666666666667, 47.2, 45.983333333333334, 46.75, 47.46666666666667, 50.86666666666667, 53.93333333333333, 54.25, 55.56666666666667, 57.31666666666667, 57.666666666666664, 57.266666666666666, 57.65, 57.31666666666667, 57.983333333333334, 58.61666666666667, 58.5, 57.266666666666666, 59.233333333333334, 58.9, 58.416666666666664, 59.18333333333333, 59.516666666666666, 60.1, 59.833333333333336, 58.516666666666666, 57.9, 57.016666666666666, 58.68333333333333, 58.06666666666667, 59.71666666666667, 60.5, 59.86666666666667, 59.766666666666666, 60.016666666666666, 59.2, 59.6, 60.483333333333334, 59.666666666666664, 59.78333333333333, 59.916666666666664, 60.86666666666667, 61.416666666666664, 60.53333333333333, 60.833333333333336, 60.81666666666667, 61.2, 61.21666666666667, 62.35, 60.2, 60.7, 62.43333333333333, 62.833333333333336, 63.11666666666667, 61.266666666666666, 61.9, 60.916666666666664, 61.5, 62.31666666666667, 62.31666666666667, 62.35, 62.1, 62.65, 62.21666666666667, 62.18333333333333, 62.45, 63.0, 62.96666666666667, 62.483333333333334, 61.983333333333334, 62.0, 61.81666666666667, 62.5, 62.516666666666666, 61.6, 63.266666666666666, 63.11666666666667, 63.11666666666667, 63.266666666666666, 63.516666666666666, 64.1, 63.86666666666667, 62.86666666666667, 62.81666666666667, 62.93333333333333, 63.2, 63.11666666666667, 63.05, 62.1, 62.35, 62.833333333333336, 63.11666666666667, 63.4, 62.166666666666664, 62.666666666666664]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.7000
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1367
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0233
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4167
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5733
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0033
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.5000
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.4733
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0267
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.3867
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3400
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6767
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1567
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4667
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2033
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 0.989, Test loss: 2.212, Test accuracy: 20.95
Round   1, Train loss: 0.771, Test loss: 1.987, Test accuracy: 21.33
Round   2, Train loss: 0.680, Test loss: 1.759, Test accuracy: 32.12
Round   3, Train loss: 0.688, Test loss: 1.337, Test accuracy: 36.10
Round   4, Train loss: 0.637, Test loss: 1.420, Test accuracy: 36.43
Round   5, Train loss: 0.621, Test loss: 1.311, Test accuracy: 38.78
Round   6, Train loss: 0.603, Test loss: 1.167, Test accuracy: 42.18
Round   7, Train loss: 0.578, Test loss: 1.171, Test accuracy: 43.25
Round   8, Train loss: 0.619, Test loss: 1.217, Test accuracy: 42.03
Round   9, Train loss: 0.571, Test loss: 1.145, Test accuracy: 44.63
Round  10, Train loss: 0.615, Test loss: 1.159, Test accuracy: 45.68
Round  11, Train loss: 0.585, Test loss: 1.189, Test accuracy: 43.12
Round  12, Train loss: 0.526, Test loss: 1.145, Test accuracy: 47.62
Round  13, Train loss: 0.472, Test loss: 1.281, Test accuracy: 44.42
Round  14, Train loss: 0.460, Test loss: 1.143, Test accuracy: 48.83
Round  15, Train loss: 0.486, Test loss: 1.130, Test accuracy: 48.25
Round  16, Train loss: 0.538, Test loss: 1.068, Test accuracy: 49.07
Round  17, Train loss: 0.466, Test loss: 1.011, Test accuracy: 50.78
Round  18, Train loss: 0.572, Test loss: 1.053, Test accuracy: 50.12
Round  19, Train loss: 0.589, Test loss: 1.112, Test accuracy: 47.63
Round  20, Train loss: 0.448, Test loss: 1.086, Test accuracy: 49.27
Round  21, Train loss: 0.452, Test loss: 1.049, Test accuracy: 51.30
Round  22, Train loss: 0.476, Test loss: 1.071, Test accuracy: 50.03
Round  23, Train loss: 0.438, Test loss: 1.069, Test accuracy: 50.62
Round  24, Train loss: 0.407, Test loss: 1.054, Test accuracy: 51.23
Round  25, Train loss: 0.484, Test loss: 1.099, Test accuracy: 51.67
Round  26, Train loss: 0.570, Test loss: 1.174, Test accuracy: 48.00
Round  27, Train loss: 0.485, Test loss: 1.175, Test accuracy: 46.62
Round  28, Train loss: 0.454, Test loss: 1.115, Test accuracy: 47.38
Round  29, Train loss: 0.487, Test loss: 1.096, Test accuracy: 47.50
Round  30, Train loss: 0.483, Test loss: 1.207, Test accuracy: 48.37
Round  31, Train loss: 0.470, Test loss: 1.256, Test accuracy: 47.25
Round  32, Train loss: 0.400, Test loss: 1.172, Test accuracy: 49.05
Round  33, Train loss: 0.521, Test loss: 1.259, Test accuracy: 48.02
Round  34, Train loss: 0.409, Test loss: 1.202, Test accuracy: 51.32
Round  35, Train loss: 0.493, Test loss: 1.255, Test accuracy: 50.13
Round  36, Train loss: 0.477, Test loss: 1.240, Test accuracy: 49.80
Round  37, Train loss: 0.452, Test loss: 1.161, Test accuracy: 51.70
Round  38, Train loss: 0.467, Test loss: 1.143, Test accuracy: 52.33
Round  39, Train loss: 0.387, Test loss: 1.126, Test accuracy: 53.70
Round  40, Train loss: 0.368, Test loss: 1.165, Test accuracy: 52.28
Round  41, Train loss: 0.476, Test loss: 1.213, Test accuracy: 51.58
Round  42, Train loss: 0.415, Test loss: 1.243, Test accuracy: 50.70
Round  43, Train loss: 0.484, Test loss: 1.147, Test accuracy: 48.78
Round  44, Train loss: 0.484, Test loss: 1.159, Test accuracy: 52.45
Round  45, Train loss: 0.421, Test loss: 1.073, Test accuracy: 54.57
Round  46, Train loss: 0.474, Test loss: 1.099, Test accuracy: 53.63
Round  47, Train loss: 0.504, Test loss: 1.115, Test accuracy: 53.08
Round  48, Train loss: 0.451, Test loss: 1.090, Test accuracy: 52.78
Round  49, Train loss: 0.478, Test loss: 1.205, Test accuracy: 50.68
Round  50, Train loss: 0.424, Test loss: 1.164, Test accuracy: 51.62
Round  51, Train loss: 0.464, Test loss: 1.208, Test accuracy: 51.85
Round  52, Train loss: 0.449, Test loss: 1.190, Test accuracy: 51.42
Round  53, Train loss: 0.467, Test loss: 1.168, Test accuracy: 51.42
Round  54, Train loss: 0.452, Test loss: 1.135, Test accuracy: 54.10
Round  55, Train loss: 0.435, Test loss: 1.189, Test accuracy: 53.52
Round  56, Train loss: 0.472, Test loss: 1.184, Test accuracy: 52.60
Round  57, Train loss: 0.360, Test loss: 1.016, Test accuracy: 56.58
Round  58, Train loss: 0.301, Test loss: 1.077, Test accuracy: 54.28
Round  59, Train loss: 0.419, Test loss: 1.168, Test accuracy: 51.40
Round  60, Train loss: 0.495, Test loss: 1.323, Test accuracy: 48.52
Round  61, Train loss: 0.380, Test loss: 1.116, Test accuracy: 54.52
Round  62, Train loss: 0.398, Test loss: 1.115, Test accuracy: 54.32
Round  63, Train loss: 0.357, Test loss: 1.076, Test accuracy: 55.52
Round  64, Train loss: 0.430, Test loss: 1.131, Test accuracy: 53.48
Round  65, Train loss: 0.403, Test loss: 1.149, Test accuracy: 54.12
Round  66, Train loss: 0.331, Test loss: 1.097, Test accuracy: 55.70
Round  67, Train loss: 0.353, Test loss: 1.179, Test accuracy: 52.55
Round  68, Train loss: 0.340, Test loss: 1.170, Test accuracy: 52.62
Round  69, Train loss: 0.431, Test loss: 1.214, Test accuracy: 51.22
Round  70, Train loss: 0.371, Test loss: 1.202, Test accuracy: 52.55
Round  71, Train loss: 0.311, Test loss: 1.041, Test accuracy: 55.48
Round  72, Train loss: 0.328, Test loss: 1.101, Test accuracy: 55.27
Round  73, Train loss: 0.289, Test loss: 1.101, Test accuracy: 55.38
Round  74, Train loss: 0.354, Test loss: 1.128, Test accuracy: 54.98
Round  75, Train loss: 0.312, Test loss: 1.062, Test accuracy: 55.22
Round  76, Train loss: 0.352, Test loss: 1.112, Test accuracy: 56.05
Round  77, Train loss: 0.283, Test loss: 1.121, Test accuracy: 54.75
Round  78, Train loss: 0.291, Test loss: 1.205, Test accuracy: 54.27
Round  79, Train loss: 0.305, Test loss: 1.256, Test accuracy: 52.62
Round  80, Train loss: 0.376, Test loss: 1.194, Test accuracy: 55.00
Round  81, Train loss: 0.390, Test loss: 1.166, Test accuracy: 54.15
Round  82, Train loss: 0.275, Test loss: 1.093, Test accuracy: 55.60
Round  83, Train loss: 0.371, Test loss: 1.124, Test accuracy: 54.38
Round  84, Train loss: 0.362, Test loss: 1.142, Test accuracy: 54.13
Round  85, Train loss: 0.278, Test loss: 1.123, Test accuracy: 54.73
Round  86, Train loss: 0.288, Test loss: 1.137, Test accuracy: 55.08
Round  87, Train loss: 0.339, Test loss: 1.221, Test accuracy: 54.52
Round  88, Train loss: 0.338, Test loss: 1.213, Test accuracy: 54.25
Round  89, Train loss: 0.327, Test loss: 1.346, Test accuracy: 51.38
Round  90, Train loss: 0.297, Test loss: 1.346, Test accuracy: 53.30
Round  91, Train loss: 0.245, Test loss: 1.252, Test accuracy: 54.52
Round  92, Train loss: 0.199, Test loss: 1.157, Test accuracy: 57.58
Round  93, Train loss: 0.253, Test loss: 1.151, Test accuracy: 56.12
Round  94, Train loss: 0.310, Test loss: 1.113, Test accuracy: 56.15
Round  95, Train loss: 0.261, Test loss: 1.190, Test accuracy: 54.08
Round  96, Train loss: 0.321, Test loss: 1.177, Test accuracy: 54.32
Round  97, Train loss: 0.327, Test loss: 1.267, Test accuracy: 51.25
Round  98, Train loss: 0.339, Test loss: 1.310, Test accuracy: 50.65
Round  99, Train loss: 0.259, Test loss: 1.262, Test accuracy: 52.32
Final Round, Train loss: 0.273, Test loss: 1.160, Test accuracy: 58.03
Average accuracy final 10 rounds: 54.02833333333333
2910.8284759521484
[7.0005316734313965, 12.062749862670898, 16.464531660079956, 20.930315017700195, 25.287970304489136, 29.66289734840393, 34.121769428253174, 38.628594636917114, 43.25336241722107, 47.674015283584595, 52.126436710357666, 56.456055879592896, 60.745075941085815, 65.14343810081482, 69.55959129333496, 73.98643827438354, 78.42743253707886, 82.86801648139954, 87.24296951293945, 91.58708906173706, 96.57175159454346, 101.54347133636475, 105.86513781547546, 110.18243789672852, 114.50895762443542, 118.83632040023804, 123.19308710098267, 127.5225133895874, 131.83580708503723, 136.15918970108032, 140.43902897834778, 144.73018336296082, 149.03101229667664, 153.32076120376587, 157.65005612373352, 161.47920632362366, 165.29786038398743, 169.11649894714355, 172.9219241142273, 176.72636008262634, 180.53087854385376, 184.33785820007324, 188.1509656906128, 191.98044776916504, 195.8035364151001, 199.62103819847107, 203.43271207809448, 207.25834608078003, 211.06440472602844, 214.87204360961914, 218.71397590637207, 222.5178508758545, 226.323575258255, 230.13983821868896, 233.95796060562134, 237.77399253845215, 242.19541025161743, 246.53087878227234, 251.16864347457886, 255.46851897239685, 259.87218475341797, 264.2707803249359, 268.5775511264801, 272.8880994319916, 277.2315080165863, 282.2091944217682, 286.51366329193115, 290.81628823280334, 295.14235639572144, 299.44306993484497, 303.73460817337036, 308.02317810058594, 312.31126165390015, 316.597558259964, 320.8863799571991, 325.1681582927704, 329.44983863830566, 333.7217893600464, 338.1095359325409, 342.49758434295654, 346.79140996932983, 351.0694308280945, 355.3695204257965, 359.65812635421753, 363.9494171142578, 368.2198414802551, 372.48006772994995, 376.76246309280396, 381.0439269542694, 385.32752990722656, 389.60155177116394, 393.85669589042664, 398.14357233047485, 402.4125380516052, 406.679318189621, 410.94724321365356, 415.218213558197, 419.4698853492737, 423.71825981140137, 427.9689030647278, 438.6137089729309]
[20.95, 21.333333333333332, 32.11666666666667, 36.1, 36.43333333333333, 38.78333333333333, 42.18333333333333, 43.25, 42.03333333333333, 44.63333333333333, 45.68333333333333, 43.11666666666667, 47.61666666666667, 44.416666666666664, 48.833333333333336, 48.25, 49.06666666666667, 50.78333333333333, 50.11666666666667, 47.63333333333333, 49.266666666666666, 51.3, 50.03333333333333, 50.61666666666667, 51.233333333333334, 51.666666666666664, 48.0, 46.61666666666667, 47.38333333333333, 47.5, 48.36666666666667, 47.25, 49.05, 48.016666666666666, 51.31666666666667, 50.13333333333333, 49.8, 51.7, 52.333333333333336, 53.7, 52.28333333333333, 51.583333333333336, 50.7, 48.78333333333333, 52.45, 54.56666666666667, 53.63333333333333, 53.083333333333336, 52.78333333333333, 50.68333333333333, 51.61666666666667, 51.85, 51.416666666666664, 51.416666666666664, 54.1, 53.516666666666666, 52.6, 56.583333333333336, 54.28333333333333, 51.4, 48.516666666666666, 54.516666666666666, 54.31666666666667, 55.516666666666666, 53.483333333333334, 54.11666666666667, 55.7, 52.55, 52.61666666666667, 51.21666666666667, 52.55, 55.483333333333334, 55.266666666666666, 55.38333333333333, 54.983333333333334, 55.21666666666667, 56.05, 54.75, 54.266666666666666, 52.61666666666667, 55.0, 54.15, 55.6, 54.38333333333333, 54.13333333333333, 54.733333333333334, 55.083333333333336, 54.516666666666666, 54.25, 51.38333333333333, 53.3, 54.516666666666666, 57.583333333333336, 56.11666666666667, 56.15, 54.083333333333336, 54.31666666666667, 51.25, 50.65, 52.31666666666667, 58.03333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: center_psl, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.7000
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1367
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0600
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4167
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5733
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0033
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.5000
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.4733
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0267
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.3867
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3400
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6667
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1567
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4667
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2033
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.433, Test loss: 2.222, Test accuracy: 15.28
Round   1, Train loss: 1.296, Test loss: 2.057, Test accuracy: 22.50
Round   2, Train loss: 1.137, Test loss: 1.823, Test accuracy: 32.93
Round   3, Train loss: 1.137, Test loss: 1.453, Test accuracy: 37.08
Round   4, Train loss: 1.015, Test loss: 1.533, Test accuracy: 39.53
Round   5, Train loss: 0.995, Test loss: 1.433, Test accuracy: 41.77
Round   6, Train loss: 0.924, Test loss: 1.269, Test accuracy: 46.22
Round   7, Train loss: 1.067, Test loss: 1.270, Test accuracy: 44.63
Round   8, Train loss: 1.031, Test loss: 1.238, Test accuracy: 45.20
Round   9, Train loss: 0.920, Test loss: 1.184, Test accuracy: 47.23
Round  10, Train loss: 1.080, Test loss: 1.181, Test accuracy: 48.85
Round  11, Train loss: 1.000, Test loss: 1.185, Test accuracy: 48.23
Round  12, Train loss: 0.803, Test loss: 1.143, Test accuracy: 50.58
Round  13, Train loss: 0.735, Test loss: 1.114, Test accuracy: 53.20
Round  14, Train loss: 0.749, Test loss: 1.107, Test accuracy: 51.97
Round  15, Train loss: 0.817, Test loss: 1.163, Test accuracy: 48.38
Round  16, Train loss: 0.830, Test loss: 1.196, Test accuracy: 48.68
Round  17, Train loss: 0.793, Test loss: 1.172, Test accuracy: 49.97
Round  18, Train loss: 0.758, Test loss: 1.058, Test accuracy: 54.67
Round  19, Train loss: 0.878, Test loss: 1.109, Test accuracy: 51.17
Round  20, Train loss: 0.697, Test loss: 1.103, Test accuracy: 52.40
Round  21, Train loss: 0.691, Test loss: 1.150, Test accuracy: 50.27
Round  22, Train loss: 0.703, Test loss: 1.091, Test accuracy: 51.82
Round  23, Train loss: 0.698, Test loss: 1.119, Test accuracy: 50.00
Round  24, Train loss: 0.570, Test loss: 1.098, Test accuracy: 51.55
Round  25, Train loss: 0.684, Test loss: 1.152, Test accuracy: 49.73
Round  26, Train loss: 0.788, Test loss: 1.158, Test accuracy: 51.53
Round  27, Train loss: 0.656, Test loss: 1.089, Test accuracy: 52.65
Round  28, Train loss: 0.596, Test loss: 1.036, Test accuracy: 55.70
Round  29, Train loss: 0.639, Test loss: 1.074, Test accuracy: 53.72
Round  30, Train loss: 0.589, Test loss: 1.049, Test accuracy: 55.72
Round  31, Train loss: 0.653, Test loss: 1.165, Test accuracy: 51.97
Round  32, Train loss: 0.506, Test loss: 1.195, Test accuracy: 52.30
Round  33, Train loss: 0.627, Test loss: 1.207, Test accuracy: 52.15
Round  34, Train loss: 0.540, Test loss: 1.141, Test accuracy: 52.57
Round  35, Train loss: 0.541, Test loss: 1.190, Test accuracy: 50.28
Round  36, Train loss: 0.550, Test loss: 1.201, Test accuracy: 50.38
Round  37, Train loss: 0.467, Test loss: 1.135, Test accuracy: 54.37
Round  38, Train loss: 0.492, Test loss: 1.143, Test accuracy: 53.72
Round  39, Train loss: 0.466, Test loss: 1.143, Test accuracy: 54.58
Round  40, Train loss: 0.309, Test loss: 1.128, Test accuracy: 55.92
Round  41, Train loss: 0.453, Test loss: 1.175, Test accuracy: 55.57
Round  42, Train loss: 0.459, Test loss: 1.067, Test accuracy: 58.27
Round  43, Train loss: 0.551, Test loss: 1.144, Test accuracy: 54.05
Round  44, Train loss: 0.534, Test loss: 1.136, Test accuracy: 53.65
Round  45, Train loss: 0.397, Test loss: 1.110, Test accuracy: 54.42
Round  46, Train loss: 0.426, Test loss: 1.138, Test accuracy: 54.67
Round  47, Train loss: 0.467, Test loss: 1.208, Test accuracy: 51.98
Round  48, Train loss: 0.444, Test loss: 1.295, Test accuracy: 49.15
Round  49, Train loss: 0.484, Test loss: 1.265, Test accuracy: 50.10
Round  50, Train loss: 0.435, Test loss: 1.283, Test accuracy: 49.50
Round  51, Train loss: 0.493, Test loss: 1.299, Test accuracy: 50.10
Round  52, Train loss: 0.445, Test loss: 1.217, Test accuracy: 51.85
Round  53, Train loss: 0.421, Test loss: 1.213, Test accuracy: 51.58
Round  54, Train loss: 0.348, Test loss: 1.172, Test accuracy: 53.00
Round  55, Train loss: 0.399, Test loss: 1.275, Test accuracy: 51.67
Round  56, Train loss: 0.415, Test loss: 1.216, Test accuracy: 52.55
Round  57, Train loss: 0.333, Test loss: 1.130, Test accuracy: 54.93
Round  58, Train loss: 0.273, Test loss: 1.098, Test accuracy: 56.42
Round  59, Train loss: 0.383, Test loss: 1.179, Test accuracy: 54.55
Round  60, Train loss: 0.417, Test loss: 1.189, Test accuracy: 53.30
Round  61, Train loss: 0.323, Test loss: 1.179, Test accuracy: 54.77
Round  62, Train loss: 0.360, Test loss: 1.213, Test accuracy: 53.63
Round  63, Train loss: 0.354, Test loss: 1.217, Test accuracy: 54.95
Round  64, Train loss: 0.398, Test loss: 1.196, Test accuracy: 53.85
Round  65, Train loss: 0.348, Test loss: 1.205, Test accuracy: 53.95
Round  66, Train loss: 0.294, Test loss: 1.165, Test accuracy: 54.90
Round  67, Train loss: 0.285, Test loss: 1.156, Test accuracy: 55.87
Round  68, Train loss: 0.251, Test loss: 1.137, Test accuracy: 56.02
Round  69, Train loss: 0.366, Test loss: 1.150, Test accuracy: 55.52
Round  70, Train loss: 0.318, Test loss: 1.176, Test accuracy: 55.70
Round  71, Train loss: 0.267, Test loss: 1.093, Test accuracy: 57.00
Round  72, Train loss: 0.260, Test loss: 1.143, Test accuracy: 55.43
Round  73, Train loss: 0.272, Test loss: 1.156, Test accuracy: 55.88
Round  74, Train loss: 0.307, Test loss: 1.243, Test accuracy: 53.13
Round  75, Train loss: 0.319, Test loss: 1.175, Test accuracy: 54.88
Round  76, Train loss: 0.296, Test loss: 1.252, Test accuracy: 54.37
Round  77, Train loss: 0.233, Test loss: 1.263, Test accuracy: 54.43
Round  78, Train loss: 0.245, Test loss: 1.231, Test accuracy: 55.97
Round  79, Train loss: 0.245, Test loss: 1.315, Test accuracy: 54.48
Round  80, Train loss: 0.277, Test loss: 1.302, Test accuracy: 53.53
Round  81, Train loss: 0.307, Test loss: 1.252, Test accuracy: 54.83
Round  82, Train loss: 0.217, Test loss: 1.217, Test accuracy: 55.43
Round  83, Train loss: 0.285, Test loss: 1.235, Test accuracy: 54.87
Round  84, Train loss: 0.302, Test loss: 1.168, Test accuracy: 54.95
Round  85, Train loss: 0.246, Test loss: 1.201, Test accuracy: 55.12
Round  86, Train loss: 0.243, Test loss: 1.245, Test accuracy: 54.68
Round  87, Train loss: 0.277, Test loss: 1.269, Test accuracy: 53.80
Round  88, Train loss: 0.266, Test loss: 1.262, Test accuracy: 54.32
Round  89, Train loss: 0.278, Test loss: 1.305, Test accuracy: 52.12
Round  90, Train loss: 0.265, Test loss: 1.210, Test accuracy: 54.83
Round  91, Train loss: 0.213, Test loss: 1.227, Test accuracy: 54.72
Round  92, Train loss: 0.167, Test loss: 1.242, Test accuracy: 54.77
Round  93, Train loss: 0.242, Test loss: 1.243, Test accuracy: 54.95
Round  94, Train loss: 0.258, Test loss: 1.251, Test accuracy: 54.77
Round  95, Train loss: 0.195, Test loss: 1.336, Test accuracy: 53.05
Round  96, Train loss: 0.269, Test loss: 1.316, Test accuracy: 52.70
Round  97, Train loss: 0.254, Test loss: 1.288, Test accuracy: 54.45
Round  98, Train loss: 0.299, Test loss: 1.341, Test accuracy: 52.55
Round  99, Train loss: 0.225, Test loss: 1.344, Test accuracy: 53.75
Final Round, Train loss: 0.300, Test loss: 1.240, Test accuracy: 57.80
Average accuracy final 10 rounds: 54.053333333333335
3327.6104469299316
[6.858402729034424, 12.23302173614502, 17.534780740737915, 22.343070030212402, 27.619622945785522, 32.69823455810547, 38.09067964553833, 43.46205163002014, 49.426621437072754, 55.37640380859375, 61.503026485443115, 67.60476064682007, 73.00846862792969, 78.36210656166077, 83.12556171417236, 87.95157408714294, 93.39008784294128, 98.67067408561707, 104.08076429367065, 109.34695649147034, 114.61691212654114, 119.418221950531, 124.6965229511261, 129.66827511787415, 135.10600209236145, 140.52244305610657, 145.51908206939697, 150.26164650917053, 155.46966004371643, 160.48538851737976, 165.81391739845276, 170.8340277671814, 176.04980325698853, 180.89902997016907, 185.74093461036682, 190.5348298549652, 195.4767792224884, 200.24870944023132, 205.59499502182007, 210.6388168334961, 215.46356010437012, 220.72863101959229, 225.99899697303772, 231.16769337654114, 236.05258584022522, 241.30638551712036, 246.5918447971344, 251.82026267051697, 257.05487060546875, 261.7433159351349, 266.65297508239746, 271.65783643722534, 276.9753634929657, 281.9475643634796, 287.2342994213104, 292.49834966659546, 297.4272904396057, 302.6580665111542, 307.3919837474823, 312.3424463272095, 317.3015215396881, 322.25961542129517, 327.49391984939575, 332.6150770187378, 337.7462921142578, 342.6985468864441, 347.8166813850403, 353.05803322792053, 358.2814826965332, 363.2125291824341, 368.32963824272156, 373.16845417022705, 378.29925870895386, 383.1616337299347, 388.40002179145813, 393.52911591529846, 398.8659896850586, 403.9040780067444, 409.0491292476654, 414.2140517234802, 419.47565031051636, 424.20159006118774, 429.3255708217621, 434.06434655189514, 439.3460018634796, 444.6258931159973, 449.76115131378174, 454.91592144966125, 460.18990325927734, 465.3694357872009, 470.2461054325104, 475.3929440975189, 480.5367910861969, 485.8138053417206, 491.0734477043152, 496.26310658454895, 501.0152015686035, 505.73674035072327, 511.08483695983887, 516.2511529922485, 529.5802540779114]
[15.283333333333333, 22.5, 32.93333333333333, 37.083333333333336, 39.53333333333333, 41.766666666666666, 46.21666666666667, 44.63333333333333, 45.2, 47.233333333333334, 48.85, 48.233333333333334, 50.583333333333336, 53.2, 51.96666666666667, 48.38333333333333, 48.68333333333333, 49.96666666666667, 54.666666666666664, 51.166666666666664, 52.4, 50.266666666666666, 51.81666666666667, 50.0, 51.55, 49.733333333333334, 51.53333333333333, 52.65, 55.7, 53.71666666666667, 55.71666666666667, 51.96666666666667, 52.3, 52.15, 52.56666666666667, 50.28333333333333, 50.38333333333333, 54.36666666666667, 53.71666666666667, 54.583333333333336, 55.916666666666664, 55.56666666666667, 58.266666666666666, 54.05, 53.65, 54.416666666666664, 54.666666666666664, 51.983333333333334, 49.15, 50.1, 49.5, 50.1, 51.85, 51.583333333333336, 53.0, 51.666666666666664, 52.55, 54.93333333333333, 56.416666666666664, 54.55, 53.3, 54.766666666666666, 53.63333333333333, 54.95, 53.85, 53.95, 54.9, 55.86666666666667, 56.016666666666666, 55.516666666666666, 55.7, 57.0, 55.43333333333333, 55.88333333333333, 53.13333333333333, 54.88333333333333, 54.36666666666667, 54.43333333333333, 55.96666666666667, 54.483333333333334, 53.53333333333333, 54.833333333333336, 55.43333333333333, 54.86666666666667, 54.95, 55.11666666666667, 54.68333333333333, 53.8, 54.31666666666667, 52.11666666666667, 54.833333333333336, 54.71666666666667, 54.766666666666666, 54.95, 54.766666666666666, 53.05, 52.7, 54.45, 52.55, 53.75, 57.8]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.7067
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.3967
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3267
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5333
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6333
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3800
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.5533
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.6433
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4033
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5700
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.4733
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6167
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3467
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4133
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5667
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.5033
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.804, Test loss: 2.188, Test accuracy: 18.68
Round   1, Train loss: 1.335, Test loss: 2.098, Test accuracy: 22.58
Round   2, Train loss: 1.282, Test loss: 1.654, Test accuracy: 29.77
Round   3, Train loss: 1.159, Test loss: 1.584, Test accuracy: 33.63
Round   4, Train loss: 1.145, Test loss: 1.471, Test accuracy: 36.05
Round   5, Train loss: 1.176, Test loss: 1.475, Test accuracy: 34.63
Round   6, Train loss: 1.226, Test loss: 1.221, Test accuracy: 42.85
Round   7, Train loss: 1.165, Test loss: 1.235, Test accuracy: 43.45
Round   8, Train loss: 1.193, Test loss: 1.195, Test accuracy: 41.50
Round   9, Train loss: 1.198, Test loss: 1.129, Test accuracy: 47.03
Round  10, Train loss: 1.149, Test loss: 1.115, Test accuracy: 44.02
Round  11, Train loss: 1.098, Test loss: 1.087, Test accuracy: 47.45
Round  12, Train loss: 1.175, Test loss: 1.080, Test accuracy: 49.57
Round  13, Train loss: 1.061, Test loss: 1.050, Test accuracy: 50.73
Round  14, Train loss: 1.156, Test loss: 1.055, Test accuracy: 48.53
Round  15, Train loss: 1.107, Test loss: 1.055, Test accuracy: 49.53
Round  16, Train loss: 1.128, Test loss: 1.029, Test accuracy: 49.67
Round  17, Train loss: 1.055, Test loss: 1.016, Test accuracy: 51.35
Round  18, Train loss: 1.114, Test loss: 1.018, Test accuracy: 50.18
Round  19, Train loss: 1.034, Test loss: 1.011, Test accuracy: 52.27
Round  20, Train loss: 1.022, Test loss: 1.000, Test accuracy: 51.85
Round  21, Train loss: 1.045, Test loss: 1.004, Test accuracy: 51.05
Round  22, Train loss: 1.116, Test loss: 1.008, Test accuracy: 52.63
Round  23, Train loss: 1.048, Test loss: 1.018, Test accuracy: 51.30
Round  24, Train loss: 1.135, Test loss: 1.007, Test accuracy: 53.97
Round  25, Train loss: 1.090, Test loss: 0.997, Test accuracy: 53.90
Round  26, Train loss: 1.116, Test loss: 0.983, Test accuracy: 52.53
Round  27, Train loss: 0.993, Test loss: 0.992, Test accuracy: 50.90
Round  28, Train loss: 1.074, Test loss: 0.985, Test accuracy: 51.38
Round  29, Train loss: 1.112, Test loss: 0.999, Test accuracy: 47.68
Round  30, Train loss: 1.029, Test loss: 0.993, Test accuracy: 48.05
Round  31, Train loss: 1.123, Test loss: 0.998, Test accuracy: 50.82
Round  32, Train loss: 1.013, Test loss: 0.997, Test accuracy: 52.20
Round  33, Train loss: 1.020, Test loss: 0.992, Test accuracy: 51.63
Round  34, Train loss: 1.101, Test loss: 0.992, Test accuracy: 51.82
Round  35, Train loss: 1.064, Test loss: 0.979, Test accuracy: 52.37
Round  36, Train loss: 1.072, Test loss: 0.981, Test accuracy: 54.23
Round  37, Train loss: 1.121, Test loss: 0.977, Test accuracy: 53.65
Round  38, Train loss: 1.010, Test loss: 0.965, Test accuracy: 54.68
Round  39, Train loss: 1.052, Test loss: 0.943, Test accuracy: 59.10
Round  40, Train loss: 1.119, Test loss: 0.954, Test accuracy: 58.22
Round  41, Train loss: 1.006, Test loss: 0.947, Test accuracy: 59.88
Round  42, Train loss: 1.097, Test loss: 0.956, Test accuracy: 58.77
Round  43, Train loss: 1.038, Test loss: 0.944, Test accuracy: 60.05
Round  44, Train loss: 0.987, Test loss: 0.933, Test accuracy: 58.73
Round  45, Train loss: 1.031, Test loss: 0.952, Test accuracy: 57.60
Round  46, Train loss: 1.042, Test loss: 0.953, Test accuracy: 56.47
Round  47, Train loss: 1.002, Test loss: 0.947, Test accuracy: 57.55
Round  48, Train loss: 1.022, Test loss: 0.947, Test accuracy: 58.27
Round  49, Train loss: 1.057, Test loss: 0.943, Test accuracy: 58.50
Round  50, Train loss: 1.105, Test loss: 0.944, Test accuracy: 56.10
Round  51, Train loss: 0.990, Test loss: 0.945, Test accuracy: 55.42
Round  52, Train loss: 0.950, Test loss: 0.937, Test accuracy: 55.42
Round  53, Train loss: 0.977, Test loss: 0.937, Test accuracy: 56.63
Round  54, Train loss: 0.921, Test loss: 0.934, Test accuracy: 57.42
Round  55, Train loss: 1.023, Test loss: 0.934, Test accuracy: 56.92
Round  56, Train loss: 1.050, Test loss: 0.934, Test accuracy: 57.77
Round  57, Train loss: 1.089, Test loss: 0.938, Test accuracy: 57.80
Round  58, Train loss: 1.054, Test loss: 0.939, Test accuracy: 57.13
Round  59, Train loss: 0.992, Test loss: 0.935, Test accuracy: 58.78
Round  60, Train loss: 1.034, Test loss: 0.943, Test accuracy: 58.13
Round  61, Train loss: 0.985, Test loss: 0.935, Test accuracy: 58.43
Round  62, Train loss: 0.948, Test loss: 0.928, Test accuracy: 57.77
Round  63, Train loss: 0.915, Test loss: 0.923, Test accuracy: 58.25
Round  64, Train loss: 1.063, Test loss: 0.924, Test accuracy: 58.25
Round  65, Train loss: 0.954, Test loss: 0.935, Test accuracy: 58.60
Round  66, Train loss: 0.977, Test loss: 0.928, Test accuracy: 59.60
Round  67, Train loss: 0.968, Test loss: 0.917, Test accuracy: 60.02
Round  68, Train loss: 0.974, Test loss: 0.916, Test accuracy: 58.73
Round  69, Train loss: 1.016, Test loss: 0.935, Test accuracy: 57.45
Round  70, Train loss: 1.002, Test loss: 0.952, Test accuracy: 55.75
Round  71, Train loss: 1.030, Test loss: 0.945, Test accuracy: 56.15
Round  72, Train loss: 0.924, Test loss: 0.922, Test accuracy: 58.47
Round  73, Train loss: 0.972, Test loss: 0.921, Test accuracy: 57.82
Round  74, Train loss: 0.917, Test loss: 0.924, Test accuracy: 57.80
Round  75, Train loss: 0.925, Test loss: 0.920, Test accuracy: 58.12
Round  76, Train loss: 1.036, Test loss: 0.930, Test accuracy: 57.78
Round  77, Train loss: 1.016, Test loss: 0.935, Test accuracy: 57.00
Round  78, Train loss: 0.930, Test loss: 0.921, Test accuracy: 58.22
Round  79, Train loss: 0.979, Test loss: 0.932, Test accuracy: 57.67
Round  80, Train loss: 0.991, Test loss: 0.928, Test accuracy: 58.53
Round  81, Train loss: 1.009, Test loss: 0.925, Test accuracy: 58.73
Round  82, Train loss: 0.890, Test loss: 0.924, Test accuracy: 58.27
Round  83, Train loss: 1.015, Test loss: 0.925, Test accuracy: 57.85
Round  84, Train loss: 1.013, Test loss: 0.930, Test accuracy: 57.47
Round  85, Train loss: 0.991, Test loss: 0.942, Test accuracy: 56.53
Round  86, Train loss: 0.928, Test loss: 0.939, Test accuracy: 56.12
Round  87, Train loss: 1.044, Test loss: 0.944, Test accuracy: 56.98
Round  88, Train loss: 0.912, Test loss: 0.938, Test accuracy: 56.87
Round  89, Train loss: 0.954, Test loss: 0.940, Test accuracy: 56.27
Round  90, Train loss: 0.879, Test loss: 0.948, Test accuracy: 55.13
Round  91, Train loss: 1.006, Test loss: 0.936, Test accuracy: 57.47
Round  92, Train loss: 1.024, Test loss: 0.954, Test accuracy: 56.17
Round  93, Train loss: 1.012, Test loss: 0.954, Test accuracy: 56.37
Round  94, Train loss: 0.940, Test loss: 0.955, Test accuracy: 56.70
Round  95, Train loss: 0.923, Test loss: 0.950, Test accuracy: 56.50
Round  96, Train loss: 0.959, Test loss: 0.949, Test accuracy: 55.98
Round  97, Train loss: 0.961, Test loss: 0.946, Test accuracy: 55.77
Round  98, Train loss: 0.969, Test loss: 0.958, Test accuracy: 55.47
Round  99, Train loss: 1.008, Test loss: 0.969, Test accuracy: 54.93
Final Round, Train loss: 0.901, Test loss: 0.957, Test accuracy: 56.07
Average accuracy final 10 rounds: 56.04833333333334
900.6732552051544
[2.9887804985046387, 4.0630152225494385, 5.174847602844238, 6.288548707962036, 7.391725301742554, 8.505488157272339, 9.588204622268677, 10.712005138397217, 11.837617874145508, 12.960913896560669, 14.087776899337769, 15.20655608177185, 16.286234855651855, 17.383495330810547, 18.461963891983032, 19.551624059677124, 20.635398626327515, 21.72701334953308, 22.801923036575317, 23.891247034072876, 24.974878549575806, 26.05730891227722, 27.138256788253784, 28.244760513305664, 29.323692083358765, 30.41221261024475, 31.50277280807495, 32.59182572364807, 33.67711114883423, 34.801605224609375, 35.9546434879303, 37.01471185684204, 38.10786414146423, 39.21354365348816, 40.324745893478394, 41.43552374839783, 42.54471707344055, 43.65657424926758, 44.77043676376343, 45.87099027633667, 46.94657325744629, 48.02237153053284, 49.24353575706482, 50.46939730644226, 51.69356346130371, 52.91589641571045, 54.14139676094055, 55.36348605155945, 56.5838189125061, 57.79781651496887, 59.042264461517334, 60.261642932891846, 61.483518838882446, 62.700252532958984, 63.91860508918762, 65.13742804527283, 66.35728144645691, 67.57443928718567, 68.79336810112, 70.01023983955383, 71.2316460609436, 72.45736503601074, 73.68240094184875, 74.90414881706238, 76.12993001937866, 77.35678386688232, 78.58038425445557, 79.79892182350159, 80.92090129852295, 82.04572010040283, 83.16890549659729, 84.28944635391235, 85.40688180923462, 86.52570033073425, 87.64807629585266, 88.765291929245, 89.88264012336731, 91.00078797340393, 92.12066459655762, 93.240718126297, 94.32861995697021, 95.41620206832886, 96.50176978111267, 97.58936834335327, 98.67392683029175, 99.75501990318298, 100.84648108482361, 101.92757153511047, 103.02269244194031, 104.10303783416748, 105.17962789535522, 106.25904560089111, 107.342453956604, 108.4242889881134, 109.51719260215759, 110.60143733024597, 111.67548155784607, 112.74971127510071, 113.85722374916077, 114.947829246521, 116.53112602233887]
[18.683333333333334, 22.583333333333332, 29.766666666666666, 33.63333333333333, 36.05, 34.63333333333333, 42.85, 43.45, 41.5, 47.03333333333333, 44.016666666666666, 47.45, 49.56666666666667, 50.733333333333334, 48.53333333333333, 49.53333333333333, 49.666666666666664, 51.35, 50.18333333333333, 52.266666666666666, 51.85, 51.05, 52.63333333333333, 51.3, 53.96666666666667, 53.9, 52.53333333333333, 50.9, 51.38333333333333, 47.68333333333333, 48.05, 50.81666666666667, 52.2, 51.63333333333333, 51.81666666666667, 52.36666666666667, 54.233333333333334, 53.65, 54.68333333333333, 59.1, 58.21666666666667, 59.88333333333333, 58.766666666666666, 60.05, 58.733333333333334, 57.6, 56.46666666666667, 57.55, 58.266666666666666, 58.5, 56.1, 55.416666666666664, 55.416666666666664, 56.63333333333333, 57.416666666666664, 56.916666666666664, 57.766666666666666, 57.8, 57.13333333333333, 58.78333333333333, 58.13333333333333, 58.43333333333333, 57.766666666666666, 58.25, 58.25, 58.6, 59.6, 60.016666666666666, 58.733333333333334, 57.45, 55.75, 56.15, 58.46666666666667, 57.81666666666667, 57.8, 58.11666666666667, 57.78333333333333, 57.0, 58.21666666666667, 57.666666666666664, 58.53333333333333, 58.733333333333334, 58.266666666666666, 57.85, 57.46666666666667, 56.53333333333333, 56.11666666666667, 56.983333333333334, 56.86666666666667, 56.266666666666666, 55.13333333333333, 57.46666666666667, 56.166666666666664, 56.36666666666667, 56.7, 56.5, 55.983333333333334, 55.766666666666666, 55.46666666666667, 54.93333333333333, 56.06666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.7067
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.3967
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3267
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5333
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6333
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3133
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.5533
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.6433
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4033
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5933
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.5533
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6133
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3467
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4867
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5667
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4500
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.736, Test loss: 2.308, Test accuracy: 18.32
Round   1, Train loss: 1.331, Test loss: 2.098, Test accuracy: 24.03
Round   2, Train loss: 1.215, Test loss: 1.706, Test accuracy: 31.30
Round   3, Train loss: 1.127, Test loss: 1.620, Test accuracy: 33.07
Round   4, Train loss: 1.100, Test loss: 1.519, Test accuracy: 34.50
Round   5, Train loss: 1.090, Test loss: 1.551, Test accuracy: 35.10
Round   6, Train loss: 1.186, Test loss: 1.230, Test accuracy: 42.12
Round   7, Train loss: 1.088, Test loss: 1.230, Test accuracy: 42.08
Round   8, Train loss: 1.091, Test loss: 1.172, Test accuracy: 42.90
Round   9, Train loss: 1.114, Test loss: 1.142, Test accuracy: 44.57
Round  10, Train loss: 1.079, Test loss: 1.115, Test accuracy: 42.20
Round  11, Train loss: 1.026, Test loss: 1.098, Test accuracy: 43.18
Round  12, Train loss: 1.075, Test loss: 1.052, Test accuracy: 46.27
Round  13, Train loss: 1.012, Test loss: 1.019, Test accuracy: 49.15
Round  14, Train loss: 1.049, Test loss: 1.031, Test accuracy: 48.18
Round  15, Train loss: 1.067, Test loss: 1.056, Test accuracy: 45.60
Round  16, Train loss: 1.056, Test loss: 1.032, Test accuracy: 49.05
Round  17, Train loss: 0.994, Test loss: 1.021, Test accuracy: 49.55
Round  18, Train loss: 1.041, Test loss: 1.040, Test accuracy: 48.88
Round  19, Train loss: 0.989, Test loss: 1.052, Test accuracy: 51.43
Round  20, Train loss: 0.966, Test loss: 1.034, Test accuracy: 53.07
Round  21, Train loss: 1.010, Test loss: 1.019, Test accuracy: 51.27
Round  22, Train loss: 1.043, Test loss: 1.019, Test accuracy: 50.30
Round  23, Train loss: 0.999, Test loss: 1.039, Test accuracy: 48.00
Round  24, Train loss: 1.042, Test loss: 1.037, Test accuracy: 50.22
Round  25, Train loss: 1.015, Test loss: 1.013, Test accuracy: 51.42
Round  26, Train loss: 1.008, Test loss: 1.041, Test accuracy: 48.92
Round  27, Train loss: 0.958, Test loss: 1.036, Test accuracy: 47.70
Round  28, Train loss: 1.002, Test loss: 1.069, Test accuracy: 48.37
Round  29, Train loss: 1.040, Test loss: 1.061, Test accuracy: 48.35
Round  30, Train loss: 0.955, Test loss: 1.105, Test accuracy: 47.55
Round  31, Train loss: 1.026, Test loss: 1.099, Test accuracy: 47.58
Round  32, Train loss: 0.941, Test loss: 1.103, Test accuracy: 48.83
Round  33, Train loss: 0.951, Test loss: 1.059, Test accuracy: 50.30
Round  34, Train loss: 1.027, Test loss: 1.096, Test accuracy: 48.83
Round  35, Train loss: 1.020, Test loss: 1.074, Test accuracy: 48.77
Round  36, Train loss: 1.000, Test loss: 1.091, Test accuracy: 48.85
Round  37, Train loss: 1.011, Test loss: 1.075, Test accuracy: 50.13
Round  38, Train loss: 0.917, Test loss: 1.052, Test accuracy: 50.53
Round  39, Train loss: 0.982, Test loss: 1.072, Test accuracy: 51.12
Round  40, Train loss: 1.016, Test loss: 1.052, Test accuracy: 51.20
Round  41, Train loss: 0.914, Test loss: 1.067, Test accuracy: 50.97
Round  42, Train loss: 1.005, Test loss: 1.079, Test accuracy: 50.57
Round  43, Train loss: 0.961, Test loss: 1.099, Test accuracy: 50.12
Round  44, Train loss: 0.906, Test loss: 1.043, Test accuracy: 53.85
Round  45, Train loss: 0.951, Test loss: 1.034, Test accuracy: 54.18
Round  46, Train loss: 0.949, Test loss: 1.043, Test accuracy: 52.42
Round  47, Train loss: 0.921, Test loss: 1.035, Test accuracy: 53.18
Round  48, Train loss: 0.938, Test loss: 1.073, Test accuracy: 51.67
Round  49, Train loss: 0.950, Test loss: 1.036, Test accuracy: 51.87
Round  50, Train loss: 1.019, Test loss: 1.052, Test accuracy: 51.47
Round  51, Train loss: 0.919, Test loss: 1.097, Test accuracy: 51.15
Round  52, Train loss: 0.870, Test loss: 1.027, Test accuracy: 52.50
Round  53, Train loss: 0.897, Test loss: 1.033, Test accuracy: 51.58
Round  54, Train loss: 0.850, Test loss: 1.004, Test accuracy: 53.32
Round  55, Train loss: 0.949, Test loss: 1.045, Test accuracy: 53.55
Round  56, Train loss: 0.956, Test loss: 1.047, Test accuracy: 53.42
Round  57, Train loss: 0.992, Test loss: 1.054, Test accuracy: 52.40
Round  58, Train loss: 0.949, Test loss: 1.064, Test accuracy: 52.42
Round  59, Train loss: 0.915, Test loss: 1.078, Test accuracy: 50.52
Round  60, Train loss: 0.937, Test loss: 1.074, Test accuracy: 49.92
Round  61, Train loss: 0.903, Test loss: 1.082, Test accuracy: 49.93
Round  62, Train loss: 0.877, Test loss: 1.030, Test accuracy: 53.70
Round  63, Train loss: 0.848, Test loss: 1.051, Test accuracy: 53.07
Round  64, Train loss: 0.942, Test loss: 1.070, Test accuracy: 52.02
Round  65, Train loss: 0.901, Test loss: 1.054, Test accuracy: 53.07
Round  66, Train loss: 0.897, Test loss: 1.041, Test accuracy: 52.85
Round  67, Train loss: 0.876, Test loss: 1.059, Test accuracy: 52.58
Round  68, Train loss: 0.911, Test loss: 1.053, Test accuracy: 51.88
Round  69, Train loss: 0.943, Test loss: 1.084, Test accuracy: 50.90
Round  70, Train loss: 0.890, Test loss: 1.062, Test accuracy: 52.65
Round  71, Train loss: 0.936, Test loss: 1.050, Test accuracy: 52.30
Round  72, Train loss: 0.857, Test loss: 1.059, Test accuracy: 52.42
Round  73, Train loss: 0.915, Test loss: 1.038, Test accuracy: 52.78
Round  74, Train loss: 0.839, Test loss: 1.037, Test accuracy: 53.22
Round  75, Train loss: 0.840, Test loss: 1.052, Test accuracy: 52.52
Round  76, Train loss: 0.952, Test loss: 1.066, Test accuracy: 52.05
Round  77, Train loss: 0.936, Test loss: 1.072, Test accuracy: 52.33
Round  78, Train loss: 0.858, Test loss: 1.058, Test accuracy: 52.52
Round  79, Train loss: 0.901, Test loss: 1.052, Test accuracy: 52.15
Round  80, Train loss: 0.929, Test loss: 1.102, Test accuracy: 50.98
Round  81, Train loss: 0.909, Test loss: 1.109, Test accuracy: 50.93
Round  82, Train loss: 0.796, Test loss: 1.068, Test accuracy: 52.07
Round  83, Train loss: 0.957, Test loss: 1.067, Test accuracy: 51.80
Round  84, Train loss: 0.891, Test loss: 1.113, Test accuracy: 51.73
Round  85, Train loss: 0.899, Test loss: 1.071, Test accuracy: 52.20
Round  86, Train loss: 0.841, Test loss: 1.086, Test accuracy: 52.88
Round  87, Train loss: 0.957, Test loss: 1.077, Test accuracy: 52.13
Round  88, Train loss: 0.858, Test loss: 1.088, Test accuracy: 52.20
Round  89, Train loss: 0.909, Test loss: 1.108, Test accuracy: 52.93
Round  90, Train loss: 0.818, Test loss: 1.091, Test accuracy: 52.30
Round  91, Train loss: 0.918, Test loss: 1.136, Test accuracy: 51.30
Round  92, Train loss: 0.948, Test loss: 1.122, Test accuracy: 50.78
Round  93, Train loss: 0.946, Test loss: 1.162, Test accuracy: 50.42
Round  94, Train loss: 0.865, Test loss: 1.164, Test accuracy: 50.88
Round  95, Train loss: 0.872, Test loss: 1.187, Test accuracy: 50.55
Round  96, Train loss: 0.877, Test loss: 1.153, Test accuracy: 49.58
Round  97, Train loss: 0.885, Test loss: 1.131, Test accuracy: 49.67
Round  98, Train loss: 0.923, Test loss: 1.127, Test accuracy: 50.08
Round  99, Train loss: 0.921, Test loss: 1.152, Test accuracy: 49.45
Final Round, Train loss: 0.834, Test loss: 1.147, Test accuracy: 50.62
Average accuracy final 10 rounds: 50.501666666666665
1607.3295304775238
[4.567406177520752, 7.205981492996216, 9.844694375991821, 12.47470211982727, 15.111549377441406, 17.414459943771362, 19.744516372680664, 22.19026017189026, 24.51761770248413, 26.82077693939209, 29.34484028816223, 31.622947692871094, 33.88607668876648, 36.16287446022034, 38.425907611846924, 40.69120383262634, 42.97613453865051, 45.25515556335449, 47.5529465675354, 49.79114651679993, 52.03673768043518, 54.30437397956848, 56.57392477989197, 58.845601320266724, 61.116164684295654, 63.387786865234375, 65.6566834449768, 67.92368173599243, 70.19767618179321, 72.4662458896637, 74.7366156578064, 77.01143789291382, 79.2858338356018, 81.5571391582489, 83.82867765426636, 86.09984922409058, 88.37586045265198, 90.65502309799194, 92.92498731613159, 95.20023703575134, 97.47060990333557, 99.7440333366394, 102.00814628601074, 104.26787328720093, 106.53059148788452, 108.80541658401489, 111.08876466751099, 113.41488075256348, 115.76112174987793, 118.09724521636963, 120.43286776542664, 122.70502758026123, 124.97911643981934, 127.25270485877991, 129.52543449401855, 131.79880785942078, 134.06641745567322, 136.34728407859802, 138.60873079299927, 140.87021040916443, 143.1383159160614, 145.40556931495667, 147.66177606582642, 149.92615747451782, 152.18144297599792, 154.43839597702026, 156.6846799850464, 158.9344003200531, 161.18736934661865, 163.44195532798767, 165.70590996742249, 167.9563865661621, 170.20755910873413, 172.46077752113342, 174.71437048912048, 176.96650171279907, 179.2468237876892, 181.50709462165833, 183.77553796768188, 186.02836537361145, 188.29208970069885, 190.54806900024414, 192.80416989326477, 195.0599491596222, 197.31190371513367, 199.5726354122162, 201.8375687599182, 204.100661277771, 206.36348366737366, 208.61576914787292, 210.87087440490723, 213.12417316436768, 215.38746118545532, 217.70841121673584, 220.0541923046112, 222.31017208099365, 224.57020092010498, 227.03701663017273, 229.28932404518127, 231.54106616973877, 234.64675951004028]
[18.316666666666666, 24.033333333333335, 31.3, 33.06666666666667, 34.5, 35.1, 42.11666666666667, 42.083333333333336, 42.9, 44.56666666666667, 42.2, 43.18333333333333, 46.266666666666666, 49.15, 48.18333333333333, 45.6, 49.05, 49.55, 48.88333333333333, 51.43333333333333, 53.06666666666667, 51.266666666666666, 50.3, 48.0, 50.21666666666667, 51.416666666666664, 48.916666666666664, 47.7, 48.36666666666667, 48.35, 47.55, 47.583333333333336, 48.833333333333336, 50.3, 48.833333333333336, 48.766666666666666, 48.85, 50.13333333333333, 50.53333333333333, 51.11666666666667, 51.2, 50.96666666666667, 50.56666666666667, 50.11666666666667, 53.85, 54.18333333333333, 52.416666666666664, 53.18333333333333, 51.666666666666664, 51.86666666666667, 51.46666666666667, 51.15, 52.5, 51.583333333333336, 53.31666666666667, 53.55, 53.416666666666664, 52.4, 52.416666666666664, 50.516666666666666, 49.916666666666664, 49.93333333333333, 53.7, 53.06666666666667, 52.016666666666666, 53.06666666666667, 52.85, 52.583333333333336, 51.88333333333333, 50.9, 52.65, 52.3, 52.416666666666664, 52.78333333333333, 53.21666666666667, 52.516666666666666, 52.05, 52.333333333333336, 52.516666666666666, 52.15, 50.983333333333334, 50.93333333333333, 52.06666666666667, 51.8, 51.733333333333334, 52.2, 52.88333333333333, 52.13333333333333, 52.2, 52.93333333333333, 52.3, 51.3, 50.78333333333333, 50.416666666666664, 50.88333333333333, 50.55, 49.583333333333336, 49.666666666666664, 50.083333333333336, 49.45, 50.61666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.7067
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.3967
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3267
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5333
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6333
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3133
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.5800
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.6433
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4033
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5700
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.4733
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6133
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3467
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4133
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5667
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4500
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.066, Test loss: 2.301, Test accuracy: 13.78
Round   1, Train loss: 0.914, Test loss: 2.016, Test accuracy: 22.43
Round   2, Train loss: 0.724, Test loss: 1.697, Test accuracy: 29.87
Round   3, Train loss: 0.630, Test loss: 1.666, Test accuracy: 32.97
Round   4, Train loss: 0.638, Test loss: 1.613, Test accuracy: 33.05
Round   5, Train loss: 0.682, Test loss: 1.614, Test accuracy: 31.25
Round   6, Train loss: 0.723, Test loss: 1.403, Test accuracy: 35.78
Round   7, Train loss: 0.668, Test loss: 1.540, Test accuracy: 35.73
Round   8, Train loss: 0.607, Test loss: 1.486, Test accuracy: 38.43
Round   9, Train loss: 0.694, Test loss: 1.392, Test accuracy: 40.37
Round  10, Train loss: 0.629, Test loss: 1.316, Test accuracy: 40.43
Round  11, Train loss: 0.563, Test loss: 1.235, Test accuracy: 43.73
Round  12, Train loss: 0.561, Test loss: 1.267, Test accuracy: 44.10
Round  13, Train loss: 0.633, Test loss: 1.252, Test accuracy: 42.77
Round  14, Train loss: 0.595, Test loss: 1.276, Test accuracy: 40.17
Round  15, Train loss: 0.594, Test loss: 1.295, Test accuracy: 40.20
Round  16, Train loss: 0.619, Test loss: 1.272, Test accuracy: 40.40
Round  17, Train loss: 0.530, Test loss: 1.227, Test accuracy: 42.75
Round  18, Train loss: 0.641, Test loss: 1.216, Test accuracy: 43.43
Round  19, Train loss: 0.534, Test loss: 1.261, Test accuracy: 42.13
Round  20, Train loss: 0.628, Test loss: 1.274, Test accuracy: 43.33
Round  21, Train loss: 0.584, Test loss: 1.287, Test accuracy: 41.00
Round  22, Train loss: 0.684, Test loss: 1.214, Test accuracy: 41.38
Round  23, Train loss: 0.535, Test loss: 1.259, Test accuracy: 41.52
Round  24, Train loss: 0.588, Test loss: 1.281, Test accuracy: 40.50
Round  25, Train loss: 0.558, Test loss: 1.269, Test accuracy: 41.05
Round  26, Train loss: 0.562, Test loss: 1.194, Test accuracy: 44.67
Round  27, Train loss: 0.550, Test loss: 1.238, Test accuracy: 44.42
Round  28, Train loss: 0.525, Test loss: 1.301, Test accuracy: 41.82
Round  29, Train loss: 0.552, Test loss: 1.452, Test accuracy: 38.77
Round  30, Train loss: 0.535, Test loss: 1.336, Test accuracy: 42.40
Round  31, Train loss: 0.558, Test loss: 1.429, Test accuracy: 40.02
Round  32, Train loss: 0.501, Test loss: 1.452, Test accuracy: 38.88
Round  33, Train loss: 0.508, Test loss: 1.327, Test accuracy: 41.70
Round  34, Train loss: 0.564, Test loss: 1.353, Test accuracy: 41.52
Round  35, Train loss: 0.579, Test loss: 1.389, Test accuracy: 42.20
Round  36, Train loss: 0.505, Test loss: 1.417, Test accuracy: 41.37
Round  37, Train loss: 0.577, Test loss: 1.425, Test accuracy: 41.08
Round  38, Train loss: 0.482, Test loss: 1.343, Test accuracy: 42.08
Round  39, Train loss: 0.560, Test loss: 1.255, Test accuracy: 45.07
Round  40, Train loss: 0.605, Test loss: 1.291, Test accuracy: 44.28
Round  41, Train loss: 0.468, Test loss: 1.176, Test accuracy: 47.28
Round  42, Train loss: 0.506, Test loss: 1.149, Test accuracy: 45.30
Round  43, Train loss: 0.522, Test loss: 1.150, Test accuracy: 46.12
Round  44, Train loss: 0.482, Test loss: 1.149, Test accuracy: 47.15
Round  45, Train loss: 0.551, Test loss: 1.193, Test accuracy: 46.03
Round  46, Train loss: 0.531, Test loss: 1.264, Test accuracy: 45.53
Round  47, Train loss: 0.522, Test loss: 1.213, Test accuracy: 46.85
Round  48, Train loss: 0.509, Test loss: 1.244, Test accuracy: 47.63
Round  49, Train loss: 0.542, Test loss: 1.246, Test accuracy: 45.80
Round  50, Train loss: 0.554, Test loss: 1.278, Test accuracy: 43.97
Round  51, Train loss: 0.446, Test loss: 1.272, Test accuracy: 43.73
Round  52, Train loss: 0.442, Test loss: 1.213, Test accuracy: 46.97
Round  53, Train loss: 0.451, Test loss: 1.297, Test accuracy: 43.28
Round  54, Train loss: 0.452, Test loss: 1.195, Test accuracy: 47.48
Round  55, Train loss: 0.489, Test loss: 1.226, Test accuracy: 45.73
Round  56, Train loss: 0.503, Test loss: 1.234, Test accuracy: 46.65
Round  57, Train loss: 0.507, Test loss: 1.238, Test accuracy: 45.13
Round  58, Train loss: 0.513, Test loss: 1.304, Test accuracy: 44.05
Round  59, Train loss: 0.428, Test loss: 1.292, Test accuracy: 43.53
Round  60, Train loss: 0.474, Test loss: 1.366, Test accuracy: 43.15
Round  61, Train loss: 0.445, Test loss: 1.363, Test accuracy: 42.68
Round  62, Train loss: 0.468, Test loss: 1.229, Test accuracy: 46.85
Round  63, Train loss: 0.422, Test loss: 1.293, Test accuracy: 46.48
Round  64, Train loss: 0.502, Test loss: 1.300, Test accuracy: 43.65
Round  65, Train loss: 0.424, Test loss: 1.303, Test accuracy: 44.68
Round  66, Train loss: 0.414, Test loss: 1.297, Test accuracy: 46.62
Round  67, Train loss: 0.396, Test loss: 1.213, Test accuracy: 47.50
Round  68, Train loss: 0.463, Test loss: 1.211, Test accuracy: 46.22
Round  69, Train loss: 0.418, Test loss: 1.253, Test accuracy: 45.33
Round  70, Train loss: 0.417, Test loss: 1.278, Test accuracy: 44.32
Round  71, Train loss: 0.440, Test loss: 1.261, Test accuracy: 45.12
Round  72, Train loss: 0.388, Test loss: 1.226, Test accuracy: 46.85
Round  73, Train loss: 0.391, Test loss: 1.234, Test accuracy: 45.75
Round  74, Train loss: 0.412, Test loss: 1.249, Test accuracy: 45.38
Round  75, Train loss: 0.384, Test loss: 1.309, Test accuracy: 44.37
Round  76, Train loss: 0.422, Test loss: 1.399, Test accuracy: 42.47
Round  77, Train loss: 0.410, Test loss: 1.378, Test accuracy: 44.58
Round  78, Train loss: 0.353, Test loss: 1.362, Test accuracy: 45.12
Round  79, Train loss: 0.399, Test loss: 1.399, Test accuracy: 44.45
Round  80, Train loss: 0.409, Test loss: 1.347, Test accuracy: 45.08
Round  81, Train loss: 0.442, Test loss: 1.401, Test accuracy: 44.10
Round  82, Train loss: 0.400, Test loss: 1.346, Test accuracy: 47.73
Round  83, Train loss: 0.410, Test loss: 1.348, Test accuracy: 46.23
Round  84, Train loss: 0.369, Test loss: 1.312, Test accuracy: 45.72
Round  85, Train loss: 0.434, Test loss: 1.363, Test accuracy: 46.32
Round  86, Train loss: 0.406, Test loss: 1.382, Test accuracy: 46.02
Round  87, Train loss: 0.424, Test loss: 1.418, Test accuracy: 45.20
Round  88, Train loss: 0.368, Test loss: 1.410, Test accuracy: 45.58
Round  89, Train loss: 0.382, Test loss: 1.460, Test accuracy: 43.45
Round  90, Train loss: 0.279, Test loss: 1.480, Test accuracy: 45.15
Round  91, Train loss: 0.403, Test loss: 1.469, Test accuracy: 45.32
Round  92, Train loss: 0.377, Test loss: 1.497, Test accuracy: 44.03
Round  93, Train loss: 0.370, Test loss: 1.582, Test accuracy: 44.43
Round  94, Train loss: 0.349, Test loss: 1.553, Test accuracy: 45.28
Round  95, Train loss: 0.345, Test loss: 1.408, Test accuracy: 46.93
Round  96, Train loss: 0.343, Test loss: 1.370, Test accuracy: 45.87
Round  97, Train loss: 0.363, Test loss: 1.488, Test accuracy: 45.67
Round  98, Train loss: 0.345, Test loss: 1.546, Test accuracy: 44.63
Round  99, Train loss: 0.360, Test loss: 1.491, Test accuracy: 44.37
Final Round, Train loss: 0.311, Test loss: 1.392, Test accuracy: 48.32
Average accuracy final 10 rounds: 45.16833333333333
2985.7648577690125
[6.426694869995117, 10.827480792999268, 15.171831369400024, 19.51180410385132, 23.85288691520691, 28.21039891242981, 32.56697368621826, 36.897167444229126, 41.23861622810364, 45.57659149169922, 49.92682504653931, 54.26631426811218, 58.610567808151245, 62.96248650550842, 67.3066635131836, 71.66272187232971, 76.00567269325256, 80.3702745437622, 84.72228288650513, 89.08483362197876, 93.41890358924866, 97.8604245185852, 102.33063054084778, 106.8141438961029, 111.18365430831909, 115.54961347579956, 119.90721559524536, 124.24761176109314, 128.59981679916382, 132.94261598587036, 137.28954124450684, 141.63720679283142, 146.01180410385132, 150.44490671157837, 154.81353068351746, 159.15948104858398, 163.52950072288513, 167.8908154964447, 172.2706699371338, 176.66958355903625, 181.7201144695282, 186.83286142349243, 191.31734228134155, 195.74768996238708, 200.04574942588806, 204.3496766090393, 208.6331868171692, 212.93618392944336, 217.26372289657593, 221.61008381843567, 225.95616269111633, 230.32575917243958, 234.6275029182434, 238.96022009849548, 243.25816917419434, 247.58335137367249, 251.89223670959473, 256.19627690315247, 260.525856256485, 264.83896803855896, 269.2202286720276, 273.52817463874817, 277.8534572124481, 282.17809081077576, 286.52758979797363, 290.84363079071045, 295.16138339042664, 299.47259974479675, 303.7997016906738, 308.1481466293335, 312.5024185180664, 316.85377049446106, 321.2096447944641, 325.56192660331726, 329.91579127311707, 334.9033660888672, 339.90479040145874, 344.23628878593445, 348.56878423690796, 352.9032230377197, 357.30558466911316, 361.68710923194885, 365.9859538078308, 370.28244805336, 374.5767095088959, 378.8665990829468, 383.2135775089264, 387.5205760002136, 391.82485485076904, 396.2456021308899, 400.8209590911865, 405.10418128967285, 409.40094017982483, 413.6657304763794, 417.95795941352844, 422.2490553855896, 426.5250186920166, 430.9379737377167, 435.342125415802, 439.74339962005615, 450.15095567703247]
[13.783333333333333, 22.433333333333334, 29.866666666666667, 32.96666666666667, 33.05, 31.25, 35.78333333333333, 35.733333333333334, 38.43333333333333, 40.36666666666667, 40.43333333333333, 43.733333333333334, 44.1, 42.766666666666666, 40.166666666666664, 40.2, 40.4, 42.75, 43.43333333333333, 42.13333333333333, 43.333333333333336, 41.0, 41.38333333333333, 41.516666666666666, 40.5, 41.05, 44.666666666666664, 44.416666666666664, 41.81666666666667, 38.766666666666666, 42.4, 40.016666666666666, 38.88333333333333, 41.7, 41.516666666666666, 42.2, 41.36666666666667, 41.083333333333336, 42.083333333333336, 45.06666666666667, 44.28333333333333, 47.28333333333333, 45.3, 46.11666666666667, 47.15, 46.03333333333333, 45.53333333333333, 46.85, 47.63333333333333, 45.8, 43.96666666666667, 43.733333333333334, 46.96666666666667, 43.28333333333333, 47.483333333333334, 45.733333333333334, 46.65, 45.13333333333333, 44.05, 43.53333333333333, 43.15, 42.68333333333333, 46.85, 46.483333333333334, 43.65, 44.68333333333333, 46.61666666666667, 47.5, 46.21666666666667, 45.333333333333336, 44.31666666666667, 45.11666666666667, 46.85, 45.75, 45.38333333333333, 44.36666666666667, 42.46666666666667, 44.583333333333336, 45.11666666666667, 44.45, 45.083333333333336, 44.1, 47.733333333333334, 46.233333333333334, 45.71666666666667, 46.31666666666667, 46.016666666666666, 45.2, 45.583333333333336, 43.45, 45.15, 45.31666666666667, 44.03333333333333, 44.43333333333333, 45.28333333333333, 46.93333333333333, 45.86666666666667, 45.666666666666664, 44.63333333333333, 44.36666666666667, 48.31666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: center_psl, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.7067
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.3967
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3267
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5333
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6467
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3133
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.5533
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.6433
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4033
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5700
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.4733
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6133
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3467
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4733
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5667
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4500
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.433, Test loss: 2.293, Test accuracy: 18.82
Round   1, Train loss: 1.330, Test loss: 2.040, Test accuracy: 22.15
Round   2, Train loss: 1.266, Test loss: 1.793, Test accuracy: 31.05
Round   3, Train loss: 1.136, Test loss: 1.723, Test accuracy: 34.78
Round   4, Train loss: 1.116, Test loss: 1.635, Test accuracy: 33.80
Round   5, Train loss: 1.157, Test loss: 1.599, Test accuracy: 35.47
Round   6, Train loss: 1.147, Test loss: 1.362, Test accuracy: 41.30
Round   7, Train loss: 1.091, Test loss: 1.431, Test accuracy: 41.72
Round   8, Train loss: 1.126, Test loss: 1.373, Test accuracy: 40.67
Round   9, Train loss: 1.150, Test loss: 1.262, Test accuracy: 41.83
Round  10, Train loss: 1.114, Test loss: 1.218, Test accuracy: 40.77
Round  11, Train loss: 0.962, Test loss: 1.120, Test accuracy: 45.48
Round  12, Train loss: 1.044, Test loss: 1.103, Test accuracy: 45.02
Round  13, Train loss: 0.940, Test loss: 1.130, Test accuracy: 42.90
Round  14, Train loss: 1.058, Test loss: 1.282, Test accuracy: 41.52
Round  15, Train loss: 0.995, Test loss: 1.398, Test accuracy: 38.65
Round  16, Train loss: 0.972, Test loss: 1.237, Test accuracy: 43.78
Round  17, Train loss: 0.925, Test loss: 1.307, Test accuracy: 42.88
Round  18, Train loss: 0.951, Test loss: 1.314, Test accuracy: 39.37
Round  19, Train loss: 0.856, Test loss: 1.302, Test accuracy: 42.62
Round  20, Train loss: 0.777, Test loss: 1.220, Test accuracy: 46.02
Round  21, Train loss: 0.859, Test loss: 1.215, Test accuracy: 45.78
Round  22, Train loss: 0.901, Test loss: 1.262, Test accuracy: 43.90
Round  23, Train loss: 0.825, Test loss: 1.258, Test accuracy: 44.40
Round  24, Train loss: 0.915, Test loss: 1.319, Test accuracy: 42.97
Round  25, Train loss: 0.798, Test loss: 1.144, Test accuracy: 47.82
Round  26, Train loss: 0.843, Test loss: 1.188, Test accuracy: 44.45
Round  27, Train loss: 0.692, Test loss: 1.223, Test accuracy: 46.23
Round  28, Train loss: 0.774, Test loss: 1.340, Test accuracy: 42.50
Round  29, Train loss: 0.777, Test loss: 1.405, Test accuracy: 41.78
Round  30, Train loss: 0.742, Test loss: 1.297, Test accuracy: 43.70
Round  31, Train loss: 0.764, Test loss: 1.313, Test accuracy: 41.45
Round  32, Train loss: 0.674, Test loss: 1.285, Test accuracy: 42.77
Round  33, Train loss: 0.651, Test loss: 1.190, Test accuracy: 46.30
Round  34, Train loss: 0.690, Test loss: 1.294, Test accuracy: 44.03
Round  35, Train loss: 0.641, Test loss: 1.300, Test accuracy: 44.23
Round  36, Train loss: 0.654, Test loss: 1.304, Test accuracy: 44.77
Round  37, Train loss: 0.706, Test loss: 1.368, Test accuracy: 42.77
Round  38, Train loss: 0.581, Test loss: 1.312, Test accuracy: 45.63
Round  39, Train loss: 0.650, Test loss: 1.333, Test accuracy: 44.67
Round  40, Train loss: 0.637, Test loss: 1.396, Test accuracy: 43.07
Round  41, Train loss: 0.610, Test loss: 1.277, Test accuracy: 45.18
Round  42, Train loss: 0.613, Test loss: 1.248, Test accuracy: 45.43
Round  43, Train loss: 0.565, Test loss: 1.235, Test accuracy: 46.13
Round  44, Train loss: 0.446, Test loss: 1.294, Test accuracy: 47.15
Round  45, Train loss: 0.521, Test loss: 1.306, Test accuracy: 46.87
Round  46, Train loss: 0.499, Test loss: 1.330, Test accuracy: 46.40
Round  47, Train loss: 0.463, Test loss: 1.289, Test accuracy: 49.47
Round  48, Train loss: 0.482, Test loss: 1.317, Test accuracy: 48.48
Round  49, Train loss: 0.544, Test loss: 1.337, Test accuracy: 47.83
Round  50, Train loss: 0.572, Test loss: 1.481, Test accuracy: 44.98
Round  51, Train loss: 0.443, Test loss: 1.438, Test accuracy: 45.68
Round  52, Train loss: 0.431, Test loss: 1.414, Test accuracy: 46.28
Round  53, Train loss: 0.419, Test loss: 1.425, Test accuracy: 46.82
Round  54, Train loss: 0.399, Test loss: 1.365, Test accuracy: 47.98
Round  55, Train loss: 0.475, Test loss: 1.409, Test accuracy: 46.03
Round  56, Train loss: 0.466, Test loss: 1.290, Test accuracy: 47.18
Round  57, Train loss: 0.459, Test loss: 1.283, Test accuracy: 46.80
Round  58, Train loss: 0.406, Test loss: 1.299, Test accuracy: 47.70
Round  59, Train loss: 0.381, Test loss: 1.324, Test accuracy: 46.40
Round  60, Train loss: 0.389, Test loss: 1.378, Test accuracy: 45.48
Round  61, Train loss: 0.408, Test loss: 1.369, Test accuracy: 45.92
Round  62, Train loss: 0.371, Test loss: 1.399, Test accuracy: 45.77
Round  63, Train loss: 0.306, Test loss: 1.413, Test accuracy: 46.65
Round  64, Train loss: 0.425, Test loss: 1.392, Test accuracy: 46.30
Round  65, Train loss: 0.357, Test loss: 1.366, Test accuracy: 45.68
Round  66, Train loss: 0.356, Test loss: 1.289, Test accuracy: 47.90
Round  67, Train loss: 0.396, Test loss: 1.369, Test accuracy: 44.83
Round  68, Train loss: 0.432, Test loss: 1.318, Test accuracy: 47.05
Round  69, Train loss: 0.397, Test loss: 1.384, Test accuracy: 46.05
Round  70, Train loss: 0.345, Test loss: 1.400, Test accuracy: 45.35
Round  71, Train loss: 0.382, Test loss: 1.413, Test accuracy: 45.03
Round  72, Train loss: 0.350, Test loss: 1.459, Test accuracy: 46.02
Round  73, Train loss: 0.349, Test loss: 1.439, Test accuracy: 45.88
Round  74, Train loss: 0.394, Test loss: 1.428, Test accuracy: 46.52
Round  75, Train loss: 0.368, Test loss: 1.473, Test accuracy: 46.40
Round  76, Train loss: 0.363, Test loss: 1.472, Test accuracy: 45.77
Round  77, Train loss: 0.334, Test loss: 1.498, Test accuracy: 45.67
Round  78, Train loss: 0.308, Test loss: 1.443, Test accuracy: 47.92
Round  79, Train loss: 0.309, Test loss: 1.393, Test accuracy: 47.57
Round  80, Train loss: 0.340, Test loss: 1.411, Test accuracy: 47.57
Round  81, Train loss: 0.341, Test loss: 1.450, Test accuracy: 46.73
Round  82, Train loss: 0.324, Test loss: 1.394, Test accuracy: 48.40
Round  83, Train loss: 0.361, Test loss: 1.480, Test accuracy: 45.75
Round  84, Train loss: 0.359, Test loss: 1.471, Test accuracy: 45.23
Round  85, Train loss: 0.339, Test loss: 1.538, Test accuracy: 43.15
Round  86, Train loss: 0.295, Test loss: 1.490, Test accuracy: 45.48
Round  87, Train loss: 0.352, Test loss: 1.543, Test accuracy: 43.98
Round  88, Train loss: 0.275, Test loss: 1.538, Test accuracy: 45.10
Round  89, Train loss: 0.309, Test loss: 1.547, Test accuracy: 45.15
Round  90, Train loss: 0.233, Test loss: 1.540, Test accuracy: 45.60
Round  91, Train loss: 0.354, Test loss: 1.499, Test accuracy: 46.22
Round  92, Train loss: 0.276, Test loss: 1.543, Test accuracy: 45.58
Round  93, Train loss: 0.287, Test loss: 1.540, Test accuracy: 44.97
Round  94, Train loss: 0.274, Test loss: 1.579, Test accuracy: 44.75
Round  95, Train loss: 0.271, Test loss: 1.585, Test accuracy: 45.40
Round  96, Train loss: 0.228, Test loss: 1.584, Test accuracy: 44.98
Round  97, Train loss: 0.265, Test loss: 1.576, Test accuracy: 44.73
Round  98, Train loss: 0.222, Test loss: 1.567, Test accuracy: 44.75
Round  99, Train loss: 0.284, Test loss: 1.589, Test accuracy: 44.55
Final Round, Train loss: 0.314, Test loss: 1.643, Test accuracy: 46.17
Average accuracy final 10 rounds: 45.153333333333336
3430.0828380584717
[6.932016611099243, 12.447355508804321, 17.83657455444336, 22.804059743881226, 27.775307178497314, 33.0189950466156, 38.473026275634766, 43.848706007003784, 49.217506408691406, 54.697649002075195, 59.670632123947144, 65.03849244117737, 70.49426674842834, 75.91648316383362, 81.32162523269653, 86.28870344161987, 91.72637343406677, 97.10988402366638, 103.17567133903503, 109.3607075214386, 115.54464149475098, 121.47536563873291, 127.58782291412354, 133.0549418926239, 138.1819658279419, 143.52192544937134, 148.96495246887207, 154.43844485282898, 159.86041378974915, 164.9155695438385, 169.80312204360962, 174.8626787662506, 179.75054669380188, 185.11075520515442, 190.2474513053894, 195.57322359085083, 200.44148516654968, 205.47897338867188, 210.74267625808716, 216.08549737930298, 221.30856895446777, 226.78289270401, 231.99927949905396, 237.45469331741333, 242.84726691246033, 248.19783401489258, 253.5755434036255, 258.97641825675964, 264.36004662513733, 269.61120891571045, 274.779132604599, 280.1760821342468, 285.46278047561646, 290.7462160587311, 296.05547046661377, 301.31794023513794, 306.6453764438629, 311.6662244796753, 316.91858434677124, 322.00860714912415, 327.29742765426636, 332.61525440216064, 337.8834457397461, 343.1358811855316, 348.354385137558, 353.17959117889404, 358.5155186653137, 363.8981947898865, 368.96538376808167, 374.30791544914246, 379.71160101890564, 385.08866119384766, 390.39167070388794, 395.7393672466278, 401.0305378437042, 406.4808316230774, 411.5457947254181, 416.8124611377716, 422.0762565135956, 427.4489438533783, 432.8099091053009, 438.1569414138794, 443.5233519077301, 448.2992458343506, 453.602224111557, 458.81349778175354, 464.09651923179626, 469.3132891654968, 474.05580377578735, 479.3235774040222, 484.572158575058, 489.881813287735, 494.92665219306946, 500.326917886734, 506.224728345871, 511.98695611953735, 517.8917031288147, 523.8501935005188, 528.9266867637634, 533.9300980567932, 547.3873021602631]
[18.816666666666666, 22.15, 31.05, 34.78333333333333, 33.8, 35.46666666666667, 41.3, 41.71666666666667, 40.666666666666664, 41.833333333333336, 40.766666666666666, 45.483333333333334, 45.016666666666666, 42.9, 41.516666666666666, 38.65, 43.78333333333333, 42.88333333333333, 39.36666666666667, 42.61666666666667, 46.016666666666666, 45.78333333333333, 43.9, 44.4, 42.96666666666667, 47.81666666666667, 44.45, 46.233333333333334, 42.5, 41.78333333333333, 43.7, 41.45, 42.766666666666666, 46.3, 44.03333333333333, 44.233333333333334, 44.766666666666666, 42.766666666666666, 45.63333333333333, 44.666666666666664, 43.06666666666667, 45.18333333333333, 45.43333333333333, 46.13333333333333, 47.15, 46.86666666666667, 46.4, 49.46666666666667, 48.483333333333334, 47.833333333333336, 44.983333333333334, 45.68333333333333, 46.28333333333333, 46.81666666666667, 47.983333333333334, 46.03333333333333, 47.18333333333333, 46.8, 47.7, 46.4, 45.483333333333334, 45.916666666666664, 45.766666666666666, 46.65, 46.3, 45.68333333333333, 47.9, 44.833333333333336, 47.05, 46.05, 45.35, 45.03333333333333, 46.016666666666666, 45.88333333333333, 46.516666666666666, 46.4, 45.766666666666666, 45.666666666666664, 47.916666666666664, 47.56666666666667, 47.56666666666667, 46.733333333333334, 48.4, 45.75, 45.233333333333334, 43.15, 45.483333333333334, 43.983333333333334, 45.1, 45.15, 45.6, 46.21666666666667, 45.583333333333336, 44.96666666666667, 44.75, 45.4, 44.983333333333334, 44.733333333333334, 44.75, 44.55, 46.166666666666664]
