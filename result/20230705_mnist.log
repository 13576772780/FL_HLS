/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.285, Test loss: 2.288, Test accuracy: 19.33 

Round   0, Global train loss: 2.285, Global test loss: 2.299, Global test accuracy: 11.67 

Round   1, Train loss: 2.149, Test loss: 2.185, Test accuracy: 29.20 

Round   1, Global train loss: 2.149, Global test loss: 2.275, Global test accuracy: 12.17 

Round   2, Train loss: 1.914, Test loss: 2.067, Test accuracy: 44.85 

Round   2, Global train loss: 1.914, Global test loss: 2.221, Global test accuracy: 22.20 

Round   3, Train loss: 1.837, Test loss: 1.906, Test accuracy: 59.53 

Round   3, Global train loss: 1.837, Global test loss: 2.200, Global test accuracy: 34.13 

Round   4, Train loss: 1.674, Test loss: 1.906, Test accuracy: 56.95 

Round   4, Global train loss: 1.674, Global test loss: 2.197, Global test accuracy: 22.62 

Round   5, Train loss: 1.555, Test loss: 1.836, Test accuracy: 65.60 

Round   5, Global train loss: 1.555, Global test loss: 2.186, Global test accuracy: 31.87 

Round   6, Train loss: 1.575, Test loss: 1.836, Test accuracy: 65.00 

Round   6, Global train loss: 1.575, Global test loss: 2.146, Global test accuracy: 31.35 

Round   7, Train loss: 1.747, Test loss: 1.780, Test accuracy: 71.20 

Round   7, Global train loss: 1.747, Global test loss: 2.133, Global test accuracy: 31.05 

Round   8, Train loss: 1.674, Test loss: 1.713, Test accuracy: 78.48 

Round   8, Global train loss: 1.674, Global test loss: 2.142, Global test accuracy: 33.63 

Round   9, Train loss: 1.624, Test loss: 1.606, Test accuracy: 89.17 

Round   9, Global train loss: 1.624, Global test loss: 2.141, Global test accuracy: 38.05 

Round  10, Train loss: 1.501, Test loss: 1.611, Test accuracy: 87.68 

Round  10, Global train loss: 1.501, Global test loss: 2.145, Global test accuracy: 30.23 

Round  11, Train loss: 1.570, Test loss: 1.563, Test accuracy: 92.98 

Round  11, Global train loss: 1.570, Global test loss: 2.122, Global test accuracy: 38.80 

Round  12, Train loss: 1.488, Test loss: 1.556, Test accuracy: 93.08 

Round  12, Global train loss: 1.488, Global test loss: 2.180, Global test accuracy: 22.70 

Round  13, Train loss: 1.528, Test loss: 1.556, Test accuracy: 93.10 

Round  13, Global train loss: 1.528, Global test loss: 2.127, Global test accuracy: 33.48 

Round  14, Train loss: 1.548, Test loss: 1.551, Test accuracy: 93.22 

Round  14, Global train loss: 1.548, Global test loss: 2.144, Global test accuracy: 33.28 

Round  15, Train loss: 1.527, Test loss: 1.547, Test accuracy: 93.45 

Round  15, Global train loss: 1.527, Global test loss: 2.150, Global test accuracy: 30.67 

Round  16, Train loss: 1.476, Test loss: 1.547, Test accuracy: 93.40 

Round  16, Global train loss: 1.476, Global test loss: 2.148, Global test accuracy: 28.75 

Round  17, Train loss: 1.471, Test loss: 1.546, Test accuracy: 93.32 

Round  17, Global train loss: 1.471, Global test loss: 2.106, Global test accuracy: 53.43 

Round  18, Train loss: 1.495, Test loss: 1.536, Test accuracy: 93.53 

Round  18, Global train loss: 1.495, Global test loss: 2.108, Global test accuracy: 37.82 

Round  19, Train loss: 1.494, Test loss: 1.525, Test accuracy: 94.83 

Round  19, Global train loss: 1.494, Global test loss: 2.228, Global test accuracy: 21.53 

Round  20, Train loss: 1.475, Test loss: 1.524, Test accuracy: 94.77 

Round  20, Global train loss: 1.475, Global test loss: 2.142, Global test accuracy: 40.18 

Round  21, Train loss: 1.468, Test loss: 1.523, Test accuracy: 94.82 

Round  21, Global train loss: 1.468, Global test loss: 2.195, Global test accuracy: 23.17 

Round  22, Train loss: 1.475, Test loss: 1.520, Test accuracy: 94.80 

Round  22, Global train loss: 1.475, Global test loss: 2.147, Global test accuracy: 27.03 

Round  23, Train loss: 1.472, Test loss: 1.520, Test accuracy: 94.87 

Round  23, Global train loss: 1.472, Global test loss: 2.203, Global test accuracy: 24.42 

Round  24, Train loss: 1.466, Test loss: 1.519, Test accuracy: 94.83 

Round  24, Global train loss: 1.466, Global test loss: 2.185, Global test accuracy: 22.88 

Round  25, Train loss: 1.471, Test loss: 1.519, Test accuracy: 94.83 

Round  25, Global train loss: 1.471, Global test loss: 2.134, Global test accuracy: 30.75 

Round  26, Train loss: 1.479, Test loss: 1.517, Test accuracy: 94.88 

Round  26, Global train loss: 1.479, Global test loss: 2.080, Global test accuracy: 38.30 

Round  27, Train loss: 1.467, Test loss: 1.517, Test accuracy: 94.92 

Round  27, Global train loss: 1.467, Global test loss: 2.238, Global test accuracy: 18.80 

Round  28, Train loss: 1.471, Test loss: 1.517, Test accuracy: 94.95 

Round  28, Global train loss: 1.471, Global test loss: 2.140, Global test accuracy: 32.65 

Round  29, Train loss: 1.474, Test loss: 1.516, Test accuracy: 94.98 

Round  29, Global train loss: 1.474, Global test loss: 2.056, Global test accuracy: 40.58 

Round  30, Train loss: 1.468, Test loss: 1.516, Test accuracy: 95.00 

Round  30, Global train loss: 1.468, Global test loss: 2.089, Global test accuracy: 34.62 

Round  31, Train loss: 1.464, Test loss: 1.515, Test accuracy: 95.05 

Round  31, Global train loss: 1.464, Global test loss: 2.135, Global test accuracy: 36.30 

Round  32, Train loss: 1.469, Test loss: 1.515, Test accuracy: 95.08 

Round  32, Global train loss: 1.469, Global test loss: 2.159, Global test accuracy: 29.92 

Round  33, Train loss: 1.470, Test loss: 1.515, Test accuracy: 95.07 

Round  33, Global train loss: 1.470, Global test loss: 2.147, Global test accuracy: 36.65 

Round  34, Train loss: 1.469, Test loss: 1.515, Test accuracy: 95.07 

Round  34, Global train loss: 1.469, Global test loss: 2.096, Global test accuracy: 44.10 

Round  35, Train loss: 1.465, Test loss: 1.515, Test accuracy: 95.08 

Round  35, Global train loss: 1.465, Global test loss: 2.199, Global test accuracy: 24.98 

Round  36, Train loss: 1.469, Test loss: 1.515, Test accuracy: 95.08 

Round  36, Global train loss: 1.469, Global test loss: 2.193, Global test accuracy: 20.92 

Round  37, Train loss: 1.466, Test loss: 1.515, Test accuracy: 95.10 

Round  37, Global train loss: 1.466, Global test loss: 2.078, Global test accuracy: 40.47 

Round  38, Train loss: 1.467, Test loss: 1.515, Test accuracy: 95.10 

Round  38, Global train loss: 1.467, Global test loss: 2.155, Global test accuracy: 27.80 

Round  39, Train loss: 1.468, Test loss: 1.515, Test accuracy: 95.13 

Round  39, Global train loss: 1.468, Global test loss: 2.098, Global test accuracy: 36.47 

Round  40, Train loss: 1.465, Test loss: 1.514, Test accuracy: 95.12 

Round  40, Global train loss: 1.465, Global test loss: 2.030, Global test accuracy: 53.27 

Round  41, Train loss: 1.464, Test loss: 1.515, Test accuracy: 95.10 

Round  41, Global train loss: 1.464, Global test loss: 2.133, Global test accuracy: 31.90 

Round  42, Train loss: 1.468, Test loss: 1.514, Test accuracy: 95.13 

Round  42, Global train loss: 1.468, Global test loss: 2.248, Global test accuracy: 17.52 

Round  43, Train loss: 1.462, Test loss: 1.514, Test accuracy: 95.13 

Round  43, Global train loss: 1.462, Global test loss: 2.144, Global test accuracy: 31.98 

Round  44, Train loss: 1.464, Test loss: 1.514, Test accuracy: 95.13 

Round  44, Global train loss: 1.464, Global test loss: 2.255, Global test accuracy: 20.45 

Round  45, Train loss: 1.469, Test loss: 1.514, Test accuracy: 95.10 

Round  45, Global train loss: 1.469, Global test loss: 2.166, Global test accuracy: 25.85 

Round  46, Train loss: 1.465, Test loss: 1.514, Test accuracy: 95.12 

Round  46, Global train loss: 1.465, Global test loss: 2.082, Global test accuracy: 35.88 

Round  47, Train loss: 1.467, Test loss: 1.514, Test accuracy: 95.12 

Round  47, Global train loss: 1.467, Global test loss: 2.069, Global test accuracy: 39.15 

Round  48, Train loss: 1.468, Test loss: 1.514, Test accuracy: 95.05 

Round  48, Global train loss: 1.468, Global test loss: 2.114, Global test accuracy: 30.08 

Round  49, Train loss: 1.470, Test loss: 1.514, Test accuracy: 95.12 

Round  49, Global train loss: 1.470, Global test loss: 2.084, Global test accuracy: 34.68 

Final Round, Train loss: 1.465, Test loss: 1.514, Test accuracy: 95.08 

Final Round, Global train loss: 1.465, Global test loss: 2.084, Global test accuracy: 34.68 

Average accuracy final 10 rounds: 95.11166666666668 

Average global accuracy final 10 rounds: 32.07666666666666 

372.6834683418274
[1.6746485233306885, 2.310037851333618, 2.944859504699707, 3.5810766220092773, 4.216512680053711, 4.855397462844849, 5.4910290241241455, 6.12060546875, 6.7568957805633545, 7.394952774047852, 8.030049562454224, 8.670394659042358, 9.31064748764038, 9.948187589645386, 10.587223052978516, 11.224716186523438, 11.857033252716064, 12.490713119506836, 13.125271797180176, 13.760094165802002, 14.398133277893066, 15.035483598709106, 15.663949728012085, 16.304457664489746, 16.9393892288208, 17.569284200668335, 18.206380128860474, 18.841083526611328, 19.4681658744812, 20.10580062866211, 20.74104619026184, 21.369694471359253, 22.005332708358765, 22.63767671585083, 23.266161680221558, 23.90332865715027, 24.53793454170227, 25.168933629989624, 25.802860021591187, 26.441609144210815, 27.07488179206848, 27.712260246276855, 28.3489248752594, 28.980885982513428, 29.615458011627197, 30.24746608734131, 30.884109497070312, 31.516053915023804, 32.15080404281616, 32.78521752357483, 34.057318687438965]
[19.333333333333332, 29.2, 44.85, 59.53333333333333, 56.95, 65.6, 65.0, 71.2, 78.48333333333333, 89.16666666666667, 87.68333333333334, 92.98333333333333, 93.08333333333333, 93.1, 93.21666666666667, 93.45, 93.4, 93.31666666666666, 93.53333333333333, 94.83333333333333, 94.76666666666667, 94.81666666666666, 94.8, 94.86666666666666, 94.83333333333333, 94.83333333333333, 94.88333333333334, 94.91666666666667, 94.95, 94.98333333333333, 95.0, 95.05, 95.08333333333333, 95.06666666666666, 95.06666666666666, 95.08333333333333, 95.08333333333333, 95.1, 95.1, 95.13333333333334, 95.11666666666666, 95.1, 95.13333333333334, 95.13333333333334, 95.13333333333334, 95.1, 95.11666666666666, 95.11666666666666, 95.05, 95.11666666666666, 95.08333333333333]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.300, Test accuracy: 17.47 

Round   0, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 17.30 

Round   1, Train loss: 2.299, Test loss: 2.299, Test accuracy: 19.92 

Round   1, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 20.33 

Round   2, Train loss: 2.296, Test loss: 2.296, Test accuracy: 25.00 

Round   2, Global train loss: 2.296, Global test loss: 2.295, Global test accuracy: 28.23 

Round   3, Train loss: 2.292, Test loss: 2.294, Test accuracy: 30.43 

Round   3, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 38.22 

Round   4, Train loss: 2.286, Test loss: 2.288, Test accuracy: 35.95 

Round   4, Global train loss: 2.286, Global test loss: 2.281, Global test accuracy: 44.92 

Round   5, Train loss: 2.271, Test loss: 2.278, Test accuracy: 36.43 

Round   5, Global train loss: 2.271, Global test loss: 2.260, Global test accuracy: 43.67 

Round   6, Train loss: 2.220, Test loss: 2.240, Test accuracy: 36.28 

Round   6, Global train loss: 2.220, Global test loss: 2.166, Global test accuracy: 39.05 

Round   7, Train loss: 2.108, Test loss: 2.183, Test accuracy: 40.43 

Round   7, Global train loss: 2.108, Global test loss: 2.035, Global test accuracy: 46.93 

Round   8, Train loss: 1.989, Test loss: 2.111, Test accuracy: 45.25 

Round   8, Global train loss: 1.989, Global test loss: 1.928, Global test accuracy: 60.52 

Round   9, Train loss: 1.858, Test loss: 2.023, Test accuracy: 53.55 

Round   9, Global train loss: 1.858, Global test loss: 1.814, Global test accuracy: 74.53 

Round  10, Train loss: 1.749, Test loss: 1.964, Test accuracy: 58.52 

Round  10, Global train loss: 1.749, Global test loss: 1.732, Global test accuracy: 79.00 

Round  11, Train loss: 1.684, Test loss: 1.879, Test accuracy: 64.42 

Round  11, Global train loss: 1.684, Global test loss: 1.683, Global test accuracy: 82.63 

Round  12, Train loss: 1.650, Test loss: 1.785, Test accuracy: 72.93 

Round  12, Global train loss: 1.650, Global test loss: 1.656, Global test accuracy: 84.37 

Round  13, Train loss: 1.618, Test loss: 1.774, Test accuracy: 73.32 

Round  13, Global train loss: 1.618, Global test loss: 1.635, Global test accuracy: 86.05 

Round  14, Train loss: 1.604, Test loss: 1.742, Test accuracy: 75.93 

Round  14, Global train loss: 1.604, Global test loss: 1.620, Global test accuracy: 86.65 

Round  15, Train loss: 1.596, Test loss: 1.711, Test accuracy: 78.87 

Round  15, Global train loss: 1.596, Global test loss: 1.610, Global test accuracy: 87.63 

Round  16, Train loss: 1.580, Test loss: 1.698, Test accuracy: 79.82 

Round  16, Global train loss: 1.580, Global test loss: 1.601, Global test accuracy: 88.15 

Round  17, Train loss: 1.574, Test loss: 1.657, Test accuracy: 83.03 

Round  17, Global train loss: 1.574, Global test loss: 1.595, Global test accuracy: 88.52 

Round  18, Train loss: 1.564, Test loss: 1.650, Test accuracy: 83.57 

Round  18, Global train loss: 1.564, Global test loss: 1.588, Global test accuracy: 89.00 

Round  19, Train loss: 1.561, Test loss: 1.617, Test accuracy: 86.58 

Round  19, Global train loss: 1.561, Global test loss: 1.583, Global test accuracy: 89.28 

Round  20, Train loss: 1.552, Test loss: 1.603, Test accuracy: 87.70 

Round  20, Global train loss: 1.552, Global test loss: 1.582, Global test accuracy: 89.30 

Round  21, Train loss: 1.546, Test loss: 1.600, Test accuracy: 87.88 

Round  21, Global train loss: 1.546, Global test loss: 1.577, Global test accuracy: 89.75 

Round  22, Train loss: 1.546, Test loss: 1.598, Test accuracy: 88.08 

Round  22, Global train loss: 1.546, Global test loss: 1.577, Global test accuracy: 89.48 

Round  23, Train loss: 1.541, Test loss: 1.596, Test accuracy: 88.03 

Round  23, Global train loss: 1.541, Global test loss: 1.575, Global test accuracy: 89.72 

Round  24, Train loss: 1.539, Test loss: 1.594, Test accuracy: 88.12 

Round  24, Global train loss: 1.539, Global test loss: 1.572, Global test accuracy: 89.97 

Round  25, Train loss: 1.540, Test loss: 1.590, Test accuracy: 88.55 

Round  25, Global train loss: 1.540, Global test loss: 1.570, Global test accuracy: 90.08 

Round  26, Train loss: 1.525, Test loss: 1.588, Test accuracy: 88.63 

Round  26, Global train loss: 1.525, Global test loss: 1.568, Global test accuracy: 90.08 

Round  27, Train loss: 1.528, Test loss: 1.584, Test accuracy: 88.98 

Round  27, Global train loss: 1.528, Global test loss: 1.566, Global test accuracy: 90.42 

Round  28, Train loss: 1.538, Test loss: 1.575, Test accuracy: 89.68 

Round  28, Global train loss: 1.538, Global test loss: 1.563, Global test accuracy: 90.60 

Round  29, Train loss: 1.518, Test loss: 1.574, Test accuracy: 89.73 

Round  29, Global train loss: 1.518, Global test loss: 1.562, Global test accuracy: 90.62 

Round  30, Train loss: 1.524, Test loss: 1.574, Test accuracy: 89.75 

Round  30, Global train loss: 1.524, Global test loss: 1.562, Global test accuracy: 90.85 

Round  31, Train loss: 1.521, Test loss: 1.572, Test accuracy: 89.83 

Round  31, Global train loss: 1.521, Global test loss: 1.561, Global test accuracy: 90.95 

Round  32, Train loss: 1.524, Test loss: 1.571, Test accuracy: 89.82 

Round  32, Global train loss: 1.524, Global test loss: 1.560, Global test accuracy: 90.77 

Round  33, Train loss: 1.517, Test loss: 1.570, Test accuracy: 89.78 

Round  33, Global train loss: 1.517, Global test loss: 1.558, Global test accuracy: 91.03 

Round  34, Train loss: 1.518, Test loss: 1.570, Test accuracy: 89.70 

Round  34, Global train loss: 1.518, Global test loss: 1.556, Global test accuracy: 91.12 

Round  35, Train loss: 1.512, Test loss: 1.569, Test accuracy: 89.93 

Round  35, Global train loss: 1.512, Global test loss: 1.558, Global test accuracy: 91.17 

Round  36, Train loss: 1.509, Test loss: 1.567, Test accuracy: 90.03 

Round  36, Global train loss: 1.509, Global test loss: 1.557, Global test accuracy: 91.07 

Round  37, Train loss: 1.508, Test loss: 1.565, Test accuracy: 90.18 

Round  37, Global train loss: 1.508, Global test loss: 1.554, Global test accuracy: 91.10 

Round  38, Train loss: 1.516, Test loss: 1.564, Test accuracy: 90.28 

Round  38, Global train loss: 1.516, Global test loss: 1.554, Global test accuracy: 91.35 

Round  39, Train loss: 1.513, Test loss: 1.564, Test accuracy: 90.27 

Round  39, Global train loss: 1.513, Global test loss: 1.553, Global test accuracy: 91.57 

Round  40, Train loss: 1.508, Test loss: 1.563, Test accuracy: 90.55 

Round  40, Global train loss: 1.508, Global test loss: 1.552, Global test accuracy: 91.53 

Round  41, Train loss: 1.508, Test loss: 1.562, Test accuracy: 90.52 

Round  41, Global train loss: 1.508, Global test loss: 1.552, Global test accuracy: 91.52 

Round  42, Train loss: 1.507, Test loss: 1.563, Test accuracy: 90.43 

Round  42, Global train loss: 1.507, Global test loss: 1.551, Global test accuracy: 91.52 

Round  43, Train loss: 1.509, Test loss: 1.561, Test accuracy: 90.45 

Round  43, Global train loss: 1.509, Global test loss: 1.552, Global test accuracy: 91.45 

Round  44, Train loss: 1.509, Test loss: 1.560, Test accuracy: 90.57 

Round  44, Global train loss: 1.509, Global test loss: 1.551, Global test accuracy: 91.50 

Round  45, Train loss: 1.513, Test loss: 1.559, Test accuracy: 90.62 

Round  45, Global train loss: 1.513, Global test loss: 1.552, Global test accuracy: 91.45 

Round  46, Train loss: 1.502, Test loss: 1.559, Test accuracy: 90.70 

Round  46, Global train loss: 1.502, Global test loss: 1.551, Global test accuracy: 91.47 

Round  47, Train loss: 1.502, Test loss: 1.558, Test accuracy: 90.83 

Round  47, Global train loss: 1.502, Global test loss: 1.550, Global test accuracy: 91.43 

Round  48, Train loss: 1.506, Test loss: 1.557, Test accuracy: 90.97 

Round  48, Global train loss: 1.506, Global test loss: 1.548, Global test accuracy: 91.73 

Round  49, Train loss: 1.503, Test loss: 1.557, Test accuracy: 91.00 

Round  49, Global train loss: 1.503, Global test loss: 1.548, Global test accuracy: 91.93 

Final Round, Train loss: 1.496, Test loss: 1.555, Test accuracy: 90.98 

Final Round, Global train loss: 1.496, Global test loss: 1.548, Global test accuracy: 91.93 

Average accuracy final 10 rounds: 90.66333333333333 

Average global accuracy final 10 rounds: 91.55333333333331 

399.8786196708679
[1.670253038406372, 2.415045738220215, 3.1595492362976074, 3.9034290313720703, 4.649512767791748, 5.39337944984436, 6.13582968711853, 6.878322601318359, 7.622657775878906, 8.370350122451782, 9.113455533981323, 9.85775351524353, 10.59973692893982, 11.34423303604126, 12.090672254562378, 12.835818529129028, 13.581237316131592, 14.322882890701294, 15.069920063018799, 15.814168214797974, 16.556063890457153, 17.295410633087158, 17.932488679885864, 18.571627378463745, 19.22725558280945, 19.861244678497314, 20.494340658187866, 21.12444806098938, 21.754431009292603, 22.38941502571106, 23.02507185935974, 23.663638830184937, 24.29634141921997, 24.919378995895386, 25.556127071380615, 26.211048126220703, 26.848187685012817, 27.485973358154297, 28.122939825057983, 28.752994298934937, 29.395017862319946, 30.030738830566406, 30.663405418395996, 31.29885220527649, 31.9399676322937, 32.57604002952576, 33.21523332595825, 33.85490965843201, 34.49737286567688, 35.135844230651855, 36.47855830192566]
[17.466666666666665, 19.916666666666668, 25.0, 30.433333333333334, 35.95, 36.43333333333333, 36.28333333333333, 40.43333333333333, 45.25, 53.55, 58.516666666666666, 64.41666666666667, 72.93333333333334, 73.31666666666666, 75.93333333333334, 78.86666666666666, 79.81666666666666, 83.03333333333333, 83.56666666666666, 86.58333333333333, 87.7, 87.88333333333334, 88.08333333333333, 88.03333333333333, 88.11666666666666, 88.55, 88.63333333333334, 88.98333333333333, 89.68333333333334, 89.73333333333333, 89.75, 89.83333333333333, 89.81666666666666, 89.78333333333333, 89.7, 89.93333333333334, 90.03333333333333, 90.18333333333334, 90.28333333333333, 90.26666666666667, 90.55, 90.51666666666667, 90.43333333333334, 90.45, 90.56666666666666, 90.61666666666666, 90.7, 90.83333333333333, 90.96666666666667, 91.0, 90.98333333333333]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 16.77 

Round   1, Train loss: 2.301, Test loss: 2.300, Test accuracy: 17.67 

Round   2, Train loss: 2.300, Test loss: 2.299, Test accuracy: 19.77 

Round   3, Train loss: 2.298, Test loss: 2.297, Test accuracy: 23.35 

Round   4, Train loss: 2.296, Test loss: 2.295, Test accuracy: 28.03 

Round   5, Train loss: 2.294, Test loss: 2.293, Test accuracy: 34.60 

Round   6, Train loss: 2.292, Test loss: 2.289, Test accuracy: 38.55 

Round   7, Train loss: 2.288, Test loss: 2.285, Test accuracy: 41.57 

Round   8, Train loss: 2.283, Test loss: 2.280, Test accuracy: 41.60 

Round   9, Train loss: 2.275, Test loss: 2.271, Test accuracy: 42.05 

Round  10, Train loss: 2.264, Test loss: 2.256, Test accuracy: 42.38 

Round  11, Train loss: 2.248, Test loss: 2.230, Test accuracy: 41.10 

Round  12, Train loss: 2.207, Test loss: 2.177, Test accuracy: 41.40 

Round  13, Train loss: 2.138, Test loss: 2.105, Test accuracy: 43.72 

Round  14, Train loss: 2.067, Test loss: 2.042, Test accuracy: 47.07 

Round  15, Train loss: 2.011, Test loss: 1.989, Test accuracy: 52.13 

Round  16, Train loss: 1.942, Test loss: 1.936, Test accuracy: 57.75 

Round  17, Train loss: 1.885, Test loss: 1.888, Test accuracy: 61.72 

Round  18, Train loss: 1.829, Test loss: 1.837, Test accuracy: 67.88 

Round  19, Train loss: 1.783, Test loss: 1.805, Test accuracy: 70.95 

Round  20, Train loss: 1.738, Test loss: 1.775, Test accuracy: 73.53 

Round  21, Train loss: 1.706, Test loss: 1.743, Test accuracy: 76.07 

Round  22, Train loss: 1.701, Test loss: 1.721, Test accuracy: 78.25 

Round  23, Train loss: 1.689, Test loss: 1.701, Test accuracy: 80.48 

Round  24, Train loss: 1.666, Test loss: 1.680, Test accuracy: 82.68 

Round  25, Train loss: 1.644, Test loss: 1.669, Test accuracy: 83.53 

Round  26, Train loss: 1.641, Test loss: 1.667, Test accuracy: 83.27 

Round  27, Train loss: 1.627, Test loss: 1.655, Test accuracy: 83.98 

Round  28, Train loss: 1.616, Test loss: 1.647, Test accuracy: 84.48 

Round  29, Train loss: 1.612, Test loss: 1.643, Test accuracy: 84.85 

Round  30, Train loss: 1.596, Test loss: 1.639, Test accuracy: 85.13 

Round  31, Train loss: 1.591, Test loss: 1.633, Test accuracy: 85.55 

Round  32, Train loss: 1.583, Test loss: 1.629, Test accuracy: 85.90 

Round  33, Train loss: 1.586, Test loss: 1.623, Test accuracy: 86.10 

Round  34, Train loss: 1.590, Test loss: 1.616, Test accuracy: 86.85 

Round  35, Train loss: 1.577, Test loss: 1.614, Test accuracy: 86.93 

Round  36, Train loss: 1.571, Test loss: 1.611, Test accuracy: 86.92 

Round  37, Train loss: 1.564, Test loss: 1.612, Test accuracy: 86.97 

Round  38, Train loss: 1.565, Test loss: 1.606, Test accuracy: 87.23 

Round  39, Train loss: 1.559, Test loss: 1.604, Test accuracy: 87.48 

Round  40, Train loss: 1.556, Test loss: 1.603, Test accuracy: 87.65 

Round  41, Train loss: 1.556, Test loss: 1.600, Test accuracy: 87.88 

Round  42, Train loss: 1.551, Test loss: 1.600, Test accuracy: 87.90 

Round  43, Train loss: 1.559, Test loss: 1.600, Test accuracy: 87.65 

Round  44, Train loss: 1.550, Test loss: 1.599, Test accuracy: 87.63 

Round  45, Train loss: 1.545, Test loss: 1.597, Test accuracy: 87.93 

Round  46, Train loss: 1.547, Test loss: 1.595, Test accuracy: 88.07 

Round  47, Train loss: 1.543, Test loss: 1.594, Test accuracy: 88.23 

Round  48, Train loss: 1.546, Test loss: 1.594, Test accuracy: 88.05 

Round  49, Train loss: 1.541, Test loss: 1.594, Test accuracy: 88.03 

Final Round, Train loss: 1.542, Test loss: 1.588, Test accuracy: 88.43 

Average accuracy final 10 rounds: 87.90333333333332 

266.05223512649536
[1.496185541152954, 2.0491793155670166, 2.6065828800201416, 3.1560964584350586, 3.7129123210906982, 4.262228965759277, 4.817057847976685, 5.370055675506592, 5.927248001098633, 6.480727434158325, 7.032749652862549, 7.61852765083313, 8.172215938568115, 8.729745626449585, 9.278610944747925, 9.83212423324585, 10.381807088851929, 10.934357166290283, 11.481483221054077, 12.033695220947266, 12.5827054977417, 13.135453462600708, 13.707649946212769, 14.262836933135986, 14.816662549972534, 15.371935367584229, 15.928874254226685, 16.592085123062134, 17.24255633354187, 17.897950410842896, 18.549693822860718, 19.125269651412964, 19.6727876663208, 20.22658634185791, 20.774383544921875, 21.32625675201416, 21.87844157218933, 22.45299243927002, 23.018441438674927, 23.617382287979126, 24.173743963241577, 24.734196424484253, 25.293569087982178, 25.84972333908081, 26.40749979019165, 26.968066930770874, 27.528899669647217, 28.119855880737305, 28.674407482147217, 29.226014137268066, 30.187812566757202]
[16.766666666666666, 17.666666666666668, 19.766666666666666, 23.35, 28.033333333333335, 34.6, 38.55, 41.56666666666667, 41.6, 42.05, 42.38333333333333, 41.1, 41.4, 43.71666666666667, 47.06666666666667, 52.13333333333333, 57.75, 61.71666666666667, 67.88333333333334, 70.95, 73.53333333333333, 76.06666666666666, 78.25, 80.48333333333333, 82.68333333333334, 83.53333333333333, 83.26666666666667, 83.98333333333333, 84.48333333333333, 84.85, 85.13333333333334, 85.55, 85.9, 86.1, 86.85, 86.93333333333334, 86.91666666666667, 86.96666666666667, 87.23333333333333, 87.48333333333333, 87.65, 87.88333333333334, 87.9, 87.65, 87.63333333333334, 87.93333333333334, 88.06666666666666, 88.23333333333333, 88.05, 88.03333333333333, 88.43333333333334]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.322, Test loss: 2.301, Test accuracy: 16.75
Round   1, Train loss: 2.300, Test loss: 2.299, Test accuracy: 18.30
Round   2, Train loss: 2.299, Test loss: 2.297, Test accuracy: 22.08
Round   3, Train loss: 2.297, Test loss: 2.295, Test accuracy: 29.38
Round   4, Train loss: 2.295, Test loss: 2.293, Test accuracy: 36.82
Round   5, Train loss: 2.292, Test loss: 2.290, Test accuracy: 39.93
Round   6, Train loss: 2.289, Test loss: 2.286, Test accuracy: 42.18
Round   7, Train loss: 2.285, Test loss: 2.281, Test accuracy: 45.32
Round   8, Train loss: 2.278, Test loss: 2.273, Test accuracy: 44.37
Round   9, Train loss: 2.270, Test loss: 2.262, Test accuracy: 45.57
Round  10, Train loss: 2.256, Test loss: 2.244, Test accuracy: 45.67
Round  11, Train loss: 2.234, Test loss: 2.211, Test accuracy: 43.78
Round  12, Train loss: 2.183, Test loss: 2.152, Test accuracy: 43.67
Round  13, Train loss: 2.132, Test loss: 2.092, Test accuracy: 47.78
Round  14, Train loss: 2.051, Test loss: 2.031, Test accuracy: 53.57
Round  15, Train loss: 1.988, Test loss: 1.971, Test accuracy: 59.23
Round  16, Train loss: 1.943, Test loss: 1.920, Test accuracy: 63.97
Round  17, Train loss: 1.893, Test loss: 1.883, Test accuracy: 66.77
Round  18, Train loss: 1.871, Test loss: 1.848, Test accuracy: 70.42
Round  19, Train loss: 1.817, Test loss: 1.817, Test accuracy: 74.03
Round  20, Train loss: 1.783, Test loss: 1.791, Test accuracy: 76.85
Round  21, Train loss: 1.764, Test loss: 1.770, Test accuracy: 78.05
Round  22, Train loss: 1.761, Test loss: 1.754, Test accuracy: 79.18
Round  23, Train loss: 1.735, Test loss: 1.739, Test accuracy: 80.07
Round  24, Train loss: 1.723, Test loss: 1.726, Test accuracy: 80.90
Round  25, Train loss: 1.729, Test loss: 1.709, Test accuracy: 82.03
Round  26, Train loss: 1.698, Test loss: 1.700, Test accuracy: 83.25
Round  27, Train loss: 1.699, Test loss: 1.690, Test accuracy: 84.48
Round  28, Train loss: 1.674, Test loss: 1.676, Test accuracy: 85.80
Round  29, Train loss: 1.682, Test loss: 1.664, Test accuracy: 87.12
Round  30, Train loss: 1.669, Test loss: 1.655, Test accuracy: 87.85
Round  31, Train loss: 1.659, Test loss: 1.647, Test accuracy: 88.25
Round  32, Train loss: 1.642, Test loss: 1.641, Test accuracy: 88.63
Round  33, Train loss: 1.629, Test loss: 1.636, Test accuracy: 88.83
Round  34, Train loss: 1.622, Test loss: 1.632, Test accuracy: 88.97
Round  35, Train loss: 1.620, Test loss: 1.626, Test accuracy: 89.17
Round  36, Train loss: 1.601, Test loss: 1.622, Test accuracy: 89.58
Round  37, Train loss: 1.607, Test loss: 1.618, Test accuracy: 89.83
Round  38, Train loss: 1.600, Test loss: 1.615, Test accuracy: 90.20
Round  39, Train loss: 1.589, Test loss: 1.612, Test accuracy: 90.30
Round  40, Train loss: 1.598, Test loss: 1.608, Test accuracy: 90.57
Round  41, Train loss: 1.600, Test loss: 1.604, Test accuracy: 90.72
Round  42, Train loss: 1.576, Test loss: 1.602, Test accuracy: 91.03
Round  43, Train loss: 1.579, Test loss: 1.600, Test accuracy: 91.07
Round  44, Train loss: 1.568, Test loss: 1.600, Test accuracy: 91.05
Round  45, Train loss: 1.574, Test loss: 1.597, Test accuracy: 91.13
Round  46, Train loss: 1.578, Test loss: 1.593, Test accuracy: 91.38
Round  47, Train loss: 1.566, Test loss: 1.592, Test accuracy: 91.37
Round  48, Train loss: 1.581, Test loss: 1.588, Test accuracy: 91.55
Round  49, Train loss: 1.585, Test loss: 1.584, Test accuracy: 91.82
Final Round, Train loss: 1.546, Test loss: 1.577, Test accuracy: 91.62
Average accuracy final 10 rounds: 91.16833333333335
307.707505941391
[1.6634526252746582, 2.3960776329040527, 3.1275153160095215, 3.8431029319763184, 4.5369956493377686, 5.233421087265015, 5.927154541015625, 6.627488613128662, 7.315176010131836, 8.014766454696655, 8.702368021011353, 9.400214910507202, 10.093098640441895, 10.787172079086304, 11.483246088027954, 12.196094989776611, 12.889341592788696, 13.584346294403076, 14.279271364212036, 14.976828336715698, 15.677608013153076, 16.374655723571777, 17.075106859207153, 17.77160668373108, 18.478135585784912, 19.17521071434021, 19.873886346817017, 20.571224689483643, 21.27218222618103, 21.96680474281311, 22.660905838012695, 23.356427907943726, 24.052727937698364, 24.746963262557983, 25.438832759857178, 26.13762068748474, 26.83282971382141, 27.529595613479614, 28.242173194885254, 28.942049741744995, 29.64451241493225, 30.34295082092285, 31.041576385498047, 31.744022846221924, 32.445903301239014, 33.14570236206055, 33.8760871887207, 34.60906505584717, 35.33782434463501, 36.0329692363739, 37.069518089294434]
[16.75, 18.3, 22.083333333333332, 29.383333333333333, 36.81666666666667, 39.93333333333333, 42.18333333333333, 45.31666666666667, 44.36666666666667, 45.56666666666667, 45.666666666666664, 43.78333333333333, 43.666666666666664, 47.78333333333333, 53.56666666666667, 59.233333333333334, 63.96666666666667, 66.76666666666667, 70.41666666666667, 74.03333333333333, 76.85, 78.05, 79.18333333333334, 80.06666666666666, 80.9, 82.03333333333333, 83.25, 84.48333333333333, 85.8, 87.11666666666666, 87.85, 88.25, 88.63333333333334, 88.83333333333333, 88.96666666666667, 89.16666666666667, 89.58333333333333, 89.83333333333333, 90.2, 90.3, 90.56666666666666, 90.71666666666667, 91.03333333333333, 91.06666666666666, 91.05, 91.13333333333334, 91.38333333333334, 91.36666666666666, 91.55, 91.81666666666666, 91.61666666666666]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 17.03
Round   1, Train loss: 2.300, Test loss: 2.298, Test accuracy: 24.72
Round   2, Train loss: 2.298, Test loss: 2.295, Test accuracy: 32.32
Round   3, Train loss: 2.294, Test loss: 2.290, Test accuracy: 42.13
Round   4, Train loss: 2.290, Test loss: 2.280, Test accuracy: 45.80
Round   5, Train loss: 2.281, Test loss: 2.251, Test accuracy: 37.85
Round   6, Train loss: 2.267, Test loss: 2.151, Test accuracy: 37.40
Round   7, Train loss: 2.181, Test loss: 2.049, Test accuracy: 45.58
Round   8, Train loss: 2.114, Test loss: 1.973, Test accuracy: 51.70
Round   9, Train loss: 2.059, Test loss: 1.894, Test accuracy: 62.42
Round  10, Train loss: 1.977, Test loss: 1.804, Test accuracy: 72.68
Round  11, Train loss: 1.845, Test loss: 1.731, Test accuracy: 78.63
Round  12, Train loss: 1.788, Test loss: 1.685, Test accuracy: 82.83
Round  13, Train loss: 1.779, Test loss: 1.649, Test accuracy: 85.70
Round  14, Train loss: 1.715, Test loss: 1.626, Test accuracy: 86.92
Round  15, Train loss: 1.648, Test loss: 1.614, Test accuracy: 87.08
Round  16, Train loss: 1.604, Test loss: 1.604, Test accuracy: 87.67
Round  17, Train loss: 1.657, Test loss: 1.597, Test accuracy: 87.73
Round  18, Train loss: 1.576, Test loss: 1.591, Test accuracy: 88.40
Round  19, Train loss: 1.591, Test loss: 1.588, Test accuracy: 88.52
Round  20, Train loss: 1.560, Test loss: 1.585, Test accuracy: 88.95
Round  21, Train loss: 1.542, Test loss: 1.581, Test accuracy: 88.95
Round  22, Train loss: 1.540, Test loss: 1.577, Test accuracy: 89.68
Round  23, Train loss: 1.553, Test loss: 1.576, Test accuracy: 89.35
Round  24, Train loss: 1.535, Test loss: 1.573, Test accuracy: 89.70
Round  25, Train loss: 1.541, Test loss: 1.572, Test accuracy: 89.98
Round  26, Train loss: 1.529, Test loss: 1.571, Test accuracy: 89.92
Round  27, Train loss: 1.528, Test loss: 1.568, Test accuracy: 90.32
Round  28, Train loss: 1.532, Test loss: 1.565, Test accuracy: 90.33
Round  29, Train loss: 1.516, Test loss: 1.567, Test accuracy: 90.00
Round  30, Train loss: 1.522, Test loss: 1.565, Test accuracy: 90.12
Round  31, Train loss: 1.512, Test loss: 1.565, Test accuracy: 90.23
Round  32, Train loss: 1.515, Test loss: 1.561, Test accuracy: 90.77
Round  33, Train loss: 1.511, Test loss: 1.562, Test accuracy: 90.72
Round  34, Train loss: 1.507, Test loss: 1.562, Test accuracy: 90.52
Round  35, Train loss: 1.501, Test loss: 1.560, Test accuracy: 90.68
Round  36, Train loss: 1.508, Test loss: 1.558, Test accuracy: 90.80
Round  37, Train loss: 1.511, Test loss: 1.557, Test accuracy: 91.13
Round  38, Train loss: 1.504, Test loss: 1.557, Test accuracy: 91.03
Round  39, Train loss: 1.512, Test loss: 1.558, Test accuracy: 90.70
Round  40, Train loss: 1.506, Test loss: 1.555, Test accuracy: 91.13
Round  41, Train loss: 1.505, Test loss: 1.555, Test accuracy: 91.17
Round  42, Train loss: 1.500, Test loss: 1.554, Test accuracy: 91.18
Round  43, Train loss: 1.507, Test loss: 1.555, Test accuracy: 91.13
Round  44, Train loss: 1.499, Test loss: 1.553, Test accuracy: 91.38
Round  45, Train loss: 1.496, Test loss: 1.554, Test accuracy: 91.27
Round  46, Train loss: 1.499, Test loss: 1.553, Test accuracy: 91.45
Round  47, Train loss: 1.489, Test loss: 1.552, Test accuracy: 91.40
Round  48, Train loss: 1.495, Test loss: 1.550, Test accuracy: 91.53
Round  49, Train loss: 1.495, Test loss: 1.551, Test accuracy: 91.52
Final Round, Train loss: 1.497, Test loss: 1.550, Test accuracy: 91.63
Average accuracy final 10 rounds: 91.31666666666668
620.7292561531067
[2.8161253929138184, 4.735254287719727, 6.375726222991943, 8.035040378570557, 9.68232250213623, 11.326132774353027, 12.983393907546997, 14.629921436309814, 16.273243188858032, 17.919546842575073, 19.566293001174927, 21.219613552093506, 22.97824215888977, 24.738739252090454, 26.49890112876892, 28.259289026260376, 30.02768611907959, 31.777425289154053, 33.51438307762146, 35.15016579627991, 36.78957176208496, 38.626376152038574, 40.267494440078735, 41.91140079498291, 43.547308921813965, 45.18964862823486, 46.92422342300415, 48.66724395751953, 50.306296586990356, 51.94517421722412, 53.582244873046875, 55.23851799964905, 56.90705633163452, 58.54401469230652, 60.17513084411621, 61.81516408920288, 63.45401644706726, 65.08764457702637, 66.72226929664612, 68.36126041412354, 70.00888228416443, 71.65037369728088, 73.28695130348206, 74.92822241783142, 76.56903910636902, 78.50729417800903, 80.44833874702454, 82.37187910079956, 84.30685758590698, 86.24333596229553, 88.17213249206543]
[17.033333333333335, 24.716666666666665, 32.31666666666667, 42.13333333333333, 45.8, 37.85, 37.4, 45.583333333333336, 51.7, 62.416666666666664, 72.68333333333334, 78.63333333333334, 82.83333333333333, 85.7, 86.91666666666667, 87.08333333333333, 87.66666666666667, 87.73333333333333, 88.4, 88.51666666666667, 88.95, 88.95, 89.68333333333334, 89.35, 89.7, 89.98333333333333, 89.91666666666667, 90.31666666666666, 90.33333333333333, 90.0, 90.11666666666666, 90.23333333333333, 90.76666666666667, 90.71666666666667, 90.51666666666667, 90.68333333333334, 90.8, 91.13333333333334, 91.03333333333333, 90.7, 91.13333333333334, 91.16666666666667, 91.18333333333334, 91.13333333333334, 91.38333333333334, 91.26666666666667, 91.45, 91.4, 91.53333333333333, 91.51666666666667, 91.63333333333334]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.710, Test loss: 2.297, Test accuracy: 12.48
Round   1, Train loss: 1.666, Test loss: 2.285, Test accuracy: 27.28
Round   2, Train loss: 1.619, Test loss: 2.262, Test accuracy: 38.53
Round   3, Train loss: 1.544, Test loss: 2.238, Test accuracy: 45.93
Round   4, Train loss: 1.462, Test loss: 2.206, Test accuracy: 53.47
Round   5, Train loss: 1.375, Test loss: 2.170, Test accuracy: 56.12
Round   6, Train loss: 1.315, Test loss: 2.141, Test accuracy: 58.92
Round   7, Train loss: 1.305, Test loss: 2.112, Test accuracy: 62.58
Round   8, Train loss: 1.293, Test loss: 2.086, Test accuracy: 65.93
Round   9, Train loss: 1.229, Test loss: 2.064, Test accuracy: 70.13
Round  10, Train loss: 1.225, Test loss: 2.050, Test accuracy: 71.17
Round  11, Train loss: 1.196, Test loss: 2.038, Test accuracy: 74.70
Round  12, Train loss: 1.171, Test loss: 2.023, Test accuracy: 77.17
Round  13, Train loss: 1.162, Test loss: 2.013, Test accuracy: 79.63
Round  14, Train loss: 1.134, Test loss: 2.003, Test accuracy: 79.87
Round  15, Train loss: 1.143, Test loss: 1.996, Test accuracy: 81.33
Round  16, Train loss: 1.134, Test loss: 1.990, Test accuracy: 81.75
Round  17, Train loss: 1.132, Test loss: 1.986, Test accuracy: 81.38
Round  18, Train loss: 1.127, Test loss: 1.981, Test accuracy: 81.47
Round  19, Train loss: 1.131, Test loss: 1.978, Test accuracy: 81.57
Round  20, Train loss: 1.124, Test loss: 1.975, Test accuracy: 81.15
Round  21, Train loss: 1.121, Test loss: 1.972, Test accuracy: 80.97
Round  22, Train loss: 1.130, Test loss: 1.969, Test accuracy: 81.58
Round  23, Train loss: 1.128, Test loss: 1.969, Test accuracy: 81.38
Round  24, Train loss: 1.118, Test loss: 1.966, Test accuracy: 81.00
Round  25, Train loss: 1.119, Test loss: 1.965, Test accuracy: 80.82
Round  26, Train loss: 1.120, Test loss: 1.962, Test accuracy: 80.48
Round  27, Train loss: 1.119, Test loss: 1.961, Test accuracy: 80.22
Round  28, Train loss: 1.119, Test loss: 1.959, Test accuracy: 80.02
Round  29, Train loss: 1.118, Test loss: 1.959, Test accuracy: 79.80
Round  30, Train loss: 1.116, Test loss: 1.959, Test accuracy: 79.70
Round  31, Train loss: 1.121, Test loss: 1.958, Test accuracy: 79.58
Round  32, Train loss: 1.116, Test loss: 1.956, Test accuracy: 79.20
Round  33, Train loss: 1.114, Test loss: 1.955, Test accuracy: 78.78
Round  34, Train loss: 1.115, Test loss: 1.953, Test accuracy: 78.83
Round  35, Train loss: 1.113, Test loss: 1.954, Test accuracy: 78.48
Round  36, Train loss: 1.117, Test loss: 1.954, Test accuracy: 77.90
Round  37, Train loss: 1.118, Test loss: 1.955, Test accuracy: 77.37
Round  38, Train loss: 1.115, Test loss: 1.954, Test accuracy: 77.05
Round  39, Train loss: 1.117, Test loss: 1.953, Test accuracy: 76.75
Round  40, Train loss: 1.114, Test loss: 1.953, Test accuracy: 76.40
Round  41, Train loss: 1.115, Test loss: 1.952, Test accuracy: 76.15
Round  42, Train loss: 1.113, Test loss: 1.951, Test accuracy: 75.98
Round  43, Train loss: 1.114, Test loss: 1.953, Test accuracy: 75.28
Round  44, Train loss: 1.113, Test loss: 1.951, Test accuracy: 75.32
Round  45, Train loss: 1.116, Test loss: 1.952, Test accuracy: 75.00
Round  46, Train loss: 1.115, Test loss: 1.953, Test accuracy: 74.53
Round  47, Train loss: 1.110, Test loss: 1.951, Test accuracy: 74.48
Round  48, Train loss: 1.114, Test loss: 1.953, Test accuracy: 74.05
Round  49, Train loss: 1.113, Test loss: 1.953, Test accuracy: 73.65
Final Round, Train loss: 1.112, Test loss: 1.953, Test accuracy: 73.33
Average accuracy final 10 rounds: 75.085
498.2933940887451
[]
[12.483333333333333, 27.283333333333335, 38.53333333333333, 45.93333333333333, 53.46666666666667, 56.11666666666667, 58.916666666666664, 62.583333333333336, 65.93333333333334, 70.13333333333334, 71.16666666666667, 74.7, 77.16666666666667, 79.63333333333334, 79.86666666666666, 81.33333333333333, 81.75, 81.38333333333334, 81.46666666666667, 81.56666666666666, 81.15, 80.96666666666667, 81.58333333333333, 81.38333333333334, 81.0, 80.81666666666666, 80.48333333333333, 80.21666666666667, 80.01666666666667, 79.8, 79.7, 79.58333333333333, 79.2, 78.78333333333333, 78.83333333333333, 78.48333333333333, 77.9, 77.36666666666666, 77.05, 76.75, 76.4, 76.15, 75.98333333333333, 75.28333333333333, 75.31666666666666, 75.0, 74.53333333333333, 74.48333333333333, 74.05, 73.65, 73.33333333333333]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12
Round   0: Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.37
Round   1, Train loss: 2.299, Test loss: 2.302, Test accuracy: 11.08
Round   1: Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 9.67
Round   2, Train loss: 2.299, Test loss: 2.301, Test accuracy: 11.20
Round   2: Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 9.70
Round   3, Train loss: 2.318, Test loss: 2.300, Test accuracy: 13.48
Round   3: Global train loss: 2.318, Global test loss: 2.302, Global test accuracy: 10.40
Round   4, Train loss: 2.295, Test loss: 2.301, Test accuracy: 13.08
Round   4: Global train loss: 2.295, Global test loss: 2.302, Global test accuracy: 10.42
Round   5, Train loss: 2.311, Test loss: 2.300, Test accuracy: 14.78
Round   5: Global train loss: 2.311, Global test loss: 2.302, Global test accuracy: 10.68
Round   6, Train loss: 2.297, Test loss: 2.299, Test accuracy: 15.85
Round   6: Global train loss: 2.297, Global test loss: 2.301, Global test accuracy: 11.52
Round   7, Train loss: 2.296, Test loss: 2.299, Test accuracy: 16.30
Round   7: Global train loss: 2.296, Global test loss: 2.301, Global test accuracy: 11.95
Round   8, Train loss: 2.301, Test loss: 2.300, Test accuracy: 15.40
Round   8: Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.30
Round   9, Train loss: 2.296, Test loss: 2.299, Test accuracy: 15.60
Round   9: Global train loss: 2.296, Global test loss: 2.301, Global test accuracy: 12.68
Round  10, Train loss: 2.281, Test loss: 2.297, Test accuracy: 16.60
Round  10: Global train loss: 2.281, Global test loss: 2.301, Global test accuracy: 13.63
Round  11, Train loss: 2.298, Test loss: 2.297, Test accuracy: 19.83
Round  11: Global train loss: 2.298, Global test loss: 2.300, Global test accuracy: 14.12
Round  12, Train loss: 2.269, Test loss: 2.293, Test accuracy: 18.92
Round  12: Global train loss: 2.269, Global test loss: 2.300, Global test accuracy: 14.67
Round  13, Train loss: 2.294, Test loss: 2.294, Test accuracy: 17.77
Round  13: Global train loss: 2.294, Global test loss: 2.300, Global test accuracy: 14.92
Round  14, Train loss: 2.292, Test loss: 2.296, Test accuracy: 21.38
Round  14: Global train loss: 2.292, Global test loss: 2.300, Global test accuracy: 14.65
Round  15, Train loss: 2.284, Test loss: 2.296, Test accuracy: 20.48
Round  15: Global train loss: 2.284, Global test loss: 2.300, Global test accuracy: 15.07
Round  16, Train loss: 2.310, Test loss: 2.297, Test accuracy: 18.15
Round  16: Global train loss: 2.310, Global test loss: 2.300, Global test accuracy: 15.30
Round  17, Train loss: 2.319, Test loss: 2.299, Test accuracy: 17.87
Round  17: Global train loss: 2.319, Global test loss: 2.300, Global test accuracy: 14.97
Round  18, Train loss: 2.292, Test loss: 2.297, Test accuracy: 19.30
Round  18: Global train loss: 2.292, Global test loss: 2.299, Global test accuracy: 15.25
Round  19, Train loss: 2.200, Test loss: 2.289, Test accuracy: 23.30
Round  19: Global train loss: 2.200, Global test loss: 2.299, Global test accuracy: 16.28
Round  20, Train loss: 2.289, Test loss: 2.290, Test accuracy: 20.80
Round  20: Global train loss: 2.289, Global test loss: 2.299, Global test accuracy: 16.42
Round  21, Train loss: 2.276, Test loss: 2.289, Test accuracy: 21.20
Round  21: Global train loss: 2.276, Global test loss: 2.298, Global test accuracy: 16.53
Round  22, Train loss: 2.199, Test loss: 2.284, Test accuracy: 22.38
Round  22: Global train loss: 2.199, Global test loss: 2.298, Global test accuracy: 18.40
Round  23, Train loss: 2.308, Test loss: 2.290, Test accuracy: 21.67
Round  23: Global train loss: 2.308, Global test loss: 2.298, Global test accuracy: 18.50
Round  24, Train loss: 2.294, Test loss: 2.289, Test accuracy: 19.65
Round  24: Global train loss: 2.294, Global test loss: 2.298, Global test accuracy: 18.50
Round  25, Train loss: 2.320, Test loss: 2.293, Test accuracy: 19.23
Round  25: Global train loss: 2.320, Global test loss: 2.298, Global test accuracy: 17.67
Round  26, Train loss: 2.182, Test loss: 2.285, Test accuracy: 21.68
Round  26: Global train loss: 2.182, Global test loss: 2.297, Global test accuracy: 20.12
Round  27, Train loss: 2.314, Test loss: 2.290, Test accuracy: 20.58
Round  27: Global train loss: 2.314, Global test loss: 2.297, Global test accuracy: 18.28
Round  28, Train loss: 2.182, Test loss: 2.280, Test accuracy: 21.67
Round  28: Global train loss: 2.182, Global test loss: 2.297, Global test accuracy: 20.40
Round  29, Train loss: 2.290, Test loss: 2.280, Test accuracy: 20.72
Round  29: Global train loss: 2.290, Global test loss: 2.297, Global test accuracy: 19.80
Round  30, Train loss: 2.288, Test loss: 2.284, Test accuracy: 21.57
Round  30: Global train loss: 2.288, Global test loss: 2.297, Global test accuracy: 19.92
Round  31, Train loss: 2.243, Test loss: 2.281, Test accuracy: 23.42
Round  31: Global train loss: 2.243, Global test loss: 2.296, Global test accuracy: 20.55
Round  32, Train loss: 2.227, Test loss: 2.277, Test accuracy: 23.92
Round  32: Global train loss: 2.227, Global test loss: 2.296, Global test accuracy: 20.98
Round  33, Train loss: 2.210, Test loss: 2.271, Test accuracy: 23.18
Round  33: Global train loss: 2.210, Global test loss: 2.296, Global test accuracy: 23.30
Round  34, Train loss: 2.094, Test loss: 2.273, Test accuracy: 23.23
Round  34: Global train loss: 2.094, Global test loss: 2.296, Global test accuracy: 24.38
Round  35, Train loss: 2.165, Test loss: 2.265, Test accuracy: 24.50
Round  35: Global train loss: 2.165, Global test loss: 2.295, Global test accuracy: 27.40
Round  36, Train loss: 2.182, Test loss: 2.263, Test accuracy: 22.73
Round  36: Global train loss: 2.182, Global test loss: 2.294, Global test accuracy: 28.88
Round  37, Train loss: 2.065, Test loss: 2.254, Test accuracy: 22.55
Round  37: Global train loss: 2.065, Global test loss: 2.293, Global test accuracy: 30.77
Round  38, Train loss: 2.204, Test loss: 2.258, Test accuracy: 23.10
Round  38: Global train loss: 2.204, Global test loss: 2.293, Global test accuracy: 29.83
Round  39, Train loss: 1.910, Test loss: 2.240, Test accuracy: 24.02
Round  39: Global train loss: 1.910, Global test loss: 2.292, Global test accuracy: 31.30
Round  40, Train loss: 2.105, Test loss: 2.249, Test accuracy: 23.72
Round  40: Global train loss: 2.105, Global test loss: 2.292, Global test accuracy: 28.83
Round  41, Train loss: 1.636, Test loss: 2.223, Test accuracy: 25.25
Round  41: Global train loss: 1.636, Global test loss: 2.290, Global test accuracy: 31.53
Round  42, Train loss: 1.939, Test loss: 2.207, Test accuracy: 27.12
Round  42: Global train loss: 1.939, Global test loss: 2.288, Global test accuracy: 35.38
Round  43, Train loss: 1.968, Test loss: 2.223, Test accuracy: 28.33
Round  43: Global train loss: 1.968, Global test loss: 2.289, Global test accuracy: 37.38
Round  44, Train loss: 2.174, Test loss: 2.234, Test accuracy: 24.65
Round  44: Global train loss: 2.174, Global test loss: 2.290, Global test accuracy: 33.15
Round  45, Train loss: 1.902, Test loss: 2.224, Test accuracy: 26.65
Round  45: Global train loss: 1.902, Global test loss: 2.289, Global test accuracy: 34.52
Round  46, Train loss: 1.627, Test loss: 2.191, Test accuracy: 29.98
Round  46: Global train loss: 1.627, Global test loss: 2.286, Global test accuracy: 42.48
Round  47, Train loss: 1.129, Test loss: 2.144, Test accuracy: 34.97
Round  47: Global train loss: 1.129, Global test loss: 2.280, Global test accuracy: 45.38
Round  48, Train loss: 2.114, Test loss: 2.184, Test accuracy: 32.10
Round  48: Global train loss: 2.114, Global test loss: 2.282, Global test accuracy: 48.20
Round  49, Train loss: 1.494, Test loss: 2.142, Test accuracy: 36.83
Round  49: Global train loss: 1.494, Global test loss: 2.277, Global test accuracy: 49.78
Final Round: Train loss: 2.207, Test loss: 2.129, Test accuracy: 38.30
Final Round: Global train loss: 2.207, Global test loss: 2.269, Global test accuracy: 48.78
Average accuracy final 10 rounds: 28.96
Average global accuracy final 10 rounds: 38.665
410.822016954422
[]
[10.116666666666667, 11.083333333333334, 11.2, 13.483333333333333, 13.083333333333334, 14.783333333333333, 15.85, 16.3, 15.4, 15.6, 16.6, 19.833333333333332, 18.916666666666668, 17.766666666666666, 21.383333333333333, 20.483333333333334, 18.15, 17.866666666666667, 19.3, 23.3, 20.8, 21.2, 22.383333333333333, 21.666666666666668, 19.65, 19.233333333333334, 21.683333333333334, 20.583333333333332, 21.666666666666668, 20.716666666666665, 21.566666666666666, 23.416666666666668, 23.916666666666668, 23.183333333333334, 23.233333333333334, 24.5, 22.733333333333334, 22.55, 23.1, 24.016666666666666, 23.716666666666665, 25.25, 27.116666666666667, 28.333333333333332, 24.65, 26.65, 29.983333333333334, 34.96666666666667, 32.1, 36.833333333333336, 38.3]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.50 

Round   0, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.48 

Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.55 

Round   1, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.55 

Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.55 

Round   2, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.55 

Round   3, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.57 

Round   3, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.55 

Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.60 

Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.62 

Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.58 

Round   5, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.65 

Round   6, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.58 

Round   6, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.65 

Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.60 

Round   7, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.63 

Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.60 

Round   8, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.62 

Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.62 

Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.65 

Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.60 

Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.70 

Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.65 

Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.72 

Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.65 

Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.75 

Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.67 

Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.78 

Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.75 

Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.82 

Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.80 

Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.82 

Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.83 

Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.80 

Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.78 

Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.87 

Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.83 

Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.90 

Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.82 

Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.97 

Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.87 

Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.05 

Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.92 

Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.05 

Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.95 

Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.10 

Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.97 

Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.18 

Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.98 

Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.23 

Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.08 

Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.27 

Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.17 

Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.33 

Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.22 

Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.33 

Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.25 

Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.42 

Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.30 

Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.40 

Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.30 

Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.42 

Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.37 

Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.45 

Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.38 

Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.47 

Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.42 

Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.52 

Round  34, Train loss: 2.301, Test loss: 2.302, Test accuracy: 16.45 

Round  34, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 16.52 

Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.45 

Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.52 

Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.50 

Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.55 

Round  37, Train loss: 2.301, Test loss: 2.302, Test accuracy: 16.48 

Round  37, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 16.50 

Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.47 

Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.52 

Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.47 

Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.50 

Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.48 

Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.57 

Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.52 

Round  41, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 16.62 

Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.55 

Round  42, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 16.62 

Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.55 

Round  43, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 16.62 

Round  44, Train loss: 2.302, Test loss: 2.301, Test accuracy: 16.58 

Round  44, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 16.63 

Round  45, Train loss: 2.302, Test loss: 2.301, Test accuracy: 16.58 

Round  45, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 16.65 

Round  46, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.62 

Round  46, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.68 

Round  47, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.63 

Round  47, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.68 

Round  48, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.65 

Round  48, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.68 

Round  49, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.65 

Round  49, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.67 

Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.75 

Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.67 

Average accuracy final 10 rounds: 16.581666666666667 

Average global accuracy final 10 rounds: 16.641666666666666 

391.2824628353119
[1.6055209636688232, 2.294818878173828, 2.9881339073181152, 3.6726176738739014, 4.354625225067139, 5.03950047492981, 5.725555181503296, 6.410434246063232, 7.089457035064697, 7.7729926109313965, 8.515003442764282, 9.255590438842773, 10.029654264450073, 10.795177936553955, 11.562010526657104, 12.284410238265991, 13.00722885131836, 13.696513891220093, 14.406408309936523, 15.100022554397583, 15.7953040599823, 16.490990161895752, 17.182628870010376, 17.87227725982666, 18.562986373901367, 19.255141496658325, 19.942513942718506, 20.633820295333862, 21.319024801254272, 22.00860857963562, 22.695472955703735, 23.38497281074524, 24.073535203933716, 24.768211364746094, 25.458314180374146, 26.142578840255737, 26.834952354431152, 27.518489599227905, 28.206554412841797, 28.894121885299683, 29.586864471435547, 30.29466438293457, 30.9823055267334, 31.678810596466064, 32.39836072921753, 33.09195828437805, 33.78622889518738, 34.47762441635132, 35.1665780544281, 35.859007835388184, 37.23670530319214]
[15.5, 15.55, 15.55, 15.566666666666666, 15.6, 15.583333333333334, 15.583333333333334, 15.6, 15.6, 15.616666666666667, 15.6, 15.65, 15.65, 15.666666666666666, 15.75, 15.8, 15.833333333333334, 15.783333333333333, 15.833333333333334, 15.816666666666666, 15.866666666666667, 15.916666666666666, 15.95, 15.966666666666667, 15.983333333333333, 16.083333333333332, 16.166666666666668, 16.216666666666665, 16.25, 16.3, 16.3, 16.366666666666667, 16.383333333333333, 16.416666666666668, 16.45, 16.45, 16.5, 16.483333333333334, 16.466666666666665, 16.466666666666665, 16.483333333333334, 16.516666666666666, 16.55, 16.55, 16.583333333333332, 16.583333333333332, 16.616666666666667, 16.633333333333333, 16.65, 16.65, 16.75]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.292, Test loss: 2.293, Test accuracy: 21.23 

Round   0, Global train loss: 2.292, Global test loss: 2.300, Global test accuracy: 14.33 

Round   1, Train loss: 2.214, Test loss: 2.226, Test accuracy: 26.23 

Round   1, Global train loss: 2.214, Global test loss: 2.268, Global test accuracy: 13.33 

Round   2, Train loss: 2.129, Test loss: 2.102, Test accuracy: 43.40 

Round   2, Global train loss: 2.129, Global test loss: 2.257, Global test accuracy: 24.63 

Round   3, Train loss: 1.910, Test loss: 1.971, Test accuracy: 59.10 

Round   3, Global train loss: 1.910, Global test loss: 2.240, Global test accuracy: 31.37 

Round   4, Train loss: 1.704, Test loss: 1.903, Test accuracy: 63.85 

Round   4, Global train loss: 1.704, Global test loss: 2.300, Global test accuracy: 10.00 

Round   5, Train loss: 1.806, Test loss: 1.740, Test accuracy: 81.13 

Round   5, Global train loss: 1.806, Global test loss: 2.196, Global test accuracy: 31.52 

Round   6, Train loss: 1.712, Test loss: 1.678, Test accuracy: 84.23 

Round   6, Global train loss: 1.712, Global test loss: 2.205, Global test accuracy: 26.75 

Round   7, Train loss: 1.576, Test loss: 1.643, Test accuracy: 87.22 

Round   7, Global train loss: 1.576, Global test loss: 2.183, Global test accuracy: 25.52 

Round   8, Train loss: 1.593, Test loss: 1.614, Test accuracy: 89.72 

Round   8, Global train loss: 1.593, Global test loss: 2.205, Global test accuracy: 21.35 

Round   9, Train loss: 1.499, Test loss: 1.600, Test accuracy: 90.23 

Round   9, Global train loss: 1.499, Global test loss: 2.145, Global test accuracy: 39.77 

Round  10, Train loss: 1.573, Test loss: 1.578, Test accuracy: 91.80 

Round  10, Global train loss: 1.573, Global test loss: 2.203, Global test accuracy: 27.27 

Round  11, Train loss: 1.481, Test loss: 1.576, Test accuracy: 91.90 

Round  11, Global train loss: 1.481, Global test loss: 2.160, Global test accuracy: 32.95 

Round  12, Train loss: 1.528, Test loss: 1.563, Test accuracy: 93.25 

Round  12, Global train loss: 1.528, Global test loss: 2.225, Global test accuracy: 21.10 

Round  13, Train loss: 1.500, Test loss: 1.552, Test accuracy: 94.03 

Round  13, Global train loss: 1.500, Global test loss: 2.212, Global test accuracy: 22.42 

Round  14, Train loss: 1.474, Test loss: 1.551, Test accuracy: 94.08 

Round  14, Global train loss: 1.474, Global test loss: 2.158, Global test accuracy: 31.18 

Round  15, Train loss: 1.491, Test loss: 1.546, Test accuracy: 94.15 

Round  15, Global train loss: 1.491, Global test loss: 2.160, Global test accuracy: 28.32 

Round  16, Train loss: 1.487, Test loss: 1.536, Test accuracy: 94.25 

Round  16, Global train loss: 1.487, Global test loss: 2.157, Global test accuracy: 25.63 

Round  17, Train loss: 1.471, Test loss: 1.535, Test accuracy: 94.28 

Round  17, Global train loss: 1.471, Global test loss: 2.112, Global test accuracy: 40.95 

Round  18, Train loss: 1.479, Test loss: 1.533, Test accuracy: 94.37 

Round  18, Global train loss: 1.479, Global test loss: 2.267, Global test accuracy: 16.05 

Round  19, Train loss: 1.474, Test loss: 1.533, Test accuracy: 94.42 

Round  19, Global train loss: 1.474, Global test loss: 2.156, Global test accuracy: 32.73 

Round  20, Train loss: 1.506, Test loss: 1.516, Test accuracy: 95.12 

Round  20, Global train loss: 1.506, Global test loss: 2.219, Global test accuracy: 20.25 

Round  21, Train loss: 1.470, Test loss: 1.516, Test accuracy: 95.17 

Round  21, Global train loss: 1.470, Global test loss: 2.166, Global test accuracy: 29.75 

Round  22, Train loss: 1.470, Test loss: 1.515, Test accuracy: 95.22 

Round  22, Global train loss: 1.470, Global test loss: 2.210, Global test accuracy: 21.53 

Round  23, Train loss: 1.469, Test loss: 1.515, Test accuracy: 95.30 

Round  23, Global train loss: 1.469, Global test loss: 2.125, Global test accuracy: 44.45 

Round  24, Train loss: 1.475, Test loss: 1.513, Test accuracy: 95.40 

Round  24, Global train loss: 1.475, Global test loss: 2.125, Global test accuracy: 35.88 

Round  25, Train loss: 1.471, Test loss: 1.512, Test accuracy: 95.37 

Round  25, Global train loss: 1.471, Global test loss: 2.151, Global test accuracy: 42.02 

Round  26, Train loss: 1.468, Test loss: 1.512, Test accuracy: 95.42 

Round  26, Global train loss: 1.468, Global test loss: 2.164, Global test accuracy: 28.67 

Round  27, Train loss: 1.466, Test loss: 1.512, Test accuracy: 95.40 

Round  27, Global train loss: 1.466, Global test loss: 2.107, Global test accuracy: 36.53 

Round  28, Train loss: 1.472, Test loss: 1.512, Test accuracy: 95.40 

Round  28, Global train loss: 1.472, Global test loss: 2.239, Global test accuracy: 16.78 

Round  29, Train loss: 1.471, Test loss: 1.511, Test accuracy: 95.50 

Round  29, Global train loss: 1.471, Global test loss: 2.221, Global test accuracy: 21.63 

Round  30, Train loss: 1.474, Test loss: 1.510, Test accuracy: 95.62 

Round  30, Global train loss: 1.474, Global test loss: 2.160, Global test accuracy: 33.12 

Round  31, Train loss: 1.467, Test loss: 1.510, Test accuracy: 95.62 

Round  31, Global train loss: 1.467, Global test loss: 2.124, Global test accuracy: 33.68 

Round  32, Train loss: 1.466, Test loss: 1.510, Test accuracy: 95.63 

Round  32, Global train loss: 1.466, Global test loss: 2.124, Global test accuracy: 37.13 

Round  33, Train loss: 1.469, Test loss: 1.509, Test accuracy: 95.75 

Round  33, Global train loss: 1.469, Global test loss: 2.235, Global test accuracy: 17.97 

Round  34, Train loss: 1.466, Test loss: 1.509, Test accuracy: 95.82 

Round  34, Global train loss: 1.466, Global test loss: 2.152, Global test accuracy: 32.75 

Round  35, Train loss: 1.470, Test loss: 1.510, Test accuracy: 95.73 

Round  35, Global train loss: 1.470, Global test loss: 2.284, Global test accuracy: 15.22 

Round  36, Train loss: 1.466, Test loss: 1.510, Test accuracy: 95.77 

Round  36, Global train loss: 1.466, Global test loss: 2.177, Global test accuracy: 25.55 

Round  37, Train loss: 1.468, Test loss: 1.509, Test accuracy: 95.73 

Round  37, Global train loss: 1.468, Global test loss: 2.284, Global test accuracy: 13.35 

Round  38, Train loss: 1.468, Test loss: 1.509, Test accuracy: 95.72 

Round  38, Global train loss: 1.468, Global test loss: 2.231, Global test accuracy: 17.92 

Round  39, Train loss: 1.467, Test loss: 1.509, Test accuracy: 95.73 

Round  39, Global train loss: 1.467, Global test loss: 2.209, Global test accuracy: 23.40 

Round  40, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.73 

Round  40, Global train loss: 1.465, Global test loss: 2.205, Global test accuracy: 20.05 

Round  41, Train loss: 1.467, Test loss: 1.509, Test accuracy: 95.78 

Round  41, Global train loss: 1.467, Global test loss: 2.188, Global test accuracy: 24.97 

Round  42, Train loss: 1.468, Test loss: 1.509, Test accuracy: 95.77 

Round  42, Global train loss: 1.468, Global test loss: 2.237, Global test accuracy: 20.80 

Round  43, Train loss: 1.467, Test loss: 1.508, Test accuracy: 95.82 

Round  43, Global train loss: 1.467, Global test loss: 2.230, Global test accuracy: 20.92 

Round  44, Train loss: 1.469, Test loss: 1.508, Test accuracy: 95.78 

Round  44, Global train loss: 1.469, Global test loss: 2.176, Global test accuracy: 27.02 

Round  45, Train loss: 1.466, Test loss: 1.508, Test accuracy: 95.78 

Round  45, Global train loss: 1.466, Global test loss: 2.140, Global test accuracy: 32.13 

Round  46, Train loss: 1.466, Test loss: 1.508, Test accuracy: 95.75 

Round  46, Global train loss: 1.466, Global test loss: 2.182, Global test accuracy: 26.30 

Round  47, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.82 

Round  47, Global train loss: 1.464, Global test loss: 2.146, Global test accuracy: 31.82 

Round  48, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.87 

Round  48, Global train loss: 1.466, Global test loss: 2.271, Global test accuracy: 14.00 

Round  49, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.87 

Round  49, Global train loss: 1.464, Global test loss: 2.181, Global test accuracy: 26.40 

Final Round, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.90 

Final Round, Global train loss: 1.466, Global test loss: 2.181, Global test accuracy: 26.40 

Average accuracy final 10 rounds: 95.79666666666668 

Average global accuracy final 10 rounds: 24.439999999999998 

372.6675181388855
[1.603868007659912, 2.2691962718963623, 2.933931350708008, 3.6008856296539307, 4.2659218311309814, 4.930827617645264, 5.622504711151123, 6.292610168457031, 6.9315385818481445, 7.615074872970581, 8.299111127853394, 8.982300043106079, 9.621215105056763, 10.260868072509766, 10.906851291656494, 11.544679164886475, 12.180970668792725, 12.819852590560913, 13.45903730392456, 14.07514762878418, 14.715867757797241, 15.361016035079956, 15.95510458946228, 16.59022045135498, 17.22299599647522, 17.859238624572754, 18.490256547927856, 19.121432542800903, 19.75407385826111, 20.38620901107788, 21.015004873275757, 21.64780020713806, 22.298375606536865, 22.93257236480713, 23.565099477767944, 24.198567628860474, 24.829133987426758, 25.46205973625183, 26.087722301483154, 26.712905406951904, 27.339550018310547, 27.9697265625, 28.607317447662354, 29.23960304260254, 29.871565580368042, 30.506739854812622, 31.144925594329834, 31.779865503311157, 32.41127419471741, 33.04644322395325, 34.31363868713379]
[21.233333333333334, 26.233333333333334, 43.4, 59.1, 63.85, 81.13333333333334, 84.23333333333333, 87.21666666666667, 89.71666666666667, 90.23333333333333, 91.8, 91.9, 93.25, 94.03333333333333, 94.08333333333333, 94.15, 94.25, 94.28333333333333, 94.36666666666666, 94.41666666666667, 95.11666666666666, 95.16666666666667, 95.21666666666667, 95.3, 95.4, 95.36666666666666, 95.41666666666667, 95.4, 95.4, 95.5, 95.61666666666666, 95.61666666666666, 95.63333333333334, 95.75, 95.81666666666666, 95.73333333333333, 95.76666666666667, 95.73333333333333, 95.71666666666667, 95.73333333333333, 95.73333333333333, 95.78333333333333, 95.76666666666667, 95.81666666666666, 95.78333333333333, 95.78333333333333, 95.75, 95.81666666666666, 95.86666666666666, 95.86666666666666, 95.9]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.289, Test loss: 2.290, Test accuracy: 24.10 

Round   0, Global train loss: 2.289, Global test loss: 2.300, Global test accuracy: 15.78 

Round   1, Train loss: 2.269, Test loss: 2.253, Test accuracy: 26.65 

Round   1, Global train loss: 2.269, Global test loss: 2.289, Global test accuracy: 13.33 

Round   2, Train loss: 2.161, Test loss: 2.162, Test accuracy: 37.52 

Round   2, Global train loss: 2.161, Global test loss: 2.267, Global test accuracy: 23.12 

Round   3, Train loss: 1.958, Test loss: 2.027, Test accuracy: 50.17 

Round   3, Global train loss: 1.958, Global test loss: 2.236, Global test accuracy: 16.48 

Round   4, Train loss: 1.812, Test loss: 1.877, Test accuracy: 69.05 

Round   4, Global train loss: 1.812, Global test loss: 2.135, Global test accuracy: 29.78 

Round   5, Train loss: 1.670, Test loss: 1.798, Test accuracy: 75.18 

Round   5, Global train loss: 1.670, Global test loss: 2.018, Global test accuracy: 51.35 

Round   6, Train loss: 1.753, Test loss: 1.718, Test accuracy: 77.52 

Round   6, Global train loss: 1.753, Global test loss: 1.975, Global test accuracy: 59.27 

Round   7, Train loss: 1.578, Test loss: 1.684, Test accuracy: 81.30 

Round   7, Global train loss: 1.578, Global test loss: 1.888, Global test accuracy: 63.40 

Round   8, Train loss: 1.544, Test loss: 1.622, Test accuracy: 86.38 

Round   8, Global train loss: 1.544, Global test loss: 1.882, Global test accuracy: 66.85 

Round   9, Train loss: 1.579, Test loss: 1.593, Test accuracy: 89.18 

Round   9, Global train loss: 1.579, Global test loss: 1.856, Global test accuracy: 62.83 

Round  10, Train loss: 1.573, Test loss: 1.594, Test accuracy: 88.85 

Round  10, Global train loss: 1.573, Global test loss: 1.816, Global test accuracy: 66.75 

Round  11, Train loss: 1.606, Test loss: 1.586, Test accuracy: 89.18 

Round  11, Global train loss: 1.606, Global test loss: 1.801, Global test accuracy: 69.60 

Round  12, Train loss: 1.510, Test loss: 1.569, Test accuracy: 90.70 

Round  12, Global train loss: 1.510, Global test loss: 1.767, Global test accuracy: 75.97 

Round  13, Train loss: 1.512, Test loss: 1.562, Test accuracy: 91.08 

Round  13, Global train loss: 1.512, Global test loss: 1.747, Global test accuracy: 73.33 

Round  14, Train loss: 1.501, Test loss: 1.542, Test accuracy: 93.03 

Round  14, Global train loss: 1.501, Global test loss: 1.746, Global test accuracy: 73.60 

Round  15, Train loss: 1.559, Test loss: 1.540, Test accuracy: 93.30 

Round  15, Global train loss: 1.559, Global test loss: 1.715, Global test accuracy: 82.07 

Round  16, Train loss: 1.488, Test loss: 1.538, Test accuracy: 93.43 

Round  16, Global train loss: 1.488, Global test loss: 1.742, Global test accuracy: 72.62 

Round  17, Train loss: 1.496, Test loss: 1.538, Test accuracy: 93.37 

Round  17, Global train loss: 1.496, Global test loss: 1.742, Global test accuracy: 73.62 

Round  18, Train loss: 1.505, Test loss: 1.536, Test accuracy: 93.55 

Round  18, Global train loss: 1.505, Global test loss: 1.738, Global test accuracy: 78.30 

Round  19, Train loss: 1.485, Test loss: 1.536, Test accuracy: 93.57 

Round  19, Global train loss: 1.485, Global test loss: 1.700, Global test accuracy: 78.17 

Round  20, Train loss: 1.545, Test loss: 1.531, Test accuracy: 93.50 

Round  20, Global train loss: 1.545, Global test loss: 1.682, Global test accuracy: 81.70 

Round  21, Train loss: 1.548, Test loss: 1.532, Test accuracy: 93.50 

Round  21, Global train loss: 1.548, Global test loss: 1.717, Global test accuracy: 77.57 

Round  22, Train loss: 1.490, Test loss: 1.531, Test accuracy: 93.53 

Round  22, Global train loss: 1.490, Global test loss: 1.691, Global test accuracy: 81.10 

Round  23, Train loss: 1.536, Test loss: 1.530, Test accuracy: 93.60 

Round  23, Global train loss: 1.536, Global test loss: 1.682, Global test accuracy: 81.05 

Round  24, Train loss: 1.551, Test loss: 1.529, Test accuracy: 93.60 

Round  24, Global train loss: 1.551, Global test loss: 1.722, Global test accuracy: 78.33 

Round  25, Train loss: 1.488, Test loss: 1.529, Test accuracy: 93.63 

Round  25, Global train loss: 1.488, Global test loss: 1.733, Global test accuracy: 74.92 

Round  26, Train loss: 1.492, Test loss: 1.528, Test accuracy: 93.75 

Round  26, Global train loss: 1.492, Global test loss: 1.712, Global test accuracy: 78.50 

Round  27, Train loss: 1.540, Test loss: 1.529, Test accuracy: 93.63 

Round  27, Global train loss: 1.540, Global test loss: 1.671, Global test accuracy: 82.98 

Round  28, Train loss: 1.537, Test loss: 1.529, Test accuracy: 93.60 

Round  28, Global train loss: 1.537, Global test loss: 1.722, Global test accuracy: 73.93 

Round  29, Train loss: 1.535, Test loss: 1.529, Test accuracy: 93.58 

Round  29, Global train loss: 1.535, Global test loss: 1.654, Global test accuracy: 84.12 

Round  30, Train loss: 1.488, Test loss: 1.529, Test accuracy: 93.55 

Round  30, Global train loss: 1.488, Global test loss: 1.656, Global test accuracy: 84.27 

Round  31, Train loss: 1.489, Test loss: 1.529, Test accuracy: 93.52 

Round  31, Global train loss: 1.489, Global test loss: 1.761, Global test accuracy: 71.97 

Round  32, Train loss: 1.540, Test loss: 1.528, Test accuracy: 93.65 

Round  32, Global train loss: 1.540, Global test loss: 1.730, Global test accuracy: 74.03 

Round  33, Train loss: 1.538, Test loss: 1.528, Test accuracy: 93.67 

Round  33, Global train loss: 1.538, Global test loss: 1.717, Global test accuracy: 74.88 

Round  34, Train loss: 1.592, Test loss: 1.529, Test accuracy: 93.50 

Round  34, Global train loss: 1.592, Global test loss: 1.668, Global test accuracy: 82.82 

Round  35, Train loss: 1.483, Test loss: 1.529, Test accuracy: 93.43 

Round  35, Global train loss: 1.483, Global test loss: 1.659, Global test accuracy: 83.45 

Round  36, Train loss: 1.536, Test loss: 1.529, Test accuracy: 93.50 

Round  36, Global train loss: 1.536, Global test loss: 1.719, Global test accuracy: 74.85 

Round  37, Train loss: 1.536, Test loss: 1.528, Test accuracy: 93.63 

Round  37, Global train loss: 1.536, Global test loss: 1.634, Global test accuracy: 85.75 

Round  38, Train loss: 1.484, Test loss: 1.526, Test accuracy: 93.92 

Round  38, Global train loss: 1.484, Global test loss: 1.663, Global test accuracy: 81.15 

Round  39, Train loss: 1.484, Test loss: 1.526, Test accuracy: 93.92 

Round  39, Global train loss: 1.484, Global test loss: 1.676, Global test accuracy: 81.43 

Round  40, Train loss: 1.529, Test loss: 1.526, Test accuracy: 93.98 

Round  40, Global train loss: 1.529, Global test loss: 1.676, Global test accuracy: 79.78 

Round  41, Train loss: 1.476, Test loss: 1.525, Test accuracy: 94.03 

Round  41, Global train loss: 1.476, Global test loss: 1.688, Global test accuracy: 78.40 

Round  42, Train loss: 1.481, Test loss: 1.525, Test accuracy: 94.00 

Round  42, Global train loss: 1.481, Global test loss: 1.663, Global test accuracy: 81.20 

Round  43, Train loss: 1.532, Test loss: 1.526, Test accuracy: 93.97 

Round  43, Global train loss: 1.532, Global test loss: 1.631, Global test accuracy: 85.58 

Round  44, Train loss: 1.531, Test loss: 1.526, Test accuracy: 93.87 

Round  44, Global train loss: 1.531, Global test loss: 1.660, Global test accuracy: 80.97 

Round  45, Train loss: 1.533, Test loss: 1.525, Test accuracy: 93.92 

Round  45, Global train loss: 1.533, Global test loss: 1.690, Global test accuracy: 78.67 

Round  46, Train loss: 1.475, Test loss: 1.525, Test accuracy: 93.83 

Round  46, Global train loss: 1.475, Global test loss: 1.642, Global test accuracy: 83.87 

Round  47, Train loss: 1.529, Test loss: 1.524, Test accuracy: 93.95 

Round  47, Global train loss: 1.529, Global test loss: 1.642, Global test accuracy: 83.38 

Round  48, Train loss: 1.530, Test loss: 1.524, Test accuracy: 93.98 

Round  48, Global train loss: 1.530, Global test loss: 1.646, Global test accuracy: 83.52 

Round  49, Train loss: 1.529, Test loss: 1.524, Test accuracy: 93.98 

Round  49, Global train loss: 1.529, Global test loss: 1.627, Global test accuracy: 85.48 

Final Round, Train loss: 1.505, Test loss: 1.523, Test accuracy: 93.97 

Final Round, Global train loss: 1.505, Global test loss: 1.627, Global test accuracy: 85.48 

Average accuracy final 10 rounds: 93.95166666666665 

Average global accuracy final 10 rounds: 82.085 

371.64142966270447
[1.5829150676727295, 2.237989664077759, 2.8812358379364014, 3.51457142829895, 4.156945705413818, 4.7961320877075195, 5.435455083847046, 6.0747411251068115, 6.713629245758057, 7.353583335876465, 7.994745969772339, 8.636387586593628, 9.27599573135376, 9.915677785873413, 10.557949781417847, 11.195756196975708, 11.834420680999756, 12.470670461654663, 13.11228060722351, 13.748557090759277, 14.385987520217896, 15.02328372001648, 15.661423206329346, 16.301353454589844, 16.94012689590454, 17.582473039627075, 18.22375988960266, 18.901712656021118, 19.562737703323364, 20.183002948760986, 20.804346561431885, 21.424548149108887, 22.044178247451782, 22.666815042495728, 23.276527404785156, 23.909085035324097, 24.525936365127563, 25.276458501815796, 26.028682708740234, 26.66422939300537, 27.33893918991089, 27.966743230819702, 28.594579219818115, 29.223127365112305, 29.85101079940796, 30.506603240966797, 31.182182550430298, 31.858068704605103, 32.48951482772827, 33.126132011413574, 34.60379695892334]
[24.1, 26.65, 37.516666666666666, 50.166666666666664, 69.05, 75.18333333333334, 77.51666666666667, 81.3, 86.38333333333334, 89.18333333333334, 88.85, 89.18333333333334, 90.7, 91.08333333333333, 93.03333333333333, 93.3, 93.43333333333334, 93.36666666666666, 93.55, 93.56666666666666, 93.5, 93.5, 93.53333333333333, 93.6, 93.6, 93.63333333333334, 93.75, 93.63333333333334, 93.6, 93.58333333333333, 93.55, 93.51666666666667, 93.65, 93.66666666666667, 93.5, 93.43333333333334, 93.5, 93.63333333333334, 93.91666666666667, 93.91666666666667, 93.98333333333333, 94.03333333333333, 94.0, 93.96666666666667, 93.86666666666666, 93.91666666666667, 93.83333333333333, 93.95, 93.98333333333333, 93.98333333333333, 93.96666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.30 

Round   1, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.75 

Round   2, Train loss: 2.301, Test loss: 2.300, Test accuracy: 11.28 

Round   3, Train loss: 2.300, Test loss: 2.299, Test accuracy: 12.85 

Round   4, Train loss: 2.299, Test loss: 2.298, Test accuracy: 15.88 

Round   5, Train loss: 2.298, Test loss: 2.297, Test accuracy: 21.72 

Round   6, Train loss: 2.296, Test loss: 2.295, Test accuracy: 30.05 

Round   7, Train loss: 2.295, Test loss: 2.293, Test accuracy: 35.65 

Round   8, Train loss: 2.293, Test loss: 2.291, Test accuracy: 40.12 

Round   9, Train loss: 2.289, Test loss: 2.288, Test accuracy: 43.20 

Round  10, Train loss: 2.286, Test loss: 2.283, Test accuracy: 43.42 

Round  11, Train loss: 2.280, Test loss: 2.276, Test accuracy: 42.28 

Round  12, Train loss: 2.269, Test loss: 2.262, Test accuracy: 36.75 

Round  13, Train loss: 2.253, Test loss: 2.239, Test accuracy: 34.20 

Round  14, Train loss: 2.223, Test loss: 2.211, Test accuracy: 37.48 

Round  15, Train loss: 2.189, Test loss: 2.176, Test accuracy: 40.30 

Round  16, Train loss: 2.143, Test loss: 2.121, Test accuracy: 41.07 

Round  17, Train loss: 2.090, Test loss: 2.073, Test accuracy: 44.55 

Round  18, Train loss: 2.056, Test loss: 2.026, Test accuracy: 50.52 

Round  19, Train loss: 2.003, Test loss: 1.974, Test accuracy: 56.97 

Round  20, Train loss: 1.938, Test loss: 1.930, Test accuracy: 59.45 

Round  21, Train loss: 1.894, Test loss: 1.887, Test accuracy: 63.28 

Round  22, Train loss: 1.847, Test loss: 1.858, Test accuracy: 66.18 

Round  23, Train loss: 1.811, Test loss: 1.823, Test accuracy: 70.07 

Round  24, Train loss: 1.785, Test loss: 1.792, Test accuracy: 72.33 

Round  25, Train loss: 1.774, Test loss: 1.764, Test accuracy: 75.18 

Round  26, Train loss: 1.745, Test loss: 1.744, Test accuracy: 76.72 

Round  27, Train loss: 1.709, Test loss: 1.724, Test accuracy: 78.37 

Round  28, Train loss: 1.694, Test loss: 1.710, Test accuracy: 79.00 

Round  29, Train loss: 1.686, Test loss: 1.698, Test accuracy: 80.43 

Round  30, Train loss: 1.664, Test loss: 1.693, Test accuracy: 80.27 

Round  31, Train loss: 1.653, Test loss: 1.671, Test accuracy: 82.48 

Round  32, Train loss: 1.639, Test loss: 1.652, Test accuracy: 83.95 

Round  33, Train loss: 1.616, Test loss: 1.643, Test accuracy: 84.78 

Round  34, Train loss: 1.611, Test loss: 1.632, Test accuracy: 85.47 

Round  35, Train loss: 1.602, Test loss: 1.626, Test accuracy: 85.95 

Round  36, Train loss: 1.596, Test loss: 1.621, Test accuracy: 86.42 

Round  37, Train loss: 1.592, Test loss: 1.620, Test accuracy: 86.20 

Round  38, Train loss: 1.576, Test loss: 1.615, Test accuracy: 86.62 

Round  39, Train loss: 1.585, Test loss: 1.611, Test accuracy: 87.13 

Round  40, Train loss: 1.575, Test loss: 1.608, Test accuracy: 87.13 

Round  41, Train loss: 1.564, Test loss: 1.604, Test accuracy: 87.62 

Round  42, Train loss: 1.570, Test loss: 1.601, Test accuracy: 87.78 

Round  43, Train loss: 1.570, Test loss: 1.598, Test accuracy: 88.05 

Round  44, Train loss: 1.567, Test loss: 1.600, Test accuracy: 87.47 

Round  45, Train loss: 1.565, Test loss: 1.600, Test accuracy: 87.23 

Round  46, Train loss: 1.555, Test loss: 1.597, Test accuracy: 87.57 

Round  47, Train loss: 1.548, Test loss: 1.597, Test accuracy: 87.48 

Round  48, Train loss: 1.567, Test loss: 1.589, Test accuracy: 88.40 

Round  49, Train loss: 1.551, Test loss: 1.588, Test accuracy: 88.67 

Final Round, Train loss: 1.553, Test loss: 1.585, Test accuracy: 88.47 

Average accuracy final 10 rounds: 87.74000000000001 

262.82090187072754
[1.5312886238098145, 2.1024813652038574, 2.6599159240722656, 3.252690553665161, 3.81182861328125, 4.308987140655518, 4.897608280181885, 5.49928092956543, 6.093276739120483, 6.689388036727905, 7.261941432952881, 7.824102401733398, 8.373975992202759, 8.927078247070312, 9.478189706802368, 10.037511110305786, 10.599047899246216, 11.149940013885498, 11.709893941879272, 12.271133422851562, 12.831196308135986, 13.385505437850952, 13.927937269210815, 14.477359771728516, 15.02661418914795, 15.577714681625366, 16.131778955459595, 16.682594537734985, 17.239014387130737, 17.781272411346436, 18.33900547027588, 18.89577555656433, 19.44439673423767, 19.997875213623047, 20.540615558624268, 21.092339277267456, 21.64637017250061, 22.19501280784607, 22.748521089553833, 23.31535577774048, 23.875105142593384, 24.426904439926147, 24.987130403518677, 25.54611039161682, 26.097545862197876, 26.65946078300476, 27.21379804611206, 27.772143840789795, 28.328120708465576, 28.876829385757446, 29.88669753074646]
[9.3, 9.75, 11.283333333333333, 12.85, 15.883333333333333, 21.716666666666665, 30.05, 35.65, 40.11666666666667, 43.2, 43.416666666666664, 42.28333333333333, 36.75, 34.2, 37.483333333333334, 40.3, 41.06666666666667, 44.55, 50.516666666666666, 56.96666666666667, 59.45, 63.28333333333333, 66.18333333333334, 70.06666666666666, 72.33333333333333, 75.18333333333334, 76.71666666666667, 78.36666666666666, 79.0, 80.43333333333334, 80.26666666666667, 82.48333333333333, 83.95, 84.78333333333333, 85.46666666666667, 85.95, 86.41666666666667, 86.2, 86.61666666666666, 87.13333333333334, 87.13333333333334, 87.61666666666666, 87.78333333333333, 88.05, 87.46666666666667, 87.23333333333333, 87.56666666666666, 87.48333333333333, 88.4, 88.66666666666667, 88.46666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.322, Test loss: 2.302, Test accuracy: 8.47
Round   1, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.30
Round   2, Train loss: 2.301, Test loss: 2.300, Test accuracy: 14.50
Round   3, Train loss: 2.300, Test loss: 2.299, Test accuracy: 18.33
Round   4, Train loss: 2.298, Test loss: 2.297, Test accuracy: 23.63
Round   5, Train loss: 2.297, Test loss: 2.296, Test accuracy: 30.77
Round   6, Train loss: 2.295, Test loss: 2.294, Test accuracy: 34.97
Round   7, Train loss: 2.293, Test loss: 2.291, Test accuracy: 37.62
Round   8, Train loss: 2.291, Test loss: 2.288, Test accuracy: 39.52
Round   9, Train loss: 2.286, Test loss: 2.283, Test accuracy: 39.17
Round  10, Train loss: 2.281, Test loss: 2.275, Test accuracy: 36.87
Round  11, Train loss: 2.270, Test loss: 2.261, Test accuracy: 32.23
Round  12, Train loss: 2.254, Test loss: 2.240, Test accuracy: 34.83
Round  13, Train loss: 2.228, Test loss: 2.220, Test accuracy: 42.82
Round  14, Train loss: 2.206, Test loss: 2.197, Test accuracy: 47.27
Round  15, Train loss: 2.189, Test loss: 2.162, Test accuracy: 48.22
Round  16, Train loss: 2.136, Test loss: 2.108, Test accuracy: 48.97
Round  17, Train loss: 2.084, Test loss: 2.046, Test accuracy: 50.43
Round  18, Train loss: 2.016, Test loss: 1.994, Test accuracy: 52.22
Round  19, Train loss: 2.000, Test loss: 1.957, Test accuracy: 58.02
Round  20, Train loss: 1.951, Test loss: 1.924, Test accuracy: 60.25
Round  21, Train loss: 1.921, Test loss: 1.899, Test accuracy: 62.60
Round  22, Train loss: 1.879, Test loss: 1.876, Test accuracy: 65.55
Round  23, Train loss: 1.855, Test loss: 1.851, Test accuracy: 67.63
Round  24, Train loss: 1.837, Test loss: 1.828, Test accuracy: 70.00
Round  25, Train loss: 1.824, Test loss: 1.804, Test accuracy: 73.30
Round  26, Train loss: 1.796, Test loss: 1.780, Test accuracy: 75.03
Round  27, Train loss: 1.780, Test loss: 1.763, Test accuracy: 77.12
Round  28, Train loss: 1.758, Test loss: 1.745, Test accuracy: 79.12
Round  29, Train loss: 1.733, Test loss: 1.729, Test accuracy: 80.38
Round  30, Train loss: 1.712, Test loss: 1.718, Test accuracy: 81.67
Round  31, Train loss: 1.752, Test loss: 1.694, Test accuracy: 83.43
Round  32, Train loss: 1.722, Test loss: 1.680, Test accuracy: 84.98
Round  33, Train loss: 1.697, Test loss: 1.668, Test accuracy: 86.20
Round  34, Train loss: 1.692, Test loss: 1.656, Test accuracy: 87.20
Round  35, Train loss: 1.665, Test loss: 1.646, Test accuracy: 87.95
Round  36, Train loss: 1.658, Test loss: 1.639, Test accuracy: 88.48
Round  37, Train loss: 1.647, Test loss: 1.630, Test accuracy: 89.12
Round  38, Train loss: 1.647, Test loss: 1.623, Test accuracy: 89.60
Round  39, Train loss: 1.631, Test loss: 1.616, Test accuracy: 90.13
Round  40, Train loss: 1.617, Test loss: 1.612, Test accuracy: 90.43
Round  41, Train loss: 1.610, Test loss: 1.608, Test accuracy: 90.52
Round  42, Train loss: 1.616, Test loss: 1.603, Test accuracy: 90.85
Round  43, Train loss: 1.624, Test loss: 1.599, Test accuracy: 90.82
Round  44, Train loss: 1.617, Test loss: 1.596, Test accuracy: 91.07
Round  45, Train loss: 1.604, Test loss: 1.593, Test accuracy: 91.28
Round  46, Train loss: 1.600, Test loss: 1.590, Test accuracy: 91.35
Round  47, Train loss: 1.596, Test loss: 1.588, Test accuracy: 91.43
Round  48, Train loss: 1.597, Test loss: 1.585, Test accuracy: 91.58
Round  49, Train loss: 1.588, Test loss: 1.582, Test accuracy: 91.68
Final Round, Train loss: 1.564, Test loss: 1.574, Test accuracy: 91.72
Average accuracy final 10 rounds: 91.10166666666667
304.3488645553589
[1.6619236469268799, 2.3789877891540527, 3.0910093784332275, 3.80318284034729, 4.510636806488037, 5.224608421325684, 5.931727409362793, 6.645653247833252, 7.35951828956604, 8.06669569015503, 8.664554119110107, 9.264504432678223, 9.864265441894531, 10.454892873764038, 11.051131248474121, 11.643348455429077, 12.345388412475586, 13.046947240829468, 13.750402688980103, 14.448437929153442, 15.157074451446533, 15.858413696289062, 16.562646627426147, 17.264450788497925, 17.964234352111816, 18.668786764144897, 19.368946313858032, 20.066471099853516, 20.760422229766846, 21.45714044570923, 22.151429414749146, 22.848133087158203, 23.553038120269775, 24.24386739730835, 24.951327323913574, 25.65198254585266, 26.35328507423401, 27.057125091552734, 27.75840473175049, 28.4628324508667, 29.162314891815186, 29.861660480499268, 30.559775829315186, 31.258390426635742, 31.956681966781616, 32.65849161148071, 33.35679030418396, 34.05486297607422, 34.75340390205383, 35.456035137176514, 36.498963594436646]
[8.466666666666667, 11.3, 14.5, 18.333333333333332, 23.633333333333333, 30.766666666666666, 34.96666666666667, 37.61666666666667, 39.516666666666666, 39.166666666666664, 36.86666666666667, 32.233333333333334, 34.833333333333336, 42.81666666666667, 47.266666666666666, 48.21666666666667, 48.96666666666667, 50.43333333333333, 52.21666666666667, 58.016666666666666, 60.25, 62.6, 65.55, 67.63333333333334, 70.0, 73.3, 75.03333333333333, 77.11666666666666, 79.11666666666666, 80.38333333333334, 81.66666666666667, 83.43333333333334, 84.98333333333333, 86.2, 87.2, 87.95, 88.48333333333333, 89.11666666666666, 89.6, 90.13333333333334, 90.43333333333334, 90.51666666666667, 90.85, 90.81666666666666, 91.06666666666666, 91.28333333333333, 91.35, 91.43333333333334, 91.58333333333333, 91.68333333333334, 91.71666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.300, Test accuracy: 18.82
Round   1, Train loss: 2.300, Test loss: 2.298, Test accuracy: 20.50
Round   2, Train loss: 2.298, Test loss: 2.294, Test accuracy: 20.78
Round   3, Train loss: 2.295, Test loss: 2.289, Test accuracy: 20.32
Round   4, Train loss: 2.288, Test loss: 2.276, Test accuracy: 23.60
Round   5, Train loss: 2.279, Test loss: 2.242, Test accuracy: 37.60
Round   6, Train loss: 2.245, Test loss: 2.098, Test accuracy: 40.37
Round   7, Train loss: 2.176, Test loss: 2.023, Test accuracy: 42.93
Round   8, Train loss: 2.069, Test loss: 1.932, Test accuracy: 57.60
Round   9, Train loss: 1.964, Test loss: 1.855, Test accuracy: 63.63
Round  10, Train loss: 1.914, Test loss: 1.797, Test accuracy: 70.32
Round  11, Train loss: 1.931, Test loss: 1.726, Test accuracy: 79.18
Round  12, Train loss: 1.809, Test loss: 1.685, Test accuracy: 81.72
Round  13, Train loss: 1.749, Test loss: 1.670, Test accuracy: 81.92
Round  14, Train loss: 1.704, Test loss: 1.654, Test accuracy: 83.07
Round  15, Train loss: 1.750, Test loss: 1.646, Test accuracy: 83.55
Round  16, Train loss: 1.673, Test loss: 1.639, Test accuracy: 83.90
Round  17, Train loss: 1.647, Test loss: 1.636, Test accuracy: 83.82
Round  18, Train loss: 1.655, Test loss: 1.632, Test accuracy: 84.15
Round  19, Train loss: 1.639, Test loss: 1.629, Test accuracy: 84.18
Round  20, Train loss: 1.629, Test loss: 1.627, Test accuracy: 84.53
Round  21, Train loss: 1.626, Test loss: 1.623, Test accuracy: 84.67
Round  22, Train loss: 1.610, Test loss: 1.622, Test accuracy: 84.60
Round  23, Train loss: 1.600, Test loss: 1.621, Test accuracy: 84.80
Round  24, Train loss: 1.612, Test loss: 1.620, Test accuracy: 84.83
Round  25, Train loss: 1.598, Test loss: 1.620, Test accuracy: 84.72
Round  26, Train loss: 1.611, Test loss: 1.619, Test accuracy: 84.83
Round  27, Train loss: 1.589, Test loss: 1.619, Test accuracy: 84.78
Round  28, Train loss: 1.608, Test loss: 1.618, Test accuracy: 84.73
Round  29, Train loss: 1.600, Test loss: 1.615, Test accuracy: 85.10
Round  30, Train loss: 1.597, Test loss: 1.616, Test accuracy: 85.12
Round  31, Train loss: 1.596, Test loss: 1.614, Test accuracy: 85.03
Round  32, Train loss: 1.594, Test loss: 1.612, Test accuracy: 84.92
Round  33, Train loss: 1.599, Test loss: 1.613, Test accuracy: 85.25
Round  34, Train loss: 1.587, Test loss: 1.612, Test accuracy: 85.35
Round  35, Train loss: 1.590, Test loss: 1.612, Test accuracy: 85.25
Round  36, Train loss: 1.592, Test loss: 1.612, Test accuracy: 85.32
Round  37, Train loss: 1.583, Test loss: 1.611, Test accuracy: 85.67
Round  38, Train loss: 1.590, Test loss: 1.610, Test accuracy: 85.38
Round  39, Train loss: 1.582, Test loss: 1.609, Test accuracy: 85.47
Round  40, Train loss: 1.570, Test loss: 1.610, Test accuracy: 85.65
Round  41, Train loss: 1.573, Test loss: 1.609, Test accuracy: 85.43
Round  42, Train loss: 1.589, Test loss: 1.610, Test accuracy: 85.35
Round  43, Train loss: 1.590, Test loss: 1.610, Test accuracy: 85.48
Round  44, Train loss: 1.587, Test loss: 1.609, Test accuracy: 85.52
Round  45, Train loss: 1.574, Test loss: 1.608, Test accuracy: 85.60
Round  46, Train loss: 1.575, Test loss: 1.608, Test accuracy: 85.53
Round  47, Train loss: 1.582, Test loss: 1.609, Test accuracy: 85.37
Round  48, Train loss: 1.587, Test loss: 1.607, Test accuracy: 85.83
Round  49, Train loss: 1.600, Test loss: 1.606, Test accuracy: 85.80
Final Round, Train loss: 1.580, Test loss: 1.606, Test accuracy: 85.82
Average accuracy final 10 rounds: 85.55666666666667
596.8772850036621
[2.6101715564727783, 4.387213468551636, 6.0267791748046875, 7.664314031600952, 9.313591480255127, 10.958561897277832, 12.60733962059021, 14.25121021270752, 15.892230033874512, 17.53675103187561, 19.17565608024597, 20.82260799407959, 22.501245498657227, 24.137717962265015, 25.780566930770874, 27.430378913879395, 29.07660174369812, 30.72156071662903, 32.36049008369446, 34.12544894218445, 35.89172959327698, 37.66239809989929, 39.43092918395996, 41.077247619628906, 42.74044728279114, 44.38342475891113, 46.01370811462402, 47.641193866729736, 49.25872087478638, 50.889230251312256, 52.5071177482605, 54.14573812484741, 55.77700662612915, 57.406458616256714, 59.03637099266052, 60.66912770271301, 62.30994749069214, 63.91646432876587, 65.49404692649841, 67.14208483695984, 68.78853869438171, 70.20279693603516, 71.83690190315247, 73.4931914806366, 75.12700319290161, 76.75669836997986, 78.37861442565918, 80.0097770690918, 81.63284611701965, 83.26894330978394, 84.9016797542572]
[18.816666666666666, 20.5, 20.783333333333335, 20.316666666666666, 23.6, 37.6, 40.36666666666667, 42.93333333333333, 57.6, 63.63333333333333, 70.31666666666666, 79.18333333333334, 81.71666666666667, 81.91666666666667, 83.06666666666666, 83.55, 83.9, 83.81666666666666, 84.15, 84.18333333333334, 84.53333333333333, 84.66666666666667, 84.6, 84.8, 84.83333333333333, 84.71666666666667, 84.83333333333333, 84.78333333333333, 84.73333333333333, 85.1, 85.11666666666666, 85.03333333333333, 84.91666666666667, 85.25, 85.35, 85.25, 85.31666666666666, 85.66666666666667, 85.38333333333334, 85.46666666666667, 85.65, 85.43333333333334, 85.35, 85.48333333333333, 85.51666666666667, 85.6, 85.53333333333333, 85.36666666666666, 85.83333333333333, 85.8, 85.81666666666666]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.721, Test loss: 2.300, Test accuracy: 15.78
Round   1, Train loss: 1.704, Test loss: 2.295, Test accuracy: 36.57
Round   2, Train loss: 1.645, Test loss: 2.283, Test accuracy: 38.47
Round   3, Train loss: 1.585, Test loss: 2.263, Test accuracy: 42.20
Round   4, Train loss: 1.498, Test loss: 2.239, Test accuracy: 47.92
Round   5, Train loss: 1.460, Test loss: 2.217, Test accuracy: 53.28
Round   6, Train loss: 1.394, Test loss: 2.192, Test accuracy: 55.90
Round   7, Train loss: 1.370, Test loss: 2.172, Test accuracy: 57.53
Round   8, Train loss: 1.331, Test loss: 2.152, Test accuracy: 60.37
Round   9, Train loss: 1.303, Test loss: 2.137, Test accuracy: 63.08
Round  10, Train loss: 1.281, Test loss: 2.123, Test accuracy: 65.05
Round  11, Train loss: 1.265, Test loss: 2.111, Test accuracy: 66.37
Round  12, Train loss: 1.275, Test loss: 2.100, Test accuracy: 68.95
Round  13, Train loss: 1.249, Test loss: 2.096, Test accuracy: 69.55
Round  14, Train loss: 1.254, Test loss: 2.092, Test accuracy: 69.05
Round  15, Train loss: 1.248, Test loss: 2.084, Test accuracy: 69.42
Round  16, Train loss: 1.238, Test loss: 2.083, Test accuracy: 68.53
Round  17, Train loss: 1.227, Test loss: 2.082, Test accuracy: 68.08
Round  18, Train loss: 1.218, Test loss: 2.083, Test accuracy: 68.03
Round  19, Train loss: 1.200, Test loss: 2.072, Test accuracy: 71.00
Round  20, Train loss: 1.194, Test loss: 2.065, Test accuracy: 74.07
Round  21, Train loss: 1.188, Test loss: 2.064, Test accuracy: 75.10
Round  22, Train loss: 1.176, Test loss: 2.060, Test accuracy: 75.53
Round  23, Train loss: 1.169, Test loss: 2.060, Test accuracy: 76.28
Round  24, Train loss: 1.154, Test loss: 2.055, Test accuracy: 77.12
Round  25, Train loss: 1.134, Test loss: 2.048, Test accuracy: 77.33
Round  26, Train loss: 1.138, Test loss: 2.044, Test accuracy: 77.92
Round  27, Train loss: 1.130, Test loss: 2.035, Test accuracy: 79.10
Round  28, Train loss: 1.130, Test loss: 2.032, Test accuracy: 79.58
Round  29, Train loss: 1.126, Test loss: 2.029, Test accuracy: 79.48
Round  30, Train loss: 1.123, Test loss: 2.025, Test accuracy: 79.25
Round  31, Train loss: 1.120, Test loss: 2.022, Test accuracy: 79.25
Round  32, Train loss: 1.119, Test loss: 2.019, Test accuracy: 79.18
Round  33, Train loss: 1.122, Test loss: 2.017, Test accuracy: 79.20
Round  34, Train loss: 1.119, Test loss: 2.016, Test accuracy: 79.05
Round  35, Train loss: 1.117, Test loss: 2.014, Test accuracy: 78.67
Round  36, Train loss: 1.115, Test loss: 2.012, Test accuracy: 78.57
Round  37, Train loss: 1.116, Test loss: 2.010, Test accuracy: 78.53
Round  38, Train loss: 1.115, Test loss: 2.010, Test accuracy: 78.03
Round  39, Train loss: 1.115, Test loss: 2.009, Test accuracy: 77.90
Round  40, Train loss: 1.115, Test loss: 2.008, Test accuracy: 77.60
Round  41, Train loss: 1.115, Test loss: 2.006, Test accuracy: 77.38
Round  42, Train loss: 1.114, Test loss: 2.006, Test accuracy: 77.23
Round  43, Train loss: 1.114, Test loss: 2.004, Test accuracy: 76.75
Round  44, Train loss: 1.116, Test loss: 2.003, Test accuracy: 76.83
Round  45, Train loss: 1.110, Test loss: 2.003, Test accuracy: 76.10
Round  46, Train loss: 1.114, Test loss: 2.002, Test accuracy: 75.78
Round  47, Train loss: 1.111, Test loss: 2.002, Test accuracy: 75.32
Round  48, Train loss: 1.113, Test loss: 2.001, Test accuracy: 75.17
Round  49, Train loss: 1.113, Test loss: 2.000, Test accuracy: 74.95
Final Round, Train loss: 1.113, Test loss: 2.000, Test accuracy: 74.12
Average accuracy final 10 rounds: 76.31166666666667
513.0545074939728
[]
[15.783333333333333, 36.56666666666667, 38.46666666666667, 42.2, 47.916666666666664, 53.28333333333333, 55.9, 57.53333333333333, 60.36666666666667, 63.083333333333336, 65.05, 66.36666666666666, 68.95, 69.55, 69.05, 69.41666666666667, 68.53333333333333, 68.08333333333333, 68.03333333333333, 71.0, 74.06666666666666, 75.1, 75.53333333333333, 76.28333333333333, 77.11666666666666, 77.33333333333333, 77.91666666666667, 79.1, 79.58333333333333, 79.48333333333333, 79.25, 79.25, 79.18333333333334, 79.2, 79.05, 78.66666666666667, 78.56666666666666, 78.53333333333333, 78.03333333333333, 77.9, 77.6, 77.38333333333334, 77.23333333333333, 76.75, 76.83333333333333, 76.1, 75.78333333333333, 75.31666666666666, 75.16666666666667, 74.95, 74.11666666666666]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.35
Round   0: Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 9.13
Round   1, Train loss: 2.292, Test loss: 2.302, Test accuracy: 9.38
Round   1: Global train loss: 2.292, Global test loss: 2.303, Global test accuracy: 9.13
Round   2, Train loss: 2.296, Test loss: 2.302, Test accuracy: 9.98
Round   2: Global train loss: 2.296, Global test loss: 2.303, Global test accuracy: 9.15
Round   3, Train loss: 2.291, Test loss: 2.302, Test accuracy: 9.38
Round   3: Global train loss: 2.291, Global test loss: 2.303, Global test accuracy: 9.15
Round   4, Train loss: 2.304, Test loss: 2.302, Test accuracy: 9.25
Round   4: Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 9.15
Round   5, Train loss: 2.303, Test loss: 2.300, Test accuracy: 11.72
Round   5: Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 9.17
Round   6, Train loss: 2.297, Test loss: 2.300, Test accuracy: 13.60
Round   6: Global train loss: 2.297, Global test loss: 2.302, Global test accuracy: 9.20
Round   7, Train loss: 2.296, Test loss: 2.300, Test accuracy: 14.10
Round   7: Global train loss: 2.296, Global test loss: 2.302, Global test accuracy: 9.20
Round   8, Train loss: 2.295, Test loss: 2.300, Test accuracy: 13.38
Round   8: Global train loss: 2.295, Global test loss: 2.302, Global test accuracy: 9.20
Round   9, Train loss: 2.303, Test loss: 2.299, Test accuracy: 15.35
Round   9: Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 9.20
Round  10, Train loss: 2.291, Test loss: 2.299, Test accuracy: 14.50
Round  10: Global train loss: 2.291, Global test loss: 2.301, Global test accuracy: 9.27
Round  11, Train loss: 2.278, Test loss: 2.297, Test accuracy: 16.73
Round  11: Global train loss: 2.278, Global test loss: 2.301, Global test accuracy: 9.35
Round  12, Train loss: 2.306, Test loss: 2.298, Test accuracy: 15.40
Round  12: Global train loss: 2.306, Global test loss: 2.301, Global test accuracy: 9.35
Round  13, Train loss: 2.312, Test loss: 2.298, Test accuracy: 15.52
Round  13: Global train loss: 2.312, Global test loss: 2.300, Global test accuracy: 9.30
Round  14, Train loss: 2.282, Test loss: 2.296, Test accuracy: 17.33
Round  14: Global train loss: 2.282, Global test loss: 2.300, Global test accuracy: 9.35
Round  15, Train loss: 2.299, Test loss: 2.296, Test accuracy: 16.35
Round  15: Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 9.32
Round  16, Train loss: 2.310, Test loss: 2.297, Test accuracy: 15.58
Round  16: Global train loss: 2.310, Global test loss: 2.300, Global test accuracy: 9.32
Round  17, Train loss: 2.280, Test loss: 2.295, Test accuracy: 15.47
Round  17: Global train loss: 2.280, Global test loss: 2.300, Global test accuracy: 9.37
Round  18, Train loss: 2.250, Test loss: 2.291, Test accuracy: 17.55
Round  18: Global train loss: 2.250, Global test loss: 2.299, Global test accuracy: 9.50
Round  19, Train loss: 2.299, Test loss: 2.292, Test accuracy: 16.75
Round  19: Global train loss: 2.299, Global test loss: 2.299, Global test accuracy: 9.55
Round  20, Train loss: 2.277, Test loss: 2.292, Test accuracy: 17.95
Round  20: Global train loss: 2.277, Global test loss: 2.299, Global test accuracy: 9.58
Round  21, Train loss: 2.255, Test loss: 2.288, Test accuracy: 20.70
Round  21: Global train loss: 2.255, Global test loss: 2.299, Global test accuracy: 10.20
Round  22, Train loss: 2.180, Test loss: 2.275, Test accuracy: 20.60
Round  22: Global train loss: 2.180, Global test loss: 2.298, Global test accuracy: 12.63
Round  23, Train loss: 2.225, Test loss: 2.277, Test accuracy: 22.47
Round  23: Global train loss: 2.225, Global test loss: 2.297, Global test accuracy: 14.33
Round  24, Train loss: 2.236, Test loss: 2.274, Test accuracy: 22.22
Round  24: Global train loss: 2.236, Global test loss: 2.297, Global test accuracy: 17.07
Round  25, Train loss: 2.233, Test loss: 2.275, Test accuracy: 20.07
Round  25: Global train loss: 2.233, Global test loss: 2.297, Global test accuracy: 18.13
Round  26, Train loss: 2.250, Test loss: 2.279, Test accuracy: 18.55
Round  26: Global train loss: 2.250, Global test loss: 2.297, Global test accuracy: 15.65
Round  27, Train loss: 2.071, Test loss: 2.265, Test accuracy: 19.30
Round  27: Global train loss: 2.071, Global test loss: 2.296, Global test accuracy: 18.87
Round  28, Train loss: 1.894, Test loss: 2.240, Test accuracy: 21.27
Round  28: Global train loss: 1.894, Global test loss: 2.295, Global test accuracy: 24.38
Round  29, Train loss: 2.130, Test loss: 2.231, Test accuracy: 25.12
Round  29: Global train loss: 2.130, Global test loss: 2.293, Global test accuracy: 29.48
Round  30, Train loss: 2.200, Test loss: 2.238, Test accuracy: 25.55
Round  30: Global train loss: 2.200, Global test loss: 2.294, Global test accuracy: 27.57
Round  31, Train loss: 1.950, Test loss: 2.213, Test accuracy: 27.73
Round  31: Global train loss: 1.950, Global test loss: 2.291, Global test accuracy: 33.23
Round  32, Train loss: 2.231, Test loss: 2.225, Test accuracy: 25.07
Round  32: Global train loss: 2.231, Global test loss: 2.291, Global test accuracy: 32.73
Round  33, Train loss: 1.899, Test loss: 2.208, Test accuracy: 25.60
Round  33: Global train loss: 1.899, Global test loss: 2.289, Global test accuracy: 33.60
Round  34, Train loss: 1.504, Test loss: 2.175, Test accuracy: 28.50
Round  34: Global train loss: 1.504, Global test loss: 2.284, Global test accuracy: 37.92
Round  35, Train loss: 1.895, Test loss: 2.183, Test accuracy: 27.43
Round  35: Global train loss: 1.895, Global test loss: 2.283, Global test accuracy: 37.38
Round  36, Train loss: 1.917, Test loss: 2.173, Test accuracy: 27.87
Round  36: Global train loss: 1.917, Global test loss: 2.280, Global test accuracy: 38.95
Round  37, Train loss: 1.910, Test loss: 2.169, Test accuracy: 30.12
Round  37: Global train loss: 1.910, Global test loss: 2.279, Global test accuracy: 38.72
Round  38, Train loss: 1.297, Test loss: 2.155, Test accuracy: 30.87
Round  38: Global train loss: 1.297, Global test loss: 2.274, Global test accuracy: 39.90
Round  39, Train loss: 1.946, Test loss: 2.166, Test accuracy: 28.90
Round  39: Global train loss: 1.946, Global test loss: 2.272, Global test accuracy: 39.57
Round  40, Train loss: 1.632, Test loss: 2.188, Test accuracy: 26.40
Round  40: Global train loss: 1.632, Global test loss: 2.275, Global test accuracy: 38.28
Round  41, Train loss: 1.288, Test loss: 2.143, Test accuracy: 31.97
Round  41: Global train loss: 1.288, Global test loss: 2.266, Global test accuracy: 41.45
Round  42, Train loss: 1.768, Test loss: 2.145, Test accuracy: 30.73
Round  42: Global train loss: 1.768, Global test loss: 2.266, Global test accuracy: 41.27
Round  43, Train loss: 2.179, Test loss: 2.179, Test accuracy: 25.58
Round  43: Global train loss: 2.179, Global test loss: 2.271, Global test accuracy: 39.47
Round  44, Train loss: 1.685, Test loss: 2.141, Test accuracy: 29.68
Round  44: Global train loss: 1.685, Global test loss: 2.265, Global test accuracy: 40.18
Round  45, Train loss: 1.060, Test loss: 2.105, Test accuracy: 33.23
Round  45: Global train loss: 1.060, Global test loss: 2.254, Global test accuracy: 42.65
Round  46, Train loss: 1.469, Test loss: 2.101, Test accuracy: 33.73
Round  46: Global train loss: 1.469, Global test loss: 2.248, Global test accuracy: 42.97
Round  47, Train loss: 1.434, Test loss: 2.118, Test accuracy: 32.77
Round  47: Global train loss: 1.434, Global test loss: 2.243, Global test accuracy: 42.37
Round  48, Train loss: 1.154, Test loss: 2.122, Test accuracy: 32.23
Round  48: Global train loss: 1.154, Global test loss: 2.235, Global test accuracy: 41.63
Round  49, Train loss: 1.461, Test loss: 2.136, Test accuracy: 31.73
Round  49: Global train loss: 1.461, Global test loss: 2.236, Global test accuracy: 41.05
Final Round: Train loss: 2.094, Test loss: 1.991, Test accuracy: 50.33
Final Round: Global train loss: 2.094, Global test loss: 2.217, Global test accuracy: 42.92
Average accuracy final 10 rounds: 30.80666666666667
Average global accuracy final 10 rounds: 41.13166666666666
397.2388770580292
[]
[9.35, 9.383333333333333, 9.983333333333333, 9.383333333333333, 9.25, 11.716666666666667, 13.6, 14.1, 13.383333333333333, 15.35, 14.5, 16.733333333333334, 15.4, 15.516666666666667, 17.333333333333332, 16.35, 15.583333333333334, 15.466666666666667, 17.55, 16.75, 17.95, 20.7, 20.6, 22.466666666666665, 22.216666666666665, 20.066666666666666, 18.55, 19.3, 21.266666666666666, 25.116666666666667, 25.55, 27.733333333333334, 25.066666666666666, 25.6, 28.5, 27.433333333333334, 27.866666666666667, 30.116666666666667, 30.866666666666667, 28.9, 26.4, 31.966666666666665, 30.733333333333334, 25.583333333333332, 29.683333333333334, 33.233333333333334, 33.733333333333334, 32.766666666666666, 32.233333333333334, 31.733333333333334, 50.333333333333336]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.02 

Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.02 

Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.02 

Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.02 

Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.02 

Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.02 

Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.02 

Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.02 

Round   4, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.02 

Round   4, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.02 

Round   5, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.02 

Round   5, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.02 

Round   6, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.02 

Round   6, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.02 

Round   7, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.02 

Round   7, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.02 

Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.02 

Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.02 

Round   9, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.02 

Round   9, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.02 

Round  10, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.02 

Round  10, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.02 

Round  11, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.02 

Round  11, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.02 

Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.02 

Round  12, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.02 

Round  13, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.02 

Round  13, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 9.03 

Round  14, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.02 

Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.03 

Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.02 

Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.03 

Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.03 

Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.03 

Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.03 

Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.05 

Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.03 

Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.05 

Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.07 

Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.07 

Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.07 

Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.07 

Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.07 

Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.07 

Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.07 

Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.08 

Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.08 

Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.08 

Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.08 

Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.08 

Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.08 

Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.08 

Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.08 

Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08 

Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.08 

Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.10 

Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.10 

Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.10 

Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.10 

Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.10 

Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.10 

Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.10 

Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.10 

Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.10 

Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.10 

Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.12 

Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.12 

Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.12 

Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.12 

Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.12 

Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.12 

Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.17 

Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.12 

Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.17 

Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.15 

Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.17 

Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.15 

Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.17 

Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.15 

Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.17 

Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.15 

Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.17 

Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.17 

Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.17 

Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.18 

Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.17 

Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.18 

Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.22 

Final Round, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.23 

Final Round, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.22 

Average accuracy final 10 rounds: 9.148333333333333 

Average global accuracy final 10 rounds: 9.166666666666668 

395.56349420547485
[1.6495537757873535, 2.3382158279418945, 3.0328025817871094, 3.7910404205322266, 4.492560386657715, 5.18733024597168, 5.881921291351318, 6.618115425109863, 7.35538911819458, 8.090232372283936, 8.8267502784729, 9.550853729248047, 10.24785852432251, 10.939231157302856, 11.627009391784668, 12.31425142288208, 13.004631757736206, 13.689563512802124, 14.379591226577759, 15.069544076919556, 15.759783506393433, 16.453599452972412, 17.14446520805359, 17.833793878555298, 18.526467084884644, 19.217122077941895, 19.90375518798828, 20.593759298324585, 21.27702569961548, 21.966604709625244, 22.65520405769348, 23.3426730632782, 24.028813362121582, 24.71674394607544, 25.405227422714233, 26.09394073486328, 26.78479528427124, 27.471948862075806, 28.15831208229065, 28.843345403671265, 29.52910351753235, 30.219341278076172, 30.911527156829834, 31.605037689208984, 32.298566579818726, 32.995787143707275, 33.69448733329773, 34.388733863830566, 35.07916593551636, 35.770002365112305, 37.41449499130249]
[9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.016666666666667, 9.033333333333333, 9.033333333333333, 9.033333333333333, 9.066666666666666, 9.066666666666666, 9.066666666666666, 9.066666666666666, 9.066666666666666, 9.066666666666666, 9.083333333333334, 9.083333333333334, 9.083333333333334, 9.083333333333334, 9.083333333333334, 9.083333333333334, 9.083333333333334, 9.083333333333334, 9.083333333333334, 9.1, 9.1, 9.1, 9.1, 9.1, 9.116666666666667, 9.116666666666667, 9.116666666666667, 9.116666666666667, 9.15, 9.15, 9.15, 9.15, 9.166666666666666, 9.183333333333334, 9.183333333333334, 9.233333333333333]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.281, Test loss: 2.275, Test accuracy: 29.82 

Round   0, Global train loss: 2.281, Global test loss: 2.292, Global test accuracy: 29.20 

Round   1, Train loss: 2.132, Test loss: 2.166, Test accuracy: 33.60 

Round   1, Global train loss: 2.132, Global test loss: 2.256, Global test accuracy: 21.45 

Round   2, Train loss: 1.959, Test loss: 2.019, Test accuracy: 53.55 

Round   2, Global train loss: 1.959, Global test loss: 2.212, Global test accuracy: 29.47 

Round   3, Train loss: 1.812, Test loss: 1.929, Test accuracy: 64.35 

Round   3, Global train loss: 1.812, Global test loss: 2.183, Global test accuracy: 53.70 

Round   4, Train loss: 1.704, Test loss: 1.856, Test accuracy: 65.52 

Round   4, Global train loss: 1.704, Global test loss: 2.170, Global test accuracy: 26.98 

Round   5, Train loss: 1.548, Test loss: 1.824, Test accuracy: 68.28 

Round   5, Global train loss: 1.548, Global test loss: 2.201, Global test accuracy: 22.85 

Round   6, Train loss: 1.631, Test loss: 1.750, Test accuracy: 76.45 

Round   6, Global train loss: 1.631, Global test loss: 2.158, Global test accuracy: 27.42 

Round   7, Train loss: 1.715, Test loss: 1.709, Test accuracy: 80.12 

Round   7, Global train loss: 1.715, Global test loss: 2.143, Global test accuracy: 35.52 

Round   8, Train loss: 1.613, Test loss: 1.673, Test accuracy: 84.50 

Round   8, Global train loss: 1.613, Global test loss: 2.178, Global test accuracy: 25.97 

Round   9, Train loss: 1.519, Test loss: 1.658, Test accuracy: 84.98 

Round   9, Global train loss: 1.519, Global test loss: 2.176, Global test accuracy: 25.67 

Round  10, Train loss: 1.705, Test loss: 1.613, Test accuracy: 88.35 

Round  10, Global train loss: 1.705, Global test loss: 2.165, Global test accuracy: 33.10 

Round  11, Train loss: 1.559, Test loss: 1.588, Test accuracy: 89.95 

Round  11, Global train loss: 1.559, Global test loss: 2.105, Global test accuracy: 47.15 

Round  12, Train loss: 1.482, Test loss: 1.586, Test accuracy: 89.97 

Round  12, Global train loss: 1.482, Global test loss: 2.095, Global test accuracy: 46.73 

Round  13, Train loss: 1.570, Test loss: 1.563, Test accuracy: 91.73 

Round  13, Global train loss: 1.570, Global test loss: 2.146, Global test accuracy: 31.68 

Round  14, Train loss: 1.487, Test loss: 1.560, Test accuracy: 91.87 

Round  14, Global train loss: 1.487, Global test loss: 2.095, Global test accuracy: 40.77 

Round  15, Train loss: 1.496, Test loss: 1.557, Test accuracy: 91.88 

Round  15, Global train loss: 1.496, Global test loss: 2.238, Global test accuracy: 17.47 

Round  16, Train loss: 1.505, Test loss: 1.548, Test accuracy: 92.62 

Round  16, Global train loss: 1.505, Global test loss: 2.088, Global test accuracy: 39.02 

Round  17, Train loss: 1.538, Test loss: 1.536, Test accuracy: 93.93 

Round  17, Global train loss: 1.538, Global test loss: 2.104, Global test accuracy: 37.15 

Round  18, Train loss: 1.494, Test loss: 1.530, Test accuracy: 94.30 

Round  18, Global train loss: 1.494, Global test loss: 2.120, Global test accuracy: 33.78 

Round  19, Train loss: 1.473, Test loss: 1.529, Test accuracy: 94.42 

Round  19, Global train loss: 1.473, Global test loss: 2.158, Global test accuracy: 25.40 

Round  20, Train loss: 1.479, Test loss: 1.529, Test accuracy: 94.38 

Round  20, Global train loss: 1.479, Global test loss: 2.144, Global test accuracy: 27.33 

Round  21, Train loss: 1.472, Test loss: 1.529, Test accuracy: 94.38 

Round  21, Global train loss: 1.472, Global test loss: 2.097, Global test accuracy: 34.50 

Round  22, Train loss: 1.471, Test loss: 1.528, Test accuracy: 94.47 

Round  22, Global train loss: 1.471, Global test loss: 2.114, Global test accuracy: 43.32 

Round  23, Train loss: 1.471, Test loss: 1.527, Test accuracy: 94.45 

Round  23, Global train loss: 1.471, Global test loss: 2.129, Global test accuracy: 31.57 

Round  24, Train loss: 1.470, Test loss: 1.527, Test accuracy: 94.42 

Round  24, Global train loss: 1.470, Global test loss: 2.135, Global test accuracy: 28.00 

Round  25, Train loss: 1.468, Test loss: 1.527, Test accuracy: 94.43 

Round  25, Global train loss: 1.468, Global test loss: 2.054, Global test accuracy: 46.28 

Round  26, Train loss: 1.465, Test loss: 1.527, Test accuracy: 94.48 

Round  26, Global train loss: 1.465, Global test loss: 2.153, Global test accuracy: 26.40 

Round  27, Train loss: 1.476, Test loss: 1.525, Test accuracy: 94.48 

Round  27, Global train loss: 1.476, Global test loss: 2.150, Global test accuracy: 27.95 

Round  28, Train loss: 1.474, Test loss: 1.524, Test accuracy: 94.33 

Round  28, Global train loss: 1.474, Global test loss: 2.120, Global test accuracy: 30.03 

Round  29, Train loss: 1.470, Test loss: 1.523, Test accuracy: 94.35 

Round  29, Global train loss: 1.470, Global test loss: 2.158, Global test accuracy: 27.93 

Round  30, Train loss: 1.469, Test loss: 1.523, Test accuracy: 94.42 

Round  30, Global train loss: 1.469, Global test loss: 2.146, Global test accuracy: 27.80 

Round  31, Train loss: 1.467, Test loss: 1.523, Test accuracy: 94.43 

Round  31, Global train loss: 1.467, Global test loss: 2.167, Global test accuracy: 29.77 

Round  32, Train loss: 1.470, Test loss: 1.522, Test accuracy: 94.45 

Round  32, Global train loss: 1.470, Global test loss: 2.223, Global test accuracy: 20.22 

Round  33, Train loss: 1.469, Test loss: 1.522, Test accuracy: 94.48 

Round  33, Global train loss: 1.469, Global test loss: 2.207, Global test accuracy: 23.30 

Round  34, Train loss: 1.465, Test loss: 1.522, Test accuracy: 94.50 

Round  34, Global train loss: 1.465, Global test loss: 2.167, Global test accuracy: 22.90 

Round  35, Train loss: 1.467, Test loss: 1.522, Test accuracy: 94.55 

Round  35, Global train loss: 1.467, Global test loss: 2.071, Global test accuracy: 39.90 

Round  36, Train loss: 1.473, Test loss: 1.520, Test accuracy: 94.75 

Round  36, Global train loss: 1.473, Global test loss: 2.059, Global test accuracy: 42.95 

Round  37, Train loss: 1.467, Test loss: 1.520, Test accuracy: 94.78 

Round  37, Global train loss: 1.467, Global test loss: 2.222, Global test accuracy: 21.72 

Round  38, Train loss: 1.468, Test loss: 1.520, Test accuracy: 94.80 

Round  38, Global train loss: 1.468, Global test loss: 2.225, Global test accuracy: 20.22 

Round  39, Train loss: 1.467, Test loss: 1.520, Test accuracy: 94.75 

Round  39, Global train loss: 1.467, Global test loss: 2.079, Global test accuracy: 39.25 

Round  40, Train loss: 1.470, Test loss: 1.519, Test accuracy: 94.80 

Round  40, Global train loss: 1.470, Global test loss: 2.072, Global test accuracy: 41.67 

Round  41, Train loss: 1.469, Test loss: 1.518, Test accuracy: 94.83 

Round  41, Global train loss: 1.469, Global test loss: 2.027, Global test accuracy: 42.45 

Round  42, Train loss: 1.467, Test loss: 1.518, Test accuracy: 94.97 

Round  42, Global train loss: 1.467, Global test loss: 2.100, Global test accuracy: 34.07 

Round  43, Train loss: 1.464, Test loss: 1.518, Test accuracy: 95.00 

Round  43, Global train loss: 1.464, Global test loss: 2.161, Global test accuracy: 27.13 

Round  44, Train loss: 1.468, Test loss: 1.518, Test accuracy: 95.02 

Round  44, Global train loss: 1.468, Global test loss: 2.113, Global test accuracy: 34.02 

Round  45, Train loss: 1.467, Test loss: 1.517, Test accuracy: 95.05 

Round  45, Global train loss: 1.467, Global test loss: 2.117, Global test accuracy: 31.60 

Round  46, Train loss: 1.465, Test loss: 1.517, Test accuracy: 95.02 

Round  46, Global train loss: 1.465, Global test loss: 2.062, Global test accuracy: 44.17 

Round  47, Train loss: 1.464, Test loss: 1.517, Test accuracy: 95.05 

Round  47, Global train loss: 1.464, Global test loss: 2.101, Global test accuracy: 34.27 

Round  48, Train loss: 1.465, Test loss: 1.517, Test accuracy: 95.08 

Round  48, Global train loss: 1.465, Global test loss: 2.030, Global test accuracy: 50.67 

Round  49, Train loss: 1.463, Test loss: 1.517, Test accuracy: 95.08 

Round  49, Global train loss: 1.463, Global test loss: 2.216, Global test accuracy: 20.92 

Final Round, Train loss: 1.465, Test loss: 1.516, Test accuracy: 95.12 

Final Round, Global train loss: 1.465, Global test loss: 2.216, Global test accuracy: 20.92 

Average accuracy final 10 rounds: 94.98999999999998 

Average global accuracy final 10 rounds: 36.095 

358.0385990142822
[1.60923433303833, 2.2624566555023193, 2.9628283977508545, 3.613572835922241, 4.267400741577148, 4.9173808097839355, 5.564174652099609, 6.215949058532715, 6.862776279449463, 7.514859676361084, 8.165144920349121, 8.811268329620361, 9.460633516311646, 10.105032444000244, 10.755089282989502, 11.403644800186157, 12.053351640701294, 12.70246934890747, 13.347959995269775, 13.99574899673462, 14.643216609954834, 15.293195724487305, 15.840843677520752, 16.38437294960022, 16.932507514953613, 17.479851007461548, 18.024110317230225, 18.5666286945343, 19.105955600738525, 19.645158052444458, 20.186270475387573, 20.727465391159058, 21.2613742351532, 21.798222303390503, 22.335915088653564, 22.87644910812378, 23.462408304214478, 24.039567708969116, 24.61091423034668, 25.170146703720093, 25.727705240249634, 26.284916162490845, 26.842501878738403, 27.397908210754395, 27.94830083847046, 28.500244855880737, 29.047086238861084, 29.587247610092163, 30.12560486793518, 30.664347887039185, 31.943856716156006]
[29.816666666666666, 33.6, 53.55, 64.35, 65.51666666666667, 68.28333333333333, 76.45, 80.11666666666666, 84.5, 84.98333333333333, 88.35, 89.95, 89.96666666666667, 91.73333333333333, 91.86666666666666, 91.88333333333334, 92.61666666666666, 93.93333333333334, 94.3, 94.41666666666667, 94.38333333333334, 94.38333333333334, 94.46666666666667, 94.45, 94.41666666666667, 94.43333333333334, 94.48333333333333, 94.48333333333333, 94.33333333333333, 94.35, 94.41666666666667, 94.43333333333334, 94.45, 94.48333333333333, 94.5, 94.55, 94.75, 94.78333333333333, 94.8, 94.75, 94.8, 94.83333333333333, 94.96666666666667, 95.0, 95.01666666666667, 95.05, 95.01666666666667, 95.05, 95.08333333333333, 95.08333333333333, 95.11666666666666]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.273, Test loss: 2.266, Test accuracy: 21.68 

Round   0, Global train loss: 2.273, Global test loss: 2.291, Global test accuracy: 17.02 

Round   1, Train loss: 2.193, Test loss: 2.181, Test accuracy: 27.98 

Round   1, Global train loss: 2.193, Global test loss: 2.236, Global test accuracy: 16.67 

Round   2, Train loss: 1.983, Test loss: 2.077, Test accuracy: 41.72 

Round   2, Global train loss: 1.983, Global test loss: 2.201, Global test accuracy: 22.87 

Round   3, Train loss: 1.862, Test loss: 1.920, Test accuracy: 59.18 

Round   3, Global train loss: 1.862, Global test loss: 2.098, Global test accuracy: 39.73 

Round   4, Train loss: 1.772, Test loss: 1.842, Test accuracy: 66.77 

Round   4, Global train loss: 1.772, Global test loss: 2.077, Global test accuracy: 41.43 

Round   5, Train loss: 1.694, Test loss: 1.764, Test accuracy: 74.08 

Round   5, Global train loss: 1.694, Global test loss: 1.998, Global test accuracy: 51.07 

Round   6, Train loss: 1.647, Test loss: 1.730, Test accuracy: 76.48 

Round   6, Global train loss: 1.647, Global test loss: 1.942, Global test accuracy: 55.70 

Round   7, Train loss: 1.652, Test loss: 1.669, Test accuracy: 81.82 

Round   7, Global train loss: 1.652, Global test loss: 1.906, Global test accuracy: 60.77 

Round   8, Train loss: 1.588, Test loss: 1.629, Test accuracy: 85.32 

Round   8, Global train loss: 1.588, Global test loss: 1.928, Global test accuracy: 52.43 

Round   9, Train loss: 1.559, Test loss: 1.567, Test accuracy: 91.87 

Round   9, Global train loss: 1.559, Global test loss: 1.874, Global test accuracy: 60.52 

Round  10, Train loss: 1.555, Test loss: 1.544, Test accuracy: 93.50 

Round  10, Global train loss: 1.555, Global test loss: 1.856, Global test accuracy: 63.42 

Round  11, Train loss: 1.545, Test loss: 1.531, Test accuracy: 94.88 

Round  11, Global train loss: 1.545, Global test loss: 1.814, Global test accuracy: 65.93 

Round  12, Train loss: 1.539, Test loss: 1.529, Test accuracy: 94.75 

Round  12, Global train loss: 1.539, Global test loss: 1.782, Global test accuracy: 71.12 

Round  13, Train loss: 1.535, Test loss: 1.524, Test accuracy: 95.08 

Round  13, Global train loss: 1.535, Global test loss: 1.833, Global test accuracy: 64.53 

Round  14, Train loss: 1.522, Test loss: 1.523, Test accuracy: 95.18 

Round  14, Global train loss: 1.522, Global test loss: 1.765, Global test accuracy: 70.60 

Round  15, Train loss: 1.537, Test loss: 1.523, Test accuracy: 95.05 

Round  15, Global train loss: 1.537, Global test loss: 1.733, Global test accuracy: 74.78 

Round  16, Train loss: 1.516, Test loss: 1.521, Test accuracy: 95.03 

Round  16, Global train loss: 1.516, Global test loss: 1.747, Global test accuracy: 74.42 

Round  17, Train loss: 1.508, Test loss: 1.520, Test accuracy: 95.08 

Round  17, Global train loss: 1.508, Global test loss: 1.773, Global test accuracy: 69.80 

Round  18, Train loss: 1.505, Test loss: 1.518, Test accuracy: 95.17 

Round  18, Global train loss: 1.505, Global test loss: 1.770, Global test accuracy: 69.02 

Round  19, Train loss: 1.508, Test loss: 1.516, Test accuracy: 95.42 

Round  19, Global train loss: 1.508, Global test loss: 1.735, Global test accuracy: 74.68 

Round  20, Train loss: 1.510, Test loss: 1.516, Test accuracy: 95.42 

Round  20, Global train loss: 1.510, Global test loss: 1.719, Global test accuracy: 75.83 

Round  21, Train loss: 1.501, Test loss: 1.515, Test accuracy: 95.53 

Round  21, Global train loss: 1.501, Global test loss: 1.707, Global test accuracy: 77.97 

Round  22, Train loss: 1.504, Test loss: 1.515, Test accuracy: 95.55 

Round  22, Global train loss: 1.504, Global test loss: 1.736, Global test accuracy: 73.02 

Round  23, Train loss: 1.496, Test loss: 1.515, Test accuracy: 95.42 

Round  23, Global train loss: 1.496, Global test loss: 1.707, Global test accuracy: 77.25 

Round  24, Train loss: 1.502, Test loss: 1.514, Test accuracy: 95.42 

Round  24, Global train loss: 1.502, Global test loss: 1.744, Global test accuracy: 72.25 

Round  25, Train loss: 1.496, Test loss: 1.513, Test accuracy: 95.48 

Round  25, Global train loss: 1.496, Global test loss: 1.705, Global test accuracy: 77.93 

Round  26, Train loss: 1.496, Test loss: 1.512, Test accuracy: 95.43 

Round  26, Global train loss: 1.496, Global test loss: 1.712, Global test accuracy: 75.45 

Round  27, Train loss: 1.497, Test loss: 1.511, Test accuracy: 95.57 

Round  27, Global train loss: 1.497, Global test loss: 1.693, Global test accuracy: 77.97 

Round  28, Train loss: 1.496, Test loss: 1.510, Test accuracy: 95.75 

Round  28, Global train loss: 1.496, Global test loss: 1.660, Global test accuracy: 81.70 

Round  29, Train loss: 1.490, Test loss: 1.509, Test accuracy: 95.77 

Round  29, Global train loss: 1.490, Global test loss: 1.674, Global test accuracy: 80.17 

Round  30, Train loss: 1.492, Test loss: 1.508, Test accuracy: 95.85 

Round  30, Global train loss: 1.492, Global test loss: 1.650, Global test accuracy: 82.97 

Round  31, Train loss: 1.490, Test loss: 1.508, Test accuracy: 95.92 

Round  31, Global train loss: 1.490, Global test loss: 1.668, Global test accuracy: 80.87 

Round  32, Train loss: 1.487, Test loss: 1.508, Test accuracy: 95.85 

Round  32, Global train loss: 1.487, Global test loss: 1.678, Global test accuracy: 79.43 

Round  33, Train loss: 1.483, Test loss: 1.509, Test accuracy: 95.77 

Round  33, Global train loss: 1.483, Global test loss: 1.679, Global test accuracy: 79.18 

Round  34, Train loss: 1.485, Test loss: 1.509, Test accuracy: 95.72 

Round  34, Global train loss: 1.485, Global test loss: 1.680, Global test accuracy: 79.62 

Round  35, Train loss: 1.488, Test loss: 1.507, Test accuracy: 95.87 

Round  35, Global train loss: 1.488, Global test loss: 1.659, Global test accuracy: 82.20 

Round  36, Train loss: 1.491, Test loss: 1.507, Test accuracy: 95.75 

Round  36, Global train loss: 1.491, Global test loss: 1.681, Global test accuracy: 78.32 

Round  37, Train loss: 1.479, Test loss: 1.506, Test accuracy: 95.87 

Round  37, Global train loss: 1.479, Global test loss: 1.690, Global test accuracy: 77.93 

Round  38, Train loss: 1.481, Test loss: 1.506, Test accuracy: 95.88 

Round  38, Global train loss: 1.481, Global test loss: 1.701, Global test accuracy: 77.15 

Round  39, Train loss: 1.483, Test loss: 1.507, Test accuracy: 95.85 

Round  39, Global train loss: 1.483, Global test loss: 1.705, Global test accuracy: 76.17 

Round  40, Train loss: 1.489, Test loss: 1.506, Test accuracy: 95.95 

Round  40, Global train loss: 1.489, Global test loss: 1.635, Global test accuracy: 85.35 

Round  41, Train loss: 1.484, Test loss: 1.505, Test accuracy: 96.07 

Round  41, Global train loss: 1.484, Global test loss: 1.623, Global test accuracy: 84.53 

Round  42, Train loss: 1.486, Test loss: 1.506, Test accuracy: 95.90 

Round  42, Global train loss: 1.486, Global test loss: 1.662, Global test accuracy: 82.17 

Round  43, Train loss: 1.484, Test loss: 1.506, Test accuracy: 95.95 

Round  43, Global train loss: 1.484, Global test loss: 1.642, Global test accuracy: 84.27 

Round  44, Train loss: 1.486, Test loss: 1.506, Test accuracy: 95.93 

Round  44, Global train loss: 1.486, Global test loss: 1.633, Global test accuracy: 85.08 

Round  45, Train loss: 1.481, Test loss: 1.507, Test accuracy: 95.82 

Round  45, Global train loss: 1.481, Global test loss: 1.663, Global test accuracy: 80.90 

Round  46, Train loss: 1.480, Test loss: 1.506, Test accuracy: 95.88 

Round  46, Global train loss: 1.480, Global test loss: 1.653, Global test accuracy: 82.90 

Round  47, Train loss: 1.476, Test loss: 1.506, Test accuracy: 95.87 

Round  47, Global train loss: 1.476, Global test loss: 1.665, Global test accuracy: 80.57 

Round  48, Train loss: 1.485, Test loss: 1.506, Test accuracy: 95.82 

Round  48, Global train loss: 1.485, Global test loss: 1.650, Global test accuracy: 82.80 

Round  49, Train loss: 1.479, Test loss: 1.504, Test accuracy: 96.02 

Round  49, Global train loss: 1.479, Global test loss: 1.673, Global test accuracy: 79.77 

Final Round, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.23 

Final Round, Global train loss: 1.476, Global test loss: 1.673, Global test accuracy: 79.77 

Average accuracy final 10 rounds: 95.92000000000002 

Average global accuracy final 10 rounds: 82.83333333333331 

354.82552313804626
[1.69716477394104, 2.4372944831848145, 3.1738650798797607, 3.7973787784576416, 4.4213948249816895, 5.048967599868774, 5.677350044250488, 6.308977365493774, 6.9608471393585205, 7.606811046600342, 8.234054565429688, 8.86320948600769, 9.49939489364624, 10.136794090270996, 10.635883808135986, 11.14291763305664, 11.646278381347656, 12.15430498123169, 12.661696672439575, 13.167759418487549, 13.675469875335693, 14.179821729660034, 14.691703796386719, 15.19814395904541, 15.70382022857666, 16.208675861358643, 16.710190534591675, 17.218781232833862, 17.723076105117798, 18.22728443145752, 18.731902599334717, 19.23623776435852, 19.742472648620605, 20.244683980941772, 20.750109434127808, 21.256298542022705, 21.76021432876587, 22.304691314697266, 22.933313131332397, 23.568676233291626, 24.205013275146484, 24.83494472503662, 25.469517469406128, 26.098862648010254, 26.722003698349, 27.356126070022583, 27.989753007888794, 28.63255739212036, 29.263132333755493, 29.894790649414062, 31.24129629135132]
[21.683333333333334, 27.983333333333334, 41.71666666666667, 59.18333333333333, 66.76666666666667, 74.08333333333333, 76.48333333333333, 81.81666666666666, 85.31666666666666, 91.86666666666666, 93.5, 94.88333333333334, 94.75, 95.08333333333333, 95.18333333333334, 95.05, 95.03333333333333, 95.08333333333333, 95.16666666666667, 95.41666666666667, 95.41666666666667, 95.53333333333333, 95.55, 95.41666666666667, 95.41666666666667, 95.48333333333333, 95.43333333333334, 95.56666666666666, 95.75, 95.76666666666667, 95.85, 95.91666666666667, 95.85, 95.76666666666667, 95.71666666666667, 95.86666666666666, 95.75, 95.86666666666666, 95.88333333333334, 95.85, 95.95, 96.06666666666666, 95.9, 95.95, 95.93333333333334, 95.81666666666666, 95.88333333333334, 95.86666666666666, 95.81666666666666, 96.01666666666667, 96.23333333333333]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.35 

Round   1, Train loss: 2.301, Test loss: 2.300, Test accuracy: 13.58 

Round   2, Train loss: 2.299, Test loss: 2.298, Test accuracy: 17.32 

Round   3, Train loss: 2.296, Test loss: 2.295, Test accuracy: 19.32 

Round   4, Train loss: 2.294, Test loss: 2.291, Test accuracy: 20.78 

Round   5, Train loss: 2.289, Test loss: 2.285, Test accuracy: 21.12 

Round   6, Train loss: 2.281, Test loss: 2.274, Test accuracy: 21.90 

Round   7, Train loss: 2.267, Test loss: 2.252, Test accuracy: 23.83 

Round   8, Train loss: 2.241, Test loss: 2.227, Test accuracy: 27.07 

Round   9, Train loss: 2.221, Test loss: 2.204, Test accuracy: 29.53 

Round  10, Train loss: 2.194, Test loss: 2.178, Test accuracy: 31.65 

Round  11, Train loss: 2.161, Test loss: 2.146, Test accuracy: 35.78 

Round  12, Train loss: 2.126, Test loss: 2.105, Test accuracy: 38.53 

Round  13, Train loss: 2.065, Test loss: 2.060, Test accuracy: 43.42 

Round  14, Train loss: 2.017, Test loss: 2.014, Test accuracy: 51.00 

Round  15, Train loss: 1.960, Test loss: 1.959, Test accuracy: 57.48 

Round  16, Train loss: 1.900, Test loss: 1.916, Test accuracy: 60.28 

Round  17, Train loss: 1.881, Test loss: 1.872, Test accuracy: 63.90 

Round  18, Train loss: 1.812, Test loss: 1.839, Test accuracy: 67.25 

Round  19, Train loss: 1.798, Test loss: 1.806, Test accuracy: 70.17 

Round  20, Train loss: 1.769, Test loss: 1.787, Test accuracy: 71.48 

Round  21, Train loss: 1.747, Test loss: 1.765, Test accuracy: 73.23 

Round  22, Train loss: 1.726, Test loss: 1.742, Test accuracy: 75.95 

Round  23, Train loss: 1.698, Test loss: 1.725, Test accuracy: 77.43 

Round  24, Train loss: 1.693, Test loss: 1.703, Test accuracy: 79.17 

Round  25, Train loss: 1.669, Test loss: 1.691, Test accuracy: 79.88 

Round  26, Train loss: 1.660, Test loss: 1.681, Test accuracy: 80.75 

Round  27, Train loss: 1.657, Test loss: 1.672, Test accuracy: 81.42 

Round  28, Train loss: 1.664, Test loss: 1.669, Test accuracy: 81.32 

Round  29, Train loss: 1.653, Test loss: 1.660, Test accuracy: 82.27 

Round  30, Train loss: 1.654, Test loss: 1.656, Test accuracy: 82.58 

Round  31, Train loss: 1.628, Test loss: 1.651, Test accuracy: 83.10 

Round  32, Train loss: 1.637, Test loss: 1.650, Test accuracy: 83.18 

Round  33, Train loss: 1.632, Test loss: 1.648, Test accuracy: 83.13 

Round  34, Train loss: 1.616, Test loss: 1.646, Test accuracy: 83.10 

Round  35, Train loss: 1.620, Test loss: 1.644, Test accuracy: 83.23 

Round  36, Train loss: 1.623, Test loss: 1.641, Test accuracy: 83.32 

Round  37, Train loss: 1.617, Test loss: 1.639, Test accuracy: 83.47 

Round  38, Train loss: 1.610, Test loss: 1.639, Test accuracy: 83.82 

Round  39, Train loss: 1.623, Test loss: 1.636, Test accuracy: 83.87 

Round  40, Train loss: 1.619, Test loss: 1.636, Test accuracy: 83.57 

Round  41, Train loss: 1.616, Test loss: 1.635, Test accuracy: 83.65 

Round  42, Train loss: 1.603, Test loss: 1.632, Test accuracy: 84.05 

Round  43, Train loss: 1.605, Test loss: 1.632, Test accuracy: 84.02 

Round  44, Train loss: 1.605, Test loss: 1.633, Test accuracy: 84.00 

Round  45, Train loss: 1.610, Test loss: 1.631, Test accuracy: 84.20 

Round  46, Train loss: 1.597, Test loss: 1.629, Test accuracy: 84.25 

Round  47, Train loss: 1.607, Test loss: 1.631, Test accuracy: 84.02 

Round  48, Train loss: 1.600, Test loss: 1.630, Test accuracy: 84.22 

Round  49, Train loss: 1.597, Test loss: 1.628, Test accuracy: 84.10 

Final Round, Train loss: 1.602, Test loss: 1.627, Test accuracy: 84.35 

Average accuracy final 10 rounds: 84.00666666666667 

264.58020663261414
[1.5386059284210205, 2.123032331466675, 2.6862738132476807, 3.2425589561462402, 3.8007423877716064, 4.379072666168213, 4.943668603897095, 5.519631624221802, 6.078800678253174, 6.635535717010498, 7.1898274421691895, 7.746149778366089, 8.303514957427979, 8.85982060432434, 9.420893430709839, 9.977937936782837, 10.536000967025757, 11.09235692024231, 11.653179407119751, 12.20732855796814, 12.76512861251831, 13.31807255744934, 13.874406337738037, 14.427248477935791, 14.981475830078125, 15.530095100402832, 16.085472583770752, 16.636810064315796, 17.188966035842896, 17.76128602027893, 18.31883192062378, 18.87060785293579, 19.431217432022095, 19.98680591583252, 20.581669092178345, 21.13483715057373, 21.694583654403687, 22.245561838150024, 22.799973249435425, 23.351948022842407, 23.909651041030884, 24.467823028564453, 25.030128240585327, 25.5895938873291, 26.149507522583008, 26.70697522163391, 27.267714262008667, 27.825897693634033, 28.38001585006714, 28.937923669815063, 29.903347969055176]
[10.35, 13.583333333333334, 17.316666666666666, 19.316666666666666, 20.783333333333335, 21.116666666666667, 21.9, 23.833333333333332, 27.066666666666666, 29.533333333333335, 31.65, 35.78333333333333, 38.53333333333333, 43.416666666666664, 51.0, 57.483333333333334, 60.28333333333333, 63.9, 67.25, 70.16666666666667, 71.48333333333333, 73.23333333333333, 75.95, 77.43333333333334, 79.16666666666667, 79.88333333333334, 80.75, 81.41666666666667, 81.31666666666666, 82.26666666666667, 82.58333333333333, 83.1, 83.18333333333334, 83.13333333333334, 83.1, 83.23333333333333, 83.31666666666666, 83.46666666666667, 83.81666666666666, 83.86666666666666, 83.56666666666666, 83.65, 84.05, 84.01666666666667, 84.0, 84.2, 84.25, 84.01666666666667, 84.21666666666667, 84.1, 84.35]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.319, Test loss: 2.301, Test accuracy: 11.63
Round   1, Train loss: 2.301, Test loss: 2.299, Test accuracy: 16.38
Round   2, Train loss: 2.298, Test loss: 2.296, Test accuracy: 19.23
Round   3, Train loss: 2.295, Test loss: 2.293, Test accuracy: 20.23
Round   4, Train loss: 2.291, Test loss: 2.288, Test accuracy: 20.60
Round   5, Train loss: 2.285, Test loss: 2.280, Test accuracy: 21.15
Round   6, Train loss: 2.277, Test loss: 2.265, Test accuracy: 21.30
Round   7, Train loss: 2.258, Test loss: 2.241, Test accuracy: 22.32
Round   8, Train loss: 2.238, Test loss: 2.221, Test accuracy: 28.17
Round   9, Train loss: 2.220, Test loss: 2.200, Test accuracy: 31.10
Round  10, Train loss: 2.191, Test loss: 2.176, Test accuracy: 31.60
Round  11, Train loss: 2.164, Test loss: 2.143, Test accuracy: 37.50
Round  12, Train loss: 2.122, Test loss: 2.105, Test accuracy: 42.15
Round  13, Train loss: 2.080, Test loss: 2.064, Test accuracy: 46.25
Round  14, Train loss: 2.047, Test loss: 2.020, Test accuracy: 54.98
Round  15, Train loss: 1.973, Test loss: 1.972, Test accuracy: 58.77
Round  16, Train loss: 1.921, Test loss: 1.934, Test accuracy: 60.87
Round  17, Train loss: 1.907, Test loss: 1.897, Test accuracy: 63.65
Round  18, Train loss: 1.880, Test loss: 1.860, Test accuracy: 68.45
Round  19, Train loss: 1.850, Test loss: 1.833, Test accuracy: 70.82
Round  20, Train loss: 1.812, Test loss: 1.812, Test accuracy: 72.22
Round  21, Train loss: 1.802, Test loss: 1.789, Test accuracy: 73.65
Round  22, Train loss: 1.775, Test loss: 1.765, Test accuracy: 77.33
Round  23, Train loss: 1.756, Test loss: 1.744, Test accuracy: 79.53
Round  24, Train loss: 1.743, Test loss: 1.720, Test accuracy: 81.20
Round  25, Train loss: 1.719, Test loss: 1.708, Test accuracy: 81.92
Round  26, Train loss: 1.729, Test loss: 1.693, Test accuracy: 82.95
Round  27, Train loss: 1.703, Test loss: 1.688, Test accuracy: 82.93
Round  28, Train loss: 1.694, Test loss: 1.681, Test accuracy: 83.55
Round  29, Train loss: 1.695, Test loss: 1.673, Test accuracy: 83.95
Round  30, Train loss: 1.693, Test loss: 1.666, Test accuracy: 84.30
Round  31, Train loss: 1.670, Test loss: 1.663, Test accuracy: 84.32
Round  32, Train loss: 1.663, Test loss: 1.661, Test accuracy: 84.47
Round  33, Train loss: 1.656, Test loss: 1.659, Test accuracy: 84.58
Round  34, Train loss: 1.678, Test loss: 1.655, Test accuracy: 84.63
Round  35, Train loss: 1.664, Test loss: 1.652, Test accuracy: 84.82
Round  36, Train loss: 1.647, Test loss: 1.651, Test accuracy: 84.80
Round  37, Train loss: 1.644, Test loss: 1.649, Test accuracy: 84.98
Round  38, Train loss: 1.662, Test loss: 1.646, Test accuracy: 84.93
Round  39, Train loss: 1.651, Test loss: 1.643, Test accuracy: 85.12
Round  40, Train loss: 1.650, Test loss: 1.642, Test accuracy: 85.08
Round  41, Train loss: 1.652, Test loss: 1.640, Test accuracy: 85.23
Round  42, Train loss: 1.637, Test loss: 1.639, Test accuracy: 85.22
Round  43, Train loss: 1.647, Test loss: 1.637, Test accuracy: 85.32
Round  44, Train loss: 1.642, Test loss: 1.636, Test accuracy: 85.50
Round  45, Train loss: 1.631, Test loss: 1.635, Test accuracy: 85.45
Round  46, Train loss: 1.645, Test loss: 1.633, Test accuracy: 85.58
Round  47, Train loss: 1.629, Test loss: 1.632, Test accuracy: 85.55
Round  48, Train loss: 1.625, Test loss: 1.631, Test accuracy: 85.70
Round  49, Train loss: 1.618, Test loss: 1.630, Test accuracy: 85.72
Final Round, Train loss: 1.609, Test loss: 1.625, Test accuracy: 85.93
Average accuracy final 10 rounds: 85.43500000000002
303.34809255599976
[1.6380925178527832, 2.3315389156341553, 3.025747060775757, 3.7213525772094727, 4.4119873046875, 5.103972434997559, 5.786400556564331, 6.486331224441528, 7.172837734222412, 7.869356393814087, 8.564352989196777, 9.260801792144775, 9.954754829406738, 10.64778995513916, 11.334481239318848, 12.016791105270386, 12.704349756240845, 13.382392644882202, 14.071659326553345, 14.760698795318604, 15.447478532791138, 16.132506370544434, 16.81680941581726, 17.503601551055908, 18.186612129211426, 18.870607137680054, 19.558069467544556, 20.24740481376648, 20.927239179611206, 21.613120794296265, 22.292637825012207, 22.977290391921997, 23.661686658859253, 24.351349353790283, 25.034891605377197, 25.72069478034973, 26.4071204662323, 27.09444236755371, 27.78017234802246, 28.460262537002563, 29.150256872177124, 29.882089376449585, 30.57634925842285, 31.264267921447754, 31.95620036125183, 32.650230407714844, 33.344438314437866, 34.034337759017944, 34.72335982322693, 35.417628049850464, 36.44386100769043]
[11.633333333333333, 16.383333333333333, 19.233333333333334, 20.233333333333334, 20.6, 21.15, 21.3, 22.316666666666666, 28.166666666666668, 31.1, 31.6, 37.5, 42.15, 46.25, 54.983333333333334, 58.766666666666666, 60.86666666666667, 63.65, 68.45, 70.81666666666666, 72.21666666666667, 73.65, 77.33333333333333, 79.53333333333333, 81.2, 81.91666666666667, 82.95, 82.93333333333334, 83.55, 83.95, 84.3, 84.31666666666666, 84.46666666666667, 84.58333333333333, 84.63333333333334, 84.81666666666666, 84.8, 84.98333333333333, 84.93333333333334, 85.11666666666666, 85.08333333333333, 85.23333333333333, 85.21666666666667, 85.31666666666666, 85.5, 85.45, 85.58333333333333, 85.55, 85.7, 85.71666666666667, 85.93333333333334]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 23.33
Round   1, Train loss: 2.300, Test loss: 2.298, Test accuracy: 27.28
Round   2, Train loss: 2.298, Test loss: 2.295, Test accuracy: 33.18
Round   3, Train loss: 2.295, Test loss: 2.289, Test accuracy: 36.08
Round   4, Train loss: 2.290, Test loss: 2.275, Test accuracy: 31.25
Round   5, Train loss: 2.273, Test loss: 2.223, Test accuracy: 30.23
Round   6, Train loss: 2.214, Test loss: 2.147, Test accuracy: 31.67
Round   7, Train loss: 2.169, Test loss: 2.087, Test accuracy: 39.90
Round   8, Train loss: 2.130, Test loss: 2.011, Test accuracy: 52.72
Round   9, Train loss: 2.073, Test loss: 1.878, Test accuracy: 67.70
Round  10, Train loss: 1.980, Test loss: 1.774, Test accuracy: 77.10
Round  11, Train loss: 1.852, Test loss: 1.714, Test accuracy: 80.15
Round  12, Train loss: 1.785, Test loss: 1.684, Test accuracy: 81.90
Round  13, Train loss: 1.736, Test loss: 1.665, Test accuracy: 82.63
Round  14, Train loss: 1.735, Test loss: 1.653, Test accuracy: 83.18
Round  15, Train loss: 1.662, Test loss: 1.646, Test accuracy: 83.48
Round  16, Train loss: 1.656, Test loss: 1.641, Test accuracy: 83.60
Round  17, Train loss: 1.687, Test loss: 1.635, Test accuracy: 83.85
Round  18, Train loss: 1.646, Test loss: 1.631, Test accuracy: 84.02
Round  19, Train loss: 1.624, Test loss: 1.629, Test accuracy: 84.03
Round  20, Train loss: 1.639, Test loss: 1.626, Test accuracy: 84.60
Round  21, Train loss: 1.614, Test loss: 1.624, Test accuracy: 84.55
Round  22, Train loss: 1.602, Test loss: 1.622, Test accuracy: 84.82
Round  23, Train loss: 1.598, Test loss: 1.622, Test accuracy: 84.72
Round  24, Train loss: 1.600, Test loss: 1.619, Test accuracy: 84.95
Round  25, Train loss: 1.597, Test loss: 1.619, Test accuracy: 84.75
Round  26, Train loss: 1.593, Test loss: 1.619, Test accuracy: 84.68
Round  27, Train loss: 1.591, Test loss: 1.618, Test accuracy: 84.85
Round  28, Train loss: 1.587, Test loss: 1.617, Test accuracy: 84.93
Round  29, Train loss: 1.591, Test loss: 1.617, Test accuracy: 85.08
Round  30, Train loss: 1.587, Test loss: 1.616, Test accuracy: 84.93
Round  31, Train loss: 1.590, Test loss: 1.614, Test accuracy: 85.18
Round  32, Train loss: 1.589, Test loss: 1.614, Test accuracy: 85.23
Round  33, Train loss: 1.588, Test loss: 1.613, Test accuracy: 85.28
Round  34, Train loss: 1.582, Test loss: 1.613, Test accuracy: 85.42
Round  35, Train loss: 1.582, Test loss: 1.613, Test accuracy: 85.28
Round  36, Train loss: 1.585, Test loss: 1.613, Test accuracy: 85.18
Round  37, Train loss: 1.587, Test loss: 1.611, Test accuracy: 85.62
Round  38, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.68
Round  39, Train loss: 1.586, Test loss: 1.609, Test accuracy: 85.87
Round  40, Train loss: 1.581, Test loss: 1.610, Test accuracy: 85.60
Round  41, Train loss: 1.582, Test loss: 1.609, Test accuracy: 85.72
Round  42, Train loss: 1.587, Test loss: 1.608, Test accuracy: 85.77
Round  43, Train loss: 1.581, Test loss: 1.608, Test accuracy: 85.70
Round  44, Train loss: 1.584, Test loss: 1.607, Test accuracy: 85.93
Round  45, Train loss: 1.585, Test loss: 1.607, Test accuracy: 85.78
Round  46, Train loss: 1.576, Test loss: 1.607, Test accuracy: 85.95
Round  47, Train loss: 1.584, Test loss: 1.607, Test accuracy: 85.75
Round  48, Train loss: 1.576, Test loss: 1.607, Test accuracy: 85.82
Round  49, Train loss: 1.574, Test loss: 1.605, Test accuracy: 86.02
Final Round, Train loss: 1.576, Test loss: 1.605, Test accuracy: 86.07
Average accuracy final 10 rounds: 85.80333333333334
582.5609040260315
[2.669454336166382, 4.404358148574829, 5.738708257675171, 7.068011999130249, 8.400266647338867, 9.733269929885864, 11.154865741729736, 12.563900232315063, 14.146222352981567, 15.768312931060791, 17.39147663116455, 19.024237871170044, 20.65165376663208, 22.30486297607422, 23.93015718460083, 25.557183027267456, 27.181919813156128, 28.80697774887085, 30.42931294441223, 32.052040815353394, 33.675328969955444, 35.31302285194397, 36.932557106018066, 38.56303381919861, 40.19190239906311, 41.81604051589966, 43.44359564781189, 45.06034469604492, 46.67278265953064, 48.294458627700806, 49.91699194908142, 51.5423743724823, 53.1730375289917, 54.79727602005005, 56.41603350639343, 58.04418992996216, 59.66698455810547, 61.28777742385864, 62.912784576416016, 64.53400778770447, 65.94367671012878, 67.35342836380005, 68.76937627792358, 70.38324689865112, 72.12076330184937, 73.85154676437378, 75.58414435386658, 77.31090021133423, 79.04147458076477, 80.75055313110352, 82.3728129863739]
[23.333333333333332, 27.283333333333335, 33.18333333333333, 36.083333333333336, 31.25, 30.233333333333334, 31.666666666666668, 39.9, 52.71666666666667, 67.7, 77.1, 80.15, 81.9, 82.63333333333334, 83.18333333333334, 83.48333333333333, 83.6, 83.85, 84.01666666666667, 84.03333333333333, 84.6, 84.55, 84.81666666666666, 84.71666666666667, 84.95, 84.75, 84.68333333333334, 84.85, 84.93333333333334, 85.08333333333333, 84.93333333333334, 85.18333333333334, 85.23333333333333, 85.28333333333333, 85.41666666666667, 85.28333333333333, 85.18333333333334, 85.61666666666666, 85.68333333333334, 85.86666666666666, 85.6, 85.71666666666667, 85.76666666666667, 85.7, 85.93333333333334, 85.78333333333333, 85.95, 85.75, 85.81666666666666, 86.01666666666667, 86.06666666666666]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.718, Test loss: 2.299, Test accuracy: 29.87
Round   1, Train loss: 1.682, Test loss: 2.291, Test accuracy: 30.48
Round   2, Train loss: 1.611, Test loss: 2.273, Test accuracy: 37.73
Round   3, Train loss: 1.528, Test loss: 2.239, Test accuracy: 45.77
Round   4, Train loss: 1.437, Test loss: 2.186, Test accuracy: 53.15
Round   5, Train loss: 1.378, Test loss: 2.144, Test accuracy: 58.55
Round   6, Train loss: 1.343, Test loss: 2.111, Test accuracy: 61.48
Round   7, Train loss: 1.329, Test loss: 2.092, Test accuracy: 62.15
Round   8, Train loss: 1.331, Test loss: 2.078, Test accuracy: 62.78
Round   9, Train loss: 1.336, Test loss: 2.069, Test accuracy: 64.07
Round  10, Train loss: 1.319, Test loss: 2.063, Test accuracy: 64.57
Round  11, Train loss: 1.332, Test loss: 2.062, Test accuracy: 64.80
Round  12, Train loss: 1.289, Test loss: 2.058, Test accuracy: 66.30
Round  13, Train loss: 1.271, Test loss: 2.046, Test accuracy: 67.70
Round  14, Train loss: 1.270, Test loss: 2.039, Test accuracy: 68.62
Round  15, Train loss: 1.258, Test loss: 2.031, Test accuracy: 69.33
Round  16, Train loss: 1.255, Test loss: 2.025, Test accuracy: 69.65
Round  17, Train loss: 1.256, Test loss: 2.023, Test accuracy: 70.73
Round  18, Train loss: 1.249, Test loss: 2.018, Test accuracy: 70.62
Round  19, Train loss: 1.250, Test loss: 2.014, Test accuracy: 70.77
Round  20, Train loss: 1.252, Test loss: 2.011, Test accuracy: 70.57
Round  21, Train loss: 1.250, Test loss: 2.009, Test accuracy: 70.48
Round  22, Train loss: 1.234, Test loss: 2.007, Test accuracy: 70.35
Round  23, Train loss: 1.239, Test loss: 2.006, Test accuracy: 70.05
Round  24, Train loss: 1.240, Test loss: 2.005, Test accuracy: 69.92
Round  25, Train loss: 1.251, Test loss: 2.004, Test accuracy: 69.87
Round  26, Train loss: 1.247, Test loss: 2.002, Test accuracy: 69.55
Round  27, Train loss: 1.250, Test loss: 2.002, Test accuracy: 69.27
Round  28, Train loss: 1.237, Test loss: 2.002, Test accuracy: 69.08
Round  29, Train loss: 1.251, Test loss: 1.999, Test accuracy: 69.32
Round  30, Train loss: 1.245, Test loss: 1.999, Test accuracy: 69.07
Round  31, Train loss: 1.241, Test loss: 1.999, Test accuracy: 68.92
Round  32, Train loss: 1.246, Test loss: 1.999, Test accuracy: 68.75
Round  33, Train loss: 1.240, Test loss: 1.998, Test accuracy: 68.73
Round  34, Train loss: 1.231, Test loss: 1.999, Test accuracy: 68.38
Round  35, Train loss: 1.234, Test loss: 1.999, Test accuracy: 68.25
Round  36, Train loss: 1.245, Test loss: 1.998, Test accuracy: 68.00
Round  37, Train loss: 1.251, Test loss: 1.998, Test accuracy: 67.60
Round  38, Train loss: 1.242, Test loss: 1.998, Test accuracy: 67.52
Round  39, Train loss: 1.240, Test loss: 1.998, Test accuracy: 67.48
Round  40, Train loss: 1.244, Test loss: 1.998, Test accuracy: 67.22
Round  41, Train loss: 1.243, Test loss: 2.000, Test accuracy: 66.98
Round  42, Train loss: 1.242, Test loss: 1.999, Test accuracy: 67.00
Round  43, Train loss: 1.243, Test loss: 2.000, Test accuracy: 66.43
Round  44, Train loss: 1.246, Test loss: 2.001, Test accuracy: 66.23
Round  45, Train loss: 1.241, Test loss: 2.002, Test accuracy: 65.37
Round  46, Train loss: 1.240, Test loss: 2.003, Test accuracy: 64.52
Round  47, Train loss: 1.233, Test loss: 2.004, Test accuracy: 63.58
Round  48, Train loss: 1.232, Test loss: 2.005, Test accuracy: 63.23
Round  49, Train loss: 1.234, Test loss: 2.005, Test accuracy: 63.00
Final Round, Train loss: 1.233, Test loss: 2.012, Test accuracy: 62.23
Average accuracy final 10 rounds: 65.35666666666667
501.34834241867065
[]
[29.866666666666667, 30.483333333333334, 37.733333333333334, 45.766666666666666, 53.15, 58.55, 61.483333333333334, 62.15, 62.78333333333333, 64.06666666666666, 64.56666666666666, 64.8, 66.3, 67.7, 68.61666666666666, 69.33333333333333, 69.65, 70.73333333333333, 70.61666666666666, 70.76666666666667, 70.56666666666666, 70.48333333333333, 70.35, 70.05, 69.91666666666667, 69.86666666666666, 69.55, 69.26666666666667, 69.08333333333333, 69.31666666666666, 69.06666666666666, 68.91666666666667, 68.75, 68.73333333333333, 68.38333333333334, 68.25, 68.0, 67.6, 67.51666666666667, 67.48333333333333, 67.21666666666667, 66.98333333333333, 67.0, 66.43333333333334, 66.23333333333333, 65.36666666666666, 64.51666666666667, 63.583333333333336, 63.233333333333334, 63.0, 62.233333333333334]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.55
Round   0: Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 4.67
Round   1, Train loss: 2.291, Test loss: 2.302, Test accuracy: 8.47
Round   1: Global train loss: 2.291, Global test loss: 2.303, Global test accuracy: 4.92
Round   2, Train loss: 2.313, Test loss: 2.301, Test accuracy: 11.47
Round   2: Global train loss: 2.313, Global test loss: 2.303, Global test accuracy: 6.17
Round   3, Train loss: 2.304, Test loss: 2.301, Test accuracy: 14.52
Round   3: Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 7.70
Round   4, Train loss: 2.292, Test loss: 2.301, Test accuracy: 14.15
Round   4: Global train loss: 2.292, Global test loss: 2.302, Global test accuracy: 8.60
Round   5, Train loss: 2.295, Test loss: 2.300, Test accuracy: 13.98
Round   5: Global train loss: 2.295, Global test loss: 2.302, Global test accuracy: 9.73
Round   6, Train loss: 2.289, Test loss: 2.301, Test accuracy: 12.25
Round   6: Global train loss: 2.289, Global test loss: 2.302, Global test accuracy: 10.47
Round   7, Train loss: 2.298, Test loss: 2.298, Test accuracy: 14.90
Round   7: Global train loss: 2.298, Global test loss: 2.301, Global test accuracy: 12.78
Round   8, Train loss: 2.289, Test loss: 2.298, Test accuracy: 14.67
Round   8: Global train loss: 2.289, Global test loss: 2.301, Global test accuracy: 13.82
Round   9, Train loss: 2.303, Test loss: 2.297, Test accuracy: 17.22
Round   9: Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 15.77
Round  10, Train loss: 2.301, Test loss: 2.297, Test accuracy: 16.88
Round  10: Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.57
Round  11, Train loss: 2.291, Test loss: 2.297, Test accuracy: 17.17
Round  11: Global train loss: 2.291, Global test loss: 2.300, Global test accuracy: 17.70
Round  12, Train loss: 2.283, Test loss: 2.296, Test accuracy: 18.72
Round  12: Global train loss: 2.283, Global test loss: 2.300, Global test accuracy: 18.90
Round  13, Train loss: 2.286, Test loss: 2.296, Test accuracy: 18.58
Round  13: Global train loss: 2.286, Global test loss: 2.300, Global test accuracy: 20.40
Round  14, Train loss: 2.290, Test loss: 2.294, Test accuracy: 17.78
Round  14: Global train loss: 2.290, Global test loss: 2.300, Global test accuracy: 20.35
Round  15, Train loss: 2.269, Test loss: 2.292, Test accuracy: 16.90
Round  15: Global train loss: 2.269, Global test loss: 2.299, Global test accuracy: 20.52
Round  16, Train loss: 2.291, Test loss: 2.290, Test accuracy: 16.55
Round  16: Global train loss: 2.291, Global test loss: 2.299, Global test accuracy: 19.95
Round  17, Train loss: 2.247, Test loss: 2.285, Test accuracy: 18.27
Round  17: Global train loss: 2.247, Global test loss: 2.298, Global test accuracy: 19.95
Round  18, Train loss: 2.263, Test loss: 2.285, Test accuracy: 18.63
Round  18: Global train loss: 2.263, Global test loss: 2.298, Global test accuracy: 20.65
Round  19, Train loss: 2.215, Test loss: 2.279, Test accuracy: 19.15
Round  19: Global train loss: 2.215, Global test loss: 2.297, Global test accuracy: 22.28
Round  20, Train loss: 2.185, Test loss: 2.272, Test accuracy: 19.22
Round  20: Global train loss: 2.185, Global test loss: 2.296, Global test accuracy: 22.52
Round  21, Train loss: 2.175, Test loss: 2.268, Test accuracy: 19.60
Round  21: Global train loss: 2.175, Global test loss: 2.295, Global test accuracy: 23.88
Round  22, Train loss: 2.252, Test loss: 2.272, Test accuracy: 20.27
Round  22: Global train loss: 2.252, Global test loss: 2.295, Global test accuracy: 25.55
Round  23, Train loss: 2.132, Test loss: 2.265, Test accuracy: 22.22
Round  23: Global train loss: 2.132, Global test loss: 2.294, Global test accuracy: 25.80
Round  24, Train loss: 2.228, Test loss: 2.272, Test accuracy: 23.07
Round  24: Global train loss: 2.228, Global test loss: 2.295, Global test accuracy: 28.88
Round  25, Train loss: 2.141, Test loss: 2.265, Test accuracy: 21.35
Round  25: Global train loss: 2.141, Global test loss: 2.294, Global test accuracy: 28.42
Round  26, Train loss: 2.149, Test loss: 2.257, Test accuracy: 21.03
Round  26: Global train loss: 2.149, Global test loss: 2.294, Global test accuracy: 29.35
Round  27, Train loss: 2.162, Test loss: 2.253, Test accuracy: 21.67
Round  27: Global train loss: 2.162, Global test loss: 2.293, Global test accuracy: 30.77
Round  28, Train loss: 2.106, Test loss: 2.242, Test accuracy: 23.67
Round  28: Global train loss: 2.106, Global test loss: 2.291, Global test accuracy: 32.88
Round  29, Train loss: 2.180, Test loss: 2.253, Test accuracy: 22.08
Round  29: Global train loss: 2.180, Global test loss: 2.292, Global test accuracy: 34.32
Round  30, Train loss: 2.089, Test loss: 2.236, Test accuracy: 24.52
Round  30: Global train loss: 2.089, Global test loss: 2.290, Global test accuracy: 36.53
Round  31, Train loss: 2.297, Test loss: 2.257, Test accuracy: 21.02
Round  31: Global train loss: 2.297, Global test loss: 2.292, Global test accuracy: 35.50
Round  32, Train loss: 2.000, Test loss: 2.244, Test accuracy: 21.90
Round  32: Global train loss: 2.000, Global test loss: 2.291, Global test accuracy: 35.12
Round  33, Train loss: 2.117, Test loss: 2.247, Test accuracy: 20.60
Round  33: Global train loss: 2.117, Global test loss: 2.291, Global test accuracy: 34.72
Round  34, Train loss: 1.860, Test loss: 2.224, Test accuracy: 24.17
Round  34: Global train loss: 1.860, Global test loss: 2.289, Global test accuracy: 36.20
Round  35, Train loss: 2.038, Test loss: 2.229, Test accuracy: 24.08
Round  35: Global train loss: 2.038, Global test loss: 2.289, Global test accuracy: 36.93
Round  36, Train loss: 2.210, Test loss: 2.231, Test accuracy: 23.07
Round  36: Global train loss: 2.210, Global test loss: 2.289, Global test accuracy: 35.48
Round  37, Train loss: 1.659, Test loss: 2.207, Test accuracy: 24.93
Round  37: Global train loss: 1.659, Global test loss: 2.286, Global test accuracy: 38.38
Round  38, Train loss: 1.837, Test loss: 2.203, Test accuracy: 25.23
Round  38: Global train loss: 1.837, Global test loss: 2.286, Global test accuracy: 39.72
Round  39, Train loss: 1.433, Test loss: 2.167, Test accuracy: 30.30
Round  39: Global train loss: 1.433, Global test loss: 2.281, Global test accuracy: 41.88
Round  40, Train loss: 0.938, Test loss: 2.124, Test accuracy: 36.42
Round  40: Global train loss: 0.938, Global test loss: 2.271, Global test accuracy: 45.78
Round  41, Train loss: 1.270, Test loss: 2.117, Test accuracy: 37.88
Round  41: Global train loss: 1.270, Global test loss: 2.263, Global test accuracy: 46.52
Round  42, Train loss: 2.174, Test loss: 2.158, Test accuracy: 32.88
Round  42: Global train loss: 2.174, Global test loss: 2.266, Global test accuracy: 44.93
Round  43, Train loss: 1.280, Test loss: 2.118, Test accuracy: 35.87
Round  43: Global train loss: 1.280, Global test loss: 2.258, Global test accuracy: 45.35
Round  44, Train loss: 0.712, Test loss: 2.058, Test accuracy: 41.83
Round  44: Global train loss: 0.712, Global test loss: 2.239, Global test accuracy: 46.78
Round  45, Train loss: 1.842, Test loss: 2.121, Test accuracy: 35.83
Round  45: Global train loss: 1.842, Global test loss: 2.236, Global test accuracy: 46.45
Round  46, Train loss: 1.218, Test loss: 2.086, Test accuracy: 40.15
Round  46: Global train loss: 1.218, Global test loss: 2.224, Global test accuracy: 46.62
Round  47, Train loss: 1.146, Test loss: 2.075, Test accuracy: 39.80
Round  47: Global train loss: 1.146, Global test loss: 2.209, Global test accuracy: 46.68
Round  48, Train loss: 1.984, Test loss: 2.097, Test accuracy: 37.60
Round  48: Global train loss: 1.984, Global test loss: 2.205, Global test accuracy: 46.02
Round  49, Train loss: 1.158, Test loss: 2.058, Test accuracy: 41.13
Round  49: Global train loss: 1.158, Global test loss: 2.192, Global test accuracy: 45.30
Final Round: Train loss: 2.044, Test loss: 1.947, Test accuracy: 56.12
Final Round: Global train loss: 2.044, Global test loss: 2.172, Global test accuracy: 47.03
Average accuracy final 10 rounds: 37.94
Average global accuracy final 10 rounds: 46.04333333333333
395.12177538871765
[]
[7.55, 8.466666666666667, 11.466666666666667, 14.516666666666667, 14.15, 13.983333333333333, 12.25, 14.9, 14.666666666666666, 17.216666666666665, 16.883333333333333, 17.166666666666668, 18.716666666666665, 18.583333333333332, 17.783333333333335, 16.9, 16.55, 18.266666666666666, 18.633333333333333, 19.15, 19.216666666666665, 19.6, 20.266666666666666, 22.216666666666665, 23.066666666666666, 21.35, 21.033333333333335, 21.666666666666668, 23.666666666666668, 22.083333333333332, 24.516666666666666, 21.016666666666666, 21.9, 20.6, 24.166666666666668, 24.083333333333332, 23.066666666666666, 24.933333333333334, 25.233333333333334, 30.3, 36.416666666666664, 37.88333333333333, 32.88333333333333, 35.86666666666667, 41.833333333333336, 35.833333333333336, 40.15, 39.8, 37.6, 41.13333333333333, 56.11666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round   0, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round   1, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round   2, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round   3, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round   3, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round   4, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.05 

Round   4, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 10.05 

Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round   5, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round   6, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round   6, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round   7, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round   8, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.05 

Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.07 

Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.07 

Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.07 

Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.07 

Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05 

Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.07 

Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.07 

Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.07 

Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.07 

Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.07 

Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.07 

Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.07 

Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.07 

Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.07 

Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.07 

Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.08 

Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.08 

Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.08 

Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.08 

Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.08 

Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.08 

Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.08 

Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.10 

Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.08 

Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.10 

Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.10 

Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.10 

Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.10 

Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.10 

Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.10 

Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.10 

Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.10 

Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.10 

Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Round  39, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.10 

Round  39, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 10.12 

Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12 

Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12 

Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12 

Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12 

Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12 

Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12 

Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12 

Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12 

Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Round  48, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.12 

Round  48, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 10.12 

Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12 

Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Final Round, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.12 

Final Round, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Average accuracy final 10 rounds: 10.116666666666667 

Average global accuracy final 10 rounds: 10.116666666666667 

396.4962658882141
[1.645862340927124, 2.3284847736358643, 3.009838104248047, 3.692166328430176, 4.377261161804199, 5.068686485290527, 5.798414468765259, 6.488516092300415, 7.2516233921051025, 7.979432106018066, 8.801943063735962, 9.49839448928833, 10.187148332595825, 10.88599419593811, 11.58141016960144, 12.273650646209717, 12.965189695358276, 13.657857418060303, 14.34821343421936, 15.036642074584961, 15.724410772323608, 16.41627025604248, 17.10839295387268, 17.80010724067688, 18.49369192123413, 19.18375849723816, 19.874150276184082, 20.56966209411621, 21.26095962524414, 21.951547622680664, 22.716246604919434, 23.420524835586548, 24.110674142837524, 24.79989790916443, 25.49133610725403, 26.18179202079773, 26.873384714126587, 27.592703819274902, 28.289677381515503, 28.982749462127686, 29.673465728759766, 30.410863876342773, 31.149449825286865, 31.88341999053955, 32.619384765625, 33.30650544166565, 33.993913888931274, 34.684372901916504, 35.3749783039093, 36.06622934341431, 37.7829110622406]
[10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.05, 10.066666666666666, 10.066666666666666, 10.066666666666666, 10.066666666666666, 10.066666666666666, 10.083333333333334, 10.083333333333334, 10.083333333333334, 10.1, 10.1, 10.1, 10.1, 10.1, 10.1, 10.1, 10.116666666666667, 10.116666666666667, 10.116666666666667, 10.116666666666667, 10.116666666666667, 10.116666666666667, 10.116666666666667, 10.116666666666667, 10.116666666666667, 10.116666666666667, 10.116666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.274, Test loss: 2.271, Test accuracy: 19.77 

Round   0, Global train loss: 2.274, Global test loss: 2.295, Global test accuracy: 10.15 

Round   1, Train loss: 2.160, Test loss: 2.180, Test accuracy: 28.78 

Round   1, Global train loss: 2.160, Global test loss: 2.272, Global test accuracy: 20.55 

Round   2, Train loss: 1.985, Test loss: 2.044, Test accuracy: 46.43 

Round   2, Global train loss: 1.985, Global test loss: 2.234, Global test accuracy: 30.05 

Round   3, Train loss: 1.848, Test loss: 1.960, Test accuracy: 53.55 

Round   3, Global train loss: 1.848, Global test loss: 2.212, Global test accuracy: 24.70 

Round   4, Train loss: 1.759, Test loss: 1.890, Test accuracy: 59.83 

Round   4, Global train loss: 1.759, Global test loss: 2.244, Global test accuracy: 18.05 

Round   5, Train loss: 1.712, Test loss: 1.781, Test accuracy: 71.27 

Round   5, Global train loss: 1.712, Global test loss: 2.182, Global test accuracy: 26.45 

Round   6, Train loss: 1.661, Test loss: 1.748, Test accuracy: 75.25 

Round   6, Global train loss: 1.661, Global test loss: 2.138, Global test accuracy: 38.22 

Round   7, Train loss: 1.607, Test loss: 1.700, Test accuracy: 79.80 

Round   7, Global train loss: 1.607, Global test loss: 2.150, Global test accuracy: 43.70 

Round   8, Train loss: 1.711, Test loss: 1.659, Test accuracy: 83.20 

Round   8, Global train loss: 1.711, Global test loss: 2.174, Global test accuracy: 35.93 

Round   9, Train loss: 1.659, Test loss: 1.643, Test accuracy: 83.95 

Round   9, Global train loss: 1.659, Global test loss: 2.130, Global test accuracy: 38.30 

Round  10, Train loss: 1.594, Test loss: 1.638, Test accuracy: 84.17 

Round  10, Global train loss: 1.594, Global test loss: 2.172, Global test accuracy: 32.12 

Round  11, Train loss: 1.645, Test loss: 1.635, Test accuracy: 84.28 

Round  11, Global train loss: 1.645, Global test loss: 2.238, Global test accuracy: 21.25 

Round  12, Train loss: 1.610, Test loss: 1.624, Test accuracy: 85.73 

Round  12, Global train loss: 1.610, Global test loss: 2.198, Global test accuracy: 21.45 

Round  13, Train loss: 1.655, Test loss: 1.617, Test accuracy: 85.92 

Round  13, Global train loss: 1.655, Global test loss: 2.202, Global test accuracy: 19.80 

Round  14, Train loss: 1.580, Test loss: 1.617, Test accuracy: 85.88 

Round  14, Global train loss: 1.580, Global test loss: 2.152, Global test accuracy: 32.70 

Round  15, Train loss: 1.528, Test loss: 1.613, Test accuracy: 86.02 

Round  15, Global train loss: 1.528, Global test loss: 2.171, Global test accuracy: 33.10 

Round  16, Train loss: 1.597, Test loss: 1.602, Test accuracy: 87.32 

Round  16, Global train loss: 1.597, Global test loss: 2.158, Global test accuracy: 42.83 

Round  17, Train loss: 1.476, Test loss: 1.600, Test accuracy: 87.43 

Round  17, Global train loss: 1.476, Global test loss: 2.154, Global test accuracy: 29.73 

Round  18, Train loss: 1.579, Test loss: 1.600, Test accuracy: 87.27 

Round  18, Global train loss: 1.579, Global test loss: 2.158, Global test accuracy: 29.43 

Round  19, Train loss: 1.529, Test loss: 1.599, Test accuracy: 87.40 

Round  19, Global train loss: 1.529, Global test loss: 2.178, Global test accuracy: 23.62 

Round  20, Train loss: 1.590, Test loss: 1.593, Test accuracy: 87.75 

Round  20, Global train loss: 1.590, Global test loss: 2.157, Global test accuracy: 32.25 

Round  21, Train loss: 1.579, Test loss: 1.591, Test accuracy: 87.97 

Round  21, Global train loss: 1.579, Global test loss: 2.237, Global test accuracy: 21.05 

Round  22, Train loss: 1.579, Test loss: 1.589, Test accuracy: 88.00 

Round  22, Global train loss: 1.579, Global test loss: 2.170, Global test accuracy: 29.30 

Round  23, Train loss: 1.683, Test loss: 1.589, Test accuracy: 87.98 

Round  23, Global train loss: 1.683, Global test loss: 2.149, Global test accuracy: 35.68 

Round  24, Train loss: 1.576, Test loss: 1.589, Test accuracy: 88.02 

Round  24, Global train loss: 1.576, Global test loss: 2.142, Global test accuracy: 47.52 

Round  25, Train loss: 1.469, Test loss: 1.589, Test accuracy: 88.02 

Round  25, Global train loss: 1.469, Global test loss: 2.179, Global test accuracy: 27.68 

Round  26, Train loss: 1.522, Test loss: 1.588, Test accuracy: 88.02 

Round  26, Global train loss: 1.522, Global test loss: 2.187, Global test accuracy: 27.67 

Round  27, Train loss: 1.575, Test loss: 1.587, Test accuracy: 88.02 

Round  27, Global train loss: 1.575, Global test loss: 2.145, Global test accuracy: 44.25 

Round  28, Train loss: 1.466, Test loss: 1.587, Test accuracy: 88.08 

Round  28, Global train loss: 1.466, Global test loss: 2.148, Global test accuracy: 35.43 

Round  29, Train loss: 1.522, Test loss: 1.586, Test accuracy: 88.07 

Round  29, Global train loss: 1.522, Global test loss: 2.100, Global test accuracy: 42.05 

Round  30, Train loss: 1.575, Test loss: 1.586, Test accuracy: 88.08 

Round  30, Global train loss: 1.575, Global test loss: 2.170, Global test accuracy: 45.22 

Round  31, Train loss: 1.574, Test loss: 1.586, Test accuracy: 88.07 

Round  31, Global train loss: 1.574, Global test loss: 2.226, Global test accuracy: 22.98 

Round  32, Train loss: 1.468, Test loss: 1.585, Test accuracy: 88.07 

Round  32, Global train loss: 1.468, Global test loss: 2.131, Global test accuracy: 43.07 

Round  33, Train loss: 1.628, Test loss: 1.585, Test accuracy: 88.03 

Round  33, Global train loss: 1.628, Global test loss: 2.147, Global test accuracy: 28.50 

Round  34, Train loss: 1.573, Test loss: 1.585, Test accuracy: 88.05 

Round  34, Global train loss: 1.573, Global test loss: 2.081, Global test accuracy: 39.97 

Round  35, Train loss: 1.628, Test loss: 1.585, Test accuracy: 88.07 

Round  35, Global train loss: 1.628, Global test loss: 2.185, Global test accuracy: 21.47 

Round  36, Train loss: 1.573, Test loss: 1.585, Test accuracy: 88.07 

Round  36, Global train loss: 1.573, Global test loss: 2.096, Global test accuracy: 38.23 

Round  37, Train loss: 1.627, Test loss: 1.584, Test accuracy: 88.07 

Round  37, Global train loss: 1.627, Global test loss: 2.168, Global test accuracy: 29.23 

Round  38, Train loss: 1.573, Test loss: 1.584, Test accuracy: 88.08 

Round  38, Global train loss: 1.573, Global test loss: 2.115, Global test accuracy: 46.48 

Round  39, Train loss: 1.520, Test loss: 1.584, Test accuracy: 88.07 

Round  39, Global train loss: 1.520, Global test loss: 2.168, Global test accuracy: 24.30 

Round  40, Train loss: 1.519, Test loss: 1.584, Test accuracy: 88.07 

Round  40, Global train loss: 1.519, Global test loss: 2.186, Global test accuracy: 25.23 

Round  41, Train loss: 1.466, Test loss: 1.584, Test accuracy: 88.07 

Round  41, Global train loss: 1.466, Global test loss: 2.176, Global test accuracy: 30.07 

Round  42, Train loss: 1.518, Test loss: 1.584, Test accuracy: 88.07 

Round  42, Global train loss: 1.518, Global test loss: 2.161, Global test accuracy: 30.07 

Round  43, Train loss: 1.520, Test loss: 1.584, Test accuracy: 88.03 

Round  43, Global train loss: 1.520, Global test loss: 2.177, Global test accuracy: 28.32 

Round  44, Train loss: 1.628, Test loss: 1.584, Test accuracy: 88.03 

Round  44, Global train loss: 1.628, Global test loss: 2.185, Global test accuracy: 27.33 

Round  45, Train loss: 1.627, Test loss: 1.583, Test accuracy: 88.07 

Round  45, Global train loss: 1.627, Global test loss: 2.224, Global test accuracy: 21.22 

Round  46, Train loss: 1.565, Test loss: 1.574, Test accuracy: 89.58 

Round  46, Global train loss: 1.565, Global test loss: 2.209, Global test accuracy: 20.50 

Round  47, Train loss: 1.521, Test loss: 1.574, Test accuracy: 89.57 

Round  47, Global train loss: 1.521, Global test loss: 2.154, Global test accuracy: 34.98 

Round  48, Train loss: 1.472, Test loss: 1.569, Test accuracy: 89.62 

Round  48, Global train loss: 1.472, Global test loss: 2.132, Global test accuracy: 33.38 

Round  49, Train loss: 1.521, Test loss: 1.569, Test accuracy: 89.65 

Round  49, Global train loss: 1.521, Global test loss: 2.159, Global test accuracy: 30.98 

Final Round, Train loss: 1.530, Test loss: 1.568, Test accuracy: 89.67 

Final Round, Global train loss: 1.530, Global test loss: 2.159, Global test accuracy: 30.98 

Average accuracy final 10 rounds: 88.67500000000001 

Average global accuracy final 10 rounds: 28.208333333333336 

369.3407664299011
[1.5645332336425781, 2.194075107574463, 2.825871467590332, 3.4516117572784424, 4.082523584365845, 4.711514711380005, 5.341128587722778, 5.966731548309326, 6.593020677566528, 7.217777967453003, 7.845570087432861, 8.47163462638855, 9.098026752471924, 9.724764108657837, 10.351420640945435, 10.980981349945068, 11.609743118286133, 12.236037969589233, 12.862110376358032, 13.486799001693726, 14.110618591308594, 14.735095739364624, 15.360011577606201, 15.98947286605835, 16.61934494972229, 17.25031566619873, 17.882753372192383, 18.502753019332886, 19.125713109970093, 19.75050973892212, 20.37083387374878, 20.994356155395508, 21.597580194473267, 22.250438451766968, 22.918283700942993, 23.66924238204956, 24.41823673248291, 25.156712532043457, 25.760834217071533, 26.39401388168335, 27.02060627937317, 27.651944160461426, 28.294913053512573, 28.924472093582153, 29.553926706314087, 30.186365365982056, 30.82155132293701, 31.457545518875122, 32.08780813217163, 32.719562292099, 33.97936677932739]
[19.766666666666666, 28.783333333333335, 46.43333333333333, 53.55, 59.833333333333336, 71.26666666666667, 75.25, 79.8, 83.2, 83.95, 84.16666666666667, 84.28333333333333, 85.73333333333333, 85.91666666666667, 85.88333333333334, 86.01666666666667, 87.31666666666666, 87.43333333333334, 87.26666666666667, 87.4, 87.75, 87.96666666666667, 88.0, 87.98333333333333, 88.01666666666667, 88.01666666666667, 88.01666666666667, 88.01666666666667, 88.08333333333333, 88.06666666666666, 88.08333333333333, 88.06666666666666, 88.06666666666666, 88.03333333333333, 88.05, 88.06666666666666, 88.06666666666666, 88.06666666666666, 88.08333333333333, 88.06666666666666, 88.06666666666666, 88.06666666666666, 88.06666666666666, 88.03333333333333, 88.03333333333333, 88.06666666666666, 89.58333333333333, 89.56666666666666, 89.61666666666666, 89.65, 89.66666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 14.23 

Round   0, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 14.48 

Round   1, Train loss: 2.299, Test loss: 2.298, Test accuracy: 19.97 

Round   1, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 22.70 

Round   2, Train loss: 2.296, Test loss: 2.295, Test accuracy: 23.48 

Round   2, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 28.00 

Round   3, Train loss: 2.286, Test loss: 2.289, Test accuracy: 22.25 

Round   3, Global train loss: 2.286, Global test loss: 2.282, Global test accuracy: 20.70 

Round   4, Train loss: 2.264, Test loss: 2.270, Test accuracy: 23.03 

Round   4, Global train loss: 2.264, Global test loss: 2.247, Global test accuracy: 22.12 

Round   5, Train loss: 2.225, Test loss: 2.257, Test accuracy: 27.85 

Round   5, Global train loss: 2.225, Global test loss: 2.213, Global test accuracy: 35.95 

Round   6, Train loss: 2.187, Test loss: 2.226, Test accuracy: 31.63 

Round   6, Global train loss: 2.187, Global test loss: 2.155, Global test accuracy: 41.68 

Round   7, Train loss: 2.099, Test loss: 2.160, Test accuracy: 36.67 

Round   7, Global train loss: 2.099, Global test loss: 2.031, Global test accuracy: 48.35 

Round   8, Train loss: 1.968, Test loss: 2.093, Test accuracy: 46.05 

Round   8, Global train loss: 1.968, Global test loss: 1.906, Global test accuracy: 67.25 

Round   9, Train loss: 1.844, Test loss: 2.050, Test accuracy: 49.47 

Round   9, Global train loss: 1.844, Global test loss: 1.803, Global test accuracy: 71.83 

Round  10, Train loss: 1.780, Test loss: 1.997, Test accuracy: 53.28 

Round  10, Global train loss: 1.780, Global test loss: 1.764, Global test accuracy: 72.93 

Round  11, Train loss: 1.750, Test loss: 1.911, Test accuracy: 61.00 

Round  11, Global train loss: 1.750, Global test loss: 1.745, Global test accuracy: 73.75 

Round  12, Train loss: 1.742, Test loss: 1.853, Test accuracy: 65.47 

Round  12, Global train loss: 1.742, Global test loss: 1.718, Global test accuracy: 75.98 

Round  13, Train loss: 1.701, Test loss: 1.788, Test accuracy: 70.93 

Round  13, Global train loss: 1.701, Global test loss: 1.689, Global test accuracy: 80.70 

Round  14, Train loss: 1.669, Test loss: 1.741, Test accuracy: 75.15 

Round  14, Global train loss: 1.669, Global test loss: 1.672, Global test accuracy: 81.27 

Round  15, Train loss: 1.652, Test loss: 1.703, Test accuracy: 78.38 

Round  15, Global train loss: 1.652, Global test loss: 1.663, Global test accuracy: 81.53 

Round  16, Train loss: 1.653, Test loss: 1.693, Test accuracy: 79.18 

Round  16, Global train loss: 1.653, Global test loss: 1.654, Global test accuracy: 82.15 

Round  17, Train loss: 1.637, Test loss: 1.678, Test accuracy: 80.38 

Round  17, Global train loss: 1.637, Global test loss: 1.648, Global test accuracy: 82.43 

Round  18, Train loss: 1.621, Test loss: 1.659, Test accuracy: 82.10 

Round  18, Global train loss: 1.621, Global test loss: 1.626, Global test accuracy: 85.92 

Round  19, Train loss: 1.601, Test loss: 1.648, Test accuracy: 83.35 

Round  19, Global train loss: 1.601, Global test loss: 1.606, Global test accuracy: 87.53 

Round  20, Train loss: 1.581, Test loss: 1.638, Test accuracy: 84.25 

Round  20, Global train loss: 1.581, Global test loss: 1.598, Global test accuracy: 87.93 

Round  21, Train loss: 1.567, Test loss: 1.635, Test accuracy: 84.53 

Round  21, Global train loss: 1.567, Global test loss: 1.591, Global test accuracy: 88.40 

Round  22, Train loss: 1.562, Test loss: 1.624, Test accuracy: 85.58 

Round  22, Global train loss: 1.562, Global test loss: 1.587, Global test accuracy: 88.80 

Round  23, Train loss: 1.559, Test loss: 1.619, Test accuracy: 86.10 

Round  23, Global train loss: 1.559, Global test loss: 1.582, Global test accuracy: 88.85 

Round  24, Train loss: 1.562, Test loss: 1.609, Test accuracy: 87.02 

Round  24, Global train loss: 1.562, Global test loss: 1.577, Global test accuracy: 89.75 

Round  25, Train loss: 1.556, Test loss: 1.600, Test accuracy: 87.77 

Round  25, Global train loss: 1.556, Global test loss: 1.574, Global test accuracy: 89.55 

Round  26, Train loss: 1.556, Test loss: 1.593, Test accuracy: 88.37 

Round  26, Global train loss: 1.556, Global test loss: 1.569, Global test accuracy: 90.27 

Round  27, Train loss: 1.543, Test loss: 1.584, Test accuracy: 89.02 

Round  27, Global train loss: 1.543, Global test loss: 1.567, Global test accuracy: 90.50 

Round  28, Train loss: 1.534, Test loss: 1.578, Test accuracy: 89.45 

Round  28, Global train loss: 1.534, Global test loss: 1.565, Global test accuracy: 90.65 

Round  29, Train loss: 1.538, Test loss: 1.578, Test accuracy: 89.37 

Round  29, Global train loss: 1.538, Global test loss: 1.563, Global test accuracy: 90.75 

Round  30, Train loss: 1.534, Test loss: 1.577, Test accuracy: 89.58 

Round  30, Global train loss: 1.534, Global test loss: 1.564, Global test accuracy: 90.83 

Round  31, Train loss: 1.539, Test loss: 1.575, Test accuracy: 89.70 

Round  31, Global train loss: 1.539, Global test loss: 1.560, Global test accuracy: 90.83 

Round  32, Train loss: 1.527, Test loss: 1.572, Test accuracy: 89.90 

Round  32, Global train loss: 1.527, Global test loss: 1.559, Global test accuracy: 91.03 

Round  33, Train loss: 1.524, Test loss: 1.571, Test accuracy: 89.93 

Round  33, Global train loss: 1.524, Global test loss: 1.558, Global test accuracy: 91.15 

Round  34, Train loss: 1.533, Test loss: 1.568, Test accuracy: 90.32 

Round  34, Global train loss: 1.533, Global test loss: 1.557, Global test accuracy: 91.10 

Round  35, Train loss: 1.525, Test loss: 1.566, Test accuracy: 90.48 

Round  35, Global train loss: 1.525, Global test loss: 1.557, Global test accuracy: 91.05 

Round  36, Train loss: 1.515, Test loss: 1.565, Test accuracy: 90.53 

Round  36, Global train loss: 1.515, Global test loss: 1.554, Global test accuracy: 91.35 

Round  37, Train loss: 1.525, Test loss: 1.564, Test accuracy: 90.52 

Round  37, Global train loss: 1.525, Global test loss: 1.553, Global test accuracy: 91.22 

Round  38, Train loss: 1.522, Test loss: 1.564, Test accuracy: 90.42 

Round  38, Global train loss: 1.522, Global test loss: 1.554, Global test accuracy: 90.98 

Round  39, Train loss: 1.513, Test loss: 1.563, Test accuracy: 90.53 

Round  39, Global train loss: 1.513, Global test loss: 1.552, Global test accuracy: 91.50 

Round  40, Train loss: 1.516, Test loss: 1.562, Test accuracy: 90.48 

Round  40, Global train loss: 1.516, Global test loss: 1.550, Global test accuracy: 91.67 

Round  41, Train loss: 1.518, Test loss: 1.558, Test accuracy: 90.83 

Round  41, Global train loss: 1.518, Global test loss: 1.552, Global test accuracy: 91.48 

Round  42, Train loss: 1.516, Test loss: 1.559, Test accuracy: 90.75 

Round  42, Global train loss: 1.516, Global test loss: 1.553, Global test accuracy: 91.38 

Round  43, Train loss: 1.513, Test loss: 1.558, Test accuracy: 91.02 

Round  43, Global train loss: 1.513, Global test loss: 1.550, Global test accuracy: 91.68 

Round  44, Train loss: 1.513, Test loss: 1.557, Test accuracy: 90.95 

Round  44, Global train loss: 1.513, Global test loss: 1.548, Global test accuracy: 91.82 

Round  45, Train loss: 1.513, Test loss: 1.557, Test accuracy: 90.95 

Round  45, Global train loss: 1.513, Global test loss: 1.549, Global test accuracy: 91.68 

Round  46, Train loss: 1.505, Test loss: 1.556, Test accuracy: 91.02 

Round  46, Global train loss: 1.505, Global test loss: 1.547, Global test accuracy: 91.63 

Round  47, Train loss: 1.511, Test loss: 1.555, Test accuracy: 91.10 

Round  47, Global train loss: 1.511, Global test loss: 1.548, Global test accuracy: 91.63 

Round  48, Train loss: 1.512, Test loss: 1.554, Test accuracy: 91.13 

Round  48, Global train loss: 1.512, Global test loss: 1.547, Global test accuracy: 91.75 

Round  49, Train loss: 1.509, Test loss: 1.554, Test accuracy: 91.12 

Round  49, Global train loss: 1.509, Global test loss: 1.548, Global test accuracy: 91.78 

Final Round, Train loss: 1.503, Test loss: 1.552, Test accuracy: 91.55 

Final Round, Global train loss: 1.503, Global test loss: 1.548, Global test accuracy: 91.78 

Average accuracy final 10 rounds: 90.935 

Average global accuracy final 10 rounds: 91.65166666666664 

373.80225706100464
[1.581700086593628, 2.220644235610962, 2.8591015338897705, 3.4996626377105713, 4.165698528289795, 4.795930862426758, 5.450517892837524, 6.090123891830444, 6.724637031555176, 7.3686254024505615, 7.99994421005249, 8.634143114089966, 9.26748275756836, 9.90035080909729, 10.58058476448059, 11.21916937828064, 11.858593463897705, 12.491849899291992, 13.150350332260132, 13.858263492584229, 14.489881753921509, 15.125657796859741, 15.762604236602783, 16.39559507369995, 17.032029628753662, 17.711565256118774, 18.341770887374878, 18.97809910774231, 19.610427379608154, 20.242741107940674, 20.9226016998291, 21.6050705909729, 22.282695055007935, 22.96725630760193, 23.663681030273438, 24.341996431350708, 24.97907257080078, 25.6085102558136, 26.2394437789917, 26.86983633041382, 27.504184007644653, 28.17838978767395, 28.841093063354492, 29.479268550872803, 30.117483139038086, 30.760864734649658, 31.3965904712677, 32.036112785339355, 32.71361017227173, 33.34784507751465, 34.614160776138306]
[14.233333333333333, 19.966666666666665, 23.483333333333334, 22.25, 23.033333333333335, 27.85, 31.633333333333333, 36.666666666666664, 46.05, 49.46666666666667, 53.28333333333333, 61.0, 65.46666666666667, 70.93333333333334, 75.15, 78.38333333333334, 79.18333333333334, 80.38333333333334, 82.1, 83.35, 84.25, 84.53333333333333, 85.58333333333333, 86.1, 87.01666666666667, 87.76666666666667, 88.36666666666666, 89.01666666666667, 89.45, 89.36666666666666, 89.58333333333333, 89.7, 89.9, 89.93333333333334, 90.31666666666666, 90.48333333333333, 90.53333333333333, 90.51666666666667, 90.41666666666667, 90.53333333333333, 90.48333333333333, 90.83333333333333, 90.75, 91.01666666666667, 90.95, 90.95, 91.01666666666667, 91.1, 91.13333333333334, 91.11666666666666, 91.55]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.10 

Round   1, Train loss: 2.301, Test loss: 2.300, Test accuracy: 15.97 

Round   2, Train loss: 2.299, Test loss: 2.298, Test accuracy: 17.93 

Round   3, Train loss: 2.297, Test loss: 2.296, Test accuracy: 21.17 

Round   4, Train loss: 2.295, Test loss: 2.293, Test accuracy: 25.22 

Round   5, Train loss: 2.292, Test loss: 2.289, Test accuracy: 27.83 

Round   6, Train loss: 2.288, Test loss: 2.284, Test accuracy: 26.30 

Round   7, Train loss: 2.281, Test loss: 2.274, Test accuracy: 25.13 

Round   8, Train loss: 2.266, Test loss: 2.254, Test accuracy: 24.88 

Round   9, Train loss: 2.246, Test loss: 2.234, Test accuracy: 32.10 

Round  10, Train loss: 2.230, Test loss: 2.214, Test accuracy: 36.75 

Round  11, Train loss: 2.200, Test loss: 2.189, Test accuracy: 38.57 

Round  12, Train loss: 2.173, Test loss: 2.153, Test accuracy: 42.02 

Round  13, Train loss: 2.130, Test loss: 2.100, Test accuracy: 46.85 

Round  14, Train loss: 2.059, Test loss: 2.040, Test accuracy: 50.15 

Round  15, Train loss: 1.987, Test loss: 1.991, Test accuracy: 55.45 

Round  16, Train loss: 1.947, Test loss: 1.936, Test accuracy: 60.67 

Round  17, Train loss: 1.888, Test loss: 1.884, Test accuracy: 64.63 

Round  18, Train loss: 1.830, Test loss: 1.842, Test accuracy: 67.72 

Round  19, Train loss: 1.788, Test loss: 1.813, Test accuracy: 69.58 

Round  20, Train loss: 1.777, Test loss: 1.791, Test accuracy: 70.72 

Round  21, Train loss: 1.744, Test loss: 1.773, Test accuracy: 72.05 

Round  22, Train loss: 1.736, Test loss: 1.764, Test accuracy: 72.77 

Round  23, Train loss: 1.713, Test loss: 1.751, Test accuracy: 73.63 

Round  24, Train loss: 1.715, Test loss: 1.741, Test accuracy: 74.87 

Round  25, Train loss: 1.719, Test loss: 1.728, Test accuracy: 75.77 

Round  26, Train loss: 1.695, Test loss: 1.710, Test accuracy: 77.63 

Round  27, Train loss: 1.667, Test loss: 1.702, Test accuracy: 78.00 

Round  28, Train loss: 1.653, Test loss: 1.698, Test accuracy: 78.25 

Round  29, Train loss: 1.646, Test loss: 1.685, Test accuracy: 79.77 

Round  30, Train loss: 1.661, Test loss: 1.680, Test accuracy: 80.20 

Round  31, Train loss: 1.634, Test loss: 1.669, Test accuracy: 81.43 

Round  32, Train loss: 1.612, Test loss: 1.662, Test accuracy: 81.97 

Round  33, Train loss: 1.602, Test loss: 1.655, Test accuracy: 82.47 

Round  34, Train loss: 1.614, Test loss: 1.647, Test accuracy: 83.37 

Round  35, Train loss: 1.608, Test loss: 1.643, Test accuracy: 83.77 

Round  36, Train loss: 1.593, Test loss: 1.635, Test accuracy: 84.77 

Round  37, Train loss: 1.584, Test loss: 1.627, Test accuracy: 85.42 

Round  38, Train loss: 1.590, Test loss: 1.627, Test accuracy: 85.35 

Round  39, Train loss: 1.563, Test loss: 1.623, Test accuracy: 85.32 

Round  40, Train loss: 1.572, Test loss: 1.622, Test accuracy: 85.87 

Round  41, Train loss: 1.577, Test loss: 1.615, Test accuracy: 86.42 

Round  42, Train loss: 1.565, Test loss: 1.613, Test accuracy: 86.42 

Round  43, Train loss: 1.572, Test loss: 1.606, Test accuracy: 87.10 

Round  44, Train loss: 1.563, Test loss: 1.606, Test accuracy: 87.12 

Round  45, Train loss: 1.574, Test loss: 1.605, Test accuracy: 87.10 

Round  46, Train loss: 1.569, Test loss: 1.602, Test accuracy: 87.15 

Round  47, Train loss: 1.559, Test loss: 1.603, Test accuracy: 87.22 

Round  48, Train loss: 1.559, Test loss: 1.599, Test accuracy: 87.77 

Round  49, Train loss: 1.559, Test loss: 1.599, Test accuracy: 87.53 

Final Round, Train loss: 1.553, Test loss: 1.593, Test accuracy: 88.03 

Average accuracy final 10 rounds: 86.96833333333333 

288.4227845668793
[1.6258609294891357, 2.3088531494140625, 2.9957034587860107, 3.6785476207733154, 4.362847805023193, 5.044358491897583, 5.727261066436768, 6.408190727233887, 7.092851638793945, 7.778221368789673, 8.458863735198975, 9.139619588851929, 9.823301076889038, 10.50825309753418, 11.126765489578247, 11.74636459350586, 12.3921639919281, 13.010443449020386, 13.595698833465576, 14.183040618896484, 14.766680479049683, 15.348232507705688, 15.932855606079102, 16.51810073852539, 17.103537797927856, 17.68522024154663, 18.267229080200195, 18.84983730316162, 19.43614387512207, 20.019732236862183, 20.602083206176758, 21.183361530303955, 21.768892765045166, 22.354267358779907, 22.941197395324707, 23.52619194984436, 24.110226154327393, 24.695313692092896, 25.27608847618103, 25.860768795013428, 26.4435396194458, 27.02583885192871, 27.60632085800171, 28.192566394805908, 28.769620895385742, 29.35157346725464, 29.926770448684692, 30.50404977798462, 31.07929539680481, 31.67259120941162, 32.673612117767334]
[13.1, 15.966666666666667, 17.933333333333334, 21.166666666666668, 25.216666666666665, 27.833333333333332, 26.3, 25.133333333333333, 24.883333333333333, 32.1, 36.75, 38.56666666666667, 42.016666666666666, 46.85, 50.15, 55.45, 60.666666666666664, 64.63333333333334, 67.71666666666667, 69.58333333333333, 70.71666666666667, 72.05, 72.76666666666667, 73.63333333333334, 74.86666666666666, 75.76666666666667, 77.63333333333334, 78.0, 78.25, 79.76666666666667, 80.2, 81.43333333333334, 81.96666666666667, 82.46666666666667, 83.36666666666666, 83.76666666666667, 84.76666666666667, 85.41666666666667, 85.35, 85.31666666666666, 85.86666666666666, 86.41666666666667, 86.41666666666667, 87.1, 87.11666666666666, 87.1, 87.15, 87.21666666666667, 87.76666666666667, 87.53333333333333, 88.03333333333333]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.301, Test accuracy: 10.03
Round   1, Train loss: 2.301, Test loss: 2.300, Test accuracy: 12.35
Round   2, Train loss: 2.299, Test loss: 2.297, Test accuracy: 17.33
Round   3, Train loss: 2.296, Test loss: 2.295, Test accuracy: 20.87
Round   4, Train loss: 2.293, Test loss: 2.291, Test accuracy: 22.17
Round   5, Train loss: 2.290, Test loss: 2.286, Test accuracy: 23.77
Round   6, Train loss: 2.285, Test loss: 2.278, Test accuracy: 25.18
Round   7, Train loss: 2.271, Test loss: 2.261, Test accuracy: 24.72
Round   8, Train loss: 2.253, Test loss: 2.243, Test accuracy: 30.72
Round   9, Train loss: 2.230, Test loss: 2.225, Test accuracy: 32.85
Round  10, Train loss: 2.218, Test loss: 2.203, Test accuracy: 33.92
Round  11, Train loss: 2.184, Test loss: 2.177, Test accuracy: 36.85
Round  12, Train loss: 2.174, Test loss: 2.145, Test accuracy: 39.53
Round  13, Train loss: 2.134, Test loss: 2.105, Test accuracy: 43.07
Round  14, Train loss: 2.084, Test loss: 2.062, Test accuracy: 51.70
Round  15, Train loss: 2.025, Test loss: 2.014, Test accuracy: 56.65
Round  16, Train loss: 1.991, Test loss: 1.957, Test accuracy: 63.75
Round  17, Train loss: 1.926, Test loss: 1.906, Test accuracy: 67.73
Round  18, Train loss: 1.864, Test loss: 1.869, Test accuracy: 69.88
Round  19, Train loss: 1.865, Test loss: 1.832, Test accuracy: 71.97
Round  20, Train loss: 1.833, Test loss: 1.808, Test accuracy: 73.03
Round  21, Train loss: 1.822, Test loss: 1.783, Test accuracy: 74.50
Round  22, Train loss: 1.794, Test loss: 1.770, Test accuracy: 76.08
Round  23, Train loss: 1.771, Test loss: 1.758, Test accuracy: 77.23
Round  24, Train loss: 1.753, Test loss: 1.740, Test accuracy: 78.67
Round  25, Train loss: 1.767, Test loss: 1.728, Test accuracy: 79.60
Round  26, Train loss: 1.723, Test loss: 1.721, Test accuracy: 80.27
Round  27, Train loss: 1.740, Test loss: 1.711, Test accuracy: 80.75
Round  28, Train loss: 1.705, Test loss: 1.705, Test accuracy: 81.33
Round  29, Train loss: 1.714, Test loss: 1.697, Test accuracy: 81.63
Round  30, Train loss: 1.703, Test loss: 1.693, Test accuracy: 81.90
Round  31, Train loss: 1.703, Test loss: 1.690, Test accuracy: 81.98
Round  32, Train loss: 1.679, Test loss: 1.687, Test accuracy: 81.97
Round  33, Train loss: 1.683, Test loss: 1.684, Test accuracy: 82.25
Round  34, Train loss: 1.699, Test loss: 1.679, Test accuracy: 82.82
Round  35, Train loss: 1.680, Test loss: 1.674, Test accuracy: 83.23
Round  36, Train loss: 1.675, Test loss: 1.669, Test accuracy: 83.65
Round  37, Train loss: 1.661, Test loss: 1.665, Test accuracy: 84.10
Round  38, Train loss: 1.665, Test loss: 1.659, Test accuracy: 85.02
Round  39, Train loss: 1.651, Test loss: 1.656, Test accuracy: 85.10
Round  40, Train loss: 1.645, Test loss: 1.651, Test accuracy: 85.77
Round  41, Train loss: 1.634, Test loss: 1.642, Test accuracy: 86.87
Round  42, Train loss: 1.633, Test loss: 1.632, Test accuracy: 88.17
Round  43, Train loss: 1.620, Test loss: 1.620, Test accuracy: 89.85
Round  44, Train loss: 1.617, Test loss: 1.614, Test accuracy: 90.07
Round  45, Train loss: 1.608, Test loss: 1.610, Test accuracy: 90.03
Round  46, Train loss: 1.613, Test loss: 1.604, Test accuracy: 90.48
Round  47, Train loss: 1.596, Test loss: 1.602, Test accuracy: 90.72
Round  48, Train loss: 1.595, Test loss: 1.599, Test accuracy: 90.72
Round  49, Train loss: 1.594, Test loss: 1.595, Test accuracy: 91.12
Final Round, Train loss: 1.567, Test loss: 1.582, Test accuracy: 91.48
Average accuracy final 10 rounds: 89.37833333333334
307.34650778770447
[1.6318745613098145, 2.3329083919525146, 3.030003070831299, 3.7278268337249756, 4.430739164352417, 5.148617506027222, 5.843322992324829, 6.548543930053711, 7.246924877166748, 7.946148157119751, 8.643449783325195, 9.33920669555664, 10.04190707206726, 10.744629383087158, 11.448158025741577, 12.15091848373413, 12.85364580154419, 13.55034327507019, 14.248879671096802, 14.94595193862915, 15.642127990722656, 16.347026824951172, 17.047396898269653, 17.747920989990234, 18.456101655960083, 19.158668756484985, 19.856181859970093, 20.55677032470703, 21.252721071243286, 21.95338010787964, 22.652240753173828, 23.354352951049805, 24.055200576782227, 24.75409436225891, 25.454435348510742, 26.14873433113098, 26.85311007499695, 27.552649974822998, 28.25625729560852, 28.95528793334961, 29.65560793876648, 30.34960651397705, 31.045265197753906, 31.790698766708374, 32.48245978355408, 33.1851851940155, 33.94813919067383, 34.651551961898804, 35.371973514556885, 36.07064461708069, 37.11805558204651]
[10.033333333333333, 12.35, 17.333333333333332, 20.866666666666667, 22.166666666666668, 23.766666666666666, 25.183333333333334, 24.716666666666665, 30.716666666666665, 32.85, 33.916666666666664, 36.85, 39.53333333333333, 43.06666666666667, 51.7, 56.65, 63.75, 67.73333333333333, 69.88333333333334, 71.96666666666667, 73.03333333333333, 74.5, 76.08333333333333, 77.23333333333333, 78.66666666666667, 79.6, 80.26666666666667, 80.75, 81.33333333333333, 81.63333333333334, 81.9, 81.98333333333333, 81.96666666666667, 82.25, 82.81666666666666, 83.23333333333333, 83.65, 84.1, 85.01666666666667, 85.1, 85.76666666666667, 86.86666666666666, 88.16666666666667, 89.85, 90.06666666666666, 90.03333333333333, 90.48333333333333, 90.71666666666667, 90.71666666666667, 91.11666666666666, 91.48333333333333]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 18.33
Round   1, Train loss: 2.300, Test loss: 2.298, Test accuracy: 23.62
Round   2, Train loss: 2.297, Test loss: 2.294, Test accuracy: 28.23
Round   3, Train loss: 2.294, Test loss: 2.286, Test accuracy: 32.30
Round   4, Train loss: 2.288, Test loss: 2.268, Test accuracy: 28.82
Round   5, Train loss: 2.269, Test loss: 2.210, Test accuracy: 37.73
Round   6, Train loss: 2.247, Test loss: 2.115, Test accuracy: 43.77
Round   7, Train loss: 2.148, Test loss: 1.993, Test accuracy: 52.70
Round   8, Train loss: 2.057, Test loss: 1.904, Test accuracy: 61.02
Round   9, Train loss: 1.995, Test loss: 1.831, Test accuracy: 70.32
Round  10, Train loss: 1.913, Test loss: 1.772, Test accuracy: 72.68
Round  11, Train loss: 1.877, Test loss: 1.724, Test accuracy: 78.67
Round  12, Train loss: 1.756, Test loss: 1.694, Test accuracy: 80.38
Round  13, Train loss: 1.722, Test loss: 1.673, Test accuracy: 81.65
Round  14, Train loss: 1.711, Test loss: 1.663, Test accuracy: 81.55
Round  15, Train loss: 1.734, Test loss: 1.652, Test accuracy: 82.55
Round  16, Train loss: 1.645, Test loss: 1.646, Test accuracy: 82.83
Round  17, Train loss: 1.661, Test loss: 1.641, Test accuracy: 83.23
Round  18, Train loss: 1.640, Test loss: 1.636, Test accuracy: 83.70
Round  19, Train loss: 1.625, Test loss: 1.633, Test accuracy: 83.63
Round  20, Train loss: 1.637, Test loss: 1.631, Test accuracy: 83.97
Round  21, Train loss: 1.616, Test loss: 1.629, Test accuracy: 83.97
Round  22, Train loss: 1.636, Test loss: 1.628, Test accuracy: 84.08
Round  23, Train loss: 1.609, Test loss: 1.626, Test accuracy: 84.27
Round  24, Train loss: 1.619, Test loss: 1.623, Test accuracy: 84.45
Round  25, Train loss: 1.603, Test loss: 1.624, Test accuracy: 84.48
Round  26, Train loss: 1.612, Test loss: 1.621, Test accuracy: 84.43
Round  27, Train loss: 1.602, Test loss: 1.620, Test accuracy: 84.75
Round  28, Train loss: 1.600, Test loss: 1.621, Test accuracy: 84.40
Round  29, Train loss: 1.603, Test loss: 1.620, Test accuracy: 84.57
Round  30, Train loss: 1.610, Test loss: 1.617, Test accuracy: 84.78
Round  31, Train loss: 1.601, Test loss: 1.617, Test accuracy: 84.78
Round  32, Train loss: 1.590, Test loss: 1.615, Test accuracy: 85.03
Round  33, Train loss: 1.602, Test loss: 1.614, Test accuracy: 85.07
Round  34, Train loss: 1.591, Test loss: 1.615, Test accuracy: 84.95
Round  35, Train loss: 1.604, Test loss: 1.615, Test accuracy: 84.87
Round  36, Train loss: 1.587, Test loss: 1.612, Test accuracy: 85.17
Round  37, Train loss: 1.589, Test loss: 1.613, Test accuracy: 85.13
Round  38, Train loss: 1.585, Test loss: 1.615, Test accuracy: 84.93
Round  39, Train loss: 1.586, Test loss: 1.613, Test accuracy: 85.00
Round  40, Train loss: 1.586, Test loss: 1.612, Test accuracy: 85.22
Round  41, Train loss: 1.593, Test loss: 1.612, Test accuracy: 85.13
Round  42, Train loss: 1.595, Test loss: 1.613, Test accuracy: 85.02
Round  43, Train loss: 1.573, Test loss: 1.611, Test accuracy: 85.08
Round  44, Train loss: 1.582, Test loss: 1.612, Test accuracy: 85.23
Round  45, Train loss: 1.593, Test loss: 1.612, Test accuracy: 85.22
Round  46, Train loss: 1.592, Test loss: 1.611, Test accuracy: 85.20
Round  47, Train loss: 1.594, Test loss: 1.611, Test accuracy: 85.18
Round  48, Train loss: 1.578, Test loss: 1.609, Test accuracy: 85.32
Round  49, Train loss: 1.581, Test loss: 1.611, Test accuracy: 85.13
Final Round, Train loss: 1.584, Test loss: 1.609, Test accuracy: 85.20
Average accuracy final 10 rounds: 85.17333333333333
600.2909593582153
[2.843095541000366, 4.757383584976196, 6.673701286315918, 8.309474229812622, 9.93907880783081, 11.568674325942993, 13.197118997573853, 14.828606843948364, 16.460721015930176, 18.097663402557373, 19.732914447784424, 21.364748001098633, 22.994823217391968, 24.62091875076294, 26.253464460372925, 27.88141655921936, 29.511242866516113, 31.25420045852661, 32.8878915309906, 34.51706409454346, 36.15080213546753, 37.786439180374146, 39.41263031959534, 41.041597843170166, 42.67025947570801, 44.31580471992493, 45.95143723487854, 47.6841402053833, 49.406306743621826, 51.13518452644348, 52.76619291305542, 54.390204191207886, 56.03372526168823, 57.666218280792236, 59.31998014450073, 60.935791969299316, 62.565712690353394, 64.18951320648193, 65.81351399421692, 67.43064498901367, 69.05126571655273, 70.67050313949585, 72.29273867607117, 73.91080570220947, 75.53660011291504, 77.15816283226013, 78.79051542282104, 80.41174650192261, 82.0334119796753, 83.65788054466248, 85.42041563987732]
[18.333333333333332, 23.616666666666667, 28.233333333333334, 32.3, 28.816666666666666, 37.733333333333334, 43.766666666666666, 52.7, 61.016666666666666, 70.31666666666666, 72.68333333333334, 78.66666666666667, 80.38333333333334, 81.65, 81.55, 82.55, 82.83333333333333, 83.23333333333333, 83.7, 83.63333333333334, 83.96666666666667, 83.96666666666667, 84.08333333333333, 84.26666666666667, 84.45, 84.48333333333333, 84.43333333333334, 84.75, 84.4, 84.56666666666666, 84.78333333333333, 84.78333333333333, 85.03333333333333, 85.06666666666666, 84.95, 84.86666666666666, 85.16666666666667, 85.13333333333334, 84.93333333333334, 85.0, 85.21666666666667, 85.13333333333334, 85.01666666666667, 85.08333333333333, 85.23333333333333, 85.21666666666667, 85.2, 85.18333333333334, 85.31666666666666, 85.13333333333334, 85.2]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.715, Test loss: 2.298, Test accuracy: 22.38
Round   1, Train loss: 1.675, Test loss: 2.289, Test accuracy: 34.95
Round   2, Train loss: 1.600, Test loss: 2.268, Test accuracy: 44.93
Round   3, Train loss: 1.493, Test loss: 2.229, Test accuracy: 48.73
Round   4, Train loss: 1.418, Test loss: 2.184, Test accuracy: 54.37
Round   5, Train loss: 1.376, Test loss: 2.152, Test accuracy: 59.97
Round   6, Train loss: 1.307, Test loss: 2.118, Test accuracy: 65.27
Round   7, Train loss: 1.260, Test loss: 2.085, Test accuracy: 69.35
Round   8, Train loss: 1.232, Test loss: 2.063, Test accuracy: 72.70
Round   9, Train loss: 1.233, Test loss: 2.048, Test accuracy: 74.60
Round  10, Train loss: 1.224, Test loss: 2.032, Test accuracy: 78.00
Round  11, Train loss: 1.201, Test loss: 2.020, Test accuracy: 78.60
Round  12, Train loss: 1.190, Test loss: 2.012, Test accuracy: 78.97
Round  13, Train loss: 1.200, Test loss: 2.005, Test accuracy: 79.10
Round  14, Train loss: 1.197, Test loss: 2.001, Test accuracy: 78.93
Round  15, Train loss: 1.186, Test loss: 1.995, Test accuracy: 78.67
Round  16, Train loss: 1.198, Test loss: 1.990, Test accuracy: 78.85
Round  17, Train loss: 1.186, Test loss: 1.986, Test accuracy: 78.98
Round  18, Train loss: 1.185, Test loss: 1.982, Test accuracy: 78.65
Round  19, Train loss: 1.194, Test loss: 1.980, Test accuracy: 78.02
Round  20, Train loss: 1.180, Test loss: 1.979, Test accuracy: 77.72
Round  21, Train loss: 1.186, Test loss: 1.979, Test accuracy: 77.20
Round  22, Train loss: 1.176, Test loss: 1.976, Test accuracy: 77.25
Round  23, Train loss: 1.187, Test loss: 1.974, Test accuracy: 76.93
Round  24, Train loss: 1.176, Test loss: 1.973, Test accuracy: 76.82
Round  25, Train loss: 1.191, Test loss: 1.972, Test accuracy: 76.57
Round  26, Train loss: 1.192, Test loss: 1.971, Test accuracy: 76.18
Round  27, Train loss: 1.185, Test loss: 1.970, Test accuracy: 76.00
Round  28, Train loss: 1.182, Test loss: 1.970, Test accuracy: 75.95
Round  29, Train loss: 1.188, Test loss: 1.968, Test accuracy: 75.47
Round  30, Train loss: 1.181, Test loss: 1.967, Test accuracy: 75.48
Round  31, Train loss: 1.178, Test loss: 1.967, Test accuracy: 75.05
Round  32, Train loss: 1.178, Test loss: 1.967, Test accuracy: 74.80
Round  33, Train loss: 1.182, Test loss: 1.967, Test accuracy: 74.37
Round  34, Train loss: 1.183, Test loss: 1.967, Test accuracy: 74.38
Round  35, Train loss: 1.183, Test loss: 1.966, Test accuracy: 74.08
Round  36, Train loss: 1.182, Test loss: 1.965, Test accuracy: 74.05
Round  37, Train loss: 1.178, Test loss: 1.965, Test accuracy: 73.57
Round  38, Train loss: 1.174, Test loss: 1.965, Test accuracy: 73.60
Round  39, Train loss: 1.184, Test loss: 1.966, Test accuracy: 73.37
Round  40, Train loss: 1.180, Test loss: 1.966, Test accuracy: 73.02
Round  41, Train loss: 1.183, Test loss: 1.966, Test accuracy: 72.92
Round  42, Train loss: 1.182, Test loss: 1.966, Test accuracy: 72.50
Round  43, Train loss: 1.179, Test loss: 1.966, Test accuracy: 72.43
Round  44, Train loss: 1.176, Test loss: 1.965, Test accuracy: 72.20
Round  45, Train loss: 1.178, Test loss: 1.966, Test accuracy: 71.78
Round  46, Train loss: 1.179, Test loss: 1.966, Test accuracy: 71.63
Round  47, Train loss: 1.177, Test loss: 1.966, Test accuracy: 71.35
Round  48, Train loss: 1.181, Test loss: 1.967, Test accuracy: 71.20
Round  49, Train loss: 1.179, Test loss: 1.967, Test accuracy: 71.03
Final Round, Train loss: 1.179, Test loss: 1.967, Test accuracy: 70.45
Average accuracy final 10 rounds: 72.00666666666667
488.42622780799866
[]
[22.383333333333333, 34.95, 44.93333333333333, 48.733333333333334, 54.36666666666667, 59.96666666666667, 65.26666666666667, 69.35, 72.7, 74.6, 78.0, 78.6, 78.96666666666667, 79.1, 78.93333333333334, 78.66666666666667, 78.85, 78.98333333333333, 78.65, 78.01666666666667, 77.71666666666667, 77.2, 77.25, 76.93333333333334, 76.81666666666666, 76.56666666666666, 76.18333333333334, 76.0, 75.95, 75.46666666666667, 75.48333333333333, 75.05, 74.8, 74.36666666666666, 74.38333333333334, 74.08333333333333, 74.05, 73.56666666666666, 73.6, 73.36666666666666, 73.01666666666667, 72.91666666666667, 72.5, 72.43333333333334, 72.2, 71.78333333333333, 71.63333333333334, 71.35, 71.2, 71.03333333333333, 70.45]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 12.72
Round   0: Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 11.55
Round   1, Train loss: 2.306, Test loss: 2.301, Test accuracy: 16.67
Round   1: Global train loss: 2.306, Global test loss: 2.302, Global test accuracy: 12.57
Round   2, Train loss: 2.310, Test loss: 2.300, Test accuracy: 20.35
Round   2: Global train loss: 2.310, Global test loss: 2.302, Global test accuracy: 13.83
Round   3, Train loss: 2.306, Test loss: 2.300, Test accuracy: 21.97
Round   3: Global train loss: 2.306, Global test loss: 2.301, Global test accuracy: 14.85
Round   4, Train loss: 2.299, Test loss: 2.300, Test accuracy: 21.47
Round   4: Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 15.65
Round   5, Train loss: 2.308, Test loss: 2.299, Test accuracy: 22.10
Round   5: Global train loss: 2.308, Global test loss: 2.301, Global test accuracy: 16.73
Round   6, Train loss: 2.295, Test loss: 2.300, Test accuracy: 21.17
Round   6: Global train loss: 2.295, Global test loss: 2.301, Global test accuracy: 17.17
Round   7, Train loss: 2.295, Test loss: 2.299, Test accuracy: 22.33
Round   7: Global train loss: 2.295, Global test loss: 2.301, Global test accuracy: 18.03
Round   8, Train loss: 2.303, Test loss: 2.299, Test accuracy: 24.30
Round   8: Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 19.32
Round   9, Train loss: 2.296, Test loss: 2.299, Test accuracy: 24.42
Round   9: Global train loss: 2.296, Global test loss: 2.300, Global test accuracy: 20.53
Round  10, Train loss: 2.296, Test loss: 2.298, Test accuracy: 26.12
Round  10: Global train loss: 2.296, Global test loss: 2.300, Global test accuracy: 22.53
Round  11, Train loss: 2.291, Test loss: 2.295, Test accuracy: 31.50
Round  11: Global train loss: 2.291, Global test loss: 2.300, Global test accuracy: 26.20
Round  12, Train loss: 2.296, Test loss: 2.295, Test accuracy: 31.10
Round  12: Global train loss: 2.296, Global test loss: 2.299, Global test accuracy: 27.08
Round  13, Train loss: 2.277, Test loss: 2.293, Test accuracy: 31.50
Round  13: Global train loss: 2.277, Global test loss: 2.299, Global test accuracy: 28.37
Round  14, Train loss: 2.287, Test loss: 2.294, Test accuracy: 29.62
Round  14: Global train loss: 2.287, Global test loss: 2.299, Global test accuracy: 28.57
Round  15, Train loss: 2.325, Test loss: 2.296, Test accuracy: 27.27
Round  15: Global train loss: 2.325, Global test loss: 2.299, Global test accuracy: 28.17
Round  16, Train loss: 2.306, Test loss: 2.296, Test accuracy: 27.15
Round  16: Global train loss: 2.306, Global test loss: 2.299, Global test accuracy: 29.15
Round  17, Train loss: 2.331, Test loss: 2.299, Test accuracy: 21.53
Round  17: Global train loss: 2.331, Global test loss: 2.299, Global test accuracy: 28.52
Round  18, Train loss: 2.263, Test loss: 2.295, Test accuracy: 21.97
Round  18: Global train loss: 2.263, Global test loss: 2.299, Global test accuracy: 29.63
Round  19, Train loss: 2.300, Test loss: 2.295, Test accuracy: 24.98
Round  19: Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 28.98
Round  20, Train loss: 2.275, Test loss: 2.292, Test accuracy: 27.78
Round  20: Global train loss: 2.275, Global test loss: 2.298, Global test accuracy: 29.85
Round  21, Train loss: 2.196, Test loss: 2.287, Test accuracy: 27.10
Round  21: Global train loss: 2.196, Global test loss: 2.298, Global test accuracy: 31.77
Round  22, Train loss: 2.188, Test loss: 2.279, Test accuracy: 31.90
Round  22: Global train loss: 2.188, Global test loss: 2.296, Global test accuracy: 33.93
Round  23, Train loss: 2.337, Test loss: 2.287, Test accuracy: 28.75
Round  23: Global train loss: 2.337, Global test loss: 2.297, Global test accuracy: 33.63
Round  24, Train loss: 2.198, Test loss: 2.279, Test accuracy: 29.00
Round  24: Global train loss: 2.198, Global test loss: 2.296, Global test accuracy: 34.48
Round  25, Train loss: 2.051, Test loss: 2.263, Test accuracy: 31.40
Round  25: Global train loss: 2.051, Global test loss: 2.294, Global test accuracy: 36.48
Round  26, Train loss: 2.202, Test loss: 2.259, Test accuracy: 31.93
Round  26: Global train loss: 2.202, Global test loss: 2.293, Global test accuracy: 35.67
Round  27, Train loss: 1.998, Test loss: 2.245, Test accuracy: 32.12
Round  27: Global train loss: 1.998, Global test loss: 2.291, Global test accuracy: 36.23
Round  28, Train loss: 2.191, Test loss: 2.242, Test accuracy: 29.40
Round  28: Global train loss: 2.191, Global test loss: 2.290, Global test accuracy: 35.77
Round  29, Train loss: 2.273, Test loss: 2.261, Test accuracy: 27.58
Round  29: Global train loss: 2.273, Global test loss: 2.291, Global test accuracy: 34.87
Round  30, Train loss: 2.313, Test loss: 2.269, Test accuracy: 24.20
Round  30: Global train loss: 2.313, Global test loss: 2.292, Global test accuracy: 33.72
Round  31, Train loss: 2.270, Test loss: 2.265, Test accuracy: 23.33
Round  31: Global train loss: 2.270, Global test loss: 2.293, Global test accuracy: 34.28
Round  32, Train loss: 2.167, Test loss: 2.252, Test accuracy: 22.28
Round  32: Global train loss: 2.167, Global test loss: 2.292, Global test accuracy: 33.97
Round  33, Train loss: 1.672, Test loss: 2.223, Test accuracy: 25.77
Round  33: Global train loss: 1.672, Global test loss: 2.289, Global test accuracy: 35.75
Round  34, Train loss: 2.089, Test loss: 2.224, Test accuracy: 25.62
Round  34: Global train loss: 2.089, Global test loss: 2.289, Global test accuracy: 36.37
Round  35, Train loss: 1.938, Test loss: 2.221, Test accuracy: 25.53
Round  35: Global train loss: 1.938, Global test loss: 2.288, Global test accuracy: 36.13
Round  36, Train loss: 1.578, Test loss: 2.203, Test accuracy: 29.10
Round  36: Global train loss: 1.578, Global test loss: 2.286, Global test accuracy: 37.98
Round  37, Train loss: 2.049, Test loss: 2.201, Test accuracy: 28.42
Round  37: Global train loss: 2.049, Global test loss: 2.285, Global test accuracy: 37.62
Round  38, Train loss: 1.781, Test loss: 2.200, Test accuracy: 28.73
Round  38: Global train loss: 1.781, Global test loss: 2.284, Global test accuracy: 37.10
Round  39, Train loss: 2.329, Test loss: 2.239, Test accuracy: 23.42
Round  39: Global train loss: 2.329, Global test loss: 2.288, Global test accuracy: 35.58
Round  40, Train loss: 1.681, Test loss: 2.207, Test accuracy: 28.07
Round  40: Global train loss: 1.681, Global test loss: 2.285, Global test accuracy: 36.75
Round  41, Train loss: 2.081, Test loss: 2.218, Test accuracy: 27.23
Round  41: Global train loss: 2.081, Global test loss: 2.287, Global test accuracy: 36.22
Round  42, Train loss: 1.938, Test loss: 2.237, Test accuracy: 23.32
Round  42: Global train loss: 1.938, Global test loss: 2.290, Global test accuracy: 33.80
Round  43, Train loss: 1.941, Test loss: 2.215, Test accuracy: 26.47
Round  43: Global train loss: 1.941, Global test loss: 2.289, Global test accuracy: 35.53
Round  44, Train loss: 0.806, Test loss: 2.157, Test accuracy: 31.28
Round  44: Global train loss: 0.806, Global test loss: 2.282, Global test accuracy: 38.32
Round  45, Train loss: 2.026, Test loss: 2.184, Test accuracy: 29.55
Round  45: Global train loss: 2.026, Global test loss: 2.284, Global test accuracy: 38.28
Round  46, Train loss: 1.730, Test loss: 2.188, Test accuracy: 29.23
Round  46: Global train loss: 1.730, Global test loss: 2.285, Global test accuracy: 38.60
Round  47, Train loss: 0.977, Test loss: 2.149, Test accuracy: 32.82
Round  47: Global train loss: 0.977, Global test loss: 2.280, Global test accuracy: 40.65
Round  48, Train loss: 1.227, Test loss: 2.143, Test accuracy: 33.82
Round  48: Global train loss: 1.227, Global test loss: 2.279, Global test accuracy: 42.80
Round  49, Train loss: 1.017, Test loss: 2.137, Test accuracy: 34.95
Round  49: Global train loss: 1.017, Global test loss: 2.275, Global test accuracy: 42.98
Final Round: Train loss: 2.204, Test loss: 2.111, Test accuracy: 42.40
Final Round: Global train loss: 2.204, Global test loss: 2.267, Global test accuracy: 44.30
Average accuracy final 10 rounds: 29.673333333333336
Average global accuracy final 10 rounds: 38.39333333333333
420.47414922714233
[]
[12.716666666666667, 16.666666666666668, 20.35, 21.966666666666665, 21.466666666666665, 22.1, 21.166666666666668, 22.333333333333332, 24.3, 24.416666666666668, 26.116666666666667, 31.5, 31.1, 31.5, 29.616666666666667, 27.266666666666666, 27.15, 21.533333333333335, 21.966666666666665, 24.983333333333334, 27.783333333333335, 27.1, 31.9, 28.75, 29.0, 31.4, 31.933333333333334, 32.11666666666667, 29.4, 27.583333333333332, 24.2, 23.333333333333332, 22.283333333333335, 25.766666666666666, 25.616666666666667, 25.533333333333335, 29.1, 28.416666666666668, 28.733333333333334, 23.416666666666668, 28.066666666666666, 27.233333333333334, 23.316666666666666, 26.466666666666665, 31.283333333333335, 29.55, 29.233333333333334, 32.81666666666667, 33.81666666666667, 34.95, 42.4]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.42 

Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.42 

Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.45 

Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.47 

Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.48 

Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.50 

Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.55 

Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.62 

Round   4, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.58 

Round   4, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.62 

Round   5, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.63 

Round   5, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.72 

Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.63 

Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.73 

Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.65 

Round   7, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.82 

Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.72 

Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.82 

Round   9, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.75 

Round   9, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.92 

Round  10, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80 

Round  10, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.93 

Round  11, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.90 

Round  11, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.98 

Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.02 

Round  12, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.07 

Round  13, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.03 

Round  13, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.20 

Round  14, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.08 

Round  14, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.22 

Round  15, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.20 

Round  15, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.30 

Round  16, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.23 

Round  16, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.32 

Round  17, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.28 

Round  17, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.32 

Round  18, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.32 

Round  18, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.32 

Round  19, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.37 

Round  19, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.37 

Round  20, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.37 

Round  20, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.38 

Round  21, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.40 

Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.43 

Round  22, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.40 

Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.52 

Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.42 

Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.58 

Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.45 

Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.62 

Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.58 

Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.63 

Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.62 

Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.70 

Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.68 

Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.92 

Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.82 

Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.05 

Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.93 

Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.17 

Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.97 

Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.32 

Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.13 

Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.40 

Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.17 

Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.47 

Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.27 

Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.52 

Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.37 

Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.62 

Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.42 

Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.72 

Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.53 

Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.80 

Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.65 

Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.87 

Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.78 

Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.92 

Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.83 

Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.93 

Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.87 

Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.98 

Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.90 

Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.03 

Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.97 

Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.03 

Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.00 

Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.08 

Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.03 

Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.17 

Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.07 

Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.25 

Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.08 

Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.25 

Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.22 

Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.30 

Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.23 

Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.32 

Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.30 

Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.33 

Final Round, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.37 

Final Round, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.33 

Average accuracy final 10 rounds: 12.066666666666666 

Average global accuracy final 10 rounds: 12.175 

395.810017824173
[1.6168100833892822, 2.311049222946167, 3.0488638877868652, 3.7834367752075195, 4.517024040222168, 5.2521491050720215, 5.942583799362183, 6.628734350204468, 7.31940770149231, 8.006584644317627, 8.696129083633423, 9.378522634506226, 10.062719106674194, 10.74817705154419, 11.435938119888306, 12.125348329544067, 12.810899496078491, 13.491679906845093, 14.180874586105347, 14.864719152450562, 15.547736644744873, 16.228328704833984, 16.917866468429565, 17.60374903678894, 18.28826355934143, 19.014147996902466, 19.74185538291931, 20.473340272903442, 21.201508283615112, 21.930978536605835, 22.66331934928894, 23.395702123641968, 24.125094175338745, 24.855059146881104, 25.584097146987915, 26.31694769859314, 27.055284023284912, 27.78811550140381, 28.51840376853943, 29.24408221244812, 29.974544048309326, 30.72473382949829, 31.452234268188477, 32.18442106246948, 32.91357374191284, 33.61883902549744, 34.3069064617157, 34.9947612285614, 35.68152451515198, 36.364601135253906, 37.797852754592896]
[9.416666666666666, 9.45, 9.483333333333333, 9.55, 9.583333333333334, 9.633333333333333, 9.633333333333333, 9.65, 9.716666666666667, 9.75, 9.8, 9.9, 10.016666666666667, 10.033333333333333, 10.083333333333334, 10.2, 10.233333333333333, 10.283333333333333, 10.316666666666666, 10.366666666666667, 10.366666666666667, 10.4, 10.4, 10.416666666666666, 10.45, 10.583333333333334, 10.616666666666667, 10.683333333333334, 10.816666666666666, 10.933333333333334, 10.966666666666667, 11.133333333333333, 11.166666666666666, 11.266666666666667, 11.366666666666667, 11.416666666666666, 11.533333333333333, 11.65, 11.783333333333333, 11.833333333333334, 11.866666666666667, 11.9, 11.966666666666667, 12.0, 12.033333333333333, 12.066666666666666, 12.083333333333334, 12.216666666666667, 12.233333333333333, 12.3, 12.366666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.279, Test loss: 2.224, Test accuracy: 34.67 

Round   0, Global train loss: 2.279, Global test loss: 2.234, Global test accuracy: 33.33 

Round   1, Train loss: 2.083, Test loss: 2.069, Test accuracy: 45.38 

Round   1, Global train loss: 2.083, Global test loss: 2.081, Global test accuracy: 40.23 

Round   2, Train loss: 1.851, Test loss: 1.976, Test accuracy: 52.63 

Round   2, Global train loss: 1.851, Global test loss: 2.037, Global test accuracy: 42.85 

Round   3, Train loss: 1.802, Test loss: 1.874, Test accuracy: 60.85 

Round   3, Global train loss: 1.802, Global test loss: 2.038, Global test accuracy: 42.05 

Round   4, Train loss: 1.669, Test loss: 1.781, Test accuracy: 71.47 

Round   4, Global train loss: 1.669, Global test loss: 2.012, Global test accuracy: 46.08 

Round   5, Train loss: 1.545, Test loss: 1.704, Test accuracy: 78.52 

Round   5, Global train loss: 1.545, Global test loss: 1.975, Global test accuracy: 47.75 

Round   6, Train loss: 1.520, Test loss: 1.677, Test accuracy: 81.67 

Round   6, Global train loss: 1.520, Global test loss: 1.978, Global test accuracy: 47.45 

Round   7, Train loss: 1.514, Test loss: 1.658, Test accuracy: 83.48 

Round   7, Global train loss: 1.514, Global test loss: 1.969, Global test accuracy: 49.00 

Round   8, Train loss: 1.680, Test loss: 1.618, Test accuracy: 86.40 

Round   8, Global train loss: 1.680, Global test loss: 2.025, Global test accuracy: 42.23 

Round   9, Train loss: 1.502, Test loss: 1.603, Test accuracy: 87.95 

Round   9, Global train loss: 1.502, Global test loss: 1.986, Global test accuracy: 47.07 

Round  10, Train loss: 1.517, Test loss: 1.580, Test accuracy: 89.88 

Round  10, Global train loss: 1.517, Global test loss: 2.024, Global test accuracy: 42.02 

Round  11, Train loss: 1.565, Test loss: 1.564, Test accuracy: 91.63 

Round  11, Global train loss: 1.565, Global test loss: 1.972, Global test accuracy: 49.03 

Round  12, Train loss: 1.545, Test loss: 1.553, Test accuracy: 92.22 

Round  12, Global train loss: 1.545, Global test loss: 2.030, Global test accuracy: 41.42 

Round  13, Train loss: 1.513, Test loss: 1.535, Test accuracy: 93.78 

Round  13, Global train loss: 1.513, Global test loss: 1.983, Global test accuracy: 48.28 

Round  14, Train loss: 1.478, Test loss: 1.534, Test accuracy: 93.78 

Round  14, Global train loss: 1.478, Global test loss: 2.026, Global test accuracy: 43.85 

Round  15, Train loss: 1.504, Test loss: 1.521, Test accuracy: 95.15 

Round  15, Global train loss: 1.504, Global test loss: 2.029, Global test accuracy: 41.28 

Round  16, Train loss: 1.477, Test loss: 1.519, Test accuracy: 95.13 

Round  16, Global train loss: 1.477, Global test loss: 1.978, Global test accuracy: 48.92 

Round  17, Train loss: 1.475, Test loss: 1.518, Test accuracy: 95.12 

Round  17, Global train loss: 1.475, Global test loss: 2.033, Global test accuracy: 40.40 

Round  18, Train loss: 1.473, Test loss: 1.517, Test accuracy: 95.17 

Round  18, Global train loss: 1.473, Global test loss: 2.011, Global test accuracy: 44.55 

Round  19, Train loss: 1.481, Test loss: 1.514, Test accuracy: 95.37 

Round  19, Global train loss: 1.481, Global test loss: 1.962, Global test accuracy: 49.88 

Round  20, Train loss: 1.471, Test loss: 1.514, Test accuracy: 95.37 

Round  20, Global train loss: 1.471, Global test loss: 2.000, Global test accuracy: 45.32 

Round  21, Train loss: 1.468, Test loss: 1.513, Test accuracy: 95.40 

Round  21, Global train loss: 1.468, Global test loss: 2.026, Global test accuracy: 43.15 

Round  22, Train loss: 1.468, Test loss: 1.513, Test accuracy: 95.42 

Round  22, Global train loss: 1.468, Global test loss: 2.015, Global test accuracy: 41.90 

Round  23, Train loss: 1.465, Test loss: 1.513, Test accuracy: 95.43 

Round  23, Global train loss: 1.465, Global test loss: 2.070, Global test accuracy: 37.07 

Round  24, Train loss: 1.473, Test loss: 1.512, Test accuracy: 95.38 

Round  24, Global train loss: 1.473, Global test loss: 1.983, Global test accuracy: 46.77 

Round  25, Train loss: 1.469, Test loss: 1.512, Test accuracy: 95.38 

Round  25, Global train loss: 1.469, Global test loss: 2.001, Global test accuracy: 45.80 

Round  26, Train loss: 1.469, Test loss: 1.512, Test accuracy: 95.37 

Round  26, Global train loss: 1.469, Global test loss: 1.986, Global test accuracy: 48.22 

Round  27, Train loss: 1.468, Test loss: 1.511, Test accuracy: 95.33 

Round  27, Global train loss: 1.468, Global test loss: 2.015, Global test accuracy: 43.42 

Round  28, Train loss: 1.468, Test loss: 1.511, Test accuracy: 95.28 

Round  28, Global train loss: 1.468, Global test loss: 1.972, Global test accuracy: 51.20 

Round  29, Train loss: 1.469, Test loss: 1.511, Test accuracy: 95.32 

Round  29, Global train loss: 1.469, Global test loss: 2.045, Global test accuracy: 39.85 

Round  30, Train loss: 1.469, Test loss: 1.511, Test accuracy: 95.30 

Round  30, Global train loss: 1.469, Global test loss: 2.026, Global test accuracy: 40.22 

Round  31, Train loss: 1.467, Test loss: 1.510, Test accuracy: 95.33 

Round  31, Global train loss: 1.467, Global test loss: 2.008, Global test accuracy: 42.37 

Round  32, Train loss: 1.464, Test loss: 1.510, Test accuracy: 95.35 

Round  32, Global train loss: 1.464, Global test loss: 1.978, Global test accuracy: 47.57 

Round  33, Train loss: 1.467, Test loss: 1.510, Test accuracy: 95.35 

Round  33, Global train loss: 1.467, Global test loss: 1.956, Global test accuracy: 50.88 

Round  34, Train loss: 1.469, Test loss: 1.510, Test accuracy: 95.37 

Round  34, Global train loss: 1.469, Global test loss: 1.958, Global test accuracy: 50.42 

Round  35, Train loss: 1.468, Test loss: 1.510, Test accuracy: 95.42 

Round  35, Global train loss: 1.468, Global test loss: 1.992, Global test accuracy: 46.13 

Round  36, Train loss: 1.464, Test loss: 1.510, Test accuracy: 95.42 

Round  36, Global train loss: 1.464, Global test loss: 1.962, Global test accuracy: 50.82 

Round  37, Train loss: 1.465, Test loss: 1.510, Test accuracy: 95.40 

Round  37, Global train loss: 1.465, Global test loss: 1.962, Global test accuracy: 49.92 

Round  38, Train loss: 1.465, Test loss: 1.510, Test accuracy: 95.45 

Round  38, Global train loss: 1.465, Global test loss: 1.982, Global test accuracy: 47.18 

Round  39, Train loss: 1.466, Test loss: 1.510, Test accuracy: 95.45 

Round  39, Global train loss: 1.466, Global test loss: 1.976, Global test accuracy: 49.02 

Round  40, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.45 

Round  40, Global train loss: 1.465, Global test loss: 1.950, Global test accuracy: 51.30 

Round  41, Train loss: 1.466, Test loss: 1.509, Test accuracy: 95.47 

Round  41, Global train loss: 1.466, Global test loss: 2.004, Global test accuracy: 45.47 

Round  42, Train loss: 1.468, Test loss: 1.509, Test accuracy: 95.45 

Round  42, Global train loss: 1.468, Global test loss: 1.969, Global test accuracy: 49.63 

Round  43, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.45 

Round  43, Global train loss: 1.465, Global test loss: 1.968, Global test accuracy: 48.63 

Round  44, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.47 

Round  44, Global train loss: 1.464, Global test loss: 2.009, Global test accuracy: 43.47 

Round  45, Train loss: 1.467, Test loss: 1.509, Test accuracy: 95.47 

Round  45, Global train loss: 1.467, Global test loss: 1.977, Global test accuracy: 48.28 

Round  46, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.47 

Round  46, Global train loss: 1.465, Global test loss: 2.033, Global test accuracy: 41.82 

Round  47, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.42 

Round  47, Global train loss: 1.464, Global test loss: 1.974, Global test accuracy: 48.00 

Round  48, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.40 

Round  48, Global train loss: 1.464, Global test loss: 1.969, Global test accuracy: 49.53 

Round  49, Train loss: 1.466, Test loss: 1.509, Test accuracy: 95.42 

Round  49, Global train loss: 1.466, Global test loss: 1.971, Global test accuracy: 47.93 

Final Round, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.52 

Final Round, Global train loss: 1.465, Global test loss: 1.971, Global test accuracy: 47.93 

Average accuracy final 10 rounds: 95.44500000000002 

Average global accuracy final 10 rounds: 47.406666666666666 

381.69644808769226
[1.6193389892578125, 2.269354820251465, 2.9128646850585938, 3.561028480529785, 4.2042555809021, 4.848474979400635, 5.493488311767578, 6.140205383300781, 6.785162687301636, 7.4271087646484375, 8.069688081741333, 8.715433835983276, 9.359224796295166, 10.003784656524658, 10.650778770446777, 11.292547941207886, 11.937102556228638, 12.578195810317993, 13.228187322616577, 13.870408296585083, 14.511260747909546, 15.154924631118774, 15.792784690856934, 16.436262607574463, 17.077144145965576, 17.722792387008667, 18.36413073539734, 19.003283500671387, 19.641706705093384, 20.281789541244507, 20.92430067062378, 21.56796431541443, 22.210503578186035, 22.859651803970337, 23.50339436531067, 24.14618229866028, 24.794462203979492, 25.435097694396973, 26.065634727478027, 26.711287021636963, 27.353251695632935, 27.996927738189697, 28.639834880828857, 29.283167600631714, 29.92846703529358, 30.56800365447998, 31.20859670639038, 31.965906620025635, 32.721559286117554, 33.483150482177734, 35.01063346862793]
[34.666666666666664, 45.38333333333333, 52.63333333333333, 60.85, 71.46666666666667, 78.51666666666667, 81.66666666666667, 83.48333333333333, 86.4, 87.95, 89.88333333333334, 91.63333333333334, 92.21666666666667, 93.78333333333333, 93.78333333333333, 95.15, 95.13333333333334, 95.11666666666666, 95.16666666666667, 95.36666666666666, 95.36666666666666, 95.4, 95.41666666666667, 95.43333333333334, 95.38333333333334, 95.38333333333334, 95.36666666666666, 95.33333333333333, 95.28333333333333, 95.31666666666666, 95.3, 95.33333333333333, 95.35, 95.35, 95.36666666666666, 95.41666666666667, 95.41666666666667, 95.4, 95.45, 95.45, 95.45, 95.46666666666667, 95.45, 95.45, 95.46666666666667, 95.46666666666667, 95.46666666666667, 95.41666666666667, 95.4, 95.41666666666667, 95.51666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.281, Test loss: 2.233, Test accuracy: 34.53 

Round   0, Global train loss: 2.281, Global test loss: 2.239, Global test accuracy: 33.33 

Round   1, Train loss: 2.065, Test loss: 2.059, Test accuracy: 46.07 

Round   1, Global train loss: 2.065, Global test loss: 2.065, Global test accuracy: 42.97 

Round   2, Train loss: 1.846, Test loss: 1.935, Test accuracy: 58.17 

Round   2, Global train loss: 1.846, Global test loss: 2.004, Global test accuracy: 44.23 

Round   3, Train loss: 1.762, Test loss: 1.873, Test accuracy: 63.77 

Round   3, Global train loss: 1.762, Global test loss: 1.995, Global test accuracy: 45.73 

Round   4, Train loss: 1.770, Test loss: 1.831, Test accuracy: 69.05 

Round   4, Global train loss: 1.770, Global test loss: 2.012, Global test accuracy: 42.98 

Round   5, Train loss: 1.676, Test loss: 1.797, Test accuracy: 71.93 

Round   5, Global train loss: 1.676, Global test loss: 1.990, Global test accuracy: 45.88 

Round   6, Train loss: 1.675, Test loss: 1.747, Test accuracy: 76.48 

Round   6, Global train loss: 1.675, Global test loss: 1.958, Global test accuracy: 50.77 

Round   7, Train loss: 1.731, Test loss: 1.686, Test accuracy: 80.10 

Round   7, Global train loss: 1.731, Global test loss: 1.950, Global test accuracy: 51.23 

Round   8, Train loss: 1.623, Test loss: 1.650, Test accuracy: 83.15 

Round   8, Global train loss: 1.623, Global test loss: 1.965, Global test accuracy: 48.67 

Round   9, Train loss: 1.537, Test loss: 1.649, Test accuracy: 83.28 

Round   9, Global train loss: 1.537, Global test loss: 1.957, Global test accuracy: 49.18 

Round  10, Train loss: 1.673, Test loss: 1.614, Test accuracy: 86.23 

Round  10, Global train loss: 1.673, Global test loss: 1.970, Global test accuracy: 47.80 

Round  11, Train loss: 1.634, Test loss: 1.622, Test accuracy: 85.35 

Round  11, Global train loss: 1.634, Global test loss: 1.964, Global test accuracy: 49.27 

Round  12, Train loss: 1.685, Test loss: 1.613, Test accuracy: 85.60 

Round  12, Global train loss: 1.685, Global test loss: 1.977, Global test accuracy: 48.43 

Round  13, Train loss: 1.557, Test loss: 1.626, Test accuracy: 84.07 

Round  13, Global train loss: 1.557, Global test loss: 1.955, Global test accuracy: 50.02 

Round  14, Train loss: 1.661, Test loss: 1.640, Test accuracy: 82.63 

Round  14, Global train loss: 1.661, Global test loss: 1.964, Global test accuracy: 48.98 

Round  15, Train loss: 1.613, Test loss: 1.640, Test accuracy: 82.60 

Round  15, Global train loss: 1.613, Global test loss: 1.956, Global test accuracy: 50.38 

Round  16, Train loss: 1.642, Test loss: 1.627, Test accuracy: 84.03 

Round  16, Global train loss: 1.642, Global test loss: 1.947, Global test accuracy: 50.58 

Round  17, Train loss: 1.553, Test loss: 1.614, Test accuracy: 85.37 

Round  17, Global train loss: 1.553, Global test loss: 1.947, Global test accuracy: 51.57 

Round  18, Train loss: 1.622, Test loss: 1.616, Test accuracy: 85.13 

Round  18, Global train loss: 1.622, Global test loss: 1.954, Global test accuracy: 50.35 

Round  19, Train loss: 1.680, Test loss: 1.611, Test accuracy: 85.62 

Round  19, Global train loss: 1.680, Global test loss: 1.983, Global test accuracy: 47.53 

Round  20, Train loss: 1.528, Test loss: 1.609, Test accuracy: 85.67 

Round  20, Global train loss: 1.528, Global test loss: 1.966, Global test accuracy: 48.90 

Round  21, Train loss: 1.627, Test loss: 1.610, Test accuracy: 85.68 

Round  21, Global train loss: 1.627, Global test loss: 1.956, Global test accuracy: 50.08 

Round  22, Train loss: 1.494, Test loss: 1.609, Test accuracy: 85.75 

Round  22, Global train loss: 1.494, Global test loss: 1.935, Global test accuracy: 52.38 

Round  23, Train loss: 1.684, Test loss: 1.609, Test accuracy: 85.55 

Round  23, Global train loss: 1.684, Global test loss: 1.962, Global test accuracy: 49.02 

Round  24, Train loss: 1.659, Test loss: 1.608, Test accuracy: 85.70 

Round  24, Global train loss: 1.659, Global test loss: 1.975, Global test accuracy: 47.62 

Round  25, Train loss: 1.653, Test loss: 1.608, Test accuracy: 85.75 

Round  25, Global train loss: 1.653, Global test loss: 1.986, Global test accuracy: 46.60 

Round  26, Train loss: 1.599, Test loss: 1.612, Test accuracy: 85.22 

Round  26, Global train loss: 1.599, Global test loss: 1.966, Global test accuracy: 48.30 

Round  27, Train loss: 1.566, Test loss: 1.611, Test accuracy: 85.30 

Round  27, Global train loss: 1.566, Global test loss: 1.944, Global test accuracy: 51.28 

Round  28, Train loss: 1.598, Test loss: 1.611, Test accuracy: 85.18 

Round  28, Global train loss: 1.598, Global test loss: 1.955, Global test accuracy: 50.18 

Round  29, Train loss: 1.597, Test loss: 1.611, Test accuracy: 85.17 

Round  29, Global train loss: 1.597, Global test loss: 1.948, Global test accuracy: 50.38 

Round  30, Train loss: 1.652, Test loss: 1.597, Test accuracy: 86.55 

Round  30, Global train loss: 1.652, Global test loss: 1.960, Global test accuracy: 49.77 

Round  31, Train loss: 1.595, Test loss: 1.596, Test accuracy: 86.65 

Round  31, Global train loss: 1.595, Global test loss: 1.955, Global test accuracy: 49.92 

Round  32, Train loss: 1.589, Test loss: 1.596, Test accuracy: 86.70 

Round  32, Global train loss: 1.589, Global test loss: 1.948, Global test accuracy: 50.55 

Round  33, Train loss: 1.556, Test loss: 1.596, Test accuracy: 86.83 

Round  33, Global train loss: 1.556, Global test loss: 1.952, Global test accuracy: 49.93 

Round  34, Train loss: 1.587, Test loss: 1.617, Test accuracy: 84.72 

Round  34, Global train loss: 1.587, Global test loss: 1.943, Global test accuracy: 51.35 

Round  35, Train loss: 1.499, Test loss: 1.609, Test accuracy: 85.55 

Round  35, Global train loss: 1.499, Global test loss: 1.935, Global test accuracy: 52.53 

Round  36, Train loss: 1.612, Test loss: 1.607, Test accuracy: 85.62 

Round  36, Global train loss: 1.612, Global test loss: 1.952, Global test accuracy: 50.05 

Round  37, Train loss: 1.716, Test loss: 1.618, Test accuracy: 84.37 

Round  37, Global train loss: 1.716, Global test loss: 1.971, Global test accuracy: 48.48 

Round  38, Train loss: 1.608, Test loss: 1.603, Test accuracy: 85.85 

Round  38, Global train loss: 1.608, Global test loss: 1.980, Global test accuracy: 47.28 

Round  39, Train loss: 1.568, Test loss: 1.588, Test accuracy: 87.40 

Round  39, Global train loss: 1.568, Global test loss: 1.940, Global test accuracy: 52.27 

Round  40, Train loss: 1.542, Test loss: 1.589, Test accuracy: 87.33 

Round  40, Global train loss: 1.542, Global test loss: 1.949, Global test accuracy: 50.57 

Round  41, Train loss: 1.626, Test loss: 1.589, Test accuracy: 87.37 

Round  41, Global train loss: 1.626, Global test loss: 1.944, Global test accuracy: 51.25 

Round  42, Train loss: 1.517, Test loss: 1.589, Test accuracy: 87.38 

Round  42, Global train loss: 1.517, Global test loss: 1.931, Global test accuracy: 52.92 

Round  43, Train loss: 1.533, Test loss: 1.590, Test accuracy: 87.27 

Round  43, Global train loss: 1.533, Global test loss: 1.938, Global test accuracy: 51.98 

Round  44, Train loss: 1.630, Test loss: 1.606, Test accuracy: 85.65 

Round  44, Global train loss: 1.630, Global test loss: 1.929, Global test accuracy: 53.05 

Round  45, Train loss: 1.504, Test loss: 1.606, Test accuracy: 85.80 

Round  45, Global train loss: 1.504, Global test loss: 1.936, Global test accuracy: 52.12 

Round  46, Train loss: 1.657, Test loss: 1.608, Test accuracy: 85.63 

Round  46, Global train loss: 1.657, Global test loss: 1.974, Global test accuracy: 48.03 

Round  47, Train loss: 1.551, Test loss: 1.606, Test accuracy: 85.78 

Round  47, Global train loss: 1.551, Global test loss: 1.936, Global test accuracy: 52.18 

Round  48, Train loss: 1.562, Test loss: 1.590, Test accuracy: 87.35 

Round  48, Global train loss: 1.562, Global test loss: 1.946, Global test accuracy: 51.28 

Round  49, Train loss: 1.490, Test loss: 1.590, Test accuracy: 87.30 

Round  49, Global train loss: 1.490, Global test loss: 1.946, Global test accuracy: 51.05 

Final Round, Train loss: 1.575, Test loss: 1.571, Test accuracy: 89.27 

Final Round, Global train loss: 1.575, Global test loss: 1.946, Global test accuracy: 51.05 

Average accuracy final 10 rounds: 86.68666666666667 

Average global accuracy final 10 rounds: 51.44333333333333 

382.6006259918213
[1.612131118774414, 2.2691664695739746, 2.9239466190338135, 3.5815093517303467, 4.270601987838745, 4.94304633140564, 5.606326103210449, 6.271333456039429, 6.939452409744263, 7.6011879444122314, 8.261592626571655, 8.920521259307861, 9.585978507995605, 10.254604816436768, 10.921483755111694, 11.581496238708496, 12.24399733543396, 12.907429933547974, 13.579716682434082, 14.243308782577515, 14.907322645187378, 15.598736047744751, 16.265459775924683, 16.933265209197998, 17.597973346710205, 18.262375116348267, 18.925118446350098, 19.59267830848694, 20.17022180557251, 20.749189615249634, 21.32395315170288, 21.896871328353882, 22.461974143981934, 23.1258704662323, 23.806802988052368, 24.516146421432495, 25.176278352737427, 25.841679096221924, 26.508334159851074, 27.173510551452637, 27.83442759513855, 28.493480920791626, 29.15833330154419, 29.82164192199707, 30.479110956192017, 31.140355587005615, 31.811588048934937, 32.479860067367554, 33.14785051345825, 33.810280561447144, 35.100069999694824]
[34.53333333333333, 46.06666666666667, 58.166666666666664, 63.766666666666666, 69.05, 71.93333333333334, 76.48333333333333, 80.1, 83.15, 83.28333333333333, 86.23333333333333, 85.35, 85.6, 84.06666666666666, 82.63333333333334, 82.6, 84.03333333333333, 85.36666666666666, 85.13333333333334, 85.61666666666666, 85.66666666666667, 85.68333333333334, 85.75, 85.55, 85.7, 85.75, 85.21666666666667, 85.3, 85.18333333333334, 85.16666666666667, 86.55, 86.65, 86.7, 86.83333333333333, 84.71666666666667, 85.55, 85.61666666666666, 84.36666666666666, 85.85, 87.4, 87.33333333333333, 87.36666666666666, 87.38333333333334, 87.26666666666667, 85.65, 85.8, 85.63333333333334, 85.78333333333333, 87.35, 87.3, 89.26666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.290, Test accuracy: 32.33 

Round   1, Train loss: 2.273, Test loss: 2.240, Test accuracy: 33.33 

Round   2, Train loss: 2.161, Test loss: 2.116, Test accuracy: 33.33 

Round   3, Train loss: 2.070, Test loss: 2.077, Test accuracy: 37.68 

Round   4, Train loss: 1.979, Test loss: 2.031, Test accuracy: 43.53 

Round   5, Train loss: 1.945, Test loss: 1.989, Test accuracy: 45.42 

Round   6, Train loss: 1.868, Test loss: 1.957, Test accuracy: 50.83 

Round   7, Train loss: 1.788, Test loss: 1.916, Test accuracy: 56.28 

Round   8, Train loss: 1.857, Test loss: 1.914, Test accuracy: 55.83 

Round   9, Train loss: 1.794, Test loss: 1.904, Test accuracy: 56.27 

Round  10, Train loss: 1.805, Test loss: 1.881, Test accuracy: 58.62 

Round  11, Train loss: 1.813, Test loss: 1.873, Test accuracy: 59.18 

Round  12, Train loss: 1.866, Test loss: 1.856, Test accuracy: 60.98 

Round  13, Train loss: 1.795, Test loss: 1.847, Test accuracy: 62.25 

Round  14, Train loss: 1.736, Test loss: 1.828, Test accuracy: 64.17 

Round  15, Train loss: 1.782, Test loss: 1.826, Test accuracy: 64.20 

Round  16, Train loss: 1.702, Test loss: 1.819, Test accuracy: 65.00 

Round  17, Train loss: 1.624, Test loss: 1.811, Test accuracy: 66.17 

Round  18, Train loss: 1.631, Test loss: 1.802, Test accuracy: 67.18 

Round  19, Train loss: 1.790, Test loss: 1.807, Test accuracy: 66.50 

Round  20, Train loss: 1.696, Test loss: 1.786, Test accuracy: 68.47 

Round  21, Train loss: 1.679, Test loss: 1.778, Test accuracy: 69.45 

Round  22, Train loss: 1.681, Test loss: 1.777, Test accuracy: 69.27 

Round  23, Train loss: 1.622, Test loss: 1.773, Test accuracy: 69.67 

Round  24, Train loss: 1.620, Test loss: 1.771, Test accuracy: 69.83 

Round  25, Train loss: 1.775, Test loss: 1.773, Test accuracy: 69.62 

Round  26, Train loss: 1.749, Test loss: 1.771, Test accuracy: 69.73 

Round  27, Train loss: 1.738, Test loss: 1.773, Test accuracy: 69.67 

Round  28, Train loss: 1.729, Test loss: 1.766, Test accuracy: 69.95 

Round  29, Train loss: 1.678, Test loss: 1.765, Test accuracy: 70.22 

Round  30, Train loss: 1.812, Test loss: 1.763, Test accuracy: 70.37 

Round  31, Train loss: 1.766, Test loss: 1.760, Test accuracy: 70.58 

Round  32, Train loss: 1.793, Test loss: 1.757, Test accuracy: 70.83 

Round  33, Train loss: 1.714, Test loss: 1.757, Test accuracy: 70.80 

Round  34, Train loss: 1.768, Test loss: 1.757, Test accuracy: 70.88 

Round  35, Train loss: 1.815, Test loss: 1.757, Test accuracy: 70.83 

Round  36, Train loss: 1.763, Test loss: 1.755, Test accuracy: 71.00 

Round  37, Train loss: 1.832, Test loss: 1.759, Test accuracy: 70.68 

Round  38, Train loss: 1.665, Test loss: 1.756, Test accuracy: 71.00 

Round  39, Train loss: 1.769, Test loss: 1.756, Test accuracy: 70.97 

Round  40, Train loss: 1.727, Test loss: 1.752, Test accuracy: 71.38 

Round  41, Train loss: 1.703, Test loss: 1.750, Test accuracy: 71.67 

Round  42, Train loss: 1.659, Test loss: 1.750, Test accuracy: 71.58 

Round  43, Train loss: 1.557, Test loss: 1.752, Test accuracy: 71.30 

Round  44, Train loss: 1.602, Test loss: 1.753, Test accuracy: 71.33 

Round  45, Train loss: 1.755, Test loss: 1.752, Test accuracy: 71.37 

Round  46, Train loss: 1.758, Test loss: 1.752, Test accuracy: 71.45 

Round  47, Train loss: 1.759, Test loss: 1.752, Test accuracy: 71.33 

Round  48, Train loss: 1.705, Test loss: 1.751, Test accuracy: 71.22 

Round  49, Train loss: 1.723, Test loss: 1.749, Test accuracy: 71.45 

Final Round, Train loss: 1.727, Test loss: 1.737, Test accuracy: 72.73 

Average accuracy final 10 rounds: 71.40833333333332 

278.6244192123413
[1.6182787418365479, 2.284130096435547, 2.944291591644287, 3.606330633163452, 4.268977403640747, 4.934533357620239, 5.600151300430298, 6.157847881317139, 6.744518280029297, 7.342102766036987, 7.90057110786438, 8.456045866012573, 9.028108835220337, 9.58893346786499, 10.145800828933716, 10.697865009307861, 11.250116348266602, 11.799034118652344, 12.35400676727295, 12.910754680633545, 13.461922645568848, 14.012569665908813, 14.558953762054443, 15.142025470733643, 15.70888876914978, 16.29320192337036, 16.85924983024597, 17.433324813842773, 17.9948832988739, 18.64520764350891, 19.306421041488647, 19.86721920967102, 20.425410270690918, 20.97539472579956, 21.544352531433105, 22.09687328338623, 22.657458066940308, 23.214603424072266, 23.77053165435791, 24.32701086997986, 24.883246183395386, 25.440011739730835, 25.99773097038269, 26.557647943496704, 27.114208698272705, 27.77182388305664, 28.434882164001465, 29.094868183135986, 29.76500391960144, 30.428135633468628, 31.55843734741211]
[32.333333333333336, 33.333333333333336, 33.333333333333336, 37.68333333333333, 43.53333333333333, 45.416666666666664, 50.833333333333336, 56.28333333333333, 55.833333333333336, 56.266666666666666, 58.61666666666667, 59.18333333333333, 60.983333333333334, 62.25, 64.16666666666667, 64.2, 65.0, 66.16666666666667, 67.18333333333334, 66.5, 68.46666666666667, 69.45, 69.26666666666667, 69.66666666666667, 69.83333333333333, 69.61666666666666, 69.73333333333333, 69.66666666666667, 69.95, 70.21666666666667, 70.36666666666666, 70.58333333333333, 70.83333333333333, 70.8, 70.88333333333334, 70.83333333333333, 71.0, 70.68333333333334, 71.0, 70.96666666666667, 71.38333333333334, 71.66666666666667, 71.58333333333333, 71.3, 71.33333333333333, 71.36666666666666, 71.45, 71.33333333333333, 71.21666666666667, 71.45, 72.73333333333333]
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 291, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 1498, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 524, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 295, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 832, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.274, Test loss: 2.227, Test accuracy: 34.35 

Round   0, Global train loss: 2.274, Global test loss: 2.234, Global test accuracy: 33.33 

Round   1, Train loss: 2.065, Test loss: 2.073, Test accuracy: 42.17 

Round   1, Global train loss: 2.065, Global test loss: 2.091, Global test accuracy: 35.90 

Round   2, Train loss: 1.817, Test loss: 1.961, Test accuracy: 55.72 

Round   2, Global train loss: 1.817, Global test loss: 2.049, Global test accuracy: 40.23 

Round   3, Train loss: 1.671, Test loss: 1.894, Test accuracy: 62.30 

Round   3, Global train loss: 1.671, Global test loss: 2.030, Global test accuracy: 43.18 

Round   4, Train loss: 1.678, Test loss: 1.785, Test accuracy: 73.45 

Round   4, Global train loss: 1.678, Global test loss: 2.028, Global test accuracy: 42.70 

Round   5, Train loss: 1.712, Test loss: 1.697, Test accuracy: 81.68 

Round   5, Global train loss: 1.712, Global test loss: 2.053, Global test accuracy: 38.93 

Round   6, Train loss: 1.528, Test loss: 1.679, Test accuracy: 82.33 

Round   6, Global train loss: 1.528, Global test loss: 2.036, Global test accuracy: 41.57 

Round   7, Train loss: 1.515, Test loss: 1.647, Test accuracy: 85.52 

Round   7, Global train loss: 1.515, Global test loss: 2.001, Global test accuracy: 46.60 

Round   8, Train loss: 1.537, Test loss: 1.639, Test accuracy: 85.58 

Round   8, Global train loss: 1.537, Global test loss: 2.011, Global test accuracy: 44.65 

Round   9, Train loss: 1.574, Test loss: 1.618, Test accuracy: 87.27 

Round   9, Global train loss: 1.574, Global test loss: 2.007, Global test accuracy: 44.50 

Round  10, Train loss: 1.605, Test loss: 1.584, Test accuracy: 90.07 

Round  10, Global train loss: 1.605, Global test loss: 2.015, Global test accuracy: 44.83 

Round  11, Train loss: 1.506, Test loss: 1.569, Test accuracy: 90.52 

Round  11, Global train loss: 1.506, Global test loss: 2.050, Global test accuracy: 39.73 

Round  12, Train loss: 1.529, Test loss: 1.561, Test accuracy: 91.48 

Round  12, Global train loss: 1.529, Global test loss: 2.051, Global test accuracy: 39.60 

Round  13, Train loss: 1.545, Test loss: 1.533, Test accuracy: 94.42 

Round  13, Global train loss: 1.545, Global test loss: 2.043, Global test accuracy: 41.55 

Round  14, Train loss: 1.479, Test loss: 1.530, Test accuracy: 94.55 

Round  14, Global train loss: 1.479, Global test loss: 2.012, Global test accuracy: 45.07 

Round  15, Train loss: 1.488, Test loss: 1.524, Test accuracy: 94.87 

Round  15, Global train loss: 1.488, Global test loss: 2.035, Global test accuracy: 41.35 

Round  16, Train loss: 1.470, Test loss: 1.523, Test accuracy: 94.90 

Round  16, Global train loss: 1.470, Global test loss: 1.997, Global test accuracy: 46.57 

Round  17, Train loss: 1.469, Test loss: 1.523, Test accuracy: 94.88 

Round  17, Global train loss: 1.469, Global test loss: 2.003, Global test accuracy: 44.93 

Round  18, Train loss: 1.477, Test loss: 1.521, Test accuracy: 94.97 

Round  18, Global train loss: 1.477, Global test loss: 2.015, Global test accuracy: 45.45 

Round  19, Train loss: 1.475, Test loss: 1.520, Test accuracy: 94.98 

Round  19, Global train loss: 1.475, Global test loss: 2.041, Global test accuracy: 39.33 

Round  20, Train loss: 1.469, Test loss: 1.520, Test accuracy: 94.92 

Round  20, Global train loss: 1.469, Global test loss: 2.031, Global test accuracy: 40.88 

Round  21, Train loss: 1.483, Test loss: 1.518, Test accuracy: 95.03 

Round  21, Global train loss: 1.483, Global test loss: 2.033, Global test accuracy: 41.88 

Round  22, Train loss: 1.473, Test loss: 1.517, Test accuracy: 95.02 

Round  22, Global train loss: 1.473, Global test loss: 2.057, Global test accuracy: 38.95 

Round  23, Train loss: 1.468, Test loss: 1.517, Test accuracy: 94.98 

Round  23, Global train loss: 1.468, Global test loss: 2.058, Global test accuracy: 38.95 

Round  24, Train loss: 1.473, Test loss: 1.516, Test accuracy: 94.98 

Round  24, Global train loss: 1.473, Global test loss: 2.034, Global test accuracy: 41.88 

Round  25, Train loss: 1.466, Test loss: 1.517, Test accuracy: 94.95 

Round  25, Global train loss: 1.466, Global test loss: 2.057, Global test accuracy: 38.60 

Round  26, Train loss: 1.467, Test loss: 1.516, Test accuracy: 94.93 

Round  26, Global train loss: 1.467, Global test loss: 2.024, Global test accuracy: 43.75 

Round  27, Train loss: 1.467, Test loss: 1.516, Test accuracy: 94.95 

Round  27, Global train loss: 1.467, Global test loss: 2.040, Global test accuracy: 40.32 

Round  28, Train loss: 1.480, Test loss: 1.516, Test accuracy: 94.87 

Round  28, Global train loss: 1.480, Global test loss: 2.040, Global test accuracy: 41.32 

Round  29, Train loss: 1.473, Test loss: 1.515, Test accuracy: 94.98 

Round  29, Global train loss: 1.473, Global test loss: 1.990, Global test accuracy: 47.15 

Round  30, Train loss: 1.467, Test loss: 1.515, Test accuracy: 95.03 

Round  30, Global train loss: 1.467, Global test loss: 2.047, Global test accuracy: 39.82 

Round  31, Train loss: 1.465, Test loss: 1.515, Test accuracy: 94.98 

Round  31, Global train loss: 1.465, Global test loss: 2.046, Global test accuracy: 39.43 

Round  32, Train loss: 1.469, Test loss: 1.515, Test accuracy: 94.97 

Round  32, Global train loss: 1.469, Global test loss: 2.030, Global test accuracy: 41.40 

Round  33, Train loss: 1.465, Test loss: 1.515, Test accuracy: 94.97 

Round  33, Global train loss: 1.465, Global test loss: 2.056, Global test accuracy: 39.02 

Round  34, Train loss: 1.473, Test loss: 1.515, Test accuracy: 94.95 

Round  34, Global train loss: 1.473, Global test loss: 2.011, Global test accuracy: 44.82 

Round  35, Train loss: 1.467, Test loss: 1.515, Test accuracy: 94.90 

Round  35, Global train loss: 1.467, Global test loss: 2.039, Global test accuracy: 41.05 

Round  36, Train loss: 1.467, Test loss: 1.515, Test accuracy: 94.90 

Round  36, Global train loss: 1.467, Global test loss: 2.031, Global test accuracy: 42.35 

Round  37, Train loss: 1.464, Test loss: 1.515, Test accuracy: 94.92 

Round  37, Global train loss: 1.464, Global test loss: 2.064, Global test accuracy: 38.42 

Round  38, Train loss: 1.466, Test loss: 1.515, Test accuracy: 94.93 

Round  38, Global train loss: 1.466, Global test loss: 2.053, Global test accuracy: 40.73 

Round  39, Train loss: 1.466, Test loss: 1.515, Test accuracy: 94.93 

Round  39, Global train loss: 1.466, Global test loss: 1.998, Global test accuracy: 45.90 

Round  40, Train loss: 1.471, Test loss: 1.515, Test accuracy: 94.92 

Round  40, Global train loss: 1.471, Global test loss: 2.021, Global test accuracy: 42.58 

Round  41, Train loss: 1.467, Test loss: 1.515, Test accuracy: 94.93 

Round  41, Global train loss: 1.467, Global test loss: 1.986, Global test accuracy: 47.12 

Round  42, Train loss: 1.470, Test loss: 1.515, Test accuracy: 94.93 

Round  42, Global train loss: 1.470, Global test loss: 2.019, Global test accuracy: 44.87 

Round  43, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.93 

Round  43, Global train loss: 1.468, Global test loss: 2.049, Global test accuracy: 39.02 

Round  44, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.95 

Round  44, Global train loss: 1.468, Global test loss: 2.046, Global test accuracy: 40.13 

Round  45, Train loss: 1.465, Test loss: 1.514, Test accuracy: 94.95 

Round  45, Global train loss: 1.465, Global test loss: 2.039, Global test accuracy: 41.17 

Round  46, Train loss: 1.465, Test loss: 1.514, Test accuracy: 94.90 

Round  46, Global train loss: 1.465, Global test loss: 2.031, Global test accuracy: 41.95 

Round  47, Train loss: 1.466, Test loss: 1.514, Test accuracy: 94.92 

Round  47, Global train loss: 1.466, Global test loss: 2.048, Global test accuracy: 39.30 

Round  48, Train loss: 1.463, Test loss: 1.514, Test accuracy: 94.92 

Round  48, Global train loss: 1.463, Global test loss: 2.030, Global test accuracy: 43.00 

Round  49, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.97 

Round  49, Global train loss: 1.468, Global test loss: 2.042, Global test accuracy: 40.33 

Final Round, Train loss: 1.466, Test loss: 1.514, Test accuracy: 94.98 

Final Round, Global train loss: 1.466, Global test loss: 2.042, Global test accuracy: 40.33 

Average accuracy final 10 rounds: 94.93166666666667 

Average global accuracy final 10 rounds: 41.946666666666665 

375.66452717781067
[1.5771331787109375, 2.2167341709136963, 2.8580989837646484, 3.493602991104126, 4.1527159214019775, 4.789771795272827, 5.424573183059692, 6.059991359710693, 6.701338052749634, 7.339710712432861, 7.975307941436768, 8.61468243598938, 9.251062393188477, 9.890896558761597, 10.530503273010254, 11.166793584823608, 11.799360752105713, 12.436126232147217, 13.075044393539429, 13.710596561431885, 14.351806163787842, 14.991716384887695, 15.624723672866821, 16.264607191085815, 16.899518489837646, 17.529240369796753, 18.165281534194946, 18.803037881851196, 19.430641889572144, 20.071298599243164, 20.707103729248047, 21.446258068084717, 22.12581157684326, 22.805822610855103, 23.484282732009888, 24.112664222717285, 24.751115560531616, 25.387954473495483, 26.01894998550415, 26.65729832649231, 27.292700052261353, 27.922332286834717, 28.558725357055664, 29.19578242301941, 29.82849907875061, 30.464443683624268, 31.10188341140747, 31.731704711914062, 32.368324518203735, 33.00467824935913, 34.35047793388367]
[34.35, 42.166666666666664, 55.71666666666667, 62.3, 73.45, 81.68333333333334, 82.33333333333333, 85.51666666666667, 85.58333333333333, 87.26666666666667, 90.06666666666666, 90.51666666666667, 91.48333333333333, 94.41666666666667, 94.55, 94.86666666666666, 94.9, 94.88333333333334, 94.96666666666667, 94.98333333333333, 94.91666666666667, 95.03333333333333, 95.01666666666667, 94.98333333333333, 94.98333333333333, 94.95, 94.93333333333334, 94.95, 94.86666666666666, 94.98333333333333, 95.03333333333333, 94.98333333333333, 94.96666666666667, 94.96666666666667, 94.95, 94.9, 94.9, 94.91666666666667, 94.93333333333334, 94.93333333333334, 94.91666666666667, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.95, 94.95, 94.9, 94.91666666666667, 94.91666666666667, 94.96666666666667, 94.98333333333333]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.270, Test loss: 2.212, Test accuracy: 34.15 

Round   0, Global train loss: 2.270, Global test loss: 2.223, Global test accuracy: 33.33 

Round   1, Train loss: 2.081, Test loss: 2.084, Test accuracy: 38.07 

Round   1, Global train loss: 2.081, Global test loss: 2.096, Global test accuracy: 37.72 

Round   2, Train loss: 1.876, Test loss: 1.980, Test accuracy: 51.73 

Round   2, Global train loss: 1.876, Global test loss: 2.048, Global test accuracy: 38.75 

Round   3, Train loss: 1.827, Test loss: 1.887, Test accuracy: 61.03 

Round   3, Global train loss: 1.827, Global test loss: 2.048, Global test accuracy: 40.65 

Round   4, Train loss: 1.694, Test loss: 1.798, Test accuracy: 70.38 

Round   4, Global train loss: 1.694, Global test loss: 2.027, Global test accuracy: 43.62 

Round   5, Train loss: 1.754, Test loss: 1.739, Test accuracy: 75.07 

Round   5, Global train loss: 1.754, Global test loss: 2.026, Global test accuracy: 43.33 

Round   6, Train loss: 1.681, Test loss: 1.701, Test accuracy: 78.17 

Round   6, Global train loss: 1.681, Global test loss: 1.996, Global test accuracy: 46.67 

Round   7, Train loss: 1.688, Test loss: 1.706, Test accuracy: 77.12 

Round   7, Global train loss: 1.688, Global test loss: 2.004, Global test accuracy: 45.62 

Round   8, Train loss: 1.718, Test loss: 1.697, Test accuracy: 77.68 

Round   8, Global train loss: 1.718, Global test loss: 2.007, Global test accuracy: 45.15 

Round   9, Train loss: 1.653, Test loss: 1.682, Test accuracy: 79.23 

Round   9, Global train loss: 1.653, Global test loss: 1.986, Global test accuracy: 48.08 

Round  10, Train loss: 1.621, Test loss: 1.682, Test accuracy: 79.22 

Round  10, Global train loss: 1.621, Global test loss: 1.976, Global test accuracy: 48.92 

Round  11, Train loss: 1.668, Test loss: 1.673, Test accuracy: 79.73 

Round  11, Global train loss: 1.668, Global test loss: 1.983, Global test accuracy: 47.80 

Round  12, Train loss: 1.635, Test loss: 1.655, Test accuracy: 81.23 

Round  12, Global train loss: 1.635, Global test loss: 1.971, Global test accuracy: 49.38 

Round  13, Train loss: 1.716, Test loss: 1.641, Test accuracy: 82.72 

Round  13, Global train loss: 1.716, Global test loss: 1.983, Global test accuracy: 47.20 

Round  14, Train loss: 1.549, Test loss: 1.641, Test accuracy: 82.70 

Round  14, Global train loss: 1.549, Global test loss: 2.003, Global test accuracy: 45.33 

Round  15, Train loss: 1.562, Test loss: 1.640, Test accuracy: 82.63 

Round  15, Global train loss: 1.562, Global test loss: 1.999, Global test accuracy: 45.50 

Round  16, Train loss: 1.696, Test loss: 1.653, Test accuracy: 81.33 

Round  16, Global train loss: 1.696, Global test loss: 2.023, Global test accuracy: 43.27 

Round  17, Train loss: 1.680, Test loss: 1.652, Test accuracy: 81.40 

Round  17, Global train loss: 1.680, Global test loss: 2.043, Global test accuracy: 40.68 

Round  18, Train loss: 1.659, Test loss: 1.636, Test accuracy: 83.02 

Round  18, Global train loss: 1.659, Global test loss: 2.062, Global test accuracy: 38.23 

Round  19, Train loss: 1.603, Test loss: 1.622, Test accuracy: 84.40 

Round  19, Global train loss: 1.603, Global test loss: 2.032, Global test accuracy: 41.87 

Round  20, Train loss: 1.576, Test loss: 1.624, Test accuracy: 84.22 

Round  20, Global train loss: 1.576, Global test loss: 2.000, Global test accuracy: 45.05 

Round  21, Train loss: 1.606, Test loss: 1.625, Test accuracy: 84.13 

Round  21, Global train loss: 1.606, Global test loss: 2.015, Global test accuracy: 43.63 

Round  22, Train loss: 1.648, Test loss: 1.640, Test accuracy: 82.60 

Round  22, Global train loss: 1.648, Global test loss: 1.993, Global test accuracy: 46.20 

Round  23, Train loss: 1.759, Test loss: 1.655, Test accuracy: 81.07 

Round  23, Global train loss: 1.759, Global test loss: 1.996, Global test accuracy: 46.17 

Round  24, Train loss: 1.648, Test loss: 1.653, Test accuracy: 81.18 

Round  24, Global train loss: 1.648, Global test loss: 1.975, Global test accuracy: 48.60 

Round  25, Train loss: 1.674, Test loss: 1.668, Test accuracy: 79.73 

Round  25, Global train loss: 1.674, Global test loss: 1.966, Global test accuracy: 49.55 

Round  26, Train loss: 1.579, Test loss: 1.654, Test accuracy: 80.95 

Round  26, Global train loss: 1.579, Global test loss: 2.003, Global test accuracy: 44.77 

Round  27, Train loss: 1.604, Test loss: 1.651, Test accuracy: 81.23 

Round  27, Global train loss: 1.604, Global test loss: 1.989, Global test accuracy: 46.32 

Round  28, Train loss: 1.580, Test loss: 1.625, Test accuracy: 83.92 

Round  28, Global train loss: 1.580, Global test loss: 1.973, Global test accuracy: 48.40 

Round  29, Train loss: 1.618, Test loss: 1.623, Test accuracy: 84.12 

Round  29, Global train loss: 1.618, Global test loss: 1.995, Global test accuracy: 46.00 

Round  30, Train loss: 1.619, Test loss: 1.613, Test accuracy: 85.18 

Round  30, Global train loss: 1.619, Global test loss: 1.979, Global test accuracy: 47.68 

Round  31, Train loss: 1.658, Test loss: 1.612, Test accuracy: 85.33 

Round  31, Global train loss: 1.658, Global test loss: 2.003, Global test accuracy: 45.10 

Round  32, Train loss: 1.565, Test loss: 1.608, Test accuracy: 85.58 

Round  32, Global train loss: 1.565, Global test loss: 2.018, Global test accuracy: 43.43 

Round  33, Train loss: 1.665, Test loss: 1.626, Test accuracy: 83.78 

Round  33, Global train loss: 1.665, Global test loss: 1.973, Global test accuracy: 48.68 

Round  34, Train loss: 1.614, Test loss: 1.597, Test accuracy: 86.70 

Round  34, Global train loss: 1.614, Global test loss: 1.960, Global test accuracy: 49.67 

Round  35, Train loss: 1.498, Test loss: 1.598, Test accuracy: 86.73 

Round  35, Global train loss: 1.498, Global test loss: 1.990, Global test accuracy: 46.57 

Round  36, Train loss: 1.646, Test loss: 1.573, Test accuracy: 89.25 

Round  36, Global train loss: 1.646, Global test loss: 1.967, Global test accuracy: 49.12 

Round  37, Train loss: 1.546, Test loss: 1.591, Test accuracy: 87.35 

Round  37, Global train loss: 1.546, Global test loss: 1.989, Global test accuracy: 46.17 

Round  38, Train loss: 1.678, Test loss: 1.597, Test accuracy: 86.73 

Round  38, Global train loss: 1.678, Global test loss: 2.013, Global test accuracy: 43.60 

Round  39, Train loss: 1.547, Test loss: 1.593, Test accuracy: 87.12 

Round  39, Global train loss: 1.547, Global test loss: 1.978, Global test accuracy: 47.68 

Round  40, Train loss: 1.587, Test loss: 1.595, Test accuracy: 86.90 

Round  40, Global train loss: 1.587, Global test loss: 1.960, Global test accuracy: 50.05 

Round  41, Train loss: 1.554, Test loss: 1.594, Test accuracy: 87.13 

Round  41, Global train loss: 1.554, Global test loss: 1.975, Global test accuracy: 47.98 

Round  42, Train loss: 1.500, Test loss: 1.578, Test accuracy: 88.72 

Round  42, Global train loss: 1.500, Global test loss: 1.978, Global test accuracy: 48.03 

Round  43, Train loss: 1.531, Test loss: 1.566, Test accuracy: 89.75 

Round  43, Global train loss: 1.531, Global test loss: 1.978, Global test accuracy: 47.85 

Round  44, Train loss: 1.639, Test loss: 1.553, Test accuracy: 91.05 

Round  44, Global train loss: 1.639, Global test loss: 1.988, Global test accuracy: 46.47 

Round  45, Train loss: 1.572, Test loss: 1.552, Test accuracy: 91.25 

Round  45, Global train loss: 1.572, Global test loss: 2.010, Global test accuracy: 44.25 

Round  46, Train loss: 1.484, Test loss: 1.536, Test accuracy: 92.98 

Round  46, Global train loss: 1.484, Global test loss: 1.981, Global test accuracy: 47.57 

Round  47, Train loss: 1.573, Test loss: 1.535, Test accuracy: 93.05 

Round  47, Global train loss: 1.573, Global test loss: 2.030, Global test accuracy: 42.28 

Round  48, Train loss: 1.498, Test loss: 1.520, Test accuracy: 94.65 

Round  48, Global train loss: 1.498, Global test loss: 2.015, Global test accuracy: 43.92 

Round  49, Train loss: 1.591, Test loss: 1.549, Test accuracy: 91.60 

Round  49, Global train loss: 1.591, Global test loss: 1.969, Global test accuracy: 48.72 

Final Round, Train loss: 1.552, Test loss: 1.529, Test accuracy: 93.60 

Final Round, Global train loss: 1.552, Global test loss: 1.969, Global test accuracy: 48.72 

Average accuracy final 10 rounds: 90.70833333333334 

Average global accuracy final 10 rounds: 46.711666666666666 

387.4443018436432
[1.7199361324310303, 2.4735257625579834, 3.2202956676483154, 3.971956491470337, 4.724100112915039, 5.476663112640381, 6.227939128875732, 6.97710394859314, 7.71976113319397, 8.358116149902344, 9.01540231704712, 9.658090591430664, 10.306960821151733, 10.948471546173096, 11.585241794586182, 12.2267427444458, 12.869807720184326, 13.593409061431885, 14.280712366104126, 14.921757936477661, 15.557114601135254, 16.199694633483887, 16.842939615249634, 17.47892951965332, 18.115679502487183, 18.775620698928833, 19.41798424720764, 20.055426359176636, 20.682244300842285, 21.38401770591736, 22.020994186401367, 22.661583185195923, 23.29838490486145, 23.933048963546753, 24.56997036933899, 25.201504468917847, 25.902869939804077, 26.590749979019165, 27.272208213806152, 27.953375101089478, 28.59189248085022, 29.227907180786133, 29.908549785614014, 30.596330881118774, 31.277987957000732, 31.96284246444702, 32.66260313987732, 33.30473780632019, 33.943520307540894, 34.585657596588135, 35.86308765411377]
[34.15, 38.06666666666667, 51.733333333333334, 61.03333333333333, 70.38333333333334, 75.06666666666666, 78.16666666666667, 77.11666666666666, 77.68333333333334, 79.23333333333333, 79.21666666666667, 79.73333333333333, 81.23333333333333, 82.71666666666667, 82.7, 82.63333333333334, 81.33333333333333, 81.4, 83.01666666666667, 84.4, 84.21666666666667, 84.13333333333334, 82.6, 81.06666666666666, 81.18333333333334, 79.73333333333333, 80.95, 81.23333333333333, 83.91666666666667, 84.11666666666666, 85.18333333333334, 85.33333333333333, 85.58333333333333, 83.78333333333333, 86.7, 86.73333333333333, 89.25, 87.35, 86.73333333333333, 87.11666666666666, 86.9, 87.13333333333334, 88.71666666666667, 89.75, 91.05, 91.25, 92.98333333333333, 93.05, 94.65, 91.6, 93.6]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.293, Test loss: 2.284, Test accuracy: 33.43 

Round   1, Train loss: 2.266, Test loss: 2.231, Test accuracy: 33.33 

Round   2, Train loss: 2.156, Test loss: 2.124, Test accuracy: 33.33 

Round   3, Train loss: 2.087, Test loss: 2.099, Test accuracy: 33.32 

Round   4, Train loss: 2.061, Test loss: 2.086, Test accuracy: 33.90 

Round   5, Train loss: 2.007, Test loss: 2.062, Test accuracy: 38.68 

Round   6, Train loss: 2.025, Test loss: 2.050, Test accuracy: 42.15 

Round   7, Train loss: 1.916, Test loss: 2.010, Test accuracy: 46.57 

Round   8, Train loss: 1.873, Test loss: 1.970, Test accuracy: 51.42 

Round   9, Train loss: 1.867, Test loss: 1.950, Test accuracy: 53.30 

Round  10, Train loss: 1.821, Test loss: 1.940, Test accuracy: 53.02 

Round  11, Train loss: 1.824, Test loss: 1.936, Test accuracy: 53.08 

Round  12, Train loss: 1.708, Test loss: 1.892, Test accuracy: 58.20 

Round  13, Train loss: 1.772, Test loss: 1.860, Test accuracy: 61.20 

Round  14, Train loss: 1.766, Test loss: 1.817, Test accuracy: 65.75 

Round  15, Train loss: 1.776, Test loss: 1.806, Test accuracy: 66.88 

Round  16, Train loss: 1.679, Test loss: 1.800, Test accuracy: 67.13 

Round  17, Train loss: 1.667, Test loss: 1.783, Test accuracy: 68.90 

Round  18, Train loss: 1.786, Test loss: 1.769, Test accuracy: 70.22 

Round  19, Train loss: 1.813, Test loss: 1.763, Test accuracy: 71.18 

Round  20, Train loss: 1.715, Test loss: 1.759, Test accuracy: 71.37 

Round  21, Train loss: 1.713, Test loss: 1.752, Test accuracy: 72.23 

Round  22, Train loss: 1.681, Test loss: 1.754, Test accuracy: 71.55 

Round  23, Train loss: 1.711, Test loss: 1.753, Test accuracy: 71.50 

Round  24, Train loss: 1.791, Test loss: 1.735, Test accuracy: 73.60 

Round  25, Train loss: 1.676, Test loss: 1.739, Test accuracy: 72.88 

Round  26, Train loss: 1.683, Test loss: 1.734, Test accuracy: 73.60 

Round  27, Train loss: 1.697, Test loss: 1.735, Test accuracy: 73.25 

Round  28, Train loss: 1.685, Test loss: 1.726, Test accuracy: 74.23 

Round  29, Train loss: 1.709, Test loss: 1.728, Test accuracy: 74.03 

Round  30, Train loss: 1.634, Test loss: 1.728, Test accuracy: 74.15 

Round  31, Train loss: 1.683, Test loss: 1.722, Test accuracy: 74.80 

Round  32, Train loss: 1.760, Test loss: 1.728, Test accuracy: 74.18 

Round  33, Train loss: 1.737, Test loss: 1.711, Test accuracy: 76.12 

Round  34, Train loss: 1.618, Test loss: 1.699, Test accuracy: 77.18 

Round  35, Train loss: 1.667, Test loss: 1.693, Test accuracy: 77.88 

Round  36, Train loss: 1.658, Test loss: 1.684, Test accuracy: 78.88 

Round  37, Train loss: 1.624, Test loss: 1.686, Test accuracy: 78.32 

Round  38, Train loss: 1.570, Test loss: 1.681, Test accuracy: 79.02 

Round  39, Train loss: 1.638, Test loss: 1.680, Test accuracy: 78.72 

Round  40, Train loss: 1.699, Test loss: 1.682, Test accuracy: 78.57 

Round  41, Train loss: 1.619, Test loss: 1.679, Test accuracy: 78.87 

Round  42, Train loss: 1.663, Test loss: 1.680, Test accuracy: 78.87 

Round  43, Train loss: 1.725, Test loss: 1.676, Test accuracy: 79.15 

Round  44, Train loss: 1.664, Test loss: 1.677, Test accuracy: 78.88 

Round  45, Train loss: 1.669, Test loss: 1.676, Test accuracy: 79.18 

Round  46, Train loss: 1.667, Test loss: 1.674, Test accuracy: 79.28 

Round  47, Train loss: 1.602, Test loss: 1.674, Test accuracy: 79.35 

Round  48, Train loss: 1.562, Test loss: 1.672, Test accuracy: 79.52 

Round  49, Train loss: 1.657, Test loss: 1.669, Test accuracy: 79.77 

Final Round, Train loss: 1.648, Test loss: 1.668, Test accuracy: 79.75 

Average accuracy final 10 rounds: 79.14333333333335 

263.97714161872864
[1.506239891052246, 2.056074857711792, 2.6107335090637207, 3.164245843887329, 3.7206430435180664, 4.276404619216919, 4.8318445682525635, 5.386919975280762, 5.94710898399353, 6.503109693527222, 7.059029817581177, 7.6167473793029785, 8.171704769134521, 8.730901956558228, 9.32152533531189, 9.915951251983643, 10.46926236152649, 11.037051677703857, 11.594938516616821, 12.152310132980347, 12.745569229125977, 13.305328130722046, 13.861201763153076, 14.418002605438232, 15.063661575317383, 15.725748062133789, 16.3856942653656, 17.032468557357788, 17.58224606513977, 18.12900447845459, 18.588463306427002, 19.059720039367676, 19.690420866012573, 20.24164628982544, 20.82904052734375, 21.37796664237976, 21.93229866027832, 22.48217248916626, 23.042333126068115, 23.59698247909546, 24.15682363510132, 24.715227127075195, 25.271607398986816, 25.831777095794678, 26.39199709892273, 26.956570863723755, 27.54211688041687, 28.127972841262817, 28.68104600906372, 29.239725589752197, 30.225103616714478]
[33.43333333333333, 33.333333333333336, 33.333333333333336, 33.31666666666667, 33.9, 38.68333333333333, 42.15, 46.56666666666667, 51.416666666666664, 53.3, 53.016666666666666, 53.083333333333336, 58.2, 61.2, 65.75, 66.88333333333334, 67.13333333333334, 68.9, 70.21666666666667, 71.18333333333334, 71.36666666666666, 72.23333333333333, 71.55, 71.5, 73.6, 72.88333333333334, 73.6, 73.25, 74.23333333333333, 74.03333333333333, 74.15, 74.8, 74.18333333333334, 76.11666666666666, 77.18333333333334, 77.88333333333334, 78.88333333333334, 78.31666666666666, 79.01666666666667, 78.71666666666667, 78.56666666666666, 78.86666666666666, 78.86666666666666, 79.15, 78.88333333333334, 79.18333333333334, 79.28333333333333, 79.35, 79.51666666666667, 79.76666666666667, 79.75]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 291, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 1498, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 524, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 295, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 832, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.267, Test loss: 2.217, Test accuracy: 34.10 

Round   0, Global train loss: 2.267, Global test loss: 2.228, Global test accuracy: 33.33 

Round   1, Train loss: 2.053, Test loss: 2.025, Test accuracy: 43.87 

Round   1, Global train loss: 2.053, Global test loss: 2.084, Global test accuracy: 35.07 

Round   2, Train loss: 1.741, Test loss: 1.875, Test accuracy: 63.98 

Round   2, Global train loss: 1.741, Global test loss: 2.049, Global test accuracy: 40.67 

Round   3, Train loss: 1.712, Test loss: 1.802, Test accuracy: 70.88 

Round   3, Global train loss: 1.712, Global test loss: 2.054, Global test accuracy: 39.70 

Round   4, Train loss: 1.623, Test loss: 1.704, Test accuracy: 80.22 

Round   4, Global train loss: 1.623, Global test loss: 2.037, Global test accuracy: 41.50 

Round   5, Train loss: 1.647, Test loss: 1.654, Test accuracy: 84.82 

Round   5, Global train loss: 1.647, Global test loss: 2.051, Global test accuracy: 40.38 

Round   6, Train loss: 1.530, Test loss: 1.598, Test accuracy: 90.82 

Round   6, Global train loss: 1.530, Global test loss: 2.051, Global test accuracy: 40.67 

Round   7, Train loss: 1.493, Test loss: 1.581, Test accuracy: 91.72 

Round   7, Global train loss: 1.493, Global test loss: 2.053, Global test accuracy: 39.30 

Round   8, Train loss: 1.506, Test loss: 1.559, Test accuracy: 93.65 

Round   8, Global train loss: 1.506, Global test loss: 2.031, Global test accuracy: 42.42 

Round   9, Train loss: 1.482, Test loss: 1.565, Test accuracy: 92.28 

Round   9, Global train loss: 1.482, Global test loss: 2.058, Global test accuracy: 39.38 

Round  10, Train loss: 1.489, Test loss: 1.555, Test accuracy: 92.90 

Round  10, Global train loss: 1.489, Global test loss: 2.045, Global test accuracy: 39.75 

Round  11, Train loss: 1.486, Test loss: 1.548, Test accuracy: 92.88 

Round  11, Global train loss: 1.486, Global test loss: 2.053, Global test accuracy: 39.63 

Round  12, Train loss: 1.502, Test loss: 1.523, Test accuracy: 95.25 

Round  12, Global train loss: 1.502, Global test loss: 2.039, Global test accuracy: 41.55 

Round  13, Train loss: 1.476, Test loss: 1.521, Test accuracy: 95.28 

Round  13, Global train loss: 1.476, Global test loss: 2.057, Global test accuracy: 37.40 

Round  14, Train loss: 1.472, Test loss: 1.520, Test accuracy: 95.25 

Round  14, Global train loss: 1.472, Global test loss: 2.046, Global test accuracy: 40.97 

Round  15, Train loss: 1.472, Test loss: 1.519, Test accuracy: 95.30 

Round  15, Global train loss: 1.472, Global test loss: 2.037, Global test accuracy: 40.65 

Round  16, Train loss: 1.466, Test loss: 1.519, Test accuracy: 95.28 

Round  16, Global train loss: 1.466, Global test loss: 2.038, Global test accuracy: 40.48 

Round  17, Train loss: 1.476, Test loss: 1.517, Test accuracy: 95.37 

Round  17, Global train loss: 1.476, Global test loss: 2.045, Global test accuracy: 41.23 

Round  18, Train loss: 1.467, Test loss: 1.516, Test accuracy: 95.38 

Round  18, Global train loss: 1.467, Global test loss: 2.048, Global test accuracy: 39.38 

Round  19, Train loss: 1.470, Test loss: 1.515, Test accuracy: 95.42 

Round  19, Global train loss: 1.470, Global test loss: 2.034, Global test accuracy: 41.35 

Round  20, Train loss: 1.475, Test loss: 1.515, Test accuracy: 95.23 

Round  20, Global train loss: 1.475, Global test loss: 2.070, Global test accuracy: 37.52 

Round  21, Train loss: 1.467, Test loss: 1.514, Test accuracy: 95.35 

Round  21, Global train loss: 1.467, Global test loss: 2.064, Global test accuracy: 37.77 

Round  22, Train loss: 1.468, Test loss: 1.514, Test accuracy: 95.35 

Round  22, Global train loss: 1.468, Global test loss: 2.072, Global test accuracy: 37.27 

Round  23, Train loss: 1.469, Test loss: 1.513, Test accuracy: 95.37 

Round  23, Global train loss: 1.469, Global test loss: 2.031, Global test accuracy: 41.93 

Round  24, Train loss: 1.468, Test loss: 1.514, Test accuracy: 95.33 

Round  24, Global train loss: 1.468, Global test loss: 2.041, Global test accuracy: 41.83 

Round  25, Train loss: 1.465, Test loss: 1.514, Test accuracy: 95.27 

Round  25, Global train loss: 1.465, Global test loss: 2.052, Global test accuracy: 38.85 

Round  26, Train loss: 1.467, Test loss: 1.513, Test accuracy: 95.28 

Round  26, Global train loss: 1.467, Global test loss: 2.041, Global test accuracy: 40.68 

Round  27, Train loss: 1.467, Test loss: 1.513, Test accuracy: 95.35 

Round  27, Global train loss: 1.467, Global test loss: 2.063, Global test accuracy: 38.08 

Round  28, Train loss: 1.466, Test loss: 1.513, Test accuracy: 95.35 

Round  28, Global train loss: 1.466, Global test loss: 2.049, Global test accuracy: 39.97 

Round  29, Train loss: 1.466, Test loss: 1.513, Test accuracy: 95.33 

Round  29, Global train loss: 1.466, Global test loss: 2.041, Global test accuracy: 41.98 

Round  30, Train loss: 1.466, Test loss: 1.512, Test accuracy: 95.32 

Round  30, Global train loss: 1.466, Global test loss: 2.062, Global test accuracy: 39.15 

Round  31, Train loss: 1.465, Test loss: 1.512, Test accuracy: 95.33 

Round  31, Global train loss: 1.465, Global test loss: 2.056, Global test accuracy: 39.83 

Round  32, Train loss: 1.463, Test loss: 1.512, Test accuracy: 95.33 

Round  32, Global train loss: 1.463, Global test loss: 2.054, Global test accuracy: 39.40 

Round  33, Train loss: 1.466, Test loss: 1.512, Test accuracy: 95.35 

Round  33, Global train loss: 1.466, Global test loss: 2.034, Global test accuracy: 41.92 

Round  34, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.32 

Round  34, Global train loss: 1.464, Global test loss: 2.033, Global test accuracy: 41.50 

Round  35, Train loss: 1.465, Test loss: 1.512, Test accuracy: 95.35 

Round  35, Global train loss: 1.465, Global test loss: 2.030, Global test accuracy: 42.13 

Round  36, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.35 

Round  36, Global train loss: 1.464, Global test loss: 2.031, Global test accuracy: 43.05 

Round  37, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.38 

Round  37, Global train loss: 1.464, Global test loss: 2.048, Global test accuracy: 40.17 

Round  38, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.35 

Round  38, Global train loss: 1.463, Global test loss: 2.042, Global test accuracy: 41.15 

Round  39, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.37 

Round  39, Global train loss: 1.464, Global test loss: 2.049, Global test accuracy: 40.77 

Round  40, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.37 

Round  40, Global train loss: 1.464, Global test loss: 2.042, Global test accuracy: 40.90 

Round  41, Train loss: 1.465, Test loss: 1.511, Test accuracy: 95.37 

Round  41, Global train loss: 1.465, Global test loss: 2.045, Global test accuracy: 40.13 

Round  42, Train loss: 1.465, Test loss: 1.511, Test accuracy: 95.38 

Round  42, Global train loss: 1.465, Global test loss: 2.038, Global test accuracy: 41.08 

Round  43, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.37 

Round  43, Global train loss: 1.464, Global test loss: 2.056, Global test accuracy: 39.20 

Round  44, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.38 

Round  44, Global train loss: 1.464, Global test loss: 2.059, Global test accuracy: 38.62 

Round  45, Train loss: 1.465, Test loss: 1.511, Test accuracy: 95.37 

Round  45, Global train loss: 1.465, Global test loss: 2.026, Global test accuracy: 42.40 

Round  46, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.38 

Round  46, Global train loss: 1.463, Global test loss: 2.024, Global test accuracy: 43.23 

Round  47, Train loss: 1.465, Test loss: 1.511, Test accuracy: 95.38 

Round  47, Global train loss: 1.465, Global test loss: 2.042, Global test accuracy: 40.67 

Round  48, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.40 

Round  48, Global train loss: 1.463, Global test loss: 2.031, Global test accuracy: 42.42 

Round  49, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.35 

Round  49, Global train loss: 1.464, Global test loss: 2.044, Global test accuracy: 40.47 

Final Round, Train loss: 1.463, Test loss: 1.510, Test accuracy: 95.42 

Final Round, Global train loss: 1.463, Global test loss: 2.044, Global test accuracy: 40.47 

Average accuracy final 10 rounds: 95.37499999999999 

Average global accuracy final 10 rounds: 40.91166666666667 

384.8419609069824
[1.6015968322753906, 2.285987615585327, 2.9442803859710693, 3.6032602787017822, 4.26263689994812, 4.922268390655518, 5.575956583023071, 6.235334873199463, 6.8902552127838135, 7.546449184417725, 8.205151557922363, 8.864353895187378, 9.52736520767212, 10.190529584884644, 10.896677732467651, 11.55074143409729, 12.207813739776611, 12.86427116394043, 13.51908564567566, 14.176271677017212, 14.83403205871582, 15.485530138015747, 16.141855239868164, 16.80107069015503, 17.462393522262573, 18.11845350265503, 18.77638816833496, 19.432942628860474, 20.093000173568726, 20.753188371658325, 21.41299867630005, 22.06639862060547, 22.72618842124939, 23.384323596954346, 24.04773736000061, 24.70528745651245, 25.366474151611328, 26.022902965545654, 26.684407711029053, 27.344276189804077, 28.01952838897705, 28.72401762008667, 29.43243670463562, 30.137743711471558, 30.845704793930054, 31.547672271728516, 32.252707719802856, 32.95653772354126, 33.656965017318726, 34.360111474990845, 35.749141454696655]
[34.1, 43.86666666666667, 63.983333333333334, 70.88333333333334, 80.21666666666667, 84.81666666666666, 90.81666666666666, 91.71666666666667, 93.65, 92.28333333333333, 92.9, 92.88333333333334, 95.25, 95.28333333333333, 95.25, 95.3, 95.28333333333333, 95.36666666666666, 95.38333333333334, 95.41666666666667, 95.23333333333333, 95.35, 95.35, 95.36666666666666, 95.33333333333333, 95.26666666666667, 95.28333333333333, 95.35, 95.35, 95.33333333333333, 95.31666666666666, 95.33333333333333, 95.33333333333333, 95.35, 95.31666666666666, 95.35, 95.35, 95.38333333333334, 95.35, 95.36666666666666, 95.36666666666666, 95.36666666666666, 95.38333333333334, 95.36666666666666, 95.38333333333334, 95.36666666666666, 95.38333333333334, 95.38333333333334, 95.4, 95.35, 95.41666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.264, Test loss: 2.209, Test accuracy: 34.77 

Round   0, Global train loss: 2.264, Global test loss: 2.222, Global test accuracy: 33.33 

Round   1, Train loss: 2.043, Test loss: 2.042, Test accuracy: 44.35 

Round   1, Global train loss: 2.043, Global test loss: 2.084, Global test accuracy: 36.78 

Round   2, Train loss: 1.809, Test loss: 1.904, Test accuracy: 59.18 

Round   2, Global train loss: 1.809, Global test loss: 2.042, Global test accuracy: 41.37 

Round   3, Train loss: 1.713, Test loss: 1.815, Test accuracy: 67.48 

Round   3, Global train loss: 1.713, Global test loss: 2.048, Global test accuracy: 38.22 

Round   4, Train loss: 1.626, Test loss: 1.748, Test accuracy: 74.00 

Round   4, Global train loss: 1.626, Global test loss: 2.044, Global test accuracy: 40.00 

Round   5, Train loss: 1.665, Test loss: 1.692, Test accuracy: 79.07 

Round   5, Global train loss: 1.665, Global test loss: 2.029, Global test accuracy: 41.67 

Round   6, Train loss: 1.765, Test loss: 1.695, Test accuracy: 78.45 

Round   6, Global train loss: 1.765, Global test loss: 2.028, Global test accuracy: 42.20 

Round   7, Train loss: 1.636, Test loss: 1.687, Test accuracy: 79.35 

Round   7, Global train loss: 1.636, Global test loss: 2.034, Global test accuracy: 41.52 

Round   8, Train loss: 1.649, Test loss: 1.653, Test accuracy: 82.30 

Round   8, Global train loss: 1.649, Global test loss: 2.021, Global test accuracy: 42.70 

Round   9, Train loss: 1.614, Test loss: 1.660, Test accuracy: 81.13 

Round   9, Global train loss: 1.614, Global test loss: 2.041, Global test accuracy: 41.08 

Round  10, Train loss: 1.642, Test loss: 1.648, Test accuracy: 82.05 

Round  10, Global train loss: 1.642, Global test loss: 2.029, Global test accuracy: 41.88 

Round  11, Train loss: 1.741, Test loss: 1.654, Test accuracy: 81.32 

Round  11, Global train loss: 1.741, Global test loss: 2.032, Global test accuracy: 41.85 

Round  12, Train loss: 1.640, Test loss: 1.643, Test accuracy: 82.28 

Round  12, Global train loss: 1.640, Global test loss: 2.024, Global test accuracy: 43.10 

Round  13, Train loss: 1.604, Test loss: 1.595, Test accuracy: 87.25 

Round  13, Global train loss: 1.604, Global test loss: 2.050, Global test accuracy: 40.38 

Round  14, Train loss: 1.691, Test loss: 1.609, Test accuracy: 85.87 

Round  14, Global train loss: 1.691, Global test loss: 2.024, Global test accuracy: 42.95 

Round  15, Train loss: 1.662, Test loss: 1.636, Test accuracy: 83.23 

Round  15, Global train loss: 1.662, Global test loss: 2.038, Global test accuracy: 41.43 

Round  16, Train loss: 1.666, Test loss: 1.639, Test accuracy: 82.75 

Round  16, Global train loss: 1.666, Global test loss: 2.017, Global test accuracy: 43.33 

Round  17, Train loss: 1.579, Test loss: 1.606, Test accuracy: 86.15 

Round  17, Global train loss: 1.579, Global test loss: 2.035, Global test accuracy: 41.68 

Round  18, Train loss: 1.667, Test loss: 1.608, Test accuracy: 85.90 

Round  18, Global train loss: 1.667, Global test loss: 2.032, Global test accuracy: 41.50 

Round  19, Train loss: 1.651, Test loss: 1.596, Test accuracy: 87.12 

Round  19, Global train loss: 1.651, Global test loss: 2.014, Global test accuracy: 43.38 

Round  20, Train loss: 1.561, Test loss: 1.595, Test accuracy: 87.17 

Round  20, Global train loss: 1.561, Global test loss: 2.014, Global test accuracy: 43.52 

Round  21, Train loss: 1.642, Test loss: 1.580, Test accuracy: 88.63 

Round  21, Global train loss: 1.642, Global test loss: 2.018, Global test accuracy: 42.43 

Round  22, Train loss: 1.530, Test loss: 1.578, Test accuracy: 88.82 

Round  22, Global train loss: 1.530, Global test loss: 2.016, Global test accuracy: 43.28 

Round  23, Train loss: 1.597, Test loss: 1.562, Test accuracy: 90.48 

Round  23, Global train loss: 1.597, Global test loss: 2.028, Global test accuracy: 42.62 

Round  24, Train loss: 1.605, Test loss: 1.561, Test accuracy: 90.58 

Round  24, Global train loss: 1.605, Global test loss: 2.023, Global test accuracy: 42.92 

Round  25, Train loss: 1.602, Test loss: 1.576, Test accuracy: 89.10 

Round  25, Global train loss: 1.602, Global test loss: 2.044, Global test accuracy: 40.25 

Round  26, Train loss: 1.612, Test loss: 1.575, Test accuracy: 89.12 

Round  26, Global train loss: 1.612, Global test loss: 2.043, Global test accuracy: 39.92 

Round  27, Train loss: 1.726, Test loss: 1.594, Test accuracy: 87.15 

Round  27, Global train loss: 1.726, Global test loss: 2.025, Global test accuracy: 42.75 

Round  28, Train loss: 1.659, Test loss: 1.592, Test accuracy: 87.35 

Round  28, Global train loss: 1.659, Global test loss: 2.027, Global test accuracy: 42.17 

Round  29, Train loss: 1.609, Test loss: 1.596, Test accuracy: 86.80 

Round  29, Global train loss: 1.609, Global test loss: 2.031, Global test accuracy: 41.88 

Round  30, Train loss: 1.488, Test loss: 1.596, Test accuracy: 86.82 

Round  30, Global train loss: 1.488, Global test loss: 2.039, Global test accuracy: 40.72 

Round  31, Train loss: 1.669, Test loss: 1.595, Test accuracy: 86.93 

Round  31, Global train loss: 1.669, Global test loss: 2.014, Global test accuracy: 44.12 

Round  32, Train loss: 1.496, Test loss: 1.596, Test accuracy: 86.88 

Round  32, Global train loss: 1.496, Global test loss: 2.001, Global test accuracy: 44.90 

Round  33, Train loss: 1.598, Test loss: 1.594, Test accuracy: 87.05 

Round  33, Global train loss: 1.598, Global test loss: 2.019, Global test accuracy: 43.20 

Round  34, Train loss: 1.591, Test loss: 1.594, Test accuracy: 86.98 

Round  34, Global train loss: 1.591, Global test loss: 2.022, Global test accuracy: 43.02 

Round  35, Train loss: 1.597, Test loss: 1.609, Test accuracy: 85.40 

Round  35, Global train loss: 1.597, Global test loss: 2.020, Global test accuracy: 42.50 

Round  36, Train loss: 1.585, Test loss: 1.595, Test accuracy: 86.87 

Round  36, Global train loss: 1.585, Global test loss: 2.043, Global test accuracy: 41.22 

Round  37, Train loss: 1.651, Test loss: 1.611, Test accuracy: 85.35 

Round  37, Global train loss: 1.651, Global test loss: 2.039, Global test accuracy: 41.53 

Round  38, Train loss: 1.787, Test loss: 1.627, Test accuracy: 83.73 

Round  38, Global train loss: 1.787, Global test loss: 2.030, Global test accuracy: 42.45 

Round  39, Train loss: 1.601, Test loss: 1.597, Test accuracy: 86.63 

Round  39, Global train loss: 1.601, Global test loss: 2.011, Global test accuracy: 43.97 

Round  40, Train loss: 1.655, Test loss: 1.589, Test accuracy: 87.50 

Round  40, Global train loss: 1.655, Global test loss: 2.035, Global test accuracy: 41.68 

Round  41, Train loss: 1.673, Test loss: 1.576, Test accuracy: 88.87 

Round  41, Global train loss: 1.673, Global test loss: 2.027, Global test accuracy: 42.67 

Round  42, Train loss: 1.584, Test loss: 1.564, Test accuracy: 90.00 

Round  42, Global train loss: 1.584, Global test loss: 2.020, Global test accuracy: 43.32 

Round  43, Train loss: 1.676, Test loss: 1.594, Test accuracy: 86.88 

Round  43, Global train loss: 1.676, Global test loss: 2.046, Global test accuracy: 40.85 

Round  44, Train loss: 1.632, Test loss: 1.581, Test accuracy: 88.27 

Round  44, Global train loss: 1.632, Global test loss: 2.024, Global test accuracy: 43.17 

Round  45, Train loss: 1.600, Test loss: 1.569, Test accuracy: 89.50 

Round  45, Global train loss: 1.600, Global test loss: 2.030, Global test accuracy: 42.05 

Round  46, Train loss: 1.521, Test loss: 1.551, Test accuracy: 91.27 

Round  46, Global train loss: 1.521, Global test loss: 2.025, Global test accuracy: 42.48 

Round  47, Train loss: 1.603, Test loss: 1.547, Test accuracy: 91.63 

Round  47, Global train loss: 1.603, Global test loss: 2.023, Global test accuracy: 42.73 

Round  48, Train loss: 1.588, Test loss: 1.548, Test accuracy: 91.57 

Round  48, Global train loss: 1.588, Global test loss: 2.038, Global test accuracy: 41.28 

Round  49, Train loss: 1.488, Test loss: 1.548, Test accuracy: 91.48 

Round  49, Global train loss: 1.488, Global test loss: 2.019, Global test accuracy: 43.37 

Final Round, Train loss: 1.577, Test loss: 1.571, Test accuracy: 89.13 

Final Round, Global train loss: 1.577, Global test loss: 2.019, Global test accuracy: 43.37 

Average accuracy final 10 rounds: 89.69666666666669 

Average global accuracy final 10 rounds: 42.36 

386.2340931892395
[1.6294903755187988, 2.2953133583068848, 2.9539973735809326, 3.6200597286224365, 4.284065246582031, 4.941546440124512, 5.604535102844238, 6.284347295761108, 6.978271722793579, 7.636226415634155, 8.312991857528687, 8.978003025054932, 9.641211986541748, 10.304056406021118, 10.967320203781128, 11.628497123718262, 12.2928786277771, 12.95756220817566, 13.616860151290894, 14.289125204086304, 14.95092248916626, 15.611505508422852, 16.274900674819946, 16.934462547302246, 17.594613313674927, 18.264713764190674, 18.92691707611084, 19.587141513824463, 20.253906726837158, 20.915903329849243, 21.580859899520874, 22.242621421813965, 22.903931140899658, 23.564292907714844, 24.214869260787964, 24.875768661499023, 25.539263248443604, 26.191047191619873, 26.85469341278076, 27.52096176147461, 28.178430795669556, 28.83441138267517, 29.487695693969727, 30.146093368530273, 30.744954824447632, 31.351442575454712, 32.01766800880432, 32.676485776901245, 33.34237027168274, 34.00701928138733, 35.330782651901245]
[34.766666666666666, 44.35, 59.18333333333333, 67.48333333333333, 74.0, 79.06666666666666, 78.45, 79.35, 82.3, 81.13333333333334, 82.05, 81.31666666666666, 82.28333333333333, 87.25, 85.86666666666666, 83.23333333333333, 82.75, 86.15, 85.9, 87.11666666666666, 87.16666666666667, 88.63333333333334, 88.81666666666666, 90.48333333333333, 90.58333333333333, 89.1, 89.11666666666666, 87.15, 87.35, 86.8, 86.81666666666666, 86.93333333333334, 86.88333333333334, 87.05, 86.98333333333333, 85.4, 86.86666666666666, 85.35, 83.73333333333333, 86.63333333333334, 87.5, 88.86666666666666, 90.0, 86.88333333333334, 88.26666666666667, 89.5, 91.26666666666667, 91.63333333333334, 91.56666666666666, 91.48333333333333, 89.13333333333334]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.291, Test loss: 2.282, Test accuracy: 33.33 

Round   1, Train loss: 2.259, Test loss: 2.220, Test accuracy: 33.33 

Round   2, Train loss: 2.142, Test loss: 2.116, Test accuracy: 33.65 

Round   3, Train loss: 2.040, Test loss: 2.085, Test accuracy: 34.67 

Round   4, Train loss: 1.956, Test loss: 2.050, Test accuracy: 40.10 

Round   5, Train loss: 1.985, Test loss: 1.995, Test accuracy: 47.28 

Round   6, Train loss: 1.881, Test loss: 1.958, Test accuracy: 51.53 

Round   7, Train loss: 1.845, Test loss: 1.938, Test accuracy: 54.98 

Round   8, Train loss: 1.909, Test loss: 1.943, Test accuracy: 53.60 

Round   9, Train loss: 1.948, Test loss: 1.912, Test accuracy: 55.75 

Round  10, Train loss: 1.821, Test loss: 1.889, Test accuracy: 58.08 

Round  11, Train loss: 1.763, Test loss: 1.854, Test accuracy: 63.08 

Round  12, Train loss: 1.763, Test loss: 1.835, Test accuracy: 65.38 

Round  13, Train loss: 1.777, Test loss: 1.824, Test accuracy: 65.35 

Round  14, Train loss: 1.783, Test loss: 1.814, Test accuracy: 66.23 

Round  15, Train loss: 1.853, Test loss: 1.801, Test accuracy: 67.78 

Round  16, Train loss: 1.798, Test loss: 1.800, Test accuracy: 67.65 

Round  17, Train loss: 1.788, Test loss: 1.799, Test accuracy: 67.52 

Round  18, Train loss: 1.712, Test loss: 1.800, Test accuracy: 67.17 

Round  19, Train loss: 1.836, Test loss: 1.794, Test accuracy: 67.97 

Round  20, Train loss: 1.794, Test loss: 1.789, Test accuracy: 68.17 

Round  21, Train loss: 1.791, Test loss: 1.784, Test accuracy: 69.07 

Round  22, Train loss: 1.703, Test loss: 1.783, Test accuracy: 68.95 

Round  23, Train loss: 1.732, Test loss: 1.769, Test accuracy: 70.32 

Round  24, Train loss: 1.741, Test loss: 1.769, Test accuracy: 70.67 

Round  25, Train loss: 1.777, Test loss: 1.767, Test accuracy: 70.75 

Round  26, Train loss: 1.805, Test loss: 1.757, Test accuracy: 71.77 

Round  27, Train loss: 1.721, Test loss: 1.758, Test accuracy: 71.78 

Round  28, Train loss: 1.737, Test loss: 1.751, Test accuracy: 72.22 

Round  29, Train loss: 1.684, Test loss: 1.746, Test accuracy: 72.22 

Round  30, Train loss: 1.728, Test loss: 1.744, Test accuracy: 72.43 

Round  31, Train loss: 1.764, Test loss: 1.745, Test accuracy: 72.42 

Round  32, Train loss: 1.713, Test loss: 1.742, Test accuracy: 72.45 

Round  33, Train loss: 1.779, Test loss: 1.740, Test accuracy: 72.48 

Round  34, Train loss: 1.664, Test loss: 1.740, Test accuracy: 72.67 

Round  35, Train loss: 1.769, Test loss: 1.739, Test accuracy: 72.65 

Round  36, Train loss: 1.629, Test loss: 1.738, Test accuracy: 72.88 

Round  37, Train loss: 1.718, Test loss: 1.742, Test accuracy: 72.35 

Round  38, Train loss: 1.676, Test loss: 1.740, Test accuracy: 72.98 

Round  39, Train loss: 1.759, Test loss: 1.736, Test accuracy: 73.15 

Round  40, Train loss: 1.710, Test loss: 1.736, Test accuracy: 72.98 

Round  41, Train loss: 1.610, Test loss: 1.735, Test accuracy: 72.98 

Round  42, Train loss: 1.767, Test loss: 1.733, Test accuracy: 73.38 

Round  43, Train loss: 1.758, Test loss: 1.734, Test accuracy: 73.10 

Round  44, Train loss: 1.710, Test loss: 1.733, Test accuracy: 73.32 

Round  45, Train loss: 1.662, Test loss: 1.734, Test accuracy: 73.25 

Round  46, Train loss: 1.659, Test loss: 1.731, Test accuracy: 73.55 

Round  47, Train loss: 1.657, Test loss: 1.731, Test accuracy: 73.33 

Round  48, Train loss: 1.759, Test loss: 1.732, Test accuracy: 73.28 

Round  49, Train loss: 1.660, Test loss: 1.730, Test accuracy: 73.52 

Final Round, Train loss: 1.705, Test loss: 1.714, Test accuracy: 75.08 

Average accuracy final 10 rounds: 73.27000000000001 

252.8898150920868
[1.5123465061187744, 2.076256513595581, 2.639749526977539, 3.1991724967956543, 3.762040615081787, 4.326517343521118, 4.891637802124023, 5.452959060668945, 6.014220237731934, 6.577899694442749, 7.139805316925049, 7.704773664474487, 8.263286113739014, 8.828561544418335, 9.425849914550781, 10.078240156173706, 10.659729957580566, 11.130985975265503, 11.597899913787842, 12.069947957992554, 12.54159688949585, 13.011608123779297, 13.484743356704712, 13.952616930007935, 14.422581434249878, 14.890494585037231, 15.35878038406372, 15.82727861404419, 16.2928364276886, 16.761300802230835, 17.234726428985596, 17.70166516304016, 18.17016100883484, 18.638006448745728, 19.10840368270874, 19.57764458656311, 20.052704095840454, 20.523184299468994, 20.99770975112915, 21.467272996902466, 21.93203043937683, 22.60794162750244, 23.19753885269165, 23.816721439361572, 24.373608827590942, 24.956698894500732, 25.546313285827637, 26.139888286590576, 26.726138830184937, 27.298989295959473, 28.285162210464478]
[33.333333333333336, 33.333333333333336, 33.65, 34.666666666666664, 40.1, 47.28333333333333, 51.53333333333333, 54.983333333333334, 53.6, 55.75, 58.083333333333336, 63.083333333333336, 65.38333333333334, 65.35, 66.23333333333333, 67.78333333333333, 67.65, 67.51666666666667, 67.16666666666667, 67.96666666666667, 68.16666666666667, 69.06666666666666, 68.95, 70.31666666666666, 70.66666666666667, 70.75, 71.76666666666667, 71.78333333333333, 72.21666666666667, 72.21666666666667, 72.43333333333334, 72.41666666666667, 72.45, 72.48333333333333, 72.66666666666667, 72.65, 72.88333333333334, 72.35, 72.98333333333333, 73.15, 72.98333333333333, 72.98333333333333, 73.38333333333334, 73.1, 73.31666666666666, 73.25, 73.55, 73.33333333333333, 73.28333333333333, 73.51666666666667, 75.08333333333333]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 291, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 1498, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 524, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 295, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 832, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.292, Test loss: 2.278, Test accuracy: 42.27 

Round   0, Global train loss: 2.292, Global test loss: 2.281, Global test accuracy: 37.85 

Round   1, Train loss: 2.144, Test loss: 2.106, Test accuracy: 40.78 

Round   1, Global train loss: 2.144, Global test loss: 2.113, Global test accuracy: 33.32 

Round   2, Train loss: 1.930, Test loss: 2.023, Test accuracy: 47.30 

Round   2, Global train loss: 1.930, Global test loss: 2.082, Global test accuracy: 34.00 

Round   3, Train loss: 1.832, Test loss: 1.914, Test accuracy: 58.83 

Round   3, Global train loss: 1.832, Global test loss: 2.050, Global test accuracy: 41.23 

Round   4, Train loss: 1.745, Test loss: 1.837, Test accuracy: 69.87 

Round   4, Global train loss: 1.745, Global test loss: 2.026, Global test accuracy: 44.67 

Round   5, Train loss: 1.609, Test loss: 1.767, Test accuracy: 76.38 

Round   5, Global train loss: 1.609, Global test loss: 2.022, Global test accuracy: 45.00 

Round   6, Train loss: 1.685, Test loss: 1.715, Test accuracy: 78.97 

Round   6, Global train loss: 1.685, Global test loss: 2.032, Global test accuracy: 43.70 

Round   7, Train loss: 1.565, Test loss: 1.680, Test accuracy: 82.33 

Round   7, Global train loss: 1.565, Global test loss: 2.018, Global test accuracy: 43.95 

Round   8, Train loss: 1.597, Test loss: 1.649, Test accuracy: 84.73 

Round   8, Global train loss: 1.597, Global test loss: 2.019, Global test accuracy: 44.60 

Round   9, Train loss: 1.528, Test loss: 1.629, Test accuracy: 87.15 

Round   9, Global train loss: 1.528, Global test loss: 2.016, Global test accuracy: 44.72 

Round  10, Train loss: 1.548, Test loss: 1.624, Test accuracy: 87.48 

Round  10, Global train loss: 1.548, Global test loss: 2.010, Global test accuracy: 45.37 

Round  11, Train loss: 1.487, Test loss: 1.611, Test accuracy: 88.77 

Round  11, Global train loss: 1.487, Global test loss: 2.003, Global test accuracy: 46.27 

Round  12, Train loss: 1.596, Test loss: 1.591, Test accuracy: 88.37 

Round  12, Global train loss: 1.596, Global test loss: 2.031, Global test accuracy: 41.85 

Round  13, Train loss: 1.477, Test loss: 1.585, Test accuracy: 89.07 

Round  13, Global train loss: 1.477, Global test loss: 2.002, Global test accuracy: 45.77 

Round  14, Train loss: 1.518, Test loss: 1.580, Test accuracy: 89.52 

Round  14, Global train loss: 1.518, Global test loss: 2.029, Global test accuracy: 42.43 

Round  15, Train loss: 1.532, Test loss: 1.563, Test accuracy: 91.50 

Round  15, Global train loss: 1.532, Global test loss: 2.022, Global test accuracy: 43.52 

Round  16, Train loss: 1.490, Test loss: 1.559, Test accuracy: 91.03 

Round  16, Global train loss: 1.490, Global test loss: 2.030, Global test accuracy: 42.70 

Round  17, Train loss: 1.475, Test loss: 1.561, Test accuracy: 90.85 

Round  17, Global train loss: 1.475, Global test loss: 2.027, Global test accuracy: 42.67 

Round  18, Train loss: 1.471, Test loss: 1.564, Test accuracy: 90.50 

Round  18, Global train loss: 1.471, Global test loss: 2.028, Global test accuracy: 41.35 

Round  19, Train loss: 1.530, Test loss: 1.540, Test accuracy: 92.97 

Round  19, Global train loss: 1.530, Global test loss: 2.055, Global test accuracy: 38.03 

Round  20, Train loss: 1.495, Test loss: 1.531, Test accuracy: 93.88 

Round  20, Global train loss: 1.495, Global test loss: 2.032, Global test accuracy: 41.93 

Round  21, Train loss: 1.464, Test loss: 1.531, Test accuracy: 93.88 

Round  21, Global train loss: 1.464, Global test loss: 2.005, Global test accuracy: 45.02 

Round  22, Train loss: 1.521, Test loss: 1.523, Test accuracy: 95.35 

Round  22, Global train loss: 1.521, Global test loss: 2.010, Global test accuracy: 44.55 

Round  23, Train loss: 1.471, Test loss: 1.521, Test accuracy: 95.33 

Round  23, Global train loss: 1.471, Global test loss: 2.030, Global test accuracy: 43.82 

Round  24, Train loss: 1.465, Test loss: 1.521, Test accuracy: 95.37 

Round  24, Global train loss: 1.465, Global test loss: 2.005, Global test accuracy: 45.25 

Round  25, Train loss: 1.465, Test loss: 1.520, Test accuracy: 95.35 

Round  25, Global train loss: 1.465, Global test loss: 1.999, Global test accuracy: 46.32 

Round  26, Train loss: 1.470, Test loss: 1.518, Test accuracy: 95.57 

Round  26, Global train loss: 1.470, Global test loss: 2.056, Global test accuracy: 40.08 

Round  27, Train loss: 1.470, Test loss: 1.517, Test accuracy: 95.68 

Round  27, Global train loss: 1.470, Global test loss: 2.036, Global test accuracy: 41.63 

Round  28, Train loss: 1.472, Test loss: 1.508, Test accuracy: 95.95 

Round  28, Global train loss: 1.472, Global test loss: 2.037, Global test accuracy: 40.88 

Round  29, Train loss: 1.467, Test loss: 1.508, Test accuracy: 96.02 

Round  29, Global train loss: 1.467, Global test loss: 2.030, Global test accuracy: 41.80 

Round  30, Train loss: 1.467, Test loss: 1.508, Test accuracy: 96.03 

Round  30, Global train loss: 1.467, Global test loss: 2.006, Global test accuracy: 44.65 

Round  31, Train loss: 1.468, Test loss: 1.508, Test accuracy: 95.98 

Round  31, Global train loss: 1.468, Global test loss: 2.052, Global test accuracy: 38.82 

Round  32, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.97 

Round  32, Global train loss: 1.464, Global test loss: 2.009, Global test accuracy: 45.12 

Round  33, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.98 

Round  33, Global train loss: 1.465, Global test loss: 1.999, Global test accuracy: 45.98 

Round  34, Train loss: 1.465, Test loss: 1.507, Test accuracy: 96.00 

Round  34, Global train loss: 1.465, Global test loss: 2.016, Global test accuracy: 43.65 

Round  35, Train loss: 1.463, Test loss: 1.507, Test accuracy: 96.03 

Round  35, Global train loss: 1.463, Global test loss: 2.003, Global test accuracy: 45.37 

Round  36, Train loss: 1.466, Test loss: 1.507, Test accuracy: 96.03 

Round  36, Global train loss: 1.466, Global test loss: 2.017, Global test accuracy: 42.48 

Round  37, Train loss: 1.468, Test loss: 1.506, Test accuracy: 96.00 

Round  37, Global train loss: 1.468, Global test loss: 2.020, Global test accuracy: 43.77 

Round  38, Train loss: 1.466, Test loss: 1.506, Test accuracy: 96.02 

Round  38, Global train loss: 1.466, Global test loss: 2.057, Global test accuracy: 39.47 

Round  39, Train loss: 1.465, Test loss: 1.506, Test accuracy: 96.05 

Round  39, Global train loss: 1.465, Global test loss: 2.005, Global test accuracy: 45.15 

Round  40, Train loss: 1.463, Test loss: 1.506, Test accuracy: 96.08 

Round  40, Global train loss: 1.463, Global test loss: 2.051, Global test accuracy: 39.22 

Round  41, Train loss: 1.466, Test loss: 1.506, Test accuracy: 96.07 

Round  41, Global train loss: 1.466, Global test loss: 2.023, Global test accuracy: 42.60 

Round  42, Train loss: 1.468, Test loss: 1.506, Test accuracy: 96.05 

Round  42, Global train loss: 1.468, Global test loss: 2.029, Global test accuracy: 42.35 

Round  43, Train loss: 1.463, Test loss: 1.505, Test accuracy: 96.03 

Round  43, Global train loss: 1.463, Global test loss: 2.011, Global test accuracy: 44.13 

Round  44, Train loss: 1.466, Test loss: 1.505, Test accuracy: 96.03 

Round  44, Global train loss: 1.466, Global test loss: 2.017, Global test accuracy: 44.17 

Round  45, Train loss: 1.464, Test loss: 1.505, Test accuracy: 96.03 

Round  45, Global train loss: 1.464, Global test loss: 1.994, Global test accuracy: 45.82 

Round  46, Train loss: 1.463, Test loss: 1.505, Test accuracy: 96.03 

Round  46, Global train loss: 1.463, Global test loss: 2.024, Global test accuracy: 42.90 

Round  47, Train loss: 1.464, Test loss: 1.505, Test accuracy: 96.02 

Round  47, Global train loss: 1.464, Global test loss: 1.992, Global test accuracy: 46.92 

Round  48, Train loss: 1.464, Test loss: 1.505, Test accuracy: 96.00 

Round  48, Global train loss: 1.464, Global test loss: 2.018, Global test accuracy: 43.57 

Round  49, Train loss: 1.465, Test loss: 1.505, Test accuracy: 96.02 

Round  49, Global train loss: 1.465, Global test loss: 2.060, Global test accuracy: 37.83 

Final Round, Train loss: 1.464, Test loss: 1.504, Test accuracy: 96.00 

Final Round, Global train loss: 1.464, Global test loss: 2.060, Global test accuracy: 37.83 

Average accuracy final 10 rounds: 96.03666666666666 

Average global accuracy final 10 rounds: 42.95 

374.472284078598
[1.5790927410125732, 2.244774341583252, 2.887460708618164, 3.528116464614868, 4.1699042320251465, 4.817570209503174, 5.457812547683716, 6.095457315444946, 6.736260890960693, 7.374924898147583, 8.010761499404907, 8.65255093574524, 9.294525146484375, 9.928468227386475, 10.569484949111938, 11.206646919250488, 11.841516017913818, 12.483811140060425, 13.12739086151123, 13.761509895324707, 14.403480529785156, 15.047404289245605, 15.686544179916382, 16.327507257461548, 16.967161893844604, 17.610411643981934, 18.251842498779297, 18.89231562614441, 19.528865575790405, 20.167478322982788, 20.800626754760742, 21.440528392791748, 22.081379175186157, 22.71759009361267, 23.353333234786987, 23.993443727493286, 24.63235902786255, 25.274111032485962, 25.914822816848755, 26.547356605529785, 27.137550354003906, 27.77760076522827, 28.41663408279419, 29.05674123764038, 29.693132162094116, 30.3279709815979, 30.968527793884277, 31.6117422580719, 32.24488806724548, 32.89107584953308, 34.17097473144531]
[42.266666666666666, 40.78333333333333, 47.3, 58.833333333333336, 69.86666666666666, 76.38333333333334, 78.96666666666667, 82.33333333333333, 84.73333333333333, 87.15, 87.48333333333333, 88.76666666666667, 88.36666666666666, 89.06666666666666, 89.51666666666667, 91.5, 91.03333333333333, 90.85, 90.5, 92.96666666666667, 93.88333333333334, 93.88333333333334, 95.35, 95.33333333333333, 95.36666666666666, 95.35, 95.56666666666666, 95.68333333333334, 95.95, 96.01666666666667, 96.03333333333333, 95.98333333333333, 95.96666666666667, 95.98333333333333, 96.0, 96.03333333333333, 96.03333333333333, 96.0, 96.01666666666667, 96.05, 96.08333333333333, 96.06666666666666, 96.05, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.01666666666667, 96.0, 96.01666666666667, 96.0]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.291, Test loss: 2.277, Test accuracy: 42.98 

Round   0, Global train loss: 2.291, Global test loss: 2.280, Global test accuracy: 38.32 

Round   1, Train loss: 2.166, Test loss: 2.123, Test accuracy: 43.43 

Round   1, Global train loss: 2.166, Global test loss: 2.114, Global test accuracy: 33.33 

Round   2, Train loss: 1.945, Test loss: 2.011, Test accuracy: 50.52 

Round   2, Global train loss: 1.945, Global test loss: 2.056, Global test accuracy: 40.47 

Round   3, Train loss: 1.768, Test loss: 1.889, Test accuracy: 59.23 

Round   3, Global train loss: 1.768, Global test loss: 2.025, Global test accuracy: 43.50 

Round   4, Train loss: 1.829, Test loss: 1.794, Test accuracy: 71.07 

Round   4, Global train loss: 1.829, Global test loss: 1.996, Global test accuracy: 47.17 

Round   5, Train loss: 1.748, Test loss: 1.786, Test accuracy: 71.42 

Round   5, Global train loss: 1.748, Global test loss: 2.020, Global test accuracy: 43.40 

Round   6, Train loss: 1.693, Test loss: 1.735, Test accuracy: 77.13 

Round   6, Global train loss: 1.693, Global test loss: 2.019, Global test accuracy: 43.82 

Round   7, Train loss: 1.745, Test loss: 1.696, Test accuracy: 78.02 

Round   7, Global train loss: 1.745, Global test loss: 2.021, Global test accuracy: 42.62 

Round   8, Train loss: 1.717, Test loss: 1.717, Test accuracy: 75.50 

Round   8, Global train loss: 1.717, Global test loss: 1.993, Global test accuracy: 46.67 

Round   9, Train loss: 1.682, Test loss: 1.714, Test accuracy: 75.62 

Round   9, Global train loss: 1.682, Global test loss: 2.038, Global test accuracy: 41.50 

Round  10, Train loss: 1.717, Test loss: 1.712, Test accuracy: 75.73 

Round  10, Global train loss: 1.717, Global test loss: 2.013, Global test accuracy: 44.48 

Round  11, Train loss: 1.759, Test loss: 1.710, Test accuracy: 76.02 

Round  11, Global train loss: 1.759, Global test loss: 2.005, Global test accuracy: 45.17 

Round  12, Train loss: 1.777, Test loss: 1.708, Test accuracy: 75.95 

Round  12, Global train loss: 1.777, Global test loss: 1.990, Global test accuracy: 46.75 

Round  13, Train loss: 1.717, Test loss: 1.708, Test accuracy: 75.93 

Round  13, Global train loss: 1.717, Global test loss: 1.993, Global test accuracy: 46.87 

Round  14, Train loss: 1.761, Test loss: 1.721, Test accuracy: 74.47 

Round  14, Global train loss: 1.761, Global test loss: 1.985, Global test accuracy: 47.57 

Round  15, Train loss: 1.752, Test loss: 1.720, Test accuracy: 74.47 

Round  15, Global train loss: 1.752, Global test loss: 2.001, Global test accuracy: 45.67 

Round  16, Train loss: 1.716, Test loss: 1.721, Test accuracy: 74.30 

Round  16, Global train loss: 1.716, Global test loss: 2.012, Global test accuracy: 44.67 

Round  17, Train loss: 1.605, Test loss: 1.707, Test accuracy: 75.90 

Round  17, Global train loss: 1.605, Global test loss: 2.033, Global test accuracy: 41.38 

Round  18, Train loss: 1.670, Test loss: 1.705, Test accuracy: 76.07 

Round  18, Global train loss: 1.670, Global test loss: 2.037, Global test accuracy: 41.88 

Round  19, Train loss: 1.761, Test loss: 1.705, Test accuracy: 76.10 

Round  19, Global train loss: 1.761, Global test loss: 1.983, Global test accuracy: 48.18 

Round  20, Train loss: 1.758, Test loss: 1.690, Test accuracy: 77.62 

Round  20, Global train loss: 1.758, Global test loss: 1.995, Global test accuracy: 46.07 

Round  21, Train loss: 1.624, Test loss: 1.689, Test accuracy: 77.73 

Round  21, Global train loss: 1.624, Global test loss: 2.041, Global test accuracy: 40.63 

Round  22, Train loss: 1.660, Test loss: 1.688, Test accuracy: 77.80 

Round  22, Global train loss: 1.660, Global test loss: 2.040, Global test accuracy: 40.77 

Round  23, Train loss: 1.708, Test loss: 1.688, Test accuracy: 77.75 

Round  23, Global train loss: 1.708, Global test loss: 1.988, Global test accuracy: 47.42 

Round  24, Train loss: 1.615, Test loss: 1.687, Test accuracy: 77.67 

Round  24, Global train loss: 1.615, Global test loss: 2.043, Global test accuracy: 39.98 

Round  25, Train loss: 1.757, Test loss: 1.687, Test accuracy: 77.72 

Round  25, Global train loss: 1.757, Global test loss: 2.001, Global test accuracy: 45.23 

Round  26, Train loss: 1.721, Test loss: 1.686, Test accuracy: 77.80 

Round  26, Global train loss: 1.721, Global test loss: 1.988, Global test accuracy: 46.78 

Round  27, Train loss: 1.753, Test loss: 1.685, Test accuracy: 77.83 

Round  27, Global train loss: 1.753, Global test loss: 1.983, Global test accuracy: 47.62 

Round  28, Train loss: 1.772, Test loss: 1.686, Test accuracy: 77.80 

Round  28, Global train loss: 1.772, Global test loss: 1.995, Global test accuracy: 46.45 

Round  29, Train loss: 1.616, Test loss: 1.687, Test accuracy: 77.67 

Round  29, Global train loss: 1.616, Global test loss: 2.026, Global test accuracy: 42.13 

Round  30, Train loss: 1.702, Test loss: 1.688, Test accuracy: 77.50 

Round  30, Global train loss: 1.702, Global test loss: 2.002, Global test accuracy: 45.90 

Round  31, Train loss: 1.693, Test loss: 1.703, Test accuracy: 76.02 

Round  31, Global train loss: 1.693, Global test loss: 2.078, Global test accuracy: 36.98 

Round  32, Train loss: 1.590, Test loss: 1.704, Test accuracy: 75.88 

Round  32, Global train loss: 1.590, Global test loss: 2.049, Global test accuracy: 40.27 

Round  33, Train loss: 1.749, Test loss: 1.705, Test accuracy: 75.92 

Round  33, Global train loss: 1.749, Global test loss: 1.989, Global test accuracy: 47.03 

Round  34, Train loss: 1.818, Test loss: 1.704, Test accuracy: 75.90 

Round  34, Global train loss: 1.818, Global test loss: 2.007, Global test accuracy: 44.68 

Round  35, Train loss: 1.752, Test loss: 1.703, Test accuracy: 76.00 

Round  35, Global train loss: 1.752, Global test loss: 1.989, Global test accuracy: 47.13 

Round  36, Train loss: 1.601, Test loss: 1.703, Test accuracy: 75.93 

Round  36, Global train loss: 1.601, Global test loss: 2.040, Global test accuracy: 40.13 

Round  37, Train loss: 1.698, Test loss: 1.703, Test accuracy: 75.93 

Round  37, Global train loss: 1.698, Global test loss: 1.993, Global test accuracy: 46.62 

Round  38, Train loss: 1.698, Test loss: 1.704, Test accuracy: 75.83 

Round  38, Global train loss: 1.698, Global test loss: 2.004, Global test accuracy: 45.22 

Round  39, Train loss: 1.600, Test loss: 1.702, Test accuracy: 75.93 

Round  39, Global train loss: 1.600, Global test loss: 2.013, Global test accuracy: 44.05 

Round  40, Train loss: 1.798, Test loss: 1.701, Test accuracy: 76.03 

Round  40, Global train loss: 1.798, Global test loss: 1.987, Global test accuracy: 47.23 

Round  41, Train loss: 1.800, Test loss: 1.702, Test accuracy: 75.98 

Round  41, Global train loss: 1.800, Global test loss: 1.996, Global test accuracy: 45.97 

Round  42, Train loss: 1.807, Test loss: 1.718, Test accuracy: 74.43 

Round  42, Global train loss: 1.807, Global test loss: 1.996, Global test accuracy: 46.05 

Round  43, Train loss: 1.754, Test loss: 1.717, Test accuracy: 74.50 

Round  43, Global train loss: 1.754, Global test loss: 1.992, Global test accuracy: 46.62 

Round  44, Train loss: 1.695, Test loss: 1.717, Test accuracy: 74.55 

Round  44, Global train loss: 1.695, Global test loss: 2.007, Global test accuracy: 44.87 

Round  45, Train loss: 1.656, Test loss: 1.702, Test accuracy: 76.00 

Round  45, Global train loss: 1.656, Global test loss: 1.988, Global test accuracy: 46.95 

Round  46, Train loss: 1.752, Test loss: 1.687, Test accuracy: 77.48 

Round  46, Global train loss: 1.752, Global test loss: 2.031, Global test accuracy: 42.07 

Round  47, Train loss: 1.647, Test loss: 1.686, Test accuracy: 77.68 

Round  47, Global train loss: 1.647, Global test loss: 2.010, Global test accuracy: 44.62 

Round  48, Train loss: 1.703, Test loss: 1.686, Test accuracy: 77.73 

Round  48, Global train loss: 1.703, Global test loss: 2.003, Global test accuracy: 45.15 

Round  49, Train loss: 1.647, Test loss: 1.685, Test accuracy: 77.77 

Round  49, Global train loss: 1.647, Global test loss: 2.011, Global test accuracy: 44.42 

Final Round, Train loss: 1.670, Test loss: 1.683, Test accuracy: 77.98 

Final Round, Global train loss: 1.670, Global test loss: 2.011, Global test accuracy: 44.42 

Average accuracy final 10 rounds: 76.21666666666667 

Average global accuracy final 10 rounds: 45.39333333333333 

373.73705673217773
[1.5967330932617188, 2.270209789276123, 2.9124717712402344, 3.5548882484436035, 4.199004888534546, 4.839650869369507, 5.486431121826172, 6.127795457839966, 6.767597436904907, 7.414156436920166, 8.054490804672241, 8.691499471664429, 9.334067583084106, 9.973118543624878, 10.613802671432495, 11.256255865097046, 11.902697801589966, 12.538570880889893, 13.179612159729004, 13.816558837890625, 14.452442407608032, 15.109788417816162, 15.744419813156128, 16.377178192138672, 17.010661602020264, 17.63994526863098, 18.27400231361389, 18.917081832885742, 19.551599979400635, 20.190043210983276, 20.83123540878296, 21.467838764190674, 22.107330799102783, 22.749197483062744, 23.458986043930054, 24.110811471939087, 24.747442722320557, 25.384067058563232, 26.016733407974243, 26.65158748626709, 27.287484407424927, 27.92201066017151, 28.557839393615723, 29.19193410873413, 29.835901260375977, 30.47888159751892, 31.113945722579956, 31.75410556793213, 32.39576578140259, 33.04957556724548, 34.33145093917847]
[42.983333333333334, 43.43333333333333, 50.516666666666666, 59.233333333333334, 71.06666666666666, 71.41666666666667, 77.13333333333334, 78.01666666666667, 75.5, 75.61666666666666, 75.73333333333333, 76.01666666666667, 75.95, 75.93333333333334, 74.46666666666667, 74.46666666666667, 74.3, 75.9, 76.06666666666666, 76.1, 77.61666666666666, 77.73333333333333, 77.8, 77.75, 77.66666666666667, 77.71666666666667, 77.8, 77.83333333333333, 77.8, 77.66666666666667, 77.5, 76.01666666666667, 75.88333333333334, 75.91666666666667, 75.9, 76.0, 75.93333333333334, 75.93333333333334, 75.83333333333333, 75.93333333333334, 76.03333333333333, 75.98333333333333, 74.43333333333334, 74.5, 74.55, 76.0, 77.48333333333333, 77.68333333333334, 77.73333333333333, 77.76666666666667, 77.98333333333333]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.297, Test accuracy: 33.37 

Round   1, Train loss: 2.288, Test loss: 2.281, Test accuracy: 36.12 

Round   2, Train loss: 2.254, Test loss: 2.216, Test accuracy: 33.33 

Round   3, Train loss: 2.129, Test loss: 2.129, Test accuracy: 34.90 

Round   4, Train loss: 2.071, Test loss: 2.095, Test accuracy: 36.83 

Round   5, Train loss: 2.044, Test loss: 2.070, Test accuracy: 39.75 

Round   6, Train loss: 1.986, Test loss: 2.050, Test accuracy: 41.87 

Round   7, Train loss: 1.981, Test loss: 2.014, Test accuracy: 44.98 

Round   8, Train loss: 1.906, Test loss: 1.975, Test accuracy: 49.33 

Round   9, Train loss: 1.824, Test loss: 1.930, Test accuracy: 54.70 

Round  10, Train loss: 1.759, Test loss: 1.903, Test accuracy: 57.33 

Round  11, Train loss: 1.764, Test loss: 1.890, Test accuracy: 58.03 

Round  12, Train loss: 1.840, Test loss: 1.848, Test accuracy: 62.53 

Round  13, Train loss: 1.794, Test loss: 1.850, Test accuracy: 62.02 

Round  14, Train loss: 1.821, Test loss: 1.823, Test accuracy: 64.93 

Round  15, Train loss: 1.746, Test loss: 1.813, Test accuracy: 66.17 

Round  16, Train loss: 1.789, Test loss: 1.813, Test accuracy: 65.97 

Round  17, Train loss: 1.736, Test loss: 1.810, Test accuracy: 66.03 

Round  18, Train loss: 1.805, Test loss: 1.803, Test accuracy: 66.63 

Round  19, Train loss: 1.768, Test loss: 1.792, Test accuracy: 68.12 

Round  20, Train loss: 1.747, Test loss: 1.786, Test accuracy: 68.13 

Round  21, Train loss: 1.754, Test loss: 1.785, Test accuracy: 68.38 

Round  22, Train loss: 1.739, Test loss: 1.785, Test accuracy: 68.58 

Round  23, Train loss: 1.724, Test loss: 1.783, Test accuracy: 68.45 

Round  24, Train loss: 1.718, Test loss: 1.783, Test accuracy: 68.30 

Round  25, Train loss: 1.735, Test loss: 1.778, Test accuracy: 68.52 

Round  26, Train loss: 1.811, Test loss: 1.778, Test accuracy: 68.63 

Round  27, Train loss: 1.780, Test loss: 1.775, Test accuracy: 69.48 

Round  28, Train loss: 1.747, Test loss: 1.770, Test accuracy: 69.82 

Round  29, Train loss: 1.704, Test loss: 1.753, Test accuracy: 71.43 

Round  30, Train loss: 1.669, Test loss: 1.751, Test accuracy: 71.60 

Round  31, Train loss: 1.730, Test loss: 1.747, Test accuracy: 72.20 

Round  32, Train loss: 1.715, Test loss: 1.749, Test accuracy: 71.78 

Round  33, Train loss: 1.685, Test loss: 1.745, Test accuracy: 72.33 

Round  34, Train loss: 1.636, Test loss: 1.736, Test accuracy: 73.27 

Round  35, Train loss: 1.726, Test loss: 1.738, Test accuracy: 73.00 

Round  36, Train loss: 1.715, Test loss: 1.737, Test accuracy: 72.95 

Round  37, Train loss: 1.607, Test loss: 1.734, Test accuracy: 73.27 

Round  38, Train loss: 1.818, Test loss: 1.732, Test accuracy: 73.50 

Round  39, Train loss: 1.710, Test loss: 1.732, Test accuracy: 73.55 

Round  40, Train loss: 1.782, Test loss: 1.728, Test accuracy: 73.85 

Round  41, Train loss: 1.682, Test loss: 1.728, Test accuracy: 73.85 

Round  42, Train loss: 1.706, Test loss: 1.728, Test accuracy: 73.88 

Round  43, Train loss: 1.672, Test loss: 1.729, Test accuracy: 73.80 

Round  44, Train loss: 1.752, Test loss: 1.727, Test accuracy: 73.83 

Round  45, Train loss: 1.661, Test loss: 1.727, Test accuracy: 73.75 

Round  46, Train loss: 1.716, Test loss: 1.724, Test accuracy: 74.20 

Round  47, Train loss: 1.763, Test loss: 1.725, Test accuracy: 74.02 

Round  48, Train loss: 1.706, Test loss: 1.726, Test accuracy: 73.80 

Round  49, Train loss: 1.701, Test loss: 1.724, Test accuracy: 73.90 

Final Round, Train loss: 1.699, Test loss: 1.715, Test accuracy: 74.92 

Average accuracy final 10 rounds: 73.88833333333334 

268.09090876579285
[1.501685380935669, 2.0631961822509766, 2.634016990661621, 3.200521469116211, 3.7685139179229736, 4.338699102401733, 4.909450531005859, 5.482148885726929, 6.131664276123047, 6.699946641921997, 7.269011497497559, 7.832310676574707, 8.39909553527832, 8.963860750198364, 9.530415296554565, 10.093179941177368, 10.659616708755493, 11.220414161682129, 11.789570569992065, 12.361553430557251, 12.931211471557617, 13.49877667427063, 14.065632104873657, 14.63514518737793, 15.196004629135132, 15.769440650939941, 16.336512804031372, 16.902148246765137, 17.462403774261475, 18.03198003768921, 18.599672079086304, 19.16247296333313, 19.722752332687378, 20.298547506332397, 20.85966944694519, 21.429309844970703, 21.99214005470276, 22.56337833404541, 23.13216996192932, 23.6987407207489, 24.26630687713623, 24.832674980163574, 25.395864725112915, 25.965870141983032, 26.537909269332886, 27.115947484970093, 27.684065341949463, 28.24480676651001, 28.811357021331787, 29.372894048690796, 30.364386558532715]
[33.36666666666667, 36.11666666666667, 33.333333333333336, 34.9, 36.833333333333336, 39.75, 41.86666666666667, 44.983333333333334, 49.333333333333336, 54.7, 57.333333333333336, 58.03333333333333, 62.53333333333333, 62.016666666666666, 64.93333333333334, 66.16666666666667, 65.96666666666667, 66.03333333333333, 66.63333333333334, 68.11666666666666, 68.13333333333334, 68.38333333333334, 68.58333333333333, 68.45, 68.3, 68.51666666666667, 68.63333333333334, 69.48333333333333, 69.81666666666666, 71.43333333333334, 71.6, 72.2, 71.78333333333333, 72.33333333333333, 73.26666666666667, 73.0, 72.95, 73.26666666666667, 73.5, 73.55, 73.85, 73.85, 73.88333333333334, 73.8, 73.83333333333333, 73.75, 74.2, 74.01666666666667, 73.8, 73.9, 74.91666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 291, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 1492, in train
    sub_clc = self.features - torch.from_numpy(class_center_batch)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.299, Test loss: 2.282, Test accuracy: 33.47
Round   1, Train loss: 2.247, Test loss: 2.111, Test accuracy: 38.25
Round   2, Train loss: 2.061, Test loss: 2.041, Test accuracy: 42.32
Round   3, Train loss: 1.816, Test loss: 2.031, Test accuracy: 41.38
Round   4, Train loss: 1.749, Test loss: 2.017, Test accuracy: 44.12
Round   5, Train loss: 1.623, Test loss: 2.028, Test accuracy: 42.35
Round   6, Train loss: 1.638, Test loss: 2.011, Test accuracy: 43.98
Round   7, Train loss: 1.678, Test loss: 1.990, Test accuracy: 46.83
Round   8, Train loss: 1.521, Test loss: 2.002, Test accuracy: 44.97
Round   9, Train loss: 1.573, Test loss: 2.024, Test accuracy: 42.03
Round  10, Train loss: 1.594, Test loss: 2.003, Test accuracy: 44.85
Round  11, Train loss: 1.525, Test loss: 1.987, Test accuracy: 46.70
Round  12, Train loss: 1.558, Test loss: 1.978, Test accuracy: 47.75
Round  13, Train loss: 1.518, Test loss: 2.000, Test accuracy: 44.98
Round  14, Train loss: 1.495, Test loss: 1.990, Test accuracy: 46.18
Round  15, Train loss: 1.500, Test loss: 1.988, Test accuracy: 46.42
Round  16, Train loss: 1.547, Test loss: 2.004, Test accuracy: 45.07
Round  17, Train loss: 1.489, Test loss: 1.990, Test accuracy: 46.88
Round  18, Train loss: 1.484, Test loss: 1.980, Test accuracy: 48.05
Round  19, Train loss: 1.514, Test loss: 2.007, Test accuracy: 44.47
Round  20, Train loss: 1.514, Test loss: 2.014, Test accuracy: 43.37
Round  21, Train loss: 1.489, Test loss: 1.994, Test accuracy: 46.22
Round  22, Train loss: 1.488, Test loss: 2.001, Test accuracy: 45.40
Round  23, Train loss: 1.489, Test loss: 1.987, Test accuracy: 47.18
Round  24, Train loss: 1.480, Test loss: 1.989, Test accuracy: 46.53
Round  25, Train loss: 1.496, Test loss: 1.982, Test accuracy: 47.70
Round  26, Train loss: 1.479, Test loss: 1.991, Test accuracy: 46.02
Round  27, Train loss: 1.480, Test loss: 1.997, Test accuracy: 45.48
Round  28, Train loss: 1.477, Test loss: 2.027, Test accuracy: 41.93
Round  29, Train loss: 1.474, Test loss: 2.016, Test accuracy: 43.62
Round  30, Train loss: 1.481, Test loss: 2.008, Test accuracy: 44.33
Round  31, Train loss: 1.479, Test loss: 2.001, Test accuracy: 45.10
Round  32, Train loss: 1.475, Test loss: 2.007, Test accuracy: 44.55
Round  33, Train loss: 1.480, Test loss: 1.986, Test accuracy: 46.73
Round  34, Train loss: 1.472, Test loss: 1.993, Test accuracy: 46.17
Round  35, Train loss: 1.473, Test loss: 1.994, Test accuracy: 45.63
Round  36, Train loss: 1.480, Test loss: 1.990, Test accuracy: 46.58
Round  37, Train loss: 1.476, Test loss: 2.009, Test accuracy: 44.37
Round  38, Train loss: 1.471, Test loss: 1.984, Test accuracy: 46.87
Round  39, Train loss: 1.475, Test loss: 1.984, Test accuracy: 47.00
Round  40, Train loss: 1.473, Test loss: 1.995, Test accuracy: 45.90
Round  41, Train loss: 1.477, Test loss: 1.979, Test accuracy: 47.88
Round  42, Train loss: 1.476, Test loss: 1.987, Test accuracy: 46.85
Round  43, Train loss: 1.473, Test loss: 1.978, Test accuracy: 47.83
Round  44, Train loss: 1.476, Test loss: 2.010, Test accuracy: 44.38
Round  45, Train loss: 1.475, Test loss: 1.991, Test accuracy: 46.32
Round  46, Train loss: 1.477, Test loss: 2.007, Test accuracy: 44.92
Round  47, Train loss: 1.472, Test loss: 2.012, Test accuracy: 43.92
Round  48, Train loss: 1.471, Test loss: 2.010, Test accuracy: 43.93
Round  49, Train loss: 1.472, Test loss: 2.000, Test accuracy: 45.28
Final Round, Train loss: 1.470, Test loss: 1.968, Test accuracy: 49.32
Average accuracy final 10 rounds: 45.721666666666664
588.0178270339966
[2.6125118732452393, 4.300322532653809, 5.931398868560791, 7.563498258590698, 9.190261602401733, 10.82364821434021, 12.481072187423706, 14.152033567428589, 15.810086727142334, 17.462146282196045, 19.12492823600769, 20.77506995201111, 22.431501388549805, 24.083658933639526, 25.737569093704224, 27.393824577331543, 29.040324687957764, 30.692878246307373, 32.343286991119385, 33.93590044975281, 35.48189926147461, 37.021198749542236, 38.56296634674072, 40.10890197753906, 41.6586012840271, 43.223764181137085, 44.77450680732727, 46.32281827926636, 47.97897291183472, 49.63634514808655, 51.28820514678955, 52.932477951049805, 54.5782995223999, 56.23027801513672, 57.88224792480469, 59.537811517715454, 61.197521686553955, 62.86367154121399, 64.52577757835388, 66.17799615859985, 67.77757000923157, 69.32013154029846, 70.86119532585144, 72.40813112258911, 73.95390248298645, 75.4997787475586, 77.0456657409668, 78.60280346870422, 80.15462136268616, 81.71738791465759, 83.27381181716919]
[33.46666666666667, 38.25, 42.31666666666667, 41.38333333333333, 44.11666666666667, 42.35, 43.983333333333334, 46.833333333333336, 44.96666666666667, 42.03333333333333, 44.85, 46.7, 47.75, 44.983333333333334, 46.18333333333333, 46.416666666666664, 45.06666666666667, 46.88333333333333, 48.05, 44.46666666666667, 43.36666666666667, 46.21666666666667, 45.4, 47.18333333333333, 46.53333333333333, 47.7, 46.016666666666666, 45.483333333333334, 41.93333333333333, 43.61666666666667, 44.333333333333336, 45.1, 44.55, 46.733333333333334, 46.166666666666664, 45.63333333333333, 46.583333333333336, 44.36666666666667, 46.86666666666667, 47.0, 45.9, 47.88333333333333, 46.85, 47.833333333333336, 44.38333333333333, 46.31666666666667, 44.916666666666664, 43.916666666666664, 43.93333333333333, 45.28333333333333, 49.31666666666667]
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 295, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/data/jij/csm/code/FL_HLS/utils/sampling.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 50, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/data/jij/csm/code/FL_HLS/models/Update.py", line 832, in train
    loss.backward()
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/data/jij/install/anaconda/envs/gflownets/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
