nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.153, Test loss: 4.214, Test accuracy: 40.87
Final Round, Global train loss: 0.153, Global test loss: 1.614, Global test accuracy: 48.08
Average accuracy final 10 rounds: 41.009499999999996 

Average global accuracy final 10 rounds: 47.254000000000005 

3973.1415796279907
[1.3184142112731934, 2.6368284225463867, 3.872910499572754, 5.108992576599121, 6.358166217803955, 7.607339859008789, 8.86343502998352, 10.119530200958252, 11.37372350692749, 12.627916812896729, 13.880046129226685, 15.13217544555664, 16.38448214530945, 17.636788845062256, 18.884851455688477, 20.132914066314697, 21.38869571685791, 22.644477367401123, 23.89133381843567, 25.138190269470215, 26.388182163238525, 27.638174057006836, 28.892081260681152, 30.14598846435547, 31.4031662940979, 32.66034412384033, 33.9192578792572, 35.17817163467407, 36.43812298774719, 37.69807434082031, 38.955204248428345, 40.21233415603638, 41.47130608558655, 42.73027801513672, 43.99457263946533, 45.258867263793945, 46.51800990104675, 47.77715253829956, 49.024434328079224, 50.27171611785889, 51.52018594741821, 52.76865577697754, 54.01274919509888, 55.256842613220215, 56.51409649848938, 57.771350383758545, 59.030126333236694, 60.288902282714844, 61.56025791168213, 62.831613540649414, 64.10936403274536, 65.38711452484131, 66.6685140132904, 67.9499135017395, 69.21994233131409, 70.48997116088867, 71.76627469062805, 73.04257822036743, 74.30637454986572, 75.57017087936401, 76.84717535972595, 78.12417984008789, 79.40823721885681, 80.69229459762573, 81.96974229812622, 83.24718999862671, 84.51974701881409, 85.79230403900146, 87.06966423988342, 88.34702444076538, 89.62698364257812, 90.90694284439087, 92.18619990348816, 93.46545696258545, 94.74985146522522, 96.03424596786499, 97.31546807289124, 98.59669017791748, 99.87258219718933, 101.14847421646118, 102.42639756202698, 103.70432090759277, 104.97640824317932, 106.24849557876587, 107.52265357971191, 108.79681158065796, 110.06970357894897, 111.34259557723999, 112.6151831150055, 113.887770652771, 115.17996835708618, 116.47216606140137, 117.74693584442139, 119.0217056274414, 120.30216550827026, 121.58262538909912, 122.86389112472534, 124.14515686035156, 125.4135262966156, 126.68189573287964, 127.96144127845764, 129.24098682403564, 130.52224349975586, 131.80350017547607, 133.08040356636047, 134.35730695724487, 135.64539289474487, 136.93347883224487, 138.20461630821228, 139.4757537841797, 140.7672393321991, 142.0587248802185, 143.34079122543335, 144.6228575706482, 145.90380120277405, 147.1847448348999, 148.473730802536, 149.76271677017212, 151.03655672073364, 152.31039667129517, 153.59187531471252, 154.87335395812988, 156.16668701171875, 157.46002006530762, 158.73598885536194, 160.01195764541626, 161.3001003265381, 162.5882430076599, 163.86817622184753, 165.14810943603516, 166.42050290107727, 167.69289636611938, 168.97742748260498, 170.26195859909058, 171.53535318374634, 172.8087477684021, 174.0871934890747, 175.36563920974731, 176.64790105819702, 177.93016290664673, 179.20437455177307, 180.4785861968994, 181.7482590675354, 183.0179319381714, 184.29228115081787, 185.56663036346436, 186.84854078292847, 188.13045120239258, 189.4081997871399, 190.6859483718872, 191.95400047302246, 193.22205257415771, 194.50528836250305, 195.7885241508484, 197.06814169883728, 198.34775924682617, 199.62430143356323, 200.9008436203003, 202.193017244339, 203.48519086837769, 204.75480842590332, 206.02442598342896, 207.30064868927002, 208.57687139511108, 209.86244201660156, 211.14801263809204, 212.42196106910706, 213.69590950012207, 214.97937679290771, 216.26284408569336, 217.545884847641, 218.82892560958862, 220.11602687835693, 221.40312814712524, 222.6804723739624, 223.95781660079956, 225.22850489616394, 226.49919319152832, 227.77374958992004, 229.04830598831177, 230.33263874053955, 231.61697149276733, 232.89634656906128, 234.17572164535522, 235.44437289237976, 236.7130241394043, 237.9914367198944, 239.26984930038452, 240.55689907073975, 241.84394884109497, 243.12391805648804, 244.4038872718811, 245.70583128929138, 247.00777530670166, 248.28401279449463, 249.5602502822876, 250.83835124969482, 252.11645221710205, 253.40106534957886, 254.68567848205566, 257.2465739250183, 259.80746936798096]
[24.4325, 24.4325, 28.1875, 28.1875, 30.67, 30.67, 31.495, 31.495, 32.0875, 32.0875, 33.015, 33.015, 33.795, 33.795, 34.155, 34.155, 35.36, 35.36, 35.395, 35.395, 36.05, 36.05, 36.2625, 36.2625, 37.3425, 37.3425, 37.67, 37.67, 38.485, 38.485, 38.7, 38.7, 38.9525, 38.9525, 39.005, 39.005, 39.1875, 39.1875, 39.4375, 39.4375, 39.5125, 39.5125, 39.8925, 39.8925, 39.745, 39.745, 40.1075, 40.1075, 39.9925, 39.9925, 39.57, 39.57, 39.8475, 39.8475, 40.5775, 40.5775, 40.6775, 40.6775, 40.7325, 40.7325, 40.85, 40.85, 40.9825, 40.9825, 41.2575, 41.2575, 40.9575, 40.9575, 40.64, 40.64, 40.59, 40.59, 40.975, 40.975, 41.0225, 41.0225, 41.0975, 41.0975, 40.8725, 40.8725, 41.085, 41.085, 40.7025, 40.7025, 40.9475, 40.9475, 40.975, 40.975, 40.47, 40.47, 40.4275, 40.4275, 40.3425, 40.3425, 40.525, 40.525, 40.2725, 40.2725, 40.81, 40.81, 41.09, 41.09, 41.2475, 41.2475, 41.005, 41.005, 41.0375, 41.0375, 41.0875, 41.0875, 41.215, 41.215, 40.9525, 40.9525, 41.065, 41.065, 41.34, 41.34, 41.45, 41.45, 41.155, 41.155, 40.6925, 40.6925, 40.745, 40.745, 40.5975, 40.5975, 40.6575, 40.6575, 40.7675, 40.7675, 40.5125, 40.5125, 40.5325, 40.5325, 40.6725, 40.6725, 40.875, 40.875, 41.0975, 41.0975, 40.89, 40.89, 40.975, 40.975, 40.91, 40.91, 40.8675, 40.8675, 40.7175, 40.7175, 40.805, 40.805, 40.985, 40.985, 40.5625, 40.5625, 40.6525, 40.6525, 40.905, 40.905, 41.2175, 41.2175, 41.165, 41.165, 40.8925, 40.8925, 40.895, 40.895, 40.5025, 40.5025, 40.5225, 40.5225, 40.7625, 40.7625, 41.0975, 41.0975, 41.1025, 41.1025, 40.895, 40.895, 41.0275, 41.0275, 40.8, 40.8, 41.0875, 41.0875, 41.1, 41.1, 41.0475, 41.0475, 41.015, 41.015, 40.955, 40.955, 41.025, 41.025, 41.1425, 41.1425, 40.865, 40.865]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.351, Test loss: 1.695, Test accuracy: 61.66
Final Round, Global train loss: 0.351, Global test loss: 1.144, Global test accuracy: 68.83
Average accuracy final 10 rounds: 62.3945 

Average global accuracy final 10 rounds: 69.02125 

3804.3268892765045
[1.463594913482666, 2.927189826965332, 4.173524379730225, 5.419858932495117, 6.650309801101685, 7.880760669708252, 9.120553016662598, 10.360345363616943, 11.602229833602905, 12.844114303588867, 14.099700927734375, 15.355287551879883, 16.607558488845825, 17.859829425811768, 19.12105107307434, 20.382272720336914, 21.64318823814392, 22.904103755950928, 24.16223168373108, 25.42035961151123, 26.66050672531128, 27.900653839111328, 29.147387266159058, 30.394120693206787, 31.647735357284546, 32.901350021362305, 34.15661334991455, 35.4118766784668, 36.671815395355225, 37.93175411224365, 39.18736815452576, 40.44298219680786, 41.705098390579224, 42.967214584350586, 44.20917296409607, 45.45113134384155, 46.70123767852783, 47.95134401321411, 49.205763816833496, 50.46018362045288, 51.71396040916443, 52.96773719787598, 54.21506428718567, 55.46239137649536, 56.71750283241272, 57.97261428833008, 59.22022771835327, 60.467841148376465, 61.708505630493164, 62.94917011260986, 64.19775652885437, 65.44634294509888, 66.69353866577148, 67.94073438644409, 69.19258713722229, 70.44443988800049, 71.68748736381531, 72.93053483963013, 74.17293953895569, 75.41534423828125, 76.65422248840332, 77.89310073852539, 78.8913164138794, 79.8895320892334, 80.90176630020142, 81.91400051116943, 82.91121220588684, 83.90842390060425, 84.90693497657776, 85.90544605255127, 86.90452647209167, 87.90360689163208, 88.90160512924194, 89.8996033668518, 91.05259871482849, 92.20559406280518, 93.3627872467041, 94.51998043060303, 95.68466758728027, 96.84935474395752, 98.0292580127716, 99.2091612815857, 100.37217569351196, 101.53519010543823, 102.69658422470093, 103.85797834396362, 104.99985003471375, 106.14172172546387, 107.30225086212158, 108.4627799987793, 109.62393498420715, 110.78508996963501, 111.95326352119446, 113.1214370727539, 114.27861499786377, 115.43579292297363, 116.59357500076294, 117.75135707855225, 118.914381980896, 120.07740688323975, 121.23703050613403, 122.39665412902832, 123.55346894264221, 124.7102837562561, 125.86521005630493, 127.02013635635376, 128.1813781261444, 129.34261989593506, 130.50107836723328, 131.6595368385315, 132.81790709495544, 133.9762773513794, 135.13382935523987, 136.29138135910034, 137.44093489646912, 138.5904884338379, 139.74878358840942, 140.90707874298096, 142.06049823760986, 143.21391773223877, 144.37494564056396, 145.53597354888916, 146.69025683403015, 147.84454011917114, 148.99522137641907, 150.145902633667, 151.30075454711914, 152.4556064605713, 153.6178424358368, 154.7800784111023, 155.94966793060303, 157.11925745010376, 158.28715085983276, 159.45504426956177, 160.6096272468567, 161.7642102241516, 162.9231469631195, 164.0820837020874, 165.2503523826599, 166.41862106323242, 167.57567763328552, 168.73273420333862, 169.89003157615662, 171.0473289489746, 172.20494627952576, 173.3625636100769, 174.51426696777344, 175.66597032546997, 176.82908415794373, 177.99219799041748, 179.15261960029602, 180.31304121017456, 181.4745810031891, 182.6361207962036, 183.79763007164001, 184.95913934707642, 186.11162066459656, 187.2641019821167, 188.4253430366516, 189.58658409118652, 190.74337697029114, 191.90016984939575, 193.05406284332275, 194.20795583724976, 195.3656463623047, 196.52333688735962, 197.68039631843567, 198.83745574951172, 199.99222087860107, 201.14698600769043, 202.3090465068817, 203.471107006073, 204.6357536315918, 205.8004002571106, 206.95791602134705, 208.1154317855835, 209.28272533416748, 210.45001888275146, 211.60696840286255, 212.76391792297363, 213.9127335548401, 215.06154918670654, 216.21824073791504, 217.37493228912354, 218.53060126304626, 219.686270236969, 220.8504238128662, 222.01457738876343, 223.1758418083191, 224.33710622787476, 225.48056602478027, 226.6240258216858, 227.78584909439087, 228.94767236709595, 230.10562562942505, 231.26357889175415, 232.41803669929504, 233.57249450683594, 234.7394504547119, 235.9064064025879, 238.24010705947876, 240.57380771636963]
[26.095, 26.095, 30.525, 30.525, 33.425, 33.425, 35.6, 35.6, 35.8875, 35.8875, 38.13, 38.13, 39.55, 39.55, 40.325, 40.325, 41.965, 41.965, 43.43, 43.43, 44.695, 44.695, 45.615, 45.615, 46.7825, 46.7825, 46.63, 46.63, 47.31, 47.31, 48.49, 48.49, 49.0525, 49.0525, 49.815, 49.815, 50.425, 50.425, 51.07, 51.07, 52.17, 52.17, 52.2225, 52.2225, 52.4925, 52.4925, 52.5825, 52.5825, 52.515, 52.515, 53.3775, 53.3775, 53.3575, 53.3575, 54.005, 54.005, 54.9775, 54.9775, 55.3725, 55.3725, 55.655, 55.655, 55.8175, 55.8175, 55.675, 55.675, 55.16, 55.16, 55.505, 55.505, 55.9025, 55.9025, 56.44, 56.44, 56.745, 56.745, 57.2775, 57.2775, 58.095, 58.095, 58.4175, 58.4175, 58.6325, 58.6325, 58.945, 58.945, 58.865, 58.865, 59.02, 59.02, 59.015, 59.015, 59.0575, 59.0575, 59.295, 59.295, 59.2525, 59.2525, 59.52, 59.52, 59.425, 59.425, 59.5725, 59.5725, 59.6225, 59.6225, 60.0825, 60.0825, 60.1475, 60.1475, 59.8225, 59.8225, 60.075, 60.075, 60.135, 60.135, 60.1375, 60.1375, 60.825, 60.825, 60.915, 60.915, 60.7325, 60.7325, 60.9225, 60.9225, 60.945, 60.945, 60.945, 60.945, 60.7025, 60.7025, 60.805, 60.805, 60.79, 60.79, 61.15, 61.15, 61.18, 61.18, 61.4375, 61.4375, 61.875, 61.875, 61.89, 61.89, 61.85, 61.85, 61.8225, 61.8225, 61.895, 61.895, 61.9575, 61.9575, 62.1025, 62.1025, 61.9275, 61.9275, 61.925, 61.925, 61.6275, 61.6275, 61.9375, 61.9375, 62.1475, 62.1475, 62.475, 62.475, 62.54, 62.54, 62.79, 62.79, 62.6225, 62.6225, 62.5075, 62.5075, 62.8525, 62.8525, 62.73, 62.73, 62.69, 62.69, 62.3725, 62.3725, 62.39, 62.39, 62.3775, 62.3775, 62.3825, 62.3825, 62.33, 62.33, 62.525, 62.525, 62.5775, 62.5775, 62.0125, 62.0125, 62.2875, 62.2875, 61.66, 61.66]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.480, Test loss: 1.324, Test accuracy: 62.23
Average accuracy final 10 rounds: 62.30125000000001 

2705.616861343384
[1.3299810886383057, 2.6599621772766113, 3.726213216781616, 4.792464256286621, 5.84838342666626, 6.904302597045898, 7.965980291366577, 9.027657985687256, 10.088757514953613, 11.14985704421997, 12.204471349716187, 13.259085655212402, 14.325732231140137, 15.392378807067871, 16.45671033859253, 17.521041870117188, 18.577317237854004, 19.63359260559082, 20.69105863571167, 21.74852466583252, 22.81809902191162, 23.887673377990723, 24.951359510421753, 26.015045642852783, 27.07797861099243, 28.14091157913208, 29.19736933708191, 30.25382709503174, 31.320019006729126, 32.386210918426514, 33.44217538833618, 34.49813985824585, 35.55541682243347, 36.612693786621094, 37.67417025566101, 38.73564672470093, 39.79645776748657, 40.85726881027222, 41.909759521484375, 42.96225023269653, 44.014899492263794, 45.067548751831055, 46.12426447868347, 47.18098020553589, 48.22936940193176, 49.27775859832764, 50.348381757736206, 51.419004917144775, 52.490161180496216, 53.561317443847656, 54.62711787223816, 55.69291830062866, 56.758856534957886, 57.82479476928711, 58.88161277770996, 59.93843078613281, 60.9958176612854, 62.05320453643799, 63.10635447502136, 64.15950441360474, 65.21132111549377, 66.26313781738281, 67.32161593437195, 68.38009405136108, 69.42959356307983, 70.47909307479858, 71.53871393203735, 72.59833478927612, 73.65232372283936, 74.70631265640259, 75.75497889518738, 76.80364513397217, 77.86104893684387, 78.91845273971558, 79.98391914367676, 81.04938554763794, 82.10983514785767, 83.17028474807739, 84.22376465797424, 85.2772445678711, 86.3291687965393, 87.38109302520752, 88.44587922096252, 89.51066541671753, 90.58251905441284, 91.65437269210815, 92.70480418205261, 93.75523567199707, 94.81996941566467, 95.88470315933228, 96.94532442092896, 98.00594568252563, 99.0830008983612, 100.16005611419678, 101.21470952033997, 102.26936292648315, 103.31835770606995, 104.36735248565674, 105.42524147033691, 106.48313045501709, 107.49816846847534, 108.5132064819336, 109.46802115440369, 110.42283582687378, 111.37822127342224, 112.3336067199707, 113.29781794548035, 114.26202917098999, 115.21641540527344, 116.17080163955688, 117.13921809196472, 118.10763454437256, 119.06335139274597, 120.01906824111938, 120.98309063911438, 121.94711303710938, 122.9041633605957, 123.86121368408203, 124.8126904964447, 125.76416730880737, 126.72804546356201, 127.69192361831665, 128.639408826828, 129.58689403533936, 130.5503408908844, 131.51378774642944, 132.47572231292725, 133.43765687942505, 134.39879965782166, 135.35994243621826, 136.32444405555725, 137.28894567489624, 138.2440938949585, 139.19924211502075, 140.15827775001526, 141.11731338500977, 142.0884246826172, 143.0595359802246, 144.11363792419434, 145.16773986816406, 146.2334542274475, 147.29916858673096, 148.35702180862427, 149.41487503051758, 150.4797670841217, 151.54465913772583, 152.62721419334412, 153.7097692489624, 154.76945281028748, 155.82913637161255, 156.89269709587097, 157.9562578201294, 159.02236199378967, 160.08846616744995, 161.15804481506348, 162.227623462677, 163.28820872306824, 164.34879398345947, 165.40759897232056, 166.46640396118164, 167.41692447662354, 168.36744499206543, 169.31960797309875, 170.27177095413208, 171.22414588928223, 172.17652082443237, 173.13046503067017, 174.08440923690796, 175.02559518814087, 175.96678113937378, 176.9368190765381, 177.9068570137024, 178.87155079841614, 179.83624458312988, 180.79522824287415, 181.7542119026184, 182.72067785263062, 183.68714380264282, 184.65784811973572, 185.6285524368286, 186.58427095413208, 187.53998947143555, 188.50804042816162, 189.4760913848877, 190.43607568740845, 191.3960599899292, 192.35075163841248, 193.30544328689575, 194.2608392238617, 195.21623516082764, 196.1723906993866, 197.12854623794556, 198.0879716873169, 199.04739713668823, 200.01048064231873, 200.97356414794922, 201.93092322349548, 202.88828229904175, 203.850115776062, 204.81194925308228, 206.70902037620544, 208.6060914993286]
[18.035, 18.035, 25.2025, 25.2025, 27.725, 27.725, 29.58, 29.58, 32.3, 32.3, 35.33, 35.33, 37.7875, 37.7875, 39.61, 39.61, 41.205, 41.205, 42.51, 42.51, 43.765, 43.765, 44.1575, 44.1575, 45.425, 45.425, 44.9875, 44.9875, 46.9625, 46.9625, 47.835, 47.835, 48.7825, 48.7825, 48.8875, 48.8875, 49.7675, 49.7675, 50.4725, 50.4725, 51.0775, 51.0775, 51.28, 51.28, 51.465, 51.465, 52.3425, 52.3425, 52.4575, 52.4575, 52.575, 52.575, 53.255, 53.255, 54.035, 54.035, 54.2925, 54.2925, 55.2675, 55.2675, 55.5025, 55.5025, 55.35, 55.35, 55.6625, 55.6625, 55.8125, 55.8125, 56.01, 56.01, 56.3475, 56.3475, 57.1275, 57.1275, 57.6175, 57.6175, 57.5075, 57.5075, 57.605, 57.605, 58.23, 58.23, 58.0925, 58.0925, 58.7125, 58.7125, 58.4225, 58.4225, 58.8475, 58.8475, 59.13, 59.13, 58.165, 58.165, 58.775, 58.775, 58.4775, 58.4775, 59.0575, 59.0575, 58.8975, 58.8975, 59.41, 59.41, 60.5225, 60.5225, 59.8975, 59.8975, 60.0525, 60.0525, 60.3475, 60.3475, 60.22, 60.22, 61.0625, 61.0625, 61.06, 61.06, 61.025, 61.025, 60.8625, 60.8625, 61.3625, 61.3625, 61.5625, 61.5625, 61.8975, 61.8975, 61.4525, 61.4525, 61.29, 61.29, 61.8925, 61.8925, 61.5375, 61.5375, 61.39, 61.39, 61.5375, 61.5375, 61.58, 61.58, 61.66, 61.66, 61.1975, 61.1975, 61.2375, 61.2375, 61.0825, 61.0825, 61.2925, 61.2925, 61.285, 61.285, 60.97, 60.97, 61.125, 61.125, 61.2375, 61.2375, 61.3125, 61.3125, 61.4075, 61.4075, 61.9, 61.9, 62.005, 62.005, 61.8525, 61.8525, 61.825, 61.825, 61.915, 61.915, 61.8975, 61.8975, 62.25, 62.25, 62.065, 62.065, 62.375, 62.375, 62.2925, 62.2925, 62.3425, 62.3425, 62.4175, 62.4175, 62.1225, 62.1225, 62.2225, 62.2225, 61.8675, 61.8675, 62.2625, 62.2625, 62.625, 62.625, 62.485, 62.485, 62.2275, 62.2275]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.316, Test loss: 1.658, Test accuracy: 60.79
Average accuracy final 10 rounds: 61.26974999999999 

2939.7534070014954
[1.4107944965362549, 2.8215889930725098, 4.126194715499878, 5.430800437927246, 6.694250583648682, 7.957700729370117, 9.229868173599243, 10.50203561782837, 11.773598670959473, 13.045161724090576, 14.320266962051392, 15.595372200012207, 16.87213683128357, 18.14890146255493, 19.422929286956787, 20.696957111358643, 21.974935054779053, 23.252912998199463, 24.527540683746338, 25.802168369293213, 27.077502727508545, 28.352837085723877, 29.61932873725891, 30.885820388793945, 32.15354800224304, 33.42127561569214, 34.68830394744873, 35.95533227920532, 37.22433304786682, 38.49333381652832, 39.76905846595764, 41.04478311538696, 42.31490707397461, 43.585031032562256, 44.85438251495361, 46.12373399734497, 47.397011518478394, 48.670289039611816, 49.93879508972168, 51.20730113983154, 52.47592067718506, 53.744540214538574, 55.015454053878784, 56.286367893218994, 57.56394863128662, 58.84152936935425, 60.1234917640686, 61.40545415878296, 62.68064832687378, 63.9558424949646, 65.22906589508057, 66.50228929519653, 67.77232027053833, 69.04235124588013, 70.31863737106323, 71.59492349624634, 72.86801815032959, 74.14111280441284, 75.41241097450256, 76.68370914459229, 77.95564079284668, 79.22757244110107, 80.49618721008301, 81.76480197906494, 83.03740859031677, 84.3100152015686, 85.59299206733704, 86.87596893310547, 88.16637063026428, 89.4567723274231, 90.73292684555054, 92.00908136367798, 93.28642392158508, 94.56376647949219, 95.84062314033508, 97.11747980117798, 98.40122985839844, 99.6849799156189, 100.97485542297363, 102.26473093032837, 103.54065442085266, 104.81657791137695, 106.0939519405365, 107.37132596969604, 108.64965224266052, 109.927978515625, 111.20244812965393, 112.47691774368286, 113.75467300415039, 115.03242826461792, 116.30822420120239, 117.58402013778687, 118.86159420013428, 120.13916826248169, 121.41312098503113, 122.68707370758057, 123.96696209907532, 125.24685049057007, 126.54789781570435, 127.84894514083862, 129.13758969306946, 130.4262342453003, 131.69019556045532, 132.95415687561035, 134.21667170524597, 135.4791865348816, 136.746652841568, 138.0141191482544, 139.2229881286621, 140.43185710906982, 141.63899445533752, 142.84613180160522, 144.05610537528992, 145.2660789489746, 146.54144930839539, 147.81681966781616, 149.07715153694153, 150.3374834060669, 151.60007739067078, 152.86267137527466, 154.1386115550995, 155.41455173492432, 156.6845715045929, 157.95459127426147, 159.23151516914368, 160.50843906402588, 161.78170585632324, 163.0549726486206, 164.33414030075073, 165.61330795288086, 166.89116430282593, 168.169020652771, 169.4490249156952, 170.72902917861938, 172.01044344902039, 173.2918577194214, 174.561199426651, 175.83054113388062, 177.09927773475647, 178.36801433563232, 179.6403489112854, 180.91268348693848, 182.18415355682373, 183.45562362670898, 184.7234857082367, 185.9913477897644, 187.26474595069885, 188.5381441116333, 189.80322313308716, 191.06830215454102, 192.32770538330078, 193.58710861206055, 194.84577465057373, 196.1044406890869, 197.36397647857666, 198.6235122680664, 199.89202070236206, 201.16052913665771, 202.43243288993835, 203.704336643219, 204.96456265449524, 206.22478866577148, 207.48556447029114, 208.7463402748108, 210.01268100738525, 211.27902173995972, 212.5416886806488, 213.8043556213379, 215.0737063884735, 216.34305715560913, 217.61006021499634, 218.87706327438354, 220.14295315742493, 221.4088430404663, 222.6786503791809, 223.9484577178955, 225.2213191986084, 226.4941806793213, 227.76675605773926, 229.03933143615723, 230.30935788154602, 231.57938432693481, 232.85276770591736, 234.1261510848999, 235.40102577209473, 236.67590045928955, 237.94946098327637, 239.22302150726318, 240.49332094192505, 241.7636203765869, 243.0297040939331, 244.2957878112793, 245.56594514846802, 246.83610248565674, 248.0486397743225, 249.26117706298828, 250.463294506073, 251.66541194915771, 252.86378526687622, 254.06215858459473, 256.0106723308563, 257.9591860771179]
[25.9525, 25.9525, 30.84, 30.84, 35.5425, 35.5425, 38.87, 38.87, 42.0475, 42.0475, 44.115, 44.115, 46.1425, 46.1425, 47.3925, 47.3925, 47.595, 47.595, 49.24, 49.24, 49.9775, 49.9775, 50.7, 50.7, 51.1025, 51.1025, 52.365, 52.365, 52.8825, 52.8825, 53.2475, 53.2475, 54.84, 54.84, 55.2825, 55.2825, 55.4375, 55.4375, 55.8775, 55.8775, 56.4625, 56.4625, 56.5175, 56.5175, 57.29, 57.29, 57.525, 57.525, 57.96, 57.96, 58.18, 58.18, 59.0375, 59.0375, 59.345, 59.345, 59.2725, 59.2725, 59.4275, 59.4275, 59.0975, 59.0975, 59.52, 59.52, 59.615, 59.615, 60.1125, 60.1125, 60.4075, 60.4075, 60.2975, 60.2975, 59.6475, 59.6475, 60.7625, 60.7625, 61.1775, 61.1775, 60.905, 60.905, 61.2375, 61.2375, 61.5375, 61.5375, 61.415, 61.415, 61.5475, 61.5475, 61.255, 61.255, 60.96, 60.96, 61.2925, 61.2925, 61.2675, 61.2675, 61.4825, 61.4825, 61.0625, 61.0625, 61.39, 61.39, 60.685, 60.685, 61.5625, 61.5625, 61.73, 61.73, 61.535, 61.535, 61.245, 61.245, 61.0825, 61.0825, 61.4175, 61.4175, 60.96, 60.96, 61.3975, 61.3975, 61.125, 61.125, 61.7475, 61.7475, 61.2, 61.2, 61.19, 61.19, 61.565, 61.565, 61.425, 61.425, 61.4475, 61.4475, 61.8075, 61.8075, 61.2375, 61.2375, 61.59, 61.59, 61.3975, 61.3975, 61.5725, 61.5725, 61.4475, 61.4475, 61.7025, 61.7025, 61.8475, 61.8475, 61.7275, 61.7275, 61.3925, 61.3925, 61.58, 61.58, 61.03, 61.03, 61.6575, 61.6575, 61.4125, 61.4125, 61.66, 61.66, 61.805, 61.805, 61.865, 61.865, 61.5725, 61.5725, 61.6575, 61.6575, 61.0375, 61.0375, 61.4925, 61.4925, 61.1925, 61.1925, 61.04, 61.04, 61.5475, 61.5475, 61.1225, 61.1225, 61.565, 61.565, 61.6425, 61.6425, 61.3575, 61.3575, 61.3875, 61.3875, 61.185, 61.185, 60.755, 60.755, 61.215, 61.215, 60.92, 60.92, 60.79, 60.79]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.093, Test loss: 4.309, Test accuracy: 42.52
Average accuracy final 10 rounds: 42.1585 

2757.988128900528
[1.4996459484100342, 2.9992918968200684, 4.007063627243042, 5.014835357666016, 6.021334886550903, 7.027834415435791, 8.036618947982788, 9.045403480529785, 10.050469875335693, 11.055536270141602, 12.0567147731781, 13.0578932762146, 14.072183609008789, 15.086473941802979, 16.095290184020996, 17.104106426239014, 18.11314821243286, 19.12218999862671, 20.13716959953308, 21.152149200439453, 22.155126094818115, 23.158102989196777, 24.1610164642334, 25.16392993927002, 26.170743227005005, 27.17755651473999, 28.186712503433228, 29.195868492126465, 30.203938722610474, 31.212008953094482, 32.221548318862915, 33.23108768463135, 34.23479509353638, 35.238502502441406, 36.244593143463135, 37.25068378448486, 38.256269216537476, 39.26185464859009, 40.27071213722229, 41.27956962585449, 42.289762020111084, 43.299954414367676, 44.3010892868042, 45.30222415924072, 46.3082389831543, 47.31425380706787, 48.322279930114746, 49.33030605316162, 50.336363077163696, 51.34242010116577, 52.350043296813965, 53.35766649246216, 54.36434555053711, 55.37102460861206, 56.36948871612549, 57.367952823638916, 58.37611198425293, 59.38427114486694, 60.389503955841064, 61.394736766815186, 62.401655197143555, 63.408573627471924, 64.41390442848206, 65.41923522949219, 66.42482089996338, 67.43040657043457, 68.42692828178406, 69.42344999313354, 70.429936170578, 71.43642234802246, 72.44115114212036, 73.44587993621826, 74.45074963569641, 75.45561933517456, 76.46069622039795, 77.46577310562134, 78.46705913543701, 79.46834516525269, 80.46934199333191, 81.47033882141113, 82.49660229682922, 83.52286577224731, 84.52715134620667, 85.53143692016602, 86.53882431983948, 87.54621171951294, 88.72014570236206, 89.89407968521118, 91.07137894630432, 92.24867820739746, 93.42046165466309, 94.59224510192871, 95.76598763465881, 96.93973016738892, 98.11465191841125, 99.2895736694336, 100.45891427993774, 101.6282548904419, 102.78648614883423, 103.94471740722656, 105.11848759651184, 106.29225778579712, 107.47051167488098, 108.64876556396484, 109.75674223899841, 110.86471891403198, 111.97703337669373, 113.08934783935547, 114.2060399055481, 115.32273197174072, 116.48344826698303, 117.64416456222534, 118.76217436790466, 119.88018417358398, 121.00538325309753, 122.13058233261108, 123.25780177116394, 124.3850212097168, 125.54531192779541, 126.70560264587402, 127.87015676498413, 129.03471088409424, 130.20933294296265, 131.38395500183105, 132.55253624916077, 133.72111749649048, 134.89136290550232, 136.06160831451416, 137.2346043586731, 138.40760040283203, 139.5706503391266, 140.73370027542114, 141.90149092674255, 143.06928157806396, 144.23946928977966, 145.40965700149536, 146.58650851249695, 147.76336002349854, 148.93513870239258, 150.10691738128662, 151.27944326400757, 152.45196914672852, 153.62266540527344, 154.79336166381836, 155.96019196510315, 157.12702226638794, 158.29970574378967, 159.4723892211914, 160.63250160217285, 161.7926139831543, 162.95064640045166, 164.10867881774902, 165.13286638259888, 166.15705394744873, 167.30863571166992, 168.4602174758911, 169.6152527332306, 170.77028799057007, 171.92711782455444, 173.08394765853882, 174.2345519065857, 175.38515615463257, 176.53992414474487, 177.69469213485718, 178.84989476203918, 180.0050973892212, 181.1743621826172, 182.34362697601318, 183.50000977516174, 184.6563925743103, 185.813884973526, 186.9713773727417, 188.13162183761597, 189.29186630249023, 190.44924473762512, 191.60662317276, 192.7626450061798, 193.9186668395996, 195.08758807182312, 196.25650930404663, 197.43350839614868, 198.61050748825073, 199.78009700775146, 200.9496865272522, 202.1128876209259, 203.2760887145996, 204.4380967617035, 205.60010480880737, 206.7667670249939, 207.93342924118042, 209.10144972801208, 210.26947021484375, 211.44395971298218, 212.6184492111206, 213.78482460975647, 214.95120000839233, 216.11590719223022, 217.28061437606812, 218.45141005516052, 219.62220573425293, 221.83999109268188, 224.05777645111084]
[23.685, 23.685, 28.99, 28.99, 32.9175, 32.9175, 34.185, 34.185, 35.675, 35.675, 36.11, 36.11, 37.15, 37.15, 38.6125, 38.6125, 38.74, 38.74, 38.5425, 38.5425, 38.895, 38.895, 39.4, 39.4, 40.4925, 40.4925, 40.6225, 40.6225, 40.9925, 40.9925, 40.425, 40.425, 40.6375, 40.6375, 40.605, 40.605, 40.815, 40.815, 41.5125, 41.5125, 41.4475, 41.4475, 41.3775, 41.3775, 41.4325, 41.4325, 41.8075, 41.8075, 42.165, 42.165, 41.85, 41.85, 41.95, 41.95, 41.9225, 41.9225, 42.01, 42.01, 41.5375, 41.5375, 41.795, 41.795, 41.9925, 41.9925, 42.0825, 42.0825, 41.84, 41.84, 41.8325, 41.8325, 41.39, 41.39, 41.7875, 41.7875, 42.035, 42.035, 41.7675, 41.7675, 41.5975, 41.5975, 41.535, 41.535, 41.54, 41.54, 41.7375, 41.7375, 41.19, 41.19, 41.805, 41.805, 41.8525, 41.8525, 42.1925, 42.1925, 41.755, 41.755, 41.925, 41.925, 41.595, 41.595, 41.3925, 41.3925, 41.4575, 41.4575, 41.5325, 41.5325, 41.7, 41.7, 41.4325, 41.4325, 41.825, 41.825, 41.955, 41.955, 41.8775, 41.8775, 41.9825, 41.9825, 41.7075, 41.7075, 41.8375, 41.8375, 42.04, 42.04, 41.775, 41.775, 42.035, 42.035, 42.3075, 42.3075, 42.4625, 42.4625, 42.1275, 42.1275, 42.1975, 42.1975, 41.995, 41.995, 42.065, 42.065, 42.2275, 42.2275, 42.0475, 42.0475, 41.9975, 41.9975, 42.445, 42.445, 42.4775, 42.4775, 42.1275, 42.1275, 42.24, 42.24, 41.985, 41.985, 42.105, 42.105, 42.135, 42.135, 41.835, 41.835, 41.8625, 41.8625, 42.025, 42.025, 42.005, 42.005, 42.1425, 42.1425, 42.11, 42.11, 42.175, 42.175, 42.485, 42.485, 42.2175, 42.2175, 41.935, 41.935, 42.195, 42.195, 41.9125, 41.9125, 42.0225, 42.0225, 42.055, 42.055, 42.0525, 42.0525, 42.275, 42.275, 42.4675, 42.4675, 42.4275, 42.4275, 41.98, 41.98, 42.1975, 42.1975, 42.515, 42.515]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 1.446, Test loss: 1.881, Test accuracy: 29.99
Round   1, Train loss: 1.262, Test loss: 1.864, Test accuracy: 30.65
Round   2, Train loss: 1.178, Test loss: 1.940, Test accuracy: 28.64
Round   3, Train loss: 1.081, Test loss: 1.944, Test accuracy: 29.75
Round   4, Train loss: 1.048, Test loss: 2.022, Test accuracy: 27.95
Round   5, Train loss: 1.004, Test loss: 2.000, Test accuracy: 30.25
Round   6, Train loss: 0.945, Test loss: 2.042, Test accuracy: 29.00
Round   7, Train loss: 0.878, Test loss: 2.078, Test accuracy: 28.21
Round   8, Train loss: 0.854, Test loss: 2.065, Test accuracy: 28.84
Round   9, Train loss: 0.816, Test loss: 2.058, Test accuracy: 29.17
Round  10, Train loss: 0.761, Test loss: 2.043, Test accuracy: 29.66
Round  11, Train loss: 0.737, Test loss: 2.028, Test accuracy: 30.95
Round  12, Train loss: 0.735, Test loss: 2.030, Test accuracy: 30.14
Round  13, Train loss: 0.673, Test loss: 2.025, Test accuracy: 30.76
Round  14, Train loss: 0.671, Test loss: 2.009, Test accuracy: 32.62
Round  15, Train loss: 0.603, Test loss: 2.010, Test accuracy: 31.89
Round  16, Train loss: 0.587, Test loss: 2.001, Test accuracy: 32.29
Round  17, Train loss: 0.595, Test loss: 1.993, Test accuracy: 33.29
Round  18, Train loss: 0.581, Test loss: 1.983, Test accuracy: 34.18
Round  19, Train loss: 0.537, Test loss: 1.978, Test accuracy: 34.34
Round  20, Train loss: 0.560, Test loss: 2.025, Test accuracy: 32.77
Round  21, Train loss: 0.500, Test loss: 2.014, Test accuracy: 33.37
Round  22, Train loss: 0.486, Test loss: 2.010, Test accuracy: 33.75
Round  23, Train loss: 0.445, Test loss: 1.993, Test accuracy: 34.97
Round  24, Train loss: 0.481, Test loss: 1.986, Test accuracy: 35.62
Round  25, Train loss: 0.422, Test loss: 1.975, Test accuracy: 36.11
Round  26, Train loss: 0.379, Test loss: 1.972, Test accuracy: 36.67
Round  27, Train loss: 0.419, Test loss: 1.965, Test accuracy: 37.30
Round  28, Train loss: 0.413, Test loss: 1.961, Test accuracy: 37.18
Round  29, Train loss: 0.417, Test loss: 1.951, Test accuracy: 37.91
Round  30, Train loss: 0.374, Test loss: 1.934, Test accuracy: 39.05
Round  31, Train loss: 0.341, Test loss: 1.925, Test accuracy: 39.37
Round  32, Train loss: 0.323, Test loss: 1.934, Test accuracy: 38.62
Round  33, Train loss: 0.334, Test loss: 1.921, Test accuracy: 39.27
Round  34, Train loss: 0.351, Test loss: 1.919, Test accuracy: 38.96
Round  35, Train loss: 0.301, Test loss: 1.901, Test accuracy: 39.62
Round  36, Train loss: 0.347, Test loss: 1.898, Test accuracy: 39.45
Round  37, Train loss: 0.278, Test loss: 1.886, Test accuracy: 40.22
Round  38, Train loss: 0.304, Test loss: 1.885, Test accuracy: 40.02
Round  39, Train loss: 0.273, Test loss: 1.873, Test accuracy: 40.50
Round  40, Train loss: 0.289, Test loss: 1.869, Test accuracy: 41.10
Round  41, Train loss: 0.265, Test loss: 1.856, Test accuracy: 41.53
Round  42, Train loss: 0.282, Test loss: 1.853, Test accuracy: 41.30
Round  43, Train loss: 0.249, Test loss: 1.847, Test accuracy: 41.58
Round  44, Train loss: 0.253, Test loss: 1.852, Test accuracy: 40.77
Round  45, Train loss: 0.255, Test loss: 1.844, Test accuracy: 41.15
Round  46, Train loss: 0.241, Test loss: 1.835, Test accuracy: 41.63
Round  47, Train loss: 0.245, Test loss: 1.828, Test accuracy: 41.92
Round  48, Train loss: 0.248, Test loss: 1.821, Test accuracy: 43.28
Round  49, Train loss: 0.219, Test loss: 1.827, Test accuracy: 42.41
Round  50, Train loss: 0.229, Test loss: 1.820, Test accuracy: 42.97
Round  51, Train loss: 0.216, Test loss: 1.813, Test accuracy: 43.45
Round  52, Train loss: 0.196, Test loss: 1.803, Test accuracy: 42.90
Round  53, Train loss: 0.233, Test loss: 1.798, Test accuracy: 42.78
Round  54, Train loss: 0.189, Test loss: 1.781, Test accuracy: 43.80
Round  55, Train loss: 0.203, Test loss: 1.785, Test accuracy: 43.55
Round  56, Train loss: 0.199, Test loss: 1.786, Test accuracy: 43.80
Round  57, Train loss: 0.201, Test loss: 1.785, Test accuracy: 43.13
Round  58, Train loss: 0.197, Test loss: 1.772, Test accuracy: 43.44
Round  59, Train loss: 0.206, Test loss: 1.774, Test accuracy: 43.74
Round  60, Train loss: 0.215, Test loss: 1.770, Test accuracy: 43.72
Round  61, Train loss: 0.190, Test loss: 1.767, Test accuracy: 43.84
Round  62, Train loss: 0.186, Test loss: 1.757, Test accuracy: 44.26
Round  63, Train loss: 0.182, Test loss: 1.757, Test accuracy: 44.01
Round  64, Train loss: 0.170, Test loss: 1.751, Test accuracy: 44.48
Round  65, Train loss: 0.172, Test loss: 1.745, Test accuracy: 44.38
Round  66, Train loss: 0.171, Test loss: 1.737, Test accuracy: 44.95
Round  67, Train loss: 0.181, Test loss: 1.735, Test accuracy: 44.47
Round  68, Train loss: 0.158, Test loss: 1.733, Test accuracy: 44.29
Round  69, Train loss: 0.170, Test loss: 1.714, Test accuracy: 45.40
Round  70, Train loss: 0.168, Test loss: 1.726, Test accuracy: 44.64
Round  71, Train loss: 0.156, Test loss: 1.723, Test accuracy: 44.97
Round  72, Train loss: 0.170, Test loss: 1.720, Test accuracy: 45.18
Round  73, Train loss: 0.159, Test loss: 1.719, Test accuracy: 44.67
Round  74, Train loss: 0.181, Test loss: 1.718, Test accuracy: 44.97
Round  75, Train loss: 0.147, Test loss: 1.718, Test accuracy: 44.48
Round  76, Train loss: 0.170, Test loss: 1.709, Test accuracy: 45.03
Round  77, Train loss: 0.143, Test loss: 1.706, Test accuracy: 44.74
Round  78, Train loss: 0.140, Test loss: 1.711, Test accuracy: 44.20
Round  79, Train loss: 0.145, Test loss: 1.708, Test accuracy: 44.25
Round  80, Train loss: 0.145, Test loss: 1.708, Test accuracy: 44.56
Round  81, Train loss: 0.138, Test loss: 1.697, Test accuracy: 45.43
Round  82, Train loss: 0.157, Test loss: 1.695, Test accuracy: 44.99
Round  83, Train loss: 0.143, Test loss: 1.700, Test accuracy: 45.00
Round  84, Train loss: 0.158, Test loss: 1.693, Test accuracy: 45.43
Round  85, Train loss: 0.136, Test loss: 1.678, Test accuracy: 46.01
Round  86, Train loss: 0.151, Test loss: 1.685, Test accuracy: 45.42
Round  87, Train loss: 0.134, Test loss: 1.687, Test accuracy: 45.05
Round  88, Train loss: 0.139, Test loss: 1.688, Test accuracy: 44.88
Round  89, Train loss: 0.131, Test loss: 1.667, Test accuracy: 45.89
Round  90, Train loss: 0.135, Test loss: 1.677, Test accuracy: 45.39
Round  91, Train loss: 0.134, Test loss: 1.673, Test accuracy: 45.47
Round  92, Train loss: 0.131, Test loss: 1.665, Test accuracy: 45.86
Round  93, Train loss: 0.129, Test loss: 1.664, Test accuracy: 46.01
Round  94, Train loss: 0.137, Test loss: 1.673, Test accuracy: 45.38
Round  95, Train loss: 0.137, Test loss: 1.665, Test accuracy: 45.82
Round  96, Train loss: 0.130, Test loss: 1.654, Test accuracy: 46.16
Round  97, Train loss: 0.141, Test loss: 1.663, Test accuracy: 46.00
Round  98, Train loss: 0.125, Test loss: 1.664, Test accuracy: 45.53
Round  99, Train loss: 0.129, Test loss: 1.662, Test accuracy: 45.57
Final Round, Train loss: 0.130, Test loss: 1.651, Test accuracy: 45.62
Average accuracy final 10 rounds: 45.71975
6184.241059780121
[]
[29.99, 30.6475, 28.6425, 29.7525, 27.9475, 30.2525, 29.0025, 28.21, 28.835, 29.1675, 29.665, 30.9475, 30.14, 30.7575, 32.62, 31.8875, 32.29, 33.2925, 34.1825, 34.3425, 32.77, 33.3725, 33.75, 34.97, 35.62, 36.1125, 36.675, 37.2975, 37.1825, 37.91, 39.0475, 39.365, 38.6175, 39.2675, 38.9625, 39.615, 39.4475, 40.22, 40.02, 40.4975, 41.105, 41.5275, 41.305, 41.575, 40.775, 41.15, 41.63, 41.92, 43.2775, 42.4125, 42.965, 43.4525, 42.895, 42.7825, 43.805, 43.5525, 43.8025, 43.1325, 43.44, 43.7375, 43.7225, 43.8425, 44.2575, 44.0125, 44.4825, 44.38, 44.945, 44.4725, 44.29, 45.4025, 44.64, 44.965, 45.1825, 44.6725, 44.9725, 44.475, 45.035, 44.7425, 44.205, 44.2475, 44.5575, 45.4325, 44.9875, 45.0, 45.4325, 46.01, 45.425, 45.045, 44.875, 45.8875, 45.3925, 45.47, 45.8625, 46.01, 45.38, 45.8225, 46.1575, 46.0, 45.5325, 45.57, 45.625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.993, Test loss: 1.178, Test accuracy: 60.17
Average accuracy final 10 rounds: 56.664
Average global accuracy final 10 rounds: 56.664
4485.145489454269
[]
[20.8025, 26.6575, 33.56, 38.18, 40.1175, 41.4325, 42.8675, 41.89, 42.615, 43.415, 43.98, 43.7175, 45.9075, 46.4025, 46.655, 46.195, 47.0275, 47.8775, 48.55, 49.0025, 49.3775, 50.0675, 49.345, 50.0625, 50.1775, 50.33, 51.2425, 52.0, 52.2025, 52.58, 52.185, 52.275, 51.965, 51.6225, 52.0925, 52.1725, 52.365, 52.555, 52.585, 52.2575, 52.3875, 52.74, 52.6375, 53.075, 52.7775, 52.84, 53.2225, 53.735, 54.33, 53.95, 53.7275, 54.3075, 54.245, 54.2675, 54.3025, 54.1375, 53.895, 54.25, 54.49, 54.0125, 54.295, 54.29, 54.57, 54.6875, 54.5625, 54.845, 54.82, 55.17, 55.05, 55.1425, 55.28, 55.2375, 55.3725, 55.43, 55.73, 55.5475, 55.1525, 55.29, 55.8625, 55.8125, 55.92, 56.035, 56.11, 56.0975, 55.965, 56.005, 56.035, 56.2125, 56.1225, 56.5875, 56.93, 57.0625, 56.935, 56.485, 56.4325, 56.54, 56.4325, 56.345, 56.445, 57.0325, 60.1725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 504, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54734 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 50211 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53965 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2181, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 55034 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.060, Test loss: 1.114, Test accuracy: 78.91
Final Round, Global train loss: 0.060, Global test loss: 2.076, Global test accuracy: 29.28
Average accuracy final 10 rounds: 78.48499999999999 

Average global accuracy final 10 rounds: 27.920833333333338 

1643.735722541809
[1.4096424579620361, 2.8192849159240723, 4.005798101425171, 5.1923112869262695, 6.379044055938721, 7.565776824951172, 8.743843078613281, 9.92190933227539, 11.110597372055054, 12.299285411834717, 13.486210346221924, 14.67313528060913, 15.857364416122437, 17.041593551635742, 18.220637559890747, 19.399681568145752, 20.578507900238037, 21.757334232330322, 22.939836502075195, 24.12233877182007, 25.300814628601074, 26.47929048538208, 27.658336400985718, 28.837382316589355, 29.971938133239746, 31.106493949890137, 32.36559581756592, 33.6246976852417, 34.825522899627686, 36.02634811401367, 37.067726612091064, 38.10910511016846, 39.14888858795166, 40.18867206573486, 41.22989201545715, 42.27111196517944, 43.30901503562927, 44.3469181060791, 45.38764500617981, 46.42837190628052, 47.47219491004944, 48.51601791381836, 49.55942177772522, 50.60282564163208, 51.652257204055786, 52.70168876647949, 53.73615860939026, 54.770628452301025, 55.81008815765381, 56.84954786300659, 57.88376069068909, 58.91797351837158, 59.96548080444336, 61.01298809051514, 62.055633783340454, 63.09827947616577, 64.14656639099121, 65.19485330581665, 66.23749661445618, 67.2801399230957, 68.4843213558197, 69.6885027885437, 70.90120601654053, 72.11390924453735, 73.32253885269165, 74.53116846084595, 75.73731803894043, 76.94346761703491, 78.15618085861206, 79.36889410018921, 80.57274317741394, 81.77659225463867, 82.98100185394287, 84.18541145324707, 85.38320207595825, 86.58099269866943, 87.6725697517395, 88.76414680480957, 89.81305050849915, 90.86195421218872, 91.91047096252441, 92.95898771286011, 94.00849866867065, 95.0580096244812, 96.09963417053223, 97.14125871658325, 98.1841983795166, 99.22713804244995, 100.26561069488525, 101.30408334732056, 102.34664416313171, 103.38920497894287, 104.42286801338196, 105.45653104782104, 106.49242234230042, 107.52831363677979, 108.57411789894104, 109.6199221611023, 110.66209483146667, 111.70426750183105, 112.75488662719727, 113.80550575256348, 114.84495377540588, 115.88440179824829, 116.92508172988892, 117.96576166152954, 119.00422549247742, 120.0426893234253, 121.08197259902954, 122.12125587463379, 123.17131781578064, 124.22137975692749, 125.26079607009888, 126.30021238327026, 127.34980297088623, 128.3993935585022, 129.43468809127808, 130.46998262405396, 131.5067434310913, 132.54350423812866, 133.5887942314148, 134.63408422470093, 135.6715223789215, 136.7089605331421, 137.75424695014954, 138.79953336715698, 139.83807969093323, 140.87662601470947, 141.92246437072754, 142.9683027267456, 144.00406002998352, 145.03981733322144, 146.08047008514404, 147.12112283706665, 148.16421175003052, 149.20730066299438, 150.24851155281067, 151.28972244262695, 152.34756779670715, 153.40541315078735, 154.4544975757599, 155.50358200073242, 156.56504201889038, 157.62650203704834, 158.67708730697632, 159.7276725769043, 160.7830364704132, 161.83840036392212, 162.8981592655182, 163.95791816711426, 165.00608158111572, 166.0542449951172, 167.11080598831177, 168.16736698150635, 169.21403574943542, 170.2607045173645, 171.3203318119049, 172.3799591064453, 177.7845058441162, 183.1890525817871, 184.23324060440063, 185.27742862701416, 186.32243394851685, 187.36743927001953, 188.40284967422485, 189.43826007843018, 190.4788818359375, 191.51950359344482, 192.54929852485657, 193.5790934562683, 194.6171498298645, 195.6552062034607, 196.67416095733643, 197.69311571121216, 198.71818232536316, 199.74324893951416, 200.77230668067932, 201.80136442184448, 202.83377528190613, 203.86618614196777, 204.90629696846008, 205.9464077949524, 206.9768168926239, 208.0072259902954, 209.04089307785034, 210.07456016540527, 211.105366230011, 212.1361722946167, 213.17325973510742, 214.21034717559814, 215.23805928230286, 216.26577138900757, 217.29660654067993, 218.3274416923523, 219.3580505847931, 220.3886594772339, 221.42114973068237, 222.45363998413086, 223.48801946640015, 224.52239894866943, 226.5773012638092, 228.63220357894897]
[21.908333333333335, 21.908333333333335, 33.38333333333333, 33.38333333333333, 47.75, 47.75, 56.625, 56.625, 59.141666666666666, 59.141666666666666, 59.416666666666664, 59.416666666666664, 63.61666666666667, 63.61666666666667, 63.891666666666666, 63.891666666666666, 66.90833333333333, 66.90833333333333, 68.975, 68.975, 69.65, 69.65, 71.94166666666666, 71.94166666666666, 72.3, 72.3, 73.275, 73.275, 73.625, 73.625, 74.13333333333334, 74.13333333333334, 74.375, 74.375, 74.76666666666667, 74.76666666666667, 74.38333333333334, 74.38333333333334, 74.51666666666667, 74.51666666666667, 74.55, 74.55, 74.70833333333333, 74.70833333333333, 74.86666666666666, 74.86666666666666, 75.20833333333333, 75.20833333333333, 75.40833333333333, 75.40833333333333, 75.71666666666667, 75.71666666666667, 75.36666666666666, 75.36666666666666, 75.53333333333333, 75.53333333333333, 76.19166666666666, 76.19166666666666, 75.83333333333333, 75.83333333333333, 76.225, 76.225, 76.26666666666667, 76.26666666666667, 76.41666666666667, 76.41666666666667, 76.45833333333333, 76.45833333333333, 76.51666666666667, 76.51666666666667, 76.76666666666667, 76.76666666666667, 76.9, 76.9, 77.48333333333333, 77.48333333333333, 77.45, 77.45, 77.28333333333333, 77.28333333333333, 77.65, 77.65, 77.46666666666667, 77.46666666666667, 77.46666666666667, 77.46666666666667, 77.08333333333333, 77.08333333333333, 77.175, 77.175, 77.3, 77.3, 77.575, 77.575, 77.6, 77.6, 78.05, 78.05, 77.89166666666667, 77.89166666666667, 77.59166666666667, 77.59166666666667, 77.33333333333333, 77.33333333333333, 77.35, 77.35, 77.71666666666667, 77.71666666666667, 77.75833333333334, 77.75833333333334, 77.8, 77.8, 77.99166666666666, 77.99166666666666, 77.54166666666667, 77.54166666666667, 77.53333333333333, 77.53333333333333, 77.85, 77.85, 77.55833333333334, 77.55833333333334, 77.55833333333334, 77.55833333333334, 77.88333333333334, 77.88333333333334, 78.00833333333334, 78.00833333333334, 78.86666666666666, 78.86666666666666, 78.33333333333333, 78.33333333333333, 78.34166666666667, 78.34166666666667, 78.28333333333333, 78.28333333333333, 78.35833333333333, 78.35833333333333, 78.2, 78.2, 78.23333333333333, 78.23333333333333, 78.49166666666666, 78.49166666666666, 78.5, 78.5, 78.54166666666667, 78.54166666666667, 78.525, 78.525, 78.6, 78.6, 78.34166666666667, 78.34166666666667, 78.69166666666666, 78.69166666666666, 78.33333333333333, 78.33333333333333, 77.98333333333333, 77.98333333333333, 78.15, 78.15, 77.96666666666667, 77.96666666666667, 77.975, 77.975, 78.2, 78.2, 78.30833333333334, 78.30833333333334, 78.43333333333334, 78.43333333333334, 78.59166666666667, 78.59166666666667, 78.33333333333333, 78.33333333333333, 77.83333333333333, 77.83333333333333, 78.15, 78.15, 78.125, 78.125, 78.28333333333333, 78.28333333333333, 77.99166666666666, 77.99166666666666, 78.34166666666667, 78.34166666666667, 78.40833333333333, 78.40833333333333, 78.8, 78.8, 78.53333333333333, 78.53333333333333, 78.86666666666666, 78.86666666666666, 78.675, 78.675, 78.825, 78.825, 78.90833333333333, 78.90833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.338, Test loss: 1.651, Test accuracy: 62.15
Final Round, Global train loss: 0.338, Global test loss: 1.099, Global test accuracy: 68.89
Average accuracy final 10 rounds: 63.23075 

Average global accuracy final 10 rounds: 69.11775 

3719.501929998398
[1.3907854557037354, 2.7815709114074707, 3.9590554237365723, 5.136539936065674, 6.321399688720703, 7.506259441375732, 8.689545392990112, 9.872831344604492, 11.059006452560425, 12.245181560516357, 13.434542655944824, 14.623903751373291, 15.81161642074585, 16.999329090118408, 18.183033227920532, 19.366737365722656, 20.553630113601685, 21.740522861480713, 22.923227071762085, 24.105931282043457, 25.29050588607788, 26.475080490112305, 27.660365343093872, 28.84565019607544, 30.027324199676514, 31.208998203277588, 32.391993284225464, 33.57498836517334, 34.75338554382324, 35.931782722473145, 37.12093734741211, 38.310091972351074, 39.49545121192932, 40.68081045150757, 41.86489748954773, 43.04898452758789, 44.06516790390015, 45.0813512802124, 46.097209453582764, 47.113067626953125, 48.1282274723053, 49.14338731765747, 50.161465644836426, 51.17954397201538, 52.19631481170654, 53.213085651397705, 54.230019092559814, 55.246952533721924, 56.2644579410553, 57.28196334838867, 58.298269748687744, 59.314576148986816, 60.332802295684814, 61.35102844238281, 62.37112736701965, 63.391226291656494, 64.40858578681946, 65.42594528198242, 66.44491648674011, 67.4638876914978, 68.48271656036377, 69.50154542922974, 70.51699042320251, 71.5324354171753, 72.5462327003479, 73.56002998352051, 74.57990980148315, 75.5997896194458, 76.61566972732544, 77.63154983520508, 78.64627742767334, 79.6610050201416, 80.68000221252441, 81.69899940490723, 82.71492886543274, 83.73085832595825, 84.75270891189575, 85.77455949783325, 86.78851342201233, 87.8024673461914, 88.81853914260864, 89.83461093902588, 90.84974193572998, 91.86487293243408, 92.87901020050049, 93.8931474685669, 94.90555357933044, 95.917959690094, 96.93396806716919, 97.94997644424438, 98.96752381324768, 99.98507118225098, 101.0013165473938, 102.01756191253662, 103.03594732284546, 104.0543327331543, 105.07051730155945, 106.0867018699646, 107.10361409187317, 108.12052631378174, 109.13743948936462, 110.15435266494751, 111.1716685295105, 112.18898439407349, 113.20526599884033, 114.22154760360718, 115.22934770584106, 116.23714780807495, 117.24541020393372, 118.25367259979248, 119.25868105888367, 120.26368951797485, 121.27324104309082, 122.28279256820679, 123.29411911964417, 124.30544567108154, 125.32227659225464, 126.33910751342773, 127.35349202156067, 128.3678765296936, 129.3813066482544, 130.39473676681519, 131.4106686115265, 132.4266004562378, 133.44148898124695, 134.4563775062561, 135.47676181793213, 136.49714612960815, 137.51771092414856, 138.53827571868896, 139.55301117897034, 140.5677466392517, 141.58279514312744, 142.59784364700317, 143.60942435264587, 144.62100505828857, 145.63530206680298, 146.64959907531738, 147.6658797264099, 148.68216037750244, 149.69507002830505, 150.70797967910767, 151.72776007652283, 152.747540473938, 153.75928282737732, 154.77102518081665, 155.78760981559753, 156.80419445037842, 157.82001066207886, 158.8358268737793, 159.8493504524231, 160.8628740310669, 161.87606263160706, 162.88925123214722, 163.90476202964783, 164.92027282714844, 165.93907928466797, 166.9578857421875, 167.97177386283875, 168.98566198349, 170.00086426734924, 171.0160665512085, 172.02915954589844, 173.04225254058838, 174.05828189849854, 175.0743112564087, 176.08791542053223, 177.10151958465576, 178.11496305465698, 179.1284065246582, 180.13979744911194, 181.15118837356567, 182.16742372512817, 183.18365907669067, 184.19618916511536, 185.20871925354004, 186.22170615196228, 187.23469305038452, 188.24855279922485, 189.26241254806519, 190.27461171150208, 191.28681087493896, 192.29818773269653, 193.3095645904541, 194.32156586647034, 195.33356714248657, 196.34793162345886, 197.36229610443115, 198.37762355804443, 199.39295101165771, 200.409508228302, 201.4260654449463, 202.44069147109985, 203.45531749725342, 204.460599899292, 205.46588230133057, 206.47117519378662, 207.47646808624268, 208.48326420783997, 209.49006032943726, 211.5103018283844, 213.53054332733154]
[24.5725, 24.5725, 28.7175, 28.7175, 31.2925, 31.2925, 35.17, 35.17, 37.1525, 37.1525, 38.6975, 38.6975, 39.105, 39.105, 40.15, 40.15, 41.87, 41.87, 42.3825, 42.3825, 43.6125, 43.6125, 44.165, 44.165, 45.8175, 45.8175, 46.665, 46.665, 47.99, 47.99, 48.9425, 48.9425, 49.4325, 49.4325, 49.75, 49.75, 49.8675, 49.8675, 50.71, 50.71, 51.5525, 51.5525, 51.82, 51.82, 52.875, 52.875, 53.7, 53.7, 53.8725, 53.8725, 54.1575, 54.1575, 54.16, 54.16, 54.1625, 54.1625, 54.4875, 54.4875, 54.9025, 54.9025, 55.0325, 55.0325, 55.845, 55.845, 56.0, 56.0, 55.975, 55.975, 56.365, 56.365, 56.795, 56.795, 56.9925, 56.9925, 57.0825, 57.0825, 57.0275, 57.0275, 57.4325, 57.4325, 57.7675, 57.7675, 57.97, 57.97, 58.17, 58.17, 58.57, 58.57, 58.815, 58.815, 58.9725, 58.9725, 59.345, 59.345, 59.6175, 59.6175, 59.645, 59.645, 59.805, 59.805, 59.9175, 59.9175, 59.93, 59.93, 60.06, 60.06, 60.55, 60.55, 60.6, 60.6, 60.38, 60.38, 60.3225, 60.3225, 60.7575, 60.7575, 60.8675, 60.8675, 60.795, 60.795, 61.08, 61.08, 61.0425, 61.0425, 61.3475, 61.3475, 61.2825, 61.2825, 61.1675, 61.1675, 61.0425, 61.0425, 61.155, 61.155, 61.17, 61.17, 61.4225, 61.4225, 61.5275, 61.5275, 61.72, 61.72, 61.8725, 61.8725, 62.0975, 62.0975, 61.7175, 61.7175, 61.7, 61.7, 61.7475, 61.7475, 61.875, 61.875, 61.885, 61.885, 62.21, 62.21, 62.2025, 62.2025, 62.6425, 62.6425, 62.4875, 62.4875, 62.425, 62.425, 62.71, 62.71, 62.4475, 62.4475, 61.985, 61.985, 62.0875, 62.0875, 62.42, 62.42, 62.9375, 62.9375, 63.0575, 63.0575, 63.1775, 63.1775, 62.8625, 62.8625, 63.085, 63.085, 63.0925, 63.0925, 63.385, 63.385, 63.3875, 63.3875, 63.2325, 63.2325, 63.435, 63.435, 63.225, 63.225, 63.425, 63.425, 62.145, 62.145]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.516, Test loss: 1.289, Test accuracy: 62.73
Average accuracy final 10 rounds: 62.61050000000001 

2691.353810787201
[1.3145081996917725, 2.629016399383545, 3.693326711654663, 4.757637023925781, 5.820624589920044, 6.883612155914307, 7.948674440383911, 9.013736724853516, 10.078823804855347, 11.143910884857178, 12.203550338745117, 13.263189792633057, 14.32628870010376, 15.389387607574463, 16.449488162994385, 17.509588718414307, 18.57675337791443, 19.64391803741455, 20.70456886291504, 21.765219688415527, 22.82656192779541, 23.887904167175293, 24.95186161994934, 26.01581907272339, 27.0753071308136, 28.13479518890381, 29.20031714439392, 30.265839099884033, 31.325196504592896, 32.38455390930176, 33.445061445236206, 34.505568981170654, 35.566640853881836, 36.62771272659302, 37.68974447250366, 38.75177621841431, 39.81450843811035, 40.8772406578064, 41.936487674713135, 42.99573469161987, 44.060351848602295, 45.12496900558472, 46.18259048461914, 47.240211963653564, 48.305758476257324, 49.371304988861084, 50.437251567840576, 51.50319814682007, 52.56637692451477, 53.62955570220947, 54.68964624404907, 55.74973678588867, 56.807790994644165, 57.86584520339966, 58.92843508720398, 59.9910249710083, 61.04995393753052, 62.108882904052734, 63.16916537284851, 64.22944784164429, 65.28834462165833, 66.34724140167236, 67.40934610366821, 68.47145080566406, 69.53184366226196, 70.59223651885986, 71.65576553344727, 72.71929454803467, 73.77877807617188, 74.83826160430908, 75.89629197120667, 76.95432233810425, 78.01688861846924, 79.07945489883423, 80.13892459869385, 81.19839429855347, 82.26174449920654, 83.32509469985962, 84.38526487350464, 85.44543504714966, 86.50335550308228, 87.56127595901489, 88.6209831237793, 89.6806902885437, 90.74411821365356, 91.80754613876343, 92.87180256843567, 93.93605899810791, 94.99521470069885, 96.0543704032898, 97.1292839050293, 98.2041974067688, 99.26539158821106, 100.32658576965332, 101.38967680931091, 102.4527678489685, 103.51044964790344, 104.56813144683838, 105.63054990768433, 106.69296836853027, 107.75058650970459, 108.8082046508789, 109.868004322052, 110.9278039932251, 111.99393582344055, 113.060067653656, 114.12198162078857, 115.18389558792114, 116.24331831932068, 117.30274105072021, 118.36271262168884, 119.42268419265747, 120.48785996437073, 121.55303573608398, 122.60402488708496, 123.65501403808594, 124.70519542694092, 125.7553768157959, 126.80437850952148, 127.85338020324707, 128.9113209247589, 129.96926164627075, 131.02591562271118, 132.0825695991516, 133.13336396217346, 134.1841583251953, 135.24178218841553, 136.29940605163574, 137.34835028648376, 138.3972945213318, 139.4451322555542, 140.4929699897766, 141.5440080165863, 142.595046043396, 143.6488835811615, 144.702721118927, 145.7581057548523, 146.8134903907776, 147.86011004447937, 148.90672969818115, 149.96022391319275, 151.01371812820435, 152.06668400764465, 153.11964988708496, 154.1717939376831, 155.22393798828125, 156.2778136730194, 157.33168935775757, 158.38259840011597, 159.43350744247437, 160.48588705062866, 161.53826665878296, 162.5922338962555, 163.64620113372803, 164.70410823822021, 165.7620153427124, 166.81864619255066, 167.87527704238892, 168.9262936115265, 169.97731018066406, 171.03015232086182, 172.08299446105957, 173.1354260444641, 174.18785762786865, 175.23703980445862, 176.28622198104858, 177.336181640625, 178.38614130020142, 179.43764233589172, 180.48914337158203, 181.53672456741333, 182.58430576324463, 183.63790082931519, 184.69149589538574, 185.74198698997498, 186.7924780845642, 187.85221314430237, 188.91194820404053, 189.96402430534363, 191.01610040664673, 192.06708645820618, 193.11807250976562, 194.16827249526978, 195.21847248077393, 196.2655532360077, 197.31263399124146, 198.3615744113922, 199.41051483154297, 200.4640703201294, 201.51762580871582, 202.5662932395935, 203.6149606704712, 204.66275453567505, 205.7105484008789, 206.7594575881958, 207.8083667755127, 208.85801553726196, 209.90766429901123, 210.9621605873108, 212.01665687561035, 213.89674019813538, 215.7768235206604]
[19.1575, 19.1575, 26.7325, 26.7325, 29.745, 29.745, 31.8025, 31.8025, 34.21, 34.21, 36.38, 36.38, 37.6875, 37.6875, 39.3175, 39.3175, 41.32, 41.32, 42.49, 42.49, 43.1475, 43.1475, 44.2525, 44.2525, 45.7, 45.7, 46.035, 46.035, 45.9375, 45.9375, 47.7475, 47.7475, 48.025, 48.025, 49.005, 49.005, 49.6425, 49.6425, 49.01, 49.01, 49.435, 49.435, 49.05, 49.05, 51.6875, 51.6875, 52.02, 52.02, 51.7525, 51.7525, 52.8925, 52.8925, 54.0425, 54.0425, 54.4025, 54.4025, 54.0975, 54.0975, 54.785, 54.785, 55.0775, 55.0775, 55.71, 55.71, 56.2175, 56.2175, 57.19, 57.19, 56.74, 56.74, 57.1575, 57.1575, 57.6675, 57.6675, 57.075, 57.075, 57.515, 57.515, 57.74, 57.74, 58.2625, 58.2625, 57.54, 57.54, 58.57, 58.57, 59.1525, 59.1525, 59.11, 59.11, 59.3825, 59.3825, 59.5225, 59.5225, 59.5475, 59.5475, 60.415, 60.415, 60.6325, 60.6325, 60.745, 60.745, 60.24, 60.24, 60.06, 60.06, 59.905, 59.905, 59.2525, 59.2525, 60.0175, 60.0175, 60.5825, 60.5825, 60.7225, 60.7225, 60.4525, 60.4525, 60.48, 60.48, 60.945, 60.945, 61.285, 61.285, 61.81, 61.81, 61.88, 61.88, 61.6975, 61.6975, 61.9025, 61.9025, 61.81, 61.81, 61.5875, 61.5875, 61.74, 61.74, 61.6975, 61.6975, 61.74, 61.74, 61.75, 61.75, 61.6975, 61.6975, 61.5925, 61.5925, 62.165, 62.165, 62.1275, 62.1275, 62.215, 62.215, 61.58, 61.58, 61.5875, 61.5875, 62.685, 62.685, 62.5275, 62.5275, 62.7375, 62.7375, 63.0275, 63.0275, 62.23, 62.23, 62.3375, 62.3375, 62.6675, 62.6675, 62.7825, 62.7825, 63.22, 63.22, 62.9725, 62.9725, 63.115, 63.115, 62.7825, 62.7825, 62.38, 62.38, 62.6325, 62.6325, 62.86, 62.86, 62.1925, 62.1925, 62.0375, 62.0375, 62.905, 62.905, 62.905, 62.905, 63.1275, 63.1275, 62.2825, 62.2825, 62.73, 62.73]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.316, Test loss: 1.602, Test accuracy: 62.37
Average accuracy final 10 rounds: 62.126999999999995 

2823.4246776103973
[1.4143154621124268, 2.8286309242248535, 4.001286268234253, 5.173941612243652, 6.3503007888793945, 7.526659965515137, 8.703540086746216, 9.880420207977295, 11.062209367752075, 12.243998527526855, 13.423400402069092, 14.602802276611328, 15.777043104171753, 16.951283931732178, 18.12452220916748, 19.297760486602783, 20.475849866867065, 21.653939247131348, 22.830906629562378, 24.007874011993408, 25.18433928489685, 26.360804557800293, 27.541914701461792, 28.72302484512329, 29.899080276489258, 31.075135707855225, 32.25536322593689, 33.435590744018555, 34.61309814453125, 35.790605545043945, 36.96273446083069, 38.13486337661743, 39.31382632255554, 40.49278926849365, 41.67472863197327, 42.85666799545288, 44.031835079193115, 45.20700216293335, 46.38089156150818, 47.55478096008301, 48.731836557388306, 49.9088921546936, 51.08522653579712, 52.261560916900635, 53.43757343292236, 54.61358594894409, 55.78891706466675, 56.964248180389404, 58.14421343803406, 59.32417869567871, 60.504945516586304, 61.6857123374939, 62.86074948310852, 64.03578662872314, 65.21214747428894, 66.38850831985474, 67.56458330154419, 68.74065828323364, 69.91948342323303, 71.09830856323242, 72.27603960037231, 73.4537706375122, 74.62277030944824, 75.79176998138428, 77.06020092964172, 78.32863187789917, 79.57774353027344, 80.8268551826477, 82.08030009269714, 83.33374500274658, 84.58441686630249, 85.8350887298584, 87.08679127693176, 88.33849382400513, 89.58656287193298, 90.83463191986084, 92.08016538619995, 93.32569885253906, 94.57190442085266, 95.81810998916626, 97.06461000442505, 98.31111001968384, 99.55388140678406, 100.79665279388428, 102.05431580543518, 103.31197881698608, 104.55923581123352, 105.80649280548096, 107.04994082450867, 108.29338884353638, 109.54649424552917, 110.79959964752197, 112.04575943946838, 113.2919192314148, 114.5397961139679, 115.787672996521, 117.03722357749939, 118.28677415847778, 119.53475546836853, 120.78273677825928, 122.02977228164673, 123.27680778503418, 124.52399277687073, 125.77117776870728, 127.0166552066803, 128.26213264465332, 129.50637531280518, 130.75061798095703, 131.9942009449005, 133.237783908844, 134.4833779335022, 135.7289719581604, 136.97407126426697, 138.21917057037354, 139.47222185134888, 140.72527313232422, 141.89882493019104, 143.07237672805786, 144.3235728740692, 145.57476902008057, 146.75703811645508, 147.9393072128296, 149.13104009628296, 150.32277297973633, 151.5118386745453, 152.70090436935425, 153.89881443977356, 155.09672451019287, 156.29376769065857, 157.49081087112427, 158.68216252326965, 159.87351417541504, 161.0632026195526, 162.25289106369019, 163.45067071914673, 164.64845037460327, 165.88118147850037, 167.11391258239746, 168.37229871749878, 169.6306848526001, 170.88717126846313, 172.14365768432617, 173.40349102020264, 174.6633243560791, 175.92462372779846, 177.18592309951782, 178.43984746932983, 179.69377183914185, 180.95191621780396, 182.21006059646606, 183.47187066078186, 184.73368072509766, 185.99800944328308, 187.2623381614685, 188.52215027809143, 189.78196239471436, 191.03783345222473, 192.2937045097351, 193.5569567680359, 194.82020902633667, 196.08067655563354, 197.34114408493042, 198.59805035591125, 199.8549566268921, 201.11336278915405, 202.37176895141602, 203.6320309638977, 204.8922929763794, 206.1472852230072, 207.402277469635, 208.65244817733765, 209.90261888504028, 211.162579536438, 212.4225401878357, 213.67898869514465, 214.9354372024536, 216.18552374839783, 217.43561029434204, 218.68453907966614, 219.93346786499023, 221.1962640285492, 222.45906019210815, 223.7127296924591, 224.96639919281006, 226.21640706062317, 227.46641492843628, 228.72470331192017, 229.98299169540405, 231.2418394088745, 232.50068712234497, 233.75590133666992, 235.01111555099487, 236.25588631629944, 237.500657081604, 238.751690864563, 240.00272464752197, 241.2538673877716, 242.50501012802124, 243.7548394203186, 245.00466871261597, 246.95563769340515, 248.90660667419434]
[28.4625, 28.4625, 32.7475, 32.7475, 36.38, 36.38, 40.08, 40.08, 41.4375, 41.4375, 42.49, 42.49, 44.9075, 44.9075, 46.5025, 46.5025, 47.97, 47.97, 48.8475, 48.8475, 49.5375, 49.5375, 50.4475, 50.4475, 51.4825, 51.4825, 52.255, 52.255, 52.4325, 52.4325, 53.8925, 53.8925, 54.825, 54.825, 56.345, 56.345, 56.2525, 56.2525, 56.3575, 56.3575, 57.2975, 57.2975, 57.74, 57.74, 58.2725, 58.2725, 58.0825, 58.0825, 58.085, 58.085, 58.915, 58.915, 58.885, 58.885, 59.345, 59.345, 59.4925, 59.4925, 59.315, 59.315, 59.7025, 59.7025, 60.2975, 60.2975, 60.2375, 60.2375, 60.2525, 60.2525, 60.375, 60.375, 59.9575, 59.9575, 60.76, 60.76, 60.5975, 60.5975, 60.95, 60.95, 61.1675, 61.1675, 60.74, 60.74, 61.02, 61.02, 60.5, 60.5, 60.7225, 60.7225, 60.275, 60.275, 60.9325, 60.9325, 61.09, 61.09, 61.3275, 61.3275, 61.4725, 61.4725, 61.5525, 61.5525, 61.69, 61.69, 61.8875, 61.8875, 61.3325, 61.3325, 61.5125, 61.5125, 61.86, 61.86, 61.7625, 61.7625, 61.345, 61.345, 61.3175, 61.3175, 61.9125, 61.9125, 61.8475, 61.8475, 61.515, 61.515, 62.0075, 62.0075, 61.8075, 61.8075, 61.74, 61.74, 61.745, 61.745, 61.5875, 61.5875, 61.9675, 61.9675, 61.6025, 61.6025, 61.5525, 61.5525, 61.6475, 61.6475, 62.235, 62.235, 61.7525, 61.7525, 61.7075, 61.7075, 62.1025, 62.1025, 61.9975, 61.9975, 61.72, 61.72, 62.12, 62.12, 62.125, 62.125, 61.55, 61.55, 61.58, 61.58, 61.995, 61.995, 62.1325, 62.1325, 61.915, 61.915, 62.2425, 62.2425, 62.485, 62.485, 62.08, 62.08, 61.95, 61.95, 62.04, 62.04, 61.8575, 61.8575, 61.7975, 61.7975, 62.325, 62.325, 61.9975, 61.9975, 62.175, 62.175, 61.8, 61.8, 62.095, 62.095, 62.2775, 62.2775, 62.405, 62.405, 62.09, 62.09, 61.8725, 61.8725, 62.2325, 62.2325, 62.3675, 62.3675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.090, Test loss: 4.189, Test accuracy: 43.38
Average accuracy final 10 rounds: 42.5405 

2767.777838230133
[1.4346685409545898, 2.8693370819091797, 4.107725381851196, 5.346113681793213, 6.590903997421265, 7.835694313049316, 9.077031373977661, 10.318368434906006, 11.565408706665039, 12.812448978424072, 14.05154037475586, 15.290631771087646, 16.535271406173706, 17.779911041259766, 19.021618366241455, 20.263325691223145, 21.499722003936768, 22.73611831665039, 23.982095956802368, 25.228073596954346, 26.474037170410156, 27.720000743865967, 28.962461233139038, 30.20492172241211, 31.444690227508545, 32.68445873260498, 33.9249541759491, 35.16544961929321, 36.401052474975586, 37.63665533065796, 38.88121223449707, 40.12576913833618, 41.36486291885376, 42.60395669937134, 43.84494924545288, 45.085941791534424, 46.3237943649292, 47.561646938323975, 48.791666746139526, 50.02168655395508, 51.25645875930786, 52.491230964660645, 53.73325037956238, 54.97526979446411, 56.22034978866577, 57.46542978286743, 58.70398163795471, 59.94253349304199, 61.17298746109009, 62.403441429138184, 63.645771503448486, 64.88810157775879, 66.14643287658691, 67.40476417541504, 68.6538417339325, 69.90291929244995, 71.14017152786255, 72.37742376327515, 73.62148547172546, 74.86554718017578, 76.1097846031189, 77.35402202606201, 78.59321165084839, 79.83240127563477, 81.07475519180298, 82.31710910797119, 83.55900859832764, 84.80090808868408, 86.04276156425476, 87.28461503982544, 88.5245954990387, 89.76457595825195, 91.00236773490906, 92.24015951156616, 93.48094487190247, 94.72173023223877, 95.96640992164612, 97.21108961105347, 98.46005177497864, 99.70901393890381, 100.95599126815796, 102.20296859741211, 103.44479513168335, 104.68662166595459, 105.93131589889526, 107.17601013183594, 108.41895389556885, 109.66189765930176, 110.91163873672485, 112.16137981414795, 113.40508270263672, 114.64878559112549, 115.89477229118347, 117.14075899124146, 118.37935018539429, 119.61794137954712, 120.8638665676117, 122.10979175567627, 123.3511791229248, 124.59256649017334, 125.84645056724548, 127.10033464431763, 128.34979844093323, 129.59926223754883, 130.85206508636475, 132.10486793518066, 133.35526585578918, 134.6056637763977, 135.85468935966492, 137.10371494293213, 138.35049653053284, 139.59727811813354, 140.844575881958, 142.09187364578247, 143.33857941627502, 144.58528518676758, 145.83675384521484, 147.0882225036621, 148.33570313453674, 149.58318376541138, 150.82891726493835, 152.07465076446533, 153.32098054885864, 154.56731033325195, 155.81622338294983, 157.0651364326477, 158.3042697906494, 159.54340314865112, 160.7908525466919, 162.03830194473267, 163.28337955474854, 164.5284571647644, 165.78037691116333, 167.03229665756226, 168.27834272384644, 169.52438879013062, 170.7696716785431, 172.01495456695557, 173.12461757659912, 174.23428058624268, 175.4336895942688, 176.63309860229492, 177.8788924217224, 179.1246862411499, 180.37400650978088, 181.62332677841187, 182.8703339099884, 184.11734104156494, 185.36348056793213, 186.60962009429932, 187.85676860809326, 189.1039171218872, 190.35056829452515, 191.5972194671631, 192.84418106079102, 194.09114265441895, 195.3392276763916, 196.58731269836426, 197.83282256126404, 199.07833242416382, 200.32539796829224, 201.57246351242065, 202.81749033927917, 204.0625171661377, 205.3075830936432, 206.55264902114868, 207.80268120765686, 209.05271339416504, 210.30045914649963, 211.54820489883423, 212.79757142066956, 214.04693794250488, 215.29382228851318, 216.54070663452148, 217.7884383201599, 219.03617000579834, 220.2799654006958, 221.52376079559326, 222.76551270484924, 224.00726461410522, 225.2478358745575, 226.48840713500977, 227.73503708839417, 228.98166704177856, 230.2283058166504, 231.47494459152222, 232.71962523460388, 233.96430587768555, 235.21234226226807, 236.4603786468506, 237.70292329788208, 238.94546794891357, 240.19612169265747, 241.44677543640137, 242.69366192817688, 243.9405484199524, 245.19356775283813, 246.44658708572388, 247.69303464889526, 248.93948221206665, 251.25898146629333, 253.57848072052002]
[24.635, 24.635, 29.4725, 29.4725, 31.7525, 31.7525, 33.85, 33.85, 35.49, 35.49, 36.63, 36.63, 37.4075, 37.4075, 38.24, 38.24, 38.12, 38.12, 38.89, 38.89, 39.6125, 39.6125, 39.855, 39.855, 40.6675, 40.6675, 40.3075, 40.3075, 40.8725, 40.8725, 40.4575, 40.4575, 40.475, 40.475, 40.6175, 40.6175, 40.7025, 40.7025, 40.91, 40.91, 41.5775, 41.5775, 41.3825, 41.3825, 41.335, 41.335, 41.67, 41.67, 41.5475, 41.5475, 41.35, 41.35, 41.625, 41.625, 41.47, 41.47, 41.685, 41.685, 41.83, 41.83, 41.8825, 41.8825, 42.09, 42.09, 42.0825, 42.0825, 41.805, 41.805, 41.805, 41.805, 41.7875, 41.7875, 41.7775, 41.7775, 42.26, 42.26, 42.1125, 42.1125, 42.0725, 42.0725, 42.29, 42.29, 42.275, 42.275, 42.0925, 42.0925, 42.1875, 42.1875, 42.5575, 42.5575, 42.255, 42.255, 42.2425, 42.2425, 42.1475, 42.1475, 42.2375, 42.2375, 42.24, 42.24, 42.525, 42.525, 42.6325, 42.6325, 42.475, 42.475, 42.4925, 42.4925, 42.7075, 42.7075, 42.33, 42.33, 42.5675, 42.5675, 42.5175, 42.5175, 42.5375, 42.5375, 42.4825, 42.4825, 42.0275, 42.0275, 42.3875, 42.3875, 42.1225, 42.1225, 42.0775, 42.0775, 42.41, 42.41, 42.39, 42.39, 42.3725, 42.3725, 42.4825, 42.4825, 42.2175, 42.2175, 41.79, 41.79, 42.0425, 42.0425, 42.3475, 42.3475, 42.3275, 42.3275, 42.57, 42.57, 42.5825, 42.5825, 42.625, 42.625, 42.75, 42.75, 42.3475, 42.3475, 42.095, 42.095, 41.7125, 41.7125, 42.0825, 42.0825, 42.0125, 42.0125, 42.455, 42.455, 42.6325, 42.6325, 42.6525, 42.6525, 42.7475, 42.7475, 42.3675, 42.3675, 42.6525, 42.6525, 42.6025, 42.6025, 42.655, 42.655, 42.235, 42.235, 42.3425, 42.3425, 42.55, 42.55, 42.6875, 42.6875, 42.59, 42.59, 42.5575, 42.5575, 42.7225, 42.7225, 42.6425, 42.6425, 42.4825, 42.4825, 42.595, 42.595, 43.38, 43.38]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 1.449, Test loss: 1.893, Test accuracy: 31.21
Round   1, Train loss: 1.255, Test loss: 1.799, Test accuracy: 34.93
Round   2, Train loss: 1.175, Test loss: 1.890, Test accuracy: 31.46
Round   3, Train loss: 1.095, Test loss: 1.889, Test accuracy: 32.85
Round   4, Train loss: 1.031, Test loss: 1.964, Test accuracy: 31.07
Round   5, Train loss: 0.967, Test loss: 1.994, Test accuracy: 30.06
Round   6, Train loss: 0.943, Test loss: 2.089, Test accuracy: 27.02
Round   7, Train loss: 0.874, Test loss: 2.071, Test accuracy: 27.45
Round   8, Train loss: 0.851, Test loss: 2.118, Test accuracy: 25.75
Round   9, Train loss: 0.834, Test loss: 2.114, Test accuracy: 25.68
Round  10, Train loss: 0.770, Test loss: 2.109, Test accuracy: 25.79
Round  11, Train loss: 0.734, Test loss: 2.094, Test accuracy: 25.91
Round  12, Train loss: 0.733, Test loss: 2.088, Test accuracy: 26.01
Round  13, Train loss: 0.669, Test loss: 2.081, Test accuracy: 26.66
Round  14, Train loss: 0.688, Test loss: 2.074, Test accuracy: 26.45
Round  15, Train loss: 0.610, Test loss: 2.064, Test accuracy: 27.07
Round  16, Train loss: 0.630, Test loss: 2.054, Test accuracy: 27.91
Round  17, Train loss: 0.574, Test loss: 2.047, Test accuracy: 27.80
Round  18, Train loss: 0.560, Test loss: 2.040, Test accuracy: 27.95
Round  19, Train loss: 0.547, Test loss: 2.031, Test accuracy: 28.45
Round  20, Train loss: 0.526, Test loss: 2.025, Test accuracy: 28.07
Round  21, Train loss: 0.512, Test loss: 2.018, Test accuracy: 29.32
Round  22, Train loss: 0.488, Test loss: 2.014, Test accuracy: 29.25
Round  23, Train loss: 0.483, Test loss: 2.007, Test accuracy: 30.35
Round  24, Train loss: 0.471, Test loss: 2.005, Test accuracy: 30.39
Round  25, Train loss: 0.411, Test loss: 1.987, Test accuracy: 31.12
Round  26, Train loss: 0.460, Test loss: 1.992, Test accuracy: 30.46
Round  27, Train loss: 0.459, Test loss: 1.986, Test accuracy: 30.80
Round  28, Train loss: 0.413, Test loss: 1.977, Test accuracy: 31.48
Round  29, Train loss: 0.370, Test loss: 1.969, Test accuracy: 31.86
Round  30, Train loss: 0.410, Test loss: 1.956, Test accuracy: 32.99
Round  31, Train loss: 0.372, Test loss: 1.943, Test accuracy: 33.81
Round  32, Train loss: 0.361, Test loss: 1.936, Test accuracy: 34.14
Round  33, Train loss: 0.356, Test loss: 1.935, Test accuracy: 33.56
Round  34, Train loss: 0.341, Test loss: 1.920, Test accuracy: 34.80
Round  35, Train loss: 0.317, Test loss: 1.919, Test accuracy: 34.38
Round  36, Train loss: 0.311, Test loss: 1.923, Test accuracy: 34.08
Round  37, Train loss: 0.301, Test loss: 1.916, Test accuracy: 34.62
Round  38, Train loss: 0.304, Test loss: 1.910, Test accuracy: 34.87
Round  39, Train loss: 0.284, Test loss: 1.907, Test accuracy: 34.92
Round  40, Train loss: 0.288, Test loss: 1.905, Test accuracy: 35.63
Round  41, Train loss: 0.314, Test loss: 1.898, Test accuracy: 36.50
Round  42, Train loss: 0.258, Test loss: 1.890, Test accuracy: 36.06
Round  43, Train loss: 0.276, Test loss: 1.881, Test accuracy: 36.75
Round  44, Train loss: 0.254, Test loss: 1.866, Test accuracy: 37.90
Round  45, Train loss: 0.243, Test loss: 1.860, Test accuracy: 37.96
Round  46, Train loss: 0.226, Test loss: 1.846, Test accuracy: 38.95
Round  47, Train loss: 0.293, Test loss: 1.853, Test accuracy: 38.18
Round  48, Train loss: 0.252, Test loss: 1.844, Test accuracy: 38.87
Round  49, Train loss: 0.227, Test loss: 1.842, Test accuracy: 38.44
Round  50, Train loss: 0.238, Test loss: 1.839, Test accuracy: 38.72
Round  51, Train loss: 0.213, Test loss: 1.835, Test accuracy: 39.14
Round  52, Train loss: 0.212, Test loss: 1.830, Test accuracy: 38.52
Round  53, Train loss: 0.225, Test loss: 1.829, Test accuracy: 39.11
Round  54, Train loss: 0.206, Test loss: 1.827, Test accuracy: 39.00
Round  55, Train loss: 0.239, Test loss: 1.817, Test accuracy: 39.80
Round  56, Train loss: 0.210, Test loss: 1.806, Test accuracy: 40.06
Round  57, Train loss: 0.217, Test loss: 1.807, Test accuracy: 39.33
Round  58, Train loss: 0.198, Test loss: 1.803, Test accuracy: 39.74
Round  59, Train loss: 0.185, Test loss: 1.794, Test accuracy: 40.14
Round  60, Train loss: 0.215, Test loss: 1.791, Test accuracy: 40.42
Round  61, Train loss: 0.186, Test loss: 1.784, Test accuracy: 40.58
Round  62, Train loss: 0.198, Test loss: 1.773, Test accuracy: 40.96
Round  63, Train loss: 0.199, Test loss: 1.766, Test accuracy: 41.20
Round  64, Train loss: 0.183, Test loss: 1.763, Test accuracy: 41.57
Round  65, Train loss: 0.191, Test loss: 1.752, Test accuracy: 42.47
Round  66, Train loss: 0.166, Test loss: 1.757, Test accuracy: 41.23
Round  67, Train loss: 0.173, Test loss: 1.766, Test accuracy: 40.96
Round  68, Train loss: 0.167, Test loss: 1.759, Test accuracy: 41.18
Round  69, Train loss: 0.169, Test loss: 1.753, Test accuracy: 41.55
Round  70, Train loss: 0.176, Test loss: 1.753, Test accuracy: 41.52
Round  71, Train loss: 0.178, Test loss: 1.750, Test accuracy: 41.29
Round  72, Train loss: 0.155, Test loss: 1.745, Test accuracy: 41.25
Round  73, Train loss: 0.163, Test loss: 1.743, Test accuracy: 41.48
Round  74, Train loss: 0.151, Test loss: 1.738, Test accuracy: 42.20
Round  75, Train loss: 0.164, Test loss: 1.732, Test accuracy: 42.76
Round  76, Train loss: 0.160, Test loss: 1.736, Test accuracy: 42.28
Round  77, Train loss: 0.156, Test loss: 1.735, Test accuracy: 42.16
Round  78, Train loss: 0.153, Test loss: 1.726, Test accuracy: 42.18
Round  79, Train loss: 0.151, Test loss: 1.731, Test accuracy: 41.89
Round  80, Train loss: 0.162, Test loss: 1.730, Test accuracy: 41.78
Round  81, Train loss: 0.148, Test loss: 1.715, Test accuracy: 42.77
Round  82, Train loss: 0.141, Test loss: 1.710, Test accuracy: 43.10
Round  83, Train loss: 0.150, Test loss: 1.707, Test accuracy: 43.31
Round  84, Train loss: 0.163, Test loss: 1.722, Test accuracy: 42.33
Round  85, Train loss: 0.138, Test loss: 1.706, Test accuracy: 43.13
Round  86, Train loss: 0.156, Test loss: 1.709, Test accuracy: 43.23
Round  87, Train loss: 0.132, Test loss: 1.697, Test accuracy: 43.28
Round  88, Train loss: 0.140, Test loss: 1.691, Test accuracy: 43.72
Round  89, Train loss: 0.139, Test loss: 1.692, Test accuracy: 43.41
Round  90, Train loss: 0.134, Test loss: 1.705, Test accuracy: 42.79
Round  91, Train loss: 0.127, Test loss: 1.688, Test accuracy: 43.30
Round  92, Train loss: 0.144, Test loss: 1.694, Test accuracy: 43.06
Round  93, Train loss: 0.130, Test loss: 1.690, Test accuracy: 43.35
Round  94, Train loss: 0.134, Test loss: 1.682, Test accuracy: 43.45
Round  95, Train loss: 0.128, Test loss: 1.677, Test accuracy: 43.75
Round  96, Train loss: 0.134, Test loss: 1.674, Test accuracy: 44.30
Round  97, Train loss: 0.147, Test loss: 1.688, Test accuracy: 43.88
Round  98, Train loss: 0.129, Test loss: 1.679, Test accuracy: 43.76
Round  99, Train loss: 0.129, Test loss: 1.678, Test accuracy: 43.97
Final Round, Train loss: 0.133, Test loss: 1.676, Test accuracy: 44.11
Average accuracy final 10 rounds: 43.56075
5957.476557970047
[]
[31.2125, 34.9325, 31.4625, 32.8475, 31.07, 30.0575, 27.02, 27.4475, 25.75, 25.675, 25.785, 25.9125, 26.01, 26.6575, 26.445, 27.07, 27.9125, 27.8025, 27.9475, 28.4525, 28.075, 29.32, 29.255, 30.3525, 30.3875, 31.1175, 30.465, 30.795, 31.485, 31.86, 32.9925, 33.81, 34.1375, 33.56, 34.795, 34.375, 34.08, 34.615, 34.865, 34.92, 35.6275, 36.5025, 36.0575, 36.7525, 37.895, 37.9575, 38.9475, 38.1775, 38.87, 38.44, 38.715, 39.1375, 38.5225, 39.1075, 39.0025, 39.7975, 40.06, 39.3325, 39.74, 40.1425, 40.4175, 40.5825, 40.9575, 41.1975, 41.57, 42.47, 41.2275, 40.9625, 41.1825, 41.5525, 41.52, 41.29, 41.25, 41.4775, 42.195, 42.76, 42.28, 42.165, 42.1775, 41.8875, 41.785, 42.7725, 43.105, 43.31, 42.3275, 43.13, 43.23, 43.2825, 43.715, 43.405, 42.79, 43.295, 43.065, 43.35, 43.445, 43.7525, 44.305, 43.8825, 43.7575, 43.965, 44.1075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 1.019, Test loss: 1.170, Test accuracy: 60.16
Average accuracy final 10 rounds: 55.96824999999999
Average global accuracy final 10 rounds: 55.96824999999999
4565.51553106308
[]
[18.285, 23.1175, 33.5675, 36.515, 36.1425, 37.42, 39.35, 41.845, 41.87, 43.2975, 43.285, 43.97, 43.1975, 44.7825, 45.7625, 45.5425, 45.95, 46.51, 47.665, 47.6775, 47.6675, 48.0725, 48.1925, 48.3225, 48.4, 49.2525, 49.62, 49.6725, 49.955, 50.1925, 50.3175, 50.075, 50.325, 50.8875, 50.9225, 51.0475, 50.985, 50.5875, 50.88, 50.9625, 51.03, 51.7425, 52.16, 52.1975, 52.6375, 52.625, 52.955, 52.845, 53.1525, 52.5725, 52.88, 53.0175, 52.8125, 52.88, 53.095, 52.9975, 53.5125, 54.0225, 54.0775, 53.84, 53.9875, 53.83, 53.19, 53.385, 53.9575, 54.3925, 54.245, 54.2425, 54.2625, 54.9475, 54.9325, 55.03, 54.8475, 54.49, 54.3, 54.6125, 54.7925, 55.14, 55.2, 55.62, 55.1225, 55.1025, 55.33, 56.2, 56.0325, 56.5375, 55.565, 55.76, 55.62, 55.85, 55.9425, 56.0825, 55.72, 55.9725, 55.765, 55.665, 55.7525, 56.01, 56.245, 56.5275, 60.16]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.252, Test loss: 2.237, Test accuracy: 20.07
Final Round, Global train loss: 2.252, Global test loss: 2.240, Global test accuracy: 20.60
Average accuracy final 10 rounds: 20.105749999999997 

Average global accuracy final 10 rounds: 20.14425 

2744.2276060581207
[1.5094592571258545, 2.8046834468841553, 3.9187333583831787, 5.033063888549805, 6.154741287231445, 7.273295164108276, 8.38644003868103, 9.503379583358765, 10.623429298400879, 11.741796016693115, 12.857769250869751, 13.973652362823486, 15.096805810928345, 16.213993310928345, 17.334542512893677, 18.44793176651001, 19.562950134277344, 20.683192491531372, 21.79681706428528, 22.911625862121582, 24.01717710494995, 25.124854803085327, 26.228692293167114, 27.34229063987732, 28.443597078323364, 29.5565025806427, 30.669560194015503, 31.774935722351074, 32.881091833114624, 33.99048089981079, 35.10447573661804, 36.2143280506134, 37.32832193374634, 38.44103717803955, 39.55797052383423, 40.67281365394592, 41.78705167770386, 42.89876866340637, 44.013726234436035, 45.12564182281494, 46.254234075546265, 47.37072443962097, 48.48449516296387, 49.60801148414612, 50.727113008499146, 51.84185171127319, 52.9529070854187, 54.07043981552124, 55.18486452102661, 56.30134916305542, 57.422152280807495, 58.534897565841675, 59.659337282180786, 60.77731919288635, 61.909876108169556, 63.022727966308594, 64.1414303779602, 65.2638373374939, 66.38578605651855, 67.49513077735901, 68.61407518386841, 69.72670674324036, 70.8427984714508, 71.95584321022034, 73.06963038444519, 74.19343852996826, 75.31565976142883, 76.43543863296509, 77.55478954315186, 78.67896676063538, 79.79454302787781, 80.91473007202148, 82.03417873382568, 83.14427089691162, 84.25892543792725, 85.3694965839386, 86.48335337638855, 87.60727071762085, 88.73245358467102, 89.84434413909912, 90.95602488517761, 92.0747447013855, 93.18926811218262, 94.31482768058777, 95.43401861190796, 96.55414175987244, 97.66630125045776, 98.7779004573822, 99.89959263801575, 101.01149559020996, 102.12791109085083, 103.2413718700409, 104.34600257873535, 105.44969201087952, 106.55386209487915, 107.6698739528656, 108.7833993434906, 109.88604593276978, 110.99966526031494, 112.10174441337585, 114.31964802742004]
[9.945, 9.9125, 9.93, 9.9375, 9.94, 9.93, 9.9225, 9.915, 10.0025, 10.065, 10.0925, 10.125, 10.185, 10.2475, 10.4725, 10.8375, 11.5275, 11.75, 12.1425, 12.38, 12.6625, 13.2775, 13.7625, 13.855, 14.0675, 14.3425, 14.545, 14.815, 14.775, 14.71, 14.9675, 15.105, 15.235, 15.3425, 15.4975, 15.8075, 16.0975, 16.1525, 16.2425, 16.39, 16.5075, 16.5875, 16.63, 16.69, 16.735, 16.7625, 16.86, 16.9425, 17.1275, 17.155, 17.2325, 17.2125, 17.39, 17.5575, 17.6775, 17.8075, 17.7775, 17.865, 18.2775, 18.5975, 18.565, 18.8325, 18.815, 18.84, 19.1225, 19.22, 19.2625, 19.1175, 19.1375, 19.0525, 18.8075, 18.945, 18.995, 18.885, 19.15, 19.2675, 19.465, 19.4225, 19.43, 19.6075, 19.7025, 19.6575, 19.7625, 19.8125, 19.6825, 19.885, 20.005, 19.9025, 19.9475, 19.9075, 19.99, 19.89, 19.9775, 19.9775, 20.215, 20.17, 20.1625, 20.2275, 20.2775, 20.17, 20.0725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.347, Test loss: 1.054, Test accuracy: 70.08
Average accuracy final 10 rounds: 69.03225
2978.4107291698456
[3.1782991886138916, 6.153939485549927, 9.13397765159607, 12.127253770828247, 15.102937936782837, 18.096007585525513, 21.091163635253906, 24.073572397232056, 27.05077886581421, 30.03474259376526, 33.041316509246826, 36.0389289855957, 39.00965142250061, 42.01278114318848, 44.999024391174316, 47.964383363723755, 51.17153882980347, 54.35471534729004, 57.54787349700928, 60.7452278137207, 63.94228458404541, 67.12812733650208, 70.31842088699341, 73.49959111213684, 76.67741060256958, 79.85217380523682, 83.04455161094666, 86.24950861930847, 89.43630766868591, 92.61610913276672, 95.78570556640625, 98.97031283378601, 102.16190648078918, 105.3503348827362, 108.5273973941803, 111.70313143730164, 114.88407683372498, 118.06605219841003, 121.24484610557556, 124.42313361167908, 127.6117856502533, 130.8020317554474, 133.9955325126648, 137.1914553642273, 140.37115383148193, 143.56216025352478, 146.74295949935913, 149.93579387664795, 153.12399554252625, 156.30310153961182, 159.36413478851318, 162.4660382270813, 165.5690586566925, 168.64887166023254, 171.64078736305237, 174.79734015464783, 177.98453831672668, 181.17978525161743, 184.3718135356903, 187.5593729019165, 190.76161932945251, 193.95089960098267, 197.14153409004211, 200.32660722732544, 203.52029967308044, 206.70633816719055, 209.90296125411987, 213.0724778175354, 216.25780320167542, 219.45661735534668, 222.64067435264587, 225.8283019065857, 229.03745937347412, 232.22406482696533, 235.4090554714203, 238.59352350234985, 241.77762866020203, 244.9671666622162, 248.12721490859985, 251.26863050460815, 254.45543885231018, 257.6110725402832, 260.8068869113922, 263.9746060371399, 267.13627409935, 270.3321816921234, 273.5378966331482, 276.74698853492737, 279.9318814277649, 283.08814096450806, 286.27462220191956, 289.45476770401, 292.66238236427307, 295.8420760631561, 299.0249660015106, 302.2431709766388, 305.44129943847656, 308.65041160583496, 311.8455889225006, 315.0411078929901, 318.2398784160614]
[26.1875, 31.9575, 36.625, 39.9375, 42.6325, 45.5475, 46.01, 48.0625, 49.5925, 50.6675, 52.265, 53.0525, 54.535, 55.2775, 55.295, 55.385, 56.8575, 57.515, 57.695, 58.655, 59.4325, 59.6625, 60.4725, 60.6725, 60.99, 60.9625, 61.81, 61.0375, 62.2875, 62.71, 62.415, 63.69, 63.88, 63.895, 63.84, 64.275, 64.3525, 65.015, 65.56, 65.33, 65.36, 65.5775, 65.925, 65.6475, 65.63, 66.13, 65.98, 66.365, 66.44, 66.73, 66.48, 67.0475, 66.66, 67.495, 67.4525, 66.51, 67.44, 67.7775, 67.5125, 67.5975, 67.445, 67.88, 68.0675, 67.99, 68.4825, 67.9375, 67.675, 67.68, 67.6925, 68.0325, 67.51, 67.775, 68.4825, 68.595, 68.32, 68.55, 68.7625, 69.2925, 69.13, 68.8275, 68.285, 68.97, 68.4975, 68.9675, 68.425, 68.7775, 68.5725, 69.08, 69.28, 68.79, 69.0, 69.365, 68.965, 68.9975, 68.98, 69.055, 69.3925, 69.175, 68.565, 68.8275, 70.085]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.544, Test loss: 0.975, Test accuracy: 67.33
Average accuracy final 10 rounds: 66.879
1798.969604253769
[1.6759135723114014, 3.03499174118042, 4.398715257644653, 5.7581305503845215, 7.12473201751709, 8.483329057693481, 9.847395896911621, 11.204219818115234, 12.615392684936523, 13.973091125488281, 15.361560344696045, 16.64089608192444, 17.860530614852905, 19.085317134857178, 20.325669050216675, 21.55538296699524, 22.801085948944092, 24.023321390151978, 25.245442390441895, 26.470438241958618, 27.691532611846924, 28.919049739837646, 30.144829034805298, 31.37591242790222, 32.602900981903076, 33.82881712913513, 35.05070614814758, 36.286484241485596, 37.53703260421753, 38.76457715034485, 39.98186278343201, 41.20395016670227, 42.437535762786865, 43.6604437828064, 44.892327547073364, 46.12145948410034, 47.35037446022034, 48.57615113258362, 49.801894426345825, 51.022433280944824, 52.25518465042114, 53.47510647773743, 54.704904317855835, 55.93667411804199, 57.16241812705994, 58.39206290245056, 59.60832691192627, 60.84118294715881, 62.0576536655426, 63.28515267372131, 64.50746464729309, 65.7237536907196, 66.94800090789795, 68.16279649734497, 69.37673902511597, 70.60266184806824, 71.82279443740845, 73.04022312164307, 74.27307271957397, 75.48876619338989, 76.71892380714417, 77.92907357215881, 79.14454460144043, 80.36198306083679, 81.57485389709473, 82.79755806922913, 84.01816368103027, 85.23811340332031, 86.45452356338501, 87.67961478233337, 88.90336918830872, 90.1292712688446, 91.34361672401428, 92.57354784011841, 93.78835892677307, 95.0111608505249, 96.22712564468384, 97.4369592666626, 98.66220450401306, 99.87740850448608, 101.09805989265442, 102.31563901901245, 103.54112195968628, 104.76227974891663, 105.98176670074463, 107.19379568099976, 108.40995121002197, 109.6246497631073, 110.83888983726501, 112.05890226364136, 113.27712488174438, 114.49679756164551, 115.71629309654236, 116.93993854522705, 118.15862107276917, 119.38435316085815, 120.60082197189331, 121.81750822067261, 123.03992438316345, 124.2569489479065, 126.23912334442139]
[21.335, 27.8975, 31.56, 34.5175, 37.3475, 39.475, 41.0625, 42.265, 43.0775, 44.1825, 45.0825, 45.2775, 46.6225, 46.9275, 47.5875, 48.4075, 49.04, 49.7675, 50.86, 51.7225, 51.945, 52.555, 53.505, 54.37, 54.4425, 55.1175, 55.6875, 55.0675, 56.7575, 56.8175, 56.62, 57.395, 57.2575, 57.3225, 57.6625, 57.8975, 58.765, 59.4725, 58.785, 58.9275, 59.5975, 59.7575, 60.98, 61.035, 61.0675, 61.1125, 61.8775, 62.4075, 61.88, 62.3475, 62.6275, 62.7775, 63.635, 63.465, 63.76, 63.6625, 63.4325, 64.415, 63.87, 64.1475, 63.97, 64.54, 64.4925, 64.945, 64.6225, 64.4275, 64.3525, 65.02, 65.18, 65.5975, 65.8825, 66.165, 66.065, 66.2, 65.9975, 66.025, 65.9975, 65.73, 66.465, 65.75, 66.1025, 66.4625, 66.44, 66.68, 66.5775, 66.7175, 66.92, 66.6, 67.1175, 66.81, 66.6425, 66.1425, 67.0925, 67.075, 66.92, 66.8425, 67.3525, 66.66, 67.0425, 67.02, 67.3275]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.553, Test loss: 0.979, Test accuracy: 67.43
Average accuracy final 10 rounds: 66.889
2912.0657184123993
[1.7845091819763184, 3.5690183639526367, 5.008523941040039, 6.448029518127441, 7.87844443321228, 9.30885934829712, 10.745048761367798, 12.181238174438477, 13.616379499435425, 15.051520824432373, 16.481515169143677, 17.91150951385498, 19.34483504295349, 20.778160572052002, 22.16747546195984, 23.556790351867676, 24.987043380737305, 26.417296409606934, 27.84633731842041, 29.275378227233887, 30.697261571884155, 32.119144916534424, 33.54241871833801, 34.9656925201416, 36.39944100379944, 37.833189487457275, 39.26385235786438, 40.694515228271484, 42.12830138206482, 43.562087535858154, 44.838831186294556, 46.11557483673096, 47.55594182014465, 48.99630880355835, 50.38849115371704, 51.78067350387573, 53.2016224861145, 54.62257146835327, 56.06152892112732, 57.50048637390137, 58.94696307182312, 60.39343976974487, 61.84424901008606, 63.295058250427246, 64.7348780632019, 66.17469787597656, 67.60599732398987, 69.03729677200317, 70.48868155479431, 71.94006633758545, 73.38411402702332, 74.82816171646118, 76.27636075019836, 77.72455978393555, 79.17185950279236, 80.61915922164917, 82.07371282577515, 83.52826642990112, 84.98010540008545, 86.43194437026978, 87.88260865211487, 89.33327293395996, 90.78605914115906, 92.23884534835815, 93.68565607070923, 95.1324667930603, 96.58449864387512, 98.03653049468994, 99.47952556610107, 100.9225206375122, 102.35407543182373, 103.78563022613525, 105.22179532051086, 106.65796041488647, 108.11156010627747, 109.56515979766846, 111.01041841506958, 112.4556770324707, 113.90544939041138, 115.35522174835205, 116.80093598365784, 118.24665021896362, 119.6909921169281, 121.13533401489258, 122.57663059234619, 124.0179271697998, 125.46087384223938, 126.90382051467896, 128.34521579742432, 129.78661108016968, 131.2184772491455, 132.65034341812134, 134.06896305084229, 135.48758268356323, 136.85310578346252, 138.21862888336182, 139.63691186904907, 141.05519485473633, 142.47693824768066, 143.898681640625, 145.30601692199707, 146.71335220336914, 148.17941904067993, 149.64548587799072, 151.02490258216858, 152.40431928634644, 153.7895495891571, 155.17477989196777, 156.55167984962463, 157.9285798072815, 159.1811077594757, 160.43363571166992, 161.67518639564514, 162.91673707962036, 164.15038013458252, 165.38402318954468, 166.6183111667633, 167.85259914398193, 169.0916817188263, 170.33076429367065, 171.56267619132996, 172.79458808898926, 174.02712178230286, 175.25965547561646, 176.4796531200409, 177.69965076446533, 178.93335485458374, 180.16705894470215, 181.39069080352783, 182.61432266235352, 183.98581838607788, 185.35731410980225, 186.5915994644165, 187.82588481903076, 189.04432153701782, 190.26275825500488, 191.48324275016785, 192.7037272453308, 193.93151140213013, 195.15929555892944, 196.3928771018982, 197.62645864486694, 198.85580015182495, 200.08514165878296, 201.31735801696777, 202.5495743751526, 203.7803499698639, 205.0111255645752, 206.23927640914917, 207.46742725372314, 208.70300769805908, 209.93858814239502, 211.17403388023376, 212.4094796180725, 213.8033275604248, 215.1971755027771, 216.5865659713745, 217.97595643997192, 219.34600830078125, 220.71606016159058, 222.09571242332458, 223.4753646850586, 224.8353145122528, 226.19526433944702, 227.5892996788025, 228.98333501815796, 230.36844897270203, 231.7535629272461, 233.14321374893188, 234.53286457061768, 235.91664624214172, 237.30042791366577, 238.69648027420044, 240.0925326347351, 241.47315788269043, 242.85378313064575, 244.24320006370544, 245.63261699676514, 247.0195038318634, 248.40639066696167, 249.64110231399536, 250.87581396102905, 252.1135289669037, 253.35124397277832, 254.58439826965332, 255.81755256652832, 257.05032086372375, 258.2830891609192, 259.5213027000427, 260.75951623916626, 261.98889780044556, 263.21827936172485, 264.4462971687317, 265.6743149757385, 266.90243577957153, 268.13055658340454, 269.497111082077, 270.8636655807495, 272.228640794754, 273.59361600875854, 275.6737220287323, 277.75382804870605]
[19.235, 19.235, 27.5125, 27.5125, 30.74, 30.74, 34.0225, 34.0225, 36.3925, 36.3925, 38.9175, 38.9175, 40.8975, 40.8975, 42.375, 42.375, 43.5575, 43.5575, 44.085, 44.085, 45.585, 45.585, 46.1275, 46.1275, 46.515, 46.515, 48.125, 48.125, 49.025, 49.025, 49.4875, 49.4875, 50.315, 50.315, 50.69, 50.69, 51.2075, 51.2075, 52.1, 52.1, 53.585, 53.585, 54.2925, 54.2925, 54.3725, 54.3725, 54.6575, 54.6575, 54.475, 54.475, 55.25, 55.25, 56.4425, 56.4425, 56.7975, 56.7975, 56.33, 56.33, 56.335, 56.335, 56.7175, 56.7175, 58.08, 58.08, 58.125, 58.125, 58.5275, 58.5275, 59.08, 59.08, 59.275, 59.275, 59.715, 59.715, 60.035, 60.035, 60.0875, 60.0875, 60.9875, 60.9875, 61.2725, 61.2725, 61.1875, 61.1875, 61.5475, 61.5475, 62.1225, 62.1225, 61.7775, 61.7775, 62.0025, 62.0025, 61.895, 61.895, 62.8025, 62.8025, 62.7875, 62.7875, 63.1675, 63.1675, 62.575, 62.575, 63.74, 63.74, 63.8925, 63.8925, 63.6275, 63.6275, 64.0525, 64.0525, 63.685, 63.685, 63.9325, 63.9325, 64.1575, 64.1575, 64.23, 64.23, 64.5725, 64.5725, 64.66, 64.66, 64.83, 64.83, 64.66, 64.66, 64.605, 64.605, 64.6225, 64.6225, 64.635, 64.635, 64.8475, 64.8475, 65.01, 65.01, 65.0475, 65.0475, 65.74, 65.74, 65.33, 65.33, 65.8025, 65.8025, 65.4775, 65.4775, 66.575, 66.575, 65.5225, 65.5225, 66.0025, 66.0025, 66.1675, 66.1675, 65.89, 65.89, 66.1025, 66.1025, 65.8475, 65.8475, 66.005, 66.005, 66.165, 66.165, 66.0975, 66.0975, 66.74, 66.74, 66.405, 66.405, 66.4075, 66.4075, 66.93, 66.93, 66.46, 66.46, 66.9175, 66.9175, 66.9075, 66.9075, 66.3, 66.3, 66.7025, 66.7025, 66.1525, 66.1525, 66.6525, 66.6525, 67.0925, 67.0925, 67.225, 67.225, 67.285, 67.285, 67.5125, 67.5125, 67.0675, 67.0675, 66.9, 66.9, 67.4275, 67.4275]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.066, Test loss: 1.248, Test accuracy: 76.97
Final Round, Global train loss: 0.066, Global test loss: 1.367, Global test accuracy: 38.09
Average accuracy final 10 rounds: 76.73500000000001 

Average global accuracy final 10 rounds: 38.219166666666666 

1653.2676424980164
[1.426077127456665, 2.85215425491333, 4.039663791656494, 5.227173328399658, 6.414833307266235, 7.6024932861328125, 8.786271572113037, 9.970049858093262, 11.154321432113647, 12.338593006134033, 13.525642395019531, 14.71269178390503, 15.897947788238525, 17.08320379257202, 18.266684770584106, 19.45016574859619, 20.645933866500854, 21.841701984405518, 23.03696894645691, 24.2322359085083, 25.428666830062866, 26.62509775161743, 27.819672107696533, 29.014246463775635, 30.20577120780945, 31.39729595184326, 32.590027809143066, 33.78275966644287, 34.966681480407715, 36.15060329437256, 37.33432078361511, 38.518038272857666, 39.69693613052368, 40.8758339881897, 42.06179642677307, 43.247758865356445, 44.364171743392944, 45.48058462142944, 46.66010808944702, 47.8396315574646, 49.031566858291626, 50.22350215911865, 51.41791033744812, 52.61231851577759, 53.80501937866211, 54.99772024154663, 56.18600559234619, 57.37429094314575, 58.573030948638916, 59.77177095413208, 60.96266412734985, 62.15355730056763, 63.266780853271484, 64.38000440597534, 65.56582355499268, 66.75164270401001, 67.86976909637451, 68.98789548873901, 70.10306692123413, 71.21823835372925, 72.33622241020203, 73.4542064666748, 74.57171988487244, 75.68923330307007, 76.82055330276489, 77.95187330245972, 79.12956237792969, 80.30725145339966, 81.43835425376892, 82.56945705413818, 83.7037718296051, 84.83808660507202, 85.96885871887207, 87.09963083267212, 88.29180479049683, 89.48397874832153, 90.67611646652222, 91.8682541847229, 93.0638644695282, 94.2594747543335, 95.45294952392578, 96.64642429351807, 97.83842873573303, 99.030433177948, 100.22339820861816, 101.41636323928833, 102.60846209526062, 103.80056095123291, 104.99234938621521, 106.18413782119751, 107.37578845024109, 108.56743907928467, 109.75842761993408, 110.9494161605835, 112.13960385322571, 113.32979154586792, 114.52281546592712, 115.71583938598633, 116.90905380249023, 118.10226821899414, 119.29702377319336, 120.49177932739258, 121.68983292579651, 122.88788652420044, 124.08975315093994, 125.29161977767944, 126.48614954948425, 127.68067932128906, 128.87541127204895, 130.07014322280884, 131.26015877723694, 132.45017433166504, 133.64562392234802, 134.841073513031, 136.04070591926575, 137.2403383255005, 138.436297416687, 139.63225650787354, 140.83013772964478, 142.02801895141602, 143.2213864326477, 144.4147539138794, 145.60616540908813, 146.79757690429688, 147.98939847946167, 149.18122005462646, 150.37293481826782, 151.56464958190918, 152.75192761421204, 153.9392056465149, 155.14895343780518, 156.35870122909546, 157.56613945960999, 158.7735776901245, 159.98525547981262, 161.19693326950073, 162.40091562271118, 163.60489797592163, 164.8071322441101, 166.00936651229858, 167.2173457145691, 168.4253249168396, 169.6208484172821, 170.8163719177246, 172.01618790626526, 173.2160038948059, 174.41091752052307, 175.60583114624023, 176.80527448654175, 178.00471782684326, 179.2043318748474, 180.40394592285156, 181.60511016845703, 182.8062744140625, 184.00067496299744, 185.19507551193237, 186.40732622146606, 187.61957693099976, 188.8110649585724, 190.00255298614502, 191.20064187049866, 192.3987307548523, 193.59669589996338, 194.79466104507446, 195.98746848106384, 197.18027591705322, 198.37542295455933, 199.57056999206543, 200.77096891403198, 201.97136783599854, 203.17477583885193, 204.37818384170532, 205.57194828987122, 206.7657127380371, 207.9546959400177, 209.1436791419983, 210.19872117042542, 211.25376319885254, 212.30276894569397, 213.3517746925354, 214.40273714065552, 215.45369958877563, 216.50822710990906, 217.56275463104248, 218.61955571174622, 219.67635679244995, 220.7373104095459, 221.79826402664185, 222.86056637763977, 223.9228687286377, 224.99667239189148, 226.07047605514526, 227.13110041618347, 228.19172477722168, 229.2528374195099, 230.3139500617981, 231.43892526626587, 232.56390047073364, 233.61986684799194, 234.67583322525024, 236.79355573654175, 238.91127824783325]
[43.1, 43.1, 51.61666666666667, 51.61666666666667, 56.1, 56.1, 63.05833333333333, 63.05833333333333, 65.38333333333334, 65.38333333333334, 65.13333333333334, 65.13333333333334, 68.6, 68.6, 69.575, 69.575, 70.63333333333334, 70.63333333333334, 70.84166666666667, 70.84166666666667, 71.75, 71.75, 72.40833333333333, 72.40833333333333, 72.35833333333333, 72.35833333333333, 72.39166666666667, 72.39166666666667, 73.05, 73.05, 73.54166666666667, 73.54166666666667, 73.89166666666667, 73.89166666666667, 73.5, 73.5, 74.09166666666667, 74.09166666666667, 74.26666666666667, 74.26666666666667, 74.63333333333334, 74.63333333333334, 74.61666666666666, 74.61666666666666, 74.99166666666666, 74.99166666666666, 74.95833333333333, 74.95833333333333, 74.68333333333334, 74.68333333333334, 75.20833333333333, 75.20833333333333, 75.425, 75.425, 75.20833333333333, 75.20833333333333, 74.775, 74.775, 74.75833333333334, 74.75833333333334, 75.08333333333333, 75.08333333333333, 74.775, 74.775, 74.91666666666667, 74.91666666666667, 74.95833333333333, 74.95833333333333, 75.275, 75.275, 74.975, 74.975, 75.225, 75.225, 74.98333333333333, 74.98333333333333, 75.28333333333333, 75.28333333333333, 75.48333333333333, 75.48333333333333, 75.1, 75.1, 74.94166666666666, 74.94166666666666, 74.99166666666666, 74.99166666666666, 75.48333333333333, 75.48333333333333, 75.38333333333334, 75.38333333333334, 75.38333333333334, 75.38333333333334, 76.26666666666667, 76.26666666666667, 76.13333333333334, 76.13333333333334, 76.05833333333334, 76.05833333333334, 76.19166666666666, 76.19166666666666, 76.10833333333333, 76.10833333333333, 76.19166666666666, 76.19166666666666, 76.09166666666667, 76.09166666666667, 76.10833333333333, 76.10833333333333, 76.55, 76.55, 76.01666666666667, 76.01666666666667, 75.90833333333333, 75.90833333333333, 76.025, 76.025, 76.40833333333333, 76.40833333333333, 76.19166666666666, 76.19166666666666, 75.76666666666667, 75.76666666666667, 75.93333333333334, 75.93333333333334, 76.08333333333333, 76.08333333333333, 75.89166666666667, 75.89166666666667, 75.675, 75.675, 75.68333333333334, 75.68333333333334, 75.725, 75.725, 75.525, 75.525, 75.71666666666667, 75.71666666666667, 75.93333333333334, 75.93333333333334, 75.95, 75.95, 76.425, 76.425, 76.4, 76.4, 76.56666666666666, 76.56666666666666, 76.68333333333334, 76.68333333333334, 76.83333333333333, 76.83333333333333, 76.65, 76.65, 76.44166666666666, 76.44166666666666, 76.025, 76.025, 76.39166666666667, 76.39166666666667, 75.95833333333333, 75.95833333333333, 75.93333333333334, 75.93333333333334, 76.075, 76.075, 75.90833333333333, 75.90833333333333, 75.78333333333333, 75.78333333333333, 75.625, 75.625, 75.70833333333333, 75.70833333333333, 76.075, 76.075, 76.63333333333334, 76.63333333333334, 76.9, 76.9, 76.89166666666667, 76.89166666666667, 77.09166666666667, 77.09166666666667, 76.73333333333333, 76.73333333333333, 76.4, 76.4, 76.29166666666667, 76.29166666666667, 77.15, 77.15, 76.75833333333334, 76.75833333333334, 76.575, 76.575, 76.81666666666666, 76.81666666666666, 76.64166666666667, 76.64166666666667, 76.975, 76.975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.194, Test loss: 0.661, Test accuracy: 80.83
Final Round, Global train loss: 0.194, Global test loss: 1.532, Global test accuracy: 46.36
Average accuracy final 10 rounds: 81.17916666666666 

Average global accuracy final 10 rounds: 45.82666666666667 

1722.317059993744
[1.6450448036193848, 3.2900896072387695, 4.6793131828308105, 6.068536758422852, 7.461817741394043, 8.855098724365234, 10.265807390213013, 11.676516056060791, 13.074554920196533, 14.472593784332275, 15.869765996932983, 17.26693820953369, 18.659975290298462, 20.053012371063232, 21.44978094100952, 22.84654951095581, 24.24513268470764, 25.643715858459473, 27.042665243148804, 28.441614627838135, 29.83768367767334, 31.233752727508545, 32.63558101654053, 34.03740930557251, 35.42379355430603, 36.81017780303955, 38.20491003990173, 39.599642276763916, 40.98820471763611, 42.3767671585083, 43.768808126449585, 45.16084909439087, 46.55363130569458, 47.94641351699829, 49.339499950408936, 50.73258638381958, 52.12137413024902, 53.51016187667847, 54.89995503425598, 56.289748191833496, 57.661810636520386, 59.033873081207275, 60.404215812683105, 61.774558544158936, 63.16615915298462, 64.5577597618103, 65.95665955543518, 67.35555934906006, 68.7446539402008, 70.13374853134155, 71.52012658119202, 72.90650463104248, 74.28928303718567, 75.67206144332886, 77.04752445220947, 78.42298746109009, 79.79807543754578, 81.17316341400146, 82.5496129989624, 83.92606258392334, 85.30686902999878, 86.68767547607422, 88.06885552406311, 89.450035572052, 90.82886171340942, 92.20768785476685, 93.58813071250916, 94.96857357025146, 96.35615730285645, 97.74374103546143, 99.12327027320862, 100.50279951095581, 101.88143253326416, 103.26006555557251, 104.64352464675903, 106.02698373794556, 107.40958571434021, 108.79218769073486, 110.18243098258972, 111.57267427444458, 112.95866250991821, 114.34465074539185, 115.72014546394348, 117.09564018249512, 118.48432564735413, 119.87301111221313, 121.25662994384766, 122.64024877548218, 124.01804876327515, 125.39584875106812, 126.77212262153625, 128.1483964920044, 129.53236031532288, 130.91632413864136, 132.3040211200714, 133.69171810150146, 135.0795521736145, 136.46738624572754, 137.85163593292236, 139.2358856201172, 140.6223590373993, 142.0088324546814, 143.3961362838745, 144.78344011306763, 146.16776371002197, 147.55208730697632, 148.9325840473175, 150.3130807876587, 151.6930329799652, 153.07298517227173, 154.44884538650513, 155.82470560073853, 157.21505665779114, 158.60540771484375, 159.98971796035767, 161.37402820587158, 162.7604558467865, 164.14688348770142, 165.53320026397705, 166.91951704025269, 168.3067421913147, 169.6939673423767, 171.0770125389099, 172.46005773544312, 173.84726881980896, 175.2344799041748, 176.6339750289917, 178.0334701538086, 179.41553926467896, 180.79760837554932, 182.18139004707336, 183.5651717185974, 184.94191074371338, 186.31864976882935, 187.70172452926636, 189.08479928970337, 190.46058416366577, 191.83636903762817, 193.21613550186157, 194.59590196609497, 195.980220079422, 197.36453819274902, 198.7481825351715, 200.131826877594, 201.5173900127411, 202.90295314788818, 204.28855538368225, 205.67415761947632, 207.05598878860474, 208.43781995773315, 209.81480741500854, 211.19179487228394, 212.57200932502747, 213.952223777771, 215.3340721130371, 216.71592044830322, 218.0872609615326, 219.45860147476196, 220.8406240940094, 222.22264671325684, 223.59498715400696, 224.96732759475708, 226.34556770324707, 227.72380781173706, 229.10832953453064, 230.49285125732422, 231.87699794769287, 233.26114463806152, 234.64473581314087, 236.02832698822021, 237.414781332016, 238.80123567581177, 240.1863558292389, 241.57147598266602, 242.95621585845947, 244.34095573425293, 245.7295515537262, 247.11814737319946, 248.50341749191284, 249.88868761062622, 251.27367854118347, 252.65866947174072, 254.04291319847107, 255.42715692520142, 256.8105535507202, 258.193950176239, 259.5801498889923, 260.9663496017456, 262.34688806533813, 263.72742652893066, 265.1057879924774, 266.48414945602417, 267.87077736854553, 269.2574052810669, 270.6279044151306, 271.99840354919434, 273.37012934684753, 274.74185514450073, 276.1227698326111, 277.50368452072144, 279.7996370792389, 282.09558963775635]
[41.43333333333333, 41.43333333333333, 49.725, 49.725, 54.266666666666666, 54.266666666666666, 57.05833333333333, 57.05833333333333, 56.075, 56.075, 66.88333333333334, 66.88333333333334, 70.325, 70.325, 71.33333333333333, 71.33333333333333, 71.80833333333334, 71.80833333333334, 72.44166666666666, 72.44166666666666, 72.55, 72.55, 73.58333333333333, 73.58333333333333, 73.68333333333334, 73.68333333333334, 74.88333333333334, 74.88333333333334, 75.16666666666667, 75.16666666666667, 75.16666666666667, 75.16666666666667, 75.0, 75.0, 74.85, 74.85, 75.66666666666667, 75.66666666666667, 75.2, 75.2, 75.7, 75.7, 76.05, 76.05, 76.73333333333333, 76.73333333333333, 77.625, 77.625, 78.05833333333334, 78.05833333333334, 78.31666666666666, 78.31666666666666, 78.39166666666667, 78.39166666666667, 78.63333333333334, 78.63333333333334, 78.48333333333333, 78.48333333333333, 78.30833333333334, 78.30833333333334, 78.11666666666666, 78.11666666666666, 77.69166666666666, 77.69166666666666, 78.33333333333333, 78.33333333333333, 78.15833333333333, 78.15833333333333, 78.575, 78.575, 78.525, 78.525, 78.375, 78.375, 78.725, 78.725, 78.675, 78.675, 78.51666666666667, 78.51666666666667, 78.88333333333334, 78.88333333333334, 78.89166666666667, 78.89166666666667, 78.93333333333334, 78.93333333333334, 79.15833333333333, 79.15833333333333, 79.3, 79.3, 79.55833333333334, 79.55833333333334, 79.875, 79.875, 79.99166666666666, 79.99166666666666, 79.64166666666667, 79.64166666666667, 80.05833333333334, 80.05833333333334, 80.59166666666667, 80.59166666666667, 80.29166666666667, 80.29166666666667, 80.175, 80.175, 80.15833333333333, 80.15833333333333, 79.90833333333333, 79.90833333333333, 80.05833333333334, 80.05833333333334, 80.08333333333333, 80.08333333333333, 79.925, 79.925, 79.9, 79.9, 80.04166666666667, 80.04166666666667, 79.91666666666667, 79.91666666666667, 79.83333333333333, 79.83333333333333, 79.625, 79.625, 79.90833333333333, 79.90833333333333, 79.69166666666666, 79.69166666666666, 79.80833333333334, 79.80833333333334, 79.83333333333333, 79.83333333333333, 80.28333333333333, 80.28333333333333, 80.575, 80.575, 80.60833333333333, 80.60833333333333, 80.475, 80.475, 80.55833333333334, 80.55833333333334, 80.3, 80.3, 80.05833333333334, 80.05833333333334, 80.31666666666666, 80.31666666666666, 79.975, 79.975, 80.14166666666667, 80.14166666666667, 79.80833333333334, 79.80833333333334, 79.275, 79.275, 79.75833333333334, 79.75833333333334, 80.16666666666667, 80.16666666666667, 80.575, 80.575, 80.83333333333333, 80.83333333333333, 81.33333333333333, 81.33333333333333, 81.36666666666666, 81.36666666666666, 81.36666666666666, 81.36666666666666, 80.94166666666666, 80.94166666666666, 81.21666666666667, 81.21666666666667, 80.80833333333334, 80.80833333333334, 81.125, 81.125, 81.28333333333333, 81.28333333333333, 80.85833333333333, 80.85833333333333, 80.86666666666666, 80.86666666666666, 81.04166666666667, 81.04166666666667, 81.34166666666667, 81.34166666666667, 81.26666666666667, 81.26666666666667, 81.24166666666666, 81.24166666666666, 81.14166666666667, 81.14166666666667, 81.29166666666667, 81.29166666666667, 81.45833333333333, 81.45833333333333, 80.83333333333333, 80.83333333333333]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 7939 (global); Percentage 2.58 (7939/307387 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

python: can't open file 'main_fedpac_k.py': [Errno 2] No such file or directory
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307384
307387
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.549, Test loss: 1.622, Test accuracy: 83.97
Final Round, Global train loss: 1.549, Global test loss: 1.623, Global test accuracy: 83.84
Average accuracy final 10 rounds: 83.837 

Average global accuracy final 10 rounds: 83.59649999999999 

4285.290937662125
[3.153989791870117, 6.307979583740234, 9.44290542602539, 12.577831268310547, 15.64026427268982, 18.702697277069092, 21.79485583305359, 24.887014389038086, 28.017870903015137, 31.148727416992188, 34.27760124206543, 37.40647506713867, 40.50376272201538, 43.60105037689209, 46.678056478500366, 49.75506258010864, 52.38529968261719, 55.01553678512573, 57.645612716674805, 60.27568864822388, 62.88087201118469, 65.48605537414551, 68.14147806167603, 70.79690074920654, 73.43310141563416, 76.06930208206177, 78.7077271938324, 81.34615230560303, 84.03907823562622, 86.73200416564941, 89.40173530578613, 92.07146644592285, 94.63297414779663, 97.19448184967041, 99.87174367904663, 102.54900550842285, 105.22569060325623, 107.9023756980896, 110.50898599624634, 113.11559629440308, 115.73024129867554, 118.344886302948, 120.9798858165741, 123.6148853302002, 126.24572229385376, 128.87655925750732, 131.50892519950867, 134.14129114151, 136.780620098114, 139.41994905471802, 142.04800152778625, 144.6760540008545, 147.31649947166443, 149.95694494247437, 152.640061378479, 155.32317781448364, 157.95470118522644, 160.58622455596924, 163.14623165130615, 165.70623874664307, 168.38608074188232, 171.06592273712158, 173.74514603614807, 176.42436933517456, 179.01863741874695, 181.61290550231934, 184.240642786026, 186.86838006973267, 189.5387020111084, 192.20902395248413, 194.8315041065216, 197.45398426055908, 200.08586049079895, 202.71773672103882, 205.33947896957397, 207.96122121810913, 210.59196972846985, 213.22271823883057, 215.89564752578735, 218.56857681274414, 221.26095628738403, 223.95333576202393, 226.5951271057129, 229.23691844940186, 231.85389304161072, 234.47086763381958, 237.15016078948975, 239.8294539451599, 242.50781536102295, 245.186176776886, 247.7275583744049, 250.26893997192383, 252.90731239318848, 255.54568481445312, 258.220911026001, 260.8961372375488, 263.54339051246643, 266.19064378738403, 268.8262724876404, 271.46190118789673, 274.09792041778564, 276.73393964767456, 279.3636426925659, 281.9933457374573, 284.6709485054016, 287.34855127334595, 290.0480020046234, 292.7474527359009, 295.3749723434448, 298.00249195098877, 300.6464502811432, 303.2904086112976, 305.9657759666443, 308.64114332199097, 311.30622386932373, 313.9713044166565, 316.5571126937866, 319.14292097091675, 321.81470799446106, 324.48649501800537, 327.1345772743225, 329.78265953063965, 332.4120771884918, 335.041494846344, 337.6534562110901, 340.2654175758362, 342.998854637146, 345.7322916984558, 348.4901907444, 351.24808979034424, 354.05618238449097, 356.8642749786377, 359.6643867492676, 362.46449851989746, 365.1388850212097, 367.813271522522, 370.55559062957764, 373.2979097366333, 376.1027765274048, 378.90764331817627, 381.6965174674988, 384.4853916168213, 387.1472113132477, 389.8090310096741, 392.48982191085815, 395.17061281204224, 397.95312428474426, 400.7356357574463, 403.4962396621704, 406.25684356689453, 409.00393176078796, 411.7510199546814, 414.49925923347473, 417.24749851226807, 419.9903984069824, 422.7332983016968, 425.4573178291321, 428.1813373565674, 430.93122911453247, 433.68112087249756, 436.45615553855896, 439.23119020462036, 441.88506269454956, 444.53893518447876, 447.28865480422974, 450.0383744239807, 452.8430314064026, 455.64768838882446, 458.4657609462738, 461.28383350372314, 463.97276425361633, 466.6616950035095, 469.28805208206177, 471.914409160614, 474.6951379776001, 477.4758667945862, 480.23245453834534, 482.9890422821045, 485.7197415828705, 488.4504408836365, 491.1937837600708, 493.9371266365051, 496.59523010253906, 499.253333568573, 501.9483664035797, 504.6433992385864, 507.41423082351685, 510.18506240844727, 513.020562171936, 515.8560619354248, 518.6006526947021, 521.3452434539795, 524.0520513057709, 526.7588591575623, 529.5367798805237, 532.3147006034851, 535.063729763031, 537.8127589225769, 540.5520684719086, 543.2913780212402, 544.6879892349243, 546.0846004486084]
[43.4025, 43.4025, 65.36, 65.36, 74.2225, 74.2225, 77.145, 77.145, 77.9625, 77.9625, 79.6225, 79.6225, 80.7325, 80.7325, 80.81, 80.81, 81.185, 81.185, 81.33, 81.33, 81.3375, 81.3375, 81.275, 81.275, 81.37, 81.37, 81.6125, 81.6125, 81.9925, 81.9925, 82.12, 82.12, 82.0925, 82.0925, 82.15, 82.15, 82.2075, 82.2075, 82.205, 82.205, 82.17, 82.17, 82.3275, 82.3275, 82.42, 82.42, 82.3925, 82.3925, 82.3575, 82.3575, 82.3425, 82.3425, 82.335, 82.335, 82.38, 82.38, 82.3825, 82.3825, 82.365, 82.365, 82.4025, 82.4025, 82.415, 82.415, 82.425, 82.425, 82.4375, 82.4375, 82.435, 82.435, 82.4425, 82.4425, 82.845, 82.845, 82.8475, 82.8475, 82.8325, 82.8325, 82.8125, 82.8125, 82.825, 82.825, 82.855, 82.855, 82.8675, 82.8675, 82.83, 82.83, 82.8475, 82.8475, 82.835, 82.835, 82.8225, 82.8225, 82.835, 82.835, 82.7725, 82.7725, 82.775, 82.775, 82.815, 82.815, 82.8475, 82.8475, 82.8425, 82.8425, 82.825, 82.825, 82.8625, 82.8625, 82.8575, 82.8575, 82.8525, 82.8525, 82.8525, 82.8525, 82.8675, 82.8675, 82.865, 82.865, 82.85, 82.85, 82.86, 82.86, 82.8575, 82.8575, 82.85, 82.85, 82.8625, 82.8625, 82.8775, 82.8775, 82.87, 82.87, 82.8625, 82.8625, 82.8625, 82.8625, 82.845, 82.845, 83.13, 83.13, 83.1425, 83.1425, 83.1675, 83.1675, 83.1775, 83.1775, 83.1825, 83.1825, 83.19, 83.19, 83.1975, 83.1975, 83.21, 83.21, 83.215, 83.215, 83.245, 83.245, 83.255, 83.255, 83.25, 83.25, 83.2125, 83.2125, 83.2025, 83.2025, 83.555, 83.555, 83.555, 83.555, 83.5725, 83.5725, 83.5725, 83.5725, 83.57, 83.57, 83.585, 83.585, 83.5925, 83.5925, 83.5625, 83.5625, 83.55, 83.55, 83.925, 83.925, 83.94, 83.94, 83.9525, 83.9525, 83.9375, 83.9375, 83.965, 83.965, 83.9725, 83.9725, 83.9725, 83.9725, 83.975, 83.975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.493, Test loss: 1.538, Test accuracy: 92.50
Final Round, Global train loss: 1.493, Global test loss: 1.535, Global test accuracy: 92.95
Average accuracy final 10 rounds: 92.56 

Average global accuracy final 10 rounds: 92.80333333333336 

727.2801961898804
[0.5871713161468506, 1.1743426322937012, 1.672114372253418, 2.1698861122131348, 2.675518751144409, 3.1811513900756836, 3.694732904434204, 4.208314418792725, 4.716522455215454, 5.224730491638184, 5.727819919586182, 6.23090934753418, 6.74533224105835, 7.2597551345825195, 7.7758119106292725, 8.291868686676025, 8.808857440948486, 9.325846195220947, 9.842485666275024, 10.359125137329102, 10.87195611000061, 11.38478708267212, 11.899868726730347, 12.414950370788574, 12.931874752044678, 13.448799133300781, 13.971772909164429, 14.494746685028076, 15.021087884902954, 15.547429084777832, 16.069389581680298, 16.591350078582764, 17.11462163925171, 17.637893199920654, 18.166335105895996, 18.694777011871338, 19.21769142150879, 19.74060583114624, 20.26274347305298, 20.784881114959717, 21.296321153640747, 21.807761192321777, 22.32321572303772, 22.838670253753662, 23.363539695739746, 23.88840913772583, 24.407052755355835, 24.92569637298584, 25.439599990844727, 25.953503608703613, 26.474668264389038, 26.995832920074463, 27.525603771209717, 28.05537462234497, 28.58319616317749, 29.11101770401001, 29.638019800186157, 30.165021896362305, 30.692843437194824, 31.220664978027344, 31.736817359924316, 32.25296974182129, 32.77305722236633, 33.29314470291138, 33.816118001937866, 34.339091300964355, 34.850773096084595, 35.362454891204834, 35.87083435058594, 36.37921380996704, 36.80563259124756, 37.232051372528076, 37.670839071273804, 38.10962677001953, 38.563172817230225, 39.01671886444092, 39.451279401779175, 39.88583993911743, 40.331995725631714, 40.778151512145996, 41.22223162651062, 41.666311740875244, 42.09805655479431, 42.52980136871338, 42.94355487823486, 43.35730838775635, 43.80402612686157, 44.2507438659668, 44.695196866989136, 45.139649868011475, 45.57868146896362, 46.01771306991577, 46.4445686340332, 46.871424198150635, 47.31039476394653, 47.74936532974243, 48.19634199142456, 48.64331865310669, 49.08497071266174, 49.5266227722168, 49.94830632209778, 50.36998987197876, 50.8078408241272, 51.245691776275635, 51.68303418159485, 52.12037658691406, 52.54925537109375, 52.97813415527344, 53.405303955078125, 53.83247375488281, 54.27204751968384, 54.71162128448486, 55.1596474647522, 55.60767364501953, 56.041358947753906, 56.47504425048828, 56.91277074813843, 57.350497245788574, 57.77179956436157, 58.19310188293457, 58.62727451324463, 59.06144714355469, 59.48690462112427, 59.91236209869385, 60.33710193634033, 60.761841773986816, 61.20290923118591, 61.64397668838501, 62.08005666732788, 62.51613664627075, 62.947325229644775, 63.3785138130188, 63.81237769126892, 64.24624156951904, 64.67851734161377, 65.1107931137085, 65.54209327697754, 65.97339344024658, 66.40408635139465, 66.83477926254272, 67.26886653900146, 67.7029538154602, 68.11426711082458, 68.52558040618896, 68.95359444618225, 69.38160848617554, 69.82159757614136, 70.26158666610718, 70.70518398284912, 71.14878129959106, 71.58008623123169, 72.01139116287231, 72.43845438957214, 72.86551761627197, 73.30540108680725, 73.74528455734253, 74.18690156936646, 74.62851858139038, 75.06773591041565, 75.50695323944092, 75.92099094390869, 76.33502864837646, 76.77691292762756, 77.21879720687866, 77.66907167434692, 78.11934614181519, 78.54973530769348, 78.98012447357178, 79.40289664268494, 79.8256688117981, 80.27059412002563, 80.71551942825317, 81.16033434867859, 81.605149269104, 82.04193949699402, 82.47872972488403, 82.90228700637817, 83.32584428787231, 83.75616955757141, 84.18649482727051, 84.62784147262573, 85.06918811798096, 85.5065107345581, 85.94383335113525, 86.37376737594604, 86.80370140075684, 87.24743032455444, 87.69115924835205, 88.1370496749878, 88.58294010162354, 89.0064754486084, 89.43001079559326, 89.86363577842712, 90.29726076126099, 90.73600339889526, 91.17474603652954, 91.61259937286377, 92.050452709198, 92.48118758201599, 92.91192245483398, 93.79554200172424, 94.6791615486145]
[15.266666666666667, 15.266666666666667, 16.933333333333334, 16.933333333333334, 16.433333333333334, 16.433333333333334, 16.1, 16.1, 20.516666666666666, 20.516666666666666, 25.3, 25.3, 29.95, 29.95, 33.9, 33.9, 41.13333333333333, 41.13333333333333, 48.03333333333333, 48.03333333333333, 56.15, 56.15, 62.083333333333336, 62.083333333333336, 64.13333333333334, 64.13333333333334, 67.53333333333333, 67.53333333333333, 73.53333333333333, 73.53333333333333, 75.05, 75.05, 79.61666666666666, 79.61666666666666, 83.73333333333333, 83.73333333333333, 84.61666666666666, 84.61666666666666, 87.4, 87.4, 87.53333333333333, 87.53333333333333, 87.91666666666667, 87.91666666666667, 88.08333333333333, 88.08333333333333, 88.25, 88.25, 88.38333333333334, 88.38333333333334, 88.36666666666666, 88.36666666666666, 88.66666666666667, 88.66666666666667, 88.71666666666667, 88.71666666666667, 88.76666666666667, 88.76666666666667, 88.88333333333334, 88.88333333333334, 89.18333333333334, 89.18333333333334, 89.51666666666667, 89.51666666666667, 89.66666666666667, 89.66666666666667, 89.76666666666667, 89.76666666666667, 90.06666666666666, 90.06666666666666, 90.08333333333333, 90.08333333333333, 90.2, 90.2, 90.28333333333333, 90.28333333333333, 90.38333333333334, 90.38333333333334, 90.5, 90.5, 90.58333333333333, 90.58333333333333, 90.53333333333333, 90.53333333333333, 90.56666666666666, 90.56666666666666, 90.6, 90.6, 90.55, 90.55, 90.73333333333333, 90.73333333333333, 90.75, 90.75, 90.8, 90.8, 90.93333333333334, 90.93333333333334, 91.01666666666667, 91.01666666666667, 91.06666666666666, 91.06666666666666, 91.16666666666667, 91.16666666666667, 91.45, 91.45, 91.46666666666667, 91.46666666666667, 91.26666666666667, 91.26666666666667, 91.43333333333334, 91.43333333333334, 91.46666666666667, 91.46666666666667, 91.56666666666666, 91.56666666666666, 91.56666666666666, 91.56666666666666, 91.65, 91.65, 91.78333333333333, 91.78333333333333, 91.75, 91.75, 91.88333333333334, 91.88333333333334, 91.9, 91.9, 91.83333333333333, 91.83333333333333, 91.9, 91.9, 92.0, 92.0, 91.93333333333334, 91.93333333333334, 92.0, 92.0, 92.0, 92.0, 92.05, 92.05, 92.05, 92.05, 92.03333333333333, 92.03333333333333, 92.03333333333333, 92.03333333333333, 92.1, 92.1, 92.16666666666667, 92.16666666666667, 92.11666666666666, 92.11666666666666, 92.13333333333334, 92.13333333333334, 92.11666666666666, 92.11666666666666, 92.18333333333334, 92.18333333333334, 92.21666666666667, 92.21666666666667, 92.28333333333333, 92.28333333333333, 92.31666666666666, 92.31666666666666, 92.33333333333333, 92.33333333333333, 92.38333333333334, 92.38333333333334, 92.4, 92.4, 92.36666666666666, 92.36666666666666, 92.43333333333334, 92.43333333333334, 92.48333333333333, 92.48333333333333, 92.48333333333333, 92.48333333333333, 92.56666666666666, 92.56666666666666, 92.58333333333333, 92.58333333333333, 92.66666666666667, 92.66666666666667, 92.53333333333333, 92.53333333333333, 92.58333333333333, 92.58333333333333, 92.56666666666666, 92.56666666666666, 92.63333333333334, 92.63333333333334, 92.55, 92.55, 92.43333333333334, 92.43333333333334, 92.48333333333333, 92.48333333333333, 92.5, 92.5]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.510, Test loss: 1.578, Test accuracy: 88.52
Average accuracy final 10 rounds: 88.55333333333334 

551.3169384002686
[0.5760805606842041, 1.1521611213684082, 1.6048352718353271, 2.057509422302246, 2.505002021789551, 2.9524946212768555, 3.3522932529449463, 3.752091884613037, 4.151350498199463, 4.550609111785889, 4.97218918800354, 5.393769264221191, 5.803007125854492, 6.212244987487793, 6.621366739273071, 7.03048849105835, 7.440708637237549, 7.850928783416748, 8.25400996208191, 8.65709114074707, 9.065695762634277, 9.474300384521484, 9.873093843460083, 10.271887302398682, 10.686851501464844, 11.101815700531006, 11.516700267791748, 11.93158483505249, 12.317862749099731, 12.704140663146973, 13.124487400054932, 13.54483413696289, 13.96263599395752, 14.380437850952148, 14.782833337783813, 15.185228824615479, 15.604031562805176, 16.022834300994873, 16.435739040374756, 16.84864377975464, 17.271652936935425, 17.69466209411621, 18.11461043357849, 18.53455877304077, 18.9441819190979, 19.35380506515503, 19.77106237411499, 20.18831968307495, 20.62575674057007, 21.063193798065186, 21.472038507461548, 21.88088321685791, 22.297902584075928, 22.714921951293945, 23.13490128517151, 23.554880619049072, 23.96540880203247, 24.37593698501587, 24.79957890510559, 25.223220825195312, 25.63799285888672, 26.052764892578125, 26.45314598083496, 26.853527069091797, 27.265745162963867, 27.677963256835938, 28.090808153152466, 28.503653049468994, 28.924376726150513, 29.34510040283203, 29.76166081428528, 30.178221225738525, 30.591071605682373, 31.00392198562622, 31.423945665359497, 31.843969345092773, 32.2660756111145, 32.68818187713623, 33.102412700653076, 33.51664352416992, 33.942458629608154, 34.36827373504639, 34.78698253631592, 35.20569133758545, 35.61378502845764, 36.021878719329834, 36.4430718421936, 36.86426496505737, 37.27816152572632, 37.692058086395264, 38.08963990211487, 38.48722171783447, 38.89580488204956, 39.30438804626465, 39.71728587150574, 40.130183696746826, 40.55039167404175, 40.97059965133667, 41.380340337753296, 41.79008102416992, 42.20737147331238, 42.624661922454834, 43.03574824333191, 43.446834564208984, 43.866180419921875, 44.285526275634766, 44.69677186012268, 45.108017444610596, 45.528143644332886, 45.948269844055176, 46.36133694648743, 46.77440404891968, 47.19625377655029, 47.61810350418091, 48.03566217422485, 48.4532208442688, 48.86616110801697, 49.27910137176514, 49.68819212913513, 50.09728288650513, 50.50335431098938, 50.90942573547363, 51.3226854801178, 51.73594522476196, 52.155129194259644, 52.574313163757324, 52.983810901641846, 53.39330863952637, 53.80590224266052, 54.21849584579468, 54.63345384597778, 55.04841184616089, 55.460116147994995, 55.8718204498291, 56.2883083820343, 56.7047963142395, 57.1279399394989, 57.5510835647583, 57.96199631690979, 58.37290906906128, 58.7961311340332, 59.21935319900513, 59.64112448692322, 60.06289577484131, 60.48188138008118, 60.900866985321045, 61.3171911239624, 61.73351526260376, 62.14824342727661, 62.56297159194946, 62.98630690574646, 63.40964221954346, 63.829049825668335, 64.24845743179321, 64.6679515838623, 65.0874457359314, 65.502037525177, 65.91662931442261, 66.33322429656982, 66.74981927871704, 67.16654324531555, 67.58326721191406, 67.99190735816956, 68.40054750442505, 68.82579398155212, 69.2510404586792, 69.67994618415833, 70.10885190963745, 70.52975034713745, 70.95064878463745, 71.3730719089508, 71.79549503326416, 72.21207213401794, 72.62864923477173, 73.04018521308899, 73.45172119140625, 73.85855340957642, 74.26538562774658, 74.67834043502808, 75.09129524230957, 75.50170612335205, 75.91211700439453, 76.32939672470093, 76.74667644500732, 77.16353249549866, 77.58038854598999, 77.99344754219055, 78.40650653839111, 78.82702279090881, 79.24753904342651, 79.66469240188599, 80.08184576034546, 80.49815034866333, 80.9144549369812, 81.33874797821045, 81.7630410194397, 82.18269467353821, 82.60234832763672, 83.01843857765198, 83.43452882766724, 84.2322371006012, 85.02994537353516]
[17.433333333333334, 17.433333333333334, 20.833333333333332, 20.833333333333332, 24.516666666666666, 24.516666666666666, 28.133333333333333, 28.133333333333333, 34.0, 34.0, 38.65, 38.65, 42.61666666666667, 42.61666666666667, 45.78333333333333, 45.78333333333333, 48.45, 48.45, 51.2, 51.2, 54.65, 54.65, 51.56666666666667, 51.56666666666667, 50.68333333333333, 50.68333333333333, 50.4, 50.4, 55.11666666666667, 55.11666666666667, 57.65, 57.65, 60.13333333333333, 60.13333333333333, 61.6, 61.6, 64.11666666666666, 64.11666666666666, 67.33333333333333, 67.33333333333333, 71.15, 71.15, 74.56666666666666, 74.56666666666666, 76.75, 76.75, 78.85, 78.85, 80.88333333333334, 80.88333333333334, 81.41666666666667, 81.41666666666667, 81.55, 81.55, 82.43333333333334, 82.43333333333334, 83.25, 83.25, 83.43333333333334, 83.43333333333334, 84.4, 84.4, 85.2, 85.2, 85.73333333333333, 85.73333333333333, 85.68333333333334, 85.68333333333334, 86.28333333333333, 86.28333333333333, 86.35, 86.35, 86.26666666666667, 86.26666666666667, 86.95, 86.95, 87.03333333333333, 87.03333333333333, 87.06666666666666, 87.06666666666666, 87.13333333333334, 87.13333333333334, 87.45, 87.45, 87.16666666666667, 87.16666666666667, 87.26666666666667, 87.26666666666667, 87.5, 87.5, 87.2, 87.2, 87.9, 87.9, 88.2, 88.2, 88.33333333333333, 88.33333333333333, 88.41666666666667, 88.41666666666667, 88.48333333333333, 88.48333333333333, 88.63333333333334, 88.63333333333334, 88.48333333333333, 88.48333333333333, 88.5, 88.5, 88.3, 88.3, 88.41666666666667, 88.41666666666667, 87.93333333333334, 87.93333333333334, 88.16666666666667, 88.16666666666667, 88.11666666666666, 88.11666666666666, 88.3, 88.3, 88.23333333333333, 88.23333333333333, 88.45, 88.45, 88.56666666666666, 88.56666666666666, 88.4, 88.4, 88.58333333333333, 88.58333333333333, 88.65, 88.65, 88.83333333333333, 88.83333333333333, 88.7, 88.7, 88.45, 88.45, 88.65, 88.65, 88.86666666666666, 88.86666666666666, 88.76666666666667, 88.76666666666667, 88.83333333333333, 88.83333333333333, 88.78333333333333, 88.78333333333333, 88.81666666666666, 88.81666666666666, 88.66666666666667, 88.66666666666667, 88.41666666666667, 88.41666666666667, 88.46666666666667, 88.46666666666667, 88.33333333333333, 88.33333333333333, 88.51666666666667, 88.51666666666667, 88.58333333333333, 88.58333333333333, 88.4, 88.4, 88.48333333333333, 88.48333333333333, 88.4, 88.4, 88.55, 88.55, 88.6, 88.6, 88.53333333333333, 88.53333333333333, 88.6, 88.6, 88.71666666666667, 88.71666666666667, 88.73333333333333, 88.73333333333333, 88.63333333333334, 88.63333333333334, 88.48333333333333, 88.48333333333333, 88.56666666666666, 88.56666666666666, 88.65, 88.65, 88.6, 88.6, 88.45, 88.45, 88.51666666666667, 88.51666666666667, 88.53333333333333, 88.53333333333333, 88.55, 88.55, 88.55, 88.55, 88.51666666666667, 88.51666666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.510, Test loss: 1.571, Test accuracy: 89.30
Average accuracy final 10 rounds: 88.12166666666667 

600.024521112442
[0.603417158126831, 1.206834316253662, 1.7053766250610352, 2.203918933868408, 2.705655336380005, 3.2073917388916016, 3.7059075832366943, 4.204423427581787, 4.706453084945679, 5.20848274230957, 5.707597494125366, 6.206712245941162, 6.705072641372681, 7.203433036804199, 7.697910308837891, 8.192387580871582, 8.687512636184692, 9.182637691497803, 9.67638373374939, 10.170129776000977, 10.673760414123535, 11.177391052246094, 11.666673183441162, 12.15595531463623, 12.65081787109375, 13.14568042755127, 13.643352746963501, 14.141025066375732, 14.637102365493774, 15.133179664611816, 15.636549711227417, 16.139919757843018, 16.63190984725952, 17.123899936676025, 17.616662740707397, 18.10942554473877, 18.605507612228394, 19.101589679718018, 19.60416841506958, 20.106747150421143, 20.597612380981445, 21.088477611541748, 21.58408284187317, 22.07968807220459, 22.57409119606018, 23.06849431991577, 23.56623339653015, 24.06397247314453, 24.564054250717163, 25.064136028289795, 25.55821681022644, 26.052297592163086, 26.550861358642578, 27.04942512512207, 27.54986882209778, 28.050312519073486, 28.54987645149231, 29.049440383911133, 29.54885220527649, 30.048264026641846, 30.549821376800537, 31.05137872695923, 31.54666781425476, 32.04195690155029, 32.5404167175293, 33.0388765335083, 33.53878164291382, 34.038686752319336, 34.539307594299316, 35.0399284362793, 35.52525520324707, 36.010581970214844, 36.5049467086792, 36.999311447143555, 37.482014894485474, 37.96471834182739, 38.45391297340393, 38.94310760498047, 39.43268823623657, 39.922268867492676, 40.415441274642944, 40.90861368179321, 41.4068078994751, 41.90500211715698, 42.39414572715759, 42.8832893371582, 43.37210822105408, 43.86092710494995, 44.349640130996704, 44.83835315704346, 45.33046770095825, 45.82258224487305, 46.333067178726196, 46.843552112579346, 47.330710887908936, 47.817869663238525, 48.309455156326294, 48.80104064941406, 49.30606436729431, 49.81108808517456, 50.295910596847534, 50.78073310852051, 51.27039837837219, 51.76006364822388, 52.24864101409912, 52.737218379974365, 53.224910259246826, 53.71260213851929, 54.417887449264526, 55.123172760009766, 55.613914251327515, 56.104655742645264, 56.595824003219604, 57.086992263793945, 57.577409982681274, 58.0678277015686, 58.56955552101135, 59.0712833404541, 59.567925214767456, 60.06456708908081, 60.55743408203125, 61.05030107498169, 61.540385723114014, 62.03047037124634, 62.512710094451904, 62.99494981765747, 63.475483655929565, 63.95601749420166, 64.4425802230835, 64.92914295196533, 65.41847562789917, 65.90780830383301, 66.39802098274231, 66.88823366165161, 67.37989640235901, 67.8715591430664, 68.36180686950684, 68.85205459594727, 69.34874606132507, 69.84543752670288, 70.33259916305542, 70.81976079940796, 71.31010127067566, 71.80044174194336, 72.29128813743591, 72.78213453292847, 73.26762294769287, 73.75311136245728, 74.23630785942078, 74.71950435638428, 75.20912384986877, 75.69874334335327, 76.17876505851746, 76.65878677368164, 77.15167450904846, 77.64456224441528, 78.13414359092712, 78.62372493743896, 79.14683222770691, 79.66993951797485, 80.16019749641418, 80.65045547485352, 81.14484477043152, 81.63923406600952, 82.15886807441711, 82.6785020828247, 83.17084431648254, 83.66318655014038, 84.15319395065308, 84.64320135116577, 85.13284063339233, 85.6224799156189, 86.11754155158997, 86.61260318756104, 87.10785245895386, 87.60310173034668, 88.09331965446472, 88.58353757858276, 89.06271600723267, 89.54189443588257, 90.02974390983582, 90.51759338378906, 91.01439905166626, 91.51120471954346, 92.00445938110352, 92.49771404266357, 92.99024271965027, 93.48277139663696, 93.97638654708862, 94.47000169754028, 94.95805406570435, 95.44610643386841, 95.94680976867676, 96.44751310348511, 96.95925736427307, 97.47100162506104, 97.98133277893066, 98.4916639328003, 99.01668405532837, 99.54170417785645, 100.38248229026794, 101.22326040267944]
[18.8, 18.8, 24.8, 24.8, 31.566666666666666, 31.566666666666666, 40.15, 40.15, 43.86666666666667, 43.86666666666667, 40.53333333333333, 40.53333333333333, 41.583333333333336, 41.583333333333336, 41.4, 41.4, 47.416666666666664, 47.416666666666664, 51.0, 51.0, 54.61666666666667, 54.61666666666667, 59.6, 59.6, 64.1, 64.1, 66.56666666666666, 66.56666666666666, 69.36666666666666, 69.36666666666666, 71.06666666666666, 71.06666666666666, 72.56666666666666, 72.56666666666666, 73.5, 73.5, 73.95, 73.95, 74.56666666666666, 74.56666666666666, 75.85, 75.85, 76.65, 76.65, 78.21666666666667, 78.21666666666667, 79.23333333333333, 79.23333333333333, 80.18333333333334, 80.18333333333334, 80.78333333333333, 80.78333333333333, 80.93333333333334, 80.93333333333334, 81.28333333333333, 81.28333333333333, 81.55, 81.55, 81.4, 81.4, 81.56666666666666, 81.56666666666666, 81.65, 81.65, 82.2, 82.2, 82.31666666666666, 82.31666666666666, 82.38333333333334, 82.38333333333334, 82.75, 82.75, 82.95, 82.95, 83.06666666666666, 83.06666666666666, 82.93333333333334, 82.93333333333334, 82.85, 82.85, 82.86666666666666, 82.86666666666666, 82.95, 82.95, 83.01666666666667, 83.01666666666667, 83.01666666666667, 83.01666666666667, 83.48333333333333, 83.48333333333333, 83.5, 83.5, 83.48333333333333, 83.48333333333333, 83.65, 83.65, 83.31666666666666, 83.31666666666666, 83.53333333333333, 83.53333333333333, 83.55, 83.55, 83.46666666666667, 83.46666666666667, 83.5, 83.5, 83.56666666666666, 83.56666666666666, 83.65, 83.65, 83.65, 83.65, 83.5, 83.5, 83.51666666666667, 83.51666666666667, 83.61666666666666, 83.61666666666666, 83.78333333333333, 83.78333333333333, 83.71666666666667, 83.71666666666667, 83.76666666666667, 83.76666666666667, 83.63333333333334, 83.63333333333334, 83.88333333333334, 83.88333333333334, 83.83333333333333, 83.83333333333333, 83.93333333333334, 83.93333333333334, 83.93333333333334, 83.93333333333334, 84.08333333333333, 84.08333333333333, 84.15, 84.15, 84.38333333333334, 84.38333333333334, 84.53333333333333, 84.53333333333333, 84.68333333333334, 84.68333333333334, 84.78333333333333, 84.78333333333333, 84.95, 84.95, 85.26666666666667, 85.26666666666667, 85.3, 85.3, 85.55, 85.55, 85.7, 85.7, 85.61666666666666, 85.61666666666666, 85.76666666666667, 85.76666666666667, 85.86666666666666, 85.86666666666666, 85.76666666666667, 85.76666666666667, 86.4, 86.4, 86.6, 86.6, 86.95, 86.95, 87.33333333333333, 87.33333333333333, 87.45, 87.45, 87.63333333333334, 87.63333333333334, 87.66666666666667, 87.66666666666667, 87.6, 87.6, 87.76666666666667, 87.76666666666667, 87.8, 87.8, 87.76666666666667, 87.76666666666667, 87.8, 87.8, 88.0, 88.0, 88.16666666666667, 88.16666666666667, 88.38333333333334, 88.38333333333334, 88.4, 88.4, 88.61666666666666, 88.61666666666666, 88.51666666666667, 88.51666666666667, 89.3, 89.3]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

Traceback (most recent call last):
  File "main_fedrep.py", line 64, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Final Round, Train loss: 1.101, Test loss: 1.713, Test accuracy: 83.98
Average accuracy final 10 rounds: 84.40074999999997
4826.960595846176
[]
[62.895, 66.995, 77.6, 79.8775, 82.5875, 83.355, 83.6175, 83.575, 83.71, 83.88, 83.905, 83.925, 83.955, 83.8575, 83.67, 83.6625, 83.6375, 83.5575, 83.4825, 83.435, 83.2925, 83.2125, 83.1475, 83.0675, 82.945, 82.8225, 82.7425, 82.755, 82.6275, 82.53, 82.4075, 82.345, 82.3, 82.1825, 82.1025, 82.1, 82.0975, 81.98, 81.855, 81.82, 81.7275, 81.61, 83.1725, 84.1675, 85.3375, 85.6925, 86.1175, 86.5675, 86.68, 86.67, 86.68, 86.8025, 87.1825, 87.0075, 86.755, 86.8375, 86.805, 86.79, 86.645, 86.655, 86.5925, 86.6725, 86.5175, 86.3875, 86.2975, 86.25, 86.1625, 86.055, 85.9475, 85.9225, 85.7275, 85.5975, 85.545, 85.485, 85.315, 85.21, 85.305, 85.255, 85.16, 85.0325, 84.9625, 84.99, 85.0025, 85.0225, 84.9425, 84.855, 84.825, 84.765, 84.7525, 84.66, 84.6525, 84.585, 84.5625, 84.4725, 84.51, 84.4725, 84.2325, 84.1675, 84.21, 84.1425, 83.985]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.560, Test loss: 1.582, Test accuracy: 89.29
Average accuracy final 10 rounds: 86.45166666666667
Average global accuracy final 10 rounds: 86.45166666666667
1128.9460797309875
[]
[14.916666666666666, 15.15, 17.375, 18.85, 18.216666666666665, 18.308333333333334, 18.391666666666666, 18.433333333333334, 19.108333333333334, 22.083333333333332, 26.075, 23.358333333333334, 19.275, 24.441666666666666, 32.391666666666666, 32.608333333333334, 28.891666666666666, 33.74166666666667, 37.358333333333334, 41.075, 45.38333333333333, 52.50833333333333, 49.94166666666667, 49.025, 46.108333333333334, 40.65833333333333, 43.958333333333336, 43.416666666666664, 47.425, 48.88333333333333, 51.083333333333336, 50.641666666666666, 52.875, 50.516666666666666, 58.34166666666667, 56.86666666666667, 56.24166666666667, 55.075, 55.25833333333333, 55.9, 55.775, 58.34166666666667, 62.141666666666666, 65.49166666666666, 66.325, 65.86666666666666, 68.55833333333334, 67.825, 67.75833333333334, 69.275, 69.25833333333334, 70.01666666666667, 73.59166666666667, 74.19166666666666, 74.3, 74.95, 75.75, 77.69166666666666, 77.58333333333333, 78.925, 80.59166666666667, 80.6, 80.46666666666667, 80.825, 82.11666666666666, 80.64166666666667, 81.40833333333333, 82.71666666666667, 82.59166666666667, 82.45, 82.8, 83.56666666666666, 84.38333333333334, 84.84166666666667, 84.58333333333333, 84.05833333333334, 84.575, 84.93333333333334, 84.40833333333333, 83.975, 84.68333333333334, 85.15, 85.68333333333334, 85.7, 86.1, 86.19166666666666, 85.975, 85.75833333333334, 85.65, 85.85, 86.2, 86.25833333333334, 86.38333333333334, 86.45833333333333, 85.975, 86.01666666666667, 86.58333333333333, 86.96666666666667, 86.80833333333334, 86.86666666666666, 89.29166666666667]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.38
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.22
Average accuracy final 10 rounds: 10.171666666666667 

Average global accuracy final 10 rounds: 10.210833333333333 

1321.3364202976227
[1.2674622535705566, 2.406761407852173, 3.5646839141845703, 4.725330352783203, 5.832299709320068, 6.970546007156372, 8.10470962524414, 9.29630732536316, 10.456959247589111, 11.661491632461548, 12.820555686950684, 14.006380558013916, 15.124264478683472, 16.27456831932068, 17.42207622528076, 18.57684826850891, 19.673542976379395, 20.800785303115845, 21.97990918159485, 23.171537399291992, 24.31376552581787, 25.461681365966797, 26.649221897125244, 27.833844661712646, 28.97479224205017, 30.13034725189209, 31.29984974861145, 32.47962522506714, 33.59456491470337, 34.71638751029968, 35.89274001121521, 37.08163595199585, 38.19179010391235, 39.33796954154968, 40.561707735061646, 41.84243845939636, 42.96368217468262, 44.13128709793091, 45.299156188964844, 46.42885947227478, 47.53045892715454, 48.72999382019043, 49.870543003082275, 51.000340938568115, 52.14201354980469, 53.43737602233887, 54.78737926483154, 56.071638345718384, 57.287954568862915, 58.48075604438782, 59.640302658081055, 60.762959718704224, 61.930976152420044, 63.09400510787964, 64.21459484100342, 65.21304297447205, 66.23931837081909, 67.28536677360535, 68.30855751037598, 69.33762979507446, 70.37551808357239, 71.40613269805908, 72.4369261264801, 73.50272917747498, 74.51200151443481, 75.67716908454895, 76.8601222038269, 78.0211272239685, 79.13406419754028, 80.28728485107422, 81.4672281742096, 82.60229873657227, 83.72739958763123, 84.86988663673401, 86.02915024757385, 87.14732480049133, 88.2833321094513, 89.42382025718689, 90.53068041801453, 91.67946147918701, 92.74068093299866, 93.76587080955505, 94.83697509765625, 95.88721513748169, 96.91295218467712, 97.94722890853882, 99.0143461227417, 100.02324485778809, 101.04770612716675, 102.0892117023468, 103.15106511116028, 104.1671953201294, 105.19963884353638, 106.21140193939209, 107.23865509033203, 108.28828072547913, 109.30956029891968, 110.34960746765137, 111.40874600410461, 112.44236969947815, 114.34681963920593]
[8.55, 8.583333333333334, 8.566666666666666, 8.55, 8.541666666666666, 8.541666666666666, 8.566666666666666, 8.591666666666667, 8.608333333333333, 8.633333333333333, 8.65, 8.65, 8.716666666666667, 8.741666666666667, 8.758333333333333, 8.791666666666666, 8.8, 8.775, 8.791666666666666, 8.791666666666666, 8.816666666666666, 8.866666666666667, 8.858333333333333, 8.9, 8.916666666666666, 8.925, 8.916666666666666, 8.933333333333334, 8.933333333333334, 9.008333333333333, 9.0, 8.983333333333333, 9.008333333333333, 9.033333333333333, 9.075, 9.083333333333334, 9.116666666666667, 9.141666666666667, 9.141666666666667, 9.191666666666666, 9.233333333333333, 9.216666666666667, 9.241666666666667, 9.241666666666667, 9.233333333333333, 9.25, 9.266666666666667, 9.266666666666667, 9.283333333333333, 9.283333333333333, 9.283333333333333, 9.291666666666666, 9.325, 9.333333333333334, 9.358333333333333, 9.366666666666667, 9.408333333333333, 9.433333333333334, 9.416666666666666, 9.433333333333334, 9.483333333333333, 9.5, 9.516666666666667, 9.533333333333333, 9.508333333333333, 9.516666666666667, 9.525, 9.55, 9.541666666666666, 9.566666666666666, 9.583333333333334, 9.616666666666667, 9.641666666666667, 9.683333333333334, 9.716666666666667, 9.7, 9.716666666666667, 9.766666666666667, 9.808333333333334, 9.816666666666666, 9.85, 9.875, 9.9, 9.891666666666667, 9.916666666666666, 10.016666666666667, 10.041666666666666, 10.025, 10.041666666666666, 10.058333333333334, 10.075, 10.075, 10.083333333333334, 10.158333333333333, 10.15, 10.191666666666666, 10.216666666666667, 10.241666666666667, 10.258333333333333, 10.266666666666667, 10.375]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Final Round, Train loss: 1.571, Test loss: 1.595, Test accuracy: 86.62
Average accuracy final 10 rounds: 86.56083333333333
1847.803294658661
[2.7503843307495117, 5.36546516418457, 7.986424922943115, 10.62277865409851, 13.232742071151733, 15.82410192489624, 18.483492136001587, 21.091028928756714, 23.692458629608154, 26.24545168876648, 28.736539125442505, 31.324002742767334, 33.889750480651855, 36.35500717163086, 38.894604206085205, 41.45033550262451, 43.948933601379395, 46.47829246520996, 48.97743272781372, 51.462663412094116, 54.01306390762329, 56.499571561813354, 58.980997800827026, 61.50694441795349, 64.03367805480957, 66.54268002510071, 69.09785866737366, 71.58499836921692, 74.1237416267395, 76.64707207679749, 79.13879036903381, 81.64173483848572, 84.14994406700134, 86.63386392593384, 89.13851380348206, 91.65715146064758, 94.1726062297821, 96.73455953598022, 99.29942083358765, 101.81985807418823, 104.30430126190186, 106.82549023628235, 109.31395626068115, 111.80866312980652, 114.33742809295654, 116.83995366096497, 119.36207461357117, 121.87295699119568, 124.34916543960571, 126.9008424282074, 129.45238399505615, 131.91877388954163, 134.4337830543518, 136.9381983280182, 139.40021300315857, 141.91351866722107, 144.4792332649231, 147.11200642585754, 149.79340314865112, 152.43903708457947, 155.06627488136292, 157.77586817741394, 160.2539939880371, 162.73873162269592, 165.2138249874115, 167.67567133903503, 170.18743109703064, 172.65171432495117, 175.11242818832397, 177.58508586883545, 180.04322814941406, 182.5193953514099, 185.03620839118958, 187.4951775074005, 189.9907510280609, 192.46892929077148, 194.93854999542236, 197.4264760017395, 199.89592170715332, 202.34593176841736, 204.83842372894287, 207.30478167533875, 209.78405928611755, 212.30172967910767, 214.75559639930725, 217.24530339241028, 219.72739553451538, 222.20168495178223, 224.69817519187927, 227.16579699516296, 229.6124472618103, 232.1072952747345, 234.57879948616028, 237.08442902565002, 239.61488127708435, 242.08312821388245, 244.6059513092041, 247.12665390968323, 249.58246159553528, 252.07927989959717, 254.21138882637024]
[31.45, 38.958333333333336, 37.416666666666664, 56.925, 65.54166666666667, 67.03333333333333, 67.36666666666666, 67.74166666666666, 68.00833333333334, 68.33333333333333, 68.39166666666667, 68.60833333333333, 68.475, 68.59166666666667, 68.65833333333333, 68.64166666666667, 68.68333333333334, 68.79166666666667, 71.93333333333334, 74.88333333333334, 75.25, 75.55833333333334, 75.55833333333334, 75.8, 83.05, 83.88333333333334, 84.11666666666666, 84.30833333333334, 84.54166666666667, 84.63333333333334, 84.575, 84.725, 84.70833333333333, 84.9, 84.90833333333333, 85.075, 85.23333333333333, 85.34166666666667, 85.29166666666667, 85.40833333333333, 85.35, 85.55833333333334, 85.50833333333334, 85.65833333333333, 85.71666666666667, 85.65833333333333, 85.68333333333334, 85.775, 85.88333333333334, 85.86666666666666, 85.8, 85.74166666666666, 86.075, 85.91666666666667, 85.80833333333334, 86.04166666666667, 85.99166666666666, 86.03333333333333, 86.04166666666667, 86.13333333333334, 86.06666666666666, 85.89166666666667, 86.06666666666666, 86.06666666666666, 86.10833333333333, 86.2, 86.18333333333334, 86.15, 86.18333333333334, 86.24166666666666, 86.31666666666666, 86.4, 86.44166666666666, 86.48333333333333, 86.45, 86.575, 86.45, 86.4, 86.3, 86.45, 86.50833333333334, 86.48333333333333, 86.4, 86.575, 86.55, 86.55, 86.525, 86.39166666666667, 86.46666666666667, 86.38333333333334, 86.65833333333333, 86.525, 86.425, 86.60833333333333, 86.575, 86.5, 86.5, 86.55833333333334, 86.61666666666666, 86.64166666666667, 86.61666666666666]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.479, Test loss: 1.508, Test accuracy: 96.27
Average accuracy final 10 rounds: 96.33000000000001
1049.9578347206116
[1.448664903640747, 2.634899854660034, 3.7772271633148193, 4.97907280921936, 6.147642135620117, 7.345025539398193, 8.566629886627197, 9.714844942092896, 10.868985891342163, 12.066795110702515, 13.326374769210815, 14.615993976593018, 15.840363502502441, 17.102760553359985, 18.386617183685303, 19.58753490447998, 20.78672170639038, 21.912323713302612, 23.064719676971436, 24.285961627960205, 25.516005992889404, 26.780191659927368, 27.89586114883423, 29.123542547225952, 30.41639494895935, 31.72014093399048, 32.98700308799744, 34.20588493347168, 35.44387125968933, 36.712581634521484, 37.975167989730835, 39.22771334648132, 40.42686581611633, 41.66021943092346, 42.97433018684387, 44.20524764060974, 45.41779136657715, 46.55408477783203, 47.707865953445435, 48.93880605697632, 50.141878604888916, 51.35922694206238, 52.54216766357422, 53.68176007270813, 54.872907400131226, 56.113757371902466, 57.290032386779785, 58.474700689315796, 59.62547588348389, 60.85180711746216, 62.055468797683716, 63.24161672592163, 64.40751767158508, 65.5793399810791, 66.76773285865784, 67.99295496940613, 69.14845967292786, 70.28954458236694, 71.46190428733826, 72.67045998573303, 73.86846828460693, 75.04450011253357, 76.22093319892883, 77.3954918384552, 78.59101390838623, 79.82266807556152, 80.9969367980957, 82.1347587108612, 83.3121886253357, 84.52569317817688, 85.73923230171204, 86.88563203811646, 88.08249378204346, 89.26188468933105, 90.48615002632141, 91.70290613174438, 92.893061876297, 94.05961656570435, 95.22022533416748, 96.3971152305603, 97.58339166641235, 98.7716965675354, 99.92049717903137, 101.18862843513489, 102.45486521720886, 103.69585824012756, 104.94752144813538, 106.15347695350647, 107.33481359481812, 108.5517168045044, 109.71314740180969, 110.90011954307556, 112.08200597763062, 113.25803875923157, 114.47286009788513, 115.66205358505249, 116.83272576332092, 118.03698539733887, 119.2182891368866, 120.3958158493042, 121.96624374389648]
[18.725, 26.341666666666665, 37.19166666666667, 46.425, 42.75833333333333, 47.65, 57.375, 67.89166666666667, 72.26666666666667, 74.225, 75.13333333333334, 75.65833333333333, 76.6, 78.50833333333334, 79.95833333333333, 81.425, 81.625, 82.28333333333333, 82.875, 83.56666666666666, 84.23333333333333, 85.175, 86.625, 89.26666666666667, 89.9, 91.14166666666667, 91.65, 92.23333333333333, 92.69166666666666, 93.075, 93.26666666666667, 93.375, 93.85, 93.88333333333334, 94.01666666666667, 94.15, 94.29166666666667, 94.46666666666667, 94.45833333333333, 94.58333333333333, 94.51666666666667, 94.64166666666667, 94.7, 94.68333333333334, 94.80833333333334, 94.8, 94.86666666666666, 94.975, 95.1, 95.23333333333333, 95.15, 95.34166666666667, 95.38333333333334, 95.35, 95.325, 95.54166666666667, 95.48333333333333, 95.56666666666666, 95.55833333333334, 95.55, 95.675, 95.66666666666667, 95.65, 95.80833333333334, 95.78333333333333, 95.825, 95.90833333333333, 95.85, 95.90833333333333, 95.85833333333333, 95.95, 96.01666666666667, 96.03333333333333, 96.06666666666666, 96.13333333333334, 96.11666666666666, 96.19166666666666, 96.2, 96.11666666666666, 96.225, 96.20833333333333, 96.19166666666666, 96.225, 96.275, 96.25833333333334, 96.25833333333334, 96.31666666666666, 96.36666666666666, 96.35833333333333, 96.35833333333333, 96.275, 96.29166666666667, 96.25, 96.30833333333334, 96.35833333333333, 96.35833333333333, 96.36666666666666, 96.33333333333333, 96.375, 96.38333333333334, 96.26666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.489, Test loss: 1.517, Test accuracy: 96.74
Average accuracy final 10 rounds: 96.6775
1379.5061376094818
[1.4153821468353271, 2.8307642936706543, 4.08476996421814, 5.338775634765625, 6.64872670173645, 7.958677768707275, 9.221203565597534, 10.483729362487793, 11.73922348022461, 12.994717597961426, 14.26640009880066, 15.538082599639893, 16.83769917488098, 18.13731575012207, 19.394479036331177, 20.651642322540283, 21.886200189590454, 23.120758056640625, 24.384368181228638, 25.64797830581665, 26.90840220451355, 28.16882610321045, 29.42667555809021, 30.68452501296997, 31.9320809841156, 33.17963695526123, 34.48162889480591, 35.783620834350586, 37.068931579589844, 38.3542423248291, 39.58463478088379, 40.81502723693848, 42.09271502494812, 43.370402812957764, 44.69597387313843, 46.02154493331909, 47.28797197341919, 48.55439901351929, 49.840550661087036, 51.126702308654785, 52.439361333847046, 53.75202035903931, 55.0093469619751, 56.26667356491089, 57.49404454231262, 58.721415519714355, 59.986724615097046, 61.252033710479736, 62.56910538673401, 63.88617706298828, 65.17206168174744, 66.45794630050659, 67.7138135433197, 68.96968078613281, 70.26341581344604, 71.55715084075928, 72.85948896408081, 74.16182708740234, 75.38470125198364, 76.60757541656494, 77.87208962440491, 79.13660383224487, 80.43273568153381, 81.72886753082275, 82.99032521247864, 84.25178289413452, 85.47389459609985, 86.69600629806519, 88.02946400642395, 89.36292171478271, 90.64816117286682, 91.93340063095093, 93.18693614006042, 94.44047164916992, 95.68934726715088, 96.93822288513184, 98.25660514831543, 99.57498741149902, 100.86745119094849, 102.15991497039795, 103.38120698928833, 104.60249900817871, 105.88597512245178, 107.16945123672485, 108.4653263092041, 109.76120138168335, 111.0176055431366, 112.27400970458984, 113.39047741889954, 114.50694513320923, 115.69702315330505, 116.88710117340088, 118.05015802383423, 119.21321487426758, 120.305819272995, 121.39842367172241, 122.50172519683838, 123.60502672195435, 124.75616502761841, 125.90730333328247, 127.10423731803894, 128.3011713027954, 129.3810589313507, 130.460946559906, 131.59909391403198, 132.73724126815796, 133.89539289474487, 135.0535445213318, 136.2047996520996, 137.35605478286743, 138.48758578300476, 139.6191167831421, 140.79177045822144, 141.96442413330078, 143.15213227272034, 144.3398404121399, 145.51337385177612, 146.68690729141235, 147.8009490966797, 148.91499090194702, 150.126788854599, 151.33858680725098, 152.51494336128235, 153.69129991531372, 154.80684089660645, 155.92238187789917, 157.02884721755981, 158.13531255722046, 159.31110954284668, 160.4869065284729, 161.68740105628967, 162.88789558410645, 164.04202008247375, 165.19614458084106, 166.32413601875305, 167.45212745666504, 168.6504111289978, 169.84869480133057, 171.05595350265503, 172.2632122039795, 173.47531247138977, 174.68741273880005, 175.90975642204285, 177.13210010528564, 178.41871094703674, 179.70532178878784, 180.9713454246521, 182.23736906051636, 183.4418249130249, 184.64628076553345, 185.88085675239563, 187.1154327392578, 188.3981261253357, 189.68081951141357, 190.91836833953857, 192.15591716766357, 193.3897511959076, 194.6235852241516, 195.86258125305176, 197.1015772819519, 198.3773729801178, 199.6531686782837, 200.83436727523804, 202.01556587219238, 203.2423493862152, 204.46913290023804, 205.73762488365173, 207.00611686706543, 208.26364278793335, 209.52116870880127, 210.7293393611908, 211.93751001358032, 213.17270636558533, 214.40790271759033, 215.6988935470581, 216.98988437652588, 218.23217582702637, 219.47446727752686, 220.71166563034058, 221.9488639831543, 223.23736214637756, 224.52586030960083, 225.75444793701172, 226.9830355644226, 228.21355319023132, 229.44407081604004, 230.64650750160217, 231.8489441871643, 233.08989834785461, 234.33085250854492, 235.54223585128784, 236.75361919403076, 237.9874815940857, 239.22134399414062, 240.45516061782837, 241.6889772415161, 242.89447712898254, 244.09997701644897, 245.23748445510864, 246.3749918937683, 247.88156986236572, 249.38814783096313]
[13.758333333333333, 13.758333333333333, 29.066666666666666, 29.066666666666666, 35.18333333333333, 35.18333333333333, 38.483333333333334, 38.483333333333334, 38.15833333333333, 38.15833333333333, 43.85, 43.85, 49.53333333333333, 49.53333333333333, 55.09166666666667, 55.09166666666667, 60.675, 60.675, 64.85, 64.85, 74.0, 74.0, 79.34166666666667, 79.34166666666667, 81.025, 81.025, 82.35833333333333, 82.35833333333333, 83.63333333333334, 83.63333333333334, 85.90833333333333, 85.90833333333333, 88.35, 88.35, 89.81666666666666, 89.81666666666666, 90.65, 90.65, 91.26666666666667, 91.26666666666667, 91.73333333333333, 91.73333333333333, 92.00833333333334, 92.00833333333334, 92.375, 92.375, 92.85833333333333, 92.85833333333333, 93.26666666666667, 93.26666666666667, 93.34166666666667, 93.34166666666667, 93.59166666666667, 93.59166666666667, 93.55833333333334, 93.55833333333334, 93.64166666666667, 93.64166666666667, 93.76666666666667, 93.76666666666667, 94.05, 94.05, 94.23333333333333, 94.23333333333333, 94.225, 94.225, 94.4, 94.4, 94.425, 94.425, 94.49166666666666, 94.49166666666666, 94.63333333333334, 94.63333333333334, 94.66666666666667, 94.66666666666667, 94.79166666666667, 94.79166666666667, 94.925, 94.925, 95.025, 95.025, 95.125, 95.125, 95.11666666666666, 95.11666666666666, 95.19166666666666, 95.19166666666666, 95.25833333333334, 95.25833333333334, 95.39166666666667, 95.39166666666667, 95.51666666666667, 95.51666666666667, 95.35833333333333, 95.35833333333333, 95.45, 95.45, 95.50833333333334, 95.50833333333334, 95.625, 95.625, 95.63333333333334, 95.63333333333334, 95.61666666666666, 95.61666666666666, 95.71666666666667, 95.71666666666667, 95.825, 95.825, 95.875, 95.875, 95.93333333333334, 95.93333333333334, 95.89166666666667, 95.89166666666667, 95.93333333333334, 95.93333333333334, 96.01666666666667, 96.01666666666667, 96.16666666666667, 96.16666666666667, 96.10833333333333, 96.10833333333333, 96.10833333333333, 96.10833333333333, 96.16666666666667, 96.16666666666667, 96.09166666666667, 96.09166666666667, 96.15833333333333, 96.15833333333333, 96.16666666666667, 96.16666666666667, 96.20833333333333, 96.20833333333333, 96.21666666666667, 96.21666666666667, 96.23333333333333, 96.23333333333333, 96.25833333333334, 96.25833333333334, 96.30833333333334, 96.30833333333334, 96.35833333333333, 96.35833333333333, 96.325, 96.325, 96.50833333333334, 96.50833333333334, 96.44166666666666, 96.44166666666666, 96.43333333333334, 96.43333333333334, 96.39166666666667, 96.39166666666667, 96.40833333333333, 96.40833333333333, 96.50833333333334, 96.50833333333334, 96.475, 96.475, 96.50833333333334, 96.50833333333334, 96.40833333333333, 96.40833333333333, 96.50833333333334, 96.50833333333334, 96.55833333333334, 96.55833333333334, 96.56666666666666, 96.56666666666666, 96.50833333333334, 96.50833333333334, 96.5, 96.5, 96.525, 96.525, 96.59166666666667, 96.59166666666667, 96.65833333333333, 96.65833333333333, 96.56666666666666, 96.56666666666666, 96.675, 96.675, 96.625, 96.625, 96.75833333333334, 96.75833333333334, 96.65833333333333, 96.65833333333333, 96.70833333333333, 96.70833333333333, 96.69166666666666, 96.69166666666666, 96.7, 96.7, 96.73333333333333, 96.73333333333333, 96.74166666666666, 96.74166666666666]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.464, Test loss: 1.499, Test accuracy: 96.36
Final Round, Global train loss: 1.464, Global test loss: 2.108, Global test accuracy: 38.64
Average accuracy final 10 rounds: 96.3616666666667 

Average global accuracy final 10 rounds: 46.471666666666664 

1524.6150691509247
[1.1237099170684814, 2.247419834136963, 3.2586100101470947, 4.269800186157227, 5.3147242069244385, 6.35964822769165, 7.381011724472046, 8.402375221252441, 9.413494348526001, 10.42461347579956, 11.425821781158447, 12.427030086517334, 13.375718355178833, 14.324406623840332, 15.346272945404053, 16.368139266967773, 17.3959219455719, 18.423704624176025, 19.442440032958984, 20.461175441741943, 21.485575437545776, 22.50997543334961, 23.531955242156982, 24.553935050964355, 25.57338285446167, 26.592830657958984, 27.611788749694824, 28.630746841430664, 29.5301411151886, 30.429535388946533, 31.355377912521362, 32.28122043609619, 33.20434331893921, 34.12746620178223, 35.06032657623291, 35.993186950683594, 36.9923357963562, 37.99148464202881, 38.9556884765625, 39.91989231109619, 40.852076292037964, 41.784260272979736, 42.6871452331543, 43.59003019332886, 44.50999975204468, 45.4299693107605, 46.37087321281433, 47.311777114868164, 48.302448987960815, 49.29312086105347, 50.187259912490845, 51.08139896392822, 52.022390604019165, 52.96338224411011, 53.89874482154846, 54.834107398986816, 55.71828556060791, 56.602463722229004, 57.464491844177246, 58.32651996612549, 59.23224949836731, 60.13797903060913, 61.078527212142944, 62.01907539367676, 62.917649269104004, 63.81622314453125, 64.73911929130554, 65.66201543807983, 66.58857131004333, 67.51512718200684, 68.43669080734253, 69.35825443267822, 70.33325219154358, 71.30824995040894, 72.20299220085144, 73.09773445129395, 74.02513289451599, 74.95253133773804, 75.78264999389648, 76.61276865005493, 77.491219997406, 78.36967134475708, 79.30288863182068, 80.23610591888428, 81.17125487327576, 82.10640382766724, 83.09457516670227, 84.0827465057373, 85.12186765670776, 86.16098880767822, 87.16596484184265, 88.17094087600708, 89.16093850135803, 90.15093612670898, 91.12814474105835, 92.10535335540771, 93.14023685455322, 94.17512035369873, 95.18703198432922, 96.19894361495972, 97.19819188117981, 98.1974401473999, 99.18943810462952, 100.18143606185913, 101.20842146873474, 102.23540687561035, 103.20440411567688, 104.17340135574341, 105.19902348518372, 106.22464561462402, 107.26042985916138, 108.29621410369873, 109.33552813529968, 110.37484216690063, 111.31635451316833, 112.25786685943604, 113.24736166000366, 114.23685646057129, 115.26444149017334, 116.29202651977539, 117.20209956169128, 118.11217260360718, 119.06238532066345, 120.01259803771973, 120.95757341384888, 121.90254878997803, 122.81791472434998, 123.73328065872192, 124.5858039855957, 125.43832731246948, 126.37866234779358, 127.31899738311768, 128.23468208312988, 129.1503667831421, 130.03357434272766, 130.91678190231323, 131.85106205940247, 132.7853422164917, 133.74390649795532, 134.70247077941895, 135.58742690086365, 136.47238302230835, 137.33014798164368, 138.187912940979, 139.1160762310028, 140.0442395210266, 140.92171239852905, 141.7991852760315, 142.7260148525238, 143.6528444290161, 144.5918412208557, 145.5308380126953, 146.47241020202637, 147.41398239135742, 148.32672238349915, 149.23946237564087, 150.1359100341797, 151.0323576927185, 152.00292563438416, 152.9734935760498, 153.89040064811707, 154.80730772018433, 155.69773650169373, 156.58816528320312, 157.5205430984497, 158.4529209136963, 159.38067197799683, 160.30842304229736, 161.22243547439575, 162.13644790649414, 163.05277156829834, 163.96909523010254, 164.8732123374939, 165.77732944488525, 166.72855377197266, 167.67977809906006, 168.60498976707458, 169.5302014350891, 170.48235392570496, 171.4345064163208, 172.36342525482178, 173.29234409332275, 174.19560503959656, 175.09886598587036, 175.99944734573364, 176.90002870559692, 177.85841250419617, 178.8167963027954, 179.75869584083557, 180.70059537887573, 181.64935088157654, 182.59810638427734, 183.56088280677795, 184.52365922927856, 185.4478244781494, 186.37198972702026, 187.2255940437317, 188.07919836044312, 189.01388359069824, 189.94856882095337, 191.54122591018677, 193.13388299942017]
[37.391666666666666, 37.391666666666666, 53.1, 53.1, 72.86666666666666, 72.86666666666666, 77.175, 77.175, 79.00833333333334, 79.00833333333334, 83.51666666666667, 83.51666666666667, 82.50833333333334, 82.50833333333334, 83.225, 83.225, 86.025, 86.025, 87.24166666666666, 87.24166666666666, 84.825, 84.825, 90.125, 90.125, 91.65, 91.65, 91.775, 91.775, 91.875, 91.875, 91.93333333333334, 91.93333333333334, 91.89166666666667, 91.89166666666667, 91.90833333333333, 91.90833333333333, 91.95833333333333, 91.95833333333333, 91.98333333333333, 91.98333333333333, 91.975, 91.975, 91.96666666666667, 91.96666666666667, 91.94166666666666, 91.94166666666666, 91.95, 91.95, 91.95, 91.95, 91.975, 91.975, 93.43333333333334, 93.43333333333334, 93.43333333333334, 93.43333333333334, 93.43333333333334, 93.43333333333334, 95.525, 95.525, 96.11666666666666, 96.11666666666666, 96.13333333333334, 96.13333333333334, 96.15, 96.15, 96.15, 96.15, 96.25, 96.25, 96.275, 96.275, 96.25, 96.25, 96.25833333333334, 96.25833333333334, 96.275, 96.275, 96.25, 96.25, 96.23333333333333, 96.23333333333333, 96.275, 96.275, 96.275, 96.275, 96.275, 96.275, 96.275, 96.275, 96.29166666666667, 96.29166666666667, 96.275, 96.275, 96.28333333333333, 96.28333333333333, 96.26666666666667, 96.26666666666667, 96.275, 96.275, 96.24166666666666, 96.24166666666666, 96.24166666666666, 96.24166666666666, 96.25, 96.25, 96.25, 96.25, 96.25833333333334, 96.25833333333334, 96.26666666666667, 96.26666666666667, 96.26666666666667, 96.26666666666667, 96.275, 96.275, 96.28333333333333, 96.28333333333333, 96.275, 96.275, 96.29166666666667, 96.29166666666667, 96.3, 96.3, 96.30833333333334, 96.30833333333334, 96.28333333333333, 96.28333333333333, 96.28333333333333, 96.28333333333333, 96.28333333333333, 96.28333333333333, 96.29166666666667, 96.29166666666667, 96.29166666666667, 96.29166666666667, 96.3, 96.3, 96.3, 96.3, 96.3, 96.3, 96.30833333333334, 96.30833333333334, 96.3, 96.3, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.3, 96.3, 96.30833333333334, 96.30833333333334, 96.31666666666666, 96.31666666666666, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.325, 96.325, 96.33333333333333, 96.33333333333333, 96.35, 96.35, 96.35833333333333, 96.35833333333333, 96.35, 96.35, 96.35, 96.35, 96.35, 96.35, 96.35833333333333, 96.35833333333333, 96.36666666666666, 96.36666666666666, 96.35, 96.35, 96.34166666666667, 96.34166666666667, 96.34166666666667, 96.34166666666667, 96.35833333333333, 96.35833333333333, 96.36666666666666, 96.36666666666666, 96.375, 96.375, 96.36666666666666, 96.36666666666666, 96.36666666666666, 96.36666666666666, 96.375, 96.375, 96.375, 96.375, 96.35833333333333, 96.35833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.570, Test loss: 1.608, Test accuracy: 85.33
Final Round, Global train loss: 1.570, Global test loss: 1.606, Global test accuracy: 85.55
Average accuracy final 10 rounds: 85.24333333333334 

Average global accuracy final 10 rounds: 85.4975 

1605.348349571228
[1.242415428161621, 2.484830856323242, 3.603156566619873, 4.721482276916504, 5.810988903045654, 6.900495529174805, 7.931682825088501, 8.962870121002197, 10.016847372055054, 11.07082462310791, 12.136815547943115, 13.20280647277832, 14.295224905014038, 15.387643337249756, 16.54142737388611, 17.69521141052246, 18.785089015960693, 19.874966621398926, 20.927483320236206, 21.980000019073486, 23.06458854675293, 24.149177074432373, 25.216465950012207, 26.28375482559204, 27.37928295135498, 28.47481107711792, 29.594972133636475, 30.71513319015503, 31.79051685333252, 32.86590051651001, 33.971585750579834, 35.07727098464966, 36.19659447669983, 37.31591796875, 38.33758521080017, 39.35925245285034, 40.56384587287903, 41.768439292907715, 42.914456844329834, 44.06047439575195, 45.16553997993469, 46.27060556411743, 47.37259650230408, 48.47458744049072, 49.61577320098877, 50.756958961486816, 51.87501907348633, 52.99307918548584, 54.04658770561218, 55.100096225738525, 56.22385549545288, 57.347614765167236, 58.43891668319702, 59.53021860122681, 60.55102753639221, 61.57183647155762, 62.66304159164429, 63.75424671173096, 64.82861137390137, 65.90297603607178, 66.97163462638855, 68.04029321670532, 69.15767359733582, 70.27505397796631, 71.37370228767395, 72.47235059738159, 73.54721212387085, 74.62207365036011, 75.7266457080841, 76.8312177658081, 77.91070032119751, 78.99018287658691, 80.0325345993042, 81.07488632202148, 82.12178444862366, 83.16868257522583, 84.22765779495239, 85.28663301467896, 86.35757565498352, 87.42851829528809, 88.48365902900696, 89.53879976272583, 90.6284646987915, 91.71812963485718, 92.7753496170044, 93.83256959915161, 94.9044623374939, 95.97635507583618, 97.10184240341187, 98.22732973098755, 99.29804134368896, 100.36875295639038, 101.43828749656677, 102.50782203674316, 103.57378816604614, 104.63975429534912, 105.69781970977783, 106.75588512420654, 107.82916331291199, 108.90244150161743, 109.97862362861633, 111.05480575561523, 112.09947776794434, 113.14414978027344, 114.22291827201843, 115.30168676376343, 116.40596652030945, 117.51024627685547, 118.58746695518494, 119.6646876335144, 120.73929977416992, 121.81391191482544, 122.91666889190674, 124.01942586898804, 125.10109972953796, 126.18277359008789, 127.2942168712616, 128.4056601524353, 129.51629424095154, 130.62692832946777, 131.76059365272522, 132.89425897598267, 134.0053060054779, 135.11635303497314, 136.24016571044922, 137.3639783859253, 138.4528453350067, 139.54171228408813, 140.6313018798828, 141.7208914756775, 142.82001209259033, 143.91913270950317, 145.05390763282776, 146.18868255615234, 147.3803265094757, 148.57197046279907, 149.70656776428223, 150.84116506576538, 151.96927905082703, 153.09739303588867, 154.25601744651794, 155.41464185714722, 156.50109481811523, 157.58754777908325, 158.70742630958557, 159.8273048400879, 160.9013695716858, 161.9754343032837, 162.94607520103455, 163.9167160987854, 164.8377649784088, 165.75881385803223, 166.69846773147583, 167.63812160491943, 168.5995876789093, 169.56105375289917, 170.4879720211029, 171.41489028930664, 172.3410336971283, 173.26717710494995, 174.17607498168945, 175.08497285842896, 176.00834131240845, 176.93170976638794, 177.9068193435669, 178.88192892074585, 179.83518075942993, 180.788432598114, 181.73404097557068, 182.67964935302734, 183.64393210411072, 184.6082148551941, 185.4982008934021, 186.3881869316101, 187.33253121376038, 188.27687549591064, 189.27239084243774, 190.26790618896484, 191.2391390800476, 192.21037197113037, 193.16237497329712, 194.11437797546387, 195.06477117538452, 196.01516437530518, 196.9874415397644, 197.95971870422363, 198.8941991329193, 199.828679561615, 200.76248025894165, 201.6962809562683, 202.67703247070312, 203.65778398513794, 204.5962781906128, 205.53477239608765, 206.447039604187, 207.35930681228638, 208.28994798660278, 209.2205891609192, 210.17447209358215, 211.12835502624512, 212.69323134422302, 214.25810766220093]
[22.233333333333334, 22.233333333333334, 32.858333333333334, 32.858333333333334, 30.916666666666668, 30.916666666666668, 39.35, 39.35, 49.3, 49.3, 55.141666666666666, 55.141666666666666, 64.18333333333334, 64.18333333333334, 68.29166666666667, 68.29166666666667, 73.18333333333334, 73.18333333333334, 73.99166666666666, 73.99166666666666, 76.78333333333333, 76.78333333333333, 77.0, 77.0, 79.65, 79.65, 79.73333333333333, 79.73333333333333, 81.29166666666667, 81.29166666666667, 81.66666666666667, 81.66666666666667, 82.04166666666667, 82.04166666666667, 82.31666666666666, 82.31666666666666, 82.49166666666666, 82.49166666666666, 82.48333333333333, 82.48333333333333, 82.7, 82.7, 82.8, 82.8, 83.025, 83.025, 83.21666666666667, 83.21666666666667, 83.26666666666667, 83.26666666666667, 83.33333333333333, 83.33333333333333, 83.35833333333333, 83.35833333333333, 83.45, 83.45, 83.575, 83.575, 83.59166666666667, 83.59166666666667, 83.73333333333333, 83.73333333333333, 83.7, 83.7, 83.775, 83.775, 83.8, 83.8, 83.88333333333334, 83.88333333333334, 83.89166666666667, 83.89166666666667, 83.99166666666666, 83.99166666666666, 84.01666666666667, 84.01666666666667, 83.98333333333333, 83.98333333333333, 84.05833333333334, 84.05833333333334, 84.15, 84.15, 84.15, 84.15, 84.15833333333333, 84.15833333333333, 84.25, 84.25, 84.325, 84.325, 84.38333333333334, 84.38333333333334, 84.41666666666667, 84.41666666666667, 84.425, 84.425, 84.475, 84.475, 84.40833333333333, 84.40833333333333, 84.41666666666667, 84.41666666666667, 84.43333333333334, 84.43333333333334, 84.5, 84.5, 84.525, 84.525, 84.54166666666667, 84.54166666666667, 84.51666666666667, 84.51666666666667, 84.55, 84.55, 84.525, 84.525, 84.575, 84.575, 84.575, 84.575, 84.60833333333333, 84.60833333333333, 84.61666666666666, 84.61666666666666, 84.625, 84.625, 84.69166666666666, 84.69166666666666, 84.71666666666667, 84.71666666666667, 84.775, 84.775, 84.775, 84.775, 84.83333333333333, 84.83333333333333, 84.83333333333333, 84.83333333333333, 84.825, 84.825, 84.85833333333333, 84.85833333333333, 84.86666666666666, 84.86666666666666, 84.93333333333334, 84.93333333333334, 84.9, 84.9, 84.94166666666666, 84.94166666666666, 84.975, 84.975, 84.98333333333333, 84.98333333333333, 85.0, 85.0, 85.01666666666667, 85.01666666666667, 84.99166666666666, 84.99166666666666, 85.01666666666667, 85.01666666666667, 85.05, 85.05, 85.03333333333333, 85.03333333333333, 84.96666666666667, 84.96666666666667, 85.025, 85.025, 85.06666666666666, 85.06666666666666, 85.04166666666667, 85.04166666666667, 85.08333333333333, 85.08333333333333, 85.125, 85.125, 85.08333333333333, 85.08333333333333, 85.13333333333334, 85.13333333333334, 85.15, 85.15, 85.225, 85.225, 85.21666666666667, 85.21666666666667, 85.19166666666666, 85.19166666666666, 85.29166666666667, 85.29166666666667, 85.26666666666667, 85.26666666666667, 85.3, 85.3, 85.31666666666666, 85.31666666666666, 85.34166666666667, 85.34166666666667, 85.33333333333333, 85.33333333333333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.591, Test loss: 1.628, Test accuracy: 83.47
Average accuracy final 10 rounds: 83.48333333333333 

1173.9678008556366
[1.036407232284546, 2.072814464569092, 2.9814670085906982, 3.8901195526123047, 4.807952404022217, 5.725785255432129, 6.647422790527344, 7.569060325622559, 8.506988525390625, 9.444916725158691, 10.352632999420166, 11.26034927368164, 12.136494636535645, 13.012639999389648, 13.907082080841064, 14.80152416229248, 15.681779861450195, 16.56203556060791, 17.401557207107544, 18.241078853607178, 19.1038339138031, 19.966588973999023, 20.84470248222351, 21.722815990447998, 22.64738440513611, 23.57195281982422, 24.472119331359863, 25.372285842895508, 26.229422330856323, 27.08655881881714, 27.93818473815918, 28.78981065750122, 29.626707315444946, 30.463603973388672, 31.319347143173218, 32.175090312957764, 33.1162531375885, 34.05741596221924, 34.96039915084839, 35.86338233947754, 36.78091216087341, 37.69844198226929, 38.640042304992676, 39.581642627716064, 40.541619062423706, 41.50159549713135, 42.421244382858276, 43.340893268585205, 44.24582648277283, 45.15075969696045, 46.04633355140686, 46.94190740585327, 47.78499460220337, 48.62808179855347, 49.506322145462036, 50.384562492370605, 51.237563610076904, 52.0905647277832, 52.93059945106506, 53.770634174346924, 54.628334045410156, 55.48603391647339, 56.36999487876892, 57.25395584106445, 58.133854389190674, 59.013752937316895, 59.86833667755127, 60.722920417785645, 61.6024444103241, 62.48196840286255, 63.41206932067871, 64.34217023849487, 65.275461435318, 66.20875263214111, 67.12831664085388, 68.04788064956665, 68.89013266563416, 69.73238468170166, 70.5975022315979, 71.46261978149414, 72.3318099975586, 73.20100021362305, 74.12316250801086, 75.04532480239868, 75.94560194015503, 76.84587907791138, 77.76268577575684, 78.6794924736023, 79.62547421455383, 80.57145595550537, 81.51925778388977, 82.46705961227417, 83.36831259727478, 84.26956558227539, 85.1616575717926, 86.05374956130981, 86.98802375793457, 87.92229795455933, 88.87075090408325, 89.81920385360718, 90.77669477462769, 91.7341856956482, 92.66269707679749, 93.59120845794678, 94.49361062049866, 95.39601278305054, 96.30035710334778, 97.20470142364502, 98.12233209609985, 99.03996276855469, 99.9680061340332, 100.89604949951172, 101.82895755767822, 102.76186561584473, 103.65573382377625, 104.54960203170776, 105.46274709701538, 106.375892162323, 107.30980253219604, 108.24371290206909, 109.15522623062134, 110.06673955917358, 110.99125576019287, 111.91577196121216, 112.81169867515564, 113.70762538909912, 114.60237288475037, 115.49712038040161, 116.41242527961731, 117.32773017883301, 118.27136445045471, 119.21499872207642, 120.13726258277893, 121.05952644348145, 121.985680103302, 122.91183376312256, 123.82158780097961, 124.73134183883667, 125.60624265670776, 126.48114347457886, 127.41258955001831, 128.34403562545776, 129.25863695144653, 130.1732382774353, 131.05972361564636, 131.94620895385742, 132.83884024620056, 133.7314715385437, 134.61763787269592, 135.50380420684814, 136.44176959991455, 137.37973499298096, 138.33668637275696, 139.29363775253296, 140.19401574134827, 141.09439373016357, 142.00244331359863, 142.9104928970337, 143.8193507194519, 144.72820854187012, 145.5979926586151, 146.4677767753601, 147.4110038280487, 148.3542308807373, 149.27362704277039, 150.19302320480347, 151.0777144432068, 151.9624056816101, 152.90560293197632, 153.84880018234253, 154.76662373542786, 155.68444728851318, 156.57060432434082, 157.45676136016846, 158.39166378974915, 159.32656621932983, 160.25351858139038, 161.18047094345093, 162.0849952697754, 162.98951959609985, 163.93476581573486, 164.88001203536987, 165.7898154258728, 166.69961881637573, 167.63569617271423, 168.57177352905273, 169.48967838287354, 170.40758323669434, 171.32127213478088, 172.23496103286743, 173.1025514602661, 173.9701418876648, 174.82796549797058, 175.68578910827637, 176.54210782051086, 177.39842653274536, 178.24543118476868, 179.092435836792, 179.9244544506073, 180.7564730644226, 182.16388463974, 183.57129621505737]
[14.116666666666667, 14.116666666666667, 20.1, 20.1, 30.35, 30.35, 39.28333333333333, 39.28333333333333, 38.18333333333333, 38.18333333333333, 34.95, 34.95, 41.425, 41.425, 51.65, 51.65, 54.233333333333334, 54.233333333333334, 57.19166666666667, 57.19166666666667, 60.56666666666667, 60.56666666666667, 62.94166666666667, 62.94166666666667, 65.55, 65.55, 68.08333333333333, 68.08333333333333, 70.675, 70.675, 71.51666666666667, 71.51666666666667, 72.5, 72.5, 73.61666666666666, 73.61666666666666, 74.10833333333333, 74.10833333333333, 74.74166666666666, 74.74166666666666, 76.175, 76.175, 77.80833333333334, 77.80833333333334, 78.64166666666667, 78.64166666666667, 79.40833333333333, 79.40833333333333, 79.91666666666667, 79.91666666666667, 80.09166666666667, 80.09166666666667, 80.075, 80.075, 80.40833333333333, 80.40833333333333, 80.55, 80.55, 80.66666666666667, 80.66666666666667, 81.20833333333333, 81.20833333333333, 81.175, 81.175, 81.24166666666666, 81.24166666666666, 81.54166666666667, 81.54166666666667, 81.71666666666667, 81.71666666666667, 81.91666666666667, 81.91666666666667, 82.34166666666667, 82.34166666666667, 82.25, 82.25, 82.3, 82.3, 82.53333333333333, 82.53333333333333, 82.74166666666666, 82.74166666666666, 82.675, 82.675, 82.65, 82.65, 82.69166666666666, 82.69166666666666, 82.70833333333333, 82.70833333333333, 82.675, 82.675, 82.64166666666667, 82.64166666666667, 82.65, 82.65, 82.66666666666667, 82.66666666666667, 82.78333333333333, 82.78333333333333, 82.78333333333333, 82.78333333333333, 82.8, 82.8, 82.73333333333333, 82.73333333333333, 82.825, 82.825, 82.83333333333333, 82.83333333333333, 82.93333333333334, 82.93333333333334, 82.775, 82.775, 82.91666666666667, 82.91666666666667, 82.98333333333333, 82.98333333333333, 82.95, 82.95, 82.89166666666667, 82.89166666666667, 82.84166666666667, 82.84166666666667, 82.84166666666667, 82.84166666666667, 82.88333333333334, 82.88333333333334, 82.85833333333333, 82.85833333333333, 82.975, 82.975, 82.94166666666666, 82.94166666666666, 82.91666666666667, 82.91666666666667, 82.99166666666666, 82.99166666666666, 82.9, 82.9, 83.09166666666667, 83.09166666666667, 83.09166666666667, 83.09166666666667, 82.80833333333334, 82.80833333333334, 82.68333333333334, 82.68333333333334, 82.88333333333334, 82.88333333333334, 82.80833333333334, 82.80833333333334, 83.10833333333333, 83.10833333333333, 83.15833333333333, 83.15833333333333, 83.13333333333334, 83.13333333333334, 83.13333333333334, 83.13333333333334, 83.28333333333333, 83.28333333333333, 83.26666666666667, 83.26666666666667, 83.33333333333333, 83.33333333333333, 83.225, 83.225, 83.2, 83.2, 83.30833333333334, 83.30833333333334, 83.325, 83.325, 83.33333333333333, 83.33333333333333, 83.375, 83.375, 83.425, 83.425, 83.54166666666667, 83.54166666666667, 83.575, 83.575, 83.49166666666666, 83.49166666666666, 83.45833333333333, 83.45833333333333, 83.475, 83.475, 83.54166666666667, 83.54166666666667, 83.44166666666666, 83.44166666666666, 83.38333333333334, 83.38333333333334, 83.45833333333333, 83.45833333333333, 83.46666666666667, 83.46666666666667, 83.46666666666667, 83.46666666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.485, Test loss: 1.527, Test accuracy: 93.51
Average accuracy final 10 rounds: 93.56666666666668 

1278.5864372253418
[1.141479730606079, 2.282959461212158, 3.336573600769043, 4.390187740325928, 5.470189094543457, 6.550190448760986, 7.609613418579102, 8.669036388397217, 9.731897115707397, 10.794757843017578, 11.859864950180054, 12.92497205734253, 13.98392105102539, 15.042870044708252, 16.14342164993286, 17.24397325515747, 18.327367067337036, 19.4107608795166, 20.489559412002563, 21.568357944488525, 22.643165349960327, 23.71797275543213, 24.796759366989136, 25.875545978546143, 26.96225118637085, 28.048956394195557, 29.150367975234985, 30.251779556274414, 31.30128312110901, 32.3507866859436, 33.4213547706604, 34.4919228553772, 35.59115743637085, 36.6903920173645, 37.77765488624573, 38.86491775512695, 39.922980070114136, 40.98104238510132, 42.04857063293457, 43.11609888076782, 44.20203995704651, 45.287981033325195, 46.386096477508545, 47.484211921691895, 48.562817096710205, 49.641422271728516, 50.71016836166382, 51.77891445159912, 52.844873666763306, 53.91083288192749, 54.99179196357727, 56.07275104522705, 57.15727615356445, 58.241801261901855, 59.30686378479004, 60.37192630767822, 61.42402625083923, 62.476126194000244, 63.547133684158325, 64.6181411743164, 65.70809507369995, 66.7980489730835, 67.86587834358215, 68.93370771408081, 69.99720907211304, 71.06071043014526, 72.12875509262085, 73.19679975509644, 74.26131701469421, 75.32583427429199, 76.42665982246399, 77.52748537063599, 78.59618663787842, 79.66488790512085, 80.74104118347168, 81.81719446182251, 82.89230942726135, 83.9674243927002, 85.04675936698914, 86.12609434127808, 87.21605253219604, 88.30601072311401, 89.38300943374634, 90.46000814437866, 91.51748704910278, 92.5749659538269, 93.64782857894897, 94.72069120407104, 95.80923986434937, 96.89778852462769, 97.97176885604858, 99.04574918746948, 100.1098141670227, 101.17387914657593, 102.23041152954102, 103.2869439125061, 104.37009644508362, 105.45324897766113, 106.55647921562195, 107.65970945358276, 108.72940683364868, 109.7991042137146, 110.87060356140137, 111.94210290908813, 113.02309155464172, 114.10408020019531, 115.18488764762878, 116.26569509506226, 117.35507249832153, 118.44444990158081, 119.52968454360962, 120.61491918563843, 121.68227052688599, 122.74962186813354, 123.89645767211914, 125.04329347610474, 126.12683153152466, 127.21036958694458, 128.26941204071045, 129.32845449447632, 130.40272331237793, 131.47699213027954, 132.54271697998047, 133.6084418296814, 134.68290972709656, 135.75737762451172, 136.8465075492859, 137.93563747406006, 138.99490237236023, 140.0541672706604, 141.12383484840393, 142.19350242614746, 143.27663731575012, 144.35977220535278, 145.42732787132263, 146.49488353729248, 147.5747480392456, 148.65461254119873, 149.72716760635376, 150.7997226715088, 151.8513777256012, 152.9030327796936, 153.98328113555908, 155.06352949142456, 156.1641161441803, 157.26470279693604, 158.31296396255493, 159.36122512817383, 160.42738795280457, 161.4935507774353, 162.5531346797943, 163.61271858215332, 164.69853949546814, 165.78436040878296, 166.86464953422546, 167.94493865966797, 168.99344611167908, 170.04195356369019, 171.1327109336853, 172.22346830368042, 173.33432126045227, 174.44517421722412, 175.50918865203857, 176.57320308685303, 177.6617612838745, 178.750319480896, 179.84683060646057, 180.94334173202515, 182.00613594055176, 183.06893014907837, 184.1463747024536, 185.22381925582886, 186.30438351631165, 187.38494777679443, 188.45436453819275, 189.52378129959106, 190.6074903011322, 191.69119930267334, 192.75178837776184, 193.81237745285034, 194.91301250457764, 196.01364755630493, 197.11449360847473, 198.21533966064453, 199.2583785057068, 200.30141735076904, 201.39147639274597, 202.4815354347229, 203.5881199836731, 204.6947045326233, 205.77776169776917, 206.86081886291504, 207.91627478599548, 208.97173070907593, 210.05208373069763, 211.13243675231934, 212.19965291023254, 213.26686906814575, 214.36888670921326, 215.47090435028076, 216.89129090309143, 218.3116774559021]
[29.083333333333332, 29.083333333333332, 41.325, 41.325, 38.95, 38.95, 56.7, 56.7, 71.575, 71.575, 78.20833333333333, 78.20833333333333, 80.46666666666667, 80.46666666666667, 81.93333333333334, 81.93333333333334, 82.95, 82.95, 83.88333333333334, 83.88333333333334, 84.39166666666667, 84.39166666666667, 84.43333333333334, 84.43333333333334, 84.93333333333334, 84.93333333333334, 85.81666666666666, 85.81666666666666, 86.75, 86.75, 87.39166666666667, 87.39166666666667, 88.40833333333333, 88.40833333333333, 88.9, 88.9, 88.9, 88.9, 89.375, 89.375, 89.45833333333333, 89.45833333333333, 89.90833333333333, 89.90833333333333, 90.30833333333334, 90.30833333333334, 90.68333333333334, 90.68333333333334, 91.525, 91.525, 91.79166666666667, 91.79166666666667, 91.975, 91.975, 92.09166666666667, 92.09166666666667, 92.225, 92.225, 92.35, 92.35, 92.44166666666666, 92.44166666666666, 92.66666666666667, 92.66666666666667, 92.725, 92.725, 92.75, 92.75, 92.725, 92.725, 92.83333333333333, 92.83333333333333, 92.925, 92.925, 92.85833333333333, 92.85833333333333, 92.85833333333333, 92.85833333333333, 92.91666666666667, 92.91666666666667, 92.89166666666667, 92.89166666666667, 92.95, 92.95, 92.95833333333333, 92.95833333333333, 92.99166666666666, 92.99166666666666, 92.94166666666666, 92.94166666666666, 93.15833333333333, 93.15833333333333, 93.2, 93.2, 93.05, 93.05, 93.11666666666666, 93.11666666666666, 93.3, 93.3, 93.31666666666666, 93.31666666666666, 93.23333333333333, 93.23333333333333, 93.25833333333334, 93.25833333333334, 93.325, 93.325, 93.33333333333333, 93.33333333333333, 93.28333333333333, 93.28333333333333, 93.23333333333333, 93.23333333333333, 93.21666666666667, 93.21666666666667, 93.3, 93.3, 93.25833333333334, 93.25833333333334, 93.24166666666666, 93.24166666666666, 93.30833333333334, 93.30833333333334, 93.31666666666666, 93.31666666666666, 93.375, 93.375, 93.35, 93.35, 93.41666666666667, 93.41666666666667, 93.4, 93.4, 93.33333333333333, 93.33333333333333, 93.39166666666667, 93.39166666666667, 93.34166666666667, 93.34166666666667, 93.41666666666667, 93.41666666666667, 93.475, 93.475, 93.39166666666667, 93.39166666666667, 93.39166666666667, 93.39166666666667, 93.43333333333334, 93.43333333333334, 93.48333333333333, 93.48333333333333, 93.575, 93.575, 93.50833333333334, 93.50833333333334, 93.43333333333334, 93.43333333333334, 93.46666666666667, 93.46666666666667, 93.49166666666666, 93.49166666666666, 93.48333333333333, 93.48333333333333, 93.49166666666666, 93.49166666666666, 93.48333333333333, 93.48333333333333, 93.46666666666667, 93.46666666666667, 93.54166666666667, 93.54166666666667, 93.55833333333334, 93.55833333333334, 93.54166666666667, 93.54166666666667, 93.54166666666667, 93.54166666666667, 93.525, 93.525, 93.59166666666667, 93.59166666666667, 93.6, 93.6, 93.525, 93.525, 93.53333333333333, 93.53333333333333, 93.56666666666666, 93.56666666666666, 93.54166666666667, 93.54166666666667, 93.55833333333334, 93.55833333333334, 93.61666666666666, 93.61666666666666, 93.56666666666666, 93.56666666666666, 93.56666666666666, 93.56666666666666, 93.50833333333334, 93.50833333333334]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.481, Test loss: 1.588, Test accuracy: 87.72
Average accuracy final 10 rounds: 87.69583333333331 

1168.6530883312225
[1.0629377365112305, 2.125875473022461, 3.157031774520874, 4.188188076019287, 5.101267337799072, 6.014346599578857, 6.879811763763428, 7.745276927947998, 8.597429037094116, 9.449581146240234, 10.331354856491089, 11.213128566741943, 12.115550756454468, 13.017972946166992, 13.911251544952393, 14.804530143737793, 15.678631782531738, 16.552733421325684, 17.433658361434937, 18.31458330154419, 19.16689372062683, 20.019204139709473, 21.039144277572632, 22.05908441543579, 22.92207670211792, 23.78506898880005, 24.631380081176758, 25.477691173553467, 26.3708393573761, 27.26398754119873, 28.14904761314392, 29.03410768508911, 29.9437255859375, 30.85334348678589, 31.734910488128662, 32.616477489471436, 33.51809501647949, 34.41971254348755, 35.34322905540466, 36.26674556732178, 37.14359498023987, 38.02044439315796, 38.92005896568298, 39.81967353820801, 40.69687247276306, 41.574071407318115, 42.467939376831055, 43.361807346343994, 44.227253675460815, 45.09270000457764, 45.98376536369324, 46.87483072280884, 47.7633330821991, 48.651835441589355, 49.52548789978027, 50.39914035797119, 51.29324436187744, 52.18734836578369, 53.04840302467346, 53.90945768356323, 54.773868560791016, 55.6382794380188, 56.53935170173645, 57.4404239654541, 58.343050479888916, 59.24567699432373, 60.120527505874634, 60.99537801742554, 61.83992004394531, 62.68446207046509, 63.57043766975403, 64.45641326904297, 65.32245755195618, 66.18850183486938, 67.08249521255493, 67.97648859024048, 68.82177972793579, 69.6670708656311, 70.56206560134888, 71.45706033706665, 72.34211754798889, 73.22717475891113, 74.09954309463501, 74.97191143035889, 75.84772849082947, 76.72354555130005, 77.58748412132263, 78.45142269134521, 79.33748388290405, 80.22354507446289, 81.11189389228821, 82.00024271011353, 82.85153484344482, 83.70282697677612, 84.60730075836182, 85.51177453994751, 86.400062084198, 87.28834962844849, 88.18139982223511, 89.07445001602173, 89.9303331375122, 90.78621625900269, 91.67933773994446, 92.57245922088623, 93.45461821556091, 94.3367772102356, 95.2244701385498, 96.11216306686401, 96.98966217041016, 97.8671612739563, 98.71683812141418, 99.56651496887207, 100.4685742855072, 101.37063360214233, 102.29738450050354, 103.22413539886475, 104.11870813369751, 105.01328086853027, 105.90978980064392, 106.80629873275757, 107.69778394699097, 108.58926916122437, 109.50030064582825, 110.41133213043213, 111.30563616752625, 112.19994020462036, 113.09337830543518, 113.98681640625, 114.86404967308044, 115.74128293991089, 116.63815689086914, 117.53503084182739, 118.44534993171692, 119.35566902160645, 120.30633282661438, 121.25699663162231, 122.17138576507568, 123.08577489852905, 123.95436978340149, 124.82296466827393, 125.73648953437805, 126.65001440048218, 127.57128262519836, 128.49255084991455, 129.4136152267456, 130.33467960357666, 131.25873923301697, 132.18279886245728, 133.0565025806427, 133.93020629882812, 134.83251309394836, 135.7348198890686, 136.63088989257812, 137.52695989608765, 138.45298981666565, 139.37901973724365, 140.2890317440033, 141.19904375076294, 142.1012420654297, 143.00344038009644, 143.90755534172058, 144.81167030334473, 145.72507333755493, 146.63847637176514, 147.53899550437927, 148.4395146369934, 149.3459939956665, 150.2524733543396, 151.16201639175415, 152.0715594291687, 152.9642460346222, 153.85693264007568, 154.74051356315613, 155.62409448623657, 156.52460193634033, 157.4251093864441, 158.32718420028687, 159.22925901412964, 160.1249589920044, 161.02065896987915, 161.9014070034027, 162.78215503692627, 163.6913890838623, 164.60062313079834, 165.50022721290588, 166.39983129501343, 167.29618287086487, 168.1925344467163, 169.0923933982849, 169.99225234985352, 170.8610327243805, 171.72981309890747, 172.58525133132935, 173.44068956375122, 174.33285689353943, 175.22502422332764, 176.14188957214355, 177.05875492095947, 177.94748163223267, 178.83620834350586, 180.29710960388184, 181.7580108642578]
[23.266666666666666, 23.266666666666666, 45.208333333333336, 45.208333333333336, 48.858333333333334, 48.858333333333334, 46.95, 46.95, 57.108333333333334, 57.108333333333334, 64.63333333333334, 64.63333333333334, 69.55833333333334, 69.55833333333334, 74.45, 74.45, 75.86666666666666, 75.86666666666666, 80.04166666666667, 80.04166666666667, 83.83333333333333, 83.83333333333333, 84.86666666666666, 84.86666666666666, 86.08333333333333, 86.08333333333333, 86.66666666666667, 86.66666666666667, 86.79166666666667, 86.79166666666667, 86.89166666666667, 86.89166666666667, 86.925, 86.925, 87.3, 87.3, 87.39166666666667, 87.39166666666667, 87.64166666666667, 87.64166666666667, 87.58333333333333, 87.58333333333333, 87.68333333333334, 87.68333333333334, 87.7, 87.7, 87.675, 87.675, 87.65833333333333, 87.65833333333333, 87.74166666666666, 87.74166666666666, 87.76666666666667, 87.76666666666667, 87.825, 87.825, 87.83333333333333, 87.83333333333333, 87.83333333333333, 87.83333333333333, 87.75833333333334, 87.75833333333334, 87.70833333333333, 87.70833333333333, 87.675, 87.675, 87.75, 87.75, 87.69166666666666, 87.69166666666666, 87.64166666666667, 87.64166666666667, 87.70833333333333, 87.70833333333333, 87.71666666666667, 87.71666666666667, 87.70833333333333, 87.70833333333333, 87.66666666666667, 87.66666666666667, 87.73333333333333, 87.73333333333333, 87.675, 87.675, 87.68333333333334, 87.68333333333334, 87.75, 87.75, 87.73333333333333, 87.73333333333333, 87.725, 87.725, 87.725, 87.725, 87.69166666666666, 87.69166666666666, 87.73333333333333, 87.73333333333333, 87.74166666666666, 87.74166666666666, 87.7, 87.7, 87.75833333333334, 87.75833333333334, 87.70833333333333, 87.70833333333333, 87.725, 87.725, 87.675, 87.675, 87.725, 87.725, 87.73333333333333, 87.73333333333333, 87.7, 87.7, 87.70833333333333, 87.70833333333333, 87.7, 87.7, 87.7, 87.7, 87.70833333333333, 87.70833333333333, 87.69166666666666, 87.69166666666666, 87.73333333333333, 87.73333333333333, 87.69166666666666, 87.69166666666666, 87.71666666666667, 87.71666666666667, 87.71666666666667, 87.71666666666667, 87.725, 87.725, 87.75833333333334, 87.75833333333334, 87.75833333333334, 87.75833333333334, 87.725, 87.725, 87.71666666666667, 87.71666666666667, 87.75833333333334, 87.75833333333334, 87.76666666666667, 87.76666666666667, 87.8, 87.8, 87.8, 87.8, 87.825, 87.825, 87.79166666666667, 87.79166666666667, 87.775, 87.775, 87.71666666666667, 87.71666666666667, 87.725, 87.725, 87.75833333333334, 87.75833333333334, 87.74166666666666, 87.74166666666666, 87.71666666666667, 87.71666666666667, 87.66666666666667, 87.66666666666667, 87.65833333333333, 87.65833333333333, 87.70833333333333, 87.70833333333333, 87.68333333333334, 87.68333333333334, 87.725, 87.725, 87.75, 87.75, 87.73333333333333, 87.73333333333333, 87.70833333333333, 87.70833333333333, 87.69166666666666, 87.69166666666666, 87.68333333333334, 87.68333333333334, 87.69166666666666, 87.69166666666666, 87.68333333333334, 87.68333333333334, 87.69166666666666, 87.69166666666666, 87.69166666666666, 87.69166666666666, 87.7, 87.7, 87.68333333333334, 87.68333333333334, 87.71666666666667, 87.71666666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Final Round, Train loss: 1.107, Test loss: 1.856, Test accuracy: 74.92
Average accuracy final 10 rounds: 75.3675
1468.620432138443
[]
[30.958333333333332, 43.05833333333333, 58.34166666666667, 63.641666666666666, 68.21666666666667, 71.025, 72.24166666666666, 72.95833333333333, 73.125, 74.15833333333333, 76.45, 77.10833333333333, 77.73333333333333, 78.0, 78.53333333333333, 78.20833333333333, 78.35833333333333, 79.00833333333334, 78.86666666666666, 78.75, 78.60833333333333, 78.65, 78.34166666666667, 78.31666666666666, 78.09166666666667, 77.73333333333333, 77.60833333333333, 77.53333333333333, 77.36666666666666, 77.25833333333334, 78.525, 80.01666666666667, 81.10833333333333, 81.525, 81.275, 81.69166666666666, 82.15, 82.33333333333333, 82.76666666666667, 82.59166666666667, 82.475, 82.70833333333333, 82.40833333333333, 82.08333333333333, 81.89166666666667, 81.79166666666667, 81.66666666666667, 81.49166666666666, 81.39166666666667, 81.00833333333334, 81.03333333333333, 80.74166666666666, 80.65, 80.50833333333334, 80.225, 79.98333333333333, 79.875, 79.73333333333333, 79.55833333333334, 79.39166666666667, 79.18333333333334, 79.14166666666667, 78.74166666666666, 78.63333333333334, 78.65, 78.43333333333334, 78.23333333333333, 78.125, 78.025, 77.86666666666666, 77.68333333333334, 77.49166666666666, 77.49166666666666, 77.475, 77.18333333333334, 77.15833333333333, 77.1, 76.89166666666667, 76.76666666666667, 76.475, 76.33333333333333, 76.29166666666667, 76.25, 76.29166666666667, 76.13333333333334, 76.04166666666667, 75.9, 76.01666666666667, 76.0, 75.76666666666667, 75.71666666666667, 75.81666666666666, 75.5, 75.46666666666667, 75.44166666666666, 75.2, 75.30833333333334, 75.15, 75.03333333333333, 75.04166666666667, 74.925]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.624, Test loss: 1.649, Test accuracy: 81.97
Average accuracy final 10 rounds: 80.69333333333333
Average global accuracy final 10 rounds: 80.69333333333333
1028.0680482387543
[]
[13.6, 15.341666666666667, 15.916666666666666, 19.783333333333335, 19.4, 21.491666666666667, 22.975, 26.358333333333334, 30.416666666666668, 30.833333333333332, 34.65, 32.65833333333333, 31.45, 37.85, 38.6, 39.30833333333333, 30.308333333333334, 32.083333333333336, 36.94166666666667, 39.2, 35.625, 44.78333333333333, 52.05, 53.40833333333333, 51.525, 53.175, 54.858333333333334, 60.19166666666667, 59.983333333333334, 61.36666666666667, 61.825, 62.28333333333333, 60.69166666666667, 60.166666666666664, 60.358333333333334, 64.64166666666667, 63.891666666666666, 64.61666666666666, 66.3, 66.23333333333333, 66.55833333333334, 66.95, 66.80833333333334, 67.7, 67.875, 67.54166666666667, 67.74166666666666, 68.96666666666667, 68.61666666666666, 69.33333333333333, 71.475, 71.75833333333334, 72.20833333333333, 71.96666666666667, 73.225, 73.29166666666667, 74.15, 74.525, 74.65833333333333, 74.575, 76.6, 77.05833333333334, 77.29166666666667, 76.48333333333333, 76.90833333333333, 75.96666666666667, 76.11666666666666, 76.34166666666667, 76.9, 78.64166666666667, 77.56666666666666, 77.39166666666667, 77.96666666666667, 78.35833333333333, 78.35833333333333, 78.75, 79.1, 79.44166666666666, 79.49166666666666, 79.50833333333334, 79.45833333333333, 79.825, 79.75833333333334, 80.15833333333333, 79.63333333333334, 79.93333333333334, 80.175, 80.25, 80.30833333333334, 80.2, 80.5, 80.775, 80.50833333333334, 80.58333333333333, 80.74166666666666, 80.9, 80.51666666666667, 80.8, 80.69166666666666, 80.91666666666667, 81.975]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.34
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.29
Average accuracy final 10 rounds: 14.653333333333334 

Average global accuracy final 10 rounds: 14.9475 

1298.7192559242249
[1.2752339839935303, 2.4514479637145996, 3.6223623752593994, 4.828964710235596, 6.007100343704224, 7.18610954284668, 8.357584953308105, 9.531097412109375, 10.696064949035645, 11.869296312332153, 13.040042638778687, 14.191711187362671, 15.341038703918457, 16.479093313217163, 17.623348236083984, 18.75667929649353, 19.886895656585693, 21.031086921691895, 22.167495489120483, 23.32225275039673, 24.492369413375854, 25.646701097488403, 26.808394193649292, 27.979653358459473, 29.14018678665161, 30.31191921234131, 31.468493700027466, 32.616249561309814, 33.76846957206726, 34.92830276489258, 36.0817174911499, 37.24819350242615, 38.41660499572754, 39.588886976242065, 40.74758815765381, 41.92297291755676, 43.08507251739502, 44.26380372047424, 45.44936513900757, 46.62545347213745, 47.77557826042175, 48.94735884666443, 50.12294888496399, 51.29566812515259, 52.467864990234375, 53.64137268066406, 54.815509557724, 55.98544216156006, 57.150026082992554, 58.31712365150452, 59.481292724609375, 60.652607917785645, 61.831782579422, 62.98651146888733, 64.14041018486023, 65.33201932907104, 66.50806021690369, 67.6863784790039, 68.86763215065002, 70.04920434951782, 71.21628642082214, 72.3992612361908, 73.56542563438416, 74.74830746650696, 75.91281175613403, 77.07176423072815, 78.21932291984558, 79.40243196487427, 80.58926510810852, 81.766188621521, 82.93679451942444, 84.0978946685791, 85.1343309879303, 86.26290774345398, 87.38652443885803, 88.50321292877197, 89.6496012210846, 90.76776671409607, 91.87720489501953, 92.99452042579651, 94.10657334327698, 95.23175525665283, 96.3462917804718, 97.47474765777588, 98.60771083831787, 99.75276327133179, 100.91440558433533, 102.04828643798828, 103.18655562400818, 104.27490997314453, 105.39615821838379, 106.49297738075256, 107.59134554862976, 108.68209838867188, 109.61032962799072, 110.53246665000916, 111.44294285774231, 112.3552463054657, 113.28477931022644, 114.19517683982849, 115.71508312225342]
[7.858333333333333, 8.0, 7.983333333333333, 7.933333333333334, 8.05, 8.108333333333333, 8.108333333333333, 8.175, 8.258333333333333, 8.375, 8.516666666666667, 8.541666666666666, 8.575, 8.666666666666666, 8.775, 8.85, 8.933333333333334, 8.975, 9.033333333333333, 9.066666666666666, 9.133333333333333, 9.183333333333334, 9.25, 9.383333333333333, 9.458333333333334, 9.55, 9.633333333333333, 9.675, 9.741666666666667, 9.791666666666666, 9.858333333333333, 9.95, 9.991666666666667, 10.025, 10.033333333333333, 10.1, 10.125, 10.191666666666666, 10.241666666666667, 10.241666666666667, 10.258333333333333, 10.283333333333333, 10.325, 10.425, 10.533333333333333, 10.583333333333334, 10.708333333333334, 10.758333333333333, 10.85, 10.975, 11.025, 11.075, 11.183333333333334, 11.183333333333334, 11.241666666666667, 11.3, 11.366666666666667, 11.475, 11.541666666666666, 11.583333333333334, 11.708333333333334, 11.75, 11.833333333333334, 11.941666666666666, 12.058333333333334, 12.133333333333333, 12.183333333333334, 12.283333333333333, 12.341666666666667, 12.416666666666666, 12.533333333333333, 12.583333333333334, 12.708333333333334, 12.85, 12.991666666666667, 13.033333333333333, 13.091666666666667, 13.158333333333333, 13.25, 13.308333333333334, 13.441666666666666, 13.483333333333333, 13.575, 13.616666666666667, 13.683333333333334, 13.733333333333333, 13.766666666666667, 13.875, 13.975, 14.066666666666666, 14.15, 14.258333333333333, 14.408333333333333, 14.516666666666667, 14.666666666666666, 14.741666666666667, 14.841666666666667, 14.9, 14.975, 15.075, 15.341666666666667]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Final Round, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.89
Average accuracy final 10 rounds: 96.93050000000001
7375.278483390808
[9.029003858566284, 17.799676656723022, 26.493372201919556, 35.27851343154907, 44.268237829208374, 53.347811698913574, 62.6358118057251, 71.78593683242798, 80.86158895492554, 90.53207921981812, 100.13780117034912, 109.87922787666321, 119.82362580299377, 129.74561667442322, 139.5903239250183, 149.42225098609924, 159.1243381500244, 168.95611929893494, 178.55160927772522, 188.2546420097351, 198.0271918773651, 207.77272725105286, 217.39309740066528, 226.8784897327423, 236.66338753700256, 246.47467017173767, 256.2392432689667, 266.077951669693, 275.94244933128357, 285.7672772407532, 295.5894777774811, 305.46490240097046, 315.29181480407715, 325.056827545166, 334.99915766716003, 344.88721323013306, 354.7010815143585, 364.412410736084, 374.127477645874, 383.954630613327, 393.5644371509552, 403.29462146759033, 413.09575152397156, 423.21602034568787, 433.043395280838, 442.7740137577057, 452.42563009262085, 462.5891089439392, 472.55981516838074, 482.568683385849, 492.51974272727966, 503.68503999710083, 514.719720363617, 525.9558866024017, 537.0026612281799, 547.6469113826752, 558.7638974189758, 569.6235036849976, 580.7723476886749, 591.8008434772491, 602.6748220920563, 613.7372643947601, 624.7535364627838, 635.7407991886139, 646.8241198062897, 657.6218914985657, 668.4883253574371, 679.5758035182953, 690.629950761795, 701.4548301696777, 712.5944402217865, 723.6185276508331, 734.6763112545013, 745.6895701885223, 756.7817580699921, 767.8766891956329, 779.0054187774658, 790.0246574878693, 800.7605683803558, 811.8975503444672, 822.871657371521, 833.5226860046387, 843.8147277832031, 854.4013032913208, 865.0496184825897, 875.7476117610931, 886.2483472824097, 896.9925301074982, 907.9007289409637, 918.5820407867432, 929.0160105228424, 939.151380777359, 949.1690292358398, 959.0933947563171, 969.7770853042603, 980.4141938686371, 990.9257590770721, 1000.8938279151917, 1010.874977350235, 1021.4937016963959, 1024.2136945724487]
[27.9075, 64.57, 67.18, 73.545, 74.2075, 74.9175, 75.1775, 75.4725, 75.62, 75.7025, 75.9225, 76.12, 76.3525, 83.925, 85.115, 85.54, 85.705, 85.8425, 86.1225, 86.1675, 86.2975, 86.5, 86.5275, 86.645, 86.77, 86.9275, 86.8825, 86.9675, 87.0825, 87.0575, 87.1175, 87.0775, 87.18, 87.195, 87.355, 87.325, 87.4725, 87.515, 87.4375, 87.435, 87.5525, 87.5875, 87.535, 87.53, 95.66, 95.9525, 95.925, 96.1775, 96.15, 96.345, 96.42, 96.39, 96.475, 96.48, 96.4125, 96.6225, 96.57, 96.5775, 96.6675, 96.7675, 96.775, 96.795, 96.795, 96.745, 96.765, 96.835, 96.79, 96.835, 96.75, 96.7825, 96.75, 96.8125, 96.7925, 96.905, 96.875, 96.76, 96.8075, 96.835, 96.9325, 96.84, 96.8175, 96.85, 96.84, 96.87, 96.82, 96.875, 96.9325, 96.8725, 96.925, 96.9525, 96.975, 96.97, 96.92, 96.9075, 96.925, 96.9725, 96.895, 96.9575, 96.94, 96.8425, 96.8875]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.473, Test loss: 1.505, Test accuracy: 96.64
Average accuracy final 10 rounds: 96.55111111111111
2118.634970188141
[2.5241611003875732, 4.890737533569336, 7.266373872756958, 9.605496406555176, 11.992495059967041, 14.48570442199707, 16.837568044662476, 19.207905530929565, 21.678259134292603, 24.019683361053467, 26.44382905960083, 28.811082124710083, 31.205907106399536, 33.58325242996216, 35.97824764251709, 38.36063194274902, 40.80664300918579, 43.23060154914856, 45.59325408935547, 48.0678551197052, 50.460646629333496, 52.84084868431091, 55.296724796295166, 57.71630382537842, 60.16617226600647, 62.60646104812622, 64.94542407989502, 67.39152431488037, 69.75807476043701, 72.11818289756775, 74.5067527294159, 76.9017550945282, 79.19235110282898, 81.62762188911438, 83.94826984405518, 86.37245011329651, 88.78895473480225, 91.19102001190186, 93.60912799835205, 96.09955835342407, 98.45827865600586, 100.91524457931519, 103.32413244247437, 105.75092506408691, 108.21178770065308, 110.54797387123108, 112.93363571166992, 115.29896378517151, 117.72663831710815, 120.20672035217285, 122.72912120819092, 125.21742415428162, 127.68595147132874, 130.1083996295929, 132.4849181175232, 134.86491203308105, 137.21486163139343, 139.58593201637268, 141.993910074234, 144.46436166763306, 146.88750576972961, 149.26944065093994, 151.60210394859314, 154.03158974647522, 156.53161883354187, 158.95790553092957, 161.33512425422668, 163.81733632087708, 166.20004105567932, 168.6495921611786, 171.1665904521942, 173.6068525314331, 176.04086065292358, 178.5083441734314, 180.9005365371704, 183.31108212471008, 185.6977653503418, 188.0910563468933, 190.5395064353943, 192.93164229393005, 195.38609981536865, 197.77899408340454, 200.11180233955383, 202.52779698371887, 204.93188977241516, 207.20223188400269, 209.53731751441956, 211.8441698551178, 214.0929172039032, 216.49336338043213, 218.81084322929382, 221.10600996017456, 223.42356204986572, 225.72883224487305, 228.06864714622498, 230.44786596298218, 232.74939036369324, 235.1106674671173, 237.41924905776978, 239.80394577980042, 241.8074085712433]
[16.383333333333333, 29.6, 36.26111111111111, 45.50555555555555, 47.294444444444444, 58.98888888888889, 63.7, 65.43888888888888, 72.15, 75.26666666666667, 79.97222222222223, 84.03333333333333, 87.03888888888889, 89.04444444444445, 90.12222222222222, 90.48333333333333, 90.83888888888889, 91.19444444444444, 92.25555555555556, 92.57777777777778, 92.86666666666666, 93.05555555555556, 93.27777777777777, 93.5111111111111, 93.69444444444444, 93.82222222222222, 93.95555555555555, 94.24444444444444, 94.31111111111112, 94.36111111111111, 94.45, 94.6, 94.63333333333334, 94.72222222222223, 94.86666666666666, 94.96111111111111, 95.02777777777777, 94.96111111111111, 95.09444444444445, 95.16111111111111, 95.12222222222222, 95.16111111111111, 95.21666666666667, 95.28333333333333, 95.28888888888889, 95.36666666666666, 95.45555555555555, 95.56666666666666, 95.61666666666666, 95.63333333333334, 95.67777777777778, 95.68333333333334, 95.67222222222222, 95.66666666666667, 95.80555555555556, 95.75, 95.82222222222222, 95.95555555555555, 95.93888888888888, 95.96666666666667, 96.0, 96.1, 96.06666666666666, 96.06111111111112, 96.06111111111112, 96.2, 96.11666666666666, 96.16111111111111, 96.19444444444444, 96.09444444444445, 96.25555555555556, 96.28333333333333, 96.2388888888889, 96.20555555555555, 96.27777777777777, 96.22777777777777, 96.22777777777777, 96.26666666666667, 96.35, 96.39444444444445, 96.34444444444445, 96.35555555555555, 96.42222222222222, 96.4, 96.43888888888888, 96.40555555555555, 96.47777777777777, 96.51666666666667, 96.47777777777777, 96.48333333333333, 96.46666666666667, 96.45555555555555, 96.56111111111112, 96.56666666666666, 96.54444444444445, 96.61111111111111, 96.56111111111112, 96.58333333333333, 96.57777777777778, 96.58333333333333, 96.64444444444445]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.478, Test loss: 1.514, Test accuracy: 96.91
Average accuracy final 10 rounds: 96.85111111111112
2867.4949791431427
[2.5100109577178955, 5.020021915435791, 7.50988507270813, 9.999748229980469, 12.475925922393799, 14.952103614807129, 17.45045280456543, 19.94880199432373, 22.422621488571167, 24.896440982818604, 27.342565536499023, 29.788690090179443, 32.2017240524292, 34.614758014678955, 37.016151666641235, 39.417545318603516, 41.9152045249939, 44.41286373138428, 46.88122224807739, 49.34958076477051, 51.77156352996826, 54.193546295166016, 56.73258876800537, 59.27163124084473, 61.773964166641235, 64.27629709243774, 66.8036675453186, 69.33103799819946, 71.8239688873291, 74.31689977645874, 76.85317182540894, 79.38944387435913, 81.88936257362366, 84.38928127288818, 87.00764036178589, 89.6259994506836, 92.13409614562988, 94.64219284057617, 97.1319375038147, 99.62168216705322, 102.18510365486145, 104.74852514266968, 107.30057907104492, 109.85263299942017, 112.4048421382904, 114.95705127716064, 117.50008487701416, 120.04311847686768, 122.48090863227844, 124.91869878768921, 127.43827939033508, 129.95785999298096, 132.3605124950409, 134.76316499710083, 137.07783484458923, 139.39250469207764, 141.74736309051514, 144.10222148895264, 146.50843477249146, 148.91464805603027, 151.34102082252502, 153.76739358901978, 156.2355878353119, 158.703782081604, 161.15045952796936, 163.59713697433472, 166.0043065547943, 168.4114761352539, 170.8765823841095, 173.3416886329651, 175.77494072914124, 178.20819282531738, 180.6450662612915, 183.08193969726562, 185.54080057144165, 187.99966144561768, 190.58035707473755, 193.16105270385742, 195.65967297554016, 198.1582932472229, 200.60452699661255, 203.0507607460022, 205.539466381073, 208.0281720161438, 210.60276699066162, 213.17736196517944, 215.7689654827118, 218.36056900024414, 220.8703281879425, 223.38008737564087, 225.8210644721985, 228.2620415687561, 230.5842523574829, 232.90646314620972, 235.36762738227844, 237.82879161834717, 240.2966001033783, 242.76440858840942, 245.23392343521118, 247.70343828201294, 250.20546913146973, 252.7074999809265, 255.27334880828857, 257.83919763565063, 260.43662881851196, 263.0340600013733, 265.55640602111816, 268.07875204086304, 270.6029326915741, 273.12711334228516, 275.6355061531067, 278.1438989639282, 280.57969665527344, 283.01549434661865, 285.42838311195374, 287.8412718772888, 290.40092372894287, 292.9605755805969, 295.5356512069702, 298.1107268333435, 300.71503734588623, 303.31934785842896, 305.80529713630676, 308.29124641418457, 310.76466965675354, 313.2380928993225, 315.6666476726532, 318.0952024459839, 320.6398193836212, 323.18443632125854, 325.82579040527344, 328.46714448928833, 330.9670648574829, 333.4669852256775, 336.07699823379517, 338.68701124191284, 341.1453981399536, 343.6037850379944, 345.9774215221405, 348.3510580062866, 350.7442362308502, 353.1374144554138, 355.52716517448425, 357.9169158935547, 360.33134961128235, 362.74578332901, 365.16643261909485, 367.5870819091797, 370.066038608551, 372.54499530792236, 375.03122448921204, 377.5174536705017, 380.134379863739, 382.7513060569763, 385.3547716140747, 387.9582371711731, 390.51389741897583, 393.06955766677856, 395.6278727054596, 398.1861877441406, 400.7172203063965, 403.24825286865234, 405.7401614189148, 408.23206996917725, 410.6542937755585, 413.0765175819397, 415.59506464004517, 418.11361169815063, 420.7318835258484, 423.35015535354614, 425.9409019947052, 428.53164863586426, 431.2121391296387, 433.8926296234131, 436.4449746608734, 438.99731969833374, 441.50254583358765, 444.00777196884155, 446.5060088634491, 449.00424575805664, 451.47894072532654, 453.95363569259644, 456.65110993385315, 459.34858417510986, 462.05414366722107, 464.7597031593323, 467.33701157569885, 469.91431999206543, 472.3901810646057, 474.866042137146, 477.3481664657593, 479.83029079437256, 482.3442006111145, 484.85811042785645, 487.40798330307007, 489.9578561782837, 492.5274512767792, 495.09704637527466, 497.6065936088562, 500.11614084243774, 502.1637644767761, 504.2113881111145]
[29.25, 29.25, 27.755555555555556, 27.755555555555556, 31.555555555555557, 31.555555555555557, 51.34444444444444, 51.34444444444444, 64.80555555555556, 64.80555555555556, 71.96111111111111, 71.96111111111111, 79.10555555555555, 79.10555555555555, 81.62222222222222, 81.62222222222222, 82.62222222222222, 82.62222222222222, 83.30555555555556, 83.30555555555556, 83.83333333333333, 83.83333333333333, 84.39444444444445, 84.39444444444445, 84.76666666666667, 84.76666666666667, 85.15555555555555, 85.15555555555555, 85.82222222222222, 85.82222222222222, 88.2611111111111, 88.2611111111111, 90.28888888888889, 90.28888888888889, 91.2388888888889, 91.2388888888889, 91.89444444444445, 91.89444444444445, 92.78888888888889, 92.78888888888889, 93.31111111111112, 93.31111111111112, 93.61666666666666, 93.61666666666666, 93.87222222222222, 93.87222222222222, 94.04444444444445, 94.04444444444445, 94.22777777777777, 94.22777777777777, 94.42222222222222, 94.42222222222222, 94.64444444444445, 94.64444444444445, 94.67222222222222, 94.67222222222222, 94.87222222222222, 94.87222222222222, 95.06111111111112, 95.06111111111112, 95.07777777777778, 95.07777777777778, 95.2388888888889, 95.2388888888889, 95.4, 95.4, 95.42222222222222, 95.42222222222222, 95.57777777777778, 95.57777777777778, 95.68888888888888, 95.68888888888888, 95.71111111111111, 95.71111111111111, 95.69444444444444, 95.69444444444444, 95.80555555555556, 95.80555555555556, 95.91666666666667, 95.91666666666667, 95.88333333333334, 95.88333333333334, 95.9888888888889, 95.9888888888889, 96.0111111111111, 96.0111111111111, 95.96666666666667, 95.96666666666667, 96.06666666666666, 96.06666666666666, 96.08333333333333, 96.08333333333333, 96.06666666666666, 96.06666666666666, 96.15555555555555, 96.15555555555555, 96.22222222222223, 96.22222222222223, 96.2611111111111, 96.2611111111111, 96.35, 96.35, 96.33888888888889, 96.33888888888889, 96.35555555555555, 96.35555555555555, 96.41666666666667, 96.41666666666667, 96.45555555555555, 96.45555555555555, 96.37777777777778, 96.37777777777778, 96.37777777777778, 96.37777777777778, 96.41111111111111, 96.41111111111111, 96.38888888888889, 96.38888888888889, 96.47222222222223, 96.47222222222223, 96.50555555555556, 96.50555555555556, 96.52222222222223, 96.52222222222223, 96.6, 96.6, 96.6, 96.6, 96.60555555555555, 96.60555555555555, 96.63333333333334, 96.63333333333334, 96.65, 96.65, 96.62777777777778, 96.62777777777778, 96.69444444444444, 96.69444444444444, 96.67777777777778, 96.67777777777778, 96.67222222222222, 96.67222222222222, 96.65, 96.65, 96.68888888888888, 96.68888888888888, 96.70555555555555, 96.70555555555555, 96.75, 96.75, 96.73333333333333, 96.73333333333333, 96.72222222222223, 96.72222222222223, 96.7, 96.7, 96.7388888888889, 96.7388888888889, 96.73333333333333, 96.73333333333333, 96.76666666666667, 96.76666666666667, 96.79444444444445, 96.79444444444445, 96.78888888888889, 96.78888888888889, 96.77222222222223, 96.77222222222223, 96.78333333333333, 96.78333333333333, 96.81666666666666, 96.81666666666666, 96.80555555555556, 96.80555555555556, 96.84444444444445, 96.84444444444445, 96.81111111111112, 96.81111111111112, 96.83888888888889, 96.83888888888889, 96.82777777777778, 96.82777777777778, 96.85555555555555, 96.85555555555555, 96.79444444444445, 96.79444444444445, 96.83888888888889, 96.83888888888889, 96.78888888888889, 96.78888888888889, 96.85, 96.85, 96.9, 96.9, 96.86111111111111, 96.86111111111111, 96.91666666666667, 96.91666666666667, 96.87777777777778, 96.87777777777778, 96.91111111111111, 96.91111111111111]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.481, Test loss: 1.513, Test accuracy: 94.93
Final Round, Global train loss: 1.481, Global test loss: 1.998, Global test accuracy: 45.25
Average accuracy final 10 rounds: 94.92583333333334 

Average global accuracy final 10 rounds: 44.38583333333333 

2195.50635433197
[1.4163155555725098, 2.8326311111450195, 4.130451202392578, 5.428271293640137, 6.685510873794556, 7.942750453948975, 9.198270320892334, 10.453790187835693, 11.774536609649658, 13.095283031463623, 14.397850275039673, 15.700417518615723, 17.00130534172058, 18.30219316482544, 19.577791213989258, 20.853389263153076, 22.205004930496216, 23.556620597839355, 24.865400314331055, 26.174180030822754, 27.491899490356445, 28.809618949890137, 30.10344362258911, 31.397268295288086, 32.68144989013672, 33.96563148498535, 35.28001141548157, 36.59439134597778, 37.923184633255005, 39.25197792053223, 40.535569190979004, 41.81916046142578, 43.1756751537323, 44.53218984603882, 45.85301947593689, 47.17384910583496, 48.461819887161255, 49.74979066848755, 51.07878541946411, 52.407780170440674, 53.71994423866272, 55.032108306884766, 56.38591146469116, 57.73971462249756, 59.04312014579773, 60.3465256690979, 61.59154510498047, 62.83656454086304, 64.17703294754028, 65.51750135421753, 66.86259317398071, 68.2076849937439, 69.49448299407959, 70.78128099441528, 72.12783718109131, 73.47439336776733, 74.79556965827942, 76.1167459487915, 77.38757133483887, 78.65839672088623, 79.99776196479797, 81.33712720870972, 82.64934754371643, 83.96156787872314, 85.26440095901489, 86.56723403930664, 87.8228075504303, 89.07838106155396, 90.37363243103027, 91.66888380050659, 93.04478693008423, 94.42069005966187, 95.73415493965149, 97.04761981964111, 98.2839868068695, 99.5203537940979, 100.86434245109558, 102.20833110809326, 103.51939415931702, 104.83045721054077, 106.11938905715942, 107.40832090377808, 108.70421004295349, 110.0000991821289, 111.34956502914429, 112.69903087615967, 114.01816129684448, 115.3372917175293, 116.67317700386047, 118.00906229019165, 119.28995871543884, 120.57085514068604, 121.89000391960144, 123.20915269851685, 124.38508319854736, 125.56101369857788, 126.72250580787659, 127.8839979171753, 129.15390968322754, 130.42382144927979, 131.5755295753479, 132.72723770141602, 133.9070954322815, 135.08695316314697, 136.2707872390747, 137.45462131500244, 138.59338092803955, 139.73214054107666, 140.93433952331543, 142.1365385055542, 143.34613728523254, 144.5557360649109, 145.76796674728394, 146.98019742965698, 148.16132283210754, 149.3424482345581, 150.48617601394653, 151.62990379333496, 152.87036561965942, 154.1108274459839, 155.29012846946716, 156.46942949295044, 157.65937638282776, 158.84932327270508, 160.08464694023132, 161.31997060775757, 162.53895950317383, 163.7579483985901, 164.9572730064392, 166.15659761428833, 167.32077836990356, 168.4849591255188, 169.72446036338806, 170.96396160125732, 172.22171211242676, 173.4794626235962, 174.65916538238525, 175.83886814117432, 176.99158430099487, 178.14430046081543, 179.3325958251953, 180.5208911895752, 181.65887308120728, 182.79685497283936, 184.00272130966187, 185.20858764648438, 186.4599301815033, 187.71127271652222, 188.91900610923767, 190.12673950195312, 191.3781235218048, 192.6295075416565, 193.88037538528442, 195.13124322891235, 196.31639742851257, 197.5015516281128, 198.7184956073761, 199.9354395866394, 201.14501214027405, 202.3545846939087, 203.56638312339783, 204.77818155288696, 205.96254110336304, 207.1469006538391, 208.40163588523865, 209.65637111663818, 210.90005707740784, 212.1437430381775, 213.34430074691772, 214.54485845565796, 215.78469729423523, 217.0245361328125, 218.28798842430115, 219.5514407157898, 220.7768669128418, 222.0022931098938, 223.189692735672, 224.3770923614502, 225.61147165298462, 226.84585094451904, 228.09550642967224, 229.34516191482544, 230.62552738189697, 231.9058928489685, 233.10688042640686, 234.30786800384521, 235.54923105239868, 236.79059410095215, 238.02104830741882, 239.2515025138855, 240.5108666419983, 241.77023077011108, 243.0178198814392, 244.26540899276733, 245.51100540161133, 246.75660181045532, 247.97763395309448, 249.19866609573364, 250.43973445892334, 251.68080282211304, 253.7581741809845, 255.83554553985596]
[52.78333333333333, 52.78333333333333, 67.74166666666666, 67.74166666666666, 70.06666666666666, 70.06666666666666, 78.025, 78.025, 81.68333333333334, 81.68333333333334, 88.225, 88.225, 91.025, 91.025, 93.05, 93.05, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.13333333333334, 93.13333333333334, 93.15, 93.15, 94.71666666666667, 94.71666666666667, 94.69166666666666, 94.69166666666666, 94.74166666666666, 94.74166666666666, 94.8, 94.8, 94.925, 94.925, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.89166666666667, 94.89166666666667, 94.93333333333334, 94.93333333333334, 94.89166666666667, 94.89166666666667, 94.88333333333334, 94.88333333333334, 94.86666666666666, 94.86666666666666, 94.86666666666666, 94.86666666666666, 94.85833333333333, 94.85833333333333, 94.85833333333333, 94.85833333333333, 94.86666666666666, 94.86666666666666, 94.875, 94.875, 94.86666666666666, 94.86666666666666, 94.9, 94.9, 94.9, 94.9, 94.9, 94.9, 94.88333333333334, 94.88333333333334, 94.89166666666667, 94.89166666666667, 94.875, 94.875, 94.89166666666667, 94.89166666666667, 94.89166666666667, 94.89166666666667, 94.89166666666667, 94.89166666666667, 94.9, 94.9, 94.91666666666667, 94.91666666666667, 94.90833333333333, 94.90833333333333, 94.91666666666667, 94.91666666666667, 94.90833333333333, 94.90833333333333, 94.9, 94.9, 94.90833333333333, 94.90833333333333, 94.93333333333334, 94.93333333333334, 94.94166666666666, 94.94166666666666, 94.95, 94.95, 94.93333333333334, 94.93333333333334, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.95833333333333, 94.95833333333333, 94.95, 94.95, 94.95, 94.95, 94.975, 94.975, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.95833333333333, 94.95833333333333, 94.95, 94.95, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.925, 94.925, 94.925, 94.925, 94.89166666666667, 94.89166666666667, 94.90833333333333, 94.90833333333333, 94.925, 94.925, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.925, 94.925, 94.925, 94.925, 94.94166666666666, 94.94166666666666, 94.96666666666667, 94.96666666666667, 94.95833333333333, 94.95833333333333, 94.95, 94.95, 94.95, 94.95, 94.95, 94.95, 94.95, 94.95, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.925, 94.925, 94.93333333333334, 94.93333333333334]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1319, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.464, Test loss: 1.490, Test accuracy: 97.35
Final Round, Global train loss: 1.464, Global test loss: 2.010, Global test accuracy: 44.47
Average accuracy final 10 rounds: 97.34583333333333 

Average global accuracy final 10 rounds: 43.22333333333333 

2199.9576365947723
[1.3884599208831787, 2.7769198417663574, 4.0889222621917725, 5.4009246826171875, 6.693346261978149, 7.985767841339111, 9.363141298294067, 10.740514755249023, 12.091770648956299, 13.443026542663574, 14.835536003112793, 16.22804546356201, 17.581835985183716, 18.93562650680542, 20.304921865463257, 21.674217224121094, 23.05083441734314, 24.427451610565186, 25.820411443710327, 27.21337127685547, 28.499653816223145, 29.78593635559082, 31.161081075668335, 32.53622579574585, 33.90761733055115, 35.279008865356445, 36.731693983078, 38.18437910079956, 39.616353034973145, 41.04832696914673, 42.39643120765686, 43.74453544616699, 45.164597034454346, 46.5846586227417, 47.98398208618164, 49.38330554962158, 50.63105082511902, 51.878796100616455, 53.188878774642944, 54.498961448669434, 55.84618067741394, 57.19339990615845, 58.45376777648926, 59.71413564682007, 61.018715381622314, 62.32329511642456, 63.61513590812683, 64.9069766998291, 66.18848180770874, 67.46998691558838, 68.76607584953308, 70.06216478347778, 71.34005403518677, 72.61794328689575, 73.91506314277649, 75.21218299865723, 76.57706809043884, 77.94195318222046, 79.2773859500885, 80.61281871795654, 82.00180244445801, 83.39078617095947, 84.85545611381531, 86.32012605667114, 87.66681981086731, 89.01351356506348, 90.36115765571594, 91.70880174636841, 93.01370859146118, 94.31861543655396, 95.67708325386047, 97.03555107116699, 98.4659104347229, 99.89626979827881, 101.24690961837769, 102.59754943847656, 104.05487251281738, 105.5121955871582, 106.83976459503174, 108.16733360290527, 109.561683177948, 110.95603275299072, 112.31936740875244, 113.68270206451416, 114.99268102645874, 116.30265998840332, 117.64255237579346, 118.9824447631836, 120.33716487884521, 121.69188499450684, 123.06981205940247, 124.4477391242981, 125.81230640411377, 127.17687368392944, 128.53559184074402, 129.8943099975586, 131.2087345123291, 132.5231590270996, 133.72985696792603, 134.93655490875244, 136.1364290714264, 137.33630323410034, 138.62689113616943, 139.91747903823853, 141.2527358531952, 142.58799266815186, 143.73267316818237, 144.8773536682129, 146.09055972099304, 147.3037657737732, 148.56592893600464, 149.82809209823608, 150.96862506866455, 152.10915803909302, 153.29547142982483, 154.48178482055664, 155.7038609981537, 156.92593717575073, 158.14508366584778, 159.36423015594482, 160.62982511520386, 161.8954200744629, 163.2614347934723, 164.6274495124817, 165.98811292648315, 167.34877634048462, 168.644629240036, 169.9404821395874, 171.25951433181763, 172.57854652404785, 173.88307690620422, 175.1876072883606, 176.55149269104004, 177.91537809371948, 179.17926836013794, 180.4431586265564, 181.82902097702026, 183.21488332748413, 184.5710415840149, 185.92719984054565, 187.20655512809753, 188.4859104156494, 189.86238050460815, 191.2388505935669, 192.47211623191833, 193.70538187026978, 194.8923523426056, 196.0793228149414, 197.31988859176636, 198.5604543685913, 199.80416584014893, 201.04787731170654, 202.27810859680176, 203.50833988189697, 204.78871369361877, 206.06908750534058, 207.41801357269287, 208.76693964004517, 210.01624393463135, 211.26554822921753, 212.5543532371521, 213.84315824508667, 215.15649604797363, 216.4698338508606, 217.75411319732666, 219.03839254379272, 220.3023762702942, 221.56635999679565, 222.8186547756195, 224.07094955444336, 225.43539023399353, 226.7998309135437, 228.09377527236938, 229.38771963119507, 230.60428047180176, 231.82084131240845, 233.05928754806519, 234.29773378372192, 235.4750576019287, 236.6523814201355, 237.8239815235138, 238.9955816268921, 240.2316198348999, 241.46765804290771, 242.67439770698547, 243.88113737106323, 245.03353762626648, 246.18593788146973, 247.39287638664246, 248.59981489181519, 249.7912425994873, 250.98267030715942, 252.16932463645935, 253.35597896575928, 254.52526879310608, 255.69455862045288, 256.87246966362, 258.0503807067871, 259.3009543418884, 260.55152797698975, 262.61278533935547, 264.6740427017212]
[49.95, 49.95, 63.958333333333336, 63.958333333333336, 66.95833333333333, 66.95833333333333, 66.79166666666667, 66.79166666666667, 76.85, 76.85, 78.18333333333334, 78.18333333333334, 83.15, 83.15, 85.23333333333333, 85.23333333333333, 90.04166666666667, 90.04166666666667, 94.0, 94.0, 95.875, 95.875, 95.875, 95.875, 95.90833333333333, 95.90833333333333, 97.30833333333334, 97.30833333333334, 97.29166666666667, 97.29166666666667, 97.225, 97.225, 97.175, 97.175, 97.19166666666666, 97.19166666666666, 97.225, 97.225, 97.23333333333333, 97.23333333333333, 97.275, 97.275, 97.24166666666666, 97.24166666666666, 97.3, 97.3, 97.25, 97.25, 97.28333333333333, 97.28333333333333, 97.25833333333334, 97.25833333333334, 97.19166666666666, 97.19166666666666, 97.19166666666666, 97.19166666666666, 97.24166666666666, 97.24166666666666, 97.26666666666667, 97.26666666666667, 97.26666666666667, 97.26666666666667, 97.23333333333333, 97.23333333333333, 97.23333333333333, 97.23333333333333, 97.225, 97.225, 97.24166666666666, 97.24166666666666, 97.21666666666667, 97.21666666666667, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.23333333333333, 97.23333333333333, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.25, 97.25, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.225, 97.225, 97.24166666666666, 97.24166666666666, 97.25, 97.25, 97.25, 97.25, 97.25833333333334, 97.25833333333334, 97.25, 97.25, 97.24166666666666, 97.24166666666666, 97.23333333333333, 97.23333333333333, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.275, 97.275, 97.28333333333333, 97.28333333333333, 97.26666666666667, 97.26666666666667, 97.275, 97.275, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.3, 97.3, 97.3, 97.3, 97.3, 97.3, 97.3, 97.3, 97.30833333333334, 97.30833333333334, 97.31666666666666, 97.31666666666666, 97.31666666666666, 97.31666666666666, 97.31666666666666, 97.31666666666666, 97.325, 97.325, 97.30833333333334, 97.30833333333334, 97.30833333333334, 97.30833333333334, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.34166666666667, 97.34166666666667, 97.35, 97.35, 97.35, 97.35, 97.35, 97.35, 97.34166666666667, 97.34166666666667, 97.35, 97.35, 97.35, 97.35, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.35, 97.35]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.668, Test loss: 1.669, Test accuracy: 79.36
Final Round, Global train loss: 1.668, Global test loss: 2.283, Global test accuracy: 15.76
Average accuracy final 10 rounds: 80.2295 

Average global accuracy final 10 rounds: 15.412999999999998 

5751.540984869003
[4.325943470001221, 8.651886940002441, 12.551169872283936, 16.45045280456543, 20.366321563720703, 24.282190322875977, 28.274302005767822, 32.26641368865967, 36.54127788543701, 40.816142082214355, 45.01203989982605, 49.207937717437744, 53.424206256866455, 57.640474796295166, 61.85140252113342, 66.06233024597168, 70.33266425132751, 74.60299825668335, 78.94653415679932, 83.29007005691528, 87.51460266113281, 91.73913526535034, 95.89140605926514, 100.04367685317993, 104.00777339935303, 107.97186994552612, 111.9566125869751, 115.94135522842407, 119.95454216003418, 123.96772909164429, 127.89823698997498, 131.82874488830566, 135.83571195602417, 139.84267902374268, 143.76684641838074, 147.6910138130188, 151.54434251785278, 155.39767122268677, 159.48122906684875, 163.56478691101074, 167.5630567073822, 171.56132650375366, 175.40736961364746, 179.25341272354126, 183.00565028190613, 186.757887840271, 190.58542680740356, 194.41296577453613, 197.7607753276825, 201.10858488082886, 204.26830768585205, 207.42803049087524, 210.5732455253601, 213.71846055984497, 216.8501536846161, 219.9818468093872, 223.04083251953125, 226.0998182296753, 229.30364322662354, 232.50746822357178, 235.73787641525269, 238.9682846069336, 242.24536395072937, 245.52244329452515, 248.6977186203003, 251.87299394607544, 255.0393795967102, 258.20576524734497, 261.36032366752625, 264.5148820877075, 267.73276138305664, 270.95064067840576, 274.151567697525, 277.3524947166443, 280.55050253868103, 283.7485103607178, 286.95737862586975, 290.16624689102173, 293.4185461997986, 296.67084550857544, 299.9172487258911, 303.1636519432068, 306.45873832702637, 309.75382471084595, 312.99484038352966, 316.2358560562134, 319.42963337898254, 322.6234107017517, 325.80386304855347, 328.9843153953552, 332.1942927837372, 335.40427017211914, 338.62198138237, 341.83969259262085, 345.228303194046, 348.6169137954712, 351.9929826259613, 355.3690514564514, 358.6755199432373, 361.9819884300232, 365.23370909690857, 368.48542976379395, 371.5339334011078, 374.58243703842163, 377.57613730430603, 380.56983757019043, 383.62025475502014, 386.67067193984985, 389.8126676082611, 392.95466327667236, 396.1549127101898, 399.3551621437073, 402.59034037590027, 405.82551860809326, 409.06570744514465, 412.30589628219604, 415.5386996269226, 418.77150297164917, 421.90226888656616, 425.03303480148315, 428.163587808609, 431.29414081573486, 434.51302766799927, 437.7319145202637, 440.9640510082245, 444.1961874961853, 447.5068693161011, 450.81755113601685, 454.12070178985596, 457.42385244369507, 460.66178131103516, 463.89971017837524, 467.0959324836731, 470.29215478897095, 473.4766426086426, 476.6611304283142, 479.8804626464844, 483.09979486465454, 486.3895072937012, 489.6792197227478, 492.9615070819855, 496.24379444122314, 499.5359802246094, 502.8281660079956, 506.08726620674133, 509.34636640548706, 512.5455191135406, 515.7446718215942, 518.9245927333832, 522.1045136451721, 525.252345085144, 528.400176525116, 531.6202630996704, 534.8403496742249, 538.1156270503998, 541.3909044265747, 544.642499923706, 547.8940954208374, 551.0370333194733, 554.1799712181091, 557.3529527187347, 560.5259342193604, 563.6784679889679, 566.8310017585754, 569.9506866931915, 573.0703716278076, 576.2296571731567, 579.3889427185059, 582.4917466640472, 585.5945506095886, 588.7353956699371, 591.8762407302856, 595.0747210979462, 598.2732014656067, 601.4458529949188, 604.618504524231, 607.7506711483002, 610.8828377723694, 614.1343333721161, 617.3858289718628, 620.5879294872284, 623.790030002594, 626.9323153495789, 630.0746006965637, 633.2221467494965, 636.3696928024292, 639.8986256122589, 643.4275584220886, 647.1671061515808, 650.906653881073, 654.6493368148804, 658.3920197486877, 662.0680687427521, 665.7441177368164, 669.4044554233551, 673.0647931098938, 676.6823437213898, 680.2998943328857, 683.9380226135254, 687.576150894165, 689.4192545413971, 691.2623581886292]
[19.2675, 19.2675, 28.755, 28.755, 36.7075, 36.7075, 39.9125, 39.9125, 40.3675, 40.3675, 44.7375, 44.7375, 45.5425, 45.5425, 46.2775, 46.2775, 45.755, 45.755, 48.7325, 48.7325, 49.715, 49.715, 49.3775, 49.3775, 49.535, 49.535, 50.16, 50.16, 51.6225, 51.6225, 52.4675, 52.4675, 51.7875, 51.7875, 53.485, 53.485, 54.97, 54.97, 56.7075, 56.7075, 58.255, 58.255, 57.9225, 57.9225, 58.425, 58.425, 58.84, 58.84, 58.675, 58.675, 59.4075, 59.4075, 59.4475, 59.4475, 61.35, 61.35, 61.48, 61.48, 62.1, 62.1, 64.9175, 64.9175, 65.68, 65.68, 65.0875, 65.0875, 64.385, 64.385, 63.83, 63.83, 64.6225, 64.6225, 65.175, 65.175, 66.7025, 66.7025, 66.9575, 66.9575, 68.85, 68.85, 69.8475, 69.8475, 71.5575, 71.5575, 71.965, 71.965, 73.25, 73.25, 73.3875, 73.3875, 72.4025, 72.4025, 73.58, 73.58, 73.91, 73.91, 75.46, 75.46, 74.83, 74.83, 74.415, 74.415, 74.5225, 74.5225, 74.565, 74.565, 75.1525, 75.1525, 75.5325, 75.5325, 75.98, 75.98, 75.65, 75.65, 75.1175, 75.1175, 75.7, 75.7, 75.8, 75.8, 75.86, 75.86, 76.7975, 76.7975, 76.835, 76.835, 76.805, 76.805, 76.1875, 76.1875, 76.465, 76.465, 76.4525, 76.4525, 76.5475, 76.5475, 76.555, 76.555, 77.82, 77.82, 77.7825, 77.7825, 77.8, 77.8, 78.2575, 78.2575, 78.4075, 78.4075, 79.08, 79.08, 79.1075, 79.1075, 79.53, 79.53, 79.1775, 79.1775, 78.76, 78.76, 78.4275, 78.4275, 77.9875, 77.9875, 77.9775, 77.9775, 78.4725, 78.4725, 78.465, 78.465, 79.315, 79.315, 79.2875, 79.2875, 79.2175, 79.2175, 79.26, 79.26, 78.47, 78.47, 80.045, 80.045, 79.9575, 79.9575, 79.3925, 79.3925, 80.02, 80.02, 80.3075, 80.3075, 80.4125, 80.4125, 80.56, 80.56, 80.5675, 80.5675, 80.19, 80.19, 80.265, 80.265, 80.6225, 80.6225, 79.365, 79.365]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.761, Test loss: 1.778, Test accuracy: 67.82
Average accuracy final 10 rounds: 66.39725000000001 

3994.2962691783905
[3.318514108657837, 6.637028217315674, 9.663052797317505, 12.689077377319336, 15.798660039901733, 18.90824270248413, 22.008241176605225, 25.10823965072632, 28.191717386245728, 31.275195121765137, 34.314754009246826, 37.354312896728516, 40.26953339576721, 43.18475389480591, 46.18015384674072, 49.17555379867554, 52.22743010520935, 55.279306411743164, 58.61990547180176, 61.96050453186035, 65.1941978931427, 68.42789125442505, 71.47357559204102, 74.51925992965698, 77.55911612510681, 80.59897232055664, 83.59311437606812, 86.58725643157959, 89.59077286720276, 92.59428930282593, 95.5416612625122, 98.48903322219849, 101.4877679347992, 104.4865026473999, 107.55968141555786, 110.63286018371582, 114.2389588356018, 117.8450574874878, 120.82882142066956, 123.81258535385132, 126.71750116348267, 129.622416973114, 132.49660658836365, 135.37079620361328, 138.28760409355164, 141.20441198349, 144.38529205322266, 147.56617212295532, 150.65087366104126, 153.7355751991272, 156.6065592765808, 159.47754335403442, 162.45642852783203, 165.43531370162964, 168.42227244377136, 171.4092311859131, 174.5556333065033, 177.7020354270935, 181.2630033493042, 184.8239712715149, 188.0283875465393, 191.23280382156372, 194.51167058944702, 197.79053735733032, 201.14947295188904, 204.50840854644775, 207.7908535003662, 211.07329845428467, 214.28090572357178, 217.4885129928589, 220.63938808441162, 223.79026317596436, 226.98290157318115, 230.17553997039795, 233.28247165679932, 236.38940334320068, 239.71844482421875, 243.04748630523682, 246.35230684280396, 249.6571273803711, 252.8157660961151, 255.97440481185913, 259.2849032878876, 262.595401763916, 265.95589780807495, 269.3163938522339, 272.6093370914459, 275.90228033065796, 279.19177532196045, 282.48127031326294, 285.8185839653015, 289.1558976173401, 292.30438590049744, 295.4528741836548, 298.74546933174133, 302.0380644798279, 305.4260094165802, 308.8139543533325, 312.1553544998169, 315.49675464630127, 318.7481174468994, 321.99948024749756, 325.2566452026367, 328.5138101577759, 331.4389271736145, 334.3640441894531, 337.23448157310486, 340.1049189567566, 343.03054666519165, 345.9561743736267, 348.80131363868713, 351.64645290374756, 354.55110216140747, 357.4557514190674, 360.52016830444336, 363.58458518981934, 366.61951088905334, 369.65443658828735, 372.6238567829132, 375.59327697753906, 378.3871486186981, 381.1810202598572, 384.0189390182495, 386.85685777664185, 389.80146169662476, 392.74606561660767, 395.80548644065857, 398.8649072647095, 401.9217731952667, 404.978639125824, 407.79839754104614, 410.6181559562683, 413.5554623603821, 416.49276876449585, 419.42992067337036, 422.3670725822449, 425.20598697662354, 428.0449013710022, 430.99422121047974, 433.9435410499573, 436.80340552330017, 439.66326999664307, 442.5863130092621, 445.5093560218811, 448.51212191581726, 451.5148878097534, 454.48028898239136, 457.4456901550293, 460.2746512889862, 463.1036124229431, 466.0425953865051, 468.98157835006714, 471.91040205955505, 474.83922576904297, 477.7990794181824, 480.7589330673218, 483.7332820892334, 486.707631111145, 489.57261753082275, 492.4376039505005, 495.3140275478363, 498.1904511451721, 501.12755012512207, 504.064649105072, 506.97891783714294, 509.89318656921387, 512.858922958374, 515.8246593475342, 518.8107650279999, 521.7968707084656, 524.7653288841248, 527.7337870597839, 530.6385097503662, 533.5432324409485, 536.412017583847, 539.2808027267456, 542.1185584068298, 544.9563140869141, 547.9502220153809, 550.9441299438477, 553.9633901119232, 556.9826502799988, 559.8686184883118, 562.7545866966248, 565.573606967926, 568.3926272392273, 571.4020953178406, 574.4115633964539, 577.2891302108765, 580.1666970252991, 582.9732577800751, 585.7798185348511, 588.7417242527008, 591.7036299705505, 594.6861839294434, 597.6687378883362, 600.6180906295776, 603.5674433708191, 606.5912053585052, 609.6149673461914, 611.0922358036041, 612.5695042610168]
[11.525, 11.525, 11.4225, 11.4225, 12.8525, 12.8525, 16.26, 16.26, 23.2175, 23.2175, 28.68, 28.68, 31.735, 31.735, 35.1, 35.1, 38.9925, 38.9925, 42.1675, 42.1675, 43.6525, 43.6525, 44.23, 44.23, 46.87, 46.87, 51.355, 51.355, 51.5275, 51.5275, 53.09, 53.09, 54.1, 54.1, 54.1525, 54.1525, 54.435, 54.435, 55.3875, 55.3875, 55.965, 55.965, 56.035, 56.035, 56.1425, 56.1425, 56.285, 56.285, 56.315, 56.315, 56.3375, 56.3375, 56.45, 56.45, 56.69, 56.69, 57.39, 57.39, 57.5125, 57.5125, 57.4025, 57.4025, 57.5125, 57.5125, 58.975, 58.975, 59.0025, 59.0025, 59.6475, 59.6475, 59.6675, 59.6675, 59.6725, 59.6725, 60.665, 60.665, 61.105, 61.105, 61.125, 61.125, 61.1725, 61.1725, 61.2625, 61.2625, 61.33, 61.33, 61.3175, 61.3175, 61.355, 61.355, 61.425, 61.425, 61.5225, 61.5225, 61.525, 61.525, 61.92, 61.92, 61.9375, 61.9375, 61.9425, 61.9425, 61.9875, 61.9875, 62.04, 62.04, 62.0425, 62.0425, 61.9975, 61.9975, 62.1475, 62.1475, 62.5525, 62.5525, 62.6725, 62.6725, 62.67, 62.67, 62.67, 62.67, 62.6975, 62.6975, 62.7225, 62.7225, 62.735, 62.735, 63.285, 63.285, 63.31, 63.31, 63.3375, 63.3375, 63.3775, 63.3775, 63.395, 63.395, 64.375, 64.375, 64.385, 64.385, 64.375, 64.375, 64.41, 64.41, 64.4125, 64.4125, 64.43, 64.43, 64.79, 64.79, 64.9025, 64.9025, 64.89, 64.89, 64.8575, 64.8575, 64.935, 64.935, 64.9625, 64.9625, 65.0175, 65.0175, 65.0075, 65.0075, 64.8825, 64.8825, 65.345, 65.345, 65.4, 65.4, 65.4375, 65.4375, 65.745, 65.745, 65.7925, 65.7925, 65.845, 65.845, 65.8425, 65.8425, 65.855, 65.855, 65.8725, 65.8725, 65.8475, 65.8475, 65.9075, 65.9075, 66.42, 66.42, 66.4375, 66.4375, 66.405, 66.405, 66.4425, 66.4425, 67.4625, 67.4625, 67.3225, 67.3225, 67.8225, 67.8225]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.624, Test loss: 1.647, Test accuracy: 81.09
Average accuracy final 10 rounds: 81.02825 

4007.789900779724
[3.435344934463501, 6.870689868927002, 9.898615598678589, 12.926541328430176, 15.90410327911377, 18.881665229797363, 21.9122633934021, 24.942861557006836, 28.018940925598145, 31.095020294189453, 34.07689023017883, 37.05876016616821, 40.09301233291626, 43.12726449966431, 46.32881164550781, 49.53035879135132, 52.68192005157471, 55.833481311798096, 59.02365255355835, 62.2138237953186, 65.41389513015747, 68.61396646499634, 71.78846216201782, 74.9629578590393, 77.90071177482605, 80.8384656906128, 83.92566585540771, 87.01286602020264, 90.18381953239441, 93.35477304458618, 96.42401695251465, 99.49326086044312, 102.44756889343262, 105.40187692642212, 108.5288438796997, 111.6558108329773, 114.73372030258179, 117.81162977218628, 120.8468725681305, 123.8821153640747, 126.90845990180969, 129.93480443954468, 132.8922758102417, 135.84974718093872, 139.0838794708252, 142.31801176071167, 145.48594903945923, 148.6538863182068, 151.69432401657104, 154.7347617149353, 157.89578938484192, 161.05681705474854, 164.20397353172302, 167.3511300086975, 170.45166444778442, 173.55219888687134, 176.68293380737305, 179.81366872787476, 183.01724243164062, 186.2208161354065, 189.33877420425415, 192.4567322731018, 195.38567876815796, 198.3146252632141, 201.51039719581604, 204.70616912841797, 207.901034116745, 211.09589910507202, 214.22402262687683, 217.35214614868164, 220.30701303482056, 223.26187992095947, 226.38636994361877, 229.51085996627808, 232.6461534500122, 235.78144693374634, 238.96098113059998, 242.1405153274536, 245.09070324897766, 248.0408911705017, 251.04572248458862, 254.05055379867554, 257.1959328651428, 260.3413119316101, 263.4620432853699, 266.58277463912964, 269.57241106033325, 272.56204748153687, 275.7000734806061, 278.8380994796753, 281.99154925346375, 285.1449990272522, 288.24243330955505, 291.3398675918579, 294.53810000419617, 297.7363324165344, 300.9342873096466, 304.1322422027588, 307.0544955730438, 309.97674894332886, 313.07867527008057, 316.1806015968323, 319.30348563194275, 322.4263696670532, 325.5521996021271, 328.6780295372009, 331.9048240184784, 335.13161849975586, 338.1860797405243, 341.2405409812927, 344.2042872905731, 347.1680335998535, 350.387743473053, 353.60745334625244, 356.82742047309875, 360.04738759994507, 363.1226074695587, 366.19782733917236, 369.16195797920227, 372.1260886192322, 375.28195905685425, 378.4378294944763, 381.52318835258484, 384.60854721069336, 387.6825022697449, 390.7564573287964, 393.75759768486023, 396.7587380409241, 399.87285923957825, 402.9869804382324, 406.00290966033936, 409.0188388824463, 412.13432240486145, 415.2498059272766, 418.35684084892273, 421.46387577056885, 424.5926842689514, 427.721492767334, 430.6633048057556, 433.60511684417725, 436.6854181289673, 439.7657194137573, 442.911908864975, 446.0580983161926, 449.14724922180176, 452.2364001274109, 455.2886130809784, 458.3408260345459, 461.3928828239441, 464.4449396133423, 467.62372517585754, 470.8025107383728, 473.9237496852875, 477.04498863220215, 480.2431375980377, 483.4412865638733, 486.46623158454895, 489.4911766052246, 492.58154368400574, 495.67191076278687, 500.0619616508484, 504.4520125389099, 507.53257417678833, 510.61313581466675, 513.8888227939606, 517.1645097732544, 520.2121207714081, 523.2597317695618, 526.3006591796875, 529.3415865898132, 532.3794984817505, 535.4174103736877, 538.4778053760529, 541.538200378418, 544.7149364948273, 547.8916726112366, 550.9618465900421, 554.0320205688477, 557.1130964756012, 560.1941723823547, 563.3241205215454, 566.4540686607361, 569.6940767765045, 572.934084892273, 576.0450041294098, 579.1559233665466, 582.0764451026917, 584.9969668388367, 588.1103222370148, 591.2236776351929, 594.3718659877777, 597.5200543403625, 600.6282529830933, 603.736451625824, 606.6891918182373, 609.6419320106506, 612.6639366149902, 615.6859412193298, 618.8270938396454, 621.9682464599609, 623.4462654590607, 624.9242844581604]
[9.295, 9.295, 15.3725, 15.3725, 22.15, 22.15, 25.8125, 25.8125, 31.39, 31.39, 35.2625, 35.2625, 37.3575, 37.3575, 42.215, 42.215, 43.805, 43.805, 48.385, 48.385, 50.585, 50.585, 52.265, 52.265, 54.435, 54.435, 57.4325, 57.4325, 59.6975, 59.6975, 61.615, 61.615, 62.9825, 62.9825, 64.1975, 64.1975, 64.755, 64.755, 65.305, 65.305, 64.9375, 64.9375, 65.605, 65.605, 66.6875, 66.6875, 66.795, 66.795, 67.185, 67.185, 67.4575, 67.4575, 68.0975, 68.0975, 68.08, 68.08, 68.205, 68.205, 69.7975, 69.7975, 70.4175, 70.4175, 70.6775, 70.6775, 71.7075, 71.7075, 71.7275, 71.7275, 72.155, 72.155, 72.355, 72.355, 73.17, 73.17, 73.3325, 73.3325, 73.2875, 73.2875, 74.0475, 74.0475, 74.1525, 74.1525, 74.2475, 74.2475, 74.2475, 74.2475, 74.29, 74.29, 74.54, 74.54, 74.5675, 74.5675, 74.6175, 74.6175, 74.73, 74.73, 75.0125, 75.0125, 75.1975, 75.1975, 75.245, 75.245, 75.5825, 75.5825, 75.72, 75.72, 75.7725, 75.7725, 75.8, 75.8, 75.9675, 75.9675, 76.0275, 76.0275, 76.215, 76.215, 76.2875, 76.2875, 76.285, 76.285, 76.8175, 76.8175, 76.785, 76.785, 76.78, 76.78, 76.8, 76.8, 77.0425, 77.0425, 77.4925, 77.4925, 77.595, 77.595, 77.58, 77.58, 77.7, 77.7, 78.2925, 78.2925, 78.825, 78.825, 78.8825, 78.8825, 78.85, 78.85, 78.91, 78.91, 78.875, 78.875, 79.85, 79.85, 79.8875, 79.8875, 79.7975, 79.7975, 79.935, 79.935, 79.9725, 79.9725, 79.9775, 79.9775, 79.9675, 79.9675, 80.0, 80.0, 80.0075, 80.0075, 79.935, 79.935, 80.3625, 80.3625, 80.9775, 80.9775, 80.915, 80.915, 80.9475, 80.9475, 80.95, 80.95, 81.0175, 81.0175, 80.93, 80.93, 80.965, 80.965, 81.025, 81.025, 81.005, 81.005, 81.0025, 81.0025, 81.135, 81.135, 81.0625, 81.0625, 81.04, 81.04, 81.1, 81.1, 81.0925, 81.0925]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1319, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.235, Test loss: 1.077, Test accuracy: 74.06
Final Round, Global train loss: 0.235, Global test loss: 0.764, Global test accuracy: 77.39
Average accuracy final 10 rounds: 73.194 

Average global accuracy final 10 rounds: 77.8285 

6207.9219744205475
[5.092774391174316, 10.185548782348633, 14.471034049987793, 18.756519317626953, 23.04603123664856, 27.335543155670166, 31.640562295913696, 35.94558143615723, 40.2458655834198, 44.54614973068237, 48.8854079246521, 53.224666118621826, 57.54579401016235, 61.86692190170288, 66.20478415489197, 70.54264640808105, 74.89909672737122, 79.25554704666138, 83.73908138275146, 88.22261571884155, 92.59877896308899, 96.97494220733643, 101.3374388217926, 105.69993543624878, 110.07094502449036, 114.44195461273193, 118.80379962921143, 123.16564464569092, 127.56043744087219, 131.95523023605347, 136.30400133132935, 140.65277242660522, 145.00032663345337, 149.3478808403015, 153.6647973060608, 157.98171377182007, 162.35556268692017, 166.72941160202026, 171.14146757125854, 175.55352354049683, 179.8664093017578, 184.1792950630188, 188.47931909561157, 192.77934312820435, 197.16330409049988, 201.5472650527954, 205.88570022583008, 210.22413539886475, 214.5717794895172, 218.91942358016968, 223.31032705307007, 227.70123052597046, 232.08993554115295, 236.47864055633545, 240.8616645336151, 245.24468851089478, 249.60449528694153, 253.96430206298828, 258.33654975891113, 262.708797454834, 267.0688569545746, 271.4289164543152, 275.78039479255676, 280.13187313079834, 284.61456871032715, 289.09726428985596, 293.51324462890625, 297.92922496795654, 302.35204458236694, 306.77486419677734, 311.10512495040894, 315.4353857040405, 319.7801034450531, 324.1248211860657, 328.46306252479553, 332.8013038635254, 337.19399213790894, 341.5866804122925, 345.9655055999756, 350.3443307876587, 354.72629737854004, 359.1082639694214, 363.4786305427551, 367.84899711608887, 372.2111027240753, 376.57320833206177, 380.9420602321625, 385.3109121322632, 389.68439745903015, 394.0578827857971, 398.43018412590027, 402.8024854660034, 407.15296959877014, 411.50345373153687, 415.8502368927002, 420.1970200538635, 424.5562150478363, 428.9154100418091, 433.31843423843384, 437.7214584350586, 442.1062617301941, 446.4910650253296, 450.9297904968262, 455.36851596832275, 459.77091789245605, 464.17331981658936, 468.5832209587097, 472.9931221008301, 477.38674688339233, 481.7803716659546, 486.4776647090912, 491.1749577522278, 495.63687205314636, 500.09878635406494, 504.5350856781006, 508.97138500213623, 513.4852893352509, 517.9991936683655, 522.4047455787659, 526.8102974891663, 531.2240009307861, 535.637704372406, 540.0640394687653, 544.4903745651245, 548.9467046260834, 553.4030346870422, 557.8277785778046, 562.2525224685669, 566.7074847221375, 571.162446975708, 575.5810897350311, 579.9997324943542, 584.4010899066925, 588.8024473190308, 593.2273619174957, 597.6522765159607, 602.0943191051483, 606.5363616943359, 610.9919447898865, 615.447527885437, 619.8911755084991, 624.3348231315613, 628.7455604076385, 633.1562976837158, 637.5979616641998, 642.0396256446838, 646.4457116127014, 650.851797580719, 655.2882258892059, 659.7246541976929, 664.1481857299805, 668.5717172622681, 673.0238318443298, 677.4759464263916, 681.8976261615753, 686.319305896759, 690.7710282802582, 695.2227506637573, 699.672905921936, 704.1230611801147, 708.5079233646393, 712.8927855491638, 717.3337967395782, 721.7748079299927, 726.1903758049011, 730.6059436798096, 735.0410459041595, 739.4761481285095, 743.8951058387756, 748.3140635490417, 752.7688472270966, 757.2236309051514, 762.3092355728149, 767.3948402404785, 772.4408705234528, 777.486900806427, 782.5067183971405, 787.526535987854, 792.5330078601837, 797.5394797325134, 802.5537104606628, 807.5679411888123, 812.5957307815552, 817.6235203742981, 822.6425039768219, 827.6614875793457, 832.6850650310516, 837.7086424827576, 842.7036061286926, 847.6985697746277, 852.1187660694122, 856.5389623641968, 860.9643883705139, 865.389814376831, 869.7909376621246, 874.1920609474182, 878.5949635505676, 882.997866153717, 887.4119558334351, 891.8260455131531, 894.0384225845337, 896.2507996559143]
[38.2125, 38.2125, 43.0975, 43.0975, 44.5875, 44.5875, 46.37, 46.37, 49.9575, 49.9575, 53.03, 53.03, 54.9425, 54.9425, 56.04, 56.04, 57.61, 57.61, 58.1075, 58.1075, 58.7025, 58.7025, 60.1725, 60.1725, 61.52, 61.52, 62.325, 62.325, 62.6975, 62.6975, 63.625, 63.625, 64.2375, 64.2375, 64.8425, 64.8425, 65.7125, 65.7125, 66.415, 66.415, 66.6625, 66.6625, 67.6025, 67.6025, 67.695, 67.695, 67.9875, 67.9875, 68.1975, 68.1975, 68.03, 68.03, 68.4825, 68.4825, 68.6975, 68.6975, 69.095, 69.095, 69.1875, 69.1875, 69.5325, 69.5325, 69.735, 69.735, 70.1125, 70.1125, 70.1975, 70.1975, 70.3875, 70.3875, 70.5725, 70.5725, 70.7325, 70.7325, 70.4325, 70.4325, 70.655, 70.655, 70.73, 70.73, 70.9475, 70.9475, 71.02, 71.02, 71.1475, 71.1475, 71.38, 71.38, 71.075, 71.075, 71.275, 71.275, 71.6, 71.6, 71.4175, 71.4175, 71.5125, 71.5125, 71.6675, 71.6675, 71.845, 71.845, 71.9325, 71.9325, 71.74, 71.74, 71.805, 71.805, 71.765, 71.765, 71.6625, 71.6625, 71.8225, 71.8225, 71.8825, 71.8825, 72.0475, 72.0475, 72.2025, 72.2025, 72.4475, 72.4475, 72.6175, 72.6175, 72.855, 72.855, 72.3425, 72.3425, 72.5575, 72.5575, 72.5275, 72.5275, 72.7725, 72.7725, 72.7625, 72.7625, 72.82, 72.82, 72.6475, 72.6475, 72.48, 72.48, 72.4425, 72.4425, 72.715, 72.715, 72.6175, 72.6175, 72.95, 72.95, 73.165, 73.165, 73.215, 73.215, 72.755, 72.755, 72.705, 72.705, 72.7725, 72.7725, 73.1325, 73.1325, 73.3225, 73.3225, 73.3, 73.3, 73.1225, 73.1225, 73.3175, 73.3175, 73.335, 73.335, 73.0925, 73.0925, 73.05, 73.05, 73.3, 73.3, 73.2025, 73.2025, 73.055, 73.055, 72.9875, 72.9875, 73.0925, 73.0925, 73.0275, 73.0275, 73.355, 73.355, 73.3825, 73.3825, 73.3825, 73.3825, 73.2025, 73.2025, 73.2525, 73.2525, 73.2025, 73.2025, 74.055, 74.055]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.193, Test loss: 0.375, Test accuracy: 86.83
Average accuracy final 10 rounds: 86.42666666666665 

1504.6943144798279
[1.737337350845337, 3.474674701690674, 4.916710138320923, 6.358745574951172, 7.762982606887817, 9.167219638824463, 10.594605684280396, 12.021991729736328, 13.447002649307251, 14.872013568878174, 16.29418706893921, 17.716360569000244, 19.127671241760254, 20.538981914520264, 21.94723105430603, 23.355480194091797, 24.599162578582764, 25.84284496307373, 27.08695888519287, 28.33107280731201, 29.573379278182983, 30.815685749053955, 32.0736129283905, 33.33154010772705, 34.5840528011322, 35.83656549453735, 37.0858736038208, 38.33518171310425, 39.556803703308105, 40.77842569351196, 42.01847052574158, 43.25851535797119, 44.50067400932312, 45.74283266067505, 46.989699363708496, 48.23656606674194, 49.48937463760376, 50.742183208465576, 51.98782396316528, 53.23346471786499, 54.479485750198364, 55.72550678253174, 56.97154974937439, 58.21759271621704, 59.45647883415222, 60.6953649520874, 61.92891216278076, 63.16245937347412, 64.39740443229675, 65.63234949111938, 66.85938262939453, 68.08641576766968, 69.3155426979065, 70.54466962814331, 71.78046417236328, 73.01625871658325, 74.25003004074097, 75.48380136489868, 76.71333765983582, 77.94287395477295, 79.17726349830627, 80.4116530418396, 81.63424968719482, 82.85684633255005, 84.0885899066925, 85.32033348083496, 86.54737281799316, 87.77441215515137, 89.01331281661987, 90.25221347808838, 91.49339962005615, 92.73458576202393, 93.96621918678284, 95.19785261154175, 96.43627071380615, 97.67468881607056, 98.91165351867676, 100.14861822128296, 101.397873878479, 102.64712953567505, 103.89466214179993, 105.1421947479248, 106.38889670372009, 107.63559865951538, 108.87246417999268, 110.10932970046997, 111.35717296600342, 112.60501623153687, 113.84074020385742, 115.07646417617798, 116.32362699508667, 117.57078981399536, 118.80766677856445, 120.04454374313354, 121.28232026100159, 122.52009677886963, 123.75994086265564, 124.99978494644165, 126.24387669563293, 127.48796844482422, 128.83283948898315, 130.1777105331421, 131.5290687084198, 132.8804268836975, 134.2231321334839, 135.56583738327026, 136.91241765022278, 138.2589979171753, 139.60422325134277, 140.94944858551025, 142.18418288230896, 143.41891717910767, 144.6470754146576, 145.87523365020752, 147.11993288993835, 148.3646321296692, 149.60304856300354, 150.8414649963379, 152.0826427936554, 153.3238205909729, 154.55951976776123, 155.79521894454956, 157.02280354499817, 158.25038814544678, 159.47300553321838, 160.69562292099, 161.9217460155487, 163.14786911010742, 164.37471556663513, 165.60156202316284, 166.83866262435913, 168.07576322555542, 169.30774307250977, 170.5397229194641, 171.7761857509613, 173.0126485824585, 174.23774933815002, 175.46285009384155, 176.7019805908203, 177.94111108779907, 179.18596959114075, 180.43082809448242, 181.66904377937317, 182.90725946426392, 184.13712406158447, 185.36698865890503, 186.60354018211365, 187.84009170532227, 189.06688499450684, 190.2936782836914, 191.52474522590637, 192.75581216812134, 193.98974990844727, 195.2236876487732, 196.4595034122467, 197.69531917572021, 198.92753076553345, 200.15974235534668, 201.39145636558533, 202.62317037582397, 203.84701895713806, 205.07086753845215, 206.30899834632874, 207.54712915420532, 208.78137016296387, 210.0156111717224, 211.24910044670105, 212.4825897216797, 213.71916127204895, 214.9557328224182, 216.18939447402954, 217.42305612564087, 218.65177655220032, 219.88049697875977, 221.11184430122375, 222.34319162368774, 223.6866180896759, 225.03004455566406, 226.3755750656128, 227.72110557556152, 229.06923985481262, 230.41737413406372, 231.76329708099365, 233.10922002792358, 234.44121766090393, 235.77321529388428, 237.11329555511475, 238.45337581634521, 239.79024243354797, 241.12710905075073, 242.46522569656372, 243.8033423423767, 245.14547538757324, 246.48760843276978, 247.83540225028992, 249.18319606781006, 250.52351903915405, 251.86384201049805, 253.22523975372314, 254.58663749694824, 256.6291494369507, 258.6716613769531]
[25.358333333333334, 25.358333333333334, 39.9, 39.9, 44.291666666666664, 44.291666666666664, 49.85, 49.85, 54.78333333333333, 54.78333333333333, 59.69166666666667, 59.69166666666667, 61.358333333333334, 61.358333333333334, 59.81666666666667, 59.81666666666667, 61.975, 61.975, 66.56666666666666, 66.56666666666666, 70.80833333333334, 70.80833333333334, 73.59166666666667, 73.59166666666667, 74.14166666666667, 74.14166666666667, 74.725, 74.725, 77.70833333333333, 77.70833333333333, 78.55, 78.55, 78.49166666666666, 78.49166666666666, 79.26666666666667, 79.26666666666667, 78.9, 78.9, 79.64166666666667, 79.64166666666667, 80.325, 80.325, 80.36666666666666, 80.36666666666666, 80.175, 80.175, 81.225, 81.225, 80.86666666666666, 80.86666666666666, 80.95, 80.95, 80.75, 80.75, 80.58333333333333, 80.58333333333333, 80.86666666666666, 80.86666666666666, 81.35, 81.35, 81.525, 81.525, 81.55, 81.55, 81.975, 81.975, 82.15, 82.15, 82.68333333333334, 82.68333333333334, 82.71666666666667, 82.71666666666667, 83.14166666666667, 83.14166666666667, 83.1, 83.1, 83.175, 83.175, 82.95, 82.95, 83.225, 83.225, 83.2, 83.2, 83.25, 83.25, 83.625, 83.625, 83.48333333333333, 83.48333333333333, 83.26666666666667, 83.26666666666667, 83.31666666666666, 83.31666666666666, 84.16666666666667, 84.16666666666667, 83.94166666666666, 83.94166666666666, 83.60833333333333, 83.60833333333333, 84.29166666666667, 84.29166666666667, 84.29166666666667, 84.29166666666667, 84.18333333333334, 84.18333333333334, 84.20833333333333, 84.20833333333333, 84.39166666666667, 84.39166666666667, 84.48333333333333, 84.48333333333333, 84.54166666666667, 84.54166666666667, 84.6, 84.6, 84.84166666666667, 84.84166666666667, 84.725, 84.725, 85.00833333333334, 85.00833333333334, 84.63333333333334, 84.63333333333334, 84.88333333333334, 84.88333333333334, 84.825, 84.825, 84.85833333333333, 84.85833333333333, 84.76666666666667, 84.76666666666667, 85.56666666666666, 85.56666666666666, 85.48333333333333, 85.48333333333333, 85.8, 85.8, 85.7, 85.7, 85.8, 85.8, 85.63333333333334, 85.63333333333334, 85.7, 85.7, 85.95, 85.95, 85.71666666666667, 85.71666666666667, 85.94166666666666, 85.94166666666666, 86.41666666666667, 86.41666666666667, 85.71666666666667, 85.71666666666667, 85.78333333333333, 85.78333333333333, 85.51666666666667, 85.51666666666667, 86.225, 86.225, 86.25, 86.25, 85.81666666666666, 85.81666666666666, 86.025, 86.025, 86.25, 86.25, 85.93333333333334, 85.93333333333334, 86.06666666666666, 86.06666666666666, 86.05, 86.05, 86.35, 86.35, 86.21666666666667, 86.21666666666667, 86.325, 86.325, 86.08333333333333, 86.08333333333333, 86.30833333333334, 86.30833333333334, 86.41666666666667, 86.41666666666667, 86.28333333333333, 86.28333333333333, 86.75, 86.75, 86.64166666666667, 86.64166666666667, 86.53333333333333, 86.53333333333333, 86.45833333333333, 86.45833333333333, 86.46666666666667, 86.46666666666667, 86.83333333333333, 86.83333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.195, Test loss: 0.334, Test accuracy: 87.43
Average accuracy final 10 rounds: 86.945
1818.5374674797058
[2.2330119609832764, 4.466023921966553, 6.273844003677368, 8.081664085388184, 9.903797388076782, 11.72593069076538, 13.5321204662323, 15.338310241699219, 17.14589023590088, 18.95347023010254, 20.744205236434937, 22.534940242767334, 24.324262619018555, 26.113584995269775, 27.920258283615112, 29.72693157196045, 31.532682418823242, 33.338433265686035, 35.16722345352173, 36.99601364135742, 38.81782865524292, 40.63964366912842, 42.44174838066101, 44.2438530921936, 46.05596470832825, 47.86807632446289, 49.69225740432739, 51.516438484191895, 53.33091139793396, 55.145384311676025, 56.95366382598877, 58.761943340301514, 60.56777858734131, 62.3736138343811, 64.19444990158081, 66.01528596878052, 67.83949255943298, 69.66369915008545, 71.49492812156677, 73.3261570930481, 75.15330767631531, 76.98045825958252, 78.69032907485962, 80.40019989013672, 82.19872665405273, 83.99725341796875, 85.80450129508972, 87.6117491722107, 89.4252438545227, 91.23873853683472, 93.0619056224823, 94.88507270812988, 96.71663546562195, 98.54819822311401, 100.32811641693115, 102.10803461074829, 103.89507269859314, 105.68211078643799, 107.48513674736023, 109.28816270828247, 111.07766604423523, 112.86716938018799, 114.66241693496704, 116.4576644897461, 118.2423985004425, 120.02713251113892, 121.81969738006592, 123.61226224899292, 125.39843344688416, 127.18460464477539, 128.98168277740479, 130.77876091003418, 132.57922291755676, 134.37968492507935, 136.1691653728485, 137.95864582061768, 139.7379596233368, 141.5172734260559, 143.3189091682434, 145.1205449104309, 146.91961908340454, 148.71869325637817, 150.50145363807678, 152.2842140197754, 154.0967984199524, 155.9093828201294, 157.6981840133667, 159.486985206604, 161.2893476486206, 163.0917100906372, 164.87693166732788, 166.66215324401855, 168.45051050186157, 170.2388677597046, 172.03619503974915, 173.8335223197937, 175.6283359527588, 177.42314958572388, 179.2178246974945, 181.01249980926514, 182.8034746646881, 184.59444952011108, 186.3814878463745, 188.16852617263794, 189.97086596488953, 191.7732057571411, 193.55716466903687, 195.34112358093262, 197.1303050518036, 198.91948652267456, 200.71398901939392, 202.50849151611328, 204.30334162712097, 206.09819173812866, 207.8705859184265, 209.64298009872437, 211.44619917869568, 213.249418258667, 215.0663025379181, 216.8831868171692, 218.69041180610657, 220.49763679504395, 222.29733300209045, 224.09702920913696, 225.89659094810486, 227.69615268707275, 229.51220059394836, 231.32824850082397, 233.14641952514648, 234.964590549469, 236.76935195922852, 238.57411336898804, 240.38751673698425, 242.20092010498047, 243.99402356147766, 245.78712701797485, 247.60065340995789, 249.41417980194092, 251.22888708114624, 253.04359436035156, 254.8627471923828, 256.68190002441406, 258.49399280548096, 260.30608558654785, 262.1353931427002, 263.96470069885254, 265.7687737941742, 267.57284688949585, 269.3794367313385, 271.18602657318115, 273.07258319854736, 274.9591398239136, 276.775137424469, 278.5911350250244, 280.42581725120544, 282.2604994773865, 284.0799889564514, 285.89947843551636, 287.72897028923035, 289.55846214294434, 291.38019585609436, 293.2019295692444, 295.00960636138916, 296.81728315353394, 298.6357789039612, 300.4542746543884, 302.2493336200714, 304.0443925857544, 305.65923976898193, 307.2740869522095, 308.9001407623291, 310.52619457244873, 312.1416127681732, 313.7570309638977, 315.37671184539795, 316.9963927268982, 318.6501965522766, 320.30400037765503, 321.9340810775757, 323.56416177749634, 325.23227190971375, 326.90038204193115, 328.52026867866516, 330.14015531539917, 331.78245735168457, 333.42475938796997, 335.05383348464966, 336.68290758132935, 338.3313264846802, 339.979745388031, 341.61176323890686, 343.2437810897827, 344.88829612731934, 346.53281116485596, 348.1598551273346, 349.78689908981323, 351.42268109321594, 353.05846309661865, 354.68781900405884, 356.317174911499, 358.4798653125763, 360.64255571365356]
[28.516666666666666, 28.516666666666666, 34.608333333333334, 34.608333333333334, 42.583333333333336, 42.583333333333336, 55.666666666666664, 55.666666666666664, 63.7, 63.7, 61.31666666666667, 61.31666666666667, 66.01666666666667, 66.01666666666667, 72.625, 72.625, 70.96666666666667, 70.96666666666667, 75.45, 75.45, 76.41666666666667, 76.41666666666667, 76.95, 76.95, 77.80833333333334, 77.80833333333334, 77.20833333333333, 77.20833333333333, 77.625, 77.625, 78.41666666666667, 78.41666666666667, 79.15, 79.15, 79.31666666666666, 79.31666666666666, 79.68333333333334, 79.68333333333334, 80.2, 80.2, 80.175, 80.175, 80.40833333333333, 80.40833333333333, 80.30833333333334, 80.30833333333334, 80.7, 80.7, 81.35833333333333, 81.35833333333333, 81.375, 81.375, 81.65833333333333, 81.65833333333333, 81.51666666666667, 81.51666666666667, 81.75, 81.75, 82.08333333333333, 82.08333333333333, 82.41666666666667, 82.41666666666667, 82.00833333333334, 82.00833333333334, 82.25, 82.25, 82.19166666666666, 82.19166666666666, 82.79166666666667, 82.79166666666667, 82.53333333333333, 82.53333333333333, 83.05833333333334, 83.05833333333334, 83.25, 83.25, 83.29166666666667, 83.29166666666667, 83.60833333333333, 83.60833333333333, 83.61666666666666, 83.61666666666666, 83.775, 83.775, 83.90833333333333, 83.90833333333333, 84.04166666666667, 84.04166666666667, 83.93333333333334, 83.93333333333334, 83.56666666666666, 83.56666666666666, 83.69166666666666, 83.69166666666666, 84.09166666666667, 84.09166666666667, 83.875, 83.875, 84.5, 84.5, 84.55833333333334, 84.55833333333334, 84.80833333333334, 84.80833333333334, 85.19166666666666, 85.19166666666666, 85.08333333333333, 85.08333333333333, 85.05833333333334, 85.05833333333334, 84.975, 84.975, 84.73333333333333, 84.73333333333333, 84.75, 84.75, 85.21666666666667, 85.21666666666667, 85.58333333333333, 85.58333333333333, 85.63333333333334, 85.63333333333334, 85.51666666666667, 85.51666666666667, 85.5, 85.5, 85.64166666666667, 85.64166666666667, 85.64166666666667, 85.64166666666667, 85.825, 85.825, 86.03333333333333, 86.03333333333333, 85.98333333333333, 85.98333333333333, 86.025, 86.025, 86.325, 86.325, 86.48333333333333, 86.48333333333333, 86.125, 86.125, 86.35833333333333, 86.35833333333333, 86.34166666666667, 86.34166666666667, 86.26666666666667, 86.26666666666667, 86.38333333333334, 86.38333333333334, 86.29166666666667, 86.29166666666667, 86.38333333333334, 86.38333333333334, 86.38333333333334, 86.38333333333334, 86.51666666666667, 86.51666666666667, 86.44166666666666, 86.44166666666666, 86.7, 86.7, 86.30833333333334, 86.30833333333334, 86.4, 86.4, 86.65833333333333, 86.65833333333333, 86.75, 86.75, 86.60833333333333, 86.60833333333333, 86.26666666666667, 86.26666666666667, 86.55833333333334, 86.55833333333334, 87.19166666666666, 87.19166666666666, 87.025, 87.025, 86.90833333333333, 86.90833333333333, 86.93333333333334, 86.93333333333334, 86.69166666666666, 86.69166666666666, 86.89166666666667, 86.89166666666667, 86.71666666666667, 86.71666666666667, 87.11666666666666, 87.11666666666666, 87.00833333333334, 87.00833333333334, 87.05, 87.05, 87.10833333333333, 87.10833333333333, 87.43333333333334, 87.43333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.158, Test loss: 1.111, Test accuracy: 67.12
Average accuracy final 10 rounds: 63.20916666666667
2552.051463365555
[4.065163850784302, 7.614247798919678, 11.190709352493286, 14.788778305053711, 18.378865718841553, 21.96125841140747, 25.577704191207886, 29.19133687019348, 32.77551484107971, 36.35343098640442, 39.92634344100952, 43.49280667304993, 47.05085062980652, 50.60004949569702, 54.12668180465698, 57.6929144859314, 61.21616864204407, 64.7408504486084, 68.25169491767883, 71.76834845542908, 75.29940533638, 78.8666844367981, 82.47112727165222, 86.07908916473389, 89.64793419837952, 93.21647238731384, 96.7781662940979, 100.3397171497345, 103.91175580024719, 107.50760769844055, 111.10125732421875, 114.63913416862488, 118.16316556930542, 121.72265601158142, 125.3011634349823, 128.88806080818176, 132.4454357624054, 136.01068949699402, 139.58678007125854, 143.1471118927002, 146.69261050224304, 150.22256422042847, 153.7850522994995, 157.37128853797913, 160.92692756652832, 164.40984511375427, 167.9153118133545, 171.4405655860901, 174.99976110458374, 178.52041029930115, 182.0443515777588, 185.52433037757874, 189.02021026611328, 192.85477948188782, 196.70544362068176, 200.5994942188263, 204.46709752082825, 208.2439422607422, 212.0615210533142, 215.75240755081177, 219.2648651599884, 222.77121782302856, 226.27501034736633, 229.72308111190796, 233.17166757583618, 236.63393568992615, 240.11395406723022, 243.5938115119934, 247.09923911094666, 250.58781695365906, 254.04053258895874, 257.5076537132263, 260.98018884658813, 264.46559047698975, 267.9220778942108, 271.42300820350647, 274.9232335090637, 278.3817002773285, 281.8699827194214, 285.357093334198, 288.7987689971924, 292.6752119064331, 296.5505635738373, 300.41190218925476, 304.30017471313477, 308.1478912830353, 312.0292317867279, 315.90454745292664, 319.79217982292175, 323.6476354598999, 327.5269637107849, 331.37442779541016, 335.24491930007935, 339.12946486473083, 343.0131866931915, 346.88794803619385, 350.7560307979584, 354.6174530982971, 358.4874348640442, 361.9540333747864, 364.89166736602783]
[30.291666666666668, 31.591666666666665, 36.2, 40.166666666666664, 31.725, 40.11666666666667, 48.05833333333333, 42.075, 40.083333333333336, 50.25, 51.35, 38.108333333333334, 51.825, 54.025, 44.38333333333333, 50.65833333333333, 48.84166666666667, 48.71666666666667, 49.85, 55.75, 47.36666666666667, 52.25833333333333, 54.28333333333333, 54.458333333333336, 53.69166666666667, 52.18333333333333, 46.225, 55.625, 56.85, 56.75833333333333, 44.05833333333333, 59.041666666666664, 59.15833333333333, 56.958333333333336, 51.9, 59.24166666666667, 56.65, 61.625, 59.69166666666667, 60.141666666666666, 59.03333333333333, 51.2, 59.083333333333336, 55.25, 58.3, 59.78333333333333, 58.625, 58.675, 55.858333333333334, 60.583333333333336, 60.40833333333333, 59.208333333333336, 56.99166666666667, 61.0, 59.90833333333333, 60.75833333333333, 57.225, 62.75, 61.6, 60.25, 52.675, 59.71666666666667, 61.516666666666666, 60.86666666666667, 56.583333333333336, 64.35833333333333, 62.666666666666664, 57.666666666666664, 60.43333333333333, 62.43333333333333, 62.74166666666667, 63.75833333333333, 60.69166666666667, 64.89166666666667, 63.275, 64.41666666666667, 59.95, 64.39166666666667, 62.65833333333333, 64.31666666666666, 64.15, 59.975, 60.925, 62.208333333333336, 63.375, 63.30833333333333, 63.208333333333336, 59.6, 64.84166666666667, 62.18333333333333, 59.166666666666664, 62.9, 62.25, 63.358333333333334, 63.483333333333334, 66.8, 63.99166666666667, 63.28333333333333, 62.6, 64.25833333333334, 67.11666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.269, Test loss: 2.259, Test accuracy: 21.86
Final Round, Global train loss: 2.269, Global test loss: 2.260, Global test accuracy: 21.63
Average accuracy final 10 rounds: 18.28325 

Average global accuracy final 10 rounds: 19.559 

4996.554930925369
[4.836015939712524, 10.014759063720703, 15.191001892089844, 20.354210138320923, 25.574259757995605, 30.78663730621338, 35.97541284561157, 41.203312158584595, 46.38679337501526, 50.97973012924194, 55.54980659484863, 60.08367395401001, 64.601402759552, 69.12454605102539, 73.68774557113647, 78.28975796699524, 82.94846153259277, 87.55504846572876, 92.19147634506226, 96.85617232322693, 101.49128866195679, 106.05291271209717, 110.65704226493835, 115.26262378692627, 119.82554244995117, 124.3844907283783, 128.96346306800842, 133.52367687225342, 138.0447540283203, 142.56916403770447, 147.13453555107117, 151.7032744884491, 156.25850462913513, 160.77314615249634, 165.2999210357666, 169.84044742584229, 174.39635801315308, 178.92869901657104, 183.42457032203674, 187.90889596939087, 192.4152331352234, 196.93618965148926, 201.46435809135437, 206.0005979537964, 210.5140025615692, 215.02591681480408, 219.5549132823944, 224.0870542526245, 228.60482048988342, 233.11274027824402, 237.61447167396545, 242.1334159374237, 246.65965628623962, 251.15383744239807, 255.64691543579102, 260.15219497680664, 264.65538144111633, 269.1909296512604, 273.68223333358765, 278.1788878440857, 282.6616551876068, 287.1647198200226, 291.67244124412537, 296.1849796772003, 300.69342517852783, 305.21436834335327, 309.72643852233887, 314.25166416168213, 318.7755672931671, 323.30641913414, 327.85049510002136, 332.36672282218933, 336.89860367774963, 341.41953206062317, 345.9467680454254, 350.442923784256, 354.94568061828613, 359.4545929431915, 363.988285779953, 368.5129554271698, 373.0293824672699, 377.57348442077637, 382.1187801361084, 386.67449855804443, 391.2131426334381, 395.78987884521484, 400.3442804813385, 404.8918204307556, 409.412544965744, 413.9178795814514, 418.4282374382019, 422.93745851516724, 427.45438504219055, 431.9789733886719, 436.5135598182678, 441.0606849193573, 445.60584568977356, 450.16298484802246, 454.72789907455444, 459.2825231552124, 461.5514483451843]
[9.6825, 9.6975, 9.71, 9.69, 9.755, 9.75, 9.8325, 9.8375, 9.815, 9.8675, 9.885, 9.9075, 10.005, 10.06, 10.1625, 10.3025, 10.2325, 10.1825, 10.2775, 10.3075, 10.3925, 10.6325, 10.88, 11.155, 11.325, 11.6025, 11.84, 12.4475, 12.5625, 13.015, 13.8775, 14.36, 14.9925, 15.01, 15.325, 15.4425, 15.3925, 15.0125, 15.215, 15.12, 14.9825, 14.9975, 15.0725, 14.3225, 14.6325, 14.485, 14.6, 14.3075, 14.64, 14.5925, 14.41, 15.205, 14.8, 14.585, 13.745, 13.1125, 12.7125, 12.6325, 12.27, 12.2025, 12.075, 11.95, 11.76, 11.77, 11.675, 11.295, 11.0375, 11.035, 11.25, 10.94, 10.8975, 10.8525, 10.7525, 10.7875, 10.97, 11.485, 11.53, 11.3575, 11.3475, 11.42, 11.6725, 11.8175, 11.85, 11.8975, 12.495, 12.6925, 13.2925, 13.1925, 13.7025, 14.1125, 15.0425, 15.895, 16.6975, 17.8275, 18.41, 18.7325, 19.3175, 19.945, 20.3475, 20.6175, 21.8625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.249, Test loss: 1.080, Test accuracy: 73.84
Final Round, Global train loss: 0.249, Global test loss: 1.111, Global test accuracy: 70.44
Average accuracy final 10 rounds: 73.16025 

Average global accuracy final 10 rounds: 69.89099999999999 

6176.949338674545
[4.880629539489746, 9.761259078979492, 14.505727291107178, 19.250195503234863, 23.415972232818604, 27.581748962402344, 31.77610182762146, 35.970454692840576, 40.15034294128418, 44.33023118972778, 48.527586460113525, 52.72494173049927, 56.92723488807678, 61.1295280456543, 65.312007188797, 69.4944863319397, 73.66690802574158, 77.83932971954346, 82.01739311218262, 86.19545650482178, 90.73012208938599, 95.2647876739502, 99.54436659812927, 103.82394552230835, 108.1315598487854, 112.43917417526245, 116.74326658248901, 121.04735898971558, 125.56756114959717, 130.08776330947876, 135.03756260871887, 139.98736190795898, 144.9384560585022, 149.8895502090454, 154.8752646446228, 159.8609790802002, 164.8221127986908, 169.7832465171814, 174.75461196899414, 179.72597742080688, 184.69669127464294, 189.667405128479, 194.6258101463318, 199.58421516418457, 204.51067519187927, 209.43713521957397, 214.39804792404175, 219.35896062850952, 224.29716563224792, 229.23537063598633, 234.19711542129517, 239.158860206604, 244.13781070709229, 249.11676120758057, 254.01546025276184, 258.9141592979431, 263.81566166877747, 268.7171640396118, 273.69770860671997, 278.6782531738281, 283.660845041275, 288.6434369087219, 292.95186591148376, 297.2602949142456, 301.56195282936096, 305.8636107444763, 310.15283250808716, 314.442054271698, 318.74169540405273, 323.04133653640747, 327.34520745277405, 331.6490783691406, 335.9065637588501, 340.16404914855957, 344.43796014785767, 348.71187114715576, 353.0294897556305, 357.3471083641052, 361.64855003356934, 365.94999170303345, 370.26438331604004, 374.57877492904663, 378.863440990448, 383.14810705184937, 387.45726132392883, 391.7664155960083, 396.07000494003296, 400.3735942840576, 404.66657161712646, 408.9595489501953, 413.2122962474823, 417.4650435447693, 421.7509205341339, 426.03679752349854, 430.33838152885437, 434.6399655342102, 438.9522659778595, 443.2645664215088, 447.60487508773804, 451.9451837539673, 456.27556347846985, 460.6059432029724, 464.95433259010315, 469.3027219772339, 473.61715149879456, 477.9315810203552, 482.21486353874207, 486.4981460571289, 490.78921341896057, 495.08028078079224, 499.3707571029663, 503.6612334251404, 507.9946799278259, 512.3281264305115, 516.615110874176, 520.9020953178406, 525.2383644580841, 529.5746335983276, 533.9013800621033, 538.2281265258789, 542.5476560592651, 546.8671855926514, 551.2040855884552, 555.540985584259, 559.8830528259277, 564.2251200675964, 568.5420446395874, 572.8589692115784, 577.1773266792297, 581.4956841468811, 585.7843751907349, 590.0730662345886, 594.3740453720093, 598.6750245094299, 602.9917402267456, 607.3084559440613, 611.5987758636475, 615.8890957832336, 620.209600687027, 624.5301055908203, 628.8609812259674, 633.1918568611145, 637.5388760566711, 641.8858952522278, 646.3404936790466, 650.7950921058655, 655.0971739292145, 659.3992557525635, 663.6945536136627, 667.989851474762, 672.2785694599152, 676.5672874450684, 680.8670227527618, 685.1667580604553, 690.1479294300079, 695.1291007995605, 700.1089515686035, 705.0888023376465, 710.0869359970093, 715.0850696563721, 720.0438210964203, 725.0025725364685, 729.9852254390717, 734.9678783416748, 739.9780216217041, 744.9881649017334, 749.9530692100525, 754.9179735183716, 759.8944070339203, 764.870840549469, 769.8544294834137, 774.8380184173584, 779.8733184337616, 784.9086184501648, 789.9167954921722, 794.9249725341797, 799.9514756202698, 804.9779787063599, 809.9962193965912, 815.0144600868225, 820.0430510044098, 825.0716419219971, 830.073344707489, 835.075047492981, 839.9823346138, 844.8896217346191, 849.2412593364716, 853.592896938324, 857.9509778022766, 862.3090586662292, 866.6541101932526, 870.9991617202759, 875.3654680252075, 879.7317743301392, 884.1430637836456, 888.5543532371521, 892.9486882686615, 897.3430233001709, 901.7263441085815, 906.1096649169922, 908.3030169010162, 910.4963688850403]
[35.165, 35.165, 41.5275, 41.5275, 44.13, 44.13, 46.7975, 46.7975, 49.28, 49.28, 51.2225, 51.2225, 52.9125, 52.9125, 55.1575, 55.1575, 56.1625, 56.1625, 57.2275, 57.2275, 57.605, 57.605, 59.6825, 59.6825, 60.0425, 60.0425, 60.5825, 60.5825, 60.935, 60.935, 61.2075, 61.2075, 61.855, 61.855, 63.7675, 63.7675, 64.0475, 64.0475, 64.5475, 64.5475, 65.875, 65.875, 66.4475, 66.4475, 66.845, 66.845, 67.0175, 67.0175, 67.2825, 67.2825, 67.3475, 67.3475, 67.75, 67.75, 67.855, 67.855, 68.3275, 68.3275, 68.6625, 68.6625, 68.26, 68.26, 68.715, 68.715, 68.9375, 68.9375, 69.5225, 69.5225, 69.825, 69.825, 69.775, 69.775, 69.58, 69.58, 69.56, 69.56, 69.585, 69.585, 69.65, 69.65, 69.9925, 69.9925, 70.045, 70.045, 70.07, 70.07, 70.425, 70.425, 70.6325, 70.6325, 70.4525, 70.4525, 70.7375, 70.7375, 70.8225, 70.8225, 70.53, 70.53, 70.6575, 70.6575, 70.865, 70.865, 71.2975, 71.2975, 71.085, 71.085, 71.045, 71.045, 71.1925, 71.1925, 71.2275, 71.2275, 71.255, 71.255, 71.515, 71.515, 71.44, 71.44, 71.6, 71.6, 71.6225, 71.6225, 71.7625, 71.7625, 71.83, 71.83, 71.8475, 71.8475, 71.895, 71.895, 72.035, 72.035, 72.15, 72.15, 72.33, 72.33, 72.78, 72.78, 72.61, 72.61, 72.3925, 72.3925, 72.6225, 72.6225, 72.6175, 72.6175, 72.4325, 72.4325, 72.6975, 72.6975, 72.36, 72.36, 72.2725, 72.2725, 72.4725, 72.4725, 72.755, 72.755, 72.8025, 72.8025, 72.6325, 72.6325, 72.785, 72.785, 73.0625, 73.0625, 72.9125, 72.9125, 72.81, 72.81, 73.01, 73.01, 73.005, 73.005, 73.0925, 73.0925, 73.225, 73.225, 72.9625, 72.9625, 73.025, 73.025, 72.98, 72.98, 73.3375, 73.3375, 73.3, 73.3, 73.2625, 73.2625, 73.195, 73.195, 73.1225, 73.1225, 73.255, 73.255, 73.0025, 73.0025, 73.1225, 73.1225, 73.8425, 73.8425]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.395, Test loss: 0.791, Test accuracy: 75.89
Average accuracy final 10 rounds: 75.819 

4698.754000663757
[4.681746959686279, 9.363493919372559, 13.64437985420227, 17.925265789031982, 22.28435182571411, 26.64343786239624, 30.91736912727356, 35.19130039215088, 39.518308877944946, 43.845317363739014, 48.13879060745239, 52.43226385116577, 56.78060960769653, 61.128955364227295, 65.53133797645569, 69.93372058868408, 73.96262049674988, 77.99152040481567, 81.98938322067261, 85.98724603652954, 90.18364882469177, 94.380051612854, 98.46524572372437, 102.55043983459473, 106.56457853317261, 110.57871723175049, 114.59337973594666, 118.60804224014282, 122.66784811019897, 126.72765398025513, 130.80190682411194, 134.87615966796875, 138.92705821990967, 142.9779567718506, 146.99307370185852, 151.00819063186646, 155.07670783996582, 159.14522504806519, 163.1769859790802, 167.20874691009521, 171.22556400299072, 175.24238109588623, 179.23099541664124, 183.21960973739624, 187.2644624710083, 191.30931520462036, 195.39130783081055, 199.47330045700073, 203.5380141735077, 207.60272789001465, 211.6351912021637, 215.66765451431274, 219.7725224494934, 223.87739038467407, 227.95783710479736, 232.03828382492065, 236.0986623764038, 240.15904092788696, 244.1952040195465, 248.23136711120605, 252.2812578678131, 256.33114862442017, 260.5635359287262, 264.7959232330322, 268.84893774986267, 272.9019522666931, 276.9102499485016, 280.91854763031006, 284.98650336265564, 289.0544590950012, 293.1088581085205, 297.1632571220398, 301.18400287628174, 305.2047486305237, 309.2535800933838, 313.3024115562439, 317.3902266025543, 321.47804164886475, 325.51802682876587, 329.558012008667, 333.56341314315796, 337.5688142776489, 341.61981225013733, 345.67081022262573, 349.7568702697754, 353.84293031692505, 357.8861243724823, 361.92931842803955, 365.9457519054413, 369.962185382843, 374.0379707813263, 378.11375617980957, 382.17664670944214, 386.2395372390747, 390.311105966568, 394.3826746940613, 398.4265236854553, 402.47037267684937, 406.50649976730347, 410.54262685775757, 414.61518263816833, 418.6877384185791, 422.72990250587463, 426.77206659317017, 430.81751704216003, 434.8629674911499, 438.89216685295105, 442.9213662147522, 446.9970531463623, 451.0727400779724, 455.1318392753601, 459.1909384727478, 463.2411060333252, 467.2912735939026, 471.3117501735687, 475.33222675323486, 479.3775749206543, 483.42292308807373, 487.4713010787964, 491.51967906951904, 495.5448260307312, 499.56997299194336, 503.5894250869751, 507.60887718200684, 511.6450619697571, 515.6812467575073, 519.7170543670654, 523.7528619766235, 527.8132832050323, 531.8737044334412, 535.9685525894165, 540.0634007453918, 544.0908479690552, 548.1182951927185, 552.1654231548309, 556.2125511169434, 560.2620165348053, 564.3114819526672, 568.3244144916534, 572.3373470306396, 576.3765285015106, 580.4157099723816, 584.4972760677338, 588.5788421630859, 592.5962371826172, 596.6136322021484, 600.6166360378265, 604.6196398735046, 608.6635956764221, 612.7075514793396, 616.7494568824768, 620.791362285614, 624.858270406723, 628.925178527832, 632.9770767688751, 637.0289750099182, 641.0564005374908, 645.0838260650635, 649.1564567089081, 653.2290873527527, 657.2477378845215, 661.2663884162903, 665.281670331955, 669.2969522476196, 673.3209958076477, 677.3450393676758, 681.4045407772064, 685.4640421867371, 689.5505657196045, 693.6370892524719, 697.6719105243683, 701.7067317962646, 705.7579429149628, 709.8091540336609, 713.8856754302979, 717.9621968269348, 722.0448288917542, 726.1274609565735, 730.1917772293091, 734.2560935020447, 738.2561056613922, 742.2561178207397, 746.2897481918335, 750.3233785629272, 754.4206337928772, 758.5178890228271, 762.565699338913, 766.6135096549988, 770.6540195941925, 774.6945295333862, 778.7663633823395, 782.8381972312927, 786.8946418762207, 790.9510865211487, 795.0226442813873, 799.094202041626, 803.1281344890594, 807.1620669364929, 811.24445271492, 815.3268384933472, 817.2533288002014, 819.1798191070557]
[28.2875, 28.2875, 36.4775, 36.4775, 42.8875, 42.8875, 45.45, 45.45, 49.0325, 49.0325, 52.015, 52.015, 54.12, 54.12, 56.155, 56.155, 57.695, 57.695, 58.6625, 58.6625, 60.115, 60.115, 61.6975, 61.6975, 62.8425, 62.8425, 64.37, 64.37, 64.0575, 64.0575, 65.55, 65.55, 66.24, 66.24, 67.0075, 67.0075, 67.3075, 67.3075, 67.4675, 67.4675, 67.9925, 67.9925, 67.7275, 67.7275, 68.825, 68.825, 69.8075, 69.8075, 70.5075, 70.5075, 70.3975, 70.3975, 70.5875, 70.5875, 70.665, 70.665, 70.785, 70.785, 71.1525, 71.1525, 71.29, 71.29, 71.4, 71.4, 71.5475, 71.5475, 71.6775, 71.6775, 72.2825, 72.2825, 72.725, 72.725, 72.66, 72.66, 73.0075, 73.0075, 72.88, 72.88, 73.06, 73.06, 73.6525, 73.6525, 73.605, 73.605, 73.1225, 73.1225, 73.955, 73.955, 73.9325, 73.9325, 73.81, 73.81, 73.93, 73.93, 74.12, 74.12, 74.09, 74.09, 74.245, 74.245, 74.69, 74.69, 74.6275, 74.6275, 74.1225, 74.1225, 74.2925, 74.2925, 74.21, 74.21, 74.6825, 74.6825, 74.7325, 74.7325, 74.9425, 74.9425, 75.005, 75.005, 75.0725, 75.0725, 75.2925, 75.2925, 75.02, 75.02, 75.185, 75.185, 75.1525, 75.1525, 74.54, 74.54, 74.67, 74.67, 74.7825, 74.7825, 75.0, 75.0, 75.5775, 75.5775, 75.625, 75.625, 75.3725, 75.3725, 75.7125, 75.7125, 75.5075, 75.5075, 75.595, 75.595, 75.795, 75.795, 75.965, 75.965, 75.925, 75.925, 75.7525, 75.7525, 75.52, 75.52, 75.88, 75.88, 75.65, 75.65, 75.65, 75.65, 75.5275, 75.5275, 75.7, 75.7, 75.855, 75.855, 75.73, 75.73, 75.1925, 75.1925, 75.95, 75.95, 75.47, 75.47, 75.7875, 75.7875, 75.18, 75.18, 75.7575, 75.7575, 75.7675, 75.7675, 75.7325, 75.7325, 75.9425, 75.9425, 76.085, 76.085, 75.765, 75.765, 75.745, 75.745, 76.265, 76.265, 75.95, 75.95, 75.885, 75.885]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.421, Test loss: 0.734, Test accuracy: 76.68
Average accuracy final 10 rounds: 76.51375
5729.493093013763
[5.858027219772339, 11.716054439544678, 17.224242448806763, 22.732430458068848, 28.346469163894653, 33.96050786972046, 39.56540131568909, 45.170294761657715, 50.79260039329529, 56.41490602493286, 62.02589988708496, 67.63689374923706, 73.28641295433044, 78.93593215942383, 84.52060413360596, 90.10527610778809, 95.81129455566406, 101.51731300354004, 107.25073623657227, 112.98415946960449, 118.63139843940735, 124.2786374092102, 129.94045758247375, 135.6022777557373, 141.3449821472168, 147.0876865386963, 152.66323566436768, 158.23878479003906, 163.86002373695374, 169.4812626838684, 175.17953896522522, 180.87781524658203, 186.63739728927612, 192.39697933197021, 198.15767765045166, 203.9183759689331, 209.67644739151, 215.4345188140869, 221.22003698349, 227.00555515289307, 232.74082851409912, 238.47610187530518, 244.2998878955841, 250.12367391586304, 255.8375940322876, 261.55151414871216, 267.2986888885498, 273.04586362838745, 278.83865904808044, 284.63145446777344, 290.40572023391724, 296.17998600006104, 301.7798182964325, 307.37965059280396, 312.9339463710785, 318.488242149353, 324.3735806941986, 330.2589192390442, 336.2261757850647, 342.1934323310852, 348.11956238746643, 354.04569244384766, 359.93847918510437, 365.8312659263611, 371.82528352737427, 377.81930112838745, 383.7254545688629, 389.6316080093384, 395.6262984275818, 401.6209888458252, 407.5577793121338, 413.4945697784424, 419.37511134147644, 425.2556529045105, 431.1519844532013, 437.0483160018921, 442.92802453041077, 448.80773305892944, 454.84409189224243, 460.8804507255554, 466.75913071632385, 472.6378107070923, 478.56364393234253, 484.4894771575928, 490.5418028831482, 496.5941286087036, 502.5872700214386, 508.5804114341736, 514.5117034912109, 520.4429955482483, 526.4350934028625, 532.4271912574768, 538.4413390159607, 544.4554867744446, 550.321900844574, 556.1883149147034, 562.1949708461761, 568.2016267776489, 574.1166868209839, 580.0317468643188, 585.9273035526276, 591.8228602409363, 597.7123262882233, 603.6017923355103, 609.4441111087799, 615.2864298820496, 621.2626872062683, 627.2389445304871, 633.1747059822083, 639.1104674339294, 645.1893174648285, 651.2681674957275, 656.8891394138336, 662.5101113319397, 668.3025455474854, 674.094979763031, 679.8890135288239, 685.6830472946167, 691.4716463088989, 697.2602453231812, 703.0231556892395, 708.7860660552979, 714.7905471324921, 720.7950282096863, 726.5555958747864, 732.3161635398865, 738.1224052906036, 743.9286470413208, 749.7098679542542, 755.4910888671875, 761.2952229976654, 767.0993571281433, 772.9343702793121, 778.769383430481, 784.6127915382385, 790.4561996459961, 796.2096729278564, 801.9631462097168, 807.7542803287506, 813.5454144477844, 819.3625340461731, 825.1796536445618, 830.963324546814, 836.7469954490662, 842.5427241325378, 848.3384528160095, 854.1478519439697, 859.9572510719299, 865.1877629756927, 870.4182748794556, 875.6851804256439, 880.9520859718323, 886.435919046402, 891.9197521209717, 897.1989924907684, 902.4782328605652, 907.7980117797852, 913.1177906990051, 918.4027070999146, 923.687623500824, 928.988322019577, 934.2890205383301, 939.6497292518616, 945.0104379653931, 950.2790229320526, 955.5476078987122, 960.8092362880707, 966.0708646774292, 971.3205351829529, 976.5702056884766, 981.8907134532928, 987.2112212181091, 992.5016934871674, 997.7921657562256, 1003.1015620231628, 1008.4109582901001, 1013.7605874538422, 1019.1102166175842, 1024.4079406261444, 1029.7056646347046, 1034.9746267795563, 1040.243588924408, 1045.5318961143494, 1050.8202033042908, 1056.400408744812, 1061.9806141853333, 1067.5825009346008, 1073.1843876838684, 1078.8070793151855, 1084.4297709465027, 1090.1786260604858, 1095.927481174469, 1101.458414554596, 1106.989347934723, 1112.6484806537628, 1118.3076133728027, 1123.9475231170654, 1129.5874328613281, 1135.2094511985779, 1140.8314695358276, 1143.1110050678253, 1145.390540599823]
[29.9, 29.9, 38.375, 38.375, 43.81, 43.81, 48.285, 48.285, 52.6025, 52.6025, 55.1575, 55.1575, 57.34, 57.34, 59.02, 59.02, 60.785, 60.785, 62.0325, 62.0325, 63.92, 63.92, 65.165, 65.165, 65.05, 65.05, 66.0, 66.0, 67.0175, 67.0175, 67.6425, 67.6425, 68.2775, 68.2775, 68.1875, 68.1875, 68.6225, 68.6225, 69.0475, 69.0475, 69.93, 69.93, 69.9475, 69.9475, 70.8725, 70.8725, 71.37, 71.37, 71.56, 71.56, 72.3725, 72.3725, 72.6925, 72.6925, 73.1225, 73.1225, 72.9325, 72.9325, 73.1625, 73.1625, 73.2225, 73.2225, 72.8625, 72.8625, 73.3625, 73.3625, 73.7175, 73.7175, 73.705, 73.705, 73.705, 73.705, 73.71, 73.71, 74.33, 74.33, 74.36, 74.36, 74.3825, 74.3825, 74.1975, 74.1975, 74.125, 74.125, 74.28, 74.28, 74.5075, 74.5075, 74.7875, 74.7875, 74.4775, 74.4775, 74.8975, 74.8975, 74.815, 74.815, 74.7725, 74.7725, 75.0125, 75.0125, 75.2825, 75.2825, 74.9875, 74.9875, 75.31, 75.31, 75.22, 75.22, 75.3875, 75.3875, 75.2025, 75.2025, 74.8475, 74.8475, 74.7375, 74.7375, 75.11, 75.11, 75.24, 75.24, 75.56, 75.56, 76.0375, 76.0375, 75.51, 75.51, 76.285, 76.285, 76.01, 76.01, 75.77, 75.77, 75.7725, 75.7725, 76.235, 76.235, 75.835, 75.835, 76.29, 76.29, 76.22, 76.22, 75.95, 75.95, 75.8175, 75.8175, 75.4925, 75.4925, 76.215, 76.215, 75.7225, 75.7225, 75.875, 75.875, 75.77, 75.77, 75.76, 75.76, 75.975, 75.975, 76.435, 76.435, 76.265, 76.265, 75.9975, 75.9975, 76.415, 76.415, 76.555, 76.555, 76.5575, 76.5575, 76.5175, 76.5175, 76.54, 76.54, 76.355, 76.355, 76.1275, 76.1275, 76.025, 76.025, 76.2325, 76.2325, 76.4025, 76.4025, 76.635, 76.635, 76.5125, 76.5125, 76.255, 76.255, 76.785, 76.785, 76.695, 76.695, 76.8425, 76.8425, 76.7525, 76.7525, 76.6775, 76.6775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.126, Test loss: 2.213, Test accuracy: 18.86
Round   1, Train loss: 1.004, Test loss: 2.191, Test accuracy: 26.55
Round   2, Train loss: 0.910, Test loss: 2.011, Test accuracy: 26.42
Round   3, Train loss: 0.789, Test loss: 1.781, Test accuracy: 31.50
Round   4, Train loss: 0.798, Test loss: 1.764, Test accuracy: 35.80
Round   5, Train loss: 0.748, Test loss: 2.311, Test accuracy: 25.28
Round   6, Train loss: 0.793, Test loss: 1.932, Test accuracy: 27.90
Round   7, Train loss: 0.700, Test loss: 1.672, Test accuracy: 35.42
Round   8, Train loss: 0.706, Test loss: 1.919, Test accuracy: 29.76
Round   9, Train loss: 0.628, Test loss: 1.710, Test accuracy: 36.89
Round  10, Train loss: 0.626, Test loss: 1.606, Test accuracy: 42.85
Round  11, Train loss: 0.629, Test loss: 1.667, Test accuracy: 45.08
Round  12, Train loss: 0.601, Test loss: 1.688, Test accuracy: 39.25
Round  13, Train loss: 0.648, Test loss: 1.723, Test accuracy: 43.42
Round  14, Train loss: 0.563, Test loss: 1.495, Test accuracy: 45.94
Round  15, Train loss: 0.564, Test loss: 1.752, Test accuracy: 40.89
Round  16, Train loss: 0.564, Test loss: 1.769, Test accuracy: 40.19
Round  17, Train loss: 0.532, Test loss: 1.769, Test accuracy: 42.34
Round  18, Train loss: 0.514, Test loss: 1.627, Test accuracy: 49.12
Round  19, Train loss: 0.500, Test loss: 1.476, Test accuracy: 49.76
Round  20, Train loss: 0.464, Test loss: 1.437, Test accuracy: 49.58
Round  21, Train loss: 0.497, Test loss: 2.056, Test accuracy: 34.38
Round  22, Train loss: 0.539, Test loss: 1.613, Test accuracy: 42.98
Round  23, Train loss: 0.553, Test loss: 1.834, Test accuracy: 42.57
Round  24, Train loss: 0.433, Test loss: 1.872, Test accuracy: 41.12
Round  25, Train loss: 0.476, Test loss: 1.603, Test accuracy: 43.91
Round  26, Train loss: 0.467, Test loss: 1.386, Test accuracy: 51.53
Round  27, Train loss: 0.477, Test loss: 1.557, Test accuracy: 46.08
Round  28, Train loss: 0.417, Test loss: 1.507, Test accuracy: 50.98
Round  29, Train loss: 0.477, Test loss: 1.528, Test accuracy: 45.83
Round  30, Train loss: 0.432, Test loss: 1.406, Test accuracy: 52.80
Round  31, Train loss: 0.406, Test loss: 1.416, Test accuracy: 56.09
Round  32, Train loss: 0.356, Test loss: 1.277, Test accuracy: 54.58
Round  33, Train loss: 0.421, Test loss: 1.422, Test accuracy: 50.41
Round  34, Train loss: 0.388, Test loss: 1.498, Test accuracy: 50.99
Round  35, Train loss: 0.429, Test loss: 1.908, Test accuracy: 42.58
Round  36, Train loss: 0.347, Test loss: 1.605, Test accuracy: 49.36
Round  37, Train loss: 0.359, Test loss: 1.485, Test accuracy: 48.77
Round  38, Train loss: 0.400, Test loss: 1.656, Test accuracy: 45.37
Round  39, Train loss: 0.431, Test loss: 1.574, Test accuracy: 47.39
Round  40, Train loss: 0.340, Test loss: 1.561, Test accuracy: 49.05
Round  41, Train loss: 0.379, Test loss: 1.494, Test accuracy: 49.22
Round  42, Train loss: 0.379, Test loss: 1.435, Test accuracy: 52.31
Round  43, Train loss: 0.357, Test loss: 1.406, Test accuracy: 52.36
Round  44, Train loss: 0.330, Test loss: 1.387, Test accuracy: 53.23
Round  45, Train loss: 0.345, Test loss: 1.616, Test accuracy: 50.12
Round  46, Train loss: 0.297, Test loss: 1.720, Test accuracy: 47.22
Round  47, Train loss: 0.306, Test loss: 1.509, Test accuracy: 49.67
Round  48, Train loss: 0.293, Test loss: 1.508, Test accuracy: 51.13
Round  49, Train loss: 0.336, Test loss: 1.364, Test accuracy: 55.33
Round  50, Train loss: 0.311, Test loss: 1.344, Test accuracy: 55.14
Round  51, Train loss: 0.344, Test loss: 1.326, Test accuracy: 54.53
Round  52, Train loss: 0.282, Test loss: 1.620, Test accuracy: 50.62
Round  53, Train loss: 0.312, Test loss: 1.434, Test accuracy: 55.88
Round  54, Train loss: 0.333, Test loss: 1.579, Test accuracy: 49.74
Round  55, Train loss: 0.336, Test loss: 1.639, Test accuracy: 46.63
Round  56, Train loss: 0.303, Test loss: 1.359, Test accuracy: 56.19
Round  57, Train loss: 0.261, Test loss: 1.524, Test accuracy: 52.00
Round  58, Train loss: 0.304, Test loss: 1.361, Test accuracy: 57.24
Round  59, Train loss: 0.283, Test loss: 1.370, Test accuracy: 56.98
Round  60, Train loss: 0.290, Test loss: 1.461, Test accuracy: 53.96
Round  61, Train loss: 0.285, Test loss: 1.559, Test accuracy: 53.83
Round  62, Train loss: 0.316, Test loss: 1.607, Test accuracy: 54.12
Round  63, Train loss: 0.227, Test loss: 1.338, Test accuracy: 58.31
Round  64, Train loss: 0.208, Test loss: 1.634, Test accuracy: 53.55
Round  65, Train loss: 0.274, Test loss: 1.429, Test accuracy: 57.62
Round  66, Train loss: 0.219, Test loss: 1.698, Test accuracy: 51.73
Round  67, Train loss: 0.242, Test loss: 1.573, Test accuracy: 56.06
Round  68, Train loss: 0.229, Test loss: 1.431, Test accuracy: 57.33
Round  69, Train loss: 0.223, Test loss: 1.584, Test accuracy: 51.83
Round  70, Train loss: 0.217, Test loss: 1.329, Test accuracy: 57.60
Round  71, Train loss: 0.188, Test loss: 1.851, Test accuracy: 53.19
Round  72, Train loss: 0.248, Test loss: 1.234, Test accuracy: 59.52
Round  73, Train loss: 0.224, Test loss: 1.378, Test accuracy: 57.82
Round  74, Train loss: 0.210, Test loss: 1.423, Test accuracy: 56.74
Round  75, Train loss: 0.228, Test loss: 1.329, Test accuracy: 56.20
Round  76, Train loss: 0.225, Test loss: 1.514, Test accuracy: 57.56
Round  77, Train loss: 0.230, Test loss: 1.182, Test accuracy: 61.06
Round  78, Train loss: 0.209, Test loss: 1.648, Test accuracy: 52.77
Round  79, Train loss: 0.250, Test loss: 1.525, Test accuracy: 54.68
Round  80, Train loss: 0.252, Test loss: 1.376, Test accuracy: 58.08
Round  81, Train loss: 0.238, Test loss: 1.352, Test accuracy: 59.09
Round  82, Train loss: 0.230, Test loss: 1.365, Test accuracy: 58.46
Round  83, Train loss: 0.217, Test loss: 1.428, Test accuracy: 56.82
Round  84, Train loss: 0.255, Test loss: 1.519, Test accuracy: 55.76
Round  85, Train loss: 0.213, Test loss: 1.711, Test accuracy: 51.61
Round  86, Train loss: 0.164, Test loss: 1.549, Test accuracy: 56.18
Round  87, Train loss: 0.237, Test loss: 1.469, Test accuracy: 56.51
Round  88, Train loss: 0.221, Test loss: 1.335, Test accuracy: 57.26
Round  89, Train loss: 0.169, Test loss: 1.307, Test accuracy: 60.12
Round  90, Train loss: 0.199, Test loss: 2.121, Test accuracy: 47.05
Round  91, Train loss: 0.231, Test loss: 1.365, Test accuracy: 56.63
Round  92, Train loss: 0.188, Test loss: 1.643, Test accuracy: 52.06
Round  93, Train loss: 0.228, Test loss: 1.407, Test accuracy: 58.67
Round  94, Train loss: 0.211, Test loss: 1.657, Test accuracy: 51.34
Round  95, Train loss: 0.246, Test loss: 1.326, Test accuracy: 60.89
Round  96, Train loss: 0.195, Test loss: 1.585, Test accuracy: 53.70
Round  97, Train loss: 0.173, Test loss: 1.795, Test accuracy: 54.96
Round  98, Train loss: 0.209, Test loss: 1.755, Test accuracy: 53.42
Round  99, Train loss: 0.182, Test loss: 1.913, Test accuracy: 50.51
Final Round, Train loss: 0.163, Test loss: 1.258, Test accuracy: 62.69
Average accuracy final 10 rounds: 53.92333333333332
2630.3565442562103
[4.202709913253784, 7.851372718811035, 11.746704339981079, 15.686001062393188, 19.60899543762207, 23.55465054512024, 27.505064725875854, 31.563205003738403, 35.48881435394287, 39.601667404174805, 43.55495619773865, 47.62731194496155, 51.63910102844238, 55.6850700378418, 59.604223012924194, 63.60042643547058, 67.63125586509705, 71.54282259941101, 75.68072700500488, 79.78978252410889, 83.79450249671936, 87.73173880577087, 91.82385969161987, 95.85545682907104, 99.76964807510376, 103.72316575050354, 107.64936184883118, 111.69298887252808, 115.77336311340332, 119.77192950248718, 123.86612224578857, 127.7835807800293, 131.91461730003357, 135.8123495578766, 139.8690459728241, 143.77276134490967, 147.7223343849182, 151.61440515518188, 155.43788623809814, 159.27591347694397, 163.10495400428772, 166.93451237678528, 170.84199810028076, 174.7053952217102, 178.65087604522705, 182.53382968902588, 186.4619882106781, 190.37008690834045, 194.28436541557312, 198.2163383960724, 202.10398316383362, 205.9953429698944, 209.5317234992981, 213.0850121974945, 216.633713722229, 220.19636154174805, 223.7207384109497, 227.30424118041992, 230.84556221961975, 234.4334545135498, 237.95523023605347, 241.46566033363342, 244.97289371490479, 248.45526814460754, 251.97959876060486, 255.4741132259369, 259.0165696144104, 262.546674489975, 266.0285370349884, 269.7282910346985, 273.2033631801605, 276.76283502578735, 280.25997972488403, 283.95300340652466, 287.4570326805115, 291.05885910987854, 294.57273960113525, 298.09868717193604, 301.59148645401, 305.1275942325592, 308.65627431869507, 312.1495797634125, 315.809992313385, 319.2887644767761, 322.90359711647034, 326.53894805908203, 330.0474259853363, 333.8422420024872, 337.29073452949524, 341.07898449897766, 344.65685391426086, 348.20157384872437, 351.80060148239136, 355.35245299339294, 358.9416534900665, 362.5097179412842, 366.0762941837311, 369.6194553375244, 373.1313307285309, 376.620463848114, 379.5437602996826]
[18.858333333333334, 26.55, 26.416666666666668, 31.5, 35.8, 25.283333333333335, 27.9, 35.425, 29.758333333333333, 36.891666666666666, 42.85, 45.075, 39.25, 43.416666666666664, 45.94166666666667, 40.891666666666666, 40.19166666666667, 42.34166666666667, 49.125, 49.75833333333333, 49.583333333333336, 34.375, 42.975, 42.56666666666667, 41.11666666666667, 43.90833333333333, 51.53333333333333, 46.083333333333336, 50.983333333333334, 45.825, 52.8, 56.09166666666667, 54.583333333333336, 50.40833333333333, 50.99166666666667, 42.583333333333336, 49.358333333333334, 48.775, 45.36666666666667, 47.391666666666666, 49.05, 49.21666666666667, 52.30833333333333, 52.358333333333334, 53.233333333333334, 50.11666666666667, 47.21666666666667, 49.675, 51.13333333333333, 55.333333333333336, 55.141666666666666, 54.53333333333333, 50.61666666666667, 55.875, 49.74166666666667, 46.63333333333333, 56.19166666666667, 52.0, 57.24166666666667, 56.983333333333334, 53.958333333333336, 53.825, 54.11666666666667, 58.30833333333333, 53.55, 57.61666666666667, 51.725, 56.05833333333333, 57.325, 51.825, 57.6, 53.19166666666667, 59.525, 57.81666666666667, 56.74166666666667, 56.2, 57.55833333333333, 61.05833333333333, 52.775, 54.68333333333333, 58.083333333333336, 59.09166666666667, 58.458333333333336, 56.81666666666667, 55.75833333333333, 51.608333333333334, 56.18333333333333, 56.50833333333333, 57.25833333333333, 60.11666666666667, 47.05, 56.63333333333333, 52.05833333333333, 58.666666666666664, 51.34166666666667, 60.891666666666666, 53.7, 54.958333333333336, 53.425, 50.50833333333333, 62.69166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.247, Test loss: 2.291, Test accuracy: 14.54
Round   0, Global train loss: 2.247, Global test loss: 2.294, Global test accuracy: 14.29
Round   1, Train loss: 2.265, Test loss: 2.291, Test accuracy: 14.75
Round   1, Global train loss: 2.265, Global test loss: 2.293, Global test accuracy: 13.96
Round   2, Train loss: 2.224, Test loss: 2.289, Test accuracy: 14.38
Round   2, Global train loss: 2.224, Global test loss: 2.294, Global test accuracy: 12.70
Round   3, Train loss: 2.275, Test loss: 2.292, Test accuracy: 12.57
Round   3, Global train loss: 2.275, Global test loss: 2.295, Global test accuracy: 12.46
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 13.37
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 9.78
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 12.03
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 11.79
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 11.79
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 9.86
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 9.86
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 9.86
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 8.33
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 8.33
Average accuracy final 10 rounds: 8.333333333333332 

Average global accuracy final 10 rounds: 8.333333333333332 

1632.3940284252167
[1.8310983180999756, 3.4056904315948486, 4.9883856773376465, 6.601240396499634, 8.203083038330078, 9.801031351089478, 11.399615287780762, 13.028984308242798, 14.625227451324463, 16.229095935821533, 17.836008310317993, 19.453379154205322, 20.86506962776184, 22.274474620819092, 23.69604992866516, 25.127878665924072, 26.553983688354492, 27.97185468673706, 29.386375427246094, 30.82649040222168, 32.213702917099, 33.59995985031128, 34.98509120941162, 36.36836528778076, 37.74586319923401, 39.172245502471924, 40.586021423339844, 41.98933720588684, 43.401922941207886, 44.81363368034363, 46.32232046127319, 47.73813605308533, 49.1601243019104, 50.63359475135803, 52.06446647644043, 53.47956418991089, 54.90571737289429, 56.42087769508362, 57.925936222076416, 59.351040840148926, 60.90223741531372, 62.37584614753723, 63.8500292301178, 65.34289145469666, 66.81640815734863, 68.23939061164856, 69.67977738380432, 71.1311604976654, 72.57243704795837, 73.99303603172302, 75.60755968093872, 77.23637843132019, 78.8543872833252, 80.47108387947083, 81.90506386756897, 83.34989547729492, 84.78845286369324, 86.27435159683228, 87.75099492073059, 89.20128297805786, 90.6804769039154, 92.14869546890259, 93.72863936424255, 95.28981423377991, 96.9233067035675, 98.41295051574707, 99.95727610588074, 101.70112466812134, 103.52804851531982, 104.99932432174683, 106.62466430664062, 108.28553199768066, 109.91079258918762, 111.60266900062561, 113.2151529788971, 114.76086974143982, 116.23307991027832, 117.6798529624939, 119.20784187316895, 120.7357428073883, 122.17875862121582, 123.73353481292725, 125.26528310775757, 126.69932007789612, 128.18280458450317, 129.63534450531006, 131.02879309654236, 132.5333297252655, 134.08120489120483, 135.5061252117157, 136.9619596004486, 138.42807745933533, 139.80589723587036, 141.22620964050293, 142.69991993904114, 144.16760087013245, 145.67608833312988, 147.0788836479187, 148.55488061904907, 150.07604241371155, 152.5102436542511]
[14.541666666666666, 14.75, 14.375, 12.566666666666666, 13.366666666666667, 9.775, 12.025, 11.791666666666666, 11.791666666666666, 9.858333333333333, 9.858333333333333, 9.858333333333333, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334, 8.333333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.978, Test loss: 1.935, Test accuracy: 31.94
Round   0, Global train loss: 1.978, Global test loss: 1.997, Global test accuracy: 31.01
Round   1, Train loss: 1.707, Test loss: 1.754, Test accuracy: 38.49
Round   1, Global train loss: 1.707, Global test loss: 1.836, Global test accuracy: 38.55
Round   2, Train loss: 1.567, Test loss: 1.646, Test accuracy: 42.24
Round   2, Global train loss: 1.567, Global test loss: 1.759, Global test accuracy: 41.97
Round   3, Train loss: 1.478, Test loss: 1.544, Test accuracy: 44.87
Round   3, Global train loss: 1.478, Global test loss: 1.681, Global test accuracy: 44.89
Round   4, Train loss: 1.381, Test loss: 1.474, Test accuracy: 47.92
Round   4, Global train loss: 1.381, Global test loss: 1.675, Global test accuracy: 46.49
Round   5, Train loss: 1.314, Test loss: 1.366, Test accuracy: 51.65
Round   5, Global train loss: 1.314, Global test loss: 1.583, Global test accuracy: 47.81
Round   6, Train loss: 1.246, Test loss: 1.337, Test accuracy: 52.55
Round   6, Global train loss: 1.246, Global test loss: 1.582, Global test accuracy: 48.31
Round   7, Train loss: 1.199, Test loss: 1.287, Test accuracy: 54.60
Round   7, Global train loss: 1.199, Global test loss: 1.508, Global test accuracy: 49.82
Round   8, Train loss: 1.149, Test loss: 1.255, Test accuracy: 55.78
Round   8, Global train loss: 1.149, Global test loss: 1.567, Global test accuracy: 50.45
Round   9, Train loss: 1.111, Test loss: 1.235, Test accuracy: 56.76
Round   9, Global train loss: 1.111, Global test loss: 1.524, Global test accuracy: 50.61
Round  10, Train loss: 1.078, Test loss: 1.186, Test accuracy: 58.57
Round  10, Global train loss: 1.078, Global test loss: 1.537, Global test accuracy: 53.03
Round  11, Train loss: 1.047, Test loss: 1.140, Test accuracy: 60.34
Round  11, Global train loss: 1.047, Global test loss: 1.381, Global test accuracy: 53.43
Round  12, Train loss: 1.009, Test loss: 1.122, Test accuracy: 61.21
Round  12, Global train loss: 1.009, Global test loss: 1.450, Global test accuracy: 52.66
Round  13, Train loss: 0.974, Test loss: 1.107, Test accuracy: 61.97
Round  13, Global train loss: 0.974, Global test loss: 1.481, Global test accuracy: 53.78
Round  14, Train loss: 0.952, Test loss: 1.091, Test accuracy: 62.67
Round  14, Global train loss: 0.952, Global test loss: 1.511, Global test accuracy: 53.26
Round  15, Train loss: 0.928, Test loss: 1.081, Test accuracy: 63.24
Round  15, Global train loss: 0.928, Global test loss: 1.580, Global test accuracy: 54.65
Round  16, Train loss: 0.901, Test loss: 1.077, Test accuracy: 63.67
Round  16, Global train loss: 0.901, Global test loss: 1.577, Global test accuracy: 55.87
Round  17, Train loss: 0.883, Test loss: 1.080, Test accuracy: 63.79
Round  17, Global train loss: 0.883, Global test loss: 1.402, Global test accuracy: 55.06
Round  18, Train loss: 0.849, Test loss: 1.072, Test accuracy: 64.12
Round  18, Global train loss: 0.849, Global test loss: 1.441, Global test accuracy: 53.40
Round  19, Train loss: 0.854, Test loss: 1.053, Test accuracy: 64.75
Round  19, Global train loss: 0.854, Global test loss: 1.464, Global test accuracy: 56.04
Round  20, Train loss: 0.830, Test loss: 1.037, Test accuracy: 65.69
Round  20, Global train loss: 0.830, Global test loss: 1.463, Global test accuracy: 56.09
Round  21, Train loss: 0.807, Test loss: 1.039, Test accuracy: 65.83
Round  21, Global train loss: 0.807, Global test loss: 1.394, Global test accuracy: 55.06
Round  22, Train loss: 0.788, Test loss: 1.038, Test accuracy: 66.06
Round  22, Global train loss: 0.788, Global test loss: 1.442, Global test accuracy: 55.40
Round  23, Train loss: 0.805, Test loss: 1.018, Test accuracy: 66.89
Round  23, Global train loss: 0.805, Global test loss: 1.515, Global test accuracy: 57.89
Round  24, Train loss: 0.757, Test loss: 1.008, Test accuracy: 67.23
Round  24, Global train loss: 0.757, Global test loss: 1.502, Global test accuracy: 57.80
Round  25, Train loss: 0.765, Test loss: 1.001, Test accuracy: 67.60
Round  25, Global train loss: 0.765, Global test loss: 1.599, Global test accuracy: 57.05
Round  26, Train loss: 0.746, Test loss: 0.999, Test accuracy: 67.72
Round  26, Global train loss: 0.746, Global test loss: 1.495, Global test accuracy: 56.27
Round  27, Train loss: 0.737, Test loss: 0.993, Test accuracy: 68.02
Round  27, Global train loss: 0.737, Global test loss: 1.422, Global test accuracy: 56.30
Round  28, Train loss: 0.727, Test loss: 0.966, Test accuracy: 68.86
Round  28, Global train loss: 0.727, Global test loss: 1.492, Global test accuracy: 55.30
Round  29, Train loss: 0.739, Test loss: 0.967, Test accuracy: 69.11
Round  29, Global train loss: 0.739, Global test loss: 1.462, Global test accuracy: 57.59
Round  30, Train loss: 0.718, Test loss: 0.970, Test accuracy: 69.28
Round  30, Global train loss: 0.718, Global test loss: 1.428, Global test accuracy: 55.92
Round  31, Train loss: 0.701, Test loss: 0.975, Test accuracy: 69.23
Round  31, Global train loss: 0.701, Global test loss: 1.488, Global test accuracy: 57.37
Round  32, Train loss: 0.689, Test loss: 0.979, Test accuracy: 69.12
Round  32, Global train loss: 0.689, Global test loss: 1.444, Global test accuracy: 55.99
Round  33, Train loss: 0.698, Test loss: 0.972, Test accuracy: 69.58
Round  33, Global train loss: 0.698, Global test loss: 1.429, Global test accuracy: 58.67
Round  34, Train loss: 0.660, Test loss: 0.969, Test accuracy: 69.69
Round  34, Global train loss: 0.660, Global test loss: 1.538, Global test accuracy: 56.14
Round  35, Train loss: 0.687, Test loss: 0.964, Test accuracy: 69.98
Round  35, Global train loss: 0.687, Global test loss: 1.399, Global test accuracy: 58.55
Round  36, Train loss: 0.649, Test loss: 0.981, Test accuracy: 69.77
Round  36, Global train loss: 0.649, Global test loss: 1.594, Global test accuracy: 57.90
Round  37, Train loss: 0.648, Test loss: 0.969, Test accuracy: 69.91
Round  37, Global train loss: 0.648, Global test loss: 1.554, Global test accuracy: 58.76
Round  38, Train loss: 0.658, Test loss: 0.962, Test accuracy: 70.16
Round  38, Global train loss: 0.658, Global test loss: 1.440, Global test accuracy: 58.70
Round  39, Train loss: 0.629, Test loss: 0.961, Test accuracy: 70.27
Round  39, Global train loss: 0.629, Global test loss: 1.924, Global test accuracy: 59.42
Round  40, Train loss: 0.618, Test loss: 0.958, Test accuracy: 70.51
Round  40, Global train loss: 0.618, Global test loss: 1.490, Global test accuracy: 57.88
Round  41, Train loss: 0.621, Test loss: 0.961, Test accuracy: 70.53
Round  41, Global train loss: 0.621, Global test loss: 1.437, Global test accuracy: 57.81
Round  42, Train loss: 0.623, Test loss: 0.951, Test accuracy: 70.98
Round  42, Global train loss: 0.623, Global test loss: 1.598, Global test accuracy: 57.76
Round  43, Train loss: 0.604, Test loss: 0.947, Test accuracy: 71.06
Round  43, Global train loss: 0.604, Global test loss: 1.495, Global test accuracy: 57.85
Round  44, Train loss: 0.598, Test loss: 0.949, Test accuracy: 71.08
Round  44, Global train loss: 0.598, Global test loss: 1.550, Global test accuracy: 59.18
Round  45, Train loss: 0.601, Test loss: 0.965, Test accuracy: 70.98
Round  45, Global train loss: 0.601, Global test loss: 1.557, Global test accuracy: 57.23
Round  46, Train loss: 0.589, Test loss: 0.965, Test accuracy: 71.08
Round  46, Global train loss: 0.589, Global test loss: 1.651, Global test accuracy: 59.12
Round  47, Train loss: 0.574, Test loss: 0.960, Test accuracy: 71.28
Round  47, Global train loss: 0.574, Global test loss: 1.471, Global test accuracy: 57.10
Round  48, Train loss: 0.576, Test loss: 0.966, Test accuracy: 71.01
Round  48, Global train loss: 0.576, Global test loss: 1.496, Global test accuracy: 57.49
Round  49, Train loss: 0.574, Test loss: 0.972, Test accuracy: 71.11
Round  49, Global train loss: 0.574, Global test loss: 1.443, Global test accuracy: 58.98
Round  50, Train loss: 0.566, Test loss: 0.962, Test accuracy: 71.33
Round  50, Global train loss: 0.566, Global test loss: 1.479, Global test accuracy: 59.90
Round  51, Train loss: 0.543, Test loss: 0.960, Test accuracy: 71.41
Round  51, Global train loss: 0.543, Global test loss: 1.611, Global test accuracy: 57.99
Round  52, Train loss: 0.555, Test loss: 0.972, Test accuracy: 71.27
Round  52, Global train loss: 0.555, Global test loss: 1.523, Global test accuracy: 58.16
Round  53, Train loss: 0.561, Test loss: 0.976, Test accuracy: 71.45
Round  53, Global train loss: 0.561, Global test loss: 1.619, Global test accuracy: 56.69
Round  54, Train loss: 0.585, Test loss: 0.978, Test accuracy: 71.39
Round  54, Global train loss: 0.585, Global test loss: 1.397, Global test accuracy: 59.23
Round  55, Train loss: 0.522, Test loss: 0.963, Test accuracy: 72.08
Round  55, Global train loss: 0.522, Global test loss: 1.637, Global test accuracy: 58.37
Round  56, Train loss: 0.552, Test loss: 0.952, Test accuracy: 72.21
Round  56, Global train loss: 0.552, Global test loss: 1.515, Global test accuracy: 59.45
Round  57, Train loss: 0.546, Test loss: 0.950, Test accuracy: 72.29
Round  57, Global train loss: 0.546, Global test loss: 1.714, Global test accuracy: 59.60
Round  58, Train loss: 0.537, Test loss: 0.957, Test accuracy: 72.21
Round  58, Global train loss: 0.537, Global test loss: 1.614, Global test accuracy: 57.29
Round  59, Train loss: 0.538, Test loss: 0.954, Test accuracy: 72.12
Round  59, Global train loss: 0.538, Global test loss: 1.432, Global test accuracy: 57.78
Round  60, Train loss: 0.537, Test loss: 0.942, Test accuracy: 72.40
Round  60, Global train loss: 0.537, Global test loss: 1.487, Global test accuracy: 58.64
Round  61, Train loss: 0.511, Test loss: 0.952, Test accuracy: 72.20
Round  61, Global train loss: 0.511, Global test loss: 1.556, Global test accuracy: 58.82
Round  62, Train loss: 0.508, Test loss: 0.951, Test accuracy: 72.38
Round  62, Global train loss: 0.508, Global test loss: 1.696, Global test accuracy: 59.46
Round  63, Train loss: 0.536, Test loss: 0.963, Test accuracy: 72.20
Round  63, Global train loss: 0.536, Global test loss: 1.535, Global test accuracy: 59.44
Round  64, Train loss: 0.515, Test loss: 0.961, Test accuracy: 72.17
Round  64, Global train loss: 0.515, Global test loss: 1.575, Global test accuracy: 58.35
Round  65, Train loss: 0.495, Test loss: 0.944, Test accuracy: 72.57
Round  65, Global train loss: 0.495, Global test loss: 1.544, Global test accuracy: 58.97
Round  66, Train loss: 0.526, Test loss: 0.955, Test accuracy: 72.53
Round  66, Global train loss: 0.526, Global test loss: 1.513, Global test accuracy: 59.26
Round  67, Train loss: 0.482, Test loss: 0.955, Test accuracy: 72.47
Round  67, Global train loss: 0.482, Global test loss: 1.453, Global test accuracy: 58.16
Round  68, Train loss: 0.533, Test loss: 0.955, Test accuracy: 72.63
Round  68, Global train loss: 0.533, Global test loss: 1.553, Global test accuracy: 60.14
Round  69, Train loss: 0.491, Test loss: 0.952, Test accuracy: 72.76
Round  69, Global train loss: 0.491, Global test loss: 1.541, Global test accuracy: 58.07
Round  70, Train loss: 0.504, Test loss: 0.955, Test accuracy: 72.72
Round  70, Global train loss: 0.504, Global test loss: 1.539, Global test accuracy: 60.79
Round  71, Train loss: 0.484, Test loss: 0.963, Test accuracy: 72.67
Round  71, Global train loss: 0.484, Global test loss: 1.508, Global test accuracy: 59.12
Round  72, Train loss: 0.489, Test loss: 0.963, Test accuracy: 72.58
Round  72, Global train loss: 0.489, Global test loss: 1.585, Global test accuracy: 57.44
Round  73, Train loss: 0.491, Test loss: 0.974, Test accuracy: 72.42
Round  73, Global train loss: 0.491, Global test loss: 1.532, Global test accuracy: 59.14
Round  74, Train loss: 0.478, Test loss: 0.988, Test accuracy: 72.21
Round  74, Global train loss: 0.478, Global test loss: 1.614, Global test accuracy: 59.69
Round  75, Train loss: 0.498, Test loss: 0.976, Test accuracy: 72.34
Round  75, Global train loss: 0.498, Global test loss: 1.584, Global test accuracy: 59.72
Round  76, Train loss: 0.470, Test loss: 0.969, Test accuracy: 72.39
Round  76, Global train loss: 0.470, Global test loss: 1.631, Global test accuracy: 58.22
Round  77, Train loss: 0.466, Test loss: 0.968, Test accuracy: 72.29
Round  77, Global train loss: 0.466, Global test loss: 1.564, Global test accuracy: 57.97
Round  78, Train loss: 0.468, Test loss: 0.967, Test accuracy: 72.59
Round  78, Global train loss: 0.468, Global test loss: 1.506, Global test accuracy: 58.49
Round  79, Train loss: 0.504, Test loss: 0.962, Test accuracy: 72.82
Round  79, Global train loss: 0.504, Global test loss: 1.422, Global test accuracy: 59.62
Round  80, Train loss: 0.474, Test loss: 0.953, Test accuracy: 73.25
Round  80, Global train loss: 0.474, Global test loss: 1.554, Global test accuracy: 59.10
Round  81, Train loss: 0.483, Test loss: 0.954, Test accuracy: 73.20
Round  81, Global train loss: 0.483, Global test loss: 1.676, Global test accuracy: 59.31
Round  82, Train loss: 0.476, Test loss: 0.952, Test accuracy: 73.29
Round  82, Global train loss: 0.476, Global test loss: 1.536, Global test accuracy: 59.86
Round  83, Train loss: 0.484, Test loss: 0.963, Test accuracy: 73.00
Round  83, Global train loss: 0.484, Global test loss: 1.478, Global test accuracy: 60.02
Round  84, Train loss: 0.450, Test loss: 0.959, Test accuracy: 73.00
Round  84, Global train loss: 0.450, Global test loss: 1.439, Global test accuracy: 57.62
Round  85, Train loss: 0.431, Test loss: 0.973, Test accuracy: 73.14
Round  85, Global train loss: 0.431, Global test loss: 1.839, Global test accuracy: 58.53
Round  86, Train loss: 0.463, Test loss: 0.979, Test accuracy: 73.12
Round  86, Global train loss: 0.463, Global test loss: 1.691, Global test accuracy: 60.34
Round  87, Train loss: 0.447, Test loss: 0.982, Test accuracy: 73.17
Round  87, Global train loss: 0.447, Global test loss: 1.640, Global test accuracy: 56.99
Round  88, Train loss: 0.441, Test loss: 0.984, Test accuracy: 73.08
Round  88, Global train loss: 0.441, Global test loss: 1.526, Global test accuracy: 58.70
Round  89, Train loss: 0.446, Test loss: 0.991, Test accuracy: 72.91
Round  89, Global train loss: 0.446, Global test loss: 1.691, Global test accuracy: 59.92
Round  90, Train loss: 0.453, Test loss: 0.988, Test accuracy: 73.02
Round  90, Global train loss: 0.453, Global test loss: 1.448, Global test accuracy: 59.26
Round  91, Train loss: 0.441, Test loss: 0.981, Test accuracy: 73.19
Round  91, Global train loss: 0.441, Global test loss: 1.564, Global test accuracy: 58.49
Round  92, Train loss: 0.429, Test loss: 0.967, Test accuracy: 73.58
Round  92, Global train loss: 0.429, Global test loss: 1.492, Global test accuracy: 57.40
Round  93, Train loss: 0.464, Test loss: 0.963, Test accuracy: 73.73
Round  93, Global train loss: 0.464, Global test loss: 1.586, Global test accuracy: 59.12
Round  94, Train loss: 0.445, Test loss: 0.953, Test accuracy: 73.81
Round  94, Global train loss: 0.445, Global test loss: 1.465, Global test accuracy: 58.05
Round  95, Train loss: 0.447, Test loss: 0.954, Test accuracy: 73.82
Round  95, Global train loss: 0.447, Global test loss: 1.542, Global test accuracy: 59.20
Round  96, Train loss: 0.418, Test loss: 0.953, Test accuracy: 73.96
Round  96, Global train loss: 0.418, Global test loss: 1.538, Global test accuracy: 59.53
Round  97, Train loss: 0.436, Test loss: 0.956, Test accuracy: 73.87
Round  97, Global train loss: 0.436, Global test loss: 1.727, Global test accuracy: 60.15
Round  98, Train loss: 0.423, Test loss: 0.958, Test accuracy: 73.68
Round  98, Global train loss: 0.423, Global test loss: 1.702, Global test accuracy: 59.70
Round  99, Train loss: 0.431, Test loss: 0.961, Test accuracy: 73.56
Round  99, Global train loss: 0.431, Global test loss: 1.503, Global test accuracy: 58.48
Final Round, Train loss: 0.278, Test loss: 1.116, Test accuracy: 73.45
Final Round, Global train loss: 0.278, Global test loss: 1.503, Global test accuracy: 58.48
Average accuracy final 10 rounds: 73.62049999999999 

Average global accuracy final 10 rounds: 58.93925000000001 

6233.5042181015015
[5.049704313278198, 10.099408626556396, 15.074789047241211, 20.050169467926025, 24.862848043441772, 29.67552661895752, 34.56322956085205, 39.45093250274658, 44.2865686416626, 49.12220478057861, 53.667861461639404, 58.213518142700195, 62.68125009536743, 67.14898204803467, 71.76532077789307, 76.38165950775146, 80.90383458137512, 85.42600965499878, 89.84692215919495, 94.26783466339111, 98.79413270950317, 103.32043075561523, 107.85936069488525, 112.39829063415527, 116.97198486328125, 121.54567909240723, 126.079833984375, 130.61398887634277, 135.09112215042114, 139.5682554244995, 144.08305048942566, 148.5978455543518, 153.11173939704895, 157.6256332397461, 162.13377857208252, 166.64192390441895, 171.6345498561859, 176.62717580795288, 181.06812572479248, 185.50907564163208, 190.0228819847107, 194.5366883277893, 199.48282432556152, 204.42896032333374, 209.3718957901001, 214.31483125686646, 219.0719883441925, 223.82914543151855, 228.37285709381104, 232.91656875610352, 237.7915222644806, 242.66647577285767, 247.06316208839417, 251.45984840393066, 255.98277974128723, 260.5057110786438, 264.9641888141632, 269.4226665496826, 273.90330243110657, 278.3839383125305, 282.85375237464905, 287.3235664367676, 291.8403162956238, 296.35706615448, 300.87045907974243, 305.3838520050049, 309.82278299331665, 314.2617139816284, 318.64418625831604, 323.02665853500366, 327.5882685184479, 332.1498785018921, 336.8448760509491, 341.5398736000061, 346.117023229599, 350.6941728591919, 355.06880164146423, 359.4434304237366, 363.8746063709259, 368.30578231811523, 372.71688747406006, 377.1279926300049, 381.5792489051819, 386.0305051803589, 390.49784684181213, 394.9651885032654, 399.3993844985962, 403.833580493927, 408.28831028938293, 412.74304008483887, 417.121164560318, 421.4992890357971, 426.4112184047699, 431.3231477737427, 436.1567132472992, 440.9902787208557, 445.80984902381897, 450.6294193267822, 455.53322553634644, 460.43703174591064, 465.2090220451355, 469.98101234436035, 474.76199293136597, 479.5429735183716, 484.38172459602356, 489.22047567367554, 493.6794047355652, 498.13833379745483, 502.43589758872986, 506.7334613800049, 510.99466252326965, 515.2558636665344, 520.055525302887, 524.8551869392395, 530.0255651473999, 535.1959433555603, 540.1163501739502, 545.0367569923401, 549.9988529682159, 554.9609489440918, 560.004860162735, 565.0487713813782, 570.075296163559, 575.1018209457397, 580.0163252353668, 584.9308295249939, 589.8662524223328, 594.8016753196716, 599.6953635215759, 604.5890517234802, 608.9890468120575, 613.3890419006348, 617.6759941577911, 621.9629464149475, 626.3199880123138, 630.6770296096802, 635.0893180370331, 639.501606464386, 643.8651638031006, 648.2287211418152, 652.5476849079132, 656.8666486740112, 661.2247204780579, 665.5827922821045, 669.9383704662323, 674.2939486503601, 678.5772197246552, 682.8604907989502, 687.9737231731415, 693.0869555473328, 698.1788604259491, 703.2707653045654, 708.4073159694672, 713.5438666343689, 718.8088705539703, 724.0738744735718, 729.1877529621124, 734.3016314506531, 739.4107232093811, 744.5198149681091, 749.0002384185791, 753.4806618690491, 757.8187398910522, 762.1568179130554, 766.5274889469147, 770.8981599807739, 775.2510452270508, 779.6039304733276, 784.5452566146851, 789.4865827560425, 794.4550468921661, 799.4235110282898, 804.438458442688, 809.4534058570862, 814.4934661388397, 819.5335264205933, 824.3853478431702, 829.2371692657471, 833.6407437324524, 838.0443181991577, 842.381368637085, 846.7184190750122, 851.125482082367, 855.5325450897217, 860.2411749362946, 864.9498047828674, 869.624929189682, 874.3000535964966, 879.0454456806183, 883.79083776474, 888.5312066078186, 893.2715754508972, 898.314798116684, 903.3580207824707, 908.0079956054688, 912.6579704284668, 917.2968978881836, 921.9358253479004, 926.598375082016, 931.2609248161316, 933.5629482269287, 935.8649716377258]
[31.9425, 31.9425, 38.4925, 38.4925, 42.2425, 42.2425, 44.87, 44.87, 47.925, 47.925, 51.6525, 51.6525, 52.545, 52.545, 54.6025, 54.6025, 55.78, 55.78, 56.755, 56.755, 58.5675, 58.5675, 60.345, 60.345, 61.2125, 61.2125, 61.965, 61.965, 62.6675, 62.6675, 63.2375, 63.2375, 63.67, 63.67, 63.79, 63.79, 64.1175, 64.1175, 64.75, 64.75, 65.69, 65.69, 65.835, 65.835, 66.0575, 66.0575, 66.8925, 66.8925, 67.23, 67.23, 67.6025, 67.6025, 67.72, 67.72, 68.0225, 68.0225, 68.855, 68.855, 69.1075, 69.1075, 69.2825, 69.2825, 69.2325, 69.2325, 69.12, 69.12, 69.5825, 69.5825, 69.6875, 69.6875, 69.9775, 69.9775, 69.765, 69.765, 69.905, 69.905, 70.155, 70.155, 70.2675, 70.2675, 70.5125, 70.5125, 70.5275, 70.5275, 70.98, 70.98, 71.06, 71.06, 71.0775, 71.0775, 70.9775, 70.9775, 71.0825, 71.0825, 71.2775, 71.2775, 71.0075, 71.0075, 71.115, 71.115, 71.3325, 71.3325, 71.41, 71.41, 71.2675, 71.2675, 71.4475, 71.4475, 71.395, 71.395, 72.075, 72.075, 72.2125, 72.2125, 72.2925, 72.2925, 72.2075, 72.2075, 72.12, 72.12, 72.3975, 72.3975, 72.2025, 72.2025, 72.375, 72.375, 72.2025, 72.2025, 72.1725, 72.1725, 72.5725, 72.5725, 72.5325, 72.5325, 72.4675, 72.4675, 72.63, 72.63, 72.7625, 72.7625, 72.725, 72.725, 72.6725, 72.6725, 72.585, 72.585, 72.4225, 72.4225, 72.2075, 72.2075, 72.3425, 72.3425, 72.39, 72.39, 72.29, 72.29, 72.595, 72.595, 72.8225, 72.8225, 73.25, 73.25, 73.1975, 73.1975, 73.29, 73.29, 72.9975, 72.9975, 73.0, 73.0, 73.14, 73.14, 73.1225, 73.1225, 73.1725, 73.1725, 73.0775, 73.0775, 72.905, 72.905, 73.015, 73.015, 73.185, 73.185, 73.5825, 73.5825, 73.7325, 73.7325, 73.805, 73.805, 73.8175, 73.8175, 73.9575, 73.9575, 73.8675, 73.8675, 73.68, 73.68, 73.5625, 73.5625, 73.4475, 73.4475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.419, Test loss: 2.095, Test accuracy: 20.16
Round   1, Train loss: 0.947, Test loss: 1.688, Test accuracy: 33.07
Round   2, Train loss: 0.849, Test loss: 1.513, Test accuracy: 42.77
Round   3, Train loss: 0.797, Test loss: 1.052, Test accuracy: 54.96
Round   4, Train loss: 0.677, Test loss: 1.090, Test accuracy: 58.88
Round   5, Train loss: 0.720, Test loss: 0.860, Test accuracy: 64.26
Round   6, Train loss: 0.752, Test loss: 0.767, Test accuracy: 66.73
Round   7, Train loss: 0.675, Test loss: 0.631, Test accuracy: 71.83
Round   8, Train loss: 0.694, Test loss: 0.623, Test accuracy: 72.64
Round   9, Train loss: 0.595, Test loss: 0.601, Test accuracy: 73.64
Round  10, Train loss: 0.546, Test loss: 0.585, Test accuracy: 74.72
Round  11, Train loss: 0.655, Test loss: 0.564, Test accuracy: 75.76
Round  12, Train loss: 0.619, Test loss: 0.546, Test accuracy: 76.55
Round  13, Train loss: 0.524, Test loss: 0.535, Test accuracy: 76.81
Round  14, Train loss: 0.540, Test loss: 0.535, Test accuracy: 76.64
Round  15, Train loss: 0.549, Test loss: 0.543, Test accuracy: 76.26
Round  16, Train loss: 0.552, Test loss: 0.522, Test accuracy: 77.45
Round  17, Train loss: 0.523, Test loss: 0.504, Test accuracy: 78.29
Round  18, Train loss: 0.522, Test loss: 0.504, Test accuracy: 78.23
Round  19, Train loss: 0.543, Test loss: 0.496, Test accuracy: 78.87
Round  20, Train loss: 0.481, Test loss: 0.498, Test accuracy: 79.33
Round  21, Train loss: 0.546, Test loss: 0.489, Test accuracy: 79.54
Round  22, Train loss: 0.451, Test loss: 0.480, Test accuracy: 79.86
Round  23, Train loss: 0.538, Test loss: 0.479, Test accuracy: 79.51
Round  24, Train loss: 0.486, Test loss: 0.458, Test accuracy: 80.50
Round  25, Train loss: 0.386, Test loss: 0.447, Test accuracy: 81.09
Round  26, Train loss: 0.529, Test loss: 0.454, Test accuracy: 81.11
Round  27, Train loss: 0.433, Test loss: 0.444, Test accuracy: 81.28
Round  28, Train loss: 0.402, Test loss: 0.439, Test accuracy: 82.00
Round  29, Train loss: 0.450, Test loss: 0.444, Test accuracy: 81.12
Round  30, Train loss: 0.384, Test loss: 0.440, Test accuracy: 81.60
Round  31, Train loss: 0.437, Test loss: 0.440, Test accuracy: 81.36
Round  32, Train loss: 0.399, Test loss: 0.430, Test accuracy: 81.83
Round  33, Train loss: 0.479, Test loss: 0.421, Test accuracy: 82.22
Round  34, Train loss: 0.399, Test loss: 0.417, Test accuracy: 82.64
Round  35, Train loss: 0.430, Test loss: 0.415, Test accuracy: 82.86
Round  36, Train loss: 0.380, Test loss: 0.422, Test accuracy: 82.41
Round  37, Train loss: 0.445, Test loss: 0.417, Test accuracy: 82.96
Round  38, Train loss: 0.400, Test loss: 0.409, Test accuracy: 83.19
Round  39, Train loss: 0.388, Test loss: 0.405, Test accuracy: 83.03
Round  40, Train loss: 0.414, Test loss: 0.397, Test accuracy: 83.56
Round  41, Train loss: 0.399, Test loss: 0.397, Test accuracy: 83.93
Round  42, Train loss: 0.443, Test loss: 0.397, Test accuracy: 84.03
Round  43, Train loss: 0.419, Test loss: 0.393, Test accuracy: 84.03
Round  44, Train loss: 0.417, Test loss: 0.393, Test accuracy: 84.09
Round  45, Train loss: 0.377, Test loss: 0.390, Test accuracy: 84.21
Round  46, Train loss: 0.348, Test loss: 0.385, Test accuracy: 84.31
Round  47, Train loss: 0.332, Test loss: 0.392, Test accuracy: 84.04
Round  48, Train loss: 0.383, Test loss: 0.385, Test accuracy: 84.28
Round  49, Train loss: 0.332, Test loss: 0.384, Test accuracy: 84.45
Round  50, Train loss: 0.416, Test loss: 0.384, Test accuracy: 84.69
Round  51, Train loss: 0.386, Test loss: 0.385, Test accuracy: 84.26
Round  52, Train loss: 0.352, Test loss: 0.384, Test accuracy: 84.39
Round  53, Train loss: 0.351, Test loss: 0.382, Test accuracy: 84.47
Round  54, Train loss: 0.309, Test loss: 0.381, Test accuracy: 84.59
Round  55, Train loss: 0.320, Test loss: 0.382, Test accuracy: 84.39
Round  56, Train loss: 0.326, Test loss: 0.367, Test accuracy: 85.39
Round  57, Train loss: 0.381, Test loss: 0.371, Test accuracy: 85.17
Round  58, Train loss: 0.362, Test loss: 0.379, Test accuracy: 84.85
Round  59, Train loss: 0.349, Test loss: 0.373, Test accuracy: 84.93
Round  60, Train loss: 0.376, Test loss: 0.377, Test accuracy: 84.83
Round  61, Train loss: 0.309, Test loss: 0.377, Test accuracy: 84.72
Round  62, Train loss: 0.356, Test loss: 0.376, Test accuracy: 84.84
Round  63, Train loss: 0.296, Test loss: 0.373, Test accuracy: 85.17
Round  64, Train loss: 0.287, Test loss: 0.366, Test accuracy: 85.46
Round  65, Train loss: 0.305, Test loss: 0.376, Test accuracy: 85.40
Round  66, Train loss: 0.292, Test loss: 0.373, Test accuracy: 85.32
Round  67, Train loss: 0.287, Test loss: 0.370, Test accuracy: 85.12
Round  68, Train loss: 0.292, Test loss: 0.369, Test accuracy: 85.27
Round  69, Train loss: 0.325, Test loss: 0.371, Test accuracy: 85.37
Round  70, Train loss: 0.317, Test loss: 0.372, Test accuracy: 85.30
Round  71, Train loss: 0.298, Test loss: 0.368, Test accuracy: 85.39
Round  72, Train loss: 0.300, Test loss: 0.373, Test accuracy: 85.49
Round  73, Train loss: 0.309, Test loss: 0.359, Test accuracy: 85.98
Round  74, Train loss: 0.274, Test loss: 0.357, Test accuracy: 86.22
Round  75, Train loss: 0.314, Test loss: 0.364, Test accuracy: 85.97
Round  76, Train loss: 0.271, Test loss: 0.354, Test accuracy: 86.30
Round  77, Train loss: 0.270, Test loss: 0.356, Test accuracy: 86.14
Round  78, Train loss: 0.264, Test loss: 0.358, Test accuracy: 86.38
Round  79, Train loss: 0.326, Test loss: 0.356, Test accuracy: 86.40
Round  80, Train loss: 0.277, Test loss: 0.358, Test accuracy: 85.72
Round  81, Train loss: 0.254, Test loss: 0.356, Test accuracy: 86.12
Round  82, Train loss: 0.197, Test loss: 0.358, Test accuracy: 86.21
Round  83, Train loss: 0.233, Test loss: 0.361, Test accuracy: 86.10
Round  84, Train loss: 0.293, Test loss: 0.361, Test accuracy: 86.38
Round  85, Train loss: 0.225, Test loss: 0.365, Test accuracy: 85.98
Round  86, Train loss: 0.250, Test loss: 0.363, Test accuracy: 86.35
Round  87, Train loss: 0.282, Test loss: 0.352, Test accuracy: 86.61
Round  88, Train loss: 0.302, Test loss: 0.343, Test accuracy: 86.63
Round  89, Train loss: 0.189, Test loss: 0.350, Test accuracy: 86.57
Round  90, Train loss: 0.271, Test loss: 0.361, Test accuracy: 86.42
Round  91, Train loss: 0.252, Test loss: 0.353, Test accuracy: 86.43
Round  92, Train loss: 0.272, Test loss: 0.357, Test accuracy: 86.32
Round  93, Train loss: 0.262, Test loss: 0.361, Test accuracy: 86.38
Round  94, Train loss: 0.262, Test loss: 0.350, Test accuracy: 86.19
Round  95, Train loss: 0.258, Test loss: 0.346, Test accuracy: 86.43
Round  96, Train loss: 0.213, Test loss: 0.347, Test accuracy: 86.46
Round  97, Train loss: 0.249, Test loss: 0.359, Test accuracy: 86.45
Round  98, Train loss: 0.271, Test loss: 0.354, Test accuracy: 86.70
Round  99, Train loss: 0.248, Test loss: 0.354, Test accuracy: 86.95
Final Round, Train loss: 0.203, Test loss: 0.356, Test accuracy: 87.07
Average accuracy final 10 rounds: 86.4725 

1545.6787700653076
[1.6257061958312988, 3.2514123916625977, 4.611368417739868, 5.971324443817139, 7.359174489974976, 8.747024536132812, 10.089752197265625, 11.432479858398438, 12.824377059936523, 14.21627426147461, 15.579880952835083, 16.943487644195557, 18.339508295059204, 19.73552894592285, 21.155483722686768, 22.575438499450684, 23.932002544403076, 25.28856658935547, 26.693992853164673, 28.099419116973877, 29.47641396522522, 30.853408813476562, 32.27533006668091, 33.697251319885254, 35.04323101043701, 36.38921070098877, 37.72918438911438, 39.06915807723999, 40.46921133995056, 41.86926460266113, 43.22221565246582, 44.57516670227051, 45.92208671569824, 47.26900672912598, 48.58275127410889, 49.8964958190918, 51.245330572128296, 52.594165325164795, 53.9183611869812, 55.24255704879761, 56.655951738357544, 58.06934642791748, 59.470900774002075, 60.87245512008667, 62.19078183174133, 63.509108543395996, 64.9489495754242, 66.38879060745239, 67.72734546661377, 69.06590032577515, 70.40947580337524, 71.75305128097534, 73.05860710144043, 74.36416292190552, 75.68048357963562, 76.99680423736572, 78.31820559501648, 79.63960695266724, 81.0017626285553, 82.36391830444336, 83.67332220077515, 84.98272609710693, 86.34513974189758, 87.70755338668823, 89.05117559432983, 90.39479780197144, 91.73043537139893, 93.06607294082642, 94.39777660369873, 95.72948026657104, 97.13526606559753, 98.54105186462402, 99.95626640319824, 101.37148094177246, 102.70097804069519, 104.03047513961792, 105.36655306816101, 106.7026309967041, 108.08729791641235, 109.4719648361206, 110.90965533256531, 112.34734582901001, 113.77611088752747, 115.20487594604492, 116.53698182106018, 117.86908769607544, 119.25011229515076, 120.63113689422607, 121.99639844894409, 123.36166000366211, 124.75159692764282, 126.14153385162354, 127.55301117897034, 128.96448850631714, 130.31948494911194, 131.67448139190674, 133.00533270835876, 134.3361840248108, 135.65788912773132, 136.97959423065186, 138.32607340812683, 139.6725525856018, 141.08918976783752, 142.50582695007324, 143.92098689079285, 145.33614683151245, 146.6698350906372, 148.00352334976196, 149.41817808151245, 150.83283281326294, 152.2525293827057, 153.67222595214844, 155.07244992256165, 156.47267389297485, 157.86490273475647, 159.2571315765381, 160.61215257644653, 161.96717357635498, 163.31270956993103, 164.65824556350708, 166.06064772605896, 167.46304988861084, 168.8759331703186, 170.28881645202637, 171.6397557258606, 172.99069499969482, 174.30213832855225, 175.61358165740967, 176.95941495895386, 178.30524826049805, 179.6131603717804, 180.92107248306274, 182.23387694358826, 183.54668140411377, 184.83277869224548, 186.1188759803772, 187.48294830322266, 188.84702062606812, 190.1903567314148, 191.53369283676147, 192.8627758026123, 194.19185876846313, 195.46161913871765, 196.73137950897217, 198.07465052604675, 199.41792154312134, 200.7671501636505, 202.1163787841797, 203.4055459499359, 204.69471311569214, 205.9838001728058, 207.27288722991943, 208.6446795463562, 210.01647186279297, 211.3834583759308, 212.7504448890686, 214.12477827072144, 215.49911165237427, 216.81599402427673, 218.1328763961792, 219.5608947277069, 220.98891305923462, 222.37469911575317, 223.76048517227173, 225.13138222694397, 226.5022792816162, 227.80745577812195, 229.11263227462769, 230.42400074005127, 231.73536920547485, 233.1048150062561, 234.47426080703735, 235.8284046649933, 237.18254852294922, 238.51150465011597, 239.84046077728271, 241.062495470047, 242.28453016281128, 243.69335532188416, 245.10218048095703, 246.53176045417786, 247.96134042739868, 249.28363180160522, 250.60592317581177, 251.88935708999634, 253.1727910041809, 254.57055139541626, 255.9683117866516, 257.3668746948242, 258.7654376029968, 260.04626059532166, 261.3270835876465, 262.66155648231506, 263.99602937698364, 265.38651990890503, 266.7770104408264, 268.1044292449951, 269.4318480491638, 270.74970626831055, 272.0675644874573, 274.26363015174866, 276.45969581604004]
[20.158333333333335, 20.158333333333335, 33.06666666666667, 33.06666666666667, 42.775, 42.775, 54.958333333333336, 54.958333333333336, 58.875, 58.875, 64.25833333333334, 64.25833333333334, 66.73333333333333, 66.73333333333333, 71.825, 71.825, 72.64166666666667, 72.64166666666667, 73.64166666666667, 73.64166666666667, 74.71666666666667, 74.71666666666667, 75.75833333333334, 75.75833333333334, 76.55, 76.55, 76.80833333333334, 76.80833333333334, 76.64166666666667, 76.64166666666667, 76.25833333333334, 76.25833333333334, 77.45, 77.45, 78.29166666666667, 78.29166666666667, 78.23333333333333, 78.23333333333333, 78.86666666666666, 78.86666666666666, 79.33333333333333, 79.33333333333333, 79.54166666666667, 79.54166666666667, 79.85833333333333, 79.85833333333333, 79.50833333333334, 79.50833333333334, 80.5, 80.5, 81.09166666666667, 81.09166666666667, 81.10833333333333, 81.10833333333333, 81.28333333333333, 81.28333333333333, 82.0, 82.0, 81.125, 81.125, 81.6, 81.6, 81.35833333333333, 81.35833333333333, 81.825, 81.825, 82.21666666666667, 82.21666666666667, 82.64166666666667, 82.64166666666667, 82.85833333333333, 82.85833333333333, 82.40833333333333, 82.40833333333333, 82.95833333333333, 82.95833333333333, 83.19166666666666, 83.19166666666666, 83.025, 83.025, 83.55833333333334, 83.55833333333334, 83.93333333333334, 83.93333333333334, 84.025, 84.025, 84.025, 84.025, 84.09166666666667, 84.09166666666667, 84.20833333333333, 84.20833333333333, 84.30833333333334, 84.30833333333334, 84.04166666666667, 84.04166666666667, 84.275, 84.275, 84.45, 84.45, 84.69166666666666, 84.69166666666666, 84.25833333333334, 84.25833333333334, 84.39166666666667, 84.39166666666667, 84.475, 84.475, 84.59166666666667, 84.59166666666667, 84.39166666666667, 84.39166666666667, 85.39166666666667, 85.39166666666667, 85.16666666666667, 85.16666666666667, 84.85, 84.85, 84.93333333333334, 84.93333333333334, 84.83333333333333, 84.83333333333333, 84.71666666666667, 84.71666666666667, 84.84166666666667, 84.84166666666667, 85.175, 85.175, 85.45833333333333, 85.45833333333333, 85.4, 85.4, 85.31666666666666, 85.31666666666666, 85.125, 85.125, 85.26666666666667, 85.26666666666667, 85.36666666666666, 85.36666666666666, 85.3, 85.3, 85.39166666666667, 85.39166666666667, 85.49166666666666, 85.49166666666666, 85.98333333333333, 85.98333333333333, 86.21666666666667, 86.21666666666667, 85.96666666666667, 85.96666666666667, 86.3, 86.3, 86.14166666666667, 86.14166666666667, 86.38333333333334, 86.38333333333334, 86.4, 86.4, 85.725, 85.725, 86.125, 86.125, 86.20833333333333, 86.20833333333333, 86.1, 86.1, 86.375, 86.375, 85.98333333333333, 85.98333333333333, 86.35, 86.35, 86.60833333333333, 86.60833333333333, 86.63333333333334, 86.63333333333334, 86.56666666666666, 86.56666666666666, 86.41666666666667, 86.41666666666667, 86.43333333333334, 86.43333333333334, 86.31666666666666, 86.31666666666666, 86.375, 86.375, 86.19166666666666, 86.19166666666666, 86.43333333333334, 86.43333333333334, 86.45833333333333, 86.45833333333333, 86.45, 86.45, 86.7, 86.7, 86.95, 86.95, 87.06666666666666, 87.06666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.519, Test loss: 2.041, Test accuracy: 26.09
Round   1, Train loss: 1.031, Test loss: 1.559, Test accuracy: 40.86
Round   2, Train loss: 0.917, Test loss: 1.387, Test accuracy: 50.77
Round   3, Train loss: 0.794, Test loss: 1.206, Test accuracy: 53.07
Round   4, Train loss: 0.821, Test loss: 0.970, Test accuracy: 60.67
Round   5, Train loss: 0.752, Test loss: 0.878, Test accuracy: 66.39
Round   6, Train loss: 0.723, Test loss: 0.797, Test accuracy: 69.45
Round   7, Train loss: 0.709, Test loss: 0.714, Test accuracy: 72.17
Round   8, Train loss: 0.658, Test loss: 0.657, Test accuracy: 73.21
Round   9, Train loss: 0.640, Test loss: 0.615, Test accuracy: 75.17
Round  10, Train loss: 0.694, Test loss: 0.603, Test accuracy: 75.56
Round  11, Train loss: 0.649, Test loss: 0.575, Test accuracy: 76.99
Round  12, Train loss: 0.597, Test loss: 0.567, Test accuracy: 77.03
Round  13, Train loss: 0.523, Test loss: 0.570, Test accuracy: 77.50
Round  14, Train loss: 0.580, Test loss: 0.556, Test accuracy: 77.86
Round  15, Train loss: 0.607, Test loss: 0.551, Test accuracy: 78.71
Round  16, Train loss: 0.566, Test loss: 0.520, Test accuracy: 79.26
Round  17, Train loss: 0.574, Test loss: 0.513, Test accuracy: 79.57
Round  18, Train loss: 0.506, Test loss: 0.505, Test accuracy: 80.14
Round  19, Train loss: 0.541, Test loss: 0.507, Test accuracy: 80.02
Round  20, Train loss: 0.527, Test loss: 0.504, Test accuracy: 80.32
Round  21, Train loss: 0.459, Test loss: 0.489, Test accuracy: 80.62
Round  22, Train loss: 0.540, Test loss: 0.469, Test accuracy: 81.18
Round  23, Train loss: 0.491, Test loss: 0.470, Test accuracy: 80.68
Round  24, Train loss: 0.575, Test loss: 0.464, Test accuracy: 81.49
Round  25, Train loss: 0.483, Test loss: 0.457, Test accuracy: 81.22
Round  26, Train loss: 0.531, Test loss: 0.443, Test accuracy: 81.92
Round  27, Train loss: 0.441, Test loss: 0.442, Test accuracy: 81.96
Round  28, Train loss: 0.464, Test loss: 0.435, Test accuracy: 82.41
Round  29, Train loss: 0.443, Test loss: 0.433, Test accuracy: 82.62
Round  30, Train loss: 0.407, Test loss: 0.430, Test accuracy: 82.63
Round  31, Train loss: 0.483, Test loss: 0.430, Test accuracy: 82.72
Round  32, Train loss: 0.484, Test loss: 0.431, Test accuracy: 82.84
Round  33, Train loss: 0.476, Test loss: 0.409, Test accuracy: 83.41
Round  34, Train loss: 0.398, Test loss: 0.426, Test accuracy: 82.71
Round  35, Train loss: 0.431, Test loss: 0.417, Test accuracy: 83.16
Round  36, Train loss: 0.479, Test loss: 0.407, Test accuracy: 83.45
Round  37, Train loss: 0.426, Test loss: 0.405, Test accuracy: 83.57
Round  38, Train loss: 0.413, Test loss: 0.401, Test accuracy: 84.11
Round  39, Train loss: 0.390, Test loss: 0.399, Test accuracy: 83.77
Round  40, Train loss: 0.440, Test loss: 0.401, Test accuracy: 84.13
Round  41, Train loss: 0.408, Test loss: 0.399, Test accuracy: 84.05
Round  42, Train loss: 0.446, Test loss: 0.392, Test accuracy: 84.42
Round  43, Train loss: 0.347, Test loss: 0.386, Test accuracy: 84.59
Round  44, Train loss: 0.366, Test loss: 0.391, Test accuracy: 84.37
Round  45, Train loss: 0.356, Test loss: 0.388, Test accuracy: 84.39
Round  46, Train loss: 0.393, Test loss: 0.382, Test accuracy: 84.58
Round  47, Train loss: 0.352, Test loss: 0.380, Test accuracy: 84.89
Round  48, Train loss: 0.347, Test loss: 0.374, Test accuracy: 84.87
Round  49, Train loss: 0.361, Test loss: 0.376, Test accuracy: 84.94
Round  50, Train loss: 0.375, Test loss: 0.376, Test accuracy: 84.89
Round  51, Train loss: 0.353, Test loss: 0.377, Test accuracy: 84.72
Round  52, Train loss: 0.372, Test loss: 0.370, Test accuracy: 85.10
Round  53, Train loss: 0.320, Test loss: 0.362, Test accuracy: 85.32
Round  54, Train loss: 0.368, Test loss: 0.365, Test accuracy: 85.37
Round  55, Train loss: 0.373, Test loss: 0.370, Test accuracy: 85.30
Round  56, Train loss: 0.328, Test loss: 0.356, Test accuracy: 85.81
Round  57, Train loss: 0.402, Test loss: 0.358, Test accuracy: 85.61
Round  58, Train loss: 0.337, Test loss: 0.358, Test accuracy: 85.75
Round  59, Train loss: 0.298, Test loss: 0.356, Test accuracy: 85.93
Round  60, Train loss: 0.342, Test loss: 0.349, Test accuracy: 86.06
Round  61, Train loss: 0.393, Test loss: 0.359, Test accuracy: 85.87
Round  62, Train loss: 0.344, Test loss: 0.348, Test accuracy: 86.57
Round  63, Train loss: 0.297, Test loss: 0.348, Test accuracy: 86.29
Round  64, Train loss: 0.315, Test loss: 0.339, Test accuracy: 86.78
Round  65, Train loss: 0.319, Test loss: 0.344, Test accuracy: 86.82
Round  66, Train loss: 0.336, Test loss: 0.343, Test accuracy: 86.44
Round  67, Train loss: 0.347, Test loss: 0.347, Test accuracy: 86.62
Round  68, Train loss: 0.327, Test loss: 0.336, Test accuracy: 87.17
Round  69, Train loss: 0.217, Test loss: 0.336, Test accuracy: 87.00
Round  70, Train loss: 0.320, Test loss: 0.341, Test accuracy: 86.77
Round  71, Train loss: 0.325, Test loss: 0.347, Test accuracy: 86.58
Round  72, Train loss: 0.295, Test loss: 0.339, Test accuracy: 86.62
Round  73, Train loss: 0.297, Test loss: 0.335, Test accuracy: 86.85
Round  74, Train loss: 0.290, Test loss: 0.335, Test accuracy: 86.77
Round  75, Train loss: 0.300, Test loss: 0.335, Test accuracy: 86.95
Round  76, Train loss: 0.312, Test loss: 0.337, Test accuracy: 86.80
Round  77, Train loss: 0.281, Test loss: 0.339, Test accuracy: 86.62
Round  78, Train loss: 0.302, Test loss: 0.335, Test accuracy: 86.91
Round  79, Train loss: 0.277, Test loss: 0.331, Test accuracy: 86.91
Round  80, Train loss: 0.320, Test loss: 0.333, Test accuracy: 86.95
Round  81, Train loss: 0.307, Test loss: 0.334, Test accuracy: 86.87
Round  82, Train loss: 0.274, Test loss: 0.333, Test accuracy: 87.12
Round  83, Train loss: 0.271, Test loss: 0.325, Test accuracy: 87.18
Round  84, Train loss: 0.267, Test loss: 0.331, Test accuracy: 87.19
Round  85, Train loss: 0.246, Test loss: 0.326, Test accuracy: 87.23
Round  86, Train loss: 0.262, Test loss: 0.327, Test accuracy: 87.11
Round  87, Train loss: 0.279, Test loss: 0.329, Test accuracy: 87.30
Round  88, Train loss: 0.256, Test loss: 0.330, Test accuracy: 87.21
Round  89, Train loss: 0.280, Test loss: 0.327, Test accuracy: 87.48
Round  90, Train loss: 0.250, Test loss: 0.324, Test accuracy: 87.47
Round  91, Train loss: 0.244, Test loss: 0.324, Test accuracy: 87.46
Round  92, Train loss: 0.210, Test loss: 0.324, Test accuracy: 87.36
Round  93, Train loss: 0.241, Test loss: 0.326, Test accuracy: 87.43
Round  94, Train loss: 0.219, Test loss: 0.323, Test accuracy: 87.60
Round  95, Train loss: 0.247, Test loss: 0.328, Test accuracy: 87.39
Round  96, Train loss: 0.268, Test loss: 0.332, Test accuracy: 86.99
Round  97, Train loss: 0.241, Test loss: 0.321, Test accuracy: 87.42
Round  98, Train loss: 0.252, Test loss: 0.321, Test accuracy: 87.42
Round  99, Train loss: 0.238, Test loss: 0.317, Test accuracy: 87.67
Final Round, Train loss: 0.202, Test loss: 0.317, Test accuracy: 87.74
Average accuracy final 10 rounds: 87.42166666666667
1742.5445733070374
[2.1864490509033203, 4.372898101806641, 6.276739120483398, 8.180580139160156, 9.906972169876099, 11.633364200592041, 13.426652431488037, 15.219940662384033, 16.93945002555847, 18.65895938873291, 20.479621648788452, 22.300283908843994, 24.131941556930542, 25.96359920501709, 27.78850746154785, 29.613415718078613, 31.44322657585144, 33.27303743362427, 35.119426012039185, 36.9658145904541, 38.77098846435547, 40.576162338256836, 42.358590841293335, 44.141019344329834, 45.80165457725525, 47.462289810180664, 49.146687746047974, 50.83108568191528, 52.47789931297302, 54.12471294403076, 55.78028464317322, 57.435856342315674, 59.04814958572388, 60.66044282913208, 62.26593279838562, 63.87142276763916, 65.52020812034607, 67.16899347305298, 68.89923405647278, 70.62947463989258, 72.28930568695068, 73.94913673400879, 75.599360704422, 77.2495846748352, 78.98952746391296, 80.72947025299072, 82.44237685203552, 84.15528345108032, 85.7639365196228, 87.37258958816528, 89.02689361572266, 90.68119764328003, 92.36679291725159, 94.05238819122314, 95.7048180103302, 97.35724782943726, 99.10006546974182, 100.84288311004639, 102.45373249053955, 104.06458187103271, 105.78178477287292, 107.49898767471313, 109.12727379798889, 110.75555992126465, 112.38231992721558, 114.0090799331665, 115.74656248092651, 117.48404502868652, 119.23294186592102, 120.98183870315552, 122.7134301662445, 124.4450216293335, 126.17794013023376, 127.91085863113403, 129.63123154640198, 131.35160446166992, 133.06289672851562, 134.77418899536133, 136.42769742012024, 138.08120584487915, 139.8379602432251, 141.59471464157104, 143.22668552398682, 144.8586564064026, 146.61945247650146, 148.38024854660034, 150.04820919036865, 151.71616983413696, 153.4718198776245, 155.22746992111206, 157.06939053535461, 158.91131114959717, 160.61926078796387, 162.32721042633057, 164.2707817554474, 166.2143530845642, 168.04985094070435, 169.88534879684448, 171.59195494651794, 173.2985610961914, 174.97821593284607, 176.65787076950073, 178.40175342559814, 180.14563608169556, 181.88616251945496, 183.62668895721436, 185.2978081703186, 186.96892738342285, 188.6904091835022, 190.41189098358154, 192.14140462875366, 193.87091827392578, 195.56325960159302, 197.25560092926025, 198.9577078819275, 200.65981483459473, 202.34657073020935, 204.03332662582397, 205.7282190322876, 207.42311143875122, 209.03654766082764, 210.64998388290405, 212.30704498291016, 213.96410608291626, 215.66987013816833, 217.3756341934204, 219.05299139022827, 220.73034858703613, 222.36372780799866, 223.99710702896118, 225.72630977630615, 227.45551252365112, 229.1889431476593, 230.92237377166748, 232.55418705940247, 234.18600034713745, 235.905996799469, 237.62599325180054, 239.34330034255981, 241.0606074333191, 242.79697918891907, 244.53335094451904, 246.26981568336487, 248.0062804222107, 249.65302896499634, 251.29977750778198, 253.01506400108337, 254.73035049438477, 256.447781085968, 258.16521167755127, 259.7329044342041, 261.30059719085693, 262.99052238464355, 264.6804475784302, 266.3597285747528, 268.03900957107544, 269.61939334869385, 271.19977712631226, 272.77309560775757, 274.3464140892029, 276.0331380367279, 277.71986198425293, 279.41775345802307, 281.1156449317932, 282.6846058368683, 284.25356674194336, 285.8818323612213, 287.51009798049927, 289.1762526035309, 290.8424072265625, 292.4007716178894, 293.9591360092163, 295.62025213241577, 297.28136825561523, 299.06225657463074, 300.84314489364624, 302.5132591724396, 304.1833734512329, 305.73466992378235, 307.2859663963318, 308.9193546772003, 310.55274295806885, 312.1723861694336, 313.79202938079834, 315.43821907043457, 317.0844087600708, 318.7274856567383, 320.37056255340576, 321.9575333595276, 323.5445041656494, 325.13927149772644, 326.73403882980347, 328.3296973705292, 329.9253559112549, 331.5014600753784, 333.07756423950195, 334.70553612709045, 336.33350801467896, 337.93523359298706, 339.53695917129517, 341.7485201358795, 343.96008110046387]
[26.091666666666665, 26.091666666666665, 40.858333333333334, 40.858333333333334, 50.766666666666666, 50.766666666666666, 53.06666666666667, 53.06666666666667, 60.666666666666664, 60.666666666666664, 66.39166666666667, 66.39166666666667, 69.45, 69.45, 72.16666666666667, 72.16666666666667, 73.20833333333333, 73.20833333333333, 75.16666666666667, 75.16666666666667, 75.55833333333334, 75.55833333333334, 76.99166666666666, 76.99166666666666, 77.025, 77.025, 77.5, 77.5, 77.85833333333333, 77.85833333333333, 78.70833333333333, 78.70833333333333, 79.25833333333334, 79.25833333333334, 79.56666666666666, 79.56666666666666, 80.14166666666667, 80.14166666666667, 80.01666666666667, 80.01666666666667, 80.31666666666666, 80.31666666666666, 80.625, 80.625, 81.18333333333334, 81.18333333333334, 80.68333333333334, 80.68333333333334, 81.49166666666666, 81.49166666666666, 81.225, 81.225, 81.91666666666667, 81.91666666666667, 81.95833333333333, 81.95833333333333, 82.40833333333333, 82.40833333333333, 82.625, 82.625, 82.63333333333334, 82.63333333333334, 82.71666666666667, 82.71666666666667, 82.84166666666667, 82.84166666666667, 83.40833333333333, 83.40833333333333, 82.70833333333333, 82.70833333333333, 83.15833333333333, 83.15833333333333, 83.45, 83.45, 83.56666666666666, 83.56666666666666, 84.10833333333333, 84.10833333333333, 83.76666666666667, 83.76666666666667, 84.13333333333334, 84.13333333333334, 84.05, 84.05, 84.425, 84.425, 84.59166666666667, 84.59166666666667, 84.36666666666666, 84.36666666666666, 84.39166666666667, 84.39166666666667, 84.575, 84.575, 84.89166666666667, 84.89166666666667, 84.86666666666666, 84.86666666666666, 84.94166666666666, 84.94166666666666, 84.89166666666667, 84.89166666666667, 84.71666666666667, 84.71666666666667, 85.1, 85.1, 85.31666666666666, 85.31666666666666, 85.36666666666666, 85.36666666666666, 85.3, 85.3, 85.80833333333334, 85.80833333333334, 85.60833333333333, 85.60833333333333, 85.75, 85.75, 85.93333333333334, 85.93333333333334, 86.05833333333334, 86.05833333333334, 85.86666666666666, 85.86666666666666, 86.56666666666666, 86.56666666666666, 86.29166666666667, 86.29166666666667, 86.78333333333333, 86.78333333333333, 86.81666666666666, 86.81666666666666, 86.44166666666666, 86.44166666666666, 86.61666666666666, 86.61666666666666, 87.175, 87.175, 87.0, 87.0, 86.76666666666667, 86.76666666666667, 86.58333333333333, 86.58333333333333, 86.61666666666666, 86.61666666666666, 86.85, 86.85, 86.76666666666667, 86.76666666666667, 86.95, 86.95, 86.8, 86.8, 86.625, 86.625, 86.90833333333333, 86.90833333333333, 86.90833333333333, 86.90833333333333, 86.95, 86.95, 86.86666666666666, 86.86666666666666, 87.125, 87.125, 87.18333333333334, 87.18333333333334, 87.19166666666666, 87.19166666666666, 87.23333333333333, 87.23333333333333, 87.10833333333333, 87.10833333333333, 87.3, 87.3, 87.20833333333333, 87.20833333333333, 87.48333333333333, 87.48333333333333, 87.475, 87.475, 87.45833333333333, 87.45833333333333, 87.35833333333333, 87.35833333333333, 87.43333333333334, 87.43333333333334, 87.6, 87.6, 87.39166666666667, 87.39166666666667, 86.99166666666666, 86.99166666666666, 87.425, 87.425, 87.41666666666667, 87.41666666666667, 87.66666666666667, 87.66666666666667, 87.74166666666666, 87.74166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.183, Test loss: 1.974, Test accuracy: 32.67
Round   1, Train loss: 1.841, Test loss: 1.847, Test accuracy: 37.69
Round   2, Train loss: 1.675, Test loss: 1.803, Test accuracy: 41.49
Round   3, Train loss: 1.574, Test loss: 1.695, Test accuracy: 43.77
Round   4, Train loss: 1.488, Test loss: 1.634, Test accuracy: 45.28
Round   5, Train loss: 1.410, Test loss: 1.617, Test accuracy: 47.64
Round   6, Train loss: 1.352, Test loss: 1.605, Test accuracy: 48.31
Round   7, Train loss: 1.286, Test loss: 1.529, Test accuracy: 50.13
Round   8, Train loss: 1.244, Test loss: 1.598, Test accuracy: 50.66
Round   9, Train loss: 1.202, Test loss: 1.629, Test accuracy: 50.93
Round  10, Train loss: 1.165, Test loss: 1.477, Test accuracy: 51.96
Round  11, Train loss: 1.133, Test loss: 1.566, Test accuracy: 52.69
Round  12, Train loss: 1.123, Test loss: 1.486, Test accuracy: 51.98
Round  13, Train loss: 1.071, Test loss: 1.510, Test accuracy: 52.78
Round  14, Train loss: 1.053, Test loss: 1.438, Test accuracy: 52.37
Round  15, Train loss: 1.011, Test loss: 1.570, Test accuracy: 54.17
Round  16, Train loss: 0.988, Test loss: 1.529, Test accuracy: 53.20
Round  17, Train loss: 0.968, Test loss: 1.468, Test accuracy: 54.32
Round  18, Train loss: 0.931, Test loss: 1.496, Test accuracy: 54.04
Round  19, Train loss: 0.941, Test loss: 1.479, Test accuracy: 54.41
Round  20, Train loss: 0.912, Test loss: 1.514, Test accuracy: 54.62
Round  21, Train loss: 0.894, Test loss: 1.458, Test accuracy: 54.33
Round  22, Train loss: 0.862, Test loss: 1.568, Test accuracy: 55.60
Round  23, Train loss: 0.874, Test loss: 1.438, Test accuracy: 56.06
Round  24, Train loss: 0.828, Test loss: 1.458, Test accuracy: 55.73
Round  25, Train loss: 0.849, Test loss: 1.530, Test accuracy: 54.81
Round  26, Train loss: 0.810, Test loss: 1.372, Test accuracy: 54.66
Round  27, Train loss: 0.814, Test loss: 1.500, Test accuracy: 56.83
Round  28, Train loss: 0.753, Test loss: 1.505, Test accuracy: 55.73
Round  29, Train loss: 0.794, Test loss: 1.458, Test accuracy: 56.98
Round  30, Train loss: 0.746, Test loss: 1.564, Test accuracy: 57.54
Round  31, Train loss: 0.750, Test loss: 1.527, Test accuracy: 56.69
Round  32, Train loss: 0.739, Test loss: 1.523, Test accuracy: 57.53
Round  33, Train loss: 0.725, Test loss: 1.512, Test accuracy: 57.33
Round  34, Train loss: 0.720, Test loss: 1.646, Test accuracy: 56.15
Round  35, Train loss: 0.732, Test loss: 1.449, Test accuracy: 56.31
Round  36, Train loss: 0.732, Test loss: 1.488, Test accuracy: 56.99
Round  37, Train loss: 0.689, Test loss: 1.435, Test accuracy: 56.69
Round  38, Train loss: 0.692, Test loss: 1.390, Test accuracy: 56.66
Round  39, Train loss: 0.697, Test loss: 1.434, Test accuracy: 57.50
Round  40, Train loss: 0.666, Test loss: 1.436, Test accuracy: 57.37
Round  41, Train loss: 0.659, Test loss: 1.697, Test accuracy: 57.38
Round  42, Train loss: 0.697, Test loss: 1.421, Test accuracy: 56.62
Round  43, Train loss: 0.649, Test loss: 1.530, Test accuracy: 57.37
Round  44, Train loss: 0.637, Test loss: 1.490, Test accuracy: 57.10
Round  45, Train loss: 0.640, Test loss: 1.435, Test accuracy: 56.54
Round  46, Train loss: 0.628, Test loss: 1.499, Test accuracy: 56.76
Round  47, Train loss: 0.665, Test loss: 1.428, Test accuracy: 56.00
Round  48, Train loss: 0.623, Test loss: 1.461, Test accuracy: 56.86
Round  49, Train loss: 0.615, Test loss: 1.537, Test accuracy: 57.35
Round  50, Train loss: 0.640, Test loss: 1.423, Test accuracy: 56.72
Round  51, Train loss: 0.625, Test loss: 1.411, Test accuracy: 57.69
Round  52, Train loss: 0.586, Test loss: 1.525, Test accuracy: 56.95
Round  53, Train loss: 0.614, Test loss: 1.428, Test accuracy: 57.45
Round  54, Train loss: 0.607, Test loss: 1.740, Test accuracy: 57.52
Round  55, Train loss: 0.562, Test loss: 1.550, Test accuracy: 57.96
Round  56, Train loss: 0.606, Test loss: 1.449, Test accuracy: 57.44
Round  57, Train loss: 0.553, Test loss: 1.604, Test accuracy: 56.97
Round  58, Train loss: 0.582, Test loss: 1.442, Test accuracy: 56.84
Round  59, Train loss: 0.601, Test loss: 1.447, Test accuracy: 57.87
Round  60, Train loss: 0.576, Test loss: 1.406, Test accuracy: 57.89
Round  61, Train loss: 0.584, Test loss: 1.574, Test accuracy: 57.34
Round  62, Train loss: 0.552, Test loss: 1.479, Test accuracy: 58.01
Round  63, Train loss: 0.598, Test loss: 1.430, Test accuracy: 56.85
Round  64, Train loss: 0.541, Test loss: 1.543, Test accuracy: 57.28
Round  65, Train loss: 0.571, Test loss: 1.953, Test accuracy: 57.95
Round  66, Train loss: 0.536, Test loss: 1.591, Test accuracy: 57.19
Round  67, Train loss: 0.536, Test loss: 1.504, Test accuracy: 57.98
Round  68, Train loss: 0.538, Test loss: 1.610, Test accuracy: 57.47
Round  69, Train loss: 0.523, Test loss: 1.429, Test accuracy: 58.18
Round  70, Train loss: 0.549, Test loss: 1.628, Test accuracy: 57.54
Round  71, Train loss: 0.547, Test loss: 1.571, Test accuracy: 58.59
Round  72, Train loss: 0.533, Test loss: 1.563, Test accuracy: 58.49
Round  73, Train loss: 0.536, Test loss: 1.612, Test accuracy: 57.63
Round  74, Train loss: 0.507, Test loss: 1.460, Test accuracy: 57.15
Round  75, Train loss: 0.549, Test loss: 1.884, Test accuracy: 58.11
Round  76, Train loss: 0.494, Test loss: 1.778, Test accuracy: 57.39
Round  77, Train loss: 0.522, Test loss: 1.557, Test accuracy: 58.07
Round  78, Train loss: 0.563, Test loss: 1.435, Test accuracy: 57.26
Round  79, Train loss: 0.493, Test loss: 1.538, Test accuracy: 58.37
Round  80, Train loss: 0.516, Test loss: 1.590, Test accuracy: 56.84
Round  81, Train loss: 0.486, Test loss: 1.706, Test accuracy: 57.83
Round  82, Train loss: 0.514, Test loss: 1.483, Test accuracy: 57.31
Round  83, Train loss: 0.482, Test loss: 1.694, Test accuracy: 58.62
Round  84, Train loss: 0.487, Test loss: 1.490, Test accuracy: 56.85
Round  85, Train loss: 0.478, Test loss: 1.596, Test accuracy: 57.49
Round  86, Train loss: 0.489, Test loss: 1.624, Test accuracy: 57.13
Round  87, Train loss: 0.516, Test loss: 1.543, Test accuracy: 57.97
Round  88, Train loss: 0.462, Test loss: 1.635, Test accuracy: 58.09
Round  89, Train loss: 0.499, Test loss: 1.718, Test accuracy: 58.08
Round  90, Train loss: 0.494, Test loss: 1.543, Test accuracy: 58.27
Round  91, Train loss: 0.491, Test loss: 1.530, Test accuracy: 56.19
Round  92, Train loss: 0.475, Test loss: 1.809, Test accuracy: 58.67
Round  93, Train loss: 0.452, Test loss: 1.827, Test accuracy: 58.46
Round  94, Train loss: 0.441, Test loss: 1.410, Test accuracy: 58.39
Round  95, Train loss: 0.487, Test loss: 1.609, Test accuracy: 58.04
Round  96, Train loss: 0.453, Test loss: 1.545, Test accuracy: 57.10
Round  97, Train loss: 0.486, Test loss: 1.633, Test accuracy: 57.17
Round  98, Train loss: 0.463, Test loss: 1.585, Test accuracy: 57.41
Round  99, Train loss: 0.467, Test loss: 1.625, Test accuracy: 57.85
Final Round, Train loss: 0.375, Test loss: 1.344, Test accuracy: 58.49
Average accuracy final 10 rounds: 57.755750000000006
8888.708071947098
[13.144790410995483, 26.220699071884155, 38.735183000564575, 51.19913172721863, 64.27124691009521, 76.81262493133545, 89.41038274765015, 101.83764362335205, 113.99682116508484, 126.26701807975769, 138.8789222240448, 151.73374843597412, 164.43844866752625, 176.72463369369507, 189.43657279014587, 202.4227364063263, 215.29021048545837, 228.0935242176056, 241.62585473060608, 254.1704020500183, 266.9020311832428, 279.88081073760986, 292.65167331695557, 305.06369853019714, 317.61857891082764, 330.26145458221436, 342.7968485355377, 355.46291732788086, 367.600225687027, 380.88126373291016, 393.56182646751404, 405.8241307735443, 418.3549745082855, 431.02975273132324, 443.52315068244934, 456.798540353775, 470.26701045036316, 483.43586707115173, 496.28883695602417, 509.15542483329773, 521.777435541153, 535.3456058502197, 548.690580368042, 562.1614470481873, 575.124577999115, 588.0155599117279, 600.8485524654388, 613.5122303962708, 626.4300048351288, 638.9994647502899, 651.603049993515, 664.8217012882233, 677.735316991806, 690.2958450317383, 703.4313962459564, 716.2827968597412, 729.494372844696, 742.8840923309326, 756.8321657180786, 770.2527101039886, 783.6161460876465, 796.7430422306061, 809.8490710258484, 823.0033495426178, 836.0999910831451, 849.5095479488373, 862.8611052036285, 876.2386617660522, 889.5370190143585, 902.9132075309753, 916.1820001602173, 929.8208274841309, 942.9101450443268, 955.9635987281799, 969.0102458000183, 982.2837629318237, 995.2677459716797, 1008.839813709259, 1022.796879529953, 1036.5998528003693, 1050.2215745449066, 1063.4889063835144, 1077.252319574356, 1090.9129085540771, 1104.4035704135895, 1117.7535848617554, 1130.7412536144257, 1144.1486930847168, 1157.8202505111694, 1171.0612065792084, 1184.4993677139282, 1197.7981536388397, 1210.9881131649017, 1224.2077090740204, 1237.2669060230255, 1250.4377648830414, 1263.7139847278595, 1276.9158186912537, 1290.083580493927, 1303.1357038021088, 1306.5757224559784]
[32.675, 37.69, 41.495, 43.7675, 45.28, 47.6375, 48.315, 50.135, 50.6575, 50.9325, 51.96, 52.69, 51.98, 52.78, 52.3725, 54.1725, 53.205, 54.3175, 54.0425, 54.41, 54.6225, 54.3275, 55.6025, 56.0575, 55.725, 54.8125, 54.665, 56.8275, 55.7275, 56.98, 57.5375, 56.6925, 57.535, 57.33, 56.1475, 56.31, 56.9925, 56.685, 56.665, 57.4975, 57.3675, 57.3825, 56.6175, 57.365, 57.1, 56.5425, 56.755, 56.0, 56.8625, 57.355, 56.72, 57.685, 56.95, 57.4475, 57.52, 57.9575, 57.44, 56.965, 56.84, 57.8725, 57.8875, 57.3375, 58.0075, 56.8525, 57.2825, 57.955, 57.1875, 57.9825, 57.4725, 58.1775, 57.54, 58.59, 58.495, 57.6325, 57.15, 58.11, 57.3925, 58.0725, 57.2625, 58.37, 56.845, 57.825, 57.3075, 58.6225, 56.85, 57.49, 57.1325, 57.97, 58.085, 58.0825, 58.265, 56.1925, 58.675, 58.4625, 58.39, 58.04, 57.1025, 57.1725, 57.41, 57.8475, 58.495]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.306, Test loss: 2.322, Test accuracy: 10.94
Round   0, Global train loss: 2.306, Global test loss: 2.323, Global test accuracy: 10.74
Round   1, Train loss: 2.274, Test loss: 2.318, Test accuracy: 11.48
Round   1, Global train loss: 2.274, Global test loss: 2.317, Global test accuracy: 10.81
Round   2, Train loss: 2.241, Test loss: 2.313, Test accuracy: 11.36
Round   2, Global train loss: 2.241, Global test loss: 2.310, Global test accuracy: 9.94
Round   3, Train loss: 2.267, Test loss: 2.311, Test accuracy: 12.89
Round   3, Global train loss: 2.267, Global test loss: 2.307, Global test accuracy: 11.44
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 14.81
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 17.73
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 18.90
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 23.88
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 20.59
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 21.04
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 23.62
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 25.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 25.00
Average accuracy final 10 rounds: 25.000000000000004 

Average global accuracy final 10 rounds: 25.000000000000004 

5121.406989336014
[4.9464333057403564, 9.563065767288208, 14.146537780761719, 18.79413652420044, 23.43882989883423, 28.072245359420776, 32.799012422561646, 37.49177861213684, 42.13590598106384, 46.76645612716675, 51.93402051925659, 56.618152141571045, 61.35902738571167, 65.93464541435242, 70.50074291229248, 75.12680745124817, 80.10738706588745, 85.06790375709534, 89.88314056396484, 94.56121253967285, 99.51134777069092, 104.6026542186737, 109.49740648269653, 114.33175325393677, 119.25444507598877, 123.89031839370728, 128.6098895072937, 133.33609557151794, 138.03084063529968, 142.66389727592468, 147.1226782798767, 151.69619846343994, 156.61758303642273, 161.81714177131653, 166.66359519958496, 171.46482181549072, 176.37823462486267, 181.1721408367157, 185.99773454666138, 190.92338490486145, 195.87992477416992, 200.93212866783142, 205.676833152771, 210.40473437309265, 215.30584907531738, 220.25712180137634, 225.17614269256592, 230.15378522872925, 235.05729269981384, 239.98022198677063, 244.93570184707642, 249.2516312599182, 253.71049737930298, 258.15373635292053, 262.46701192855835, 266.841183423996, 271.1719214916229, 275.587881565094, 280.07416677474976, 284.5132381916046, 289.0491211414337, 293.4652135372162, 297.89762806892395, 302.5002999305725, 306.94645380973816, 311.3764679431915, 315.97556376457214, 320.54756808280945, 325.1508038043976, 329.60660910606384, 334.10755133628845, 338.66663002967834, 343.1215064525604, 347.5561239719391, 352.5815670490265, 357.58889508247375, 362.44583106040955, 367.12306904792786, 372.0026228427887, 376.95428442955017, 381.9628508090973, 386.6392443180084, 391.2109785079956, 395.7938802242279, 400.3933901786804, 404.8284909725189, 409.45811104774475, 414.11442399024963, 418.66676449775696, 423.21676540374756, 427.8252844810486, 432.37296748161316, 436.8490388393402, 441.38918471336365, 446.00631070137024, 450.6095931529999, 455.23819184303284, 459.84703612327576, 464.47610449790955, 469.1048879623413, 471.69263434410095]
[10.944444444444445, 11.477777777777778, 11.363888888888889, 12.886111111111111, 14.808333333333334, 17.725, 18.897222222222222, 23.883333333333333, 20.594444444444445, 21.038888888888888, 23.622222222222224, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004, 25.000000000000004]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.987, Test loss: 2.011, Test accuracy: 30.59
Round   0, Global train loss: 1.987, Global test loss: 2.103, Global test accuracy: 28.66
Round   1, Train loss: 1.731, Test loss: 1.823, Test accuracy: 36.90
Round   1, Global train loss: 1.731, Global test loss: 1.993, Global test accuracy: 33.48
Round   2, Train loss: 1.563, Test loss: 1.712, Test accuracy: 40.23
Round   2, Global train loss: 1.563, Global test loss: 1.963, Global test accuracy: 35.97
Round   3, Train loss: 1.488, Test loss: 1.633, Test accuracy: 43.15
Round   3, Global train loss: 1.488, Global test loss: 1.899, Global test accuracy: 37.70
Round   4, Train loss: 1.406, Test loss: 1.526, Test accuracy: 46.28
Round   4, Global train loss: 1.406, Global test loss: 1.848, Global test accuracy: 39.53
Round   5, Train loss: 1.344, Test loss: 1.473, Test accuracy: 47.78
Round   5, Global train loss: 1.344, Global test loss: 1.814, Global test accuracy: 38.46
Round   6, Train loss: 1.279, Test loss: 1.439, Test accuracy: 49.49
Round   6, Global train loss: 1.279, Global test loss: 1.883, Global test accuracy: 40.28
Round   7, Train loss: 1.235, Test loss: 1.382, Test accuracy: 51.48
Round   7, Global train loss: 1.235, Global test loss: 1.803, Global test accuracy: 40.12
Round   8, Train loss: 1.198, Test loss: 1.332, Test accuracy: 53.20
Round   8, Global train loss: 1.198, Global test loss: 1.820, Global test accuracy: 40.46
Round   9, Train loss: 1.136, Test loss: 1.271, Test accuracy: 55.33
Round   9, Global train loss: 1.136, Global test loss: 1.810, Global test accuracy: 40.97
Round  10, Train loss: 1.118, Test loss: 1.211, Test accuracy: 57.30
Round  10, Global train loss: 1.118, Global test loss: 1.831, Global test accuracy: 41.20
Round  11, Train loss: 1.054, Test loss: 1.215, Test accuracy: 57.90
Round  11, Global train loss: 1.054, Global test loss: 1.813, Global test accuracy: 43.50
Round  12, Train loss: 1.054, Test loss: 1.161, Test accuracy: 59.65
Round  12, Global train loss: 1.054, Global test loss: 1.782, Global test accuracy: 41.85
Round  13, Train loss: 0.977, Test loss: 1.149, Test accuracy: 60.24
Round  13, Global train loss: 0.977, Global test loss: 1.894, Global test accuracy: 43.93
Round  14, Train loss: 0.964, Test loss: 1.123, Test accuracy: 61.18
Round  14, Global train loss: 0.964, Global test loss: 1.907, Global test accuracy: 43.95
Round  15, Train loss: 0.947, Test loss: 1.107, Test accuracy: 61.78
Round  15, Global train loss: 0.947, Global test loss: 1.837, Global test accuracy: 44.94
Round  16, Train loss: 0.957, Test loss: 1.097, Test accuracy: 62.43
Round  16, Global train loss: 0.957, Global test loss: 1.784, Global test accuracy: 43.70
Round  17, Train loss: 0.892, Test loss: 1.099, Test accuracy: 62.44
Round  17, Global train loss: 0.892, Global test loss: 1.871, Global test accuracy: 43.85
Round  18, Train loss: 0.894, Test loss: 1.095, Test accuracy: 62.73
Round  18, Global train loss: 0.894, Global test loss: 1.858, Global test accuracy: 42.96
Round  19, Train loss: 0.865, Test loss: 1.086, Test accuracy: 63.12
Round  19, Global train loss: 0.865, Global test loss: 1.851, Global test accuracy: 46.03
Round  20, Train loss: 0.853, Test loss: 1.082, Test accuracy: 63.61
Round  20, Global train loss: 0.853, Global test loss: 1.779, Global test accuracy: 44.07
Round  21, Train loss: 0.823, Test loss: 1.066, Test accuracy: 64.39
Round  21, Global train loss: 0.823, Global test loss: 1.935, Global test accuracy: 45.24
Round  22, Train loss: 0.815, Test loss: 1.054, Test accuracy: 65.07
Round  22, Global train loss: 0.815, Global test loss: 1.790, Global test accuracy: 44.48
Round  23, Train loss: 0.802, Test loss: 1.037, Test accuracy: 65.77
Round  23, Global train loss: 0.802, Global test loss: 1.767, Global test accuracy: 44.68
Round  24, Train loss: 0.771, Test loss: 1.029, Test accuracy: 65.95
Round  24, Global train loss: 0.771, Global test loss: 1.785, Global test accuracy: 45.48
Round  25, Train loss: 0.769, Test loss: 1.035, Test accuracy: 66.00
Round  25, Global train loss: 0.769, Global test loss: 1.785, Global test accuracy: 43.92
Round  26, Train loss: 0.749, Test loss: 1.002, Test accuracy: 67.16
Round  26, Global train loss: 0.749, Global test loss: 1.921, Global test accuracy: 45.11
Round  27, Train loss: 0.749, Test loss: 1.016, Test accuracy: 66.97
Round  27, Global train loss: 0.749, Global test loss: 1.739, Global test accuracy: 45.98
Round  28, Train loss: 0.745, Test loss: 1.008, Test accuracy: 67.39
Round  28, Global train loss: 0.745, Global test loss: 1.760, Global test accuracy: 44.66
Round  29, Train loss: 0.711, Test loss: 0.992, Test accuracy: 67.83
Round  29, Global train loss: 0.711, Global test loss: 1.930, Global test accuracy: 47.05
Round  30, Train loss: 0.711, Test loss: 0.982, Test accuracy: 68.38
Round  30, Global train loss: 0.711, Global test loss: 1.788, Global test accuracy: 45.34
Round  31, Train loss: 0.703, Test loss: 0.978, Test accuracy: 68.62
Round  31, Global train loss: 0.703, Global test loss: 1.890, Global test accuracy: 45.09
Round  32, Train loss: 0.698, Test loss: 0.982, Test accuracy: 68.54
Round  32, Global train loss: 0.698, Global test loss: 1.875, Global test accuracy: 44.63
Round  33, Train loss: 0.709, Test loss: 0.975, Test accuracy: 68.98
Round  33, Global train loss: 0.709, Global test loss: 1.867, Global test accuracy: 45.33
Round  34, Train loss: 0.669, Test loss: 0.980, Test accuracy: 69.18
Round  34, Global train loss: 0.669, Global test loss: 1.904, Global test accuracy: 45.69
Round  35, Train loss: 0.695, Test loss: 0.988, Test accuracy: 69.06
Round  35, Global train loss: 0.695, Global test loss: 2.013, Global test accuracy: 44.80
Round  36, Train loss: 0.681, Test loss: 0.999, Test accuracy: 69.22
Round  36, Global train loss: 0.681, Global test loss: 1.833, Global test accuracy: 45.20
Round  37, Train loss: 0.658, Test loss: 0.991, Test accuracy: 69.35
Round  37, Global train loss: 0.658, Global test loss: 1.821, Global test accuracy: 45.18
Round  38, Train loss: 0.651, Test loss: 0.980, Test accuracy: 69.62
Round  38, Global train loss: 0.651, Global test loss: 1.945, Global test accuracy: 46.05
Round  39, Train loss: 0.639, Test loss: 0.982, Test accuracy: 69.37
Round  39, Global train loss: 0.639, Global test loss: 1.830, Global test accuracy: 44.75
Round  40, Train loss: 0.645, Test loss: 0.977, Test accuracy: 69.58
Round  40, Global train loss: 0.645, Global test loss: 1.953, Global test accuracy: 48.09
Round  41, Train loss: 0.639, Test loss: 0.972, Test accuracy: 69.90
Round  41, Global train loss: 0.639, Global test loss: 1.822, Global test accuracy: 46.10
Round  42, Train loss: 0.605, Test loss: 0.982, Test accuracy: 69.75
Round  42, Global train loss: 0.605, Global test loss: 2.022, Global test accuracy: 44.64
Round  43, Train loss: 0.625, Test loss: 0.981, Test accuracy: 69.89
Round  43, Global train loss: 0.625, Global test loss: 1.773, Global test accuracy: 45.75
Round  44, Train loss: 0.605, Test loss: 0.982, Test accuracy: 69.97
Round  44, Global train loss: 0.605, Global test loss: 1.760, Global test accuracy: 47.65
Round  45, Train loss: 0.592, Test loss: 0.973, Test accuracy: 70.19
Round  45, Global train loss: 0.592, Global test loss: 1.999, Global test accuracy: 46.94
Round  46, Train loss: 0.577, Test loss: 0.970, Test accuracy: 70.28
Round  46, Global train loss: 0.577, Global test loss: 1.895, Global test accuracy: 45.99
Round  47, Train loss: 0.575, Test loss: 0.978, Test accuracy: 70.39
Round  47, Global train loss: 0.575, Global test loss: 1.986, Global test accuracy: 46.61
Round  48, Train loss: 0.563, Test loss: 0.978, Test accuracy: 70.42
Round  48, Global train loss: 0.563, Global test loss: 1.961, Global test accuracy: 45.43
Round  49, Train loss: 0.594, Test loss: 0.965, Test accuracy: 70.93
Round  49, Global train loss: 0.594, Global test loss: 1.918, Global test accuracy: 45.09
Round  50, Train loss: 0.533, Test loss: 0.965, Test accuracy: 71.00
Round  50, Global train loss: 0.533, Global test loss: 2.061, Global test accuracy: 45.06
Round  51, Train loss: 0.586, Test loss: 0.966, Test accuracy: 70.94
Round  51, Global train loss: 0.586, Global test loss: 1.875, Global test accuracy: 44.60
Round  52, Train loss: 0.564, Test loss: 0.961, Test accuracy: 71.27
Round  52, Global train loss: 0.564, Global test loss: 2.027, Global test accuracy: 47.81
Round  53, Train loss: 0.569, Test loss: 0.963, Test accuracy: 71.43
Round  53, Global train loss: 0.569, Global test loss: 1.944, Global test accuracy: 46.70
Round  54, Train loss: 0.565, Test loss: 0.961, Test accuracy: 71.31
Round  54, Global train loss: 0.565, Global test loss: 1.884, Global test accuracy: 47.00
Round  55, Train loss: 0.555, Test loss: 0.963, Test accuracy: 70.95
Round  55, Global train loss: 0.555, Global test loss: 1.925, Global test accuracy: 46.84
Round  56, Train loss: 0.557, Test loss: 0.954, Test accuracy: 71.32
Round  56, Global train loss: 0.557, Global test loss: 1.812, Global test accuracy: 44.38
Round  57, Train loss: 0.555, Test loss: 0.963, Test accuracy: 71.36
Round  57, Global train loss: 0.555, Global test loss: 1.836, Global test accuracy: 45.09
Round  58, Train loss: 0.518, Test loss: 0.969, Test accuracy: 71.43
Round  58, Global train loss: 0.518, Global test loss: 1.927, Global test accuracy: 46.09
Round  59, Train loss: 0.540, Test loss: 0.978, Test accuracy: 71.33
Round  59, Global train loss: 0.540, Global test loss: 1.907, Global test accuracy: 46.84
Round  60, Train loss: 0.507, Test loss: 0.989, Test accuracy: 71.09
Round  60, Global train loss: 0.507, Global test loss: 2.088, Global test accuracy: 47.47
Round  61, Train loss: 0.514, Test loss: 0.995, Test accuracy: 70.97
Round  61, Global train loss: 0.514, Global test loss: 1.876, Global test accuracy: 46.35
Round  62, Train loss: 0.508, Test loss: 0.987, Test accuracy: 71.08
Round  62, Global train loss: 0.508, Global test loss: 1.962, Global test accuracy: 46.84
Round  63, Train loss: 0.547, Test loss: 1.002, Test accuracy: 70.83
Round  63, Global train loss: 0.547, Global test loss: 1.860, Global test accuracy: 46.81
Round  64, Train loss: 0.471, Test loss: 0.993, Test accuracy: 71.26
Round  64, Global train loss: 0.471, Global test loss: 2.062, Global test accuracy: 47.00
Round  65, Train loss: 0.520, Test loss: 0.989, Test accuracy: 71.61
Round  65, Global train loss: 0.520, Global test loss: 1.907, Global test accuracy: 45.95
Round  66, Train loss: 0.515, Test loss: 0.983, Test accuracy: 71.90
Round  66, Global train loss: 0.515, Global test loss: 2.055, Global test accuracy: 45.04
Round  67, Train loss: 0.495, Test loss: 0.972, Test accuracy: 72.16
Round  67, Global train loss: 0.495, Global test loss: 2.056, Global test accuracy: 45.81
Round  68, Train loss: 0.528, Test loss: 0.969, Test accuracy: 72.29
Round  68, Global train loss: 0.528, Global test loss: 2.165, Global test accuracy: 47.88
Round  69, Train loss: 0.473, Test loss: 0.963, Test accuracy: 72.35
Round  69, Global train loss: 0.473, Global test loss: 2.032, Global test accuracy: 46.13
Round  70, Train loss: 0.493, Test loss: 0.978, Test accuracy: 72.09
Round  70, Global train loss: 0.493, Global test loss: 2.050, Global test accuracy: 47.22
Round  71, Train loss: 0.467, Test loss: 0.988, Test accuracy: 72.00
Round  71, Global train loss: 0.467, Global test loss: 1.980, Global test accuracy: 46.40
Round  72, Train loss: 0.495, Test loss: 0.984, Test accuracy: 72.11
Round  72, Global train loss: 0.495, Global test loss: 1.945, Global test accuracy: 47.96
Round  73, Train loss: 0.494, Test loss: 0.982, Test accuracy: 72.25
Round  73, Global train loss: 0.494, Global test loss: 2.128, Global test accuracy: 47.31
Round  74, Train loss: 0.478, Test loss: 0.988, Test accuracy: 72.22
Round  74, Global train loss: 0.478, Global test loss: 2.075, Global test accuracy: 47.45
Round  75, Train loss: 0.504, Test loss: 1.001, Test accuracy: 72.10
Round  75, Global train loss: 0.504, Global test loss: 1.925, Global test accuracy: 44.88
Round  76, Train loss: 0.456, Test loss: 0.979, Test accuracy: 72.69
Round  76, Global train loss: 0.456, Global test loss: 2.089, Global test accuracy: 45.63
Round  77, Train loss: 0.451, Test loss: 0.997, Test accuracy: 72.27
Round  77, Global train loss: 0.451, Global test loss: 1.856, Global test accuracy: 46.30
Round  78, Train loss: 0.471, Test loss: 0.987, Test accuracy: 72.29
Round  78, Global train loss: 0.471, Global test loss: 1.876, Global test accuracy: 45.17
Round  79, Train loss: 0.467, Test loss: 0.981, Test accuracy: 72.29
Round  79, Global train loss: 0.467, Global test loss: 2.091, Global test accuracy: 43.70
Round  80, Train loss: 0.439, Test loss: 0.989, Test accuracy: 72.23
Round  80, Global train loss: 0.439, Global test loss: 2.214, Global test accuracy: 44.72
Round  81, Train loss: 0.469, Test loss: 0.984, Test accuracy: 72.39
Round  81, Global train loss: 0.469, Global test loss: 2.236, Global test accuracy: 46.22
Round  82, Train loss: 0.448, Test loss: 0.997, Test accuracy: 72.13
Round  82, Global train loss: 0.448, Global test loss: 2.041, Global test accuracy: 44.27
Round  83, Train loss: 0.461, Test loss: 0.990, Test accuracy: 72.30
Round  83, Global train loss: 0.461, Global test loss: 2.013, Global test accuracy: 47.55
Round  84, Train loss: 0.459, Test loss: 1.000, Test accuracy: 72.12
Round  84, Global train loss: 0.459, Global test loss: 2.151, Global test accuracy: 45.22
Round  85, Train loss: 0.433, Test loss: 1.013, Test accuracy: 71.82
Round  85, Global train loss: 0.433, Global test loss: 2.200, Global test accuracy: 47.47
Round  86, Train loss: 0.498, Test loss: 1.002, Test accuracy: 72.03
Round  86, Global train loss: 0.498, Global test loss: 2.077, Global test accuracy: 46.00
Round  87, Train loss: 0.458, Test loss: 0.995, Test accuracy: 72.51
Round  87, Global train loss: 0.458, Global test loss: 1.888, Global test accuracy: 46.23
Round  88, Train loss: 0.441, Test loss: 0.985, Test accuracy: 73.04
Round  88, Global train loss: 0.441, Global test loss: 1.987, Global test accuracy: 44.32
Round  89, Train loss: 0.458, Test loss: 0.988, Test accuracy: 73.16
Round  89, Global train loss: 0.458, Global test loss: 1.934, Global test accuracy: 45.97
Round  90, Train loss: 0.440, Test loss: 0.995, Test accuracy: 73.06
Round  90, Global train loss: 0.440, Global test loss: 1.947, Global test accuracy: 45.66
Round  91, Train loss: 0.456, Test loss: 1.000, Test accuracy: 73.06
Round  91, Global train loss: 0.456, Global test loss: 2.015, Global test accuracy: 45.85
Round  92, Train loss: 0.416, Test loss: 1.010, Test accuracy: 73.00
Round  92, Global train loss: 0.416, Global test loss: 2.009, Global test accuracy: 46.65
Round  93, Train loss: 0.413, Test loss: 1.018, Test accuracy: 72.78
Round  93, Global train loss: 0.413, Global test loss: 2.092, Global test accuracy: 45.06
Round  94, Train loss: 0.432, Test loss: 1.006, Test accuracy: 73.03
Round  94, Global train loss: 0.432, Global test loss: 2.141, Global test accuracy: 48.18
Round  95, Train loss: 0.456, Test loss: 1.012, Test accuracy: 72.84
Round  95, Global train loss: 0.456, Global test loss: 2.007, Global test accuracy: 47.00
Round  96, Train loss: 0.439, Test loss: 1.005, Test accuracy: 73.04
Round  96, Global train loss: 0.439, Global test loss: 1.964, Global test accuracy: 46.93
Round  97, Train loss: 0.413, Test loss: 1.009, Test accuracy: 72.84
Round  97, Global train loss: 0.413, Global test loss: 2.109, Global test accuracy: 45.24
Round  98, Train loss: 0.427, Test loss: 1.013, Test accuracy: 72.81
Round  98, Global train loss: 0.427, Global test loss: 1.936, Global test accuracy: 46.23
Round  99, Train loss: 0.421, Test loss: 1.017, Test accuracy: 72.72
Round  99, Global train loss: 0.421, Global test loss: 2.175, Global test accuracy: 45.34
Final Round, Train loss: 0.286, Test loss: 1.108, Test accuracy: 73.44
Final Round, Global train loss: 0.286, Global test loss: 2.175, Global test accuracy: 45.34
Average accuracy final 10 rounds: 72.918 

Average global accuracy final 10 rounds: 46.213499999999996 

6698.512730360031
[5.233429431915283, 10.466858863830566, 15.644017696380615, 20.821176528930664, 25.929871082305908, 31.038565635681152, 36.135982036590576, 41.2333984375, 46.1887264251709, 51.1440544128418, 56.17900848388672, 61.21396255493164, 66.24644088745117, 71.2789192199707, 76.24998092651367, 81.22104263305664, 86.12847638130188, 91.03591012954712, 95.65814447402954, 100.28037881851196, 104.96953225135803, 109.6586856842041, 114.30311608314514, 118.94754648208618, 123.48605513572693, 128.02456378936768, 132.610276222229, 137.19598865509033, 141.80412673950195, 146.41226482391357, 151.14201855659485, 155.87177228927612, 160.72219061851501, 165.5726089477539, 170.2127194404602, 174.8528299331665, 179.53298592567444, 184.21314191818237, 188.92117953300476, 193.62921714782715, 198.35814905166626, 203.08708095550537, 207.65262174606323, 212.2181625366211, 216.8129904270172, 221.40781831741333, 226.08378219604492, 230.7597460746765, 235.45285820960999, 240.14597034454346, 244.79335284233093, 249.4407353401184, 254.12184405326843, 258.80295276641846, 263.55435848236084, 268.3057641983032, 273.0676279067993, 277.8294916152954, 282.382572889328, 286.9356541633606, 291.6383981704712, 296.3411421775818, 301.0859212875366, 305.83070039749146, 310.4182870388031, 315.00587368011475, 319.58493661880493, 324.1639995574951, 328.8604974746704, 333.5569953918457, 338.2465760707855, 342.93615674972534, 347.5305709838867, 352.1249852180481, 357.0177068710327, 361.91042852401733, 366.9209897518158, 371.93155097961426, 376.8588502407074, 381.78614950180054, 386.7222135066986, 391.6582775115967, 396.412713766098, 401.16715002059937, 406.1659200191498, 411.1646900177002, 416.17937207221985, 421.1940541267395, 426.12432050704956, 431.0545868873596, 435.506982088089, 439.95937728881836, 444.39550828933716, 448.83163928985596, 453.2587378025055, 457.68583631515503, 462.50498056411743, 467.32412481307983, 472.0841157436371, 476.84410667419434, 481.5807754993439, 486.3174443244934, 491.0525608062744, 495.7876772880554, 500.4987270832062, 505.20977687835693, 509.88823652267456, 514.5666961669922, 519.138279914856, 523.7098636627197, 528.2913393974304, 532.8728151321411, 537.4593875408173, 542.0459599494934, 546.6174166202545, 551.1888732910156, 555.926201581955, 560.6635298728943, 565.1694567203522, 569.6753835678101, 574.1967568397522, 578.7181301116943, 583.2969198226929, 587.8757095336914, 592.4566376209259, 597.0375657081604, 601.5384895801544, 606.0394134521484, 610.7582101821899, 615.4770069122314, 620.225307226181, 624.9736075401306, 629.3720536231995, 633.7704997062683, 638.4784667491913, 643.1864337921143, 647.9029026031494, 652.6193714141846, 657.1069757938385, 661.5945801734924, 666.0085506439209, 670.4225211143494, 674.8770699501038, 679.3316187858582, 683.7365863323212, 688.1415538787842, 692.5567800998688, 696.9720063209534, 701.4336075782776, 705.8952088356018, 710.4108316898346, 714.9264545440674, 719.3746600151062, 723.822865486145, 728.3064501285553, 732.7900347709656, 737.3786172866821, 741.9671998023987, 746.366304397583, 750.7654089927673, 755.2340369224548, 759.7026648521423, 764.1534740924835, 768.6042833328247, 773.0250127315521, 777.4457421302795, 782.0329637527466, 786.6201853752136, 791.3314332962036, 796.0426812171936, 800.7397077083588, 805.4367341995239, 810.2138648033142, 814.9909954071045, 819.8259620666504, 824.6609287261963, 829.445553779602, 834.2301788330078, 838.9799792766571, 843.7297797203064, 848.5521104335785, 853.3744411468506, 858.246096611023, 863.1177520751953, 867.9252851009369, 872.7328181266785, 877.5144143104553, 882.2960104942322, 887.099889755249, 891.9037690162659, 896.7452311515808, 901.5866932868958, 906.331969499588, 911.0772457122803, 915.8224956989288, 920.5677456855774, 925.3635137081146, 930.1592817306519, 934.8395359516144, 939.5197901725769, 941.93416929245, 944.348548412323]
[30.5925, 30.5925, 36.895, 36.895, 40.2275, 40.2275, 43.1525, 43.1525, 46.2775, 46.2775, 47.7775, 47.7775, 49.4875, 49.4875, 51.475, 51.475, 53.1975, 53.1975, 55.3325, 55.3325, 57.295, 57.295, 57.8975, 57.8975, 59.6475, 59.6475, 60.245, 60.245, 61.1825, 61.1825, 61.7825, 61.7825, 62.4275, 62.4275, 62.44, 62.44, 62.735, 62.735, 63.12, 63.12, 63.6125, 63.6125, 64.395, 64.395, 65.07, 65.07, 65.765, 65.765, 65.9475, 65.9475, 65.9975, 65.9975, 67.1575, 67.1575, 66.9675, 66.9675, 67.395, 67.395, 67.83, 67.83, 68.3775, 68.3775, 68.6225, 68.6225, 68.54, 68.54, 68.98, 68.98, 69.18, 69.18, 69.055, 69.055, 69.22, 69.22, 69.3475, 69.3475, 69.625, 69.625, 69.3675, 69.3675, 69.5825, 69.5825, 69.8975, 69.8975, 69.75, 69.75, 69.89, 69.89, 69.965, 69.965, 70.1925, 70.1925, 70.275, 70.275, 70.395, 70.395, 70.415, 70.415, 70.9275, 70.9275, 71.005, 71.005, 70.9425, 70.9425, 71.2675, 71.2675, 71.4325, 71.4325, 71.305, 71.305, 70.955, 70.955, 71.32, 71.32, 71.3575, 71.3575, 71.43, 71.43, 71.335, 71.335, 71.0925, 71.0925, 70.9725, 70.9725, 71.085, 71.085, 70.835, 70.835, 71.2625, 71.2625, 71.6125, 71.6125, 71.8975, 71.8975, 72.155, 72.155, 72.29, 72.29, 72.3525, 72.3525, 72.095, 72.095, 71.9975, 71.9975, 72.1125, 72.1125, 72.2475, 72.2475, 72.2175, 72.2175, 72.1025, 72.1025, 72.685, 72.685, 72.2725, 72.2725, 72.2875, 72.2875, 72.29, 72.29, 72.2325, 72.2325, 72.39, 72.39, 72.1275, 72.1275, 72.2975, 72.2975, 72.12, 72.12, 71.82, 71.82, 72.025, 72.025, 72.51, 72.51, 73.0375, 73.0375, 73.1575, 73.1575, 73.0575, 73.0575, 73.0575, 73.0575, 73.005, 73.005, 72.7825, 72.7825, 73.0325, 73.0325, 72.8375, 72.8375, 73.04, 73.04, 72.84, 72.84, 72.8125, 72.8125, 72.715, 72.715, 73.4375, 73.4375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 56086 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2181, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 50815 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 504, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 51148 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 50735 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.007, Test loss: 2.076, Test accuracy: 24.52
Round   0, Global train loss: 2.007, Global test loss: 2.189, Global test accuracy: 20.68
Round   1, Train loss: 1.767, Test loss: 1.958, Test accuracy: 30.60
Round   1, Global train loss: 1.767, Global test loss: 2.185, Global test accuracy: 23.02
Round   2, Train loss: 1.643, Test loss: 1.813, Test accuracy: 34.77
Round   2, Global train loss: 1.643, Global test loss: 2.162, Global test accuracy: 23.97
Round   3, Train loss: 1.550, Test loss: 1.712, Test accuracy: 37.75
Round   3, Global train loss: 1.550, Global test loss: 2.118, Global test accuracy: 24.95
Round   4, Train loss: 1.455, Test loss: 1.666, Test accuracy: 40.94
Round   4, Global train loss: 1.455, Global test loss: 2.234, Global test accuracy: 27.07
Round   5, Train loss: 1.412, Test loss: 1.498, Test accuracy: 46.16
Round   5, Global train loss: 1.412, Global test loss: 2.115, Global test accuracy: 25.93
Round   6, Train loss: 1.342, Test loss: 1.451, Test accuracy: 48.28
Round   6, Global train loss: 1.342, Global test loss: 2.088, Global test accuracy: 26.33
Round   7, Train loss: 1.287, Test loss: 1.390, Test accuracy: 51.05
Round   7, Global train loss: 1.287, Global test loss: 2.194, Global test accuracy: 26.39
Round   8, Train loss: 1.233, Test loss: 1.316, Test accuracy: 53.46
Round   8, Global train loss: 1.233, Global test loss: 2.076, Global test accuracy: 27.89
Round   9, Train loss: 1.207, Test loss: 1.281, Test accuracy: 55.13
Round   9, Global train loss: 1.207, Global test loss: 2.116, Global test accuracy: 27.59
Round  10, Train loss: 1.152, Test loss: 1.271, Test accuracy: 56.03
Round  10, Global train loss: 1.152, Global test loss: 2.117, Global test accuracy: 27.20
Round  11, Train loss: 1.118, Test loss: 1.190, Test accuracy: 58.47
Round  11, Global train loss: 1.118, Global test loss: 2.169, Global test accuracy: 27.79
Round  12, Train loss: 1.087, Test loss: 1.170, Test accuracy: 59.36
Round  12, Global train loss: 1.087, Global test loss: 2.235, Global test accuracy: 27.59
Round  13, Train loss: 1.058, Test loss: 1.161, Test accuracy: 59.76
Round  13, Global train loss: 1.058, Global test loss: 2.213, Global test accuracy: 27.78
Round  14, Train loss: 1.039, Test loss: 1.142, Test accuracy: 60.38
Round  14, Global train loss: 1.039, Global test loss: 2.258, Global test accuracy: 27.88
Round  15, Train loss: 1.027, Test loss: 1.138, Test accuracy: 60.65
Round  15, Global train loss: 1.027, Global test loss: 2.141, Global test accuracy: 28.63
Round  16, Train loss: 0.978, Test loss: 1.134, Test accuracy: 61.06
Round  16, Global train loss: 0.978, Global test loss: 2.195, Global test accuracy: 27.81
Round  17, Train loss: 0.946, Test loss: 1.131, Test accuracy: 61.38
Round  17, Global train loss: 0.946, Global test loss: 2.248, Global test accuracy: 28.27
Round  18, Train loss: 0.946, Test loss: 1.099, Test accuracy: 62.28
Round  18, Global train loss: 0.946, Global test loss: 2.141, Global test accuracy: 28.24
Round  19, Train loss: 0.913, Test loss: 1.090, Test accuracy: 62.77
Round  19, Global train loss: 0.913, Global test loss: 2.207, Global test accuracy: 29.60
Round  20, Train loss: 0.903, Test loss: 1.086, Test accuracy: 63.18
Round  20, Global train loss: 0.903, Global test loss: 2.215, Global test accuracy: 28.42
Round  21, Train loss: 0.879, Test loss: 1.074, Test accuracy: 63.78
Round  21, Global train loss: 0.879, Global test loss: 2.299, Global test accuracy: 27.79
Round  22, Train loss: 0.858, Test loss: 1.068, Test accuracy: 64.14
Round  22, Global train loss: 0.858, Global test loss: 2.238, Global test accuracy: 28.02
Round  23, Train loss: 0.885, Test loss: 1.051, Test accuracy: 64.98
Round  23, Global train loss: 0.885, Global test loss: 2.231, Global test accuracy: 29.30
Round  24, Train loss: 0.836, Test loss: 1.059, Test accuracy: 65.03
Round  24, Global train loss: 0.836, Global test loss: 2.250, Global test accuracy: 28.92
Round  25, Train loss: 0.819, Test loss: 1.064, Test accuracy: 65.25
Round  25, Global train loss: 0.819, Global test loss: 2.374, Global test accuracy: 28.30
Round  26, Train loss: 0.860, Test loss: 1.044, Test accuracy: 65.98
Round  26, Global train loss: 0.860, Global test loss: 2.120, Global test accuracy: 28.18
Round  27, Train loss: 0.810, Test loss: 1.037, Test accuracy: 66.33
Round  27, Global train loss: 0.810, Global test loss: 2.265, Global test accuracy: 29.02
Round  28, Train loss: 0.807, Test loss: 1.019, Test accuracy: 66.81
Round  28, Global train loss: 0.807, Global test loss: 2.108, Global test accuracy: 29.45
Round  29, Train loss: 0.774, Test loss: 1.018, Test accuracy: 67.01
Round  29, Global train loss: 0.774, Global test loss: 2.281, Global test accuracy: 28.72
Round  30, Train loss: 0.780, Test loss: 1.018, Test accuracy: 67.07
Round  30, Global train loss: 0.780, Global test loss: 2.365, Global test accuracy: 28.73
Round  31, Train loss: 0.773, Test loss: 1.009, Test accuracy: 67.52
Round  31, Global train loss: 0.773, Global test loss: 2.238, Global test accuracy: 28.79
Round  32, Train loss: 0.757, Test loss: 1.007, Test accuracy: 67.77
Round  32, Global train loss: 0.757, Global test loss: 2.194, Global test accuracy: 29.14
Round  33, Train loss: 0.739, Test loss: 1.001, Test accuracy: 68.05
Round  33, Global train loss: 0.739, Global test loss: 2.346, Global test accuracy: 29.52
Round  34, Train loss: 0.729, Test loss: 0.991, Test accuracy: 68.38
Round  34, Global train loss: 0.729, Global test loss: 2.367, Global test accuracy: 29.25
Round  35, Train loss: 0.733, Test loss: 0.994, Test accuracy: 68.53
Round  35, Global train loss: 0.733, Global test loss: 2.359, Global test accuracy: 28.51
Round  36, Train loss: 0.720, Test loss: 1.002, Test accuracy: 68.56
Round  36, Global train loss: 0.720, Global test loss: 2.357, Global test accuracy: 28.79
Round  37, Train loss: 0.712, Test loss: 0.995, Test accuracy: 68.88
Round  37, Global train loss: 0.712, Global test loss: 2.547, Global test accuracy: 29.35
Round  38, Train loss: 0.699, Test loss: 1.000, Test accuracy: 68.84
Round  38, Global train loss: 0.699, Global test loss: 2.243, Global test accuracy: 28.21
Round  39, Train loss: 0.681, Test loss: 0.999, Test accuracy: 69.04
Round  39, Global train loss: 0.681, Global test loss: 2.560, Global test accuracy: 29.53
Round  40, Train loss: 0.694, Test loss: 1.003, Test accuracy: 69.04
Round  40, Global train loss: 0.694, Global test loss: 2.424, Global test accuracy: 29.11
Round  41, Train loss: 0.665, Test loss: 0.988, Test accuracy: 69.50
Round  41, Global train loss: 0.665, Global test loss: 2.466, Global test accuracy: 28.65
Round  42, Train loss: 0.648, Test loss: 0.993, Test accuracy: 69.48
Round  42, Global train loss: 0.648, Global test loss: 2.298, Global test accuracy: 28.27
Round  43, Train loss: 0.675, Test loss: 0.977, Test accuracy: 69.90
Round  43, Global train loss: 0.675, Global test loss: 2.291, Global test accuracy: 28.95
Round  44, Train loss: 0.661, Test loss: 0.976, Test accuracy: 70.00
Round  44, Global train loss: 0.661, Global test loss: 2.231, Global test accuracy: 29.59
Round  45, Train loss: 0.630, Test loss: 0.982, Test accuracy: 70.13
Round  45, Global train loss: 0.630, Global test loss: 2.450, Global test accuracy: 29.46
Round  46, Train loss: 0.667, Test loss: 0.978, Test accuracy: 70.27
Round  46, Global train loss: 0.667, Global test loss: 2.279, Global test accuracy: 28.43
Round  47, Train loss: 0.621, Test loss: 0.985, Test accuracy: 70.25
Round  47, Global train loss: 0.621, Global test loss: 2.551, Global test accuracy: 30.07
Round  48, Train loss: 0.650, Test loss: 0.985, Test accuracy: 70.14
Round  48, Global train loss: 0.650, Global test loss: 2.435, Global test accuracy: 28.94
Round  49, Train loss: 0.628, Test loss: 0.988, Test accuracy: 70.20
Round  49, Global train loss: 0.628, Global test loss: 2.362, Global test accuracy: 28.88
Round  50, Train loss: 0.610, Test loss: 0.985, Test accuracy: 70.35
Round  50, Global train loss: 0.610, Global test loss: 2.428, Global test accuracy: 29.62
Round  51, Train loss: 0.603, Test loss: 0.988, Test accuracy: 70.27
Round  51, Global train loss: 0.603, Global test loss: 2.228, Global test accuracy: 29.15
Round  52, Train loss: 0.586, Test loss: 0.984, Test accuracy: 70.55
Round  52, Global train loss: 0.586, Global test loss: 2.523, Global test accuracy: 30.21
Round  53, Train loss: 0.597, Test loss: 0.987, Test accuracy: 70.66
Round  53, Global train loss: 0.597, Global test loss: 2.424, Global test accuracy: 29.82
Round  54, Train loss: 0.622, Test loss: 0.993, Test accuracy: 70.61
Round  54, Global train loss: 0.622, Global test loss: 2.302, Global test accuracy: 29.88
Round  55, Train loss: 0.574, Test loss: 0.996, Test accuracy: 70.56
Round  55, Global train loss: 0.574, Global test loss: 2.334, Global test accuracy: 29.52
Round  56, Train loss: 0.583, Test loss: 0.988, Test accuracy: 70.60
Round  56, Global train loss: 0.583, Global test loss: 2.372, Global test accuracy: 29.32
Round  57, Train loss: 0.599, Test loss: 0.985, Test accuracy: 70.70
Round  57, Global train loss: 0.599, Global test loss: 2.646, Global test accuracy: 28.66
Round  58, Train loss: 0.568, Test loss: 0.991, Test accuracy: 70.56
Round  58, Global train loss: 0.568, Global test loss: 2.430, Global test accuracy: 29.49
Round  59, Train loss: 0.563, Test loss: 0.991, Test accuracy: 70.80
Round  59, Global train loss: 0.563, Global test loss: 2.531, Global test accuracy: 30.41
Round  60, Train loss: 0.547, Test loss: 0.990, Test accuracy: 70.88
Round  60, Global train loss: 0.547, Global test loss: 2.627, Global test accuracy: 29.54
Round  61, Train loss: 0.556, Test loss: 0.991, Test accuracy: 71.02
Round  61, Global train loss: 0.556, Global test loss: 2.439, Global test accuracy: 28.73
Round  62, Train loss: 0.562, Test loss: 0.983, Test accuracy: 71.20
Round  62, Global train loss: 0.562, Global test loss: 2.468, Global test accuracy: 29.71
Round  63, Train loss: 0.584, Test loss: 0.966, Test accuracy: 71.67
Round  63, Global train loss: 0.584, Global test loss: 2.342, Global test accuracy: 29.40
Round  64, Train loss: 0.555, Test loss: 0.978, Test accuracy: 71.28
Round  64, Global train loss: 0.555, Global test loss: 2.453, Global test accuracy: 29.45
Round  65, Train loss: 0.559, Test loss: 0.975, Test accuracy: 71.36
Round  65, Global train loss: 0.559, Global test loss: 2.395, Global test accuracy: 29.06
Round  66, Train loss: 0.554, Test loss: 0.969, Test accuracy: 71.67
Round  66, Global train loss: 0.554, Global test loss: 2.401, Global test accuracy: 28.86
Round  67, Train loss: 0.525, Test loss: 0.977, Test accuracy: 71.78
Round  67, Global train loss: 0.525, Global test loss: 2.389, Global test accuracy: 29.08
Round  68, Train loss: 0.555, Test loss: 0.966, Test accuracy: 72.06
Round  68, Global train loss: 0.555, Global test loss: 2.448, Global test accuracy: 29.14
Round  69, Train loss: 0.554, Test loss: 0.966, Test accuracy: 72.22
Round  69, Global train loss: 0.554, Global test loss: 2.457, Global test accuracy: 28.91
Round  70, Train loss: 0.533, Test loss: 0.972, Test accuracy: 72.08
Round  70, Global train loss: 0.533, Global test loss: 2.614, Global test accuracy: 29.83
Round  71, Train loss: 0.523, Test loss: 0.975, Test accuracy: 72.00
Round  71, Global train loss: 0.523, Global test loss: 2.443, Global test accuracy: 30.27
Round  72, Train loss: 0.516, Test loss: 0.962, Test accuracy: 72.22
Round  72, Global train loss: 0.516, Global test loss: 2.497, Global test accuracy: 29.95
Round  73, Train loss: 0.504, Test loss: 0.958, Test accuracy: 72.36
Round  73, Global train loss: 0.504, Global test loss: 2.411, Global test accuracy: 29.80
Round  74, Train loss: 0.508, Test loss: 0.960, Test accuracy: 72.36
Round  74, Global train loss: 0.508, Global test loss: 2.421, Global test accuracy: 28.95
Round  75, Train loss: 0.538, Test loss: 0.972, Test accuracy: 72.12
Round  75, Global train loss: 0.538, Global test loss: 2.389, Global test accuracy: 28.92
Round  76, Train loss: 0.489, Test loss: 0.976, Test accuracy: 72.09
Round  76, Global train loss: 0.489, Global test loss: 2.529, Global test accuracy: 29.11
Round  77, Train loss: 0.510, Test loss: 0.962, Test accuracy: 72.34
Round  77, Global train loss: 0.510, Global test loss: 2.444, Global test accuracy: 29.33
Round  78, Train loss: 0.495, Test loss: 0.963, Test accuracy: 72.39
Round  78, Global train loss: 0.495, Global test loss: 2.496, Global test accuracy: 29.46
Round  79, Train loss: 0.489, Test loss: 0.979, Test accuracy: 72.09
Round  79, Global train loss: 0.489, Global test loss: 2.456, Global test accuracy: 28.99
Round  80, Train loss: 0.481, Test loss: 0.967, Test accuracy: 72.41
Round  80, Global train loss: 0.481, Global test loss: 2.433, Global test accuracy: 29.44
Round  81, Train loss: 0.484, Test loss: 0.974, Test accuracy: 72.49
Round  81, Global train loss: 0.484, Global test loss: 2.478, Global test accuracy: 29.14
Round  82, Train loss: 0.484, Test loss: 0.983, Test accuracy: 72.33
Round  82, Global train loss: 0.484, Global test loss: 2.686, Global test accuracy: 29.74
Round  83, Train loss: 0.482, Test loss: 1.001, Test accuracy: 72.17
Round  83, Global train loss: 0.482, Global test loss: 2.296, Global test accuracy: 29.25
Round  84, Train loss: 0.492, Test loss: 0.993, Test accuracy: 72.41
Round  84, Global train loss: 0.492, Global test loss: 2.551, Global test accuracy: 29.98
Round  85, Train loss: 0.485, Test loss: 0.995, Test accuracy: 72.53
Round  85, Global train loss: 0.485, Global test loss: 2.442, Global test accuracy: 29.11
Round  86, Train loss: 0.494, Test loss: 0.984, Test accuracy: 72.69
Round  86, Global train loss: 0.494, Global test loss: 2.402, Global test accuracy: 30.00
Round  87, Train loss: 0.472, Test loss: 0.984, Test accuracy: 72.63
Round  87, Global train loss: 0.472, Global test loss: 2.499, Global test accuracy: 29.35
Round  88, Train loss: 0.488, Test loss: 0.991, Test accuracy: 72.62
Round  88, Global train loss: 0.488, Global test loss: 2.462, Global test accuracy: 29.80
Round  89, Train loss: 0.463, Test loss: 1.001, Test accuracy: 72.53
Round  89, Global train loss: 0.463, Global test loss: 2.880, Global test accuracy: 31.05
Round  90, Train loss: 0.477, Test loss: 0.982, Test accuracy: 72.79
Round  90, Global train loss: 0.477, Global test loss: 2.552, Global test accuracy: 29.24
Round  91, Train loss: 0.447, Test loss: 0.984, Test accuracy: 73.00
Round  91, Global train loss: 0.447, Global test loss: 2.426, Global test accuracy: 29.80
Round  92, Train loss: 0.460, Test loss: 0.984, Test accuracy: 72.91
Round  92, Global train loss: 0.460, Global test loss: 2.658, Global test accuracy: 30.82
Round  93, Train loss: 0.483, Test loss: 0.994, Test accuracy: 72.73
Round  93, Global train loss: 0.483, Global test loss: 2.723, Global test accuracy: 28.49
Round  94, Train loss: 0.455, Test loss: 0.982, Test accuracy: 72.79
Round  94, Global train loss: 0.455, Global test loss: 2.388, Global test accuracy: 28.92
Round  95, Train loss: 0.457, Test loss: 0.974, Test accuracy: 72.93
Round  95, Global train loss: 0.457, Global test loss: 2.755, Global test accuracy: 28.98
Round  96, Train loss: 0.435, Test loss: 0.984, Test accuracy: 73.08
Round  96, Global train loss: 0.435, Global test loss: 2.403, Global test accuracy: 29.07
Round  97, Train loss: 0.442, Test loss: 0.987, Test accuracy: 73.11
Round  97, Global train loss: 0.442, Global test loss: 2.587, Global test accuracy: 30.27
Round  98, Train loss: 0.451, Test loss: 1.010, Test accuracy: 73.00
Round  98, Global train loss: 0.451, Global test loss: 2.374, Global test accuracy: 28.41
Round  99, Train loss: 0.461, Test loss: 0.995, Test accuracy: 73.00
Round  99, Global train loss: 0.461, Global test loss: 2.346, Global test accuracy: 29.25
Final Round, Train loss: 0.311, Test loss: 1.075, Test accuracy: 73.64
Final Round, Global train loss: 0.311, Global test loss: 2.346, Global test accuracy: 29.25
Average accuracy final 10 rounds: 72.93275 

Average global accuracy final 10 rounds: 29.32625 

6928.736225128174
[5.6839354038238525, 11.367870807647705, 16.78198766708374, 22.196104526519775, 27.368363857269287, 32.5406231880188, 37.94085335731506, 43.34108352661133, 48.28649282455444, 53.23190212249756, 58.103848934173584, 62.97579574584961, 67.81043648719788, 72.64507722854614, 77.47201347351074, 82.29894971847534, 87.13218760490417, 91.96542549133301, 96.86832785606384, 101.77123022079468, 106.59205770492554, 111.4128851890564, 116.2569682598114, 121.1010513305664, 125.96824717521667, 130.83544301986694, 135.5002920627594, 140.16514110565186, 145.02769470214844, 149.89024829864502, 154.69646668434143, 159.50268507003784, 164.23428225517273, 168.96587944030762, 173.80621910095215, 178.64655876159668, 183.48297309875488, 188.3193874359131, 193.1376097202301, 197.95583200454712, 202.8331229686737, 207.7104139328003, 212.48632049560547, 217.26222705841064, 222.0389220714569, 226.81561708450317, 231.6760745048523, 236.53653192520142, 241.39395904541016, 246.2513861656189, 251.0731942653656, 255.8950023651123, 260.7490575313568, 265.6031126976013, 270.44173288345337, 275.2803530693054, 280.1764132976532, 285.072473526001, 289.9128911495209, 294.75330877304077, 299.4667589664459, 304.1802091598511, 309.07610726356506, 313.97200536727905, 318.81920623779297, 323.6664071083069, 328.3784031867981, 333.0903992652893, 337.88809156417847, 342.6857838630676, 347.51521134376526, 352.3446388244629, 357.1977677345276, 362.0508966445923, 367.02615451812744, 372.0014123916626, 376.9765160083771, 381.95161962509155, 386.7713131904602, 391.59100675582886, 396.286657333374, 400.9823079109192, 405.838182926178, 410.69405794143677, 415.48295187950134, 420.2718458175659, 425.16555738449097, 430.059268951416, 434.8621244430542, 439.6649799346924, 444.51671075820923, 449.3684415817261, 454.19738388061523, 459.0263261795044, 463.86672163009644, 468.7071170806885, 473.3705065250397, 478.03389596939087, 482.78743505477905, 487.54097414016724, 492.2440974712372, 496.94722080230713, 501.6395511627197, 506.3318815231323, 510.9606463909149, 515.5894112586975, 520.188291311264, 524.7871713638306, 529.6037590503693, 534.420346736908, 539.3504157066345, 544.2804846763611, 549.1263470649719, 553.9722094535828, 558.7428760528564, 563.5135426521301, 568.3860366344452, 573.2585306167603, 577.866950750351, 582.4753708839417, 587.0693643093109, 591.6633577346802, 596.3022046089172, 600.9410514831543, 605.7429659366608, 610.5448803901672, 615.2213642597198, 619.8978481292725, 624.6045250892639, 629.3112020492554, 633.9558746814728, 638.6005473136902, 643.2368104457855, 647.8730735778809, 652.5198185443878, 657.1665635108948, 661.8029596805573, 666.4393558502197, 671.1058180332184, 675.772280216217, 680.4501669406891, 685.1280536651611, 689.7734174728394, 694.4187812805176, 699.1303329467773, 703.8418846130371, 708.6114959716797, 713.3811073303223, 717.9981279373169, 722.6151485443115, 727.1954305171967, 731.7757124900818, 736.4052999019623, 741.0348873138428, 745.6372666358948, 750.2396459579468, 754.8229658603668, 759.4062857627869, 764.048775434494, 768.6912651062012, 773.3184564113617, 777.9456477165222, 782.5631723403931, 787.1806969642639, 791.7912058830261, 796.4017148017883, 800.8961153030396, 805.3905158042908, 809.838552236557, 814.2865886688232, 818.7356724739075, 823.1847562789917, 827.6658115386963, 832.1468667984009, 836.6291987895966, 841.1115307807922, 845.5972797870636, 850.083028793335, 854.5357055664062, 858.9883823394775, 863.4815912246704, 867.9748001098633, 872.4586999416351, 876.942599773407, 881.3673281669617, 885.7920565605164, 890.2313220500946, 894.6705875396729, 899.1330671310425, 903.5955467224121, 908.0384016036987, 912.4812564849854, 916.9240484237671, 921.3668403625488, 925.8270318508148, 930.2872233390808, 934.7183825969696, 939.1495418548584, 943.6268961429596, 948.1042504310608, 950.3720977306366, 952.6399450302124]
[24.515, 24.515, 30.6025, 30.6025, 34.7725, 34.7725, 37.7475, 37.7475, 40.935, 40.935, 46.155, 46.155, 48.285, 48.285, 51.05, 51.05, 53.4625, 53.4625, 55.13, 55.13, 56.03, 56.03, 58.4675, 58.4675, 59.3575, 59.3575, 59.7625, 59.7625, 60.385, 60.385, 60.6525, 60.6525, 61.06, 61.06, 61.385, 61.385, 62.285, 62.285, 62.7675, 62.7675, 63.1825, 63.1825, 63.7775, 63.7775, 64.145, 64.145, 64.9775, 64.9775, 65.035, 65.035, 65.2525, 65.2525, 65.98, 65.98, 66.325, 66.325, 66.815, 66.815, 67.0125, 67.0125, 67.0675, 67.0675, 67.5225, 67.5225, 67.7725, 67.7725, 68.05, 68.05, 68.38, 68.38, 68.535, 68.535, 68.56, 68.56, 68.875, 68.875, 68.84, 68.84, 69.04, 69.04, 69.0375, 69.0375, 69.505, 69.505, 69.48, 69.48, 69.9025, 69.9025, 70.005, 70.005, 70.13, 70.13, 70.265, 70.265, 70.2475, 70.2475, 70.1425, 70.1425, 70.2, 70.2, 70.3475, 70.3475, 70.2675, 70.2675, 70.545, 70.545, 70.655, 70.655, 70.61, 70.61, 70.5625, 70.5625, 70.5975, 70.5975, 70.7025, 70.7025, 70.56, 70.56, 70.7975, 70.7975, 70.8775, 70.8775, 71.0175, 71.0175, 71.2025, 71.2025, 71.67, 71.67, 71.28, 71.28, 71.36, 71.36, 71.6675, 71.6675, 71.775, 71.775, 72.055, 72.055, 72.2225, 72.2225, 72.085, 72.085, 72.0025, 72.0025, 72.22, 72.22, 72.3575, 72.3575, 72.3575, 72.3575, 72.12, 72.12, 72.095, 72.095, 72.3425, 72.3425, 72.385, 72.385, 72.0875, 72.0875, 72.4125, 72.4125, 72.4875, 72.4875, 72.325, 72.325, 72.175, 72.175, 72.41, 72.41, 72.535, 72.535, 72.695, 72.695, 72.6325, 72.6325, 72.62, 72.62, 72.53, 72.53, 72.7875, 72.7875, 73.0025, 73.0025, 72.905, 72.905, 72.7325, 72.7325, 72.7925, 72.7925, 72.9325, 72.9325, 73.0775, 73.0775, 73.105, 73.105, 72.995, 72.995, 72.9975, 72.9975, 73.6425, 73.6425]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 58988 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2181, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 50566 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 504, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54984 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 50480 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.196, Test loss: 2.419, Test accuracy: 22.67
Round   0, Global train loss: 1.196, Global test loss: 2.692, Global test accuracy: 16.55
Round   1, Train loss: 1.017, Test loss: 1.787, Test accuracy: 33.23
Round   1, Global train loss: 1.017, Global test loss: 2.258, Global test accuracy: 20.95
Round   2, Train loss: 0.946, Test loss: 1.408, Test accuracy: 44.87
Round   2, Global train loss: 0.946, Global test loss: 2.125, Global test accuracy: 19.83
Round   3, Train loss: 0.850, Test loss: 1.010, Test accuracy: 56.23
Round   3, Global train loss: 0.850, Global test loss: 1.907, Global test accuracy: 28.83
Round   4, Train loss: 0.780, Test loss: 0.900, Test accuracy: 60.35
Round   4, Global train loss: 0.780, Global test loss: 2.051, Global test accuracy: 28.55
Round   5, Train loss: 0.764, Test loss: 0.946, Test accuracy: 61.70
Round   5, Global train loss: 0.764, Global test loss: 2.541, Global test accuracy: 24.60
Round   6, Train loss: 0.732, Test loss: 0.872, Test accuracy: 63.05
Round   6, Global train loss: 0.732, Global test loss: 2.104, Global test accuracy: 25.68
Round   7, Train loss: 0.776, Test loss: 0.812, Test accuracy: 65.97
Round   7, Global train loss: 0.776, Global test loss: 1.811, Global test accuracy: 30.60
Round   8, Train loss: 0.687, Test loss: 0.670, Test accuracy: 70.93
Round   8, Global train loss: 0.687, Global test loss: 1.908, Global test accuracy: 31.82
Round   9, Train loss: 0.666, Test loss: 0.650, Test accuracy: 72.10
Round   9, Global train loss: 0.666, Global test loss: 1.779, Global test accuracy: 35.03
Round  10, Train loss: 0.668, Test loss: 0.613, Test accuracy: 74.25
Round  10, Global train loss: 0.668, Global test loss: 2.024, Global test accuracy: 33.45
Round  11, Train loss: 0.688, Test loss: 0.606, Test accuracy: 74.80
Round  11, Global train loss: 0.688, Global test loss: 1.985, Global test accuracy: 37.82
Round  12, Train loss: 0.622, Test loss: 0.598, Test accuracy: 74.75
Round  12, Global train loss: 0.622, Global test loss: 1.713, Global test accuracy: 40.33
Round  13, Train loss: 0.711, Test loss: 0.590, Test accuracy: 74.93
Round  13, Global train loss: 0.711, Global test loss: 1.859, Global test accuracy: 30.48
Round  14, Train loss: 0.619, Test loss: 0.577, Test accuracy: 75.95
Round  14, Global train loss: 0.619, Global test loss: 1.600, Global test accuracy: 42.38
Round  15, Train loss: 0.585, Test loss: 0.576, Test accuracy: 75.83
Round  15, Global train loss: 0.585, Global test loss: 1.663, Global test accuracy: 41.27
Round  16, Train loss: 0.633, Test loss: 0.572, Test accuracy: 76.03
Round  16, Global train loss: 0.633, Global test loss: 2.325, Global test accuracy: 31.63
Round  17, Train loss: 0.554, Test loss: 0.578, Test accuracy: 76.20
Round  17, Global train loss: 0.554, Global test loss: 1.655, Global test accuracy: 41.70
Round  18, Train loss: 0.570, Test loss: 0.583, Test accuracy: 76.27
Round  18, Global train loss: 0.570, Global test loss: 1.534, Global test accuracy: 44.92
Round  19, Train loss: 0.634, Test loss: 0.577, Test accuracy: 76.68
Round  19, Global train loss: 0.634, Global test loss: 1.698, Global test accuracy: 39.58
Round  20, Train loss: 0.588, Test loss: 0.575, Test accuracy: 76.73
Round  20, Global train loss: 0.588, Global test loss: 1.861, Global test accuracy: 38.27
Round  21, Train loss: 0.568, Test loss: 0.578, Test accuracy: 76.40
Round  21, Global train loss: 0.568, Global test loss: 1.548, Global test accuracy: 43.67
Round  22, Train loss: 0.538, Test loss: 0.580, Test accuracy: 76.58
Round  22, Global train loss: 0.538, Global test loss: 1.556, Global test accuracy: 42.17
Round  23, Train loss: 0.529, Test loss: 0.575, Test accuracy: 77.18
Round  23, Global train loss: 0.529, Global test loss: 1.655, Global test accuracy: 40.82
Round  24, Train loss: 0.578, Test loss: 0.585, Test accuracy: 76.72
Round  24, Global train loss: 0.578, Global test loss: 1.617, Global test accuracy: 40.20
Round  25, Train loss: 0.528, Test loss: 0.578, Test accuracy: 76.98
Round  25, Global train loss: 0.528, Global test loss: 1.688, Global test accuracy: 40.43
Round  26, Train loss: 0.570, Test loss: 0.579, Test accuracy: 76.90
Round  26, Global train loss: 0.570, Global test loss: 1.760, Global test accuracy: 40.53
Round  27, Train loss: 0.585, Test loss: 0.577, Test accuracy: 77.52
Round  27, Global train loss: 0.585, Global test loss: 1.758, Global test accuracy: 38.52
Round  28, Train loss: 0.504, Test loss: 0.557, Test accuracy: 78.35
Round  28, Global train loss: 0.504, Global test loss: 1.331, Global test accuracy: 51.12
Round  29, Train loss: 0.474, Test loss: 0.541, Test accuracy: 78.87
Round  29, Global train loss: 0.474, Global test loss: 1.567, Global test accuracy: 45.23
Round  30, Train loss: 0.474, Test loss: 0.531, Test accuracy: 78.75
Round  30, Global train loss: 0.474, Global test loss: 1.559, Global test accuracy: 46.37
Round  31, Train loss: 0.470, Test loss: 0.522, Test accuracy: 79.15
Round  31, Global train loss: 0.470, Global test loss: 1.670, Global test accuracy: 41.37
Round  32, Train loss: 0.432, Test loss: 0.521, Test accuracy: 79.78
Round  32, Global train loss: 0.432, Global test loss: 1.316, Global test accuracy: 51.33
Round  33, Train loss: 0.420, Test loss: 0.533, Test accuracy: 79.58
Round  33, Global train loss: 0.420, Global test loss: 1.501, Global test accuracy: 46.72
Round  34, Train loss: 0.516, Test loss: 0.534, Test accuracy: 79.53
Round  34, Global train loss: 0.516, Global test loss: 1.657, Global test accuracy: 44.18
Round  35, Train loss: 0.387, Test loss: 0.550, Test accuracy: 79.02
Round  35, Global train loss: 0.387, Global test loss: 1.417, Global test accuracy: 49.70
Round  36, Train loss: 0.439, Test loss: 0.546, Test accuracy: 79.02
Round  36, Global train loss: 0.439, Global test loss: 1.540, Global test accuracy: 47.05
Round  37, Train loss: 0.453, Test loss: 0.527, Test accuracy: 79.57
Round  37, Global train loss: 0.453, Global test loss: 1.815, Global test accuracy: 40.10
Round  38, Train loss: 0.402, Test loss: 0.521, Test accuracy: 80.10
Round  38, Global train loss: 0.402, Global test loss: 1.283, Global test accuracy: 54.05
Round  39, Train loss: 0.422, Test loss: 0.519, Test accuracy: 80.15
Round  39, Global train loss: 0.422, Global test loss: 1.808, Global test accuracy: 40.38
Round  40, Train loss: 0.416, Test loss: 0.529, Test accuracy: 79.82
Round  40, Global train loss: 0.416, Global test loss: 1.586, Global test accuracy: 47.27
Round  41, Train loss: 0.475, Test loss: 0.523, Test accuracy: 80.50
Round  41, Global train loss: 0.475, Global test loss: 1.336, Global test accuracy: 51.90
Round  42, Train loss: 0.408, Test loss: 0.521, Test accuracy: 80.38
Round  42, Global train loss: 0.408, Global test loss: 1.405, Global test accuracy: 51.22
Round  43, Train loss: 0.387, Test loss: 0.520, Test accuracy: 80.52
Round  43, Global train loss: 0.387, Global test loss: 1.522, Global test accuracy: 48.27
Round  44, Train loss: 0.384, Test loss: 0.491, Test accuracy: 81.62
Round  44, Global train loss: 0.384, Global test loss: 1.737, Global test accuracy: 41.08
Round  45, Train loss: 0.376, Test loss: 0.476, Test accuracy: 82.03
Round  45, Global train loss: 0.376, Global test loss: 1.322, Global test accuracy: 54.55
Round  46, Train loss: 0.381, Test loss: 0.474, Test accuracy: 82.15
Round  46, Global train loss: 0.381, Global test loss: 1.632, Global test accuracy: 44.42
Round  47, Train loss: 0.344, Test loss: 0.484, Test accuracy: 82.03
Round  47, Global train loss: 0.344, Global test loss: 1.379, Global test accuracy: 51.18
Round  48, Train loss: 0.422, Test loss: 0.518, Test accuracy: 80.47
Round  48, Global train loss: 0.422, Global test loss: 1.430, Global test accuracy: 50.90
Round  49, Train loss: 0.379, Test loss: 0.519, Test accuracy: 80.42
Round  49, Global train loss: 0.379, Global test loss: 1.447, Global test accuracy: 50.90
Round  50, Train loss: 0.422, Test loss: 0.499, Test accuracy: 81.22
Round  50, Global train loss: 0.422, Global test loss: 1.253, Global test accuracy: 55.87
Round  51, Train loss: 0.350, Test loss: 0.503, Test accuracy: 80.95
Round  51, Global train loss: 0.350, Global test loss: 1.319, Global test accuracy: 53.75
Round  52, Train loss: 0.363, Test loss: 0.492, Test accuracy: 81.52
Round  52, Global train loss: 0.363, Global test loss: 1.477, Global test accuracy: 52.02
Round  53, Train loss: 0.332, Test loss: 0.467, Test accuracy: 82.43
Round  53, Global train loss: 0.332, Global test loss: 1.548, Global test accuracy: 49.40
Round  54, Train loss: 0.350, Test loss: 0.469, Test accuracy: 82.37
Round  54, Global train loss: 0.350, Global test loss: 1.689, Global test accuracy: 45.25
Round  55, Train loss: 0.315, Test loss: 0.497, Test accuracy: 81.92
Round  55, Global train loss: 0.315, Global test loss: 1.399, Global test accuracy: 51.48
Round  56, Train loss: 0.319, Test loss: 0.488, Test accuracy: 81.97
Round  56, Global train loss: 0.319, Global test loss: 1.266, Global test accuracy: 55.85
Round  57, Train loss: 0.331, Test loss: 0.469, Test accuracy: 82.35
Round  57, Global train loss: 0.331, Global test loss: 1.513, Global test accuracy: 50.27
Round  58, Train loss: 0.322, Test loss: 0.477, Test accuracy: 82.03
Round  58, Global train loss: 0.322, Global test loss: 2.542, Global test accuracy: 35.55
Round  59, Train loss: 0.376, Test loss: 0.479, Test accuracy: 82.30
Round  59, Global train loss: 0.376, Global test loss: 1.505, Global test accuracy: 49.58
Round  60, Train loss: 0.291, Test loss: 0.483, Test accuracy: 82.32
Round  60, Global train loss: 0.291, Global test loss: 1.422, Global test accuracy: 53.83
Round  61, Train loss: 0.386, Test loss: 0.481, Test accuracy: 82.58
Round  61, Global train loss: 0.386, Global test loss: 1.302, Global test accuracy: 55.22
Round  62, Train loss: 0.305, Test loss: 0.488, Test accuracy: 82.58
Round  62, Global train loss: 0.305, Global test loss: 1.344, Global test accuracy: 54.48
Round  63, Train loss: 0.315, Test loss: 0.473, Test accuracy: 82.78
Round  63, Global train loss: 0.315, Global test loss: 1.371, Global test accuracy: 53.72
Round  64, Train loss: 0.283, Test loss: 0.485, Test accuracy: 82.60
Round  64, Global train loss: 0.283, Global test loss: 1.600, Global test accuracy: 51.67
Round  65, Train loss: 0.277, Test loss: 0.459, Test accuracy: 83.25
Round  65, Global train loss: 0.277, Global test loss: 1.338, Global test accuracy: 54.67
Round  66, Train loss: 0.315, Test loss: 0.435, Test accuracy: 83.98
Round  66, Global train loss: 0.315, Global test loss: 1.487, Global test accuracy: 52.67
Round  67, Train loss: 0.306, Test loss: 0.456, Test accuracy: 83.87
Round  67, Global train loss: 0.306, Global test loss: 1.558, Global test accuracy: 49.47
Round  68, Train loss: 0.294, Test loss: 0.445, Test accuracy: 84.18
Round  68, Global train loss: 0.294, Global test loss: 1.482, Global test accuracy: 51.40
Round  69, Train loss: 0.314, Test loss: 0.459, Test accuracy: 83.95
Round  69, Global train loss: 0.314, Global test loss: 1.278, Global test accuracy: 56.68
Round  70, Train loss: 0.300, Test loss: 0.471, Test accuracy: 83.43
Round  70, Global train loss: 0.300, Global test loss: 1.182, Global test accuracy: 58.18
Round  71, Train loss: 0.257, Test loss: 0.462, Test accuracy: 83.60
Round  71, Global train loss: 0.257, Global test loss: 1.217, Global test accuracy: 58.28
Round  72, Train loss: 0.248, Test loss: 0.476, Test accuracy: 83.20
Round  72, Global train loss: 0.248, Global test loss: 1.246, Global test accuracy: 57.85
Round  73, Train loss: 0.280, Test loss: 0.485, Test accuracy: 83.10
Round  73, Global train loss: 0.280, Global test loss: 1.417, Global test accuracy: 55.32
Round  74, Train loss: 0.261, Test loss: 0.487, Test accuracy: 83.08
Round  74, Global train loss: 0.261, Global test loss: 1.300, Global test accuracy: 56.68
Round  75, Train loss: 0.249, Test loss: 0.488, Test accuracy: 82.82
Round  75, Global train loss: 0.249, Global test loss: 1.456, Global test accuracy: 54.17
Round  76, Train loss: 0.263, Test loss: 0.479, Test accuracy: 83.07
Round  76, Global train loss: 0.263, Global test loss: 1.234, Global test accuracy: 58.95
Round  77, Train loss: 0.286, Test loss: 0.482, Test accuracy: 82.73
Round  77, Global train loss: 0.286, Global test loss: 1.558, Global test accuracy: 50.27
Round  78, Train loss: 0.272, Test loss: 0.477, Test accuracy: 83.02
Round  78, Global train loss: 0.272, Global test loss: 1.346, Global test accuracy: 56.08
Round  79, Train loss: 0.226, Test loss: 0.465, Test accuracy: 83.12
Round  79, Global train loss: 0.226, Global test loss: 1.295, Global test accuracy: 57.27
Round  80, Train loss: 0.239, Test loss: 0.467, Test accuracy: 83.33
Round  80, Global train loss: 0.239, Global test loss: 1.740, Global test accuracy: 48.78
Round  81, Train loss: 0.234, Test loss: 0.486, Test accuracy: 83.28
Round  81, Global train loss: 0.234, Global test loss: 1.254, Global test accuracy: 58.32
Round  82, Train loss: 0.258, Test loss: 0.492, Test accuracy: 83.05
Round  82, Global train loss: 0.258, Global test loss: 1.496, Global test accuracy: 54.20
Round  83, Train loss: 0.258, Test loss: 0.499, Test accuracy: 82.88
Round  83, Global train loss: 0.258, Global test loss: 1.526, Global test accuracy: 53.45
Round  84, Train loss: 0.257, Test loss: 0.492, Test accuracy: 83.28
Round  84, Global train loss: 0.257, Global test loss: 1.386, Global test accuracy: 54.95
Round  85, Train loss: 0.219, Test loss: 0.474, Test accuracy: 83.73
Round  85, Global train loss: 0.219, Global test loss: 1.356, Global test accuracy: 54.98
Round  86, Train loss: 0.288, Test loss: 0.479, Test accuracy: 83.25
Round  86, Global train loss: 0.288, Global test loss: 1.434, Global test accuracy: 53.88
Round  87, Train loss: 0.229, Test loss: 0.497, Test accuracy: 83.10
Round  87, Global train loss: 0.229, Global test loss: 1.954, Global test accuracy: 43.73
Round  88, Train loss: 0.228, Test loss: 0.491, Test accuracy: 83.22
Round  88, Global train loss: 0.228, Global test loss: 1.527, Global test accuracy: 51.78
Round  89, Train loss: 0.214, Test loss: 0.503, Test accuracy: 82.87
Round  89, Global train loss: 0.214, Global test loss: 1.227, Global test accuracy: 59.20
Round  90, Train loss: 0.245, Test loss: 0.481, Test accuracy: 83.37
Round  90, Global train loss: 0.245, Global test loss: 1.354, Global test accuracy: 54.92
Round  91, Train loss: 0.215, Test loss: 0.472, Test accuracy: 83.68
Round  91, Global train loss: 0.215, Global test loss: 1.496, Global test accuracy: 54.80
Round  92, Train loss: 0.221, Test loss: 0.475, Test accuracy: 83.83
Round  92, Global train loss: 0.221, Global test loss: 1.279, Global test accuracy: 57.38
Round  93, Train loss: 0.189, Test loss: 0.493, Test accuracy: 83.82
Round  93, Global train loss: 0.189, Global test loss: 1.574, Global test accuracy: 54.33
Round  94, Train loss: 0.224, Test loss: 0.485, Test accuracy: 84.17
Round  94, Global train loss: 0.224, Global test loss: 1.358, Global test accuracy: 56.77
Round  95, Train loss: 0.223, Test loss: 0.478, Test accuracy: 84.08
Round  95, Global train loss: 0.223, Global test loss: 1.372, Global test accuracy: 56.95
Round  96, Train loss: 0.210, Test loss: 0.469, Test accuracy: 84.48
Round  96, Global train loss: 0.210, Global test loss: 1.499, Global test accuracy: 53.22
Round  97, Train loss: 0.244, Test loss: 0.456, Test accuracy: 85.00
Round  97, Global train loss: 0.244, Global test loss: 1.377, Global test accuracy: 56.12
Round  98, Train loss: 0.218, Test loss: 0.456, Test accuracy: 85.03
Round  98, Global train loss: 0.218, Global test loss: 1.248, Global test accuracy: 59.22
Round  99, Train loss: 0.185, Test loss: 0.473, Test accuracy: 84.85
Round  99, Global train loss: 0.185, Global test loss: 1.276, Global test accuracy: 59.80
Final Round, Train loss: 0.164, Test loss: 0.560, Test accuracy: 83.90
Final Round, Global train loss: 0.164, Global test loss: 1.276, Global test accuracy: 59.80
Average accuracy final 10 rounds: 84.23166666666667 

Average global accuracy final 10 rounds: 56.349999999999994 

1008.0360682010651
[0.9426045417785645, 1.885209083557129, 2.564967393875122, 3.2447257041931152, 3.9286930561065674, 4.6126604080200195, 5.290202617645264, 5.967744827270508, 6.6338889598846436, 7.300033092498779, 7.9745259284973145, 8.64901876449585, 9.337228059768677, 10.025437355041504, 10.716465711593628, 11.407494068145752, 12.065792560577393, 12.724091053009033, 13.379920244216919, 14.035749435424805, 14.71232008934021, 15.388890743255615, 16.150506734848022, 16.91212272644043, 17.689975261688232, 18.467827796936035, 19.230787992477417, 19.9937481880188, 20.75806450843811, 21.522380828857422, 22.28985357284546, 23.057326316833496, 23.836112022399902, 24.61489772796631, 25.36547088623047, 26.11604404449463, 26.89399790763855, 27.67195177078247, 28.437007904052734, 29.202064037322998, 29.977362155914307, 30.752660274505615, 31.534454584121704, 32.31624889373779, 33.09097242355347, 33.86569595336914, 34.65302014350891, 35.44034433364868, 36.22036337852478, 37.00038242340088, 37.784282207489014, 38.56818199157715, 39.32645297050476, 40.08472394943237, 40.851341247558594, 41.617958545684814, 42.387582302093506, 43.1572060585022, 43.941399812698364, 44.72559356689453, 45.49718976020813, 46.26878595352173, 47.03861975669861, 47.80845355987549, 48.57533144950867, 49.342209339141846, 50.03881359100342, 50.73541784286499, 51.40101861953735, 52.06661939620972, 52.72909140586853, 53.391563415527344, 54.07031869888306, 54.74907398223877, 55.43823432922363, 56.127394676208496, 56.827723026275635, 57.52805137634277, 58.211053133010864, 58.894054889678955, 59.57588720321655, 60.25771951675415, 60.944732904434204, 61.63174629211426, 62.2963593006134, 62.96097230911255, 63.65047574043274, 64.33997917175293, 65.00453448295593, 65.66908979415894, 66.32874703407288, 66.98840427398682, 67.66097021102905, 68.33353614807129, 69.00940561294556, 69.68527507781982, 70.35921883583069, 71.03316259384155, 71.70122146606445, 72.36928033828735, 73.03005790710449, 73.69083547592163, 74.38952779769897, 75.08822011947632, 75.75703740119934, 76.42585468292236, 77.10027694702148, 77.7746992111206, 78.44163513183594, 79.10857105255127, 79.80136108398438, 80.49415111541748, 81.18588829040527, 81.87762546539307, 82.54308724403381, 83.20854902267456, 83.87786316871643, 84.5471773147583, 85.21871972084045, 85.89026212692261, 86.5585572719574, 87.22685241699219, 87.89992356300354, 88.57299470901489, 89.24301147460938, 89.91302824020386, 90.57642674446106, 91.23982524871826, 91.91248655319214, 92.58514785766602, 93.24914622306824, 93.91314458847046, 94.58264589309692, 95.25214719772339, 95.91949558258057, 96.58684396743774, 97.26291537284851, 97.93898677825928, 98.61576008796692, 99.29253339767456, 99.97430276870728, 100.65607213973999, 101.32397079467773, 101.99186944961548, 102.66157984733582, 103.33129024505615, 104.0025086402893, 104.67372703552246, 105.34566950798035, 106.01761198043823, 106.69884753227234, 107.38008308410645, 108.05572295188904, 108.73136281967163, 109.40199708938599, 110.07263135910034, 110.75146007537842, 111.4302887916565, 112.10822033882141, 112.78615188598633, 113.45653820037842, 114.12692451477051, 114.81714034080505, 115.5073561668396, 116.17524886131287, 116.84314155578613, 117.51479530334473, 118.18644905090332, 118.86099576950073, 119.53554248809814, 120.20946002006531, 120.88337755203247, 121.54904294013977, 122.21470832824707, 122.89538645744324, 123.5760645866394, 124.24116539955139, 124.90626621246338, 125.56242394447327, 126.21858167648315, 126.89093255996704, 127.56328344345093, 128.2342767715454, 128.9052700996399, 129.58695125579834, 130.2686324119568, 130.94011688232422, 131.61160135269165, 132.28463172912598, 132.9576621055603, 133.62838435173035, 134.2991065979004, 134.97713708877563, 135.65516757965088, 136.3164246082306, 136.9776816368103, 137.6436324119568, 138.30958318710327, 138.98413491249084, 139.65868663787842, 141.01770186424255, 142.3767170906067]
[22.666666666666668, 22.666666666666668, 33.233333333333334, 33.233333333333334, 44.86666666666667, 44.86666666666667, 56.233333333333334, 56.233333333333334, 60.35, 60.35, 61.7, 61.7, 63.05, 63.05, 65.96666666666667, 65.96666666666667, 70.93333333333334, 70.93333333333334, 72.1, 72.1, 74.25, 74.25, 74.8, 74.8, 74.75, 74.75, 74.93333333333334, 74.93333333333334, 75.95, 75.95, 75.83333333333333, 75.83333333333333, 76.03333333333333, 76.03333333333333, 76.2, 76.2, 76.26666666666667, 76.26666666666667, 76.68333333333334, 76.68333333333334, 76.73333333333333, 76.73333333333333, 76.4, 76.4, 76.58333333333333, 76.58333333333333, 77.18333333333334, 77.18333333333334, 76.71666666666667, 76.71666666666667, 76.98333333333333, 76.98333333333333, 76.9, 76.9, 77.51666666666667, 77.51666666666667, 78.35, 78.35, 78.86666666666666, 78.86666666666666, 78.75, 78.75, 79.15, 79.15, 79.78333333333333, 79.78333333333333, 79.58333333333333, 79.58333333333333, 79.53333333333333, 79.53333333333333, 79.01666666666667, 79.01666666666667, 79.01666666666667, 79.01666666666667, 79.56666666666666, 79.56666666666666, 80.1, 80.1, 80.15, 80.15, 79.81666666666666, 79.81666666666666, 80.5, 80.5, 80.38333333333334, 80.38333333333334, 80.51666666666667, 80.51666666666667, 81.61666666666666, 81.61666666666666, 82.03333333333333, 82.03333333333333, 82.15, 82.15, 82.03333333333333, 82.03333333333333, 80.46666666666667, 80.46666666666667, 80.41666666666667, 80.41666666666667, 81.21666666666667, 81.21666666666667, 80.95, 80.95, 81.51666666666667, 81.51666666666667, 82.43333333333334, 82.43333333333334, 82.36666666666666, 82.36666666666666, 81.91666666666667, 81.91666666666667, 81.96666666666667, 81.96666666666667, 82.35, 82.35, 82.03333333333333, 82.03333333333333, 82.3, 82.3, 82.31666666666666, 82.31666666666666, 82.58333333333333, 82.58333333333333, 82.58333333333333, 82.58333333333333, 82.78333333333333, 82.78333333333333, 82.6, 82.6, 83.25, 83.25, 83.98333333333333, 83.98333333333333, 83.86666666666666, 83.86666666666666, 84.18333333333334, 84.18333333333334, 83.95, 83.95, 83.43333333333334, 83.43333333333334, 83.6, 83.6, 83.2, 83.2, 83.1, 83.1, 83.08333333333333, 83.08333333333333, 82.81666666666666, 82.81666666666666, 83.06666666666666, 83.06666666666666, 82.73333333333333, 82.73333333333333, 83.01666666666667, 83.01666666666667, 83.11666666666666, 83.11666666666666, 83.33333333333333, 83.33333333333333, 83.28333333333333, 83.28333333333333, 83.05, 83.05, 82.88333333333334, 82.88333333333334, 83.28333333333333, 83.28333333333333, 83.73333333333333, 83.73333333333333, 83.25, 83.25, 83.1, 83.1, 83.21666666666667, 83.21666666666667, 82.86666666666666, 82.86666666666666, 83.36666666666666, 83.36666666666666, 83.68333333333334, 83.68333333333334, 83.83333333333333, 83.83333333333333, 83.81666666666666, 83.81666666666666, 84.16666666666667, 84.16666666666667, 84.08333333333333, 84.08333333333333, 84.48333333333333, 84.48333333333333, 85.0, 85.0, 85.03333333333333, 85.03333333333333, 84.85, 84.85, 83.9, 83.9]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.606, Test loss: 2.215, Test accuracy: 22.38
Round   1, Train loss: 0.989, Test loss: 2.318, Test accuracy: 31.32
Round   2, Train loss: 0.963, Test loss: 1.581, Test accuracy: 37.98
Round   3, Train loss: 0.933, Test loss: 1.177, Test accuracy: 50.47
Round   4, Train loss: 0.793, Test loss: 1.302, Test accuracy: 51.30
Round   5, Train loss: 0.787, Test loss: 1.152, Test accuracy: 55.52
Round   6, Train loss: 0.745, Test loss: 0.857, Test accuracy: 61.98
Round   7, Train loss: 0.739, Test loss: 0.936, Test accuracy: 64.38
Round   8, Train loss: 0.801, Test loss: 0.706, Test accuracy: 67.88
Round   9, Train loss: 0.759, Test loss: 0.673, Test accuracy: 69.18
Round  10, Train loss: 0.722, Test loss: 0.651, Test accuracy: 70.38
Round  11, Train loss: 0.656, Test loss: 0.666, Test accuracy: 70.30
Round  12, Train loss: 0.665, Test loss: 0.614, Test accuracy: 72.33
Round  13, Train loss: 0.619, Test loss: 0.603, Test accuracy: 73.77
Round  14, Train loss: 0.676, Test loss: 0.587, Test accuracy: 74.40
Round  15, Train loss: 0.629, Test loss: 0.576, Test accuracy: 75.27
Round  16, Train loss: 0.593, Test loss: 0.585, Test accuracy: 75.17
Round  17, Train loss: 0.603, Test loss: 0.570, Test accuracy: 75.70
Round  18, Train loss: 0.634, Test loss: 0.573, Test accuracy: 75.15
Round  19, Train loss: 0.660, Test loss: 0.580, Test accuracy: 75.17
Round  20, Train loss: 0.645, Test loss: 0.563, Test accuracy: 75.82
Round  21, Train loss: 0.570, Test loss: 0.554, Test accuracy: 76.15
Round  22, Train loss: 0.546, Test loss: 0.557, Test accuracy: 75.80
Round  23, Train loss: 0.596, Test loss: 0.556, Test accuracy: 76.07
Round  24, Train loss: 0.523, Test loss: 0.538, Test accuracy: 76.40
Round  25, Train loss: 0.524, Test loss: 0.524, Test accuracy: 77.42
Round  26, Train loss: 0.555, Test loss: 0.525, Test accuracy: 77.02
Round  27, Train loss: 0.553, Test loss: 0.515, Test accuracy: 77.73
Round  28, Train loss: 0.523, Test loss: 0.509, Test accuracy: 78.42
Round  29, Train loss: 0.557, Test loss: 0.508, Test accuracy: 78.67
Round  30, Train loss: 0.501, Test loss: 0.512, Test accuracy: 78.03
Round  31, Train loss: 0.509, Test loss: 0.506, Test accuracy: 78.50
Round  32, Train loss: 0.563, Test loss: 0.497, Test accuracy: 79.10
Round  33, Train loss: 0.526, Test loss: 0.494, Test accuracy: 79.42
Round  34, Train loss: 0.489, Test loss: 0.479, Test accuracy: 79.87
Round  35, Train loss: 0.493, Test loss: 0.474, Test accuracy: 80.25
Round  36, Train loss: 0.521, Test loss: 0.473, Test accuracy: 80.55
Round  37, Train loss: 0.492, Test loss: 0.472, Test accuracy: 80.23
Round  38, Train loss: 0.529, Test loss: 0.472, Test accuracy: 80.38
Round  39, Train loss: 0.520, Test loss: 0.465, Test accuracy: 80.52
Round  40, Train loss: 0.479, Test loss: 0.468, Test accuracy: 80.80
Round  41, Train loss: 0.494, Test loss: 0.457, Test accuracy: 81.05
Round  42, Train loss: 0.508, Test loss: 0.466, Test accuracy: 80.90
Round  43, Train loss: 0.477, Test loss: 0.464, Test accuracy: 80.78
Round  44, Train loss: 0.393, Test loss: 0.468, Test accuracy: 80.02
Round  45, Train loss: 0.447, Test loss: 0.458, Test accuracy: 80.65
Round  46, Train loss: 0.453, Test loss: 0.448, Test accuracy: 81.32
Round  47, Train loss: 0.421, Test loss: 0.442, Test accuracy: 81.60
Round  48, Train loss: 0.414, Test loss: 0.437, Test accuracy: 81.95
Round  49, Train loss: 0.409, Test loss: 0.440, Test accuracy: 81.93
Round  50, Train loss: 0.363, Test loss: 0.425, Test accuracy: 82.37
Round  51, Train loss: 0.451, Test loss: 0.415, Test accuracy: 83.13
Round  52, Train loss: 0.378, Test loss: 0.425, Test accuracy: 82.98
Round  53, Train loss: 0.376, Test loss: 0.421, Test accuracy: 82.50
Round  54, Train loss: 0.376, Test loss: 0.422, Test accuracy: 82.18
Round  55, Train loss: 0.435, Test loss: 0.426, Test accuracy: 82.35
Round  56, Train loss: 0.369, Test loss: 0.422, Test accuracy: 82.75
Round  57, Train loss: 0.364, Test loss: 0.438, Test accuracy: 82.25
Round  58, Train loss: 0.411, Test loss: 0.421, Test accuracy: 82.63
Round  59, Train loss: 0.339, Test loss: 0.412, Test accuracy: 82.85
Round  60, Train loss: 0.352, Test loss: 0.407, Test accuracy: 83.20
Round  61, Train loss: 0.332, Test loss: 0.402, Test accuracy: 83.72
Round  62, Train loss: 0.364, Test loss: 0.408, Test accuracy: 83.60
Round  63, Train loss: 0.399, Test loss: 0.405, Test accuracy: 83.40
Round  64, Train loss: 0.300, Test loss: 0.408, Test accuracy: 83.63
Round  65, Train loss: 0.343, Test loss: 0.403, Test accuracy: 83.68
Round  66, Train loss: 0.321, Test loss: 0.405, Test accuracy: 83.38
Round  67, Train loss: 0.370, Test loss: 0.396, Test accuracy: 84.10
Round  68, Train loss: 0.309, Test loss: 0.395, Test accuracy: 84.05
Round  69, Train loss: 0.380, Test loss: 0.396, Test accuracy: 83.63
Round  70, Train loss: 0.317, Test loss: 0.394, Test accuracy: 83.83
Round  71, Train loss: 0.362, Test loss: 0.388, Test accuracy: 84.32
Round  72, Train loss: 0.314, Test loss: 0.381, Test accuracy: 84.82
Round  73, Train loss: 0.325, Test loss: 0.378, Test accuracy: 84.37
Round  74, Train loss: 0.344, Test loss: 0.376, Test accuracy: 85.00
Round  75, Train loss: 0.274, Test loss: 0.381, Test accuracy: 84.78
Round  76, Train loss: 0.349, Test loss: 0.382, Test accuracy: 84.63
Round  77, Train loss: 0.351, Test loss: 0.379, Test accuracy: 84.78
Round  78, Train loss: 0.314, Test loss: 0.380, Test accuracy: 84.87
Round  79, Train loss: 0.291, Test loss: 0.384, Test accuracy: 84.52
Round  80, Train loss: 0.304, Test loss: 0.380, Test accuracy: 84.68
Round  81, Train loss: 0.258, Test loss: 0.376, Test accuracy: 85.17
Round  82, Train loss: 0.333, Test loss: 0.375, Test accuracy: 85.02
Round  83, Train loss: 0.313, Test loss: 0.383, Test accuracy: 84.52
Round  84, Train loss: 0.312, Test loss: 0.380, Test accuracy: 85.02
Round  85, Train loss: 0.260, Test loss: 0.389, Test accuracy: 84.62
Round  86, Train loss: 0.299, Test loss: 0.382, Test accuracy: 85.13
Round  87, Train loss: 0.371, Test loss: 0.376, Test accuracy: 85.23
Round  88, Train loss: 0.283, Test loss: 0.378, Test accuracy: 84.72
Round  89, Train loss: 0.286, Test loss: 0.386, Test accuracy: 84.52
Round  90, Train loss: 0.274, Test loss: 0.382, Test accuracy: 84.53
Round  91, Train loss: 0.244, Test loss: 0.374, Test accuracy: 84.88
Round  92, Train loss: 0.264, Test loss: 0.381, Test accuracy: 84.57
Round  93, Train loss: 0.255, Test loss: 0.384, Test accuracy: 84.98
Round  94, Train loss: 0.267, Test loss: 0.369, Test accuracy: 84.87
Round  95, Train loss: 0.296, Test loss: 0.366, Test accuracy: 85.18
Round  96, Train loss: 0.237, Test loss: 0.373, Test accuracy: 84.97
Round  97, Train loss: 0.216, Test loss: 0.376, Test accuracy: 84.60
Round  98, Train loss: 0.262, Test loss: 0.378, Test accuracy: 84.83
Round  99, Train loss: 0.280, Test loss: 0.368, Test accuracy: 85.47
Final Round, Train loss: 0.235, Test loss: 0.370, Test accuracy: 85.10
Average accuracy final 10 rounds: 84.88833333333334 

818.2729363441467
[1.012037992477417, 2.024075984954834, 2.7389681339263916, 3.453860282897949, 4.162135362625122, 4.870410442352295, 5.577085018157959, 6.283759593963623, 7.0083088874816895, 7.732858180999756, 8.453869342803955, 9.174880504608154, 9.899601936340332, 10.62432336807251, 11.3443021774292, 12.064280986785889, 12.725150346755981, 13.386019706726074, 14.04559063911438, 14.705161571502686, 15.357226133346558, 16.00929069519043, 16.669347524642944, 17.32940435409546, 17.977380752563477, 18.625357151031494, 19.267056703567505, 19.908756256103516, 20.551929235458374, 21.195102214813232, 21.84332251548767, 22.49154281616211, 23.151657104492188, 23.811771392822266, 24.469246864318848, 25.12672233581543, 25.777146339416504, 26.427570343017578, 27.083372354507446, 27.739174365997314, 28.393930673599243, 29.048686981201172, 29.70722460746765, 30.36576223373413, 31.019773244857788, 31.673784255981445, 32.333688735961914, 32.99359321594238, 33.645604610443115, 34.29761600494385, 34.948527812957764, 35.59943962097168, 36.25356149673462, 36.90768337249756, 37.56179094314575, 38.215898513793945, 38.882527589797974, 39.549156665802, 40.198607206344604, 40.84805774688721, 41.50791954994202, 42.167781352996826, 42.82390332221985, 43.48002529144287, 44.13821077346802, 44.796396255493164, 45.45493936538696, 46.11348247528076, 46.77011013031006, 47.426737785339355, 48.08019781112671, 48.73365783691406, 49.387022256851196, 50.04038667678833, 50.70843458175659, 51.37648248672485, 52.03385591506958, 52.69122934341431, 53.35770845413208, 54.02418756484985, 54.68488931655884, 55.34559106826782, 56.00714159011841, 56.668692111968994, 57.32606101036072, 57.98342990875244, 58.635854721069336, 59.28827953338623, 59.93269991874695, 60.577120304107666, 61.247272968292236, 61.91742563247681, 62.56584167480469, 63.21425771713257, 63.87891244888306, 64.54356718063354, 65.2079164981842, 65.87226581573486, 66.53517293930054, 67.19808006286621, 67.86403250694275, 68.52998495101929, 69.1876699924469, 69.84535503387451, 70.51361513137817, 71.18187522888184, 71.84009027481079, 72.49830532073975, 73.16176319122314, 73.82522106170654, 74.48591899871826, 75.14661693572998, 75.80813527107239, 76.4696536064148, 77.1257746219635, 77.7818956375122, 78.44981718063354, 79.11773872375488, 79.78669714927673, 80.45565557479858, 81.13964319229126, 81.82363080978394, 82.4937493801117, 83.16386795043945, 83.81633830070496, 84.46880865097046, 85.1338152885437, 85.79882192611694, 86.45414018630981, 87.10945844650269, 87.76474952697754, 88.42004060745239, 89.07709646224976, 89.73415231704712, 90.40217232704163, 91.07019233703613, 91.7295343875885, 92.38887643814087, 93.0519106388092, 93.71494483947754, 94.37742209434509, 95.03989934921265, 95.70465016365051, 96.36940097808838, 97.03631210327148, 97.70322322845459, 98.35396003723145, 99.0046968460083, 99.66076231002808, 100.31682777404785, 100.96768021583557, 101.61853265762329, 102.28232598304749, 102.94611930847168, 103.60093593597412, 104.25575256347656, 104.91491293907166, 105.57407331466675, 106.22632598876953, 106.87857866287231, 107.54215407371521, 108.2057294845581, 108.85777616500854, 109.50982284545898, 110.16619658470154, 110.82257032394409, 111.48125457763672, 112.13993883132935, 112.79356074333191, 113.44718265533447, 114.10819387435913, 114.76920509338379, 115.4182801246643, 116.06735515594482, 116.72054624557495, 117.37373733520508, 118.01881742477417, 118.66389751434326, 119.3212251663208, 119.97855281829834, 120.62850904464722, 121.2784652709961, 121.93363094329834, 122.58879661560059, 123.24687027931213, 123.90494394302368, 124.56682753562927, 125.22871112823486, 125.87864851951599, 126.52858591079712, 127.20156288146973, 127.87453985214233, 128.54522895812988, 129.21591806411743, 129.87025451660156, 130.5245909690857, 131.18145966529846, 131.83832836151123, 132.48646783828735, 133.13460731506348, 134.38031768798828, 135.6260280609131]
[22.383333333333333, 22.383333333333333, 31.316666666666666, 31.316666666666666, 37.983333333333334, 37.983333333333334, 50.46666666666667, 50.46666666666667, 51.3, 51.3, 55.516666666666666, 55.516666666666666, 61.983333333333334, 61.983333333333334, 64.38333333333334, 64.38333333333334, 67.88333333333334, 67.88333333333334, 69.18333333333334, 69.18333333333334, 70.38333333333334, 70.38333333333334, 70.3, 70.3, 72.33333333333333, 72.33333333333333, 73.76666666666667, 73.76666666666667, 74.4, 74.4, 75.26666666666667, 75.26666666666667, 75.16666666666667, 75.16666666666667, 75.7, 75.7, 75.15, 75.15, 75.16666666666667, 75.16666666666667, 75.81666666666666, 75.81666666666666, 76.15, 76.15, 75.8, 75.8, 76.06666666666666, 76.06666666666666, 76.4, 76.4, 77.41666666666667, 77.41666666666667, 77.01666666666667, 77.01666666666667, 77.73333333333333, 77.73333333333333, 78.41666666666667, 78.41666666666667, 78.66666666666667, 78.66666666666667, 78.03333333333333, 78.03333333333333, 78.5, 78.5, 79.1, 79.1, 79.41666666666667, 79.41666666666667, 79.86666666666666, 79.86666666666666, 80.25, 80.25, 80.55, 80.55, 80.23333333333333, 80.23333333333333, 80.38333333333334, 80.38333333333334, 80.51666666666667, 80.51666666666667, 80.8, 80.8, 81.05, 81.05, 80.9, 80.9, 80.78333333333333, 80.78333333333333, 80.01666666666667, 80.01666666666667, 80.65, 80.65, 81.31666666666666, 81.31666666666666, 81.6, 81.6, 81.95, 81.95, 81.93333333333334, 81.93333333333334, 82.36666666666666, 82.36666666666666, 83.13333333333334, 83.13333333333334, 82.98333333333333, 82.98333333333333, 82.5, 82.5, 82.18333333333334, 82.18333333333334, 82.35, 82.35, 82.75, 82.75, 82.25, 82.25, 82.63333333333334, 82.63333333333334, 82.85, 82.85, 83.2, 83.2, 83.71666666666667, 83.71666666666667, 83.6, 83.6, 83.4, 83.4, 83.63333333333334, 83.63333333333334, 83.68333333333334, 83.68333333333334, 83.38333333333334, 83.38333333333334, 84.1, 84.1, 84.05, 84.05, 83.63333333333334, 83.63333333333334, 83.83333333333333, 83.83333333333333, 84.31666666666666, 84.31666666666666, 84.81666666666666, 84.81666666666666, 84.36666666666666, 84.36666666666666, 85.0, 85.0, 84.78333333333333, 84.78333333333333, 84.63333333333334, 84.63333333333334, 84.78333333333333, 84.78333333333333, 84.86666666666666, 84.86666666666666, 84.51666666666667, 84.51666666666667, 84.68333333333334, 84.68333333333334, 85.16666666666667, 85.16666666666667, 85.01666666666667, 85.01666666666667, 84.51666666666667, 84.51666666666667, 85.01666666666667, 85.01666666666667, 84.61666666666666, 84.61666666666666, 85.13333333333334, 85.13333333333334, 85.23333333333333, 85.23333333333333, 84.71666666666667, 84.71666666666667, 84.51666666666667, 84.51666666666667, 84.53333333333333, 84.53333333333333, 84.88333333333334, 84.88333333333334, 84.56666666666666, 84.56666666666666, 84.98333333333333, 84.98333333333333, 84.86666666666666, 84.86666666666666, 85.18333333333334, 85.18333333333334, 84.96666666666667, 84.96666666666667, 84.6, 84.6, 84.83333333333333, 84.83333333333333, 85.46666666666667, 85.46666666666667, 85.1, 85.1]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.582, Test loss: 2.044, Test accuracy: 29.53
Round   1, Train loss: 1.038, Test loss: 1.678, Test accuracy: 36.57
Round   2, Train loss: 0.945, Test loss: 1.407, Test accuracy: 44.14
Round   3, Train loss: 0.938, Test loss: 1.167, Test accuracy: 53.30
Round   4, Train loss: 0.809, Test loss: 1.096, Test accuracy: 55.57
Round   5, Train loss: 0.828, Test loss: 0.958, Test accuracy: 60.88
Round   6, Train loss: 0.773, Test loss: 0.977, Test accuracy: 62.17
Round   7, Train loss: 0.820, Test loss: 0.737, Test accuracy: 69.24
Round   8, Train loss: 0.701, Test loss: 0.724, Test accuracy: 71.27
Round   9, Train loss: 0.735, Test loss: 0.685, Test accuracy: 72.67
Round  10, Train loss: 0.640, Test loss: 0.668, Test accuracy: 73.26
Round  11, Train loss: 0.631, Test loss: 0.662, Test accuracy: 73.40
Round  12, Train loss: 0.574, Test loss: 0.659, Test accuracy: 73.41
Round  13, Train loss: 0.627, Test loss: 0.639, Test accuracy: 73.88
Round  14, Train loss: 0.656, Test loss: 0.630, Test accuracy: 74.59
Round  15, Train loss: 0.600, Test loss: 0.618, Test accuracy: 74.84
Round  16, Train loss: 0.657, Test loss: 0.616, Test accuracy: 75.00
Round  17, Train loss: 0.579, Test loss: 0.608, Test accuracy: 74.98
Round  18, Train loss: 0.607, Test loss: 0.614, Test accuracy: 74.80
Round  19, Train loss: 0.687, Test loss: 0.604, Test accuracy: 75.16
Round  20, Train loss: 0.624, Test loss: 0.607, Test accuracy: 75.57
Round  21, Train loss: 0.687, Test loss: 0.577, Test accuracy: 76.44
Round  22, Train loss: 0.589, Test loss: 0.578, Test accuracy: 76.92
Round  23, Train loss: 0.524, Test loss: 0.562, Test accuracy: 77.36
Round  24, Train loss: 0.551, Test loss: 0.555, Test accuracy: 77.83
Round  25, Train loss: 0.571, Test loss: 0.552, Test accuracy: 77.63
Round  26, Train loss: 0.600, Test loss: 0.543, Test accuracy: 78.33
Round  27, Train loss: 0.517, Test loss: 0.537, Test accuracy: 78.00
Round  28, Train loss: 0.515, Test loss: 0.529, Test accuracy: 78.48
Round  29, Train loss: 0.492, Test loss: 0.521, Test accuracy: 79.04
Round  30, Train loss: 0.538, Test loss: 0.524, Test accuracy: 79.07
Round  31, Train loss: 0.533, Test loss: 0.509, Test accuracy: 79.48
Round  32, Train loss: 0.499, Test loss: 0.503, Test accuracy: 79.63
Round  33, Train loss: 0.451, Test loss: 0.504, Test accuracy: 79.74
Round  34, Train loss: 0.486, Test loss: 0.503, Test accuracy: 79.57
Round  35, Train loss: 0.467, Test loss: 0.490, Test accuracy: 80.39
Round  36, Train loss: 0.463, Test loss: 0.488, Test accuracy: 80.52
Round  37, Train loss: 0.457, Test loss: 0.489, Test accuracy: 80.22
Round  38, Train loss: 0.424, Test loss: 0.480, Test accuracy: 80.51
Round  39, Train loss: 0.424, Test loss: 0.488, Test accuracy: 80.10
Round  40, Train loss: 0.526, Test loss: 0.481, Test accuracy: 80.84
Round  41, Train loss: 0.429, Test loss: 0.481, Test accuracy: 80.59
Round  42, Train loss: 0.490, Test loss: 0.474, Test accuracy: 81.26
Round  43, Train loss: 0.440, Test loss: 0.466, Test accuracy: 81.09
Round  44, Train loss: 0.436, Test loss: 0.475, Test accuracy: 80.74
Round  45, Train loss: 0.505, Test loss: 0.467, Test accuracy: 81.29
Round  46, Train loss: 0.459, Test loss: 0.461, Test accuracy: 81.59
Round  47, Train loss: 0.476, Test loss: 0.464, Test accuracy: 81.81
Round  48, Train loss: 0.399, Test loss: 0.457, Test accuracy: 81.67
Round  49, Train loss: 0.377, Test loss: 0.463, Test accuracy: 81.18
Round  50, Train loss: 0.512, Test loss: 0.468, Test accuracy: 81.32
Round  51, Train loss: 0.340, Test loss: 0.458, Test accuracy: 81.60
Round  52, Train loss: 0.418, Test loss: 0.453, Test accuracy: 81.98
Round  53, Train loss: 0.397, Test loss: 0.450, Test accuracy: 82.16
Round  54, Train loss: 0.482, Test loss: 0.452, Test accuracy: 82.10
Round  55, Train loss: 0.458, Test loss: 0.441, Test accuracy: 82.34
Round  56, Train loss: 0.411, Test loss: 0.446, Test accuracy: 82.17
Round  57, Train loss: 0.393, Test loss: 0.441, Test accuracy: 82.60
Round  58, Train loss: 0.381, Test loss: 0.433, Test accuracy: 82.60
Round  59, Train loss: 0.379, Test loss: 0.438, Test accuracy: 82.33
Round  60, Train loss: 0.455, Test loss: 0.437, Test accuracy: 82.59
Round  61, Train loss: 0.348, Test loss: 0.430, Test accuracy: 82.82
Round  62, Train loss: 0.384, Test loss: 0.446, Test accuracy: 82.42
Round  63, Train loss: 0.371, Test loss: 0.436, Test accuracy: 82.53
Round  64, Train loss: 0.419, Test loss: 0.431, Test accuracy: 82.81
Round  65, Train loss: 0.395, Test loss: 0.424, Test accuracy: 83.00
Round  66, Train loss: 0.351, Test loss: 0.429, Test accuracy: 82.90
Round  67, Train loss: 0.403, Test loss: 0.427, Test accuracy: 83.17
Round  68, Train loss: 0.419, Test loss: 0.423, Test accuracy: 83.21
Round  69, Train loss: 0.391, Test loss: 0.425, Test accuracy: 83.32
Round  70, Train loss: 0.367, Test loss: 0.429, Test accuracy: 83.04
Round  71, Train loss: 0.365, Test loss: 0.426, Test accuracy: 83.13
Round  72, Train loss: 0.337, Test loss: 0.423, Test accuracy: 83.26
Round  73, Train loss: 0.377, Test loss: 0.421, Test accuracy: 83.20
Round  74, Train loss: 0.378, Test loss: 0.419, Test accuracy: 83.22
Round  75, Train loss: 0.358, Test loss: 0.417, Test accuracy: 83.60
Round  76, Train loss: 0.340, Test loss: 0.418, Test accuracy: 83.45
Round  77, Train loss: 0.370, Test loss: 0.423, Test accuracy: 83.24
Round  78, Train loss: 0.343, Test loss: 0.416, Test accuracy: 83.45
Round  79, Train loss: 0.252, Test loss: 0.416, Test accuracy: 83.62
Round  80, Train loss: 0.320, Test loss: 0.416, Test accuracy: 83.46
Round  81, Train loss: 0.391, Test loss: 0.422, Test accuracy: 83.04
Round  82, Train loss: 0.347, Test loss: 0.422, Test accuracy: 82.88
Round  83, Train loss: 0.279, Test loss: 0.422, Test accuracy: 83.31
Round  84, Train loss: 0.281, Test loss: 0.421, Test accuracy: 83.37
Round  85, Train loss: 0.357, Test loss: 0.419, Test accuracy: 83.41
Round  86, Train loss: 0.258, Test loss: 0.414, Test accuracy: 83.65
Round  87, Train loss: 0.327, Test loss: 0.411, Test accuracy: 83.94
Round  88, Train loss: 0.309, Test loss: 0.413, Test accuracy: 83.72
Round  89, Train loss: 0.301, Test loss: 0.413, Test accuracy: 83.73
Round  90, Train loss: 0.315, Test loss: 0.414, Test accuracy: 83.72
Round  91, Train loss: 0.297, Test loss: 0.406, Test accuracy: 83.67
Round  92, Train loss: 0.309, Test loss: 0.410, Test accuracy: 83.76
Round  93, Train loss: 0.331, Test loss: 0.408, Test accuracy: 83.80
Round  94, Train loss: 0.388, Test loss: 0.407, Test accuracy: 83.74
Round  95, Train loss: 0.380, Test loss: 0.407, Test accuracy: 83.99
Round  96, Train loss: 0.311, Test loss: 0.415, Test accuracy: 83.69
Round  97, Train loss: 0.249, Test loss: 0.412, Test accuracy: 83.88
Round  98, Train loss: 0.243, Test loss: 0.421, Test accuracy: 83.51
Round  99, Train loss: 0.297, Test loss: 0.418, Test accuracy: 83.35
Final Round, Train loss: 0.249, Test loss: 0.416, Test accuracy: 83.63
Average accuracy final 10 rounds: 83.71249999999999
1878.6700778007507
[2.260038137435913, 4.520076274871826, 6.4821507930755615, 8.444225311279297, 10.34883427619934, 12.253443241119385, 14.112427949905396, 15.971412658691406, 17.834575176239014, 19.69773769378662, 21.554548501968384, 23.411359310150146, 25.299484968185425, 27.187610626220703, 29.09695267677307, 31.00629472732544, 32.90072441101074, 34.795154094696045, 36.70474076271057, 38.6143274307251, 40.37792205810547, 42.14151668548584, 43.877102851867676, 45.61268901824951, 47.35991382598877, 49.10713863372803, 50.81909728050232, 52.53105592727661, 54.267433881759644, 56.003811836242676, 57.738999128341675, 59.474186420440674, 61.23701858520508, 62.99985074996948, 64.83196973800659, 66.6640887260437, 68.40171265602112, 70.13933658599854, 71.86822199821472, 73.59710741043091, 75.39300060272217, 77.18889379501343, 78.96198415756226, 80.73507452011108, 82.46894812583923, 84.20282173156738, 85.96134328842163, 87.71986484527588, 89.45853018760681, 91.19719552993774, 92.91326403617859, 94.62933254241943, 96.3326027393341, 98.03587293624878, 99.76522874832153, 101.49458456039429, 103.27927875518799, 105.06397294998169, 106.76598286628723, 108.46799278259277, 110.19163250923157, 111.91527223587036, 113.63359332084656, 115.35191440582275, 117.08253788948059, 118.81316137313843, 120.54605197906494, 122.27894258499146, 124.0335464477539, 125.78815031051636, 127.51717066764832, 129.24619102478027, 131.02444767951965, 132.80270433425903, 134.54867386817932, 136.2946434020996, 138.0085949897766, 139.7225465774536, 141.45821690559387, 143.19388723373413, 144.94557166099548, 146.69725608825684, 148.45575499534607, 150.2142539024353, 151.94875860214233, 153.68326330184937, 155.42199444770813, 157.1607255935669, 158.88500714302063, 160.60928869247437, 162.31775617599487, 164.02622365951538, 165.77732706069946, 167.52843046188354, 169.28466248512268, 171.04089450836182, 172.79351115226746, 174.5461277961731, 176.2684404850006, 177.99075317382812, 179.7151985168457, 181.43964385986328, 183.17181038856506, 184.90397691726685, 186.63298439979553, 188.36199188232422, 190.1000518798828, 191.8381118774414, 193.55862426757812, 195.27913665771484, 197.0432379245758, 198.80733919143677, 200.52809023857117, 202.24884128570557, 203.96636199951172, 205.68388271331787, 207.47048711776733, 209.2570915222168, 210.99698066711426, 212.73686981201172, 214.4884421825409, 216.24001455307007, 217.9470591545105, 219.65410375595093, 221.40340423583984, 223.15270471572876, 224.92933297157288, 226.705961227417, 228.4268777370453, 230.14779424667358, 231.8536958694458, 233.55959749221802, 235.39089179039001, 237.222186088562, 238.9782886505127, 240.73439121246338, 242.46825766563416, 244.20212411880493, 245.96216416358948, 247.72220420837402, 249.4560945034027, 251.1899847984314, 252.9649088382721, 254.7398328781128, 256.4317030906677, 258.12357330322266, 259.8329236507416, 261.5422739982605, 263.4394793510437, 265.3366847038269, 267.1904571056366, 269.0442295074463, 270.9163234233856, 272.78841733932495, 274.6894233226776, 276.5904293060303, 278.48215079307556, 280.37387228012085, 282.26298904418945, 284.15210580825806, 286.02587819099426, 287.89965057373047, 289.8175804615021, 291.7355103492737, 293.61982774734497, 295.50414514541626, 297.3652307987213, 299.22631645202637, 301.10402846336365, 302.9817404747009, 304.86295795440674, 306.74417543411255, 308.63718152046204, 310.5301876068115, 312.4186325073242, 314.3070774078369, 316.237340927124, 318.16760444641113, 320.11260771751404, 322.05761098861694, 323.8724694252014, 325.6873278617859, 327.5631835460663, 329.4390392303467, 331.1319057941437, 332.8247723579407, 334.5454134941101, 336.26605463027954, 337.9965283870697, 339.72700214385986, 341.4108624458313, 343.09472274780273, 344.84925627708435, 346.60378980636597, 348.29116129875183, 349.9785327911377, 351.6721739768982, 353.3658151626587, 355.0781762599945, 356.7905373573303, 359.02401542663574, 361.25749349594116]
[29.533333333333335, 29.533333333333335, 36.56666666666667, 36.56666666666667, 44.141666666666666, 44.141666666666666, 53.3, 53.3, 55.56666666666667, 55.56666666666667, 60.875, 60.875, 62.175, 62.175, 69.24166666666666, 69.24166666666666, 71.26666666666667, 71.26666666666667, 72.66666666666667, 72.66666666666667, 73.25833333333334, 73.25833333333334, 73.4, 73.4, 73.40833333333333, 73.40833333333333, 73.88333333333334, 73.88333333333334, 74.59166666666667, 74.59166666666667, 74.84166666666667, 74.84166666666667, 75.0, 75.0, 74.98333333333333, 74.98333333333333, 74.8, 74.8, 75.15833333333333, 75.15833333333333, 75.56666666666666, 75.56666666666666, 76.44166666666666, 76.44166666666666, 76.91666666666667, 76.91666666666667, 77.35833333333333, 77.35833333333333, 77.825, 77.825, 77.63333333333334, 77.63333333333334, 78.325, 78.325, 78.0, 78.0, 78.48333333333333, 78.48333333333333, 79.04166666666667, 79.04166666666667, 79.06666666666666, 79.06666666666666, 79.48333333333333, 79.48333333333333, 79.63333333333334, 79.63333333333334, 79.74166666666666, 79.74166666666666, 79.56666666666666, 79.56666666666666, 80.39166666666667, 80.39166666666667, 80.51666666666667, 80.51666666666667, 80.225, 80.225, 80.50833333333334, 80.50833333333334, 80.1, 80.1, 80.84166666666667, 80.84166666666667, 80.59166666666667, 80.59166666666667, 81.25833333333334, 81.25833333333334, 81.09166666666667, 81.09166666666667, 80.74166666666666, 80.74166666666666, 81.29166666666667, 81.29166666666667, 81.59166666666667, 81.59166666666667, 81.80833333333334, 81.80833333333334, 81.66666666666667, 81.66666666666667, 81.18333333333334, 81.18333333333334, 81.31666666666666, 81.31666666666666, 81.6, 81.6, 81.98333333333333, 81.98333333333333, 82.15833333333333, 82.15833333333333, 82.1, 82.1, 82.34166666666667, 82.34166666666667, 82.175, 82.175, 82.6, 82.6, 82.6, 82.6, 82.33333333333333, 82.33333333333333, 82.59166666666667, 82.59166666666667, 82.81666666666666, 82.81666666666666, 82.41666666666667, 82.41666666666667, 82.53333333333333, 82.53333333333333, 82.80833333333334, 82.80833333333334, 83.0, 83.0, 82.9, 82.9, 83.175, 83.175, 83.20833333333333, 83.20833333333333, 83.31666666666666, 83.31666666666666, 83.04166666666667, 83.04166666666667, 83.13333333333334, 83.13333333333334, 83.25833333333334, 83.25833333333334, 83.2, 83.2, 83.225, 83.225, 83.6, 83.6, 83.45, 83.45, 83.24166666666666, 83.24166666666666, 83.45, 83.45, 83.625, 83.625, 83.45833333333333, 83.45833333333333, 83.04166666666667, 83.04166666666667, 82.875, 82.875, 83.30833333333334, 83.30833333333334, 83.36666666666666, 83.36666666666666, 83.40833333333333, 83.40833333333333, 83.65, 83.65, 83.94166666666666, 83.94166666666666, 83.71666666666667, 83.71666666666667, 83.73333333333333, 83.73333333333333, 83.725, 83.725, 83.675, 83.675, 83.75833333333334, 83.75833333333334, 83.8, 83.8, 83.74166666666666, 83.74166666666666, 83.99166666666666, 83.99166666666666, 83.69166666666666, 83.69166666666666, 83.88333333333334, 83.88333333333334, 83.50833333333334, 83.50833333333334, 83.35, 83.35, 83.63333333333334, 83.63333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.142, Test loss: 2.292, Test accuracy: 21.41
Round   1, Train loss: 0.962, Test loss: 2.475, Test accuracy: 23.28
Round   2, Train loss: 0.886, Test loss: 1.997, Test accuracy: 29.70
Round   3, Train loss: 0.842, Test loss: 2.081, Test accuracy: 26.24
Round   4, Train loss: 0.798, Test loss: 1.951, Test accuracy: 29.52
Round   5, Train loss: 0.765, Test loss: 1.925, Test accuracy: 35.20
Round   6, Train loss: 0.717, Test loss: 2.020, Test accuracy: 31.82
Round   7, Train loss: 0.662, Test loss: 1.963, Test accuracy: 32.58
Round   8, Train loss: 0.667, Test loss: 1.999, Test accuracy: 30.48
Round   9, Train loss: 0.686, Test loss: 1.716, Test accuracy: 39.27
Round  10, Train loss: 0.607, Test loss: 1.742, Test accuracy: 36.26
Round  11, Train loss: 0.641, Test loss: 1.691, Test accuracy: 42.41
Round  12, Train loss: 0.616, Test loss: 1.727, Test accuracy: 35.93
Round  13, Train loss: 0.613, Test loss: 1.516, Test accuracy: 47.39
Round  14, Train loss: 0.610, Test loss: 1.559, Test accuracy: 46.34
Round  15, Train loss: 0.576, Test loss: 1.555, Test accuracy: 43.18
Round  16, Train loss: 0.573, Test loss: 1.416, Test accuracy: 50.73
Round  17, Train loss: 0.513, Test loss: 1.856, Test accuracy: 36.24
Round  18, Train loss: 0.574, Test loss: 1.434, Test accuracy: 47.68
Round  19, Train loss: 0.506, Test loss: 1.517, Test accuracy: 45.77
Round  20, Train loss: 0.500, Test loss: 1.677, Test accuracy: 46.36
Round  21, Train loss: 0.519, Test loss: 1.406, Test accuracy: 52.32
Round  22, Train loss: 0.459, Test loss: 1.460, Test accuracy: 50.72
Round  23, Train loss: 0.487, Test loss: 1.424, Test accuracy: 50.15
Round  24, Train loss: 0.495, Test loss: 1.345, Test accuracy: 53.10
Round  25, Train loss: 0.468, Test loss: 1.394, Test accuracy: 52.54
Round  26, Train loss: 0.450, Test loss: 1.434, Test accuracy: 51.00
Round  27, Train loss: 0.423, Test loss: 1.309, Test accuracy: 55.16
Round  28, Train loss: 0.428, Test loss: 1.193, Test accuracy: 58.98
Round  29, Train loss: 0.450, Test loss: 1.356, Test accuracy: 53.43
Round  30, Train loss: 0.405, Test loss: 1.455, Test accuracy: 50.63
Round  31, Train loss: 0.408, Test loss: 1.299, Test accuracy: 54.60
Round  32, Train loss: 0.381, Test loss: 1.152, Test accuracy: 60.33
Round  33, Train loss: 0.392, Test loss: 1.787, Test accuracy: 44.62
Round  34, Train loss: 0.417, Test loss: 1.578, Test accuracy: 47.43
Round  35, Train loss: 0.377, Test loss: 1.408, Test accuracy: 52.34
Round  36, Train loss: 0.397, Test loss: 1.299, Test accuracy: 54.21
Round  37, Train loss: 0.357, Test loss: 1.196, Test accuracy: 58.87
Round  38, Train loss: 0.367, Test loss: 1.326, Test accuracy: 55.64
Round  39, Train loss: 0.376, Test loss: 1.230, Test accuracy: 57.43
Round  40, Train loss: 0.343, Test loss: 1.425, Test accuracy: 52.36
Round  41, Train loss: 0.329, Test loss: 1.107, Test accuracy: 62.19
Round  42, Train loss: 0.368, Test loss: 1.514, Test accuracy: 49.17
Round  43, Train loss: 0.331, Test loss: 1.267, Test accuracy: 57.53
Round  44, Train loss: 0.382, Test loss: 1.111, Test accuracy: 60.08
Round  45, Train loss: 0.345, Test loss: 1.226, Test accuracy: 57.64
Round  46, Train loss: 0.321, Test loss: 1.356, Test accuracy: 55.73
Round  47, Train loss: 0.314, Test loss: 1.144, Test accuracy: 61.62
Round  48, Train loss: 0.292, Test loss: 1.341, Test accuracy: 56.27
Round  49, Train loss: 0.324, Test loss: 1.157, Test accuracy: 62.33
Round  50, Train loss: 0.303, Test loss: 1.507, Test accuracy: 53.84
Round  51, Train loss: 0.320, Test loss: 1.356, Test accuracy: 54.23
Round  52, Train loss: 0.319, Test loss: 1.225, Test accuracy: 57.24
Round  53, Train loss: 0.334, Test loss: 1.089, Test accuracy: 62.38
Round  54, Train loss: 0.278, Test loss: 1.231, Test accuracy: 59.83
Round  55, Train loss: 0.319, Test loss: 1.163, Test accuracy: 61.32
Round  56, Train loss: 0.300, Test loss: 1.138, Test accuracy: 61.25
Round  57, Train loss: 0.266, Test loss: 1.319, Test accuracy: 56.56
Round  58, Train loss: 0.266, Test loss: 1.344, Test accuracy: 57.15
Round  59, Train loss: 0.252, Test loss: 1.202, Test accuracy: 59.88
Round  60, Train loss: 0.252, Test loss: 1.362, Test accuracy: 55.46
Round  61, Train loss: 0.287, Test loss: 1.580, Test accuracy: 50.47
Round  62, Train loss: 0.243, Test loss: 1.104, Test accuracy: 63.73
Round  63, Train loss: 0.242, Test loss: 1.366, Test accuracy: 54.82
Round  64, Train loss: 0.279, Test loss: 1.156, Test accuracy: 61.46
Round  65, Train loss: 0.266, Test loss: 1.672, Test accuracy: 51.47
Round  66, Train loss: 0.242, Test loss: 1.154, Test accuracy: 60.67
Round  67, Train loss: 0.222, Test loss: 1.112, Test accuracy: 61.88
Round  68, Train loss: 0.202, Test loss: 1.375, Test accuracy: 56.62
Round  69, Train loss: 0.255, Test loss: 1.382, Test accuracy: 58.50
Round  70, Train loss: 0.239, Test loss: 1.311, Test accuracy: 60.18
Round  71, Train loss: 0.237, Test loss: 1.383, Test accuracy: 57.72
Round  72, Train loss: 0.256, Test loss: 1.232, Test accuracy: 61.79
Round  73, Train loss: 0.233, Test loss: 1.366, Test accuracy: 59.00
Round  74, Train loss: 0.257, Test loss: 1.152, Test accuracy: 63.83
Round  75, Train loss: 0.230, Test loss: 1.225, Test accuracy: 61.88
Round  76, Train loss: 0.229, Test loss: 1.097, Test accuracy: 64.01
Round  77, Train loss: 0.204, Test loss: 1.101, Test accuracy: 64.24
Round  78, Train loss: 0.216, Test loss: 1.083, Test accuracy: 63.85
Round  79, Train loss: 0.202, Test loss: 1.318, Test accuracy: 60.44
Round  80, Train loss: 0.213, Test loss: 1.362, Test accuracy: 56.68
Round  81, Train loss: 0.202, Test loss: 1.092, Test accuracy: 63.53
Round  82, Train loss: 0.207, Test loss: 1.309, Test accuracy: 60.33
Round  83, Train loss: 0.196, Test loss: 1.253, Test accuracy: 61.98
Round  84, Train loss: 0.185, Test loss: 1.252, Test accuracy: 63.06
Round  85, Train loss: 0.225, Test loss: 1.231, Test accuracy: 60.52
Round  86, Train loss: 0.181, Test loss: 1.117, Test accuracy: 63.72
Round  87, Train loss: 0.183, Test loss: 1.165, Test accuracy: 62.37
Round  88, Train loss: 0.158, Test loss: 1.136, Test accuracy: 63.08
Round  89, Train loss: 0.197, Test loss: 1.079, Test accuracy: 64.79
Round  90, Train loss: 0.189, Test loss: 1.197, Test accuracy: 62.07
Round  91, Train loss: 0.156, Test loss: 1.433, Test accuracy: 59.58
Round  92, Train loss: 0.158, Test loss: 1.336, Test accuracy: 58.84
Round  93, Train loss: 0.177, Test loss: 1.289, Test accuracy: 62.96
Round  94, Train loss: 0.151, Test loss: 1.106, Test accuracy: 65.12
Round  95, Train loss: 0.165, Test loss: 1.122, Test accuracy: 64.59
Round  96, Train loss: 0.161, Test loss: 1.179, Test accuracy: 63.67
Round  97, Train loss: 0.209, Test loss: 1.257, Test accuracy: 61.92
Round  98, Train loss: 0.173, Test loss: 1.612, Test accuracy: 54.39
Round  99, Train loss: 0.162, Test loss: 1.128, Test accuracy: 65.12
Final Round, Train loss: 0.162, Test loss: 1.027, Test accuracy: 67.25
Average accuracy final 10 rounds: 61.82750000000001
2793.2418320178986
[4.160329103469849, 7.833895206451416, 11.91856050491333, 16.02781367301941, 20.150737285614014, 24.261935710906982, 28.37200903892517, 32.565507650375366, 36.66941857337952, 40.860432863235474, 45.03669261932373, 49.30689883232117, 53.35780596733093, 57.423325538635254, 61.489675998687744, 65.53164076805115, 69.57319235801697, 73.67465424537659, 77.6912956237793, 81.94163346290588, 86.00114870071411, 90.1332540512085, 94.26888084411621, 98.33639335632324, 102.40824484825134, 106.44284009933472, 110.73891854286194, 114.96344304084778, 118.77486848831177, 122.70207858085632, 126.5802972316742, 130.40724992752075, 134.20620799064636, 138.05826449394226, 141.86329889297485, 145.75076484680176, 149.43854141235352, 153.15425825119019, 157.00498294830322, 160.81117296218872, 164.69869685173035, 168.66237783432007, 172.44313669204712, 176.11963629722595, 179.7966582775116, 183.45766758918762, 187.58665251731873, 191.76143598556519, 195.93613815307617, 200.04455828666687, 204.24131035804749, 208.08041548728943, 211.77576851844788, 215.4843933582306, 219.18770599365234, 222.90206146240234, 226.58284187316895, 230.26568460464478, 234.01680183410645, 237.683899641037, 241.38579320907593, 245.429194688797, 249.4455213546753, 253.5384168624878, 257.5677614212036, 261.6405153274536, 265.6758186817169, 269.73879742622375, 273.83839297294617, 277.9087407588959, 282.04050397872925, 286.143079996109, 290.2378532886505, 294.2934105396271, 298.35728096961975, 302.4665586948395, 306.58270502090454, 310.67861580848694, 314.74107599258423, 318.84418869018555, 322.9095227718353, 327.05400347709656, 331.15261793136597, 335.2595806121826, 339.35058188438416, 343.39327025413513, 347.50072836875916, 351.56990218162537, 355.67310309410095, 359.7181680202484, 363.8409881591797, 367.9288635253906, 371.9793610572815, 376.1006193161011, 380.15389251708984, 383.86954975128174, 387.5871856212616, 391.2866621017456, 395.0198962688446, 398.87279772758484, 402.2112832069397]
[21.408333333333335, 23.283333333333335, 29.7, 26.241666666666667, 29.516666666666666, 35.2, 31.816666666666666, 32.583333333333336, 30.483333333333334, 39.275, 36.25833333333333, 42.40833333333333, 35.93333333333333, 47.391666666666666, 46.34166666666667, 43.18333333333333, 50.733333333333334, 36.24166666666667, 47.68333333333333, 45.775, 46.358333333333334, 52.31666666666667, 50.71666666666667, 50.15, 53.1, 52.541666666666664, 51.0, 55.15833333333333, 58.983333333333334, 53.43333333333333, 50.63333333333333, 54.6, 60.333333333333336, 44.61666666666667, 47.43333333333333, 52.34166666666667, 54.208333333333336, 58.86666666666667, 55.641666666666666, 57.43333333333333, 52.358333333333334, 62.19166666666667, 49.175, 57.53333333333333, 60.083333333333336, 57.641666666666666, 55.725, 61.61666666666667, 56.266666666666666, 62.333333333333336, 53.84166666666667, 54.233333333333334, 57.24166666666667, 62.375, 59.833333333333336, 61.31666666666667, 61.25, 56.55833333333333, 57.15, 59.875, 55.458333333333336, 50.46666666666667, 63.725, 54.81666666666667, 61.458333333333336, 51.46666666666667, 60.675, 61.88333333333333, 56.61666666666667, 58.5, 60.18333333333333, 57.71666666666667, 61.791666666666664, 59.0, 63.833333333333336, 61.875, 64.00833333333334, 64.24166666666666, 63.85, 60.44166666666667, 56.68333333333333, 63.53333333333333, 60.325, 61.983333333333334, 63.05833333333333, 60.525, 63.71666666666667, 62.36666666666667, 63.075, 64.79166666666667, 62.06666666666667, 59.583333333333336, 58.84166666666667, 62.958333333333336, 65.125, 64.59166666666667, 63.666666666666664, 61.925, 54.391666666666666, 65.125, 67.25]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.296, Test loss: 2.303, Test accuracy: 10.14
Round   0, Global train loss: 2.296, Global test loss: 2.304, Global test accuracy: 10.09
Round   1, Train loss: 2.285, Test loss: 2.304, Test accuracy: 10.20
Round   1, Global train loss: 2.285, Global test loss: 2.305, Global test accuracy: 10.05
Round   2, Train loss: 2.314, Test loss: 2.308, Test accuracy: 10.19
Round   2, Global train loss: 2.314, Global test loss: 2.307, Global test accuracy: 10.03
Round   3, Train loss: nan, Test loss: nan, Test accuracy: 17.74
Round   3, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 17.91
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 16.24
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 16.24
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 17.91
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 17.91
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 19.57
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 19.57
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 100, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 101, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 102, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 103, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 104, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 105, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 106, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 107, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 108, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 109, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 110, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 111, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 112, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 113, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 114, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 115, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 116, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 117, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 118, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 119, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 120, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 121, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 122, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 123, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 124, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 125, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 126, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 127, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 128, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 129, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 130, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 131, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 132, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 133, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 134, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Average accuracy final 10 rounds: 16.666666666666664 

Average global accuracy final 10 rounds: 16.666666666666664 

4998.33448600769
[1.8031227588653564, 3.380220651626587, 4.971369743347168, 6.544109582901001, 8.133181810379028, 9.739470481872559, 11.369309186935425, 13.015011548995972, 14.671688556671143, 16.318346738815308, 17.970605850219727, 19.608070373535156, 21.244600772857666, 22.894636869430542, 24.49761176109314, 26.06783390045166, 27.682977437973022, 29.28650403022766, 30.89490056037903, 32.50354290008545, 34.113561153411865, 35.719640016555786, 37.33468532562256, 38.96227264404297, 40.61991500854492, 42.245084047317505, 43.871596574783325, 45.5027289390564, 47.119828939437866, 48.73170828819275, 50.35045599937439, 51.958625078201294, 53.59061074256897, 55.20990490913391, 56.82568311691284, 58.45744252204895, 60.062546491622925, 61.67004442214966, 63.28902196884155, 64.91719579696655, 66.53705644607544, 68.15214729309082, 69.77500033378601, 71.40417218208313, 73.03163242340088, 74.64485955238342, 76.28402471542358, 77.93007111549377, 79.57144379615784, 81.20818376541138, 82.8352746963501, 84.4558355808258, 86.07377219200134, 87.68491530418396, 89.30595850944519, 90.92841982841492, 92.54860067367554, 94.17357683181763, 95.78608775138855, 97.3946681022644, 98.95661520957947, 100.5719747543335, 102.20912480354309, 103.82935190200806, 105.44705748558044, 107.07224440574646, 108.69982194900513, 110.32432126998901, 111.94632720947266, 113.58584117889404, 115.22457337379456, 116.84951186180115, 118.47910714149475, 120.11691427230835, 121.75317215919495, 123.37203478813171, 125.00403618812561, 126.63535141944885, 128.27716302871704, 129.90758848190308, 131.53989720344543, 133.14882373809814, 134.76048302650452, 136.37812733650208, 138.00190687179565, 139.645765542984, 141.28024315834045, 142.93979573249817, 144.58451199531555, 146.22382760047913, 147.85659289360046, 149.49238324165344, 151.15112590789795, 152.77940797805786, 154.38968420028687, 156.01772046089172, 157.642103433609, 159.21326088905334, 160.77599835395813, 162.32059741020203, 163.87099432945251, 165.432546377182, 167.02207040786743, 168.61068677902222, 170.29631090164185, 171.9353325366974, 173.5548083782196, 175.1897964477539, 176.87894916534424, 178.54400157928467, 180.17650628089905, 181.8288435935974, 183.47243309020996, 185.1028938293457, 186.72199702262878, 188.36962127685547, 190.00021052360535, 191.62164282798767, 193.24504494667053, 194.8723533153534, 196.5145709514618, 198.166015625, 199.77881169319153, 201.41735291481018, 203.0717625617981, 204.72553658485413, 206.37439274787903, 208.01236391067505, 209.66269969940186, 211.3311746120453, 212.94949293136597, 214.55730175971985, 216.16037821769714, 217.81511902809143, 219.42914867401123, 221.07235288619995, 222.72262144088745, 224.36099696159363, 225.99267411231995, 227.6446921825409, 229.32458639144897, 230.97136521339417, 232.60337090492249, 234.26851081848145, 235.93677687644958, 237.5764000415802, 239.21115136146545, 240.86569595336914, 242.51868844032288, 244.16400790214539, 245.78892159461975, 247.43321871757507, 249.07057976722717, 250.71840691566467, 252.3640968799591, 254.02629446983337, 255.67491674423218, 257.3184232711792, 258.97049379348755, 260.632257938385, 262.30328154563904, 263.9304881095886, 265.5902624130249, 267.2557101249695, 268.89791440963745, 270.54277062416077, 272.1990716457367, 273.83753418922424, 275.48348808288574, 277.1390857696533, 278.7824754714966, 280.4244775772095, 282.0806391239166, 283.7275731563568, 285.36698484420776, 287.02698731422424, 288.67529010772705, 290.3282251358032, 291.9404194355011, 293.5572237968445, 295.16660380363464, 296.7485520839691, 298.19005393981934, 299.6286449432373, 301.07661056518555, 302.49488043785095, 303.8487424850464, 305.2726149559021, 306.69918608665466, 308.1086804866791, 309.51602268218994, 310.9042570590973, 312.3217351436615, 313.731871843338, 315.1219210624695, 316.53334617614746, 317.9406955242157, 319.4414312839508, 320.8504490852356, 322.25858545303345, 323.73033905029297, 325.16389060020447, 326.56842136383057, 327.98997688293457, 329.3800287246704, 330.79105257987976, 332.20059418678284, 333.598105430603, 335.04174304008484, 336.4703435897827, 337.94349217414856, 339.369313955307, 340.75508999824524, 342.16443729400635, 343.5792291164398, 344.98653078079224, 346.43040919303894, 347.8376085758209, 349.23698019981384, 350.65446734428406, 352.0880300998688, 353.4975788593292, 354.8989791870117, 356.29614329338074, 357.7770833969116, 359.20213866233826, 360.61210465431213, 362.03474044799805, 363.4172351360321, 364.826358795166, 366.2431569099426, 367.65890431404114, 369.1256182193756, 370.53737449645996, 371.9501097202301, 373.3595983982086, 374.7627239227295, 376.2016532421112, 377.6105444431305, 379.05153703689575, 380.51897835731506, 381.92503666877747, 383.3207895755768, 384.7404682636261, 386.1236402988434, 387.5467231273651, 388.9446530342102, 390.3697683811188, 391.86761927604675, 393.2772116661072, 394.68447709083557, 396.09810304641724, 397.5001413822174, 398.9340624809265, 400.34295773506165, 401.7688536643982, 403.2395701408386, 404.64143800735474, 406.04023337364197, 407.4494559764862, 408.8637571334839, 410.2944960594177, 411.6977689266205, 413.1155834197998, 414.6255216598511, 416.0147490501404, 417.4170513153076, 418.83707213401794, 420.25239968299866, 421.68246603012085, 423.0860106945038, 424.5665192604065, 426.0324821472168, 427.42847418785095, 428.849321603775, 430.26283955574036, 431.6804656982422, 433.11695647239685, 434.52216148376465, 435.93282294273376, 437.3478915691376, 438.7184534072876, 440.12842178344727, 441.54148745536804, 442.9536738395691, 444.38788890838623, 445.7786455154419, 447.201762676239, 448.62165451049805, 450.0078320503235, 451.4359984397888, 452.86433243751526, 454.2894928455353, 455.71084332466125, 457.10103607177734, 458.5049798488617, 459.93184065818787, 461.3201787471771, 462.7320365905762, 464.13914132118225, 466.5762355327606]
[10.141666666666667, 10.2, 10.191666666666666, 17.741666666666667, 17.908333333333335, 16.241666666666667, 16.241666666666667, 17.908333333333335, 17.908333333333335, 19.575, 19.575, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.105, Test loss: 2.022, Test accuracy: 24.48
Round   0, Global train loss: 1.105, Global test loss: 2.345, Global test accuracy: 13.43
Round   1, Train loss: 0.894, Test loss: 1.599, Test accuracy: 42.30
Round   1, Global train loss: 0.894, Global test loss: 2.200, Global test accuracy: 24.53
Round   2, Train loss: 0.854, Test loss: 1.288, Test accuracy: 51.21
Round   2, Global train loss: 0.854, Global test loss: 2.160, Global test accuracy: 18.35
Round   3, Train loss: 0.764, Test loss: 0.989, Test accuracy: 59.04
Round   3, Global train loss: 0.764, Global test loss: 1.848, Global test accuracy: 31.57
Round   4, Train loss: 0.791, Test loss: 0.822, Test accuracy: 65.52
Round   4, Global train loss: 0.791, Global test loss: 1.789, Global test accuracy: 34.93
Round   5, Train loss: 0.651, Test loss: 0.725, Test accuracy: 69.23
Round   5, Global train loss: 0.651, Global test loss: 1.860, Global test accuracy: 33.20
Round   6, Train loss: 0.690, Test loss: 0.697, Test accuracy: 70.42
Round   6, Global train loss: 0.690, Global test loss: 1.860, Global test accuracy: 36.97
Round   7, Train loss: 0.614, Test loss: 0.690, Test accuracy: 71.37
Round   7, Global train loss: 0.614, Global test loss: 2.418, Global test accuracy: 30.82
Round   8, Train loss: 0.687, Test loss: 0.673, Test accuracy: 72.12
Round   8, Global train loss: 0.687, Global test loss: 1.857, Global test accuracy: 32.01
Round   9, Train loss: 0.642, Test loss: 0.674, Test accuracy: 72.32
Round   9, Global train loss: 0.642, Global test loss: 1.797, Global test accuracy: 37.09
Round  10, Train loss: 0.561, Test loss: 0.649, Test accuracy: 73.16
Round  10, Global train loss: 0.561, Global test loss: 1.628, Global test accuracy: 43.91
Round  11, Train loss: 0.491, Test loss: 0.632, Test accuracy: 73.47
Round  11, Global train loss: 0.491, Global test loss: 1.949, Global test accuracy: 36.25
Round  12, Train loss: 0.566, Test loss: 0.585, Test accuracy: 75.36
Round  12, Global train loss: 0.566, Global test loss: 1.556, Global test accuracy: 44.63
Round  13, Train loss: 0.599, Test loss: 0.586, Test accuracy: 75.58
Round  13, Global train loss: 0.599, Global test loss: 1.783, Global test accuracy: 35.92
Round  14, Train loss: 0.476, Test loss: 0.573, Test accuracy: 76.32
Round  14, Global train loss: 0.476, Global test loss: 1.871, Global test accuracy: 38.83
Round  15, Train loss: 0.538, Test loss: 0.577, Test accuracy: 76.33
Round  15, Global train loss: 0.538, Global test loss: 1.571, Global test accuracy: 45.65
Round  16, Train loss: 0.517, Test loss: 0.567, Test accuracy: 76.86
Round  16, Global train loss: 0.517, Global test loss: 1.445, Global test accuracy: 49.38
Round  17, Train loss: 0.456, Test loss: 0.553, Test accuracy: 77.44
Round  17, Global train loss: 0.456, Global test loss: 1.697, Global test accuracy: 43.06
Round  18, Train loss: 0.551, Test loss: 0.557, Test accuracy: 77.59
Round  18, Global train loss: 0.551, Global test loss: 1.423, Global test accuracy: 49.11
Round  19, Train loss: 0.476, Test loss: 0.538, Test accuracy: 78.07
Round  19, Global train loss: 0.476, Global test loss: 1.633, Global test accuracy: 48.80
Round  20, Train loss: 0.435, Test loss: 0.544, Test accuracy: 77.83
Round  20, Global train loss: 0.435, Global test loss: 1.355, Global test accuracy: 54.11
Round  21, Train loss: 0.417, Test loss: 0.528, Test accuracy: 78.22
Round  21, Global train loss: 0.417, Global test loss: 1.287, Global test accuracy: 55.28
Round  22, Train loss: 0.415, Test loss: 0.534, Test accuracy: 78.03
Round  22, Global train loss: 0.415, Global test loss: 1.256, Global test accuracy: 56.77
Round  23, Train loss: 0.504, Test loss: 0.528, Test accuracy: 78.75
Round  23, Global train loss: 0.504, Global test loss: 1.272, Global test accuracy: 55.67
Round  24, Train loss: 0.446, Test loss: 0.526, Test accuracy: 78.88
Round  24, Global train loss: 0.446, Global test loss: 1.338, Global test accuracy: 52.65
Round  25, Train loss: 0.372, Test loss: 0.506, Test accuracy: 79.47
Round  25, Global train loss: 0.372, Global test loss: 1.438, Global test accuracy: 52.70
Round  26, Train loss: 0.456, Test loss: 0.522, Test accuracy: 79.27
Round  26, Global train loss: 0.456, Global test loss: 1.254, Global test accuracy: 56.71
Round  27, Train loss: 0.391, Test loss: 0.520, Test accuracy: 79.60
Round  27, Global train loss: 0.391, Global test loss: 1.677, Global test accuracy: 48.09
Round  28, Train loss: 0.427, Test loss: 0.514, Test accuracy: 79.83
Round  28, Global train loss: 0.427, Global test loss: 1.400, Global test accuracy: 50.78
Round  29, Train loss: 0.349, Test loss: 0.509, Test accuracy: 80.17
Round  29, Global train loss: 0.349, Global test loss: 1.313, Global test accuracy: 56.23
Round  30, Train loss: 0.386, Test loss: 0.499, Test accuracy: 80.58
Round  30, Global train loss: 0.386, Global test loss: 1.367, Global test accuracy: 54.94
Round  31, Train loss: 0.376, Test loss: 0.488, Test accuracy: 81.03
Round  31, Global train loss: 0.376, Global test loss: 1.433, Global test accuracy: 54.12
Round  32, Train loss: 0.400, Test loss: 0.484, Test accuracy: 81.40
Round  32, Global train loss: 0.400, Global test loss: 1.532, Global test accuracy: 52.31
Round  33, Train loss: 0.361, Test loss: 0.480, Test accuracy: 81.55
Round  33, Global train loss: 0.361, Global test loss: 1.440, Global test accuracy: 53.92
Round  34, Train loss: 0.540, Test loss: 0.472, Test accuracy: 81.92
Round  34, Global train loss: 0.540, Global test loss: 1.221, Global test accuracy: 57.96
Round  35, Train loss: 0.415, Test loss: 0.468, Test accuracy: 81.99
Round  35, Global train loss: 0.415, Global test loss: 1.355, Global test accuracy: 53.58
Round  36, Train loss: 0.403, Test loss: 0.486, Test accuracy: 81.26
Round  36, Global train loss: 0.403, Global test loss: 1.341, Global test accuracy: 54.48
Round  37, Train loss: 0.406, Test loss: 0.479, Test accuracy: 81.75
Round  37, Global train loss: 0.406, Global test loss: 1.232, Global test accuracy: 56.91
Round  38, Train loss: 0.306, Test loss: 0.480, Test accuracy: 81.72
Round  38, Global train loss: 0.306, Global test loss: 1.227, Global test accuracy: 57.79
Round  39, Train loss: 0.354, Test loss: 0.469, Test accuracy: 82.16
Round  39, Global train loss: 0.354, Global test loss: 1.554, Global test accuracy: 51.00
Round  40, Train loss: 0.406, Test loss: 0.474, Test accuracy: 82.22
Round  40, Global train loss: 0.406, Global test loss: 1.603, Global test accuracy: 47.85
Round  41, Train loss: 0.332, Test loss: 0.478, Test accuracy: 82.14
Round  41, Global train loss: 0.332, Global test loss: 1.193, Global test accuracy: 59.62
Round  42, Train loss: 0.364, Test loss: 0.461, Test accuracy: 83.04
Round  42, Global train loss: 0.364, Global test loss: 1.503, Global test accuracy: 52.33
Round  43, Train loss: 0.348, Test loss: 0.462, Test accuracy: 82.87
Round  43, Global train loss: 0.348, Global test loss: 1.818, Global test accuracy: 46.81
Round  44, Train loss: 0.403, Test loss: 0.477, Test accuracy: 82.67
Round  44, Global train loss: 0.403, Global test loss: 1.602, Global test accuracy: 48.20
Round  45, Train loss: 0.377, Test loss: 0.478, Test accuracy: 82.42
Round  45, Global train loss: 0.377, Global test loss: 1.304, Global test accuracy: 56.92
Round  46, Train loss: 0.384, Test loss: 0.473, Test accuracy: 82.46
Round  46, Global train loss: 0.384, Global test loss: 1.182, Global test accuracy: 59.38
Round  47, Train loss: 0.300, Test loss: 0.479, Test accuracy: 82.37
Round  47, Global train loss: 0.300, Global test loss: 1.341, Global test accuracy: 55.49
Round  48, Train loss: 0.241, Test loss: 0.482, Test accuracy: 82.89
Round  48, Global train loss: 0.241, Global test loss: 1.503, Global test accuracy: 54.52
Round  49, Train loss: 0.251, Test loss: 0.480, Test accuracy: 82.93
Round  49, Global train loss: 0.251, Global test loss: 1.803, Global test accuracy: 51.69
Round  50, Train loss: 0.303, Test loss: 0.468, Test accuracy: 83.14
Round  50, Global train loss: 0.303, Global test loss: 1.204, Global test accuracy: 60.64
Round  51, Train loss: 0.259, Test loss: 0.479, Test accuracy: 83.04
Round  51, Global train loss: 0.259, Global test loss: 1.802, Global test accuracy: 51.62
Round  52, Train loss: 0.320, Test loss: 0.492, Test accuracy: 82.61
Round  52, Global train loss: 0.320, Global test loss: 1.434, Global test accuracy: 55.11
Round  53, Train loss: 0.322, Test loss: 0.497, Test accuracy: 82.33
Round  53, Global train loss: 0.322, Global test loss: 1.454, Global test accuracy: 53.47
Round  54, Train loss: 0.337, Test loss: 0.474, Test accuracy: 83.72
Round  54, Global train loss: 0.337, Global test loss: 1.302, Global test accuracy: 57.19
Round  55, Train loss: 0.252, Test loss: 0.482, Test accuracy: 83.35
Round  55, Global train loss: 0.252, Global test loss: 1.345, Global test accuracy: 56.76
Round  56, Train loss: 0.293, Test loss: 0.485, Test accuracy: 83.12
Round  56, Global train loss: 0.293, Global test loss: 1.200, Global test accuracy: 60.91
Round  57, Train loss: 0.315, Test loss: 0.489, Test accuracy: 82.95
Round  57, Global train loss: 0.315, Global test loss: 1.418, Global test accuracy: 56.77
Round  58, Train loss: 0.298, Test loss: 0.486, Test accuracy: 83.28
Round  58, Global train loss: 0.298, Global test loss: 1.474, Global test accuracy: 55.48
Round  59, Train loss: 0.211, Test loss: 0.484, Test accuracy: 83.24
Round  59, Global train loss: 0.211, Global test loss: 1.473, Global test accuracy: 58.33
Round  60, Train loss: 0.255, Test loss: 0.484, Test accuracy: 83.22
Round  60, Global train loss: 0.255, Global test loss: 1.314, Global test accuracy: 59.31
Round  61, Train loss: 0.331, Test loss: 0.501, Test accuracy: 83.06
Round  61, Global train loss: 0.331, Global test loss: 1.424, Global test accuracy: 56.80
Round  62, Train loss: 0.244, Test loss: 0.495, Test accuracy: 83.38
Round  62, Global train loss: 0.244, Global test loss: 1.330, Global test accuracy: 58.64
Round  63, Train loss: 0.260, Test loss: 0.504, Test accuracy: 83.19
Round  63, Global train loss: 0.260, Global test loss: 1.119, Global test accuracy: 62.79
Round  64, Train loss: 0.244, Test loss: 0.493, Test accuracy: 83.71
Round  64, Global train loss: 0.244, Global test loss: 1.339, Global test accuracy: 58.15
Round  65, Train loss: 0.230, Test loss: 0.504, Test accuracy: 83.62
Round  65, Global train loss: 0.230, Global test loss: 1.314, Global test accuracy: 59.54
Round  66, Train loss: 0.218, Test loss: 0.514, Test accuracy: 83.10
Round  66, Global train loss: 0.218, Global test loss: 1.232, Global test accuracy: 60.98
Round  67, Train loss: 0.199, Test loss: 0.503, Test accuracy: 83.42
Round  67, Global train loss: 0.199, Global test loss: 1.289, Global test accuracy: 60.84
Round  68, Train loss: 0.273, Test loss: 0.486, Test accuracy: 83.76
Round  68, Global train loss: 0.273, Global test loss: 1.328, Global test accuracy: 58.39
Round  69, Train loss: 0.258, Test loss: 0.463, Test accuracy: 84.36
Round  69, Global train loss: 0.258, Global test loss: 1.340, Global test accuracy: 58.29
Round  70, Train loss: 0.219, Test loss: 0.462, Test accuracy: 84.40
Round  70, Global train loss: 0.219, Global test loss: 1.314, Global test accuracy: 60.85
Round  71, Train loss: 0.314, Test loss: 0.456, Test accuracy: 84.33
Round  71, Global train loss: 0.314, Global test loss: 1.388, Global test accuracy: 59.20
Round  72, Train loss: 0.215, Test loss: 0.468, Test accuracy: 84.07
Round  72, Global train loss: 0.215, Global test loss: 1.583, Global test accuracy: 57.48
Round  73, Train loss: 0.298, Test loss: 0.468, Test accuracy: 84.45
Round  73, Global train loss: 0.298, Global test loss: 1.228, Global test accuracy: 62.52
Round  74, Train loss: 0.194, Test loss: 0.484, Test accuracy: 83.99
Round  74, Global train loss: 0.194, Global test loss: 1.358, Global test accuracy: 59.23
Round  75, Train loss: 0.200, Test loss: 0.502, Test accuracy: 83.58
Round  75, Global train loss: 0.200, Global test loss: 1.221, Global test accuracy: 62.08
Round  76, Train loss: 0.234, Test loss: 0.482, Test accuracy: 84.12
Round  76, Global train loss: 0.234, Global test loss: 1.105, Global test accuracy: 64.99
Round  77, Train loss: 0.300, Test loss: 0.487, Test accuracy: 83.79
Round  77, Global train loss: 0.300, Global test loss: 1.182, Global test accuracy: 61.85
Round  78, Train loss: 0.217, Test loss: 0.511, Test accuracy: 83.38
Round  78, Global train loss: 0.217, Global test loss: 1.170, Global test accuracy: 61.99
Round  79, Train loss: 0.224, Test loss: 0.483, Test accuracy: 83.97
Round  79, Global train loss: 0.224, Global test loss: 1.315, Global test accuracy: 59.34
Round  80, Train loss: 0.223, Test loss: 0.469, Test accuracy: 84.62
Round  80, Global train loss: 0.223, Global test loss: 1.379, Global test accuracy: 58.02
Round  81, Train loss: 0.194, Test loss: 0.471, Test accuracy: 84.65
Round  81, Global train loss: 0.194, Global test loss: 1.472, Global test accuracy: 56.78
Round  82, Train loss: 0.170, Test loss: 0.477, Test accuracy: 84.63
Round  82, Global train loss: 0.170, Global test loss: 1.357, Global test accuracy: 59.52
Round  83, Train loss: 0.166, Test loss: 0.483, Test accuracy: 84.56
Round  83, Global train loss: 0.166, Global test loss: 1.283, Global test accuracy: 62.93
Round  84, Train loss: 0.222, Test loss: 0.493, Test accuracy: 84.33
Round  84, Global train loss: 0.222, Global test loss: 1.724, Global test accuracy: 51.89
Round  85, Train loss: 0.222, Test loss: 0.487, Test accuracy: 84.60
Round  85, Global train loss: 0.222, Global test loss: 1.517, Global test accuracy: 54.42
Round  86, Train loss: 0.202, Test loss: 0.492, Test accuracy: 84.64
Round  86, Global train loss: 0.202, Global test loss: 1.233, Global test accuracy: 60.78
Round  87, Train loss: 0.172, Test loss: 0.488, Test accuracy: 84.78
Round  87, Global train loss: 0.172, Global test loss: 1.410, Global test accuracy: 58.90
Round  88, Train loss: 0.239, Test loss: 0.504, Test accuracy: 84.33
Round  88, Global train loss: 0.239, Global test loss: 1.346, Global test accuracy: 59.72
Round  89, Train loss: 0.232, Test loss: 0.518, Test accuracy: 84.28
Round  89, Global train loss: 0.232, Global test loss: 1.379, Global test accuracy: 58.52
Round  90, Train loss: 0.228, Test loss: 0.513, Test accuracy: 84.45
Round  90, Global train loss: 0.228, Global test loss: 1.136, Global test accuracy: 63.89
Round  91, Train loss: 0.184, Test loss: 0.528, Test accuracy: 84.22
Round  91, Global train loss: 0.184, Global test loss: 1.489, Global test accuracy: 57.86
Round  92, Train loss: 0.167, Test loss: 0.534, Test accuracy: 84.08
Round  92, Global train loss: 0.167, Global test loss: 1.212, Global test accuracy: 62.99
Round  93, Train loss: 0.211, Test loss: 0.535, Test accuracy: 83.96
Round  93, Global train loss: 0.211, Global test loss: 1.435, Global test accuracy: 58.99
Round  94, Train loss: 0.170, Test loss: 0.545, Test accuracy: 83.77
Round  94, Global train loss: 0.170, Global test loss: 1.391, Global test accuracy: 60.92
Round  95, Train loss: 0.218, Test loss: 0.534, Test accuracy: 83.80
Round  95, Global train loss: 0.218, Global test loss: 1.561, Global test accuracy: 54.09
Round  96, Train loss: 0.167, Test loss: 0.533, Test accuracy: 83.67
Round  96, Global train loss: 0.167, Global test loss: 1.404, Global test accuracy: 59.79
Round  97, Train loss: 0.181, Test loss: 0.525, Test accuracy: 83.87
Round  97, Global train loss: 0.181, Global test loss: 1.170, Global test accuracy: 64.28
Round  98, Train loss: 0.151, Test loss: 0.492, Test accuracy: 84.88
Round  98, Global train loss: 0.151, Global test loss: 1.307, Global test accuracy: 60.93
Round  99, Train loss: 0.169, Test loss: 0.503, Test accuracy: 84.47
Round  99, Global train loss: 0.169, Global test loss: 1.583, Global test accuracy: 57.27
Final Round, Train loss: 0.153, Test loss: 0.544, Test accuracy: 84.89
Final Round, Global train loss: 0.153, Global test loss: 1.583, Global test accuracy: 57.27
Average accuracy final 10 rounds: 84.11666666666666 

Average global accuracy final 10 rounds: 60.1025 

1905.119427204132
[1.6085634231567383, 3.2171268463134766, 4.503911018371582, 5.7906951904296875, 7.10267448425293, 8.414653778076172, 9.690369367599487, 10.966084957122803, 12.268597841262817, 13.571110725402832, 14.872902870178223, 16.174695014953613, 17.48241353034973, 18.79013204574585, 20.108712434768677, 21.427292823791504, 22.745381355285645, 24.063469886779785, 25.366788148880005, 26.670106410980225, 27.965479373931885, 29.260852336883545, 30.611445426940918, 31.96203851699829, 33.25127816200256, 34.540517807006836, 35.8503737449646, 37.16022968292236, 38.455095052719116, 39.74996042251587, 41.04557180404663, 42.34118318557739, 43.617247343063354, 44.893311500549316, 46.26629400253296, 47.6392765045166, 48.91570687294006, 50.192137241363525, 51.505245208740234, 52.81835317611694, 54.12252616882324, 55.42669916152954, 56.722996950149536, 58.01929473876953, 59.33236026763916, 60.64542579650879, 61.94471073150635, 63.243995666503906, 64.64528179168701, 66.04656791687012, 67.33251786231995, 68.61846780776978, 69.92158269882202, 71.22469758987427, 72.56902956962585, 73.91336154937744, 75.21169805526733, 76.51003456115723, 77.80894875526428, 79.10786294937134, 80.41289758682251, 81.71793222427368, 83.02156090736389, 84.3251895904541, 85.62700176239014, 86.92881393432617, 88.2224235534668, 89.51603317260742, 90.81440567970276, 92.1127781867981, 93.40527844429016, 94.69777870178223, 95.99147582054138, 97.28517293930054, 98.68675804138184, 100.08834314346313, 101.35789513587952, 102.6274471282959, 103.93550705909729, 105.24356698989868, 106.59289455413818, 107.94222211837769, 109.2535252571106, 110.5648283958435, 111.87245488166809, 113.18008136749268, 114.49186515808105, 115.80364894866943, 117.1692762374878, 118.53490352630615, 119.82792139053345, 121.12093925476074, 122.44208025932312, 123.7632212638855, 125.06798458099365, 126.3727478981018, 127.68669843673706, 129.00064897537231, 130.3034679889679, 131.60628700256348, 132.91727805137634, 134.2282691001892, 135.52209949493408, 136.81592988967896, 138.107684135437, 139.39943838119507, 140.6880226135254, 141.9766068458557, 143.27538108825684, 144.57415533065796, 145.8792107105255, 147.18426609039307, 148.48142790794373, 149.77858972549438, 151.14598274230957, 152.51337575912476, 153.81392121315002, 155.1144666671753, 156.4098722934723, 157.7052779197693, 159.0361568927765, 160.3670358657837, 161.67878985404968, 162.99054384231567, 164.29810523986816, 165.60566663742065, 166.91795110702515, 168.23023557662964, 169.64788269996643, 171.06552982330322, 172.34441471099854, 173.62329959869385, 174.91923427581787, 176.2151689529419, 177.54229736328125, 178.8694257736206, 180.15598225593567, 181.44253873825073, 182.71631002426147, 183.99008131027222, 185.30171275138855, 186.61334419250488, 187.8864517211914, 189.15955924987793, 190.4570677280426, 191.75457620620728, 193.01713109016418, 194.2796859741211, 195.61878514289856, 196.95788431167603, 198.2515025138855, 199.54512071609497, 200.8409652709961, 202.13680982589722, 203.51638102531433, 204.89595222473145, 206.20151114463806, 207.50707006454468, 208.80001258850098, 210.09295511245728, 211.41015720367432, 212.72735929489136, 214.03504920005798, 215.3427391052246, 216.63959121704102, 217.93644332885742, 219.22239136695862, 220.50833940505981, 221.8324272632599, 223.15651512145996, 224.44298148155212, 225.7294478416443, 227.03747653961182, 228.34550523757935, 229.70121812820435, 231.05693101882935, 232.35870552062988, 233.66048002243042, 234.96522736549377, 236.26997470855713, 237.65643429756165, 239.04289388656616, 240.32172656059265, 241.60055923461914, 242.90604448318481, 244.2115297317505, 245.54521679878235, 246.8789038658142, 248.262855052948, 249.6468062400818, 250.94761443138123, 252.24842262268066, 253.55516934394836, 254.86191606521606, 256.21601486206055, 257.57011365890503, 258.8884975910187, 260.2068815231323, 261.51296830177307, 262.8190550804138, 265.03991293907166, 267.2607707977295]
[24.475, 24.475, 42.3, 42.3, 51.208333333333336, 51.208333333333336, 59.041666666666664, 59.041666666666664, 65.51666666666667, 65.51666666666667, 69.23333333333333, 69.23333333333333, 70.425, 70.425, 71.36666666666666, 71.36666666666666, 72.125, 72.125, 72.31666666666666, 72.31666666666666, 73.15833333333333, 73.15833333333333, 73.46666666666667, 73.46666666666667, 75.35833333333333, 75.35833333333333, 75.58333333333333, 75.58333333333333, 76.31666666666666, 76.31666666666666, 76.325, 76.325, 76.85833333333333, 76.85833333333333, 77.44166666666666, 77.44166666666666, 77.59166666666667, 77.59166666666667, 78.06666666666666, 78.06666666666666, 77.825, 77.825, 78.225, 78.225, 78.03333333333333, 78.03333333333333, 78.75, 78.75, 78.875, 78.875, 79.46666666666667, 79.46666666666667, 79.26666666666667, 79.26666666666667, 79.6, 79.6, 79.83333333333333, 79.83333333333333, 80.175, 80.175, 80.58333333333333, 80.58333333333333, 81.025, 81.025, 81.4, 81.4, 81.55, 81.55, 81.925, 81.925, 81.99166666666666, 81.99166666666666, 81.25833333333334, 81.25833333333334, 81.75, 81.75, 81.71666666666667, 81.71666666666667, 82.15833333333333, 82.15833333333333, 82.21666666666667, 82.21666666666667, 82.14166666666667, 82.14166666666667, 83.04166666666667, 83.04166666666667, 82.86666666666666, 82.86666666666666, 82.675, 82.675, 82.425, 82.425, 82.45833333333333, 82.45833333333333, 82.36666666666666, 82.36666666666666, 82.89166666666667, 82.89166666666667, 82.93333333333334, 82.93333333333334, 83.14166666666667, 83.14166666666667, 83.04166666666667, 83.04166666666667, 82.60833333333333, 82.60833333333333, 82.325, 82.325, 83.725, 83.725, 83.35, 83.35, 83.125, 83.125, 82.95, 82.95, 83.275, 83.275, 83.24166666666666, 83.24166666666666, 83.225, 83.225, 83.05833333333334, 83.05833333333334, 83.38333333333334, 83.38333333333334, 83.19166666666666, 83.19166666666666, 83.70833333333333, 83.70833333333333, 83.61666666666666, 83.61666666666666, 83.1, 83.1, 83.425, 83.425, 83.75833333333334, 83.75833333333334, 84.35833333333333, 84.35833333333333, 84.4, 84.4, 84.33333333333333, 84.33333333333333, 84.06666666666666, 84.06666666666666, 84.45, 84.45, 83.99166666666666, 83.99166666666666, 83.58333333333333, 83.58333333333333, 84.11666666666666, 84.11666666666666, 83.79166666666667, 83.79166666666667, 83.38333333333334, 83.38333333333334, 83.975, 83.975, 84.61666666666666, 84.61666666666666, 84.65, 84.65, 84.63333333333334, 84.63333333333334, 84.55833333333334, 84.55833333333334, 84.33333333333333, 84.33333333333333, 84.6, 84.6, 84.64166666666667, 84.64166666666667, 84.775, 84.775, 84.325, 84.325, 84.28333333333333, 84.28333333333333, 84.45, 84.45, 84.21666666666667, 84.21666666666667, 84.075, 84.075, 83.95833333333333, 83.95833333333333, 83.76666666666667, 83.76666666666667, 83.8, 83.8, 83.675, 83.675, 83.86666666666666, 83.86666666666666, 84.88333333333334, 84.88333333333334, 84.475, 84.475, 84.89166666666667, 84.89166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.439, Test loss: 2.183, Test accuracy: 20.07
Round   1, Train loss: 0.942, Test loss: 1.690, Test accuracy: 38.39
Round   2, Train loss: 0.864, Test loss: 1.820, Test accuracy: 42.81
Round   3, Train loss: 0.809, Test loss: 1.428, Test accuracy: 47.53
Round   4, Train loss: 0.769, Test loss: 1.285, Test accuracy: 56.02
Round   5, Train loss: 0.695, Test loss: 0.953, Test accuracy: 62.79
Round   6, Train loss: 0.731, Test loss: 0.746, Test accuracy: 68.22
Round   7, Train loss: 0.648, Test loss: 0.803, Test accuracy: 67.97
Round   8, Train loss: 0.667, Test loss: 0.734, Test accuracy: 68.77
Round   9, Train loss: 0.615, Test loss: 0.649, Test accuracy: 71.47
Round  10, Train loss: 0.582, Test loss: 0.619, Test accuracy: 72.85
Round  11, Train loss: 0.623, Test loss: 0.615, Test accuracy: 72.47
Round  12, Train loss: 0.616, Test loss: 0.612, Test accuracy: 73.15
Round  13, Train loss: 0.664, Test loss: 0.623, Test accuracy: 72.65
Round  14, Train loss: 0.561, Test loss: 0.599, Test accuracy: 73.54
Round  15, Train loss: 0.598, Test loss: 0.577, Test accuracy: 74.83
Round  16, Train loss: 0.509, Test loss: 0.574, Test accuracy: 75.12
Round  17, Train loss: 0.561, Test loss: 0.567, Test accuracy: 75.26
Round  18, Train loss: 0.613, Test loss: 0.563, Test accuracy: 75.86
Round  19, Train loss: 0.471, Test loss: 0.571, Test accuracy: 75.09
Round  20, Train loss: 0.529, Test loss: 0.538, Test accuracy: 76.73
Round  21, Train loss: 0.546, Test loss: 0.528, Test accuracy: 77.77
Round  22, Train loss: 0.463, Test loss: 0.526, Test accuracy: 77.44
Round  23, Train loss: 0.546, Test loss: 0.513, Test accuracy: 78.14
Round  24, Train loss: 0.494, Test loss: 0.510, Test accuracy: 78.09
Round  25, Train loss: 0.419, Test loss: 0.503, Test accuracy: 78.58
Round  26, Train loss: 0.485, Test loss: 0.507, Test accuracy: 78.33
Round  27, Train loss: 0.461, Test loss: 0.493, Test accuracy: 79.19
Round  28, Train loss: 0.481, Test loss: 0.499, Test accuracy: 78.78
Round  29, Train loss: 0.415, Test loss: 0.488, Test accuracy: 79.47
Round  30, Train loss: 0.511, Test loss: 0.487, Test accuracy: 79.55
Round  31, Train loss: 0.507, Test loss: 0.492, Test accuracy: 79.25
Round  32, Train loss: 0.437, Test loss: 0.480, Test accuracy: 79.83
Round  33, Train loss: 0.418, Test loss: 0.467, Test accuracy: 80.55
Round  34, Train loss: 0.604, Test loss: 0.466, Test accuracy: 80.63
Round  35, Train loss: 0.354, Test loss: 0.453, Test accuracy: 81.18
Round  36, Train loss: 0.528, Test loss: 0.454, Test accuracy: 80.97
Round  37, Train loss: 0.347, Test loss: 0.455, Test accuracy: 80.62
Round  38, Train loss: 0.326, Test loss: 0.445, Test accuracy: 81.54
Round  39, Train loss: 0.413, Test loss: 0.438, Test accuracy: 82.17
Round  40, Train loss: 0.418, Test loss: 0.440, Test accuracy: 81.88
Round  41, Train loss: 0.514, Test loss: 0.447, Test accuracy: 81.57
Round  42, Train loss: 0.536, Test loss: 0.439, Test accuracy: 81.70
Round  43, Train loss: 0.421, Test loss: 0.433, Test accuracy: 82.36
Round  44, Train loss: 0.391, Test loss: 0.437, Test accuracy: 82.03
Round  45, Train loss: 0.373, Test loss: 0.435, Test accuracy: 82.15
Round  46, Train loss: 0.312, Test loss: 0.441, Test accuracy: 82.04
Round  47, Train loss: 0.359, Test loss: 0.442, Test accuracy: 82.18
Round  48, Train loss: 0.408, Test loss: 0.436, Test accuracy: 82.51
Round  49, Train loss: 0.391, Test loss: 0.426, Test accuracy: 82.75
Round  50, Train loss: 0.350, Test loss: 0.427, Test accuracy: 82.75
Round  51, Train loss: 0.434, Test loss: 0.434, Test accuracy: 82.53
Round  52, Train loss: 0.363, Test loss: 0.427, Test accuracy: 82.77
Round  53, Train loss: 0.434, Test loss: 0.428, Test accuracy: 82.59
Round  54, Train loss: 0.326, Test loss: 0.419, Test accuracy: 83.20
Round  55, Train loss: 0.397, Test loss: 0.415, Test accuracy: 83.56
Round  56, Train loss: 0.379, Test loss: 0.428, Test accuracy: 82.76
Round  57, Train loss: 0.362, Test loss: 0.422, Test accuracy: 83.22
Round  58, Train loss: 0.395, Test loss: 0.434, Test accuracy: 82.91
Round  59, Train loss: 0.364, Test loss: 0.412, Test accuracy: 83.53
Round  60, Train loss: 0.331, Test loss: 0.404, Test accuracy: 83.78
Round  61, Train loss: 0.325, Test loss: 0.402, Test accuracy: 84.10
Round  62, Train loss: 0.312, Test loss: 0.405, Test accuracy: 83.92
Round  63, Train loss: 0.287, Test loss: 0.404, Test accuracy: 83.96
Round  64, Train loss: 0.242, Test loss: 0.403, Test accuracy: 83.95
Round  65, Train loss: 0.313, Test loss: 0.401, Test accuracy: 84.08
Round  66, Train loss: 0.319, Test loss: 0.404, Test accuracy: 83.87
Round  67, Train loss: 0.299, Test loss: 0.403, Test accuracy: 84.39
Round  68, Train loss: 0.349, Test loss: 0.401, Test accuracy: 84.24
Round  69, Train loss: 0.267, Test loss: 0.405, Test accuracy: 84.33
Round  70, Train loss: 0.258, Test loss: 0.409, Test accuracy: 84.11
Round  71, Train loss: 0.350, Test loss: 0.400, Test accuracy: 84.34
Round  72, Train loss: 0.229, Test loss: 0.401, Test accuracy: 84.48
Round  73, Train loss: 0.329, Test loss: 0.401, Test accuracy: 84.38
Round  74, Train loss: 0.229, Test loss: 0.412, Test accuracy: 84.03
Round  75, Train loss: 0.208, Test loss: 0.408, Test accuracy: 84.22
Round  76, Train loss: 0.338, Test loss: 0.407, Test accuracy: 84.17
Round  77, Train loss: 0.369, Test loss: 0.415, Test accuracy: 84.22
Round  78, Train loss: 0.323, Test loss: 0.408, Test accuracy: 84.48
Round  79, Train loss: 0.296, Test loss: 0.407, Test accuracy: 84.38
Round  80, Train loss: 0.340, Test loss: 0.409, Test accuracy: 84.45
Round  81, Train loss: 0.275, Test loss: 0.417, Test accuracy: 84.22
Round  82, Train loss: 0.212, Test loss: 0.414, Test accuracy: 84.12
Round  83, Train loss: 0.286, Test loss: 0.408, Test accuracy: 84.59
Round  84, Train loss: 0.344, Test loss: 0.412, Test accuracy: 84.31
Round  85, Train loss: 0.287, Test loss: 0.400, Test accuracy: 85.15
Round  86, Train loss: 0.210, Test loss: 0.397, Test accuracy: 84.93
Round  87, Train loss: 0.234, Test loss: 0.398, Test accuracy: 85.24
Round  88, Train loss: 0.323, Test loss: 0.402, Test accuracy: 84.92
Round  89, Train loss: 0.275, Test loss: 0.408, Test accuracy: 84.94
Round  90, Train loss: 0.318, Test loss: 0.410, Test accuracy: 84.42
Round  91, Train loss: 0.215, Test loss: 0.414, Test accuracy: 84.81
Round  92, Train loss: 0.178, Test loss: 0.406, Test accuracy: 84.94
Round  93, Train loss: 0.182, Test loss: 0.419, Test accuracy: 84.52
Round  94, Train loss: 0.224, Test loss: 0.413, Test accuracy: 84.50
Round  95, Train loss: 0.281, Test loss: 0.417, Test accuracy: 84.38
Round  96, Train loss: 0.236, Test loss: 0.409, Test accuracy: 84.53
Round  97, Train loss: 0.233, Test loss: 0.415, Test accuracy: 84.60
Round  98, Train loss: 0.197, Test loss: 0.408, Test accuracy: 84.90
Round  99, Train loss: 0.166, Test loss: 0.410, Test accuracy: 84.95
Final Round, Train loss: 0.209, Test loss: 0.416, Test accuracy: 85.10
Average accuracy final 10 rounds: 84.65333333333332 

1543.7497251033783
[1.5438807010650635, 3.087761402130127, 4.397630214691162, 5.707499027252197, 7.0666279792785645, 8.425756931304932, 9.76135778427124, 11.096958637237549, 12.465986728668213, 13.835014820098877, 15.208863258361816, 16.582711696624756, 17.94256854057312, 19.302425384521484, 20.654500007629395, 22.006574630737305, 23.38629937171936, 24.766024112701416, 26.150578260421753, 27.53513240814209, 28.866544723510742, 30.197957038879395, 31.551448345184326, 32.90493965148926, 34.25058698654175, 35.59623432159424, 36.96102499961853, 38.32581567764282, 39.65432906150818, 40.982842445373535, 42.321409463882446, 43.65997648239136, 45.01791858673096, 46.37586069107056, 47.738691329956055, 49.10152196884155, 50.42908334732056, 51.75664472579956, 53.09388613700867, 54.43112754821777, 55.86601376533508, 57.30089998245239, 58.76653504371643, 60.23217010498047, 61.57160568237305, 62.911041259765625, 64.25940203666687, 65.60776281356812, 66.96713876724243, 68.32651472091675, 69.70714521408081, 71.08777570724487, 72.33422017097473, 73.58066463470459, 74.92035222053528, 76.26003980636597, 77.69360828399658, 79.1271767616272, 80.57742667198181, 82.02767658233643, 83.37631559371948, 84.72495460510254, 86.07924127578735, 87.43352794647217, 88.84067273139954, 90.2478175163269, 91.63488984107971, 93.02196216583252, 94.36698484420776, 95.71200752258301, 97.07511138916016, 98.4382152557373, 99.81616425514221, 101.19411325454712, 102.57752847671509, 103.96094369888306, 105.22604393959045, 106.49114418029785, 107.8455924987793, 109.20004081726074, 110.47474956512451, 111.74945831298828, 113.18054795265198, 114.61163759231567, 115.98929905891418, 117.3669605255127, 118.60732817649841, 119.84769582748413, 121.08925676345825, 122.33081769943237, 123.7082302570343, 125.08564281463623, 126.44309902191162, 127.80055522918701, 129.03036332130432, 130.26017141342163, 131.50124716758728, 132.74232292175293, 134.10975790023804, 135.47719287872314, 136.83201432228088, 138.18683576583862, 139.44507789611816, 140.7033200263977, 141.93322348594666, 143.1631269454956, 144.41578269004822, 145.66843843460083, 147.040025472641, 148.41161251068115, 149.67949080467224, 150.94736909866333, 152.1940906047821, 153.44081211090088, 154.69718599319458, 155.95355987548828, 157.32991337776184, 158.7062668800354, 159.9636082649231, 161.2209496498108, 162.46104836463928, 163.70114707946777, 164.94628429412842, 166.19142150878906, 167.55905294418335, 168.92668437957764, 170.18748092651367, 171.4482774734497, 172.70396661758423, 173.95965576171875, 175.19895124435425, 176.43824672698975, 177.71609449386597, 178.9939422607422, 180.36247420310974, 181.7310061454773, 182.9891345500946, 184.2472629547119, 185.4939239025116, 186.74058485031128, 187.9915952682495, 189.24260568618774, 190.61032676696777, 191.9780478477478, 193.34630632400513, 194.71456480026245, 195.97194862365723, 197.229332447052, 198.4624960422516, 199.69565963745117, 200.9385633468628, 202.1814670562744, 203.44250750541687, 204.70354795455933, 205.9686999320984, 207.23385190963745, 208.48406553268433, 209.7342791557312, 210.98784375190735, 212.2414083480835, 213.49861097335815, 214.7558135986328, 216.0180788040161, 217.2803440093994, 218.5229992866516, 219.7656545639038, 221.02613019943237, 222.28660583496094, 223.59227061271667, 224.8979353904724, 226.15217351913452, 227.40641164779663, 228.6904058456421, 229.97440004348755, 231.22144842147827, 232.468496799469, 233.70620727539062, 234.94391775131226, 236.31538677215576, 237.68685579299927, 238.9507954120636, 240.21473503112793, 241.47211718559265, 242.72949934005737, 243.98435950279236, 245.23921966552734, 246.50448417663574, 247.76974868774414, 249.05372643470764, 250.33770418167114, 251.58804178237915, 252.83837938308716, 254.11583638191223, 255.3932933807373, 256.7206311225891, 258.0479688644409, 259.34332489967346, 260.638680934906, 261.8815050125122, 263.1243290901184, 265.1677224636078, 267.21111583709717]
[20.075, 20.075, 38.391666666666666, 38.391666666666666, 42.80833333333333, 42.80833333333333, 47.53333333333333, 47.53333333333333, 56.025, 56.025, 62.791666666666664, 62.791666666666664, 68.225, 68.225, 67.96666666666667, 67.96666666666667, 68.76666666666667, 68.76666666666667, 71.46666666666667, 71.46666666666667, 72.85, 72.85, 72.46666666666667, 72.46666666666667, 73.15, 73.15, 72.65, 72.65, 73.54166666666667, 73.54166666666667, 74.825, 74.825, 75.11666666666666, 75.11666666666666, 75.25833333333334, 75.25833333333334, 75.85833333333333, 75.85833333333333, 75.09166666666667, 75.09166666666667, 76.73333333333333, 76.73333333333333, 77.76666666666667, 77.76666666666667, 77.44166666666666, 77.44166666666666, 78.14166666666667, 78.14166666666667, 78.09166666666667, 78.09166666666667, 78.58333333333333, 78.58333333333333, 78.33333333333333, 78.33333333333333, 79.19166666666666, 79.19166666666666, 78.775, 78.775, 79.475, 79.475, 79.55, 79.55, 79.25, 79.25, 79.83333333333333, 79.83333333333333, 80.55, 80.55, 80.63333333333334, 80.63333333333334, 81.18333333333334, 81.18333333333334, 80.975, 80.975, 80.625, 80.625, 81.54166666666667, 81.54166666666667, 82.175, 82.175, 81.88333333333334, 81.88333333333334, 81.56666666666666, 81.56666666666666, 81.7, 81.7, 82.35833333333333, 82.35833333333333, 82.025, 82.025, 82.15, 82.15, 82.04166666666667, 82.04166666666667, 82.18333333333334, 82.18333333333334, 82.50833333333334, 82.50833333333334, 82.75, 82.75, 82.75, 82.75, 82.525, 82.525, 82.76666666666667, 82.76666666666667, 82.59166666666667, 82.59166666666667, 83.2, 83.2, 83.55833333333334, 83.55833333333334, 82.75833333333334, 82.75833333333334, 83.225, 83.225, 82.90833333333333, 82.90833333333333, 83.525, 83.525, 83.78333333333333, 83.78333333333333, 84.1, 84.1, 83.91666666666667, 83.91666666666667, 83.95833333333333, 83.95833333333333, 83.95, 83.95, 84.08333333333333, 84.08333333333333, 83.86666666666666, 83.86666666666666, 84.39166666666667, 84.39166666666667, 84.24166666666666, 84.24166666666666, 84.325, 84.325, 84.10833333333333, 84.10833333333333, 84.34166666666667, 84.34166666666667, 84.48333333333333, 84.48333333333333, 84.38333333333334, 84.38333333333334, 84.03333333333333, 84.03333333333333, 84.21666666666667, 84.21666666666667, 84.175, 84.175, 84.21666666666667, 84.21666666666667, 84.48333333333333, 84.48333333333333, 84.38333333333334, 84.38333333333334, 84.45, 84.45, 84.21666666666667, 84.21666666666667, 84.125, 84.125, 84.59166666666667, 84.59166666666667, 84.30833333333334, 84.30833333333334, 85.15, 85.15, 84.93333333333334, 84.93333333333334, 85.24166666666666, 85.24166666666666, 84.91666666666667, 84.91666666666667, 84.94166666666666, 84.94166666666666, 84.41666666666667, 84.41666666666667, 84.80833333333334, 84.80833333333334, 84.94166666666666, 84.94166666666666, 84.51666666666667, 84.51666666666667, 84.5, 84.5, 84.375, 84.375, 84.525, 84.525, 84.6, 84.6, 84.9, 84.9, 84.95, 84.95, 85.1, 85.1]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.541, Test loss: 1.976, Test accuracy: 24.65
Round   1, Train loss: 0.997, Test loss: 1.786, Test accuracy: 37.61
Round   2, Train loss: 0.894, Test loss: 1.379, Test accuracy: 47.23
Round   3, Train loss: 0.760, Test loss: 1.309, Test accuracy: 53.40
Round   4, Train loss: 0.728, Test loss: 1.181, Test accuracy: 58.77
Round   5, Train loss: 0.718, Test loss: 1.169, Test accuracy: 54.16
Round   6, Train loss: 0.766, Test loss: 1.046, Test accuracy: 60.13
Round   7, Train loss: 0.689, Test loss: 0.769, Test accuracy: 70.52
Round   8, Train loss: 0.543, Test loss: 0.774, Test accuracy: 70.97
Round   9, Train loss: 0.676, Test loss: 0.850, Test accuracy: 67.32
Round  10, Train loss: 0.636, Test loss: 0.785, Test accuracy: 69.97
Round  11, Train loss: 0.555, Test loss: 0.655, Test accuracy: 73.19
Round  12, Train loss: 0.661, Test loss: 0.675, Test accuracy: 74.49
Round  13, Train loss: 0.572, Test loss: 0.681, Test accuracy: 73.65
Round  14, Train loss: 0.535, Test loss: 0.628, Test accuracy: 75.23
Round  15, Train loss: 0.618, Test loss: 0.602, Test accuracy: 76.72
Round  16, Train loss: 0.575, Test loss: 0.633, Test accuracy: 74.28
Round  17, Train loss: 0.552, Test loss: 0.543, Test accuracy: 78.25
Round  18, Train loss: 0.544, Test loss: 0.535, Test accuracy: 78.58
Round  19, Train loss: 0.477, Test loss: 0.535, Test accuracy: 78.40
Round  20, Train loss: 0.625, Test loss: 0.537, Test accuracy: 78.62
Round  21, Train loss: 0.464, Test loss: 0.537, Test accuracy: 78.65
Round  22, Train loss: 0.569, Test loss: 0.520, Test accuracy: 79.22
Round  23, Train loss: 0.481, Test loss: 0.507, Test accuracy: 79.62
Round  24, Train loss: 0.420, Test loss: 0.514, Test accuracy: 79.08
Round  25, Train loss: 0.630, Test loss: 0.498, Test accuracy: 80.30
Round  26, Train loss: 0.418, Test loss: 0.490, Test accuracy: 79.85
Round  27, Train loss: 0.378, Test loss: 0.485, Test accuracy: 80.45
Round  28, Train loss: 0.536, Test loss: 0.494, Test accuracy: 80.97
Round  29, Train loss: 0.460, Test loss: 0.482, Test accuracy: 80.67
Round  30, Train loss: 0.491, Test loss: 0.464, Test accuracy: 81.24
Round  31, Train loss: 0.472, Test loss: 0.476, Test accuracy: 81.38
Round  32, Train loss: 0.491, Test loss: 0.481, Test accuracy: 80.69
Round  33, Train loss: 0.608, Test loss: 0.471, Test accuracy: 81.38
Round  34, Train loss: 0.481, Test loss: 0.467, Test accuracy: 81.47
Round  35, Train loss: 0.410, Test loss: 0.463, Test accuracy: 81.64
Round  36, Train loss: 0.424, Test loss: 0.455, Test accuracy: 81.78
Round  37, Train loss: 0.400, Test loss: 0.455, Test accuracy: 81.72
Round  38, Train loss: 0.357, Test loss: 0.452, Test accuracy: 81.73
Round  39, Train loss: 0.396, Test loss: 0.451, Test accuracy: 81.93
Round  40, Train loss: 0.403, Test loss: 0.447, Test accuracy: 82.14
Round  41, Train loss: 0.484, Test loss: 0.440, Test accuracy: 82.38
Round  42, Train loss: 0.321, Test loss: 0.436, Test accuracy: 82.36
Round  43, Train loss: 0.387, Test loss: 0.431, Test accuracy: 82.81
Round  44, Train loss: 0.330, Test loss: 0.429, Test accuracy: 82.62
Round  45, Train loss: 0.304, Test loss: 0.427, Test accuracy: 82.80
Round  46, Train loss: 0.403, Test loss: 0.426, Test accuracy: 82.95
Round  47, Train loss: 0.302, Test loss: 0.428, Test accuracy: 82.95
Round  48, Train loss: 0.286, Test loss: 0.423, Test accuracy: 83.08
Round  49, Train loss: 0.327, Test loss: 0.423, Test accuracy: 83.03
Round  50, Train loss: 0.360, Test loss: 0.418, Test accuracy: 83.23
Round  51, Train loss: 0.328, Test loss: 0.416, Test accuracy: 83.33
Round  52, Train loss: 0.342, Test loss: 0.420, Test accuracy: 83.11
Round  53, Train loss: 0.375, Test loss: 0.417, Test accuracy: 83.38
Round  54, Train loss: 0.447, Test loss: 0.413, Test accuracy: 83.35
Round  55, Train loss: 0.400, Test loss: 0.415, Test accuracy: 83.31
Round  56, Train loss: 0.333, Test loss: 0.414, Test accuracy: 83.71
Round  57, Train loss: 0.377, Test loss: 0.414, Test accuracy: 83.44
Round  58, Train loss: 0.270, Test loss: 0.404, Test accuracy: 83.68
Round  59, Train loss: 0.253, Test loss: 0.408, Test accuracy: 83.39
Round  60, Train loss: 0.227, Test loss: 0.403, Test accuracy: 84.05
Round  61, Train loss: 0.276, Test loss: 0.407, Test accuracy: 83.72
Round  62, Train loss: 0.368, Test loss: 0.407, Test accuracy: 83.92
Round  63, Train loss: 0.310, Test loss: 0.401, Test accuracy: 84.01
Round  64, Train loss: 0.348, Test loss: 0.399, Test accuracy: 83.96
Round  65, Train loss: 0.322, Test loss: 0.399, Test accuracy: 84.20
Round  66, Train loss: 0.318, Test loss: 0.399, Test accuracy: 84.26
Round  67, Train loss: 0.264, Test loss: 0.398, Test accuracy: 84.33
Round  68, Train loss: 0.271, Test loss: 0.402, Test accuracy: 83.92
Round  69, Train loss: 0.225, Test loss: 0.398, Test accuracy: 84.21
Round  70, Train loss: 0.373, Test loss: 0.398, Test accuracy: 84.10
Round  71, Train loss: 0.298, Test loss: 0.394, Test accuracy: 84.28
Round  72, Train loss: 0.288, Test loss: 0.395, Test accuracy: 84.22
Round  73, Train loss: 0.301, Test loss: 0.399, Test accuracy: 83.97
Round  74, Train loss: 0.243, Test loss: 0.392, Test accuracy: 84.47
Round  75, Train loss: 0.214, Test loss: 0.387, Test accuracy: 84.62
Round  76, Train loss: 0.339, Test loss: 0.391, Test accuracy: 84.49
Round  77, Train loss: 0.284, Test loss: 0.388, Test accuracy: 84.70
Round  78, Train loss: 0.322, Test loss: 0.386, Test accuracy: 84.77
Round  79, Train loss: 0.276, Test loss: 0.392, Test accuracy: 84.62
Round  80, Train loss: 0.337, Test loss: 0.390, Test accuracy: 84.76
Round  81, Train loss: 0.316, Test loss: 0.392, Test accuracy: 84.58
Round  82, Train loss: 0.324, Test loss: 0.390, Test accuracy: 84.65
Round  83, Train loss: 0.227, Test loss: 0.388, Test accuracy: 84.66
Round  84, Train loss: 0.304, Test loss: 0.391, Test accuracy: 84.51
Round  85, Train loss: 0.213, Test loss: 0.382, Test accuracy: 84.84
Round  86, Train loss: 0.196, Test loss: 0.390, Test accuracy: 84.75
Round  87, Train loss: 0.216, Test loss: 0.386, Test accuracy: 84.82
Round  88, Train loss: 0.243, Test loss: 0.383, Test accuracy: 84.76
Round  89, Train loss: 0.235, Test loss: 0.383, Test accuracy: 84.97
Round  90, Train loss: 0.212, Test loss: 0.390, Test accuracy: 84.64
Round  91, Train loss: 0.220, Test loss: 0.382, Test accuracy: 85.08
Round  92, Train loss: 0.253, Test loss: 0.378, Test accuracy: 85.07
Round  93, Train loss: 0.208, Test loss: 0.377, Test accuracy: 85.40
Round  94, Train loss: 0.238, Test loss: 0.386, Test accuracy: 85.02
Round  95, Train loss: 0.245, Test loss: 0.386, Test accuracy: 85.06
Round  96, Train loss: 0.193, Test loss: 0.385, Test accuracy: 85.03
Round  97, Train loss: 0.264, Test loss: 0.391, Test accuracy: 84.99
Round  98, Train loss: 0.346, Test loss: 0.387, Test accuracy: 85.09
Round  99, Train loss: 0.259, Test loss: 0.380, Test accuracy: 85.41
Final Round, Train loss: 0.200, Test loss: 0.379, Test accuracy: 85.37
Average accuracy final 10 rounds: 85.07750000000001
1816.0598964691162
[2.373826503753662, 4.747653007507324, 6.469630479812622, 8.19160795211792, 9.92484712600708, 11.65808629989624, 13.400260210037231, 15.142434120178223, 16.9374577999115, 18.732481479644775, 20.452951908111572, 22.17342233657837, 23.909005641937256, 25.644588947296143, 27.42357850074768, 29.20256805419922, 31.00486731529236, 32.8071665763855, 34.56989526748657, 36.33262395858765, 38.08982825279236, 39.84703254699707, 41.67127466201782, 43.495516777038574, 45.40119791030884, 47.3068790435791, 49.15677046775818, 51.006661891937256, 52.90849161148071, 54.81032133102417, 56.706599712371826, 58.60287809371948, 60.460052728652954, 62.317227363586426, 64.1450424194336, 65.97285747528076, 67.799143075943, 69.62542867660522, 71.36532831192017, 73.10522794723511, 74.91861820220947, 76.73200845718384, 78.55068135261536, 80.36935424804688, 82.18266439437866, 83.99597454071045, 85.93506669998169, 87.87415885925293, 89.78212141990662, 91.6900839805603, 93.40221834182739, 95.11435270309448, 96.837331533432, 98.56031036376953, 100.2882490158081, 102.01618766784668, 103.71947145462036, 105.42275524139404, 107.11515665054321, 108.80755805969238, 110.5096504688263, 112.2117428779602, 113.95282912254333, 115.69391536712646, 117.42559719085693, 119.1572790145874, 120.86546850204468, 122.57365798950195, 124.269451379776, 125.96524477005005, 127.70081615447998, 129.4363875389099, 131.15577483177185, 132.8751621246338, 134.60045170783997, 136.32574129104614, 138.03803539276123, 139.75032949447632, 141.46263551712036, 143.1749415397644, 144.87760591506958, 146.58027029037476, 148.28891038894653, 149.9975504875183, 151.72064304351807, 153.44373559951782, 155.14100337028503, 156.83827114105225, 158.53141021728516, 160.22454929351807, 161.93832969665527, 163.65211009979248, 165.38915467262268, 167.12619924545288, 168.84357953071594, 170.560959815979, 172.24601459503174, 173.93106937408447, 175.64761304855347, 177.36415672302246, 179.08360624313354, 180.80305576324463, 182.50693583488464, 184.21081590652466, 185.9101026058197, 187.60938930511475, 189.32063627243042, 191.0318832397461, 192.74379086494446, 194.45569849014282, 196.12832856178284, 197.80095863342285, 199.49046087265015, 201.17996311187744, 202.86103892326355, 204.54211473464966, 206.2371621131897, 207.93220949172974, 209.64321517944336, 211.35422086715698, 213.04930543899536, 214.74439001083374, 216.4418168067932, 218.13924360275269, 219.8311789035797, 221.52311420440674, 223.2040569782257, 224.88499975204468, 226.58393359184265, 228.28286743164062, 229.997784614563, 231.71270179748535, 233.40505623817444, 235.09741067886353, 236.775808095932, 238.4542055130005, 240.10789561271667, 241.76158571243286, 243.4538061618805, 245.14602661132812, 246.83084082603455, 248.51565504074097, 250.19552040100098, 251.875385761261, 253.57564616203308, 255.27590656280518, 256.9375240802765, 258.5991415977478, 260.4599070549011, 262.32067251205444, 264.01599621772766, 265.7113199234009, 267.39560413360596, 269.07988834381104, 270.75575280189514, 272.43161725997925, 274.1397511959076, 275.84788513183594, 277.55178141593933, 279.2556777000427, 280.94136595726013, 282.62705421447754, 284.3012681007385, 285.9754819869995, 287.6824460029602, 289.3894100189209, 291.09296703338623, 292.79652404785156, 294.5704960823059, 296.34446811676025, 298.055401802063, 299.7663354873657, 301.4520125389099, 303.1376895904541, 304.814327955246, 306.49096632003784, 308.1805033683777, 309.87004041671753, 311.5566051006317, 313.2431697845459, 314.94372844696045, 316.644287109375, 318.3543846607208, 320.06448221206665, 321.7589690685272, 323.4534559249878, 325.14873456954956, 326.8440132141113, 328.5501492023468, 330.2562851905823, 331.9643108844757, 333.67233657836914, 335.35538601875305, 337.03843545913696, 338.72717475891113, 340.4159140586853, 342.0957410335541, 343.77556800842285, 345.46502709388733, 347.1544861793518, 349.39265489578247, 351.63082361221313]
[24.65, 24.65, 37.608333333333334, 37.608333333333334, 47.233333333333334, 47.233333333333334, 53.4, 53.4, 58.766666666666666, 58.766666666666666, 54.15833333333333, 54.15833333333333, 60.13333333333333, 60.13333333333333, 70.51666666666667, 70.51666666666667, 70.96666666666667, 70.96666666666667, 67.31666666666666, 67.31666666666666, 69.975, 69.975, 73.19166666666666, 73.19166666666666, 74.49166666666666, 74.49166666666666, 73.65, 73.65, 75.23333333333333, 75.23333333333333, 76.71666666666667, 76.71666666666667, 74.275, 74.275, 78.25, 78.25, 78.58333333333333, 78.58333333333333, 78.4, 78.4, 78.625, 78.625, 78.65, 78.65, 79.21666666666667, 79.21666666666667, 79.625, 79.625, 79.08333333333333, 79.08333333333333, 80.3, 80.3, 79.85, 79.85, 80.45, 80.45, 80.975, 80.975, 80.66666666666667, 80.66666666666667, 81.24166666666666, 81.24166666666666, 81.38333333333334, 81.38333333333334, 80.69166666666666, 80.69166666666666, 81.38333333333334, 81.38333333333334, 81.46666666666667, 81.46666666666667, 81.64166666666667, 81.64166666666667, 81.775, 81.775, 81.71666666666667, 81.71666666666667, 81.73333333333333, 81.73333333333333, 81.93333333333334, 81.93333333333334, 82.14166666666667, 82.14166666666667, 82.38333333333334, 82.38333333333334, 82.35833333333333, 82.35833333333333, 82.80833333333334, 82.80833333333334, 82.625, 82.625, 82.8, 82.8, 82.95, 82.95, 82.95, 82.95, 83.08333333333333, 83.08333333333333, 83.025, 83.025, 83.23333333333333, 83.23333333333333, 83.325, 83.325, 83.10833333333333, 83.10833333333333, 83.38333333333334, 83.38333333333334, 83.35, 83.35, 83.30833333333334, 83.30833333333334, 83.70833333333333, 83.70833333333333, 83.44166666666666, 83.44166666666666, 83.68333333333334, 83.68333333333334, 83.39166666666667, 83.39166666666667, 84.05, 84.05, 83.725, 83.725, 83.91666666666667, 83.91666666666667, 84.00833333333334, 84.00833333333334, 83.95833333333333, 83.95833333333333, 84.2, 84.2, 84.25833333333334, 84.25833333333334, 84.325, 84.325, 83.91666666666667, 83.91666666666667, 84.20833333333333, 84.20833333333333, 84.1, 84.1, 84.28333333333333, 84.28333333333333, 84.21666666666667, 84.21666666666667, 83.975, 83.975, 84.475, 84.475, 84.61666666666666, 84.61666666666666, 84.49166666666666, 84.49166666666666, 84.7, 84.7, 84.76666666666667, 84.76666666666667, 84.625, 84.625, 84.75833333333334, 84.75833333333334, 84.58333333333333, 84.58333333333333, 84.65, 84.65, 84.65833333333333, 84.65833333333333, 84.50833333333334, 84.50833333333334, 84.84166666666667, 84.84166666666667, 84.75, 84.75, 84.81666666666666, 84.81666666666666, 84.75833333333334, 84.75833333333334, 84.96666666666667, 84.96666666666667, 84.64166666666667, 84.64166666666667, 85.075, 85.075, 85.06666666666666, 85.06666666666666, 85.4, 85.4, 85.01666666666667, 85.01666666666667, 85.05833333333334, 85.05833333333334, 85.025, 85.025, 84.99166666666666, 84.99166666666666, 85.09166666666667, 85.09166666666667, 85.40833333333333, 85.40833333333333, 85.36666666666666, 85.36666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.173, Test loss: 2.228, Test accuracy: 14.20
Round   1, Train loss: 0.995, Test loss: 2.557, Test accuracy: 19.93
Round   2, Train loss: 0.887, Test loss: 2.121, Test accuracy: 27.34
Round   3, Train loss: 0.809, Test loss: 2.319, Test accuracy: 24.80
Round   4, Train loss: 0.726, Test loss: 2.144, Test accuracy: 29.97
Round   5, Train loss: 0.813, Test loss: 1.852, Test accuracy: 36.27
Round   6, Train loss: 0.610, Test loss: 2.076, Test accuracy: 36.67
Round   7, Train loss: 0.585, Test loss: 1.761, Test accuracy: 44.72
Round   8, Train loss: 0.558, Test loss: 2.009, Test accuracy: 38.27
Round   9, Train loss: 0.571, Test loss: 1.648, Test accuracy: 46.53
Round  10, Train loss: 0.593, Test loss: 1.613, Test accuracy: 44.68
Round  11, Train loss: 0.501, Test loss: 1.819, Test accuracy: 44.84
Round  12, Train loss: 0.733, Test loss: 1.782, Test accuracy: 36.74
Round  13, Train loss: 0.539, Test loss: 1.582, Test accuracy: 46.07
Round  14, Train loss: 0.562, Test loss: 1.562, Test accuracy: 45.41
Round  15, Train loss: 0.680, Test loss: 1.408, Test accuracy: 49.76
Round  16, Train loss: 0.476, Test loss: 1.460, Test accuracy: 51.82
Round  17, Train loss: 0.571, Test loss: 1.551, Test accuracy: 46.18
Round  18, Train loss: 0.585, Test loss: 1.856, Test accuracy: 40.41
Round  19, Train loss: 0.491, Test loss: 1.788, Test accuracy: 44.17
Round  20, Train loss: 0.493, Test loss: 1.555, Test accuracy: 47.39
Round  21, Train loss: 0.407, Test loss: 1.728, Test accuracy: 45.12
Round  22, Train loss: 0.402, Test loss: 1.501, Test accuracy: 52.31
Round  23, Train loss: 0.478, Test loss: 1.649, Test accuracy: 48.01
Round  24, Train loss: 0.476, Test loss: 1.655, Test accuracy: 45.09
Round  25, Train loss: 0.507, Test loss: 1.474, Test accuracy: 50.69
Round  26, Train loss: 0.379, Test loss: 1.409, Test accuracy: 53.15
Round  27, Train loss: 0.436, Test loss: 1.619, Test accuracy: 48.27
Round  28, Train loss: 0.443, Test loss: 1.408, Test accuracy: 51.69
Round  29, Train loss: 0.447, Test loss: 1.449, Test accuracy: 50.20
Round  30, Train loss: 0.405, Test loss: 1.302, Test accuracy: 54.65
Round  31, Train loss: 0.300, Test loss: 1.693, Test accuracy: 49.68
Round  32, Train loss: 0.394, Test loss: 1.262, Test accuracy: 57.20
Round  33, Train loss: 0.338, Test loss: 1.367, Test accuracy: 53.98
Round  34, Train loss: 0.340, Test loss: 1.627, Test accuracy: 49.67
Round  35, Train loss: 0.364, Test loss: 1.315, Test accuracy: 55.95
Round  36, Train loss: 0.388, Test loss: 1.352, Test accuracy: 54.86
Round  37, Train loss: 0.504, Test loss: 1.218, Test accuracy: 57.40
Round  38, Train loss: 0.533, Test loss: 1.354, Test accuracy: 52.19
Round  39, Train loss: 0.398, Test loss: 1.436, Test accuracy: 51.90
Round  40, Train loss: 0.365, Test loss: 1.823, Test accuracy: 48.19
Round  41, Train loss: 0.317, Test loss: 1.321, Test accuracy: 57.14
Round  42, Train loss: 0.355, Test loss: 1.730, Test accuracy: 48.31
Round  43, Train loss: 0.366, Test loss: 1.403, Test accuracy: 56.04
Round  44, Train loss: 0.335, Test loss: 1.099, Test accuracy: 62.47
Round  45, Train loss: 0.379, Test loss: 1.335, Test accuracy: 55.75
Round  46, Train loss: 0.354, Test loss: 1.337, Test accuracy: 55.67
Round  47, Train loss: 0.304, Test loss: 1.192, Test accuracy: 60.21
Round  48, Train loss: 0.325, Test loss: 1.308, Test accuracy: 58.21
Round  49, Train loss: 0.326, Test loss: 1.263, Test accuracy: 58.23
Round  50, Train loss: 0.318, Test loss: 1.511, Test accuracy: 54.38
Round  51, Train loss: 0.341, Test loss: 1.209, Test accuracy: 60.69
Round  52, Train loss: 0.280, Test loss: 1.281, Test accuracy: 58.15
Round  53, Train loss: 0.351, Test loss: 1.476, Test accuracy: 55.77
Round  54, Train loss: 0.302, Test loss: 1.170, Test accuracy: 61.92
Round  55, Train loss: 0.320, Test loss: 1.223, Test accuracy: 59.84
Round  56, Train loss: 0.281, Test loss: 1.192, Test accuracy: 61.66
Round  57, Train loss: 0.307, Test loss: 1.319, Test accuracy: 56.11
Round  58, Train loss: 0.213, Test loss: 1.504, Test accuracy: 56.05
Round  59, Train loss: 0.243, Test loss: 1.257, Test accuracy: 60.71
Round  60, Train loss: 0.190, Test loss: 1.417, Test accuracy: 60.17
Round  61, Train loss: 0.274, Test loss: 1.591, Test accuracy: 53.33
Round  62, Train loss: 0.266, Test loss: 1.233, Test accuracy: 60.87
Round  63, Train loss: 0.336, Test loss: 1.387, Test accuracy: 55.03
Round  64, Train loss: 0.259, Test loss: 1.544, Test accuracy: 55.28
Round  65, Train loss: 0.195, Test loss: 1.256, Test accuracy: 60.27
Round  66, Train loss: 0.189, Test loss: 1.577, Test accuracy: 56.34
Round  67, Train loss: 0.310, Test loss: 1.303, Test accuracy: 59.64
Round  68, Train loss: 0.233, Test loss: 1.303, Test accuracy: 59.12
Round  69, Train loss: 0.283, Test loss: 1.386, Test accuracy: 56.42
Round  70, Train loss: 0.194, Test loss: 1.441, Test accuracy: 57.73
Round  71, Train loss: 0.290, Test loss: 1.414, Test accuracy: 56.98
Round  72, Train loss: 0.199, Test loss: 1.458, Test accuracy: 58.87
Round  73, Train loss: 0.192, Test loss: 1.302, Test accuracy: 59.84
Round  74, Train loss: 0.195, Test loss: 1.554, Test accuracy: 55.06
Round  75, Train loss: 0.152, Test loss: 1.296, Test accuracy: 60.69
Round  76, Train loss: 0.166, Test loss: 1.480, Test accuracy: 57.07
Round  77, Train loss: 0.298, Test loss: 1.410, Test accuracy: 57.83
Round  78, Train loss: 0.305, Test loss: 1.451, Test accuracy: 53.92
Round  79, Train loss: 0.206, Test loss: 1.406, Test accuracy: 56.26
Round  80, Train loss: 0.198, Test loss: 1.308, Test accuracy: 59.00
Round  81, Train loss: 0.230, Test loss: 1.703, Test accuracy: 54.02
Round  82, Train loss: 0.243, Test loss: 1.485, Test accuracy: 53.48
Round  83, Train loss: 0.177, Test loss: 1.472, Test accuracy: 58.65
Round  84, Train loss: 0.158, Test loss: 1.309, Test accuracy: 63.04
Round  85, Train loss: 0.122, Test loss: 1.570, Test accuracy: 60.48
Round  86, Train loss: 0.217, Test loss: 1.298, Test accuracy: 61.77
Round  87, Train loss: 0.224, Test loss: 1.236, Test accuracy: 61.17
Round  88, Train loss: 0.196, Test loss: 1.502, Test accuracy: 57.79
Round  89, Train loss: 0.224, Test loss: 1.291, Test accuracy: 62.44
Round  90, Train loss: 0.153, Test loss: 1.511, Test accuracy: 58.19
Round  91, Train loss: 0.113, Test loss: 1.331, Test accuracy: 61.77
Round  92, Train loss: 0.181, Test loss: 1.288, Test accuracy: 62.55
Round  93, Train loss: 0.159, Test loss: 1.534, Test accuracy: 58.24
Round  94, Train loss: 0.133, Test loss: 1.164, Test accuracy: 64.25
Round  95, Train loss: 0.105, Test loss: 1.167, Test accuracy: 63.80
Round  96, Train loss: 0.164, Test loss: 1.590, Test accuracy: 56.75
Round  97, Train loss: 0.149, Test loss: 1.227, Test accuracy: 62.60
Round  98, Train loss: 0.173, Test loss: 1.390, Test accuracy: 58.96
Round  99, Train loss: 0.214, Test loss: 1.209, Test accuracy: 62.30
Final Round, Train loss: 0.167, Test loss: 1.114, Test accuracy: 65.44
Average accuracy final 10 rounds: 60.94166666666666
2800.725564479828
[4.3751161098480225, 8.004260540008545, 11.654547929763794, 15.517273664474487, 19.315839052200317, 22.953577995300293, 26.58871054649353, 30.1916766166687, 33.83795380592346, 37.45175290107727, 41.54574728012085, 45.592167139053345, 49.604750633239746, 53.18275761604309, 56.74801230430603, 60.29404878616333, 63.87168025970459, 67.49481797218323, 71.11077094078064, 74.71745991706848, 78.61312627792358, 82.19401240348816, 85.78020644187927, 89.35289883613586, 92.95435380935669, 96.48284196853638, 100.06105399131775, 104.17793273925781, 108.30160403251648, 112.38655948638916, 116.49518632888794, 120.59565353393555, 124.73753142356873, 128.82989168167114, 132.94997262954712, 137.03754925727844, 141.11820888519287, 145.21226263046265, 149.3424789905548, 153.49362802505493, 157.6219744682312, 161.75374174118042, 165.90414261817932, 170.06974530220032, 174.21719479560852, 178.35517191886902, 182.47542691230774, 186.60556769371033, 190.7527301311493, 194.87582993507385, 198.9957435131073, 203.1350393295288, 207.2463653087616, 211.3550353050232, 215.48352026939392, 219.60563802719116, 223.7061471939087, 227.82153511047363, 231.8963520526886, 236.01227116584778, 240.1283836364746, 244.2431788444519, 248.33798170089722, 252.4394588470459, 256.5493242740631, 260.53080129623413, 264.6232690811157, 268.76072430610657, 272.90920758247375, 277.043879032135, 281.1712691783905, 285.3162844181061, 289.4391174316406, 293.55321764945984, 297.654447555542, 301.75610184669495, 305.8890047073364, 310.07024025917053, 314.2296154499054, 318.3588161468506, 322.53275966644287, 326.7187337875366, 330.8722848892212, 335.06172609329224, 339.2216775417328, 343.3512809276581, 347.5163493156433, 351.678751707077, 355.8703625202179, 360.02046060562134, 364.201274394989, 368.41358494758606, 372.6067531108856, 376.7552909851074, 380.89350414276123, 385.02641248703003, 389.16195034980774, 393.2746834754944, 397.39807415008545, 401.51925349235535, 404.96522545814514]
[14.2, 19.925, 27.341666666666665, 24.8, 29.966666666666665, 36.275, 36.666666666666664, 44.71666666666667, 38.275, 46.53333333333333, 44.68333333333333, 44.84166666666667, 36.74166666666667, 46.06666666666667, 45.40833333333333, 49.75833333333333, 51.81666666666667, 46.18333333333333, 40.40833333333333, 44.166666666666664, 47.391666666666666, 45.125, 52.30833333333333, 48.00833333333333, 45.09166666666667, 50.69166666666667, 53.15, 48.266666666666666, 51.69166666666667, 50.2, 54.65, 49.68333333333333, 57.2, 53.983333333333334, 49.666666666666664, 55.95, 54.858333333333334, 57.4, 52.19166666666667, 51.9, 48.19166666666667, 57.141666666666666, 48.30833333333333, 56.041666666666664, 62.46666666666667, 55.75, 55.675, 60.208333333333336, 58.208333333333336, 58.225, 54.38333333333333, 60.69166666666667, 58.15, 55.766666666666666, 61.916666666666664, 59.84166666666667, 61.65833333333333, 56.108333333333334, 56.05, 60.708333333333336, 60.166666666666664, 53.333333333333336, 60.86666666666667, 55.03333333333333, 55.28333333333333, 60.275, 56.34166666666667, 59.641666666666666, 59.11666666666667, 56.425, 57.733333333333334, 56.983333333333334, 58.86666666666667, 59.84166666666667, 55.05833333333333, 60.69166666666667, 57.06666666666667, 57.833333333333336, 53.916666666666664, 56.25833333333333, 59.0, 54.025, 53.475, 58.65, 63.041666666666664, 60.483333333333334, 61.766666666666666, 61.166666666666664, 57.791666666666664, 62.44166666666667, 58.19166666666667, 61.775, 62.55, 58.24166666666667, 64.25, 63.8, 56.75, 62.6, 58.958333333333336, 62.3, 65.44166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.260, Test loss: 2.285, Test accuracy: 7.62
Round   0, Global train loss: 2.260, Global test loss: 2.287, Global test accuracy: 7.27
Round   1, Train loss: 2.256, Test loss: 2.281, Test accuracy: 8.37
Round   1, Global train loss: 2.256, Global test loss: 2.285, Global test accuracy: 7.39
Round   2, Train loss: 2.303, Test loss: 2.286, Test accuracy: 7.47
Round   2, Global train loss: 2.303, Global test loss: 2.286, Global test accuracy: 6.70
Round   3, Train loss: nan, Test loss: nan, Test accuracy: 8.98
Round   3, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 6.53
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 6.47
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 9.74
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 100, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 101, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 102, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 103, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 104, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 105, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 106, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 107, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 108, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 109, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 110, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 111, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 112, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 113, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 114, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 115, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 116, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 117, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 118, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 119, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 120, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 121, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 122, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 123, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 124, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 125, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 126, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 127, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 128, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 129, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 130, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 131, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 132, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 133, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 134, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Average accuracy final 10 rounds: 11.666666666666663 

Average global accuracy final 10 rounds: 11.666666666666663 

4709.9998915195465
[1.8657188415527344, 3.4450607299804688, 5.02601957321167, 6.636572360992432, 8.299960136413574, 9.888696193695068, 11.469542503356934, 13.079919815063477, 14.67038083076477, 16.27389144897461, 17.828507900238037, 19.40765404701233, 20.975961685180664, 22.56899380683899, 24.163869857788086, 25.749095916748047, 27.337616443634033, 28.92098307609558, 30.507192134857178, 32.09300518035889, 33.668455839157104, 35.25545954704285, 36.84385919570923, 38.28343057632446, 39.67425012588501, 41.08486747741699, 42.4799907207489, 43.87095355987549, 45.271034717559814, 46.67695093154907, 48.09316062927246, 49.4905047416687, 50.887027978897095, 52.275803089141846, 53.670878410339355, 55.07433223724365, 56.469404458999634, 57.874738931655884, 59.2790412902832, 60.68076944351196, 62.074716567993164, 63.466028928756714, 64.8528208732605, 66.24996519088745, 67.64124631881714, 69.04895544052124, 70.44212555885315, 71.83208537101746, 73.2295572757721, 74.61792540550232, 76.00924682617188, 77.41209936141968, 78.80862307548523, 80.25722765922546, 81.65221238136292, 83.05079388618469, 84.45940279960632, 85.86225962638855, 87.25054597854614, 88.64374589920044, 90.03919458389282, 91.42210483551025, 92.81800174713135, 94.21650314331055, 95.6087441444397, 97.00697994232178, 98.39723253250122, 99.77676463127136, 101.16588091850281, 102.55182099342346, 103.93183445930481, 105.33089017868042, 106.73396348953247, 108.12778568267822, 109.52253723144531, 110.9344744682312, 112.33805561065674, 113.71793150901794, 115.10646986961365, 116.49647116661072, 117.93407893180847, 119.32967376708984, 120.72492122650146, 122.14632415771484, 123.54488635063171, 124.92670369148254, 126.30486869812012, 127.6775906085968, 129.08603143692017, 130.50815677642822, 131.8950798511505, 133.27500557899475, 134.6864230632782, 136.08616995811462, 137.46969318389893, 138.85805463790894, 140.26660537719727, 141.65374112129211, 143.0314917564392, 144.41609573364258, 145.8271586894989, 147.21443152427673, 148.59428191184998, 149.97870326042175, 151.36064195632935, 152.7544858455658, 154.15476632118225, 155.53359651565552, 156.91543650627136, 158.3198914527893, 159.7001509666443, 161.0773742198944, 162.46985268592834, 163.842848777771, 165.2122404575348, 166.6057858467102, 167.99454474449158, 169.39053583145142, 170.7970371246338, 172.1764702796936, 173.57610297203064, 174.99224209785461, 176.37225079536438, 177.75937485694885, 179.14418816566467, 180.5471875667572, 182.00549364089966, 183.39654421806335, 184.78466153144836, 186.16879892349243, 187.55877661705017, 188.94343090057373, 190.31497621536255, 191.69951009750366, 193.1128602027893, 194.50282168388367, 195.87998270988464, 197.2757852077484, 198.65839791297913, 200.04282093048096, 201.45030975341797, 202.83924317359924, 204.23033666610718, 205.64447021484375, 207.03382349014282, 208.42825770378113, 209.82977056503296, 211.2259213924408, 212.63685536384583, 214.04380559921265, 215.43009662628174, 216.83954739570618, 218.232764005661, 219.9518916606903, 221.54698657989502, 223.16900300979614, 224.82038712501526, 226.4678750038147, 228.09822297096252, 229.7404749393463, 231.39388918876648, 232.9895305633545, 234.43945026397705, 235.8943648338318, 237.32811903953552, 238.77361965179443, 240.19812273979187, 241.63517332077026, 243.07219791412354, 244.49940276145935, 245.94602632522583, 247.39957094192505, 248.80920815467834, 250.25023198127747, 251.69467425346375, 253.11907839775085, 254.56031441688538, 255.99711084365845, 257.4297549724579, 258.86346673965454, 260.2993631362915, 261.73478150367737, 263.1930058002472, 264.6361050605774, 266.06346106529236, 267.5079834461212, 268.9424457550049, 270.3662645816803, 271.8076708316803, 273.23331093788147, 274.66156816482544, 276.1152846813202, 277.5385842323303, 278.97591757774353, 280.4079511165619, 281.83402919769287, 283.2876832485199, 284.7222430706024, 286.15107250213623, 287.59631752967834, 289.03978085517883, 290.46489453315735, 291.9028694629669, 293.3445289134979, 294.7846465110779, 296.222843170166, 297.6486403942108, 299.1125645637512, 300.5618405342102, 301.98794507980347, 303.4281373023987, 304.8854823112488, 306.3223102092743, 307.7493438720703, 309.1916286945343, 310.6280052661896, 312.076251745224, 313.50503873825073, 314.9228754043579, 316.3881630897522, 317.8246147632599, 319.2545328140259, 320.7061836719513, 322.1316611766815, 323.57678270339966, 325.0103642940521, 326.4497616291046, 327.8880536556244, 329.34330224990845, 330.7781562805176, 332.211932182312, 333.6555395126343, 335.0828700065613, 336.53447222709656, 337.9767498970032, 339.39984345436096, 340.85817098617554, 342.3716857433319, 343.79198598861694, 345.2286694049835, 346.6780276298523, 348.1196537017822, 349.5552043914795, 350.9752402305603, 352.4263195991516, 353.8809132575989, 355.3002963066101, 356.7382261753082, 358.19111728668213, 359.6191029548645, 361.053414106369, 362.49620151519775, 363.9469430446625, 365.38341188430786, 366.8291115760803, 368.2638852596283, 369.70731115341187, 371.14762139320374, 372.5981614589691, 374.0513246059418, 375.4821548461914, 376.9143633842468, 378.3746528625488, 379.79847264289856, 381.24013090133667, 382.6791684627533, 384.1193377971649, 385.561847448349, 386.9893524646759, 388.438595533371, 389.8936939239502, 391.3296298980713, 392.7581081390381, 394.2117838859558, 395.65959453582764, 397.0799479484558, 398.5265779495239, 399.97149896621704, 401.4093680381775, 402.84813833236694, 404.2767655849457, 405.73084020614624, 407.1823105812073, 408.59998774528503, 410.0390479564667, 411.49180841445923, 412.9109220504761, 414.34156370162964, 415.79407691955566, 417.22972345352173, 418.6797249317169, 420.10498881340027, 421.5394637584686, 422.9933924674988, 424.4187250137329, 425.85171031951904, 427.2866575717926, 428.71610832214355, 430.1529474258423, 431.5961825847626, 434.03081464767456]
[7.625, 8.366666666666667, 7.475, 8.983333333333333, 6.525, 6.466666666666667, 9.741666666666667, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.976, Test loss: 1.768, Test accuracy: 29.97
Round   0, Global train loss: 0.976, Global test loss: 2.158, Global test accuracy: 17.24
Round   1, Train loss: 0.808, Test loss: 1.390, Test accuracy: 48.52
Round   1, Global train loss: 0.808, Global test loss: 1.990, Global test accuracy: 32.33
Round   2, Train loss: 0.786, Test loss: 1.204, Test accuracy: 55.67
Round   2, Global train loss: 0.786, Global test loss: 1.976, Global test accuracy: 34.06
Round   3, Train loss: 0.869, Test loss: 1.065, Test accuracy: 59.89
Round   3, Global train loss: 0.869, Global test loss: 2.177, Global test accuracy: 26.51
Round   4, Train loss: 0.734, Test loss: 1.034, Test accuracy: 62.22
Round   4, Global train loss: 0.734, Global test loss: 2.202, Global test accuracy: 29.13
Round   5, Train loss: 0.773, Test loss: 0.832, Test accuracy: 66.02
Round   5, Global train loss: 0.773, Global test loss: 1.795, Global test accuracy: 31.95
Round   6, Train loss: 0.640, Test loss: 0.841, Test accuracy: 66.29
Round   6, Global train loss: 0.640, Global test loss: 1.741, Global test accuracy: 36.56
Round   7, Train loss: 0.638, Test loss: 0.685, Test accuracy: 70.79
Round   7, Global train loss: 0.638, Global test loss: 1.574, Global test accuracy: 40.18
Round   8, Train loss: 0.633, Test loss: 0.618, Test accuracy: 73.27
Round   8, Global train loss: 0.633, Global test loss: 1.587, Global test accuracy: 43.77
Round   9, Train loss: 0.584, Test loss: 0.613, Test accuracy: 73.63
Round   9, Global train loss: 0.584, Global test loss: 1.699, Global test accuracy: 41.27
Round  10, Train loss: 0.584, Test loss: 0.599, Test accuracy: 74.36
Round  10, Global train loss: 0.584, Global test loss: 1.619, Global test accuracy: 44.03
Round  11, Train loss: 0.472, Test loss: 0.567, Test accuracy: 75.92
Round  11, Global train loss: 0.472, Global test loss: 1.522, Global test accuracy: 45.67
Round  12, Train loss: 0.511, Test loss: 0.554, Test accuracy: 76.62
Round  12, Global train loss: 0.511, Global test loss: 1.530, Global test accuracy: 48.53
Round  13, Train loss: 0.553, Test loss: 0.556, Test accuracy: 76.67
Round  13, Global train loss: 0.553, Global test loss: 1.628, Global test accuracy: 45.67
Round  14, Train loss: 0.517, Test loss: 0.553, Test accuracy: 76.84
Round  14, Global train loss: 0.517, Global test loss: 1.264, Global test accuracy: 57.17
Round  15, Train loss: 0.554, Test loss: 0.546, Test accuracy: 77.36
Round  15, Global train loss: 0.554, Global test loss: 1.455, Global test accuracy: 51.23
Round  16, Train loss: 0.500, Test loss: 0.534, Test accuracy: 78.41
Round  16, Global train loss: 0.500, Global test loss: 1.611, Global test accuracy: 49.55
Round  17, Train loss: 0.573, Test loss: 0.527, Test accuracy: 78.97
Round  17, Global train loss: 0.573, Global test loss: 1.617, Global test accuracy: 49.35
Round  18, Train loss: 0.520, Test loss: 0.501, Test accuracy: 80.17
Round  18, Global train loss: 0.520, Global test loss: 1.617, Global test accuracy: 48.04
Round  19, Train loss: 0.546, Test loss: 0.506, Test accuracy: 79.86
Round  19, Global train loss: 0.546, Global test loss: 1.425, Global test accuracy: 54.62
Round  20, Train loss: 0.470, Test loss: 0.493, Test accuracy: 80.20
Round  20, Global train loss: 0.470, Global test loss: 1.373, Global test accuracy: 52.74
Round  21, Train loss: 0.492, Test loss: 0.514, Test accuracy: 79.80
Round  21, Global train loss: 0.492, Global test loss: 1.435, Global test accuracy: 51.51
Round  22, Train loss: 0.411, Test loss: 0.509, Test accuracy: 79.92
Round  22, Global train loss: 0.411, Global test loss: 1.407, Global test accuracy: 52.53
Round  23, Train loss: 0.373, Test loss: 0.519, Test accuracy: 79.47
Round  23, Global train loss: 0.373, Global test loss: 1.296, Global test accuracy: 56.09
Round  24, Train loss: 0.531, Test loss: 0.499, Test accuracy: 80.43
Round  24, Global train loss: 0.531, Global test loss: 1.250, Global test accuracy: 57.42
Round  25, Train loss: 0.525, Test loss: 0.482, Test accuracy: 80.84
Round  25, Global train loss: 0.525, Global test loss: 1.207, Global test accuracy: 57.81
Round  26, Train loss: 0.480, Test loss: 0.490, Test accuracy: 80.89
Round  26, Global train loss: 0.480, Global test loss: 1.185, Global test accuracy: 58.84
Round  27, Train loss: 0.389, Test loss: 0.485, Test accuracy: 81.11
Round  27, Global train loss: 0.389, Global test loss: 1.151, Global test accuracy: 60.73
Round  28, Train loss: 0.438, Test loss: 0.489, Test accuracy: 81.10
Round  28, Global train loss: 0.438, Global test loss: 1.241, Global test accuracy: 56.06
Round  29, Train loss: 0.386, Test loss: 0.482, Test accuracy: 81.16
Round  29, Global train loss: 0.386, Global test loss: 1.259, Global test accuracy: 56.84
Round  30, Train loss: 0.499, Test loss: 0.502, Test accuracy: 80.79
Round  30, Global train loss: 0.499, Global test loss: 1.180, Global test accuracy: 59.38
Round  31, Train loss: 0.306, Test loss: 0.499, Test accuracy: 81.04
Round  31, Global train loss: 0.306, Global test loss: 1.530, Global test accuracy: 53.72
Round  32, Train loss: 0.371, Test loss: 0.481, Test accuracy: 81.67
Round  32, Global train loss: 0.371, Global test loss: 1.356, Global test accuracy: 54.75
Round  33, Train loss: 0.393, Test loss: 0.497, Test accuracy: 80.86
Round  33, Global train loss: 0.393, Global test loss: 1.207, Global test accuracy: 57.71
Round  34, Train loss: 0.466, Test loss: 0.468, Test accuracy: 81.86
Round  34, Global train loss: 0.466, Global test loss: 1.617, Global test accuracy: 45.22
Round  35, Train loss: 0.398, Test loss: 0.464, Test accuracy: 82.20
Round  35, Global train loss: 0.398, Global test loss: 1.198, Global test accuracy: 58.51
Round  36, Train loss: 0.323, Test loss: 0.464, Test accuracy: 82.24
Round  36, Global train loss: 0.323, Global test loss: 1.152, Global test accuracy: 61.98
Round  37, Train loss: 0.424, Test loss: 0.458, Test accuracy: 82.38
Round  37, Global train loss: 0.424, Global test loss: 1.124, Global test accuracy: 61.43
Round  38, Train loss: 0.359, Test loss: 0.470, Test accuracy: 82.13
Round  38, Global train loss: 0.359, Global test loss: 1.239, Global test accuracy: 58.77
Round  39, Train loss: 0.355, Test loss: 0.475, Test accuracy: 82.11
Round  39, Global train loss: 0.355, Global test loss: 1.105, Global test accuracy: 62.69
Round  40, Train loss: 0.390, Test loss: 0.468, Test accuracy: 82.45
Round  40, Global train loss: 0.390, Global test loss: 1.132, Global test accuracy: 61.14
Round  41, Train loss: 0.326, Test loss: 0.475, Test accuracy: 82.47
Round  41, Global train loss: 0.326, Global test loss: 1.200, Global test accuracy: 60.59
Round  42, Train loss: 0.390, Test loss: 0.466, Test accuracy: 82.97
Round  42, Global train loss: 0.390, Global test loss: 1.533, Global test accuracy: 50.64
Round  43, Train loss: 0.348, Test loss: 0.468, Test accuracy: 83.06
Round  43, Global train loss: 0.348, Global test loss: 1.210, Global test accuracy: 58.97
Round  44, Train loss: 0.374, Test loss: 0.471, Test accuracy: 82.98
Round  44, Global train loss: 0.374, Global test loss: 1.111, Global test accuracy: 62.03
Round  45, Train loss: 0.403, Test loss: 0.462, Test accuracy: 83.34
Round  45, Global train loss: 0.403, Global test loss: 1.291, Global test accuracy: 56.56
Round  46, Train loss: 0.267, Test loss: 0.450, Test accuracy: 83.49
Round  46, Global train loss: 0.267, Global test loss: 1.375, Global test accuracy: 56.14
Round  47, Train loss: 0.338, Test loss: 0.463, Test accuracy: 83.23
Round  47, Global train loss: 0.338, Global test loss: 1.172, Global test accuracy: 61.24
Round  48, Train loss: 0.291, Test loss: 0.473, Test accuracy: 82.98
Round  48, Global train loss: 0.291, Global test loss: 1.306, Global test accuracy: 58.89
Round  49, Train loss: 0.309, Test loss: 0.469, Test accuracy: 83.19
Round  49, Global train loss: 0.309, Global test loss: 1.360, Global test accuracy: 58.78
Round  50, Train loss: 0.371, Test loss: 0.465, Test accuracy: 83.26
Round  50, Global train loss: 0.371, Global test loss: 1.601, Global test accuracy: 50.93
Round  51, Train loss: 0.332, Test loss: 0.472, Test accuracy: 82.93
Round  51, Global train loss: 0.332, Global test loss: 1.207, Global test accuracy: 60.20
Round  52, Train loss: 0.276, Test loss: 0.458, Test accuracy: 83.50
Round  52, Global train loss: 0.276, Global test loss: 1.168, Global test accuracy: 61.93
Round  53, Train loss: 0.260, Test loss: 0.473, Test accuracy: 83.14
Round  53, Global train loss: 0.260, Global test loss: 1.083, Global test accuracy: 64.96
Round  54, Train loss: 0.264, Test loss: 0.474, Test accuracy: 83.16
Round  54, Global train loss: 0.264, Global test loss: 1.215, Global test accuracy: 61.26
Round  55, Train loss: 0.269, Test loss: 0.469, Test accuracy: 83.42
Round  55, Global train loss: 0.269, Global test loss: 1.138, Global test accuracy: 63.37
Round  56, Train loss: 0.269, Test loss: 0.460, Test accuracy: 83.69
Round  56, Global train loss: 0.269, Global test loss: 1.229, Global test accuracy: 61.49
Round  57, Train loss: 0.308, Test loss: 0.459, Test accuracy: 83.99
Round  57, Global train loss: 0.308, Global test loss: 1.244, Global test accuracy: 59.86
Round  58, Train loss: 0.235, Test loss: 0.469, Test accuracy: 83.71
Round  58, Global train loss: 0.235, Global test loss: 1.126, Global test accuracy: 64.51
Round  59, Train loss: 0.215, Test loss: 0.471, Test accuracy: 83.66
Round  59, Global train loss: 0.215, Global test loss: 1.171, Global test accuracy: 62.78
Round  60, Train loss: 0.266, Test loss: 0.478, Test accuracy: 83.56
Round  60, Global train loss: 0.266, Global test loss: 1.181, Global test accuracy: 62.01
Round  61, Train loss: 0.325, Test loss: 0.474, Test accuracy: 83.82
Round  61, Global train loss: 0.325, Global test loss: 1.060, Global test accuracy: 64.16
Round  62, Train loss: 0.342, Test loss: 0.474, Test accuracy: 83.53
Round  62, Global train loss: 0.342, Global test loss: 1.261, Global test accuracy: 59.92
Round  63, Train loss: 0.302, Test loss: 0.476, Test accuracy: 83.32
Round  63, Global train loss: 0.302, Global test loss: 1.041, Global test accuracy: 64.23
Round  64, Train loss: 0.225, Test loss: 0.466, Test accuracy: 83.76
Round  64, Global train loss: 0.225, Global test loss: 1.008, Global test accuracy: 67.36
Round  65, Train loss: 0.262, Test loss: 0.494, Test accuracy: 83.41
Round  65, Global train loss: 0.262, Global test loss: 1.104, Global test accuracy: 63.39
Round  66, Train loss: 0.256, Test loss: 0.464, Test accuracy: 84.00
Round  66, Global train loss: 0.256, Global test loss: 1.489, Global test accuracy: 58.12
Round  67, Train loss: 0.305, Test loss: 0.467, Test accuracy: 84.08
Round  67, Global train loss: 0.305, Global test loss: 1.239, Global test accuracy: 60.81
Round  68, Train loss: 0.221, Test loss: 0.471, Test accuracy: 83.91
Round  68, Global train loss: 0.221, Global test loss: 1.198, Global test accuracy: 62.70
Round  69, Train loss: 0.226, Test loss: 0.470, Test accuracy: 84.16
Round  69, Global train loss: 0.226, Global test loss: 1.191, Global test accuracy: 62.01
Round  70, Train loss: 0.289, Test loss: 0.472, Test accuracy: 84.03
Round  70, Global train loss: 0.289, Global test loss: 1.669, Global test accuracy: 53.28
Round  71, Train loss: 0.203, Test loss: 0.464, Test accuracy: 84.38
Round  71, Global train loss: 0.203, Global test loss: 1.230, Global test accuracy: 61.79
Round  72, Train loss: 0.295, Test loss: 0.500, Test accuracy: 83.77
Round  72, Global train loss: 0.295, Global test loss: 1.226, Global test accuracy: 59.44
Round  73, Train loss: 0.205, Test loss: 0.493, Test accuracy: 83.94
Round  73, Global train loss: 0.205, Global test loss: 1.180, Global test accuracy: 63.13
Round  74, Train loss: 0.309, Test loss: 0.470, Test accuracy: 84.36
Round  74, Global train loss: 0.309, Global test loss: 1.163, Global test accuracy: 62.19
Round  75, Train loss: 0.228, Test loss: 0.477, Test accuracy: 84.16
Round  75, Global train loss: 0.228, Global test loss: 1.237, Global test accuracy: 62.13
Round  76, Train loss: 0.241, Test loss: 0.482, Test accuracy: 84.11
Round  76, Global train loss: 0.241, Global test loss: 1.242, Global test accuracy: 61.51
Round  77, Train loss: 0.240, Test loss: 0.479, Test accuracy: 84.36
Round  77, Global train loss: 0.240, Global test loss: 1.030, Global test accuracy: 66.73
Round  78, Train loss: 0.240, Test loss: 0.473, Test accuracy: 84.43
Round  78, Global train loss: 0.240, Global test loss: 1.186, Global test accuracy: 63.24
Round  79, Train loss: 0.257, Test loss: 0.492, Test accuracy: 84.21
Round  79, Global train loss: 0.257, Global test loss: 1.040, Global test accuracy: 67.07
Round  80, Train loss: 0.195, Test loss: 0.502, Test accuracy: 84.21
Round  80, Global train loss: 0.195, Global test loss: 1.382, Global test accuracy: 59.31
Round  81, Train loss: 0.299, Test loss: 0.517, Test accuracy: 83.94
Round  81, Global train loss: 0.299, Global test loss: 1.461, Global test accuracy: 56.61
Round  82, Train loss: 0.292, Test loss: 0.516, Test accuracy: 84.11
Round  82, Global train loss: 0.292, Global test loss: 1.152, Global test accuracy: 63.00
Round  83, Train loss: 0.243, Test loss: 0.516, Test accuracy: 84.04
Round  83, Global train loss: 0.243, Global test loss: 1.239, Global test accuracy: 61.02
Round  84, Train loss: 0.212, Test loss: 0.502, Test accuracy: 84.18
Round  84, Global train loss: 0.212, Global test loss: 1.121, Global test accuracy: 64.09
Round  85, Train loss: 0.236, Test loss: 0.502, Test accuracy: 84.36
Round  85, Global train loss: 0.236, Global test loss: 1.258, Global test accuracy: 60.02
Round  86, Train loss: 0.185, Test loss: 0.500, Test accuracy: 84.29
Round  86, Global train loss: 0.185, Global test loss: 1.113, Global test accuracy: 65.42
Round  87, Train loss: 0.218, Test loss: 0.493, Test accuracy: 84.37
Round  87, Global train loss: 0.218, Global test loss: 1.118, Global test accuracy: 64.25
Round  88, Train loss: 0.206, Test loss: 0.490, Test accuracy: 84.45
Round  88, Global train loss: 0.206, Global test loss: 1.221, Global test accuracy: 63.29
Round  89, Train loss: 0.186, Test loss: 0.488, Test accuracy: 84.51
Round  89, Global train loss: 0.186, Global test loss: 1.157, Global test accuracy: 63.82
Round  90, Train loss: 0.234, Test loss: 0.494, Test accuracy: 84.62
Round  90, Global train loss: 0.234, Global test loss: 1.457, Global test accuracy: 57.36
Round  91, Train loss: 0.145, Test loss: 0.489, Test accuracy: 84.66
Round  91, Global train loss: 0.145, Global test loss: 1.098, Global test accuracy: 66.39
Round  92, Train loss: 0.229, Test loss: 0.474, Test accuracy: 84.76
Round  92, Global train loss: 0.229, Global test loss: 1.204, Global test accuracy: 63.55
Round  93, Train loss: 0.221, Test loss: 0.479, Test accuracy: 84.89
Round  93, Global train loss: 0.221, Global test loss: 1.194, Global test accuracy: 63.75
Round  94, Train loss: 0.168, Test loss: 0.491, Test accuracy: 84.69
Round  94, Global train loss: 0.168, Global test loss: 1.133, Global test accuracy: 65.83
Round  95, Train loss: 0.236, Test loss: 0.487, Test accuracy: 84.69
Round  95, Global train loss: 0.236, Global test loss: 1.494, Global test accuracy: 57.87
Round  96, Train loss: 0.157, Test loss: 0.500, Test accuracy: 84.65
Round  96, Global train loss: 0.157, Global test loss: 1.276, Global test accuracy: 62.81
Round  97, Train loss: 0.177, Test loss: 0.490, Test accuracy: 84.79
Round  97, Global train loss: 0.177, Global test loss: 1.239, Global test accuracy: 63.62
Round  98, Train loss: 0.206, Test loss: 0.489, Test accuracy: 84.78
Round  98, Global train loss: 0.206, Global test loss: 1.179, Global test accuracy: 65.06
Round  99, Train loss: 0.195, Test loss: 0.490, Test accuracy: 84.77
Round  99, Global train loss: 0.195, Global test loss: 1.206, Global test accuracy: 64.67
Final Round, Train loss: 0.147, Test loss: 0.543, Test accuracy: 84.97
Final Round, Global train loss: 0.147, Global test loss: 1.206, Global test accuracy: 64.67
Average accuracy final 10 rounds: 84.73166666666665 

Average global accuracy final 10 rounds: 63.09 

2798.091973543167
[2.534569025039673, 5.069138050079346, 7.281458616256714, 9.493779182434082, 11.719677209854126, 13.94557523727417, 16.22577166557312, 18.50596809387207, 20.75559163093567, 23.005215167999268, 25.001649618148804, 26.99808406829834, 28.9397714138031, 30.88145875930786, 32.815717458724976, 34.74997615814209, 36.7021222114563, 38.65426826477051, 40.59529685974121, 42.536325454711914, 44.47364377975464, 46.41096210479736, 48.361663818359375, 50.31236553192139, 52.25321054458618, 54.19405555725098, 56.14269709587097, 58.09133863449097, 60.03438186645508, 61.97742509841919, 63.92851161956787, 65.87959814071655, 67.86194491386414, 69.84429168701172, 71.78445291519165, 73.72461414337158, 75.6691107749939, 77.61360740661621, 79.55280041694641, 81.49199342727661, 83.41697812080383, 85.34196281433105, 87.2765564918518, 89.21115016937256, 91.14301776885986, 93.07488536834717, 95.02545833587646, 96.97603130340576, 98.93542742729187, 100.89482355117798, 102.83583116531372, 104.77683877944946, 106.7192108631134, 108.66158294677734, 110.60259199142456, 112.54360103607178, 114.47672414779663, 116.40984725952148, 118.35319328308105, 120.29653930664062, 122.21477007865906, 124.13300085067749, 126.09368085861206, 128.05436086654663, 129.97595643997192, 131.89755201339722, 133.83946466445923, 135.78137731552124, 137.71837282180786, 139.65536832809448, 141.5998022556305, 143.5442361831665, 145.47982621192932, 147.41541624069214, 149.34263467788696, 151.2698531150818, 153.23748111724854, 155.20510911941528, 157.16642475128174, 159.1277403831482, 161.07599663734436, 163.02425289154053, 164.97970008850098, 166.93514728546143, 168.87734866142273, 170.81955003738403, 172.77714157104492, 174.7347331047058, 176.68477082252502, 178.63480854034424, 180.58300065994263, 182.53119277954102, 184.46980905532837, 186.40842533111572, 188.35304498672485, 190.29766464233398, 192.25086212158203, 194.20405960083008, 196.15661144256592, 198.10916328430176, 200.05997347831726, 202.01078367233276, 203.95196914672852, 205.89315462112427, 207.83644318580627, 209.77973175048828, 211.7257115840912, 213.6716914176941, 215.63164567947388, 217.59159994125366, 219.53652358055115, 221.48144721984863, 223.42827916145325, 225.37511110305786, 227.3213288784027, 229.26754665374756, 231.21522164344788, 233.1628966331482, 235.09750127792358, 237.03210592269897, 238.9896366596222, 240.9471673965454, 242.89362502098083, 244.84008264541626, 246.7875018119812, 248.73492097854614, 250.67157316207886, 252.60822534561157, 254.55309176445007, 256.4979581832886, 258.44857907295227, 260.39919996261597, 262.3483142852783, 264.2974286079407, 266.2407741546631, 268.1841197013855, 270.1188895702362, 272.0536594390869, 273.9969069957733, 275.9401545524597, 277.87169456481934, 279.80323457717896, 281.74369525909424, 283.6841559410095, 285.610812664032, 287.53746938705444, 289.4892909526825, 291.44111251831055, 293.37356662750244, 295.30602073669434, 297.2556095123291, 299.20519828796387, 301.14352107048035, 303.0818438529968, 305.04712557792664, 307.01240730285645, 308.9578652381897, 310.90332317352295, 312.8579077720642, 314.81249237060547, 316.76985812187195, 318.7272238731384, 320.6960139274597, 322.664803981781, 324.6205360889435, 326.57626819610596, 328.52728605270386, 330.47830390930176, 332.4283404350281, 334.3783769607544, 336.3354206085205, 338.2924642562866, 340.23883390426636, 342.1852035522461, 344.1459584236145, 346.1067132949829, 348.0556871891022, 350.00466108322144, 351.9539291858673, 353.9031972885132, 355.8551070690155, 357.8070168495178, 359.75906562805176, 361.7111144065857, 363.64635825157166, 365.5816020965576, 367.52565145492554, 369.46970081329346, 371.41315054893494, 373.3566002845764, 375.3032817840576, 377.2499632835388, 379.1853940486908, 381.1208248138428, 383.05002784729004, 384.9792308807373, 386.93814420700073, 388.89705753326416, 390.86100363731384, 392.8249497413635, 394.9887535572052, 397.1525573730469]
[29.966666666666665, 29.966666666666665, 48.522222222222226, 48.522222222222226, 55.672222222222224, 55.672222222222224, 59.888888888888886, 59.888888888888886, 62.21666666666667, 62.21666666666667, 66.01666666666667, 66.01666666666667, 66.28888888888889, 66.28888888888889, 70.78888888888889, 70.78888888888889, 73.27222222222223, 73.27222222222223, 73.63333333333334, 73.63333333333334, 74.36111111111111, 74.36111111111111, 75.92222222222222, 75.92222222222222, 76.62222222222222, 76.62222222222222, 76.67222222222222, 76.67222222222222, 76.84444444444445, 76.84444444444445, 77.35555555555555, 77.35555555555555, 78.41111111111111, 78.41111111111111, 78.96666666666667, 78.96666666666667, 80.17222222222222, 80.17222222222222, 79.85555555555555, 79.85555555555555, 80.2, 80.2, 79.8, 79.8, 79.91666666666667, 79.91666666666667, 79.47222222222223, 79.47222222222223, 80.43333333333334, 80.43333333333334, 80.83888888888889, 80.83888888888889, 80.88888888888889, 80.88888888888889, 81.10555555555555, 81.10555555555555, 81.1, 81.1, 81.15555555555555, 81.15555555555555, 80.79444444444445, 80.79444444444445, 81.03888888888889, 81.03888888888889, 81.66666666666667, 81.66666666666667, 80.86111111111111, 80.86111111111111, 81.85555555555555, 81.85555555555555, 82.2, 82.2, 82.24444444444444, 82.24444444444444, 82.37777777777778, 82.37777777777778, 82.13333333333334, 82.13333333333334, 82.11111111111111, 82.11111111111111, 82.45, 82.45, 82.46666666666667, 82.46666666666667, 82.97222222222223, 82.97222222222223, 83.06111111111112, 83.06111111111112, 82.98333333333333, 82.98333333333333, 83.33888888888889, 83.33888888888889, 83.4888888888889, 83.4888888888889, 83.22777777777777, 83.22777777777777, 82.97777777777777, 82.97777777777777, 83.18888888888888, 83.18888888888888, 83.2611111111111, 83.2611111111111, 82.92777777777778, 82.92777777777778, 83.5, 83.5, 83.13888888888889, 83.13888888888889, 83.15555555555555, 83.15555555555555, 83.42222222222222, 83.42222222222222, 83.68888888888888, 83.68888888888888, 83.9888888888889, 83.9888888888889, 83.71111111111111, 83.71111111111111, 83.65555555555555, 83.65555555555555, 83.55555555555556, 83.55555555555556, 83.82222222222222, 83.82222222222222, 83.53333333333333, 83.53333333333333, 83.32222222222222, 83.32222222222222, 83.75555555555556, 83.75555555555556, 83.41111111111111, 83.41111111111111, 84.0, 84.0, 84.07777777777778, 84.07777777777778, 83.91111111111111, 83.91111111111111, 84.15555555555555, 84.15555555555555, 84.03333333333333, 84.03333333333333, 84.37777777777778, 84.37777777777778, 83.76666666666667, 83.76666666666667, 83.94444444444444, 83.94444444444444, 84.35555555555555, 84.35555555555555, 84.15555555555555, 84.15555555555555, 84.10555555555555, 84.10555555555555, 84.36111111111111, 84.36111111111111, 84.43333333333334, 84.43333333333334, 84.21111111111111, 84.21111111111111, 84.21111111111111, 84.21111111111111, 83.94444444444444, 83.94444444444444, 84.10555555555555, 84.10555555555555, 84.03888888888889, 84.03888888888889, 84.17777777777778, 84.17777777777778, 84.35555555555555, 84.35555555555555, 84.28888888888889, 84.28888888888889, 84.36666666666666, 84.36666666666666, 84.45, 84.45, 84.5111111111111, 84.5111111111111, 84.62222222222222, 84.62222222222222, 84.66111111111111, 84.66111111111111, 84.7611111111111, 84.7611111111111, 84.89444444444445, 84.89444444444445, 84.69444444444444, 84.69444444444444, 84.69444444444444, 84.69444444444444, 84.65, 84.65, 84.79444444444445, 84.79444444444445, 84.77777777777777, 84.77777777777777, 84.76666666666667, 84.76666666666667, 84.97222222222223, 84.97222222222223]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.476, Test loss: 1.990, Test accuracy: 32.35
Round   1, Train loss: 1.006, Test loss: 1.560, Test accuracy: 41.89
Round   2, Train loss: 0.931, Test loss: 1.553, Test accuracy: 42.50
Round   3, Train loss: 0.838, Test loss: 1.072, Test accuracy: 56.02
Round   4, Train loss: 0.722, Test loss: 1.004, Test accuracy: 59.64
Round   5, Train loss: 0.712, Test loss: 0.884, Test accuracy: 62.00
Round   6, Train loss: 0.718, Test loss: 0.719, Test accuracy: 67.62
Round   7, Train loss: 0.703, Test loss: 0.684, Test accuracy: 68.86
Round   8, Train loss: 0.723, Test loss: 0.653, Test accuracy: 70.78
Round   9, Train loss: 0.623, Test loss: 0.642, Test accuracy: 71.88
Round  10, Train loss: 0.675, Test loss: 0.623, Test accuracy: 72.22
Round  11, Train loss: 0.673, Test loss: 0.621, Test accuracy: 72.65
Round  12, Train loss: 0.674, Test loss: 0.604, Test accuracy: 74.53
Round  13, Train loss: 0.550, Test loss: 0.590, Test accuracy: 75.22
Round  14, Train loss: 0.573, Test loss: 0.574, Test accuracy: 75.34
Round  15, Train loss: 0.499, Test loss: 0.559, Test accuracy: 76.51
Round  16, Train loss: 0.614, Test loss: 0.553, Test accuracy: 77.10
Round  17, Train loss: 0.529, Test loss: 0.545, Test accuracy: 77.37
Round  18, Train loss: 0.560, Test loss: 0.524, Test accuracy: 78.28
Round  19, Train loss: 0.556, Test loss: 0.522, Test accuracy: 78.18
Round  20, Train loss: 0.512, Test loss: 0.534, Test accuracy: 77.67
Round  21, Train loss: 0.570, Test loss: 0.516, Test accuracy: 78.62
Round  22, Train loss: 0.521, Test loss: 0.492, Test accuracy: 79.60
Round  23, Train loss: 0.451, Test loss: 0.511, Test accuracy: 79.22
Round  24, Train loss: 0.568, Test loss: 0.487, Test accuracy: 79.96
Round  25, Train loss: 0.429, Test loss: 0.495, Test accuracy: 79.65
Round  26, Train loss: 0.479, Test loss: 0.470, Test accuracy: 81.19
Round  27, Train loss: 0.535, Test loss: 0.471, Test accuracy: 81.16
Round  28, Train loss: 0.451, Test loss: 0.467, Test accuracy: 81.67
Round  29, Train loss: 0.386, Test loss: 0.451, Test accuracy: 81.72
Round  30, Train loss: 0.570, Test loss: 0.454, Test accuracy: 81.88
Round  31, Train loss: 0.360, Test loss: 0.451, Test accuracy: 81.78
Round  32, Train loss: 0.448, Test loss: 0.459, Test accuracy: 81.57
Round  33, Train loss: 0.431, Test loss: 0.455, Test accuracy: 81.58
Round  34, Train loss: 0.431, Test loss: 0.452, Test accuracy: 81.81
Round  35, Train loss: 0.348, Test loss: 0.460, Test accuracy: 81.70
Round  36, Train loss: 0.417, Test loss: 0.453, Test accuracy: 82.15
Round  37, Train loss: 0.417, Test loss: 0.444, Test accuracy: 82.58
Round  38, Train loss: 0.329, Test loss: 0.431, Test accuracy: 83.03
Round  39, Train loss: 0.386, Test loss: 0.430, Test accuracy: 82.73
Round  40, Train loss: 0.445, Test loss: 0.436, Test accuracy: 82.29
Round  41, Train loss: 0.381, Test loss: 0.419, Test accuracy: 83.10
Round  42, Train loss: 0.507, Test loss: 0.427, Test accuracy: 83.03
Round  43, Train loss: 0.316, Test loss: 0.412, Test accuracy: 83.46
Round  44, Train loss: 0.437, Test loss: 0.409, Test accuracy: 83.93
Round  45, Train loss: 0.396, Test loss: 0.405, Test accuracy: 83.72
Round  46, Train loss: 0.310, Test loss: 0.420, Test accuracy: 83.07
Round  47, Train loss: 0.343, Test loss: 0.403, Test accuracy: 83.97
Round  48, Train loss: 0.342, Test loss: 0.408, Test accuracy: 83.83
Round  49, Train loss: 0.316, Test loss: 0.404, Test accuracy: 84.22
Round  50, Train loss: 0.294, Test loss: 0.406, Test accuracy: 84.29
Round  51, Train loss: 0.348, Test loss: 0.399, Test accuracy: 84.62
Round  52, Train loss: 0.421, Test loss: 0.393, Test accuracy: 84.75
Round  53, Train loss: 0.297, Test loss: 0.402, Test accuracy: 84.45
Round  54, Train loss: 0.367, Test loss: 0.398, Test accuracy: 84.63
Round  55, Train loss: 0.382, Test loss: 0.391, Test accuracy: 84.96
Round  56, Train loss: 0.390, Test loss: 0.392, Test accuracy: 84.78
Round  57, Train loss: 0.379, Test loss: 0.387, Test accuracy: 84.86
Round  58, Train loss: 0.363, Test loss: 0.396, Test accuracy: 84.56
Round  59, Train loss: 0.341, Test loss: 0.395, Test accuracy: 84.58
Round  60, Train loss: 0.255, Test loss: 0.391, Test accuracy: 84.87
Round  61, Train loss: 0.302, Test loss: 0.392, Test accuracy: 84.97
Round  62, Train loss: 0.340, Test loss: 0.387, Test accuracy: 85.47
Round  63, Train loss: 0.390, Test loss: 0.382, Test accuracy: 85.48
Round  64, Train loss: 0.265, Test loss: 0.380, Test accuracy: 85.47
Round  65, Train loss: 0.334, Test loss: 0.382, Test accuracy: 85.38
Round  66, Train loss: 0.237, Test loss: 0.381, Test accuracy: 85.91
Round  67, Train loss: 0.313, Test loss: 0.380, Test accuracy: 85.63
Round  68, Train loss: 0.303, Test loss: 0.373, Test accuracy: 85.91
Round  69, Train loss: 0.273, Test loss: 0.388, Test accuracy: 85.00
Round  70, Train loss: 0.278, Test loss: 0.380, Test accuracy: 85.68
Round  71, Train loss: 0.249, Test loss: 0.376, Test accuracy: 85.89
Round  72, Train loss: 0.233, Test loss: 0.384, Test accuracy: 85.73
Round  73, Train loss: 0.194, Test loss: 0.369, Test accuracy: 86.36
Round  74, Train loss: 0.308, Test loss: 0.378, Test accuracy: 85.85
Round  75, Train loss: 0.208, Test loss: 0.372, Test accuracy: 86.41
Round  76, Train loss: 0.281, Test loss: 0.371, Test accuracy: 86.22
Round  77, Train loss: 0.310, Test loss: 0.372, Test accuracy: 85.97
Round  78, Train loss: 0.313, Test loss: 0.375, Test accuracy: 85.76
Round  79, Train loss: 0.245, Test loss: 0.371, Test accuracy: 86.12
Round  80, Train loss: 0.244, Test loss: 0.373, Test accuracy: 85.94
Round  81, Train loss: 0.205, Test loss: 0.374, Test accuracy: 86.24
Round  82, Train loss: 0.264, Test loss: 0.371, Test accuracy: 86.36
Round  83, Train loss: 0.256, Test loss: 0.363, Test accuracy: 86.37
Round  84, Train loss: 0.282, Test loss: 0.365, Test accuracy: 86.46
Round  85, Train loss: 0.216, Test loss: 0.368, Test accuracy: 86.55
Round  86, Train loss: 0.254, Test loss: 0.359, Test accuracy: 86.81
Round  87, Train loss: 0.189, Test loss: 0.370, Test accuracy: 86.61
Round  88, Train loss: 0.200, Test loss: 0.379, Test accuracy: 86.23
Round  89, Train loss: 0.207, Test loss: 0.383, Test accuracy: 86.39
Round  90, Train loss: 0.228, Test loss: 0.377, Test accuracy: 86.42
Round  91, Train loss: 0.275, Test loss: 0.389, Test accuracy: 86.22
Round  92, Train loss: 0.281, Test loss: 0.383, Test accuracy: 86.33
Round  93, Train loss: 0.149, Test loss: 0.375, Test accuracy: 86.79
Round  94, Train loss: 0.232, Test loss: 0.371, Test accuracy: 86.23
Round  95, Train loss: 0.211, Test loss: 0.371, Test accuracy: 86.67
Round  96, Train loss: 0.227, Test loss: 0.364, Test accuracy: 87.15
Round  97, Train loss: 0.280, Test loss: 0.358, Test accuracy: 87.05
Round  98, Train loss: 0.140, Test loss: 0.366, Test accuracy: 86.96
Round  99, Train loss: 0.284, Test loss: 0.373, Test accuracy: 86.59
Final Round, Train loss: 0.187, Test loss: 0.370, Test accuracy: 86.93
Average accuracy final 10 rounds: 86.64083333333335 

1471.6351323127747
[1.4792571067810059, 2.9585142135620117, 4.2755773067474365, 5.592640399932861, 6.906155109405518, 8.219669818878174, 9.594048738479614, 10.968427658081055, 12.299711465835571, 13.630995273590088, 14.982306957244873, 16.333618640899658, 17.659507751464844, 18.98539686203003, 20.343497037887573, 21.701597213745117, 23.069982290267944, 24.43836736679077, 25.765339136123657, 27.092310905456543, 28.423428773880005, 29.754546642303467, 31.095118284225464, 32.43568992614746, 33.81410813331604, 35.19252634048462, 36.534276247024536, 37.87602615356445, 39.21489715576172, 40.553768157958984, 41.887781858444214, 43.22179555892944, 44.48150038719177, 45.7412052154541, 46.983460903167725, 48.22571659088135, 49.46719455718994, 50.708672523498535, 51.95262360572815, 53.196574687957764, 54.45058488845825, 55.70459508895874, 56.933000326156616, 58.16140556335449, 59.406723976135254, 60.652042388916016, 61.9000940322876, 63.14814567565918, 64.39350962638855, 65.63887357711792, 66.86692023277283, 68.09496688842773, 69.31170725822449, 70.52844762802124, 71.74699282646179, 72.96553802490234, 74.19230580329895, 75.41907358169556, 76.65284204483032, 77.88661050796509, 79.12270069122314, 80.3587908744812, 81.57639741897583, 82.79400396347046, 84.00846529006958, 85.2229266166687, 86.4548933506012, 87.68686008453369, 88.92868304252625, 90.1705060005188, 91.37940001487732, 92.58829402923584, 93.81290602684021, 95.03751802444458, 96.28542900085449, 97.5333399772644, 98.79095125198364, 100.04856252670288, 101.27972602844238, 102.51088953018188, 103.73331904411316, 104.95574855804443, 106.18204498291016, 107.40834140777588, 108.6383547782898, 109.86836814880371, 111.11495399475098, 112.36153984069824, 113.60554504394531, 114.84955024719238, 116.0712296962738, 117.29290914535522, 118.52171349525452, 119.75051784515381, 120.97556447982788, 122.20061111450195, 123.45109105110168, 124.70157098770142, 125.91645956039429, 127.13134813308716, 128.35253810882568, 129.5737280845642, 130.80931854248047, 132.04490900039673, 133.2633535861969, 134.48179817199707, 135.69994449615479, 136.9180908203125, 138.1401755809784, 139.3622603416443, 140.58454871177673, 141.80683708190918, 143.03942441940308, 144.27201175689697, 145.49878883361816, 146.72556591033936, 147.93768072128296, 149.14979553222656, 150.38958096504211, 151.62936639785767, 152.83644580841064, 154.04352521896362, 155.2786364555359, 156.51374769210815, 157.72612524032593, 158.9385027885437, 160.1596381664276, 161.38077354431152, 162.58889031410217, 163.79700708389282, 165.02924942970276, 166.2614917755127, 167.47225618362427, 168.68302059173584, 169.90385723114014, 171.12469387054443, 172.33432173728943, 173.54394960403442, 174.77453684806824, 176.00512409210205, 177.20673060417175, 178.40833711624146, 179.6470468044281, 180.88575649261475, 182.09242987632751, 183.29910326004028, 184.52937722206116, 185.75965118408203, 186.95776295661926, 188.1558747291565, 189.40033650398254, 190.6447982788086, 191.85624861717224, 193.0676989555359, 194.28703570365906, 195.50637245178223, 196.71029686927795, 197.91422128677368, 199.1468002796173, 200.37937927246094, 201.57997155189514, 202.78056383132935, 203.99936985969543, 205.21817588806152, 206.43480348587036, 207.6514310836792, 208.8679313659668, 210.0844316482544, 211.29548120498657, 212.50653076171875, 213.72244215011597, 214.93835353851318, 216.16279935836792, 217.38724517822266, 218.60744524002075, 219.82764530181885, 221.0372440814972, 222.24684286117554, 223.47708535194397, 224.7073278427124, 225.9118959903717, 227.116464138031, 228.3241970539093, 229.5319299697876, 230.75029110908508, 231.96865224838257, 233.18109560012817, 234.39353895187378, 235.59790754318237, 236.80227613449097, 238.0193898677826, 239.23650360107422, 240.45674896240234, 241.67699432373047, 242.8803734779358, 244.0837526321411, 245.28592824935913, 246.48810386657715, 247.7156240940094, 248.94314432144165, 250.86889696121216, 252.79464960098267]
[32.35, 32.35, 41.891666666666666, 41.891666666666666, 42.5, 42.5, 56.016666666666666, 56.016666666666666, 59.641666666666666, 59.641666666666666, 62.0, 62.0, 67.61666666666666, 67.61666666666666, 68.85833333333333, 68.85833333333333, 70.78333333333333, 70.78333333333333, 71.875, 71.875, 72.21666666666667, 72.21666666666667, 72.65, 72.65, 74.53333333333333, 74.53333333333333, 75.21666666666667, 75.21666666666667, 75.34166666666667, 75.34166666666667, 76.50833333333334, 76.50833333333334, 77.1, 77.1, 77.36666666666666, 77.36666666666666, 78.275, 78.275, 78.18333333333334, 78.18333333333334, 77.675, 77.675, 78.61666666666666, 78.61666666666666, 79.6, 79.6, 79.225, 79.225, 79.95833333333333, 79.95833333333333, 79.65, 79.65, 81.19166666666666, 81.19166666666666, 81.15833333333333, 81.15833333333333, 81.675, 81.675, 81.725, 81.725, 81.875, 81.875, 81.775, 81.775, 81.56666666666666, 81.56666666666666, 81.58333333333333, 81.58333333333333, 81.80833333333334, 81.80833333333334, 81.7, 81.7, 82.15, 82.15, 82.575, 82.575, 83.03333333333333, 83.03333333333333, 82.73333333333333, 82.73333333333333, 82.29166666666667, 82.29166666666667, 83.1, 83.1, 83.025, 83.025, 83.45833333333333, 83.45833333333333, 83.93333333333334, 83.93333333333334, 83.725, 83.725, 83.06666666666666, 83.06666666666666, 83.96666666666667, 83.96666666666667, 83.825, 83.825, 84.21666666666667, 84.21666666666667, 84.29166666666667, 84.29166666666667, 84.61666666666666, 84.61666666666666, 84.75, 84.75, 84.45, 84.45, 84.63333333333334, 84.63333333333334, 84.95833333333333, 84.95833333333333, 84.775, 84.775, 84.85833333333333, 84.85833333333333, 84.55833333333334, 84.55833333333334, 84.58333333333333, 84.58333333333333, 84.86666666666666, 84.86666666666666, 84.975, 84.975, 85.46666666666667, 85.46666666666667, 85.48333333333333, 85.48333333333333, 85.475, 85.475, 85.38333333333334, 85.38333333333334, 85.90833333333333, 85.90833333333333, 85.63333333333334, 85.63333333333334, 85.90833333333333, 85.90833333333333, 85.0, 85.0, 85.68333333333334, 85.68333333333334, 85.89166666666667, 85.89166666666667, 85.73333333333333, 85.73333333333333, 86.35833333333333, 86.35833333333333, 85.85, 85.85, 86.40833333333333, 86.40833333333333, 86.21666666666667, 86.21666666666667, 85.96666666666667, 85.96666666666667, 85.75833333333334, 85.75833333333334, 86.11666666666666, 86.11666666666666, 85.94166666666666, 85.94166666666666, 86.24166666666666, 86.24166666666666, 86.35833333333333, 86.35833333333333, 86.36666666666666, 86.36666666666666, 86.45833333333333, 86.45833333333333, 86.55, 86.55, 86.80833333333334, 86.80833333333334, 86.60833333333333, 86.60833333333333, 86.23333333333333, 86.23333333333333, 86.39166666666667, 86.39166666666667, 86.41666666666667, 86.41666666666667, 86.21666666666667, 86.21666666666667, 86.325, 86.325, 86.79166666666667, 86.79166666666667, 86.23333333333333, 86.23333333333333, 86.675, 86.675, 87.15, 87.15, 87.05, 87.05, 86.95833333333333, 86.95833333333333, 86.59166666666667, 86.59166666666667, 86.93333333333334, 86.93333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.495, Test loss: 1.922, Test accuracy: 35.39
Round   1, Train loss: 0.911, Test loss: 1.831, Test accuracy: 41.40
Round   2, Train loss: 0.881, Test loss: 1.230, Test accuracy: 54.62
Round   3, Train loss: 0.823, Test loss: 1.228, Test accuracy: 55.27
Round   4, Train loss: 0.753, Test loss: 1.173, Test accuracy: 58.23
Round   5, Train loss: 0.823, Test loss: 0.902, Test accuracy: 64.01
Round   6, Train loss: 0.698, Test loss: 0.699, Test accuracy: 71.11
Round   7, Train loss: 0.719, Test loss: 0.683, Test accuracy: 72.72
Round   8, Train loss: 0.608, Test loss: 0.666, Test accuracy: 73.14
Round   9, Train loss: 0.642, Test loss: 0.660, Test accuracy: 73.26
Round  10, Train loss: 0.563, Test loss: 0.628, Test accuracy: 74.78
Round  11, Train loss: 0.694, Test loss: 0.609, Test accuracy: 75.98
Round  12, Train loss: 0.556, Test loss: 0.595, Test accuracy: 76.37
Round  13, Train loss: 0.646, Test loss: 0.566, Test accuracy: 77.02
Round  14, Train loss: 0.525, Test loss: 0.573, Test accuracy: 77.20
Round  15, Train loss: 0.563, Test loss: 0.550, Test accuracy: 78.37
Round  16, Train loss: 0.490, Test loss: 0.544, Test accuracy: 78.72
Round  17, Train loss: 0.540, Test loss: 0.544, Test accuracy: 78.87
Round  18, Train loss: 0.656, Test loss: 0.537, Test accuracy: 79.31
Round  19, Train loss: 0.495, Test loss: 0.516, Test accuracy: 79.74
Round  20, Train loss: 0.538, Test loss: 0.529, Test accuracy: 79.98
Round  21, Train loss: 0.463, Test loss: 0.503, Test accuracy: 80.51
Round  22, Train loss: 0.478, Test loss: 0.491, Test accuracy: 80.74
Round  23, Train loss: 0.552, Test loss: 0.491, Test accuracy: 81.07
Round  24, Train loss: 0.456, Test loss: 0.481, Test accuracy: 81.72
Round  25, Train loss: 0.497, Test loss: 0.466, Test accuracy: 81.86
Round  26, Train loss: 0.401, Test loss: 0.474, Test accuracy: 81.05
Round  27, Train loss: 0.435, Test loss: 0.460, Test accuracy: 82.21
Round  28, Train loss: 0.444, Test loss: 0.445, Test accuracy: 82.42
Round  29, Train loss: 0.418, Test loss: 0.446, Test accuracy: 82.85
Round  30, Train loss: 0.484, Test loss: 0.442, Test accuracy: 83.09
Round  31, Train loss: 0.453, Test loss: 0.435, Test accuracy: 82.90
Round  32, Train loss: 0.434, Test loss: 0.449, Test accuracy: 82.49
Round  33, Train loss: 0.498, Test loss: 0.425, Test accuracy: 83.73
Round  34, Train loss: 0.423, Test loss: 0.422, Test accuracy: 83.63
Round  35, Train loss: 0.444, Test loss: 0.425, Test accuracy: 83.67
Round  36, Train loss: 0.424, Test loss: 0.410, Test accuracy: 83.85
Round  37, Train loss: 0.390, Test loss: 0.413, Test accuracy: 84.03
Round  38, Train loss: 0.310, Test loss: 0.411, Test accuracy: 83.72
Round  39, Train loss: 0.516, Test loss: 0.405, Test accuracy: 84.12
Round  40, Train loss: 0.451, Test loss: 0.413, Test accuracy: 84.28
Round  41, Train loss: 0.368, Test loss: 0.407, Test accuracy: 84.08
Round  42, Train loss: 0.337, Test loss: 0.405, Test accuracy: 84.31
Round  43, Train loss: 0.463, Test loss: 0.396, Test accuracy: 84.95
Round  44, Train loss: 0.362, Test loss: 0.400, Test accuracy: 84.53
Round  45, Train loss: 0.434, Test loss: 0.391, Test accuracy: 85.19
Round  46, Train loss: 0.373, Test loss: 0.382, Test accuracy: 85.51
Round  47, Train loss: 0.383, Test loss: 0.382, Test accuracy: 85.51
Round  48, Train loss: 0.421, Test loss: 0.386, Test accuracy: 85.14
Round  49, Train loss: 0.329, Test loss: 0.387, Test accuracy: 85.09
Round  50, Train loss: 0.268, Test loss: 0.373, Test accuracy: 85.79
Round  51, Train loss: 0.350, Test loss: 0.377, Test accuracy: 85.64
Round  52, Train loss: 0.331, Test loss: 0.374, Test accuracy: 85.54
Round  53, Train loss: 0.321, Test loss: 0.369, Test accuracy: 85.83
Round  54, Train loss: 0.301, Test loss: 0.366, Test accuracy: 86.30
Round  55, Train loss: 0.366, Test loss: 0.368, Test accuracy: 85.92
Round  56, Train loss: 0.306, Test loss: 0.351, Test accuracy: 86.45
Round  57, Train loss: 0.354, Test loss: 0.359, Test accuracy: 86.17
Round  58, Train loss: 0.285, Test loss: 0.368, Test accuracy: 85.50
Round  59, Train loss: 0.301, Test loss: 0.359, Test accuracy: 86.05
Round  60, Train loss: 0.346, Test loss: 0.358, Test accuracy: 86.17
Round  61, Train loss: 0.293, Test loss: 0.357, Test accuracy: 86.18
Round  62, Train loss: 0.429, Test loss: 0.359, Test accuracy: 86.05
Round  63, Train loss: 0.285, Test loss: 0.355, Test accuracy: 86.33
Round  64, Train loss: 0.379, Test loss: 0.357, Test accuracy: 86.30
Round  65, Train loss: 0.311, Test loss: 0.356, Test accuracy: 86.26
Round  66, Train loss: 0.274, Test loss: 0.352, Test accuracy: 86.19
Round  67, Train loss: 0.220, Test loss: 0.352, Test accuracy: 86.41
Round  68, Train loss: 0.215, Test loss: 0.353, Test accuracy: 86.28
Round  69, Train loss: 0.249, Test loss: 0.354, Test accuracy: 86.33
Round  70, Train loss: 0.249, Test loss: 0.353, Test accuracy: 86.49
Round  71, Train loss: 0.333, Test loss: 0.356, Test accuracy: 86.47
Round  72, Train loss: 0.283, Test loss: 0.359, Test accuracy: 86.47
Round  73, Train loss: 0.292, Test loss: 0.351, Test accuracy: 86.72
Round  74, Train loss: 0.210, Test loss: 0.354, Test accuracy: 86.60
Round  75, Train loss: 0.268, Test loss: 0.347, Test accuracy: 86.72
Round  76, Train loss: 0.299, Test loss: 0.350, Test accuracy: 86.77
Round  77, Train loss: 0.242, Test loss: 0.344, Test accuracy: 86.70
Round  78, Train loss: 0.258, Test loss: 0.341, Test accuracy: 87.04
Round  79, Train loss: 0.268, Test loss: 0.344, Test accuracy: 87.01
Round  80, Train loss: 0.249, Test loss: 0.342, Test accuracy: 86.95
Round  81, Train loss: 0.196, Test loss: 0.348, Test accuracy: 86.51
Round  82, Train loss: 0.261, Test loss: 0.346, Test accuracy: 86.71
Round  83, Train loss: 0.248, Test loss: 0.344, Test accuracy: 86.86
Round  84, Train loss: 0.276, Test loss: 0.336, Test accuracy: 87.22
Round  85, Train loss: 0.226, Test loss: 0.350, Test accuracy: 86.67
Round  86, Train loss: 0.297, Test loss: 0.334, Test accuracy: 87.18
Round  87, Train loss: 0.280, Test loss: 0.336, Test accuracy: 87.28
Round  88, Train loss: 0.189, Test loss: 0.346, Test accuracy: 86.77
Round  89, Train loss: 0.246, Test loss: 0.340, Test accuracy: 87.12
Round  90, Train loss: 0.285, Test loss: 0.338, Test accuracy: 87.25
Round  91, Train loss: 0.208, Test loss: 0.333, Test accuracy: 87.12
Round  92, Train loss: 0.243, Test loss: 0.341, Test accuracy: 86.99
Round  93, Train loss: 0.218, Test loss: 0.340, Test accuracy: 87.19
Round  94, Train loss: 0.199, Test loss: 0.338, Test accuracy: 87.05
Round  95, Train loss: 0.271, Test loss: 0.335, Test accuracy: 87.46
Round  96, Train loss: 0.256, Test loss: 0.341, Test accuracy: 86.99
Round  97, Train loss: 0.217, Test loss: 0.340, Test accuracy: 87.35
Round  98, Train loss: 0.182, Test loss: 0.344, Test accuracy: 86.83
Round  99, Train loss: 0.223, Test loss: 0.339, Test accuracy: 87.17
Final Round, Train loss: 0.177, Test loss: 0.336, Test accuracy: 87.47
Average accuracy final 10 rounds: 87.14083333333335
1705.394785642624
[2.1264870166778564, 4.252974033355713, 6.006672143936157, 7.760370254516602, 9.486616849899292, 11.212863445281982, 12.980887174606323, 14.748910903930664, 16.482911825180054, 18.216912746429443, 19.82479214668274, 21.432671546936035, 23.16531467437744, 24.897957801818848, 26.48731803894043, 28.07667827606201, 29.71672034263611, 31.356762409210205, 32.929882287979126, 34.50300216674805, 36.18115544319153, 37.85930871963501, 39.4784414768219, 41.09757423400879, 42.67575693130493, 44.253939628601074, 45.96778202056885, 47.68162441253662, 49.27741360664368, 50.87320280075073, 52.44534969329834, 54.01749658584595, 55.618643283843994, 57.21978998184204, 58.810348987579346, 60.40090799331665, 62.00917410850525, 63.61744022369385, 65.22478795051575, 66.83213567733765, 68.41520047187805, 69.99826526641846, 71.5879111289978, 73.17755699157715, 74.74482941627502, 76.3121018409729, 77.95115733146667, 79.59021282196045, 81.20688104629517, 82.82354927062988, 84.41530537605286, 86.00706148147583, 87.65358185768127, 89.30010223388672, 90.88622784614563, 92.47235345840454, 94.09407758712769, 95.71580171585083, 97.3391010761261, 98.96240043640137, 100.57768440246582, 102.19296836853027, 103.81214380264282, 105.43131923675537, 107.02335858345032, 108.61539793014526, 110.32923364639282, 112.04306936264038, 113.66541481018066, 115.28776025772095, 116.93227338790894, 118.57678651809692, 120.18539452552795, 121.79400253295898, 123.39576292037964, 124.9975233078003, 126.60782408714294, 128.2181248664856, 129.9478678703308, 131.67761087417603, 133.26935338974, 134.86109590530396, 136.48155283927917, 138.1020097732544, 139.71340107917786, 141.32479238510132, 142.9010751247406, 144.47735786437988, 146.0821831226349, 147.6870083808899, 149.29589223861694, 150.904776096344, 152.52100539207458, 154.13723468780518, 155.75855350494385, 157.37987232208252, 158.98678493499756, 160.5936975479126, 162.19826436042786, 163.80283117294312, 165.42051315307617, 167.03819513320923, 168.6664333343506, 170.29467153549194, 171.90369701385498, 173.51272249221802, 175.10389637947083, 176.69507026672363, 178.2967119216919, 179.89835357666016, 181.52516222000122, 183.15197086334229, 184.79860472679138, 186.44523859024048, 188.05868864059448, 189.6721386909485, 191.2574474811554, 192.8427562713623, 194.44547748565674, 196.04819869995117, 197.64339518547058, 199.23859167099, 200.85103750228882, 202.46348333358765, 204.07020735740662, 205.6769313812256, 207.30746388435364, 208.9379963874817, 210.56298232078552, 212.18796825408936, 213.79794239997864, 215.40791654586792, 217.00940895080566, 218.6109013557434, 220.22854232788086, 221.8461833000183, 223.43944001197815, 225.032696723938, 226.63365864753723, 228.23462057113647, 229.83265256881714, 231.4306845664978, 233.04982924461365, 234.6689739227295, 236.2988042831421, 237.9286346435547, 239.56188106536865, 241.19512748718262, 242.7926197052002, 244.39011192321777, 246.00563168525696, 247.62115144729614, 249.2330493927002, 250.84494733810425, 252.54209423065186, 254.23924112319946, 255.8503134250641, 257.4613857269287, 259.06937766075134, 260.677369594574, 262.28960275650024, 263.9018359184265, 265.49786162376404, 267.09388732910156, 268.7059066295624, 270.3179259300232, 271.9452795982361, 273.572633266449, 275.1910490989685, 276.80946493148804, 278.39320945739746, 279.9769539833069, 281.58320331573486, 283.18945264816284, 284.81175804138184, 286.43406343460083, 288.05547738075256, 289.6768913269043, 291.28731417655945, 292.8977370262146, 294.4960193634033, 296.09430170059204, 297.7321865558624, 299.3700714111328, 300.98743748664856, 302.6048035621643, 304.2481837272644, 305.8915638923645, 307.52803587913513, 309.16450786590576, 310.81700706481934, 312.4695062637329, 314.08881878852844, 315.708131313324, 317.31174874305725, 318.9153661727905, 320.5222120285034, 322.1290578842163, 323.7726216316223, 325.4161853790283, 327.56238865852356, 329.7085919380188]
[35.391666666666666, 35.391666666666666, 41.4, 41.4, 54.61666666666667, 54.61666666666667, 55.266666666666666, 55.266666666666666, 58.233333333333334, 58.233333333333334, 64.00833333333334, 64.00833333333334, 71.10833333333333, 71.10833333333333, 72.71666666666667, 72.71666666666667, 73.14166666666667, 73.14166666666667, 73.25833333333334, 73.25833333333334, 74.775, 74.775, 75.98333333333333, 75.98333333333333, 76.36666666666666, 76.36666666666666, 77.01666666666667, 77.01666666666667, 77.2, 77.2, 78.36666666666666, 78.36666666666666, 78.725, 78.725, 78.86666666666666, 78.86666666666666, 79.30833333333334, 79.30833333333334, 79.74166666666666, 79.74166666666666, 79.98333333333333, 79.98333333333333, 80.50833333333334, 80.50833333333334, 80.74166666666666, 80.74166666666666, 81.06666666666666, 81.06666666666666, 81.725, 81.725, 81.85833333333333, 81.85833333333333, 81.05, 81.05, 82.20833333333333, 82.20833333333333, 82.425, 82.425, 82.85, 82.85, 83.09166666666667, 83.09166666666667, 82.9, 82.9, 82.49166666666666, 82.49166666666666, 83.73333333333333, 83.73333333333333, 83.63333333333334, 83.63333333333334, 83.66666666666667, 83.66666666666667, 83.85, 83.85, 84.025, 84.025, 83.71666666666667, 83.71666666666667, 84.125, 84.125, 84.275, 84.275, 84.08333333333333, 84.08333333333333, 84.30833333333334, 84.30833333333334, 84.95, 84.95, 84.525, 84.525, 85.19166666666666, 85.19166666666666, 85.50833333333334, 85.50833333333334, 85.50833333333334, 85.50833333333334, 85.14166666666667, 85.14166666666667, 85.09166666666667, 85.09166666666667, 85.79166666666667, 85.79166666666667, 85.64166666666667, 85.64166666666667, 85.54166666666667, 85.54166666666667, 85.825, 85.825, 86.3, 86.3, 85.91666666666667, 85.91666666666667, 86.45, 86.45, 86.175, 86.175, 85.5, 85.5, 86.05, 86.05, 86.175, 86.175, 86.18333333333334, 86.18333333333334, 86.05, 86.05, 86.33333333333333, 86.33333333333333, 86.3, 86.3, 86.25833333333334, 86.25833333333334, 86.19166666666666, 86.19166666666666, 86.40833333333333, 86.40833333333333, 86.275, 86.275, 86.325, 86.325, 86.49166666666666, 86.49166666666666, 86.46666666666667, 86.46666666666667, 86.475, 86.475, 86.71666666666667, 86.71666666666667, 86.6, 86.6, 86.71666666666667, 86.71666666666667, 86.76666666666667, 86.76666666666667, 86.7, 86.7, 87.04166666666667, 87.04166666666667, 87.00833333333334, 87.00833333333334, 86.95, 86.95, 86.50833333333334, 86.50833333333334, 86.70833333333333, 86.70833333333333, 86.85833333333333, 86.85833333333333, 87.225, 87.225, 86.675, 86.675, 87.18333333333334, 87.18333333333334, 87.275, 87.275, 86.76666666666667, 86.76666666666667, 87.125, 87.125, 87.25, 87.25, 87.11666666666666, 87.11666666666666, 86.99166666666666, 86.99166666666666, 87.19166666666666, 87.19166666666666, 87.05, 87.05, 87.45833333333333, 87.45833333333333, 86.99166666666666, 86.99166666666666, 87.35, 87.35, 86.83333333333333, 86.83333333333333, 87.175, 87.175, 87.46666666666667, 87.46666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.144, Test loss: 2.378, Test accuracy: 18.33
Round   1, Train loss: 0.965, Test loss: 2.113, Test accuracy: 25.67
Round   2, Train loss: 0.890, Test loss: 2.418, Test accuracy: 33.31
Round   3, Train loss: 0.853, Test loss: 2.201, Test accuracy: 32.47
Round   4, Train loss: 0.728, Test loss: 2.333, Test accuracy: 28.43
Round   5, Train loss: 0.760, Test loss: 1.949, Test accuracy: 35.91
Round   6, Train loss: 0.775, Test loss: 2.061, Test accuracy: 33.27
Round   7, Train loss: 0.698, Test loss: 2.130, Test accuracy: 31.59
Round   8, Train loss: 0.742, Test loss: 1.667, Test accuracy: 40.70
Round   9, Train loss: 0.628, Test loss: 1.769, Test accuracy: 41.20
Round  10, Train loss: 0.694, Test loss: 1.548, Test accuracy: 45.84
Round  11, Train loss: 0.612, Test loss: 1.692, Test accuracy: 43.31
Round  12, Train loss: 0.584, Test loss: 1.704, Test accuracy: 40.58
Round  13, Train loss: 0.615, Test loss: 1.680, Test accuracy: 42.22
Round  14, Train loss: 0.600, Test loss: 1.600, Test accuracy: 47.12
Round  15, Train loss: 0.556, Test loss: 1.610, Test accuracy: 46.42
Round  16, Train loss: 0.643, Test loss: 1.591, Test accuracy: 44.17
Round  17, Train loss: 0.546, Test loss: 1.639, Test accuracy: 45.98
Round  18, Train loss: 0.467, Test loss: 1.386, Test accuracy: 52.01
Round  19, Train loss: 0.475, Test loss: 1.422, Test accuracy: 50.88
Round  20, Train loss: 0.548, Test loss: 1.464, Test accuracy: 50.77
Round  21, Train loss: 0.519, Test loss: 1.362, Test accuracy: 54.23
Round  22, Train loss: 0.402, Test loss: 1.385, Test accuracy: 52.42
Round  23, Train loss: 0.350, Test loss: 1.305, Test accuracy: 56.67
Round  24, Train loss: 0.519, Test loss: 1.348, Test accuracy: 55.89
Round  25, Train loss: 0.419, Test loss: 1.427, Test accuracy: 51.45
Round  26, Train loss: 0.495, Test loss: 1.286, Test accuracy: 56.30
Round  27, Train loss: 0.518, Test loss: 1.397, Test accuracy: 53.67
Round  28, Train loss: 0.383, Test loss: 1.500, Test accuracy: 54.70
Round  29, Train loss: 0.430, Test loss: 1.336, Test accuracy: 57.80
Round  30, Train loss: 0.513, Test loss: 1.740, Test accuracy: 44.59
Round  31, Train loss: 0.419, Test loss: 1.416, Test accuracy: 55.50
Round  32, Train loss: 0.413, Test loss: 1.249, Test accuracy: 56.24
Round  33, Train loss: 0.334, Test loss: 1.236, Test accuracy: 58.95
Round  34, Train loss: 0.428, Test loss: 1.089, Test accuracy: 62.79
Round  35, Train loss: 0.342, Test loss: 1.154, Test accuracy: 60.91
Round  36, Train loss: 0.352, Test loss: 1.336, Test accuracy: 55.35
Round  37, Train loss: 0.357, Test loss: 1.248, Test accuracy: 58.40
Round  38, Train loss: 0.341, Test loss: 1.271, Test accuracy: 57.96
Round  39, Train loss: 0.306, Test loss: 1.217, Test accuracy: 61.95
Round  40, Train loss: 0.280, Test loss: 1.163, Test accuracy: 61.63
Round  41, Train loss: 0.345, Test loss: 1.454, Test accuracy: 51.68
Round  42, Train loss: 0.253, Test loss: 1.171, Test accuracy: 61.77
Round  43, Train loss: 0.358, Test loss: 1.239, Test accuracy: 60.41
Round  44, Train loss: 0.350, Test loss: 1.268, Test accuracy: 59.88
Round  45, Train loss: 0.356, Test loss: 1.302, Test accuracy: 58.88
Round  46, Train loss: 0.337, Test loss: 1.312, Test accuracy: 57.90
Round  47, Train loss: 0.290, Test loss: 1.275, Test accuracy: 61.30
Round  48, Train loss: 0.260, Test loss: 1.480, Test accuracy: 57.63
Round  49, Train loss: 0.296, Test loss: 1.201, Test accuracy: 60.50
Round  50, Train loss: 0.347, Test loss: 1.278, Test accuracy: 61.77
Round  51, Train loss: 0.309, Test loss: 1.357, Test accuracy: 58.97
Round  52, Train loss: 0.360, Test loss: 1.445, Test accuracy: 56.65
Round  53, Train loss: 0.290, Test loss: 1.221, Test accuracy: 60.96
Round  54, Train loss: 0.320, Test loss: 1.278, Test accuracy: 57.70
Round  55, Train loss: 0.252, Test loss: 1.094, Test accuracy: 63.61
Round  56, Train loss: 0.265, Test loss: 1.295, Test accuracy: 59.08
Round  57, Train loss: 0.228, Test loss: 1.271, Test accuracy: 60.40
Round  58, Train loss: 0.284, Test loss: 1.419, Test accuracy: 56.47
Round  59, Train loss: 0.271, Test loss: 1.107, Test accuracy: 64.29
Round  60, Train loss: 0.276, Test loss: 1.056, Test accuracy: 65.03
Round  61, Train loss: 0.203, Test loss: 1.178, Test accuracy: 64.10
Round  62, Train loss: 0.273, Test loss: 1.331, Test accuracy: 59.85
Round  63, Train loss: 0.290, Test loss: 1.084, Test accuracy: 63.82
Round  64, Train loss: 0.158, Test loss: 1.235, Test accuracy: 61.82
Round  65, Train loss: 0.269, Test loss: 1.462, Test accuracy: 56.95
Round  66, Train loss: 0.299, Test loss: 1.260, Test accuracy: 60.49
Round  67, Train loss: 0.179, Test loss: 1.356, Test accuracy: 60.01
Round  68, Train loss: 0.277, Test loss: 1.195, Test accuracy: 62.83
Round  69, Train loss: 0.232, Test loss: 1.193, Test accuracy: 61.84
Round  70, Train loss: 0.268, Test loss: 1.089, Test accuracy: 64.30
Round  71, Train loss: 0.231, Test loss: 1.603, Test accuracy: 55.58
Round  72, Train loss: 0.209, Test loss: 1.269, Test accuracy: 61.17
Round  73, Train loss: 0.182, Test loss: 1.242, Test accuracy: 63.04
Round  74, Train loss: 0.237, Test loss: 1.155, Test accuracy: 63.87
Round  75, Train loss: 0.208, Test loss: 1.377, Test accuracy: 61.29
Round  76, Train loss: 0.197, Test loss: 1.279, Test accuracy: 62.40
Round  77, Train loss: 0.291, Test loss: 1.708, Test accuracy: 53.86
Round  78, Train loss: 0.165, Test loss: 1.454, Test accuracy: 58.10
Round  79, Train loss: 0.198, Test loss: 1.481, Test accuracy: 58.40
Round  80, Train loss: 0.184, Test loss: 1.692, Test accuracy: 55.13
Round  81, Train loss: 0.245, Test loss: 1.273, Test accuracy: 61.99
Round  82, Train loss: 0.214, Test loss: 1.199, Test accuracy: 64.60
Round  83, Train loss: 0.137, Test loss: 1.662, Test accuracy: 56.93
Round  84, Train loss: 0.176, Test loss: 1.520, Test accuracy: 59.50
Round  85, Train loss: 0.179, Test loss: 1.324, Test accuracy: 62.89
Round  86, Train loss: 0.187, Test loss: 1.213, Test accuracy: 63.98
Round  87, Train loss: 0.179, Test loss: 1.451, Test accuracy: 62.48
Round  88, Train loss: 0.143, Test loss: 1.445, Test accuracy: 61.96
Round  89, Train loss: 0.209, Test loss: 1.030, Test accuracy: 66.67
Round  90, Train loss: 0.174, Test loss: 1.599, Test accuracy: 59.52
Round  91, Train loss: 0.175, Test loss: 1.244, Test accuracy: 63.36
Round  92, Train loss: 0.191, Test loss: 1.275, Test accuracy: 63.14
Round  93, Train loss: 0.174, Test loss: 1.223, Test accuracy: 63.41
Round  94, Train loss: 0.141, Test loss: 1.449, Test accuracy: 62.15
Round  95, Train loss: 0.177, Test loss: 1.171, Test accuracy: 65.22
Round  96, Train loss: 0.203, Test loss: 1.632, Test accuracy: 58.30
Round  97, Train loss: 0.183, Test loss: 1.312, Test accuracy: 64.72
Round  98, Train loss: 0.167, Test loss: 1.424, Test accuracy: 63.34
Round  99, Train loss: 0.145, Test loss: 1.581, Test accuracy: 61.29
Final Round, Train loss: 0.151, Test loss: 1.184, Test accuracy: 65.78
Average accuracy final 10 rounds: 62.44499999999999
2490.819168806076
[3.9048562049865723, 7.570211172103882, 11.18273377418518, 14.791985511779785, 18.441410064697266, 22.035999059677124, 25.66119933128357, 29.26446557044983, 32.888142108917236, 36.51406145095825, 40.13220191001892, 43.703064918518066, 47.31633806228638, 50.90389370918274, 54.467154026031494, 58.06322264671326, 61.63748574256897, 65.23449087142944, 68.84305667877197, 72.45374751091003, 76.0667154788971, 79.68722105026245, 83.28771138191223, 86.88974356651306, 90.49205946922302, 94.09557318687439, 97.68939805030823, 101.2582631111145, 105.25395965576172, 109.25717282295227, 113.25492763519287, 117.1775963306427, 121.114177942276, 125.1352379322052, 128.62840700149536, 132.0883514881134, 135.5798237323761, 139.0784146785736, 142.57982325553894, 146.07781624794006, 149.5458846092224, 153.00080370903015, 156.50246810913086, 159.9735140800476, 163.4514980316162, 166.9320890903473, 170.41181087493896, 173.87620282173157, 177.37935662269592, 180.84083819389343, 184.30459761619568, 187.7914662361145, 191.29101300239563, 194.7887852191925, 198.25602316856384, 201.6929624080658, 205.17282128334045, 208.6844563484192, 212.0732295513153, 215.5459020137787, 219.03589344024658, 222.49279117584229, 225.97503185272217, 229.45385122299194, 232.90174555778503, 236.3500075340271, 239.78002190589905, 243.26469111442566, 246.6769061088562, 250.17471837997437, 253.60032439231873, 257.0241186618805, 260.49875569343567, 263.92818665504456, 267.37473583221436, 270.7693612575531, 274.19625902175903, 277.6244535446167, 281.0532178878784, 284.4876403808594, 287.93302512168884, 291.3720791339874, 294.7990872859955, 298.22158885002136, 301.5931432247162, 305.18314266204834, 308.5991005897522, 312.0900356769562, 315.5338833332062, 318.9523265361786, 322.34523606300354, 325.7704908847809, 329.16243624687195, 332.5964467525482, 336.0325217247009, 339.67392349243164, 343.1467673778534, 346.61779022216797, 350.23169136047363, 353.6700313091278, 356.5938494205475]
[18.333333333333332, 25.666666666666668, 33.30833333333333, 32.46666666666667, 28.425, 35.90833333333333, 33.275, 31.591666666666665, 40.7, 41.2, 45.84166666666667, 43.30833333333333, 40.583333333333336, 42.21666666666667, 47.11666666666667, 46.425, 44.175, 45.983333333333334, 52.00833333333333, 50.875, 50.775, 54.225, 52.425, 56.666666666666664, 55.891666666666666, 51.45, 56.3, 53.675, 54.7, 57.8, 44.59166666666667, 55.5, 56.24166666666667, 58.95, 62.791666666666664, 60.90833333333333, 55.35, 58.4, 57.958333333333336, 61.95, 61.63333333333333, 51.68333333333333, 61.766666666666666, 60.40833333333333, 59.875, 58.875, 57.9, 61.3, 57.63333333333333, 60.5, 61.766666666666666, 58.96666666666667, 56.65, 60.958333333333336, 57.7, 63.608333333333334, 59.075, 60.4, 56.46666666666667, 64.29166666666667, 65.03333333333333, 64.1, 59.85, 63.81666666666667, 61.81666666666667, 56.95, 60.49166666666667, 60.00833333333333, 62.825, 61.84166666666667, 64.3, 55.575, 61.166666666666664, 63.041666666666664, 63.86666666666667, 61.291666666666664, 62.4, 53.858333333333334, 58.1, 58.4, 55.13333333333333, 61.99166666666667, 64.6, 56.93333333333333, 59.5, 62.891666666666666, 63.975, 62.475, 61.958333333333336, 66.66666666666667, 59.516666666666666, 63.358333333333334, 63.141666666666666, 63.40833333333333, 62.15, 65.225, 58.3, 64.71666666666667, 63.34166666666667, 61.291666666666664, 65.775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.305, Test loss: 2.305, Test accuracy: 9.99
Round   0, Global train loss: 2.305, Global test loss: 2.305, Global test accuracy: 9.98
Round   1, Train loss: 2.305, Test loss: 2.305, Test accuracy: 10.00
Round   1, Global train loss: 2.305, Global test loss: 2.305, Global test accuracy: 9.99
Round   2, Train loss: 2.305, Test loss: 2.305, Test accuracy: 10.00
Round   2, Global train loss: 2.305, Global test loss: 2.305, Global test accuracy: 10.00
Round   3, Train loss: 2.305, Test loss: 2.305, Test accuracy: 10.01
Round   3, Global train loss: 2.305, Global test loss: 2.305, Global test accuracy: 10.00
Round   4, Train loss: 2.305, Test loss: 2.305, Test accuracy: 10.01
Round   4, Global train loss: 2.305, Global test loss: 2.305, Global test accuracy: 10.04
Round   5, Train loss: 2.304, Test loss: 2.305, Test accuracy: 10.02
Round   5, Global train loss: 2.304, Global test loss: 2.304, Global test accuracy: 10.04
Round   6, Train loss: 2.304, Test loss: 2.304, Test accuracy: 10.03
Round   6, Global train loss: 2.304, Global test loss: 2.304, Global test accuracy: 10.04
Round   7, Train loss: 2.304, Test loss: 2.304, Test accuracy: 10.04
Round   7, Global train loss: 2.304, Global test loss: 2.304, Global test accuracy: 10.05
Round   8, Train loss: 2.304, Test loss: 2.304, Test accuracy: 10.04
Round   8, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.05
Round   9, Train loss: 2.304, Test loss: 2.304, Test accuracy: 10.03
Round   9, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.06
Round  10, Train loss: 2.304, Test loss: 2.303, Test accuracy: 10.04
Round  10, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.04
Round  11, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.03
Round  11, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.03
Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.04
Round  12, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 10.04
Round  13, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.04
Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.04
Round  14, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.04
Round  14, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 10.03
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.04
Round  15, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.05
Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.06
Round  16, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.10
Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.08
Round  17, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.12
Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.10
Round  18, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.15
Round  19, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.12
Round  19, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.16
Round  20, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.09
Round  20, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.21
Round  21, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.13
Round  21, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.25
Round  22, Train loss: 2.300, Test loss: 2.300, Test accuracy: 10.11
Round  22, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 10.29
Round  23, Train loss: 2.301, Test loss: 2.300, Test accuracy: 10.15
Round  23, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 10.21
Round  24, Train loss: 2.300, Test loss: 2.300, Test accuracy: 10.18
Round  24, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 10.30
Round  25, Train loss: 2.300, Test loss: 2.300, Test accuracy: 10.14
Round  25, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 10.25
Round  26, Train loss: 2.300, Test loss: 2.300, Test accuracy: 10.16
Round  26, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 10.26
Round  27, Train loss: 2.300, Test loss: 2.299, Test accuracy: 10.23
Round  27, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 10.21
Round  28, Train loss: 2.299, Test loss: 2.299, Test accuracy: 10.28
Round  28, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 10.17
Round  29, Train loss: 2.299, Test loss: 2.299, Test accuracy: 10.28
Round  29, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 10.32
Round  30, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.29
Round  30, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 10.32
Round  31, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.29
Round  31, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 10.37
Round  32, Train loss: 2.298, Test loss: 2.298, Test accuracy: 10.33
Round  32, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 10.39
Round  33, Train loss: 2.298, Test loss: 2.298, Test accuracy: 10.52
Round  33, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 10.54
Round  34, Train loss: 2.298, Test loss: 2.297, Test accuracy: 10.55
Round  34, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 10.41
Round  35, Train loss: 2.298, Test loss: 2.297, Test accuracy: 10.55
Round  35, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 10.46
Round  36, Train loss: 2.298, Test loss: 2.297, Test accuracy: 10.49
Round  36, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 10.39
Round  37, Train loss: 2.297, Test loss: 2.297, Test accuracy: 10.59
Round  37, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 10.55
Round  38, Train loss: 2.297, Test loss: 2.297, Test accuracy: 10.69
Round  38, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 10.62
Round  39, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.72
Round  39, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 10.65
Round  40, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.81
Round  40, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.84
Round  41, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.91
Round  41, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.77
Round  42, Train loss: 2.296, Test loss: 2.295, Test accuracy: 10.98
Round  42, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 10.80
Round  43, Train loss: 2.296, Test loss: 2.295, Test accuracy: 10.93
Round  43, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 10.66
Round  44, Train loss: 2.295, Test loss: 2.294, Test accuracy: 10.99
Round  44, Global train loss: 2.295, Global test loss: 2.294, Global test accuracy: 10.89
Round  45, Train loss: 2.295, Test loss: 2.294, Test accuracy: 11.00
Round  45, Global train loss: 2.295, Global test loss: 2.294, Global test accuracy: 10.97
Round  46, Train loss: 2.295, Test loss: 2.294, Test accuracy: 10.91
Round  46, Global train loss: 2.295, Global test loss: 2.294, Global test accuracy: 10.76
Round  47, Train loss: 2.295, Test loss: 2.294, Test accuracy: 10.85
Round  47, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 10.90
Round  48, Train loss: 2.294, Test loss: 2.294, Test accuracy: 10.94
Round  48, Global train loss: 2.294, Global test loss: 2.293, Global test accuracy: 11.16
Round  49, Train loss: 2.294, Test loss: 2.293, Test accuracy: 11.05
Round  49, Global train loss: 2.294, Global test loss: 2.293, Global test accuracy: 10.98
Round  50, Train loss: 2.294, Test loss: 2.293, Test accuracy: 11.01
Round  50, Global train loss: 2.294, Global test loss: 2.292, Global test accuracy: 11.11
Round  51, Train loss: 2.294, Test loss: 2.293, Test accuracy: 11.06
Round  51, Global train loss: 2.294, Global test loss: 2.292, Global test accuracy: 11.70
Round  52, Train loss: 2.294, Test loss: 2.292, Test accuracy: 11.14
Round  52, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 11.76
Round  53, Train loss: 2.293, Test loss: 2.292, Test accuracy: 11.36
Round  53, Global train loss: 2.293, Global test loss: 2.291, Global test accuracy: 11.66
Round  54, Train loss: 2.293, Test loss: 2.292, Test accuracy: 11.49
Round  54, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 11.74
Round  55, Train loss: 2.293, Test loss: 2.291, Test accuracy: 11.75
Round  55, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 11.98
Round  56, Train loss: 2.292, Test loss: 2.291, Test accuracy: 11.84
Round  56, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 12.19
Round  57, Train loss: 2.292, Test loss: 2.290, Test accuracy: 12.04
Round  57, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 12.18
Round  58, Train loss: 2.292, Test loss: 2.290, Test accuracy: 12.20
Round  58, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 12.30
Round  59, Train loss: 2.291, Test loss: 2.289, Test accuracy: 12.26
Round  59, Global train loss: 2.291, Global test loss: 2.289, Global test accuracy: 12.19
Round  60, Train loss: 2.291, Test loss: 2.289, Test accuracy: 12.23
Round  60, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 12.13
Round  61, Train loss: 2.291, Test loss: 2.289, Test accuracy: 12.41
Round  61, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 12.52
Round  62, Train loss: 2.291, Test loss: 2.288, Test accuracy: 12.58
Round  62, Global train loss: 2.291, Global test loss: 2.287, Global test accuracy: 12.86
Round  63, Train loss: 2.290, Test loss: 2.288, Test accuracy: 12.69
Round  63, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 12.59
Round  64, Train loss: 2.290, Test loss: 2.288, Test accuracy: 12.68
Round  64, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 12.38
Round  65, Train loss: 2.291, Test loss: 2.287, Test accuracy: 12.70
Round  65, Global train loss: 2.291, Global test loss: 2.286, Global test accuracy: 12.69
Round  66, Train loss: 2.290, Test loss: 2.287, Test accuracy: 12.58
Round  66, Global train loss: 2.290, Global test loss: 2.286, Global test accuracy: 12.44
Round  67, Train loss: 2.289, Test loss: 2.286, Test accuracy: 12.71
Round  67, Global train loss: 2.289, Global test loss: 2.285, Global test accuracy: 12.78
Round  68, Train loss: 2.289, Test loss: 2.285, Test accuracy: 12.77
Round  68, Global train loss: 2.289, Global test loss: 2.285, Global test accuracy: 13.09
Round  69, Train loss: 2.289, Test loss: 2.285, Test accuracy: 12.92
Round  69, Global train loss: 2.289, Global test loss: 2.284, Global test accuracy: 13.09
Round  70, Train loss: 2.288, Test loss: 2.285, Test accuracy: 12.83
Round  70, Global train loss: 2.288, Global test loss: 2.284, Global test accuracy: 12.79
Round  71, Train loss: 2.288, Test loss: 2.284, Test accuracy: 12.95
Round  71, Global train loss: 2.288, Global test loss: 2.283, Global test accuracy: 13.02
Round  72, Train loss: 2.288, Test loss: 2.284, Test accuracy: 12.94
Round  72, Global train loss: 2.288, Global test loss: 2.282, Global test accuracy: 12.92
Round  73, Train loss: 2.288, Test loss: 2.283, Test accuracy: 13.12
Round  73, Global train loss: 2.288, Global test loss: 2.282, Global test accuracy: 13.05
Round  74, Train loss: 2.287, Test loss: 2.282, Test accuracy: 13.07
Round  74, Global train loss: 2.287, Global test loss: 2.281, Global test accuracy: 13.10
Round  75, Train loss: 2.287, Test loss: 2.282, Test accuracy: 12.94
Round  75, Global train loss: 2.287, Global test loss: 2.281, Global test accuracy: 13.14
Round  76, Train loss: 2.286, Test loss: 2.282, Test accuracy: 12.94
Round  76, Global train loss: 2.286, Global test loss: 2.280, Global test accuracy: 13.01
Round  77, Train loss: 2.285, Test loss: 2.281, Test accuracy: 13.12
Round  77, Global train loss: 2.285, Global test loss: 2.279, Global test accuracy: 13.39
Round  78, Train loss: 2.285, Test loss: 2.280, Test accuracy: 13.40
Round  78, Global train loss: 2.285, Global test loss: 2.278, Global test accuracy: 13.51
Round  79, Train loss: 2.284, Test loss: 2.279, Test accuracy: 13.58
Round  79, Global train loss: 2.284, Global test loss: 2.278, Global test accuracy: 13.59
Round  80, Train loss: 2.284, Test loss: 2.278, Test accuracy: 13.50
Round  80, Global train loss: 2.284, Global test loss: 2.277, Global test accuracy: 13.35
Round  81, Train loss: 2.283, Test loss: 2.278, Test accuracy: 13.37
Round  81, Global train loss: 2.283, Global test loss: 2.276, Global test accuracy: 13.22
Round  82, Train loss: 2.284, Test loss: 2.277, Test accuracy: 13.42
Round  82, Global train loss: 2.284, Global test loss: 2.275, Global test accuracy: 13.72
Round  83, Train loss: 2.283, Test loss: 2.277, Test accuracy: 13.43
Round  83, Global train loss: 2.283, Global test loss: 2.275, Global test accuracy: 13.79
Round  84, Train loss: 2.282, Test loss: 2.276, Test accuracy: 13.43
Round  84, Global train loss: 2.282, Global test loss: 2.274, Global test accuracy: 13.61
Round  85, Train loss: 2.283, Test loss: 2.275, Test accuracy: 13.53
Round  85, Global train loss: 2.283, Global test loss: 2.273, Global test accuracy: 13.51
Round  86, Train loss: 2.282, Test loss: 2.274, Test accuracy: 13.62
Round  86, Global train loss: 2.282, Global test loss: 2.272, Global test accuracy: 13.36
Round  87, Train loss: 2.281, Test loss: 2.273, Test accuracy: 13.57
Round  87, Global train loss: 2.281, Global test loss: 2.272, Global test accuracy: 13.30
Round  88, Train loss: 2.282, Test loss: 2.272, Test accuracy: 13.58
Round  88, Global train loss: 2.282, Global test loss: 2.271, Global test accuracy: 13.55
Round  89, Train loss: 2.282, Test loss: 2.272, Test accuracy: 13.86
Round  89, Global train loss: 2.282, Global test loss: 2.270, Global test accuracy: 13.84
Round  90, Train loss: 2.280, Test loss: 2.271, Test accuracy: 14.33
Round  90, Global train loss: 2.280, Global test loss: 2.270, Global test accuracy: 14.85
Round  91, Train loss: 2.280, Test loss: 2.271, Test accuracy: 14.71
Round  91, Global train loss: 2.280, Global test loss: 2.269, Global test accuracy: 15.86
Round  92, Train loss: 2.279, Test loss: 2.270, Test accuracy: 15.23
Round  92, Global train loss: 2.279, Global test loss: 2.268, Global test accuracy: 15.81
Round  93, Train loss: 2.279, Test loss: 2.270, Test accuracy: 15.34
Round  93, Global train loss: 2.279, Global test loss: 2.268, Global test accuracy: 16.16
Round  94, Train loss: 2.278, Test loss: 2.269, Test accuracy: 15.49
Round  94, Global train loss: 2.278, Global test loss: 2.267, Global test accuracy: 15.99
Round  95, Train loss: 2.276, Test loss: 2.268, Test accuracy: 15.60
Round  95, Global train loss: 2.276, Global test loss: 2.266, Global test accuracy: 15.82
Round  96, Train loss: 2.277, Test loss: 2.267, Test accuracy: 15.63
Round  96, Global train loss: 2.277, Global test loss: 2.265, Global test accuracy: 15.76
Round  97, Train loss: 2.276, Test loss: 2.267, Test accuracy: 15.67
Round  97, Global train loss: 2.276, Global test loss: 2.264, Global test accuracy: 16.31
Round  98, Train loss: 2.275, Test loss: 2.266, Test accuracy: 15.78
Round  98, Global train loss: 2.275, Global test loss: 2.264, Global test accuracy: 16.44
Round  99, Train loss: 2.274, Test loss: 2.265, Test accuracy: 15.95
Round  99, Global train loss: 2.274, Global test loss: 2.263, Global test accuracy: 16.57
Round 100, Train loss: 2.273, Test loss: 2.264, Test accuracy: 16.12
Round 100, Global train loss: 2.273, Global test loss: 2.262, Global test accuracy: 16.05
Round 101, Train loss: 2.273, Test loss: 2.264, Test accuracy: 16.31
Round 101, Global train loss: 2.273, Global test loss: 2.262, Global test accuracy: 16.48
Round 102, Train loss: 2.272, Test loss: 2.263, Test accuracy: 16.42
Round 102, Global train loss: 2.272, Global test loss: 2.261, Global test accuracy: 16.43
Round 103, Train loss: 2.270, Test loss: 2.263, Test accuracy: 16.30
Round 103, Global train loss: 2.270, Global test loss: 2.260, Global test accuracy: 16.61
Round 104, Train loss: 2.271, Test loss: 2.262, Test accuracy: 16.48
Round 104, Global train loss: 2.271, Global test loss: 2.260, Global test accuracy: 16.95
Round 105, Train loss: 2.270, Test loss: 2.261, Test accuracy: 16.55
Round 105, Global train loss: 2.270, Global test loss: 2.259, Global test accuracy: 17.12
Round 106, Train loss: 2.270, Test loss: 2.261, Test accuracy: 16.66
Round 106, Global train loss: 2.270, Global test loss: 2.258, Global test accuracy: 17.10
Round 107, Train loss: 2.269, Test loss: 2.260, Test accuracy: 16.55
Round 107, Global train loss: 2.269, Global test loss: 2.257, Global test accuracy: 16.64
Round 108, Train loss: 2.269, Test loss: 2.259, Test accuracy: 16.54
Round 108, Global train loss: 2.269, Global test loss: 2.256, Global test accuracy: 16.91
Round 109, Train loss: 2.268, Test loss: 2.259, Test accuracy: 16.51
Round 109, Global train loss: 2.268, Global test loss: 2.257, Global test accuracy: 16.80
Round 110, Train loss: 2.268, Test loss: 2.258, Test accuracy: 16.52
Round 110, Global train loss: 2.268, Global test loss: 2.256, Global test accuracy: 16.62
Round 111, Train loss: 2.268, Test loss: 2.257, Test accuracy: 16.61
Round 111, Global train loss: 2.268, Global test loss: 2.255, Global test accuracy: 16.98
Round 112, Train loss: 2.266, Test loss: 2.257, Test accuracy: 16.16
Round 112, Global train loss: 2.266, Global test loss: 2.256, Global test accuracy: 15.59
Round 113, Train loss: 2.267, Test loss: 2.257, Test accuracy: 16.23
Round 113, Global train loss: 2.267, Global test loss: 2.256, Global test accuracy: 15.90
Round 114, Train loss: 2.265, Test loss: 2.256, Test accuracy: 16.24
Round 114, Global train loss: 2.265, Global test loss: 2.254, Global test accuracy: 16.67
Round 115, Train loss: 2.265, Test loss: 2.255, Test accuracy: 16.52
Round 115, Global train loss: 2.265, Global test loss: 2.253, Global test accuracy: 17.09
Round 116, Train loss: 2.263, Test loss: 2.254, Test accuracy: 16.52
Round 116, Global train loss: 2.263, Global test loss: 2.252, Global test accuracy: 16.95
Round 117, Train loss: 2.263, Test loss: 2.253, Test accuracy: 16.91
Round 117, Global train loss: 2.263, Global test loss: 2.250, Global test accuracy: 18.01
Round 118, Train loss: 2.264, Test loss: 2.252, Test accuracy: 17.57
Round 118, Global train loss: 2.264, Global test loss: 2.250, Global test accuracy: 19.52
Round 119, Train loss: 2.263, Test loss: 2.252, Test accuracy: 18.10
Round 119, Global train loss: 2.263, Global test loss: 2.250, Global test accuracy: 20.00
Round 120, Train loss: 2.263, Test loss: 2.251, Test accuracy: 18.43
Round 120, Global train loss: 2.263, Global test loss: 2.249, Global test accuracy: 19.96
Round 121, Train loss: 2.262, Test loss: 2.250, Test accuracy: 18.84
Round 121, Global train loss: 2.262, Global test loss: 2.247, Global test accuracy: 20.42
Round 122, Train loss: 2.260, Test loss: 2.249, Test accuracy: 18.82
Round 122, Global train loss: 2.260, Global test loss: 2.247, Global test accuracy: 19.86
Round 123, Train loss: 2.262, Test loss: 2.248, Test accuracy: 19.26
Round 123, Global train loss: 2.262, Global test loss: 2.245, Global test accuracy: 20.01
Round 124, Train loss: 2.260, Test loss: 2.246, Test accuracy: 19.60
Round 124, Global train loss: 2.260, Global test loss: 2.244, Global test accuracy: 20.32
Round 125, Train loss: 2.258, Test loss: 2.246, Test accuracy: 19.54
Round 125, Global train loss: 2.258, Global test loss: 2.243, Global test accuracy: 20.30
Round 126, Train loss: 2.255, Test loss: 2.244, Test accuracy: 19.94
Round 126, Global train loss: 2.255, Global test loss: 2.241, Global test accuracy: 20.51
Round 127, Train loss: 2.256, Test loss: 2.243, Test accuracy: 20.09
Round 127, Global train loss: 2.256, Global test loss: 2.241, Global test accuracy: 19.90
Round 128, Train loss: 2.254, Test loss: 2.243, Test accuracy: 19.99
Round 128, Global train loss: 2.254, Global test loss: 2.241, Global test accuracy: 19.95
Round 129, Train loss: 2.255, Test loss: 2.242, Test accuracy: 20.09
Round 129, Global train loss: 2.255, Global test loss: 2.239, Global test accuracy: 20.24
Round 130, Train loss: 2.257, Test loss: 2.241, Test accuracy: 19.81
Round 130, Global train loss: 2.257, Global test loss: 2.237, Global test accuracy: 20.09
Round 131, Train loss: 2.253, Test loss: 2.240, Test accuracy: 20.15
Round 131, Global train loss: 2.253, Global test loss: 2.237, Global test accuracy: 20.60
Round 132, Train loss: 2.254, Test loss: 2.239, Test accuracy: 20.27
Round 132, Global train loss: 2.254, Global test loss: 2.236, Global test accuracy: 20.50
Round 133, Train loss: 2.252, Test loss: 2.238, Test accuracy: 20.43
Round 133, Global train loss: 2.252, Global test loss: 2.235, Global test accuracy: 20.73
Round 134, Train loss: 2.251, Test loss: 2.237, Test accuracy: 20.55
Round 134, Global train loss: 2.251, Global test loss: 2.234, Global test accuracy: 21.69
Round 135, Train loss: 2.250, Test loss: 2.236, Test accuracy: 20.81
Round 135, Global train loss: 2.250, Global test loss: 2.232, Global test accuracy: 22.12
Round 136, Train loss: 2.250, Test loss: 2.236, Test accuracy: 21.10
Round 136, Global train loss: 2.250, Global test loss: 2.232, Global test accuracy: 22.47
Round 137, Train loss: 2.249, Test loss: 2.233, Test accuracy: 21.38
Round 137, Global train loss: 2.249, Global test loss: 2.231, Global test accuracy: 22.51
Round 138, Train loss: 2.250, Test loss: 2.232, Test accuracy: 21.54
Round 138, Global train loss: 2.250, Global test loss: 2.228, Global test accuracy: 23.10
Round 139, Train loss: 2.247, Test loss: 2.231, Test accuracy: 21.67
Round 139, Global train loss: 2.247, Global test loss: 2.228, Global test accuracy: 22.68
Round 140, Train loss: 2.246, Test loss: 2.230, Test accuracy: 21.86
Round 140, Global train loss: 2.246, Global test loss: 2.227, Global test accuracy: 22.76
Round 141, Train loss: 2.247, Test loss: 2.229, Test accuracy: 22.19
Round 141, Global train loss: 2.247, Global test loss: 2.226, Global test accuracy: 23.23
Round 142, Train loss: 2.243, Test loss: 2.227, Test accuracy: 22.43
Round 142, Global train loss: 2.243, Global test loss: 2.225, Global test accuracy: 22.78
Round 143, Train loss: 2.245, Test loss: 2.227, Test accuracy: 22.30
Round 143, Global train loss: 2.245, Global test loss: 2.224, Global test accuracy: 22.88
Round 144, Train loss: 2.243, Test loss: 2.226, Test accuracy: 22.20
Round 144, Global train loss: 2.243, Global test loss: 2.223, Global test accuracy: 22.84
Round 145, Train loss: 2.243, Test loss: 2.225, Test accuracy: 22.27
Round 145, Global train loss: 2.243, Global test loss: 2.221, Global test accuracy: 22.60
Round 146, Train loss: 2.242, Test loss: 2.223, Test accuracy: 22.39
Round 146, Global train loss: 2.242, Global test loss: 2.218, Global test accuracy: 22.82
Round 147, Train loss: 2.242, Test loss: 2.220, Test accuracy: 22.57
Round 147, Global train loss: 2.242, Global test loss: 2.218, Global test accuracy: 22.77
Round 148, Train loss: 2.241, Test loss: 2.220, Test accuracy: 22.37
Round 148, Global train loss: 2.241, Global test loss: 2.219, Global test accuracy: 22.36
Round 149, Train loss: 2.241, Test loss: 2.220, Test accuracy: 22.07
Round 149, Global train loss: 2.241, Global test loss: 2.219, Global test accuracy: 21.62
Round 150, Train loss: 2.240, Test loss: 2.220, Test accuracy: 22.20
Round 150, Global train loss: 2.240, Global test loss: 2.217, Global test accuracy: 22.32
Round 151, Train loss: 2.239, Test loss: 2.218, Test accuracy: 21.97
Round 151, Global train loss: 2.239, Global test loss: 2.216, Global test accuracy: 21.27
Round 152, Train loss: 2.238, Test loss: 2.218, Test accuracy: 21.50
Round 152, Global train loss: 2.238, Global test loss: 2.217, Global test accuracy: 21.13
Round 153, Train loss: 2.235, Test loss: 2.217, Test accuracy: 21.40
Round 153, Global train loss: 2.235, Global test loss: 2.214, Global test accuracy: 21.76
Round 154, Train loss: 2.236, Test loss: 2.215, Test accuracy: 21.21
Round 154, Global train loss: 2.236, Global test loss: 2.212, Global test accuracy: 21.91
Round 155, Train loss: 2.234, Test loss: 2.213, Test accuracy: 21.44
Round 155, Global train loss: 2.234, Global test loss: 2.208, Global test accuracy: 22.20
Round 156, Train loss: 2.233, Test loss: 2.210, Test accuracy: 21.76
Round 156, Global train loss: 2.233, Global test loss: 2.205, Global test accuracy: 22.83
Round 157, Train loss: 2.236, Test loss: 2.210, Test accuracy: 21.84
Round 157, Global train loss: 2.236, Global test loss: 2.206, Global test accuracy: 22.56
Round 158, Train loss: 2.231, Test loss: 2.209, Test accuracy: 21.98
Round 158, Global train loss: 2.231, Global test loss: 2.204, Global test accuracy: 22.81
Round 159, Train loss: 2.233, Test loss: 2.206, Test accuracy: 22.03
Round 159, Global train loss: 2.233, Global test loss: 2.202, Global test accuracy: 22.38
Round 160, Train loss: 2.230, Test loss: 2.205, Test accuracy: 21.89
Round 160, Global train loss: 2.230, Global test loss: 2.201, Global test accuracy: 21.47
Round 161, Train loss: 2.230, Test loss: 2.202, Test accuracy: 21.93
Round 161, Global train loss: 2.230, Global test loss: 2.199, Global test accuracy: 21.73
Round 162, Train loss: 2.229, Test loss: 2.202, Test accuracy: 21.79
Round 162, Global train loss: 2.229, Global test loss: 2.199, Global test accuracy: 21.44
Round 163, Train loss: 2.228, Test loss: 2.201, Test accuracy: 21.79
Round 163, Global train loss: 2.228, Global test loss: 2.198, Global test accuracy: 21.34
Round 164, Train loss: 2.229, Test loss: 2.199, Test accuracy: 21.79
Round 164, Global train loss: 2.229, Global test loss: 2.195, Global test accuracy: 21.86
Round 165, Train loss: 2.227, Test loss: 2.198, Test accuracy: 22.11
Round 165, Global train loss: 2.227, Global test loss: 2.191, Global test accuracy: 22.79
Round 166, Train loss: 2.228, Test loss: 2.194, Test accuracy: 21.93
Round 166, Global train loss: 2.228, Global test loss: 2.190, Global test accuracy: 22.15
Round 167, Train loss: 2.225, Test loss: 2.193, Test accuracy: 22.09
Round 167, Global train loss: 2.225, Global test loss: 2.187, Global test accuracy: 22.82
Round 168, Train loss: 2.223, Test loss: 2.192, Test accuracy: 22.29
Round 168, Global train loss: 2.223, Global test loss: 2.185, Global test accuracy: 22.86
Round 169, Train loss: 2.224, Test loss: 2.189, Test accuracy: 22.29
Round 169, Global train loss: 2.224, Global test loss: 2.184, Global test accuracy: 22.66
Round 170, Train loss: 2.223, Test loss: 2.188, Test accuracy: 22.21
Round 170, Global train loss: 2.223, Global test loss: 2.182, Global test accuracy: 22.55
Round 171, Train loss: 2.227, Test loss: 2.184, Test accuracy: 22.19
Round 171, Global train loss: 2.227, Global test loss: 2.180, Global test accuracy: 22.55
Round 172, Train loss: 2.223, Test loss: 2.182, Test accuracy: 22.28
Round 172, Global train loss: 2.223, Global test loss: 2.179, Global test accuracy: 22.53
Round 173, Train loss: 2.228, Test loss: 2.180, Test accuracy: 22.28
Round 173, Global train loss: 2.228, Global test loss: 2.177, Global test accuracy: 22.68
Round 174, Train loss: 2.225, Test loss: 2.180, Test accuracy: 22.27
Round 174, Global train loss: 2.225, Global test loss: 2.176, Global test accuracy: 22.99
Round 175, Train loss: 2.221, Test loss: 2.178, Test accuracy: 22.36
Round 175, Global train loss: 2.221, Global test loss: 2.177, Global test accuracy: 23.04
Round 176, Train loss: 2.224, Test loss: 2.178, Test accuracy: 22.43
Round 176, Global train loss: 2.224, Global test loss: 2.175, Global test accuracy: 23.14
Round 177, Train loss: 2.222, Test loss: 2.177, Test accuracy: 22.62
Round 177, Global train loss: 2.222, Global test loss: 2.174, Global test accuracy: 22.60
Round 178, Train loss: 2.221, Test loss: 2.177, Test accuracy: 22.42
Round 178, Global train loss: 2.221, Global test loss: 2.175, Global test accuracy: 22.38
Round 179, Train loss: 2.221, Test loss: 2.176, Test accuracy: 22.22
Round 179, Global train loss: 2.221, Global test loss: 2.174, Global test accuracy: 22.00
Round 180, Train loss: 2.220, Test loss: 2.176, Test accuracy: 22.14
Round 180, Global train loss: 2.220, Global test loss: 2.174, Global test accuracy: 21.79
Round 181, Train loss: 2.220, Test loss: 2.175, Test accuracy: 22.15
Round 181, Global train loss: 2.220, Global test loss: 2.174, Global test accuracy: 22.18
Round 182, Train loss: 2.217, Test loss: 2.175, Test accuracy: 22.03
Round 182, Global train loss: 2.217, Global test loss: 2.174, Global test accuracy: 22.21
Round 183, Train loss: 2.215, Test loss: 2.175, Test accuracy: 22.02
Round 183, Global train loss: 2.215, Global test loss: 2.174, Global test accuracy: 22.06
Round 184, Train loss: 2.217, Test loss: 2.175, Test accuracy: 21.93
Round 184, Global train loss: 2.217, Global test loss: 2.173, Global test accuracy: 21.86
Round 185, Train loss: 2.217, Test loss: 2.174, Test accuracy: 21.64
Round 185, Global train loss: 2.217, Global test loss: 2.171, Global test accuracy: 22.14
Round 186, Train loss: 2.213, Test loss: 2.173, Test accuracy: 21.63
Round 186, Global train loss: 2.213, Global test loss: 2.168, Global test accuracy: 22.16
Round 187, Train loss: 2.213, Test loss: 2.171, Test accuracy: 21.62
Round 187, Global train loss: 2.213, Global test loss: 2.166, Global test accuracy: 22.23
Round 188, Train loss: 2.210, Test loss: 2.170, Test accuracy: 21.90
Round 188, Global train loss: 2.210, Global test loss: 2.164, Global test accuracy: 22.14
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 20.83
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 17.95
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 15.01
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 13.72
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 11.16
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

15914.046528339386
[5.450862884521484, 10.55721926689148, 15.695327997207642, 20.981921672821045, 26.204752922058105, 31.28299307823181, 36.434711933135986, 41.59097385406494, 46.64418578147888, 51.831753969192505, 56.91569209098816, 62.086278200149536, 67.31589269638062, 72.40305924415588, 77.48526287078857, 82.63029646873474, 87.76637983322144, 92.94157528877258, 98.13497281074524, 103.11315965652466, 108.34387350082397, 113.50037574768066, 118.58535647392273, 123.73113870620728, 128.91775250434875, 134.0616295337677, 139.3041045665741, 144.34796833992004, 149.4415099620819, 154.55398058891296, 159.5897262096405, 164.75108098983765, 169.90336322784424, 175.12349367141724, 180.38881468772888, 185.59912538528442, 190.8900330066681, 195.95269989967346, 201.0034646987915, 206.14124631881714, 211.2156479358673, 216.4647045135498, 221.54874968528748, 226.75303316116333, 231.93797659873962, 237.01438236236572, 242.2391881942749, 247.33752274513245, 252.48246598243713, 257.66830801963806, 262.76287603378296, 267.8687160015106, 273.09137201309204, 278.1695816516876, 283.1935956478119, 288.40915060043335, 293.5366487503052, 298.69785475730896, 303.9610893726349, 309.11125349998474, 314.39214873313904, 319.6298098564148, 324.81516194343567, 330.0123589038849, 335.16236186027527, 340.394522190094, 345.6201148033142, 350.6807084083557, 355.764568567276, 360.9744760990143, 366.1441864967346, 371.18168473243713, 376.42093777656555, 381.635324716568, 386.8720426559448, 392.1208565235138, 397.3182876110077, 402.5348937511444, 407.6772005558014, 412.23892974853516, 416.80644488334656, 421.3523094654083, 425.93009519577026, 430.4608905315399, 435.0148079395294, 439.5579950809479, 444.1100854873657, 448.70022797584534, 453.295282125473, 457.88947677612305, 463.5352272987366, 469.1235308647156, 474.7529253959656, 480.3868658542633, 486.0578238964081, 491.6736397743225, 497.29687333106995, 502.91507053375244, 508.57857966423035, 514.2629446983337, 519.9323244094849, 525.5955080986023, 531.2806532382965, 536.9321591854095, 542.5940062999725, 548.2754411697388, 553.9379570484161, 559.6329684257507, 565.2996733188629, 570.9336125850677, 576.5967547893524, 582.2062268257141, 587.7175350189209, 593.3280189037323, 598.8942167758942, 604.523603439331, 610.2131979465485, 615.7989513874054, 621.2274518013, 626.842746257782, 632.4225499629974, 638.0755271911621, 643.664698600769, 649.2773268222809, 654.6834471225739, 660.3704853057861, 665.981066942215, 671.5941548347473, 677.0861783027649, 682.0667822360992, 686.8746190071106, 691.6780099868774, 696.9564688205719, 702.1668815612793, 707.0659828186035, 711.886666059494, 716.6924359798431, 721.4881167411804, 726.2809717655182, 731.076099395752, 735.9344124794006, 740.7922480106354, 745.6291093826294, 750.4808514118195, 755.3631253242493, 760.1304857730865, 764.919527053833, 769.7351911067963, 774.6041467189789, 779.50901222229, 784.363573551178, 789.2048833370209, 794.0255017280579, 799.3784596920013, 804.7335367202759, 809.5104870796204, 814.2752184867859, 819.0942203998566, 823.9377748966217, 828.6800327301025, 833.451169013977, 838.2634029388428, 842.9861340522766, 847.7564172744751, 852.5245764255524, 857.4126904010773, 862.2613081932068, 867.087420463562, 871.8696715831757, 876.7238719463348, 881.5655691623688, 886.3939530849457, 891.742089509964, 897.116250038147, 902.4681916236877, 907.7686991691589, 913.0392661094666, 918.356231212616, 923.6330707073212, 928.8550419807434, 934.1999111175537, 939.2259647846222, 944.1883318424225, 949.155725479126, 954.1024224758148, 959.0277194976807, 963.9016072750092, 968.7848281860352, 973.6681160926819, 978.6123480796814, 983.5538482666016, 988.4497578144073, 993.3691756725311, 998.3668169975281, 1003.3089365959167, 1008.1842143535614, 1013.1425783634186, 1018.0767402648926, 1023.0990223884583, 1028.0707795619965, 1032.9652962684631, 1037.8930041790009, 1042.9350097179413, 1047.8828942775726, 1052.880387544632, 1057.7398726940155, 1062.6719634532928, 1067.632426738739, 1072.660516500473, 1077.6490242481232, 1082.6758449077606, 1087.55703997612, 1092.4688518047333, 1097.3801555633545, 1102.3627891540527, 1107.3197858333588, 1112.1910054683685, 1117.15114736557, 1122.0218489170074, 1126.8873734474182, 1131.7325613498688, 1136.5738034248352, 1141.474708557129, 1146.3265883922577, 1151.122316122055, 1155.9417011737823, 1160.7915749549866, 1165.6405022144318, 1170.4776978492737, 1175.2976155281067, 1180.089864730835, 1184.953693151474, 1189.786527633667, 1194.6004056930542, 1199.4723122119904, 1204.342536687851, 1209.144008398056, 1213.90602850914, 1218.732914686203, 1223.6270170211792, 1228.4722521305084, 1233.2725088596344, 1238.0644829273224, 1242.8674676418304, 1247.693255662918, 1252.4849662780762, 1257.2698013782501, 1262.0923247337341, 1266.9155888557434, 1271.7450156211853, 1276.4867494106293, 1281.2337119579315, 1286.0796537399292, 1290.8831820487976, 1295.7197270393372, 1300.4962997436523, 1305.3411407470703, 1310.1229164600372, 1314.8856382369995, 1319.6467940807343, 1324.3577432632446, 1329.6364679336548, 1335.2232069969177, 1340.5946753025055, 1345.9361929893494, 1351.4830992221832, 1356.8548965454102, 1361.7415981292725, 1366.4573776721954, 1371.1894736289978, 1375.9272692203522, 1380.6823334693909, 1385.456265449524, 1390.1983680725098, 1394.9323127269745, 1399.6923480033875, 1404.4651165008545, 1409.4027028083801, 1414.2955584526062, 1419.1896741390228, 1424.0121448040009, 1428.8439555168152, 1433.6034636497498, 1438.3472275733948, 1443.1487770080566, 1448.5109474658966, 1453.8321690559387, 1459.1321415901184, 1464.400066614151, 1469.7713196277618, 1475.030953168869, 1480.277684211731, 1485.6349074840546, 1490.9529139995575, 1495.692042350769, 1500.4901530742645, 1505.2691397666931, 1510.0364825725555, 1514.8533072471619, 1519.712000131607, 1522.2255988121033]
[9.99, 10.0025, 10.0, 10.005, 10.0125, 10.02, 10.03, 10.035, 10.0375, 10.0325, 10.035, 10.0325, 10.035, 10.04, 10.035, 10.035, 10.0575, 10.0775, 10.1, 10.115, 10.0875, 10.13, 10.1075, 10.155, 10.1775, 10.1425, 10.1625, 10.23, 10.2775, 10.2775, 10.2875, 10.285, 10.33, 10.5175, 10.55, 10.55, 10.4875, 10.585, 10.685, 10.7175, 10.8125, 10.91, 10.9775, 10.9275, 10.99, 11.0, 10.91, 10.8525, 10.9375, 11.05, 11.0075, 11.065, 11.14, 11.3575, 11.495, 11.7525, 11.835, 12.04, 12.1975, 12.255, 12.235, 12.41, 12.5825, 12.6925, 12.675, 12.6975, 12.5825, 12.7075, 12.77, 12.9225, 12.8325, 12.9475, 12.9425, 13.1225, 13.075, 12.935, 12.935, 13.1225, 13.3975, 13.5825, 13.5025, 13.3725, 13.4175, 13.4325, 13.425, 13.5325, 13.6175, 13.5725, 13.58, 13.8625, 14.33, 14.715, 15.2325, 15.335, 15.49, 15.5975, 15.63, 15.67, 15.775, 15.9525, 16.125, 16.3125, 16.4175, 16.2975, 16.485, 16.5525, 16.6575, 16.555, 16.535, 16.51, 16.52, 16.615, 16.16, 16.2275, 16.2375, 16.52, 16.525, 16.9075, 17.565, 18.0975, 18.435, 18.835, 18.82, 19.26, 19.6, 19.535, 19.9375, 20.09, 19.9925, 20.0875, 19.8125, 20.15, 20.275, 20.4325, 20.55, 20.81, 21.0975, 21.3775, 21.535, 21.6725, 21.86, 22.1925, 22.425, 22.295, 22.2, 22.2725, 22.3925, 22.5675, 22.37, 22.065, 22.205, 21.9675, 21.5, 21.4025, 21.2125, 21.4375, 21.7575, 21.8425, 21.98, 22.0325, 21.8875, 21.925, 21.7875, 21.79, 21.79, 22.105, 21.93, 22.0925, 22.29, 22.2925, 22.215, 22.1875, 22.28, 22.2775, 22.27, 22.355, 22.43, 22.6175, 22.4225, 22.2225, 22.135, 22.1525, 22.0325, 22.0175, 21.925, 21.64, 21.63, 21.62, 21.8975, 20.83, 17.955, 15.0075, 13.7225, 11.165, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.934, Test loss: 1.885, Test accuracy: 27.63
Round   0, Global train loss: 0.934, Global test loss: 2.261, Global test accuracy: 16.35
Round   1, Train loss: 0.760, Test loss: 1.893, Test accuracy: 37.82
Round   1, Global train loss: 0.760, Global test loss: 2.624, Global test accuracy: 21.04
Round   2, Train loss: 0.636, Test loss: 1.244, Test accuracy: 60.07
Round   2, Global train loss: 0.636, Global test loss: 2.196, Global test accuracy: 35.52
Round   3, Train loss: 0.658, Test loss: 0.965, Test accuracy: 65.30
Round   3, Global train loss: 0.658, Global test loss: 1.839, Global test accuracy: 37.65
Round   4, Train loss: 0.684, Test loss: 0.839, Test accuracy: 69.19
Round   4, Global train loss: 0.684, Global test loss: 1.591, Global test accuracy: 49.05
Round   5, Train loss: 0.615, Test loss: 0.732, Test accuracy: 70.08
Round   5, Global train loss: 0.615, Global test loss: 1.529, Global test accuracy: 45.75
Round   6, Train loss: 0.489, Test loss: 0.695, Test accuracy: 73.28
Round   6, Global train loss: 0.489, Global test loss: 1.688, Global test accuracy: 47.43
Round   7, Train loss: 0.547, Test loss: 0.751, Test accuracy: 71.55
Round   7, Global train loss: 0.547, Global test loss: 2.039, Global test accuracy: 35.71
Round   8, Train loss: 0.599, Test loss: 0.578, Test accuracy: 76.83
Round   8, Global train loss: 0.599, Global test loss: 1.371, Global test accuracy: 53.06
Round   9, Train loss: 0.506, Test loss: 0.589, Test accuracy: 76.98
Round   9, Global train loss: 0.506, Global test loss: 1.371, Global test accuracy: 54.53
Round  10, Train loss: 0.575, Test loss: 0.516, Test accuracy: 79.19
Round  10, Global train loss: 0.575, Global test loss: 1.452, Global test accuracy: 50.59
Round  11, Train loss: 0.526, Test loss: 0.520, Test accuracy: 79.02
Round  11, Global train loss: 0.526, Global test loss: 1.401, Global test accuracy: 53.14
Round  12, Train loss: 0.405, Test loss: 0.564, Test accuracy: 79.10
Round  12, Global train loss: 0.405, Global test loss: 1.801, Global test accuracy: 50.89
Round  13, Train loss: 0.376, Test loss: 0.554, Test accuracy: 78.90
Round  13, Global train loss: 0.376, Global test loss: 1.527, Global test accuracy: 50.92
Round  14, Train loss: 0.456, Test loss: 0.447, Test accuracy: 82.04
Round  14, Global train loss: 0.456, Global test loss: 1.248, Global test accuracy: 58.07
Round  15, Train loss: 0.484, Test loss: 0.436, Test accuracy: 82.79
Round  15, Global train loss: 0.484, Global test loss: 1.208, Global test accuracy: 56.22
Round  16, Train loss: 0.385, Test loss: 0.428, Test accuracy: 83.13
Round  16, Global train loss: 0.385, Global test loss: 1.537, Global test accuracy: 52.23
Round  17, Train loss: 0.430, Test loss: 0.423, Test accuracy: 83.17
Round  17, Global train loss: 0.430, Global test loss: 1.282, Global test accuracy: 58.12
Round  18, Train loss: 0.368, Test loss: 0.421, Test accuracy: 83.68
Round  18, Global train loss: 0.368, Global test loss: 1.676, Global test accuracy: 50.52
Round  19, Train loss: 0.348, Test loss: 0.405, Test accuracy: 84.32
Round  19, Global train loss: 0.348, Global test loss: 1.527, Global test accuracy: 53.00
Round  20, Train loss: 0.347, Test loss: 0.394, Test accuracy: 84.74
Round  20, Global train loss: 0.347, Global test loss: 1.269, Global test accuracy: 61.86
Round  21, Train loss: 0.413, Test loss: 0.381, Test accuracy: 85.22
Round  21, Global train loss: 0.413, Global test loss: 1.238, Global test accuracy: 61.02
Round  22, Train loss: 0.348, Test loss: 0.375, Test accuracy: 85.61
Round  22, Global train loss: 0.348, Global test loss: 1.219, Global test accuracy: 62.54
Round  23, Train loss: 0.368, Test loss: 0.377, Test accuracy: 85.53
Round  23, Global train loss: 0.368, Global test loss: 1.115, Global test accuracy: 65.76
Round  24, Train loss: 0.336, Test loss: 0.376, Test accuracy: 85.63
Round  24, Global train loss: 0.336, Global test loss: 1.185, Global test accuracy: 63.74
Round  25, Train loss: 0.338, Test loss: 0.379, Test accuracy: 85.58
Round  25, Global train loss: 0.338, Global test loss: 1.051, Global test accuracy: 65.48
Round  26, Train loss: 0.300, Test loss: 0.376, Test accuracy: 85.77
Round  26, Global train loss: 0.300, Global test loss: 1.382, Global test accuracy: 58.12
Round  27, Train loss: 0.388, Test loss: 0.368, Test accuracy: 86.40
Round  27, Global train loss: 0.388, Global test loss: 1.189, Global test accuracy: 61.19
Round  28, Train loss: 0.311, Test loss: 0.373, Test accuracy: 86.14
Round  28, Global train loss: 0.311, Global test loss: 1.421, Global test accuracy: 57.70
Round  29, Train loss: 0.308, Test loss: 0.371, Test accuracy: 86.11
Round  29, Global train loss: 0.308, Global test loss: 1.000, Global test accuracy: 67.80
Round  30, Train loss: 0.314, Test loss: 0.371, Test accuracy: 86.12
Round  30, Global train loss: 0.314, Global test loss: 1.198, Global test accuracy: 63.25
Round  31, Train loss: 0.397, Test loss: 0.366, Test accuracy: 86.07
Round  31, Global train loss: 0.397, Global test loss: 1.177, Global test accuracy: 59.48
Round  32, Train loss: 0.262, Test loss: 0.354, Test accuracy: 86.45
Round  32, Global train loss: 0.262, Global test loss: 1.094, Global test accuracy: 66.24
Round  33, Train loss: 0.266, Test loss: 0.361, Test accuracy: 86.28
Round  33, Global train loss: 0.266, Global test loss: 1.194, Global test accuracy: 62.44
Round  34, Train loss: 0.266, Test loss: 0.354, Test accuracy: 86.57
Round  34, Global train loss: 0.266, Global test loss: 1.221, Global test accuracy: 62.39
Round  35, Train loss: 0.216, Test loss: 0.351, Test accuracy: 87.03
Round  35, Global train loss: 0.216, Global test loss: 1.316, Global test accuracy: 62.62
Round  36, Train loss: 0.283, Test loss: 0.353, Test accuracy: 87.03
Round  36, Global train loss: 0.283, Global test loss: 1.137, Global test accuracy: 65.11
Round  37, Train loss: 0.287, Test loss: 0.360, Test accuracy: 86.80
Round  37, Global train loss: 0.287, Global test loss: 0.998, Global test accuracy: 68.30
Round  38, Train loss: 0.258, Test loss: 0.352, Test accuracy: 87.06
Round  38, Global train loss: 0.258, Global test loss: 1.173, Global test accuracy: 65.15
Round  39, Train loss: 0.227, Test loss: 0.348, Test accuracy: 87.30
Round  39, Global train loss: 0.227, Global test loss: 1.153, Global test accuracy: 64.91
Round  40, Train loss: 0.338, Test loss: 0.351, Test accuracy: 87.09
Round  40, Global train loss: 0.338, Global test loss: 1.239, Global test accuracy: 62.61
Round  41, Train loss: 0.202, Test loss: 0.342, Test accuracy: 87.38
Round  41, Global train loss: 0.202, Global test loss: 1.132, Global test accuracy: 66.10
Round  42, Train loss: 0.312, Test loss: 0.345, Test accuracy: 87.32
Round  42, Global train loss: 0.312, Global test loss: 1.214, Global test accuracy: 61.60
Round  43, Train loss: 0.235, Test loss: 0.354, Test accuracy: 87.06
Round  43, Global train loss: 0.235, Global test loss: 1.015, Global test accuracy: 67.39
Round  44, Train loss: 0.226, Test loss: 0.349, Test accuracy: 87.22
Round  44, Global train loss: 0.226, Global test loss: 0.997, Global test accuracy: 68.65
Round  45, Train loss: 0.243, Test loss: 0.353, Test accuracy: 87.35
Round  45, Global train loss: 0.243, Global test loss: 1.481, Global test accuracy: 58.92
Round  46, Train loss: 0.251, Test loss: 0.344, Test accuracy: 87.57
Round  46, Global train loss: 0.251, Global test loss: 1.009, Global test accuracy: 67.53
Round  47, Train loss: 0.318, Test loss: 0.344, Test accuracy: 87.55
Round  47, Global train loss: 0.318, Global test loss: 0.960, Global test accuracy: 68.89
Round  48, Train loss: 0.273, Test loss: 0.339, Test accuracy: 87.81
Round  48, Global train loss: 0.273, Global test loss: 0.916, Global test accuracy: 70.28
Round  49, Train loss: 0.265, Test loss: 0.338, Test accuracy: 87.75
Round  49, Global train loss: 0.265, Global test loss: 0.973, Global test accuracy: 68.76
Round  50, Train loss: 0.226, Test loss: 0.353, Test accuracy: 87.43
Round  50, Global train loss: 0.226, Global test loss: 0.913, Global test accuracy: 71.03
Round  51, Train loss: 0.231, Test loss: 0.357, Test accuracy: 87.30
Round  51, Global train loss: 0.231, Global test loss: 1.287, Global test accuracy: 63.87
Round  52, Train loss: 0.220, Test loss: 0.358, Test accuracy: 87.49
Round  52, Global train loss: 0.220, Global test loss: 1.152, Global test accuracy: 65.83
Round  53, Train loss: 0.197, Test loss: 0.355, Test accuracy: 87.70
Round  53, Global train loss: 0.197, Global test loss: 1.024, Global test accuracy: 68.05
Round  54, Train loss: 0.251, Test loss: 0.353, Test accuracy: 87.83
Round  54, Global train loss: 0.251, Global test loss: 1.043, Global test accuracy: 68.53
Round  55, Train loss: 0.213, Test loss: 0.341, Test accuracy: 88.06
Round  55, Global train loss: 0.213, Global test loss: 1.374, Global test accuracy: 60.63
Round  56, Train loss: 0.190, Test loss: 0.353, Test accuracy: 87.79
Round  56, Global train loss: 0.190, Global test loss: 1.148, Global test accuracy: 65.62
Round  57, Train loss: 0.199, Test loss: 0.360, Test accuracy: 87.46
Round  57, Global train loss: 0.199, Global test loss: 1.014, Global test accuracy: 69.47
Round  58, Train loss: 0.252, Test loss: 0.348, Test accuracy: 87.93
Round  58, Global train loss: 0.252, Global test loss: 0.864, Global test accuracy: 72.64
Round  59, Train loss: 0.218, Test loss: 0.348, Test accuracy: 87.85
Round  59, Global train loss: 0.218, Global test loss: 1.655, Global test accuracy: 57.05
Round  60, Train loss: 0.162, Test loss: 0.346, Test accuracy: 88.05
Round  60, Global train loss: 0.162, Global test loss: 1.276, Global test accuracy: 65.79
Round  61, Train loss: 0.277, Test loss: 0.350, Test accuracy: 88.19
Round  61, Global train loss: 0.277, Global test loss: 1.118, Global test accuracy: 66.23
Round  62, Train loss: 0.167, Test loss: 0.348, Test accuracy: 88.12
Round  62, Global train loss: 0.167, Global test loss: 1.045, Global test accuracy: 69.22
Round  63, Train loss: 0.193, Test loss: 0.357, Test accuracy: 87.83
Round  63, Global train loss: 0.193, Global test loss: 0.976, Global test accuracy: 70.20
Round  64, Train loss: 0.189, Test loss: 0.349, Test accuracy: 88.03
Round  64, Global train loss: 0.189, Global test loss: 1.143, Global test accuracy: 66.38
Round  65, Train loss: 0.177, Test loss: 0.350, Test accuracy: 88.04
Round  65, Global train loss: 0.177, Global test loss: 1.301, Global test accuracy: 62.44
Round  66, Train loss: 0.268, Test loss: 0.352, Test accuracy: 88.03
Round  66, Global train loss: 0.268, Global test loss: 0.939, Global test accuracy: 69.81
Round  67, Train loss: 0.198, Test loss: 0.357, Test accuracy: 88.05
Round  67, Global train loss: 0.198, Global test loss: 1.032, Global test accuracy: 68.66
Round  68, Train loss: 0.238, Test loss: 0.352, Test accuracy: 88.23
Round  68, Global train loss: 0.238, Global test loss: 1.058, Global test accuracy: 68.15
Round  69, Train loss: 0.220, Test loss: 0.358, Test accuracy: 88.09
Round  69, Global train loss: 0.220, Global test loss: 0.987, Global test accuracy: 69.93
Round  70, Train loss: 0.269, Test loss: 0.357, Test accuracy: 88.33
Round  70, Global train loss: 0.269, Global test loss: 1.227, Global test accuracy: 65.10
Round  71, Train loss: 0.222, Test loss: 0.362, Test accuracy: 88.21
Round  71, Global train loss: 0.222, Global test loss: 0.984, Global test accuracy: 69.66
Round  72, Train loss: 0.202, Test loss: 0.363, Test accuracy: 88.08
Round  72, Global train loss: 0.202, Global test loss: 1.169, Global test accuracy: 66.63
Round  73, Train loss: 0.226, Test loss: 0.361, Test accuracy: 88.31
Round  73, Global train loss: 0.226, Global test loss: 1.144, Global test accuracy: 67.75
Round  74, Train loss: 0.145, Test loss: 0.365, Test accuracy: 88.10
Round  74, Global train loss: 0.145, Global test loss: 1.349, Global test accuracy: 64.52
Round  75, Train loss: 0.179, Test loss: 0.368, Test accuracy: 87.95
Round  75, Global train loss: 0.179, Global test loss: 1.049, Global test accuracy: 69.53
Round  76, Train loss: 0.169, Test loss: 0.360, Test accuracy: 88.05
Round  76, Global train loss: 0.169, Global test loss: 1.173, Global test accuracy: 67.36
Round  77, Train loss: 0.242, Test loss: 0.361, Test accuracy: 88.03
Round  77, Global train loss: 0.242, Global test loss: 1.113, Global test accuracy: 66.35
Round  78, Train loss: 0.174, Test loss: 0.362, Test accuracy: 88.30
Round  78, Global train loss: 0.174, Global test loss: 1.258, Global test accuracy: 66.30
Round  79, Train loss: 0.213, Test loss: 0.356, Test accuracy: 88.43
Round  79, Global train loss: 0.213, Global test loss: 0.967, Global test accuracy: 69.93
Round  80, Train loss: 0.227, Test loss: 0.366, Test accuracy: 88.16
Round  80, Global train loss: 0.227, Global test loss: 1.226, Global test accuracy: 63.07
Round  81, Train loss: 0.181, Test loss: 0.365, Test accuracy: 88.21
Round  81, Global train loss: 0.181, Global test loss: 1.001, Global test accuracy: 70.68
Round  82, Train loss: 0.208, Test loss: 0.359, Test accuracy: 88.54
Round  82, Global train loss: 0.208, Global test loss: 1.286, Global test accuracy: 63.60
Round  83, Train loss: 0.215, Test loss: 0.362, Test accuracy: 88.45
Round  83, Global train loss: 0.215, Global test loss: 1.082, Global test accuracy: 67.35
Round  84, Train loss: 0.144, Test loss: 0.354, Test accuracy: 88.64
Round  84, Global train loss: 0.144, Global test loss: 0.998, Global test accuracy: 70.83
Round  85, Train loss: 0.172, Test loss: 0.351, Test accuracy: 88.70
Round  85, Global train loss: 0.172, Global test loss: 1.039, Global test accuracy: 69.98
Round  86, Train loss: 0.128, Test loss: 0.346, Test accuracy: 88.95
Round  86, Global train loss: 0.128, Global test loss: 0.994, Global test accuracy: 71.24
Round  87, Train loss: 0.192, Test loss: 0.349, Test accuracy: 88.91
Round  87, Global train loss: 0.192, Global test loss: 1.764, Global test accuracy: 59.86
Round  88, Train loss: 0.204, Test loss: 0.354, Test accuracy: 88.83
Round  88, Global train loss: 0.204, Global test loss: 1.313, Global test accuracy: 64.29
Round  89, Train loss: 0.123, Test loss: 0.354, Test accuracy: 88.71
Round  89, Global train loss: 0.123, Global test loss: 1.101, Global test accuracy: 71.06
Round  90, Train loss: 0.198, Test loss: 0.360, Test accuracy: 88.70
Round  90, Global train loss: 0.198, Global test loss: 1.056, Global test accuracy: 69.10
Round  91, Train loss: 0.167, Test loss: 0.357, Test accuracy: 88.64
Round  91, Global train loss: 0.167, Global test loss: 1.003, Global test accuracy: 70.38
Round  92, Train loss: 0.150, Test loss: 0.350, Test accuracy: 88.79
Round  92, Global train loss: 0.150, Global test loss: 1.084, Global test accuracy: 70.24
Round  93, Train loss: 0.187, Test loss: 0.350, Test accuracy: 88.89
Round  93, Global train loss: 0.187, Global test loss: 1.174, Global test accuracy: 67.49
Round  94, Train loss: 0.155, Test loss: 0.358, Test accuracy: 88.73
Round  94, Global train loss: 0.155, Global test loss: 0.988, Global test accuracy: 71.90
Round  95, Train loss: 0.160, Test loss: 0.360, Test accuracy: 88.64
Round  95, Global train loss: 0.160, Global test loss: 1.272, Global test accuracy: 65.85
Round  96, Train loss: 0.181, Test loss: 0.366, Test accuracy: 88.56
Round  96, Global train loss: 0.181, Global test loss: 1.056, Global test accuracy: 68.64
Round  97, Train loss: 0.128, Test loss: 0.369, Test accuracy: 88.54
Round  97, Global train loss: 0.128, Global test loss: 1.518, Global test accuracy: 62.78
Round  98, Train loss: 0.161, Test loss: 0.369, Test accuracy: 88.69
Round  98, Global train loss: 0.161, Global test loss: 1.027, Global test accuracy: 69.93
Round  99, Train loss: 0.166, Test loss: 0.385, Test accuracy: 88.36
Round  99, Global train loss: 0.166, Global test loss: 1.037, Global test accuracy: 69.82
Final Round, Train loss: 0.115, Test loss: 0.411, Test accuracy: 88.55
Final Round, Global train loss: 0.115, Global test loss: 1.037, Global test accuracy: 69.82
Average accuracy final 10 rounds: 88.65375 

Average global accuracy final 10 rounds: 68.61375 

3830.5343351364136
[3.2043380737304688, 6.4086761474609375, 9.470546245574951, 12.532416343688965, 15.466352939605713, 18.40028953552246, 21.242225646972656, 24.08416175842285, 26.824262619018555, 29.564363479614258, 32.242756843566895, 34.92115020751953, 37.603073835372925, 40.28499746322632, 43.317533016204834, 46.35006856918335, 49.02875638008118, 51.707444190979004, 54.48706769943237, 57.26669120788574, 59.96773672103882, 62.668782234191895, 65.38083076477051, 68.09287929534912, 71.07168579101562, 74.05049228668213, 77.02820825576782, 80.00592422485352, 82.97039699554443, 85.93486976623535, 88.925532579422, 91.91619539260864, 94.89696526527405, 97.87773513793945, 100.81606864929199, 103.75440216064453, 106.76811242103577, 109.781822681427, 112.8238615989685, 115.86590051651001, 118.57791447639465, 121.2899284362793, 123.99179291725159, 126.69365739822388, 129.4141547679901, 132.13465213775635, 134.7974569797516, 137.46026182174683, 140.24197459220886, 143.0236873626709, 145.72410249710083, 148.42451763153076, 151.143212556839, 153.86190748214722, 156.58545684814453, 159.30900621414185, 162.02958273887634, 164.75015926361084, 167.54287385940552, 170.3355884552002, 173.1938238143921, 176.05205917358398, 178.95181941986084, 181.8515796661377, 184.68745946884155, 187.5233392715454, 190.20664525032043, 192.88995122909546, 195.60969805717468, 198.3294448852539, 201.06901574134827, 203.80858659744263, 206.5589997768402, 209.3094129562378, 212.28026485443115, 215.2511167526245, 218.1845245361328, 221.1179323196411, 224.08721256256104, 227.05649280548096, 230.0738182067871, 233.09114360809326, 236.1030855178833, 239.11502742767334, 242.12890219688416, 245.14277696609497, 247.95882296562195, 250.77486896514893, 253.42880821228027, 256.0827474594116, 258.7149782180786, 261.3472089767456, 263.9551832675934, 266.56315755844116, 269.16131949424744, 271.7594814300537, 274.3510699272156, 276.94265842437744, 279.5565869808197, 282.17051553726196, 284.780277967453, 287.39004039764404, 290.0770380496979, 292.7640357017517, 295.802122592926, 298.84020948410034, 301.5399305820465, 304.2396516799927, 307.0717270374298, 309.90380239486694, 312.6294832229614, 315.3551640510559, 318.0820915699005, 320.8090190887451, 323.5768713951111, 326.34472370147705, 329.06891226768494, 331.7931008338928, 334.8414828777313, 337.8898649215698, 340.91023325920105, 343.9306015968323, 346.93559527397156, 349.94058895111084, 352.9298930168152, 355.91919708251953, 358.92598032951355, 361.93276357650757, 364.83850717544556, 367.74425077438354, 370.48021626472473, 373.2161817550659, 376.2327468395233, 379.2493119239807, 382.2556200027466, 385.26192808151245, 388.2930717468262, 391.3242154121399, 394.3501102924347, 397.3760051727295, 400.20711398124695, 403.0382227897644, 405.7630696296692, 408.487916469574, 411.2541768550873, 414.0204372406006, 417.1156599521637, 420.2108826637268, 422.91128373146057, 425.61168479919434, 428.36755180358887, 431.1234188079834, 434.08186531066895, 437.0403118133545, 439.84086894989014, 442.6414260864258, 445.37826442718506, 448.11510276794434, 450.9153974056244, 453.71569204330444, 456.5239555835724, 459.33221912384033, 462.06268286705017, 464.79314661026, 467.76243472099304, 470.7317228317261, 473.4739980697632, 476.2162733078003, 478.98644828796387, 481.75662326812744, 484.50317335128784, 487.24972343444824, 490.0323791503906, 492.815034866333, 495.54558777809143, 498.27614068984985, 501.01363945007324, 503.75113821029663, 506.5404031276703, 509.32966804504395, 512.1110808849335, 514.892493724823, 517.6939997673035, 520.4955058097839, 523.2556948661804, 526.0158839225769, 528.782445192337, 531.5490064620972, 534.2983593940735, 537.0477123260498, 539.7851111888885, 542.5225100517273, 545.2560431957245, 547.9895763397217, 550.6691615581512, 553.3487467765808, 556.05806183815, 558.7673768997192, 561.7682678699493, 564.7691588401794, 567.2795164585114, 569.7898740768433]
[27.629166666666666, 27.629166666666666, 37.81666666666667, 37.81666666666667, 60.07083333333333, 60.07083333333333, 65.30416666666666, 65.30416666666666, 69.19166666666666, 69.19166666666666, 70.075, 70.075, 73.27916666666667, 73.27916666666667, 71.55, 71.55, 76.82916666666667, 76.82916666666667, 76.97916666666667, 76.97916666666667, 79.19166666666666, 79.19166666666666, 79.01666666666667, 79.01666666666667, 79.1, 79.1, 78.90416666666667, 78.90416666666667, 82.0375, 82.0375, 82.79166666666667, 82.79166666666667, 83.13333333333334, 83.13333333333334, 83.16666666666667, 83.16666666666667, 83.67916666666666, 83.67916666666666, 84.32083333333334, 84.32083333333334, 84.74166666666666, 84.74166666666666, 85.225, 85.225, 85.6125, 85.6125, 85.53333333333333, 85.53333333333333, 85.63333333333334, 85.63333333333334, 85.58333333333333, 85.58333333333333, 85.77083333333333, 85.77083333333333, 86.39583333333333, 86.39583333333333, 86.14166666666667, 86.14166666666667, 86.10833333333333, 86.10833333333333, 86.12083333333334, 86.12083333333334, 86.06666666666666, 86.06666666666666, 86.45, 86.45, 86.275, 86.275, 86.56666666666666, 86.56666666666666, 87.03333333333333, 87.03333333333333, 87.02916666666667, 87.02916666666667, 86.8, 86.8, 87.05833333333334, 87.05833333333334, 87.3, 87.3, 87.09166666666667, 87.09166666666667, 87.375, 87.375, 87.31666666666666, 87.31666666666666, 87.05833333333334, 87.05833333333334, 87.22083333333333, 87.22083333333333, 87.35, 87.35, 87.57083333333334, 87.57083333333334, 87.55416666666666, 87.55416666666666, 87.80833333333334, 87.80833333333334, 87.75, 87.75, 87.42916666666666, 87.42916666666666, 87.30416666666666, 87.30416666666666, 87.4875, 87.4875, 87.70416666666667, 87.70416666666667, 87.825, 87.825, 88.05833333333334, 88.05833333333334, 87.79166666666667, 87.79166666666667, 87.45833333333333, 87.45833333333333, 87.92916666666666, 87.92916666666666, 87.84583333333333, 87.84583333333333, 88.05, 88.05, 88.1875, 88.1875, 88.125, 88.125, 87.82916666666667, 87.82916666666667, 88.02916666666667, 88.02916666666667, 88.0375, 88.0375, 88.025, 88.025, 88.05, 88.05, 88.22916666666667, 88.22916666666667, 88.0875, 88.0875, 88.325, 88.325, 88.20833333333333, 88.20833333333333, 88.07916666666667, 88.07916666666667, 88.3125, 88.3125, 88.1, 88.1, 87.94583333333334, 87.94583333333334, 88.05, 88.05, 88.025, 88.025, 88.3, 88.3, 88.43333333333334, 88.43333333333334, 88.15833333333333, 88.15833333333333, 88.20833333333333, 88.20833333333333, 88.5375, 88.5375, 88.45416666666667, 88.45416666666667, 88.64166666666667, 88.64166666666667, 88.7, 88.7, 88.95416666666667, 88.95416666666667, 88.9125, 88.9125, 88.82916666666667, 88.82916666666667, 88.7125, 88.7125, 88.7, 88.7, 88.6375, 88.6375, 88.79166666666667, 88.79166666666667, 88.89166666666667, 88.89166666666667, 88.72916666666667, 88.72916666666667, 88.6375, 88.6375, 88.55833333333334, 88.55833333333334, 88.5375, 88.5375, 88.69166666666666, 88.69166666666666, 88.3625, 88.3625, 88.55, 88.55]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.269, Test loss: 2.158, Test accuracy: 26.68
Round   1, Train loss: 0.806, Test loss: 1.729, Test accuracy: 45.50
Round   2, Train loss: 0.636, Test loss: 1.569, Test accuracy: 53.60
Round   3, Train loss: 0.674, Test loss: 1.226, Test accuracy: 57.90
Round   4, Train loss: 0.727, Test loss: 0.943, Test accuracy: 63.27
Round   5, Train loss: 0.582, Test loss: 0.698, Test accuracy: 70.81
Round   6, Train loss: 0.613, Test loss: 0.580, Test accuracy: 74.15
Round   7, Train loss: 0.528, Test loss: 0.586, Test accuracy: 73.91
Round   8, Train loss: 0.503, Test loss: 0.575, Test accuracy: 74.89
Round   9, Train loss: 0.541, Test loss: 0.557, Test accuracy: 75.73
Round  10, Train loss: 0.586, Test loss: 0.538, Test accuracy: 76.81
Round  11, Train loss: 0.457, Test loss: 0.506, Test accuracy: 78.29
Round  12, Train loss: 0.571, Test loss: 0.495, Test accuracy: 78.89
Round  13, Train loss: 0.559, Test loss: 0.464, Test accuracy: 80.15
Round  14, Train loss: 0.492, Test loss: 0.450, Test accuracy: 81.28
Round  15, Train loss: 0.400, Test loss: 0.456, Test accuracy: 81.29
Round  16, Train loss: 0.465, Test loss: 0.449, Test accuracy: 81.40
Round  17, Train loss: 0.388, Test loss: 0.440, Test accuracy: 81.59
Round  18, Train loss: 0.474, Test loss: 0.427, Test accuracy: 82.28
Round  19, Train loss: 0.443, Test loss: 0.420, Test accuracy: 82.69
Round  20, Train loss: 0.474, Test loss: 0.422, Test accuracy: 82.63
Round  21, Train loss: 0.444, Test loss: 0.398, Test accuracy: 83.59
Round  22, Train loss: 0.379, Test loss: 0.391, Test accuracy: 83.79
Round  23, Train loss: 0.377, Test loss: 0.391, Test accuracy: 83.86
Round  24, Train loss: 0.345, Test loss: 0.395, Test accuracy: 83.77
Round  25, Train loss: 0.462, Test loss: 0.388, Test accuracy: 84.08
Round  26, Train loss: 0.465, Test loss: 0.384, Test accuracy: 84.62
Round  27, Train loss: 0.401, Test loss: 0.377, Test accuracy: 84.63
Round  28, Train loss: 0.453, Test loss: 0.368, Test accuracy: 85.11
Round  29, Train loss: 0.386, Test loss: 0.369, Test accuracy: 85.00
Round  30, Train loss: 0.366, Test loss: 0.361, Test accuracy: 85.35
Round  31, Train loss: 0.290, Test loss: 0.364, Test accuracy: 85.05
Round  32, Train loss: 0.415, Test loss: 0.350, Test accuracy: 86.07
Round  33, Train loss: 0.383, Test loss: 0.343, Test accuracy: 86.56
Round  34, Train loss: 0.381, Test loss: 0.343, Test accuracy: 86.45
Round  35, Train loss: 0.265, Test loss: 0.348, Test accuracy: 86.23
Round  36, Train loss: 0.276, Test loss: 0.351, Test accuracy: 86.22
Round  37, Train loss: 0.298, Test loss: 0.335, Test accuracy: 86.67
Round  38, Train loss: 0.383, Test loss: 0.339, Test accuracy: 86.68
Round  39, Train loss: 0.377, Test loss: 0.328, Test accuracy: 87.05
Round  40, Train loss: 0.361, Test loss: 0.328, Test accuracy: 87.09
Round  41, Train loss: 0.281, Test loss: 0.323, Test accuracy: 87.26
Round  42, Train loss: 0.243, Test loss: 0.330, Test accuracy: 86.99
Round  43, Train loss: 0.317, Test loss: 0.324, Test accuracy: 87.31
Round  44, Train loss: 0.288, Test loss: 0.321, Test accuracy: 87.12
Round  45, Train loss: 0.319, Test loss: 0.326, Test accuracy: 87.06
Round  46, Train loss: 0.272, Test loss: 0.325, Test accuracy: 87.23
Round  47, Train loss: 0.299, Test loss: 0.322, Test accuracy: 87.38
Round  48, Train loss: 0.260, Test loss: 0.330, Test accuracy: 86.97
Round  49, Train loss: 0.278, Test loss: 0.325, Test accuracy: 87.12
Round  50, Train loss: 0.251, Test loss: 0.320, Test accuracy: 87.34
Round  51, Train loss: 0.270, Test loss: 0.317, Test accuracy: 87.60
Round  52, Train loss: 0.283, Test loss: 0.327, Test accuracy: 87.43
Round  53, Train loss: 0.271, Test loss: 0.314, Test accuracy: 87.76
Round  54, Train loss: 0.275, Test loss: 0.315, Test accuracy: 87.67
Round  55, Train loss: 0.223, Test loss: 0.313, Test accuracy: 87.63
Round  56, Train loss: 0.265, Test loss: 0.310, Test accuracy: 87.85
Round  57, Train loss: 0.300, Test loss: 0.313, Test accuracy: 87.79
Round  58, Train loss: 0.324, Test loss: 0.320, Test accuracy: 87.50
Round  59, Train loss: 0.335, Test loss: 0.313, Test accuracy: 87.81
Round  60, Train loss: 0.265, Test loss: 0.311, Test accuracy: 88.02
Round  61, Train loss: 0.286, Test loss: 0.304, Test accuracy: 88.29
Round  62, Train loss: 0.300, Test loss: 0.313, Test accuracy: 88.02
Round  63, Train loss: 0.307, Test loss: 0.303, Test accuracy: 88.24
Round  64, Train loss: 0.204, Test loss: 0.307, Test accuracy: 88.19
Round  65, Train loss: 0.282, Test loss: 0.301, Test accuracy: 88.40
Round  66, Train loss: 0.274, Test loss: 0.300, Test accuracy: 88.46
Round  67, Train loss: 0.199, Test loss: 0.310, Test accuracy: 87.96
Round  68, Train loss: 0.236, Test loss: 0.300, Test accuracy: 88.47
Round  69, Train loss: 0.250, Test loss: 0.305, Test accuracy: 88.15
Round  70, Train loss: 0.255, Test loss: 0.304, Test accuracy: 88.17
Round  71, Train loss: 0.220, Test loss: 0.300, Test accuracy: 88.22
Round  72, Train loss: 0.250, Test loss: 0.303, Test accuracy: 88.34
Round  73, Train loss: 0.218, Test loss: 0.304, Test accuracy: 88.17
Round  74, Train loss: 0.248, Test loss: 0.298, Test accuracy: 88.52
Round  75, Train loss: 0.269, Test loss: 0.305, Test accuracy: 88.22
Round  76, Train loss: 0.268, Test loss: 0.295, Test accuracy: 88.37
Round  77, Train loss: 0.180, Test loss: 0.313, Test accuracy: 88.03
Round  78, Train loss: 0.189, Test loss: 0.303, Test accuracy: 88.25
Round  79, Train loss: 0.260, Test loss: 0.298, Test accuracy: 88.46
Round  80, Train loss: 0.251, Test loss: 0.301, Test accuracy: 88.44
Round  81, Train loss: 0.257, Test loss: 0.297, Test accuracy: 88.60
Round  82, Train loss: 0.247, Test loss: 0.296, Test accuracy: 88.62
Round  83, Train loss: 0.199, Test loss: 0.296, Test accuracy: 88.53
Round  84, Train loss: 0.211, Test loss: 0.303, Test accuracy: 88.22
Round  85, Train loss: 0.172, Test loss: 0.302, Test accuracy: 88.43
Round  86, Train loss: 0.187, Test loss: 0.306, Test accuracy: 88.17
Round  87, Train loss: 0.257, Test loss: 0.303, Test accuracy: 88.21
Round  88, Train loss: 0.249, Test loss: 0.306, Test accuracy: 88.32
Round  89, Train loss: 0.153, Test loss: 0.310, Test accuracy: 88.22
Round  90, Train loss: 0.196, Test loss: 0.313, Test accuracy: 88.30
Round  91, Train loss: 0.214, Test loss: 0.316, Test accuracy: 88.03
Round  92, Train loss: 0.172, Test loss: 0.313, Test accuracy: 88.30
Round  93, Train loss: 0.153, Test loss: 0.314, Test accuracy: 88.17
Round  94, Train loss: 0.183, Test loss: 0.308, Test accuracy: 88.50
Round  95, Train loss: 0.242, Test loss: 0.303, Test accuracy: 88.16
Round  96, Train loss: 0.179, Test loss: 0.306, Test accuracy: 88.46
Round  97, Train loss: 0.143, Test loss: 0.312, Test accuracy: 88.40
Round  98, Train loss: 0.217, Test loss: 0.305, Test accuracy: 88.47
Round  99, Train loss: 0.199, Test loss: 0.305, Test accuracy: 88.44
Final Round, Train loss: 0.170, Test loss: 0.306, Test accuracy: 88.48
Average accuracy final 10 rounds: 88.32208333333334 

2976.808210134506
[2.893096923828125, 5.78619384765625, 8.473061800003052, 11.159929752349854, 13.98518180847168, 16.810433864593506, 19.532132387161255, 22.253830909729004, 25.054377555847168, 27.854924201965332, 30.74352788925171, 33.632131576538086, 36.48588442802429, 39.3396372795105, 42.171523094177246, 45.003408908843994, 47.84139943122864, 50.67938995361328, 53.50348734855652, 56.327584743499756, 59.156824588775635, 61.986064434051514, 64.79945349693298, 67.61284255981445, 70.36242318153381, 73.11200380325317, 75.89688014984131, 78.68175649642944, 81.42850613594055, 84.17525577545166, 86.98665452003479, 89.79805326461792, 92.5764832496643, 95.3549132347107, 98.17859244346619, 101.00227165222168, 103.79246950149536, 106.58266735076904, 109.39657330513, 112.21047925949097, 115.00437045097351, 117.79826164245605, 120.62919020652771, 123.46011877059937, 126.29577422142029, 129.1314296722412, 131.957368850708, 134.7833080291748, 137.63421416282654, 140.48512029647827, 143.33051586151123, 146.1759114265442, 149.0322003364563, 151.8884892463684, 154.71479630470276, 157.5411033630371, 160.36374688148499, 163.18639039993286, 166.00646138191223, 168.8265323638916, 171.6463713645935, 174.4662103652954, 177.2867786884308, 180.10734701156616, 182.92068147659302, 185.73401594161987, 188.56718111038208, 191.4003462791443, 194.21039247512817, 197.02043867111206, 199.85126566886902, 202.68209266662598, 205.4879982471466, 208.29390382766724, 210.766948223114, 213.2399926185608, 215.72927951812744, 218.2185664176941, 220.6981964111328, 223.17782640457153, 225.69178009033203, 228.20573377609253, 230.71792817115784, 233.23012256622314, 235.7179684638977, 238.20581436157227, 240.7211377620697, 243.23646116256714, 245.7242021560669, 248.21194314956665, 250.70668292045593, 253.20142269134521, 255.70626831054688, 258.21111392974854, 260.68366265296936, 263.1562113761902, 265.6343719959259, 268.1125326156616, 270.58204555511475, 273.05155849456787, 275.5241334438324, 277.9967083930969, 280.47657108306885, 282.95643377304077, 285.5436429977417, 288.1308522224426, 290.758296251297, 293.38574028015137, 295.9034414291382, 298.421142578125, 300.88595366477966, 303.3507647514343, 305.8469877243042, 308.3432106971741, 310.8080985546112, 313.27298641204834, 315.79786920547485, 318.32275199890137, 320.83301997184753, 323.3432879447937, 325.81670236587524, 328.2901167869568, 330.78188014030457, 333.27364349365234, 335.74593687057495, 338.21823024749756, 340.7615096569061, 343.3047890663147, 345.7919442653656, 348.2790994644165, 350.76051020622253, 353.24192094802856, 355.6994197368622, 358.1569185256958, 360.58780813217163, 363.01869773864746, 365.45686960220337, 367.8950414657593, 370.336177110672, 372.7773127555847, 375.2415044307709, 377.70569610595703, 380.14178681373596, 382.5778775215149, 385.0533046722412, 387.52873182296753, 389.9898133277893, 392.4508948326111, 394.89213466644287, 397.33337450027466, 399.78024649620056, 402.22711849212646, 404.6991939544678, 407.1712694168091, 409.660906791687, 412.15054416656494, 414.6136498451233, 417.07675552368164, 419.5495843887329, 422.0224132537842, 424.45258951187134, 426.8827657699585, 429.3194217681885, 431.75607776641846, 434.20218682289124, 436.648295879364, 439.0945842266083, 441.54087257385254, 443.9927227497101, 446.4445729255676, 448.89586877822876, 451.3471646308899, 453.84083008766174, 456.3344955444336, 458.79410672187805, 461.2537178993225, 463.69325590133667, 466.13279390335083, 468.5667951107025, 471.0007963180542, 473.4405519962311, 475.88030767440796, 478.30672550201416, 480.73314332962036, 483.16373014450073, 485.5943169593811, 488.0442085266113, 490.49410009384155, 492.92905616760254, 495.3640122413635, 497.7970094680786, 500.2300066947937, 502.6678137779236, 505.10562086105347, 507.5531122684479, 510.0006036758423, 512.6208970546722, 515.2411904335022, 517.6861159801483, 520.1310415267944, 522.2679584026337, 524.4048752784729]
[26.679166666666667, 26.679166666666667, 45.50416666666667, 45.50416666666667, 53.6, 53.6, 57.90416666666667, 57.90416666666667, 63.266666666666666, 63.266666666666666, 70.8125, 70.8125, 74.14583333333333, 74.14583333333333, 73.90833333333333, 73.90833333333333, 74.8875, 74.8875, 75.73333333333333, 75.73333333333333, 76.8125, 76.8125, 78.29166666666667, 78.29166666666667, 78.89166666666667, 78.89166666666667, 80.15, 80.15, 81.275, 81.275, 81.2875, 81.2875, 81.4, 81.4, 81.5875, 81.5875, 82.275, 82.275, 82.69166666666666, 82.69166666666666, 82.62916666666666, 82.62916666666666, 83.59166666666667, 83.59166666666667, 83.7875, 83.7875, 83.8625, 83.8625, 83.77083333333333, 83.77083333333333, 84.07916666666667, 84.07916666666667, 84.625, 84.625, 84.62916666666666, 84.62916666666666, 85.1125, 85.1125, 84.99583333333334, 84.99583333333334, 85.35416666666667, 85.35416666666667, 85.04583333333333, 85.04583333333333, 86.06666666666666, 86.06666666666666, 86.5625, 86.5625, 86.45416666666667, 86.45416666666667, 86.23333333333333, 86.23333333333333, 86.21666666666667, 86.21666666666667, 86.67083333333333, 86.67083333333333, 86.68333333333334, 86.68333333333334, 87.04583333333333, 87.04583333333333, 87.0875, 87.0875, 87.2625, 87.2625, 86.99166666666666, 86.99166666666666, 87.30833333333334, 87.30833333333334, 87.125, 87.125, 87.05833333333334, 87.05833333333334, 87.22916666666667, 87.22916666666667, 87.37916666666666, 87.37916666666666, 86.975, 86.975, 87.11666666666666, 87.11666666666666, 87.3375, 87.3375, 87.60416666666667, 87.60416666666667, 87.42916666666666, 87.42916666666666, 87.75833333333334, 87.75833333333334, 87.675, 87.675, 87.62916666666666, 87.62916666666666, 87.84583333333333, 87.84583333333333, 87.7875, 87.7875, 87.5, 87.5, 87.8125, 87.8125, 88.01666666666667, 88.01666666666667, 88.2875, 88.2875, 88.01666666666667, 88.01666666666667, 88.2375, 88.2375, 88.19166666666666, 88.19166666666666, 88.40416666666667, 88.40416666666667, 88.45833333333333, 88.45833333333333, 87.95833333333333, 87.95833333333333, 88.46666666666667, 88.46666666666667, 88.14583333333333, 88.14583333333333, 88.17083333333333, 88.17083333333333, 88.21666666666667, 88.21666666666667, 88.3375, 88.3375, 88.16666666666667, 88.16666666666667, 88.51666666666667, 88.51666666666667, 88.21666666666667, 88.21666666666667, 88.36666666666666, 88.36666666666666, 88.02916666666667, 88.02916666666667, 88.25416666666666, 88.25416666666666, 88.45833333333333, 88.45833333333333, 88.44166666666666, 88.44166666666666, 88.59583333333333, 88.59583333333333, 88.62083333333334, 88.62083333333334, 88.52916666666667, 88.52916666666667, 88.22083333333333, 88.22083333333333, 88.43333333333334, 88.43333333333334, 88.175, 88.175, 88.2125, 88.2125, 88.31666666666666, 88.31666666666666, 88.22083333333333, 88.22083333333333, 88.3, 88.3, 88.02916666666667, 88.02916666666667, 88.3, 88.3, 88.17083333333333, 88.17083333333333, 88.49583333333334, 88.49583333333334, 88.15833333333333, 88.15833333333333, 88.45833333333333, 88.45833333333333, 88.40416666666667, 88.40416666666667, 88.46666666666667, 88.46666666666667, 88.4375, 88.4375, 88.48333333333333, 88.48333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2181, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 58739 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 504, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 59212 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 57853 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.838, Test loss: 1.736, Test accuracy: 46.39
Round   0, Global train loss: 0.838, Global test loss: 2.119, Global test accuracy: 36.77
Round   1, Train loss: 0.688, Test loss: 1.193, Test accuracy: 56.59
Round   1, Global train loss: 0.688, Global test loss: 1.923, Global test accuracy: 33.83
Round   2, Train loss: 0.614, Test loss: 0.828, Test accuracy: 69.16
Round   2, Global train loss: 0.614, Global test loss: 1.836, Global test accuracy: 38.03
Round   3, Train loss: 0.596, Test loss: 0.869, Test accuracy: 70.33
Round   3, Global train loss: 0.596, Global test loss: 2.393, Global test accuracy: 34.02
Round   4, Train loss: 0.564, Test loss: 0.796, Test accuracy: 72.10
Round   4, Global train loss: 0.564, Global test loss: 2.352, Global test accuracy: 36.41
Round   5, Train loss: 0.489, Test loss: 0.625, Test accuracy: 75.93
Round   5, Global train loss: 0.489, Global test loss: 2.036, Global test accuracy: 42.24
Round   6, Train loss: 0.484, Test loss: 0.497, Test accuracy: 80.40
Round   6, Global train loss: 0.484, Global test loss: 1.615, Global test accuracy: 49.29
Round   7, Train loss: 0.529, Test loss: 0.514, Test accuracy: 80.44
Round   7, Global train loss: 0.529, Global test loss: 1.981, Global test accuracy: 40.84
Round   8, Train loss: 0.434, Test loss: 0.490, Test accuracy: 80.62
Round   8, Global train loss: 0.434, Global test loss: 1.741, Global test accuracy: 41.99
Round   9, Train loss: 0.496, Test loss: 0.441, Test accuracy: 82.51
Round   9, Global train loss: 0.496, Global test loss: 1.660, Global test accuracy: 48.13
Round  10, Train loss: 0.416, Test loss: 0.429, Test accuracy: 83.12
Round  10, Global train loss: 0.416, Global test loss: 1.400, Global test accuracy: 53.63
Round  11, Train loss: 0.448, Test loss: 0.422, Test accuracy: 83.39
Round  11, Global train loss: 0.448, Global test loss: 1.735, Global test accuracy: 45.00
Round  12, Train loss: 0.403, Test loss: 0.407, Test accuracy: 83.90
Round  12, Global train loss: 0.403, Global test loss: 1.858, Global test accuracy: 48.86
Round  13, Train loss: 0.368, Test loss: 0.378, Test accuracy: 85.21
Round  13, Global train loss: 0.368, Global test loss: 1.773, Global test accuracy: 46.73
Traceback (most recent call last):
  File "main_fedrep.py", line 287, in <module>
    acc_test, loss_test = test_img_local_all(net_glob, args, dataset_test, dict_users_test,
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 133, in test_img_local_all
    a, b = test_img_local(net_local, dataset_test, args, user_idx=idx, idxs=dict_users_test[idx], concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 97, in test_img_local
    test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.241, Test loss: 2.798, Test accuracy: 27.96
Round   1, Train loss: 0.768, Test loss: 1.556, Test accuracy: 42.83
Round   2, Train loss: 0.694, Test loss: 1.124, Test accuracy: 60.63
Round   3, Train loss: 0.612, Test loss: 0.900, Test accuracy: 66.63
Round   4, Train loss: 0.597, Test loss: 0.693, Test accuracy: 71.37
Round   5, Train loss: 0.598, Test loss: 0.717, Test accuracy: 73.81
Round   6, Train loss: 0.546, Test loss: 0.731, Test accuracy: 73.50
Round   7, Train loss: 0.547, Test loss: 0.688, Test accuracy: 75.30
Round   8, Train loss: 0.511, Test loss: 0.531, Test accuracy: 77.34
Round   9, Train loss: 0.494, Test loss: 0.493, Test accuracy: 79.52
Round  10, Train loss: 0.515, Test loss: 0.490, Test accuracy: 80.33
Round  11, Train loss: 0.468, Test loss: 0.486, Test accuracy: 80.13
Round  12, Train loss: 0.433, Test loss: 0.450, Test accuracy: 81.66
Round  13, Train loss: 0.429, Test loss: 0.456, Test accuracy: 81.11
Round  14, Train loss: 0.456, Test loss: 0.421, Test accuracy: 82.95
Round  15, Train loss: 0.431, Test loss: 0.401, Test accuracy: 83.63
Round  16, Train loss: 0.380, Test loss: 0.390, Test accuracy: 84.52
Round  17, Train loss: 0.337, Test loss: 0.400, Test accuracy: 84.00
Round  18, Train loss: 0.344, Test loss: 0.398, Test accuracy: 83.81
Round  19, Train loss: 0.413, Test loss: 0.370, Test accuracy: 85.14
Round  20, Train loss: 0.348, Test loss: 0.378, Test accuracy: 84.98
Round  21, Train loss: 0.365, Test loss: 0.363, Test accuracy: 85.75
Round  22, Train loss: 0.373, Test loss: 0.353, Test accuracy: 86.23
Round  23, Train loss: 0.358, Test loss: 0.351, Test accuracy: 86.39
Round  24, Train loss: 0.330, Test loss: 0.329, Test accuracy: 86.99
Round  25, Train loss: 0.346, Test loss: 0.329, Test accuracy: 87.15
Round  26, Train loss: 0.323, Test loss: 0.326, Test accuracy: 87.31
Round  27, Train loss: 0.307, Test loss: 0.324, Test accuracy: 87.45
Round  28, Train loss: 0.337, Test loss: 0.317, Test accuracy: 87.79
Round  29, Train loss: 0.274, Test loss: 0.312, Test accuracy: 87.83
Round  30, Train loss: 0.293, Test loss: 0.305, Test accuracy: 88.35
Round  31, Train loss: 0.323, Test loss: 0.305, Test accuracy: 88.35
Round  32, Train loss: 0.291, Test loss: 0.301, Test accuracy: 88.47
Round  33, Train loss: 0.283, Test loss: 0.298, Test accuracy: 88.60
Round  34, Train loss: 0.237, Test loss: 0.296, Test accuracy: 88.67
Round  35, Train loss: 0.322, Test loss: 0.290, Test accuracy: 88.90
Round  36, Train loss: 0.306, Test loss: 0.298, Test accuracy: 88.69
Round  37, Train loss: 0.252, Test loss: 0.294, Test accuracy: 88.83
Round  38, Train loss: 0.270, Test loss: 0.291, Test accuracy: 89.02
Round  39, Train loss: 0.219, Test loss: 0.298, Test accuracy: 88.67
Round  40, Train loss: 0.281, Test loss: 0.284, Test accuracy: 89.18
Round  41, Train loss: 0.223, Test loss: 0.289, Test accuracy: 89.03
Round  42, Train loss: 0.268, Test loss: 0.285, Test accuracy: 89.23
Round  43, Train loss: 0.276, Test loss: 0.282, Test accuracy: 89.40
Round  44, Train loss: 0.279, Test loss: 0.280, Test accuracy: 89.37
Round  45, Train loss: 0.261, Test loss: 0.279, Test accuracy: 89.23
Round  46, Train loss: 0.251, Test loss: 0.282, Test accuracy: 89.28
Round  47, Train loss: 0.229, Test loss: 0.281, Test accuracy: 89.06
Round  48, Train loss: 0.267, Test loss: 0.288, Test accuracy: 88.87
Round  49, Train loss: 0.222, Test loss: 0.284, Test accuracy: 89.45
Round  50, Train loss: 0.225, Test loss: 0.278, Test accuracy: 89.55
Round  51, Train loss: 0.281, Test loss: 0.275, Test accuracy: 89.69
Round  52, Train loss: 0.233, Test loss: 0.270, Test accuracy: 89.76
Round  53, Train loss: 0.251, Test loss: 0.273, Test accuracy: 89.78
Round  54, Train loss: 0.248, Test loss: 0.271, Test accuracy: 89.60
Round  55, Train loss: 0.230, Test loss: 0.275, Test accuracy: 89.59
Round  56, Train loss: 0.249, Test loss: 0.272, Test accuracy: 89.66
Round  57, Train loss: 0.204, Test loss: 0.263, Test accuracy: 90.02
Round  58, Train loss: 0.211, Test loss: 0.276, Test accuracy: 89.57
Round  59, Train loss: 0.185, Test loss: 0.262, Test accuracy: 90.08
Round  60, Train loss: 0.215, Test loss: 0.264, Test accuracy: 90.07
Round  61, Train loss: 0.206, Test loss: 0.261, Test accuracy: 90.24
Round  62, Train loss: 0.220, Test loss: 0.266, Test accuracy: 90.09
Round  63, Train loss: 0.220, Test loss: 0.265, Test accuracy: 90.10
Round  64, Train loss: 0.226, Test loss: 0.263, Test accuracy: 89.92
Round  65, Train loss: 0.194, Test loss: 0.267, Test accuracy: 89.80
Round  66, Train loss: 0.202, Test loss: 0.265, Test accuracy: 89.91
Round  67, Train loss: 0.239, Test loss: 0.260, Test accuracy: 90.07
Round  68, Train loss: 0.184, Test loss: 0.256, Test accuracy: 90.31
Round  69, Train loss: 0.245, Test loss: 0.263, Test accuracy: 90.07
Round  70, Train loss: 0.195, Test loss: 0.262, Test accuracy: 90.18
Round  71, Train loss: 0.214, Test loss: 0.258, Test accuracy: 90.33
Round  72, Train loss: 0.199, Test loss: 0.265, Test accuracy: 90.13
Round  73, Train loss: 0.215, Test loss: 0.256, Test accuracy: 90.42
Round  74, Train loss: 0.195, Test loss: 0.258, Test accuracy: 90.22
Round  75, Train loss: 0.200, Test loss: 0.263, Test accuracy: 90.14
Round  76, Train loss: 0.202, Test loss: 0.262, Test accuracy: 90.16
Round  77, Train loss: 0.181, Test loss: 0.259, Test accuracy: 90.40
Round  78, Train loss: 0.183, Test loss: 0.257, Test accuracy: 90.54
Round  79, Train loss: 0.192, Test loss: 0.256, Test accuracy: 90.49
Round  80, Train loss: 0.198, Test loss: 0.254, Test accuracy: 90.60
Round  81, Train loss: 0.205, Test loss: 0.251, Test accuracy: 90.70
Round  82, Train loss: 0.196, Test loss: 0.258, Test accuracy: 90.48
Round  83, Train loss: 0.160, Test loss: 0.259, Test accuracy: 90.48
Round  84, Train loss: 0.172, Test loss: 0.263, Test accuracy: 90.21
Round  85, Train loss: 0.176, Test loss: 0.257, Test accuracy: 90.70
Round  86, Train loss: 0.191, Test loss: 0.257, Test accuracy: 90.68
Round  87, Train loss: 0.133, Test loss: 0.270, Test accuracy: 90.35
Round  88, Train loss: 0.199, Test loss: 0.260, Test accuracy: 90.53
Round  89, Train loss: 0.166, Test loss: 0.269, Test accuracy: 90.34
Round  90, Train loss: 0.174, Test loss: 0.267, Test accuracy: 90.42
Round  91, Train loss: 0.191, Test loss: 0.260, Test accuracy: 90.58
Round  92, Train loss: 0.167, Test loss: 0.262, Test accuracy: 90.44
Round  93, Train loss: 0.130, Test loss: 0.258, Test accuracy: 90.70
Round  94, Train loss: 0.143, Test loss: 0.259, Test accuracy: 90.56
Round  95, Train loss: 0.178, Test loss: 0.258, Test accuracy: 90.58
Round  96, Train loss: 0.182, Test loss: 0.260, Test accuracy: 90.58
Round  97, Train loss: 0.166, Test loss: 0.264, Test accuracy: 90.34
Round  98, Train loss: 0.137, Test loss: 0.263, Test accuracy: 90.41
Round  99, Train loss: 0.172, Test loss: 0.259, Test accuracy: 90.50
Final Round, Train loss: 0.142, Test loss: 0.256, Test accuracy: 90.74
Average accuracy final 10 rounds: 90.511 

3560.733338356018
[3.1987273693084717, 6.397454738616943, 9.603716611862183, 12.809978485107422, 16.01529097557068, 19.220603466033936, 22.43312120437622, 25.645638942718506, 28.76817011833191, 31.890701293945312, 34.97975516319275, 38.068809032440186, 41.16692876815796, 44.26504850387573, 47.34492254257202, 50.42479658126831, 53.50444030761719, 56.584084033966064, 59.65795826911926, 62.73183250427246, 65.8250424861908, 68.91825246810913, 72.00223016738892, 75.0862078666687, 78.16505670547485, 81.243905544281, 84.31766843795776, 87.39143133163452, 90.48634791374207, 93.58126449584961, 96.80285263061523, 100.02444076538086, 103.25508332252502, 106.48572587966919, 109.70591259002686, 112.92609930038452, 116.13848638534546, 119.3508734703064, 122.58476233482361, 125.81865119934082, 129.05075120925903, 132.28285121917725, 135.52661895751953, 138.77038669586182, 141.98014545440674, 145.18990421295166, 148.41159343719482, 151.633282661438, 154.88017106056213, 158.12705945968628, 161.38690328598022, 164.64674711227417, 167.89496421813965, 171.14318132400513, 174.37706851959229, 177.61095571517944, 180.84989857673645, 184.08884143829346, 187.3334619998932, 190.57808256149292, 193.8357470035553, 197.09341144561768, 200.34585213661194, 203.5982928276062, 206.81761384010315, 210.0369348526001, 213.27983713150024, 216.5227394104004, 219.76587319374084, 223.0090069770813, 226.27041792869568, 229.53182888031006, 232.78794741630554, 236.04406595230103, 239.27742385864258, 242.51078176498413, 245.7693440914154, 249.02790641784668, 252.27643275260925, 255.52495908737183, 258.7964701652527, 262.06798124313354, 265.29182171821594, 268.51566219329834, 271.7552661895752, 274.99487018585205, 278.23146748542786, 281.46806478500366, 284.7201054096222, 287.9721460342407, 291.23224115371704, 294.49233627319336, 297.7311809062958, 300.9700255393982, 304.2145493030548, 307.4590730667114, 310.7041988372803, 313.9493246078491, 317.2002673149109, 320.45121002197266, 323.70783495903015, 326.96445989608765, 330.2116241455078, 333.458788394928, 336.69762134552, 339.93645429611206, 343.1758556365967, 346.4152569770813, 349.65610671043396, 352.8969564437866, 356.14028549194336, 359.3836145401001, 362.6405849456787, 365.8975553512573, 369.1427891254425, 372.3880228996277, 375.6235604286194, 378.8590979576111, 382.10529947280884, 385.3515009880066, 388.58783411979675, 391.8241672515869, 395.05126190185547, 398.278356552124, 401.46105456352234, 404.64375257492065, 408.0054421424866, 411.3671317100525, 414.6007478237152, 417.83436393737793, 421.1416609287262, 424.44895792007446, 427.7072570323944, 430.96555614471436, 434.192928314209, 437.4203004837036, 440.66562724113464, 443.9109539985657, 447.14427161216736, 450.37758922576904, 453.6158483028412, 456.85410737991333, 460.07663798332214, 463.29916858673096, 466.51334524154663, 469.7275218963623, 472.96509528160095, 476.2026686668396, 479.4133937358856, 482.62411880493164, 485.76918148994446, 488.9142441749573, 492.1645038127899, 495.41476345062256, 498.63743686676025, 501.86011028289795, 505.26824736595154, 508.6763844490051, 511.917439699173, 515.1584949493408, 518.3903937339783, 521.6222925186157, 524.8541307449341, 528.0859689712524, 531.0226686000824, 533.9593682289124, 536.9390761852264, 539.9187841415405, 542.8895561695099, 545.8603281974792, 548.9550278186798, 552.0497274398804, 555.0464587211609, 558.0431900024414, 561.0058212280273, 563.9684524536133, 566.9661712646484, 569.9638900756836, 572.8731515407562, 575.7824130058289, 578.7116684913635, 581.6409239768982, 584.5738520622253, 587.5067801475525, 590.4735591411591, 593.4403381347656, 596.3956804275513, 599.3510227203369, 602.301177740097, 605.2513327598572, 608.1972260475159, 611.1431193351746, 614.2083630561829, 617.2736067771912, 620.2551805973053, 623.2367544174194, 626.1955132484436, 629.1542720794678, 632.0977580547333, 635.0412440299988, 636.9969499111176, 638.9526557922363]
[27.963333333333335, 27.963333333333335, 42.82666666666667, 42.82666666666667, 60.63, 60.63, 66.63, 66.63, 71.37333333333333, 71.37333333333333, 73.81, 73.81, 73.5, 73.5, 75.30333333333333, 75.30333333333333, 77.34, 77.34, 79.51666666666667, 79.51666666666667, 80.33333333333333, 80.33333333333333, 80.13333333333334, 80.13333333333334, 81.66, 81.66, 81.10666666666667, 81.10666666666667, 82.95333333333333, 82.95333333333333, 83.63333333333334, 83.63333333333334, 84.52333333333333, 84.52333333333333, 84.0, 84.0, 83.81333333333333, 83.81333333333333, 85.13666666666667, 85.13666666666667, 84.97666666666667, 84.97666666666667, 85.75333333333333, 85.75333333333333, 86.22666666666667, 86.22666666666667, 86.39, 86.39, 86.98666666666666, 86.98666666666666, 87.14666666666666, 87.14666666666666, 87.30666666666667, 87.30666666666667, 87.44666666666667, 87.44666666666667, 87.79, 87.79, 87.83, 87.83, 88.35333333333334, 88.35333333333334, 88.34666666666666, 88.34666666666666, 88.47, 88.47, 88.6, 88.6, 88.67333333333333, 88.67333333333333, 88.90333333333334, 88.90333333333334, 88.69333333333333, 88.69333333333333, 88.83, 88.83, 89.02333333333333, 89.02333333333333, 88.66666666666667, 88.66666666666667, 89.18333333333334, 89.18333333333334, 89.03, 89.03, 89.23, 89.23, 89.40333333333334, 89.40333333333334, 89.37333333333333, 89.37333333333333, 89.22666666666667, 89.22666666666667, 89.28333333333333, 89.28333333333333, 89.05666666666667, 89.05666666666667, 88.86666666666666, 88.86666666666666, 89.44666666666667, 89.44666666666667, 89.55, 89.55, 89.69, 89.69, 89.75666666666666, 89.75666666666666, 89.78, 89.78, 89.60333333333334, 89.60333333333334, 89.59333333333333, 89.59333333333333, 89.66, 89.66, 90.02333333333333, 90.02333333333333, 89.56666666666666, 89.56666666666666, 90.08333333333333, 90.08333333333333, 90.06666666666666, 90.06666666666666, 90.24, 90.24, 90.08666666666667, 90.08666666666667, 90.09666666666666, 90.09666666666666, 89.91666666666667, 89.91666666666667, 89.79666666666667, 89.79666666666667, 89.90666666666667, 89.90666666666667, 90.06666666666666, 90.06666666666666, 90.30666666666667, 90.30666666666667, 90.06666666666666, 90.06666666666666, 90.18333333333334, 90.18333333333334, 90.33, 90.33, 90.13, 90.13, 90.42333333333333, 90.42333333333333, 90.22, 90.22, 90.13666666666667, 90.13666666666667, 90.15666666666667, 90.15666666666667, 90.39666666666666, 90.39666666666666, 90.54333333333334, 90.54333333333334, 90.48666666666666, 90.48666666666666, 90.59666666666666, 90.59666666666666, 90.7, 90.7, 90.48, 90.48, 90.48, 90.48, 90.21333333333334, 90.21333333333334, 90.70333333333333, 90.70333333333333, 90.68333333333334, 90.68333333333334, 90.35333333333334, 90.35333333333334, 90.53333333333333, 90.53333333333333, 90.33666666666667, 90.33666666666667, 90.42, 90.42, 90.57666666666667, 90.57666666666667, 90.43666666666667, 90.43666666666667, 90.70333333333333, 90.70333333333333, 90.56, 90.56, 90.58, 90.58, 90.58333333333333, 90.58333333333333, 90.34, 90.34, 90.41, 90.41, 90.5, 90.5, 90.74333333333334, 90.74333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.236, Test loss: 1.971, Test accuracy: 31.77
Round   1, Train loss: 1.895, Test loss: 1.649, Test accuracy: 41.19
Round   2, Train loss: 1.675, Test loss: 1.510, Test accuracy: 46.68
Round   3, Train loss: 1.573, Test loss: 1.439, Test accuracy: 49.87
Round   4, Train loss: 1.498, Test loss: 1.399, Test accuracy: 52.60
Round   5, Train loss: 1.443, Test loss: 1.366, Test accuracy: 55.02
Round   6, Train loss: 1.399, Test loss: 1.293, Test accuracy: 57.08
Round   7, Train loss: 1.344, Test loss: 1.215, Test accuracy: 58.56
Round   8, Train loss: 1.281, Test loss: 1.181, Test accuracy: 60.00
Round   9, Train loss: 1.236, Test loss: 1.151, Test accuracy: 61.13
Round  10, Train loss: 1.196, Test loss: 1.105, Test accuracy: 62.35
Round  11, Train loss: 1.164, Test loss: 1.069, Test accuracy: 63.02
Round  12, Train loss: 1.108, Test loss: 1.058, Test accuracy: 64.30
Round  13, Train loss: 1.098, Test loss: 1.024, Test accuracy: 65.11
Round  14, Train loss: 1.040, Test loss: 1.020, Test accuracy: 65.75
Round  15, Train loss: 1.049, Test loss: 0.965, Test accuracy: 67.09
Round  16, Train loss: 1.031, Test loss: 0.938, Test accuracy: 68.15
Round  17, Train loss: 0.982, Test loss: 0.943, Test accuracy: 68.25
Round  18, Train loss: 0.969, Test loss: 0.913, Test accuracy: 69.39
Round  19, Train loss: 0.941, Test loss: 0.906, Test accuracy: 69.85
Round  20, Train loss: 0.933, Test loss: 0.911, Test accuracy: 69.43
Round  21, Train loss: 0.941, Test loss: 0.893, Test accuracy: 69.59
Round  22, Train loss: 0.874, Test loss: 0.879, Test accuracy: 70.69
Round  23, Train loss: 0.877, Test loss: 0.862, Test accuracy: 71.15
Round  24, Train loss: 0.888, Test loss: 0.856, Test accuracy: 71.00
Round  25, Train loss: 0.869, Test loss: 0.835, Test accuracy: 71.75
Round  26, Train loss: 0.822, Test loss: 0.852, Test accuracy: 71.32
Round  27, Train loss: 0.880, Test loss: 0.818, Test accuracy: 72.37
Round  28, Train loss: 0.833, Test loss: 0.812, Test accuracy: 72.59
Round  29, Train loss: 0.806, Test loss: 0.803, Test accuracy: 72.64
Round  30, Train loss: 0.797, Test loss: 0.808, Test accuracy: 73.34
Round  31, Train loss: 0.791, Test loss: 0.796, Test accuracy: 73.04
Round  32, Train loss: 0.773, Test loss: 0.788, Test accuracy: 73.25
Round  33, Train loss: 0.773, Test loss: 0.784, Test accuracy: 73.35
Round  34, Train loss: 0.761, Test loss: 0.778, Test accuracy: 73.56
Round  35, Train loss: 0.739, Test loss: 0.780, Test accuracy: 74.21
Round  36, Train loss: 0.746, Test loss: 0.775, Test accuracy: 73.78
Round  37, Train loss: 0.729, Test loss: 0.770, Test accuracy: 73.67
Round  38, Train loss: 0.723, Test loss: 0.776, Test accuracy: 74.00
Round  39, Train loss: 0.710, Test loss: 0.772, Test accuracy: 74.08
Round  40, Train loss: 0.742, Test loss: 0.761, Test accuracy: 74.05
Round  41, Train loss: 0.706, Test loss: 0.765, Test accuracy: 74.35
Round  42, Train loss: 0.695, Test loss: 0.762, Test accuracy: 74.12
Round  43, Train loss: 0.690, Test loss: 0.765, Test accuracy: 74.06
Round  44, Train loss: 0.685, Test loss: 0.767, Test accuracy: 73.78
Round  45, Train loss: 0.660, Test loss: 0.769, Test accuracy: 73.83
Round  46, Train loss: 0.720, Test loss: 0.755, Test accuracy: 74.56
Round  47, Train loss: 0.676, Test loss: 0.753, Test accuracy: 75.06
Round  48, Train loss: 0.663, Test loss: 0.755, Test accuracy: 74.47
Round  49, Train loss: 0.680, Test loss: 0.745, Test accuracy: 75.12
Round  50, Train loss: 0.644, Test loss: 0.752, Test accuracy: 74.73
Round  51, Train loss: 0.649, Test loss: 0.752, Test accuracy: 74.83
Round  52, Train loss: 0.645, Test loss: 0.747, Test accuracy: 74.92
Round  53, Train loss: 0.629, Test loss: 0.748, Test accuracy: 75.12
Round  54, Train loss: 0.634, Test loss: 0.739, Test accuracy: 75.47
Round  55, Train loss: 0.625, Test loss: 0.751, Test accuracy: 74.89
Round  56, Train loss: 0.624, Test loss: 0.748, Test accuracy: 74.99
Round  57, Train loss: 0.641, Test loss: 0.734, Test accuracy: 75.75
Round  58, Train loss: 0.621, Test loss: 0.729, Test accuracy: 75.62
Round  59, Train loss: 0.627, Test loss: 0.725, Test accuracy: 75.90
Round  60, Train loss: 0.619, Test loss: 0.736, Test accuracy: 75.36
Round  61, Train loss: 0.607, Test loss: 0.743, Test accuracy: 75.20
Round  62, Train loss: 0.628, Test loss: 0.743, Test accuracy: 75.17
Round  63, Train loss: 0.635, Test loss: 0.743, Test accuracy: 75.36
Round  64, Train loss: 0.592, Test loss: 0.738, Test accuracy: 75.65
Round  65, Train loss: 0.594, Test loss: 0.746, Test accuracy: 75.56
Round  66, Train loss: 0.584, Test loss: 0.735, Test accuracy: 75.77
Round  67, Train loss: 0.590, Test loss: 0.743, Test accuracy: 75.84
Round  68, Train loss: 0.577, Test loss: 0.742, Test accuracy: 75.47
Round  69, Train loss: 0.566, Test loss: 0.743, Test accuracy: 75.35
Round  70, Train loss: 0.573, Test loss: 0.730, Test accuracy: 76.20
Round  71, Train loss: 0.568, Test loss: 0.741, Test accuracy: 75.72
Round  72, Train loss: 0.582, Test loss: 0.733, Test accuracy: 75.71
Round  73, Train loss: 0.589, Test loss: 0.725, Test accuracy: 76.44
Round  74, Train loss: 0.542, Test loss: 0.737, Test accuracy: 75.91
Round  75, Train loss: 0.537, Test loss: 0.737, Test accuracy: 76.00
Round  76, Train loss: 0.562, Test loss: 0.733, Test accuracy: 76.31
Round  77, Train loss: 0.564, Test loss: 0.736, Test accuracy: 76.32
Round  78, Train loss: 0.548, Test loss: 0.747, Test accuracy: 75.88
Round  79, Train loss: 0.536, Test loss: 0.733, Test accuracy: 76.03
Round  80, Train loss: 0.586, Test loss: 0.724, Test accuracy: 76.48
Round  81, Train loss: 0.559, Test loss: 0.733, Test accuracy: 76.46
Round  82, Train loss: 0.557, Test loss: 0.746, Test accuracy: 75.95
Round  83, Train loss: 0.541, Test loss: 0.739, Test accuracy: 75.87
Round  84, Train loss: 0.526, Test loss: 0.731, Test accuracy: 76.02
Round  85, Train loss: 0.551, Test loss: 0.731, Test accuracy: 75.91
Round  86, Train loss: 0.538, Test loss: 0.736, Test accuracy: 76.15
Round  87, Train loss: 0.522, Test loss: 0.731, Test accuracy: 76.12
Round  88, Train loss: 0.542, Test loss: 0.728, Test accuracy: 76.23
Round  89, Train loss: 0.500, Test loss: 0.730, Test accuracy: 76.23
Round  90, Train loss: 0.522, Test loss: 0.735, Test accuracy: 76.16
Round  91, Train loss: 0.489, Test loss: 0.740, Test accuracy: 76.00
Round  92, Train loss: 0.543, Test loss: 0.744, Test accuracy: 76.15
Round  93, Train loss: 0.535, Test loss: 0.743, Test accuracy: 76.15
Round  94, Train loss: 0.521, Test loss: 0.740, Test accuracy: 76.28
Round  95, Train loss: 0.491, Test loss: 0.749, Test accuracy: 76.20
Round  96, Train loss: 0.510, Test loss: 0.737, Test accuracy: 76.69
Round  97, Train loss: 0.517, Test loss: 0.730, Test accuracy: 76.48
Round  98, Train loss: 0.548, Test loss: 0.730, Test accuracy: 76.38
Round  99, Train loss: 0.517, Test loss: 0.724, Test accuracy: 76.76
Final Round, Train loss: 0.407, Test loss: 0.721, Test accuracy: 77.04
Average accuracy final 10 rounds: 76.3255
5733.162272691727
[6.372679948806763, 12.745359897613525, 18.7161865234375, 24.687013149261475, 30.649366855621338, 36.6117205619812, 42.651488304138184, 48.691256046295166, 54.62556028366089, 60.55986452102661, 66.49826383590698, 72.43666315078735, 78.34497356414795, 84.25328397750854, 90.23140716552734, 96.20953035354614, 102.15304183959961, 108.09655332565308, 113.882732629776, 119.66891193389893, 125.39262318611145, 131.11633443832397, 136.85264945030212, 142.58896446228027, 148.42283129692078, 154.25669813156128, 160.12773370742798, 165.99876928329468, 171.94878458976746, 177.89879989624023, 183.8672297000885, 189.83565950393677, 195.76659536361694, 201.69753122329712, 207.61978006362915, 213.54202890396118, 219.51611590385437, 225.49020290374756, 231.30868530273438, 237.1271677017212, 242.76017498970032, 248.39318227767944, 254.27339458465576, 260.1536068916321, 266.04643511772156, 271.93926334381104, 277.8013072013855, 283.66335105895996, 289.60292291641235, 295.54249477386475, 301.4723446369171, 307.4021944999695, 313.3519570827484, 319.30171966552734, 325.15881514549255, 331.01591062545776, 336.9700791835785, 342.9242477416992, 348.78374552726746, 354.6432433128357, 360.51050329208374, 366.3777632713318, 372.24289536476135, 378.1080274581909, 383.68295907974243, 389.25789070129395, 395.08608746528625, 400.91428422927856, 406.87193417549133, 412.8295841217041, 418.7169723510742, 424.60436058044434, 430.4920070171356, 436.3796534538269, 442.2679867744446, 448.15632009506226, 453.9075312614441, 459.6587424278259, 465.5534255504608, 471.4481086730957, 477.40547227859497, 483.36283588409424, 489.1033477783203, 494.8438596725464, 501.0022704601288, 507.1606812477112, 513.2484209537506, 519.33616065979, 525.316490650177, 531.296820640564, 537.0821771621704, 542.8675336837769, 548.7740955352783, 554.6806573867798, 560.6578006744385, 566.6349439620972, 572.5939605236053, 578.5529770851135, 584.4260892868042, 590.2992014884949, 596.2066633701324, 602.11412525177, 608.0426440238953, 613.9711627960205, 619.9343075752258, 625.8974523544312, 631.6636700630188, 637.4298877716064, 643.3951280117035, 649.3603682518005, 655.2506239414215, 661.1408796310425, 667.1604673862457, 673.180055141449, 679.2286846637726, 685.2773141860962, 691.2332992553711, 697.189284324646, 703.1359004974365, 709.082516670227, 714.9916698932648, 720.9008231163025, 726.8420855998993, 732.7833480834961, 738.6349415779114, 744.4865350723267, 750.3748531341553, 756.2631711959839, 762.0904970169067, 767.9178228378296, 773.6951603889465, 779.4724979400635, 785.3990662097931, 791.3256344795227, 797.3042461872101, 803.2828578948975, 809.1577117443085, 815.0325655937195, 820.9991099834442, 826.965654373169, 832.910003900528, 838.854353427887, 844.8286647796631, 850.8029761314392, 856.7831432819366, 862.7633104324341, 868.7035596370697, 874.6438088417053, 880.6387720108032, 886.6337351799011, 892.6029713153839, 898.5722074508667, 904.5218963623047, 910.4715852737427, 916.4373624324799, 922.403139591217, 928.3457975387573, 934.2884554862976, 940.2928991317749, 946.2973427772522, 952.3201560974121, 958.342969417572, 964.2942085266113, 970.2454476356506, 976.2590618133545, 982.2726759910583, 988.2605218887329, 994.2483677864075, 999.8189942836761, 1005.3896207809448, 1011.3776476383209, 1017.365674495697, 1023.2654976844788, 1029.1653208732605, 1034.9044773578644, 1040.6436338424683, 1046.4435567855835, 1052.2434797286987, 1058.0992984771729, 1063.955117225647, 1069.6341242790222, 1075.3131313323975, 1081.129436969757, 1086.9457426071167, 1092.6540911197662, 1098.3624396324158, 1104.2330119609833, 1110.1035842895508, 1115.9847507476807, 1121.8659172058105, 1127.8138766288757, 1133.761836051941, 1139.606768131256, 1145.4517002105713, 1151.1990303993225, 1156.9463605880737, 1162.6773254871368, 1168.4082903862, 1173.8106486797333, 1179.2130069732666, 1181.3484251499176, 1183.4838433265686]
[31.765, 31.765, 41.19, 41.19, 46.6775, 46.6775, 49.8675, 49.8675, 52.5975, 52.5975, 55.02, 55.02, 57.0825, 57.0825, 58.5575, 58.5575, 60.0, 60.0, 61.135, 61.135, 62.3475, 62.3475, 63.0225, 63.0225, 64.3, 64.3, 65.1125, 65.1125, 65.745, 65.745, 67.09, 67.09, 68.1475, 68.1475, 68.2525, 68.2525, 69.3875, 69.3875, 69.85, 69.85, 69.4325, 69.4325, 69.5925, 69.5925, 70.6925, 70.6925, 71.1525, 71.1525, 71.0, 71.0, 71.75, 71.75, 71.32, 71.32, 72.37, 72.37, 72.5875, 72.5875, 72.6425, 72.6425, 73.3375, 73.3375, 73.04, 73.04, 73.25, 73.25, 73.3525, 73.3525, 73.56, 73.56, 74.21, 74.21, 73.775, 73.775, 73.6725, 73.6725, 73.995, 73.995, 74.075, 74.075, 74.05, 74.05, 74.3475, 74.3475, 74.1225, 74.1225, 74.0625, 74.0625, 73.775, 73.775, 73.8275, 73.8275, 74.565, 74.565, 75.06, 75.06, 74.47, 74.47, 75.1225, 75.1225, 74.7275, 74.7275, 74.83, 74.83, 74.915, 74.915, 75.1175, 75.1175, 75.47, 75.47, 74.8925, 74.8925, 74.9875, 74.9875, 75.7525, 75.7525, 75.6225, 75.6225, 75.9025, 75.9025, 75.36, 75.36, 75.2, 75.2, 75.17, 75.17, 75.36, 75.36, 75.65, 75.65, 75.5625, 75.5625, 75.7725, 75.7725, 75.8375, 75.8375, 75.47, 75.47, 75.3525, 75.3525, 76.2, 76.2, 75.7225, 75.7225, 75.7125, 75.7125, 76.435, 76.435, 75.9075, 75.9075, 75.9975, 75.9975, 76.305, 76.305, 76.32, 76.32, 75.88, 75.88, 76.03, 76.03, 76.485, 76.485, 76.4625, 76.4625, 75.9525, 75.9525, 75.8675, 75.8675, 76.02, 76.02, 75.9075, 75.9075, 76.1475, 76.1475, 76.125, 76.125, 76.2325, 76.2325, 76.2325, 76.2325, 76.1625, 76.1625, 76.0, 76.0, 76.15, 76.15, 76.15, 76.15, 76.2825, 76.2825, 76.1975, 76.1975, 76.6925, 76.6925, 76.4775, 76.4775, 76.3825, 76.3825, 76.76, 76.76, 77.0375, 77.0375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.163, Test loss: 1.706, Test accuracy: 40.02
Round   1, Train loss: 1.763, Test loss: 1.446, Test accuracy: 47.87
Round   2, Train loss: 1.598, Test loss: 1.334, Test accuracy: 51.96
Round   3, Train loss: 1.479, Test loss: 1.242, Test accuracy: 55.59
Round   4, Train loss: 1.401, Test loss: 1.184, Test accuracy: 57.88
Round   5, Train loss: 1.298, Test loss: 1.105, Test accuracy: 59.84
Round   6, Train loss: 1.254, Test loss: 1.060, Test accuracy: 62.59
Round   7, Train loss: 1.167, Test loss: 1.022, Test accuracy: 64.28
Round   8, Train loss: 1.132, Test loss: 0.990, Test accuracy: 65.56
Round   9, Train loss: 1.083, Test loss: 0.962, Test accuracy: 66.17
Round  10, Train loss: 1.066, Test loss: 0.928, Test accuracy: 67.63
Round  11, Train loss: 1.029, Test loss: 0.907, Test accuracy: 68.52
Round  12, Train loss: 1.014, Test loss: 0.868, Test accuracy: 69.91
Round  13, Train loss: 0.960, Test loss: 0.863, Test accuracy: 70.32
Round  14, Train loss: 0.901, Test loss: 0.847, Test accuracy: 70.31
Round  15, Train loss: 0.922, Test loss: 0.832, Test accuracy: 71.08
Round  16, Train loss: 0.861, Test loss: 0.825, Test accuracy: 71.80
Round  17, Train loss: 0.835, Test loss: 0.815, Test accuracy: 72.70
Round  18, Train loss: 0.842, Test loss: 0.801, Test accuracy: 72.54
Round  19, Train loss: 0.820, Test loss: 0.793, Test accuracy: 72.69
Round  20, Train loss: 0.792, Test loss: 0.788, Test accuracy: 73.55
Round  21, Train loss: 0.784, Test loss: 0.775, Test accuracy: 73.54
Round  22, Train loss: 0.757, Test loss: 0.788, Test accuracy: 73.07
Round  23, Train loss: 0.733, Test loss: 0.776, Test accuracy: 73.99
Round  24, Train loss: 0.743, Test loss: 0.776, Test accuracy: 73.92
Round  25, Train loss: 0.727, Test loss: 0.774, Test accuracy: 73.97
Round  26, Train loss: 0.744, Test loss: 0.762, Test accuracy: 74.59
Round  27, Train loss: 0.697, Test loss: 0.755, Test accuracy: 74.50
Round  28, Train loss: 0.697, Test loss: 0.755, Test accuracy: 74.89
Round  29, Train loss: 0.720, Test loss: 0.750, Test accuracy: 74.72
Round  30, Train loss: 0.662, Test loss: 0.745, Test accuracy: 75.45
Round  31, Train loss: 0.658, Test loss: 0.742, Test accuracy: 75.92
Round  32, Train loss: 0.653, Test loss: 0.747, Test accuracy: 75.60
Round  33, Train loss: 0.655, Test loss: 0.737, Test accuracy: 75.42
Round  34, Train loss: 0.642, Test loss: 0.753, Test accuracy: 75.44
Round  35, Train loss: 0.610, Test loss: 0.742, Test accuracy: 75.78
Round  36, Train loss: 0.602, Test loss: 0.765, Test accuracy: 75.11
Round  37, Train loss: 0.576, Test loss: 0.759, Test accuracy: 75.05
Round  38, Train loss: 0.589, Test loss: 0.740, Test accuracy: 76.11
Round  39, Train loss: 0.543, Test loss: 0.769, Test accuracy: 75.33
Round  40, Train loss: 0.619, Test loss: 0.738, Test accuracy: 76.09
Round  41, Train loss: 0.608, Test loss: 0.730, Test accuracy: 75.81
Round  42, Train loss: 0.609, Test loss: 0.730, Test accuracy: 76.02
Round  43, Train loss: 0.576, Test loss: 0.729, Test accuracy: 76.59
Round  44, Train loss: 0.563, Test loss: 0.733, Test accuracy: 76.45
Round  45, Train loss: 0.597, Test loss: 0.719, Test accuracy: 76.50
Round  46, Train loss: 0.537, Test loss: 0.742, Test accuracy: 76.58
Round  47, Train loss: 0.554, Test loss: 0.728, Test accuracy: 76.49
Round  48, Train loss: 0.566, Test loss: 0.731, Test accuracy: 77.21
Round  49, Train loss: 0.551, Test loss: 0.730, Test accuracy: 76.92
Round  50, Train loss: 0.542, Test loss: 0.733, Test accuracy: 76.92
Round  51, Train loss: 0.533, Test loss: 0.736, Test accuracy: 76.99
Round  52, Train loss: 0.541, Test loss: 0.727, Test accuracy: 77.12
Round  53, Train loss: 0.512, Test loss: 0.745, Test accuracy: 76.19
Round  54, Train loss: 0.490, Test loss: 0.749, Test accuracy: 76.89
Round  55, Train loss: 0.514, Test loss: 0.751, Test accuracy: 76.61
Round  56, Train loss: 0.544, Test loss: 0.732, Test accuracy: 77.04
Round  57, Train loss: 0.494, Test loss: 0.732, Test accuracy: 77.12
Round  58, Train loss: 0.530, Test loss: 0.722, Test accuracy: 77.43
Round  59, Train loss: 0.481, Test loss: 0.761, Test accuracy: 76.57
Round  60, Train loss: 0.530, Test loss: 0.742, Test accuracy: 77.08
Round  61, Train loss: 0.442, Test loss: 0.748, Test accuracy: 76.86
Round  62, Train loss: 0.463, Test loss: 0.747, Test accuracy: 76.72
Round  63, Train loss: 0.483, Test loss: 0.749, Test accuracy: 76.82
Round  64, Train loss: 0.501, Test loss: 0.742, Test accuracy: 77.12
Round  65, Train loss: 0.504, Test loss: 0.723, Test accuracy: 77.27
Round  66, Train loss: 0.485, Test loss: 0.716, Test accuracy: 77.61
Round  67, Train loss: 0.454, Test loss: 0.736, Test accuracy: 77.38
Round  68, Train loss: 0.434, Test loss: 0.750, Test accuracy: 77.18
Round  69, Train loss: 0.510, Test loss: 0.719, Test accuracy: 77.61
Round  70, Train loss: 0.457, Test loss: 0.726, Test accuracy: 77.55
Round  71, Train loss: 0.471, Test loss: 0.730, Test accuracy: 77.69
Round  72, Train loss: 0.473, Test loss: 0.725, Test accuracy: 77.73
Round  73, Train loss: 0.421, Test loss: 0.749, Test accuracy: 77.69
Round  74, Train loss: 0.453, Test loss: 0.746, Test accuracy: 77.96
Round  75, Train loss: 0.477, Test loss: 0.726, Test accuracy: 77.33
Round  76, Train loss: 0.439, Test loss: 0.742, Test accuracy: 77.44
Round  77, Train loss: 0.431, Test loss: 0.744, Test accuracy: 77.81
Round  78, Train loss: 0.424, Test loss: 0.760, Test accuracy: 77.58
Round  79, Train loss: 0.452, Test loss: 0.735, Test accuracy: 77.88
Round  80, Train loss: 0.449, Test loss: 0.729, Test accuracy: 77.56
Round  81, Train loss: 0.435, Test loss: 0.746, Test accuracy: 77.36
Round  82, Train loss: 0.431, Test loss: 0.751, Test accuracy: 77.21
Round  83, Train loss: 0.417, Test loss: 0.756, Test accuracy: 78.07
Round  84, Train loss: 0.403, Test loss: 0.765, Test accuracy: 77.22
Round  85, Train loss: 0.435, Test loss: 0.744, Test accuracy: 77.77
Round  86, Train loss: 0.414, Test loss: 0.748, Test accuracy: 77.80
Round  87, Train loss: 0.432, Test loss: 0.729, Test accuracy: 77.84
Round  88, Train loss: 0.409, Test loss: 0.751, Test accuracy: 78.01
Round  89, Train loss: 0.408, Test loss: 0.758, Test accuracy: 78.05
Round  90, Train loss: 0.398, Test loss: 0.763, Test accuracy: 78.07
Round  91, Train loss: 0.382, Test loss: 0.755, Test accuracy: 77.92
Round  92, Train loss: 0.422, Test loss: 0.755, Test accuracy: 78.10
Round  93, Train loss: 0.415, Test loss: 0.745, Test accuracy: 78.01
Round  94, Train loss: 0.406, Test loss: 0.763, Test accuracy: 77.68
Round  95, Train loss: 0.429, Test loss: 0.738, Test accuracy: 78.03
Round  96, Train loss: 0.399, Test loss: 0.767, Test accuracy: 77.65
Round  97, Train loss: 0.406, Test loss: 0.753, Test accuracy: 77.89
Round  98, Train loss: 0.379, Test loss: 0.784, Test accuracy: 77.65
Round  99, Train loss: 0.380, Test loss: 0.778, Test accuracy: 77.97
Final Round, Train loss: 0.303, Test loss: 0.778, Test accuracy: 78.27
Average accuracy final 10 rounds: 77.89825
8016.982305526733
[13.106188774108887, 24.733839988708496, 36.408496141433716, 48.02352523803711, 59.502758741378784, 70.98087096214294, 82.77799129486084, 94.38206243515015, 106.09014415740967, 117.93661904335022, 129.9477663040161, 141.60218048095703, 153.24920868873596, 164.92739343643188, 176.6866009235382, 188.30433177947998, 199.7884669303894, 211.09025239944458, 222.30275917053223, 233.54016590118408, 244.88914036750793, 256.13854789733887, 267.37707233428955, 278.6566607952118, 289.8449196815491, 301.18989610671997, 312.49083638191223, 323.8603150844574, 335.13410353660583, 346.491263628006, 357.838978767395, 369.18360567092896, 380.5388855934143, 391.8212466239929, 403.1584315299988, 414.5049777030945, 425.82712864875793, 437.25499844551086, 448.5837013721466, 460.1679847240448, 472.4128932952881, 483.8233058452606, 495.3244004249573, 506.73106622695923, 518.2018649578094, 529.6210911273956, 541.045921087265, 552.5232911109924, 563.9481468200684, 575.1574347019196, 586.632081747055, 598.0358548164368, 609.5804815292358, 621.1051888465881, 632.5939197540283, 644.168726682663, 655.732275724411, 667.2633962631226, 678.8627886772156, 690.589058637619, 702.2564988136292, 713.8062403202057, 725.2724459171295, 736.8514728546143, 748.4689447879791, 760.1075978279114, 771.6696312427521, 783.2006902694702, 794.8268065452576, 806.4427669048309, 818.0222585201263, 829.4738523960114, 840.9998831748962, 852.5449278354645, 864.1488749980927, 875.9402210712433, 887.5251638889313, 899.096834897995, 910.6796176433563, 922.3616523742676, 933.9661660194397, 945.6104369163513, 957.311484336853, 968.8978621959686, 980.5925490856171, 992.2462084293365, 1003.773110628128, 1015.4131259918213, 1026.9803402423859, 1038.6349890232086, 1050.2760469913483, 1061.8735506534576, 1073.6078114509583, 1085.2896552085876, 1097.0302884578705, 1108.7757279872894, 1120.3470628261566, 1131.9225544929504, 1143.3806281089783, 1154.9739985466003, 1157.8955800533295]
[40.0175, 47.8725, 51.9575, 55.59, 57.8825, 59.8425, 62.5925, 64.28, 65.555, 66.165, 67.63, 68.5225, 69.905, 70.3225, 70.3125, 71.0775, 71.8, 72.705, 72.5425, 72.695, 73.5475, 73.5425, 73.0675, 73.9875, 73.92, 73.9675, 74.59, 74.505, 74.89, 74.7225, 75.4475, 75.925, 75.6025, 75.4175, 75.435, 75.775, 75.11, 75.05, 76.1075, 75.33, 76.0925, 75.81, 76.02, 76.595, 76.4525, 76.5, 76.5775, 76.4875, 77.2075, 76.9175, 76.915, 76.99, 77.12, 76.195, 76.8925, 76.6125, 77.04, 77.1225, 77.4325, 76.57, 77.08, 76.8625, 76.715, 76.8225, 77.1175, 77.2675, 77.6075, 77.3775, 77.1775, 77.615, 77.5475, 77.685, 77.7275, 77.685, 77.9575, 77.335, 77.4375, 77.8075, 77.5775, 77.875, 77.56, 77.36, 77.21, 78.0725, 77.225, 77.7675, 77.8, 77.845, 78.0125, 78.0475, 78.0675, 77.915, 78.1025, 78.01, 77.68, 78.035, 77.6525, 77.895, 77.65, 77.975, 78.27]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.307, Test loss: 2.307, Test accuracy: 9.80
Round   0, Global train loss: 2.307, Global test loss: 2.307, Global test accuracy: 9.83
Round   1, Train loss: 2.306, Test loss: 2.306, Test accuracy: 9.74
Round   1, Global train loss: 2.306, Global test loss: 2.306, Global test accuracy: 9.82
Round   2, Train loss: 2.306, Test loss: 2.306, Test accuracy: 9.70
Round   2, Global train loss: 2.306, Global test loss: 2.306, Global test accuracy: 9.73
Round   3, Train loss: 2.306, Test loss: 2.306, Test accuracy: 9.66
Round   3, Global train loss: 2.306, Global test loss: 2.306, Global test accuracy: 9.61
Round   4, Train loss: 2.306, Test loss: 2.305, Test accuracy: 9.60
Round   4, Global train loss: 2.306, Global test loss: 2.305, Global test accuracy: 9.52
Round   5, Train loss: 2.305, Test loss: 2.305, Test accuracy: 9.61
Round   5, Global train loss: 2.305, Global test loss: 2.305, Global test accuracy: 9.55
Round   6, Train loss: 2.305, Test loss: 2.305, Test accuracy: 9.76
Round   6, Global train loss: 2.305, Global test loss: 2.304, Global test accuracy: 9.60
Round   7, Train loss: 2.305, Test loss: 2.305, Test accuracy: 9.72
Round   7, Global train loss: 2.305, Global test loss: 2.304, Global test accuracy: 9.88
Round   8, Train loss: 2.305, Test loss: 2.304, Test accuracy: 9.62
Round   8, Global train loss: 2.305, Global test loss: 2.304, Global test accuracy: 9.87
Round   9, Train loss: 2.304, Test loss: 2.304, Test accuracy: 9.71
Round   9, Global train loss: 2.304, Global test loss: 2.304, Global test accuracy: 9.93
Round  10, Train loss: 2.304, Test loss: 2.304, Test accuracy: 9.84
Round  10, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.02
Round  11, Train loss: 2.304, Test loss: 2.303, Test accuracy: 9.92
Round  11, Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 9.99
Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.99
Round  12, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 10.19
Round  13, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.12
Round  13, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 10.31
Round  14, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.27
Round  14, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 10.40
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.34
Round  15, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.66
Round  16, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.46
Round  16, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 10.72
Round  17, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.61
Round  17, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 11.03
Round  18, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.69
Round  18, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.99
Round  19, Train loss: 2.301, Test loss: 2.300, Test accuracy: 10.66
Round  19, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 11.18
Round  20, Train loss: 2.301, Test loss: 2.300, Test accuracy: 10.81
Round  20, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 11.38
Round  21, Train loss: 2.301, Test loss: 2.300, Test accuracy: 10.96
Round  21, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 11.36
Round  22, Train loss: 2.300, Test loss: 2.299, Test accuracy: 11.11
Round  22, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 11.34
Round  23, Train loss: 2.300, Test loss: 2.299, Test accuracy: 11.33
Round  23, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 11.42
Round  24, Train loss: 2.299, Test loss: 2.299, Test accuracy: 11.44
Round  24, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 11.56
Round  25, Train loss: 2.299, Test loss: 2.298, Test accuracy: 11.56
Round  25, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 11.82
Round  26, Train loss: 2.299, Test loss: 2.298, Test accuracy: 11.68
Round  26, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 11.96
Round  27, Train loss: 2.299, Test loss: 2.298, Test accuracy: 11.70
Round  27, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 11.78
Round  28, Train loss: 2.298, Test loss: 2.297, Test accuracy: 11.64
Round  28, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 11.89
Round  29, Train loss: 2.298, Test loss: 2.297, Test accuracy: 11.59
Round  29, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 11.76
Round  30, Train loss: 2.298, Test loss: 2.297, Test accuracy: 11.63
Round  30, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 11.77
Round  31, Train loss: 2.297, Test loss: 2.296, Test accuracy: 11.74
Round  31, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 11.94
Round  32, Train loss: 2.297, Test loss: 2.296, Test accuracy: 11.82
Round  32, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 11.80
Round  33, Train loss: 2.297, Test loss: 2.296, Test accuracy: 11.86
Round  33, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 12.04
Round  34, Train loss: 2.297, Test loss: 2.295, Test accuracy: 11.83
Round  34, Global train loss: 2.297, Global test loss: 2.294, Global test accuracy: 12.12
Round  35, Train loss: 2.296, Test loss: 2.295, Test accuracy: 11.90
Round  35, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 12.26
Round  36, Train loss: 2.296, Test loss: 2.294, Test accuracy: 12.13
Round  36, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 12.60
Round  37, Train loss: 2.295, Test loss: 2.294, Test accuracy: 12.35
Round  37, Global train loss: 2.295, Global test loss: 2.292, Global test accuracy: 12.58
Round  38, Train loss: 2.295, Test loss: 2.293, Test accuracy: 12.32
Round  38, Global train loss: 2.295, Global test loss: 2.292, Global test accuracy: 12.39
Round  39, Train loss: 2.295, Test loss: 2.293, Test accuracy: 12.36
Round  39, Global train loss: 2.295, Global test loss: 2.292, Global test accuracy: 12.46
Round  40, Train loss: 2.294, Test loss: 2.292, Test accuracy: 12.55
Round  40, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 12.50
Round  41, Train loss: 2.293, Test loss: 2.292, Test accuracy: 12.50
Round  41, Global train loss: 2.293, Global test loss: 2.291, Global test accuracy: 12.52
Round  42, Train loss: 2.293, Test loss: 2.291, Test accuracy: 12.48
Round  42, Global train loss: 2.293, Global test loss: 2.291, Global test accuracy: 12.46
Round  43, Train loss: 2.293, Test loss: 2.291, Test accuracy: 12.51
Round  43, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 12.58
Round  44, Train loss: 2.292, Test loss: 2.291, Test accuracy: 12.46
Round  44, Global train loss: 2.292, Global test loss: 2.290, Global test accuracy: 12.71
Round  45, Train loss: 2.292, Test loss: 2.290, Test accuracy: 12.54
Round  45, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 13.03
Round  46, Train loss: 2.292, Test loss: 2.290, Test accuracy: 12.76
Round  46, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 13.28
Round  47, Train loss: 2.291, Test loss: 2.289, Test accuracy: 12.87
Round  47, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 13.11
Round  48, Train loss: 2.291, Test loss: 2.289, Test accuracy: 12.98
Round  48, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 13.48
Round  49, Train loss: 2.291, Test loss: 2.289, Test accuracy: 13.14
Round  49, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 13.55
Round  50, Train loss: 2.290, Test loss: 2.288, Test accuracy: 13.41
Round  50, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 13.71
Round  51, Train loss: 2.289, Test loss: 2.288, Test accuracy: 13.51
Round  51, Global train loss: 2.289, Global test loss: 2.286, Global test accuracy: 13.99
Round  52, Train loss: 2.289, Test loss: 2.287, Test accuracy: 13.61
Round  52, Global train loss: 2.289, Global test loss: 2.285, Global test accuracy: 14.22
Round  53, Train loss: 2.289, Test loss: 2.286, Test accuracy: 13.71
Round  53, Global train loss: 2.289, Global test loss: 2.284, Global test accuracy: 14.03
Round  54, Train loss: 2.287, Test loss: 2.285, Test accuracy: 13.92
Round  54, Global train loss: 2.287, Global test loss: 2.284, Global test accuracy: 14.50
Round  55, Train loss: 2.288, Test loss: 2.285, Test accuracy: 13.99
Round  55, Global train loss: 2.288, Global test loss: 2.283, Global test accuracy: 14.40
Round  56, Train loss: 2.286, Test loss: 2.284, Test accuracy: 14.09
Round  56, Global train loss: 2.286, Global test loss: 2.283, Global test accuracy: 14.67
Round  57, Train loss: 2.286, Test loss: 2.283, Test accuracy: 14.09
Round  57, Global train loss: 2.286, Global test loss: 2.282, Global test accuracy: 14.84
Round  58, Train loss: 2.286, Test loss: 2.283, Test accuracy: 14.16
Round  58, Global train loss: 2.286, Global test loss: 2.281, Global test accuracy: 14.87
Round  59, Train loss: 2.285, Test loss: 2.282, Test accuracy: 14.42
Round  59, Global train loss: 2.285, Global test loss: 2.280, Global test accuracy: 15.13
Round  60, Train loss: 2.284, Test loss: 2.281, Test accuracy: 14.61
Round  60, Global train loss: 2.284, Global test loss: 2.280, Global test accuracy: 15.11
Round  61, Train loss: 2.284, Test loss: 2.281, Test accuracy: 14.59
Round  61, Global train loss: 2.284, Global test loss: 2.279, Global test accuracy: 15.02
Round  62, Train loss: 2.283, Test loss: 2.280, Test accuracy: 14.65
Round  62, Global train loss: 2.283, Global test loss: 2.278, Global test accuracy: 15.37
Round  63, Train loss: 2.282, Test loss: 2.279, Test accuracy: 14.73
Round  63, Global train loss: 2.282, Global test loss: 2.277, Global test accuracy: 15.35
Round  64, Train loss: 2.281, Test loss: 2.278, Test accuracy: 14.95
Round  64, Global train loss: 2.281, Global test loss: 2.276, Global test accuracy: 15.44
Round  65, Train loss: 2.282, Test loss: 2.277, Test accuracy: 15.13
Round  65, Global train loss: 2.282, Global test loss: 2.275, Global test accuracy: 15.16
Round  66, Train loss: 2.280, Test loss: 2.277, Test accuracy: 15.12
Round  66, Global train loss: 2.280, Global test loss: 2.274, Global test accuracy: 15.32
Round  67, Train loss: 2.280, Test loss: 2.276, Test accuracy: 15.14
Round  67, Global train loss: 2.280, Global test loss: 2.273, Global test accuracy: 15.48
Round  68, Train loss: 2.279, Test loss: 2.275, Test accuracy: 15.15
Round  68, Global train loss: 2.279, Global test loss: 2.273, Global test accuracy: 15.38
Round  69, Train loss: 2.279, Test loss: 2.274, Test accuracy: 15.09
Round  69, Global train loss: 2.279, Global test loss: 2.271, Global test accuracy: 15.24
Round  70, Train loss: 2.278, Test loss: 2.273, Test accuracy: 15.24
Round  70, Global train loss: 2.278, Global test loss: 2.270, Global test accuracy: 15.60
Round  71, Train loss: 2.278, Test loss: 2.272, Test accuracy: 15.22
Round  71, Global train loss: 2.278, Global test loss: 2.269, Global test accuracy: 15.34
Round  72, Train loss: 2.277, Test loss: 2.271, Test accuracy: 15.20
Round  72, Global train loss: 2.277, Global test loss: 2.268, Global test accuracy: 15.51
Round  73, Train loss: 2.276, Test loss: 2.270, Test accuracy: 15.42
Round  73, Global train loss: 2.276, Global test loss: 2.267, Global test accuracy: 15.91
Round  74, Train loss: 2.275, Test loss: 2.269, Test accuracy: 15.36
Round  74, Global train loss: 2.275, Global test loss: 2.266, Global test accuracy: 15.84
Round  75, Train loss: 2.276, Test loss: 2.267, Test accuracy: 15.44
Round  75, Global train loss: 2.276, Global test loss: 2.265, Global test accuracy: 16.02
Round  76, Train loss: 2.274, Test loss: 2.266, Test accuracy: 15.55
Round  76, Global train loss: 2.274, Global test loss: 2.264, Global test accuracy: 15.69
Round  77, Train loss: 2.272, Test loss: 2.265, Test accuracy: 15.43
Round  77, Global train loss: 2.272, Global test loss: 2.262, Global test accuracy: 15.05
Round  78, Train loss: 2.273, Test loss: 2.264, Test accuracy: 15.54
Round  78, Global train loss: 2.273, Global test loss: 2.261, Global test accuracy: 15.60
Round  79, Train loss: 2.272, Test loss: 2.263, Test accuracy: 15.72
Round  79, Global train loss: 2.272, Global test loss: 2.260, Global test accuracy: 15.95
Round  80, Train loss: 2.272, Test loss: 2.262, Test accuracy: 15.95
Round  80, Global train loss: 2.272, Global test loss: 2.258, Global test accuracy: 15.94
Round  81, Train loss: 2.269, Test loss: 2.260, Test accuracy: 15.92
Round  81, Global train loss: 2.269, Global test loss: 2.256, Global test accuracy: 15.71
Round  82, Train loss: 2.269, Test loss: 2.259, Test accuracy: 15.64
Round  82, Global train loss: 2.269, Global test loss: 2.255, Global test accuracy: 14.95
Round  83, Train loss: 2.268, Test loss: 2.258, Test accuracy: 15.81
Round  83, Global train loss: 2.268, Global test loss: 2.254, Global test accuracy: 15.25
Round  84, Train loss: 2.266, Test loss: 2.257, Test accuracy: 15.91
Round  84, Global train loss: 2.266, Global test loss: 2.254, Global test accuracy: 16.16
Round  85, Train loss: 2.267, Test loss: 2.256, Test accuracy: 16.27
Round  85, Global train loss: 2.267, Global test loss: 2.253, Global test accuracy: 16.36
Round  86, Train loss: 2.266, Test loss: 2.255, Test accuracy: 16.23
Round  86, Global train loss: 2.266, Global test loss: 2.253, Global test accuracy: 16.00
Round  87, Train loss: 2.266, Test loss: 2.254, Test accuracy: 16.34
Round  87, Global train loss: 2.266, Global test loss: 2.251, Global test accuracy: 16.45
Round  88, Train loss: 2.264, Test loss: 2.253, Test accuracy: 16.25
Round  88, Global train loss: 2.264, Global test loss: 2.250, Global test accuracy: 16.40
Round  89, Train loss: 2.263, Test loss: 2.252, Test accuracy: 16.37
Round  89, Global train loss: 2.263, Global test loss: 2.249, Global test accuracy: 16.77
Round  90, Train loss: 2.263, Test loss: 2.250, Test accuracy: 16.64
Round  90, Global train loss: 2.263, Global test loss: 2.248, Global test accuracy: 16.82
Round  91, Train loss: 2.261, Test loss: 2.250, Test accuracy: 16.74
Round  91, Global train loss: 2.261, Global test loss: 2.247, Global test accuracy: 17.18
Round  92, Train loss: 2.260, Test loss: 2.248, Test accuracy: 17.10
Round  92, Global train loss: 2.260, Global test loss: 2.246, Global test accuracy: 17.96
Round  93, Train loss: 2.261, Test loss: 2.247, Test accuracy: 17.20
Round  93, Global train loss: 2.261, Global test loss: 2.244, Global test accuracy: 18.21
Round  94, Train loss: 2.258, Test loss: 2.246, Test accuracy: 17.31
Round  94, Global train loss: 2.258, Global test loss: 2.242, Global test accuracy: 18.41
Round  95, Train loss: 2.258, Test loss: 2.244, Test accuracy: 17.61
Round  95, Global train loss: 2.258, Global test loss: 2.241, Global test accuracy: 17.84
Round  96, Train loss: 2.257, Test loss: 2.244, Test accuracy: 17.55
Round  96, Global train loss: 2.257, Global test loss: 2.240, Global test accuracy: 17.84
Round  97, Train loss: 2.255, Test loss: 2.242, Test accuracy: 17.71
Round  97, Global train loss: 2.255, Global test loss: 2.238, Global test accuracy: 18.13
Round  98, Train loss: 2.257, Test loss: 2.240, Test accuracy: 17.92
Round  98, Global train loss: 2.257, Global test loss: 2.236, Global test accuracy: 17.93
Round  99, Train loss: 2.253, Test loss: 2.239, Test accuracy: 18.09
Round  99, Global train loss: 2.253, Global test loss: 2.235, Global test accuracy: 18.27
Round 100, Train loss: 2.253, Test loss: 2.237, Test accuracy: 18.07
Round 100, Global train loss: 2.253, Global test loss: 2.234, Global test accuracy: 18.05
Round 101, Train loss: 2.252, Test loss: 2.236, Test accuracy: 18.24
Round 101, Global train loss: 2.252, Global test loss: 2.232, Global test accuracy: 18.71
Round 102, Train loss: 2.251, Test loss: 2.235, Test accuracy: 18.34
Round 102, Global train loss: 2.251, Global test loss: 2.232, Global test accuracy: 18.55
Round 103, Train loss: 2.251, Test loss: 2.233, Test accuracy: 18.80
Round 103, Global train loss: 2.251, Global test loss: 2.229, Global test accuracy: 19.23
Round 104, Train loss: 2.252, Test loss: 2.232, Test accuracy: 18.75
Round 104, Global train loss: 2.252, Global test loss: 2.228, Global test accuracy: 18.76
Round 105, Train loss: 2.248, Test loss: 2.231, Test accuracy: 18.81
Round 105, Global train loss: 2.248, Global test loss: 2.227, Global test accuracy: 19.11
Round 106, Train loss: 2.245, Test loss: 2.229, Test accuracy: 18.66
Round 106, Global train loss: 2.245, Global test loss: 2.226, Global test accuracy: 19.10
Round 107, Train loss: 2.249, Test loss: 2.228, Test accuracy: 18.84
Round 107, Global train loss: 2.249, Global test loss: 2.225, Global test accuracy: 18.67
Round 108, Train loss: nan, Test loss: nan, Test accuracy: 18.66
Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 109, Train loss: nan, Test loss: nan, Test accuracy: 15.96
Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 110, Train loss: nan, Test loss: nan, Test accuracy: 14.08
Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 111, Train loss: nan, Test loss: nan, Test accuracy: 13.21
Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 112, Train loss: nan, Test loss: nan, Test accuracy: 11.77
Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 113, Train loss: nan, Test loss: nan, Test accuracy: 11.77
Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 114, Train loss: nan, Test loss: nan, Test accuracy: 11.36
Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 115, Train loss: nan, Test loss: nan, Test accuracy: 10.84
Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 116, Train loss: nan, Test loss: nan, Test accuracy: 10.43
Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 117, Train loss: nan, Test loss: nan, Test accuracy: 10.43
Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 118, Train loss: nan, Test loss: nan, Test accuracy: 10.43
Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 119, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 120, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 121, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 122, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 123, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 124, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 125, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 126, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 127, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 128, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 129, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 130, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 131, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 132, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 133, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 134, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

15297.954424619675
[4.886464834213257, 9.513616561889648, 14.212945938110352, 18.839595317840576, 23.455089569091797, 28.05285120010376, 32.67190718650818, 37.30144667625427, 41.93981456756592, 46.59971833229065, 51.20448637008667, 55.828012466430664, 60.436954975128174, 65.08599805831909, 69.71006083488464, 74.32694005966187, 78.92377734184265, 83.5642261505127, 88.1946849822998, 92.80177640914917, 97.40670013427734, 102.01048254966736, 106.59718704223633, 111.18655633926392, 115.79123258590698, 120.38315987586975, 125.0118339061737, 129.6417055130005, 134.26072096824646, 138.8613736629486, 143.57020354270935, 148.31925415992737, 152.96654152870178, 157.59940433502197, 162.2290277481079, 166.81082773208618, 171.4204716682434, 176.03887724876404, 180.68604111671448, 185.31405401229858, 189.90590500831604, 194.52002263069153, 199.16274976730347, 203.79616737365723, 208.40443992614746, 213.19101071357727, 217.8811981678009, 222.5045349597931, 227.11751556396484, 231.6968550682068, 236.2895052433014, 240.89597988128662, 245.5301959514618, 250.1593518257141, 254.81344485282898, 259.67604875564575, 264.3091764450073, 269.19518208503723, 273.8061435222626, 278.40165090560913, 283.00762462615967, 287.6225965023041, 292.21778297424316, 296.8297219276428, 301.4311716556549, 306.0335774421692, 310.6766369342804, 315.29220819473267, 319.8683407306671, 324.4376792907715, 329.0095512866974, 333.6039311885834, 338.1961758136749, 342.7969400882721, 347.4125819206238, 352.0215177536011, 356.6468689441681, 361.2819561958313, 365.8601813316345, 370.4819083213806, 375.08627343177795, 379.7051215171814, 384.3353199958801, 388.936975479126, 393.53224182128906, 398.12396812438965, 402.77399706840515, 407.40527868270874, 412.0058274269104, 416.62052822113037, 421.2219820022583, 425.80604219436646, 430.36162185668945, 434.90293741226196, 439.45330333709717, 444.01780128479004, 448.5921223163605, 453.1636128425598, 457.7577795982361, 462.3480625152588, 466.9113140106201, 471.5036072731018, 476.0564193725586, 480.61235070228577, 485.17100286483765, 489.7491548061371, 494.31131887435913, 498.88235569000244, 503.4658739566803, 508.0555942058563, 512.6142776012421, 517.172694683075, 521.718709230423, 526.308207988739, 530.8966705799103, 535.5033087730408, 540.0967304706573, 544.6817207336426, 549.242467880249, 553.8788154125214, 558.4537506103516, 563.0287382602692, 567.6133923530579, 572.1914989948273, 576.7769107818604, 581.365889787674, 586.0195269584656, 590.63574385643, 595.239394903183, 599.8040490150452, 604.4246308803558, 609.0559618473053, 613.6577205657959, 618.2556684017181, 622.8802466392517, 627.4587604999542, 632.0310747623444, 636.6102020740509, 641.2007327079773, 645.8048930168152, 650.37562251091, 654.9345810413361, 659.5218424797058, 664.1087427139282, 668.6567423343658, 673.239179611206, 677.8420202732086, 682.4276354312897, 687.0069689750671, 691.5961880683899, 696.1977095603943, 700.7995359897614, 705.4107592105865, 710.0081055164337, 714.6299755573273, 719.1818099021912, 723.7810707092285, 728.3741023540497, 732.9659304618835, 737.5447998046875, 742.1433291435242, 746.7431199550629, 751.3604176044464, 755.9609677791595, 760.5723648071289, 765.1688265800476, 769.7644069194794, 774.3685698509216, 778.9804203510284, 783.5576660633087, 788.1320126056671, 792.7292087078094, 797.3248307704926, 801.9391157627106, 806.5299730300903, 811.1153314113617, 815.7424230575562, 820.3536581993103, 825.0124661922455, 829.6572389602661, 834.315042257309, 838.9649894237518, 843.6191771030426, 848.2966561317444, 852.9553678035736, 857.6085159778595, 862.2503435611725, 866.889571428299, 871.535148859024, 876.182737827301, 880.8284387588501, 885.4806025028229, 890.1135201454163, 894.7479116916656, 899.3892376422882, 904.0347511768341, 908.683296918869, 913.3131983280182, 917.9661891460419, 922.6157324314117, 927.2405915260315, 931.8698072433472, 936.5032477378845, 941.1319906711578, 945.7652463912964, 950.3782289028168, 955.0207841396332, 959.667486667633, 964.298752784729, 968.9626243114471, 973.6213607788086, 978.2857446670532, 982.9321479797363, 987.5650062561035, 992.1957609653473, 996.8676249980927, 1001.5446331501007, 1006.2294337749481, 1010.9035792350769, 1015.5888600349426, 1020.2811236381531, 1024.9634456634521, 1029.6591591835022, 1034.3535416126251, 1038.9962584972382, 1043.6595106124878, 1048.316790819168, 1052.985030412674, 1057.6631634235382, 1062.3289103507996, 1066.9772715568542, 1071.674390077591, 1076.3346750736237, 1080.9796237945557, 1085.6846714019775, 1090.3228480815887, 1094.9598438739777, 1099.58282828331, 1104.2130053043365, 1108.8218355178833, 1113.4372811317444, 1118.1030695438385, 1122.744021654129, 1127.3943412303925, 1132.019249200821, 1136.6382322311401, 1141.264916419983, 1145.9112684726715, 1150.5491135120392, 1155.186939239502, 1159.820125579834, 1164.4432723522186, 1169.0744144916534, 1173.7011981010437, 1178.3595435619354, 1183.0015165805817, 1187.6673040390015, 1193.3948292732239, 1199.11070728302, 1204.813026189804, 1210.5528042316437, 1216.2508382797241, 1221.9746561050415, 1227.6633484363556, 1233.1333684921265, 1238.713722229004, 1243.5192523002625, 1248.318032026291, 1253.1677124500275, 1258.0366604328156, 1262.9100425243378, 1267.7462885379791, 1272.6213529109955, 1277.4261655807495, 1282.277342557907, 1287.159262418747, 1291.989014863968, 1296.807655096054, 1301.6353733539581, 1306.472063779831, 1311.303029537201, 1316.147697687149, 1321.0492160320282, 1325.8635551929474, 1330.71168923378, 1335.6017796993256, 1340.4250373840332, 1345.2923307418823, 1350.106345653534, 1354.9471726417542, 1359.7989501953125, 1364.6953809261322, 1369.5263757705688, 1374.698950767517, 1379.5384554862976, 1384.3953034877777, 1389.3794407844543, 1394.2319288253784, 1399.0735993385315, 1403.9323675632477, 1406.3700332641602]
[9.7975, 9.74, 9.695, 9.6575, 9.6, 9.6075, 9.7625, 9.725, 9.62, 9.71, 9.835, 9.9225, 9.9925, 10.125, 10.2675, 10.34, 10.4625, 10.6075, 10.6925, 10.6575, 10.815, 10.9625, 11.11, 11.3275, 11.435, 11.565, 11.6825, 11.7025, 11.6425, 11.59, 11.63, 11.745, 11.8175, 11.855, 11.83, 11.9, 12.1275, 12.3475, 12.3225, 12.3625, 12.5475, 12.5025, 12.4825, 12.51, 12.4625, 12.5425, 12.7575, 12.87, 12.9825, 13.14, 13.415, 13.51, 13.605, 13.705, 13.9175, 13.9875, 14.09, 14.085, 14.165, 14.42, 14.6075, 14.5925, 14.6475, 14.73, 14.95, 15.1275, 15.1225, 15.145, 15.1475, 15.0925, 15.24, 15.2225, 15.195, 15.4175, 15.355, 15.4375, 15.555, 15.43, 15.5375, 15.7225, 15.945, 15.92, 15.64, 15.815, 15.9075, 16.265, 16.2325, 16.34, 16.2525, 16.3725, 16.635, 16.7425, 17.1025, 17.1975, 17.3125, 17.605, 17.5475, 17.7075, 17.9225, 18.09, 18.065, 18.2425, 18.3425, 18.7975, 18.75, 18.81, 18.655, 18.8375, 18.6575, 15.955, 14.0825, 13.2125, 11.77, 11.77, 11.355, 10.835, 10.43, 10.43, 10.43, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.876, Test loss: 2.111, Test accuracy: 31.57
Round   0, Global train loss: 0.876, Global test loss: 2.429, Global test accuracy: 24.54
Round   1, Train loss: 0.685, Test loss: 1.284, Test accuracy: 53.67
Round   1, Global train loss: 0.685, Global test loss: 2.062, Global test accuracy: 29.29
Round   2, Train loss: 0.676, Test loss: 1.116, Test accuracy: 59.38
Round   2, Global train loss: 0.676, Global test loss: 1.904, Global test accuracy: 37.20
Round   3, Train loss: 0.643, Test loss: 0.816, Test accuracy: 69.34
Round   3, Global train loss: 0.643, Global test loss: 1.710, Global test accuracy: 41.67
Round   4, Train loss: 0.599, Test loss: 0.851, Test accuracy: 67.55
Round   4, Global train loss: 0.599, Global test loss: 1.933, Global test accuracy: 34.36
Round   5, Train loss: 0.524, Test loss: 0.745, Test accuracy: 70.80
Round   5, Global train loss: 0.524, Global test loss: 1.581, Global test accuracy: 44.29
Round   6, Train loss: 0.538, Test loss: 0.684, Test accuracy: 73.27
Round   6, Global train loss: 0.538, Global test loss: 1.659, Global test accuracy: 43.35
Round   7, Train loss: 0.442, Test loss: 0.541, Test accuracy: 78.06
Round   7, Global train loss: 0.442, Global test loss: 1.431, Global test accuracy: 49.17
Round   8, Train loss: 0.558, Test loss: 0.513, Test accuracy: 79.38
Round   8, Global train loss: 0.558, Global test loss: 1.522, Global test accuracy: 45.39
Round   9, Train loss: 0.474, Test loss: 0.451, Test accuracy: 81.88
Round   9, Global train loss: 0.474, Global test loss: 1.479, Global test accuracy: 49.15
Round  10, Train loss: 0.452, Test loss: 0.430, Test accuracy: 82.87
Round  10, Global train loss: 0.452, Global test loss: 1.486, Global test accuracy: 47.50
Round  11, Train loss: 0.403, Test loss: 0.422, Test accuracy: 83.20
Round  11, Global train loss: 0.403, Global test loss: 1.419, Global test accuracy: 49.83
Round  12, Train loss: 0.409, Test loss: 0.424, Test accuracy: 83.08
Round  12, Global train loss: 0.409, Global test loss: 1.873, Global test accuracy: 45.61
Round  13, Train loss: 0.436, Test loss: 0.415, Test accuracy: 83.59
Round  13, Global train loss: 0.436, Global test loss: 1.553, Global test accuracy: 48.52
Round  14, Train loss: 0.408, Test loss: 0.403, Test accuracy: 84.15
Round  14, Global train loss: 0.408, Global test loss: 1.362, Global test accuracy: 52.81
Round  15, Train loss: 0.386, Test loss: 0.401, Test accuracy: 84.43
Round  15, Global train loss: 0.386, Global test loss: 1.456, Global test accuracy: 54.35
Round  16, Train loss: 0.385, Test loss: 0.392, Test accuracy: 84.81
Round  16, Global train loss: 0.385, Global test loss: 1.334, Global test accuracy: 54.36
Round  17, Train loss: 0.284, Test loss: 0.392, Test accuracy: 84.86
Round  17, Global train loss: 0.284, Global test loss: 1.647, Global test accuracy: 53.71
Round  18, Train loss: 0.425, Test loss: 0.394, Test accuracy: 84.68
Round  18, Global train loss: 0.425, Global test loss: 1.147, Global test accuracy: 61.12
Round  19, Train loss: 0.372, Test loss: 0.385, Test accuracy: 85.16
Round  19, Global train loss: 0.372, Global test loss: 1.316, Global test accuracy: 56.60
Round  20, Train loss: 0.398, Test loss: 0.392, Test accuracy: 85.06
Round  20, Global train loss: 0.398, Global test loss: 1.128, Global test accuracy: 60.06
Round  21, Train loss: 0.399, Test loss: 0.381, Test accuracy: 85.53
Round  21, Global train loss: 0.399, Global test loss: 1.456, Global test accuracy: 49.85
Round  22, Train loss: 0.358, Test loss: 0.376, Test accuracy: 85.69
Round  22, Global train loss: 0.358, Global test loss: 1.413, Global test accuracy: 50.86
Round  23, Train loss: 0.364, Test loss: 0.377, Test accuracy: 85.78
Round  23, Global train loss: 0.364, Global test loss: 1.271, Global test accuracy: 56.54
Round  24, Train loss: 0.319, Test loss: 0.374, Test accuracy: 85.95
Round  24, Global train loss: 0.319, Global test loss: 1.456, Global test accuracy: 53.80
Round  25, Train loss: 0.270, Test loss: 0.388, Test accuracy: 85.59
Round  25, Global train loss: 0.270, Global test loss: 1.477, Global test accuracy: 55.19
Round  26, Train loss: 0.242, Test loss: 0.364, Test accuracy: 86.27
Round  26, Global train loss: 0.242, Global test loss: 1.547, Global test accuracy: 55.48
Round  27, Train loss: 0.288, Test loss: 0.354, Test accuracy: 86.65
Round  27, Global train loss: 0.288, Global test loss: 1.114, Global test accuracy: 61.53
Round  28, Train loss: 0.286, Test loss: 0.357, Test accuracy: 86.59
Round  28, Global train loss: 0.286, Global test loss: 1.242, Global test accuracy: 59.48
Round  29, Train loss: 0.260, Test loss: 0.361, Test accuracy: 86.54
Round  29, Global train loss: 0.260, Global test loss: 1.409, Global test accuracy: 57.13
Round  30, Train loss: 0.284, Test loss: 0.360, Test accuracy: 86.42
Round  30, Global train loss: 0.284, Global test loss: 1.061, Global test accuracy: 62.99
Round  31, Train loss: 0.245, Test loss: 0.346, Test accuracy: 87.05
Round  31, Global train loss: 0.245, Global test loss: 1.124, Global test accuracy: 63.78
Round  32, Train loss: 0.264, Test loss: 0.349, Test accuracy: 86.98
Round  32, Global train loss: 0.264, Global test loss: 1.105, Global test accuracy: 63.06
Round  33, Train loss: 0.284, Test loss: 0.357, Test accuracy: 86.77
Round  33, Global train loss: 0.284, Global test loss: 1.104, Global test accuracy: 62.86
Round  34, Train loss: 0.198, Test loss: 0.353, Test accuracy: 87.00
Round  34, Global train loss: 0.198, Global test loss: 1.646, Global test accuracy: 52.68
Round  35, Train loss: 0.296, Test loss: 0.366, Test accuracy: 86.50
Round  35, Global train loss: 0.296, Global test loss: 1.424, Global test accuracy: 59.41
Round  36, Train loss: 0.244, Test loss: 0.360, Test accuracy: 86.72
Round  36, Global train loss: 0.244, Global test loss: 1.216, Global test accuracy: 61.15
Round  37, Train loss: 0.262, Test loss: 0.357, Test accuracy: 86.92
Round  37, Global train loss: 0.262, Global test loss: 1.174, Global test accuracy: 61.60
Round  38, Train loss: 0.296, Test loss: 0.359, Test accuracy: 87.02
Round  38, Global train loss: 0.296, Global test loss: 1.145, Global test accuracy: 62.06
Round  39, Train loss: 0.320, Test loss: 0.349, Test accuracy: 87.49
Round  39, Global train loss: 0.320, Global test loss: 1.298, Global test accuracy: 55.93
Round  40, Train loss: 0.308, Test loss: 0.350, Test accuracy: 87.60
Round  40, Global train loss: 0.308, Global test loss: 1.096, Global test accuracy: 61.64
Round  41, Train loss: 0.197, Test loss: 0.360, Test accuracy: 87.46
Round  41, Global train loss: 0.197, Global test loss: 1.282, Global test accuracy: 60.71
Round  42, Train loss: 0.272, Test loss: 0.363, Test accuracy: 87.26
Round  42, Global train loss: 0.272, Global test loss: 1.184, Global test accuracy: 59.20
Round  43, Train loss: 0.269, Test loss: 0.375, Test accuracy: 86.95
Round  43, Global train loss: 0.269, Global test loss: 1.276, Global test accuracy: 59.76
Round  44, Train loss: 0.240, Test loss: 0.366, Test accuracy: 87.19
Round  44, Global train loss: 0.240, Global test loss: 0.975, Global test accuracy: 67.22
Round  45, Train loss: 0.232, Test loss: 0.372, Test accuracy: 87.15
Round  45, Global train loss: 0.232, Global test loss: 0.941, Global test accuracy: 67.19
Round  46, Train loss: 0.209, Test loss: 0.368, Test accuracy: 87.28
Round  46, Global train loss: 0.209, Global test loss: 1.024, Global test accuracy: 65.26
Round  47, Train loss: 0.276, Test loss: 0.379, Test accuracy: 86.94
Round  47, Global train loss: 0.276, Global test loss: 1.005, Global test accuracy: 64.57
Round  48, Train loss: 0.274, Test loss: 0.371, Test accuracy: 87.33
Round  48, Global train loss: 0.274, Global test loss: 1.418, Global test accuracy: 58.71
Round  49, Train loss: 0.230, Test loss: 0.346, Test accuracy: 87.81
Round  49, Global train loss: 0.230, Global test loss: 1.190, Global test accuracy: 62.71
Round  50, Train loss: 0.243, Test loss: 0.348, Test accuracy: 87.54
Round  50, Global train loss: 0.243, Global test loss: 1.225, Global test accuracy: 60.81
Round  51, Train loss: 0.236, Test loss: 0.345, Test accuracy: 87.90
Round  51, Global train loss: 0.236, Global test loss: 1.113, Global test accuracy: 63.71
Round  52, Train loss: 0.203, Test loss: 0.348, Test accuracy: 87.83
Round  52, Global train loss: 0.203, Global test loss: 1.103, Global test accuracy: 63.68
Round  53, Train loss: 0.270, Test loss: 0.344, Test accuracy: 87.98
Round  53, Global train loss: 0.270, Global test loss: 1.122, Global test accuracy: 62.28
Round  54, Train loss: 0.188, Test loss: 0.351, Test accuracy: 87.85
Round  54, Global train loss: 0.188, Global test loss: 1.346, Global test accuracy: 60.12
Round  55, Train loss: 0.231, Test loss: 0.349, Test accuracy: 87.88
Round  55, Global train loss: 0.231, Global test loss: 1.346, Global test accuracy: 58.07
Round  56, Train loss: 0.236, Test loss: 0.346, Test accuracy: 87.97
Round  56, Global train loss: 0.236, Global test loss: 1.261, Global test accuracy: 62.85
Round  57, Train loss: 0.202, Test loss: 0.357, Test accuracy: 87.82
Round  57, Global train loss: 0.202, Global test loss: 1.368, Global test accuracy: 59.52
Round  58, Train loss: 0.183, Test loss: 0.351, Test accuracy: 88.09
Round  58, Global train loss: 0.183, Global test loss: 1.431, Global test accuracy: 58.85
Round  59, Train loss: 0.209, Test loss: 0.351, Test accuracy: 88.02
Round  59, Global train loss: 0.209, Global test loss: 0.976, Global test accuracy: 66.67
Round  60, Train loss: 0.182, Test loss: 0.352, Test accuracy: 88.15
Round  60, Global train loss: 0.182, Global test loss: 1.160, Global test accuracy: 64.34
Round  61, Train loss: 0.214, Test loss: 0.347, Test accuracy: 88.14
Round  61, Global train loss: 0.214, Global test loss: 1.294, Global test accuracy: 60.87
Round  62, Train loss: 0.220, Test loss: 0.348, Test accuracy: 88.11
Round  62, Global train loss: 0.220, Global test loss: 0.950, Global test accuracy: 67.86
Round  63, Train loss: 0.236, Test loss: 0.345, Test accuracy: 88.24
Round  63, Global train loss: 0.236, Global test loss: 1.042, Global test accuracy: 65.38
Round  64, Train loss: 0.260, Test loss: 0.352, Test accuracy: 88.23
Round  64, Global train loss: 0.260, Global test loss: 1.017, Global test accuracy: 65.86
Round  65, Train loss: 0.178, Test loss: 0.349, Test accuracy: 88.29
Round  65, Global train loss: 0.178, Global test loss: 0.931, Global test accuracy: 68.84
Round  66, Train loss: 0.149, Test loss: 0.344, Test accuracy: 88.50
Round  66, Global train loss: 0.149, Global test loss: 1.077, Global test accuracy: 66.34
Round  67, Train loss: 0.209, Test loss: 0.352, Test accuracy: 88.29
Round  67, Global train loss: 0.209, Global test loss: 1.320, Global test accuracy: 60.83
Round  68, Train loss: 0.248, Test loss: 0.347, Test accuracy: 88.46
Round  68, Global train loss: 0.248, Global test loss: 1.198, Global test accuracy: 61.86
Round  69, Train loss: 0.188, Test loss: 0.352, Test accuracy: 88.47
Round  69, Global train loss: 0.188, Global test loss: 1.327, Global test accuracy: 61.14
Round  70, Train loss: 0.236, Test loss: 0.360, Test accuracy: 88.42
Round  70, Global train loss: 0.236, Global test loss: 1.085, Global test accuracy: 63.99
Round  71, Train loss: 0.172, Test loss: 0.359, Test accuracy: 88.39
Round  71, Global train loss: 0.172, Global test loss: 1.027, Global test accuracy: 66.69
Round  72, Train loss: 0.146, Test loss: 0.357, Test accuracy: 88.45
Round  72, Global train loss: 0.146, Global test loss: 1.243, Global test accuracy: 64.27
Round  73, Train loss: 0.216, Test loss: 0.359, Test accuracy: 88.44
Round  73, Global train loss: 0.216, Global test loss: 1.190, Global test accuracy: 63.32
Round  74, Train loss: 0.238, Test loss: 0.357, Test accuracy: 88.28
Round  74, Global train loss: 0.238, Global test loss: 1.466, Global test accuracy: 58.08
Round  75, Train loss: 0.194, Test loss: 0.361, Test accuracy: 88.11
Round  75, Global train loss: 0.194, Global test loss: 1.299, Global test accuracy: 61.24
Round  76, Train loss: 0.177, Test loss: 0.356, Test accuracy: 88.26
Round  76, Global train loss: 0.177, Global test loss: 1.192, Global test accuracy: 62.87
Round  77, Train loss: 0.178, Test loss: 0.360, Test accuracy: 88.17
Round  77, Global train loss: 0.178, Global test loss: 0.936, Global test accuracy: 68.75
Round  78, Train loss: 0.167, Test loss: 0.369, Test accuracy: 87.88
Round  78, Global train loss: 0.167, Global test loss: 1.023, Global test accuracy: 67.22
Round  79, Train loss: 0.160, Test loss: 0.362, Test accuracy: 88.19
Round  79, Global train loss: 0.160, Global test loss: 1.150, Global test accuracy: 64.27
Round  80, Train loss: 0.145, Test loss: 0.357, Test accuracy: 88.38
Round  80, Global train loss: 0.145, Global test loss: 1.145, Global test accuracy: 65.58
Round  81, Train loss: 0.126, Test loss: 0.359, Test accuracy: 88.36
Round  81, Global train loss: 0.126, Global test loss: 1.325, Global test accuracy: 64.38
Round  82, Train loss: 0.154, Test loss: 0.353, Test accuracy: 88.68
Round  82, Global train loss: 0.154, Global test loss: 1.186, Global test accuracy: 64.24
Round  83, Train loss: 0.200, Test loss: 0.361, Test accuracy: 88.51
Round  83, Global train loss: 0.200, Global test loss: 1.424, Global test accuracy: 60.09
Round  84, Train loss: 0.148, Test loss: 0.367, Test accuracy: 88.46
Round  84, Global train loss: 0.148, Global test loss: 1.310, Global test accuracy: 64.26
Round  85, Train loss: 0.184, Test loss: 0.373, Test accuracy: 88.32
Round  85, Global train loss: 0.184, Global test loss: 1.627, Global test accuracy: 60.12
Round  86, Train loss: 0.154, Test loss: 0.375, Test accuracy: 88.29
Round  86, Global train loss: 0.154, Global test loss: 1.272, Global test accuracy: 63.37
Round  87, Train loss: 0.190, Test loss: 0.369, Test accuracy: 88.46
Round  87, Global train loss: 0.190, Global test loss: 1.346, Global test accuracy: 61.79
Round  88, Train loss: 0.152, Test loss: 0.376, Test accuracy: 88.23
Round  88, Global train loss: 0.152, Global test loss: 1.020, Global test accuracy: 68.29
Round  89, Train loss: 0.180, Test loss: 0.371, Test accuracy: 88.45
Round  89, Global train loss: 0.180, Global test loss: 1.182, Global test accuracy: 64.21
Round  90, Train loss: 0.160, Test loss: 0.359, Test accuracy: 88.76
Round  90, Global train loss: 0.160, Global test loss: 0.945, Global test accuracy: 68.86
Round  91, Train loss: 0.175, Test loss: 0.363, Test accuracy: 88.67
Round  91, Global train loss: 0.175, Global test loss: 1.034, Global test accuracy: 67.35
Round  92, Train loss: 0.142, Test loss: 0.360, Test accuracy: 88.87
Round  92, Global train loss: 0.142, Global test loss: 1.232, Global test accuracy: 64.02
Round  93, Train loss: 0.147, Test loss: 0.351, Test accuracy: 88.99
Round  93, Global train loss: 0.147, Global test loss: 1.052, Global test accuracy: 67.78
Round  94, Train loss: 0.196, Test loss: 0.353, Test accuracy: 88.83
Round  94, Global train loss: 0.196, Global test loss: 0.924, Global test accuracy: 69.37
Round  95, Train loss: 0.162, Test loss: 0.353, Test accuracy: 88.87
Round  95, Global train loss: 0.162, Global test loss: 1.625, Global test accuracy: 60.73
Round  96, Train loss: 0.215, Test loss: 0.355, Test accuracy: 88.86
Round  96, Global train loss: 0.215, Global test loss: 1.126, Global test accuracy: 66.83
Round  97, Train loss: 0.195, Test loss: 0.356, Test accuracy: 88.91
Round  97, Global train loss: 0.195, Global test loss: 0.912, Global test accuracy: 70.03
Round  98, Train loss: 0.153, Test loss: 0.358, Test accuracy: 88.91
Round  98, Global train loss: 0.153, Global test loss: 1.356, Global test accuracy: 62.06
Round  99, Train loss: 0.127, Test loss: 0.359, Test accuracy: 88.86
Round  99, Global train loss: 0.127, Global test loss: 1.163, Global test accuracy: 65.87
Final Round, Train loss: 0.110, Test loss: 0.404, Test accuracy: 88.63
Final Round, Global train loss: 0.110, Global test loss: 1.163, Global test accuracy: 65.87
Average accuracy final 10 rounds: 88.85194444444444 

Average global accuracy final 10 rounds: 66.29027777777777 

5411.5549483299255
[4.644559621810913, 9.289119243621826, 13.65707540512085, 18.025031566619873, 22.38120746612549, 26.737383365631104, 31.10378885269165, 35.4701943397522, 39.84956765174866, 44.22894096374512, 48.59604048728943, 52.96314001083374, 57.334577322006226, 61.70601463317871, 66.10397529602051, 70.5019359588623, 74.90537714958191, 79.30881834030151, 83.71923589706421, 88.1296534538269, 92.51360583305359, 96.89755821228027, 101.25244665145874, 105.6073350906372, 109.82768225669861, 114.04802942276001, 117.8596842288971, 121.67133903503418, 125.49598550796509, 129.320631980896, 133.11882972717285, 136.9170274734497, 140.7107322216034, 144.50443696975708, 148.3080174922943, 152.11159801483154, 155.92906141281128, 159.74652481079102, 163.55166578292847, 167.35680675506592, 171.16306591033936, 174.9693250656128, 178.7535331249237, 182.53774118423462, 186.32863450050354, 190.11952781677246, 193.9140133857727, 197.70849895477295, 201.52267169952393, 205.3368444442749, 209.14712572097778, 212.95740699768066, 216.77045106887817, 220.58349514007568, 224.4005584716797, 228.2176218032837, 232.01877284049988, 235.81992387771606, 239.64388966560364, 243.4678554534912, 247.3094823360443, 251.1511092185974, 254.96103811264038, 258.77096700668335, 262.5968942642212, 266.42282152175903, 270.2382462024689, 274.0536708831787, 277.8492646217346, 281.6448583602905, 285.4995274543762, 289.3541965484619, 293.1741111278534, 296.9940257072449, 300.8016676902771, 304.6093096733093, 308.4267780780792, 312.2442464828491, 316.0390536785126, 319.833860874176, 323.6292452812195, 327.42462968826294, 331.21541261672974, 335.00619554519653, 338.85070300102234, 342.69521045684814, 346.552273273468, 350.4093360900879, 354.24395275115967, 358.07856941223145, 361.9214971065521, 365.7644248008728, 369.6223826408386, 373.48034048080444, 377.29079723358154, 381.10125398635864, 384.93949794769287, 388.7777419090271, 392.6002974510193, 396.4228529930115, 400.2603380680084, 404.09782314300537, 407.92257475852966, 411.74732637405396, 415.58130145072937, 419.4152765274048, 423.2404999732971, 427.06572341918945, 430.8746771812439, 434.68363094329834, 438.5037739276886, 442.32391691207886, 446.1380069255829, 449.9520969390869, 453.7569532394409, 457.5618095397949, 461.3942720890045, 465.2267346382141, 469.01452231407166, 472.8023099899292, 476.59066700935364, 480.3790240287781, 484.1812512874603, 487.9834785461426, 491.7410614490509, 495.49864435195923, 499.2926490306854, 503.0866537094116, 506.8787999153137, 510.6709461212158, 514.4493572711945, 518.2277684211731, 522.1091430187225, 525.990517616272, 529.8384785652161, 533.6864395141602, 537.5173633098602, 541.3482871055603, 545.1547908782959, 548.9612946510315, 552.7584218978882, 556.5555491447449, 560.3706369400024, 564.18572473526, 568.0217535495758, 571.8577823638916, 575.6901829242706, 579.5225834846497, 583.3827917575836, 587.2430000305176, 591.0371775627136, 594.8313550949097, 598.6568803787231, 602.4824056625366, 606.285224199295, 610.0880427360535, 613.872341632843, 617.6566405296326, 621.4543976783752, 625.2521548271179, 629.0357203483582, 632.8192858695984, 636.6107349395752, 640.402184009552, 644.230441570282, 648.058699131012, 651.8971934318542, 655.7356877326965, 659.5626089572906, 663.3895301818848, 667.2145671844482, 671.0396041870117, 674.8287537097931, 678.6179032325745, 682.4475064277649, 686.2771096229553, 690.0954015254974, 693.9136934280396, 697.7261779308319, 701.5386624336243, 705.3717052936554, 709.2047481536865, 713.1065983772278, 717.008448600769, 720.8797962665558, 724.7511439323425, 728.6416752338409, 732.5322065353394, 736.4331796169281, 740.3341526985168, 744.1587707996368, 747.9833889007568, 751.7898547649384, 755.5963206291199, 759.4297204017639, 763.263120174408, 767.0796656608582, 770.8962111473083, 774.7198355197906, 778.543459892273, 780.7054121494293, 782.8673644065857]
[31.566666666666666, 31.566666666666666, 53.666666666666664, 53.666666666666664, 59.38333333333333, 59.38333333333333, 69.33611111111111, 69.33611111111111, 67.55277777777778, 67.55277777777778, 70.8, 70.8, 73.26944444444445, 73.26944444444445, 78.05833333333334, 78.05833333333334, 79.38055555555556, 79.38055555555556, 81.88333333333334, 81.88333333333334, 82.86944444444444, 82.86944444444444, 83.2, 83.2, 83.075, 83.075, 83.58611111111111, 83.58611111111111, 84.15277777777777, 84.15277777777777, 84.43055555555556, 84.43055555555556, 84.81388888888888, 84.81388888888888, 84.85555555555555, 84.85555555555555, 84.68333333333334, 84.68333333333334, 85.16111111111111, 85.16111111111111, 85.06388888888888, 85.06388888888888, 85.53333333333333, 85.53333333333333, 85.68611111111112, 85.68611111111112, 85.775, 85.775, 85.94722222222222, 85.94722222222222, 85.58611111111111, 85.58611111111111, 86.26666666666667, 86.26666666666667, 86.65277777777777, 86.65277777777777, 86.59444444444445, 86.59444444444445, 86.54166666666667, 86.54166666666667, 86.41666666666667, 86.41666666666667, 87.05, 87.05, 86.98055555555555, 86.98055555555555, 86.77222222222223, 86.77222222222223, 87.0, 87.0, 86.5, 86.5, 86.71666666666667, 86.71666666666667, 86.92222222222222, 86.92222222222222, 87.02222222222223, 87.02222222222223, 87.49444444444444, 87.49444444444444, 87.6, 87.6, 87.45555555555555, 87.45555555555555, 87.26388888888889, 87.26388888888889, 86.95, 86.95, 87.18611111111112, 87.18611111111112, 87.14722222222223, 87.14722222222223, 87.28333333333333, 87.28333333333333, 86.93888888888888, 86.93888888888888, 87.325, 87.325, 87.80833333333334, 87.80833333333334, 87.54444444444445, 87.54444444444445, 87.89722222222223, 87.89722222222223, 87.82777777777778, 87.82777777777778, 87.97777777777777, 87.97777777777777, 87.85277777777777, 87.85277777777777, 87.88333333333334, 87.88333333333334, 87.96666666666667, 87.96666666666667, 87.81666666666666, 87.81666666666666, 88.09444444444445, 88.09444444444445, 88.02222222222223, 88.02222222222223, 88.15, 88.15, 88.14166666666667, 88.14166666666667, 88.11111111111111, 88.11111111111111, 88.23611111111111, 88.23611111111111, 88.22777777777777, 88.22777777777777, 88.28611111111111, 88.28611111111111, 88.49722222222222, 88.49722222222222, 88.28611111111111, 88.28611111111111, 88.45555555555555, 88.45555555555555, 88.47222222222223, 88.47222222222223, 88.425, 88.425, 88.3861111111111, 88.3861111111111, 88.44722222222222, 88.44722222222222, 88.43611111111112, 88.43611111111112, 88.28333333333333, 88.28333333333333, 88.10555555555555, 88.10555555555555, 88.25555555555556, 88.25555555555556, 88.17222222222222, 88.17222222222222, 87.88055555555556, 87.88055555555556, 88.19444444444444, 88.19444444444444, 88.38333333333334, 88.38333333333334, 88.3638888888889, 88.3638888888889, 88.67777777777778, 88.67777777777778, 88.50555555555556, 88.50555555555556, 88.46111111111111, 88.46111111111111, 88.32222222222222, 88.32222222222222, 88.28888888888889, 88.28888888888889, 88.46111111111111, 88.46111111111111, 88.23055555555555, 88.23055555555555, 88.44722222222222, 88.44722222222222, 88.75833333333334, 88.75833333333334, 88.66666666666667, 88.66666666666667, 88.86944444444444, 88.86944444444444, 88.9888888888889, 88.9888888888889, 88.82777777777778, 88.82777777777778, 88.86944444444444, 88.86944444444444, 88.86111111111111, 88.86111111111111, 88.91388888888889, 88.91388888888889, 88.90555555555555, 88.90555555555555, 88.85833333333333, 88.85833333333333, 88.62777777777778, 88.62777777777778]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.215, Test loss: 2.227, Test accuracy: 27.04
Round   1, Train loss: 0.831, Test loss: 1.829, Test accuracy: 38.79
Round   2, Train loss: 0.729, Test loss: 1.367, Test accuracy: 47.16
Round   3, Train loss: 0.661, Test loss: 1.114, Test accuracy: 56.06
Round   4, Train loss: 0.665, Test loss: 1.269, Test accuracy: 58.12
Round   5, Train loss: 0.704, Test loss: 1.303, Test accuracy: 58.31
Round   6, Train loss: 0.677, Test loss: 0.926, Test accuracy: 64.82
Round   7, Train loss: 0.553, Test loss: 0.712, Test accuracy: 72.36
Round   8, Train loss: 0.633, Test loss: 0.571, Test accuracy: 76.67
Round   9, Train loss: 0.577, Test loss: 0.550, Test accuracy: 77.35
Round  10, Train loss: 0.532, Test loss: 0.542, Test accuracy: 78.41
Round  11, Train loss: 0.504, Test loss: 0.524, Test accuracy: 78.74
Round  12, Train loss: 0.406, Test loss: 0.510, Test accuracy: 79.44
Round  13, Train loss: 0.542, Test loss: 0.496, Test accuracy: 79.83
Round  14, Train loss: 0.392, Test loss: 0.480, Test accuracy: 80.85
Round  15, Train loss: 0.509, Test loss: 0.474, Test accuracy: 81.52
Round  16, Train loss: 0.461, Test loss: 0.463, Test accuracy: 81.93
Round  17, Train loss: 0.446, Test loss: 0.452, Test accuracy: 82.18
Round  18, Train loss: 0.522, Test loss: 0.438, Test accuracy: 83.02
Round  19, Train loss: 0.419, Test loss: 0.429, Test accuracy: 83.03
Round  20, Train loss: 0.465, Test loss: 0.413, Test accuracy: 83.94
Round  21, Train loss: 0.467, Test loss: 0.406, Test accuracy: 84.06
Round  22, Train loss: 0.319, Test loss: 0.399, Test accuracy: 83.83
Round  23, Train loss: 0.397, Test loss: 0.391, Test accuracy: 84.15
Round  24, Train loss: 0.386, Test loss: 0.390, Test accuracy: 84.38
Round  25, Train loss: 0.417, Test loss: 0.387, Test accuracy: 84.99
Round  26, Train loss: 0.342, Test loss: 0.374, Test accuracy: 85.26
Round  27, Train loss: 0.347, Test loss: 0.375, Test accuracy: 84.86
Round  28, Train loss: 0.279, Test loss: 0.376, Test accuracy: 84.96
Round  29, Train loss: 0.334, Test loss: 0.374, Test accuracy: 85.05
Round  30, Train loss: 0.281, Test loss: 0.375, Test accuracy: 84.80
Round  31, Train loss: 0.409, Test loss: 0.380, Test accuracy: 84.81
Round  32, Train loss: 0.399, Test loss: 0.361, Test accuracy: 85.79
Round  33, Train loss: 0.352, Test loss: 0.358, Test accuracy: 85.92
Round  34, Train loss: 0.319, Test loss: 0.357, Test accuracy: 85.91
Round  35, Train loss: 0.310, Test loss: 0.351, Test accuracy: 86.10
Round  36, Train loss: 0.386, Test loss: 0.343, Test accuracy: 86.56
Round  37, Train loss: 0.269, Test loss: 0.341, Test accuracy: 86.49
Round  38, Train loss: 0.289, Test loss: 0.338, Test accuracy: 86.64
Round  39, Train loss: 0.330, Test loss: 0.341, Test accuracy: 86.50
Round  40, Train loss: 0.319, Test loss: 0.336, Test accuracy: 87.06
Round  41, Train loss: 0.292, Test loss: 0.339, Test accuracy: 86.84
Round  42, Train loss: 0.310, Test loss: 0.331, Test accuracy: 87.17
Round  43, Train loss: 0.315, Test loss: 0.331, Test accuracy: 87.19
Round  44, Train loss: 0.289, Test loss: 0.332, Test accuracy: 86.96
Round  45, Train loss: 0.387, Test loss: 0.338, Test accuracy: 86.85
Round  46, Train loss: 0.203, Test loss: 0.334, Test accuracy: 86.82
Round  47, Train loss: 0.309, Test loss: 0.331, Test accuracy: 86.93
Round  48, Train loss: 0.311, Test loss: 0.329, Test accuracy: 87.25
Round  49, Train loss: 0.261, Test loss: 0.330, Test accuracy: 87.33
Round  50, Train loss: 0.308, Test loss: 0.328, Test accuracy: 87.43
Round  51, Train loss: 0.349, Test loss: 0.327, Test accuracy: 87.36
Round  52, Train loss: 0.235, Test loss: 0.330, Test accuracy: 87.14
Round  53, Train loss: 0.252, Test loss: 0.318, Test accuracy: 87.70
Round  54, Train loss: 0.283, Test loss: 0.319, Test accuracy: 87.66
Round  55, Train loss: 0.259, Test loss: 0.326, Test accuracy: 87.46
Round  56, Train loss: 0.315, Test loss: 0.325, Test accuracy: 87.44
Round  57, Train loss: 0.259, Test loss: 0.325, Test accuracy: 87.35
Round  58, Train loss: 0.354, Test loss: 0.325, Test accuracy: 87.55
Round  59, Train loss: 0.225, Test loss: 0.329, Test accuracy: 87.28
Round  60, Train loss: 0.357, Test loss: 0.329, Test accuracy: 87.49
Round  61, Train loss: 0.268, Test loss: 0.324, Test accuracy: 87.85
Round  62, Train loss: 0.259, Test loss: 0.324, Test accuracy: 87.69
Round  63, Train loss: 0.245, Test loss: 0.328, Test accuracy: 87.67
Round  64, Train loss: 0.289, Test loss: 0.321, Test accuracy: 87.70
Round  65, Train loss: 0.230, Test loss: 0.330, Test accuracy: 87.46
Round  66, Train loss: 0.285, Test loss: 0.322, Test accuracy: 87.70
Round  67, Train loss: 0.277, Test loss: 0.325, Test accuracy: 87.77
Round  68, Train loss: 0.310, Test loss: 0.336, Test accuracy: 87.46
Round  69, Train loss: 0.229, Test loss: 0.322, Test accuracy: 87.83
Round  70, Train loss: 0.283, Test loss: 0.327, Test accuracy: 87.71
Round  71, Train loss: 0.208, Test loss: 0.330, Test accuracy: 87.83
Round  72, Train loss: 0.286, Test loss: 0.324, Test accuracy: 88.07
Round  73, Train loss: 0.317, Test loss: 0.325, Test accuracy: 87.85
Round  74, Train loss: 0.216, Test loss: 0.328, Test accuracy: 87.66
Round  75, Train loss: 0.216, Test loss: 0.325, Test accuracy: 88.11
Round  76, Train loss: 0.202, Test loss: 0.319, Test accuracy: 88.09
Round  77, Train loss: 0.229, Test loss: 0.321, Test accuracy: 88.19
Round  78, Train loss: 0.213, Test loss: 0.324, Test accuracy: 87.76
Round  79, Train loss: 0.239, Test loss: 0.318, Test accuracy: 88.27
Round  80, Train loss: 0.195, Test loss: 0.321, Test accuracy: 87.91
Round  81, Train loss: 0.239, Test loss: 0.316, Test accuracy: 88.22
Round  82, Train loss: 0.198, Test loss: 0.321, Test accuracy: 88.09
Round  83, Train loss: 0.234, Test loss: 0.315, Test accuracy: 88.19
Round  84, Train loss: 0.247, Test loss: 0.314, Test accuracy: 88.38
Round  85, Train loss: 0.198, Test loss: 0.323, Test accuracy: 87.98
Round  86, Train loss: 0.196, Test loss: 0.317, Test accuracy: 88.22
Round  87, Train loss: 0.255, Test loss: 0.309, Test accuracy: 88.54
Round  88, Train loss: 0.210, Test loss: 0.312, Test accuracy: 88.63
Round  89, Train loss: 0.207, Test loss: 0.308, Test accuracy: 88.63
Round  90, Train loss: 0.320, Test loss: 0.313, Test accuracy: 88.42
Round  91, Train loss: 0.200, Test loss: 0.309, Test accuracy: 88.56
Round  92, Train loss: 0.242, Test loss: 0.322, Test accuracy: 88.03
Round  93, Train loss: 0.260, Test loss: 0.320, Test accuracy: 88.01
Round  94, Train loss: 0.186, Test loss: 0.319, Test accuracy: 88.13
Round  95, Train loss: 0.227, Test loss: 0.315, Test accuracy: 88.13
Round  96, Train loss: 0.243, Test loss: 0.316, Test accuracy: 88.19
Round  97, Train loss: 0.208, Test loss: 0.322, Test accuracy: 87.98
Round  98, Train loss: 0.226, Test loss: 0.319, Test accuracy: 88.22
Round  99, Train loss: 0.185, Test loss: 0.334, Test accuracy: 87.99
Final Round, Train loss: 0.177, Test loss: 0.324, Test accuracy: 88.31
Average accuracy final 10 rounds: 88.16583333333334 

4434.594607830048
[4.070453882217407, 8.140907764434814, 12.127805709838867, 16.11470365524292, 20.10258436203003, 24.09046506881714, 28.153299808502197, 32.216134548187256, 36.234370946884155, 40.252607345581055, 44.2443265914917, 48.236045837402344, 52.250401735305786, 56.26475763320923, 60.304858684539795, 64.34495973587036, 68.37533712387085, 72.40571451187134, 76.40333008766174, 80.40094566345215, 84.43921208381653, 88.47747850418091, 92.52697443962097, 96.57647037506104, 100.54891991615295, 104.52136945724487, 108.54088377952576, 112.56039810180664, 116.60074853897095, 120.64109897613525, 124.42453479766846, 128.20797061920166, 131.84910082817078, 135.4902310371399, 139.23868012428284, 142.98712921142578, 146.97545337677002, 150.96377754211426, 154.67036628723145, 158.37695503234863, 162.3069965839386, 166.23703813552856, 170.23589515686035, 174.23475217819214, 178.19076204299927, 182.1467719078064, 186.11615014076233, 190.08552837371826, 194.07485055923462, 198.06417274475098, 202.07548260688782, 206.08679246902466, 210.07009768486023, 214.0534029006958, 218.01005387306213, 221.96670484542847, 225.94170379638672, 229.91670274734497, 233.8803219795227, 237.84394121170044, 241.82685828208923, 245.80977535247803, 249.80294966697693, 253.79612398147583, 257.8146016597748, 261.83307933807373, 265.84561920166016, 269.8581590652466, 273.83030891418457, 277.80245876312256, 281.77771186828613, 285.7529649734497, 289.7553377151489, 293.75771045684814, 297.72672414779663, 301.6957378387451, 305.64363718032837, 309.5915365219116, 313.5824761390686, 317.5734157562256, 321.62086248397827, 325.66830921173096, 329.67628240585327, 333.6842555999756, 337.60856223106384, 341.5328688621521, 345.50708079338074, 349.4812927246094, 353.46124792099, 357.4412031173706, 361.41509652137756, 365.3889899253845, 369.3480360507965, 373.3070821762085, 377.31352257728577, 381.31996297836304, 385.3158595561981, 389.3117561340332, 393.30796575546265, 397.3041753768921, 401.27956366539, 405.25495195388794, 409.2325246334076, 413.21009731292725, 417.2088484764099, 421.2075996398926, 425.1841814517975, 429.1607632637024, 433.10553550720215, 437.0503077507019, 441.04099464416504, 445.0316815376282, 449.0301249027252, 453.02856826782227, 456.98199820518494, 460.9354281425476, 464.87672090530396, 468.8180136680603, 472.797025680542, 476.7760376930237, 480.7567358016968, 484.7374339103699, 488.67145586013794, 492.605477809906, 496.58118963241577, 500.55690145492554, 504.53952407836914, 508.52214670181274, 512.4935610294342, 516.4649753570557, 520.4328572750092, 524.4007391929626, 528.3453788757324, 532.2900185585022, 536.2903063297272, 540.2905941009521, 544.2779052257538, 548.2652163505554, 552.2258129119873, 556.1864094734192, 560.1397347450256, 564.0930600166321, 568.0712637901306, 572.0494675636292, 576.0236692428589, 579.9978709220886, 583.9713118076324, 587.9447526931763, 591.9002566337585, 595.8557605743408, 599.8431353569031, 603.8305101394653, 607.8104438781738, 611.7903776168823, 615.7459616661072, 619.701545715332, 623.6502573490143, 627.5989689826965, 631.599720954895, 635.6004729270935, 639.5912230014801, 643.5819730758667, 647.5694289207458, 651.556884765625, 655.4924890995026, 659.4280934333801, 663.3951814174652, 667.3622694015503, 671.3587508201599, 675.3552322387695, 679.3237133026123, 683.2921943664551, 687.2332260608673, 691.1742577552795, 695.1339876651764, 699.0937175750732, 703.0791873931885, 707.0646572113037, 711.0003569126129, 714.9360566139221, 718.8874652385712, 722.8388738632202, 726.8116128444672, 730.7843518257141, 734.7268333435059, 738.6693148612976, 742.6114327907562, 746.5535507202148, 750.5138454437256, 754.4741401672363, 758.4752061367035, 762.4762721061707, 766.4461555480957, 770.4160389900208, 774.2214207649231, 778.0268025398254, 781.987637758255, 785.9484729766846, 789.8880884647369, 793.8277039527893, 795.8214182853699, 797.8151326179504]
[27.041666666666668, 27.041666666666668, 38.78888888888889, 38.78888888888889, 47.16388888888889, 47.16388888888889, 56.05555555555556, 56.05555555555556, 58.11666666666667, 58.11666666666667, 58.30833333333333, 58.30833333333333, 64.81944444444444, 64.81944444444444, 72.3638888888889, 72.3638888888889, 76.67222222222222, 76.67222222222222, 77.35, 77.35, 78.40555555555555, 78.40555555555555, 78.74166666666666, 78.74166666666666, 79.43611111111112, 79.43611111111112, 79.83333333333333, 79.83333333333333, 80.84722222222223, 80.84722222222223, 81.51666666666667, 81.51666666666667, 81.93055555555556, 81.93055555555556, 82.18333333333334, 82.18333333333334, 83.01944444444445, 83.01944444444445, 83.025, 83.025, 83.93888888888888, 83.93888888888888, 84.06111111111112, 84.06111111111112, 83.83055555555555, 83.83055555555555, 84.14722222222223, 84.14722222222223, 84.37777777777778, 84.37777777777778, 84.9888888888889, 84.9888888888889, 85.25555555555556, 85.25555555555556, 84.85833333333333, 84.85833333333333, 84.96111111111111, 84.96111111111111, 85.04722222222222, 85.04722222222222, 84.79722222222222, 84.79722222222222, 84.80833333333334, 84.80833333333334, 85.78888888888889, 85.78888888888889, 85.925, 85.925, 85.90555555555555, 85.90555555555555, 86.09722222222223, 86.09722222222223, 86.55833333333334, 86.55833333333334, 86.49444444444444, 86.49444444444444, 86.64444444444445, 86.64444444444445, 86.50277777777778, 86.50277777777778, 87.05833333333334, 87.05833333333334, 86.84444444444445, 86.84444444444445, 87.16944444444445, 87.16944444444445, 87.18888888888888, 87.18888888888888, 86.96388888888889, 86.96388888888889, 86.85277777777777, 86.85277777777777, 86.81666666666666, 86.81666666666666, 86.93333333333334, 86.93333333333334, 87.25, 87.25, 87.32777777777778, 87.32777777777778, 87.43055555555556, 87.43055555555556, 87.35555555555555, 87.35555555555555, 87.14444444444445, 87.14444444444445, 87.70277777777778, 87.70277777777778, 87.66388888888889, 87.66388888888889, 87.45555555555555, 87.45555555555555, 87.43888888888888, 87.43888888888888, 87.35277777777777, 87.35277777777777, 87.55, 87.55, 87.28055555555555, 87.28055555555555, 87.48611111111111, 87.48611111111111, 87.85, 87.85, 87.69444444444444, 87.69444444444444, 87.66666666666667, 87.66666666666667, 87.69722222222222, 87.69722222222222, 87.46111111111111, 87.46111111111111, 87.70277777777778, 87.70277777777778, 87.77222222222223, 87.77222222222223, 87.46388888888889, 87.46388888888889, 87.83333333333333, 87.83333333333333, 87.71111111111111, 87.71111111111111, 87.83055555555555, 87.83055555555555, 88.06666666666666, 88.06666666666666, 87.84722222222223, 87.84722222222223, 87.66111111111111, 87.66111111111111, 88.11111111111111, 88.11111111111111, 88.08611111111111, 88.08611111111111, 88.18888888888888, 88.18888888888888, 87.75555555555556, 87.75555555555556, 88.26944444444445, 88.26944444444445, 87.91111111111111, 87.91111111111111, 88.225, 88.225, 88.09444444444445, 88.09444444444445, 88.18888888888888, 88.18888888888888, 88.375, 88.375, 87.98055555555555, 87.98055555555555, 88.21944444444445, 88.21944444444445, 88.54166666666667, 88.54166666666667, 88.63333333333334, 88.63333333333334, 88.63055555555556, 88.63055555555556, 88.425, 88.425, 88.55833333333334, 88.55833333333334, 88.025, 88.025, 88.00833333333334, 88.00833333333334, 88.13055555555556, 88.13055555555556, 88.13333333333334, 88.13333333333334, 88.19166666666666, 88.19166666666666, 87.97777777777777, 87.97777777777777, 88.22222222222223, 88.22222222222223, 87.98611111111111, 87.98611111111111, 88.30555555555556, 88.30555555555556]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.270, Test loss: 2.264, Test accuracy: 27.36
Round   1, Train loss: 0.963, Test loss: 2.143, Test accuracy: 34.09
Round   2, Train loss: 0.862, Test loss: 1.569, Test accuracy: 43.40
Round   3, Train loss: 0.742, Test loss: 1.144, Test accuracy: 58.91
Round   4, Train loss: 0.773, Test loss: 0.938, Test accuracy: 65.07
Round   5, Train loss: 0.658, Test loss: 0.675, Test accuracy: 71.18
Round   6, Train loss: 0.608, Test loss: 0.654, Test accuracy: 73.44
Round   7, Train loss: 0.659, Test loss: 0.629, Test accuracy: 75.67
Round   8, Train loss: 0.546, Test loss: 0.593, Test accuracy: 76.03
Round   9, Train loss: 0.524, Test loss: 0.586, Test accuracy: 77.50
Round  10, Train loss: 0.568, Test loss: 0.535, Test accuracy: 79.03
Round  11, Train loss: 0.527, Test loss: 0.511, Test accuracy: 80.02
Round  12, Train loss: 0.389, Test loss: 0.513, Test accuracy: 80.13
Round  13, Train loss: 0.504, Test loss: 0.500, Test accuracy: 80.55
Round  14, Train loss: 0.508, Test loss: 0.474, Test accuracy: 81.65
Round  15, Train loss: 0.534, Test loss: 0.455, Test accuracy: 82.68
Round  16, Train loss: 0.457, Test loss: 0.457, Test accuracy: 82.78
Round  17, Train loss: 0.448, Test loss: 0.439, Test accuracy: 82.94
Round  18, Train loss: 0.372, Test loss: 0.435, Test accuracy: 83.29
Round  19, Train loss: 0.596, Test loss: 0.429, Test accuracy: 83.42
Round  20, Train loss: 0.372, Test loss: 0.411, Test accuracy: 84.23
Round  21, Train loss: 0.456, Test loss: 0.405, Test accuracy: 84.34
Round  22, Train loss: 0.464, Test loss: 0.399, Test accuracy: 84.79
Round  23, Train loss: 0.411, Test loss: 0.400, Test accuracy: 84.88
Round  24, Train loss: 0.446, Test loss: 0.389, Test accuracy: 84.92
Round  25, Train loss: 0.420, Test loss: 0.389, Test accuracy: 85.17
Round  26, Train loss: 0.430, Test loss: 0.381, Test accuracy: 85.33
Round  27, Train loss: 0.403, Test loss: 0.374, Test accuracy: 85.37
Round  28, Train loss: 0.368, Test loss: 0.377, Test accuracy: 85.38
Round  29, Train loss: 0.423, Test loss: 0.367, Test accuracy: 86.02
Round  30, Train loss: 0.371, Test loss: 0.356, Test accuracy: 86.35
Round  31, Train loss: 0.343, Test loss: 0.352, Test accuracy: 86.37
Round  32, Train loss: 0.343, Test loss: 0.357, Test accuracy: 86.03
Round  33, Train loss: 0.371, Test loss: 0.361, Test accuracy: 85.72
Round  34, Train loss: 0.298, Test loss: 0.357, Test accuracy: 86.03
Round  35, Train loss: 0.319, Test loss: 0.348, Test accuracy: 86.32
Round  36, Train loss: 0.317, Test loss: 0.344, Test accuracy: 86.69
Round  37, Train loss: 0.293, Test loss: 0.345, Test accuracy: 86.38
Round  38, Train loss: 0.374, Test loss: 0.342, Test accuracy: 86.79
Round  39, Train loss: 0.371, Test loss: 0.342, Test accuracy: 86.82
Round  40, Train loss: 0.311, Test loss: 0.333, Test accuracy: 87.01
Round  41, Train loss: 0.340, Test loss: 0.340, Test accuracy: 86.76
Round  42, Train loss: 0.284, Test loss: 0.339, Test accuracy: 86.97
Round  43, Train loss: 0.326, Test loss: 0.343, Test accuracy: 86.75
Round  44, Train loss: 0.358, Test loss: 0.334, Test accuracy: 87.09
Round  45, Train loss: 0.308, Test loss: 0.332, Test accuracy: 87.04
Round  46, Train loss: 0.260, Test loss: 0.330, Test accuracy: 87.22
Round  47, Train loss: 0.287, Test loss: 0.329, Test accuracy: 87.34
Round  48, Train loss: 0.277, Test loss: 0.336, Test accuracy: 87.20
Round  49, Train loss: 0.363, Test loss: 0.332, Test accuracy: 87.19
Round  50, Train loss: 0.249, Test loss: 0.329, Test accuracy: 87.16
Round  51, Train loss: 0.315, Test loss: 0.319, Test accuracy: 87.83
Round  52, Train loss: 0.281, Test loss: 0.321, Test accuracy: 87.60
Round  53, Train loss: 0.286, Test loss: 0.323, Test accuracy: 87.61
Round  54, Train loss: 0.273, Test loss: 0.323, Test accuracy: 87.59
Round  55, Train loss: 0.263, Test loss: 0.328, Test accuracy: 87.61
Round  56, Train loss: 0.309, Test loss: 0.321, Test accuracy: 87.66
Round  57, Train loss: 0.244, Test loss: 0.316, Test accuracy: 87.79
Round  58, Train loss: 0.318, Test loss: 0.315, Test accuracy: 87.83
Round  59, Train loss: 0.239, Test loss: 0.314, Test accuracy: 87.91
Round  60, Train loss: 0.334, Test loss: 0.313, Test accuracy: 88.02
Round  61, Train loss: 0.235, Test loss: 0.323, Test accuracy: 87.62
Round  62, Train loss: 0.281, Test loss: 0.315, Test accuracy: 87.97
Round  63, Train loss: 0.269, Test loss: 0.313, Test accuracy: 88.09
Round  64, Train loss: 0.312, Test loss: 0.311, Test accuracy: 88.04
Round  65, Train loss: 0.257, Test loss: 0.309, Test accuracy: 88.09
Round  66, Train loss: 0.276, Test loss: 0.312, Test accuracy: 88.12
Round  67, Train loss: 0.324, Test loss: 0.312, Test accuracy: 88.01
Round  68, Train loss: 0.254, Test loss: 0.312, Test accuracy: 87.89
Round  69, Train loss: 0.245, Test loss: 0.318, Test accuracy: 87.66
Round  70, Train loss: 0.219, Test loss: 0.314, Test accuracy: 87.88
Round  71, Train loss: 0.213, Test loss: 0.310, Test accuracy: 87.90
Round  72, Train loss: 0.274, Test loss: 0.305, Test accuracy: 88.22
Round  73, Train loss: 0.286, Test loss: 0.309, Test accuracy: 88.35
Round  74, Train loss: 0.244, Test loss: 0.312, Test accuracy: 88.06
Round  75, Train loss: 0.233, Test loss: 0.308, Test accuracy: 88.12
Round  76, Train loss: 0.258, Test loss: 0.313, Test accuracy: 88.10
Round  77, Train loss: 0.264, Test loss: 0.302, Test accuracy: 88.51
Round  78, Train loss: 0.274, Test loss: 0.305, Test accuracy: 88.45
Round  79, Train loss: 0.235, Test loss: 0.301, Test accuracy: 88.48
Round  80, Train loss: 0.255, Test loss: 0.305, Test accuracy: 88.34
Round  81, Train loss: 0.273, Test loss: 0.302, Test accuracy: 88.47
Round  82, Train loss: 0.243, Test loss: 0.308, Test accuracy: 88.34
Round  83, Train loss: 0.211, Test loss: 0.309, Test accuracy: 88.55
Round  84, Train loss: 0.239, Test loss: 0.303, Test accuracy: 88.53
Round  85, Train loss: 0.188, Test loss: 0.305, Test accuracy: 88.45
Round  86, Train loss: 0.179, Test loss: 0.303, Test accuracy: 88.59
Round  87, Train loss: 0.216, Test loss: 0.307, Test accuracy: 88.53
Round  88, Train loss: 0.228, Test loss: 0.306, Test accuracy: 88.43
Round  89, Train loss: 0.225, Test loss: 0.304, Test accuracy: 88.57
Round  90, Train loss: 0.222, Test loss: 0.299, Test accuracy: 88.65
Round  91, Train loss: 0.219, Test loss: 0.303, Test accuracy: 88.57
Round  92, Train loss: 0.198, Test loss: 0.299, Test accuracy: 88.56
Round  93, Train loss: 0.288, Test loss: 0.304, Test accuracy: 88.79
Round  94, Train loss: 0.232, Test loss: 0.306, Test accuracy: 88.67
Round  95, Train loss: 0.275, Test loss: 0.302, Test accuracy: 88.65
Round  96, Train loss: 0.252, Test loss: 0.298, Test accuracy: 88.81
Round  97, Train loss: 0.219, Test loss: 0.302, Test accuracy: 88.76
Round  98, Train loss: 0.185, Test loss: 0.302, Test accuracy: 88.77
Round  99, Train loss: 0.243, Test loss: 0.298, Test accuracy: 88.86
Final Round, Train loss: 0.180, Test loss: 0.298, Test accuracy: 88.86
Average accuracy final 10 rounds: 88.70777777777778
4932.046870946884
[5.787741422653198, 11.575482845306396, 16.859297037124634, 22.14311122894287, 27.412410974502563, 32.681710720062256, 37.818453311920166, 42.955195903778076, 48.1203396320343, 53.28548336029053, 58.33981657028198, 63.39414978027344, 68.48172807693481, 73.56930637359619, 78.31650686264038, 83.06370735168457, 87.75872373580933, 92.45374011993408, 97.73239278793335, 103.01104545593262, 108.22579264640808, 113.44053983688354, 118.6636962890625, 123.88685274124146, 128.60847735404968, 133.3301019668579, 138.09228014945984, 142.85445833206177, 147.53727793693542, 152.22009754180908, 156.93633151054382, 161.65256547927856, 166.43012237548828, 171.207679271698, 175.99872303009033, 180.78976678848267, 185.53201365470886, 190.27426052093506, 194.9651916027069, 199.65612268447876, 204.3585660457611, 209.06100940704346, 213.81461834907532, 218.56822729110718, 223.30136919021606, 228.03451108932495, 232.7061984539032, 237.37788581848145, 242.1470284461975, 246.91617107391357, 251.65527772903442, 256.3943843841553, 261.11950516700745, 265.8446259498596, 270.5048499107361, 275.16507387161255, 279.88180708885193, 284.5985403060913, 289.3513650894165, 294.1041898727417, 298.849072933197, 303.59395599365234, 308.31699085235596, 313.04002571105957, 317.7926125526428, 322.5451993942261, 327.23012232780457, 331.91504526138306, 336.51391410827637, 341.1127829551697, 345.74598813056946, 350.37919330596924, 355.0523028373718, 359.7254123687744, 364.39129281044006, 369.0571732521057, 373.733535528183, 378.40989780426025, 383.0448434352875, 387.6797890663147, 392.3317277431488, 396.9836664199829, 401.642338514328, 406.3010106086731, 411.0010323524475, 415.7010540962219, 420.36270451545715, 425.0243549346924, 429.65750432014465, 434.2906537055969, 438.9055755138397, 443.5204973220825, 448.1783986091614, 452.83629989624023, 457.5104773044586, 462.184654712677, 466.8566665649414, 471.5286784172058, 476.17994713783264, 480.8312158584595, 485.4865026473999, 490.14178943634033, 494.81907200813293, 499.49635457992554, 504.1703143119812, 508.84427404403687, 513.4855573177338, 518.1268405914307, 522.8259751796722, 527.5251097679138, 532.1450095176697, 536.7649092674255, 541.4374508857727, 546.1099925041199, 550.785130739212, 555.4602689743042, 560.1415493488312, 564.8228297233582, 569.5255360603333, 574.2282423973083, 578.9122183322906, 583.596194267273, 588.2413809299469, 592.8865675926208, 597.5494141578674, 602.212260723114, 606.8764867782593, 611.5407128334045, 616.1954615116119, 620.8502101898193, 625.4912002086639, 630.1321902275085, 634.7635610103607, 639.3949317932129, 644.0513319969177, 648.7077322006226, 653.3657538890839, 658.0237755775452, 662.6918976306915, 667.3600196838379, 672.004611492157, 676.6492033004761, 681.2972645759583, 685.9453258514404, 690.6285970211029, 695.3118681907654, 700.008157491684, 704.7044467926025, 709.3809204101562, 714.05739402771, 718.6847870349884, 723.3121800422668, 727.9257080554962, 732.5392360687256, 737.1692967414856, 741.7993574142456, 746.4324963092804, 751.0656352043152, 755.7089102268219, 760.3521852493286, 765.0169019699097, 769.6816186904907, 774.2968323230743, 778.912045955658, 783.5681753158569, 788.2243046760559, 792.8175094127655, 797.4107141494751, 802.5396363735199, 807.6685585975647, 812.8504271507263, 818.0322957038879, 822.9733645915985, 827.9144334793091, 832.9385614395142, 837.9626893997192, 843.1071758270264, 848.2516622543335, 853.4478936195374, 858.6441249847412, 863.6758899688721, 868.7076549530029, 873.9474279880524, 879.1872010231018, 884.4084737300873, 889.6297464370728, 894.6670303344727, 899.7043142318726, 904.7864818572998, 909.868649482727, 915.0851891040802, 920.3017287254333, 925.573410987854, 930.8450932502747, 936.1175093650818, 941.3899254798889, 946.68479347229, 951.9796614646912, 957.2378933429718, 962.4961252212524, 964.6790587902069, 966.8619923591614]
[27.36111111111111, 27.36111111111111, 34.09444444444444, 34.09444444444444, 43.4, 43.4, 58.91111111111111, 58.91111111111111, 65.06944444444444, 65.06944444444444, 71.18333333333334, 71.18333333333334, 73.43611111111112, 73.43611111111112, 75.66666666666667, 75.66666666666667, 76.03333333333333, 76.03333333333333, 77.50277777777778, 77.50277777777778, 79.03055555555555, 79.03055555555555, 80.02222222222223, 80.02222222222223, 80.13333333333334, 80.13333333333334, 80.55, 80.55, 81.65277777777777, 81.65277777777777, 82.68055555555556, 82.68055555555556, 82.775, 82.775, 82.94444444444444, 82.94444444444444, 83.28611111111111, 83.28611111111111, 83.41666666666667, 83.41666666666667, 84.23333333333333, 84.23333333333333, 84.33888888888889, 84.33888888888889, 84.78888888888889, 84.78888888888889, 84.875, 84.875, 84.91944444444445, 84.91944444444445, 85.16666666666667, 85.16666666666667, 85.33333333333333, 85.33333333333333, 85.36944444444444, 85.36944444444444, 85.375, 85.375, 86.01944444444445, 86.01944444444445, 86.35277777777777, 86.35277777777777, 86.36944444444444, 86.36944444444444, 86.025, 86.025, 85.725, 85.725, 86.025, 86.025, 86.31666666666666, 86.31666666666666, 86.69166666666666, 86.69166666666666, 86.38055555555556, 86.38055555555556, 86.79444444444445, 86.79444444444445, 86.81944444444444, 86.81944444444444, 87.01388888888889, 87.01388888888889, 86.7611111111111, 86.7611111111111, 86.96666666666667, 86.96666666666667, 86.75, 86.75, 87.09444444444445, 87.09444444444445, 87.03611111111111, 87.03611111111111, 87.22222222222223, 87.22222222222223, 87.34166666666667, 87.34166666666667, 87.20277777777778, 87.20277777777778, 87.18611111111112, 87.18611111111112, 87.16388888888889, 87.16388888888889, 87.825, 87.825, 87.59722222222223, 87.59722222222223, 87.60833333333333, 87.60833333333333, 87.58611111111111, 87.58611111111111, 87.6138888888889, 87.6138888888889, 87.65833333333333, 87.65833333333333, 87.78888888888889, 87.78888888888889, 87.82777777777778, 87.82777777777778, 87.90555555555555, 87.90555555555555, 88.01944444444445, 88.01944444444445, 87.61944444444444, 87.61944444444444, 87.96944444444445, 87.96944444444445, 88.09166666666667, 88.09166666666667, 88.04444444444445, 88.04444444444445, 88.08611111111111, 88.08611111111111, 88.11944444444444, 88.11944444444444, 88.00555555555556, 88.00555555555556, 87.89166666666667, 87.89166666666667, 87.66111111111111, 87.66111111111111, 87.88055555555556, 87.88055555555556, 87.9, 87.9, 88.225, 88.225, 88.35277777777777, 88.35277777777777, 88.05555555555556, 88.05555555555556, 88.12222222222222, 88.12222222222222, 88.09722222222223, 88.09722222222223, 88.50555555555556, 88.50555555555556, 88.45277777777778, 88.45277777777778, 88.48055555555555, 88.48055555555555, 88.33611111111111, 88.33611111111111, 88.475, 88.475, 88.33888888888889, 88.33888888888889, 88.55, 88.55, 88.53333333333333, 88.53333333333333, 88.44722222222222, 88.44722222222222, 88.59166666666667, 88.59166666666667, 88.53333333333333, 88.53333333333333, 88.43333333333334, 88.43333333333334, 88.56944444444444, 88.56944444444444, 88.65, 88.65, 88.57222222222222, 88.57222222222222, 88.55833333333334, 88.55833333333334, 88.79166666666667, 88.79166666666667, 88.66944444444445, 88.66944444444445, 88.65, 88.65, 88.80555555555556, 88.80555555555556, 88.75833333333334, 88.75833333333334, 88.76666666666667, 88.76666666666667, 88.85555555555555, 88.85555555555555, 88.85555555555555, 88.85555555555555]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.158, Test loss: 1.678, Test accuracy: 39.52
Round   1, Train loss: 1.771, Test loss: 1.460, Test accuracy: 46.97
Round   2, Train loss: 1.599, Test loss: 1.332, Test accuracy: 51.65
Round   3, Train loss: 1.477, Test loss: 1.230, Test accuracy: 56.45
Round   4, Train loss: 1.396, Test loss: 1.152, Test accuracy: 59.29
Round   5, Train loss: 1.287, Test loss: 1.098, Test accuracy: 61.31
Round   6, Train loss: 1.257, Test loss: 1.060, Test accuracy: 62.89
Round   7, Train loss: 1.178, Test loss: 1.011, Test accuracy: 64.42
Round   8, Train loss: 1.138, Test loss: 0.986, Test accuracy: 64.17
Round   9, Train loss: 1.080, Test loss: 0.951, Test accuracy: 66.51
Round  10, Train loss: 1.070, Test loss: 0.935, Test accuracy: 66.79
Round  11, Train loss: 1.036, Test loss: 0.879, Test accuracy: 69.89
Round  12, Train loss: 0.978, Test loss: 0.863, Test accuracy: 70.14
Round  13, Train loss: 0.976, Test loss: 0.850, Test accuracy: 70.61
Round  14, Train loss: 0.906, Test loss: 0.833, Test accuracy: 71.19
Round  15, Train loss: 0.881, Test loss: 0.821, Test accuracy: 71.39
Round  16, Train loss: 0.887, Test loss: 0.802, Test accuracy: 72.11
Round  17, Train loss: 0.840, Test loss: 0.805, Test accuracy: 72.01
Round  18, Train loss: 0.861, Test loss: 0.789, Test accuracy: 73.01
Round  19, Train loss: 0.806, Test loss: 0.787, Test accuracy: 72.89
Round  20, Train loss: 0.782, Test loss: 0.774, Test accuracy: 73.63
Round  21, Train loss: 0.799, Test loss: 0.770, Test accuracy: 73.70
Round  22, Train loss: 0.814, Test loss: 0.764, Test accuracy: 74.23
Round  23, Train loss: 0.739, Test loss: 0.749, Test accuracy: 74.43
Round  24, Train loss: 0.728, Test loss: 0.756, Test accuracy: 74.33
Round  25, Train loss: 0.739, Test loss: 0.752, Test accuracy: 74.11
Round  26, Train loss: 0.707, Test loss: 0.744, Test accuracy: 74.79
Round  27, Train loss: 0.707, Test loss: 0.741, Test accuracy: 75.40
Round  28, Train loss: 0.709, Test loss: 0.733, Test accuracy: 74.97
Round  29, Train loss: 0.674, Test loss: 0.736, Test accuracy: 75.23
Round  30, Train loss: 0.676, Test loss: 0.737, Test accuracy: 75.39
Round  31, Train loss: 0.662, Test loss: 0.729, Test accuracy: 75.53
Round  32, Train loss: 0.648, Test loss: 0.724, Test accuracy: 75.94
Round  33, Train loss: 0.647, Test loss: 0.729, Test accuracy: 75.51
Round  34, Train loss: 0.656, Test loss: 0.710, Test accuracy: 76.23
Round  35, Train loss: 0.652, Test loss: 0.711, Test accuracy: 76.70
Round  36, Train loss: 0.621, Test loss: 0.713, Test accuracy: 76.47
Round  37, Train loss: 0.595, Test loss: 0.723, Test accuracy: 76.26
Round  38, Train loss: 0.644, Test loss: 0.716, Test accuracy: 76.05
Round  39, Train loss: 0.600, Test loss: 0.712, Test accuracy: 76.52
Round  40, Train loss: 0.606, Test loss: 0.701, Test accuracy: 76.95
Round  41, Train loss: 0.628, Test loss: 0.700, Test accuracy: 76.78
Round  42, Train loss: 0.562, Test loss: 0.703, Test accuracy: 76.68
Round  43, Train loss: 0.606, Test loss: 0.702, Test accuracy: 76.67
Round  44, Train loss: 0.584, Test loss: 0.691, Test accuracy: 77.16
Round  45, Train loss: 0.551, Test loss: 0.710, Test accuracy: 77.02
Round  46, Train loss: 0.572, Test loss: 0.693, Test accuracy: 77.19
Round  47, Train loss: 0.563, Test loss: 0.703, Test accuracy: 77.36
Round  48, Train loss: 0.543, Test loss: 0.702, Test accuracy: 77.56
Round  49, Train loss: 0.514, Test loss: 0.697, Test accuracy: 77.73
Round  50, Train loss: 0.544, Test loss: 0.701, Test accuracy: 77.62
Round  51, Train loss: 0.523, Test loss: 0.704, Test accuracy: 77.11
Round  52, Train loss: 0.516, Test loss: 0.705, Test accuracy: 77.49
Round  53, Train loss: 0.526, Test loss: 0.706, Test accuracy: 77.56
Round  54, Train loss: 0.527, Test loss: 0.702, Test accuracy: 77.63
Round  55, Train loss: 0.555, Test loss: 0.686, Test accuracy: 78.16
Round  56, Train loss: 0.521, Test loss: 0.691, Test accuracy: 78.08
Round  57, Train loss: 0.525, Test loss: 0.690, Test accuracy: 77.96
Round  58, Train loss: 0.469, Test loss: 0.702, Test accuracy: 78.31
Round  59, Train loss: 0.512, Test loss: 0.695, Test accuracy: 77.73
Round  60, Train loss: 0.520, Test loss: 0.694, Test accuracy: 78.00
Round  61, Train loss: 0.511, Test loss: 0.702, Test accuracy: 78.13
Round  62, Train loss: 0.530, Test loss: 0.699, Test accuracy: 77.69
Round  63, Train loss: 0.479, Test loss: 0.701, Test accuracy: 77.55
Round  64, Train loss: 0.479, Test loss: 0.695, Test accuracy: 78.00
Round  65, Train loss: 0.464, Test loss: 0.705, Test accuracy: 78.03
Round  66, Train loss: 0.460, Test loss: 0.722, Test accuracy: 77.40
Round  67, Train loss: 0.489, Test loss: 0.723, Test accuracy: 77.68
Round  68, Train loss: 0.524, Test loss: 0.686, Test accuracy: 78.42
Round  69, Train loss: 0.486, Test loss: 0.694, Test accuracy: 77.92
Round  70, Train loss: 0.489, Test loss: 0.698, Test accuracy: 78.46
Round  71, Train loss: 0.458, Test loss: 0.706, Test accuracy: 78.06
Round  72, Train loss: 0.441, Test loss: 0.707, Test accuracy: 78.08
Round  73, Train loss: 0.436, Test loss: 0.704, Test accuracy: 78.45
Round  74, Train loss: 0.491, Test loss: 0.684, Test accuracy: 78.47
Round  75, Train loss: 0.459, Test loss: 0.704, Test accuracy: 78.49
Round  76, Train loss: 0.448, Test loss: 0.699, Test accuracy: 78.53
Round  77, Train loss: 0.399, Test loss: 0.716, Test accuracy: 78.35
Round  78, Train loss: 0.470, Test loss: 0.696, Test accuracy: 78.62
Round  79, Train loss: 0.477, Test loss: 0.691, Test accuracy: 78.73
Round  80, Train loss: 0.425, Test loss: 0.696, Test accuracy: 78.68
Round  81, Train loss: 0.420, Test loss: 0.709, Test accuracy: 78.67
Round  82, Train loss: 0.408, Test loss: 0.728, Test accuracy: 78.65
Round  83, Train loss: 0.424, Test loss: 0.710, Test accuracy: 78.57
Round  84, Train loss: 0.439, Test loss: 0.699, Test accuracy: 78.54
Round  85, Train loss: 0.454, Test loss: 0.697, Test accuracy: 78.78
Round  86, Train loss: 0.437, Test loss: 0.710, Test accuracy: 78.74
Round  87, Train loss: 0.453, Test loss: 0.691, Test accuracy: 78.94
Round  88, Train loss: 0.430, Test loss: 0.699, Test accuracy: 78.46
Round  89, Train loss: 0.444, Test loss: 0.688, Test accuracy: 78.59
Round  90, Train loss: 0.416, Test loss: 0.711, Test accuracy: 79.00
Round  91, Train loss: 0.411, Test loss: 0.700, Test accuracy: 78.92
Round  92, Train loss: 0.406, Test loss: 0.703, Test accuracy: 78.67
Round  93, Train loss: 0.440, Test loss: 0.703, Test accuracy: 78.37
Round  94, Train loss: 0.422, Test loss: 0.698, Test accuracy: 78.58
Round  95, Train loss: 0.403, Test loss: 0.700, Test accuracy: 78.59
Round  96, Train loss: 0.413, Test loss: 0.711, Test accuracy: 78.78
Round  97, Train loss: 0.363, Test loss: 0.711, Test accuracy: 79.02
Round  98, Train loss: 0.391, Test loss: 0.719, Test accuracy: 78.58
Round  99, Train loss: 0.388, Test loss: 0.702, Test accuracy: 78.88
Final Round, Train loss: 0.321, Test loss: 0.707, Test accuracy: 79.33
Average accuracy final 10 rounds: 78.73925
7872.652192831039
[12.545121908187866, 23.85123610496521, 35.13083553314209, 46.36467790603638, 57.69316649436951, 69.0070858001709, 80.26910400390625, 91.68743133544922, 103.03697657585144, 114.37576174736023, 125.73037672042847, 137.03097414970398, 148.37915921211243, 159.61228775978088, 170.88279056549072, 182.19569444656372, 193.51985001564026, 204.84315872192383, 216.1338152885437, 227.38856720924377, 238.65946531295776, 249.94306635856628, 261.27307868003845, 272.5849964618683, 283.9788942337036, 295.39636182785034, 306.8588285446167, 318.20855021476746, 329.5365688800812, 340.8556089401245, 352.071964263916, 363.31560349464417, 374.53966760635376, 385.80205821990967, 397.0347023010254, 408.2914500236511, 419.5205364227295, 430.7273805141449, 441.9047532081604, 453.0746703147888, 464.3394820690155, 475.5822043418884, 486.9075994491577, 498.2272312641144, 509.4572296142578, 520.7643203735352, 532.0125417709351, 543.2994265556335, 554.5929179191589, 565.8518755435944, 577.1442375183105, 588.3791983127594, 599.6541323661804, 610.8374035358429, 622.0949199199677, 633.4111642837524, 644.5809631347656, 655.7777938842773, 666.8696956634521, 677.9900982379913, 689.0524258613586, 700.2362217903137, 711.4935233592987, 722.7432520389557, 734.0051531791687, 745.3490891456604, 756.7996826171875, 768.1409611701965, 779.488046169281, 791.0065829753876, 802.2778537273407, 813.5685524940491, 825.1095430850983, 836.4472949504852, 847.7499544620514, 860.4482259750366, 871.7779977321625, 883.0719790458679, 894.3540105819702, 905.5964524745941, 916.9674446582794, 928.2338485717773, 939.4379844665527, 950.809602022171, 962.0907771587372, 973.4226689338684, 984.6996450424194, 995.8794088363647, 1007.1639933586121, 1018.4013981819153, 1029.557354927063, 1040.7602071762085, 1052.0009922981262, 1063.2182033061981, 1074.3678441047668, 1085.587914466858, 1096.7474076747894, 1107.950679063797, 1119.1868560314178, 1130.3895454406738, 1133.2031128406525]
[39.5225, 46.9675, 51.645, 56.4475, 59.2875, 61.31, 62.8925, 64.4225, 64.17, 66.5125, 66.79, 69.8875, 70.1375, 70.6075, 71.19, 71.395, 72.105, 72.01, 73.0075, 72.8925, 73.6325, 73.7025, 74.2275, 74.4275, 74.335, 74.11, 74.79, 75.4025, 74.9675, 75.235, 75.3875, 75.53, 75.935, 75.51, 76.2325, 76.6975, 76.465, 76.2625, 76.05, 76.5225, 76.9475, 76.785, 76.6775, 76.6675, 77.16, 77.0225, 77.185, 77.3625, 77.5575, 77.7275, 77.6175, 77.11, 77.4925, 77.565, 77.63, 78.155, 78.075, 77.9625, 78.3075, 77.735, 77.995, 78.1325, 77.6875, 77.5525, 77.995, 78.025, 77.4025, 77.68, 78.425, 77.92, 78.4575, 78.0625, 78.085, 78.455, 78.475, 78.49, 78.5325, 78.3475, 78.625, 78.73, 78.68, 78.67, 78.6475, 78.5725, 78.5375, 78.7775, 78.7425, 78.935, 78.46, 78.5875, 78.9975, 78.9225, 78.67, 78.37, 78.5825, 78.595, 78.7775, 79.02, 78.5825, 78.875, 79.3275]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.304, Test loss: 2.303, Test accuracy: 10.00
Round   0, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.00
Round   1, Train loss: 2.304, Test loss: 2.303, Test accuracy: 10.00
Round   1, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.00
Round   2, Train loss: 2.304, Test loss: 2.303, Test accuracy: 10.01
Round   2, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.01
Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.02
Round   3, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 10.01
Round   4, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.04
Round   4, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 10.02
Round   5, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.03
Round   5, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 10.02
Round   6, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.05
Round   6, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.06
Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.06
Round   7, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.02
Round   8, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.09
Round   8, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 10.10
Round   9, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.10
Round   9, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 10.10
Round  10, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.13
Round  10, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.12
Round  11, Train loss: 2.301, Test loss: 2.300, Test accuracy: 10.21
Round  11, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 10.24
Round  12, Train loss: 2.301, Test loss: 2.300, Test accuracy: 10.30
Round  12, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 10.27
Round  13, Train loss: 2.300, Test loss: 2.299, Test accuracy: 10.34
Round  13, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 10.33
Round  14, Train loss: 2.300, Test loss: 2.299, Test accuracy: 10.39
Round  14, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 10.74
Round  15, Train loss: 2.300, Test loss: 2.299, Test accuracy: 10.35
Round  15, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 10.75
Round  16, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.74
Round  16, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 11.21
Round  17, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.74
Round  17, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 11.26
Round  18, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.62
Round  18, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 11.19
Round  19, Train loss: 2.299, Test loss: 2.297, Test accuracy: 11.22
Round  19, Global train loss: 2.299, Global test loss: 2.296, Global test accuracy: 12.02
Round  20, Train loss: 2.298, Test loss: 2.297, Test accuracy: 11.37
Round  20, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 12.11
Round  21, Train loss: 2.298, Test loss: 2.297, Test accuracy: 11.38
Round  21, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 12.08
Round  22, Train loss: 2.297, Test loss: 2.296, Test accuracy: 11.71
Round  22, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 12.47
Round  23, Train loss: 2.297, Test loss: 2.296, Test accuracy: 12.14
Round  23, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 12.97
Round  24, Train loss: 2.297, Test loss: 2.296, Test accuracy: 12.64
Round  24, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 13.95
Round  25, Train loss: 2.297, Test loss: 2.295, Test accuracy: 13.16
Round  25, Global train loss: 2.297, Global test loss: 2.294, Global test accuracy: 13.37
Round  26, Train loss: 2.296, Test loss: 2.295, Test accuracy: 13.21
Round  26, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 13.37
Round  27, Train loss: 2.296, Test loss: 2.295, Test accuracy: 13.29
Round  27, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 13.23
Round  28, Train loss: 2.296, Test loss: 2.294, Test accuracy: 13.59
Round  28, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 13.91
Round  29, Train loss: 2.295, Test loss: 2.294, Test accuracy: 13.59
Round  29, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 13.55
Round  30, Train loss: 2.295, Test loss: 2.293, Test accuracy: 13.81
Round  30, Global train loss: 2.295, Global test loss: 2.292, Global test accuracy: 14.23
Round  31, Train loss: 2.295, Test loss: 2.293, Test accuracy: 13.84
Round  31, Global train loss: 2.295, Global test loss: 2.292, Global test accuracy: 14.20
Round  32, Train loss: 2.294, Test loss: 2.292, Test accuracy: 14.05
Round  32, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 15.01
Round  33, Train loss: 2.293, Test loss: 2.292, Test accuracy: 14.41
Round  33, Global train loss: 2.293, Global test loss: 2.291, Global test accuracy: 15.20
Round  34, Train loss: 2.293, Test loss: 2.292, Test accuracy: 14.52
Round  34, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 16.23
Round  35, Train loss: 2.292, Test loss: 2.291, Test accuracy: 15.01
Round  35, Global train loss: 2.292, Global test loss: 2.290, Global test accuracy: 15.74
Round  36, Train loss: 2.292, Test loss: 2.291, Test accuracy: 15.04
Round  36, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 16.22
Round  37, Train loss: 2.292, Test loss: 2.290, Test accuracy: 15.52
Round  37, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 17.01
Round  38, Train loss: 2.292, Test loss: 2.290, Test accuracy: 15.75
Round  38, Global train loss: 2.292, Global test loss: 2.288, Global test accuracy: 17.25
Round  39, Train loss: 2.291, Test loss: 2.289, Test accuracy: 16.28
Round  39, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 18.07
Round  40, Train loss: 2.291, Test loss: 2.288, Test accuracy: 16.92
Round  40, Global train loss: 2.291, Global test loss: 2.287, Global test accuracy: 18.36
Round  41, Train loss: 2.290, Test loss: 2.288, Test accuracy: 17.54
Round  41, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 18.45
Round  42, Train loss: 2.290, Test loss: 2.287, Test accuracy: 17.81
Round  42, Global train loss: 2.290, Global test loss: 2.286, Global test accuracy: 18.41
Round  43, Train loss: 2.290, Test loss: 2.287, Test accuracy: 18.02
Round  43, Global train loss: 2.290, Global test loss: 2.286, Global test accuracy: 18.33
Round  44, Train loss: 2.289, Test loss: 2.286, Test accuracy: 18.06
Round  44, Global train loss: 2.289, Global test loss: 2.285, Global test accuracy: 18.88
Round  45, Train loss: 2.289, Test loss: 2.285, Test accuracy: 18.39
Round  45, Global train loss: 2.289, Global test loss: 2.284, Global test accuracy: 20.16
Round  46, Train loss: 2.289, Test loss: 2.285, Test accuracy: 18.58
Round  46, Global train loss: 2.289, Global test loss: 2.284, Global test accuracy: 19.81
Round  47, Train loss: 2.288, Test loss: 2.284, Test accuracy: 18.80
Round  47, Global train loss: 2.288, Global test loss: 2.283, Global test accuracy: 19.50
Round  48, Train loss: 2.287, Test loss: 2.284, Test accuracy: 18.82
Round  48, Global train loss: 2.287, Global test loss: 2.282, Global test accuracy: 19.51
Round  49, Train loss: 2.288, Test loss: 2.284, Test accuracy: 18.63
Round  49, Global train loss: 2.288, Global test loss: 2.282, Global test accuracy: 18.57
Round  50, Train loss: 2.286, Test loss: 2.283, Test accuracy: 18.67
Round  50, Global train loss: 2.286, Global test loss: 2.282, Global test accuracy: 19.23
Round  51, Train loss: 2.286, Test loss: 2.282, Test accuracy: 18.79
Round  51, Global train loss: 2.286, Global test loss: 2.281, Global test accuracy: 19.79
Round  52, Train loss: 2.286, Test loss: 2.282, Test accuracy: 19.04
Round  52, Global train loss: 2.286, Global test loss: 2.280, Global test accuracy: 19.74
Round  53, Train loss: 2.285, Test loss: 2.281, Test accuracy: 18.97
Round  53, Global train loss: 2.285, Global test loss: 2.280, Global test accuracy: 19.03
Round  54, Train loss: 2.284, Test loss: 2.280, Test accuracy: 19.01
Round  54, Global train loss: 2.284, Global test loss: 2.279, Global test accuracy: 19.34
Round  55, Train loss: 2.284, Test loss: 2.280, Test accuracy: 18.98
Round  55, Global train loss: 2.284, Global test loss: 2.278, Global test accuracy: 19.94
Round  56, Train loss: 2.284, Test loss: 2.279, Test accuracy: 19.26
Round  56, Global train loss: 2.284, Global test loss: 2.277, Global test accuracy: 20.39
Round  57, Train loss: 2.283, Test loss: 2.279, Test accuracy: 19.21
Round  57, Global train loss: 2.283, Global test loss: 2.277, Global test accuracy: 20.12
Round  58, Train loss: 2.283, Test loss: 2.278, Test accuracy: 19.35
Round  58, Global train loss: 2.283, Global test loss: 2.276, Global test accuracy: 19.84
Round  59, Train loss: 2.282, Test loss: 2.278, Test accuracy: 19.50
Round  59, Global train loss: 2.282, Global test loss: 2.275, Global test accuracy: 19.66
Round  60, Train loss: 2.281, Test loss: 2.277, Test accuracy: 19.49
Round  60, Global train loss: 2.281, Global test loss: 2.274, Global test accuracy: 19.62
Round  61, Train loss: 2.280, Test loss: 2.276, Test accuracy: 19.32
Round  61, Global train loss: 2.280, Global test loss: 2.274, Global test accuracy: 19.29
Round  62, Train loss: 2.280, Test loss: 2.276, Test accuracy: 19.20
Round  62, Global train loss: 2.280, Global test loss: 2.273, Global test accuracy: 19.55
Round  63, Train loss: 2.280, Test loss: 2.275, Test accuracy: 19.18
Round  63, Global train loss: 2.280, Global test loss: 2.273, Global test accuracy: 19.57
Round  64, Train loss: 2.279, Test loss: 2.274, Test accuracy: 19.11
Round  64, Global train loss: 2.279, Global test loss: 2.272, Global test accuracy: 19.40
Round  65, Train loss: 2.278, Test loss: 2.274, Test accuracy: 19.15
Round  65, Global train loss: 2.278, Global test loss: 2.271, Global test accuracy: 19.25
Round  66, Train loss: 2.278, Test loss: 2.273, Test accuracy: 19.36
Round  66, Global train loss: 2.278, Global test loss: 2.270, Global test accuracy: 19.55
Round  67, Train loss: 2.277, Test loss: 2.272, Test accuracy: 19.48
Round  67, Global train loss: 2.277, Global test loss: 2.269, Global test accuracy: 19.90
Round  68, Train loss: 2.276, Test loss: 2.271, Test accuracy: 19.63
Round  68, Global train loss: 2.276, Global test loss: 2.268, Global test accuracy: 19.75
Round  69, Train loss: 2.276, Test loss: 2.270, Test accuracy: 19.70
Round  69, Global train loss: 2.276, Global test loss: 2.267, Global test accuracy: 19.45
Round  70, Train loss: 2.274, Test loss: 2.269, Test accuracy: 19.75
Round  70, Global train loss: 2.274, Global test loss: 2.266, Global test accuracy: 19.81
Round  71, Train loss: 2.274, Test loss: 2.268, Test accuracy: 19.80
Round  71, Global train loss: 2.274, Global test loss: 2.266, Global test accuracy: 19.92
Round  72, Train loss: 2.273, Test loss: 2.267, Test accuracy: 19.92
Round  72, Global train loss: 2.273, Global test loss: 2.265, Global test accuracy: 19.77
Round  73, Train loss: 2.273, Test loss: 2.266, Test accuracy: 19.81
Round  73, Global train loss: 2.273, Global test loss: 2.264, Global test accuracy: 19.32
Round  74, Train loss: 2.272, Test loss: 2.265, Test accuracy: 19.72
Round  74, Global train loss: 2.272, Global test loss: 2.263, Global test accuracy: 19.00
Round  75, Train loss: 2.272, Test loss: 2.265, Test accuracy: 19.62
Round  75, Global train loss: 2.272, Global test loss: 2.262, Global test accuracy: 18.99
Round  76, Train loss: 2.272, Test loss: 2.264, Test accuracy: 19.30
Round  76, Global train loss: 2.272, Global test loss: 2.261, Global test accuracy: 19.20
Round  77, Train loss: 2.271, Test loss: 2.263, Test accuracy: 19.33
Round  77, Global train loss: 2.271, Global test loss: 2.261, Global test accuracy: 19.47
Round  78, Train loss: 2.270, Test loss: 2.261, Test accuracy: 19.37
Round  78, Global train loss: 2.270, Global test loss: 2.260, Global test accuracy: 19.54
Round  79, Train loss: 2.269, Test loss: 2.261, Test accuracy: 19.51
Round  79, Global train loss: 2.269, Global test loss: 2.258, Global test accuracy: 19.69
Round  80, Train loss: 2.269, Test loss: 2.260, Test accuracy: 19.67
Round  80, Global train loss: 2.269, Global test loss: 2.258, Global test accuracy: 19.98
Round  81, Train loss: 2.268, Test loss: 2.259, Test accuracy: 20.03
Round  81, Global train loss: 2.268, Global test loss: 2.257, Global test accuracy: 20.39
Round  82, Train loss: 2.267, Test loss: 2.258, Test accuracy: 20.45
Round  82, Global train loss: 2.267, Global test loss: 2.255, Global test accuracy: 20.73
Round  83, Train loss: 2.265, Test loss: 2.257, Test accuracy: 20.54
Round  83, Global train loss: 2.265, Global test loss: 2.254, Global test accuracy: 21.26
Round  84, Train loss: 2.265, Test loss: 2.256, Test accuracy: 20.35
Round  84, Global train loss: 2.265, Global test loss: 2.253, Global test accuracy: 21.33
Round  85, Train loss: 2.265, Test loss: 2.255, Test accuracy: 20.71
Round  85, Global train loss: 2.265, Global test loss: 2.251, Global test accuracy: 21.52
Round  86, Train loss: 2.263, Test loss: 2.253, Test accuracy: 20.88
Round  86, Global train loss: 2.263, Global test loss: 2.250, Global test accuracy: 21.32
Round  87, Train loss: 2.263, Test loss: 2.251, Test accuracy: 21.17
Round  87, Global train loss: 2.263, Global test loss: 2.248, Global test accuracy: 21.29
Round  88, Train loss: 2.263, Test loss: 2.250, Test accuracy: 21.24
Round  88, Global train loss: 2.263, Global test loss: 2.247, Global test accuracy: 20.63
Round  89, Train loss: 2.262, Test loss: 2.249, Test accuracy: 21.21
Round  89, Global train loss: 2.262, Global test loss: 2.246, Global test accuracy: 21.04
Round  90, Train loss: 2.259, Test loss: 2.248, Test accuracy: 20.92
Round  90, Global train loss: 2.259, Global test loss: 2.244, Global test accuracy: 20.56
Round  91, Train loss: 2.260, Test loss: 2.247, Test accuracy: 20.57
Round  91, Global train loss: 2.260, Global test loss: 2.244, Global test accuracy: 20.27
Round  92, Train loss: 2.260, Test loss: 2.246, Test accuracy: 20.58
Round  92, Global train loss: 2.260, Global test loss: 2.242, Global test accuracy: 20.30
Round  93, Train loss: 2.257, Test loss: 2.245, Test accuracy: 20.73
Round  93, Global train loss: 2.257, Global test loss: 2.240, Global test accuracy: 20.43
Round  94, Train loss: 2.258, Test loss: 2.243, Test accuracy: 20.57
Round  94, Global train loss: 2.258, Global test loss: 2.239, Global test accuracy: 20.42
Round  95, Train loss: 2.256, Test loss: 2.241, Test accuracy: 20.52
Round  95, Global train loss: 2.256, Global test loss: 2.237, Global test accuracy: 20.31
Round  96, Train loss: 2.256, Test loss: 2.239, Test accuracy: 20.16
Round  96, Global train loss: 2.256, Global test loss: 2.235, Global test accuracy: 19.54
Round  97, Train loss: 2.255, Test loss: 2.238, Test accuracy: 20.12
Round  97, Global train loss: 2.255, Global test loss: 2.234, Global test accuracy: 19.66
Round  98, Train loss: 2.253, Test loss: 2.236, Test accuracy: 19.79
Round  98, Global train loss: 2.253, Global test loss: 2.232, Global test accuracy: 19.77
Round  99, Train loss: 2.252, Test loss: 2.234, Test accuracy: 19.91
Round  99, Global train loss: 2.252, Global test loss: 2.230, Global test accuracy: 20.16
Round 100, Train loss: 2.250, Test loss: 2.232, Test accuracy: 20.10
Round 100, Global train loss: 2.250, Global test loss: 2.228, Global test accuracy: 19.66
Round 101, Train loss: 2.251, Test loss: 2.231, Test accuracy: 19.97
Round 101, Global train loss: 2.251, Global test loss: 2.226, Global test accuracy: 19.63
Round 102, Train loss: 2.250, Test loss: 2.230, Test accuracy: 19.97
Round 102, Global train loss: 2.250, Global test loss: 2.225, Global test accuracy: 20.04
Round 103, Train loss: 2.247, Test loss: 2.228, Test accuracy: 20.07
Round 103, Global train loss: 2.247, Global test loss: 2.221, Global test accuracy: 19.68
Round 104, Train loss: 2.247, Test loss: 2.226, Test accuracy: 20.17
Round 104, Global train loss: 2.247, Global test loss: 2.220, Global test accuracy: 20.61
Round 105, Train loss: 2.248, Test loss: 2.225, Test accuracy: 20.42
Round 105, Global train loss: 2.248, Global test loss: 2.219, Global test accuracy: 20.68
Round 106, Train loss: 2.245, Test loss: 2.223, Test accuracy: 20.61
Round 106, Global train loss: 2.245, Global test loss: 2.218, Global test accuracy: 21.13
Round 107, Train loss: 2.245, Test loss: 2.222, Test accuracy: 20.84
Round 107, Global train loss: 2.245, Global test loss: 2.217, Global test accuracy: 21.18
Round 108, Train loss: 2.243, Test loss: 2.221, Test accuracy: 20.54
Round 108, Global train loss: 2.243, Global test loss: 2.215, Global test accuracy: 19.79
Round 109, Train loss: 2.241, Test loss: 2.219, Test accuracy: 20.38
Round 109, Global train loss: 2.241, Global test loss: 2.213, Global test accuracy: 19.33
Round 110, Train loss: 2.244, Test loss: 2.218, Test accuracy: 20.24
Round 110, Global train loss: 2.244, Global test loss: 2.212, Global test accuracy: 19.59
Round 111, Train loss: 2.239, Test loss: 2.215, Test accuracy: 20.36
Round 111, Global train loss: 2.239, Global test loss: 2.212, Global test accuracy: 20.57
Round 112, Train loss: 2.240, Test loss: 2.215, Test accuracy: 20.65
Round 112, Global train loss: 2.240, Global test loss: 2.211, Global test accuracy: 21.14
Round 113, Train loss: 2.238, Test loss: 2.213, Test accuracy: 20.85
Round 113, Global train loss: 2.238, Global test loss: 2.209, Global test accuracy: 22.24
Round 114, Train loss: 2.237, Test loss: 2.212, Test accuracy: 21.50
Round 114, Global train loss: 2.237, Global test loss: 2.206, Global test accuracy: 22.59
Round 115, Train loss: 2.238, Test loss: 2.210, Test accuracy: 21.90
Round 115, Global train loss: 2.238, Global test loss: 2.204, Global test accuracy: 22.38
Round 116, Train loss: 2.236, Test loss: 2.208, Test accuracy: 21.85
Round 116, Global train loss: 2.236, Global test loss: 2.202, Global test accuracy: 21.86
Round 117, Train loss: 2.236, Test loss: 2.205, Test accuracy: 21.52
Round 117, Global train loss: 2.236, Global test loss: 2.200, Global test accuracy: 21.43
Round 118, Train loss: 2.235, Test loss: 2.203, Test accuracy: 21.19
Round 118, Global train loss: 2.235, Global test loss: 2.199, Global test accuracy: 20.89
Round 119, Train loss: 2.236, Test loss: 2.202, Test accuracy: 21.11
Round 119, Global train loss: 2.236, Global test loss: 2.197, Global test accuracy: 21.18
Round 120, Train loss: 2.232, Test loss: 2.201, Test accuracy: 21.06
Round 120, Global train loss: 2.232, Global test loss: 2.196, Global test accuracy: 21.50
Round 121, Train loss: 2.230, Test loss: 2.200, Test accuracy: 21.25
Round 121, Global train loss: 2.230, Global test loss: 2.196, Global test accuracy: 21.25
Round 122, Train loss: 2.229, Test loss: 2.199, Test accuracy: 21.24
Round 122, Global train loss: 2.229, Global test loss: 2.194, Global test accuracy: 20.92
Round 123, Train loss: 2.228, Test loss: 2.197, Test accuracy: 21.62
Round 123, Global train loss: 2.228, Global test loss: 2.191, Global test accuracy: 22.02
Round 124, Train loss: 2.231, Test loss: 2.195, Test accuracy: 21.95
Round 124, Global train loss: 2.231, Global test loss: 2.189, Global test accuracy: 22.47
Round 125, Train loss: 2.228, Test loss: 2.193, Test accuracy: 22.01
Round 125, Global train loss: 2.228, Global test loss: 2.188, Global test accuracy: 22.62
Round 126, Train loss: 2.224, Test loss: 2.189, Test accuracy: 22.56
Round 126, Global train loss: 2.224, Global test loss: 2.185, Global test accuracy: 24.62
Round 127, Train loss: 2.226, Test loss: 2.188, Test accuracy: 22.51
Round 127, Global train loss: 2.226, Global test loss: 2.185, Global test accuracy: 23.34
Round 128, Train loss: 2.226, Test loss: 2.187, Test accuracy: 22.22
Round 128, Global train loss: 2.226, Global test loss: 2.185, Global test accuracy: 22.95
Round 129, Train loss: 2.222, Test loss: 2.186, Test accuracy: 21.93
Round 129, Global train loss: 2.222, Global test loss: 2.183, Global test accuracy: 22.57
Round 130, Train loss: 2.221, Test loss: 2.184, Test accuracy: 22.30
Round 130, Global train loss: 2.221, Global test loss: 2.181, Global test accuracy: 23.72
Round 131, Train loss: 2.221, Test loss: 2.183, Test accuracy: 23.14
Round 131, Global train loss: 2.221, Global test loss: 2.178, Global test accuracy: 25.80
Round 132, Train loss: 2.222, Test loss: 2.181, Test accuracy: 23.62
Round 132, Global train loss: 2.222, Global test loss: 2.177, Global test accuracy: 26.12
Round 133, Train loss: 2.218, Test loss: 2.182, Test accuracy: 23.40
Round 133, Global train loss: 2.218, Global test loss: 2.177, Global test accuracy: 25.60
Round 134, Train loss: 2.219, Test loss: 2.180, Test accuracy: 23.61
Round 134, Global train loss: 2.219, Global test loss: 2.174, Global test accuracy: 25.43
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 23.18
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 18.82
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 16.90
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 14.14
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 12.76
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 11.41
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 10.65
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 10.65
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 10.65
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 10.65
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 10.65
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 10.65
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 10.65
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

15881.830345869064
[5.506284713745117, 10.816937685012817, 16.108338594436646, 21.395159482955933, 26.710369110107422, 32.009188652038574, 37.314597368240356, 42.62277340888977, 47.88827395439148, 53.20505213737488, 58.49290609359741, 63.78109812736511, 69.08135890960693, 74.36930084228516, 79.68608856201172, 85.01335978507996, 90.30720806121826, 95.55354738235474, 100.79856634140015, 106.03562092781067, 111.26002311706543, 116.50468420982361, 121.73697590827942, 126.9559965133667, 132.18439102172852, 137.42818570137024, 142.46070194244385, 147.8010618686676, 153.10128664970398, 158.39052891731262, 163.68819880485535, 169.02756714820862, 174.3048186302185, 179.60002827644348, 184.88567233085632, 190.204416513443, 195.49895524978638, 200.76546716690063, 206.09596467018127, 211.42316699028015, 216.75986981391907, 222.07567930221558, 227.40442895889282, 232.73397541046143, 238.05970549583435, 243.4101665019989, 248.75359272956848, 254.10374975204468, 259.43452763557434, 264.753897190094, 270.091050863266, 275.42513728141785, 280.7218027114868, 286.0155746936798, 291.269211769104, 296.5658929347992, 301.868604183197, 307.18396973609924, 312.47572898864746, 317.76340317726135, 323.03823709487915, 328.3263211250305, 333.60520482063293, 338.92843985557556, 344.2019953727722, 349.478253364563, 354.7092080116272, 359.99980759620667, 365.3086779117584, 370.62716603279114, 375.9479913711548, 381.2504427433014, 386.5476076602936, 391.85711908340454, 397.1496834754944, 402.45076394081116, 407.7593619823456, 413.06643319129944, 418.37035632133484, 423.69267225265503, 429.0188989639282, 434.36321353912354, 439.7131493091583, 445.0669975280762, 450.42642521858215, 455.7579050064087, 461.11307168006897, 466.4097831249237, 471.7127854824066, 477.04929757118225, 482.4439251422882, 487.80505084991455, 493.1774561405182, 498.54657220840454, 503.91294717788696, 509.24432349205017, 514.5884389877319, 519.9151265621185, 525.2309248447418, 530.554102897644, 535.875944852829, 541.1681764125824, 545.8411254882812, 550.4847455024719, 555.1350409984589, 559.807097196579, 564.48424077034, 569.153139591217, 573.8313689231873, 578.4896306991577, 583.1436474323273, 587.8436393737793, 592.4917964935303, 597.1647098064423, 601.8549437522888, 606.5305254459381, 611.2240614891052, 615.9158108234406, 620.562584400177, 625.2350926399231, 629.9021356105804, 634.58571600914, 639.2503504753113, 643.9512870311737, 648.6043858528137, 653.299751996994, 658.6110651493073, 663.9536805152893, 669.3167991638184, 673.9824078083038, 678.6304695606232, 683.2825198173523, 687.9859642982483, 692.6902928352356, 697.3709952831268, 702.0374958515167, 706.7021369934082, 711.3799295425415, 716.0304598808289, 720.694568157196, 725.3149757385254, 729.9556686878204, 734.645432472229, 739.3379652500153, 744.0036284923553, 748.6900572776794, 753.3474926948547, 757.9889400005341, 762.6772627830505, 767.3714964389801, 772.1466403007507, 777.0285265445709, 781.8220961093903, 786.7902636528015, 791.6526248455048, 796.5799291133881, 801.5131194591522, 806.2813146114349, 810.9919641017914, 815.769939661026, 820.5270075798035, 825.3236181735992, 830.0872640609741, 834.8505079746246, 839.648449420929, 844.442405462265, 849.2328610420227, 853.9827036857605, 858.7766318321228, 863.548540353775, 868.3224651813507, 873.082355260849, 877.8282673358917, 882.6952042579651, 887.421186208725, 892.1398408412933, 896.886313199997, 901.6089787483215, 906.3738446235657, 911.1372201442719, 915.8436324596405, 920.5954191684723, 925.349326133728, 930.1172301769257, 934.9242622852325, 939.7098519802094, 944.5484375953674, 949.3589990139008, 954.210307598114, 958.9512648582458, 963.7448456287384, 968.4359471797943, 973.133460521698, 977.886696100235, 982.6051068305969, 987.2981588840485, 992.0496609210968, 996.7884886264801, 1001.5161147117615, 1006.2987804412842, 1011.0406515598297, 1015.8563132286072, 1020.6330006122589, 1025.4052112102509, 1030.1951727867126, 1034.9118995666504, 1040.1328508853912, 1045.297434091568, 1050.4063820838928, 1055.4866354465485, 1060.7000613212585, 1065.9533185958862, 1071.2435009479523, 1076.52423787117, 1081.7523686885834, 1086.8561825752258, 1092.1995768547058, 1097.482274055481, 1102.7719585895538, 1108.027791261673, 1113.2649943828583, 1118.4743206501007, 1123.6957392692566, 1128.9467513561249, 1134.2103214263916, 1139.45925116539, 1144.7282502651215, 1150.012640953064, 1155.2606842517853, 1160.5408163070679, 1165.8043682575226, 1171.0322561264038, 1176.1279044151306, 1181.2349390983582, 1186.5358109474182, 1191.8589932918549, 1197.136459350586, 1202.3137936592102, 1207.4790632724762, 1212.7571403980255, 1217.8620145320892, 1222.9579439163208, 1227.6488189697266, 1232.3468191623688, 1237.0678732395172, 1241.802927017212, 1246.4879884719849, 1251.1957058906555, 1255.9153170585632, 1260.6096057891846, 1265.3138206005096, 1270.0068485736847, 1274.7273380756378, 1279.4626927375793, 1284.1919565200806, 1288.913693189621, 1293.6096966266632, 1298.341023683548, 1303.063349723816, 1307.7810945510864, 1312.5276012420654, 1317.2240600585938, 1321.9191341400146, 1326.584969997406, 1331.2544541358948, 1335.980224609375, 1340.6897230148315, 1345.3918347358704, 1350.0692348480225, 1354.7644419670105, 1359.4913220405579, 1364.2127940654755, 1368.905133008957, 1373.617954492569, 1378.3199546337128, 1383.0156409740448, 1387.722160100937, 1392.4001955986023, 1397.0989651679993, 1401.8181881904602, 1406.522786617279, 1411.1995074748993, 1415.8977439403534, 1420.5742802619934, 1425.2352821826935, 1429.8936784267426, 1434.549739599228, 1439.2275850772858, 1443.9680089950562, 1448.6885948181152, 1453.392992258072, 1458.0912730693817, 1462.783490896225, 1467.5028903484344, 1472.1878938674927, 1476.8610439300537, 1481.5494103431702, 1486.232741355896, 1490.8785738945007, 1495.5325763225555, 1497.8894023895264]
[10.0, 10.0025, 10.0075, 10.0225, 10.035, 10.0325, 10.0475, 10.0575, 10.085, 10.095, 10.135, 10.21, 10.3025, 10.3425, 10.39, 10.3475, 10.74, 10.745, 10.6175, 11.225, 11.365, 11.3825, 11.7075, 12.1375, 12.6375, 13.165, 13.2125, 13.285, 13.5925, 13.585, 13.8125, 13.835, 14.055, 14.4125, 14.5225, 15.0125, 15.0425, 15.5225, 15.7525, 16.2775, 16.9175, 17.5375, 17.81, 18.015, 18.06, 18.39, 18.5825, 18.8025, 18.825, 18.6325, 18.6725, 18.7875, 19.035, 18.97, 19.0075, 18.9775, 19.26, 19.215, 19.35, 19.505, 19.4875, 19.32, 19.1975, 19.185, 19.105, 19.15, 19.3625, 19.485, 19.6325, 19.7, 19.745, 19.8, 19.9225, 19.8125, 19.7175, 19.6225, 19.305, 19.3275, 19.3675, 19.51, 19.6725, 20.0325, 20.455, 20.5375, 20.3525, 20.7125, 20.8775, 21.1725, 21.24, 21.215, 20.9225, 20.5675, 20.5775, 20.735, 20.5725, 20.525, 20.1575, 20.1225, 19.7925, 19.9075, 20.1, 19.9725, 19.9675, 20.0675, 20.1675, 20.42, 20.6125, 20.845, 20.54, 20.3825, 20.24, 20.365, 20.6525, 20.85, 21.495, 21.8975, 21.85, 21.515, 21.1925, 21.1125, 21.06, 21.25, 21.2375, 21.625, 21.955, 22.0075, 22.5625, 22.5075, 22.2175, 21.9275, 22.2975, 23.135, 23.6175, 23.3975, 23.605, 23.175, 18.82, 16.9, 14.1375, 12.7625, 11.4125, 10.6525, 10.6525, 10.6525, 10.6525, 10.6525, 10.6525, 10.6525, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
