nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.153, Test loss: 4.214, Test accuracy: 40.87
Final Round, Global train loss: 0.153, Global test loss: 1.614, Global test accuracy: 48.08
Average accuracy final 10 rounds: 41.009499999999996 

Average global accuracy final 10 rounds: 47.254000000000005 

3973.1415796279907
[1.3184142112731934, 2.6368284225463867, 3.872910499572754, 5.108992576599121, 6.358166217803955, 7.607339859008789, 8.86343502998352, 10.119530200958252, 11.37372350692749, 12.627916812896729, 13.880046129226685, 15.13217544555664, 16.38448214530945, 17.636788845062256, 18.884851455688477, 20.132914066314697, 21.38869571685791, 22.644477367401123, 23.89133381843567, 25.138190269470215, 26.388182163238525, 27.638174057006836, 28.892081260681152, 30.14598846435547, 31.4031662940979, 32.66034412384033, 33.9192578792572, 35.17817163467407, 36.43812298774719, 37.69807434082031, 38.955204248428345, 40.21233415603638, 41.47130608558655, 42.73027801513672, 43.99457263946533, 45.258867263793945, 46.51800990104675, 47.77715253829956, 49.024434328079224, 50.27171611785889, 51.52018594741821, 52.76865577697754, 54.01274919509888, 55.256842613220215, 56.51409649848938, 57.771350383758545, 59.030126333236694, 60.288902282714844, 61.56025791168213, 62.831613540649414, 64.10936403274536, 65.38711452484131, 66.6685140132904, 67.9499135017395, 69.21994233131409, 70.48997116088867, 71.76627469062805, 73.04257822036743, 74.30637454986572, 75.57017087936401, 76.84717535972595, 78.12417984008789, 79.40823721885681, 80.69229459762573, 81.96974229812622, 83.24718999862671, 84.51974701881409, 85.79230403900146, 87.06966423988342, 88.34702444076538, 89.62698364257812, 90.90694284439087, 92.18619990348816, 93.46545696258545, 94.74985146522522, 96.03424596786499, 97.31546807289124, 98.59669017791748, 99.87258219718933, 101.14847421646118, 102.42639756202698, 103.70432090759277, 104.97640824317932, 106.24849557876587, 107.52265357971191, 108.79681158065796, 110.06970357894897, 111.34259557723999, 112.6151831150055, 113.887770652771, 115.17996835708618, 116.47216606140137, 117.74693584442139, 119.0217056274414, 120.30216550827026, 121.58262538909912, 122.86389112472534, 124.14515686035156, 125.4135262966156, 126.68189573287964, 127.96144127845764, 129.24098682403564, 130.52224349975586, 131.80350017547607, 133.08040356636047, 134.35730695724487, 135.64539289474487, 136.93347883224487, 138.20461630821228, 139.4757537841797, 140.7672393321991, 142.0587248802185, 143.34079122543335, 144.6228575706482, 145.90380120277405, 147.1847448348999, 148.473730802536, 149.76271677017212, 151.03655672073364, 152.31039667129517, 153.59187531471252, 154.87335395812988, 156.16668701171875, 157.46002006530762, 158.73598885536194, 160.01195764541626, 161.3001003265381, 162.5882430076599, 163.86817622184753, 165.14810943603516, 166.42050290107727, 167.69289636611938, 168.97742748260498, 170.26195859909058, 171.53535318374634, 172.8087477684021, 174.0871934890747, 175.36563920974731, 176.64790105819702, 177.93016290664673, 179.20437455177307, 180.4785861968994, 181.7482590675354, 183.0179319381714, 184.29228115081787, 185.56663036346436, 186.84854078292847, 188.13045120239258, 189.4081997871399, 190.6859483718872, 191.95400047302246, 193.22205257415771, 194.50528836250305, 195.7885241508484, 197.06814169883728, 198.34775924682617, 199.62430143356323, 200.9008436203003, 202.193017244339, 203.48519086837769, 204.75480842590332, 206.02442598342896, 207.30064868927002, 208.57687139511108, 209.86244201660156, 211.14801263809204, 212.42196106910706, 213.69590950012207, 214.97937679290771, 216.26284408569336, 217.545884847641, 218.82892560958862, 220.11602687835693, 221.40312814712524, 222.6804723739624, 223.95781660079956, 225.22850489616394, 226.49919319152832, 227.77374958992004, 229.04830598831177, 230.33263874053955, 231.61697149276733, 232.89634656906128, 234.17572164535522, 235.44437289237976, 236.7130241394043, 237.9914367198944, 239.26984930038452, 240.55689907073975, 241.84394884109497, 243.12391805648804, 244.4038872718811, 245.70583128929138, 247.00777530670166, 248.28401279449463, 249.5602502822876, 250.83835124969482, 252.11645221710205, 253.40106534957886, 254.68567848205566, 257.2465739250183, 259.80746936798096]
[24.4325, 24.4325, 28.1875, 28.1875, 30.67, 30.67, 31.495, 31.495, 32.0875, 32.0875, 33.015, 33.015, 33.795, 33.795, 34.155, 34.155, 35.36, 35.36, 35.395, 35.395, 36.05, 36.05, 36.2625, 36.2625, 37.3425, 37.3425, 37.67, 37.67, 38.485, 38.485, 38.7, 38.7, 38.9525, 38.9525, 39.005, 39.005, 39.1875, 39.1875, 39.4375, 39.4375, 39.5125, 39.5125, 39.8925, 39.8925, 39.745, 39.745, 40.1075, 40.1075, 39.9925, 39.9925, 39.57, 39.57, 39.8475, 39.8475, 40.5775, 40.5775, 40.6775, 40.6775, 40.7325, 40.7325, 40.85, 40.85, 40.9825, 40.9825, 41.2575, 41.2575, 40.9575, 40.9575, 40.64, 40.64, 40.59, 40.59, 40.975, 40.975, 41.0225, 41.0225, 41.0975, 41.0975, 40.8725, 40.8725, 41.085, 41.085, 40.7025, 40.7025, 40.9475, 40.9475, 40.975, 40.975, 40.47, 40.47, 40.4275, 40.4275, 40.3425, 40.3425, 40.525, 40.525, 40.2725, 40.2725, 40.81, 40.81, 41.09, 41.09, 41.2475, 41.2475, 41.005, 41.005, 41.0375, 41.0375, 41.0875, 41.0875, 41.215, 41.215, 40.9525, 40.9525, 41.065, 41.065, 41.34, 41.34, 41.45, 41.45, 41.155, 41.155, 40.6925, 40.6925, 40.745, 40.745, 40.5975, 40.5975, 40.6575, 40.6575, 40.7675, 40.7675, 40.5125, 40.5125, 40.5325, 40.5325, 40.6725, 40.6725, 40.875, 40.875, 41.0975, 41.0975, 40.89, 40.89, 40.975, 40.975, 40.91, 40.91, 40.8675, 40.8675, 40.7175, 40.7175, 40.805, 40.805, 40.985, 40.985, 40.5625, 40.5625, 40.6525, 40.6525, 40.905, 40.905, 41.2175, 41.2175, 41.165, 41.165, 40.8925, 40.8925, 40.895, 40.895, 40.5025, 40.5025, 40.5225, 40.5225, 40.7625, 40.7625, 41.0975, 41.0975, 41.1025, 41.1025, 40.895, 40.895, 41.0275, 41.0275, 40.8, 40.8, 41.0875, 41.0875, 41.1, 41.1, 41.0475, 41.0475, 41.015, 41.015, 40.955, 40.955, 41.025, 41.025, 41.1425, 41.1425, 40.865, 40.865]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.351, Test loss: 1.695, Test accuracy: 61.66
Final Round, Global train loss: 0.351, Global test loss: 1.144, Global test accuracy: 68.83
Average accuracy final 10 rounds: 62.3945 

Average global accuracy final 10 rounds: 69.02125 

3804.3268892765045
[1.463594913482666, 2.927189826965332, 4.173524379730225, 5.419858932495117, 6.650309801101685, 7.880760669708252, 9.120553016662598, 10.360345363616943, 11.602229833602905, 12.844114303588867, 14.099700927734375, 15.355287551879883, 16.607558488845825, 17.859829425811768, 19.12105107307434, 20.382272720336914, 21.64318823814392, 22.904103755950928, 24.16223168373108, 25.42035961151123, 26.66050672531128, 27.900653839111328, 29.147387266159058, 30.394120693206787, 31.647735357284546, 32.901350021362305, 34.15661334991455, 35.4118766784668, 36.671815395355225, 37.93175411224365, 39.18736815452576, 40.44298219680786, 41.705098390579224, 42.967214584350586, 44.20917296409607, 45.45113134384155, 46.70123767852783, 47.95134401321411, 49.205763816833496, 50.46018362045288, 51.71396040916443, 52.96773719787598, 54.21506428718567, 55.46239137649536, 56.71750283241272, 57.97261428833008, 59.22022771835327, 60.467841148376465, 61.708505630493164, 62.94917011260986, 64.19775652885437, 65.44634294509888, 66.69353866577148, 67.94073438644409, 69.19258713722229, 70.44443988800049, 71.68748736381531, 72.93053483963013, 74.17293953895569, 75.41534423828125, 76.65422248840332, 77.89310073852539, 78.8913164138794, 79.8895320892334, 80.90176630020142, 81.91400051116943, 82.91121220588684, 83.90842390060425, 84.90693497657776, 85.90544605255127, 86.90452647209167, 87.90360689163208, 88.90160512924194, 89.8996033668518, 91.05259871482849, 92.20559406280518, 93.3627872467041, 94.51998043060303, 95.68466758728027, 96.84935474395752, 98.0292580127716, 99.2091612815857, 100.37217569351196, 101.53519010543823, 102.69658422470093, 103.85797834396362, 104.99985003471375, 106.14172172546387, 107.30225086212158, 108.4627799987793, 109.62393498420715, 110.78508996963501, 111.95326352119446, 113.1214370727539, 114.27861499786377, 115.43579292297363, 116.59357500076294, 117.75135707855225, 118.914381980896, 120.07740688323975, 121.23703050613403, 122.39665412902832, 123.55346894264221, 124.7102837562561, 125.86521005630493, 127.02013635635376, 128.1813781261444, 129.34261989593506, 130.50107836723328, 131.6595368385315, 132.81790709495544, 133.9762773513794, 135.13382935523987, 136.29138135910034, 137.44093489646912, 138.5904884338379, 139.74878358840942, 140.90707874298096, 142.06049823760986, 143.21391773223877, 144.37494564056396, 145.53597354888916, 146.69025683403015, 147.84454011917114, 148.99522137641907, 150.145902633667, 151.30075454711914, 152.4556064605713, 153.6178424358368, 154.7800784111023, 155.94966793060303, 157.11925745010376, 158.28715085983276, 159.45504426956177, 160.6096272468567, 161.7642102241516, 162.9231469631195, 164.0820837020874, 165.2503523826599, 166.41862106323242, 167.57567763328552, 168.73273420333862, 169.89003157615662, 171.0473289489746, 172.20494627952576, 173.3625636100769, 174.51426696777344, 175.66597032546997, 176.82908415794373, 177.99219799041748, 179.15261960029602, 180.31304121017456, 181.4745810031891, 182.6361207962036, 183.79763007164001, 184.95913934707642, 186.11162066459656, 187.2641019821167, 188.4253430366516, 189.58658409118652, 190.74337697029114, 191.90016984939575, 193.05406284332275, 194.20795583724976, 195.3656463623047, 196.52333688735962, 197.68039631843567, 198.83745574951172, 199.99222087860107, 201.14698600769043, 202.3090465068817, 203.471107006073, 204.6357536315918, 205.8004002571106, 206.95791602134705, 208.1154317855835, 209.28272533416748, 210.45001888275146, 211.60696840286255, 212.76391792297363, 213.9127335548401, 215.06154918670654, 216.21824073791504, 217.37493228912354, 218.53060126304626, 219.686270236969, 220.8504238128662, 222.01457738876343, 223.1758418083191, 224.33710622787476, 225.48056602478027, 226.6240258216858, 227.78584909439087, 228.94767236709595, 230.10562562942505, 231.26357889175415, 232.41803669929504, 233.57249450683594, 234.7394504547119, 235.9064064025879, 238.24010705947876, 240.57380771636963]
[26.095, 26.095, 30.525, 30.525, 33.425, 33.425, 35.6, 35.6, 35.8875, 35.8875, 38.13, 38.13, 39.55, 39.55, 40.325, 40.325, 41.965, 41.965, 43.43, 43.43, 44.695, 44.695, 45.615, 45.615, 46.7825, 46.7825, 46.63, 46.63, 47.31, 47.31, 48.49, 48.49, 49.0525, 49.0525, 49.815, 49.815, 50.425, 50.425, 51.07, 51.07, 52.17, 52.17, 52.2225, 52.2225, 52.4925, 52.4925, 52.5825, 52.5825, 52.515, 52.515, 53.3775, 53.3775, 53.3575, 53.3575, 54.005, 54.005, 54.9775, 54.9775, 55.3725, 55.3725, 55.655, 55.655, 55.8175, 55.8175, 55.675, 55.675, 55.16, 55.16, 55.505, 55.505, 55.9025, 55.9025, 56.44, 56.44, 56.745, 56.745, 57.2775, 57.2775, 58.095, 58.095, 58.4175, 58.4175, 58.6325, 58.6325, 58.945, 58.945, 58.865, 58.865, 59.02, 59.02, 59.015, 59.015, 59.0575, 59.0575, 59.295, 59.295, 59.2525, 59.2525, 59.52, 59.52, 59.425, 59.425, 59.5725, 59.5725, 59.6225, 59.6225, 60.0825, 60.0825, 60.1475, 60.1475, 59.8225, 59.8225, 60.075, 60.075, 60.135, 60.135, 60.1375, 60.1375, 60.825, 60.825, 60.915, 60.915, 60.7325, 60.7325, 60.9225, 60.9225, 60.945, 60.945, 60.945, 60.945, 60.7025, 60.7025, 60.805, 60.805, 60.79, 60.79, 61.15, 61.15, 61.18, 61.18, 61.4375, 61.4375, 61.875, 61.875, 61.89, 61.89, 61.85, 61.85, 61.8225, 61.8225, 61.895, 61.895, 61.9575, 61.9575, 62.1025, 62.1025, 61.9275, 61.9275, 61.925, 61.925, 61.6275, 61.6275, 61.9375, 61.9375, 62.1475, 62.1475, 62.475, 62.475, 62.54, 62.54, 62.79, 62.79, 62.6225, 62.6225, 62.5075, 62.5075, 62.8525, 62.8525, 62.73, 62.73, 62.69, 62.69, 62.3725, 62.3725, 62.39, 62.39, 62.3775, 62.3775, 62.3825, 62.3825, 62.33, 62.33, 62.525, 62.525, 62.5775, 62.5775, 62.0125, 62.0125, 62.2875, 62.2875, 61.66, 61.66]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.480, Test loss: 1.324, Test accuracy: 62.23
Average accuracy final 10 rounds: 62.30125000000001 

2705.616861343384
[1.3299810886383057, 2.6599621772766113, 3.726213216781616, 4.792464256286621, 5.84838342666626, 6.904302597045898, 7.965980291366577, 9.027657985687256, 10.088757514953613, 11.14985704421997, 12.204471349716187, 13.259085655212402, 14.325732231140137, 15.392378807067871, 16.45671033859253, 17.521041870117188, 18.577317237854004, 19.63359260559082, 20.69105863571167, 21.74852466583252, 22.81809902191162, 23.887673377990723, 24.951359510421753, 26.015045642852783, 27.07797861099243, 28.14091157913208, 29.19736933708191, 30.25382709503174, 31.320019006729126, 32.386210918426514, 33.44217538833618, 34.49813985824585, 35.55541682243347, 36.612693786621094, 37.67417025566101, 38.73564672470093, 39.79645776748657, 40.85726881027222, 41.909759521484375, 42.96225023269653, 44.014899492263794, 45.067548751831055, 46.12426447868347, 47.18098020553589, 48.22936940193176, 49.27775859832764, 50.348381757736206, 51.419004917144775, 52.490161180496216, 53.561317443847656, 54.62711787223816, 55.69291830062866, 56.758856534957886, 57.82479476928711, 58.88161277770996, 59.93843078613281, 60.9958176612854, 62.05320453643799, 63.10635447502136, 64.15950441360474, 65.21132111549377, 66.26313781738281, 67.32161593437195, 68.38009405136108, 69.42959356307983, 70.47909307479858, 71.53871393203735, 72.59833478927612, 73.65232372283936, 74.70631265640259, 75.75497889518738, 76.80364513397217, 77.86104893684387, 78.91845273971558, 79.98391914367676, 81.04938554763794, 82.10983514785767, 83.17028474807739, 84.22376465797424, 85.2772445678711, 86.3291687965393, 87.38109302520752, 88.44587922096252, 89.51066541671753, 90.58251905441284, 91.65437269210815, 92.70480418205261, 93.75523567199707, 94.81996941566467, 95.88470315933228, 96.94532442092896, 98.00594568252563, 99.0830008983612, 100.16005611419678, 101.21470952033997, 102.26936292648315, 103.31835770606995, 104.36735248565674, 105.42524147033691, 106.48313045501709, 107.49816846847534, 108.5132064819336, 109.46802115440369, 110.42283582687378, 111.37822127342224, 112.3336067199707, 113.29781794548035, 114.26202917098999, 115.21641540527344, 116.17080163955688, 117.13921809196472, 118.10763454437256, 119.06335139274597, 120.01906824111938, 120.98309063911438, 121.94711303710938, 122.9041633605957, 123.86121368408203, 124.8126904964447, 125.76416730880737, 126.72804546356201, 127.69192361831665, 128.639408826828, 129.58689403533936, 130.5503408908844, 131.51378774642944, 132.47572231292725, 133.43765687942505, 134.39879965782166, 135.35994243621826, 136.32444405555725, 137.28894567489624, 138.2440938949585, 139.19924211502075, 140.15827775001526, 141.11731338500977, 142.0884246826172, 143.0595359802246, 144.11363792419434, 145.16773986816406, 146.2334542274475, 147.29916858673096, 148.35702180862427, 149.41487503051758, 150.4797670841217, 151.54465913772583, 152.62721419334412, 153.7097692489624, 154.76945281028748, 155.82913637161255, 156.89269709587097, 157.9562578201294, 159.02236199378967, 160.08846616744995, 161.15804481506348, 162.227623462677, 163.28820872306824, 164.34879398345947, 165.40759897232056, 166.46640396118164, 167.41692447662354, 168.36744499206543, 169.31960797309875, 170.27177095413208, 171.22414588928223, 172.17652082443237, 173.13046503067017, 174.08440923690796, 175.02559518814087, 175.96678113937378, 176.9368190765381, 177.9068570137024, 178.87155079841614, 179.83624458312988, 180.79522824287415, 181.7542119026184, 182.72067785263062, 183.68714380264282, 184.65784811973572, 185.6285524368286, 186.58427095413208, 187.53998947143555, 188.50804042816162, 189.4760913848877, 190.43607568740845, 191.3960599899292, 192.35075163841248, 193.30544328689575, 194.2608392238617, 195.21623516082764, 196.1723906993866, 197.12854623794556, 198.0879716873169, 199.04739713668823, 200.01048064231873, 200.97356414794922, 201.93092322349548, 202.88828229904175, 203.850115776062, 204.81194925308228, 206.70902037620544, 208.6060914993286]
[18.035, 18.035, 25.2025, 25.2025, 27.725, 27.725, 29.58, 29.58, 32.3, 32.3, 35.33, 35.33, 37.7875, 37.7875, 39.61, 39.61, 41.205, 41.205, 42.51, 42.51, 43.765, 43.765, 44.1575, 44.1575, 45.425, 45.425, 44.9875, 44.9875, 46.9625, 46.9625, 47.835, 47.835, 48.7825, 48.7825, 48.8875, 48.8875, 49.7675, 49.7675, 50.4725, 50.4725, 51.0775, 51.0775, 51.28, 51.28, 51.465, 51.465, 52.3425, 52.3425, 52.4575, 52.4575, 52.575, 52.575, 53.255, 53.255, 54.035, 54.035, 54.2925, 54.2925, 55.2675, 55.2675, 55.5025, 55.5025, 55.35, 55.35, 55.6625, 55.6625, 55.8125, 55.8125, 56.01, 56.01, 56.3475, 56.3475, 57.1275, 57.1275, 57.6175, 57.6175, 57.5075, 57.5075, 57.605, 57.605, 58.23, 58.23, 58.0925, 58.0925, 58.7125, 58.7125, 58.4225, 58.4225, 58.8475, 58.8475, 59.13, 59.13, 58.165, 58.165, 58.775, 58.775, 58.4775, 58.4775, 59.0575, 59.0575, 58.8975, 58.8975, 59.41, 59.41, 60.5225, 60.5225, 59.8975, 59.8975, 60.0525, 60.0525, 60.3475, 60.3475, 60.22, 60.22, 61.0625, 61.0625, 61.06, 61.06, 61.025, 61.025, 60.8625, 60.8625, 61.3625, 61.3625, 61.5625, 61.5625, 61.8975, 61.8975, 61.4525, 61.4525, 61.29, 61.29, 61.8925, 61.8925, 61.5375, 61.5375, 61.39, 61.39, 61.5375, 61.5375, 61.58, 61.58, 61.66, 61.66, 61.1975, 61.1975, 61.2375, 61.2375, 61.0825, 61.0825, 61.2925, 61.2925, 61.285, 61.285, 60.97, 60.97, 61.125, 61.125, 61.2375, 61.2375, 61.3125, 61.3125, 61.4075, 61.4075, 61.9, 61.9, 62.005, 62.005, 61.8525, 61.8525, 61.825, 61.825, 61.915, 61.915, 61.8975, 61.8975, 62.25, 62.25, 62.065, 62.065, 62.375, 62.375, 62.2925, 62.2925, 62.3425, 62.3425, 62.4175, 62.4175, 62.1225, 62.1225, 62.2225, 62.2225, 61.8675, 61.8675, 62.2625, 62.2625, 62.625, 62.625, 62.485, 62.485, 62.2275, 62.2275]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.316, Test loss: 1.658, Test accuracy: 60.79
Average accuracy final 10 rounds: 61.26974999999999 

2939.7534070014954
[1.4107944965362549, 2.8215889930725098, 4.126194715499878, 5.430800437927246, 6.694250583648682, 7.957700729370117, 9.229868173599243, 10.50203561782837, 11.773598670959473, 13.045161724090576, 14.320266962051392, 15.595372200012207, 16.87213683128357, 18.14890146255493, 19.422929286956787, 20.696957111358643, 21.974935054779053, 23.252912998199463, 24.527540683746338, 25.802168369293213, 27.077502727508545, 28.352837085723877, 29.61932873725891, 30.885820388793945, 32.15354800224304, 33.42127561569214, 34.68830394744873, 35.95533227920532, 37.22433304786682, 38.49333381652832, 39.76905846595764, 41.04478311538696, 42.31490707397461, 43.585031032562256, 44.85438251495361, 46.12373399734497, 47.397011518478394, 48.670289039611816, 49.93879508972168, 51.20730113983154, 52.47592067718506, 53.744540214538574, 55.015454053878784, 56.286367893218994, 57.56394863128662, 58.84152936935425, 60.1234917640686, 61.40545415878296, 62.68064832687378, 63.9558424949646, 65.22906589508057, 66.50228929519653, 67.77232027053833, 69.04235124588013, 70.31863737106323, 71.59492349624634, 72.86801815032959, 74.14111280441284, 75.41241097450256, 76.68370914459229, 77.95564079284668, 79.22757244110107, 80.49618721008301, 81.76480197906494, 83.03740859031677, 84.3100152015686, 85.59299206733704, 86.87596893310547, 88.16637063026428, 89.4567723274231, 90.73292684555054, 92.00908136367798, 93.28642392158508, 94.56376647949219, 95.84062314033508, 97.11747980117798, 98.40122985839844, 99.6849799156189, 100.97485542297363, 102.26473093032837, 103.54065442085266, 104.81657791137695, 106.0939519405365, 107.37132596969604, 108.64965224266052, 109.927978515625, 111.20244812965393, 112.47691774368286, 113.75467300415039, 115.03242826461792, 116.30822420120239, 117.58402013778687, 118.86159420013428, 120.13916826248169, 121.41312098503113, 122.68707370758057, 123.96696209907532, 125.24685049057007, 126.54789781570435, 127.84894514083862, 129.13758969306946, 130.4262342453003, 131.69019556045532, 132.95415687561035, 134.21667170524597, 135.4791865348816, 136.746652841568, 138.0141191482544, 139.2229881286621, 140.43185710906982, 141.63899445533752, 142.84613180160522, 144.05610537528992, 145.2660789489746, 146.54144930839539, 147.81681966781616, 149.07715153694153, 150.3374834060669, 151.60007739067078, 152.86267137527466, 154.1386115550995, 155.41455173492432, 156.6845715045929, 157.95459127426147, 159.23151516914368, 160.50843906402588, 161.78170585632324, 163.0549726486206, 164.33414030075073, 165.61330795288086, 166.89116430282593, 168.169020652771, 169.4490249156952, 170.72902917861938, 172.01044344902039, 173.2918577194214, 174.561199426651, 175.83054113388062, 177.09927773475647, 178.36801433563232, 179.6403489112854, 180.91268348693848, 182.18415355682373, 183.45562362670898, 184.7234857082367, 185.9913477897644, 187.26474595069885, 188.5381441116333, 189.80322313308716, 191.06830215454102, 192.32770538330078, 193.58710861206055, 194.84577465057373, 196.1044406890869, 197.36397647857666, 198.6235122680664, 199.89202070236206, 201.16052913665771, 202.43243288993835, 203.704336643219, 204.96456265449524, 206.22478866577148, 207.48556447029114, 208.7463402748108, 210.01268100738525, 211.27902173995972, 212.5416886806488, 213.8043556213379, 215.0737063884735, 216.34305715560913, 217.61006021499634, 218.87706327438354, 220.14295315742493, 221.4088430404663, 222.6786503791809, 223.9484577178955, 225.2213191986084, 226.4941806793213, 227.76675605773926, 229.03933143615723, 230.30935788154602, 231.57938432693481, 232.85276770591736, 234.1261510848999, 235.40102577209473, 236.67590045928955, 237.94946098327637, 239.22302150726318, 240.49332094192505, 241.7636203765869, 243.0297040939331, 244.2957878112793, 245.56594514846802, 246.83610248565674, 248.0486397743225, 249.26117706298828, 250.463294506073, 251.66541194915771, 252.86378526687622, 254.06215858459473, 256.0106723308563, 257.9591860771179]
[25.9525, 25.9525, 30.84, 30.84, 35.5425, 35.5425, 38.87, 38.87, 42.0475, 42.0475, 44.115, 44.115, 46.1425, 46.1425, 47.3925, 47.3925, 47.595, 47.595, 49.24, 49.24, 49.9775, 49.9775, 50.7, 50.7, 51.1025, 51.1025, 52.365, 52.365, 52.8825, 52.8825, 53.2475, 53.2475, 54.84, 54.84, 55.2825, 55.2825, 55.4375, 55.4375, 55.8775, 55.8775, 56.4625, 56.4625, 56.5175, 56.5175, 57.29, 57.29, 57.525, 57.525, 57.96, 57.96, 58.18, 58.18, 59.0375, 59.0375, 59.345, 59.345, 59.2725, 59.2725, 59.4275, 59.4275, 59.0975, 59.0975, 59.52, 59.52, 59.615, 59.615, 60.1125, 60.1125, 60.4075, 60.4075, 60.2975, 60.2975, 59.6475, 59.6475, 60.7625, 60.7625, 61.1775, 61.1775, 60.905, 60.905, 61.2375, 61.2375, 61.5375, 61.5375, 61.415, 61.415, 61.5475, 61.5475, 61.255, 61.255, 60.96, 60.96, 61.2925, 61.2925, 61.2675, 61.2675, 61.4825, 61.4825, 61.0625, 61.0625, 61.39, 61.39, 60.685, 60.685, 61.5625, 61.5625, 61.73, 61.73, 61.535, 61.535, 61.245, 61.245, 61.0825, 61.0825, 61.4175, 61.4175, 60.96, 60.96, 61.3975, 61.3975, 61.125, 61.125, 61.7475, 61.7475, 61.2, 61.2, 61.19, 61.19, 61.565, 61.565, 61.425, 61.425, 61.4475, 61.4475, 61.8075, 61.8075, 61.2375, 61.2375, 61.59, 61.59, 61.3975, 61.3975, 61.5725, 61.5725, 61.4475, 61.4475, 61.7025, 61.7025, 61.8475, 61.8475, 61.7275, 61.7275, 61.3925, 61.3925, 61.58, 61.58, 61.03, 61.03, 61.6575, 61.6575, 61.4125, 61.4125, 61.66, 61.66, 61.805, 61.805, 61.865, 61.865, 61.5725, 61.5725, 61.6575, 61.6575, 61.0375, 61.0375, 61.4925, 61.4925, 61.1925, 61.1925, 61.04, 61.04, 61.5475, 61.5475, 61.1225, 61.1225, 61.565, 61.565, 61.6425, 61.6425, 61.3575, 61.3575, 61.3875, 61.3875, 61.185, 61.185, 60.755, 60.755, 61.215, 61.215, 60.92, 60.92, 60.79, 60.79]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.093, Test loss: 4.309, Test accuracy: 42.52
Average accuracy final 10 rounds: 42.1585 

2757.988128900528
[1.4996459484100342, 2.9992918968200684, 4.007063627243042, 5.014835357666016, 6.021334886550903, 7.027834415435791, 8.036618947982788, 9.045403480529785, 10.050469875335693, 11.055536270141602, 12.0567147731781, 13.0578932762146, 14.072183609008789, 15.086473941802979, 16.095290184020996, 17.104106426239014, 18.11314821243286, 19.12218999862671, 20.13716959953308, 21.152149200439453, 22.155126094818115, 23.158102989196777, 24.1610164642334, 25.16392993927002, 26.170743227005005, 27.17755651473999, 28.186712503433228, 29.195868492126465, 30.203938722610474, 31.212008953094482, 32.221548318862915, 33.23108768463135, 34.23479509353638, 35.238502502441406, 36.244593143463135, 37.25068378448486, 38.256269216537476, 39.26185464859009, 40.27071213722229, 41.27956962585449, 42.289762020111084, 43.299954414367676, 44.3010892868042, 45.30222415924072, 46.3082389831543, 47.31425380706787, 48.322279930114746, 49.33030605316162, 50.336363077163696, 51.34242010116577, 52.350043296813965, 53.35766649246216, 54.36434555053711, 55.37102460861206, 56.36948871612549, 57.367952823638916, 58.37611198425293, 59.38427114486694, 60.389503955841064, 61.394736766815186, 62.401655197143555, 63.408573627471924, 64.41390442848206, 65.41923522949219, 66.42482089996338, 67.43040657043457, 68.42692828178406, 69.42344999313354, 70.429936170578, 71.43642234802246, 72.44115114212036, 73.44587993621826, 74.45074963569641, 75.45561933517456, 76.46069622039795, 77.46577310562134, 78.46705913543701, 79.46834516525269, 80.46934199333191, 81.47033882141113, 82.49660229682922, 83.52286577224731, 84.52715134620667, 85.53143692016602, 86.53882431983948, 87.54621171951294, 88.72014570236206, 89.89407968521118, 91.07137894630432, 92.24867820739746, 93.42046165466309, 94.59224510192871, 95.76598763465881, 96.93973016738892, 98.11465191841125, 99.2895736694336, 100.45891427993774, 101.6282548904419, 102.78648614883423, 103.94471740722656, 105.11848759651184, 106.29225778579712, 107.47051167488098, 108.64876556396484, 109.75674223899841, 110.86471891403198, 111.97703337669373, 113.08934783935547, 114.2060399055481, 115.32273197174072, 116.48344826698303, 117.64416456222534, 118.76217436790466, 119.88018417358398, 121.00538325309753, 122.13058233261108, 123.25780177116394, 124.3850212097168, 125.54531192779541, 126.70560264587402, 127.87015676498413, 129.03471088409424, 130.20933294296265, 131.38395500183105, 132.55253624916077, 133.72111749649048, 134.89136290550232, 136.06160831451416, 137.2346043586731, 138.40760040283203, 139.5706503391266, 140.73370027542114, 141.90149092674255, 143.06928157806396, 144.23946928977966, 145.40965700149536, 146.58650851249695, 147.76336002349854, 148.93513870239258, 150.10691738128662, 151.27944326400757, 152.45196914672852, 153.62266540527344, 154.79336166381836, 155.96019196510315, 157.12702226638794, 158.29970574378967, 159.4723892211914, 160.63250160217285, 161.7926139831543, 162.95064640045166, 164.10867881774902, 165.13286638259888, 166.15705394744873, 167.30863571166992, 168.4602174758911, 169.6152527332306, 170.77028799057007, 171.92711782455444, 173.08394765853882, 174.2345519065857, 175.38515615463257, 176.53992414474487, 177.69469213485718, 178.84989476203918, 180.0050973892212, 181.1743621826172, 182.34362697601318, 183.50000977516174, 184.6563925743103, 185.813884973526, 186.9713773727417, 188.13162183761597, 189.29186630249023, 190.44924473762512, 191.60662317276, 192.7626450061798, 193.9186668395996, 195.08758807182312, 196.25650930404663, 197.43350839614868, 198.61050748825073, 199.78009700775146, 200.9496865272522, 202.1128876209259, 203.2760887145996, 204.4380967617035, 205.60010480880737, 206.7667670249939, 207.93342924118042, 209.10144972801208, 210.26947021484375, 211.44395971298218, 212.6184492111206, 213.78482460975647, 214.95120000839233, 216.11590719223022, 217.28061437606812, 218.45141005516052, 219.62220573425293, 221.83999109268188, 224.05777645111084]
[23.685, 23.685, 28.99, 28.99, 32.9175, 32.9175, 34.185, 34.185, 35.675, 35.675, 36.11, 36.11, 37.15, 37.15, 38.6125, 38.6125, 38.74, 38.74, 38.5425, 38.5425, 38.895, 38.895, 39.4, 39.4, 40.4925, 40.4925, 40.6225, 40.6225, 40.9925, 40.9925, 40.425, 40.425, 40.6375, 40.6375, 40.605, 40.605, 40.815, 40.815, 41.5125, 41.5125, 41.4475, 41.4475, 41.3775, 41.3775, 41.4325, 41.4325, 41.8075, 41.8075, 42.165, 42.165, 41.85, 41.85, 41.95, 41.95, 41.9225, 41.9225, 42.01, 42.01, 41.5375, 41.5375, 41.795, 41.795, 41.9925, 41.9925, 42.0825, 42.0825, 41.84, 41.84, 41.8325, 41.8325, 41.39, 41.39, 41.7875, 41.7875, 42.035, 42.035, 41.7675, 41.7675, 41.5975, 41.5975, 41.535, 41.535, 41.54, 41.54, 41.7375, 41.7375, 41.19, 41.19, 41.805, 41.805, 41.8525, 41.8525, 42.1925, 42.1925, 41.755, 41.755, 41.925, 41.925, 41.595, 41.595, 41.3925, 41.3925, 41.4575, 41.4575, 41.5325, 41.5325, 41.7, 41.7, 41.4325, 41.4325, 41.825, 41.825, 41.955, 41.955, 41.8775, 41.8775, 41.9825, 41.9825, 41.7075, 41.7075, 41.8375, 41.8375, 42.04, 42.04, 41.775, 41.775, 42.035, 42.035, 42.3075, 42.3075, 42.4625, 42.4625, 42.1275, 42.1275, 42.1975, 42.1975, 41.995, 41.995, 42.065, 42.065, 42.2275, 42.2275, 42.0475, 42.0475, 41.9975, 41.9975, 42.445, 42.445, 42.4775, 42.4775, 42.1275, 42.1275, 42.24, 42.24, 41.985, 41.985, 42.105, 42.105, 42.135, 42.135, 41.835, 41.835, 41.8625, 41.8625, 42.025, 42.025, 42.005, 42.005, 42.1425, 42.1425, 42.11, 42.11, 42.175, 42.175, 42.485, 42.485, 42.2175, 42.2175, 41.935, 41.935, 42.195, 42.195, 41.9125, 41.9125, 42.0225, 42.0225, 42.055, 42.055, 42.0525, 42.0525, 42.275, 42.275, 42.4675, 42.4675, 42.4275, 42.4275, 41.98, 41.98, 42.1975, 42.1975, 42.515, 42.515]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 1.446, Test loss: 1.881, Test accuracy: 29.99
Round   1, Train loss: 1.262, Test loss: 1.864, Test accuracy: 30.65
Round   2, Train loss: 1.178, Test loss: 1.940, Test accuracy: 28.64
Round   3, Train loss: 1.081, Test loss: 1.944, Test accuracy: 29.75
Round   4, Train loss: 1.048, Test loss: 2.022, Test accuracy: 27.95
Round   5, Train loss: 1.004, Test loss: 2.000, Test accuracy: 30.25
Round   6, Train loss: 0.945, Test loss: 2.042, Test accuracy: 29.00
Round   7, Train loss: 0.878, Test loss: 2.078, Test accuracy: 28.21
Round   8, Train loss: 0.854, Test loss: 2.065, Test accuracy: 28.84
Round   9, Train loss: 0.816, Test loss: 2.058, Test accuracy: 29.17
Round  10, Train loss: 0.761, Test loss: 2.043, Test accuracy: 29.66
Round  11, Train loss: 0.737, Test loss: 2.028, Test accuracy: 30.95
Round  12, Train loss: 0.735, Test loss: 2.030, Test accuracy: 30.14
Round  13, Train loss: 0.673, Test loss: 2.025, Test accuracy: 30.76
Round  14, Train loss: 0.671, Test loss: 2.009, Test accuracy: 32.62
Round  15, Train loss: 0.603, Test loss: 2.010, Test accuracy: 31.89
Round  16, Train loss: 0.587, Test loss: 2.001, Test accuracy: 32.29
Round  17, Train loss: 0.595, Test loss: 1.993, Test accuracy: 33.29
Round  18, Train loss: 0.581, Test loss: 1.983, Test accuracy: 34.18
Round  19, Train loss: 0.537, Test loss: 1.978, Test accuracy: 34.34
Round  20, Train loss: 0.560, Test loss: 2.025, Test accuracy: 32.77
Round  21, Train loss: 0.500, Test loss: 2.014, Test accuracy: 33.37
Round  22, Train loss: 0.486, Test loss: 2.010, Test accuracy: 33.75
Round  23, Train loss: 0.445, Test loss: 1.993, Test accuracy: 34.97
Round  24, Train loss: 0.481, Test loss: 1.986, Test accuracy: 35.62
Round  25, Train loss: 0.422, Test loss: 1.975, Test accuracy: 36.11
Round  26, Train loss: 0.379, Test loss: 1.972, Test accuracy: 36.67
Round  27, Train loss: 0.419, Test loss: 1.965, Test accuracy: 37.30
Round  28, Train loss: 0.413, Test loss: 1.961, Test accuracy: 37.18
Round  29, Train loss: 0.417, Test loss: 1.951, Test accuracy: 37.91
Round  30, Train loss: 0.374, Test loss: 1.934, Test accuracy: 39.05
Round  31, Train loss: 0.341, Test loss: 1.925, Test accuracy: 39.37
Round  32, Train loss: 0.323, Test loss: 1.934, Test accuracy: 38.62
Round  33, Train loss: 0.334, Test loss: 1.921, Test accuracy: 39.27
Round  34, Train loss: 0.351, Test loss: 1.919, Test accuracy: 38.96
Round  35, Train loss: 0.301, Test loss: 1.901, Test accuracy: 39.62
Round  36, Train loss: 0.347, Test loss: 1.898, Test accuracy: 39.45
Round  37, Train loss: 0.278, Test loss: 1.886, Test accuracy: 40.22
Round  38, Train loss: 0.304, Test loss: 1.885, Test accuracy: 40.02
Round  39, Train loss: 0.273, Test loss: 1.873, Test accuracy: 40.50
Round  40, Train loss: 0.289, Test loss: 1.869, Test accuracy: 41.10
Round  41, Train loss: 0.265, Test loss: 1.856, Test accuracy: 41.53
Round  42, Train loss: 0.282, Test loss: 1.853, Test accuracy: 41.30
Round  43, Train loss: 0.249, Test loss: 1.847, Test accuracy: 41.58
Round  44, Train loss: 0.253, Test loss: 1.852, Test accuracy: 40.77
Round  45, Train loss: 0.255, Test loss: 1.844, Test accuracy: 41.15
Round  46, Train loss: 0.241, Test loss: 1.835, Test accuracy: 41.63
Round  47, Train loss: 0.245, Test loss: 1.828, Test accuracy: 41.92
Round  48, Train loss: 0.248, Test loss: 1.821, Test accuracy: 43.28
Round  49, Train loss: 0.219, Test loss: 1.827, Test accuracy: 42.41
Round  50, Train loss: 0.229, Test loss: 1.820, Test accuracy: 42.97
Round  51, Train loss: 0.216, Test loss: 1.813, Test accuracy: 43.45
Round  52, Train loss: 0.196, Test loss: 1.803, Test accuracy: 42.90
Round  53, Train loss: 0.233, Test loss: 1.798, Test accuracy: 42.78
Round  54, Train loss: 0.189, Test loss: 1.781, Test accuracy: 43.80
Round  55, Train loss: 0.203, Test loss: 1.785, Test accuracy: 43.55
Round  56, Train loss: 0.199, Test loss: 1.786, Test accuracy: 43.80
Round  57, Train loss: 0.201, Test loss: 1.785, Test accuracy: 43.13
Round  58, Train loss: 0.197, Test loss: 1.772, Test accuracy: 43.44
Round  59, Train loss: 0.206, Test loss: 1.774, Test accuracy: 43.74
Round  60, Train loss: 0.215, Test loss: 1.770, Test accuracy: 43.72
Round  61, Train loss: 0.190, Test loss: 1.767, Test accuracy: 43.84
Round  62, Train loss: 0.186, Test loss: 1.757, Test accuracy: 44.26
Round  63, Train loss: 0.182, Test loss: 1.757, Test accuracy: 44.01
Round  64, Train loss: 0.170, Test loss: 1.751, Test accuracy: 44.48
Round  65, Train loss: 0.172, Test loss: 1.745, Test accuracy: 44.38
Round  66, Train loss: 0.171, Test loss: 1.737, Test accuracy: 44.95
Round  67, Train loss: 0.181, Test loss: 1.735, Test accuracy: 44.47
Round  68, Train loss: 0.158, Test loss: 1.733, Test accuracy: 44.29
Round  69, Train loss: 0.170, Test loss: 1.714, Test accuracy: 45.40
Round  70, Train loss: 0.168, Test loss: 1.726, Test accuracy: 44.64
Round  71, Train loss: 0.156, Test loss: 1.723, Test accuracy: 44.97
Round  72, Train loss: 0.170, Test loss: 1.720, Test accuracy: 45.18
Round  73, Train loss: 0.159, Test loss: 1.719, Test accuracy: 44.67
Round  74, Train loss: 0.181, Test loss: 1.718, Test accuracy: 44.97
Round  75, Train loss: 0.147, Test loss: 1.718, Test accuracy: 44.48
Round  76, Train loss: 0.170, Test loss: 1.709, Test accuracy: 45.03
Round  77, Train loss: 0.143, Test loss: 1.706, Test accuracy: 44.74
Round  78, Train loss: 0.140, Test loss: 1.711, Test accuracy: 44.20
Round  79, Train loss: 0.145, Test loss: 1.708, Test accuracy: 44.25
Round  80, Train loss: 0.145, Test loss: 1.708, Test accuracy: 44.56
Round  81, Train loss: 0.138, Test loss: 1.697, Test accuracy: 45.43
Round  82, Train loss: 0.157, Test loss: 1.695, Test accuracy: 44.99
Round  83, Train loss: 0.143, Test loss: 1.700, Test accuracy: 45.00
Round  84, Train loss: 0.158, Test loss: 1.693, Test accuracy: 45.43
Round  85, Train loss: 0.136, Test loss: 1.678, Test accuracy: 46.01
Round  86, Train loss: 0.151, Test loss: 1.685, Test accuracy: 45.42
Round  87, Train loss: 0.134, Test loss: 1.687, Test accuracy: 45.05
Round  88, Train loss: 0.139, Test loss: 1.688, Test accuracy: 44.88
Round  89, Train loss: 0.131, Test loss: 1.667, Test accuracy: 45.89
Round  90, Train loss: 0.135, Test loss: 1.677, Test accuracy: 45.39
Round  91, Train loss: 0.134, Test loss: 1.673, Test accuracy: 45.47
Round  92, Train loss: 0.131, Test loss: 1.665, Test accuracy: 45.86
Round  93, Train loss: 0.129, Test loss: 1.664, Test accuracy: 46.01
Round  94, Train loss: 0.137, Test loss: 1.673, Test accuracy: 45.38
Round  95, Train loss: 0.137, Test loss: 1.665, Test accuracy: 45.82
Round  96, Train loss: 0.130, Test loss: 1.654, Test accuracy: 46.16
Round  97, Train loss: 0.141, Test loss: 1.663, Test accuracy: 46.00
Round  98, Train loss: 0.125, Test loss: 1.664, Test accuracy: 45.53
Round  99, Train loss: 0.129, Test loss: 1.662, Test accuracy: 45.57
Final Round, Train loss: 0.130, Test loss: 1.651, Test accuracy: 45.62
Average accuracy final 10 rounds: 45.71975
6184.241059780121
[]
[29.99, 30.6475, 28.6425, 29.7525, 27.9475, 30.2525, 29.0025, 28.21, 28.835, 29.1675, 29.665, 30.9475, 30.14, 30.7575, 32.62, 31.8875, 32.29, 33.2925, 34.1825, 34.3425, 32.77, 33.3725, 33.75, 34.97, 35.62, 36.1125, 36.675, 37.2975, 37.1825, 37.91, 39.0475, 39.365, 38.6175, 39.2675, 38.9625, 39.615, 39.4475, 40.22, 40.02, 40.4975, 41.105, 41.5275, 41.305, 41.575, 40.775, 41.15, 41.63, 41.92, 43.2775, 42.4125, 42.965, 43.4525, 42.895, 42.7825, 43.805, 43.5525, 43.8025, 43.1325, 43.44, 43.7375, 43.7225, 43.8425, 44.2575, 44.0125, 44.4825, 44.38, 44.945, 44.4725, 44.29, 45.4025, 44.64, 44.965, 45.1825, 44.6725, 44.9725, 44.475, 45.035, 44.7425, 44.205, 44.2475, 44.5575, 45.4325, 44.9875, 45.0, 45.4325, 46.01, 45.425, 45.045, 44.875, 45.8875, 45.3925, 45.47, 45.8625, 46.01, 45.38, 45.8225, 46.1575, 46.0, 45.5325, 45.57, 45.625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.993, Test loss: 1.178, Test accuracy: 60.17
Average accuracy final 10 rounds: 56.664
Average global accuracy final 10 rounds: 56.664
4485.145489454269
[]
[20.8025, 26.6575, 33.56, 38.18, 40.1175, 41.4325, 42.8675, 41.89, 42.615, 43.415, 43.98, 43.7175, 45.9075, 46.4025, 46.655, 46.195, 47.0275, 47.8775, 48.55, 49.0025, 49.3775, 50.0675, 49.345, 50.0625, 50.1775, 50.33, 51.2425, 52.0, 52.2025, 52.58, 52.185, 52.275, 51.965, 51.6225, 52.0925, 52.1725, 52.365, 52.555, 52.585, 52.2575, 52.3875, 52.74, 52.6375, 53.075, 52.7775, 52.84, 53.2225, 53.735, 54.33, 53.95, 53.7275, 54.3075, 54.245, 54.2675, 54.3025, 54.1375, 53.895, 54.25, 54.49, 54.0125, 54.295, 54.29, 54.57, 54.6875, 54.5625, 54.845, 54.82, 55.17, 55.05, 55.1425, 55.28, 55.2375, 55.3725, 55.43, 55.73, 55.5475, 55.1525, 55.29, 55.8625, 55.8125, 55.92, 56.035, 56.11, 56.0975, 55.965, 56.005, 56.035, 56.2125, 56.1225, 56.5875, 56.93, 57.0625, 56.935, 56.485, 56.4325, 56.54, 56.4325, 56.345, 56.445, 57.0325, 60.1725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 504, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54734 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 50211 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53965 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2181, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 55034 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.060, Test loss: 1.114, Test accuracy: 78.91
Final Round, Global train loss: 0.060, Global test loss: 2.076, Global test accuracy: 29.28
Average accuracy final 10 rounds: 78.48499999999999 

Average global accuracy final 10 rounds: 27.920833333333338 

1643.735722541809
[1.4096424579620361, 2.8192849159240723, 4.005798101425171, 5.1923112869262695, 6.379044055938721, 7.565776824951172, 8.743843078613281, 9.92190933227539, 11.110597372055054, 12.299285411834717, 13.486210346221924, 14.67313528060913, 15.857364416122437, 17.041593551635742, 18.220637559890747, 19.399681568145752, 20.578507900238037, 21.757334232330322, 22.939836502075195, 24.12233877182007, 25.300814628601074, 26.47929048538208, 27.658336400985718, 28.837382316589355, 29.971938133239746, 31.106493949890137, 32.36559581756592, 33.6246976852417, 34.825522899627686, 36.02634811401367, 37.067726612091064, 38.10910511016846, 39.14888858795166, 40.18867206573486, 41.22989201545715, 42.27111196517944, 43.30901503562927, 44.3469181060791, 45.38764500617981, 46.42837190628052, 47.47219491004944, 48.51601791381836, 49.55942177772522, 50.60282564163208, 51.652257204055786, 52.70168876647949, 53.73615860939026, 54.770628452301025, 55.81008815765381, 56.84954786300659, 57.88376069068909, 58.91797351837158, 59.96548080444336, 61.01298809051514, 62.055633783340454, 63.09827947616577, 64.14656639099121, 65.19485330581665, 66.23749661445618, 67.2801399230957, 68.4843213558197, 69.6885027885437, 70.90120601654053, 72.11390924453735, 73.32253885269165, 74.53116846084595, 75.73731803894043, 76.94346761703491, 78.15618085861206, 79.36889410018921, 80.57274317741394, 81.77659225463867, 82.98100185394287, 84.18541145324707, 85.38320207595825, 86.58099269866943, 87.6725697517395, 88.76414680480957, 89.81305050849915, 90.86195421218872, 91.91047096252441, 92.95898771286011, 94.00849866867065, 95.0580096244812, 96.09963417053223, 97.14125871658325, 98.1841983795166, 99.22713804244995, 100.26561069488525, 101.30408334732056, 102.34664416313171, 103.38920497894287, 104.42286801338196, 105.45653104782104, 106.49242234230042, 107.52831363677979, 108.57411789894104, 109.6199221611023, 110.66209483146667, 111.70426750183105, 112.75488662719727, 113.80550575256348, 114.84495377540588, 115.88440179824829, 116.92508172988892, 117.96576166152954, 119.00422549247742, 120.0426893234253, 121.08197259902954, 122.12125587463379, 123.17131781578064, 124.22137975692749, 125.26079607009888, 126.30021238327026, 127.34980297088623, 128.3993935585022, 129.43468809127808, 130.46998262405396, 131.5067434310913, 132.54350423812866, 133.5887942314148, 134.63408422470093, 135.6715223789215, 136.7089605331421, 137.75424695014954, 138.79953336715698, 139.83807969093323, 140.87662601470947, 141.92246437072754, 142.9683027267456, 144.00406002998352, 145.03981733322144, 146.08047008514404, 147.12112283706665, 148.16421175003052, 149.20730066299438, 150.24851155281067, 151.28972244262695, 152.34756779670715, 153.40541315078735, 154.4544975757599, 155.50358200073242, 156.56504201889038, 157.62650203704834, 158.67708730697632, 159.7276725769043, 160.7830364704132, 161.83840036392212, 162.8981592655182, 163.95791816711426, 165.00608158111572, 166.0542449951172, 167.11080598831177, 168.16736698150635, 169.21403574943542, 170.2607045173645, 171.3203318119049, 172.3799591064453, 177.7845058441162, 183.1890525817871, 184.23324060440063, 185.27742862701416, 186.32243394851685, 187.36743927001953, 188.40284967422485, 189.43826007843018, 190.4788818359375, 191.51950359344482, 192.54929852485657, 193.5790934562683, 194.6171498298645, 195.6552062034607, 196.67416095733643, 197.69311571121216, 198.71818232536316, 199.74324893951416, 200.77230668067932, 201.80136442184448, 202.83377528190613, 203.86618614196777, 204.90629696846008, 205.9464077949524, 206.9768168926239, 208.0072259902954, 209.04089307785034, 210.07456016540527, 211.105366230011, 212.1361722946167, 213.17325973510742, 214.21034717559814, 215.23805928230286, 216.26577138900757, 217.29660654067993, 218.3274416923523, 219.3580505847931, 220.3886594772339, 221.42114973068237, 222.45363998413086, 223.48801946640015, 224.52239894866943, 226.5773012638092, 228.63220357894897]
[21.908333333333335, 21.908333333333335, 33.38333333333333, 33.38333333333333, 47.75, 47.75, 56.625, 56.625, 59.141666666666666, 59.141666666666666, 59.416666666666664, 59.416666666666664, 63.61666666666667, 63.61666666666667, 63.891666666666666, 63.891666666666666, 66.90833333333333, 66.90833333333333, 68.975, 68.975, 69.65, 69.65, 71.94166666666666, 71.94166666666666, 72.3, 72.3, 73.275, 73.275, 73.625, 73.625, 74.13333333333334, 74.13333333333334, 74.375, 74.375, 74.76666666666667, 74.76666666666667, 74.38333333333334, 74.38333333333334, 74.51666666666667, 74.51666666666667, 74.55, 74.55, 74.70833333333333, 74.70833333333333, 74.86666666666666, 74.86666666666666, 75.20833333333333, 75.20833333333333, 75.40833333333333, 75.40833333333333, 75.71666666666667, 75.71666666666667, 75.36666666666666, 75.36666666666666, 75.53333333333333, 75.53333333333333, 76.19166666666666, 76.19166666666666, 75.83333333333333, 75.83333333333333, 76.225, 76.225, 76.26666666666667, 76.26666666666667, 76.41666666666667, 76.41666666666667, 76.45833333333333, 76.45833333333333, 76.51666666666667, 76.51666666666667, 76.76666666666667, 76.76666666666667, 76.9, 76.9, 77.48333333333333, 77.48333333333333, 77.45, 77.45, 77.28333333333333, 77.28333333333333, 77.65, 77.65, 77.46666666666667, 77.46666666666667, 77.46666666666667, 77.46666666666667, 77.08333333333333, 77.08333333333333, 77.175, 77.175, 77.3, 77.3, 77.575, 77.575, 77.6, 77.6, 78.05, 78.05, 77.89166666666667, 77.89166666666667, 77.59166666666667, 77.59166666666667, 77.33333333333333, 77.33333333333333, 77.35, 77.35, 77.71666666666667, 77.71666666666667, 77.75833333333334, 77.75833333333334, 77.8, 77.8, 77.99166666666666, 77.99166666666666, 77.54166666666667, 77.54166666666667, 77.53333333333333, 77.53333333333333, 77.85, 77.85, 77.55833333333334, 77.55833333333334, 77.55833333333334, 77.55833333333334, 77.88333333333334, 77.88333333333334, 78.00833333333334, 78.00833333333334, 78.86666666666666, 78.86666666666666, 78.33333333333333, 78.33333333333333, 78.34166666666667, 78.34166666666667, 78.28333333333333, 78.28333333333333, 78.35833333333333, 78.35833333333333, 78.2, 78.2, 78.23333333333333, 78.23333333333333, 78.49166666666666, 78.49166666666666, 78.5, 78.5, 78.54166666666667, 78.54166666666667, 78.525, 78.525, 78.6, 78.6, 78.34166666666667, 78.34166666666667, 78.69166666666666, 78.69166666666666, 78.33333333333333, 78.33333333333333, 77.98333333333333, 77.98333333333333, 78.15, 78.15, 77.96666666666667, 77.96666666666667, 77.975, 77.975, 78.2, 78.2, 78.30833333333334, 78.30833333333334, 78.43333333333334, 78.43333333333334, 78.59166666666667, 78.59166666666667, 78.33333333333333, 78.33333333333333, 77.83333333333333, 77.83333333333333, 78.15, 78.15, 78.125, 78.125, 78.28333333333333, 78.28333333333333, 77.99166666666666, 77.99166666666666, 78.34166666666667, 78.34166666666667, 78.40833333333333, 78.40833333333333, 78.8, 78.8, 78.53333333333333, 78.53333333333333, 78.86666666666666, 78.86666666666666, 78.675, 78.675, 78.825, 78.825, 78.90833333333333, 78.90833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.338, Test loss: 1.651, Test accuracy: 62.15
Final Round, Global train loss: 0.338, Global test loss: 1.099, Global test accuracy: 68.89
Average accuracy final 10 rounds: 63.23075 

Average global accuracy final 10 rounds: 69.11775 

3719.501929998398
[1.3907854557037354, 2.7815709114074707, 3.9590554237365723, 5.136539936065674, 6.321399688720703, 7.506259441375732, 8.689545392990112, 9.872831344604492, 11.059006452560425, 12.245181560516357, 13.434542655944824, 14.623903751373291, 15.81161642074585, 16.999329090118408, 18.183033227920532, 19.366737365722656, 20.553630113601685, 21.740522861480713, 22.923227071762085, 24.105931282043457, 25.29050588607788, 26.475080490112305, 27.660365343093872, 28.84565019607544, 30.027324199676514, 31.208998203277588, 32.391993284225464, 33.57498836517334, 34.75338554382324, 35.931782722473145, 37.12093734741211, 38.310091972351074, 39.49545121192932, 40.68081045150757, 41.86489748954773, 43.04898452758789, 44.06516790390015, 45.0813512802124, 46.097209453582764, 47.113067626953125, 48.1282274723053, 49.14338731765747, 50.161465644836426, 51.17954397201538, 52.19631481170654, 53.213085651397705, 54.230019092559814, 55.246952533721924, 56.2644579410553, 57.28196334838867, 58.298269748687744, 59.314576148986816, 60.332802295684814, 61.35102844238281, 62.37112736701965, 63.391226291656494, 64.40858578681946, 65.42594528198242, 66.44491648674011, 67.4638876914978, 68.48271656036377, 69.50154542922974, 70.51699042320251, 71.5324354171753, 72.5462327003479, 73.56002998352051, 74.57990980148315, 75.5997896194458, 76.61566972732544, 77.63154983520508, 78.64627742767334, 79.6610050201416, 80.68000221252441, 81.69899940490723, 82.71492886543274, 83.73085832595825, 84.75270891189575, 85.77455949783325, 86.78851342201233, 87.8024673461914, 88.81853914260864, 89.83461093902588, 90.84974193572998, 91.86487293243408, 92.87901020050049, 93.8931474685669, 94.90555357933044, 95.917959690094, 96.93396806716919, 97.94997644424438, 98.96752381324768, 99.98507118225098, 101.0013165473938, 102.01756191253662, 103.03594732284546, 104.0543327331543, 105.07051730155945, 106.0867018699646, 107.10361409187317, 108.12052631378174, 109.13743948936462, 110.15435266494751, 111.1716685295105, 112.18898439407349, 113.20526599884033, 114.22154760360718, 115.22934770584106, 116.23714780807495, 117.24541020393372, 118.25367259979248, 119.25868105888367, 120.26368951797485, 121.27324104309082, 122.28279256820679, 123.29411911964417, 124.30544567108154, 125.32227659225464, 126.33910751342773, 127.35349202156067, 128.3678765296936, 129.3813066482544, 130.39473676681519, 131.4106686115265, 132.4266004562378, 133.44148898124695, 134.4563775062561, 135.47676181793213, 136.49714612960815, 137.51771092414856, 138.53827571868896, 139.55301117897034, 140.5677466392517, 141.58279514312744, 142.59784364700317, 143.60942435264587, 144.62100505828857, 145.63530206680298, 146.64959907531738, 147.6658797264099, 148.68216037750244, 149.69507002830505, 150.70797967910767, 151.72776007652283, 152.747540473938, 153.75928282737732, 154.77102518081665, 155.78760981559753, 156.80419445037842, 157.82001066207886, 158.8358268737793, 159.8493504524231, 160.8628740310669, 161.87606263160706, 162.88925123214722, 163.90476202964783, 164.92027282714844, 165.93907928466797, 166.9578857421875, 167.97177386283875, 168.98566198349, 170.00086426734924, 171.0160665512085, 172.02915954589844, 173.04225254058838, 174.05828189849854, 175.0743112564087, 176.08791542053223, 177.10151958465576, 178.11496305465698, 179.1284065246582, 180.13979744911194, 181.15118837356567, 182.16742372512817, 183.18365907669067, 184.19618916511536, 185.20871925354004, 186.22170615196228, 187.23469305038452, 188.24855279922485, 189.26241254806519, 190.27461171150208, 191.28681087493896, 192.29818773269653, 193.3095645904541, 194.32156586647034, 195.33356714248657, 196.34793162345886, 197.36229610443115, 198.37762355804443, 199.39295101165771, 200.409508228302, 201.4260654449463, 202.44069147109985, 203.45531749725342, 204.460599899292, 205.46588230133057, 206.47117519378662, 207.47646808624268, 208.48326420783997, 209.49006032943726, 211.5103018283844, 213.53054332733154]
[24.5725, 24.5725, 28.7175, 28.7175, 31.2925, 31.2925, 35.17, 35.17, 37.1525, 37.1525, 38.6975, 38.6975, 39.105, 39.105, 40.15, 40.15, 41.87, 41.87, 42.3825, 42.3825, 43.6125, 43.6125, 44.165, 44.165, 45.8175, 45.8175, 46.665, 46.665, 47.99, 47.99, 48.9425, 48.9425, 49.4325, 49.4325, 49.75, 49.75, 49.8675, 49.8675, 50.71, 50.71, 51.5525, 51.5525, 51.82, 51.82, 52.875, 52.875, 53.7, 53.7, 53.8725, 53.8725, 54.1575, 54.1575, 54.16, 54.16, 54.1625, 54.1625, 54.4875, 54.4875, 54.9025, 54.9025, 55.0325, 55.0325, 55.845, 55.845, 56.0, 56.0, 55.975, 55.975, 56.365, 56.365, 56.795, 56.795, 56.9925, 56.9925, 57.0825, 57.0825, 57.0275, 57.0275, 57.4325, 57.4325, 57.7675, 57.7675, 57.97, 57.97, 58.17, 58.17, 58.57, 58.57, 58.815, 58.815, 58.9725, 58.9725, 59.345, 59.345, 59.6175, 59.6175, 59.645, 59.645, 59.805, 59.805, 59.9175, 59.9175, 59.93, 59.93, 60.06, 60.06, 60.55, 60.55, 60.6, 60.6, 60.38, 60.38, 60.3225, 60.3225, 60.7575, 60.7575, 60.8675, 60.8675, 60.795, 60.795, 61.08, 61.08, 61.0425, 61.0425, 61.3475, 61.3475, 61.2825, 61.2825, 61.1675, 61.1675, 61.0425, 61.0425, 61.155, 61.155, 61.17, 61.17, 61.4225, 61.4225, 61.5275, 61.5275, 61.72, 61.72, 61.8725, 61.8725, 62.0975, 62.0975, 61.7175, 61.7175, 61.7, 61.7, 61.7475, 61.7475, 61.875, 61.875, 61.885, 61.885, 62.21, 62.21, 62.2025, 62.2025, 62.6425, 62.6425, 62.4875, 62.4875, 62.425, 62.425, 62.71, 62.71, 62.4475, 62.4475, 61.985, 61.985, 62.0875, 62.0875, 62.42, 62.42, 62.9375, 62.9375, 63.0575, 63.0575, 63.1775, 63.1775, 62.8625, 62.8625, 63.085, 63.085, 63.0925, 63.0925, 63.385, 63.385, 63.3875, 63.3875, 63.2325, 63.2325, 63.435, 63.435, 63.225, 63.225, 63.425, 63.425, 62.145, 62.145]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.516, Test loss: 1.289, Test accuracy: 62.73
Average accuracy final 10 rounds: 62.61050000000001 

2691.353810787201
[1.3145081996917725, 2.629016399383545, 3.693326711654663, 4.757637023925781, 5.820624589920044, 6.883612155914307, 7.948674440383911, 9.013736724853516, 10.078823804855347, 11.143910884857178, 12.203550338745117, 13.263189792633057, 14.32628870010376, 15.389387607574463, 16.449488162994385, 17.509588718414307, 18.57675337791443, 19.64391803741455, 20.70456886291504, 21.765219688415527, 22.82656192779541, 23.887904167175293, 24.95186161994934, 26.01581907272339, 27.0753071308136, 28.13479518890381, 29.20031714439392, 30.265839099884033, 31.325196504592896, 32.38455390930176, 33.445061445236206, 34.505568981170654, 35.566640853881836, 36.62771272659302, 37.68974447250366, 38.75177621841431, 39.81450843811035, 40.8772406578064, 41.936487674713135, 42.99573469161987, 44.060351848602295, 45.12496900558472, 46.18259048461914, 47.240211963653564, 48.305758476257324, 49.371304988861084, 50.437251567840576, 51.50319814682007, 52.56637692451477, 53.62955570220947, 54.68964624404907, 55.74973678588867, 56.807790994644165, 57.86584520339966, 58.92843508720398, 59.9910249710083, 61.04995393753052, 62.108882904052734, 63.16916537284851, 64.22944784164429, 65.28834462165833, 66.34724140167236, 67.40934610366821, 68.47145080566406, 69.53184366226196, 70.59223651885986, 71.65576553344727, 72.71929454803467, 73.77877807617188, 74.83826160430908, 75.89629197120667, 76.95432233810425, 78.01688861846924, 79.07945489883423, 80.13892459869385, 81.19839429855347, 82.26174449920654, 83.32509469985962, 84.38526487350464, 85.44543504714966, 86.50335550308228, 87.56127595901489, 88.6209831237793, 89.6806902885437, 90.74411821365356, 91.80754613876343, 92.87180256843567, 93.93605899810791, 94.99521470069885, 96.0543704032898, 97.1292839050293, 98.2041974067688, 99.26539158821106, 100.32658576965332, 101.38967680931091, 102.4527678489685, 103.51044964790344, 104.56813144683838, 105.63054990768433, 106.69296836853027, 107.75058650970459, 108.8082046508789, 109.868004322052, 110.9278039932251, 111.99393582344055, 113.060067653656, 114.12198162078857, 115.18389558792114, 116.24331831932068, 117.30274105072021, 118.36271262168884, 119.42268419265747, 120.48785996437073, 121.55303573608398, 122.60402488708496, 123.65501403808594, 124.70519542694092, 125.7553768157959, 126.80437850952148, 127.85338020324707, 128.9113209247589, 129.96926164627075, 131.02591562271118, 132.0825695991516, 133.13336396217346, 134.1841583251953, 135.24178218841553, 136.29940605163574, 137.34835028648376, 138.3972945213318, 139.4451322555542, 140.4929699897766, 141.5440080165863, 142.595046043396, 143.6488835811615, 144.702721118927, 145.7581057548523, 146.8134903907776, 147.86011004447937, 148.90672969818115, 149.96022391319275, 151.01371812820435, 152.06668400764465, 153.11964988708496, 154.1717939376831, 155.22393798828125, 156.2778136730194, 157.33168935775757, 158.38259840011597, 159.43350744247437, 160.48588705062866, 161.53826665878296, 162.5922338962555, 163.64620113372803, 164.70410823822021, 165.7620153427124, 166.81864619255066, 167.87527704238892, 168.9262936115265, 169.97731018066406, 171.03015232086182, 172.08299446105957, 173.1354260444641, 174.18785762786865, 175.23703980445862, 176.28622198104858, 177.336181640625, 178.38614130020142, 179.43764233589172, 180.48914337158203, 181.53672456741333, 182.58430576324463, 183.63790082931519, 184.69149589538574, 185.74198698997498, 186.7924780845642, 187.85221314430237, 188.91194820404053, 189.96402430534363, 191.01610040664673, 192.06708645820618, 193.11807250976562, 194.16827249526978, 195.21847248077393, 196.2655532360077, 197.31263399124146, 198.3615744113922, 199.41051483154297, 200.4640703201294, 201.51762580871582, 202.5662932395935, 203.6149606704712, 204.66275453567505, 205.7105484008789, 206.7594575881958, 207.8083667755127, 208.85801553726196, 209.90766429901123, 210.9621605873108, 212.01665687561035, 213.89674019813538, 215.7768235206604]
[19.1575, 19.1575, 26.7325, 26.7325, 29.745, 29.745, 31.8025, 31.8025, 34.21, 34.21, 36.38, 36.38, 37.6875, 37.6875, 39.3175, 39.3175, 41.32, 41.32, 42.49, 42.49, 43.1475, 43.1475, 44.2525, 44.2525, 45.7, 45.7, 46.035, 46.035, 45.9375, 45.9375, 47.7475, 47.7475, 48.025, 48.025, 49.005, 49.005, 49.6425, 49.6425, 49.01, 49.01, 49.435, 49.435, 49.05, 49.05, 51.6875, 51.6875, 52.02, 52.02, 51.7525, 51.7525, 52.8925, 52.8925, 54.0425, 54.0425, 54.4025, 54.4025, 54.0975, 54.0975, 54.785, 54.785, 55.0775, 55.0775, 55.71, 55.71, 56.2175, 56.2175, 57.19, 57.19, 56.74, 56.74, 57.1575, 57.1575, 57.6675, 57.6675, 57.075, 57.075, 57.515, 57.515, 57.74, 57.74, 58.2625, 58.2625, 57.54, 57.54, 58.57, 58.57, 59.1525, 59.1525, 59.11, 59.11, 59.3825, 59.3825, 59.5225, 59.5225, 59.5475, 59.5475, 60.415, 60.415, 60.6325, 60.6325, 60.745, 60.745, 60.24, 60.24, 60.06, 60.06, 59.905, 59.905, 59.2525, 59.2525, 60.0175, 60.0175, 60.5825, 60.5825, 60.7225, 60.7225, 60.4525, 60.4525, 60.48, 60.48, 60.945, 60.945, 61.285, 61.285, 61.81, 61.81, 61.88, 61.88, 61.6975, 61.6975, 61.9025, 61.9025, 61.81, 61.81, 61.5875, 61.5875, 61.74, 61.74, 61.6975, 61.6975, 61.74, 61.74, 61.75, 61.75, 61.6975, 61.6975, 61.5925, 61.5925, 62.165, 62.165, 62.1275, 62.1275, 62.215, 62.215, 61.58, 61.58, 61.5875, 61.5875, 62.685, 62.685, 62.5275, 62.5275, 62.7375, 62.7375, 63.0275, 63.0275, 62.23, 62.23, 62.3375, 62.3375, 62.6675, 62.6675, 62.7825, 62.7825, 63.22, 63.22, 62.9725, 62.9725, 63.115, 63.115, 62.7825, 62.7825, 62.38, 62.38, 62.6325, 62.6325, 62.86, 62.86, 62.1925, 62.1925, 62.0375, 62.0375, 62.905, 62.905, 62.905, 62.905, 63.1275, 63.1275, 62.2825, 62.2825, 62.73, 62.73]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.316, Test loss: 1.602, Test accuracy: 62.37
Average accuracy final 10 rounds: 62.126999999999995 

2823.4246776103973
[1.4143154621124268, 2.8286309242248535, 4.001286268234253, 5.173941612243652, 6.3503007888793945, 7.526659965515137, 8.703540086746216, 9.880420207977295, 11.062209367752075, 12.243998527526855, 13.423400402069092, 14.602802276611328, 15.777043104171753, 16.951283931732178, 18.12452220916748, 19.297760486602783, 20.475849866867065, 21.653939247131348, 22.830906629562378, 24.007874011993408, 25.18433928489685, 26.360804557800293, 27.541914701461792, 28.72302484512329, 29.899080276489258, 31.075135707855225, 32.25536322593689, 33.435590744018555, 34.61309814453125, 35.790605545043945, 36.96273446083069, 38.13486337661743, 39.31382632255554, 40.49278926849365, 41.67472863197327, 42.85666799545288, 44.031835079193115, 45.20700216293335, 46.38089156150818, 47.55478096008301, 48.731836557388306, 49.9088921546936, 51.08522653579712, 52.261560916900635, 53.43757343292236, 54.61358594894409, 55.78891706466675, 56.964248180389404, 58.14421343803406, 59.32417869567871, 60.504945516586304, 61.6857123374939, 62.86074948310852, 64.03578662872314, 65.21214747428894, 66.38850831985474, 67.56458330154419, 68.74065828323364, 69.91948342323303, 71.09830856323242, 72.27603960037231, 73.4537706375122, 74.62277030944824, 75.79176998138428, 77.06020092964172, 78.32863187789917, 79.57774353027344, 80.8268551826477, 82.08030009269714, 83.33374500274658, 84.58441686630249, 85.8350887298584, 87.08679127693176, 88.33849382400513, 89.58656287193298, 90.83463191986084, 92.08016538619995, 93.32569885253906, 94.57190442085266, 95.81810998916626, 97.06461000442505, 98.31111001968384, 99.55388140678406, 100.79665279388428, 102.05431580543518, 103.31197881698608, 104.55923581123352, 105.80649280548096, 107.04994082450867, 108.29338884353638, 109.54649424552917, 110.79959964752197, 112.04575943946838, 113.2919192314148, 114.5397961139679, 115.787672996521, 117.03722357749939, 118.28677415847778, 119.53475546836853, 120.78273677825928, 122.02977228164673, 123.27680778503418, 124.52399277687073, 125.77117776870728, 127.0166552066803, 128.26213264465332, 129.50637531280518, 130.75061798095703, 131.9942009449005, 133.237783908844, 134.4833779335022, 135.7289719581604, 136.97407126426697, 138.21917057037354, 139.47222185134888, 140.72527313232422, 141.89882493019104, 143.07237672805786, 144.3235728740692, 145.57476902008057, 146.75703811645508, 147.9393072128296, 149.13104009628296, 150.32277297973633, 151.5118386745453, 152.70090436935425, 153.89881443977356, 155.09672451019287, 156.29376769065857, 157.49081087112427, 158.68216252326965, 159.87351417541504, 161.0632026195526, 162.25289106369019, 163.45067071914673, 164.64845037460327, 165.88118147850037, 167.11391258239746, 168.37229871749878, 169.6306848526001, 170.88717126846313, 172.14365768432617, 173.40349102020264, 174.6633243560791, 175.92462372779846, 177.18592309951782, 178.43984746932983, 179.69377183914185, 180.95191621780396, 182.21006059646606, 183.47187066078186, 184.73368072509766, 185.99800944328308, 187.2623381614685, 188.52215027809143, 189.78196239471436, 191.03783345222473, 192.2937045097351, 193.5569567680359, 194.82020902633667, 196.08067655563354, 197.34114408493042, 198.59805035591125, 199.8549566268921, 201.11336278915405, 202.37176895141602, 203.6320309638977, 204.8922929763794, 206.1472852230072, 207.402277469635, 208.65244817733765, 209.90261888504028, 211.162579536438, 212.4225401878357, 213.67898869514465, 214.9354372024536, 216.18552374839783, 217.43561029434204, 218.68453907966614, 219.93346786499023, 221.1962640285492, 222.45906019210815, 223.7127296924591, 224.96639919281006, 226.21640706062317, 227.46641492843628, 228.72470331192017, 229.98299169540405, 231.2418394088745, 232.50068712234497, 233.75590133666992, 235.01111555099487, 236.25588631629944, 237.500657081604, 238.751690864563, 240.00272464752197, 241.2538673877716, 242.50501012802124, 243.7548394203186, 245.00466871261597, 246.95563769340515, 248.90660667419434]
[28.4625, 28.4625, 32.7475, 32.7475, 36.38, 36.38, 40.08, 40.08, 41.4375, 41.4375, 42.49, 42.49, 44.9075, 44.9075, 46.5025, 46.5025, 47.97, 47.97, 48.8475, 48.8475, 49.5375, 49.5375, 50.4475, 50.4475, 51.4825, 51.4825, 52.255, 52.255, 52.4325, 52.4325, 53.8925, 53.8925, 54.825, 54.825, 56.345, 56.345, 56.2525, 56.2525, 56.3575, 56.3575, 57.2975, 57.2975, 57.74, 57.74, 58.2725, 58.2725, 58.0825, 58.0825, 58.085, 58.085, 58.915, 58.915, 58.885, 58.885, 59.345, 59.345, 59.4925, 59.4925, 59.315, 59.315, 59.7025, 59.7025, 60.2975, 60.2975, 60.2375, 60.2375, 60.2525, 60.2525, 60.375, 60.375, 59.9575, 59.9575, 60.76, 60.76, 60.5975, 60.5975, 60.95, 60.95, 61.1675, 61.1675, 60.74, 60.74, 61.02, 61.02, 60.5, 60.5, 60.7225, 60.7225, 60.275, 60.275, 60.9325, 60.9325, 61.09, 61.09, 61.3275, 61.3275, 61.4725, 61.4725, 61.5525, 61.5525, 61.69, 61.69, 61.8875, 61.8875, 61.3325, 61.3325, 61.5125, 61.5125, 61.86, 61.86, 61.7625, 61.7625, 61.345, 61.345, 61.3175, 61.3175, 61.9125, 61.9125, 61.8475, 61.8475, 61.515, 61.515, 62.0075, 62.0075, 61.8075, 61.8075, 61.74, 61.74, 61.745, 61.745, 61.5875, 61.5875, 61.9675, 61.9675, 61.6025, 61.6025, 61.5525, 61.5525, 61.6475, 61.6475, 62.235, 62.235, 61.7525, 61.7525, 61.7075, 61.7075, 62.1025, 62.1025, 61.9975, 61.9975, 61.72, 61.72, 62.12, 62.12, 62.125, 62.125, 61.55, 61.55, 61.58, 61.58, 61.995, 61.995, 62.1325, 62.1325, 61.915, 61.915, 62.2425, 62.2425, 62.485, 62.485, 62.08, 62.08, 61.95, 61.95, 62.04, 62.04, 61.8575, 61.8575, 61.7975, 61.7975, 62.325, 62.325, 61.9975, 61.9975, 62.175, 62.175, 61.8, 61.8, 62.095, 62.095, 62.2775, 62.2775, 62.405, 62.405, 62.09, 62.09, 61.8725, 61.8725, 62.2325, 62.2325, 62.3675, 62.3675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.090, Test loss: 4.189, Test accuracy: 43.38
Average accuracy final 10 rounds: 42.5405 

2767.777838230133
[1.4346685409545898, 2.8693370819091797, 4.107725381851196, 5.346113681793213, 6.590903997421265, 7.835694313049316, 9.077031373977661, 10.318368434906006, 11.565408706665039, 12.812448978424072, 14.05154037475586, 15.290631771087646, 16.535271406173706, 17.779911041259766, 19.021618366241455, 20.263325691223145, 21.499722003936768, 22.73611831665039, 23.982095956802368, 25.228073596954346, 26.474037170410156, 27.720000743865967, 28.962461233139038, 30.20492172241211, 31.444690227508545, 32.68445873260498, 33.9249541759491, 35.16544961929321, 36.401052474975586, 37.63665533065796, 38.88121223449707, 40.12576913833618, 41.36486291885376, 42.60395669937134, 43.84494924545288, 45.085941791534424, 46.3237943649292, 47.561646938323975, 48.791666746139526, 50.02168655395508, 51.25645875930786, 52.491230964660645, 53.73325037956238, 54.97526979446411, 56.22034978866577, 57.46542978286743, 58.70398163795471, 59.94253349304199, 61.17298746109009, 62.403441429138184, 63.645771503448486, 64.88810157775879, 66.14643287658691, 67.40476417541504, 68.6538417339325, 69.90291929244995, 71.14017152786255, 72.37742376327515, 73.62148547172546, 74.86554718017578, 76.1097846031189, 77.35402202606201, 78.59321165084839, 79.83240127563477, 81.07475519180298, 82.31710910797119, 83.55900859832764, 84.80090808868408, 86.04276156425476, 87.28461503982544, 88.5245954990387, 89.76457595825195, 91.00236773490906, 92.24015951156616, 93.48094487190247, 94.72173023223877, 95.96640992164612, 97.21108961105347, 98.46005177497864, 99.70901393890381, 100.95599126815796, 102.20296859741211, 103.44479513168335, 104.68662166595459, 105.93131589889526, 107.17601013183594, 108.41895389556885, 109.66189765930176, 110.91163873672485, 112.16137981414795, 113.40508270263672, 114.64878559112549, 115.89477229118347, 117.14075899124146, 118.37935018539429, 119.61794137954712, 120.8638665676117, 122.10979175567627, 123.3511791229248, 124.59256649017334, 125.84645056724548, 127.10033464431763, 128.34979844093323, 129.59926223754883, 130.85206508636475, 132.10486793518066, 133.35526585578918, 134.6056637763977, 135.85468935966492, 137.10371494293213, 138.35049653053284, 139.59727811813354, 140.844575881958, 142.09187364578247, 143.33857941627502, 144.58528518676758, 145.83675384521484, 147.0882225036621, 148.33570313453674, 149.58318376541138, 150.82891726493835, 152.07465076446533, 153.32098054885864, 154.56731033325195, 155.81622338294983, 157.0651364326477, 158.3042697906494, 159.54340314865112, 160.7908525466919, 162.03830194473267, 163.28337955474854, 164.5284571647644, 165.78037691116333, 167.03229665756226, 168.27834272384644, 169.52438879013062, 170.7696716785431, 172.01495456695557, 173.12461757659912, 174.23428058624268, 175.4336895942688, 176.63309860229492, 177.8788924217224, 179.1246862411499, 180.37400650978088, 181.62332677841187, 182.8703339099884, 184.11734104156494, 185.36348056793213, 186.60962009429932, 187.85676860809326, 189.1039171218872, 190.35056829452515, 191.5972194671631, 192.84418106079102, 194.09114265441895, 195.3392276763916, 196.58731269836426, 197.83282256126404, 199.07833242416382, 200.32539796829224, 201.57246351242065, 202.81749033927917, 204.0625171661377, 205.3075830936432, 206.55264902114868, 207.80268120765686, 209.05271339416504, 210.30045914649963, 211.54820489883423, 212.79757142066956, 214.04693794250488, 215.29382228851318, 216.54070663452148, 217.7884383201599, 219.03617000579834, 220.2799654006958, 221.52376079559326, 222.76551270484924, 224.00726461410522, 225.2478358745575, 226.48840713500977, 227.73503708839417, 228.98166704177856, 230.2283058166504, 231.47494459152222, 232.71962523460388, 233.96430587768555, 235.21234226226807, 236.4603786468506, 237.70292329788208, 238.94546794891357, 240.19612169265747, 241.44677543640137, 242.69366192817688, 243.9405484199524, 245.19356775283813, 246.44658708572388, 247.69303464889526, 248.93948221206665, 251.25898146629333, 253.57848072052002]
[24.635, 24.635, 29.4725, 29.4725, 31.7525, 31.7525, 33.85, 33.85, 35.49, 35.49, 36.63, 36.63, 37.4075, 37.4075, 38.24, 38.24, 38.12, 38.12, 38.89, 38.89, 39.6125, 39.6125, 39.855, 39.855, 40.6675, 40.6675, 40.3075, 40.3075, 40.8725, 40.8725, 40.4575, 40.4575, 40.475, 40.475, 40.6175, 40.6175, 40.7025, 40.7025, 40.91, 40.91, 41.5775, 41.5775, 41.3825, 41.3825, 41.335, 41.335, 41.67, 41.67, 41.5475, 41.5475, 41.35, 41.35, 41.625, 41.625, 41.47, 41.47, 41.685, 41.685, 41.83, 41.83, 41.8825, 41.8825, 42.09, 42.09, 42.0825, 42.0825, 41.805, 41.805, 41.805, 41.805, 41.7875, 41.7875, 41.7775, 41.7775, 42.26, 42.26, 42.1125, 42.1125, 42.0725, 42.0725, 42.29, 42.29, 42.275, 42.275, 42.0925, 42.0925, 42.1875, 42.1875, 42.5575, 42.5575, 42.255, 42.255, 42.2425, 42.2425, 42.1475, 42.1475, 42.2375, 42.2375, 42.24, 42.24, 42.525, 42.525, 42.6325, 42.6325, 42.475, 42.475, 42.4925, 42.4925, 42.7075, 42.7075, 42.33, 42.33, 42.5675, 42.5675, 42.5175, 42.5175, 42.5375, 42.5375, 42.4825, 42.4825, 42.0275, 42.0275, 42.3875, 42.3875, 42.1225, 42.1225, 42.0775, 42.0775, 42.41, 42.41, 42.39, 42.39, 42.3725, 42.3725, 42.4825, 42.4825, 42.2175, 42.2175, 41.79, 41.79, 42.0425, 42.0425, 42.3475, 42.3475, 42.3275, 42.3275, 42.57, 42.57, 42.5825, 42.5825, 42.625, 42.625, 42.75, 42.75, 42.3475, 42.3475, 42.095, 42.095, 41.7125, 41.7125, 42.0825, 42.0825, 42.0125, 42.0125, 42.455, 42.455, 42.6325, 42.6325, 42.6525, 42.6525, 42.7475, 42.7475, 42.3675, 42.3675, 42.6525, 42.6525, 42.6025, 42.6025, 42.655, 42.655, 42.235, 42.235, 42.3425, 42.3425, 42.55, 42.55, 42.6875, 42.6875, 42.59, 42.59, 42.5575, 42.5575, 42.7225, 42.7225, 42.6425, 42.6425, 42.4825, 42.4825, 42.595, 42.595, 43.38, 43.38]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 1.449, Test loss: 1.893, Test accuracy: 31.21
Round   1, Train loss: 1.255, Test loss: 1.799, Test accuracy: 34.93
Round   2, Train loss: 1.175, Test loss: 1.890, Test accuracy: 31.46
Round   3, Train loss: 1.095, Test loss: 1.889, Test accuracy: 32.85
Round   4, Train loss: 1.031, Test loss: 1.964, Test accuracy: 31.07
Round   5, Train loss: 0.967, Test loss: 1.994, Test accuracy: 30.06
Round   6, Train loss: 0.943, Test loss: 2.089, Test accuracy: 27.02
Round   7, Train loss: 0.874, Test loss: 2.071, Test accuracy: 27.45
Round   8, Train loss: 0.851, Test loss: 2.118, Test accuracy: 25.75
Round   9, Train loss: 0.834, Test loss: 2.114, Test accuracy: 25.68
Round  10, Train loss: 0.770, Test loss: 2.109, Test accuracy: 25.79
Round  11, Train loss: 0.734, Test loss: 2.094, Test accuracy: 25.91
Round  12, Train loss: 0.733, Test loss: 2.088, Test accuracy: 26.01
Round  13, Train loss: 0.669, Test loss: 2.081, Test accuracy: 26.66
Round  14, Train loss: 0.688, Test loss: 2.074, Test accuracy: 26.45
Round  15, Train loss: 0.610, Test loss: 2.064, Test accuracy: 27.07
Round  16, Train loss: 0.630, Test loss: 2.054, Test accuracy: 27.91
Round  17, Train loss: 0.574, Test loss: 2.047, Test accuracy: 27.80
Round  18, Train loss: 0.560, Test loss: 2.040, Test accuracy: 27.95
Round  19, Train loss: 0.547, Test loss: 2.031, Test accuracy: 28.45
Round  20, Train loss: 0.526, Test loss: 2.025, Test accuracy: 28.07
Round  21, Train loss: 0.512, Test loss: 2.018, Test accuracy: 29.32
Round  22, Train loss: 0.488, Test loss: 2.014, Test accuracy: 29.25
Round  23, Train loss: 0.483, Test loss: 2.007, Test accuracy: 30.35
Round  24, Train loss: 0.471, Test loss: 2.005, Test accuracy: 30.39
Round  25, Train loss: 0.411, Test loss: 1.987, Test accuracy: 31.12
Round  26, Train loss: 0.460, Test loss: 1.992, Test accuracy: 30.46
Round  27, Train loss: 0.459, Test loss: 1.986, Test accuracy: 30.80
Round  28, Train loss: 0.413, Test loss: 1.977, Test accuracy: 31.48
Round  29, Train loss: 0.370, Test loss: 1.969, Test accuracy: 31.86
Round  30, Train loss: 0.410, Test loss: 1.956, Test accuracy: 32.99
Round  31, Train loss: 0.372, Test loss: 1.943, Test accuracy: 33.81
Round  32, Train loss: 0.361, Test loss: 1.936, Test accuracy: 34.14
Round  33, Train loss: 0.356, Test loss: 1.935, Test accuracy: 33.56
Round  34, Train loss: 0.341, Test loss: 1.920, Test accuracy: 34.80
Round  35, Train loss: 0.317, Test loss: 1.919, Test accuracy: 34.38
Round  36, Train loss: 0.311, Test loss: 1.923, Test accuracy: 34.08
Round  37, Train loss: 0.301, Test loss: 1.916, Test accuracy: 34.62
Round  38, Train loss: 0.304, Test loss: 1.910, Test accuracy: 34.87
Round  39, Train loss: 0.284, Test loss: 1.907, Test accuracy: 34.92
Round  40, Train loss: 0.288, Test loss: 1.905, Test accuracy: 35.63
Round  41, Train loss: 0.314, Test loss: 1.898, Test accuracy: 36.50
Round  42, Train loss: 0.258, Test loss: 1.890, Test accuracy: 36.06
Round  43, Train loss: 0.276, Test loss: 1.881, Test accuracy: 36.75
Round  44, Train loss: 0.254, Test loss: 1.866, Test accuracy: 37.90
Round  45, Train loss: 0.243, Test loss: 1.860, Test accuracy: 37.96
Round  46, Train loss: 0.226, Test loss: 1.846, Test accuracy: 38.95
Round  47, Train loss: 0.293, Test loss: 1.853, Test accuracy: 38.18
Round  48, Train loss: 0.252, Test loss: 1.844, Test accuracy: 38.87
Round  49, Train loss: 0.227, Test loss: 1.842, Test accuracy: 38.44
Round  50, Train loss: 0.238, Test loss: 1.839, Test accuracy: 38.72
Round  51, Train loss: 0.213, Test loss: 1.835, Test accuracy: 39.14
Round  52, Train loss: 0.212, Test loss: 1.830, Test accuracy: 38.52
Round  53, Train loss: 0.225, Test loss: 1.829, Test accuracy: 39.11
Round  54, Train loss: 0.206, Test loss: 1.827, Test accuracy: 39.00
Round  55, Train loss: 0.239, Test loss: 1.817, Test accuracy: 39.80
Round  56, Train loss: 0.210, Test loss: 1.806, Test accuracy: 40.06
Round  57, Train loss: 0.217, Test loss: 1.807, Test accuracy: 39.33
Round  58, Train loss: 0.198, Test loss: 1.803, Test accuracy: 39.74
Round  59, Train loss: 0.185, Test loss: 1.794, Test accuracy: 40.14
Round  60, Train loss: 0.215, Test loss: 1.791, Test accuracy: 40.42
Round  61, Train loss: 0.186, Test loss: 1.784, Test accuracy: 40.58
Round  62, Train loss: 0.198, Test loss: 1.773, Test accuracy: 40.96
Round  63, Train loss: 0.199, Test loss: 1.766, Test accuracy: 41.20
Round  64, Train loss: 0.183, Test loss: 1.763, Test accuracy: 41.57
Round  65, Train loss: 0.191, Test loss: 1.752, Test accuracy: 42.47
Round  66, Train loss: 0.166, Test loss: 1.757, Test accuracy: 41.23
Round  67, Train loss: 0.173, Test loss: 1.766, Test accuracy: 40.96
Round  68, Train loss: 0.167, Test loss: 1.759, Test accuracy: 41.18
Round  69, Train loss: 0.169, Test loss: 1.753, Test accuracy: 41.55
Round  70, Train loss: 0.176, Test loss: 1.753, Test accuracy: 41.52
Round  71, Train loss: 0.178, Test loss: 1.750, Test accuracy: 41.29
Round  72, Train loss: 0.155, Test loss: 1.745, Test accuracy: 41.25
Round  73, Train loss: 0.163, Test loss: 1.743, Test accuracy: 41.48
Round  74, Train loss: 0.151, Test loss: 1.738, Test accuracy: 42.20
Round  75, Train loss: 0.164, Test loss: 1.732, Test accuracy: 42.76
Round  76, Train loss: 0.160, Test loss: 1.736, Test accuracy: 42.28
Round  77, Train loss: 0.156, Test loss: 1.735, Test accuracy: 42.16
Round  78, Train loss: 0.153, Test loss: 1.726, Test accuracy: 42.18
Round  79, Train loss: 0.151, Test loss: 1.731, Test accuracy: 41.89
Round  80, Train loss: 0.162, Test loss: 1.730, Test accuracy: 41.78
Round  81, Train loss: 0.148, Test loss: 1.715, Test accuracy: 42.77
Round  82, Train loss: 0.141, Test loss: 1.710, Test accuracy: 43.10
Round  83, Train loss: 0.150, Test loss: 1.707, Test accuracy: 43.31
Round  84, Train loss: 0.163, Test loss: 1.722, Test accuracy: 42.33
Round  85, Train loss: 0.138, Test loss: 1.706, Test accuracy: 43.13
Round  86, Train loss: 0.156, Test loss: 1.709, Test accuracy: 43.23
Round  87, Train loss: 0.132, Test loss: 1.697, Test accuracy: 43.28
Round  88, Train loss: 0.140, Test loss: 1.691, Test accuracy: 43.72
Round  89, Train loss: 0.139, Test loss: 1.692, Test accuracy: 43.41
Round  90, Train loss: 0.134, Test loss: 1.705, Test accuracy: 42.79
Round  91, Train loss: 0.127, Test loss: 1.688, Test accuracy: 43.30
Round  92, Train loss: 0.144, Test loss: 1.694, Test accuracy: 43.06
Round  93, Train loss: 0.130, Test loss: 1.690, Test accuracy: 43.35
Round  94, Train loss: 0.134, Test loss: 1.682, Test accuracy: 43.45
Round  95, Train loss: 0.128, Test loss: 1.677, Test accuracy: 43.75
Round  96, Train loss: 0.134, Test loss: 1.674, Test accuracy: 44.30
Round  97, Train loss: 0.147, Test loss: 1.688, Test accuracy: 43.88
Round  98, Train loss: 0.129, Test loss: 1.679, Test accuracy: 43.76
Round  99, Train loss: 0.129, Test loss: 1.678, Test accuracy: 43.97
Final Round, Train loss: 0.133, Test loss: 1.676, Test accuracy: 44.11
Average accuracy final 10 rounds: 43.56075
5957.476557970047
[]
[31.2125, 34.9325, 31.4625, 32.8475, 31.07, 30.0575, 27.02, 27.4475, 25.75, 25.675, 25.785, 25.9125, 26.01, 26.6575, 26.445, 27.07, 27.9125, 27.8025, 27.9475, 28.4525, 28.075, 29.32, 29.255, 30.3525, 30.3875, 31.1175, 30.465, 30.795, 31.485, 31.86, 32.9925, 33.81, 34.1375, 33.56, 34.795, 34.375, 34.08, 34.615, 34.865, 34.92, 35.6275, 36.5025, 36.0575, 36.7525, 37.895, 37.9575, 38.9475, 38.1775, 38.87, 38.44, 38.715, 39.1375, 38.5225, 39.1075, 39.0025, 39.7975, 40.06, 39.3325, 39.74, 40.1425, 40.4175, 40.5825, 40.9575, 41.1975, 41.57, 42.47, 41.2275, 40.9625, 41.1825, 41.5525, 41.52, 41.29, 41.25, 41.4775, 42.195, 42.76, 42.28, 42.165, 42.1775, 41.8875, 41.785, 42.7725, 43.105, 43.31, 42.3275, 43.13, 43.23, 43.2825, 43.715, 43.405, 42.79, 43.295, 43.065, 43.35, 43.445, 43.7525, 44.305, 43.8825, 43.7575, 43.965, 44.1075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 1.019, Test loss: 1.170, Test accuracy: 60.16
Average accuracy final 10 rounds: 55.96824999999999
Average global accuracy final 10 rounds: 55.96824999999999
4565.51553106308
[]
[18.285, 23.1175, 33.5675, 36.515, 36.1425, 37.42, 39.35, 41.845, 41.87, 43.2975, 43.285, 43.97, 43.1975, 44.7825, 45.7625, 45.5425, 45.95, 46.51, 47.665, 47.6775, 47.6675, 48.0725, 48.1925, 48.3225, 48.4, 49.2525, 49.62, 49.6725, 49.955, 50.1925, 50.3175, 50.075, 50.325, 50.8875, 50.9225, 51.0475, 50.985, 50.5875, 50.88, 50.9625, 51.03, 51.7425, 52.16, 52.1975, 52.6375, 52.625, 52.955, 52.845, 53.1525, 52.5725, 52.88, 53.0175, 52.8125, 52.88, 53.095, 52.9975, 53.5125, 54.0225, 54.0775, 53.84, 53.9875, 53.83, 53.19, 53.385, 53.9575, 54.3925, 54.245, 54.2425, 54.2625, 54.9475, 54.9325, 55.03, 54.8475, 54.49, 54.3, 54.6125, 54.7925, 55.14, 55.2, 55.62, 55.1225, 55.1025, 55.33, 56.2, 56.0325, 56.5375, 55.565, 55.76, 55.62, 55.85, 55.9425, 56.0825, 55.72, 55.9725, 55.765, 55.665, 55.7525, 56.01, 56.245, 56.5275, 60.16]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.252, Test loss: 2.237, Test accuracy: 20.07
Final Round, Global train loss: 2.252, Global test loss: 2.240, Global test accuracy: 20.60
Average accuracy final 10 rounds: 20.105749999999997 

Average global accuracy final 10 rounds: 20.14425 

2744.2276060581207
[1.5094592571258545, 2.8046834468841553, 3.9187333583831787, 5.033063888549805, 6.154741287231445, 7.273295164108276, 8.38644003868103, 9.503379583358765, 10.623429298400879, 11.741796016693115, 12.857769250869751, 13.973652362823486, 15.096805810928345, 16.213993310928345, 17.334542512893677, 18.44793176651001, 19.562950134277344, 20.683192491531372, 21.79681706428528, 22.911625862121582, 24.01717710494995, 25.124854803085327, 26.228692293167114, 27.34229063987732, 28.443597078323364, 29.5565025806427, 30.669560194015503, 31.774935722351074, 32.881091833114624, 33.99048089981079, 35.10447573661804, 36.2143280506134, 37.32832193374634, 38.44103717803955, 39.55797052383423, 40.67281365394592, 41.78705167770386, 42.89876866340637, 44.013726234436035, 45.12564182281494, 46.254234075546265, 47.37072443962097, 48.48449516296387, 49.60801148414612, 50.727113008499146, 51.84185171127319, 52.9529070854187, 54.07043981552124, 55.18486452102661, 56.30134916305542, 57.422152280807495, 58.534897565841675, 59.659337282180786, 60.77731919288635, 61.909876108169556, 63.022727966308594, 64.1414303779602, 65.2638373374939, 66.38578605651855, 67.49513077735901, 68.61407518386841, 69.72670674324036, 70.8427984714508, 71.95584321022034, 73.06963038444519, 74.19343852996826, 75.31565976142883, 76.43543863296509, 77.55478954315186, 78.67896676063538, 79.79454302787781, 80.91473007202148, 82.03417873382568, 83.14427089691162, 84.25892543792725, 85.3694965839386, 86.48335337638855, 87.60727071762085, 88.73245358467102, 89.84434413909912, 90.95602488517761, 92.0747447013855, 93.18926811218262, 94.31482768058777, 95.43401861190796, 96.55414175987244, 97.66630125045776, 98.7779004573822, 99.89959263801575, 101.01149559020996, 102.12791109085083, 103.2413718700409, 104.34600257873535, 105.44969201087952, 106.55386209487915, 107.6698739528656, 108.7833993434906, 109.88604593276978, 110.99966526031494, 112.10174441337585, 114.31964802742004]
[9.945, 9.9125, 9.93, 9.9375, 9.94, 9.93, 9.9225, 9.915, 10.0025, 10.065, 10.0925, 10.125, 10.185, 10.2475, 10.4725, 10.8375, 11.5275, 11.75, 12.1425, 12.38, 12.6625, 13.2775, 13.7625, 13.855, 14.0675, 14.3425, 14.545, 14.815, 14.775, 14.71, 14.9675, 15.105, 15.235, 15.3425, 15.4975, 15.8075, 16.0975, 16.1525, 16.2425, 16.39, 16.5075, 16.5875, 16.63, 16.69, 16.735, 16.7625, 16.86, 16.9425, 17.1275, 17.155, 17.2325, 17.2125, 17.39, 17.5575, 17.6775, 17.8075, 17.7775, 17.865, 18.2775, 18.5975, 18.565, 18.8325, 18.815, 18.84, 19.1225, 19.22, 19.2625, 19.1175, 19.1375, 19.0525, 18.8075, 18.945, 18.995, 18.885, 19.15, 19.2675, 19.465, 19.4225, 19.43, 19.6075, 19.7025, 19.6575, 19.7625, 19.8125, 19.6825, 19.885, 20.005, 19.9025, 19.9475, 19.9075, 19.99, 19.89, 19.9775, 19.9775, 20.215, 20.17, 20.1625, 20.2275, 20.2775, 20.17, 20.0725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.347, Test loss: 1.054, Test accuracy: 70.08
Average accuracy final 10 rounds: 69.03225
2978.4107291698456
[3.1782991886138916, 6.153939485549927, 9.13397765159607, 12.127253770828247, 15.102937936782837, 18.096007585525513, 21.091163635253906, 24.073572397232056, 27.05077886581421, 30.03474259376526, 33.041316509246826, 36.0389289855957, 39.00965142250061, 42.01278114318848, 44.999024391174316, 47.964383363723755, 51.17153882980347, 54.35471534729004, 57.54787349700928, 60.7452278137207, 63.94228458404541, 67.12812733650208, 70.31842088699341, 73.49959111213684, 76.67741060256958, 79.85217380523682, 83.04455161094666, 86.24950861930847, 89.43630766868591, 92.61610913276672, 95.78570556640625, 98.97031283378601, 102.16190648078918, 105.3503348827362, 108.5273973941803, 111.70313143730164, 114.88407683372498, 118.06605219841003, 121.24484610557556, 124.42313361167908, 127.6117856502533, 130.8020317554474, 133.9955325126648, 137.1914553642273, 140.37115383148193, 143.56216025352478, 146.74295949935913, 149.93579387664795, 153.12399554252625, 156.30310153961182, 159.36413478851318, 162.4660382270813, 165.5690586566925, 168.64887166023254, 171.64078736305237, 174.79734015464783, 177.98453831672668, 181.17978525161743, 184.3718135356903, 187.5593729019165, 190.76161932945251, 193.95089960098267, 197.14153409004211, 200.32660722732544, 203.52029967308044, 206.70633816719055, 209.90296125411987, 213.0724778175354, 216.25780320167542, 219.45661735534668, 222.64067435264587, 225.8283019065857, 229.03745937347412, 232.22406482696533, 235.4090554714203, 238.59352350234985, 241.77762866020203, 244.9671666622162, 248.12721490859985, 251.26863050460815, 254.45543885231018, 257.6110725402832, 260.8068869113922, 263.9746060371399, 267.13627409935, 270.3321816921234, 273.5378966331482, 276.74698853492737, 279.9318814277649, 283.08814096450806, 286.27462220191956, 289.45476770401, 292.66238236427307, 295.8420760631561, 299.0249660015106, 302.2431709766388, 305.44129943847656, 308.65041160583496, 311.8455889225006, 315.0411078929901, 318.2398784160614]
[26.1875, 31.9575, 36.625, 39.9375, 42.6325, 45.5475, 46.01, 48.0625, 49.5925, 50.6675, 52.265, 53.0525, 54.535, 55.2775, 55.295, 55.385, 56.8575, 57.515, 57.695, 58.655, 59.4325, 59.6625, 60.4725, 60.6725, 60.99, 60.9625, 61.81, 61.0375, 62.2875, 62.71, 62.415, 63.69, 63.88, 63.895, 63.84, 64.275, 64.3525, 65.015, 65.56, 65.33, 65.36, 65.5775, 65.925, 65.6475, 65.63, 66.13, 65.98, 66.365, 66.44, 66.73, 66.48, 67.0475, 66.66, 67.495, 67.4525, 66.51, 67.44, 67.7775, 67.5125, 67.5975, 67.445, 67.88, 68.0675, 67.99, 68.4825, 67.9375, 67.675, 67.68, 67.6925, 68.0325, 67.51, 67.775, 68.4825, 68.595, 68.32, 68.55, 68.7625, 69.2925, 69.13, 68.8275, 68.285, 68.97, 68.4975, 68.9675, 68.425, 68.7775, 68.5725, 69.08, 69.28, 68.79, 69.0, 69.365, 68.965, 68.9975, 68.98, 69.055, 69.3925, 69.175, 68.565, 68.8275, 70.085]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.544, Test loss: 0.975, Test accuracy: 67.33
Average accuracy final 10 rounds: 66.879
1798.969604253769
[1.6759135723114014, 3.03499174118042, 4.398715257644653, 5.7581305503845215, 7.12473201751709, 8.483329057693481, 9.847395896911621, 11.204219818115234, 12.615392684936523, 13.973091125488281, 15.361560344696045, 16.64089608192444, 17.860530614852905, 19.085317134857178, 20.325669050216675, 21.55538296699524, 22.801085948944092, 24.023321390151978, 25.245442390441895, 26.470438241958618, 27.691532611846924, 28.919049739837646, 30.144829034805298, 31.37591242790222, 32.602900981903076, 33.82881712913513, 35.05070614814758, 36.286484241485596, 37.53703260421753, 38.76457715034485, 39.98186278343201, 41.20395016670227, 42.437535762786865, 43.6604437828064, 44.892327547073364, 46.12145948410034, 47.35037446022034, 48.57615113258362, 49.801894426345825, 51.022433280944824, 52.25518465042114, 53.47510647773743, 54.704904317855835, 55.93667411804199, 57.16241812705994, 58.39206290245056, 59.60832691192627, 60.84118294715881, 62.0576536655426, 63.28515267372131, 64.50746464729309, 65.7237536907196, 66.94800090789795, 68.16279649734497, 69.37673902511597, 70.60266184806824, 71.82279443740845, 73.04022312164307, 74.27307271957397, 75.48876619338989, 76.71892380714417, 77.92907357215881, 79.14454460144043, 80.36198306083679, 81.57485389709473, 82.79755806922913, 84.01816368103027, 85.23811340332031, 86.45452356338501, 87.67961478233337, 88.90336918830872, 90.1292712688446, 91.34361672401428, 92.57354784011841, 93.78835892677307, 95.0111608505249, 96.22712564468384, 97.4369592666626, 98.66220450401306, 99.87740850448608, 101.09805989265442, 102.31563901901245, 103.54112195968628, 104.76227974891663, 105.98176670074463, 107.19379568099976, 108.40995121002197, 109.6246497631073, 110.83888983726501, 112.05890226364136, 113.27712488174438, 114.49679756164551, 115.71629309654236, 116.93993854522705, 118.15862107276917, 119.38435316085815, 120.60082197189331, 121.81750822067261, 123.03992438316345, 124.2569489479065, 126.23912334442139]
[21.335, 27.8975, 31.56, 34.5175, 37.3475, 39.475, 41.0625, 42.265, 43.0775, 44.1825, 45.0825, 45.2775, 46.6225, 46.9275, 47.5875, 48.4075, 49.04, 49.7675, 50.86, 51.7225, 51.945, 52.555, 53.505, 54.37, 54.4425, 55.1175, 55.6875, 55.0675, 56.7575, 56.8175, 56.62, 57.395, 57.2575, 57.3225, 57.6625, 57.8975, 58.765, 59.4725, 58.785, 58.9275, 59.5975, 59.7575, 60.98, 61.035, 61.0675, 61.1125, 61.8775, 62.4075, 61.88, 62.3475, 62.6275, 62.7775, 63.635, 63.465, 63.76, 63.6625, 63.4325, 64.415, 63.87, 64.1475, 63.97, 64.54, 64.4925, 64.945, 64.6225, 64.4275, 64.3525, 65.02, 65.18, 65.5975, 65.8825, 66.165, 66.065, 66.2, 65.9975, 66.025, 65.9975, 65.73, 66.465, 65.75, 66.1025, 66.4625, 66.44, 66.68, 66.5775, 66.7175, 66.92, 66.6, 67.1175, 66.81, 66.6425, 66.1425, 67.0925, 67.075, 66.92, 66.8425, 67.3525, 66.66, 67.0425, 67.02, 67.3275]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.553, Test loss: 0.979, Test accuracy: 67.43
Average accuracy final 10 rounds: 66.889
2912.0657184123993
[1.7845091819763184, 3.5690183639526367, 5.008523941040039, 6.448029518127441, 7.87844443321228, 9.30885934829712, 10.745048761367798, 12.181238174438477, 13.616379499435425, 15.051520824432373, 16.481515169143677, 17.91150951385498, 19.34483504295349, 20.778160572052002, 22.16747546195984, 23.556790351867676, 24.987043380737305, 26.417296409606934, 27.84633731842041, 29.275378227233887, 30.697261571884155, 32.119144916534424, 33.54241871833801, 34.9656925201416, 36.39944100379944, 37.833189487457275, 39.26385235786438, 40.694515228271484, 42.12830138206482, 43.562087535858154, 44.838831186294556, 46.11557483673096, 47.55594182014465, 48.99630880355835, 50.38849115371704, 51.78067350387573, 53.2016224861145, 54.62257146835327, 56.06152892112732, 57.50048637390137, 58.94696307182312, 60.39343976974487, 61.84424901008606, 63.295058250427246, 64.7348780632019, 66.17469787597656, 67.60599732398987, 69.03729677200317, 70.48868155479431, 71.94006633758545, 73.38411402702332, 74.82816171646118, 76.27636075019836, 77.72455978393555, 79.17185950279236, 80.61915922164917, 82.07371282577515, 83.52826642990112, 84.98010540008545, 86.43194437026978, 87.88260865211487, 89.33327293395996, 90.78605914115906, 92.23884534835815, 93.68565607070923, 95.1324667930603, 96.58449864387512, 98.03653049468994, 99.47952556610107, 100.9225206375122, 102.35407543182373, 103.78563022613525, 105.22179532051086, 106.65796041488647, 108.11156010627747, 109.56515979766846, 111.01041841506958, 112.4556770324707, 113.90544939041138, 115.35522174835205, 116.80093598365784, 118.24665021896362, 119.6909921169281, 121.13533401489258, 122.57663059234619, 124.0179271697998, 125.46087384223938, 126.90382051467896, 128.34521579742432, 129.78661108016968, 131.2184772491455, 132.65034341812134, 134.06896305084229, 135.48758268356323, 136.85310578346252, 138.21862888336182, 139.63691186904907, 141.05519485473633, 142.47693824768066, 143.898681640625, 145.30601692199707, 146.71335220336914, 148.17941904067993, 149.64548587799072, 151.02490258216858, 152.40431928634644, 153.7895495891571, 155.17477989196777, 156.55167984962463, 157.9285798072815, 159.1811077594757, 160.43363571166992, 161.67518639564514, 162.91673707962036, 164.15038013458252, 165.38402318954468, 166.6183111667633, 167.85259914398193, 169.0916817188263, 170.33076429367065, 171.56267619132996, 172.79458808898926, 174.02712178230286, 175.25965547561646, 176.4796531200409, 177.69965076446533, 178.93335485458374, 180.16705894470215, 181.39069080352783, 182.61432266235352, 183.98581838607788, 185.35731410980225, 186.5915994644165, 187.82588481903076, 189.04432153701782, 190.26275825500488, 191.48324275016785, 192.7037272453308, 193.93151140213013, 195.15929555892944, 196.3928771018982, 197.62645864486694, 198.85580015182495, 200.08514165878296, 201.31735801696777, 202.5495743751526, 203.7803499698639, 205.0111255645752, 206.23927640914917, 207.46742725372314, 208.70300769805908, 209.93858814239502, 211.17403388023376, 212.4094796180725, 213.8033275604248, 215.1971755027771, 216.5865659713745, 217.97595643997192, 219.34600830078125, 220.71606016159058, 222.09571242332458, 223.4753646850586, 224.8353145122528, 226.19526433944702, 227.5892996788025, 228.98333501815796, 230.36844897270203, 231.7535629272461, 233.14321374893188, 234.53286457061768, 235.91664624214172, 237.30042791366577, 238.69648027420044, 240.0925326347351, 241.47315788269043, 242.85378313064575, 244.24320006370544, 245.63261699676514, 247.0195038318634, 248.40639066696167, 249.64110231399536, 250.87581396102905, 252.1135289669037, 253.35124397277832, 254.58439826965332, 255.81755256652832, 257.05032086372375, 258.2830891609192, 259.5213027000427, 260.75951623916626, 261.98889780044556, 263.21827936172485, 264.4462971687317, 265.6743149757385, 266.90243577957153, 268.13055658340454, 269.497111082077, 270.8636655807495, 272.228640794754, 273.59361600875854, 275.6737220287323, 277.75382804870605]
[19.235, 19.235, 27.5125, 27.5125, 30.74, 30.74, 34.0225, 34.0225, 36.3925, 36.3925, 38.9175, 38.9175, 40.8975, 40.8975, 42.375, 42.375, 43.5575, 43.5575, 44.085, 44.085, 45.585, 45.585, 46.1275, 46.1275, 46.515, 46.515, 48.125, 48.125, 49.025, 49.025, 49.4875, 49.4875, 50.315, 50.315, 50.69, 50.69, 51.2075, 51.2075, 52.1, 52.1, 53.585, 53.585, 54.2925, 54.2925, 54.3725, 54.3725, 54.6575, 54.6575, 54.475, 54.475, 55.25, 55.25, 56.4425, 56.4425, 56.7975, 56.7975, 56.33, 56.33, 56.335, 56.335, 56.7175, 56.7175, 58.08, 58.08, 58.125, 58.125, 58.5275, 58.5275, 59.08, 59.08, 59.275, 59.275, 59.715, 59.715, 60.035, 60.035, 60.0875, 60.0875, 60.9875, 60.9875, 61.2725, 61.2725, 61.1875, 61.1875, 61.5475, 61.5475, 62.1225, 62.1225, 61.7775, 61.7775, 62.0025, 62.0025, 61.895, 61.895, 62.8025, 62.8025, 62.7875, 62.7875, 63.1675, 63.1675, 62.575, 62.575, 63.74, 63.74, 63.8925, 63.8925, 63.6275, 63.6275, 64.0525, 64.0525, 63.685, 63.685, 63.9325, 63.9325, 64.1575, 64.1575, 64.23, 64.23, 64.5725, 64.5725, 64.66, 64.66, 64.83, 64.83, 64.66, 64.66, 64.605, 64.605, 64.6225, 64.6225, 64.635, 64.635, 64.8475, 64.8475, 65.01, 65.01, 65.0475, 65.0475, 65.74, 65.74, 65.33, 65.33, 65.8025, 65.8025, 65.4775, 65.4775, 66.575, 66.575, 65.5225, 65.5225, 66.0025, 66.0025, 66.1675, 66.1675, 65.89, 65.89, 66.1025, 66.1025, 65.8475, 65.8475, 66.005, 66.005, 66.165, 66.165, 66.0975, 66.0975, 66.74, 66.74, 66.405, 66.405, 66.4075, 66.4075, 66.93, 66.93, 66.46, 66.46, 66.9175, 66.9175, 66.9075, 66.9075, 66.3, 66.3, 66.7025, 66.7025, 66.1525, 66.1525, 66.6525, 66.6525, 67.0925, 67.0925, 67.225, 67.225, 67.285, 67.285, 67.5125, 67.5125, 67.0675, 67.0675, 66.9, 66.9, 67.4275, 67.4275]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.066, Test loss: 1.248, Test accuracy: 76.97
Final Round, Global train loss: 0.066, Global test loss: 1.367, Global test accuracy: 38.09
Average accuracy final 10 rounds: 76.73500000000001 

Average global accuracy final 10 rounds: 38.219166666666666 

1653.2676424980164
[1.426077127456665, 2.85215425491333, 4.039663791656494, 5.227173328399658, 6.414833307266235, 7.6024932861328125, 8.786271572113037, 9.970049858093262, 11.154321432113647, 12.338593006134033, 13.525642395019531, 14.71269178390503, 15.897947788238525, 17.08320379257202, 18.266684770584106, 19.45016574859619, 20.645933866500854, 21.841701984405518, 23.03696894645691, 24.2322359085083, 25.428666830062866, 26.62509775161743, 27.819672107696533, 29.014246463775635, 30.20577120780945, 31.39729595184326, 32.590027809143066, 33.78275966644287, 34.966681480407715, 36.15060329437256, 37.33432078361511, 38.518038272857666, 39.69693613052368, 40.8758339881897, 42.06179642677307, 43.247758865356445, 44.364171743392944, 45.48058462142944, 46.66010808944702, 47.8396315574646, 49.031566858291626, 50.22350215911865, 51.41791033744812, 52.61231851577759, 53.80501937866211, 54.99772024154663, 56.18600559234619, 57.37429094314575, 58.573030948638916, 59.77177095413208, 60.96266412734985, 62.15355730056763, 63.266780853271484, 64.38000440597534, 65.56582355499268, 66.75164270401001, 67.86976909637451, 68.98789548873901, 70.10306692123413, 71.21823835372925, 72.33622241020203, 73.4542064666748, 74.57171988487244, 75.68923330307007, 76.82055330276489, 77.95187330245972, 79.12956237792969, 80.30725145339966, 81.43835425376892, 82.56945705413818, 83.7037718296051, 84.83808660507202, 85.96885871887207, 87.09963083267212, 88.29180479049683, 89.48397874832153, 90.67611646652222, 91.8682541847229, 93.0638644695282, 94.2594747543335, 95.45294952392578, 96.64642429351807, 97.83842873573303, 99.030433177948, 100.22339820861816, 101.41636323928833, 102.60846209526062, 103.80056095123291, 104.99234938621521, 106.18413782119751, 107.37578845024109, 108.56743907928467, 109.75842761993408, 110.9494161605835, 112.13960385322571, 113.32979154586792, 114.52281546592712, 115.71583938598633, 116.90905380249023, 118.10226821899414, 119.29702377319336, 120.49177932739258, 121.68983292579651, 122.88788652420044, 124.08975315093994, 125.29161977767944, 126.48614954948425, 127.68067932128906, 128.87541127204895, 130.07014322280884, 131.26015877723694, 132.45017433166504, 133.64562392234802, 134.841073513031, 136.04070591926575, 137.2403383255005, 138.436297416687, 139.63225650787354, 140.83013772964478, 142.02801895141602, 143.2213864326477, 144.4147539138794, 145.60616540908813, 146.79757690429688, 147.98939847946167, 149.18122005462646, 150.37293481826782, 151.56464958190918, 152.75192761421204, 153.9392056465149, 155.14895343780518, 156.35870122909546, 157.56613945960999, 158.7735776901245, 159.98525547981262, 161.19693326950073, 162.40091562271118, 163.60489797592163, 164.8071322441101, 166.00936651229858, 167.2173457145691, 168.4253249168396, 169.6208484172821, 170.8163719177246, 172.01618790626526, 173.2160038948059, 174.41091752052307, 175.60583114624023, 176.80527448654175, 178.00471782684326, 179.2043318748474, 180.40394592285156, 181.60511016845703, 182.8062744140625, 184.00067496299744, 185.19507551193237, 186.40732622146606, 187.61957693099976, 188.8110649585724, 190.00255298614502, 191.20064187049866, 192.3987307548523, 193.59669589996338, 194.79466104507446, 195.98746848106384, 197.18027591705322, 198.37542295455933, 199.57056999206543, 200.77096891403198, 201.97136783599854, 203.17477583885193, 204.37818384170532, 205.57194828987122, 206.7657127380371, 207.9546959400177, 209.1436791419983, 210.19872117042542, 211.25376319885254, 212.30276894569397, 213.3517746925354, 214.40273714065552, 215.45369958877563, 216.50822710990906, 217.56275463104248, 218.61955571174622, 219.67635679244995, 220.7373104095459, 221.79826402664185, 222.86056637763977, 223.9228687286377, 224.99667239189148, 226.07047605514526, 227.13110041618347, 228.19172477722168, 229.2528374195099, 230.3139500617981, 231.43892526626587, 232.56390047073364, 233.61986684799194, 234.67583322525024, 236.79355573654175, 238.91127824783325]
[43.1, 43.1, 51.61666666666667, 51.61666666666667, 56.1, 56.1, 63.05833333333333, 63.05833333333333, 65.38333333333334, 65.38333333333334, 65.13333333333334, 65.13333333333334, 68.6, 68.6, 69.575, 69.575, 70.63333333333334, 70.63333333333334, 70.84166666666667, 70.84166666666667, 71.75, 71.75, 72.40833333333333, 72.40833333333333, 72.35833333333333, 72.35833333333333, 72.39166666666667, 72.39166666666667, 73.05, 73.05, 73.54166666666667, 73.54166666666667, 73.89166666666667, 73.89166666666667, 73.5, 73.5, 74.09166666666667, 74.09166666666667, 74.26666666666667, 74.26666666666667, 74.63333333333334, 74.63333333333334, 74.61666666666666, 74.61666666666666, 74.99166666666666, 74.99166666666666, 74.95833333333333, 74.95833333333333, 74.68333333333334, 74.68333333333334, 75.20833333333333, 75.20833333333333, 75.425, 75.425, 75.20833333333333, 75.20833333333333, 74.775, 74.775, 74.75833333333334, 74.75833333333334, 75.08333333333333, 75.08333333333333, 74.775, 74.775, 74.91666666666667, 74.91666666666667, 74.95833333333333, 74.95833333333333, 75.275, 75.275, 74.975, 74.975, 75.225, 75.225, 74.98333333333333, 74.98333333333333, 75.28333333333333, 75.28333333333333, 75.48333333333333, 75.48333333333333, 75.1, 75.1, 74.94166666666666, 74.94166666666666, 74.99166666666666, 74.99166666666666, 75.48333333333333, 75.48333333333333, 75.38333333333334, 75.38333333333334, 75.38333333333334, 75.38333333333334, 76.26666666666667, 76.26666666666667, 76.13333333333334, 76.13333333333334, 76.05833333333334, 76.05833333333334, 76.19166666666666, 76.19166666666666, 76.10833333333333, 76.10833333333333, 76.19166666666666, 76.19166666666666, 76.09166666666667, 76.09166666666667, 76.10833333333333, 76.10833333333333, 76.55, 76.55, 76.01666666666667, 76.01666666666667, 75.90833333333333, 75.90833333333333, 76.025, 76.025, 76.40833333333333, 76.40833333333333, 76.19166666666666, 76.19166666666666, 75.76666666666667, 75.76666666666667, 75.93333333333334, 75.93333333333334, 76.08333333333333, 76.08333333333333, 75.89166666666667, 75.89166666666667, 75.675, 75.675, 75.68333333333334, 75.68333333333334, 75.725, 75.725, 75.525, 75.525, 75.71666666666667, 75.71666666666667, 75.93333333333334, 75.93333333333334, 75.95, 75.95, 76.425, 76.425, 76.4, 76.4, 76.56666666666666, 76.56666666666666, 76.68333333333334, 76.68333333333334, 76.83333333333333, 76.83333333333333, 76.65, 76.65, 76.44166666666666, 76.44166666666666, 76.025, 76.025, 76.39166666666667, 76.39166666666667, 75.95833333333333, 75.95833333333333, 75.93333333333334, 75.93333333333334, 76.075, 76.075, 75.90833333333333, 75.90833333333333, 75.78333333333333, 75.78333333333333, 75.625, 75.625, 75.70833333333333, 75.70833333333333, 76.075, 76.075, 76.63333333333334, 76.63333333333334, 76.9, 76.9, 76.89166666666667, 76.89166666666667, 77.09166666666667, 77.09166666666667, 76.73333333333333, 76.73333333333333, 76.4, 76.4, 76.29166666666667, 76.29166666666667, 77.15, 77.15, 76.75833333333334, 76.75833333333334, 76.575, 76.575, 76.81666666666666, 76.81666666666666, 76.64166666666667, 76.64166666666667, 76.975, 76.975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.194, Test loss: 0.661, Test accuracy: 80.83
Final Round, Global train loss: 0.194, Global test loss: 1.532, Global test accuracy: 46.36
Average accuracy final 10 rounds: 81.17916666666666 

Average global accuracy final 10 rounds: 45.82666666666667 

1722.317059993744
[1.6450448036193848, 3.2900896072387695, 4.6793131828308105, 6.068536758422852, 7.461817741394043, 8.855098724365234, 10.265807390213013, 11.676516056060791, 13.074554920196533, 14.472593784332275, 15.869765996932983, 17.26693820953369, 18.659975290298462, 20.053012371063232, 21.44978094100952, 22.84654951095581, 24.24513268470764, 25.643715858459473, 27.042665243148804, 28.441614627838135, 29.83768367767334, 31.233752727508545, 32.63558101654053, 34.03740930557251, 35.42379355430603, 36.81017780303955, 38.20491003990173, 39.599642276763916, 40.98820471763611, 42.3767671585083, 43.768808126449585, 45.16084909439087, 46.55363130569458, 47.94641351699829, 49.339499950408936, 50.73258638381958, 52.12137413024902, 53.51016187667847, 54.89995503425598, 56.289748191833496, 57.661810636520386, 59.033873081207275, 60.404215812683105, 61.774558544158936, 63.16615915298462, 64.5577597618103, 65.95665955543518, 67.35555934906006, 68.7446539402008, 70.13374853134155, 71.52012658119202, 72.90650463104248, 74.28928303718567, 75.67206144332886, 77.04752445220947, 78.42298746109009, 79.79807543754578, 81.17316341400146, 82.5496129989624, 83.92606258392334, 85.30686902999878, 86.68767547607422, 88.06885552406311, 89.450035572052, 90.82886171340942, 92.20768785476685, 93.58813071250916, 94.96857357025146, 96.35615730285645, 97.74374103546143, 99.12327027320862, 100.50279951095581, 101.88143253326416, 103.26006555557251, 104.64352464675903, 106.02698373794556, 107.40958571434021, 108.79218769073486, 110.18243098258972, 111.57267427444458, 112.95866250991821, 114.34465074539185, 115.72014546394348, 117.09564018249512, 118.48432564735413, 119.87301111221313, 121.25662994384766, 122.64024877548218, 124.01804876327515, 125.39584875106812, 126.77212262153625, 128.1483964920044, 129.53236031532288, 130.91632413864136, 132.3040211200714, 133.69171810150146, 135.0795521736145, 136.46738624572754, 137.85163593292236, 139.2358856201172, 140.6223590373993, 142.0088324546814, 143.3961362838745, 144.78344011306763, 146.16776371002197, 147.55208730697632, 148.9325840473175, 150.3130807876587, 151.6930329799652, 153.07298517227173, 154.44884538650513, 155.82470560073853, 157.21505665779114, 158.60540771484375, 159.98971796035767, 161.37402820587158, 162.7604558467865, 164.14688348770142, 165.53320026397705, 166.91951704025269, 168.3067421913147, 169.6939673423767, 171.0770125389099, 172.46005773544312, 173.84726881980896, 175.2344799041748, 176.6339750289917, 178.0334701538086, 179.41553926467896, 180.79760837554932, 182.18139004707336, 183.5651717185974, 184.94191074371338, 186.31864976882935, 187.70172452926636, 189.08479928970337, 190.46058416366577, 191.83636903762817, 193.21613550186157, 194.59590196609497, 195.980220079422, 197.36453819274902, 198.7481825351715, 200.131826877594, 201.5173900127411, 202.90295314788818, 204.28855538368225, 205.67415761947632, 207.05598878860474, 208.43781995773315, 209.81480741500854, 211.19179487228394, 212.57200932502747, 213.952223777771, 215.3340721130371, 216.71592044830322, 218.0872609615326, 219.45860147476196, 220.8406240940094, 222.22264671325684, 223.59498715400696, 224.96732759475708, 226.34556770324707, 227.72380781173706, 229.10832953453064, 230.49285125732422, 231.87699794769287, 233.26114463806152, 234.64473581314087, 236.02832698822021, 237.414781332016, 238.80123567581177, 240.1863558292389, 241.57147598266602, 242.95621585845947, 244.34095573425293, 245.7295515537262, 247.11814737319946, 248.50341749191284, 249.88868761062622, 251.27367854118347, 252.65866947174072, 254.04291319847107, 255.42715692520142, 256.8105535507202, 258.193950176239, 259.5801498889923, 260.9663496017456, 262.34688806533813, 263.72742652893066, 265.1057879924774, 266.48414945602417, 267.87077736854553, 269.2574052810669, 270.6279044151306, 271.99840354919434, 273.37012934684753, 274.74185514450073, 276.1227698326111, 277.50368452072144, 279.7996370792389, 282.09558963775635]
[41.43333333333333, 41.43333333333333, 49.725, 49.725, 54.266666666666666, 54.266666666666666, 57.05833333333333, 57.05833333333333, 56.075, 56.075, 66.88333333333334, 66.88333333333334, 70.325, 70.325, 71.33333333333333, 71.33333333333333, 71.80833333333334, 71.80833333333334, 72.44166666666666, 72.44166666666666, 72.55, 72.55, 73.58333333333333, 73.58333333333333, 73.68333333333334, 73.68333333333334, 74.88333333333334, 74.88333333333334, 75.16666666666667, 75.16666666666667, 75.16666666666667, 75.16666666666667, 75.0, 75.0, 74.85, 74.85, 75.66666666666667, 75.66666666666667, 75.2, 75.2, 75.7, 75.7, 76.05, 76.05, 76.73333333333333, 76.73333333333333, 77.625, 77.625, 78.05833333333334, 78.05833333333334, 78.31666666666666, 78.31666666666666, 78.39166666666667, 78.39166666666667, 78.63333333333334, 78.63333333333334, 78.48333333333333, 78.48333333333333, 78.30833333333334, 78.30833333333334, 78.11666666666666, 78.11666666666666, 77.69166666666666, 77.69166666666666, 78.33333333333333, 78.33333333333333, 78.15833333333333, 78.15833333333333, 78.575, 78.575, 78.525, 78.525, 78.375, 78.375, 78.725, 78.725, 78.675, 78.675, 78.51666666666667, 78.51666666666667, 78.88333333333334, 78.88333333333334, 78.89166666666667, 78.89166666666667, 78.93333333333334, 78.93333333333334, 79.15833333333333, 79.15833333333333, 79.3, 79.3, 79.55833333333334, 79.55833333333334, 79.875, 79.875, 79.99166666666666, 79.99166666666666, 79.64166666666667, 79.64166666666667, 80.05833333333334, 80.05833333333334, 80.59166666666667, 80.59166666666667, 80.29166666666667, 80.29166666666667, 80.175, 80.175, 80.15833333333333, 80.15833333333333, 79.90833333333333, 79.90833333333333, 80.05833333333334, 80.05833333333334, 80.08333333333333, 80.08333333333333, 79.925, 79.925, 79.9, 79.9, 80.04166666666667, 80.04166666666667, 79.91666666666667, 79.91666666666667, 79.83333333333333, 79.83333333333333, 79.625, 79.625, 79.90833333333333, 79.90833333333333, 79.69166666666666, 79.69166666666666, 79.80833333333334, 79.80833333333334, 79.83333333333333, 79.83333333333333, 80.28333333333333, 80.28333333333333, 80.575, 80.575, 80.60833333333333, 80.60833333333333, 80.475, 80.475, 80.55833333333334, 80.55833333333334, 80.3, 80.3, 80.05833333333334, 80.05833333333334, 80.31666666666666, 80.31666666666666, 79.975, 79.975, 80.14166666666667, 80.14166666666667, 79.80833333333334, 79.80833333333334, 79.275, 79.275, 79.75833333333334, 79.75833333333334, 80.16666666666667, 80.16666666666667, 80.575, 80.575, 80.83333333333333, 80.83333333333333, 81.33333333333333, 81.33333333333333, 81.36666666666666, 81.36666666666666, 81.36666666666666, 81.36666666666666, 80.94166666666666, 80.94166666666666, 81.21666666666667, 81.21666666666667, 80.80833333333334, 80.80833333333334, 81.125, 81.125, 81.28333333333333, 81.28333333333333, 80.85833333333333, 80.85833333333333, 80.86666666666666, 80.86666666666666, 81.04166666666667, 81.04166666666667, 81.34166666666667, 81.34166666666667, 81.26666666666667, 81.26666666666667, 81.24166666666666, 81.24166666666666, 81.14166666666667, 81.14166666666667, 81.29166666666667, 81.29166666666667, 81.45833333333333, 81.45833333333333, 80.83333333333333, 80.83333333333333]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 7939 (global); Percentage 2.58 (7939/307387 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

python: can't open file 'main_fedpac_k.py': [Errno 2] No such file or directory
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307384
307387
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.549, Test loss: 1.622, Test accuracy: 83.97
Final Round, Global train loss: 1.549, Global test loss: 1.623, Global test accuracy: 83.84
Average accuracy final 10 rounds: 83.837 

Average global accuracy final 10 rounds: 83.59649999999999 

4285.290937662125
[3.153989791870117, 6.307979583740234, 9.44290542602539, 12.577831268310547, 15.64026427268982, 18.702697277069092, 21.79485583305359, 24.887014389038086, 28.017870903015137, 31.148727416992188, 34.27760124206543, 37.40647506713867, 40.50376272201538, 43.60105037689209, 46.678056478500366, 49.75506258010864, 52.38529968261719, 55.01553678512573, 57.645612716674805, 60.27568864822388, 62.88087201118469, 65.48605537414551, 68.14147806167603, 70.79690074920654, 73.43310141563416, 76.06930208206177, 78.7077271938324, 81.34615230560303, 84.03907823562622, 86.73200416564941, 89.40173530578613, 92.07146644592285, 94.63297414779663, 97.19448184967041, 99.87174367904663, 102.54900550842285, 105.22569060325623, 107.9023756980896, 110.50898599624634, 113.11559629440308, 115.73024129867554, 118.344886302948, 120.9798858165741, 123.6148853302002, 126.24572229385376, 128.87655925750732, 131.50892519950867, 134.14129114151, 136.780620098114, 139.41994905471802, 142.04800152778625, 144.6760540008545, 147.31649947166443, 149.95694494247437, 152.640061378479, 155.32317781448364, 157.95470118522644, 160.58622455596924, 163.14623165130615, 165.70623874664307, 168.38608074188232, 171.06592273712158, 173.74514603614807, 176.42436933517456, 179.01863741874695, 181.61290550231934, 184.240642786026, 186.86838006973267, 189.5387020111084, 192.20902395248413, 194.8315041065216, 197.45398426055908, 200.08586049079895, 202.71773672103882, 205.33947896957397, 207.96122121810913, 210.59196972846985, 213.22271823883057, 215.89564752578735, 218.56857681274414, 221.26095628738403, 223.95333576202393, 226.5951271057129, 229.23691844940186, 231.85389304161072, 234.47086763381958, 237.15016078948975, 239.8294539451599, 242.50781536102295, 245.186176776886, 247.7275583744049, 250.26893997192383, 252.90731239318848, 255.54568481445312, 258.220911026001, 260.8961372375488, 263.54339051246643, 266.19064378738403, 268.8262724876404, 271.46190118789673, 274.09792041778564, 276.73393964767456, 279.3636426925659, 281.9933457374573, 284.6709485054016, 287.34855127334595, 290.0480020046234, 292.7474527359009, 295.3749723434448, 298.00249195098877, 300.6464502811432, 303.2904086112976, 305.9657759666443, 308.64114332199097, 311.30622386932373, 313.9713044166565, 316.5571126937866, 319.14292097091675, 321.81470799446106, 324.48649501800537, 327.1345772743225, 329.78265953063965, 332.4120771884918, 335.041494846344, 337.6534562110901, 340.2654175758362, 342.998854637146, 345.7322916984558, 348.4901907444, 351.24808979034424, 354.05618238449097, 356.8642749786377, 359.6643867492676, 362.46449851989746, 365.1388850212097, 367.813271522522, 370.55559062957764, 373.2979097366333, 376.1027765274048, 378.90764331817627, 381.6965174674988, 384.4853916168213, 387.1472113132477, 389.8090310096741, 392.48982191085815, 395.17061281204224, 397.95312428474426, 400.7356357574463, 403.4962396621704, 406.25684356689453, 409.00393176078796, 411.7510199546814, 414.49925923347473, 417.24749851226807, 419.9903984069824, 422.7332983016968, 425.4573178291321, 428.1813373565674, 430.93122911453247, 433.68112087249756, 436.45615553855896, 439.23119020462036, 441.88506269454956, 444.53893518447876, 447.28865480422974, 450.0383744239807, 452.8430314064026, 455.64768838882446, 458.4657609462738, 461.28383350372314, 463.97276425361633, 466.6616950035095, 469.28805208206177, 471.914409160614, 474.6951379776001, 477.4758667945862, 480.23245453834534, 482.9890422821045, 485.7197415828705, 488.4504408836365, 491.1937837600708, 493.9371266365051, 496.59523010253906, 499.253333568573, 501.9483664035797, 504.6433992385864, 507.41423082351685, 510.18506240844727, 513.020562171936, 515.8560619354248, 518.6006526947021, 521.3452434539795, 524.0520513057709, 526.7588591575623, 529.5367798805237, 532.3147006034851, 535.063729763031, 537.8127589225769, 540.5520684719086, 543.2913780212402, 544.6879892349243, 546.0846004486084]
[43.4025, 43.4025, 65.36, 65.36, 74.2225, 74.2225, 77.145, 77.145, 77.9625, 77.9625, 79.6225, 79.6225, 80.7325, 80.7325, 80.81, 80.81, 81.185, 81.185, 81.33, 81.33, 81.3375, 81.3375, 81.275, 81.275, 81.37, 81.37, 81.6125, 81.6125, 81.9925, 81.9925, 82.12, 82.12, 82.0925, 82.0925, 82.15, 82.15, 82.2075, 82.2075, 82.205, 82.205, 82.17, 82.17, 82.3275, 82.3275, 82.42, 82.42, 82.3925, 82.3925, 82.3575, 82.3575, 82.3425, 82.3425, 82.335, 82.335, 82.38, 82.38, 82.3825, 82.3825, 82.365, 82.365, 82.4025, 82.4025, 82.415, 82.415, 82.425, 82.425, 82.4375, 82.4375, 82.435, 82.435, 82.4425, 82.4425, 82.845, 82.845, 82.8475, 82.8475, 82.8325, 82.8325, 82.8125, 82.8125, 82.825, 82.825, 82.855, 82.855, 82.8675, 82.8675, 82.83, 82.83, 82.8475, 82.8475, 82.835, 82.835, 82.8225, 82.8225, 82.835, 82.835, 82.7725, 82.7725, 82.775, 82.775, 82.815, 82.815, 82.8475, 82.8475, 82.8425, 82.8425, 82.825, 82.825, 82.8625, 82.8625, 82.8575, 82.8575, 82.8525, 82.8525, 82.8525, 82.8525, 82.8675, 82.8675, 82.865, 82.865, 82.85, 82.85, 82.86, 82.86, 82.8575, 82.8575, 82.85, 82.85, 82.8625, 82.8625, 82.8775, 82.8775, 82.87, 82.87, 82.8625, 82.8625, 82.8625, 82.8625, 82.845, 82.845, 83.13, 83.13, 83.1425, 83.1425, 83.1675, 83.1675, 83.1775, 83.1775, 83.1825, 83.1825, 83.19, 83.19, 83.1975, 83.1975, 83.21, 83.21, 83.215, 83.215, 83.245, 83.245, 83.255, 83.255, 83.25, 83.25, 83.2125, 83.2125, 83.2025, 83.2025, 83.555, 83.555, 83.555, 83.555, 83.5725, 83.5725, 83.5725, 83.5725, 83.57, 83.57, 83.585, 83.585, 83.5925, 83.5925, 83.5625, 83.5625, 83.55, 83.55, 83.925, 83.925, 83.94, 83.94, 83.9525, 83.9525, 83.9375, 83.9375, 83.965, 83.965, 83.9725, 83.9725, 83.9725, 83.9725, 83.975, 83.975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.493, Test loss: 1.538, Test accuracy: 92.50
Final Round, Global train loss: 1.493, Global test loss: 1.535, Global test accuracy: 92.95
Average accuracy final 10 rounds: 92.56 

Average global accuracy final 10 rounds: 92.80333333333336 

727.2801961898804
[0.5871713161468506, 1.1743426322937012, 1.672114372253418, 2.1698861122131348, 2.675518751144409, 3.1811513900756836, 3.694732904434204, 4.208314418792725, 4.716522455215454, 5.224730491638184, 5.727819919586182, 6.23090934753418, 6.74533224105835, 7.2597551345825195, 7.7758119106292725, 8.291868686676025, 8.808857440948486, 9.325846195220947, 9.842485666275024, 10.359125137329102, 10.87195611000061, 11.38478708267212, 11.899868726730347, 12.414950370788574, 12.931874752044678, 13.448799133300781, 13.971772909164429, 14.494746685028076, 15.021087884902954, 15.547429084777832, 16.069389581680298, 16.591350078582764, 17.11462163925171, 17.637893199920654, 18.166335105895996, 18.694777011871338, 19.21769142150879, 19.74060583114624, 20.26274347305298, 20.784881114959717, 21.296321153640747, 21.807761192321777, 22.32321572303772, 22.838670253753662, 23.363539695739746, 23.88840913772583, 24.407052755355835, 24.92569637298584, 25.439599990844727, 25.953503608703613, 26.474668264389038, 26.995832920074463, 27.525603771209717, 28.05537462234497, 28.58319616317749, 29.11101770401001, 29.638019800186157, 30.165021896362305, 30.692843437194824, 31.220664978027344, 31.736817359924316, 32.25296974182129, 32.77305722236633, 33.29314470291138, 33.816118001937866, 34.339091300964355, 34.850773096084595, 35.362454891204834, 35.87083435058594, 36.37921380996704, 36.80563259124756, 37.232051372528076, 37.670839071273804, 38.10962677001953, 38.563172817230225, 39.01671886444092, 39.451279401779175, 39.88583993911743, 40.331995725631714, 40.778151512145996, 41.22223162651062, 41.666311740875244, 42.09805655479431, 42.52980136871338, 42.94355487823486, 43.35730838775635, 43.80402612686157, 44.2507438659668, 44.695196866989136, 45.139649868011475, 45.57868146896362, 46.01771306991577, 46.4445686340332, 46.871424198150635, 47.31039476394653, 47.74936532974243, 48.19634199142456, 48.64331865310669, 49.08497071266174, 49.5266227722168, 49.94830632209778, 50.36998987197876, 50.8078408241272, 51.245691776275635, 51.68303418159485, 52.12037658691406, 52.54925537109375, 52.97813415527344, 53.405303955078125, 53.83247375488281, 54.27204751968384, 54.71162128448486, 55.1596474647522, 55.60767364501953, 56.041358947753906, 56.47504425048828, 56.91277074813843, 57.350497245788574, 57.77179956436157, 58.19310188293457, 58.62727451324463, 59.06144714355469, 59.48690462112427, 59.91236209869385, 60.33710193634033, 60.761841773986816, 61.20290923118591, 61.64397668838501, 62.08005666732788, 62.51613664627075, 62.947325229644775, 63.3785138130188, 63.81237769126892, 64.24624156951904, 64.67851734161377, 65.1107931137085, 65.54209327697754, 65.97339344024658, 66.40408635139465, 66.83477926254272, 67.26886653900146, 67.7029538154602, 68.11426711082458, 68.52558040618896, 68.95359444618225, 69.38160848617554, 69.82159757614136, 70.26158666610718, 70.70518398284912, 71.14878129959106, 71.58008623123169, 72.01139116287231, 72.43845438957214, 72.86551761627197, 73.30540108680725, 73.74528455734253, 74.18690156936646, 74.62851858139038, 75.06773591041565, 75.50695323944092, 75.92099094390869, 76.33502864837646, 76.77691292762756, 77.21879720687866, 77.66907167434692, 78.11934614181519, 78.54973530769348, 78.98012447357178, 79.40289664268494, 79.8256688117981, 80.27059412002563, 80.71551942825317, 81.16033434867859, 81.605149269104, 82.04193949699402, 82.47872972488403, 82.90228700637817, 83.32584428787231, 83.75616955757141, 84.18649482727051, 84.62784147262573, 85.06918811798096, 85.5065107345581, 85.94383335113525, 86.37376737594604, 86.80370140075684, 87.24743032455444, 87.69115924835205, 88.1370496749878, 88.58294010162354, 89.0064754486084, 89.43001079559326, 89.86363577842712, 90.29726076126099, 90.73600339889526, 91.17474603652954, 91.61259937286377, 92.050452709198, 92.48118758201599, 92.91192245483398, 93.79554200172424, 94.6791615486145]
[15.266666666666667, 15.266666666666667, 16.933333333333334, 16.933333333333334, 16.433333333333334, 16.433333333333334, 16.1, 16.1, 20.516666666666666, 20.516666666666666, 25.3, 25.3, 29.95, 29.95, 33.9, 33.9, 41.13333333333333, 41.13333333333333, 48.03333333333333, 48.03333333333333, 56.15, 56.15, 62.083333333333336, 62.083333333333336, 64.13333333333334, 64.13333333333334, 67.53333333333333, 67.53333333333333, 73.53333333333333, 73.53333333333333, 75.05, 75.05, 79.61666666666666, 79.61666666666666, 83.73333333333333, 83.73333333333333, 84.61666666666666, 84.61666666666666, 87.4, 87.4, 87.53333333333333, 87.53333333333333, 87.91666666666667, 87.91666666666667, 88.08333333333333, 88.08333333333333, 88.25, 88.25, 88.38333333333334, 88.38333333333334, 88.36666666666666, 88.36666666666666, 88.66666666666667, 88.66666666666667, 88.71666666666667, 88.71666666666667, 88.76666666666667, 88.76666666666667, 88.88333333333334, 88.88333333333334, 89.18333333333334, 89.18333333333334, 89.51666666666667, 89.51666666666667, 89.66666666666667, 89.66666666666667, 89.76666666666667, 89.76666666666667, 90.06666666666666, 90.06666666666666, 90.08333333333333, 90.08333333333333, 90.2, 90.2, 90.28333333333333, 90.28333333333333, 90.38333333333334, 90.38333333333334, 90.5, 90.5, 90.58333333333333, 90.58333333333333, 90.53333333333333, 90.53333333333333, 90.56666666666666, 90.56666666666666, 90.6, 90.6, 90.55, 90.55, 90.73333333333333, 90.73333333333333, 90.75, 90.75, 90.8, 90.8, 90.93333333333334, 90.93333333333334, 91.01666666666667, 91.01666666666667, 91.06666666666666, 91.06666666666666, 91.16666666666667, 91.16666666666667, 91.45, 91.45, 91.46666666666667, 91.46666666666667, 91.26666666666667, 91.26666666666667, 91.43333333333334, 91.43333333333334, 91.46666666666667, 91.46666666666667, 91.56666666666666, 91.56666666666666, 91.56666666666666, 91.56666666666666, 91.65, 91.65, 91.78333333333333, 91.78333333333333, 91.75, 91.75, 91.88333333333334, 91.88333333333334, 91.9, 91.9, 91.83333333333333, 91.83333333333333, 91.9, 91.9, 92.0, 92.0, 91.93333333333334, 91.93333333333334, 92.0, 92.0, 92.0, 92.0, 92.05, 92.05, 92.05, 92.05, 92.03333333333333, 92.03333333333333, 92.03333333333333, 92.03333333333333, 92.1, 92.1, 92.16666666666667, 92.16666666666667, 92.11666666666666, 92.11666666666666, 92.13333333333334, 92.13333333333334, 92.11666666666666, 92.11666666666666, 92.18333333333334, 92.18333333333334, 92.21666666666667, 92.21666666666667, 92.28333333333333, 92.28333333333333, 92.31666666666666, 92.31666666666666, 92.33333333333333, 92.33333333333333, 92.38333333333334, 92.38333333333334, 92.4, 92.4, 92.36666666666666, 92.36666666666666, 92.43333333333334, 92.43333333333334, 92.48333333333333, 92.48333333333333, 92.48333333333333, 92.48333333333333, 92.56666666666666, 92.56666666666666, 92.58333333333333, 92.58333333333333, 92.66666666666667, 92.66666666666667, 92.53333333333333, 92.53333333333333, 92.58333333333333, 92.58333333333333, 92.56666666666666, 92.56666666666666, 92.63333333333334, 92.63333333333334, 92.55, 92.55, 92.43333333333334, 92.43333333333334, 92.48333333333333, 92.48333333333333, 92.5, 92.5]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.510, Test loss: 1.578, Test accuracy: 88.52
Average accuracy final 10 rounds: 88.55333333333334 

551.3169384002686
[0.5760805606842041, 1.1521611213684082, 1.6048352718353271, 2.057509422302246, 2.505002021789551, 2.9524946212768555, 3.3522932529449463, 3.752091884613037, 4.151350498199463, 4.550609111785889, 4.97218918800354, 5.393769264221191, 5.803007125854492, 6.212244987487793, 6.621366739273071, 7.03048849105835, 7.440708637237549, 7.850928783416748, 8.25400996208191, 8.65709114074707, 9.065695762634277, 9.474300384521484, 9.873093843460083, 10.271887302398682, 10.686851501464844, 11.101815700531006, 11.516700267791748, 11.93158483505249, 12.317862749099731, 12.704140663146973, 13.124487400054932, 13.54483413696289, 13.96263599395752, 14.380437850952148, 14.782833337783813, 15.185228824615479, 15.604031562805176, 16.022834300994873, 16.435739040374756, 16.84864377975464, 17.271652936935425, 17.69466209411621, 18.11461043357849, 18.53455877304077, 18.9441819190979, 19.35380506515503, 19.77106237411499, 20.18831968307495, 20.62575674057007, 21.063193798065186, 21.472038507461548, 21.88088321685791, 22.297902584075928, 22.714921951293945, 23.13490128517151, 23.554880619049072, 23.96540880203247, 24.37593698501587, 24.79957890510559, 25.223220825195312, 25.63799285888672, 26.052764892578125, 26.45314598083496, 26.853527069091797, 27.265745162963867, 27.677963256835938, 28.090808153152466, 28.503653049468994, 28.924376726150513, 29.34510040283203, 29.76166081428528, 30.178221225738525, 30.591071605682373, 31.00392198562622, 31.423945665359497, 31.843969345092773, 32.2660756111145, 32.68818187713623, 33.102412700653076, 33.51664352416992, 33.942458629608154, 34.36827373504639, 34.78698253631592, 35.20569133758545, 35.61378502845764, 36.021878719329834, 36.4430718421936, 36.86426496505737, 37.27816152572632, 37.692058086395264, 38.08963990211487, 38.48722171783447, 38.89580488204956, 39.30438804626465, 39.71728587150574, 40.130183696746826, 40.55039167404175, 40.97059965133667, 41.380340337753296, 41.79008102416992, 42.20737147331238, 42.624661922454834, 43.03574824333191, 43.446834564208984, 43.866180419921875, 44.285526275634766, 44.69677186012268, 45.108017444610596, 45.528143644332886, 45.948269844055176, 46.36133694648743, 46.77440404891968, 47.19625377655029, 47.61810350418091, 48.03566217422485, 48.4532208442688, 48.86616110801697, 49.27910137176514, 49.68819212913513, 50.09728288650513, 50.50335431098938, 50.90942573547363, 51.3226854801178, 51.73594522476196, 52.155129194259644, 52.574313163757324, 52.983810901641846, 53.39330863952637, 53.80590224266052, 54.21849584579468, 54.63345384597778, 55.04841184616089, 55.460116147994995, 55.8718204498291, 56.2883083820343, 56.7047963142395, 57.1279399394989, 57.5510835647583, 57.96199631690979, 58.37290906906128, 58.7961311340332, 59.21935319900513, 59.64112448692322, 60.06289577484131, 60.48188138008118, 60.900866985321045, 61.3171911239624, 61.73351526260376, 62.14824342727661, 62.56297159194946, 62.98630690574646, 63.40964221954346, 63.829049825668335, 64.24845743179321, 64.6679515838623, 65.0874457359314, 65.502037525177, 65.91662931442261, 66.33322429656982, 66.74981927871704, 67.16654324531555, 67.58326721191406, 67.99190735816956, 68.40054750442505, 68.82579398155212, 69.2510404586792, 69.67994618415833, 70.10885190963745, 70.52975034713745, 70.95064878463745, 71.3730719089508, 71.79549503326416, 72.21207213401794, 72.62864923477173, 73.04018521308899, 73.45172119140625, 73.85855340957642, 74.26538562774658, 74.67834043502808, 75.09129524230957, 75.50170612335205, 75.91211700439453, 76.32939672470093, 76.74667644500732, 77.16353249549866, 77.58038854598999, 77.99344754219055, 78.40650653839111, 78.82702279090881, 79.24753904342651, 79.66469240188599, 80.08184576034546, 80.49815034866333, 80.9144549369812, 81.33874797821045, 81.7630410194397, 82.18269467353821, 82.60234832763672, 83.01843857765198, 83.43452882766724, 84.2322371006012, 85.02994537353516]
[17.433333333333334, 17.433333333333334, 20.833333333333332, 20.833333333333332, 24.516666666666666, 24.516666666666666, 28.133333333333333, 28.133333333333333, 34.0, 34.0, 38.65, 38.65, 42.61666666666667, 42.61666666666667, 45.78333333333333, 45.78333333333333, 48.45, 48.45, 51.2, 51.2, 54.65, 54.65, 51.56666666666667, 51.56666666666667, 50.68333333333333, 50.68333333333333, 50.4, 50.4, 55.11666666666667, 55.11666666666667, 57.65, 57.65, 60.13333333333333, 60.13333333333333, 61.6, 61.6, 64.11666666666666, 64.11666666666666, 67.33333333333333, 67.33333333333333, 71.15, 71.15, 74.56666666666666, 74.56666666666666, 76.75, 76.75, 78.85, 78.85, 80.88333333333334, 80.88333333333334, 81.41666666666667, 81.41666666666667, 81.55, 81.55, 82.43333333333334, 82.43333333333334, 83.25, 83.25, 83.43333333333334, 83.43333333333334, 84.4, 84.4, 85.2, 85.2, 85.73333333333333, 85.73333333333333, 85.68333333333334, 85.68333333333334, 86.28333333333333, 86.28333333333333, 86.35, 86.35, 86.26666666666667, 86.26666666666667, 86.95, 86.95, 87.03333333333333, 87.03333333333333, 87.06666666666666, 87.06666666666666, 87.13333333333334, 87.13333333333334, 87.45, 87.45, 87.16666666666667, 87.16666666666667, 87.26666666666667, 87.26666666666667, 87.5, 87.5, 87.2, 87.2, 87.9, 87.9, 88.2, 88.2, 88.33333333333333, 88.33333333333333, 88.41666666666667, 88.41666666666667, 88.48333333333333, 88.48333333333333, 88.63333333333334, 88.63333333333334, 88.48333333333333, 88.48333333333333, 88.5, 88.5, 88.3, 88.3, 88.41666666666667, 88.41666666666667, 87.93333333333334, 87.93333333333334, 88.16666666666667, 88.16666666666667, 88.11666666666666, 88.11666666666666, 88.3, 88.3, 88.23333333333333, 88.23333333333333, 88.45, 88.45, 88.56666666666666, 88.56666666666666, 88.4, 88.4, 88.58333333333333, 88.58333333333333, 88.65, 88.65, 88.83333333333333, 88.83333333333333, 88.7, 88.7, 88.45, 88.45, 88.65, 88.65, 88.86666666666666, 88.86666666666666, 88.76666666666667, 88.76666666666667, 88.83333333333333, 88.83333333333333, 88.78333333333333, 88.78333333333333, 88.81666666666666, 88.81666666666666, 88.66666666666667, 88.66666666666667, 88.41666666666667, 88.41666666666667, 88.46666666666667, 88.46666666666667, 88.33333333333333, 88.33333333333333, 88.51666666666667, 88.51666666666667, 88.58333333333333, 88.58333333333333, 88.4, 88.4, 88.48333333333333, 88.48333333333333, 88.4, 88.4, 88.55, 88.55, 88.6, 88.6, 88.53333333333333, 88.53333333333333, 88.6, 88.6, 88.71666666666667, 88.71666666666667, 88.73333333333333, 88.73333333333333, 88.63333333333334, 88.63333333333334, 88.48333333333333, 88.48333333333333, 88.56666666666666, 88.56666666666666, 88.65, 88.65, 88.6, 88.6, 88.45, 88.45, 88.51666666666667, 88.51666666666667, 88.53333333333333, 88.53333333333333, 88.55, 88.55, 88.55, 88.55, 88.51666666666667, 88.51666666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.510, Test loss: 1.571, Test accuracy: 89.30
Average accuracy final 10 rounds: 88.12166666666667 

600.024521112442
[0.603417158126831, 1.206834316253662, 1.7053766250610352, 2.203918933868408, 2.705655336380005, 3.2073917388916016, 3.7059075832366943, 4.204423427581787, 4.706453084945679, 5.20848274230957, 5.707597494125366, 6.206712245941162, 6.705072641372681, 7.203433036804199, 7.697910308837891, 8.192387580871582, 8.687512636184692, 9.182637691497803, 9.67638373374939, 10.170129776000977, 10.673760414123535, 11.177391052246094, 11.666673183441162, 12.15595531463623, 12.65081787109375, 13.14568042755127, 13.643352746963501, 14.141025066375732, 14.637102365493774, 15.133179664611816, 15.636549711227417, 16.139919757843018, 16.63190984725952, 17.123899936676025, 17.616662740707397, 18.10942554473877, 18.605507612228394, 19.101589679718018, 19.60416841506958, 20.106747150421143, 20.597612380981445, 21.088477611541748, 21.58408284187317, 22.07968807220459, 22.57409119606018, 23.06849431991577, 23.56623339653015, 24.06397247314453, 24.564054250717163, 25.064136028289795, 25.55821681022644, 26.052297592163086, 26.550861358642578, 27.04942512512207, 27.54986882209778, 28.050312519073486, 28.54987645149231, 29.049440383911133, 29.54885220527649, 30.048264026641846, 30.549821376800537, 31.05137872695923, 31.54666781425476, 32.04195690155029, 32.5404167175293, 33.0388765335083, 33.53878164291382, 34.038686752319336, 34.539307594299316, 35.0399284362793, 35.52525520324707, 36.010581970214844, 36.5049467086792, 36.999311447143555, 37.482014894485474, 37.96471834182739, 38.45391297340393, 38.94310760498047, 39.43268823623657, 39.922268867492676, 40.415441274642944, 40.90861368179321, 41.4068078994751, 41.90500211715698, 42.39414572715759, 42.8832893371582, 43.37210822105408, 43.86092710494995, 44.349640130996704, 44.83835315704346, 45.33046770095825, 45.82258224487305, 46.333067178726196, 46.843552112579346, 47.330710887908936, 47.817869663238525, 48.309455156326294, 48.80104064941406, 49.30606436729431, 49.81108808517456, 50.295910596847534, 50.78073310852051, 51.27039837837219, 51.76006364822388, 52.24864101409912, 52.737218379974365, 53.224910259246826, 53.71260213851929, 54.417887449264526, 55.123172760009766, 55.613914251327515, 56.104655742645264, 56.595824003219604, 57.086992263793945, 57.577409982681274, 58.0678277015686, 58.56955552101135, 59.0712833404541, 59.567925214767456, 60.06456708908081, 60.55743408203125, 61.05030107498169, 61.540385723114014, 62.03047037124634, 62.512710094451904, 62.99494981765747, 63.475483655929565, 63.95601749420166, 64.4425802230835, 64.92914295196533, 65.41847562789917, 65.90780830383301, 66.39802098274231, 66.88823366165161, 67.37989640235901, 67.8715591430664, 68.36180686950684, 68.85205459594727, 69.34874606132507, 69.84543752670288, 70.33259916305542, 70.81976079940796, 71.31010127067566, 71.80044174194336, 72.29128813743591, 72.78213453292847, 73.26762294769287, 73.75311136245728, 74.23630785942078, 74.71950435638428, 75.20912384986877, 75.69874334335327, 76.17876505851746, 76.65878677368164, 77.15167450904846, 77.64456224441528, 78.13414359092712, 78.62372493743896, 79.14683222770691, 79.66993951797485, 80.16019749641418, 80.65045547485352, 81.14484477043152, 81.63923406600952, 82.15886807441711, 82.6785020828247, 83.17084431648254, 83.66318655014038, 84.15319395065308, 84.64320135116577, 85.13284063339233, 85.6224799156189, 86.11754155158997, 86.61260318756104, 87.10785245895386, 87.60310173034668, 88.09331965446472, 88.58353757858276, 89.06271600723267, 89.54189443588257, 90.02974390983582, 90.51759338378906, 91.01439905166626, 91.51120471954346, 92.00445938110352, 92.49771404266357, 92.99024271965027, 93.48277139663696, 93.97638654708862, 94.47000169754028, 94.95805406570435, 95.44610643386841, 95.94680976867676, 96.44751310348511, 96.95925736427307, 97.47100162506104, 97.98133277893066, 98.4916639328003, 99.01668405532837, 99.54170417785645, 100.38248229026794, 101.22326040267944]
[18.8, 18.8, 24.8, 24.8, 31.566666666666666, 31.566666666666666, 40.15, 40.15, 43.86666666666667, 43.86666666666667, 40.53333333333333, 40.53333333333333, 41.583333333333336, 41.583333333333336, 41.4, 41.4, 47.416666666666664, 47.416666666666664, 51.0, 51.0, 54.61666666666667, 54.61666666666667, 59.6, 59.6, 64.1, 64.1, 66.56666666666666, 66.56666666666666, 69.36666666666666, 69.36666666666666, 71.06666666666666, 71.06666666666666, 72.56666666666666, 72.56666666666666, 73.5, 73.5, 73.95, 73.95, 74.56666666666666, 74.56666666666666, 75.85, 75.85, 76.65, 76.65, 78.21666666666667, 78.21666666666667, 79.23333333333333, 79.23333333333333, 80.18333333333334, 80.18333333333334, 80.78333333333333, 80.78333333333333, 80.93333333333334, 80.93333333333334, 81.28333333333333, 81.28333333333333, 81.55, 81.55, 81.4, 81.4, 81.56666666666666, 81.56666666666666, 81.65, 81.65, 82.2, 82.2, 82.31666666666666, 82.31666666666666, 82.38333333333334, 82.38333333333334, 82.75, 82.75, 82.95, 82.95, 83.06666666666666, 83.06666666666666, 82.93333333333334, 82.93333333333334, 82.85, 82.85, 82.86666666666666, 82.86666666666666, 82.95, 82.95, 83.01666666666667, 83.01666666666667, 83.01666666666667, 83.01666666666667, 83.48333333333333, 83.48333333333333, 83.5, 83.5, 83.48333333333333, 83.48333333333333, 83.65, 83.65, 83.31666666666666, 83.31666666666666, 83.53333333333333, 83.53333333333333, 83.55, 83.55, 83.46666666666667, 83.46666666666667, 83.5, 83.5, 83.56666666666666, 83.56666666666666, 83.65, 83.65, 83.65, 83.65, 83.5, 83.5, 83.51666666666667, 83.51666666666667, 83.61666666666666, 83.61666666666666, 83.78333333333333, 83.78333333333333, 83.71666666666667, 83.71666666666667, 83.76666666666667, 83.76666666666667, 83.63333333333334, 83.63333333333334, 83.88333333333334, 83.88333333333334, 83.83333333333333, 83.83333333333333, 83.93333333333334, 83.93333333333334, 83.93333333333334, 83.93333333333334, 84.08333333333333, 84.08333333333333, 84.15, 84.15, 84.38333333333334, 84.38333333333334, 84.53333333333333, 84.53333333333333, 84.68333333333334, 84.68333333333334, 84.78333333333333, 84.78333333333333, 84.95, 84.95, 85.26666666666667, 85.26666666666667, 85.3, 85.3, 85.55, 85.55, 85.7, 85.7, 85.61666666666666, 85.61666666666666, 85.76666666666667, 85.76666666666667, 85.86666666666666, 85.86666666666666, 85.76666666666667, 85.76666666666667, 86.4, 86.4, 86.6, 86.6, 86.95, 86.95, 87.33333333333333, 87.33333333333333, 87.45, 87.45, 87.63333333333334, 87.63333333333334, 87.66666666666667, 87.66666666666667, 87.6, 87.6, 87.76666666666667, 87.76666666666667, 87.8, 87.8, 87.76666666666667, 87.76666666666667, 87.8, 87.8, 88.0, 88.0, 88.16666666666667, 88.16666666666667, 88.38333333333334, 88.38333333333334, 88.4, 88.4, 88.61666666666666, 88.61666666666666, 88.51666666666667, 88.51666666666667, 89.3, 89.3]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

Traceback (most recent call last):
  File "main_fedrep.py", line 64, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Final Round, Train loss: 1.101, Test loss: 1.713, Test accuracy: 83.98
Average accuracy final 10 rounds: 84.40074999999997
4826.960595846176
[]
[62.895, 66.995, 77.6, 79.8775, 82.5875, 83.355, 83.6175, 83.575, 83.71, 83.88, 83.905, 83.925, 83.955, 83.8575, 83.67, 83.6625, 83.6375, 83.5575, 83.4825, 83.435, 83.2925, 83.2125, 83.1475, 83.0675, 82.945, 82.8225, 82.7425, 82.755, 82.6275, 82.53, 82.4075, 82.345, 82.3, 82.1825, 82.1025, 82.1, 82.0975, 81.98, 81.855, 81.82, 81.7275, 81.61, 83.1725, 84.1675, 85.3375, 85.6925, 86.1175, 86.5675, 86.68, 86.67, 86.68, 86.8025, 87.1825, 87.0075, 86.755, 86.8375, 86.805, 86.79, 86.645, 86.655, 86.5925, 86.6725, 86.5175, 86.3875, 86.2975, 86.25, 86.1625, 86.055, 85.9475, 85.9225, 85.7275, 85.5975, 85.545, 85.485, 85.315, 85.21, 85.305, 85.255, 85.16, 85.0325, 84.9625, 84.99, 85.0025, 85.0225, 84.9425, 84.855, 84.825, 84.765, 84.7525, 84.66, 84.6525, 84.585, 84.5625, 84.4725, 84.51, 84.4725, 84.2325, 84.1675, 84.21, 84.1425, 83.985]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.560, Test loss: 1.582, Test accuracy: 89.29
Average accuracy final 10 rounds: 86.45166666666667
Average global accuracy final 10 rounds: 86.45166666666667
1128.9460797309875
[]
[14.916666666666666, 15.15, 17.375, 18.85, 18.216666666666665, 18.308333333333334, 18.391666666666666, 18.433333333333334, 19.108333333333334, 22.083333333333332, 26.075, 23.358333333333334, 19.275, 24.441666666666666, 32.391666666666666, 32.608333333333334, 28.891666666666666, 33.74166666666667, 37.358333333333334, 41.075, 45.38333333333333, 52.50833333333333, 49.94166666666667, 49.025, 46.108333333333334, 40.65833333333333, 43.958333333333336, 43.416666666666664, 47.425, 48.88333333333333, 51.083333333333336, 50.641666666666666, 52.875, 50.516666666666666, 58.34166666666667, 56.86666666666667, 56.24166666666667, 55.075, 55.25833333333333, 55.9, 55.775, 58.34166666666667, 62.141666666666666, 65.49166666666666, 66.325, 65.86666666666666, 68.55833333333334, 67.825, 67.75833333333334, 69.275, 69.25833333333334, 70.01666666666667, 73.59166666666667, 74.19166666666666, 74.3, 74.95, 75.75, 77.69166666666666, 77.58333333333333, 78.925, 80.59166666666667, 80.6, 80.46666666666667, 80.825, 82.11666666666666, 80.64166666666667, 81.40833333333333, 82.71666666666667, 82.59166666666667, 82.45, 82.8, 83.56666666666666, 84.38333333333334, 84.84166666666667, 84.58333333333333, 84.05833333333334, 84.575, 84.93333333333334, 84.40833333333333, 83.975, 84.68333333333334, 85.15, 85.68333333333334, 85.7, 86.1, 86.19166666666666, 85.975, 85.75833333333334, 85.65, 85.85, 86.2, 86.25833333333334, 86.38333333333334, 86.45833333333333, 85.975, 86.01666666666667, 86.58333333333333, 86.96666666666667, 86.80833333333334, 86.86666666666666, 89.29166666666667]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.38
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.22
Average accuracy final 10 rounds: 10.171666666666667 

Average global accuracy final 10 rounds: 10.210833333333333 

1321.3364202976227
[1.2674622535705566, 2.406761407852173, 3.5646839141845703, 4.725330352783203, 5.832299709320068, 6.970546007156372, 8.10470962524414, 9.29630732536316, 10.456959247589111, 11.661491632461548, 12.820555686950684, 14.006380558013916, 15.124264478683472, 16.27456831932068, 17.42207622528076, 18.57684826850891, 19.673542976379395, 20.800785303115845, 21.97990918159485, 23.171537399291992, 24.31376552581787, 25.461681365966797, 26.649221897125244, 27.833844661712646, 28.97479224205017, 30.13034725189209, 31.29984974861145, 32.47962522506714, 33.59456491470337, 34.71638751029968, 35.89274001121521, 37.08163595199585, 38.19179010391235, 39.33796954154968, 40.561707735061646, 41.84243845939636, 42.96368217468262, 44.13128709793091, 45.299156188964844, 46.42885947227478, 47.53045892715454, 48.72999382019043, 49.870543003082275, 51.000340938568115, 52.14201354980469, 53.43737602233887, 54.78737926483154, 56.071638345718384, 57.287954568862915, 58.48075604438782, 59.640302658081055, 60.762959718704224, 61.930976152420044, 63.09400510787964, 64.21459484100342, 65.21304297447205, 66.23931837081909, 67.28536677360535, 68.30855751037598, 69.33762979507446, 70.37551808357239, 71.40613269805908, 72.4369261264801, 73.50272917747498, 74.51200151443481, 75.67716908454895, 76.8601222038269, 78.0211272239685, 79.13406419754028, 80.28728485107422, 81.4672281742096, 82.60229873657227, 83.72739958763123, 84.86988663673401, 86.02915024757385, 87.14732480049133, 88.2833321094513, 89.42382025718689, 90.53068041801453, 91.67946147918701, 92.74068093299866, 93.76587080955505, 94.83697509765625, 95.88721513748169, 96.91295218467712, 97.94722890853882, 99.0143461227417, 100.02324485778809, 101.04770612716675, 102.0892117023468, 103.15106511116028, 104.1671953201294, 105.19963884353638, 106.21140193939209, 107.23865509033203, 108.28828072547913, 109.30956029891968, 110.34960746765137, 111.40874600410461, 112.44236969947815, 114.34681963920593]
[8.55, 8.583333333333334, 8.566666666666666, 8.55, 8.541666666666666, 8.541666666666666, 8.566666666666666, 8.591666666666667, 8.608333333333333, 8.633333333333333, 8.65, 8.65, 8.716666666666667, 8.741666666666667, 8.758333333333333, 8.791666666666666, 8.8, 8.775, 8.791666666666666, 8.791666666666666, 8.816666666666666, 8.866666666666667, 8.858333333333333, 8.9, 8.916666666666666, 8.925, 8.916666666666666, 8.933333333333334, 8.933333333333334, 9.008333333333333, 9.0, 8.983333333333333, 9.008333333333333, 9.033333333333333, 9.075, 9.083333333333334, 9.116666666666667, 9.141666666666667, 9.141666666666667, 9.191666666666666, 9.233333333333333, 9.216666666666667, 9.241666666666667, 9.241666666666667, 9.233333333333333, 9.25, 9.266666666666667, 9.266666666666667, 9.283333333333333, 9.283333333333333, 9.283333333333333, 9.291666666666666, 9.325, 9.333333333333334, 9.358333333333333, 9.366666666666667, 9.408333333333333, 9.433333333333334, 9.416666666666666, 9.433333333333334, 9.483333333333333, 9.5, 9.516666666666667, 9.533333333333333, 9.508333333333333, 9.516666666666667, 9.525, 9.55, 9.541666666666666, 9.566666666666666, 9.583333333333334, 9.616666666666667, 9.641666666666667, 9.683333333333334, 9.716666666666667, 9.7, 9.716666666666667, 9.766666666666667, 9.808333333333334, 9.816666666666666, 9.85, 9.875, 9.9, 9.891666666666667, 9.916666666666666, 10.016666666666667, 10.041666666666666, 10.025, 10.041666666666666, 10.058333333333334, 10.075, 10.075, 10.083333333333334, 10.158333333333333, 10.15, 10.191666666666666, 10.216666666666667, 10.241666666666667, 10.258333333333333, 10.266666666666667, 10.375]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Final Round, Train loss: 1.571, Test loss: 1.595, Test accuracy: 86.62
Average accuracy final 10 rounds: 86.56083333333333
1847.803294658661
[2.7503843307495117, 5.36546516418457, 7.986424922943115, 10.62277865409851, 13.232742071151733, 15.82410192489624, 18.483492136001587, 21.091028928756714, 23.692458629608154, 26.24545168876648, 28.736539125442505, 31.324002742767334, 33.889750480651855, 36.35500717163086, 38.894604206085205, 41.45033550262451, 43.948933601379395, 46.47829246520996, 48.97743272781372, 51.462663412094116, 54.01306390762329, 56.499571561813354, 58.980997800827026, 61.50694441795349, 64.03367805480957, 66.54268002510071, 69.09785866737366, 71.58499836921692, 74.1237416267395, 76.64707207679749, 79.13879036903381, 81.64173483848572, 84.14994406700134, 86.63386392593384, 89.13851380348206, 91.65715146064758, 94.1726062297821, 96.73455953598022, 99.29942083358765, 101.81985807418823, 104.30430126190186, 106.82549023628235, 109.31395626068115, 111.80866312980652, 114.33742809295654, 116.83995366096497, 119.36207461357117, 121.87295699119568, 124.34916543960571, 126.9008424282074, 129.45238399505615, 131.91877388954163, 134.4337830543518, 136.9381983280182, 139.40021300315857, 141.91351866722107, 144.4792332649231, 147.11200642585754, 149.79340314865112, 152.43903708457947, 155.06627488136292, 157.77586817741394, 160.2539939880371, 162.73873162269592, 165.2138249874115, 167.67567133903503, 170.18743109703064, 172.65171432495117, 175.11242818832397, 177.58508586883545, 180.04322814941406, 182.5193953514099, 185.03620839118958, 187.4951775074005, 189.9907510280609, 192.46892929077148, 194.93854999542236, 197.4264760017395, 199.89592170715332, 202.34593176841736, 204.83842372894287, 207.30478167533875, 209.78405928611755, 212.30172967910767, 214.75559639930725, 217.24530339241028, 219.72739553451538, 222.20168495178223, 224.69817519187927, 227.16579699516296, 229.6124472618103, 232.1072952747345, 234.57879948616028, 237.08442902565002, 239.61488127708435, 242.08312821388245, 244.6059513092041, 247.12665390968323, 249.58246159553528, 252.07927989959717, 254.21138882637024]
[31.45, 38.958333333333336, 37.416666666666664, 56.925, 65.54166666666667, 67.03333333333333, 67.36666666666666, 67.74166666666666, 68.00833333333334, 68.33333333333333, 68.39166666666667, 68.60833333333333, 68.475, 68.59166666666667, 68.65833333333333, 68.64166666666667, 68.68333333333334, 68.79166666666667, 71.93333333333334, 74.88333333333334, 75.25, 75.55833333333334, 75.55833333333334, 75.8, 83.05, 83.88333333333334, 84.11666666666666, 84.30833333333334, 84.54166666666667, 84.63333333333334, 84.575, 84.725, 84.70833333333333, 84.9, 84.90833333333333, 85.075, 85.23333333333333, 85.34166666666667, 85.29166666666667, 85.40833333333333, 85.35, 85.55833333333334, 85.50833333333334, 85.65833333333333, 85.71666666666667, 85.65833333333333, 85.68333333333334, 85.775, 85.88333333333334, 85.86666666666666, 85.8, 85.74166666666666, 86.075, 85.91666666666667, 85.80833333333334, 86.04166666666667, 85.99166666666666, 86.03333333333333, 86.04166666666667, 86.13333333333334, 86.06666666666666, 85.89166666666667, 86.06666666666666, 86.06666666666666, 86.10833333333333, 86.2, 86.18333333333334, 86.15, 86.18333333333334, 86.24166666666666, 86.31666666666666, 86.4, 86.44166666666666, 86.48333333333333, 86.45, 86.575, 86.45, 86.4, 86.3, 86.45, 86.50833333333334, 86.48333333333333, 86.4, 86.575, 86.55, 86.55, 86.525, 86.39166666666667, 86.46666666666667, 86.38333333333334, 86.65833333333333, 86.525, 86.425, 86.60833333333333, 86.575, 86.5, 86.5, 86.55833333333334, 86.61666666666666, 86.64166666666667, 86.61666666666666]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.479, Test loss: 1.508, Test accuracy: 96.27
Average accuracy final 10 rounds: 96.33000000000001
1049.9578347206116
[1.448664903640747, 2.634899854660034, 3.7772271633148193, 4.97907280921936, 6.147642135620117, 7.345025539398193, 8.566629886627197, 9.714844942092896, 10.868985891342163, 12.066795110702515, 13.326374769210815, 14.615993976593018, 15.840363502502441, 17.102760553359985, 18.386617183685303, 19.58753490447998, 20.78672170639038, 21.912323713302612, 23.064719676971436, 24.285961627960205, 25.516005992889404, 26.780191659927368, 27.89586114883423, 29.123542547225952, 30.41639494895935, 31.72014093399048, 32.98700308799744, 34.20588493347168, 35.44387125968933, 36.712581634521484, 37.975167989730835, 39.22771334648132, 40.42686581611633, 41.66021943092346, 42.97433018684387, 44.20524764060974, 45.41779136657715, 46.55408477783203, 47.707865953445435, 48.93880605697632, 50.141878604888916, 51.35922694206238, 52.54216766357422, 53.68176007270813, 54.872907400131226, 56.113757371902466, 57.290032386779785, 58.474700689315796, 59.62547588348389, 60.85180711746216, 62.055468797683716, 63.24161672592163, 64.40751767158508, 65.5793399810791, 66.76773285865784, 67.99295496940613, 69.14845967292786, 70.28954458236694, 71.46190428733826, 72.67045998573303, 73.86846828460693, 75.04450011253357, 76.22093319892883, 77.3954918384552, 78.59101390838623, 79.82266807556152, 80.9969367980957, 82.1347587108612, 83.3121886253357, 84.52569317817688, 85.73923230171204, 86.88563203811646, 88.08249378204346, 89.26188468933105, 90.48615002632141, 91.70290613174438, 92.893061876297, 94.05961656570435, 95.22022533416748, 96.3971152305603, 97.58339166641235, 98.7716965675354, 99.92049717903137, 101.18862843513489, 102.45486521720886, 103.69585824012756, 104.94752144813538, 106.15347695350647, 107.33481359481812, 108.5517168045044, 109.71314740180969, 110.90011954307556, 112.08200597763062, 113.25803875923157, 114.47286009788513, 115.66205358505249, 116.83272576332092, 118.03698539733887, 119.2182891368866, 120.3958158493042, 121.96624374389648]
[18.725, 26.341666666666665, 37.19166666666667, 46.425, 42.75833333333333, 47.65, 57.375, 67.89166666666667, 72.26666666666667, 74.225, 75.13333333333334, 75.65833333333333, 76.6, 78.50833333333334, 79.95833333333333, 81.425, 81.625, 82.28333333333333, 82.875, 83.56666666666666, 84.23333333333333, 85.175, 86.625, 89.26666666666667, 89.9, 91.14166666666667, 91.65, 92.23333333333333, 92.69166666666666, 93.075, 93.26666666666667, 93.375, 93.85, 93.88333333333334, 94.01666666666667, 94.15, 94.29166666666667, 94.46666666666667, 94.45833333333333, 94.58333333333333, 94.51666666666667, 94.64166666666667, 94.7, 94.68333333333334, 94.80833333333334, 94.8, 94.86666666666666, 94.975, 95.1, 95.23333333333333, 95.15, 95.34166666666667, 95.38333333333334, 95.35, 95.325, 95.54166666666667, 95.48333333333333, 95.56666666666666, 95.55833333333334, 95.55, 95.675, 95.66666666666667, 95.65, 95.80833333333334, 95.78333333333333, 95.825, 95.90833333333333, 95.85, 95.90833333333333, 95.85833333333333, 95.95, 96.01666666666667, 96.03333333333333, 96.06666666666666, 96.13333333333334, 96.11666666666666, 96.19166666666666, 96.2, 96.11666666666666, 96.225, 96.20833333333333, 96.19166666666666, 96.225, 96.275, 96.25833333333334, 96.25833333333334, 96.31666666666666, 96.36666666666666, 96.35833333333333, 96.35833333333333, 96.275, 96.29166666666667, 96.25, 96.30833333333334, 96.35833333333333, 96.35833333333333, 96.36666666666666, 96.33333333333333, 96.375, 96.38333333333334, 96.26666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.489, Test loss: 1.517, Test accuracy: 96.74
Average accuracy final 10 rounds: 96.6775
1379.5061376094818
[1.4153821468353271, 2.8307642936706543, 4.08476996421814, 5.338775634765625, 6.64872670173645, 7.958677768707275, 9.221203565597534, 10.483729362487793, 11.73922348022461, 12.994717597961426, 14.26640009880066, 15.538082599639893, 16.83769917488098, 18.13731575012207, 19.394479036331177, 20.651642322540283, 21.886200189590454, 23.120758056640625, 24.384368181228638, 25.64797830581665, 26.90840220451355, 28.16882610321045, 29.42667555809021, 30.68452501296997, 31.9320809841156, 33.17963695526123, 34.48162889480591, 35.783620834350586, 37.068931579589844, 38.3542423248291, 39.58463478088379, 40.81502723693848, 42.09271502494812, 43.370402812957764, 44.69597387313843, 46.02154493331909, 47.28797197341919, 48.55439901351929, 49.840550661087036, 51.126702308654785, 52.439361333847046, 53.75202035903931, 55.0093469619751, 56.26667356491089, 57.49404454231262, 58.721415519714355, 59.986724615097046, 61.252033710479736, 62.56910538673401, 63.88617706298828, 65.17206168174744, 66.45794630050659, 67.7138135433197, 68.96968078613281, 70.26341581344604, 71.55715084075928, 72.85948896408081, 74.16182708740234, 75.38470125198364, 76.60757541656494, 77.87208962440491, 79.13660383224487, 80.43273568153381, 81.72886753082275, 82.99032521247864, 84.25178289413452, 85.47389459609985, 86.69600629806519, 88.02946400642395, 89.36292171478271, 90.64816117286682, 91.93340063095093, 93.18693614006042, 94.44047164916992, 95.68934726715088, 96.93822288513184, 98.25660514831543, 99.57498741149902, 100.86745119094849, 102.15991497039795, 103.38120698928833, 104.60249900817871, 105.88597512245178, 107.16945123672485, 108.4653263092041, 109.76120138168335, 111.0176055431366, 112.27400970458984, 113.39047741889954, 114.50694513320923, 115.69702315330505, 116.88710117340088, 118.05015802383423, 119.21321487426758, 120.305819272995, 121.39842367172241, 122.50172519683838, 123.60502672195435, 124.75616502761841, 125.90730333328247, 127.10423731803894, 128.3011713027954, 129.3810589313507, 130.460946559906, 131.59909391403198, 132.73724126815796, 133.89539289474487, 135.0535445213318, 136.2047996520996, 137.35605478286743, 138.48758578300476, 139.6191167831421, 140.79177045822144, 141.96442413330078, 143.15213227272034, 144.3398404121399, 145.51337385177612, 146.68690729141235, 147.8009490966797, 148.91499090194702, 150.126788854599, 151.33858680725098, 152.51494336128235, 153.69129991531372, 154.80684089660645, 155.92238187789917, 157.02884721755981, 158.13531255722046, 159.31110954284668, 160.4869065284729, 161.68740105628967, 162.88789558410645, 164.04202008247375, 165.19614458084106, 166.32413601875305, 167.45212745666504, 168.6504111289978, 169.84869480133057, 171.05595350265503, 172.2632122039795, 173.47531247138977, 174.68741273880005, 175.90975642204285, 177.13210010528564, 178.41871094703674, 179.70532178878784, 180.9713454246521, 182.23736906051636, 183.4418249130249, 184.64628076553345, 185.88085675239563, 187.1154327392578, 188.3981261253357, 189.68081951141357, 190.91836833953857, 192.15591716766357, 193.3897511959076, 194.6235852241516, 195.86258125305176, 197.1015772819519, 198.3773729801178, 199.6531686782837, 200.83436727523804, 202.01556587219238, 203.2423493862152, 204.46913290023804, 205.73762488365173, 207.00611686706543, 208.26364278793335, 209.52116870880127, 210.7293393611908, 211.93751001358032, 213.17270636558533, 214.40790271759033, 215.6988935470581, 216.98988437652588, 218.23217582702637, 219.47446727752686, 220.71166563034058, 221.9488639831543, 223.23736214637756, 224.52586030960083, 225.75444793701172, 226.9830355644226, 228.21355319023132, 229.44407081604004, 230.64650750160217, 231.8489441871643, 233.08989834785461, 234.33085250854492, 235.54223585128784, 236.75361919403076, 237.9874815940857, 239.22134399414062, 240.45516061782837, 241.6889772415161, 242.89447712898254, 244.09997701644897, 245.23748445510864, 246.3749918937683, 247.88156986236572, 249.38814783096313]
[13.758333333333333, 13.758333333333333, 29.066666666666666, 29.066666666666666, 35.18333333333333, 35.18333333333333, 38.483333333333334, 38.483333333333334, 38.15833333333333, 38.15833333333333, 43.85, 43.85, 49.53333333333333, 49.53333333333333, 55.09166666666667, 55.09166666666667, 60.675, 60.675, 64.85, 64.85, 74.0, 74.0, 79.34166666666667, 79.34166666666667, 81.025, 81.025, 82.35833333333333, 82.35833333333333, 83.63333333333334, 83.63333333333334, 85.90833333333333, 85.90833333333333, 88.35, 88.35, 89.81666666666666, 89.81666666666666, 90.65, 90.65, 91.26666666666667, 91.26666666666667, 91.73333333333333, 91.73333333333333, 92.00833333333334, 92.00833333333334, 92.375, 92.375, 92.85833333333333, 92.85833333333333, 93.26666666666667, 93.26666666666667, 93.34166666666667, 93.34166666666667, 93.59166666666667, 93.59166666666667, 93.55833333333334, 93.55833333333334, 93.64166666666667, 93.64166666666667, 93.76666666666667, 93.76666666666667, 94.05, 94.05, 94.23333333333333, 94.23333333333333, 94.225, 94.225, 94.4, 94.4, 94.425, 94.425, 94.49166666666666, 94.49166666666666, 94.63333333333334, 94.63333333333334, 94.66666666666667, 94.66666666666667, 94.79166666666667, 94.79166666666667, 94.925, 94.925, 95.025, 95.025, 95.125, 95.125, 95.11666666666666, 95.11666666666666, 95.19166666666666, 95.19166666666666, 95.25833333333334, 95.25833333333334, 95.39166666666667, 95.39166666666667, 95.51666666666667, 95.51666666666667, 95.35833333333333, 95.35833333333333, 95.45, 95.45, 95.50833333333334, 95.50833333333334, 95.625, 95.625, 95.63333333333334, 95.63333333333334, 95.61666666666666, 95.61666666666666, 95.71666666666667, 95.71666666666667, 95.825, 95.825, 95.875, 95.875, 95.93333333333334, 95.93333333333334, 95.89166666666667, 95.89166666666667, 95.93333333333334, 95.93333333333334, 96.01666666666667, 96.01666666666667, 96.16666666666667, 96.16666666666667, 96.10833333333333, 96.10833333333333, 96.10833333333333, 96.10833333333333, 96.16666666666667, 96.16666666666667, 96.09166666666667, 96.09166666666667, 96.15833333333333, 96.15833333333333, 96.16666666666667, 96.16666666666667, 96.20833333333333, 96.20833333333333, 96.21666666666667, 96.21666666666667, 96.23333333333333, 96.23333333333333, 96.25833333333334, 96.25833333333334, 96.30833333333334, 96.30833333333334, 96.35833333333333, 96.35833333333333, 96.325, 96.325, 96.50833333333334, 96.50833333333334, 96.44166666666666, 96.44166666666666, 96.43333333333334, 96.43333333333334, 96.39166666666667, 96.39166666666667, 96.40833333333333, 96.40833333333333, 96.50833333333334, 96.50833333333334, 96.475, 96.475, 96.50833333333334, 96.50833333333334, 96.40833333333333, 96.40833333333333, 96.50833333333334, 96.50833333333334, 96.55833333333334, 96.55833333333334, 96.56666666666666, 96.56666666666666, 96.50833333333334, 96.50833333333334, 96.5, 96.5, 96.525, 96.525, 96.59166666666667, 96.59166666666667, 96.65833333333333, 96.65833333333333, 96.56666666666666, 96.56666666666666, 96.675, 96.675, 96.625, 96.625, 96.75833333333334, 96.75833333333334, 96.65833333333333, 96.65833333333333, 96.70833333333333, 96.70833333333333, 96.69166666666666, 96.69166666666666, 96.7, 96.7, 96.73333333333333, 96.73333333333333, 96.74166666666666, 96.74166666666666]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.464, Test loss: 1.499, Test accuracy: 96.36
Final Round, Global train loss: 1.464, Global test loss: 2.108, Global test accuracy: 38.64
Average accuracy final 10 rounds: 96.3616666666667 

Average global accuracy final 10 rounds: 46.471666666666664 

1524.6150691509247
[1.1237099170684814, 2.247419834136963, 3.2586100101470947, 4.269800186157227, 5.3147242069244385, 6.35964822769165, 7.381011724472046, 8.402375221252441, 9.413494348526001, 10.42461347579956, 11.425821781158447, 12.427030086517334, 13.375718355178833, 14.324406623840332, 15.346272945404053, 16.368139266967773, 17.3959219455719, 18.423704624176025, 19.442440032958984, 20.461175441741943, 21.485575437545776, 22.50997543334961, 23.531955242156982, 24.553935050964355, 25.57338285446167, 26.592830657958984, 27.611788749694824, 28.630746841430664, 29.5301411151886, 30.429535388946533, 31.355377912521362, 32.28122043609619, 33.20434331893921, 34.12746620178223, 35.06032657623291, 35.993186950683594, 36.9923357963562, 37.99148464202881, 38.9556884765625, 39.91989231109619, 40.852076292037964, 41.784260272979736, 42.6871452331543, 43.59003019332886, 44.50999975204468, 45.4299693107605, 46.37087321281433, 47.311777114868164, 48.302448987960815, 49.29312086105347, 50.187259912490845, 51.08139896392822, 52.022390604019165, 52.96338224411011, 53.89874482154846, 54.834107398986816, 55.71828556060791, 56.602463722229004, 57.464491844177246, 58.32651996612549, 59.23224949836731, 60.13797903060913, 61.078527212142944, 62.01907539367676, 62.917649269104004, 63.81622314453125, 64.73911929130554, 65.66201543807983, 66.58857131004333, 67.51512718200684, 68.43669080734253, 69.35825443267822, 70.33325219154358, 71.30824995040894, 72.20299220085144, 73.09773445129395, 74.02513289451599, 74.95253133773804, 75.78264999389648, 76.61276865005493, 77.491219997406, 78.36967134475708, 79.30288863182068, 80.23610591888428, 81.17125487327576, 82.10640382766724, 83.09457516670227, 84.0827465057373, 85.12186765670776, 86.16098880767822, 87.16596484184265, 88.17094087600708, 89.16093850135803, 90.15093612670898, 91.12814474105835, 92.10535335540771, 93.14023685455322, 94.17512035369873, 95.18703198432922, 96.19894361495972, 97.19819188117981, 98.1974401473999, 99.18943810462952, 100.18143606185913, 101.20842146873474, 102.23540687561035, 103.20440411567688, 104.17340135574341, 105.19902348518372, 106.22464561462402, 107.26042985916138, 108.29621410369873, 109.33552813529968, 110.37484216690063, 111.31635451316833, 112.25786685943604, 113.24736166000366, 114.23685646057129, 115.26444149017334, 116.29202651977539, 117.20209956169128, 118.11217260360718, 119.06238532066345, 120.01259803771973, 120.95757341384888, 121.90254878997803, 122.81791472434998, 123.73328065872192, 124.5858039855957, 125.43832731246948, 126.37866234779358, 127.31899738311768, 128.23468208312988, 129.1503667831421, 130.03357434272766, 130.91678190231323, 131.85106205940247, 132.7853422164917, 133.74390649795532, 134.70247077941895, 135.58742690086365, 136.47238302230835, 137.33014798164368, 138.187912940979, 139.1160762310028, 140.0442395210266, 140.92171239852905, 141.7991852760315, 142.7260148525238, 143.6528444290161, 144.5918412208557, 145.5308380126953, 146.47241020202637, 147.41398239135742, 148.32672238349915, 149.23946237564087, 150.1359100341797, 151.0323576927185, 152.00292563438416, 152.9734935760498, 153.89040064811707, 154.80730772018433, 155.69773650169373, 156.58816528320312, 157.5205430984497, 158.4529209136963, 159.38067197799683, 160.30842304229736, 161.22243547439575, 162.13644790649414, 163.05277156829834, 163.96909523010254, 164.8732123374939, 165.77732944488525, 166.72855377197266, 167.67977809906006, 168.60498976707458, 169.5302014350891, 170.48235392570496, 171.4345064163208, 172.36342525482178, 173.29234409332275, 174.19560503959656, 175.09886598587036, 175.99944734573364, 176.90002870559692, 177.85841250419617, 178.8167963027954, 179.75869584083557, 180.70059537887573, 181.64935088157654, 182.59810638427734, 183.56088280677795, 184.52365922927856, 185.4478244781494, 186.37198972702026, 187.2255940437317, 188.07919836044312, 189.01388359069824, 189.94856882095337, 191.54122591018677, 193.13388299942017]
[37.391666666666666, 37.391666666666666, 53.1, 53.1, 72.86666666666666, 72.86666666666666, 77.175, 77.175, 79.00833333333334, 79.00833333333334, 83.51666666666667, 83.51666666666667, 82.50833333333334, 82.50833333333334, 83.225, 83.225, 86.025, 86.025, 87.24166666666666, 87.24166666666666, 84.825, 84.825, 90.125, 90.125, 91.65, 91.65, 91.775, 91.775, 91.875, 91.875, 91.93333333333334, 91.93333333333334, 91.89166666666667, 91.89166666666667, 91.90833333333333, 91.90833333333333, 91.95833333333333, 91.95833333333333, 91.98333333333333, 91.98333333333333, 91.975, 91.975, 91.96666666666667, 91.96666666666667, 91.94166666666666, 91.94166666666666, 91.95, 91.95, 91.95, 91.95, 91.975, 91.975, 93.43333333333334, 93.43333333333334, 93.43333333333334, 93.43333333333334, 93.43333333333334, 93.43333333333334, 95.525, 95.525, 96.11666666666666, 96.11666666666666, 96.13333333333334, 96.13333333333334, 96.15, 96.15, 96.15, 96.15, 96.25, 96.25, 96.275, 96.275, 96.25, 96.25, 96.25833333333334, 96.25833333333334, 96.275, 96.275, 96.25, 96.25, 96.23333333333333, 96.23333333333333, 96.275, 96.275, 96.275, 96.275, 96.275, 96.275, 96.275, 96.275, 96.29166666666667, 96.29166666666667, 96.275, 96.275, 96.28333333333333, 96.28333333333333, 96.26666666666667, 96.26666666666667, 96.275, 96.275, 96.24166666666666, 96.24166666666666, 96.24166666666666, 96.24166666666666, 96.25, 96.25, 96.25, 96.25, 96.25833333333334, 96.25833333333334, 96.26666666666667, 96.26666666666667, 96.26666666666667, 96.26666666666667, 96.275, 96.275, 96.28333333333333, 96.28333333333333, 96.275, 96.275, 96.29166666666667, 96.29166666666667, 96.3, 96.3, 96.30833333333334, 96.30833333333334, 96.28333333333333, 96.28333333333333, 96.28333333333333, 96.28333333333333, 96.28333333333333, 96.28333333333333, 96.29166666666667, 96.29166666666667, 96.29166666666667, 96.29166666666667, 96.3, 96.3, 96.3, 96.3, 96.3, 96.3, 96.30833333333334, 96.30833333333334, 96.3, 96.3, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.3, 96.3, 96.30833333333334, 96.30833333333334, 96.31666666666666, 96.31666666666666, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.30833333333334, 96.325, 96.325, 96.33333333333333, 96.33333333333333, 96.35, 96.35, 96.35833333333333, 96.35833333333333, 96.35, 96.35, 96.35, 96.35, 96.35, 96.35, 96.35833333333333, 96.35833333333333, 96.36666666666666, 96.36666666666666, 96.35, 96.35, 96.34166666666667, 96.34166666666667, 96.34166666666667, 96.34166666666667, 96.35833333333333, 96.35833333333333, 96.36666666666666, 96.36666666666666, 96.375, 96.375, 96.36666666666666, 96.36666666666666, 96.36666666666666, 96.36666666666666, 96.375, 96.375, 96.375, 96.375, 96.35833333333333, 96.35833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.570, Test loss: 1.608, Test accuracy: 85.33
Final Round, Global train loss: 1.570, Global test loss: 1.606, Global test accuracy: 85.55
Average accuracy final 10 rounds: 85.24333333333334 

Average global accuracy final 10 rounds: 85.4975 

1605.348349571228
[1.242415428161621, 2.484830856323242, 3.603156566619873, 4.721482276916504, 5.810988903045654, 6.900495529174805, 7.931682825088501, 8.962870121002197, 10.016847372055054, 11.07082462310791, 12.136815547943115, 13.20280647277832, 14.295224905014038, 15.387643337249756, 16.54142737388611, 17.69521141052246, 18.785089015960693, 19.874966621398926, 20.927483320236206, 21.980000019073486, 23.06458854675293, 24.149177074432373, 25.216465950012207, 26.28375482559204, 27.37928295135498, 28.47481107711792, 29.594972133636475, 30.71513319015503, 31.79051685333252, 32.86590051651001, 33.971585750579834, 35.07727098464966, 36.19659447669983, 37.31591796875, 38.33758521080017, 39.35925245285034, 40.56384587287903, 41.768439292907715, 42.914456844329834, 44.06047439575195, 45.16553997993469, 46.27060556411743, 47.37259650230408, 48.47458744049072, 49.61577320098877, 50.756958961486816, 51.87501907348633, 52.99307918548584, 54.04658770561218, 55.100096225738525, 56.22385549545288, 57.347614765167236, 58.43891668319702, 59.53021860122681, 60.55102753639221, 61.57183647155762, 62.66304159164429, 63.75424671173096, 64.82861137390137, 65.90297603607178, 66.97163462638855, 68.04029321670532, 69.15767359733582, 70.27505397796631, 71.37370228767395, 72.47235059738159, 73.54721212387085, 74.62207365036011, 75.7266457080841, 76.8312177658081, 77.91070032119751, 78.99018287658691, 80.0325345993042, 81.07488632202148, 82.12178444862366, 83.16868257522583, 84.22765779495239, 85.28663301467896, 86.35757565498352, 87.42851829528809, 88.48365902900696, 89.53879976272583, 90.6284646987915, 91.71812963485718, 92.7753496170044, 93.83256959915161, 94.9044623374939, 95.97635507583618, 97.10184240341187, 98.22732973098755, 99.29804134368896, 100.36875295639038, 101.43828749656677, 102.50782203674316, 103.57378816604614, 104.63975429534912, 105.69781970977783, 106.75588512420654, 107.82916331291199, 108.90244150161743, 109.97862362861633, 111.05480575561523, 112.09947776794434, 113.14414978027344, 114.22291827201843, 115.30168676376343, 116.40596652030945, 117.51024627685547, 118.58746695518494, 119.6646876335144, 120.73929977416992, 121.81391191482544, 122.91666889190674, 124.01942586898804, 125.10109972953796, 126.18277359008789, 127.2942168712616, 128.4056601524353, 129.51629424095154, 130.62692832946777, 131.76059365272522, 132.89425897598267, 134.0053060054779, 135.11635303497314, 136.24016571044922, 137.3639783859253, 138.4528453350067, 139.54171228408813, 140.6313018798828, 141.7208914756775, 142.82001209259033, 143.91913270950317, 145.05390763282776, 146.18868255615234, 147.3803265094757, 148.57197046279907, 149.70656776428223, 150.84116506576538, 151.96927905082703, 153.09739303588867, 154.25601744651794, 155.41464185714722, 156.50109481811523, 157.58754777908325, 158.70742630958557, 159.8273048400879, 160.9013695716858, 161.9754343032837, 162.94607520103455, 163.9167160987854, 164.8377649784088, 165.75881385803223, 166.69846773147583, 167.63812160491943, 168.5995876789093, 169.56105375289917, 170.4879720211029, 171.41489028930664, 172.3410336971283, 173.26717710494995, 174.17607498168945, 175.08497285842896, 176.00834131240845, 176.93170976638794, 177.9068193435669, 178.88192892074585, 179.83518075942993, 180.788432598114, 181.73404097557068, 182.67964935302734, 183.64393210411072, 184.6082148551941, 185.4982008934021, 186.3881869316101, 187.33253121376038, 188.27687549591064, 189.27239084243774, 190.26790618896484, 191.2391390800476, 192.21037197113037, 193.16237497329712, 194.11437797546387, 195.06477117538452, 196.01516437530518, 196.9874415397644, 197.95971870422363, 198.8941991329193, 199.828679561615, 200.76248025894165, 201.6962809562683, 202.67703247070312, 203.65778398513794, 204.5962781906128, 205.53477239608765, 206.447039604187, 207.35930681228638, 208.28994798660278, 209.2205891609192, 210.17447209358215, 211.12835502624512, 212.69323134422302, 214.25810766220093]
[22.233333333333334, 22.233333333333334, 32.858333333333334, 32.858333333333334, 30.916666666666668, 30.916666666666668, 39.35, 39.35, 49.3, 49.3, 55.141666666666666, 55.141666666666666, 64.18333333333334, 64.18333333333334, 68.29166666666667, 68.29166666666667, 73.18333333333334, 73.18333333333334, 73.99166666666666, 73.99166666666666, 76.78333333333333, 76.78333333333333, 77.0, 77.0, 79.65, 79.65, 79.73333333333333, 79.73333333333333, 81.29166666666667, 81.29166666666667, 81.66666666666667, 81.66666666666667, 82.04166666666667, 82.04166666666667, 82.31666666666666, 82.31666666666666, 82.49166666666666, 82.49166666666666, 82.48333333333333, 82.48333333333333, 82.7, 82.7, 82.8, 82.8, 83.025, 83.025, 83.21666666666667, 83.21666666666667, 83.26666666666667, 83.26666666666667, 83.33333333333333, 83.33333333333333, 83.35833333333333, 83.35833333333333, 83.45, 83.45, 83.575, 83.575, 83.59166666666667, 83.59166666666667, 83.73333333333333, 83.73333333333333, 83.7, 83.7, 83.775, 83.775, 83.8, 83.8, 83.88333333333334, 83.88333333333334, 83.89166666666667, 83.89166666666667, 83.99166666666666, 83.99166666666666, 84.01666666666667, 84.01666666666667, 83.98333333333333, 83.98333333333333, 84.05833333333334, 84.05833333333334, 84.15, 84.15, 84.15, 84.15, 84.15833333333333, 84.15833333333333, 84.25, 84.25, 84.325, 84.325, 84.38333333333334, 84.38333333333334, 84.41666666666667, 84.41666666666667, 84.425, 84.425, 84.475, 84.475, 84.40833333333333, 84.40833333333333, 84.41666666666667, 84.41666666666667, 84.43333333333334, 84.43333333333334, 84.5, 84.5, 84.525, 84.525, 84.54166666666667, 84.54166666666667, 84.51666666666667, 84.51666666666667, 84.55, 84.55, 84.525, 84.525, 84.575, 84.575, 84.575, 84.575, 84.60833333333333, 84.60833333333333, 84.61666666666666, 84.61666666666666, 84.625, 84.625, 84.69166666666666, 84.69166666666666, 84.71666666666667, 84.71666666666667, 84.775, 84.775, 84.775, 84.775, 84.83333333333333, 84.83333333333333, 84.83333333333333, 84.83333333333333, 84.825, 84.825, 84.85833333333333, 84.85833333333333, 84.86666666666666, 84.86666666666666, 84.93333333333334, 84.93333333333334, 84.9, 84.9, 84.94166666666666, 84.94166666666666, 84.975, 84.975, 84.98333333333333, 84.98333333333333, 85.0, 85.0, 85.01666666666667, 85.01666666666667, 84.99166666666666, 84.99166666666666, 85.01666666666667, 85.01666666666667, 85.05, 85.05, 85.03333333333333, 85.03333333333333, 84.96666666666667, 84.96666666666667, 85.025, 85.025, 85.06666666666666, 85.06666666666666, 85.04166666666667, 85.04166666666667, 85.08333333333333, 85.08333333333333, 85.125, 85.125, 85.08333333333333, 85.08333333333333, 85.13333333333334, 85.13333333333334, 85.15, 85.15, 85.225, 85.225, 85.21666666666667, 85.21666666666667, 85.19166666666666, 85.19166666666666, 85.29166666666667, 85.29166666666667, 85.26666666666667, 85.26666666666667, 85.3, 85.3, 85.31666666666666, 85.31666666666666, 85.34166666666667, 85.34166666666667, 85.33333333333333, 85.33333333333333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.591, Test loss: 1.628, Test accuracy: 83.47
Average accuracy final 10 rounds: 83.48333333333333 

1173.9678008556366
[1.036407232284546, 2.072814464569092, 2.9814670085906982, 3.8901195526123047, 4.807952404022217, 5.725785255432129, 6.647422790527344, 7.569060325622559, 8.506988525390625, 9.444916725158691, 10.352632999420166, 11.26034927368164, 12.136494636535645, 13.012639999389648, 13.907082080841064, 14.80152416229248, 15.681779861450195, 16.56203556060791, 17.401557207107544, 18.241078853607178, 19.1038339138031, 19.966588973999023, 20.84470248222351, 21.722815990447998, 22.64738440513611, 23.57195281982422, 24.472119331359863, 25.372285842895508, 26.229422330856323, 27.08655881881714, 27.93818473815918, 28.78981065750122, 29.626707315444946, 30.463603973388672, 31.319347143173218, 32.175090312957764, 33.1162531375885, 34.05741596221924, 34.96039915084839, 35.86338233947754, 36.78091216087341, 37.69844198226929, 38.640042304992676, 39.581642627716064, 40.541619062423706, 41.50159549713135, 42.421244382858276, 43.340893268585205, 44.24582648277283, 45.15075969696045, 46.04633355140686, 46.94190740585327, 47.78499460220337, 48.62808179855347, 49.506322145462036, 50.384562492370605, 51.237563610076904, 52.0905647277832, 52.93059945106506, 53.770634174346924, 54.628334045410156, 55.48603391647339, 56.36999487876892, 57.25395584106445, 58.133854389190674, 59.013752937316895, 59.86833667755127, 60.722920417785645, 61.6024444103241, 62.48196840286255, 63.41206932067871, 64.34217023849487, 65.275461435318, 66.20875263214111, 67.12831664085388, 68.04788064956665, 68.89013266563416, 69.73238468170166, 70.5975022315979, 71.46261978149414, 72.3318099975586, 73.20100021362305, 74.12316250801086, 75.04532480239868, 75.94560194015503, 76.84587907791138, 77.76268577575684, 78.6794924736023, 79.62547421455383, 80.57145595550537, 81.51925778388977, 82.46705961227417, 83.36831259727478, 84.26956558227539, 85.1616575717926, 86.05374956130981, 86.98802375793457, 87.92229795455933, 88.87075090408325, 89.81920385360718, 90.77669477462769, 91.7341856956482, 92.66269707679749, 93.59120845794678, 94.49361062049866, 95.39601278305054, 96.30035710334778, 97.20470142364502, 98.12233209609985, 99.03996276855469, 99.9680061340332, 100.89604949951172, 101.82895755767822, 102.76186561584473, 103.65573382377625, 104.54960203170776, 105.46274709701538, 106.375892162323, 107.30980253219604, 108.24371290206909, 109.15522623062134, 110.06673955917358, 110.99125576019287, 111.91577196121216, 112.81169867515564, 113.70762538909912, 114.60237288475037, 115.49712038040161, 116.41242527961731, 117.32773017883301, 118.27136445045471, 119.21499872207642, 120.13726258277893, 121.05952644348145, 121.985680103302, 122.91183376312256, 123.82158780097961, 124.73134183883667, 125.60624265670776, 126.48114347457886, 127.41258955001831, 128.34403562545776, 129.25863695144653, 130.1732382774353, 131.05972361564636, 131.94620895385742, 132.83884024620056, 133.7314715385437, 134.61763787269592, 135.50380420684814, 136.44176959991455, 137.37973499298096, 138.33668637275696, 139.29363775253296, 140.19401574134827, 141.09439373016357, 142.00244331359863, 142.9104928970337, 143.8193507194519, 144.72820854187012, 145.5979926586151, 146.4677767753601, 147.4110038280487, 148.3542308807373, 149.27362704277039, 150.19302320480347, 151.0777144432068, 151.9624056816101, 152.90560293197632, 153.84880018234253, 154.76662373542786, 155.68444728851318, 156.57060432434082, 157.45676136016846, 158.39166378974915, 159.32656621932983, 160.25351858139038, 161.18047094345093, 162.0849952697754, 162.98951959609985, 163.93476581573486, 164.88001203536987, 165.7898154258728, 166.69961881637573, 167.63569617271423, 168.57177352905273, 169.48967838287354, 170.40758323669434, 171.32127213478088, 172.23496103286743, 173.1025514602661, 173.9701418876648, 174.82796549797058, 175.68578910827637, 176.54210782051086, 177.39842653274536, 178.24543118476868, 179.092435836792, 179.9244544506073, 180.7564730644226, 182.16388463974, 183.57129621505737]
[14.116666666666667, 14.116666666666667, 20.1, 20.1, 30.35, 30.35, 39.28333333333333, 39.28333333333333, 38.18333333333333, 38.18333333333333, 34.95, 34.95, 41.425, 41.425, 51.65, 51.65, 54.233333333333334, 54.233333333333334, 57.19166666666667, 57.19166666666667, 60.56666666666667, 60.56666666666667, 62.94166666666667, 62.94166666666667, 65.55, 65.55, 68.08333333333333, 68.08333333333333, 70.675, 70.675, 71.51666666666667, 71.51666666666667, 72.5, 72.5, 73.61666666666666, 73.61666666666666, 74.10833333333333, 74.10833333333333, 74.74166666666666, 74.74166666666666, 76.175, 76.175, 77.80833333333334, 77.80833333333334, 78.64166666666667, 78.64166666666667, 79.40833333333333, 79.40833333333333, 79.91666666666667, 79.91666666666667, 80.09166666666667, 80.09166666666667, 80.075, 80.075, 80.40833333333333, 80.40833333333333, 80.55, 80.55, 80.66666666666667, 80.66666666666667, 81.20833333333333, 81.20833333333333, 81.175, 81.175, 81.24166666666666, 81.24166666666666, 81.54166666666667, 81.54166666666667, 81.71666666666667, 81.71666666666667, 81.91666666666667, 81.91666666666667, 82.34166666666667, 82.34166666666667, 82.25, 82.25, 82.3, 82.3, 82.53333333333333, 82.53333333333333, 82.74166666666666, 82.74166666666666, 82.675, 82.675, 82.65, 82.65, 82.69166666666666, 82.69166666666666, 82.70833333333333, 82.70833333333333, 82.675, 82.675, 82.64166666666667, 82.64166666666667, 82.65, 82.65, 82.66666666666667, 82.66666666666667, 82.78333333333333, 82.78333333333333, 82.78333333333333, 82.78333333333333, 82.8, 82.8, 82.73333333333333, 82.73333333333333, 82.825, 82.825, 82.83333333333333, 82.83333333333333, 82.93333333333334, 82.93333333333334, 82.775, 82.775, 82.91666666666667, 82.91666666666667, 82.98333333333333, 82.98333333333333, 82.95, 82.95, 82.89166666666667, 82.89166666666667, 82.84166666666667, 82.84166666666667, 82.84166666666667, 82.84166666666667, 82.88333333333334, 82.88333333333334, 82.85833333333333, 82.85833333333333, 82.975, 82.975, 82.94166666666666, 82.94166666666666, 82.91666666666667, 82.91666666666667, 82.99166666666666, 82.99166666666666, 82.9, 82.9, 83.09166666666667, 83.09166666666667, 83.09166666666667, 83.09166666666667, 82.80833333333334, 82.80833333333334, 82.68333333333334, 82.68333333333334, 82.88333333333334, 82.88333333333334, 82.80833333333334, 82.80833333333334, 83.10833333333333, 83.10833333333333, 83.15833333333333, 83.15833333333333, 83.13333333333334, 83.13333333333334, 83.13333333333334, 83.13333333333334, 83.28333333333333, 83.28333333333333, 83.26666666666667, 83.26666666666667, 83.33333333333333, 83.33333333333333, 83.225, 83.225, 83.2, 83.2, 83.30833333333334, 83.30833333333334, 83.325, 83.325, 83.33333333333333, 83.33333333333333, 83.375, 83.375, 83.425, 83.425, 83.54166666666667, 83.54166666666667, 83.575, 83.575, 83.49166666666666, 83.49166666666666, 83.45833333333333, 83.45833333333333, 83.475, 83.475, 83.54166666666667, 83.54166666666667, 83.44166666666666, 83.44166666666666, 83.38333333333334, 83.38333333333334, 83.45833333333333, 83.45833333333333, 83.46666666666667, 83.46666666666667, 83.46666666666667, 83.46666666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.485, Test loss: 1.527, Test accuracy: 93.51
Average accuracy final 10 rounds: 93.56666666666668 

1278.5864372253418
[1.141479730606079, 2.282959461212158, 3.336573600769043, 4.390187740325928, 5.470189094543457, 6.550190448760986, 7.609613418579102, 8.669036388397217, 9.731897115707397, 10.794757843017578, 11.859864950180054, 12.92497205734253, 13.98392105102539, 15.042870044708252, 16.14342164993286, 17.24397325515747, 18.327367067337036, 19.4107608795166, 20.489559412002563, 21.568357944488525, 22.643165349960327, 23.71797275543213, 24.796759366989136, 25.875545978546143, 26.96225118637085, 28.048956394195557, 29.150367975234985, 30.251779556274414, 31.30128312110901, 32.3507866859436, 33.4213547706604, 34.4919228553772, 35.59115743637085, 36.6903920173645, 37.77765488624573, 38.86491775512695, 39.922980070114136, 40.98104238510132, 42.04857063293457, 43.11609888076782, 44.20203995704651, 45.287981033325195, 46.386096477508545, 47.484211921691895, 48.562817096710205, 49.641422271728516, 50.71016836166382, 51.77891445159912, 52.844873666763306, 53.91083288192749, 54.99179196357727, 56.07275104522705, 57.15727615356445, 58.241801261901855, 59.30686378479004, 60.37192630767822, 61.42402625083923, 62.476126194000244, 63.547133684158325, 64.6181411743164, 65.70809507369995, 66.7980489730835, 67.86587834358215, 68.93370771408081, 69.99720907211304, 71.06071043014526, 72.12875509262085, 73.19679975509644, 74.26131701469421, 75.32583427429199, 76.42665982246399, 77.52748537063599, 78.59618663787842, 79.66488790512085, 80.74104118347168, 81.81719446182251, 82.89230942726135, 83.9674243927002, 85.04675936698914, 86.12609434127808, 87.21605253219604, 88.30601072311401, 89.38300943374634, 90.46000814437866, 91.51748704910278, 92.5749659538269, 93.64782857894897, 94.72069120407104, 95.80923986434937, 96.89778852462769, 97.97176885604858, 99.04574918746948, 100.1098141670227, 101.17387914657593, 102.23041152954102, 103.2869439125061, 104.37009644508362, 105.45324897766113, 106.55647921562195, 107.65970945358276, 108.72940683364868, 109.7991042137146, 110.87060356140137, 111.94210290908813, 113.02309155464172, 114.10408020019531, 115.18488764762878, 116.26569509506226, 117.35507249832153, 118.44444990158081, 119.52968454360962, 120.61491918563843, 121.68227052688599, 122.74962186813354, 123.89645767211914, 125.04329347610474, 126.12683153152466, 127.21036958694458, 128.26941204071045, 129.32845449447632, 130.40272331237793, 131.47699213027954, 132.54271697998047, 133.6084418296814, 134.68290972709656, 135.75737762451172, 136.8465075492859, 137.93563747406006, 138.99490237236023, 140.0541672706604, 141.12383484840393, 142.19350242614746, 143.27663731575012, 144.35977220535278, 145.42732787132263, 146.49488353729248, 147.5747480392456, 148.65461254119873, 149.72716760635376, 150.7997226715088, 151.8513777256012, 152.9030327796936, 153.98328113555908, 155.06352949142456, 156.1641161441803, 157.26470279693604, 158.31296396255493, 159.36122512817383, 160.42738795280457, 161.4935507774353, 162.5531346797943, 163.61271858215332, 164.69853949546814, 165.78436040878296, 166.86464953422546, 167.94493865966797, 168.99344611167908, 170.04195356369019, 171.1327109336853, 172.22346830368042, 173.33432126045227, 174.44517421722412, 175.50918865203857, 176.57320308685303, 177.6617612838745, 178.750319480896, 179.84683060646057, 180.94334173202515, 182.00613594055176, 183.06893014907837, 184.1463747024536, 185.22381925582886, 186.30438351631165, 187.38494777679443, 188.45436453819275, 189.52378129959106, 190.6074903011322, 191.69119930267334, 192.75178837776184, 193.81237745285034, 194.91301250457764, 196.01364755630493, 197.11449360847473, 198.21533966064453, 199.2583785057068, 200.30141735076904, 201.39147639274597, 202.4815354347229, 203.5881199836731, 204.6947045326233, 205.77776169776917, 206.86081886291504, 207.91627478599548, 208.97173070907593, 210.05208373069763, 211.13243675231934, 212.19965291023254, 213.26686906814575, 214.36888670921326, 215.47090435028076, 216.89129090309143, 218.3116774559021]
[29.083333333333332, 29.083333333333332, 41.325, 41.325, 38.95, 38.95, 56.7, 56.7, 71.575, 71.575, 78.20833333333333, 78.20833333333333, 80.46666666666667, 80.46666666666667, 81.93333333333334, 81.93333333333334, 82.95, 82.95, 83.88333333333334, 83.88333333333334, 84.39166666666667, 84.39166666666667, 84.43333333333334, 84.43333333333334, 84.93333333333334, 84.93333333333334, 85.81666666666666, 85.81666666666666, 86.75, 86.75, 87.39166666666667, 87.39166666666667, 88.40833333333333, 88.40833333333333, 88.9, 88.9, 88.9, 88.9, 89.375, 89.375, 89.45833333333333, 89.45833333333333, 89.90833333333333, 89.90833333333333, 90.30833333333334, 90.30833333333334, 90.68333333333334, 90.68333333333334, 91.525, 91.525, 91.79166666666667, 91.79166666666667, 91.975, 91.975, 92.09166666666667, 92.09166666666667, 92.225, 92.225, 92.35, 92.35, 92.44166666666666, 92.44166666666666, 92.66666666666667, 92.66666666666667, 92.725, 92.725, 92.75, 92.75, 92.725, 92.725, 92.83333333333333, 92.83333333333333, 92.925, 92.925, 92.85833333333333, 92.85833333333333, 92.85833333333333, 92.85833333333333, 92.91666666666667, 92.91666666666667, 92.89166666666667, 92.89166666666667, 92.95, 92.95, 92.95833333333333, 92.95833333333333, 92.99166666666666, 92.99166666666666, 92.94166666666666, 92.94166666666666, 93.15833333333333, 93.15833333333333, 93.2, 93.2, 93.05, 93.05, 93.11666666666666, 93.11666666666666, 93.3, 93.3, 93.31666666666666, 93.31666666666666, 93.23333333333333, 93.23333333333333, 93.25833333333334, 93.25833333333334, 93.325, 93.325, 93.33333333333333, 93.33333333333333, 93.28333333333333, 93.28333333333333, 93.23333333333333, 93.23333333333333, 93.21666666666667, 93.21666666666667, 93.3, 93.3, 93.25833333333334, 93.25833333333334, 93.24166666666666, 93.24166666666666, 93.30833333333334, 93.30833333333334, 93.31666666666666, 93.31666666666666, 93.375, 93.375, 93.35, 93.35, 93.41666666666667, 93.41666666666667, 93.4, 93.4, 93.33333333333333, 93.33333333333333, 93.39166666666667, 93.39166666666667, 93.34166666666667, 93.34166666666667, 93.41666666666667, 93.41666666666667, 93.475, 93.475, 93.39166666666667, 93.39166666666667, 93.39166666666667, 93.39166666666667, 93.43333333333334, 93.43333333333334, 93.48333333333333, 93.48333333333333, 93.575, 93.575, 93.50833333333334, 93.50833333333334, 93.43333333333334, 93.43333333333334, 93.46666666666667, 93.46666666666667, 93.49166666666666, 93.49166666666666, 93.48333333333333, 93.48333333333333, 93.49166666666666, 93.49166666666666, 93.48333333333333, 93.48333333333333, 93.46666666666667, 93.46666666666667, 93.54166666666667, 93.54166666666667, 93.55833333333334, 93.55833333333334, 93.54166666666667, 93.54166666666667, 93.54166666666667, 93.54166666666667, 93.525, 93.525, 93.59166666666667, 93.59166666666667, 93.6, 93.6, 93.525, 93.525, 93.53333333333333, 93.53333333333333, 93.56666666666666, 93.56666666666666, 93.54166666666667, 93.54166666666667, 93.55833333333334, 93.55833333333334, 93.61666666666666, 93.61666666666666, 93.56666666666666, 93.56666666666666, 93.56666666666666, 93.56666666666666, 93.50833333333334, 93.50833333333334]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.481, Test loss: 1.588, Test accuracy: 87.72
Average accuracy final 10 rounds: 87.69583333333331 

1168.6530883312225
[1.0629377365112305, 2.125875473022461, 3.157031774520874, 4.188188076019287, 5.101267337799072, 6.014346599578857, 6.879811763763428, 7.745276927947998, 8.597429037094116, 9.449581146240234, 10.331354856491089, 11.213128566741943, 12.115550756454468, 13.017972946166992, 13.911251544952393, 14.804530143737793, 15.678631782531738, 16.552733421325684, 17.433658361434937, 18.31458330154419, 19.16689372062683, 20.019204139709473, 21.039144277572632, 22.05908441543579, 22.92207670211792, 23.78506898880005, 24.631380081176758, 25.477691173553467, 26.3708393573761, 27.26398754119873, 28.14904761314392, 29.03410768508911, 29.9437255859375, 30.85334348678589, 31.734910488128662, 32.616477489471436, 33.51809501647949, 34.41971254348755, 35.34322905540466, 36.26674556732178, 37.14359498023987, 38.02044439315796, 38.92005896568298, 39.81967353820801, 40.69687247276306, 41.574071407318115, 42.467939376831055, 43.361807346343994, 44.227253675460815, 45.09270000457764, 45.98376536369324, 46.87483072280884, 47.7633330821991, 48.651835441589355, 49.52548789978027, 50.39914035797119, 51.29324436187744, 52.18734836578369, 53.04840302467346, 53.90945768356323, 54.773868560791016, 55.6382794380188, 56.53935170173645, 57.4404239654541, 58.343050479888916, 59.24567699432373, 60.120527505874634, 60.99537801742554, 61.83992004394531, 62.68446207046509, 63.57043766975403, 64.45641326904297, 65.32245755195618, 66.18850183486938, 67.08249521255493, 67.97648859024048, 68.82177972793579, 69.6670708656311, 70.56206560134888, 71.45706033706665, 72.34211754798889, 73.22717475891113, 74.09954309463501, 74.97191143035889, 75.84772849082947, 76.72354555130005, 77.58748412132263, 78.45142269134521, 79.33748388290405, 80.22354507446289, 81.11189389228821, 82.00024271011353, 82.85153484344482, 83.70282697677612, 84.60730075836182, 85.51177453994751, 86.400062084198, 87.28834962844849, 88.18139982223511, 89.07445001602173, 89.9303331375122, 90.78621625900269, 91.67933773994446, 92.57245922088623, 93.45461821556091, 94.3367772102356, 95.2244701385498, 96.11216306686401, 96.98966217041016, 97.8671612739563, 98.71683812141418, 99.56651496887207, 100.4685742855072, 101.37063360214233, 102.29738450050354, 103.22413539886475, 104.11870813369751, 105.01328086853027, 105.90978980064392, 106.80629873275757, 107.69778394699097, 108.58926916122437, 109.50030064582825, 110.41133213043213, 111.30563616752625, 112.19994020462036, 113.09337830543518, 113.98681640625, 114.86404967308044, 115.74128293991089, 116.63815689086914, 117.53503084182739, 118.44534993171692, 119.35566902160645, 120.30633282661438, 121.25699663162231, 122.17138576507568, 123.08577489852905, 123.95436978340149, 124.82296466827393, 125.73648953437805, 126.65001440048218, 127.57128262519836, 128.49255084991455, 129.4136152267456, 130.33467960357666, 131.25873923301697, 132.18279886245728, 133.0565025806427, 133.93020629882812, 134.83251309394836, 135.7348198890686, 136.63088989257812, 137.52695989608765, 138.45298981666565, 139.37901973724365, 140.2890317440033, 141.19904375076294, 142.1012420654297, 143.00344038009644, 143.90755534172058, 144.81167030334473, 145.72507333755493, 146.63847637176514, 147.53899550437927, 148.4395146369934, 149.3459939956665, 150.2524733543396, 151.16201639175415, 152.0715594291687, 152.9642460346222, 153.85693264007568, 154.74051356315613, 155.62409448623657, 156.52460193634033, 157.4251093864441, 158.32718420028687, 159.22925901412964, 160.1249589920044, 161.02065896987915, 161.9014070034027, 162.78215503692627, 163.6913890838623, 164.60062313079834, 165.50022721290588, 166.39983129501343, 167.29618287086487, 168.1925344467163, 169.0923933982849, 169.99225234985352, 170.8610327243805, 171.72981309890747, 172.58525133132935, 173.44068956375122, 174.33285689353943, 175.22502422332764, 176.14188957214355, 177.05875492095947, 177.94748163223267, 178.83620834350586, 180.29710960388184, 181.7580108642578]
[23.266666666666666, 23.266666666666666, 45.208333333333336, 45.208333333333336, 48.858333333333334, 48.858333333333334, 46.95, 46.95, 57.108333333333334, 57.108333333333334, 64.63333333333334, 64.63333333333334, 69.55833333333334, 69.55833333333334, 74.45, 74.45, 75.86666666666666, 75.86666666666666, 80.04166666666667, 80.04166666666667, 83.83333333333333, 83.83333333333333, 84.86666666666666, 84.86666666666666, 86.08333333333333, 86.08333333333333, 86.66666666666667, 86.66666666666667, 86.79166666666667, 86.79166666666667, 86.89166666666667, 86.89166666666667, 86.925, 86.925, 87.3, 87.3, 87.39166666666667, 87.39166666666667, 87.64166666666667, 87.64166666666667, 87.58333333333333, 87.58333333333333, 87.68333333333334, 87.68333333333334, 87.7, 87.7, 87.675, 87.675, 87.65833333333333, 87.65833333333333, 87.74166666666666, 87.74166666666666, 87.76666666666667, 87.76666666666667, 87.825, 87.825, 87.83333333333333, 87.83333333333333, 87.83333333333333, 87.83333333333333, 87.75833333333334, 87.75833333333334, 87.70833333333333, 87.70833333333333, 87.675, 87.675, 87.75, 87.75, 87.69166666666666, 87.69166666666666, 87.64166666666667, 87.64166666666667, 87.70833333333333, 87.70833333333333, 87.71666666666667, 87.71666666666667, 87.70833333333333, 87.70833333333333, 87.66666666666667, 87.66666666666667, 87.73333333333333, 87.73333333333333, 87.675, 87.675, 87.68333333333334, 87.68333333333334, 87.75, 87.75, 87.73333333333333, 87.73333333333333, 87.725, 87.725, 87.725, 87.725, 87.69166666666666, 87.69166666666666, 87.73333333333333, 87.73333333333333, 87.74166666666666, 87.74166666666666, 87.7, 87.7, 87.75833333333334, 87.75833333333334, 87.70833333333333, 87.70833333333333, 87.725, 87.725, 87.675, 87.675, 87.725, 87.725, 87.73333333333333, 87.73333333333333, 87.7, 87.7, 87.70833333333333, 87.70833333333333, 87.7, 87.7, 87.7, 87.7, 87.70833333333333, 87.70833333333333, 87.69166666666666, 87.69166666666666, 87.73333333333333, 87.73333333333333, 87.69166666666666, 87.69166666666666, 87.71666666666667, 87.71666666666667, 87.71666666666667, 87.71666666666667, 87.725, 87.725, 87.75833333333334, 87.75833333333334, 87.75833333333334, 87.75833333333334, 87.725, 87.725, 87.71666666666667, 87.71666666666667, 87.75833333333334, 87.75833333333334, 87.76666666666667, 87.76666666666667, 87.8, 87.8, 87.8, 87.8, 87.825, 87.825, 87.79166666666667, 87.79166666666667, 87.775, 87.775, 87.71666666666667, 87.71666666666667, 87.725, 87.725, 87.75833333333334, 87.75833333333334, 87.74166666666666, 87.74166666666666, 87.71666666666667, 87.71666666666667, 87.66666666666667, 87.66666666666667, 87.65833333333333, 87.65833333333333, 87.70833333333333, 87.70833333333333, 87.68333333333334, 87.68333333333334, 87.725, 87.725, 87.75, 87.75, 87.73333333333333, 87.73333333333333, 87.70833333333333, 87.70833333333333, 87.69166666666666, 87.69166666666666, 87.68333333333334, 87.68333333333334, 87.69166666666666, 87.69166666666666, 87.68333333333334, 87.68333333333334, 87.69166666666666, 87.69166666666666, 87.69166666666666, 87.69166666666666, 87.7, 87.7, 87.68333333333334, 87.68333333333334, 87.71666666666667, 87.71666666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Final Round, Train loss: 1.107, Test loss: 1.856, Test accuracy: 74.92
Average accuracy final 10 rounds: 75.3675
1468.620432138443
[]
[30.958333333333332, 43.05833333333333, 58.34166666666667, 63.641666666666666, 68.21666666666667, 71.025, 72.24166666666666, 72.95833333333333, 73.125, 74.15833333333333, 76.45, 77.10833333333333, 77.73333333333333, 78.0, 78.53333333333333, 78.20833333333333, 78.35833333333333, 79.00833333333334, 78.86666666666666, 78.75, 78.60833333333333, 78.65, 78.34166666666667, 78.31666666666666, 78.09166666666667, 77.73333333333333, 77.60833333333333, 77.53333333333333, 77.36666666666666, 77.25833333333334, 78.525, 80.01666666666667, 81.10833333333333, 81.525, 81.275, 81.69166666666666, 82.15, 82.33333333333333, 82.76666666666667, 82.59166666666667, 82.475, 82.70833333333333, 82.40833333333333, 82.08333333333333, 81.89166666666667, 81.79166666666667, 81.66666666666667, 81.49166666666666, 81.39166666666667, 81.00833333333334, 81.03333333333333, 80.74166666666666, 80.65, 80.50833333333334, 80.225, 79.98333333333333, 79.875, 79.73333333333333, 79.55833333333334, 79.39166666666667, 79.18333333333334, 79.14166666666667, 78.74166666666666, 78.63333333333334, 78.65, 78.43333333333334, 78.23333333333333, 78.125, 78.025, 77.86666666666666, 77.68333333333334, 77.49166666666666, 77.49166666666666, 77.475, 77.18333333333334, 77.15833333333333, 77.1, 76.89166666666667, 76.76666666666667, 76.475, 76.33333333333333, 76.29166666666667, 76.25, 76.29166666666667, 76.13333333333334, 76.04166666666667, 75.9, 76.01666666666667, 76.0, 75.76666666666667, 75.71666666666667, 75.81666666666666, 75.5, 75.46666666666667, 75.44166666666666, 75.2, 75.30833333333334, 75.15, 75.03333333333333, 75.04166666666667, 74.925]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.624, Test loss: 1.649, Test accuracy: 81.97
Average accuracy final 10 rounds: 80.69333333333333
Average global accuracy final 10 rounds: 80.69333333333333
1028.0680482387543
[]
[13.6, 15.341666666666667, 15.916666666666666, 19.783333333333335, 19.4, 21.491666666666667, 22.975, 26.358333333333334, 30.416666666666668, 30.833333333333332, 34.65, 32.65833333333333, 31.45, 37.85, 38.6, 39.30833333333333, 30.308333333333334, 32.083333333333336, 36.94166666666667, 39.2, 35.625, 44.78333333333333, 52.05, 53.40833333333333, 51.525, 53.175, 54.858333333333334, 60.19166666666667, 59.983333333333334, 61.36666666666667, 61.825, 62.28333333333333, 60.69166666666667, 60.166666666666664, 60.358333333333334, 64.64166666666667, 63.891666666666666, 64.61666666666666, 66.3, 66.23333333333333, 66.55833333333334, 66.95, 66.80833333333334, 67.7, 67.875, 67.54166666666667, 67.74166666666666, 68.96666666666667, 68.61666666666666, 69.33333333333333, 71.475, 71.75833333333334, 72.20833333333333, 71.96666666666667, 73.225, 73.29166666666667, 74.15, 74.525, 74.65833333333333, 74.575, 76.6, 77.05833333333334, 77.29166666666667, 76.48333333333333, 76.90833333333333, 75.96666666666667, 76.11666666666666, 76.34166666666667, 76.9, 78.64166666666667, 77.56666666666666, 77.39166666666667, 77.96666666666667, 78.35833333333333, 78.35833333333333, 78.75, 79.1, 79.44166666666666, 79.49166666666666, 79.50833333333334, 79.45833333333333, 79.825, 79.75833333333334, 80.15833333333333, 79.63333333333334, 79.93333333333334, 80.175, 80.25, 80.30833333333334, 80.2, 80.5, 80.775, 80.50833333333334, 80.58333333333333, 80.74166666666666, 80.9, 80.51666666666667, 80.8, 80.69166666666666, 80.91666666666667, 81.975]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.34
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.29
Average accuracy final 10 rounds: 14.653333333333334 

Average global accuracy final 10 rounds: 14.9475 

1298.7192559242249
[1.2752339839935303, 2.4514479637145996, 3.6223623752593994, 4.828964710235596, 6.007100343704224, 7.18610954284668, 8.357584953308105, 9.531097412109375, 10.696064949035645, 11.869296312332153, 13.040042638778687, 14.191711187362671, 15.341038703918457, 16.479093313217163, 17.623348236083984, 18.75667929649353, 19.886895656585693, 21.031086921691895, 22.167495489120483, 23.32225275039673, 24.492369413375854, 25.646701097488403, 26.808394193649292, 27.979653358459473, 29.14018678665161, 30.31191921234131, 31.468493700027466, 32.616249561309814, 33.76846957206726, 34.92830276489258, 36.0817174911499, 37.24819350242615, 38.41660499572754, 39.588886976242065, 40.74758815765381, 41.92297291755676, 43.08507251739502, 44.26380372047424, 45.44936513900757, 46.62545347213745, 47.77557826042175, 48.94735884666443, 50.12294888496399, 51.29566812515259, 52.467864990234375, 53.64137268066406, 54.815509557724, 55.98544216156006, 57.150026082992554, 58.31712365150452, 59.481292724609375, 60.652607917785645, 61.831782579422, 62.98651146888733, 64.14041018486023, 65.33201932907104, 66.50806021690369, 67.6863784790039, 68.86763215065002, 70.04920434951782, 71.21628642082214, 72.3992612361908, 73.56542563438416, 74.74830746650696, 75.91281175613403, 77.07176423072815, 78.21932291984558, 79.40243196487427, 80.58926510810852, 81.766188621521, 82.93679451942444, 84.0978946685791, 85.1343309879303, 86.26290774345398, 87.38652443885803, 88.50321292877197, 89.6496012210846, 90.76776671409607, 91.87720489501953, 92.99452042579651, 94.10657334327698, 95.23175525665283, 96.3462917804718, 97.47474765777588, 98.60771083831787, 99.75276327133179, 100.91440558433533, 102.04828643798828, 103.18655562400818, 104.27490997314453, 105.39615821838379, 106.49297738075256, 107.59134554862976, 108.68209838867188, 109.61032962799072, 110.53246665000916, 111.44294285774231, 112.3552463054657, 113.28477931022644, 114.19517683982849, 115.71508312225342]
[7.858333333333333, 8.0, 7.983333333333333, 7.933333333333334, 8.05, 8.108333333333333, 8.108333333333333, 8.175, 8.258333333333333, 8.375, 8.516666666666667, 8.541666666666666, 8.575, 8.666666666666666, 8.775, 8.85, 8.933333333333334, 8.975, 9.033333333333333, 9.066666666666666, 9.133333333333333, 9.183333333333334, 9.25, 9.383333333333333, 9.458333333333334, 9.55, 9.633333333333333, 9.675, 9.741666666666667, 9.791666666666666, 9.858333333333333, 9.95, 9.991666666666667, 10.025, 10.033333333333333, 10.1, 10.125, 10.191666666666666, 10.241666666666667, 10.241666666666667, 10.258333333333333, 10.283333333333333, 10.325, 10.425, 10.533333333333333, 10.583333333333334, 10.708333333333334, 10.758333333333333, 10.85, 10.975, 11.025, 11.075, 11.183333333333334, 11.183333333333334, 11.241666666666667, 11.3, 11.366666666666667, 11.475, 11.541666666666666, 11.583333333333334, 11.708333333333334, 11.75, 11.833333333333334, 11.941666666666666, 12.058333333333334, 12.133333333333333, 12.183333333333334, 12.283333333333333, 12.341666666666667, 12.416666666666666, 12.533333333333333, 12.583333333333334, 12.708333333333334, 12.85, 12.991666666666667, 13.033333333333333, 13.091666666666667, 13.158333333333333, 13.25, 13.308333333333334, 13.441666666666666, 13.483333333333333, 13.575, 13.616666666666667, 13.683333333333334, 13.733333333333333, 13.766666666666667, 13.875, 13.975, 14.066666666666666, 14.15, 14.258333333333333, 14.408333333333333, 14.516666666666667, 14.666666666666666, 14.741666666666667, 14.841666666666667, 14.9, 14.975, 15.075, 15.341666666666667]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Final Round, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.89
Average accuracy final 10 rounds: 96.93050000000001
7375.278483390808
[9.029003858566284, 17.799676656723022, 26.493372201919556, 35.27851343154907, 44.268237829208374, 53.347811698913574, 62.6358118057251, 71.78593683242798, 80.86158895492554, 90.53207921981812, 100.13780117034912, 109.87922787666321, 119.82362580299377, 129.74561667442322, 139.5903239250183, 149.42225098609924, 159.1243381500244, 168.95611929893494, 178.55160927772522, 188.2546420097351, 198.0271918773651, 207.77272725105286, 217.39309740066528, 226.8784897327423, 236.66338753700256, 246.47467017173767, 256.2392432689667, 266.077951669693, 275.94244933128357, 285.7672772407532, 295.5894777774811, 305.46490240097046, 315.29181480407715, 325.056827545166, 334.99915766716003, 344.88721323013306, 354.7010815143585, 364.412410736084, 374.127477645874, 383.954630613327, 393.5644371509552, 403.29462146759033, 413.09575152397156, 423.21602034568787, 433.043395280838, 442.7740137577057, 452.42563009262085, 462.5891089439392, 472.55981516838074, 482.568683385849, 492.51974272727966, 503.68503999710083, 514.719720363617, 525.9558866024017, 537.0026612281799, 547.6469113826752, 558.7638974189758, 569.6235036849976, 580.7723476886749, 591.8008434772491, 602.6748220920563, 613.7372643947601, 624.7535364627838, 635.7407991886139, 646.8241198062897, 657.6218914985657, 668.4883253574371, 679.5758035182953, 690.629950761795, 701.4548301696777, 712.5944402217865, 723.6185276508331, 734.6763112545013, 745.6895701885223, 756.7817580699921, 767.8766891956329, 779.0054187774658, 790.0246574878693, 800.7605683803558, 811.8975503444672, 822.871657371521, 833.5226860046387, 843.8147277832031, 854.4013032913208, 865.0496184825897, 875.7476117610931, 886.2483472824097, 896.9925301074982, 907.9007289409637, 918.5820407867432, 929.0160105228424, 939.151380777359, 949.1690292358398, 959.0933947563171, 969.7770853042603, 980.4141938686371, 990.9257590770721, 1000.8938279151917, 1010.874977350235, 1021.4937016963959, 1024.2136945724487]
[27.9075, 64.57, 67.18, 73.545, 74.2075, 74.9175, 75.1775, 75.4725, 75.62, 75.7025, 75.9225, 76.12, 76.3525, 83.925, 85.115, 85.54, 85.705, 85.8425, 86.1225, 86.1675, 86.2975, 86.5, 86.5275, 86.645, 86.77, 86.9275, 86.8825, 86.9675, 87.0825, 87.0575, 87.1175, 87.0775, 87.18, 87.195, 87.355, 87.325, 87.4725, 87.515, 87.4375, 87.435, 87.5525, 87.5875, 87.535, 87.53, 95.66, 95.9525, 95.925, 96.1775, 96.15, 96.345, 96.42, 96.39, 96.475, 96.48, 96.4125, 96.6225, 96.57, 96.5775, 96.6675, 96.7675, 96.775, 96.795, 96.795, 96.745, 96.765, 96.835, 96.79, 96.835, 96.75, 96.7825, 96.75, 96.8125, 96.7925, 96.905, 96.875, 96.76, 96.8075, 96.835, 96.9325, 96.84, 96.8175, 96.85, 96.84, 96.87, 96.82, 96.875, 96.9325, 96.8725, 96.925, 96.9525, 96.975, 96.97, 96.92, 96.9075, 96.925, 96.9725, 96.895, 96.9575, 96.94, 96.8425, 96.8875]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.473, Test loss: 1.505, Test accuracy: 96.64
Average accuracy final 10 rounds: 96.55111111111111
2118.634970188141
[2.5241611003875732, 4.890737533569336, 7.266373872756958, 9.605496406555176, 11.992495059967041, 14.48570442199707, 16.837568044662476, 19.207905530929565, 21.678259134292603, 24.019683361053467, 26.44382905960083, 28.811082124710083, 31.205907106399536, 33.58325242996216, 35.97824764251709, 38.36063194274902, 40.80664300918579, 43.23060154914856, 45.59325408935547, 48.0678551197052, 50.460646629333496, 52.84084868431091, 55.296724796295166, 57.71630382537842, 60.16617226600647, 62.60646104812622, 64.94542407989502, 67.39152431488037, 69.75807476043701, 72.11818289756775, 74.5067527294159, 76.9017550945282, 79.19235110282898, 81.62762188911438, 83.94826984405518, 86.37245011329651, 88.78895473480225, 91.19102001190186, 93.60912799835205, 96.09955835342407, 98.45827865600586, 100.91524457931519, 103.32413244247437, 105.75092506408691, 108.21178770065308, 110.54797387123108, 112.93363571166992, 115.29896378517151, 117.72663831710815, 120.20672035217285, 122.72912120819092, 125.21742415428162, 127.68595147132874, 130.1083996295929, 132.4849181175232, 134.86491203308105, 137.21486163139343, 139.58593201637268, 141.993910074234, 144.46436166763306, 146.88750576972961, 149.26944065093994, 151.60210394859314, 154.03158974647522, 156.53161883354187, 158.95790553092957, 161.33512425422668, 163.81733632087708, 166.20004105567932, 168.6495921611786, 171.1665904521942, 173.6068525314331, 176.04086065292358, 178.5083441734314, 180.9005365371704, 183.31108212471008, 185.6977653503418, 188.0910563468933, 190.5395064353943, 192.93164229393005, 195.38609981536865, 197.77899408340454, 200.11180233955383, 202.52779698371887, 204.93188977241516, 207.20223188400269, 209.53731751441956, 211.8441698551178, 214.0929172039032, 216.49336338043213, 218.81084322929382, 221.10600996017456, 223.42356204986572, 225.72883224487305, 228.06864714622498, 230.44786596298218, 232.74939036369324, 235.1106674671173, 237.41924905776978, 239.80394577980042, 241.8074085712433]
[16.383333333333333, 29.6, 36.26111111111111, 45.50555555555555, 47.294444444444444, 58.98888888888889, 63.7, 65.43888888888888, 72.15, 75.26666666666667, 79.97222222222223, 84.03333333333333, 87.03888888888889, 89.04444444444445, 90.12222222222222, 90.48333333333333, 90.83888888888889, 91.19444444444444, 92.25555555555556, 92.57777777777778, 92.86666666666666, 93.05555555555556, 93.27777777777777, 93.5111111111111, 93.69444444444444, 93.82222222222222, 93.95555555555555, 94.24444444444444, 94.31111111111112, 94.36111111111111, 94.45, 94.6, 94.63333333333334, 94.72222222222223, 94.86666666666666, 94.96111111111111, 95.02777777777777, 94.96111111111111, 95.09444444444445, 95.16111111111111, 95.12222222222222, 95.16111111111111, 95.21666666666667, 95.28333333333333, 95.28888888888889, 95.36666666666666, 95.45555555555555, 95.56666666666666, 95.61666666666666, 95.63333333333334, 95.67777777777778, 95.68333333333334, 95.67222222222222, 95.66666666666667, 95.80555555555556, 95.75, 95.82222222222222, 95.95555555555555, 95.93888888888888, 95.96666666666667, 96.0, 96.1, 96.06666666666666, 96.06111111111112, 96.06111111111112, 96.2, 96.11666666666666, 96.16111111111111, 96.19444444444444, 96.09444444444445, 96.25555555555556, 96.28333333333333, 96.2388888888889, 96.20555555555555, 96.27777777777777, 96.22777777777777, 96.22777777777777, 96.26666666666667, 96.35, 96.39444444444445, 96.34444444444445, 96.35555555555555, 96.42222222222222, 96.4, 96.43888888888888, 96.40555555555555, 96.47777777777777, 96.51666666666667, 96.47777777777777, 96.48333333333333, 96.46666666666667, 96.45555555555555, 96.56111111111112, 96.56666666666666, 96.54444444444445, 96.61111111111111, 96.56111111111112, 96.58333333333333, 96.57777777777778, 96.58333333333333, 96.64444444444445]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.478, Test loss: 1.514, Test accuracy: 96.91
Average accuracy final 10 rounds: 96.85111111111112
2867.4949791431427
[2.5100109577178955, 5.020021915435791, 7.50988507270813, 9.999748229980469, 12.475925922393799, 14.952103614807129, 17.45045280456543, 19.94880199432373, 22.422621488571167, 24.896440982818604, 27.342565536499023, 29.788690090179443, 32.2017240524292, 34.614758014678955, 37.016151666641235, 39.417545318603516, 41.9152045249939, 44.41286373138428, 46.88122224807739, 49.34958076477051, 51.77156352996826, 54.193546295166016, 56.73258876800537, 59.27163124084473, 61.773964166641235, 64.27629709243774, 66.8036675453186, 69.33103799819946, 71.8239688873291, 74.31689977645874, 76.85317182540894, 79.38944387435913, 81.88936257362366, 84.38928127288818, 87.00764036178589, 89.6259994506836, 92.13409614562988, 94.64219284057617, 97.1319375038147, 99.62168216705322, 102.18510365486145, 104.74852514266968, 107.30057907104492, 109.85263299942017, 112.4048421382904, 114.95705127716064, 117.50008487701416, 120.04311847686768, 122.48090863227844, 124.91869878768921, 127.43827939033508, 129.95785999298096, 132.3605124950409, 134.76316499710083, 137.07783484458923, 139.39250469207764, 141.74736309051514, 144.10222148895264, 146.50843477249146, 148.91464805603027, 151.34102082252502, 153.76739358901978, 156.2355878353119, 158.703782081604, 161.15045952796936, 163.59713697433472, 166.0043065547943, 168.4114761352539, 170.8765823841095, 173.3416886329651, 175.77494072914124, 178.20819282531738, 180.6450662612915, 183.08193969726562, 185.54080057144165, 187.99966144561768, 190.58035707473755, 193.16105270385742, 195.65967297554016, 198.1582932472229, 200.60452699661255, 203.0507607460022, 205.539466381073, 208.0281720161438, 210.60276699066162, 213.17736196517944, 215.7689654827118, 218.36056900024414, 220.8703281879425, 223.38008737564087, 225.8210644721985, 228.2620415687561, 230.5842523574829, 232.90646314620972, 235.36762738227844, 237.82879161834717, 240.2966001033783, 242.76440858840942, 245.23392343521118, 247.70343828201294, 250.20546913146973, 252.7074999809265, 255.27334880828857, 257.83919763565063, 260.43662881851196, 263.0340600013733, 265.55640602111816, 268.07875204086304, 270.6029326915741, 273.12711334228516, 275.6355061531067, 278.1438989639282, 280.57969665527344, 283.01549434661865, 285.42838311195374, 287.8412718772888, 290.40092372894287, 292.9605755805969, 295.5356512069702, 298.1107268333435, 300.71503734588623, 303.31934785842896, 305.80529713630676, 308.29124641418457, 310.76466965675354, 313.2380928993225, 315.6666476726532, 318.0952024459839, 320.6398193836212, 323.18443632125854, 325.82579040527344, 328.46714448928833, 330.9670648574829, 333.4669852256775, 336.07699823379517, 338.68701124191284, 341.1453981399536, 343.6037850379944, 345.9774215221405, 348.3510580062866, 350.7442362308502, 353.1374144554138, 355.52716517448425, 357.9169158935547, 360.33134961128235, 362.74578332901, 365.16643261909485, 367.5870819091797, 370.066038608551, 372.54499530792236, 375.03122448921204, 377.5174536705017, 380.134379863739, 382.7513060569763, 385.3547716140747, 387.9582371711731, 390.51389741897583, 393.06955766677856, 395.6278727054596, 398.1861877441406, 400.7172203063965, 403.24825286865234, 405.7401614189148, 408.23206996917725, 410.6542937755585, 413.0765175819397, 415.59506464004517, 418.11361169815063, 420.7318835258484, 423.35015535354614, 425.9409019947052, 428.53164863586426, 431.2121391296387, 433.8926296234131, 436.4449746608734, 438.99731969833374, 441.50254583358765, 444.00777196884155, 446.5060088634491, 449.00424575805664, 451.47894072532654, 453.95363569259644, 456.65110993385315, 459.34858417510986, 462.05414366722107, 464.7597031593323, 467.33701157569885, 469.91431999206543, 472.3901810646057, 474.866042137146, 477.3481664657593, 479.83029079437256, 482.3442006111145, 484.85811042785645, 487.40798330307007, 489.9578561782837, 492.5274512767792, 495.09704637527466, 497.6065936088562, 500.11614084243774, 502.1637644767761, 504.2113881111145]
[29.25, 29.25, 27.755555555555556, 27.755555555555556, 31.555555555555557, 31.555555555555557, 51.34444444444444, 51.34444444444444, 64.80555555555556, 64.80555555555556, 71.96111111111111, 71.96111111111111, 79.10555555555555, 79.10555555555555, 81.62222222222222, 81.62222222222222, 82.62222222222222, 82.62222222222222, 83.30555555555556, 83.30555555555556, 83.83333333333333, 83.83333333333333, 84.39444444444445, 84.39444444444445, 84.76666666666667, 84.76666666666667, 85.15555555555555, 85.15555555555555, 85.82222222222222, 85.82222222222222, 88.2611111111111, 88.2611111111111, 90.28888888888889, 90.28888888888889, 91.2388888888889, 91.2388888888889, 91.89444444444445, 91.89444444444445, 92.78888888888889, 92.78888888888889, 93.31111111111112, 93.31111111111112, 93.61666666666666, 93.61666666666666, 93.87222222222222, 93.87222222222222, 94.04444444444445, 94.04444444444445, 94.22777777777777, 94.22777777777777, 94.42222222222222, 94.42222222222222, 94.64444444444445, 94.64444444444445, 94.67222222222222, 94.67222222222222, 94.87222222222222, 94.87222222222222, 95.06111111111112, 95.06111111111112, 95.07777777777778, 95.07777777777778, 95.2388888888889, 95.2388888888889, 95.4, 95.4, 95.42222222222222, 95.42222222222222, 95.57777777777778, 95.57777777777778, 95.68888888888888, 95.68888888888888, 95.71111111111111, 95.71111111111111, 95.69444444444444, 95.69444444444444, 95.80555555555556, 95.80555555555556, 95.91666666666667, 95.91666666666667, 95.88333333333334, 95.88333333333334, 95.9888888888889, 95.9888888888889, 96.0111111111111, 96.0111111111111, 95.96666666666667, 95.96666666666667, 96.06666666666666, 96.06666666666666, 96.08333333333333, 96.08333333333333, 96.06666666666666, 96.06666666666666, 96.15555555555555, 96.15555555555555, 96.22222222222223, 96.22222222222223, 96.2611111111111, 96.2611111111111, 96.35, 96.35, 96.33888888888889, 96.33888888888889, 96.35555555555555, 96.35555555555555, 96.41666666666667, 96.41666666666667, 96.45555555555555, 96.45555555555555, 96.37777777777778, 96.37777777777778, 96.37777777777778, 96.37777777777778, 96.41111111111111, 96.41111111111111, 96.38888888888889, 96.38888888888889, 96.47222222222223, 96.47222222222223, 96.50555555555556, 96.50555555555556, 96.52222222222223, 96.52222222222223, 96.6, 96.6, 96.6, 96.6, 96.60555555555555, 96.60555555555555, 96.63333333333334, 96.63333333333334, 96.65, 96.65, 96.62777777777778, 96.62777777777778, 96.69444444444444, 96.69444444444444, 96.67777777777778, 96.67777777777778, 96.67222222222222, 96.67222222222222, 96.65, 96.65, 96.68888888888888, 96.68888888888888, 96.70555555555555, 96.70555555555555, 96.75, 96.75, 96.73333333333333, 96.73333333333333, 96.72222222222223, 96.72222222222223, 96.7, 96.7, 96.7388888888889, 96.7388888888889, 96.73333333333333, 96.73333333333333, 96.76666666666667, 96.76666666666667, 96.79444444444445, 96.79444444444445, 96.78888888888889, 96.78888888888889, 96.77222222222223, 96.77222222222223, 96.78333333333333, 96.78333333333333, 96.81666666666666, 96.81666666666666, 96.80555555555556, 96.80555555555556, 96.84444444444445, 96.84444444444445, 96.81111111111112, 96.81111111111112, 96.83888888888889, 96.83888888888889, 96.82777777777778, 96.82777777777778, 96.85555555555555, 96.85555555555555, 96.79444444444445, 96.79444444444445, 96.83888888888889, 96.83888888888889, 96.78888888888889, 96.78888888888889, 96.85, 96.85, 96.9, 96.9, 96.86111111111111, 96.86111111111111, 96.91666666666667, 96.91666666666667, 96.87777777777778, 96.87777777777778, 96.91111111111111, 96.91111111111111]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.481, Test loss: 1.513, Test accuracy: 94.93
Final Round, Global train loss: 1.481, Global test loss: 1.998, Global test accuracy: 45.25
Average accuracy final 10 rounds: 94.92583333333334 

Average global accuracy final 10 rounds: 44.38583333333333 

2195.50635433197
[1.4163155555725098, 2.8326311111450195, 4.130451202392578, 5.428271293640137, 6.685510873794556, 7.942750453948975, 9.198270320892334, 10.453790187835693, 11.774536609649658, 13.095283031463623, 14.397850275039673, 15.700417518615723, 17.00130534172058, 18.30219316482544, 19.577791213989258, 20.853389263153076, 22.205004930496216, 23.556620597839355, 24.865400314331055, 26.174180030822754, 27.491899490356445, 28.809618949890137, 30.10344362258911, 31.397268295288086, 32.68144989013672, 33.96563148498535, 35.28001141548157, 36.59439134597778, 37.923184633255005, 39.25197792053223, 40.535569190979004, 41.81916046142578, 43.1756751537323, 44.53218984603882, 45.85301947593689, 47.17384910583496, 48.461819887161255, 49.74979066848755, 51.07878541946411, 52.407780170440674, 53.71994423866272, 55.032108306884766, 56.38591146469116, 57.73971462249756, 59.04312014579773, 60.3465256690979, 61.59154510498047, 62.83656454086304, 64.17703294754028, 65.51750135421753, 66.86259317398071, 68.2076849937439, 69.49448299407959, 70.78128099441528, 72.12783718109131, 73.47439336776733, 74.79556965827942, 76.1167459487915, 77.38757133483887, 78.65839672088623, 79.99776196479797, 81.33712720870972, 82.64934754371643, 83.96156787872314, 85.26440095901489, 86.56723403930664, 87.8228075504303, 89.07838106155396, 90.37363243103027, 91.66888380050659, 93.04478693008423, 94.42069005966187, 95.73415493965149, 97.04761981964111, 98.2839868068695, 99.5203537940979, 100.86434245109558, 102.20833110809326, 103.51939415931702, 104.83045721054077, 106.11938905715942, 107.40832090377808, 108.70421004295349, 110.0000991821289, 111.34956502914429, 112.69903087615967, 114.01816129684448, 115.3372917175293, 116.67317700386047, 118.00906229019165, 119.28995871543884, 120.57085514068604, 121.89000391960144, 123.20915269851685, 124.38508319854736, 125.56101369857788, 126.72250580787659, 127.8839979171753, 129.15390968322754, 130.42382144927979, 131.5755295753479, 132.72723770141602, 133.9070954322815, 135.08695316314697, 136.2707872390747, 137.45462131500244, 138.59338092803955, 139.73214054107666, 140.93433952331543, 142.1365385055542, 143.34613728523254, 144.5557360649109, 145.76796674728394, 146.98019742965698, 148.16132283210754, 149.3424482345581, 150.48617601394653, 151.62990379333496, 152.87036561965942, 154.1108274459839, 155.29012846946716, 156.46942949295044, 157.65937638282776, 158.84932327270508, 160.08464694023132, 161.31997060775757, 162.53895950317383, 163.7579483985901, 164.9572730064392, 166.15659761428833, 167.32077836990356, 168.4849591255188, 169.72446036338806, 170.96396160125732, 172.22171211242676, 173.4794626235962, 174.65916538238525, 175.83886814117432, 176.99158430099487, 178.14430046081543, 179.3325958251953, 180.5208911895752, 181.65887308120728, 182.79685497283936, 184.00272130966187, 185.20858764648438, 186.4599301815033, 187.71127271652222, 188.91900610923767, 190.12673950195312, 191.3781235218048, 192.6295075416565, 193.88037538528442, 195.13124322891235, 196.31639742851257, 197.5015516281128, 198.7184956073761, 199.9354395866394, 201.14501214027405, 202.3545846939087, 203.56638312339783, 204.77818155288696, 205.96254110336304, 207.1469006538391, 208.40163588523865, 209.65637111663818, 210.90005707740784, 212.1437430381775, 213.34430074691772, 214.54485845565796, 215.78469729423523, 217.0245361328125, 218.28798842430115, 219.5514407157898, 220.7768669128418, 222.0022931098938, 223.189692735672, 224.3770923614502, 225.61147165298462, 226.84585094451904, 228.09550642967224, 229.34516191482544, 230.62552738189697, 231.9058928489685, 233.10688042640686, 234.30786800384521, 235.54923105239868, 236.79059410095215, 238.02104830741882, 239.2515025138855, 240.5108666419983, 241.77023077011108, 243.0178198814392, 244.26540899276733, 245.51100540161133, 246.75660181045532, 247.97763395309448, 249.19866609573364, 250.43973445892334, 251.68080282211304, 253.7581741809845, 255.83554553985596]
[52.78333333333333, 52.78333333333333, 67.74166666666666, 67.74166666666666, 70.06666666666666, 70.06666666666666, 78.025, 78.025, 81.68333333333334, 81.68333333333334, 88.225, 88.225, 91.025, 91.025, 93.05, 93.05, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.13333333333334, 93.13333333333334, 93.15, 93.15, 94.71666666666667, 94.71666666666667, 94.69166666666666, 94.69166666666666, 94.74166666666666, 94.74166666666666, 94.8, 94.8, 94.925, 94.925, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.89166666666667, 94.89166666666667, 94.93333333333334, 94.93333333333334, 94.89166666666667, 94.89166666666667, 94.88333333333334, 94.88333333333334, 94.86666666666666, 94.86666666666666, 94.86666666666666, 94.86666666666666, 94.85833333333333, 94.85833333333333, 94.85833333333333, 94.85833333333333, 94.86666666666666, 94.86666666666666, 94.875, 94.875, 94.86666666666666, 94.86666666666666, 94.9, 94.9, 94.9, 94.9, 94.9, 94.9, 94.88333333333334, 94.88333333333334, 94.89166666666667, 94.89166666666667, 94.875, 94.875, 94.89166666666667, 94.89166666666667, 94.89166666666667, 94.89166666666667, 94.89166666666667, 94.89166666666667, 94.9, 94.9, 94.91666666666667, 94.91666666666667, 94.90833333333333, 94.90833333333333, 94.91666666666667, 94.91666666666667, 94.90833333333333, 94.90833333333333, 94.9, 94.9, 94.90833333333333, 94.90833333333333, 94.93333333333334, 94.93333333333334, 94.94166666666666, 94.94166666666666, 94.95, 94.95, 94.93333333333334, 94.93333333333334, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.95833333333333, 94.95833333333333, 94.95, 94.95, 94.95, 94.95, 94.975, 94.975, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.98333333333333, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.96666666666667, 94.95833333333333, 94.95833333333333, 94.95, 94.95, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.94166666666666, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.925, 94.925, 94.925, 94.925, 94.89166666666667, 94.89166666666667, 94.90833333333333, 94.90833333333333, 94.925, 94.925, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.925, 94.925, 94.925, 94.925, 94.94166666666666, 94.94166666666666, 94.96666666666667, 94.96666666666667, 94.95833333333333, 94.95833333333333, 94.95, 94.95, 94.95, 94.95, 94.95, 94.95, 94.95, 94.95, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.925, 94.925, 94.93333333333334, 94.93333333333334]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1319, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.464, Test loss: 1.490, Test accuracy: 97.35
Final Round, Global train loss: 1.464, Global test loss: 2.010, Global test accuracy: 44.47
Average accuracy final 10 rounds: 97.34583333333333 

Average global accuracy final 10 rounds: 43.22333333333333 

2199.9576365947723
[1.3884599208831787, 2.7769198417663574, 4.0889222621917725, 5.4009246826171875, 6.693346261978149, 7.985767841339111, 9.363141298294067, 10.740514755249023, 12.091770648956299, 13.443026542663574, 14.835536003112793, 16.22804546356201, 17.581835985183716, 18.93562650680542, 20.304921865463257, 21.674217224121094, 23.05083441734314, 24.427451610565186, 25.820411443710327, 27.21337127685547, 28.499653816223145, 29.78593635559082, 31.161081075668335, 32.53622579574585, 33.90761733055115, 35.279008865356445, 36.731693983078, 38.18437910079956, 39.616353034973145, 41.04832696914673, 42.39643120765686, 43.74453544616699, 45.164597034454346, 46.5846586227417, 47.98398208618164, 49.38330554962158, 50.63105082511902, 51.878796100616455, 53.188878774642944, 54.498961448669434, 55.84618067741394, 57.19339990615845, 58.45376777648926, 59.71413564682007, 61.018715381622314, 62.32329511642456, 63.61513590812683, 64.9069766998291, 66.18848180770874, 67.46998691558838, 68.76607584953308, 70.06216478347778, 71.34005403518677, 72.61794328689575, 73.91506314277649, 75.21218299865723, 76.57706809043884, 77.94195318222046, 79.2773859500885, 80.61281871795654, 82.00180244445801, 83.39078617095947, 84.85545611381531, 86.32012605667114, 87.66681981086731, 89.01351356506348, 90.36115765571594, 91.70880174636841, 93.01370859146118, 94.31861543655396, 95.67708325386047, 97.03555107116699, 98.4659104347229, 99.89626979827881, 101.24690961837769, 102.59754943847656, 104.05487251281738, 105.5121955871582, 106.83976459503174, 108.16733360290527, 109.561683177948, 110.95603275299072, 112.31936740875244, 113.68270206451416, 114.99268102645874, 116.30265998840332, 117.64255237579346, 118.9824447631836, 120.33716487884521, 121.69188499450684, 123.06981205940247, 124.4477391242981, 125.81230640411377, 127.17687368392944, 128.53559184074402, 129.8943099975586, 131.2087345123291, 132.5231590270996, 133.72985696792603, 134.93655490875244, 136.1364290714264, 137.33630323410034, 138.62689113616943, 139.91747903823853, 141.2527358531952, 142.58799266815186, 143.73267316818237, 144.8773536682129, 146.09055972099304, 147.3037657737732, 148.56592893600464, 149.82809209823608, 150.96862506866455, 152.10915803909302, 153.29547142982483, 154.48178482055664, 155.7038609981537, 156.92593717575073, 158.14508366584778, 159.36423015594482, 160.62982511520386, 161.8954200744629, 163.2614347934723, 164.6274495124817, 165.98811292648315, 167.34877634048462, 168.644629240036, 169.9404821395874, 171.25951433181763, 172.57854652404785, 173.88307690620422, 175.1876072883606, 176.55149269104004, 177.91537809371948, 179.17926836013794, 180.4431586265564, 181.82902097702026, 183.21488332748413, 184.5710415840149, 185.92719984054565, 187.20655512809753, 188.4859104156494, 189.86238050460815, 191.2388505935669, 192.47211623191833, 193.70538187026978, 194.8923523426056, 196.0793228149414, 197.31988859176636, 198.5604543685913, 199.80416584014893, 201.04787731170654, 202.27810859680176, 203.50833988189697, 204.78871369361877, 206.06908750534058, 207.41801357269287, 208.76693964004517, 210.01624393463135, 211.26554822921753, 212.5543532371521, 213.84315824508667, 215.15649604797363, 216.4698338508606, 217.75411319732666, 219.03839254379272, 220.3023762702942, 221.56635999679565, 222.8186547756195, 224.07094955444336, 225.43539023399353, 226.7998309135437, 228.09377527236938, 229.38771963119507, 230.60428047180176, 231.82084131240845, 233.05928754806519, 234.29773378372192, 235.4750576019287, 236.6523814201355, 237.8239815235138, 238.9955816268921, 240.2316198348999, 241.46765804290771, 242.67439770698547, 243.88113737106323, 245.03353762626648, 246.18593788146973, 247.39287638664246, 248.59981489181519, 249.7912425994873, 250.98267030715942, 252.16932463645935, 253.35597896575928, 254.52526879310608, 255.69455862045288, 256.87246966362, 258.0503807067871, 259.3009543418884, 260.55152797698975, 262.61278533935547, 264.6740427017212]
[49.95, 49.95, 63.958333333333336, 63.958333333333336, 66.95833333333333, 66.95833333333333, 66.79166666666667, 66.79166666666667, 76.85, 76.85, 78.18333333333334, 78.18333333333334, 83.15, 83.15, 85.23333333333333, 85.23333333333333, 90.04166666666667, 90.04166666666667, 94.0, 94.0, 95.875, 95.875, 95.875, 95.875, 95.90833333333333, 95.90833333333333, 97.30833333333334, 97.30833333333334, 97.29166666666667, 97.29166666666667, 97.225, 97.225, 97.175, 97.175, 97.19166666666666, 97.19166666666666, 97.225, 97.225, 97.23333333333333, 97.23333333333333, 97.275, 97.275, 97.24166666666666, 97.24166666666666, 97.3, 97.3, 97.25, 97.25, 97.28333333333333, 97.28333333333333, 97.25833333333334, 97.25833333333334, 97.19166666666666, 97.19166666666666, 97.19166666666666, 97.19166666666666, 97.24166666666666, 97.24166666666666, 97.26666666666667, 97.26666666666667, 97.26666666666667, 97.26666666666667, 97.23333333333333, 97.23333333333333, 97.23333333333333, 97.23333333333333, 97.225, 97.225, 97.24166666666666, 97.24166666666666, 97.21666666666667, 97.21666666666667, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.23333333333333, 97.23333333333333, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.25, 97.25, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.225, 97.225, 97.24166666666666, 97.24166666666666, 97.25, 97.25, 97.25, 97.25, 97.25833333333334, 97.25833333333334, 97.25, 97.25, 97.24166666666666, 97.24166666666666, 97.23333333333333, 97.23333333333333, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.24166666666666, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.25, 97.275, 97.275, 97.28333333333333, 97.28333333333333, 97.26666666666667, 97.26666666666667, 97.275, 97.275, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.28333333333333, 97.3, 97.3, 97.3, 97.3, 97.3, 97.3, 97.3, 97.3, 97.30833333333334, 97.30833333333334, 97.31666666666666, 97.31666666666666, 97.31666666666666, 97.31666666666666, 97.31666666666666, 97.31666666666666, 97.325, 97.325, 97.30833333333334, 97.30833333333334, 97.30833333333334, 97.30833333333334, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.33333333333333, 97.34166666666667, 97.34166666666667, 97.35, 97.35, 97.35, 97.35, 97.35, 97.35, 97.34166666666667, 97.34166666666667, 97.35, 97.35, 97.35, 97.35, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.34166666666667, 97.35, 97.35]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.668, Test loss: 1.669, Test accuracy: 79.36
Final Round, Global train loss: 1.668, Global test loss: 2.283, Global test accuracy: 15.76
Average accuracy final 10 rounds: 80.2295 

Average global accuracy final 10 rounds: 15.412999999999998 

5751.540984869003
[4.325943470001221, 8.651886940002441, 12.551169872283936, 16.45045280456543, 20.366321563720703, 24.282190322875977, 28.274302005767822, 32.26641368865967, 36.54127788543701, 40.816142082214355, 45.01203989982605, 49.207937717437744, 53.424206256866455, 57.640474796295166, 61.85140252113342, 66.06233024597168, 70.33266425132751, 74.60299825668335, 78.94653415679932, 83.29007005691528, 87.51460266113281, 91.73913526535034, 95.89140605926514, 100.04367685317993, 104.00777339935303, 107.97186994552612, 111.9566125869751, 115.94135522842407, 119.95454216003418, 123.96772909164429, 127.89823698997498, 131.82874488830566, 135.83571195602417, 139.84267902374268, 143.76684641838074, 147.6910138130188, 151.54434251785278, 155.39767122268677, 159.48122906684875, 163.56478691101074, 167.5630567073822, 171.56132650375366, 175.40736961364746, 179.25341272354126, 183.00565028190613, 186.757887840271, 190.58542680740356, 194.41296577453613, 197.7607753276825, 201.10858488082886, 204.26830768585205, 207.42803049087524, 210.5732455253601, 213.71846055984497, 216.8501536846161, 219.9818468093872, 223.04083251953125, 226.0998182296753, 229.30364322662354, 232.50746822357178, 235.73787641525269, 238.9682846069336, 242.24536395072937, 245.52244329452515, 248.6977186203003, 251.87299394607544, 255.0393795967102, 258.20576524734497, 261.36032366752625, 264.5148820877075, 267.73276138305664, 270.95064067840576, 274.151567697525, 277.3524947166443, 280.55050253868103, 283.7485103607178, 286.95737862586975, 290.16624689102173, 293.4185461997986, 296.67084550857544, 299.9172487258911, 303.1636519432068, 306.45873832702637, 309.75382471084595, 312.99484038352966, 316.2358560562134, 319.42963337898254, 322.6234107017517, 325.80386304855347, 328.9843153953552, 332.1942927837372, 335.40427017211914, 338.62198138237, 341.83969259262085, 345.228303194046, 348.6169137954712, 351.9929826259613, 355.3690514564514, 358.6755199432373, 361.9819884300232, 365.23370909690857, 368.48542976379395, 371.5339334011078, 374.58243703842163, 377.57613730430603, 380.56983757019043, 383.62025475502014, 386.67067193984985, 389.8126676082611, 392.95466327667236, 396.1549127101898, 399.3551621437073, 402.59034037590027, 405.82551860809326, 409.06570744514465, 412.30589628219604, 415.5386996269226, 418.77150297164917, 421.90226888656616, 425.03303480148315, 428.163587808609, 431.29414081573486, 434.51302766799927, 437.7319145202637, 440.9640510082245, 444.1961874961853, 447.5068693161011, 450.81755113601685, 454.12070178985596, 457.42385244369507, 460.66178131103516, 463.89971017837524, 467.0959324836731, 470.29215478897095, 473.4766426086426, 476.6611304283142, 479.8804626464844, 483.09979486465454, 486.3895072937012, 489.6792197227478, 492.9615070819855, 496.24379444122314, 499.5359802246094, 502.8281660079956, 506.08726620674133, 509.34636640548706, 512.5455191135406, 515.7446718215942, 518.9245927333832, 522.1045136451721, 525.252345085144, 528.400176525116, 531.6202630996704, 534.8403496742249, 538.1156270503998, 541.3909044265747, 544.642499923706, 547.8940954208374, 551.0370333194733, 554.1799712181091, 557.3529527187347, 560.5259342193604, 563.6784679889679, 566.8310017585754, 569.9506866931915, 573.0703716278076, 576.2296571731567, 579.3889427185059, 582.4917466640472, 585.5945506095886, 588.7353956699371, 591.8762407302856, 595.0747210979462, 598.2732014656067, 601.4458529949188, 604.618504524231, 607.7506711483002, 610.8828377723694, 614.1343333721161, 617.3858289718628, 620.5879294872284, 623.790030002594, 626.9323153495789, 630.0746006965637, 633.2221467494965, 636.3696928024292, 639.8986256122589, 643.4275584220886, 647.1671061515808, 650.906653881073, 654.6493368148804, 658.3920197486877, 662.0680687427521, 665.7441177368164, 669.4044554233551, 673.0647931098938, 676.6823437213898, 680.2998943328857, 683.9380226135254, 687.576150894165, 689.4192545413971, 691.2623581886292]
[19.2675, 19.2675, 28.755, 28.755, 36.7075, 36.7075, 39.9125, 39.9125, 40.3675, 40.3675, 44.7375, 44.7375, 45.5425, 45.5425, 46.2775, 46.2775, 45.755, 45.755, 48.7325, 48.7325, 49.715, 49.715, 49.3775, 49.3775, 49.535, 49.535, 50.16, 50.16, 51.6225, 51.6225, 52.4675, 52.4675, 51.7875, 51.7875, 53.485, 53.485, 54.97, 54.97, 56.7075, 56.7075, 58.255, 58.255, 57.9225, 57.9225, 58.425, 58.425, 58.84, 58.84, 58.675, 58.675, 59.4075, 59.4075, 59.4475, 59.4475, 61.35, 61.35, 61.48, 61.48, 62.1, 62.1, 64.9175, 64.9175, 65.68, 65.68, 65.0875, 65.0875, 64.385, 64.385, 63.83, 63.83, 64.6225, 64.6225, 65.175, 65.175, 66.7025, 66.7025, 66.9575, 66.9575, 68.85, 68.85, 69.8475, 69.8475, 71.5575, 71.5575, 71.965, 71.965, 73.25, 73.25, 73.3875, 73.3875, 72.4025, 72.4025, 73.58, 73.58, 73.91, 73.91, 75.46, 75.46, 74.83, 74.83, 74.415, 74.415, 74.5225, 74.5225, 74.565, 74.565, 75.1525, 75.1525, 75.5325, 75.5325, 75.98, 75.98, 75.65, 75.65, 75.1175, 75.1175, 75.7, 75.7, 75.8, 75.8, 75.86, 75.86, 76.7975, 76.7975, 76.835, 76.835, 76.805, 76.805, 76.1875, 76.1875, 76.465, 76.465, 76.4525, 76.4525, 76.5475, 76.5475, 76.555, 76.555, 77.82, 77.82, 77.7825, 77.7825, 77.8, 77.8, 78.2575, 78.2575, 78.4075, 78.4075, 79.08, 79.08, 79.1075, 79.1075, 79.53, 79.53, 79.1775, 79.1775, 78.76, 78.76, 78.4275, 78.4275, 77.9875, 77.9875, 77.9775, 77.9775, 78.4725, 78.4725, 78.465, 78.465, 79.315, 79.315, 79.2875, 79.2875, 79.2175, 79.2175, 79.26, 79.26, 78.47, 78.47, 80.045, 80.045, 79.9575, 79.9575, 79.3925, 79.3925, 80.02, 80.02, 80.3075, 80.3075, 80.4125, 80.4125, 80.56, 80.56, 80.5675, 80.5675, 80.19, 80.19, 80.265, 80.265, 80.6225, 80.6225, 79.365, 79.365]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.761, Test loss: 1.778, Test accuracy: 67.82
Average accuracy final 10 rounds: 66.39725000000001 

3994.2962691783905
[3.318514108657837, 6.637028217315674, 9.663052797317505, 12.689077377319336, 15.798660039901733, 18.90824270248413, 22.008241176605225, 25.10823965072632, 28.191717386245728, 31.275195121765137, 34.314754009246826, 37.354312896728516, 40.26953339576721, 43.18475389480591, 46.18015384674072, 49.17555379867554, 52.22743010520935, 55.279306411743164, 58.61990547180176, 61.96050453186035, 65.1941978931427, 68.42789125442505, 71.47357559204102, 74.51925992965698, 77.55911612510681, 80.59897232055664, 83.59311437606812, 86.58725643157959, 89.59077286720276, 92.59428930282593, 95.5416612625122, 98.48903322219849, 101.4877679347992, 104.4865026473999, 107.55968141555786, 110.63286018371582, 114.2389588356018, 117.8450574874878, 120.82882142066956, 123.81258535385132, 126.71750116348267, 129.622416973114, 132.49660658836365, 135.37079620361328, 138.28760409355164, 141.20441198349, 144.38529205322266, 147.56617212295532, 150.65087366104126, 153.7355751991272, 156.6065592765808, 159.47754335403442, 162.45642852783203, 165.43531370162964, 168.42227244377136, 171.4092311859131, 174.5556333065033, 177.7020354270935, 181.2630033493042, 184.8239712715149, 188.0283875465393, 191.23280382156372, 194.51167058944702, 197.79053735733032, 201.14947295188904, 204.50840854644775, 207.7908535003662, 211.07329845428467, 214.28090572357178, 217.4885129928589, 220.63938808441162, 223.79026317596436, 226.98290157318115, 230.17553997039795, 233.28247165679932, 236.38940334320068, 239.71844482421875, 243.04748630523682, 246.35230684280396, 249.6571273803711, 252.8157660961151, 255.97440481185913, 259.2849032878876, 262.595401763916, 265.95589780807495, 269.3163938522339, 272.6093370914459, 275.90228033065796, 279.19177532196045, 282.48127031326294, 285.8185839653015, 289.1558976173401, 292.30438590049744, 295.4528741836548, 298.74546933174133, 302.0380644798279, 305.4260094165802, 308.8139543533325, 312.1553544998169, 315.49675464630127, 318.7481174468994, 321.99948024749756, 325.2566452026367, 328.5138101577759, 331.4389271736145, 334.3640441894531, 337.23448157310486, 340.1049189567566, 343.03054666519165, 345.9561743736267, 348.80131363868713, 351.64645290374756, 354.55110216140747, 357.4557514190674, 360.52016830444336, 363.58458518981934, 366.61951088905334, 369.65443658828735, 372.6238567829132, 375.59327697753906, 378.3871486186981, 381.1810202598572, 384.0189390182495, 386.85685777664185, 389.80146169662476, 392.74606561660767, 395.80548644065857, 398.8649072647095, 401.9217731952667, 404.978639125824, 407.79839754104614, 410.6181559562683, 413.5554623603821, 416.49276876449585, 419.42992067337036, 422.3670725822449, 425.20598697662354, 428.0449013710022, 430.99422121047974, 433.9435410499573, 436.80340552330017, 439.66326999664307, 442.5863130092621, 445.5093560218811, 448.51212191581726, 451.5148878097534, 454.48028898239136, 457.4456901550293, 460.2746512889862, 463.1036124229431, 466.0425953865051, 468.98157835006714, 471.91040205955505, 474.83922576904297, 477.7990794181824, 480.7589330673218, 483.7332820892334, 486.707631111145, 489.57261753082275, 492.4376039505005, 495.3140275478363, 498.1904511451721, 501.12755012512207, 504.064649105072, 506.97891783714294, 509.89318656921387, 512.858922958374, 515.8246593475342, 518.8107650279999, 521.7968707084656, 524.7653288841248, 527.7337870597839, 530.6385097503662, 533.5432324409485, 536.412017583847, 539.2808027267456, 542.1185584068298, 544.9563140869141, 547.9502220153809, 550.9441299438477, 553.9633901119232, 556.9826502799988, 559.8686184883118, 562.7545866966248, 565.573606967926, 568.3926272392273, 571.4020953178406, 574.4115633964539, 577.2891302108765, 580.1666970252991, 582.9732577800751, 585.7798185348511, 588.7417242527008, 591.7036299705505, 594.6861839294434, 597.6687378883362, 600.6180906295776, 603.5674433708191, 606.5912053585052, 609.6149673461914, 611.0922358036041, 612.5695042610168]
[11.525, 11.525, 11.4225, 11.4225, 12.8525, 12.8525, 16.26, 16.26, 23.2175, 23.2175, 28.68, 28.68, 31.735, 31.735, 35.1, 35.1, 38.9925, 38.9925, 42.1675, 42.1675, 43.6525, 43.6525, 44.23, 44.23, 46.87, 46.87, 51.355, 51.355, 51.5275, 51.5275, 53.09, 53.09, 54.1, 54.1, 54.1525, 54.1525, 54.435, 54.435, 55.3875, 55.3875, 55.965, 55.965, 56.035, 56.035, 56.1425, 56.1425, 56.285, 56.285, 56.315, 56.315, 56.3375, 56.3375, 56.45, 56.45, 56.69, 56.69, 57.39, 57.39, 57.5125, 57.5125, 57.4025, 57.4025, 57.5125, 57.5125, 58.975, 58.975, 59.0025, 59.0025, 59.6475, 59.6475, 59.6675, 59.6675, 59.6725, 59.6725, 60.665, 60.665, 61.105, 61.105, 61.125, 61.125, 61.1725, 61.1725, 61.2625, 61.2625, 61.33, 61.33, 61.3175, 61.3175, 61.355, 61.355, 61.425, 61.425, 61.5225, 61.5225, 61.525, 61.525, 61.92, 61.92, 61.9375, 61.9375, 61.9425, 61.9425, 61.9875, 61.9875, 62.04, 62.04, 62.0425, 62.0425, 61.9975, 61.9975, 62.1475, 62.1475, 62.5525, 62.5525, 62.6725, 62.6725, 62.67, 62.67, 62.67, 62.67, 62.6975, 62.6975, 62.7225, 62.7225, 62.735, 62.735, 63.285, 63.285, 63.31, 63.31, 63.3375, 63.3375, 63.3775, 63.3775, 63.395, 63.395, 64.375, 64.375, 64.385, 64.385, 64.375, 64.375, 64.41, 64.41, 64.4125, 64.4125, 64.43, 64.43, 64.79, 64.79, 64.9025, 64.9025, 64.89, 64.89, 64.8575, 64.8575, 64.935, 64.935, 64.9625, 64.9625, 65.0175, 65.0175, 65.0075, 65.0075, 64.8825, 64.8825, 65.345, 65.345, 65.4, 65.4, 65.4375, 65.4375, 65.745, 65.745, 65.7925, 65.7925, 65.845, 65.845, 65.8425, 65.8425, 65.855, 65.855, 65.8725, 65.8725, 65.8475, 65.8475, 65.9075, 65.9075, 66.42, 66.42, 66.4375, 66.4375, 66.405, 66.405, 66.4425, 66.4425, 67.4625, 67.4625, 67.3225, 67.3225, 67.8225, 67.8225]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.624, Test loss: 1.647, Test accuracy: 81.09
Average accuracy final 10 rounds: 81.02825 

4007.789900779724
[3.435344934463501, 6.870689868927002, 9.898615598678589, 12.926541328430176, 15.90410327911377, 18.881665229797363, 21.9122633934021, 24.942861557006836, 28.018940925598145, 31.095020294189453, 34.07689023017883, 37.05876016616821, 40.09301233291626, 43.12726449966431, 46.32881164550781, 49.53035879135132, 52.68192005157471, 55.833481311798096, 59.02365255355835, 62.2138237953186, 65.41389513015747, 68.61396646499634, 71.78846216201782, 74.9629578590393, 77.90071177482605, 80.8384656906128, 83.92566585540771, 87.01286602020264, 90.18381953239441, 93.35477304458618, 96.42401695251465, 99.49326086044312, 102.44756889343262, 105.40187692642212, 108.5288438796997, 111.6558108329773, 114.73372030258179, 117.81162977218628, 120.8468725681305, 123.8821153640747, 126.90845990180969, 129.93480443954468, 132.8922758102417, 135.84974718093872, 139.0838794708252, 142.31801176071167, 145.48594903945923, 148.6538863182068, 151.69432401657104, 154.7347617149353, 157.89578938484192, 161.05681705474854, 164.20397353172302, 167.3511300086975, 170.45166444778442, 173.55219888687134, 176.68293380737305, 179.81366872787476, 183.01724243164062, 186.2208161354065, 189.33877420425415, 192.4567322731018, 195.38567876815796, 198.3146252632141, 201.51039719581604, 204.70616912841797, 207.901034116745, 211.09589910507202, 214.22402262687683, 217.35214614868164, 220.30701303482056, 223.26187992095947, 226.38636994361877, 229.51085996627808, 232.6461534500122, 235.78144693374634, 238.96098113059998, 242.1405153274536, 245.09070324897766, 248.0408911705017, 251.04572248458862, 254.05055379867554, 257.1959328651428, 260.3413119316101, 263.4620432853699, 266.58277463912964, 269.57241106033325, 272.56204748153687, 275.7000734806061, 278.8380994796753, 281.99154925346375, 285.1449990272522, 288.24243330955505, 291.3398675918579, 294.53810000419617, 297.7363324165344, 300.9342873096466, 304.1322422027588, 307.0544955730438, 309.97674894332886, 313.07867527008057, 316.1806015968323, 319.30348563194275, 322.4263696670532, 325.5521996021271, 328.6780295372009, 331.9048240184784, 335.13161849975586, 338.1860797405243, 341.2405409812927, 344.2042872905731, 347.1680335998535, 350.387743473053, 353.60745334625244, 356.82742047309875, 360.04738759994507, 363.1226074695587, 366.19782733917236, 369.16195797920227, 372.1260886192322, 375.28195905685425, 378.4378294944763, 381.52318835258484, 384.60854721069336, 387.6825022697449, 390.7564573287964, 393.75759768486023, 396.7587380409241, 399.87285923957825, 402.9869804382324, 406.00290966033936, 409.0188388824463, 412.13432240486145, 415.2498059272766, 418.35684084892273, 421.46387577056885, 424.5926842689514, 427.721492767334, 430.6633048057556, 433.60511684417725, 436.6854181289673, 439.7657194137573, 442.911908864975, 446.0580983161926, 449.14724922180176, 452.2364001274109, 455.2886130809784, 458.3408260345459, 461.3928828239441, 464.4449396133423, 467.62372517585754, 470.8025107383728, 473.9237496852875, 477.04498863220215, 480.2431375980377, 483.4412865638733, 486.46623158454895, 489.4911766052246, 492.58154368400574, 495.67191076278687, 500.0619616508484, 504.4520125389099, 507.53257417678833, 510.61313581466675, 513.8888227939606, 517.1645097732544, 520.2121207714081, 523.2597317695618, 526.3006591796875, 529.3415865898132, 532.3794984817505, 535.4174103736877, 538.4778053760529, 541.538200378418, 544.7149364948273, 547.8916726112366, 550.9618465900421, 554.0320205688477, 557.1130964756012, 560.1941723823547, 563.3241205215454, 566.4540686607361, 569.6940767765045, 572.934084892273, 576.0450041294098, 579.1559233665466, 582.0764451026917, 584.9969668388367, 588.1103222370148, 591.2236776351929, 594.3718659877777, 597.5200543403625, 600.6282529830933, 603.736451625824, 606.6891918182373, 609.6419320106506, 612.6639366149902, 615.6859412193298, 618.8270938396454, 621.9682464599609, 623.4462654590607, 624.9242844581604]
[9.295, 9.295, 15.3725, 15.3725, 22.15, 22.15, 25.8125, 25.8125, 31.39, 31.39, 35.2625, 35.2625, 37.3575, 37.3575, 42.215, 42.215, 43.805, 43.805, 48.385, 48.385, 50.585, 50.585, 52.265, 52.265, 54.435, 54.435, 57.4325, 57.4325, 59.6975, 59.6975, 61.615, 61.615, 62.9825, 62.9825, 64.1975, 64.1975, 64.755, 64.755, 65.305, 65.305, 64.9375, 64.9375, 65.605, 65.605, 66.6875, 66.6875, 66.795, 66.795, 67.185, 67.185, 67.4575, 67.4575, 68.0975, 68.0975, 68.08, 68.08, 68.205, 68.205, 69.7975, 69.7975, 70.4175, 70.4175, 70.6775, 70.6775, 71.7075, 71.7075, 71.7275, 71.7275, 72.155, 72.155, 72.355, 72.355, 73.17, 73.17, 73.3325, 73.3325, 73.2875, 73.2875, 74.0475, 74.0475, 74.1525, 74.1525, 74.2475, 74.2475, 74.2475, 74.2475, 74.29, 74.29, 74.54, 74.54, 74.5675, 74.5675, 74.6175, 74.6175, 74.73, 74.73, 75.0125, 75.0125, 75.1975, 75.1975, 75.245, 75.245, 75.5825, 75.5825, 75.72, 75.72, 75.7725, 75.7725, 75.8, 75.8, 75.9675, 75.9675, 76.0275, 76.0275, 76.215, 76.215, 76.2875, 76.2875, 76.285, 76.285, 76.8175, 76.8175, 76.785, 76.785, 76.78, 76.78, 76.8, 76.8, 77.0425, 77.0425, 77.4925, 77.4925, 77.595, 77.595, 77.58, 77.58, 77.7, 77.7, 78.2925, 78.2925, 78.825, 78.825, 78.8825, 78.8825, 78.85, 78.85, 78.91, 78.91, 78.875, 78.875, 79.85, 79.85, 79.8875, 79.8875, 79.7975, 79.7975, 79.935, 79.935, 79.9725, 79.9725, 79.9775, 79.9775, 79.9675, 79.9675, 80.0, 80.0, 80.0075, 80.0075, 79.935, 79.935, 80.3625, 80.3625, 80.9775, 80.9775, 80.915, 80.915, 80.9475, 80.9475, 80.95, 80.95, 81.0175, 81.0175, 80.93, 80.93, 80.965, 80.965, 81.025, 81.025, 81.005, 81.005, 81.0025, 81.0025, 81.135, 81.135, 81.0625, 81.0625, 81.04, 81.04, 81.1, 81.1, 81.0925, 81.0925]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1319, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.235, Test loss: 1.077, Test accuracy: 74.06
Final Round, Global train loss: 0.235, Global test loss: 0.764, Global test accuracy: 77.39
Average accuracy final 10 rounds: 73.194 

Average global accuracy final 10 rounds: 77.8285 

6207.9219744205475
[5.092774391174316, 10.185548782348633, 14.471034049987793, 18.756519317626953, 23.04603123664856, 27.335543155670166, 31.640562295913696, 35.94558143615723, 40.2458655834198, 44.54614973068237, 48.8854079246521, 53.224666118621826, 57.54579401016235, 61.86692190170288, 66.20478415489197, 70.54264640808105, 74.89909672737122, 79.25554704666138, 83.73908138275146, 88.22261571884155, 92.59877896308899, 96.97494220733643, 101.3374388217926, 105.69993543624878, 110.07094502449036, 114.44195461273193, 118.80379962921143, 123.16564464569092, 127.56043744087219, 131.95523023605347, 136.30400133132935, 140.65277242660522, 145.00032663345337, 149.3478808403015, 153.6647973060608, 157.98171377182007, 162.35556268692017, 166.72941160202026, 171.14146757125854, 175.55352354049683, 179.8664093017578, 184.1792950630188, 188.47931909561157, 192.77934312820435, 197.16330409049988, 201.5472650527954, 205.88570022583008, 210.22413539886475, 214.5717794895172, 218.91942358016968, 223.31032705307007, 227.70123052597046, 232.08993554115295, 236.47864055633545, 240.8616645336151, 245.24468851089478, 249.60449528694153, 253.96430206298828, 258.33654975891113, 262.708797454834, 267.0688569545746, 271.4289164543152, 275.78039479255676, 280.13187313079834, 284.61456871032715, 289.09726428985596, 293.51324462890625, 297.92922496795654, 302.35204458236694, 306.77486419677734, 311.10512495040894, 315.4353857040405, 319.7801034450531, 324.1248211860657, 328.46306252479553, 332.8013038635254, 337.19399213790894, 341.5866804122925, 345.9655055999756, 350.3443307876587, 354.72629737854004, 359.1082639694214, 363.4786305427551, 367.84899711608887, 372.2111027240753, 376.57320833206177, 380.9420602321625, 385.3109121322632, 389.68439745903015, 394.0578827857971, 398.43018412590027, 402.8024854660034, 407.15296959877014, 411.50345373153687, 415.8502368927002, 420.1970200538635, 424.5562150478363, 428.9154100418091, 433.31843423843384, 437.7214584350586, 442.1062617301941, 446.4910650253296, 450.9297904968262, 455.36851596832275, 459.77091789245605, 464.17331981658936, 468.5832209587097, 472.9931221008301, 477.38674688339233, 481.7803716659546, 486.4776647090912, 491.1749577522278, 495.63687205314636, 500.09878635406494, 504.5350856781006, 508.97138500213623, 513.4852893352509, 517.9991936683655, 522.4047455787659, 526.8102974891663, 531.2240009307861, 535.637704372406, 540.0640394687653, 544.4903745651245, 548.9467046260834, 553.4030346870422, 557.8277785778046, 562.2525224685669, 566.7074847221375, 571.162446975708, 575.5810897350311, 579.9997324943542, 584.4010899066925, 588.8024473190308, 593.2273619174957, 597.6522765159607, 602.0943191051483, 606.5363616943359, 610.9919447898865, 615.447527885437, 619.8911755084991, 624.3348231315613, 628.7455604076385, 633.1562976837158, 637.5979616641998, 642.0396256446838, 646.4457116127014, 650.851797580719, 655.2882258892059, 659.7246541976929, 664.1481857299805, 668.5717172622681, 673.0238318443298, 677.4759464263916, 681.8976261615753, 686.319305896759, 690.7710282802582, 695.2227506637573, 699.672905921936, 704.1230611801147, 708.5079233646393, 712.8927855491638, 717.3337967395782, 721.7748079299927, 726.1903758049011, 730.6059436798096, 735.0410459041595, 739.4761481285095, 743.8951058387756, 748.3140635490417, 752.7688472270966, 757.2236309051514, 762.3092355728149, 767.3948402404785, 772.4408705234528, 777.486900806427, 782.5067183971405, 787.526535987854, 792.5330078601837, 797.5394797325134, 802.5537104606628, 807.5679411888123, 812.5957307815552, 817.6235203742981, 822.6425039768219, 827.6614875793457, 832.6850650310516, 837.7086424827576, 842.7036061286926, 847.6985697746277, 852.1187660694122, 856.5389623641968, 860.9643883705139, 865.389814376831, 869.7909376621246, 874.1920609474182, 878.5949635505676, 882.997866153717, 887.4119558334351, 891.8260455131531, 894.0384225845337, 896.2507996559143]
[38.2125, 38.2125, 43.0975, 43.0975, 44.5875, 44.5875, 46.37, 46.37, 49.9575, 49.9575, 53.03, 53.03, 54.9425, 54.9425, 56.04, 56.04, 57.61, 57.61, 58.1075, 58.1075, 58.7025, 58.7025, 60.1725, 60.1725, 61.52, 61.52, 62.325, 62.325, 62.6975, 62.6975, 63.625, 63.625, 64.2375, 64.2375, 64.8425, 64.8425, 65.7125, 65.7125, 66.415, 66.415, 66.6625, 66.6625, 67.6025, 67.6025, 67.695, 67.695, 67.9875, 67.9875, 68.1975, 68.1975, 68.03, 68.03, 68.4825, 68.4825, 68.6975, 68.6975, 69.095, 69.095, 69.1875, 69.1875, 69.5325, 69.5325, 69.735, 69.735, 70.1125, 70.1125, 70.1975, 70.1975, 70.3875, 70.3875, 70.5725, 70.5725, 70.7325, 70.7325, 70.4325, 70.4325, 70.655, 70.655, 70.73, 70.73, 70.9475, 70.9475, 71.02, 71.02, 71.1475, 71.1475, 71.38, 71.38, 71.075, 71.075, 71.275, 71.275, 71.6, 71.6, 71.4175, 71.4175, 71.5125, 71.5125, 71.6675, 71.6675, 71.845, 71.845, 71.9325, 71.9325, 71.74, 71.74, 71.805, 71.805, 71.765, 71.765, 71.6625, 71.6625, 71.8225, 71.8225, 71.8825, 71.8825, 72.0475, 72.0475, 72.2025, 72.2025, 72.4475, 72.4475, 72.6175, 72.6175, 72.855, 72.855, 72.3425, 72.3425, 72.5575, 72.5575, 72.5275, 72.5275, 72.7725, 72.7725, 72.7625, 72.7625, 72.82, 72.82, 72.6475, 72.6475, 72.48, 72.48, 72.4425, 72.4425, 72.715, 72.715, 72.6175, 72.6175, 72.95, 72.95, 73.165, 73.165, 73.215, 73.215, 72.755, 72.755, 72.705, 72.705, 72.7725, 72.7725, 73.1325, 73.1325, 73.3225, 73.3225, 73.3, 73.3, 73.1225, 73.1225, 73.3175, 73.3175, 73.335, 73.335, 73.0925, 73.0925, 73.05, 73.05, 73.3, 73.3, 73.2025, 73.2025, 73.055, 73.055, 72.9875, 72.9875, 73.0925, 73.0925, 73.0275, 73.0275, 73.355, 73.355, 73.3825, 73.3825, 73.3825, 73.3825, 73.2025, 73.2025, 73.2525, 73.2525, 73.2025, 73.2025, 74.055, 74.055]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.193, Test loss: 0.375, Test accuracy: 86.83
Average accuracy final 10 rounds: 86.42666666666665 

1504.6943144798279
[1.737337350845337, 3.474674701690674, 4.916710138320923, 6.358745574951172, 7.762982606887817, 9.167219638824463, 10.594605684280396, 12.021991729736328, 13.447002649307251, 14.872013568878174, 16.29418706893921, 17.716360569000244, 19.127671241760254, 20.538981914520264, 21.94723105430603, 23.355480194091797, 24.599162578582764, 25.84284496307373, 27.08695888519287, 28.33107280731201, 29.573379278182983, 30.815685749053955, 32.0736129283905, 33.33154010772705, 34.5840528011322, 35.83656549453735, 37.0858736038208, 38.33518171310425, 39.556803703308105, 40.77842569351196, 42.01847052574158, 43.25851535797119, 44.50067400932312, 45.74283266067505, 46.989699363708496, 48.23656606674194, 49.48937463760376, 50.742183208465576, 51.98782396316528, 53.23346471786499, 54.479485750198364, 55.72550678253174, 56.97154974937439, 58.21759271621704, 59.45647883415222, 60.6953649520874, 61.92891216278076, 63.16245937347412, 64.39740443229675, 65.63234949111938, 66.85938262939453, 68.08641576766968, 69.3155426979065, 70.54466962814331, 71.78046417236328, 73.01625871658325, 74.25003004074097, 75.48380136489868, 76.71333765983582, 77.94287395477295, 79.17726349830627, 80.4116530418396, 81.63424968719482, 82.85684633255005, 84.0885899066925, 85.32033348083496, 86.54737281799316, 87.77441215515137, 89.01331281661987, 90.25221347808838, 91.49339962005615, 92.73458576202393, 93.96621918678284, 95.19785261154175, 96.43627071380615, 97.67468881607056, 98.91165351867676, 100.14861822128296, 101.397873878479, 102.64712953567505, 103.89466214179993, 105.1421947479248, 106.38889670372009, 107.63559865951538, 108.87246417999268, 110.10932970046997, 111.35717296600342, 112.60501623153687, 113.84074020385742, 115.07646417617798, 116.32362699508667, 117.57078981399536, 118.80766677856445, 120.04454374313354, 121.28232026100159, 122.52009677886963, 123.75994086265564, 124.99978494644165, 126.24387669563293, 127.48796844482422, 128.83283948898315, 130.1777105331421, 131.5290687084198, 132.8804268836975, 134.2231321334839, 135.56583738327026, 136.91241765022278, 138.2589979171753, 139.60422325134277, 140.94944858551025, 142.18418288230896, 143.41891717910767, 144.6470754146576, 145.87523365020752, 147.11993288993835, 148.3646321296692, 149.60304856300354, 150.8414649963379, 152.0826427936554, 153.3238205909729, 154.55951976776123, 155.79521894454956, 157.02280354499817, 158.25038814544678, 159.47300553321838, 160.69562292099, 161.9217460155487, 163.14786911010742, 164.37471556663513, 165.60156202316284, 166.83866262435913, 168.07576322555542, 169.30774307250977, 170.5397229194641, 171.7761857509613, 173.0126485824585, 174.23774933815002, 175.46285009384155, 176.7019805908203, 177.94111108779907, 179.18596959114075, 180.43082809448242, 181.66904377937317, 182.90725946426392, 184.13712406158447, 185.36698865890503, 186.60354018211365, 187.84009170532227, 189.06688499450684, 190.2936782836914, 191.52474522590637, 192.75581216812134, 193.98974990844727, 195.2236876487732, 196.4595034122467, 197.69531917572021, 198.92753076553345, 200.15974235534668, 201.39145636558533, 202.62317037582397, 203.84701895713806, 205.07086753845215, 206.30899834632874, 207.54712915420532, 208.78137016296387, 210.0156111717224, 211.24910044670105, 212.4825897216797, 213.71916127204895, 214.9557328224182, 216.18939447402954, 217.42305612564087, 218.65177655220032, 219.88049697875977, 221.11184430122375, 222.34319162368774, 223.6866180896759, 225.03004455566406, 226.3755750656128, 227.72110557556152, 229.06923985481262, 230.41737413406372, 231.76329708099365, 233.10922002792358, 234.44121766090393, 235.77321529388428, 237.11329555511475, 238.45337581634521, 239.79024243354797, 241.12710905075073, 242.46522569656372, 243.8033423423767, 245.14547538757324, 246.48760843276978, 247.83540225028992, 249.18319606781006, 250.52351903915405, 251.86384201049805, 253.22523975372314, 254.58663749694824, 256.6291494369507, 258.6716613769531]
[25.358333333333334, 25.358333333333334, 39.9, 39.9, 44.291666666666664, 44.291666666666664, 49.85, 49.85, 54.78333333333333, 54.78333333333333, 59.69166666666667, 59.69166666666667, 61.358333333333334, 61.358333333333334, 59.81666666666667, 59.81666666666667, 61.975, 61.975, 66.56666666666666, 66.56666666666666, 70.80833333333334, 70.80833333333334, 73.59166666666667, 73.59166666666667, 74.14166666666667, 74.14166666666667, 74.725, 74.725, 77.70833333333333, 77.70833333333333, 78.55, 78.55, 78.49166666666666, 78.49166666666666, 79.26666666666667, 79.26666666666667, 78.9, 78.9, 79.64166666666667, 79.64166666666667, 80.325, 80.325, 80.36666666666666, 80.36666666666666, 80.175, 80.175, 81.225, 81.225, 80.86666666666666, 80.86666666666666, 80.95, 80.95, 80.75, 80.75, 80.58333333333333, 80.58333333333333, 80.86666666666666, 80.86666666666666, 81.35, 81.35, 81.525, 81.525, 81.55, 81.55, 81.975, 81.975, 82.15, 82.15, 82.68333333333334, 82.68333333333334, 82.71666666666667, 82.71666666666667, 83.14166666666667, 83.14166666666667, 83.1, 83.1, 83.175, 83.175, 82.95, 82.95, 83.225, 83.225, 83.2, 83.2, 83.25, 83.25, 83.625, 83.625, 83.48333333333333, 83.48333333333333, 83.26666666666667, 83.26666666666667, 83.31666666666666, 83.31666666666666, 84.16666666666667, 84.16666666666667, 83.94166666666666, 83.94166666666666, 83.60833333333333, 83.60833333333333, 84.29166666666667, 84.29166666666667, 84.29166666666667, 84.29166666666667, 84.18333333333334, 84.18333333333334, 84.20833333333333, 84.20833333333333, 84.39166666666667, 84.39166666666667, 84.48333333333333, 84.48333333333333, 84.54166666666667, 84.54166666666667, 84.6, 84.6, 84.84166666666667, 84.84166666666667, 84.725, 84.725, 85.00833333333334, 85.00833333333334, 84.63333333333334, 84.63333333333334, 84.88333333333334, 84.88333333333334, 84.825, 84.825, 84.85833333333333, 84.85833333333333, 84.76666666666667, 84.76666666666667, 85.56666666666666, 85.56666666666666, 85.48333333333333, 85.48333333333333, 85.8, 85.8, 85.7, 85.7, 85.8, 85.8, 85.63333333333334, 85.63333333333334, 85.7, 85.7, 85.95, 85.95, 85.71666666666667, 85.71666666666667, 85.94166666666666, 85.94166666666666, 86.41666666666667, 86.41666666666667, 85.71666666666667, 85.71666666666667, 85.78333333333333, 85.78333333333333, 85.51666666666667, 85.51666666666667, 86.225, 86.225, 86.25, 86.25, 85.81666666666666, 85.81666666666666, 86.025, 86.025, 86.25, 86.25, 85.93333333333334, 85.93333333333334, 86.06666666666666, 86.06666666666666, 86.05, 86.05, 86.35, 86.35, 86.21666666666667, 86.21666666666667, 86.325, 86.325, 86.08333333333333, 86.08333333333333, 86.30833333333334, 86.30833333333334, 86.41666666666667, 86.41666666666667, 86.28333333333333, 86.28333333333333, 86.75, 86.75, 86.64166666666667, 86.64166666666667, 86.53333333333333, 86.53333333333333, 86.45833333333333, 86.45833333333333, 86.46666666666667, 86.46666666666667, 86.83333333333333, 86.83333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.195, Test loss: 0.334, Test accuracy: 87.43
Average accuracy final 10 rounds: 86.945
1818.5374674797058
[2.2330119609832764, 4.466023921966553, 6.273844003677368, 8.081664085388184, 9.903797388076782, 11.72593069076538, 13.5321204662323, 15.338310241699219, 17.14589023590088, 18.95347023010254, 20.744205236434937, 22.534940242767334, 24.324262619018555, 26.113584995269775, 27.920258283615112, 29.72693157196045, 31.532682418823242, 33.338433265686035, 35.16722345352173, 36.99601364135742, 38.81782865524292, 40.63964366912842, 42.44174838066101, 44.2438530921936, 46.05596470832825, 47.86807632446289, 49.69225740432739, 51.516438484191895, 53.33091139793396, 55.145384311676025, 56.95366382598877, 58.761943340301514, 60.56777858734131, 62.3736138343811, 64.19444990158081, 66.01528596878052, 67.83949255943298, 69.66369915008545, 71.49492812156677, 73.3261570930481, 75.15330767631531, 76.98045825958252, 78.69032907485962, 80.40019989013672, 82.19872665405273, 83.99725341796875, 85.80450129508972, 87.6117491722107, 89.4252438545227, 91.23873853683472, 93.0619056224823, 94.88507270812988, 96.71663546562195, 98.54819822311401, 100.32811641693115, 102.10803461074829, 103.89507269859314, 105.68211078643799, 107.48513674736023, 109.28816270828247, 111.07766604423523, 112.86716938018799, 114.66241693496704, 116.4576644897461, 118.2423985004425, 120.02713251113892, 121.81969738006592, 123.61226224899292, 125.39843344688416, 127.18460464477539, 128.98168277740479, 130.77876091003418, 132.57922291755676, 134.37968492507935, 136.1691653728485, 137.95864582061768, 139.7379596233368, 141.5172734260559, 143.3189091682434, 145.1205449104309, 146.91961908340454, 148.71869325637817, 150.50145363807678, 152.2842140197754, 154.0967984199524, 155.9093828201294, 157.6981840133667, 159.486985206604, 161.2893476486206, 163.0917100906372, 164.87693166732788, 166.66215324401855, 168.45051050186157, 170.2388677597046, 172.03619503974915, 173.8335223197937, 175.6283359527588, 177.42314958572388, 179.2178246974945, 181.01249980926514, 182.8034746646881, 184.59444952011108, 186.3814878463745, 188.16852617263794, 189.97086596488953, 191.7732057571411, 193.55716466903687, 195.34112358093262, 197.1303050518036, 198.91948652267456, 200.71398901939392, 202.50849151611328, 204.30334162712097, 206.09819173812866, 207.8705859184265, 209.64298009872437, 211.44619917869568, 213.249418258667, 215.0663025379181, 216.8831868171692, 218.69041180610657, 220.49763679504395, 222.29733300209045, 224.09702920913696, 225.89659094810486, 227.69615268707275, 229.51220059394836, 231.32824850082397, 233.14641952514648, 234.964590549469, 236.76935195922852, 238.57411336898804, 240.38751673698425, 242.20092010498047, 243.99402356147766, 245.78712701797485, 247.60065340995789, 249.41417980194092, 251.22888708114624, 253.04359436035156, 254.8627471923828, 256.68190002441406, 258.49399280548096, 260.30608558654785, 262.1353931427002, 263.96470069885254, 265.7687737941742, 267.57284688949585, 269.3794367313385, 271.18602657318115, 273.07258319854736, 274.9591398239136, 276.775137424469, 278.5911350250244, 280.42581725120544, 282.2604994773865, 284.0799889564514, 285.89947843551636, 287.72897028923035, 289.55846214294434, 291.38019585609436, 293.2019295692444, 295.00960636138916, 296.81728315353394, 298.6357789039612, 300.4542746543884, 302.2493336200714, 304.0443925857544, 305.65923976898193, 307.2740869522095, 308.9001407623291, 310.52619457244873, 312.1416127681732, 313.7570309638977, 315.37671184539795, 316.9963927268982, 318.6501965522766, 320.30400037765503, 321.9340810775757, 323.56416177749634, 325.23227190971375, 326.90038204193115, 328.52026867866516, 330.14015531539917, 331.78245735168457, 333.42475938796997, 335.05383348464966, 336.68290758132935, 338.3313264846802, 339.979745388031, 341.61176323890686, 343.2437810897827, 344.88829612731934, 346.53281116485596, 348.1598551273346, 349.78689908981323, 351.42268109321594, 353.05846309661865, 354.68781900405884, 356.317174911499, 358.4798653125763, 360.64255571365356]
[28.516666666666666, 28.516666666666666, 34.608333333333334, 34.608333333333334, 42.583333333333336, 42.583333333333336, 55.666666666666664, 55.666666666666664, 63.7, 63.7, 61.31666666666667, 61.31666666666667, 66.01666666666667, 66.01666666666667, 72.625, 72.625, 70.96666666666667, 70.96666666666667, 75.45, 75.45, 76.41666666666667, 76.41666666666667, 76.95, 76.95, 77.80833333333334, 77.80833333333334, 77.20833333333333, 77.20833333333333, 77.625, 77.625, 78.41666666666667, 78.41666666666667, 79.15, 79.15, 79.31666666666666, 79.31666666666666, 79.68333333333334, 79.68333333333334, 80.2, 80.2, 80.175, 80.175, 80.40833333333333, 80.40833333333333, 80.30833333333334, 80.30833333333334, 80.7, 80.7, 81.35833333333333, 81.35833333333333, 81.375, 81.375, 81.65833333333333, 81.65833333333333, 81.51666666666667, 81.51666666666667, 81.75, 81.75, 82.08333333333333, 82.08333333333333, 82.41666666666667, 82.41666666666667, 82.00833333333334, 82.00833333333334, 82.25, 82.25, 82.19166666666666, 82.19166666666666, 82.79166666666667, 82.79166666666667, 82.53333333333333, 82.53333333333333, 83.05833333333334, 83.05833333333334, 83.25, 83.25, 83.29166666666667, 83.29166666666667, 83.60833333333333, 83.60833333333333, 83.61666666666666, 83.61666666666666, 83.775, 83.775, 83.90833333333333, 83.90833333333333, 84.04166666666667, 84.04166666666667, 83.93333333333334, 83.93333333333334, 83.56666666666666, 83.56666666666666, 83.69166666666666, 83.69166666666666, 84.09166666666667, 84.09166666666667, 83.875, 83.875, 84.5, 84.5, 84.55833333333334, 84.55833333333334, 84.80833333333334, 84.80833333333334, 85.19166666666666, 85.19166666666666, 85.08333333333333, 85.08333333333333, 85.05833333333334, 85.05833333333334, 84.975, 84.975, 84.73333333333333, 84.73333333333333, 84.75, 84.75, 85.21666666666667, 85.21666666666667, 85.58333333333333, 85.58333333333333, 85.63333333333334, 85.63333333333334, 85.51666666666667, 85.51666666666667, 85.5, 85.5, 85.64166666666667, 85.64166666666667, 85.64166666666667, 85.64166666666667, 85.825, 85.825, 86.03333333333333, 86.03333333333333, 85.98333333333333, 85.98333333333333, 86.025, 86.025, 86.325, 86.325, 86.48333333333333, 86.48333333333333, 86.125, 86.125, 86.35833333333333, 86.35833333333333, 86.34166666666667, 86.34166666666667, 86.26666666666667, 86.26666666666667, 86.38333333333334, 86.38333333333334, 86.29166666666667, 86.29166666666667, 86.38333333333334, 86.38333333333334, 86.38333333333334, 86.38333333333334, 86.51666666666667, 86.51666666666667, 86.44166666666666, 86.44166666666666, 86.7, 86.7, 86.30833333333334, 86.30833333333334, 86.4, 86.4, 86.65833333333333, 86.65833333333333, 86.75, 86.75, 86.60833333333333, 86.60833333333333, 86.26666666666667, 86.26666666666667, 86.55833333333334, 86.55833333333334, 87.19166666666666, 87.19166666666666, 87.025, 87.025, 86.90833333333333, 86.90833333333333, 86.93333333333334, 86.93333333333334, 86.69166666666666, 86.69166666666666, 86.89166666666667, 86.89166666666667, 86.71666666666667, 86.71666666666667, 87.11666666666666, 87.11666666666666, 87.00833333333334, 87.00833333333334, 87.05, 87.05, 87.10833333333333, 87.10833333333333, 87.43333333333334, 87.43333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.158, Test loss: 1.111, Test accuracy: 67.12
Average accuracy final 10 rounds: 63.20916666666667
2552.051463365555
[4.065163850784302, 7.614247798919678, 11.190709352493286, 14.788778305053711, 18.378865718841553, 21.96125841140747, 25.577704191207886, 29.19133687019348, 32.77551484107971, 36.35343098640442, 39.92634344100952, 43.49280667304993, 47.05085062980652, 50.60004949569702, 54.12668180465698, 57.6929144859314, 61.21616864204407, 64.7408504486084, 68.25169491767883, 71.76834845542908, 75.29940533638, 78.8666844367981, 82.47112727165222, 86.07908916473389, 89.64793419837952, 93.21647238731384, 96.7781662940979, 100.3397171497345, 103.91175580024719, 107.50760769844055, 111.10125732421875, 114.63913416862488, 118.16316556930542, 121.72265601158142, 125.3011634349823, 128.88806080818176, 132.4454357624054, 136.01068949699402, 139.58678007125854, 143.1471118927002, 146.69261050224304, 150.22256422042847, 153.7850522994995, 157.37128853797913, 160.92692756652832, 164.40984511375427, 167.9153118133545, 171.4405655860901, 174.99976110458374, 178.52041029930115, 182.0443515777588, 185.52433037757874, 189.02021026611328, 192.85477948188782, 196.70544362068176, 200.5994942188263, 204.46709752082825, 208.2439422607422, 212.0615210533142, 215.75240755081177, 219.2648651599884, 222.77121782302856, 226.27501034736633, 229.72308111190796, 233.17166757583618, 236.63393568992615, 240.11395406723022, 243.5938115119934, 247.09923911094666, 250.58781695365906, 254.04053258895874, 257.5076537132263, 260.98018884658813, 264.46559047698975, 267.9220778942108, 271.42300820350647, 274.9232335090637, 278.3817002773285, 281.8699827194214, 285.357093334198, 288.7987689971924, 292.6752119064331, 296.5505635738373, 300.41190218925476, 304.30017471313477, 308.1478912830353, 312.0292317867279, 315.90454745292664, 319.79217982292175, 323.6476354598999, 327.5269637107849, 331.37442779541016, 335.24491930007935, 339.12946486473083, 343.0131866931915, 346.88794803619385, 350.7560307979584, 354.6174530982971, 358.4874348640442, 361.9540333747864, 364.89166736602783]
[30.291666666666668, 31.591666666666665, 36.2, 40.166666666666664, 31.725, 40.11666666666667, 48.05833333333333, 42.075, 40.083333333333336, 50.25, 51.35, 38.108333333333334, 51.825, 54.025, 44.38333333333333, 50.65833333333333, 48.84166666666667, 48.71666666666667, 49.85, 55.75, 47.36666666666667, 52.25833333333333, 54.28333333333333, 54.458333333333336, 53.69166666666667, 52.18333333333333, 46.225, 55.625, 56.85, 56.75833333333333, 44.05833333333333, 59.041666666666664, 59.15833333333333, 56.958333333333336, 51.9, 59.24166666666667, 56.65, 61.625, 59.69166666666667, 60.141666666666666, 59.03333333333333, 51.2, 59.083333333333336, 55.25, 58.3, 59.78333333333333, 58.625, 58.675, 55.858333333333334, 60.583333333333336, 60.40833333333333, 59.208333333333336, 56.99166666666667, 61.0, 59.90833333333333, 60.75833333333333, 57.225, 62.75, 61.6, 60.25, 52.675, 59.71666666666667, 61.516666666666666, 60.86666666666667, 56.583333333333336, 64.35833333333333, 62.666666666666664, 57.666666666666664, 60.43333333333333, 62.43333333333333, 62.74166666666667, 63.75833333333333, 60.69166666666667, 64.89166666666667, 63.275, 64.41666666666667, 59.95, 64.39166666666667, 62.65833333333333, 64.31666666666666, 64.15, 59.975, 60.925, 62.208333333333336, 63.375, 63.30833333333333, 63.208333333333336, 59.6, 64.84166666666667, 62.18333333333333, 59.166666666666664, 62.9, 62.25, 63.358333333333334, 63.483333333333334, 66.8, 63.99166666666667, 63.28333333333333, 62.6, 64.25833333333334, 67.11666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.269, Test loss: 2.259, Test accuracy: 21.86
Final Round, Global train loss: 2.269, Global test loss: 2.260, Global test accuracy: 21.63
Average accuracy final 10 rounds: 18.28325 

Average global accuracy final 10 rounds: 19.559 

4996.554930925369
[4.836015939712524, 10.014759063720703, 15.191001892089844, 20.354210138320923, 25.574259757995605, 30.78663730621338, 35.97541284561157, 41.203312158584595, 46.38679337501526, 50.97973012924194, 55.54980659484863, 60.08367395401001, 64.601402759552, 69.12454605102539, 73.68774557113647, 78.28975796699524, 82.94846153259277, 87.55504846572876, 92.19147634506226, 96.85617232322693, 101.49128866195679, 106.05291271209717, 110.65704226493835, 115.26262378692627, 119.82554244995117, 124.3844907283783, 128.96346306800842, 133.52367687225342, 138.0447540283203, 142.56916403770447, 147.13453555107117, 151.7032744884491, 156.25850462913513, 160.77314615249634, 165.2999210357666, 169.84044742584229, 174.39635801315308, 178.92869901657104, 183.42457032203674, 187.90889596939087, 192.4152331352234, 196.93618965148926, 201.46435809135437, 206.0005979537964, 210.5140025615692, 215.02591681480408, 219.5549132823944, 224.0870542526245, 228.60482048988342, 233.11274027824402, 237.61447167396545, 242.1334159374237, 246.65965628623962, 251.15383744239807, 255.64691543579102, 260.15219497680664, 264.65538144111633, 269.1909296512604, 273.68223333358765, 278.1788878440857, 282.6616551876068, 287.1647198200226, 291.67244124412537, 296.1849796772003, 300.69342517852783, 305.21436834335327, 309.72643852233887, 314.25166416168213, 318.7755672931671, 323.30641913414, 327.85049510002136, 332.36672282218933, 336.89860367774963, 341.41953206062317, 345.9467680454254, 350.442923784256, 354.94568061828613, 359.4545929431915, 363.988285779953, 368.5129554271698, 373.0293824672699, 377.57348442077637, 382.1187801361084, 386.67449855804443, 391.2131426334381, 395.78987884521484, 400.3442804813385, 404.8918204307556, 409.412544965744, 413.9178795814514, 418.4282374382019, 422.93745851516724, 427.45438504219055, 431.9789733886719, 436.5135598182678, 441.0606849193573, 445.60584568977356, 450.16298484802246, 454.72789907455444, 459.2825231552124, 461.5514483451843]
[9.6825, 9.6975, 9.71, 9.69, 9.755, 9.75, 9.8325, 9.8375, 9.815, 9.8675, 9.885, 9.9075, 10.005, 10.06, 10.1625, 10.3025, 10.2325, 10.1825, 10.2775, 10.3075, 10.3925, 10.6325, 10.88, 11.155, 11.325, 11.6025, 11.84, 12.4475, 12.5625, 13.015, 13.8775, 14.36, 14.9925, 15.01, 15.325, 15.4425, 15.3925, 15.0125, 15.215, 15.12, 14.9825, 14.9975, 15.0725, 14.3225, 14.6325, 14.485, 14.6, 14.3075, 14.64, 14.5925, 14.41, 15.205, 14.8, 14.585, 13.745, 13.1125, 12.7125, 12.6325, 12.27, 12.2025, 12.075, 11.95, 11.76, 11.77, 11.675, 11.295, 11.0375, 11.035, 11.25, 10.94, 10.8975, 10.8525, 10.7525, 10.7875, 10.97, 11.485, 11.53, 11.3575, 11.3475, 11.42, 11.6725, 11.8175, 11.85, 11.8975, 12.495, 12.6925, 13.2925, 13.1925, 13.7025, 14.1125, 15.0425, 15.895, 16.6975, 17.8275, 18.41, 18.7325, 19.3175, 19.945, 20.3475, 20.6175, 21.8625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.249, Test loss: 1.080, Test accuracy: 73.84
Final Round, Global train loss: 0.249, Global test loss: 1.111, Global test accuracy: 70.44
Average accuracy final 10 rounds: 73.16025 

Average global accuracy final 10 rounds: 69.89099999999999 

6176.949338674545
[4.880629539489746, 9.761259078979492, 14.505727291107178, 19.250195503234863, 23.415972232818604, 27.581748962402344, 31.77610182762146, 35.970454692840576, 40.15034294128418, 44.33023118972778, 48.527586460113525, 52.72494173049927, 56.92723488807678, 61.1295280456543, 65.312007188797, 69.4944863319397, 73.66690802574158, 77.83932971954346, 82.01739311218262, 86.19545650482178, 90.73012208938599, 95.2647876739502, 99.54436659812927, 103.82394552230835, 108.1315598487854, 112.43917417526245, 116.74326658248901, 121.04735898971558, 125.56756114959717, 130.08776330947876, 135.03756260871887, 139.98736190795898, 144.9384560585022, 149.8895502090454, 154.8752646446228, 159.8609790802002, 164.8221127986908, 169.7832465171814, 174.75461196899414, 179.72597742080688, 184.69669127464294, 189.667405128479, 194.6258101463318, 199.58421516418457, 204.51067519187927, 209.43713521957397, 214.39804792404175, 219.35896062850952, 224.29716563224792, 229.23537063598633, 234.19711542129517, 239.158860206604, 244.13781070709229, 249.11676120758057, 254.01546025276184, 258.9141592979431, 263.81566166877747, 268.7171640396118, 273.69770860671997, 278.6782531738281, 283.660845041275, 288.6434369087219, 292.95186591148376, 297.2602949142456, 301.56195282936096, 305.8636107444763, 310.15283250808716, 314.442054271698, 318.74169540405273, 323.04133653640747, 327.34520745277405, 331.6490783691406, 335.9065637588501, 340.16404914855957, 344.43796014785767, 348.71187114715576, 353.0294897556305, 357.3471083641052, 361.64855003356934, 365.94999170303345, 370.26438331604004, 374.57877492904663, 378.863440990448, 383.14810705184937, 387.45726132392883, 391.7664155960083, 396.07000494003296, 400.3735942840576, 404.66657161712646, 408.9595489501953, 413.2122962474823, 417.4650435447693, 421.7509205341339, 426.03679752349854, 430.33838152885437, 434.6399655342102, 438.9522659778595, 443.2645664215088, 447.60487508773804, 451.9451837539673, 456.27556347846985, 460.6059432029724, 464.95433259010315, 469.3027219772339, 473.61715149879456, 477.9315810203552, 482.21486353874207, 486.4981460571289, 490.78921341896057, 495.08028078079224, 499.3707571029663, 503.6612334251404, 507.9946799278259, 512.3281264305115, 516.615110874176, 520.9020953178406, 525.2383644580841, 529.5746335983276, 533.9013800621033, 538.2281265258789, 542.5476560592651, 546.8671855926514, 551.2040855884552, 555.540985584259, 559.8830528259277, 564.2251200675964, 568.5420446395874, 572.8589692115784, 577.1773266792297, 581.4956841468811, 585.7843751907349, 590.0730662345886, 594.3740453720093, 598.6750245094299, 602.9917402267456, 607.3084559440613, 611.5987758636475, 615.8890957832336, 620.209600687027, 624.5301055908203, 628.8609812259674, 633.1918568611145, 637.5388760566711, 641.8858952522278, 646.3404936790466, 650.7950921058655, 655.0971739292145, 659.3992557525635, 663.6945536136627, 667.989851474762, 672.2785694599152, 676.5672874450684, 680.8670227527618, 685.1667580604553, 690.1479294300079, 695.1291007995605, 700.1089515686035, 705.0888023376465, 710.0869359970093, 715.0850696563721, 720.0438210964203, 725.0025725364685, 729.9852254390717, 734.9678783416748, 739.9780216217041, 744.9881649017334, 749.9530692100525, 754.9179735183716, 759.8944070339203, 764.870840549469, 769.8544294834137, 774.8380184173584, 779.8733184337616, 784.9086184501648, 789.9167954921722, 794.9249725341797, 799.9514756202698, 804.9779787063599, 809.9962193965912, 815.0144600868225, 820.0430510044098, 825.0716419219971, 830.073344707489, 835.075047492981, 839.9823346138, 844.8896217346191, 849.2412593364716, 853.592896938324, 857.9509778022766, 862.3090586662292, 866.6541101932526, 870.9991617202759, 875.3654680252075, 879.7317743301392, 884.1430637836456, 888.5543532371521, 892.9486882686615, 897.3430233001709, 901.7263441085815, 906.1096649169922, 908.3030169010162, 910.4963688850403]
[35.165, 35.165, 41.5275, 41.5275, 44.13, 44.13, 46.7975, 46.7975, 49.28, 49.28, 51.2225, 51.2225, 52.9125, 52.9125, 55.1575, 55.1575, 56.1625, 56.1625, 57.2275, 57.2275, 57.605, 57.605, 59.6825, 59.6825, 60.0425, 60.0425, 60.5825, 60.5825, 60.935, 60.935, 61.2075, 61.2075, 61.855, 61.855, 63.7675, 63.7675, 64.0475, 64.0475, 64.5475, 64.5475, 65.875, 65.875, 66.4475, 66.4475, 66.845, 66.845, 67.0175, 67.0175, 67.2825, 67.2825, 67.3475, 67.3475, 67.75, 67.75, 67.855, 67.855, 68.3275, 68.3275, 68.6625, 68.6625, 68.26, 68.26, 68.715, 68.715, 68.9375, 68.9375, 69.5225, 69.5225, 69.825, 69.825, 69.775, 69.775, 69.58, 69.58, 69.56, 69.56, 69.585, 69.585, 69.65, 69.65, 69.9925, 69.9925, 70.045, 70.045, 70.07, 70.07, 70.425, 70.425, 70.6325, 70.6325, 70.4525, 70.4525, 70.7375, 70.7375, 70.8225, 70.8225, 70.53, 70.53, 70.6575, 70.6575, 70.865, 70.865, 71.2975, 71.2975, 71.085, 71.085, 71.045, 71.045, 71.1925, 71.1925, 71.2275, 71.2275, 71.255, 71.255, 71.515, 71.515, 71.44, 71.44, 71.6, 71.6, 71.6225, 71.6225, 71.7625, 71.7625, 71.83, 71.83, 71.8475, 71.8475, 71.895, 71.895, 72.035, 72.035, 72.15, 72.15, 72.33, 72.33, 72.78, 72.78, 72.61, 72.61, 72.3925, 72.3925, 72.6225, 72.6225, 72.6175, 72.6175, 72.4325, 72.4325, 72.6975, 72.6975, 72.36, 72.36, 72.2725, 72.2725, 72.4725, 72.4725, 72.755, 72.755, 72.8025, 72.8025, 72.6325, 72.6325, 72.785, 72.785, 73.0625, 73.0625, 72.9125, 72.9125, 72.81, 72.81, 73.01, 73.01, 73.005, 73.005, 73.0925, 73.0925, 73.225, 73.225, 72.9625, 72.9625, 73.025, 73.025, 72.98, 72.98, 73.3375, 73.3375, 73.3, 73.3, 73.2625, 73.2625, 73.195, 73.195, 73.1225, 73.1225, 73.255, 73.255, 73.0025, 73.0025, 73.1225, 73.1225, 73.8425, 73.8425]
