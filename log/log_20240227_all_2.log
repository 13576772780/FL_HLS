nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.153, Test loss: 4.214, Test accuracy: 40.87
Final Round, Global train loss: 0.153, Global test loss: 1.614, Global test accuracy: 48.08
Average accuracy final 10 rounds: 41.009499999999996 

Average global accuracy final 10 rounds: 47.254000000000005 

3973.1415796279907
[1.3184142112731934, 2.6368284225463867, 3.872910499572754, 5.108992576599121, 6.358166217803955, 7.607339859008789, 8.86343502998352, 10.119530200958252, 11.37372350692749, 12.627916812896729, 13.880046129226685, 15.13217544555664, 16.38448214530945, 17.636788845062256, 18.884851455688477, 20.132914066314697, 21.38869571685791, 22.644477367401123, 23.89133381843567, 25.138190269470215, 26.388182163238525, 27.638174057006836, 28.892081260681152, 30.14598846435547, 31.4031662940979, 32.66034412384033, 33.9192578792572, 35.17817163467407, 36.43812298774719, 37.69807434082031, 38.955204248428345, 40.21233415603638, 41.47130608558655, 42.73027801513672, 43.99457263946533, 45.258867263793945, 46.51800990104675, 47.77715253829956, 49.024434328079224, 50.27171611785889, 51.52018594741821, 52.76865577697754, 54.01274919509888, 55.256842613220215, 56.51409649848938, 57.771350383758545, 59.030126333236694, 60.288902282714844, 61.56025791168213, 62.831613540649414, 64.10936403274536, 65.38711452484131, 66.6685140132904, 67.9499135017395, 69.21994233131409, 70.48997116088867, 71.76627469062805, 73.04257822036743, 74.30637454986572, 75.57017087936401, 76.84717535972595, 78.12417984008789, 79.40823721885681, 80.69229459762573, 81.96974229812622, 83.24718999862671, 84.51974701881409, 85.79230403900146, 87.06966423988342, 88.34702444076538, 89.62698364257812, 90.90694284439087, 92.18619990348816, 93.46545696258545, 94.74985146522522, 96.03424596786499, 97.31546807289124, 98.59669017791748, 99.87258219718933, 101.14847421646118, 102.42639756202698, 103.70432090759277, 104.97640824317932, 106.24849557876587, 107.52265357971191, 108.79681158065796, 110.06970357894897, 111.34259557723999, 112.6151831150055, 113.887770652771, 115.17996835708618, 116.47216606140137, 117.74693584442139, 119.0217056274414, 120.30216550827026, 121.58262538909912, 122.86389112472534, 124.14515686035156, 125.4135262966156, 126.68189573287964, 127.96144127845764, 129.24098682403564, 130.52224349975586, 131.80350017547607, 133.08040356636047, 134.35730695724487, 135.64539289474487, 136.93347883224487, 138.20461630821228, 139.4757537841797, 140.7672393321991, 142.0587248802185, 143.34079122543335, 144.6228575706482, 145.90380120277405, 147.1847448348999, 148.473730802536, 149.76271677017212, 151.03655672073364, 152.31039667129517, 153.59187531471252, 154.87335395812988, 156.16668701171875, 157.46002006530762, 158.73598885536194, 160.01195764541626, 161.3001003265381, 162.5882430076599, 163.86817622184753, 165.14810943603516, 166.42050290107727, 167.69289636611938, 168.97742748260498, 170.26195859909058, 171.53535318374634, 172.8087477684021, 174.0871934890747, 175.36563920974731, 176.64790105819702, 177.93016290664673, 179.20437455177307, 180.4785861968994, 181.7482590675354, 183.0179319381714, 184.29228115081787, 185.56663036346436, 186.84854078292847, 188.13045120239258, 189.4081997871399, 190.6859483718872, 191.95400047302246, 193.22205257415771, 194.50528836250305, 195.7885241508484, 197.06814169883728, 198.34775924682617, 199.62430143356323, 200.9008436203003, 202.193017244339, 203.48519086837769, 204.75480842590332, 206.02442598342896, 207.30064868927002, 208.57687139511108, 209.86244201660156, 211.14801263809204, 212.42196106910706, 213.69590950012207, 214.97937679290771, 216.26284408569336, 217.545884847641, 218.82892560958862, 220.11602687835693, 221.40312814712524, 222.6804723739624, 223.95781660079956, 225.22850489616394, 226.49919319152832, 227.77374958992004, 229.04830598831177, 230.33263874053955, 231.61697149276733, 232.89634656906128, 234.17572164535522, 235.44437289237976, 236.7130241394043, 237.9914367198944, 239.26984930038452, 240.55689907073975, 241.84394884109497, 243.12391805648804, 244.4038872718811, 245.70583128929138, 247.00777530670166, 248.28401279449463, 249.5602502822876, 250.83835124969482, 252.11645221710205, 253.40106534957886, 254.68567848205566, 257.2465739250183, 259.80746936798096]
[24.4325, 24.4325, 28.1875, 28.1875, 30.67, 30.67, 31.495, 31.495, 32.0875, 32.0875, 33.015, 33.015, 33.795, 33.795, 34.155, 34.155, 35.36, 35.36, 35.395, 35.395, 36.05, 36.05, 36.2625, 36.2625, 37.3425, 37.3425, 37.67, 37.67, 38.485, 38.485, 38.7, 38.7, 38.9525, 38.9525, 39.005, 39.005, 39.1875, 39.1875, 39.4375, 39.4375, 39.5125, 39.5125, 39.8925, 39.8925, 39.745, 39.745, 40.1075, 40.1075, 39.9925, 39.9925, 39.57, 39.57, 39.8475, 39.8475, 40.5775, 40.5775, 40.6775, 40.6775, 40.7325, 40.7325, 40.85, 40.85, 40.9825, 40.9825, 41.2575, 41.2575, 40.9575, 40.9575, 40.64, 40.64, 40.59, 40.59, 40.975, 40.975, 41.0225, 41.0225, 41.0975, 41.0975, 40.8725, 40.8725, 41.085, 41.085, 40.7025, 40.7025, 40.9475, 40.9475, 40.975, 40.975, 40.47, 40.47, 40.4275, 40.4275, 40.3425, 40.3425, 40.525, 40.525, 40.2725, 40.2725, 40.81, 40.81, 41.09, 41.09, 41.2475, 41.2475, 41.005, 41.005, 41.0375, 41.0375, 41.0875, 41.0875, 41.215, 41.215, 40.9525, 40.9525, 41.065, 41.065, 41.34, 41.34, 41.45, 41.45, 41.155, 41.155, 40.6925, 40.6925, 40.745, 40.745, 40.5975, 40.5975, 40.6575, 40.6575, 40.7675, 40.7675, 40.5125, 40.5125, 40.5325, 40.5325, 40.6725, 40.6725, 40.875, 40.875, 41.0975, 41.0975, 40.89, 40.89, 40.975, 40.975, 40.91, 40.91, 40.8675, 40.8675, 40.7175, 40.7175, 40.805, 40.805, 40.985, 40.985, 40.5625, 40.5625, 40.6525, 40.6525, 40.905, 40.905, 41.2175, 41.2175, 41.165, 41.165, 40.8925, 40.8925, 40.895, 40.895, 40.5025, 40.5025, 40.5225, 40.5225, 40.7625, 40.7625, 41.0975, 41.0975, 41.1025, 41.1025, 40.895, 40.895, 41.0275, 41.0275, 40.8, 40.8, 41.0875, 41.0875, 41.1, 41.1, 41.0475, 41.0475, 41.015, 41.015, 40.955, 40.955, 41.025, 41.025, 41.1425, 41.1425, 40.865, 40.865]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.351, Test loss: 1.695, Test accuracy: 61.66
Final Round, Global train loss: 0.351, Global test loss: 1.144, Global test accuracy: 68.83
Average accuracy final 10 rounds: 62.3945 

Average global accuracy final 10 rounds: 69.02125 

3804.3268892765045
[1.463594913482666, 2.927189826965332, 4.173524379730225, 5.419858932495117, 6.650309801101685, 7.880760669708252, 9.120553016662598, 10.360345363616943, 11.602229833602905, 12.844114303588867, 14.099700927734375, 15.355287551879883, 16.607558488845825, 17.859829425811768, 19.12105107307434, 20.382272720336914, 21.64318823814392, 22.904103755950928, 24.16223168373108, 25.42035961151123, 26.66050672531128, 27.900653839111328, 29.147387266159058, 30.394120693206787, 31.647735357284546, 32.901350021362305, 34.15661334991455, 35.4118766784668, 36.671815395355225, 37.93175411224365, 39.18736815452576, 40.44298219680786, 41.705098390579224, 42.967214584350586, 44.20917296409607, 45.45113134384155, 46.70123767852783, 47.95134401321411, 49.205763816833496, 50.46018362045288, 51.71396040916443, 52.96773719787598, 54.21506428718567, 55.46239137649536, 56.71750283241272, 57.97261428833008, 59.22022771835327, 60.467841148376465, 61.708505630493164, 62.94917011260986, 64.19775652885437, 65.44634294509888, 66.69353866577148, 67.94073438644409, 69.19258713722229, 70.44443988800049, 71.68748736381531, 72.93053483963013, 74.17293953895569, 75.41534423828125, 76.65422248840332, 77.89310073852539, 78.8913164138794, 79.8895320892334, 80.90176630020142, 81.91400051116943, 82.91121220588684, 83.90842390060425, 84.90693497657776, 85.90544605255127, 86.90452647209167, 87.90360689163208, 88.90160512924194, 89.8996033668518, 91.05259871482849, 92.20559406280518, 93.3627872467041, 94.51998043060303, 95.68466758728027, 96.84935474395752, 98.0292580127716, 99.2091612815857, 100.37217569351196, 101.53519010543823, 102.69658422470093, 103.85797834396362, 104.99985003471375, 106.14172172546387, 107.30225086212158, 108.4627799987793, 109.62393498420715, 110.78508996963501, 111.95326352119446, 113.1214370727539, 114.27861499786377, 115.43579292297363, 116.59357500076294, 117.75135707855225, 118.914381980896, 120.07740688323975, 121.23703050613403, 122.39665412902832, 123.55346894264221, 124.7102837562561, 125.86521005630493, 127.02013635635376, 128.1813781261444, 129.34261989593506, 130.50107836723328, 131.6595368385315, 132.81790709495544, 133.9762773513794, 135.13382935523987, 136.29138135910034, 137.44093489646912, 138.5904884338379, 139.74878358840942, 140.90707874298096, 142.06049823760986, 143.21391773223877, 144.37494564056396, 145.53597354888916, 146.69025683403015, 147.84454011917114, 148.99522137641907, 150.145902633667, 151.30075454711914, 152.4556064605713, 153.6178424358368, 154.7800784111023, 155.94966793060303, 157.11925745010376, 158.28715085983276, 159.45504426956177, 160.6096272468567, 161.7642102241516, 162.9231469631195, 164.0820837020874, 165.2503523826599, 166.41862106323242, 167.57567763328552, 168.73273420333862, 169.89003157615662, 171.0473289489746, 172.20494627952576, 173.3625636100769, 174.51426696777344, 175.66597032546997, 176.82908415794373, 177.99219799041748, 179.15261960029602, 180.31304121017456, 181.4745810031891, 182.6361207962036, 183.79763007164001, 184.95913934707642, 186.11162066459656, 187.2641019821167, 188.4253430366516, 189.58658409118652, 190.74337697029114, 191.90016984939575, 193.05406284332275, 194.20795583724976, 195.3656463623047, 196.52333688735962, 197.68039631843567, 198.83745574951172, 199.99222087860107, 201.14698600769043, 202.3090465068817, 203.471107006073, 204.6357536315918, 205.8004002571106, 206.95791602134705, 208.1154317855835, 209.28272533416748, 210.45001888275146, 211.60696840286255, 212.76391792297363, 213.9127335548401, 215.06154918670654, 216.21824073791504, 217.37493228912354, 218.53060126304626, 219.686270236969, 220.8504238128662, 222.01457738876343, 223.1758418083191, 224.33710622787476, 225.48056602478027, 226.6240258216858, 227.78584909439087, 228.94767236709595, 230.10562562942505, 231.26357889175415, 232.41803669929504, 233.57249450683594, 234.7394504547119, 235.9064064025879, 238.24010705947876, 240.57380771636963]
[26.095, 26.095, 30.525, 30.525, 33.425, 33.425, 35.6, 35.6, 35.8875, 35.8875, 38.13, 38.13, 39.55, 39.55, 40.325, 40.325, 41.965, 41.965, 43.43, 43.43, 44.695, 44.695, 45.615, 45.615, 46.7825, 46.7825, 46.63, 46.63, 47.31, 47.31, 48.49, 48.49, 49.0525, 49.0525, 49.815, 49.815, 50.425, 50.425, 51.07, 51.07, 52.17, 52.17, 52.2225, 52.2225, 52.4925, 52.4925, 52.5825, 52.5825, 52.515, 52.515, 53.3775, 53.3775, 53.3575, 53.3575, 54.005, 54.005, 54.9775, 54.9775, 55.3725, 55.3725, 55.655, 55.655, 55.8175, 55.8175, 55.675, 55.675, 55.16, 55.16, 55.505, 55.505, 55.9025, 55.9025, 56.44, 56.44, 56.745, 56.745, 57.2775, 57.2775, 58.095, 58.095, 58.4175, 58.4175, 58.6325, 58.6325, 58.945, 58.945, 58.865, 58.865, 59.02, 59.02, 59.015, 59.015, 59.0575, 59.0575, 59.295, 59.295, 59.2525, 59.2525, 59.52, 59.52, 59.425, 59.425, 59.5725, 59.5725, 59.6225, 59.6225, 60.0825, 60.0825, 60.1475, 60.1475, 59.8225, 59.8225, 60.075, 60.075, 60.135, 60.135, 60.1375, 60.1375, 60.825, 60.825, 60.915, 60.915, 60.7325, 60.7325, 60.9225, 60.9225, 60.945, 60.945, 60.945, 60.945, 60.7025, 60.7025, 60.805, 60.805, 60.79, 60.79, 61.15, 61.15, 61.18, 61.18, 61.4375, 61.4375, 61.875, 61.875, 61.89, 61.89, 61.85, 61.85, 61.8225, 61.8225, 61.895, 61.895, 61.9575, 61.9575, 62.1025, 62.1025, 61.9275, 61.9275, 61.925, 61.925, 61.6275, 61.6275, 61.9375, 61.9375, 62.1475, 62.1475, 62.475, 62.475, 62.54, 62.54, 62.79, 62.79, 62.6225, 62.6225, 62.5075, 62.5075, 62.8525, 62.8525, 62.73, 62.73, 62.69, 62.69, 62.3725, 62.3725, 62.39, 62.39, 62.3775, 62.3775, 62.3825, 62.3825, 62.33, 62.33, 62.525, 62.525, 62.5775, 62.5775, 62.0125, 62.0125, 62.2875, 62.2875, 61.66, 61.66]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.480, Test loss: 1.324, Test accuracy: 62.23
Average accuracy final 10 rounds: 62.30125000000001 

2705.616861343384
[1.3299810886383057, 2.6599621772766113, 3.726213216781616, 4.792464256286621, 5.84838342666626, 6.904302597045898, 7.965980291366577, 9.027657985687256, 10.088757514953613, 11.14985704421997, 12.204471349716187, 13.259085655212402, 14.325732231140137, 15.392378807067871, 16.45671033859253, 17.521041870117188, 18.577317237854004, 19.63359260559082, 20.69105863571167, 21.74852466583252, 22.81809902191162, 23.887673377990723, 24.951359510421753, 26.015045642852783, 27.07797861099243, 28.14091157913208, 29.19736933708191, 30.25382709503174, 31.320019006729126, 32.386210918426514, 33.44217538833618, 34.49813985824585, 35.55541682243347, 36.612693786621094, 37.67417025566101, 38.73564672470093, 39.79645776748657, 40.85726881027222, 41.909759521484375, 42.96225023269653, 44.014899492263794, 45.067548751831055, 46.12426447868347, 47.18098020553589, 48.22936940193176, 49.27775859832764, 50.348381757736206, 51.419004917144775, 52.490161180496216, 53.561317443847656, 54.62711787223816, 55.69291830062866, 56.758856534957886, 57.82479476928711, 58.88161277770996, 59.93843078613281, 60.9958176612854, 62.05320453643799, 63.10635447502136, 64.15950441360474, 65.21132111549377, 66.26313781738281, 67.32161593437195, 68.38009405136108, 69.42959356307983, 70.47909307479858, 71.53871393203735, 72.59833478927612, 73.65232372283936, 74.70631265640259, 75.75497889518738, 76.80364513397217, 77.86104893684387, 78.91845273971558, 79.98391914367676, 81.04938554763794, 82.10983514785767, 83.17028474807739, 84.22376465797424, 85.2772445678711, 86.3291687965393, 87.38109302520752, 88.44587922096252, 89.51066541671753, 90.58251905441284, 91.65437269210815, 92.70480418205261, 93.75523567199707, 94.81996941566467, 95.88470315933228, 96.94532442092896, 98.00594568252563, 99.0830008983612, 100.16005611419678, 101.21470952033997, 102.26936292648315, 103.31835770606995, 104.36735248565674, 105.42524147033691, 106.48313045501709, 107.49816846847534, 108.5132064819336, 109.46802115440369, 110.42283582687378, 111.37822127342224, 112.3336067199707, 113.29781794548035, 114.26202917098999, 115.21641540527344, 116.17080163955688, 117.13921809196472, 118.10763454437256, 119.06335139274597, 120.01906824111938, 120.98309063911438, 121.94711303710938, 122.9041633605957, 123.86121368408203, 124.8126904964447, 125.76416730880737, 126.72804546356201, 127.69192361831665, 128.639408826828, 129.58689403533936, 130.5503408908844, 131.51378774642944, 132.47572231292725, 133.43765687942505, 134.39879965782166, 135.35994243621826, 136.32444405555725, 137.28894567489624, 138.2440938949585, 139.19924211502075, 140.15827775001526, 141.11731338500977, 142.0884246826172, 143.0595359802246, 144.11363792419434, 145.16773986816406, 146.2334542274475, 147.29916858673096, 148.35702180862427, 149.41487503051758, 150.4797670841217, 151.54465913772583, 152.62721419334412, 153.7097692489624, 154.76945281028748, 155.82913637161255, 156.89269709587097, 157.9562578201294, 159.02236199378967, 160.08846616744995, 161.15804481506348, 162.227623462677, 163.28820872306824, 164.34879398345947, 165.40759897232056, 166.46640396118164, 167.41692447662354, 168.36744499206543, 169.31960797309875, 170.27177095413208, 171.22414588928223, 172.17652082443237, 173.13046503067017, 174.08440923690796, 175.02559518814087, 175.96678113937378, 176.9368190765381, 177.9068570137024, 178.87155079841614, 179.83624458312988, 180.79522824287415, 181.7542119026184, 182.72067785263062, 183.68714380264282, 184.65784811973572, 185.6285524368286, 186.58427095413208, 187.53998947143555, 188.50804042816162, 189.4760913848877, 190.43607568740845, 191.3960599899292, 192.35075163841248, 193.30544328689575, 194.2608392238617, 195.21623516082764, 196.1723906993866, 197.12854623794556, 198.0879716873169, 199.04739713668823, 200.01048064231873, 200.97356414794922, 201.93092322349548, 202.88828229904175, 203.850115776062, 204.81194925308228, 206.70902037620544, 208.6060914993286]
[18.035, 18.035, 25.2025, 25.2025, 27.725, 27.725, 29.58, 29.58, 32.3, 32.3, 35.33, 35.33, 37.7875, 37.7875, 39.61, 39.61, 41.205, 41.205, 42.51, 42.51, 43.765, 43.765, 44.1575, 44.1575, 45.425, 45.425, 44.9875, 44.9875, 46.9625, 46.9625, 47.835, 47.835, 48.7825, 48.7825, 48.8875, 48.8875, 49.7675, 49.7675, 50.4725, 50.4725, 51.0775, 51.0775, 51.28, 51.28, 51.465, 51.465, 52.3425, 52.3425, 52.4575, 52.4575, 52.575, 52.575, 53.255, 53.255, 54.035, 54.035, 54.2925, 54.2925, 55.2675, 55.2675, 55.5025, 55.5025, 55.35, 55.35, 55.6625, 55.6625, 55.8125, 55.8125, 56.01, 56.01, 56.3475, 56.3475, 57.1275, 57.1275, 57.6175, 57.6175, 57.5075, 57.5075, 57.605, 57.605, 58.23, 58.23, 58.0925, 58.0925, 58.7125, 58.7125, 58.4225, 58.4225, 58.8475, 58.8475, 59.13, 59.13, 58.165, 58.165, 58.775, 58.775, 58.4775, 58.4775, 59.0575, 59.0575, 58.8975, 58.8975, 59.41, 59.41, 60.5225, 60.5225, 59.8975, 59.8975, 60.0525, 60.0525, 60.3475, 60.3475, 60.22, 60.22, 61.0625, 61.0625, 61.06, 61.06, 61.025, 61.025, 60.8625, 60.8625, 61.3625, 61.3625, 61.5625, 61.5625, 61.8975, 61.8975, 61.4525, 61.4525, 61.29, 61.29, 61.8925, 61.8925, 61.5375, 61.5375, 61.39, 61.39, 61.5375, 61.5375, 61.58, 61.58, 61.66, 61.66, 61.1975, 61.1975, 61.2375, 61.2375, 61.0825, 61.0825, 61.2925, 61.2925, 61.285, 61.285, 60.97, 60.97, 61.125, 61.125, 61.2375, 61.2375, 61.3125, 61.3125, 61.4075, 61.4075, 61.9, 61.9, 62.005, 62.005, 61.8525, 61.8525, 61.825, 61.825, 61.915, 61.915, 61.8975, 61.8975, 62.25, 62.25, 62.065, 62.065, 62.375, 62.375, 62.2925, 62.2925, 62.3425, 62.3425, 62.4175, 62.4175, 62.1225, 62.1225, 62.2225, 62.2225, 61.8675, 61.8675, 62.2625, 62.2625, 62.625, 62.625, 62.485, 62.485, 62.2275, 62.2275]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.316, Test loss: 1.658, Test accuracy: 60.79
Average accuracy final 10 rounds: 61.26974999999999 

2939.7534070014954
[1.4107944965362549, 2.8215889930725098, 4.126194715499878, 5.430800437927246, 6.694250583648682, 7.957700729370117, 9.229868173599243, 10.50203561782837, 11.773598670959473, 13.045161724090576, 14.320266962051392, 15.595372200012207, 16.87213683128357, 18.14890146255493, 19.422929286956787, 20.696957111358643, 21.974935054779053, 23.252912998199463, 24.527540683746338, 25.802168369293213, 27.077502727508545, 28.352837085723877, 29.61932873725891, 30.885820388793945, 32.15354800224304, 33.42127561569214, 34.68830394744873, 35.95533227920532, 37.22433304786682, 38.49333381652832, 39.76905846595764, 41.04478311538696, 42.31490707397461, 43.585031032562256, 44.85438251495361, 46.12373399734497, 47.397011518478394, 48.670289039611816, 49.93879508972168, 51.20730113983154, 52.47592067718506, 53.744540214538574, 55.015454053878784, 56.286367893218994, 57.56394863128662, 58.84152936935425, 60.1234917640686, 61.40545415878296, 62.68064832687378, 63.9558424949646, 65.22906589508057, 66.50228929519653, 67.77232027053833, 69.04235124588013, 70.31863737106323, 71.59492349624634, 72.86801815032959, 74.14111280441284, 75.41241097450256, 76.68370914459229, 77.95564079284668, 79.22757244110107, 80.49618721008301, 81.76480197906494, 83.03740859031677, 84.3100152015686, 85.59299206733704, 86.87596893310547, 88.16637063026428, 89.4567723274231, 90.73292684555054, 92.00908136367798, 93.28642392158508, 94.56376647949219, 95.84062314033508, 97.11747980117798, 98.40122985839844, 99.6849799156189, 100.97485542297363, 102.26473093032837, 103.54065442085266, 104.81657791137695, 106.0939519405365, 107.37132596969604, 108.64965224266052, 109.927978515625, 111.20244812965393, 112.47691774368286, 113.75467300415039, 115.03242826461792, 116.30822420120239, 117.58402013778687, 118.86159420013428, 120.13916826248169, 121.41312098503113, 122.68707370758057, 123.96696209907532, 125.24685049057007, 126.54789781570435, 127.84894514083862, 129.13758969306946, 130.4262342453003, 131.69019556045532, 132.95415687561035, 134.21667170524597, 135.4791865348816, 136.746652841568, 138.0141191482544, 139.2229881286621, 140.43185710906982, 141.63899445533752, 142.84613180160522, 144.05610537528992, 145.2660789489746, 146.54144930839539, 147.81681966781616, 149.07715153694153, 150.3374834060669, 151.60007739067078, 152.86267137527466, 154.1386115550995, 155.41455173492432, 156.6845715045929, 157.95459127426147, 159.23151516914368, 160.50843906402588, 161.78170585632324, 163.0549726486206, 164.33414030075073, 165.61330795288086, 166.89116430282593, 168.169020652771, 169.4490249156952, 170.72902917861938, 172.01044344902039, 173.2918577194214, 174.561199426651, 175.83054113388062, 177.09927773475647, 178.36801433563232, 179.6403489112854, 180.91268348693848, 182.18415355682373, 183.45562362670898, 184.7234857082367, 185.9913477897644, 187.26474595069885, 188.5381441116333, 189.80322313308716, 191.06830215454102, 192.32770538330078, 193.58710861206055, 194.84577465057373, 196.1044406890869, 197.36397647857666, 198.6235122680664, 199.89202070236206, 201.16052913665771, 202.43243288993835, 203.704336643219, 204.96456265449524, 206.22478866577148, 207.48556447029114, 208.7463402748108, 210.01268100738525, 211.27902173995972, 212.5416886806488, 213.8043556213379, 215.0737063884735, 216.34305715560913, 217.61006021499634, 218.87706327438354, 220.14295315742493, 221.4088430404663, 222.6786503791809, 223.9484577178955, 225.2213191986084, 226.4941806793213, 227.76675605773926, 229.03933143615723, 230.30935788154602, 231.57938432693481, 232.85276770591736, 234.1261510848999, 235.40102577209473, 236.67590045928955, 237.94946098327637, 239.22302150726318, 240.49332094192505, 241.7636203765869, 243.0297040939331, 244.2957878112793, 245.56594514846802, 246.83610248565674, 248.0486397743225, 249.26117706298828, 250.463294506073, 251.66541194915771, 252.86378526687622, 254.06215858459473, 256.0106723308563, 257.9591860771179]
[25.9525, 25.9525, 30.84, 30.84, 35.5425, 35.5425, 38.87, 38.87, 42.0475, 42.0475, 44.115, 44.115, 46.1425, 46.1425, 47.3925, 47.3925, 47.595, 47.595, 49.24, 49.24, 49.9775, 49.9775, 50.7, 50.7, 51.1025, 51.1025, 52.365, 52.365, 52.8825, 52.8825, 53.2475, 53.2475, 54.84, 54.84, 55.2825, 55.2825, 55.4375, 55.4375, 55.8775, 55.8775, 56.4625, 56.4625, 56.5175, 56.5175, 57.29, 57.29, 57.525, 57.525, 57.96, 57.96, 58.18, 58.18, 59.0375, 59.0375, 59.345, 59.345, 59.2725, 59.2725, 59.4275, 59.4275, 59.0975, 59.0975, 59.52, 59.52, 59.615, 59.615, 60.1125, 60.1125, 60.4075, 60.4075, 60.2975, 60.2975, 59.6475, 59.6475, 60.7625, 60.7625, 61.1775, 61.1775, 60.905, 60.905, 61.2375, 61.2375, 61.5375, 61.5375, 61.415, 61.415, 61.5475, 61.5475, 61.255, 61.255, 60.96, 60.96, 61.2925, 61.2925, 61.2675, 61.2675, 61.4825, 61.4825, 61.0625, 61.0625, 61.39, 61.39, 60.685, 60.685, 61.5625, 61.5625, 61.73, 61.73, 61.535, 61.535, 61.245, 61.245, 61.0825, 61.0825, 61.4175, 61.4175, 60.96, 60.96, 61.3975, 61.3975, 61.125, 61.125, 61.7475, 61.7475, 61.2, 61.2, 61.19, 61.19, 61.565, 61.565, 61.425, 61.425, 61.4475, 61.4475, 61.8075, 61.8075, 61.2375, 61.2375, 61.59, 61.59, 61.3975, 61.3975, 61.5725, 61.5725, 61.4475, 61.4475, 61.7025, 61.7025, 61.8475, 61.8475, 61.7275, 61.7275, 61.3925, 61.3925, 61.58, 61.58, 61.03, 61.03, 61.6575, 61.6575, 61.4125, 61.4125, 61.66, 61.66, 61.805, 61.805, 61.865, 61.865, 61.5725, 61.5725, 61.6575, 61.6575, 61.0375, 61.0375, 61.4925, 61.4925, 61.1925, 61.1925, 61.04, 61.04, 61.5475, 61.5475, 61.1225, 61.1225, 61.565, 61.565, 61.6425, 61.6425, 61.3575, 61.3575, 61.3875, 61.3875, 61.185, 61.185, 60.755, 60.755, 61.215, 61.215, 60.92, 60.92, 60.79, 60.79]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.093, Test loss: 4.309, Test accuracy: 42.52
Average accuracy final 10 rounds: 42.1585 

2757.988128900528
[1.4996459484100342, 2.9992918968200684, 4.007063627243042, 5.014835357666016, 6.021334886550903, 7.027834415435791, 8.036618947982788, 9.045403480529785, 10.050469875335693, 11.055536270141602, 12.0567147731781, 13.0578932762146, 14.072183609008789, 15.086473941802979, 16.095290184020996, 17.104106426239014, 18.11314821243286, 19.12218999862671, 20.13716959953308, 21.152149200439453, 22.155126094818115, 23.158102989196777, 24.1610164642334, 25.16392993927002, 26.170743227005005, 27.17755651473999, 28.186712503433228, 29.195868492126465, 30.203938722610474, 31.212008953094482, 32.221548318862915, 33.23108768463135, 34.23479509353638, 35.238502502441406, 36.244593143463135, 37.25068378448486, 38.256269216537476, 39.26185464859009, 40.27071213722229, 41.27956962585449, 42.289762020111084, 43.299954414367676, 44.3010892868042, 45.30222415924072, 46.3082389831543, 47.31425380706787, 48.322279930114746, 49.33030605316162, 50.336363077163696, 51.34242010116577, 52.350043296813965, 53.35766649246216, 54.36434555053711, 55.37102460861206, 56.36948871612549, 57.367952823638916, 58.37611198425293, 59.38427114486694, 60.389503955841064, 61.394736766815186, 62.401655197143555, 63.408573627471924, 64.41390442848206, 65.41923522949219, 66.42482089996338, 67.43040657043457, 68.42692828178406, 69.42344999313354, 70.429936170578, 71.43642234802246, 72.44115114212036, 73.44587993621826, 74.45074963569641, 75.45561933517456, 76.46069622039795, 77.46577310562134, 78.46705913543701, 79.46834516525269, 80.46934199333191, 81.47033882141113, 82.49660229682922, 83.52286577224731, 84.52715134620667, 85.53143692016602, 86.53882431983948, 87.54621171951294, 88.72014570236206, 89.89407968521118, 91.07137894630432, 92.24867820739746, 93.42046165466309, 94.59224510192871, 95.76598763465881, 96.93973016738892, 98.11465191841125, 99.2895736694336, 100.45891427993774, 101.6282548904419, 102.78648614883423, 103.94471740722656, 105.11848759651184, 106.29225778579712, 107.47051167488098, 108.64876556396484, 109.75674223899841, 110.86471891403198, 111.97703337669373, 113.08934783935547, 114.2060399055481, 115.32273197174072, 116.48344826698303, 117.64416456222534, 118.76217436790466, 119.88018417358398, 121.00538325309753, 122.13058233261108, 123.25780177116394, 124.3850212097168, 125.54531192779541, 126.70560264587402, 127.87015676498413, 129.03471088409424, 130.20933294296265, 131.38395500183105, 132.55253624916077, 133.72111749649048, 134.89136290550232, 136.06160831451416, 137.2346043586731, 138.40760040283203, 139.5706503391266, 140.73370027542114, 141.90149092674255, 143.06928157806396, 144.23946928977966, 145.40965700149536, 146.58650851249695, 147.76336002349854, 148.93513870239258, 150.10691738128662, 151.27944326400757, 152.45196914672852, 153.62266540527344, 154.79336166381836, 155.96019196510315, 157.12702226638794, 158.29970574378967, 159.4723892211914, 160.63250160217285, 161.7926139831543, 162.95064640045166, 164.10867881774902, 165.13286638259888, 166.15705394744873, 167.30863571166992, 168.4602174758911, 169.6152527332306, 170.77028799057007, 171.92711782455444, 173.08394765853882, 174.2345519065857, 175.38515615463257, 176.53992414474487, 177.69469213485718, 178.84989476203918, 180.0050973892212, 181.1743621826172, 182.34362697601318, 183.50000977516174, 184.6563925743103, 185.813884973526, 186.9713773727417, 188.13162183761597, 189.29186630249023, 190.44924473762512, 191.60662317276, 192.7626450061798, 193.9186668395996, 195.08758807182312, 196.25650930404663, 197.43350839614868, 198.61050748825073, 199.78009700775146, 200.9496865272522, 202.1128876209259, 203.2760887145996, 204.4380967617035, 205.60010480880737, 206.7667670249939, 207.93342924118042, 209.10144972801208, 210.26947021484375, 211.44395971298218, 212.6184492111206, 213.78482460975647, 214.95120000839233, 216.11590719223022, 217.28061437606812, 218.45141005516052, 219.62220573425293, 221.83999109268188, 224.05777645111084]
[23.685, 23.685, 28.99, 28.99, 32.9175, 32.9175, 34.185, 34.185, 35.675, 35.675, 36.11, 36.11, 37.15, 37.15, 38.6125, 38.6125, 38.74, 38.74, 38.5425, 38.5425, 38.895, 38.895, 39.4, 39.4, 40.4925, 40.4925, 40.6225, 40.6225, 40.9925, 40.9925, 40.425, 40.425, 40.6375, 40.6375, 40.605, 40.605, 40.815, 40.815, 41.5125, 41.5125, 41.4475, 41.4475, 41.3775, 41.3775, 41.4325, 41.4325, 41.8075, 41.8075, 42.165, 42.165, 41.85, 41.85, 41.95, 41.95, 41.9225, 41.9225, 42.01, 42.01, 41.5375, 41.5375, 41.795, 41.795, 41.9925, 41.9925, 42.0825, 42.0825, 41.84, 41.84, 41.8325, 41.8325, 41.39, 41.39, 41.7875, 41.7875, 42.035, 42.035, 41.7675, 41.7675, 41.5975, 41.5975, 41.535, 41.535, 41.54, 41.54, 41.7375, 41.7375, 41.19, 41.19, 41.805, 41.805, 41.8525, 41.8525, 42.1925, 42.1925, 41.755, 41.755, 41.925, 41.925, 41.595, 41.595, 41.3925, 41.3925, 41.4575, 41.4575, 41.5325, 41.5325, 41.7, 41.7, 41.4325, 41.4325, 41.825, 41.825, 41.955, 41.955, 41.8775, 41.8775, 41.9825, 41.9825, 41.7075, 41.7075, 41.8375, 41.8375, 42.04, 42.04, 41.775, 41.775, 42.035, 42.035, 42.3075, 42.3075, 42.4625, 42.4625, 42.1275, 42.1275, 42.1975, 42.1975, 41.995, 41.995, 42.065, 42.065, 42.2275, 42.2275, 42.0475, 42.0475, 41.9975, 41.9975, 42.445, 42.445, 42.4775, 42.4775, 42.1275, 42.1275, 42.24, 42.24, 41.985, 41.985, 42.105, 42.105, 42.135, 42.135, 41.835, 41.835, 41.8625, 41.8625, 42.025, 42.025, 42.005, 42.005, 42.1425, 42.1425, 42.11, 42.11, 42.175, 42.175, 42.485, 42.485, 42.2175, 42.2175, 41.935, 41.935, 42.195, 42.195, 41.9125, 41.9125, 42.0225, 42.0225, 42.055, 42.055, 42.0525, 42.0525, 42.275, 42.275, 42.4675, 42.4675, 42.4275, 42.4275, 41.98, 41.98, 42.1975, 42.1975, 42.515, 42.515]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 1.446, Test loss: 1.881, Test accuracy: 29.99
Round   1, Train loss: 1.262, Test loss: 1.864, Test accuracy: 30.65
Round   2, Train loss: 1.178, Test loss: 1.940, Test accuracy: 28.64
Round   3, Train loss: 1.081, Test loss: 1.944, Test accuracy: 29.75
Round   4, Train loss: 1.048, Test loss: 2.022, Test accuracy: 27.95
Round   5, Train loss: 1.004, Test loss: 2.000, Test accuracy: 30.25
Round   6, Train loss: 0.945, Test loss: 2.042, Test accuracy: 29.00
Round   7, Train loss: 0.878, Test loss: 2.078, Test accuracy: 28.21
Round   8, Train loss: 0.854, Test loss: 2.065, Test accuracy: 28.84
Round   9, Train loss: 0.816, Test loss: 2.058, Test accuracy: 29.17
Round  10, Train loss: 0.761, Test loss: 2.043, Test accuracy: 29.66
Round  11, Train loss: 0.737, Test loss: 2.028, Test accuracy: 30.95
Round  12, Train loss: 0.735, Test loss: 2.030, Test accuracy: 30.14
Round  13, Train loss: 0.673, Test loss: 2.025, Test accuracy: 30.76
Round  14, Train loss: 0.671, Test loss: 2.009, Test accuracy: 32.62
Round  15, Train loss: 0.603, Test loss: 2.010, Test accuracy: 31.89
Round  16, Train loss: 0.587, Test loss: 2.001, Test accuracy: 32.29
Round  17, Train loss: 0.595, Test loss: 1.993, Test accuracy: 33.29
Round  18, Train loss: 0.581, Test loss: 1.983, Test accuracy: 34.18
Round  19, Train loss: 0.537, Test loss: 1.978, Test accuracy: 34.34
Round  20, Train loss: 0.560, Test loss: 2.025, Test accuracy: 32.77
Round  21, Train loss: 0.500, Test loss: 2.014, Test accuracy: 33.37
Round  22, Train loss: 0.486, Test loss: 2.010, Test accuracy: 33.75
Round  23, Train loss: 0.445, Test loss: 1.993, Test accuracy: 34.97
Round  24, Train loss: 0.481, Test loss: 1.986, Test accuracy: 35.62
Round  25, Train loss: 0.422, Test loss: 1.975, Test accuracy: 36.11
Round  26, Train loss: 0.379, Test loss: 1.972, Test accuracy: 36.67
Round  27, Train loss: 0.419, Test loss: 1.965, Test accuracy: 37.30
Round  28, Train loss: 0.413, Test loss: 1.961, Test accuracy: 37.18
Round  29, Train loss: 0.417, Test loss: 1.951, Test accuracy: 37.91
Round  30, Train loss: 0.374, Test loss: 1.934, Test accuracy: 39.05
Round  31, Train loss: 0.341, Test loss: 1.925, Test accuracy: 39.37
Round  32, Train loss: 0.323, Test loss: 1.934, Test accuracy: 38.62
Round  33, Train loss: 0.334, Test loss: 1.921, Test accuracy: 39.27
Round  34, Train loss: 0.351, Test loss: 1.919, Test accuracy: 38.96
Round  35, Train loss: 0.301, Test loss: 1.901, Test accuracy: 39.62
Round  36, Train loss: 0.347, Test loss: 1.898, Test accuracy: 39.45
Round  37, Train loss: 0.278, Test loss: 1.886, Test accuracy: 40.22
Round  38, Train loss: 0.304, Test loss: 1.885, Test accuracy: 40.02
Round  39, Train loss: 0.273, Test loss: 1.873, Test accuracy: 40.50
Round  40, Train loss: 0.289, Test loss: 1.869, Test accuracy: 41.10
Round  41, Train loss: 0.265, Test loss: 1.856, Test accuracy: 41.53
Round  42, Train loss: 0.282, Test loss: 1.853, Test accuracy: 41.30
Round  43, Train loss: 0.249, Test loss: 1.847, Test accuracy: 41.58
Round  44, Train loss: 0.253, Test loss: 1.852, Test accuracy: 40.77
Round  45, Train loss: 0.255, Test loss: 1.844, Test accuracy: 41.15
Round  46, Train loss: 0.241, Test loss: 1.835, Test accuracy: 41.63
Round  47, Train loss: 0.245, Test loss: 1.828, Test accuracy: 41.92
Round  48, Train loss: 0.248, Test loss: 1.821, Test accuracy: 43.28
Round  49, Train loss: 0.219, Test loss: 1.827, Test accuracy: 42.41
Round  50, Train loss: 0.229, Test loss: 1.820, Test accuracy: 42.97
Round  51, Train loss: 0.216, Test loss: 1.813, Test accuracy: 43.45
Round  52, Train loss: 0.196, Test loss: 1.803, Test accuracy: 42.90
Round  53, Train loss: 0.233, Test loss: 1.798, Test accuracy: 42.78
Round  54, Train loss: 0.189, Test loss: 1.781, Test accuracy: 43.80
Round  55, Train loss: 0.203, Test loss: 1.785, Test accuracy: 43.55
Round  56, Train loss: 0.199, Test loss: 1.786, Test accuracy: 43.80
Round  57, Train loss: 0.201, Test loss: 1.785, Test accuracy: 43.13
Round  58, Train loss: 0.197, Test loss: 1.772, Test accuracy: 43.44
Round  59, Train loss: 0.206, Test loss: 1.774, Test accuracy: 43.74
Round  60, Train loss: 0.215, Test loss: 1.770, Test accuracy: 43.72
Round  61, Train loss: 0.190, Test loss: 1.767, Test accuracy: 43.84
Round  62, Train loss: 0.186, Test loss: 1.757, Test accuracy: 44.26
Round  63, Train loss: 0.182, Test loss: 1.757, Test accuracy: 44.01
Round  64, Train loss: 0.170, Test loss: 1.751, Test accuracy: 44.48
Round  65, Train loss: 0.172, Test loss: 1.745, Test accuracy: 44.38
Round  66, Train loss: 0.171, Test loss: 1.737, Test accuracy: 44.95
Round  67, Train loss: 0.181, Test loss: 1.735, Test accuracy: 44.47
Round  68, Train loss: 0.158, Test loss: 1.733, Test accuracy: 44.29
Round  69, Train loss: 0.170, Test loss: 1.714, Test accuracy: 45.40
Round  70, Train loss: 0.168, Test loss: 1.726, Test accuracy: 44.64
Round  71, Train loss: 0.156, Test loss: 1.723, Test accuracy: 44.97
Round  72, Train loss: 0.170, Test loss: 1.720, Test accuracy: 45.18
Round  73, Train loss: 0.159, Test loss: 1.719, Test accuracy: 44.67
Round  74, Train loss: 0.181, Test loss: 1.718, Test accuracy: 44.97
Round  75, Train loss: 0.147, Test loss: 1.718, Test accuracy: 44.48
Round  76, Train loss: 0.170, Test loss: 1.709, Test accuracy: 45.03
Round  77, Train loss: 0.143, Test loss: 1.706, Test accuracy: 44.74
Round  78, Train loss: 0.140, Test loss: 1.711, Test accuracy: 44.20
Round  79, Train loss: 0.145, Test loss: 1.708, Test accuracy: 44.25
Round  80, Train loss: 0.145, Test loss: 1.708, Test accuracy: 44.56
Round  81, Train loss: 0.138, Test loss: 1.697, Test accuracy: 45.43
Round  82, Train loss: 0.157, Test loss: 1.695, Test accuracy: 44.99
Round  83, Train loss: 0.143, Test loss: 1.700, Test accuracy: 45.00
Round  84, Train loss: 0.158, Test loss: 1.693, Test accuracy: 45.43
Round  85, Train loss: 0.136, Test loss: 1.678, Test accuracy: 46.01
Round  86, Train loss: 0.151, Test loss: 1.685, Test accuracy: 45.42
Round  87, Train loss: 0.134, Test loss: 1.687, Test accuracy: 45.05
Round  88, Train loss: 0.139, Test loss: 1.688, Test accuracy: 44.88
Round  89, Train loss: 0.131, Test loss: 1.667, Test accuracy: 45.89
Round  90, Train loss: 0.135, Test loss: 1.677, Test accuracy: 45.39
Round  91, Train loss: 0.134, Test loss: 1.673, Test accuracy: 45.47
Round  92, Train loss: 0.131, Test loss: 1.665, Test accuracy: 45.86
Round  93, Train loss: 0.129, Test loss: 1.664, Test accuracy: 46.01
Round  94, Train loss: 0.137, Test loss: 1.673, Test accuracy: 45.38
Round  95, Train loss: 0.137, Test loss: 1.665, Test accuracy: 45.82
Round  96, Train loss: 0.130, Test loss: 1.654, Test accuracy: 46.16
Round  97, Train loss: 0.141, Test loss: 1.663, Test accuracy: 46.00
Round  98, Train loss: 0.125, Test loss: 1.664, Test accuracy: 45.53
Round  99, Train loss: 0.129, Test loss: 1.662, Test accuracy: 45.57
Final Round, Train loss: 0.130, Test loss: 1.651, Test accuracy: 45.62
Average accuracy final 10 rounds: 45.71975
6184.241059780121
[]
[29.99, 30.6475, 28.6425, 29.7525, 27.9475, 30.2525, 29.0025, 28.21, 28.835, 29.1675, 29.665, 30.9475, 30.14, 30.7575, 32.62, 31.8875, 32.29, 33.2925, 34.1825, 34.3425, 32.77, 33.3725, 33.75, 34.97, 35.62, 36.1125, 36.675, 37.2975, 37.1825, 37.91, 39.0475, 39.365, 38.6175, 39.2675, 38.9625, 39.615, 39.4475, 40.22, 40.02, 40.4975, 41.105, 41.5275, 41.305, 41.575, 40.775, 41.15, 41.63, 41.92, 43.2775, 42.4125, 42.965, 43.4525, 42.895, 42.7825, 43.805, 43.5525, 43.8025, 43.1325, 43.44, 43.7375, 43.7225, 43.8425, 44.2575, 44.0125, 44.4825, 44.38, 44.945, 44.4725, 44.29, 45.4025, 44.64, 44.965, 45.1825, 44.6725, 44.9725, 44.475, 45.035, 44.7425, 44.205, 44.2475, 44.5575, 45.4325, 44.9875, 45.0, 45.4325, 46.01, 45.425, 45.045, 44.875, 45.8875, 45.3925, 45.47, 45.8625, 46.01, 45.38, 45.8225, 46.1575, 46.0, 45.5325, 45.57, 45.625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.993, Test loss: 1.178, Test accuracy: 60.17
Average accuracy final 10 rounds: 56.664
Average global accuracy final 10 rounds: 56.664
4485.145489454269
[]
[20.8025, 26.6575, 33.56, 38.18, 40.1175, 41.4325, 42.8675, 41.89, 42.615, 43.415, 43.98, 43.7175, 45.9075, 46.4025, 46.655, 46.195, 47.0275, 47.8775, 48.55, 49.0025, 49.3775, 50.0675, 49.345, 50.0625, 50.1775, 50.33, 51.2425, 52.0, 52.2025, 52.58, 52.185, 52.275, 51.965, 51.6225, 52.0925, 52.1725, 52.365, 52.555, 52.585, 52.2575, 52.3875, 52.74, 52.6375, 53.075, 52.7775, 52.84, 53.2225, 53.735, 54.33, 53.95, 53.7275, 54.3075, 54.245, 54.2675, 54.3025, 54.1375, 53.895, 54.25, 54.49, 54.0125, 54.295, 54.29, 54.57, 54.6875, 54.5625, 54.845, 54.82, 55.17, 55.05, 55.1425, 55.28, 55.2375, 55.3725, 55.43, 55.73, 55.5475, 55.1525, 55.29, 55.8625, 55.8125, 55.92, 56.035, 56.11, 56.0975, 55.965, 56.005, 56.035, 56.2125, 56.1225, 56.5875, 56.93, 57.0625, 56.935, 56.485, 56.4325, 56.54, 56.4325, 56.345, 56.445, 57.0325, 60.1725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 504, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54734 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 50211 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53965 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2181, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 55034 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.060, Test loss: 1.114, Test accuracy: 78.91
Final Round, Global train loss: 0.060, Global test loss: 2.076, Global test accuracy: 29.28
Average accuracy final 10 rounds: 78.48499999999999 

Average global accuracy final 10 rounds: 27.920833333333338 

1643.735722541809
[1.4096424579620361, 2.8192849159240723, 4.005798101425171, 5.1923112869262695, 6.379044055938721, 7.565776824951172, 8.743843078613281, 9.92190933227539, 11.110597372055054, 12.299285411834717, 13.486210346221924, 14.67313528060913, 15.857364416122437, 17.041593551635742, 18.220637559890747, 19.399681568145752, 20.578507900238037, 21.757334232330322, 22.939836502075195, 24.12233877182007, 25.300814628601074, 26.47929048538208, 27.658336400985718, 28.837382316589355, 29.971938133239746, 31.106493949890137, 32.36559581756592, 33.6246976852417, 34.825522899627686, 36.02634811401367, 37.067726612091064, 38.10910511016846, 39.14888858795166, 40.18867206573486, 41.22989201545715, 42.27111196517944, 43.30901503562927, 44.3469181060791, 45.38764500617981, 46.42837190628052, 47.47219491004944, 48.51601791381836, 49.55942177772522, 50.60282564163208, 51.652257204055786, 52.70168876647949, 53.73615860939026, 54.770628452301025, 55.81008815765381, 56.84954786300659, 57.88376069068909, 58.91797351837158, 59.96548080444336, 61.01298809051514, 62.055633783340454, 63.09827947616577, 64.14656639099121, 65.19485330581665, 66.23749661445618, 67.2801399230957, 68.4843213558197, 69.6885027885437, 70.90120601654053, 72.11390924453735, 73.32253885269165, 74.53116846084595, 75.73731803894043, 76.94346761703491, 78.15618085861206, 79.36889410018921, 80.57274317741394, 81.77659225463867, 82.98100185394287, 84.18541145324707, 85.38320207595825, 86.58099269866943, 87.6725697517395, 88.76414680480957, 89.81305050849915, 90.86195421218872, 91.91047096252441, 92.95898771286011, 94.00849866867065, 95.0580096244812, 96.09963417053223, 97.14125871658325, 98.1841983795166, 99.22713804244995, 100.26561069488525, 101.30408334732056, 102.34664416313171, 103.38920497894287, 104.42286801338196, 105.45653104782104, 106.49242234230042, 107.52831363677979, 108.57411789894104, 109.6199221611023, 110.66209483146667, 111.70426750183105, 112.75488662719727, 113.80550575256348, 114.84495377540588, 115.88440179824829, 116.92508172988892, 117.96576166152954, 119.00422549247742, 120.0426893234253, 121.08197259902954, 122.12125587463379, 123.17131781578064, 124.22137975692749, 125.26079607009888, 126.30021238327026, 127.34980297088623, 128.3993935585022, 129.43468809127808, 130.46998262405396, 131.5067434310913, 132.54350423812866, 133.5887942314148, 134.63408422470093, 135.6715223789215, 136.7089605331421, 137.75424695014954, 138.79953336715698, 139.83807969093323, 140.87662601470947, 141.92246437072754, 142.9683027267456, 144.00406002998352, 145.03981733322144, 146.08047008514404, 147.12112283706665, 148.16421175003052, 149.20730066299438, 150.24851155281067, 151.28972244262695, 152.34756779670715, 153.40541315078735, 154.4544975757599, 155.50358200073242, 156.56504201889038, 157.62650203704834, 158.67708730697632, 159.7276725769043, 160.7830364704132, 161.83840036392212, 162.8981592655182, 163.95791816711426, 165.00608158111572, 166.0542449951172, 167.11080598831177, 168.16736698150635, 169.21403574943542, 170.2607045173645, 171.3203318119049, 172.3799591064453, 177.7845058441162, 183.1890525817871, 184.23324060440063, 185.27742862701416, 186.32243394851685, 187.36743927001953, 188.40284967422485, 189.43826007843018, 190.4788818359375, 191.51950359344482, 192.54929852485657, 193.5790934562683, 194.6171498298645, 195.6552062034607, 196.67416095733643, 197.69311571121216, 198.71818232536316, 199.74324893951416, 200.77230668067932, 201.80136442184448, 202.83377528190613, 203.86618614196777, 204.90629696846008, 205.9464077949524, 206.9768168926239, 208.0072259902954, 209.04089307785034, 210.07456016540527, 211.105366230011, 212.1361722946167, 213.17325973510742, 214.21034717559814, 215.23805928230286, 216.26577138900757, 217.29660654067993, 218.3274416923523, 219.3580505847931, 220.3886594772339, 221.42114973068237, 222.45363998413086, 223.48801946640015, 224.52239894866943, 226.5773012638092, 228.63220357894897]
[21.908333333333335, 21.908333333333335, 33.38333333333333, 33.38333333333333, 47.75, 47.75, 56.625, 56.625, 59.141666666666666, 59.141666666666666, 59.416666666666664, 59.416666666666664, 63.61666666666667, 63.61666666666667, 63.891666666666666, 63.891666666666666, 66.90833333333333, 66.90833333333333, 68.975, 68.975, 69.65, 69.65, 71.94166666666666, 71.94166666666666, 72.3, 72.3, 73.275, 73.275, 73.625, 73.625, 74.13333333333334, 74.13333333333334, 74.375, 74.375, 74.76666666666667, 74.76666666666667, 74.38333333333334, 74.38333333333334, 74.51666666666667, 74.51666666666667, 74.55, 74.55, 74.70833333333333, 74.70833333333333, 74.86666666666666, 74.86666666666666, 75.20833333333333, 75.20833333333333, 75.40833333333333, 75.40833333333333, 75.71666666666667, 75.71666666666667, 75.36666666666666, 75.36666666666666, 75.53333333333333, 75.53333333333333, 76.19166666666666, 76.19166666666666, 75.83333333333333, 75.83333333333333, 76.225, 76.225, 76.26666666666667, 76.26666666666667, 76.41666666666667, 76.41666666666667, 76.45833333333333, 76.45833333333333, 76.51666666666667, 76.51666666666667, 76.76666666666667, 76.76666666666667, 76.9, 76.9, 77.48333333333333, 77.48333333333333, 77.45, 77.45, 77.28333333333333, 77.28333333333333, 77.65, 77.65, 77.46666666666667, 77.46666666666667, 77.46666666666667, 77.46666666666667, 77.08333333333333, 77.08333333333333, 77.175, 77.175, 77.3, 77.3, 77.575, 77.575, 77.6, 77.6, 78.05, 78.05, 77.89166666666667, 77.89166666666667, 77.59166666666667, 77.59166666666667, 77.33333333333333, 77.33333333333333, 77.35, 77.35, 77.71666666666667, 77.71666666666667, 77.75833333333334, 77.75833333333334, 77.8, 77.8, 77.99166666666666, 77.99166666666666, 77.54166666666667, 77.54166666666667, 77.53333333333333, 77.53333333333333, 77.85, 77.85, 77.55833333333334, 77.55833333333334, 77.55833333333334, 77.55833333333334, 77.88333333333334, 77.88333333333334, 78.00833333333334, 78.00833333333334, 78.86666666666666, 78.86666666666666, 78.33333333333333, 78.33333333333333, 78.34166666666667, 78.34166666666667, 78.28333333333333, 78.28333333333333, 78.35833333333333, 78.35833333333333, 78.2, 78.2, 78.23333333333333, 78.23333333333333, 78.49166666666666, 78.49166666666666, 78.5, 78.5, 78.54166666666667, 78.54166666666667, 78.525, 78.525, 78.6, 78.6, 78.34166666666667, 78.34166666666667, 78.69166666666666, 78.69166666666666, 78.33333333333333, 78.33333333333333, 77.98333333333333, 77.98333333333333, 78.15, 78.15, 77.96666666666667, 77.96666666666667, 77.975, 77.975, 78.2, 78.2, 78.30833333333334, 78.30833333333334, 78.43333333333334, 78.43333333333334, 78.59166666666667, 78.59166666666667, 78.33333333333333, 78.33333333333333, 77.83333333333333, 77.83333333333333, 78.15, 78.15, 78.125, 78.125, 78.28333333333333, 78.28333333333333, 77.99166666666666, 77.99166666666666, 78.34166666666667, 78.34166666666667, 78.40833333333333, 78.40833333333333, 78.8, 78.8, 78.53333333333333, 78.53333333333333, 78.86666666666666, 78.86666666666666, 78.675, 78.675, 78.825, 78.825, 78.90833333333333, 78.90833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.338, Test loss: 1.651, Test accuracy: 62.15
Final Round, Global train loss: 0.338, Global test loss: 1.099, Global test accuracy: 68.89
Average accuracy final 10 rounds: 63.23075 

Average global accuracy final 10 rounds: 69.11775 

3719.501929998398
[1.3907854557037354, 2.7815709114074707, 3.9590554237365723, 5.136539936065674, 6.321399688720703, 7.506259441375732, 8.689545392990112, 9.872831344604492, 11.059006452560425, 12.245181560516357, 13.434542655944824, 14.623903751373291, 15.81161642074585, 16.999329090118408, 18.183033227920532, 19.366737365722656, 20.553630113601685, 21.740522861480713, 22.923227071762085, 24.105931282043457, 25.29050588607788, 26.475080490112305, 27.660365343093872, 28.84565019607544, 30.027324199676514, 31.208998203277588, 32.391993284225464, 33.57498836517334, 34.75338554382324, 35.931782722473145, 37.12093734741211, 38.310091972351074, 39.49545121192932, 40.68081045150757, 41.86489748954773, 43.04898452758789, 44.06516790390015, 45.0813512802124, 46.097209453582764, 47.113067626953125, 48.1282274723053, 49.14338731765747, 50.161465644836426, 51.17954397201538, 52.19631481170654, 53.213085651397705, 54.230019092559814, 55.246952533721924, 56.2644579410553, 57.28196334838867, 58.298269748687744, 59.314576148986816, 60.332802295684814, 61.35102844238281, 62.37112736701965, 63.391226291656494, 64.40858578681946, 65.42594528198242, 66.44491648674011, 67.4638876914978, 68.48271656036377, 69.50154542922974, 70.51699042320251, 71.5324354171753, 72.5462327003479, 73.56002998352051, 74.57990980148315, 75.5997896194458, 76.61566972732544, 77.63154983520508, 78.64627742767334, 79.6610050201416, 80.68000221252441, 81.69899940490723, 82.71492886543274, 83.73085832595825, 84.75270891189575, 85.77455949783325, 86.78851342201233, 87.8024673461914, 88.81853914260864, 89.83461093902588, 90.84974193572998, 91.86487293243408, 92.87901020050049, 93.8931474685669, 94.90555357933044, 95.917959690094, 96.93396806716919, 97.94997644424438, 98.96752381324768, 99.98507118225098, 101.0013165473938, 102.01756191253662, 103.03594732284546, 104.0543327331543, 105.07051730155945, 106.0867018699646, 107.10361409187317, 108.12052631378174, 109.13743948936462, 110.15435266494751, 111.1716685295105, 112.18898439407349, 113.20526599884033, 114.22154760360718, 115.22934770584106, 116.23714780807495, 117.24541020393372, 118.25367259979248, 119.25868105888367, 120.26368951797485, 121.27324104309082, 122.28279256820679, 123.29411911964417, 124.30544567108154, 125.32227659225464, 126.33910751342773, 127.35349202156067, 128.3678765296936, 129.3813066482544, 130.39473676681519, 131.4106686115265, 132.4266004562378, 133.44148898124695, 134.4563775062561, 135.47676181793213, 136.49714612960815, 137.51771092414856, 138.53827571868896, 139.55301117897034, 140.5677466392517, 141.58279514312744, 142.59784364700317, 143.60942435264587, 144.62100505828857, 145.63530206680298, 146.64959907531738, 147.6658797264099, 148.68216037750244, 149.69507002830505, 150.70797967910767, 151.72776007652283, 152.747540473938, 153.75928282737732, 154.77102518081665, 155.78760981559753, 156.80419445037842, 157.82001066207886, 158.8358268737793, 159.8493504524231, 160.8628740310669, 161.87606263160706, 162.88925123214722, 163.90476202964783, 164.92027282714844, 165.93907928466797, 166.9578857421875, 167.97177386283875, 168.98566198349, 170.00086426734924, 171.0160665512085, 172.02915954589844, 173.04225254058838, 174.05828189849854, 175.0743112564087, 176.08791542053223, 177.10151958465576, 178.11496305465698, 179.1284065246582, 180.13979744911194, 181.15118837356567, 182.16742372512817, 183.18365907669067, 184.19618916511536, 185.20871925354004, 186.22170615196228, 187.23469305038452, 188.24855279922485, 189.26241254806519, 190.27461171150208, 191.28681087493896, 192.29818773269653, 193.3095645904541, 194.32156586647034, 195.33356714248657, 196.34793162345886, 197.36229610443115, 198.37762355804443, 199.39295101165771, 200.409508228302, 201.4260654449463, 202.44069147109985, 203.45531749725342, 204.460599899292, 205.46588230133057, 206.47117519378662, 207.47646808624268, 208.48326420783997, 209.49006032943726, 211.5103018283844, 213.53054332733154]
[24.5725, 24.5725, 28.7175, 28.7175, 31.2925, 31.2925, 35.17, 35.17, 37.1525, 37.1525, 38.6975, 38.6975, 39.105, 39.105, 40.15, 40.15, 41.87, 41.87, 42.3825, 42.3825, 43.6125, 43.6125, 44.165, 44.165, 45.8175, 45.8175, 46.665, 46.665, 47.99, 47.99, 48.9425, 48.9425, 49.4325, 49.4325, 49.75, 49.75, 49.8675, 49.8675, 50.71, 50.71, 51.5525, 51.5525, 51.82, 51.82, 52.875, 52.875, 53.7, 53.7, 53.8725, 53.8725, 54.1575, 54.1575, 54.16, 54.16, 54.1625, 54.1625, 54.4875, 54.4875, 54.9025, 54.9025, 55.0325, 55.0325, 55.845, 55.845, 56.0, 56.0, 55.975, 55.975, 56.365, 56.365, 56.795, 56.795, 56.9925, 56.9925, 57.0825, 57.0825, 57.0275, 57.0275, 57.4325, 57.4325, 57.7675, 57.7675, 57.97, 57.97, 58.17, 58.17, 58.57, 58.57, 58.815, 58.815, 58.9725, 58.9725, 59.345, 59.345, 59.6175, 59.6175, 59.645, 59.645, 59.805, 59.805, 59.9175, 59.9175, 59.93, 59.93, 60.06, 60.06, 60.55, 60.55, 60.6, 60.6, 60.38, 60.38, 60.3225, 60.3225, 60.7575, 60.7575, 60.8675, 60.8675, 60.795, 60.795, 61.08, 61.08, 61.0425, 61.0425, 61.3475, 61.3475, 61.2825, 61.2825, 61.1675, 61.1675, 61.0425, 61.0425, 61.155, 61.155, 61.17, 61.17, 61.4225, 61.4225, 61.5275, 61.5275, 61.72, 61.72, 61.8725, 61.8725, 62.0975, 62.0975, 61.7175, 61.7175, 61.7, 61.7, 61.7475, 61.7475, 61.875, 61.875, 61.885, 61.885, 62.21, 62.21, 62.2025, 62.2025, 62.6425, 62.6425, 62.4875, 62.4875, 62.425, 62.425, 62.71, 62.71, 62.4475, 62.4475, 61.985, 61.985, 62.0875, 62.0875, 62.42, 62.42, 62.9375, 62.9375, 63.0575, 63.0575, 63.1775, 63.1775, 62.8625, 62.8625, 63.085, 63.085, 63.0925, 63.0925, 63.385, 63.385, 63.3875, 63.3875, 63.2325, 63.2325, 63.435, 63.435, 63.225, 63.225, 63.425, 63.425, 62.145, 62.145]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.516, Test loss: 1.289, Test accuracy: 62.73
Average accuracy final 10 rounds: 62.61050000000001 

2691.353810787201
[1.3145081996917725, 2.629016399383545, 3.693326711654663, 4.757637023925781, 5.820624589920044, 6.883612155914307, 7.948674440383911, 9.013736724853516, 10.078823804855347, 11.143910884857178, 12.203550338745117, 13.263189792633057, 14.32628870010376, 15.389387607574463, 16.449488162994385, 17.509588718414307, 18.57675337791443, 19.64391803741455, 20.70456886291504, 21.765219688415527, 22.82656192779541, 23.887904167175293, 24.95186161994934, 26.01581907272339, 27.0753071308136, 28.13479518890381, 29.20031714439392, 30.265839099884033, 31.325196504592896, 32.38455390930176, 33.445061445236206, 34.505568981170654, 35.566640853881836, 36.62771272659302, 37.68974447250366, 38.75177621841431, 39.81450843811035, 40.8772406578064, 41.936487674713135, 42.99573469161987, 44.060351848602295, 45.12496900558472, 46.18259048461914, 47.240211963653564, 48.305758476257324, 49.371304988861084, 50.437251567840576, 51.50319814682007, 52.56637692451477, 53.62955570220947, 54.68964624404907, 55.74973678588867, 56.807790994644165, 57.86584520339966, 58.92843508720398, 59.9910249710083, 61.04995393753052, 62.108882904052734, 63.16916537284851, 64.22944784164429, 65.28834462165833, 66.34724140167236, 67.40934610366821, 68.47145080566406, 69.53184366226196, 70.59223651885986, 71.65576553344727, 72.71929454803467, 73.77877807617188, 74.83826160430908, 75.89629197120667, 76.95432233810425, 78.01688861846924, 79.07945489883423, 80.13892459869385, 81.19839429855347, 82.26174449920654, 83.32509469985962, 84.38526487350464, 85.44543504714966, 86.50335550308228, 87.56127595901489, 88.6209831237793, 89.6806902885437, 90.74411821365356, 91.80754613876343, 92.87180256843567, 93.93605899810791, 94.99521470069885, 96.0543704032898, 97.1292839050293, 98.2041974067688, 99.26539158821106, 100.32658576965332, 101.38967680931091, 102.4527678489685, 103.51044964790344, 104.56813144683838, 105.63054990768433, 106.69296836853027, 107.75058650970459, 108.8082046508789, 109.868004322052, 110.9278039932251, 111.99393582344055, 113.060067653656, 114.12198162078857, 115.18389558792114, 116.24331831932068, 117.30274105072021, 118.36271262168884, 119.42268419265747, 120.48785996437073, 121.55303573608398, 122.60402488708496, 123.65501403808594, 124.70519542694092, 125.7553768157959, 126.80437850952148, 127.85338020324707, 128.9113209247589, 129.96926164627075, 131.02591562271118, 132.0825695991516, 133.13336396217346, 134.1841583251953, 135.24178218841553, 136.29940605163574, 137.34835028648376, 138.3972945213318, 139.4451322555542, 140.4929699897766, 141.5440080165863, 142.595046043396, 143.6488835811615, 144.702721118927, 145.7581057548523, 146.8134903907776, 147.86011004447937, 148.90672969818115, 149.96022391319275, 151.01371812820435, 152.06668400764465, 153.11964988708496, 154.1717939376831, 155.22393798828125, 156.2778136730194, 157.33168935775757, 158.38259840011597, 159.43350744247437, 160.48588705062866, 161.53826665878296, 162.5922338962555, 163.64620113372803, 164.70410823822021, 165.7620153427124, 166.81864619255066, 167.87527704238892, 168.9262936115265, 169.97731018066406, 171.03015232086182, 172.08299446105957, 173.1354260444641, 174.18785762786865, 175.23703980445862, 176.28622198104858, 177.336181640625, 178.38614130020142, 179.43764233589172, 180.48914337158203, 181.53672456741333, 182.58430576324463, 183.63790082931519, 184.69149589538574, 185.74198698997498, 186.7924780845642, 187.85221314430237, 188.91194820404053, 189.96402430534363, 191.01610040664673, 192.06708645820618, 193.11807250976562, 194.16827249526978, 195.21847248077393, 196.2655532360077, 197.31263399124146, 198.3615744113922, 199.41051483154297, 200.4640703201294, 201.51762580871582, 202.5662932395935, 203.6149606704712, 204.66275453567505, 205.7105484008789, 206.7594575881958, 207.8083667755127, 208.85801553726196, 209.90766429901123, 210.9621605873108, 212.01665687561035, 213.89674019813538, 215.7768235206604]
[19.1575, 19.1575, 26.7325, 26.7325, 29.745, 29.745, 31.8025, 31.8025, 34.21, 34.21, 36.38, 36.38, 37.6875, 37.6875, 39.3175, 39.3175, 41.32, 41.32, 42.49, 42.49, 43.1475, 43.1475, 44.2525, 44.2525, 45.7, 45.7, 46.035, 46.035, 45.9375, 45.9375, 47.7475, 47.7475, 48.025, 48.025, 49.005, 49.005, 49.6425, 49.6425, 49.01, 49.01, 49.435, 49.435, 49.05, 49.05, 51.6875, 51.6875, 52.02, 52.02, 51.7525, 51.7525, 52.8925, 52.8925, 54.0425, 54.0425, 54.4025, 54.4025, 54.0975, 54.0975, 54.785, 54.785, 55.0775, 55.0775, 55.71, 55.71, 56.2175, 56.2175, 57.19, 57.19, 56.74, 56.74, 57.1575, 57.1575, 57.6675, 57.6675, 57.075, 57.075, 57.515, 57.515, 57.74, 57.74, 58.2625, 58.2625, 57.54, 57.54, 58.57, 58.57, 59.1525, 59.1525, 59.11, 59.11, 59.3825, 59.3825, 59.5225, 59.5225, 59.5475, 59.5475, 60.415, 60.415, 60.6325, 60.6325, 60.745, 60.745, 60.24, 60.24, 60.06, 60.06, 59.905, 59.905, 59.2525, 59.2525, 60.0175, 60.0175, 60.5825, 60.5825, 60.7225, 60.7225, 60.4525, 60.4525, 60.48, 60.48, 60.945, 60.945, 61.285, 61.285, 61.81, 61.81, 61.88, 61.88, 61.6975, 61.6975, 61.9025, 61.9025, 61.81, 61.81, 61.5875, 61.5875, 61.74, 61.74, 61.6975, 61.6975, 61.74, 61.74, 61.75, 61.75, 61.6975, 61.6975, 61.5925, 61.5925, 62.165, 62.165, 62.1275, 62.1275, 62.215, 62.215, 61.58, 61.58, 61.5875, 61.5875, 62.685, 62.685, 62.5275, 62.5275, 62.7375, 62.7375, 63.0275, 63.0275, 62.23, 62.23, 62.3375, 62.3375, 62.6675, 62.6675, 62.7825, 62.7825, 63.22, 63.22, 62.9725, 62.9725, 63.115, 63.115, 62.7825, 62.7825, 62.38, 62.38, 62.6325, 62.6325, 62.86, 62.86, 62.1925, 62.1925, 62.0375, 62.0375, 62.905, 62.905, 62.905, 62.905, 63.1275, 63.1275, 62.2825, 62.2825, 62.73, 62.73]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.316, Test loss: 1.602, Test accuracy: 62.37
Average accuracy final 10 rounds: 62.126999999999995 

2823.4246776103973
[1.4143154621124268, 2.8286309242248535, 4.001286268234253, 5.173941612243652, 6.3503007888793945, 7.526659965515137, 8.703540086746216, 9.880420207977295, 11.062209367752075, 12.243998527526855, 13.423400402069092, 14.602802276611328, 15.777043104171753, 16.951283931732178, 18.12452220916748, 19.297760486602783, 20.475849866867065, 21.653939247131348, 22.830906629562378, 24.007874011993408, 25.18433928489685, 26.360804557800293, 27.541914701461792, 28.72302484512329, 29.899080276489258, 31.075135707855225, 32.25536322593689, 33.435590744018555, 34.61309814453125, 35.790605545043945, 36.96273446083069, 38.13486337661743, 39.31382632255554, 40.49278926849365, 41.67472863197327, 42.85666799545288, 44.031835079193115, 45.20700216293335, 46.38089156150818, 47.55478096008301, 48.731836557388306, 49.9088921546936, 51.08522653579712, 52.261560916900635, 53.43757343292236, 54.61358594894409, 55.78891706466675, 56.964248180389404, 58.14421343803406, 59.32417869567871, 60.504945516586304, 61.6857123374939, 62.86074948310852, 64.03578662872314, 65.21214747428894, 66.38850831985474, 67.56458330154419, 68.74065828323364, 69.91948342323303, 71.09830856323242, 72.27603960037231, 73.4537706375122, 74.62277030944824, 75.79176998138428, 77.06020092964172, 78.32863187789917, 79.57774353027344, 80.8268551826477, 82.08030009269714, 83.33374500274658, 84.58441686630249, 85.8350887298584, 87.08679127693176, 88.33849382400513, 89.58656287193298, 90.83463191986084, 92.08016538619995, 93.32569885253906, 94.57190442085266, 95.81810998916626, 97.06461000442505, 98.31111001968384, 99.55388140678406, 100.79665279388428, 102.05431580543518, 103.31197881698608, 104.55923581123352, 105.80649280548096, 107.04994082450867, 108.29338884353638, 109.54649424552917, 110.79959964752197, 112.04575943946838, 113.2919192314148, 114.5397961139679, 115.787672996521, 117.03722357749939, 118.28677415847778, 119.53475546836853, 120.78273677825928, 122.02977228164673, 123.27680778503418, 124.52399277687073, 125.77117776870728, 127.0166552066803, 128.26213264465332, 129.50637531280518, 130.75061798095703, 131.9942009449005, 133.237783908844, 134.4833779335022, 135.7289719581604, 136.97407126426697, 138.21917057037354, 139.47222185134888, 140.72527313232422, 141.89882493019104, 143.07237672805786, 144.3235728740692, 145.57476902008057, 146.75703811645508, 147.9393072128296, 149.13104009628296, 150.32277297973633, 151.5118386745453, 152.70090436935425, 153.89881443977356, 155.09672451019287, 156.29376769065857, 157.49081087112427, 158.68216252326965, 159.87351417541504, 161.0632026195526, 162.25289106369019, 163.45067071914673, 164.64845037460327, 165.88118147850037, 167.11391258239746, 168.37229871749878, 169.6306848526001, 170.88717126846313, 172.14365768432617, 173.40349102020264, 174.6633243560791, 175.92462372779846, 177.18592309951782, 178.43984746932983, 179.69377183914185, 180.95191621780396, 182.21006059646606, 183.47187066078186, 184.73368072509766, 185.99800944328308, 187.2623381614685, 188.52215027809143, 189.78196239471436, 191.03783345222473, 192.2937045097351, 193.5569567680359, 194.82020902633667, 196.08067655563354, 197.34114408493042, 198.59805035591125, 199.8549566268921, 201.11336278915405, 202.37176895141602, 203.6320309638977, 204.8922929763794, 206.1472852230072, 207.402277469635, 208.65244817733765, 209.90261888504028, 211.162579536438, 212.4225401878357, 213.67898869514465, 214.9354372024536, 216.18552374839783, 217.43561029434204, 218.68453907966614, 219.93346786499023, 221.1962640285492, 222.45906019210815, 223.7127296924591, 224.96639919281006, 226.21640706062317, 227.46641492843628, 228.72470331192017, 229.98299169540405, 231.2418394088745, 232.50068712234497, 233.75590133666992, 235.01111555099487, 236.25588631629944, 237.500657081604, 238.751690864563, 240.00272464752197, 241.2538673877716, 242.50501012802124, 243.7548394203186, 245.00466871261597, 246.95563769340515, 248.90660667419434]
[28.4625, 28.4625, 32.7475, 32.7475, 36.38, 36.38, 40.08, 40.08, 41.4375, 41.4375, 42.49, 42.49, 44.9075, 44.9075, 46.5025, 46.5025, 47.97, 47.97, 48.8475, 48.8475, 49.5375, 49.5375, 50.4475, 50.4475, 51.4825, 51.4825, 52.255, 52.255, 52.4325, 52.4325, 53.8925, 53.8925, 54.825, 54.825, 56.345, 56.345, 56.2525, 56.2525, 56.3575, 56.3575, 57.2975, 57.2975, 57.74, 57.74, 58.2725, 58.2725, 58.0825, 58.0825, 58.085, 58.085, 58.915, 58.915, 58.885, 58.885, 59.345, 59.345, 59.4925, 59.4925, 59.315, 59.315, 59.7025, 59.7025, 60.2975, 60.2975, 60.2375, 60.2375, 60.2525, 60.2525, 60.375, 60.375, 59.9575, 59.9575, 60.76, 60.76, 60.5975, 60.5975, 60.95, 60.95, 61.1675, 61.1675, 60.74, 60.74, 61.02, 61.02, 60.5, 60.5, 60.7225, 60.7225, 60.275, 60.275, 60.9325, 60.9325, 61.09, 61.09, 61.3275, 61.3275, 61.4725, 61.4725, 61.5525, 61.5525, 61.69, 61.69, 61.8875, 61.8875, 61.3325, 61.3325, 61.5125, 61.5125, 61.86, 61.86, 61.7625, 61.7625, 61.345, 61.345, 61.3175, 61.3175, 61.9125, 61.9125, 61.8475, 61.8475, 61.515, 61.515, 62.0075, 62.0075, 61.8075, 61.8075, 61.74, 61.74, 61.745, 61.745, 61.5875, 61.5875, 61.9675, 61.9675, 61.6025, 61.6025, 61.5525, 61.5525, 61.6475, 61.6475, 62.235, 62.235, 61.7525, 61.7525, 61.7075, 61.7075, 62.1025, 62.1025, 61.9975, 61.9975, 61.72, 61.72, 62.12, 62.12, 62.125, 62.125, 61.55, 61.55, 61.58, 61.58, 61.995, 61.995, 62.1325, 62.1325, 61.915, 61.915, 62.2425, 62.2425, 62.485, 62.485, 62.08, 62.08, 61.95, 61.95, 62.04, 62.04, 61.8575, 61.8575, 61.7975, 61.7975, 62.325, 62.325, 61.9975, 61.9975, 62.175, 62.175, 61.8, 61.8, 62.095, 62.095, 62.2775, 62.2775, 62.405, 62.405, 62.09, 62.09, 61.8725, 61.8725, 62.2325, 62.2325, 62.3675, 62.3675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.090, Test loss: 4.189, Test accuracy: 43.38
Average accuracy final 10 rounds: 42.5405 

2767.777838230133
[1.4346685409545898, 2.8693370819091797, 4.107725381851196, 5.346113681793213, 6.590903997421265, 7.835694313049316, 9.077031373977661, 10.318368434906006, 11.565408706665039, 12.812448978424072, 14.05154037475586, 15.290631771087646, 16.535271406173706, 17.779911041259766, 19.021618366241455, 20.263325691223145, 21.499722003936768, 22.73611831665039, 23.982095956802368, 25.228073596954346, 26.474037170410156, 27.720000743865967, 28.962461233139038, 30.20492172241211, 31.444690227508545, 32.68445873260498, 33.9249541759491, 35.16544961929321, 36.401052474975586, 37.63665533065796, 38.88121223449707, 40.12576913833618, 41.36486291885376, 42.60395669937134, 43.84494924545288, 45.085941791534424, 46.3237943649292, 47.561646938323975, 48.791666746139526, 50.02168655395508, 51.25645875930786, 52.491230964660645, 53.73325037956238, 54.97526979446411, 56.22034978866577, 57.46542978286743, 58.70398163795471, 59.94253349304199, 61.17298746109009, 62.403441429138184, 63.645771503448486, 64.88810157775879, 66.14643287658691, 67.40476417541504, 68.6538417339325, 69.90291929244995, 71.14017152786255, 72.37742376327515, 73.62148547172546, 74.86554718017578, 76.1097846031189, 77.35402202606201, 78.59321165084839, 79.83240127563477, 81.07475519180298, 82.31710910797119, 83.55900859832764, 84.80090808868408, 86.04276156425476, 87.28461503982544, 88.5245954990387, 89.76457595825195, 91.00236773490906, 92.24015951156616, 93.48094487190247, 94.72173023223877, 95.96640992164612, 97.21108961105347, 98.46005177497864, 99.70901393890381, 100.95599126815796, 102.20296859741211, 103.44479513168335, 104.68662166595459, 105.93131589889526, 107.17601013183594, 108.41895389556885, 109.66189765930176, 110.91163873672485, 112.16137981414795, 113.40508270263672, 114.64878559112549, 115.89477229118347, 117.14075899124146, 118.37935018539429, 119.61794137954712, 120.8638665676117, 122.10979175567627, 123.3511791229248, 124.59256649017334, 125.84645056724548, 127.10033464431763, 128.34979844093323, 129.59926223754883, 130.85206508636475, 132.10486793518066, 133.35526585578918, 134.6056637763977, 135.85468935966492, 137.10371494293213, 138.35049653053284, 139.59727811813354, 140.844575881958, 142.09187364578247, 143.33857941627502, 144.58528518676758, 145.83675384521484, 147.0882225036621, 148.33570313453674, 149.58318376541138, 150.82891726493835, 152.07465076446533, 153.32098054885864, 154.56731033325195, 155.81622338294983, 157.0651364326477, 158.3042697906494, 159.54340314865112, 160.7908525466919, 162.03830194473267, 163.28337955474854, 164.5284571647644, 165.78037691116333, 167.03229665756226, 168.27834272384644, 169.52438879013062, 170.7696716785431, 172.01495456695557, 173.12461757659912, 174.23428058624268, 175.4336895942688, 176.63309860229492, 177.8788924217224, 179.1246862411499, 180.37400650978088, 181.62332677841187, 182.8703339099884, 184.11734104156494, 185.36348056793213, 186.60962009429932, 187.85676860809326, 189.1039171218872, 190.35056829452515, 191.5972194671631, 192.84418106079102, 194.09114265441895, 195.3392276763916, 196.58731269836426, 197.83282256126404, 199.07833242416382, 200.32539796829224, 201.57246351242065, 202.81749033927917, 204.0625171661377, 205.3075830936432, 206.55264902114868, 207.80268120765686, 209.05271339416504, 210.30045914649963, 211.54820489883423, 212.79757142066956, 214.04693794250488, 215.29382228851318, 216.54070663452148, 217.7884383201599, 219.03617000579834, 220.2799654006958, 221.52376079559326, 222.76551270484924, 224.00726461410522, 225.2478358745575, 226.48840713500977, 227.73503708839417, 228.98166704177856, 230.2283058166504, 231.47494459152222, 232.71962523460388, 233.96430587768555, 235.21234226226807, 236.4603786468506, 237.70292329788208, 238.94546794891357, 240.19612169265747, 241.44677543640137, 242.69366192817688, 243.9405484199524, 245.19356775283813, 246.44658708572388, 247.69303464889526, 248.93948221206665, 251.25898146629333, 253.57848072052002]
[24.635, 24.635, 29.4725, 29.4725, 31.7525, 31.7525, 33.85, 33.85, 35.49, 35.49, 36.63, 36.63, 37.4075, 37.4075, 38.24, 38.24, 38.12, 38.12, 38.89, 38.89, 39.6125, 39.6125, 39.855, 39.855, 40.6675, 40.6675, 40.3075, 40.3075, 40.8725, 40.8725, 40.4575, 40.4575, 40.475, 40.475, 40.6175, 40.6175, 40.7025, 40.7025, 40.91, 40.91, 41.5775, 41.5775, 41.3825, 41.3825, 41.335, 41.335, 41.67, 41.67, 41.5475, 41.5475, 41.35, 41.35, 41.625, 41.625, 41.47, 41.47, 41.685, 41.685, 41.83, 41.83, 41.8825, 41.8825, 42.09, 42.09, 42.0825, 42.0825, 41.805, 41.805, 41.805, 41.805, 41.7875, 41.7875, 41.7775, 41.7775, 42.26, 42.26, 42.1125, 42.1125, 42.0725, 42.0725, 42.29, 42.29, 42.275, 42.275, 42.0925, 42.0925, 42.1875, 42.1875, 42.5575, 42.5575, 42.255, 42.255, 42.2425, 42.2425, 42.1475, 42.1475, 42.2375, 42.2375, 42.24, 42.24, 42.525, 42.525, 42.6325, 42.6325, 42.475, 42.475, 42.4925, 42.4925, 42.7075, 42.7075, 42.33, 42.33, 42.5675, 42.5675, 42.5175, 42.5175, 42.5375, 42.5375, 42.4825, 42.4825, 42.0275, 42.0275, 42.3875, 42.3875, 42.1225, 42.1225, 42.0775, 42.0775, 42.41, 42.41, 42.39, 42.39, 42.3725, 42.3725, 42.4825, 42.4825, 42.2175, 42.2175, 41.79, 41.79, 42.0425, 42.0425, 42.3475, 42.3475, 42.3275, 42.3275, 42.57, 42.57, 42.5825, 42.5825, 42.625, 42.625, 42.75, 42.75, 42.3475, 42.3475, 42.095, 42.095, 41.7125, 41.7125, 42.0825, 42.0825, 42.0125, 42.0125, 42.455, 42.455, 42.6325, 42.6325, 42.6525, 42.6525, 42.7475, 42.7475, 42.3675, 42.3675, 42.6525, 42.6525, 42.6025, 42.6025, 42.655, 42.655, 42.235, 42.235, 42.3425, 42.3425, 42.55, 42.55, 42.6875, 42.6875, 42.59, 42.59, 42.5575, 42.5575, 42.7225, 42.7225, 42.6425, 42.6425, 42.4825, 42.4825, 42.595, 42.595, 43.38, 43.38]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 1.449, Test loss: 1.893, Test accuracy: 31.21
Round   1, Train loss: 1.255, Test loss: 1.799, Test accuracy: 34.93
Round   2, Train loss: 1.175, Test loss: 1.890, Test accuracy: 31.46
Round   3, Train loss: 1.095, Test loss: 1.889, Test accuracy: 32.85
Round   4, Train loss: 1.031, Test loss: 1.964, Test accuracy: 31.07
Round   5, Train loss: 0.967, Test loss: 1.994, Test accuracy: 30.06
Round   6, Train loss: 0.943, Test loss: 2.089, Test accuracy: 27.02
Round   7, Train loss: 0.874, Test loss: 2.071, Test accuracy: 27.45
Round   8, Train loss: 0.851, Test loss: 2.118, Test accuracy: 25.75
Round   9, Train loss: 0.834, Test loss: 2.114, Test accuracy: 25.68
Round  10, Train loss: 0.770, Test loss: 2.109, Test accuracy: 25.79
Round  11, Train loss: 0.734, Test loss: 2.094, Test accuracy: 25.91
Round  12, Train loss: 0.733, Test loss: 2.088, Test accuracy: 26.01
Round  13, Train loss: 0.669, Test loss: 2.081, Test accuracy: 26.66
Round  14, Train loss: 0.688, Test loss: 2.074, Test accuracy: 26.45
Round  15, Train loss: 0.610, Test loss: 2.064, Test accuracy: 27.07
Round  16, Train loss: 0.630, Test loss: 2.054, Test accuracy: 27.91
Round  17, Train loss: 0.574, Test loss: 2.047, Test accuracy: 27.80
Round  18, Train loss: 0.560, Test loss: 2.040, Test accuracy: 27.95
Round  19, Train loss: 0.547, Test loss: 2.031, Test accuracy: 28.45
Round  20, Train loss: 0.526, Test loss: 2.025, Test accuracy: 28.07
Round  21, Train loss: 0.512, Test loss: 2.018, Test accuracy: 29.32
Round  22, Train loss: 0.488, Test loss: 2.014, Test accuracy: 29.25
Round  23, Train loss: 0.483, Test loss: 2.007, Test accuracy: 30.35
Round  24, Train loss: 0.471, Test loss: 2.005, Test accuracy: 30.39
Round  25, Train loss: 0.411, Test loss: 1.987, Test accuracy: 31.12
Round  26, Train loss: 0.460, Test loss: 1.992, Test accuracy: 30.46
Round  27, Train loss: 0.459, Test loss: 1.986, Test accuracy: 30.80
Round  28, Train loss: 0.413, Test loss: 1.977, Test accuracy: 31.48
Round  29, Train loss: 0.370, Test loss: 1.969, Test accuracy: 31.86
Round  30, Train loss: 0.410, Test loss: 1.956, Test accuracy: 32.99
Round  31, Train loss: 0.372, Test loss: 1.943, Test accuracy: 33.81
Round  32, Train loss: 0.361, Test loss: 1.936, Test accuracy: 34.14
Round  33, Train loss: 0.356, Test loss: 1.935, Test accuracy: 33.56
Round  34, Train loss: 0.341, Test loss: 1.920, Test accuracy: 34.80
Round  35, Train loss: 0.317, Test loss: 1.919, Test accuracy: 34.38
Round  36, Train loss: 0.311, Test loss: 1.923, Test accuracy: 34.08
Round  37, Train loss: 0.301, Test loss: 1.916, Test accuracy: 34.62
Round  38, Train loss: 0.304, Test loss: 1.910, Test accuracy: 34.87
Round  39, Train loss: 0.284, Test loss: 1.907, Test accuracy: 34.92
Round  40, Train loss: 0.288, Test loss: 1.905, Test accuracy: 35.63
Round  41, Train loss: 0.314, Test loss: 1.898, Test accuracy: 36.50
Round  42, Train loss: 0.258, Test loss: 1.890, Test accuracy: 36.06
Round  43, Train loss: 0.276, Test loss: 1.881, Test accuracy: 36.75
Round  44, Train loss: 0.254, Test loss: 1.866, Test accuracy: 37.90
Round  45, Train loss: 0.243, Test loss: 1.860, Test accuracy: 37.96
Round  46, Train loss: 0.226, Test loss: 1.846, Test accuracy: 38.95
Round  47, Train loss: 0.293, Test loss: 1.853, Test accuracy: 38.18
Round  48, Train loss: 0.252, Test loss: 1.844, Test accuracy: 38.87
Round  49, Train loss: 0.227, Test loss: 1.842, Test accuracy: 38.44
Round  50, Train loss: 0.238, Test loss: 1.839, Test accuracy: 38.72
Round  51, Train loss: 0.213, Test loss: 1.835, Test accuracy: 39.14
Round  52, Train loss: 0.212, Test loss: 1.830, Test accuracy: 38.52
Round  53, Train loss: 0.225, Test loss: 1.829, Test accuracy: 39.11
Round  54, Train loss: 0.206, Test loss: 1.827, Test accuracy: 39.00
Round  55, Train loss: 0.239, Test loss: 1.817, Test accuracy: 39.80
Round  56, Train loss: 0.210, Test loss: 1.806, Test accuracy: 40.06
Round  57, Train loss: 0.217, Test loss: 1.807, Test accuracy: 39.33
Round  58, Train loss: 0.198, Test loss: 1.803, Test accuracy: 39.74
Round  59, Train loss: 0.185, Test loss: 1.794, Test accuracy: 40.14
Round  60, Train loss: 0.215, Test loss: 1.791, Test accuracy: 40.42
Round  61, Train loss: 0.186, Test loss: 1.784, Test accuracy: 40.58
Round  62, Train loss: 0.198, Test loss: 1.773, Test accuracy: 40.96
Round  63, Train loss: 0.199, Test loss: 1.766, Test accuracy: 41.20
Round  64, Train loss: 0.183, Test loss: 1.763, Test accuracy: 41.57
Round  65, Train loss: 0.191, Test loss: 1.752, Test accuracy: 42.47
Round  66, Train loss: 0.166, Test loss: 1.757, Test accuracy: 41.23
Round  67, Train loss: 0.173, Test loss: 1.766, Test accuracy: 40.96
Round  68, Train loss: 0.167, Test loss: 1.759, Test accuracy: 41.18
Round  69, Train loss: 0.169, Test loss: 1.753, Test accuracy: 41.55
Round  70, Train loss: 0.176, Test loss: 1.753, Test accuracy: 41.52
Round  71, Train loss: 0.178, Test loss: 1.750, Test accuracy: 41.29
Round  72, Train loss: 0.155, Test loss: 1.745, Test accuracy: 41.25
Round  73, Train loss: 0.163, Test loss: 1.743, Test accuracy: 41.48
Round  74, Train loss: 0.151, Test loss: 1.738, Test accuracy: 42.20
Round  75, Train loss: 0.164, Test loss: 1.732, Test accuracy: 42.76
Round  76, Train loss: 0.160, Test loss: 1.736, Test accuracy: 42.28
Round  77, Train loss: 0.156, Test loss: 1.735, Test accuracy: 42.16
Round  78, Train loss: 0.153, Test loss: 1.726, Test accuracy: 42.18
Round  79, Train loss: 0.151, Test loss: 1.731, Test accuracy: 41.89
Round  80, Train loss: 0.162, Test loss: 1.730, Test accuracy: 41.78
Round  81, Train loss: 0.148, Test loss: 1.715, Test accuracy: 42.77
Round  82, Train loss: 0.141, Test loss: 1.710, Test accuracy: 43.10
Round  83, Train loss: 0.150, Test loss: 1.707, Test accuracy: 43.31
Round  84, Train loss: 0.163, Test loss: 1.722, Test accuracy: 42.33
Round  85, Train loss: 0.138, Test loss: 1.706, Test accuracy: 43.13
Round  86, Train loss: 0.156, Test loss: 1.709, Test accuracy: 43.23
Round  87, Train loss: 0.132, Test loss: 1.697, Test accuracy: 43.28
Round  88, Train loss: 0.140, Test loss: 1.691, Test accuracy: 43.72
Round  89, Train loss: 0.139, Test loss: 1.692, Test accuracy: 43.41
Round  90, Train loss: 0.134, Test loss: 1.705, Test accuracy: 42.79
Round  91, Train loss: 0.127, Test loss: 1.688, Test accuracy: 43.30
Round  92, Train loss: 0.144, Test loss: 1.694, Test accuracy: 43.06
Round  93, Train loss: 0.130, Test loss: 1.690, Test accuracy: 43.35
Round  94, Train loss: 0.134, Test loss: 1.682, Test accuracy: 43.45
Round  95, Train loss: 0.128, Test loss: 1.677, Test accuracy: 43.75
Round  96, Train loss: 0.134, Test loss: 1.674, Test accuracy: 44.30
Round  97, Train loss: 0.147, Test loss: 1.688, Test accuracy: 43.88
Round  98, Train loss: 0.129, Test loss: 1.679, Test accuracy: 43.76
Round  99, Train loss: 0.129, Test loss: 1.678, Test accuracy: 43.97
Final Round, Train loss: 0.133, Test loss: 1.676, Test accuracy: 44.11
Average accuracy final 10 rounds: 43.56075
5957.476557970047
[]
[31.2125, 34.9325, 31.4625, 32.8475, 31.07, 30.0575, 27.02, 27.4475, 25.75, 25.675, 25.785, 25.9125, 26.01, 26.6575, 26.445, 27.07, 27.9125, 27.8025, 27.9475, 28.4525, 28.075, 29.32, 29.255, 30.3525, 30.3875, 31.1175, 30.465, 30.795, 31.485, 31.86, 32.9925, 33.81, 34.1375, 33.56, 34.795, 34.375, 34.08, 34.615, 34.865, 34.92, 35.6275, 36.5025, 36.0575, 36.7525, 37.895, 37.9575, 38.9475, 38.1775, 38.87, 38.44, 38.715, 39.1375, 38.5225, 39.1075, 39.0025, 39.7975, 40.06, 39.3325, 39.74, 40.1425, 40.4175, 40.5825, 40.9575, 41.1975, 41.57, 42.47, 41.2275, 40.9625, 41.1825, 41.5525, 41.52, 41.29, 41.25, 41.4775, 42.195, 42.76, 42.28, 42.165, 42.1775, 41.8875, 41.785, 42.7725, 43.105, 43.31, 42.3275, 43.13, 43.23, 43.2825, 43.715, 43.405, 42.79, 43.295, 43.065, 43.35, 43.445, 43.7525, 44.305, 43.8825, 43.7575, 43.965, 44.1075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 1.019, Test loss: 1.170, Test accuracy: 60.16
Average accuracy final 10 rounds: 55.96824999999999
Average global accuracy final 10 rounds: 55.96824999999999
4565.51553106308
[]
[18.285, 23.1175, 33.5675, 36.515, 36.1425, 37.42, 39.35, 41.845, 41.87, 43.2975, 43.285, 43.97, 43.1975, 44.7825, 45.7625, 45.5425, 45.95, 46.51, 47.665, 47.6775, 47.6675, 48.0725, 48.1925, 48.3225, 48.4, 49.2525, 49.62, 49.6725, 49.955, 50.1925, 50.3175, 50.075, 50.325, 50.8875, 50.9225, 51.0475, 50.985, 50.5875, 50.88, 50.9625, 51.03, 51.7425, 52.16, 52.1975, 52.6375, 52.625, 52.955, 52.845, 53.1525, 52.5725, 52.88, 53.0175, 52.8125, 52.88, 53.095, 52.9975, 53.5125, 54.0225, 54.0775, 53.84, 53.9875, 53.83, 53.19, 53.385, 53.9575, 54.3925, 54.245, 54.2425, 54.2625, 54.9475, 54.9325, 55.03, 54.8475, 54.49, 54.3, 54.6125, 54.7925, 55.14, 55.2, 55.62, 55.1225, 55.1025, 55.33, 56.2, 56.0325, 56.5375, 55.565, 55.76, 55.62, 55.85, 55.9425, 56.0825, 55.72, 55.9725, 55.765, 55.665, 55.7525, 56.01, 56.245, 56.5275, 60.16]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.252, Test loss: 2.237, Test accuracy: 20.07
Final Round, Global train loss: 2.252, Global test loss: 2.240, Global test accuracy: 20.60
Average accuracy final 10 rounds: 20.105749999999997 

Average global accuracy final 10 rounds: 20.14425 

2744.2276060581207
[1.5094592571258545, 2.8046834468841553, 3.9187333583831787, 5.033063888549805, 6.154741287231445, 7.273295164108276, 8.38644003868103, 9.503379583358765, 10.623429298400879, 11.741796016693115, 12.857769250869751, 13.973652362823486, 15.096805810928345, 16.213993310928345, 17.334542512893677, 18.44793176651001, 19.562950134277344, 20.683192491531372, 21.79681706428528, 22.911625862121582, 24.01717710494995, 25.124854803085327, 26.228692293167114, 27.34229063987732, 28.443597078323364, 29.5565025806427, 30.669560194015503, 31.774935722351074, 32.881091833114624, 33.99048089981079, 35.10447573661804, 36.2143280506134, 37.32832193374634, 38.44103717803955, 39.55797052383423, 40.67281365394592, 41.78705167770386, 42.89876866340637, 44.013726234436035, 45.12564182281494, 46.254234075546265, 47.37072443962097, 48.48449516296387, 49.60801148414612, 50.727113008499146, 51.84185171127319, 52.9529070854187, 54.07043981552124, 55.18486452102661, 56.30134916305542, 57.422152280807495, 58.534897565841675, 59.659337282180786, 60.77731919288635, 61.909876108169556, 63.022727966308594, 64.1414303779602, 65.2638373374939, 66.38578605651855, 67.49513077735901, 68.61407518386841, 69.72670674324036, 70.8427984714508, 71.95584321022034, 73.06963038444519, 74.19343852996826, 75.31565976142883, 76.43543863296509, 77.55478954315186, 78.67896676063538, 79.79454302787781, 80.91473007202148, 82.03417873382568, 83.14427089691162, 84.25892543792725, 85.3694965839386, 86.48335337638855, 87.60727071762085, 88.73245358467102, 89.84434413909912, 90.95602488517761, 92.0747447013855, 93.18926811218262, 94.31482768058777, 95.43401861190796, 96.55414175987244, 97.66630125045776, 98.7779004573822, 99.89959263801575, 101.01149559020996, 102.12791109085083, 103.2413718700409, 104.34600257873535, 105.44969201087952, 106.55386209487915, 107.6698739528656, 108.7833993434906, 109.88604593276978, 110.99966526031494, 112.10174441337585, 114.31964802742004]
[9.945, 9.9125, 9.93, 9.9375, 9.94, 9.93, 9.9225, 9.915, 10.0025, 10.065, 10.0925, 10.125, 10.185, 10.2475, 10.4725, 10.8375, 11.5275, 11.75, 12.1425, 12.38, 12.6625, 13.2775, 13.7625, 13.855, 14.0675, 14.3425, 14.545, 14.815, 14.775, 14.71, 14.9675, 15.105, 15.235, 15.3425, 15.4975, 15.8075, 16.0975, 16.1525, 16.2425, 16.39, 16.5075, 16.5875, 16.63, 16.69, 16.735, 16.7625, 16.86, 16.9425, 17.1275, 17.155, 17.2325, 17.2125, 17.39, 17.5575, 17.6775, 17.8075, 17.7775, 17.865, 18.2775, 18.5975, 18.565, 18.8325, 18.815, 18.84, 19.1225, 19.22, 19.2625, 19.1175, 19.1375, 19.0525, 18.8075, 18.945, 18.995, 18.885, 19.15, 19.2675, 19.465, 19.4225, 19.43, 19.6075, 19.7025, 19.6575, 19.7625, 19.8125, 19.6825, 19.885, 20.005, 19.9025, 19.9475, 19.9075, 19.99, 19.89, 19.9775, 19.9775, 20.215, 20.17, 20.1625, 20.2275, 20.2775, 20.17, 20.0725]
