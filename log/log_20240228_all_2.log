nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.148, Test loss: 2.829, Test accuracy: 56.01
Final Round, Global train loss: 0.148, Global test loss: 1.278, Global test accuracy: 57.76
Average accuracy final 10 rounds: 55.86999999999999 

Average global accuracy final 10 rounds: 58.88699999999999 

6120.565320491791
[4.359459400177002, 8.718918800354004, 12.738895893096924, 16.758872985839844, 21.355366945266724, 25.951860904693604, 30.418733596801758, 34.88560628890991, 38.985806941986084, 43.086007595062256, 48.2911114692688, 53.49621534347534, 58.64113092422485, 63.786046504974365, 68.94348526000977, 74.10092401504517, 79.16656494140625, 84.23220586776733, 89.38104200363159, 94.52987813949585, 99.63354825973511, 104.73721837997437, 110.03837871551514, 115.33953905105591, 120.54706811904907, 125.75459718704224, 130.22351837158203, 134.69243955612183, 139.04897117614746, 143.4055027961731, 147.78437304496765, 152.1632432937622, 156.5509147644043, 160.9385862350464, 165.3150405883789, 169.69149494171143, 174.04103255271912, 178.3905701637268, 182.7310175895691, 187.07146501541138, 191.38056755065918, 195.68967008590698, 200.02093029022217, 204.35219049453735, 208.70774841308594, 213.06330633163452, 217.4295551776886, 221.79580402374268, 226.11359000205994, 230.4313759803772, 234.79906964302063, 239.16676330566406, 243.54628443717957, 247.92580556869507, 252.30716633796692, 256.68852710723877, 261.0646708011627, 265.44081449508667, 269.7514169216156, 274.06201934814453, 278.4470043182373, 282.8319892883301, 287.2468616962433, 291.6617341041565, 296.1098704338074, 300.55800676345825, 304.9259498119354, 309.2938928604126, 313.64004492759705, 317.9861969947815, 322.33534240722656, 326.68448781967163, 331.04424929618835, 335.4040107727051, 339.79327368736267, 344.18253660202026, 348.5033223628998, 352.8241081237793, 357.1890766620636, 361.5540452003479, 365.91806387901306, 370.2820825576782, 374.63504672050476, 378.9880108833313, 383.40219831466675, 387.8163857460022, 392.2220296859741, 396.62767362594604, 401.5333435535431, 406.43901348114014, 412.0222198963165, 417.6054263114929, 422.52097630500793, 427.43652629852295, 431.90410327911377, 436.3716802597046, 440.86436223983765, 445.3570442199707, 450.01376128196716, 454.6704783439636, 459.0807156562805, 463.4909529685974, 468.0792541503906, 472.66755533218384, 477.2071170806885, 481.7466788291931, 486.282071352005, 490.8174638748169, 495.33455061912537, 499.85163736343384, 504.259281873703, 508.66692638397217, 513.5680801868439, 518.4692339897156, 522.9324672222137, 527.3957004547119, 531.8937358856201, 536.3917713165283, 541.1266806125641, 545.8615899085999, 550.2722527980804, 554.682915687561, 559.0685513019562, 563.4541869163513, 567.8622627258301, 572.2703385353088, 576.6009533405304, 580.931568145752, 585.3237104415894, 589.7158527374268, 594.1109828948975, 598.5061130523682, 602.8978645801544, 607.2896161079407, 611.6242916584015, 615.9589672088623, 620.3382420539856, 624.7175168991089, 629.0960011482239, 633.4744853973389, 637.8785717487335, 642.2826581001282, 646.5977900028229, 650.9129219055176, 655.3295874595642, 659.7462530136108, 664.1971659660339, 668.648078918457, 673.1151595115662, 677.5822401046753, 682.4870295524597, 687.3918190002441, 692.1978380680084, 697.0038571357727, 701.9010679721832, 706.7982788085938, 711.1963446140289, 715.5944104194641, 719.9716844558716, 724.348958492279, 728.779951095581, 733.210943698883, 738.126473903656, 743.042004108429, 747.9454164505005, 752.848828792572, 757.957923412323, 763.067018032074, 767.9241285324097, 772.7812390327454, 778.25958776474, 783.7379364967346, 789.0288825035095, 794.3198285102844, 799.179701089859, 804.0395736694336, 808.9261019229889, 813.8126301765442, 818.4228241443634, 823.0330181121826, 827.4629316329956, 831.8928451538086, 836.3459475040436, 840.7990498542786, 845.7533147335052, 850.7075796127319, 855.565267086029, 860.4229545593262, 865.3722279071808, 870.3215012550354, 875.1541562080383, 879.9868111610413, 884.2915015220642, 888.5961918830872, 893.0022618770599, 897.4083318710327, 901.8473432064056, 906.2863545417786, 910.6143913269043, 914.94242811203, 917.1267502307892, 919.3110723495483]
[37.8025, 37.8025, 42.4575, 42.4575, 44.63, 44.63, 45.7825, 45.7825, 46.6425, 46.6425, 48.155, 48.155, 48.3575, 48.3575, 49.39, 49.39, 50.0725, 50.0725, 50.4975, 50.4975, 50.6325, 50.6325, 51.0725, 51.0725, 52.04, 52.04, 52.4025, 52.4025, 52.8825, 52.8825, 53.115, 53.115, 53.75, 53.75, 53.8675, 53.8675, 54.0675, 54.0675, 53.975, 53.975, 54.13, 54.13, 54.435, 54.435, 54.355, 54.355, 54.4625, 54.4625, 54.7575, 54.7575, 55.0925, 55.0925, 55.2125, 55.2125, 55.0275, 55.0275, 54.97, 54.97, 54.8025, 54.8025, 54.8, 54.8, 55.0825, 55.0825, 54.98, 54.98, 55.14, 55.14, 55.3475, 55.3475, 55.3825, 55.3825, 55.745, 55.745, 55.6125, 55.6125, 55.3475, 55.3475, 55.39, 55.39, 55.3275, 55.3275, 55.42, 55.42, 55.35, 55.35, 55.4175, 55.4175, 55.045, 55.045, 54.89, 54.89, 55.1175, 55.1175, 55.2, 55.2, 55.24, 55.24, 55.0875, 55.0875, 55.205, 55.205, 55.0075, 55.0075, 55.1, 55.1, 54.93, 54.93, 54.965, 54.965, 55.19, 55.19, 55.39, 55.39, 55.24, 55.24, 55.2025, 55.2025, 55.4625, 55.4625, 55.7425, 55.7425, 55.72, 55.72, 55.6225, 55.6225, 55.575, 55.575, 55.5325, 55.5325, 55.4125, 55.4125, 55.52, 55.52, 55.8425, 55.8425, 55.37, 55.37, 55.57, 55.57, 55.7725, 55.7725, 55.85, 55.85, 55.635, 55.635, 55.2875, 55.2875, 55.2525, 55.2525, 55.3975, 55.3975, 55.7075, 55.7075, 56.1125, 56.1125, 55.7875, 55.7875, 55.755, 55.755, 55.95, 55.95, 55.9225, 55.9225, 56.0475, 56.0475, 56.08, 56.08, 55.915, 55.915, 55.8975, 55.8975, 55.92, 55.92, 56.01, 56.01, 56.0175, 56.0175, 56.0775, 56.0775, 56.0375, 56.0375, 55.865, 55.865, 55.775, 55.775, 55.7125, 55.7125, 55.725, 55.725, 55.83, 55.83, 56.145, 56.145, 56.07, 56.07, 55.815, 55.815, 55.725, 55.725, 56.01, 56.01]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.149, Test loss: 0.485, Test accuracy: 85.73
Final Round, Global train loss: 0.149, Global test loss: 1.141, Global test accuracy: 64.77
Average accuracy final 10 rounds: 85.32833333333335 

Average global accuracy final 10 rounds: 63.2825 

1887.54816365242
[1.658266305923462, 3.316532611846924, 4.702439308166504, 6.088346004486084, 7.45285701751709, 8.817368030548096, 10.245531558990479, 11.673695087432861, 13.04361891746521, 14.413542747497559, 15.774561166763306, 17.135579586029053, 18.511882543563843, 19.888185501098633, 21.276273012161255, 22.664360523223877, 24.038406372070312, 25.412452220916748, 26.819921731948853, 28.227391242980957, 29.62997269630432, 31.032554149627686, 32.40773391723633, 33.78291368484497, 35.18319749832153, 36.583481311798096, 37.9827401638031, 39.381999015808105, 40.7051465511322, 42.0282940864563, 43.34870624542236, 44.66911840438843, 45.96249079704285, 47.255863189697266, 48.530866622924805, 49.805870056152344, 51.097692251205444, 52.389514446258545, 53.70807480812073, 55.02663516998291, 56.47720432281494, 57.92777347564697, 59.32890462875366, 60.73003578186035, 62.1725378036499, 63.61503982543945, 64.91448783874512, 66.21393585205078, 67.52333688735962, 68.83273792266846, 70.11592245101929, 71.39910697937012, 72.85809516906738, 74.31708335876465, 75.76099967956543, 77.20491600036621, 78.65593791007996, 80.1069598197937, 81.57109260559082, 83.03522539138794, 84.4160041809082, 85.79678297042847, 87.1028344631195, 88.40888595581055, 89.7513837814331, 91.09388160705566, 92.40048694610596, 93.70709228515625, 95.03455829620361, 96.36202430725098, 97.69263815879822, 99.02325201034546, 100.37803292274475, 101.73281383514404, 103.10589694976807, 104.47898006439209, 105.79511594772339, 107.11125183105469, 108.42756795883179, 109.74388408660889, 111.0691180229187, 112.39435195922852, 113.71374273300171, 115.0331335067749, 116.3397171497345, 117.64630079269409, 118.97092962265015, 120.2955584526062, 121.61316752433777, 122.93077659606934, 124.2299394607544, 125.52910232543945, 126.83301091194153, 128.1369194984436, 129.42879605293274, 130.72067260742188, 132.0175621509552, 133.31445169448853, 134.77861380577087, 136.24277591705322, 137.69056367874146, 139.1383514404297, 140.5811848640442, 142.0240182876587, 143.4962387084961, 144.9684591293335, 146.33866620063782, 147.70887327194214, 149.1485879421234, 150.5883026123047, 152.0263273715973, 153.4643521308899, 154.88813281059265, 156.3119134902954, 157.73874044418335, 159.1655673980713, 160.61321377754211, 162.06086015701294, 163.5120677947998, 164.96327543258667, 166.42130589485168, 167.8793363571167, 169.33049964904785, 170.781662940979, 172.28182864189148, 173.78199434280396, 175.23255395889282, 176.6831135749817, 178.15845823287964, 179.6338028907776, 181.07723593711853, 182.52066898345947, 183.96655559539795, 185.41244220733643, 186.80572748184204, 188.19901275634766, 189.6330063343048, 191.06699991226196, 192.45473861694336, 193.84247732162476, 195.25153017044067, 196.6605830192566, 198.10963201522827, 199.55868101119995, 200.94778156280518, 202.3368821144104, 203.79866194725037, 205.26044178009033, 206.69190192222595, 208.12336206436157, 209.59122443199158, 211.05908679962158, 212.51291418075562, 213.96674156188965, 215.40447092056274, 216.84220027923584, 218.29829907417297, 219.7543978691101, 221.2009632587433, 222.64752864837646, 224.08628153800964, 225.52503442764282, 226.92036199569702, 228.31568956375122, 229.74225330352783, 231.16881704330444, 232.60605645179749, 234.04329586029053, 235.49136447906494, 236.93943309783936, 238.31193327903748, 239.6844334602356, 241.06320023536682, 242.44196701049805, 243.83334183692932, 245.2247166633606, 246.56257963180542, 247.90044260025024, 249.33962965011597, 250.7788166999817, 252.15870714187622, 253.53859758377075, 254.95623087882996, 256.37386417388916, 257.76793575286865, 259.16200733184814, 260.548326253891, 261.93464517593384, 263.31553316116333, 264.6964211463928, 266.08719778060913, 267.47797441482544, 268.86774158477783, 270.2575087547302, 271.6458806991577, 273.0342526435852, 274.42569041252136, 275.8171281814575, 277.240106344223, 278.6630845069885, 281.04016613960266, 283.4172477722168]
[29.283333333333335, 29.283333333333335, 40.708333333333336, 40.708333333333336, 52.325, 52.325, 60.38333333333333, 60.38333333333333, 61.983333333333334, 61.983333333333334, 64.0, 64.0, 67.35833333333333, 67.35833333333333, 66.04166666666667, 66.04166666666667, 67.66666666666667, 67.66666666666667, 70.875, 70.875, 72.78333333333333, 72.78333333333333, 74.65833333333333, 74.65833333333333, 74.0, 74.0, 74.00833333333334, 74.00833333333334, 74.56666666666666, 74.56666666666666, 74.75833333333334, 74.75833333333334, 74.56666666666666, 74.56666666666666, 75.65, 75.65, 76.70833333333333, 76.70833333333333, 77.79166666666667, 77.79166666666667, 78.48333333333333, 78.48333333333333, 78.09166666666667, 78.09166666666667, 78.28333333333333, 78.28333333333333, 78.01666666666667, 78.01666666666667, 78.14166666666667, 78.14166666666667, 79.125, 79.125, 80.075, 80.075, 80.675, 80.675, 81.14166666666667, 81.14166666666667, 81.30833333333334, 81.30833333333334, 81.4, 81.4, 80.46666666666667, 80.46666666666667, 81.89166666666667, 81.89166666666667, 81.93333333333334, 81.93333333333334, 81.4, 81.4, 81.89166666666667, 81.89166666666667, 81.58333333333333, 81.58333333333333, 82.125, 82.125, 82.13333333333334, 82.13333333333334, 82.3, 82.3, 82.63333333333334, 82.63333333333334, 82.55, 82.55, 82.65, 82.65, 82.5, 82.5, 82.73333333333333, 82.73333333333333, 82.16666666666667, 82.16666666666667, 82.45833333333333, 82.45833333333333, 82.475, 82.475, 82.96666666666667, 82.96666666666667, 82.89166666666667, 82.89166666666667, 84.10833333333333, 84.10833333333333, 83.85833333333333, 83.85833333333333, 83.58333333333333, 83.58333333333333, 83.56666666666666, 83.56666666666666, 83.44166666666666, 83.44166666666666, 83.025, 83.025, 82.86666666666666, 82.86666666666666, 83.225, 83.225, 83.35, 83.35, 83.3, 83.3, 83.70833333333333, 83.70833333333333, 83.51666666666667, 83.51666666666667, 84.075, 84.075, 84.65, 84.65, 84.49166666666666, 84.49166666666666, 84.98333333333333, 84.98333333333333, 84.69166666666666, 84.69166666666666, 84.03333333333333, 84.03333333333333, 83.98333333333333, 83.98333333333333, 84.26666666666667, 84.26666666666667, 84.275, 84.275, 84.775, 84.775, 84.78333333333333, 84.78333333333333, 85.05, 85.05, 84.59166666666667, 84.59166666666667, 84.31666666666666, 84.31666666666666, 84.325, 84.325, 84.38333333333334, 84.38333333333334, 84.68333333333334, 84.68333333333334, 84.64166666666667, 84.64166666666667, 84.7, 84.7, 84.84166666666667, 84.84166666666667, 85.01666666666667, 85.01666666666667, 85.14166666666667, 85.14166666666667, 85.08333333333333, 85.08333333333333, 84.925, 84.925, 84.90833333333333, 84.90833333333333, 84.93333333333334, 84.93333333333334, 85.05, 85.05, 85.33333333333333, 85.33333333333333, 85.425, 85.425, 85.45, 85.45, 85.43333333333334, 85.43333333333334, 85.6, 85.6, 85.425, 85.425, 85.30833333333334, 85.30833333333334, 84.98333333333333, 84.98333333333333, 85.15, 85.15, 85.24166666666666, 85.24166666666666, 85.26666666666667, 85.26666666666667, 85.73333333333333, 85.73333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.206, Test loss: 0.371, Test accuracy: 86.33
Average accuracy final 10 rounds: 85.48083333333335 

1461.2493076324463
[1.6189014911651611, 3.2378029823303223, 4.555332660675049, 5.872862339019775, 7.217505216598511, 8.562148094177246, 9.885499715805054, 11.208851337432861, 12.559505939483643, 13.910160541534424, 15.257428169250488, 16.604695796966553, 17.96135687828064, 19.318017959594727, 20.62562584877014, 21.933233737945557, 23.273985862731934, 24.61473798751831, 25.9570472240448, 27.29935646057129, 28.62447953224182, 29.949602603912354, 31.243125200271606, 32.53664779663086, 33.84381079673767, 35.15097379684448, 36.4836859703064, 37.81639814376831, 39.103628635406494, 40.39085912704468, 41.76149272918701, 43.132126331329346, 44.4644980430603, 45.79686975479126, 47.07120943069458, 48.3455491065979, 49.67646551132202, 51.00738191604614, 52.36697959899902, 53.726577281951904, 55.0731897354126, 56.41980218887329, 57.744449615478516, 59.06909704208374, 60.446982860565186, 61.82486867904663, 63.15502738952637, 64.4851861000061, 65.77196431159973, 67.05874252319336, 68.32835245132446, 69.59796237945557, 70.83807039260864, 72.07817840576172, 73.3808069229126, 74.68343544006348, 76.00529646873474, 77.327157497406, 78.62224841117859, 79.91733932495117, 81.19602131843567, 82.47470331192017, 83.74262022972107, 85.01053714752197, 86.23027420043945, 87.45001125335693, 88.67727303504944, 89.90453481674194, 91.11649370193481, 92.32845258712769, 93.54439687728882, 94.76034116744995, 95.98234939575195, 97.20435762405396, 98.43484210968018, 99.6653265953064, 100.87245535850525, 102.0795841217041, 103.30541515350342, 104.53124618530273, 105.78185033798218, 107.03245449066162, 108.27114772796631, 109.509840965271, 110.72844290733337, 111.94704484939575, 113.19027304649353, 114.43350124359131, 115.66411471366882, 116.89472818374634, 118.10345602035522, 119.31218385696411, 120.53212356567383, 121.75206327438354, 122.96848344802856, 124.18490362167358, 125.4115583896637, 126.63821315765381, 127.87030673027039, 129.10240030288696, 130.33195090293884, 131.56150150299072, 132.79084968566895, 134.02019786834717, 135.2346019744873, 136.44900608062744, 137.6686544418335, 138.88830280303955, 140.21279001235962, 141.5372772216797, 142.84531450271606, 144.15335178375244, 145.43755316734314, 146.72175455093384, 148.03427815437317, 149.3468017578125, 150.64858555793762, 151.95036935806274, 153.24896907806396, 154.54756879806519, 155.85064244270325, 157.1537160873413, 158.45760345458984, 159.76149082183838, 160.98714089393616, 162.21279096603394, 163.43456935882568, 164.65634775161743, 165.89711570739746, 167.1378836631775, 168.4359061717987, 169.73392868041992, 171.04330921173096, 172.352689743042, 173.66999316215515, 174.9872965812683, 176.32072043418884, 177.65414428710938, 178.98027920722961, 180.30641412734985, 181.60345149040222, 182.9004888534546, 184.22644329071045, 185.5523977279663, 186.8724570274353, 188.1925163269043, 189.4933180809021, 190.7941198348999, 192.07918429374695, 193.364248752594, 194.66614270210266, 195.96803665161133, 197.3180763721466, 198.66811609268188, 199.96243047714233, 201.25674486160278, 202.49742770195007, 203.73811054229736, 204.96290969848633, 206.1877088546753, 207.41093516349792, 208.63416147232056, 209.86312294006348, 211.0920844078064, 212.31420302391052, 213.53632164001465, 214.76426672935486, 215.99221181869507, 217.2018759250641, 218.4115400314331, 219.6351580619812, 220.8587760925293, 222.0842320919037, 223.30968809127808, 224.520733833313, 225.7317795753479, 226.94824719429016, 228.16471481323242, 229.4404137134552, 230.71611261367798, 231.94646286964417, 233.17681312561035, 234.38955283164978, 235.6022925376892, 236.8184356689453, 238.03457880020142, 239.25257658958435, 240.47057437896729, 241.7183437347412, 242.96611309051514, 244.1610140800476, 245.35591506958008, 246.57174229621887, 247.78756952285767, 249.01408433914185, 250.24059915542603, 251.47654128074646, 252.7124834060669, 253.90333104133606, 255.09417867660522, 257.0201404094696, 258.946102142334]
[24.316666666666666, 24.316666666666666, 32.225, 32.225, 42.825, 42.825, 49.733333333333334, 49.733333333333334, 53.083333333333336, 53.083333333333336, 57.083333333333336, 57.083333333333336, 58.608333333333334, 58.608333333333334, 64.54166666666667, 64.54166666666667, 66.325, 66.325, 67.51666666666667, 67.51666666666667, 71.73333333333333, 71.73333333333333, 71.38333333333334, 71.38333333333334, 72.90833333333333, 72.90833333333333, 75.525, 75.525, 75.9, 75.9, 75.96666666666667, 75.96666666666667, 76.74166666666666, 76.74166666666666, 76.98333333333333, 76.98333333333333, 77.03333333333333, 77.03333333333333, 77.36666666666666, 77.36666666666666, 78.23333333333333, 78.23333333333333, 78.08333333333333, 78.08333333333333, 78.3, 78.3, 78.275, 78.275, 79.18333333333334, 79.18333333333334, 79.7, 79.7, 80.00833333333334, 80.00833333333334, 79.6, 79.6, 79.5, 79.5, 80.01666666666667, 80.01666666666667, 80.33333333333333, 80.33333333333333, 80.30833333333334, 80.30833333333334, 80.525, 80.525, 80.83333333333333, 80.83333333333333, 81.21666666666667, 81.21666666666667, 81.25833333333334, 81.25833333333334, 81.18333333333334, 81.18333333333334, 81.33333333333333, 81.33333333333333, 81.65833333333333, 81.65833333333333, 81.725, 81.725, 82.225, 82.225, 82.44166666666666, 82.44166666666666, 82.35, 82.35, 82.30833333333334, 82.30833333333334, 82.35833333333333, 82.35833333333333, 82.3, 82.3, 82.30833333333334, 82.30833333333334, 82.46666666666667, 82.46666666666667, 82.25, 82.25, 82.61666666666666, 82.61666666666666, 82.71666666666667, 82.71666666666667, 82.91666666666667, 82.91666666666667, 83.35833333333333, 83.35833333333333, 83.38333333333334, 83.38333333333334, 83.16666666666667, 83.16666666666667, 83.075, 83.075, 83.3, 83.3, 83.38333333333334, 83.38333333333334, 83.49166666666666, 83.49166666666666, 83.93333333333334, 83.93333333333334, 83.91666666666667, 83.91666666666667, 83.23333333333333, 83.23333333333333, 83.36666666666666, 83.36666666666666, 83.60833333333333, 83.60833333333333, 83.89166666666667, 83.89166666666667, 83.93333333333334, 83.93333333333334, 84.33333333333333, 84.33333333333333, 83.88333333333334, 83.88333333333334, 83.8, 83.8, 83.65833333333333, 83.65833333333333, 84.00833333333334, 84.00833333333334, 84.13333333333334, 84.13333333333334, 84.21666666666667, 84.21666666666667, 84.66666666666667, 84.66666666666667, 84.8, 84.8, 84.53333333333333, 84.53333333333333, 84.54166666666667, 84.54166666666667, 84.80833333333334, 84.80833333333334, 84.56666666666666, 84.56666666666666, 84.61666666666666, 84.61666666666666, 84.51666666666667, 84.51666666666667, 84.85833333333333, 84.85833333333333, 85.15833333333333, 85.15833333333333, 84.93333333333334, 84.93333333333334, 84.99166666666666, 84.99166666666666, 84.98333333333333, 84.98333333333333, 85.24166666666666, 85.24166666666666, 85.30833333333334, 85.30833333333334, 85.40833333333333, 85.40833333333333, 85.64166666666667, 85.64166666666667, 85.66666666666667, 85.66666666666667, 85.325, 85.325, 85.61666666666666, 85.61666666666666, 85.09166666666667, 85.09166666666667, 85.45833333333333, 85.45833333333333, 85.59166666666667, 85.59166666666667, 85.43333333333334, 85.43333333333334, 85.26666666666667, 85.26666666666667, 85.48333333333333, 85.48333333333333, 85.875, 85.875, 86.33333333333333, 86.33333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.132, Test loss: 0.398, Test accuracy: 86.78
Average accuracy final 10 rounds: 85.71083333333334 

1449.7867629528046
[1.6854722499847412, 3.3709444999694824, 4.714811325073242, 6.058678150177002, 7.478066921234131, 8.89745569229126, 10.22470235824585, 11.55194902420044, 12.934211730957031, 14.316474437713623, 15.783438920974731, 17.25040340423584, 18.676037788391113, 20.101672172546387, 21.520442008972168, 22.93921184539795, 24.414520263671875, 25.8898286819458, 27.27459144592285, 28.659354209899902, 30.035486698150635, 31.411619186401367, 32.834994077682495, 34.25836896896362, 35.637847900390625, 37.01732683181763, 38.368165016174316, 39.719003200531006, 41.04007029533386, 42.36113739013672, 43.73379373550415, 45.10645008087158, 46.42357420921326, 47.74069833755493, 48.99320340156555, 50.24570846557617, 51.500051736831665, 52.75439500808716, 54.01965141296387, 55.284907817840576, 56.53670573234558, 57.788503646850586, 59.05277466773987, 60.31704568862915, 61.59370970726013, 62.87037372589111, 64.13595175743103, 65.40152978897095, 66.6793270111084, 67.95712423324585, 69.23823428153992, 70.51934432983398, 71.78121733665466, 73.04309034347534, 74.30197048187256, 75.56085062026978, 76.81664252281189, 78.072434425354, 79.36626720428467, 80.66009998321533, 81.91435527801514, 83.16861057281494, 84.42357683181763, 85.67854309082031, 86.95629572868347, 88.23404836654663, 89.48123550415039, 90.72842264175415, 91.99372816085815, 93.25903367996216, 94.53147768974304, 95.80392169952393, 97.07230854034424, 98.34069538116455, 99.60363602638245, 100.86657667160034, 102.11610913276672, 103.3656415939331, 104.63621973991394, 105.90679788589478, 107.19679164886475, 108.48678541183472, 109.74934720993042, 111.01190900802612, 112.28275966644287, 113.55361032485962, 114.80597162246704, 116.05833292007446, 117.34263348579407, 118.62693405151367, 119.93351006507874, 121.2400860786438, 122.5048897266388, 123.76969337463379, 125.06832432746887, 126.36695528030396, 127.63973259925842, 128.9125099182129, 130.17211079597473, 131.43171167373657, 132.68623280525208, 133.94075393676758, 135.2198827266693, 136.49901151657104, 137.75961184501648, 139.0202121734619, 140.30375051498413, 141.58728885650635, 142.85989785194397, 144.1325068473816, 145.4101083278656, 146.6877098083496, 147.9491901397705, 149.2106704711914, 150.49478840827942, 151.77890634536743, 153.03990149497986, 154.30089664459229, 155.55960822105408, 156.81831979751587, 158.08139657974243, 159.344473361969, 160.59552836418152, 161.84658336639404, 163.09260249137878, 164.33862161636353, 165.62163162231445, 166.90464162826538, 168.19230246543884, 169.4799633026123, 170.72904896736145, 171.9781346321106, 173.25680303573608, 174.53547143936157, 175.8315827846527, 177.12769412994385, 178.42410683631897, 179.7205195426941, 181.00658059120178, 182.29264163970947, 183.5865535736084, 184.88046550750732, 186.18539810180664, 187.49033069610596, 188.7366738319397, 189.98301696777344, 191.27604484558105, 192.56907272338867, 193.86092162132263, 195.1527705192566, 196.43457961082458, 197.71638870239258, 198.96722793579102, 200.21806716918945, 201.49573135375977, 202.77339553833008, 204.062237739563, 205.3510799407959, 206.59336400032043, 207.83564805984497, 209.11219143867493, 210.38873481750488, 211.66133546829224, 212.9339361190796, 214.20386576652527, 215.47379541397095, 216.73216032981873, 217.9905252456665, 219.2697982788086, 220.54907131195068, 221.8189868927002, 223.0889024734497, 224.34539771080017, 225.60189294815063, 226.86060881614685, 228.11932468414307, 229.40076112747192, 230.68219757080078, 232.00380325317383, 233.32540893554688, 234.5909297466278, 235.85645055770874, 237.11922192573547, 238.3819932937622, 239.6602373123169, 240.93848133087158, 242.22081351280212, 243.50314569473267, 244.9015998840332, 246.30005407333374, 247.73320698738098, 249.16635990142822, 250.60274481773376, 252.0391297340393, 253.44116377830505, 254.8431978225708, 256.2388060092926, 257.6344141960144, 258.99273347854614, 260.3510527610779, 262.2740750312805, 264.19709730148315]
[25.808333333333334, 25.808333333333334, 32.90833333333333, 32.90833333333333, 40.7, 40.7, 57.40833333333333, 57.40833333333333, 59.80833333333333, 59.80833333333333, 63.28333333333333, 63.28333333333333, 67.275, 67.275, 68.43333333333334, 68.43333333333334, 73.50833333333334, 73.50833333333334, 73.63333333333334, 73.63333333333334, 74.34166666666667, 74.34166666666667, 72.96666666666667, 72.96666666666667, 75.16666666666667, 75.16666666666667, 76.25833333333334, 76.25833333333334, 75.8, 75.8, 77.29166666666667, 77.29166666666667, 77.4, 77.4, 77.30833333333334, 77.30833333333334, 78.46666666666667, 78.46666666666667, 78.40833333333333, 78.40833333333333, 79.51666666666667, 79.51666666666667, 79.9, 79.9, 80.38333333333334, 80.38333333333334, 80.375, 80.375, 80.875, 80.875, 80.53333333333333, 80.53333333333333, 81.55, 81.55, 81.31666666666666, 81.31666666666666, 81.33333333333333, 81.33333333333333, 81.59166666666667, 81.59166666666667, 81.8, 81.8, 82.01666666666667, 82.01666666666667, 82.54166666666667, 82.54166666666667, 81.96666666666667, 81.96666666666667, 82.43333333333334, 82.43333333333334, 82.525, 82.525, 82.08333333333333, 82.08333333333333, 82.4, 82.4, 82.41666666666667, 82.41666666666667, 83.18333333333334, 83.18333333333334, 83.34166666666667, 83.34166666666667, 83.29166666666667, 83.29166666666667, 83.55, 83.55, 83.75, 83.75, 83.70833333333333, 83.70833333333333, 83.175, 83.175, 83.64166666666667, 83.64166666666667, 84.23333333333333, 84.23333333333333, 84.29166666666667, 84.29166666666667, 83.34166666666667, 83.34166666666667, 83.48333333333333, 83.48333333333333, 84.31666666666666, 84.31666666666666, 84.84166666666667, 84.84166666666667, 84.25833333333334, 84.25833333333334, 84.525, 84.525, 84.41666666666667, 84.41666666666667, 84.24166666666666, 84.24166666666666, 84.15833333333333, 84.15833333333333, 84.33333333333333, 84.33333333333333, 84.74166666666666, 84.74166666666666, 84.28333333333333, 84.28333333333333, 84.81666666666666, 84.81666666666666, 84.83333333333333, 84.83333333333333, 84.99166666666666, 84.99166666666666, 84.9, 84.9, 84.74166666666666, 84.74166666666666, 84.9, 84.9, 84.85833333333333, 84.85833333333333, 84.75833333333334, 84.75833333333334, 85.08333333333333, 85.08333333333333, 85.21666666666667, 85.21666666666667, 85.19166666666666, 85.19166666666666, 85.31666666666666, 85.31666666666666, 85.15, 85.15, 85.89166666666667, 85.89166666666667, 85.59166666666667, 85.59166666666667, 85.25833333333334, 85.25833333333334, 85.50833333333334, 85.50833333333334, 85.45833333333333, 85.45833333333333, 85.61666666666666, 85.61666666666666, 85.13333333333334, 85.13333333333334, 85.35, 85.35, 85.70833333333333, 85.70833333333333, 85.31666666666666, 85.31666666666666, 85.41666666666667, 85.41666666666667, 85.25833333333334, 85.25833333333334, 85.34166666666667, 85.34166666666667, 85.30833333333334, 85.30833333333334, 85.5, 85.5, 85.39166666666667, 85.39166666666667, 85.4, 85.4, 85.68333333333334, 85.68333333333334, 85.65, 85.65, 85.95, 85.95, 85.64166666666667, 85.64166666666667, 85.63333333333334, 85.63333333333334, 85.75, 85.75, 86.03333333333333, 86.03333333333333, 85.56666666666666, 85.56666666666666, 85.8, 85.8, 86.78333333333333, 86.78333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.038, Test loss: 0.977, Test accuracy: 81.47
Average accuracy final 10 rounds: 81.19083333333333 

1459.739337682724
[1.6091408729553223, 3.2182817459106445, 4.606249094009399, 5.994216442108154, 7.386534690856934, 8.778852939605713, 10.138027906417847, 11.49720287322998, 12.854233026504517, 14.211263179779053, 15.61318302154541, 17.015102863311768, 18.39157462120056, 19.768046379089355, 21.147654056549072, 22.52726173400879, 23.87855815887451, 25.229854583740234, 26.51396679878235, 27.798079013824463, 29.038511276245117, 30.27894353866577, 31.50681495666504, 32.73468637466431, 34.01202583312988, 35.28936529159546, 36.54839611053467, 37.80742692947388, 39.035974740982056, 40.264522552490234, 41.53060746192932, 42.79669237136841, 44.07342767715454, 45.350162982940674, 46.59254336357117, 47.83492374420166, 49.06786918640137, 50.300814628601074, 51.5571403503418, 52.81346607208252, 54.0928852558136, 55.37230443954468, 56.623188734054565, 57.87407302856445, 59.26377201080322, 60.65347099304199, 62.03183579444885, 63.41020059585571, 64.82780456542969, 66.24540853500366, 67.6354591846466, 69.02550983428955, 70.44088125228882, 71.85625267028809, 73.27284932136536, 74.68944597244263, 76.08547282218933, 77.48149967193604, 78.8878562450409, 80.29421281814575, 81.71568703651428, 83.13716125488281, 84.55169582366943, 85.96623039245605, 87.37948489189148, 88.7927393913269, 90.23700714111328, 91.68127489089966, 93.08202695846558, 94.4827790260315, 95.84279918670654, 97.20281934738159, 98.63162136077881, 100.06042337417603, 101.42672991752625, 102.79303646087646, 104.14947867393494, 105.50592088699341, 106.93833661079407, 108.37075233459473, 109.62839007377625, 110.88602781295776, 112.13539862632751, 113.38476943969727, 114.6532974243164, 115.92182540893555, 117.1847312450409, 118.44763708114624, 119.70854806900024, 120.96945905685425, 122.24449181556702, 123.51952457427979, 124.8075213432312, 126.09551811218262, 127.36114001274109, 128.62676191329956, 129.8880581855774, 131.14935445785522, 132.43008661270142, 133.7108187675476, 134.97099614143372, 136.23117351531982, 137.47252535820007, 138.71387720108032, 140.16456055641174, 141.61524391174316, 142.9839859008789, 144.35272789001465, 145.81413412094116, 147.27554035186768, 148.66759061813354, 150.0596408843994, 151.47060704231262, 152.88157320022583, 154.33398699760437, 155.7864007949829, 157.12935709953308, 158.47231340408325, 159.75962495803833, 161.0469365119934, 162.5569405555725, 164.0669445991516, 165.48954343795776, 166.91214227676392, 168.30398035049438, 169.69581842422485, 171.0057816505432, 172.31574487686157, 173.63839387893677, 174.96104288101196, 176.29514408111572, 177.62924528121948, 179.02929854393005, 180.42935180664062, 181.82291531562805, 183.21647882461548, 184.46108603477478, 185.70569324493408, 186.9732882976532, 188.24088335037231, 189.51040887832642, 190.77993440628052, 192.02258920669556, 193.2652440071106, 194.5517234802246, 195.83820295333862, 197.09462904930115, 198.35105514526367, 199.62136054039001, 200.89166593551636, 202.15514826774597, 203.4186305999756, 204.6576874256134, 205.89674425125122, 207.17258048057556, 208.4484167098999, 209.71193623542786, 210.9754557609558, 212.23557710647583, 213.49569845199585, 214.76411604881287, 216.03253364562988, 217.32924723625183, 218.62596082687378, 219.88711810112, 221.1482753753662, 222.4156937599182, 223.68311214447021, 224.93789219856262, 226.19267225265503, 227.47953152656555, 228.76639080047607, 229.996319770813, 231.2262487411499, 232.47965812683105, 233.7330675125122, 234.9998071193695, 236.2665467262268, 237.54285264015198, 238.81915855407715, 240.0638861656189, 241.30861377716064, 242.56017756462097, 243.8117413520813, 245.11963057518005, 246.4275197982788, 247.68649291992188, 248.94546604156494, 250.21140718460083, 251.47734832763672, 252.72565817832947, 253.97396802902222, 255.2204031944275, 256.46683835983276, 257.75631642341614, 259.0457944869995, 260.29476284980774, 261.54373121261597, 262.83413577079773, 264.1245403289795, 266.4278070926666, 268.73107385635376]
[24.4, 24.4, 34.15833333333333, 34.15833333333333, 49.958333333333336, 49.958333333333336, 50.05833333333333, 50.05833333333333, 51.86666666666667, 51.86666666666667, 52.358333333333334, 52.358333333333334, 66.425, 66.425, 68.675, 68.675, 71.36666666666666, 71.36666666666666, 70.0, 70.0, 71.81666666666666, 71.81666666666666, 72.90833333333333, 72.90833333333333, 73.18333333333334, 73.18333333333334, 73.95, 73.95, 73.2, 73.2, 73.30833333333334, 73.30833333333334, 75.025, 75.025, 74.725, 74.725, 75.775, 75.775, 76.09166666666667, 76.09166666666667, 77.275, 77.275, 77.40833333333333, 77.40833333333333, 76.475, 76.475, 77.44166666666666, 77.44166666666666, 77.85833333333333, 77.85833333333333, 77.88333333333334, 77.88333333333334, 77.275, 77.275, 78.33333333333333, 78.33333333333333, 78.6, 78.6, 78.26666666666667, 78.26666666666667, 78.78333333333333, 78.78333333333333, 79.05833333333334, 79.05833333333334, 79.24166666666666, 79.24166666666666, 78.6, 78.6, 78.875, 78.875, 79.31666666666666, 79.31666666666666, 79.25, 79.25, 79.25833333333334, 79.25833333333334, 78.775, 78.775, 78.75833333333334, 78.75833333333334, 79.75, 79.75, 79.68333333333334, 79.68333333333334, 80.13333333333334, 80.13333333333334, 80.33333333333333, 80.33333333333333, 80.11666666666666, 80.11666666666666, 80.26666666666667, 80.26666666666667, 80.09166666666667, 80.09166666666667, 80.36666666666666, 80.36666666666666, 80.9, 80.9, 80.76666666666667, 80.76666666666667, 80.59166666666667, 80.59166666666667, 80.625, 80.625, 80.64166666666667, 80.64166666666667, 80.525, 80.525, 80.91666666666667, 80.91666666666667, 80.83333333333333, 80.83333333333333, 80.23333333333333, 80.23333333333333, 80.18333333333334, 80.18333333333334, 80.59166666666667, 80.59166666666667, 80.54166666666667, 80.54166666666667, 80.79166666666667, 80.79166666666667, 80.84166666666667, 80.84166666666667, 80.8, 80.8, 80.39166666666667, 80.39166666666667, 80.275, 80.275, 80.55, 80.55, 80.91666666666667, 80.91666666666667, 80.89166666666667, 80.89166666666667, 80.575, 80.575, 80.875, 80.875, 80.83333333333333, 80.83333333333333, 80.95, 80.95, 81.125, 81.125, 80.975, 80.975, 80.925, 80.925, 80.9, 80.9, 80.85, 80.85, 80.88333333333334, 80.88333333333334, 80.70833333333333, 80.70833333333333, 80.3, 80.3, 80.76666666666667, 80.76666666666667, 80.96666666666667, 80.96666666666667, 81.19166666666666, 81.19166666666666, 81.075, 81.075, 80.9, 80.9, 81.05833333333334, 81.05833333333334, 81.21666666666667, 81.21666666666667, 81.15833333333333, 81.15833333333333, 81.06666666666666, 81.06666666666666, 81.05, 81.05, 81.09166666666667, 81.09166666666667, 81.10833333333333, 81.10833333333333, 81.05833333333334, 81.05833333333334, 80.98333333333333, 80.98333333333333, 81.06666666666666, 81.06666666666666, 81.00833333333334, 81.00833333333334, 81.25, 81.25, 81.44166666666666, 81.44166666666666, 81.45, 81.45, 81.45, 81.45, 81.475, 81.475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Final Round, Train loss: 0.060, Test loss: 0.713, Test accuracy: 70.24
Average accuracy final 10 rounds: 69.58083333333333
1895.272759437561
[]
[19.475, 34.25, 34.141666666666666, 38.775, 42.75833333333333, 41.80833333333333, 43.983333333333334, 47.11666666666667, 50.041666666666664, 50.45, 51.55, 52.61666666666667, 55.4, 53.666666666666664, 55.108333333333334, 54.05, 56.68333333333333, 58.03333333333333, 57.34166666666667, 59.25833333333333, 60.78333333333333, 59.625, 58.25833333333333, 59.2, 59.19166666666667, 58.791666666666664, 60.25833333333333, 60.61666666666667, 61.208333333333336, 62.05833333333333, 62.2, 62.11666666666667, 62.34166666666667, 61.99166666666667, 63.71666666666667, 63.7, 64.90833333333333, 64.43333333333334, 66.08333333333333, 65.05, 65.05, 64.43333333333334, 65.59166666666667, 65.19166666666666, 65.59166666666667, 65.58333333333333, 66.3, 66.81666666666666, 68.15, 68.25833333333334, 69.05833333333334, 68.09166666666667, 68.2, 69.03333333333333, 69.26666666666667, 69.04166666666667, 69.48333333333333, 68.725, 68.975, 68.525, 67.28333333333333, 67.56666666666666, 67.85, 67.98333333333333, 67.7, 68.81666666666666, 68.9, 68.95833333333333, 69.34166666666667, 69.16666666666667, 68.56666666666666, 70.46666666666667, 71.50833333333334, 70.0, 70.76666666666667, 70.75833333333334, 71.25833333333334, 71.10833333333333, 69.54166666666667, 69.325, 68.6, 69.01666666666667, 68.65833333333333, 68.39166666666667, 69.55, 69.68333333333334, 69.375, 69.69166666666666, 69.425, 69.675, 70.35833333333333, 69.9, 69.35833333333333, 69.59166666666667, 68.7, 68.51666666666667, 68.6, 69.64166666666667, 70.65833333333333, 70.48333333333333, 70.24166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.570, Test loss: 0.564, Test accuracy: 76.72
Average accuracy final 10 rounds: 76.75416666666666
Average global accuracy final 10 rounds: 76.75416666666666
1362.8424985408783
[]
[26.15, 42.63333333333333, 49.93333333333333, 55.225, 60.93333333333333, 64.08333333333333, 60.541666666666664, 62.06666666666667, 64.10833333333333, 64.21666666666667, 64.85833333333333, 64.26666666666667, 66.425, 67.55, 68.30833333333334, 70.21666666666667, 70.91666666666667, 71.34166666666667, 71.35833333333333, 70.8, 70.64166666666667, 70.625, 70.75, 70.74166666666666, 71.73333333333333, 71.04166666666667, 72.03333333333333, 73.03333333333333, 72.54166666666667, 73.28333333333333, 73.04166666666667, 73.23333333333333, 72.88333333333334, 72.84166666666667, 73.05, 73.25, 73.4, 73.11666666666666, 73.89166666666667, 74.2, 74.71666666666667, 74.625, 75.40833333333333, 75.31666666666666, 75.25833333333334, 75.11666666666666, 75.16666666666667, 74.76666666666667, 74.78333333333333, 75.13333333333334, 74.575, 74.825, 75.59166666666667, 74.98333333333333, 75.03333333333333, 74.86666666666666, 74.44166666666666, 73.96666666666667, 73.90833333333333, 73.71666666666667, 73.89166666666667, 73.875, 74.98333333333333, 75.55, 75.475, 75.20833333333333, 76.14166666666667, 75.88333333333334, 75.44166666666666, 75.55, 74.925, 74.96666666666667, 75.00833333333334, 76.01666666666667, 75.58333333333333, 74.975, 75.58333333333333, 74.7, 75.81666666666666, 74.94166666666666, 75.10833333333333, 75.30833333333334, 75.79166666666667, 76.075, 76.25, 75.85833333333333, 75.85, 75.51666666666667, 75.775, 76.05, 76.59166666666667, 76.10833333333333, 76.60833333333333, 76.98333333333333, 76.925, 77.39166666666667, 77.075, 76.73333333333333, 76.38333333333334, 76.74166666666666, 76.725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.155, Test loss: 0.964, Test accuracy: 69.69
Average accuracy final 10 rounds: 65.72
2447.4591653347015
[3.8644943237304688, 7.488119125366211, 11.154989004135132, 14.741856098175049, 18.3591525554657, 21.953367471694946, 25.538928747177124, 29.150879859924316, 32.76177382469177, 36.346662759780884, 39.95781755447388, 43.556289196014404, 47.1662323474884, 50.803494691848755, 54.45432424545288, 58.105247259140015, 61.74561142921448, 65.41852712631226, 69.04208827018738, 72.70627927780151, 76.34647035598755, 79.9730167388916, 83.63514447212219, 87.28495693206787, 90.94180727005005, 94.60454964637756, 98.26420593261719, 101.90854454040527, 105.55574798583984, 109.18428158760071, 112.81820940971375, 116.46355295181274, 120.10664558410645, 123.76938796043396, 127.42789769172668, 131.0773491859436, 134.70920038223267, 138.33400440216064, 141.9610104560852, 145.61343622207642, 149.2510805130005, 152.8903033733368, 156.51804161071777, 160.15584254264832, 163.79457807540894, 167.45203137397766, 171.10771489143372, 174.74120259284973, 178.39453172683716, 182.04065871238708, 185.6900293827057, 189.34883737564087, 192.98670625686646, 196.64803767204285, 200.2984220981598, 203.93010807037354, 207.5625729560852, 211.18706727027893, 214.8436417579651, 218.44509506225586, 221.83219623565674, 225.1144359111786, 228.35147666931152, 231.63203287124634, 234.88214635849, 238.13133478164673, 241.43884921073914, 244.6886179447174, 247.97748708724976, 251.24485278129578, 254.50376272201538, 257.77961325645447, 261.03246307373047, 264.28071188926697, 267.5514883995056, 270.79471945762634, 274.0654196739197, 277.3123688697815, 280.5641551017761, 283.83007311820984, 287.0799446105957, 290.32497692108154, 293.59170722961426, 296.8561372756958, 300.13222551345825, 303.3980858325958, 306.669748544693, 309.90555357933044, 313.1398103237152, 316.40080523490906, 319.6272768974304, 322.8868567943573, 326.51477098464966, 329.93162727355957, 333.2697899341583, 336.5392162799835, 339.8443179130554, 343.1349446773529, 346.3990705013275, 349.69568943977356, 352.46370697021484]
[14.166666666666666, 20.675, 28.483333333333334, 37.03333333333333, 37.19166666666667, 36.46666666666667, 37.53333333333333, 37.94166666666667, 45.825, 38.075, 43.13333333333333, 45.666666666666664, 45.45, 45.583333333333336, 46.56666666666667, 46.25833333333333, 46.80833333333333, 55.31666666666667, 51.40833333333333, 53.88333333333333, 51.766666666666666, 51.675, 52.916666666666664, 57.525, 57.4, 54.458333333333336, 57.95, 56.9, 59.233333333333334, 58.875, 51.90833333333333, 52.13333333333333, 54.583333333333336, 52.025, 57.458333333333336, 57.3, 56.641666666666666, 62.15833333333333, 54.333333333333336, 57.35, 59.666666666666664, 50.525, 55.725, 60.65, 59.11666666666667, 58.891666666666666, 52.041666666666664, 61.325, 58.15, 63.75, 66.10833333333333, 62.916666666666664, 47.94166666666667, 59.525, 61.475, 55.425, 61.31666666666667, 63.166666666666664, 61.541666666666664, 63.975, 66.425, 65.775, 62.65, 61.9, 60.00833333333333, 63.38333333333333, 65.23333333333333, 62.325, 66.225, 63.516666666666666, 64.96666666666667, 63.125, 66.525, 65.7, 64.95833333333333, 67.26666666666667, 66.63333333333334, 63.625, 58.225, 60.99166666666667, 63.541666666666664, 63.483333333333334, 64.75, 63.891666666666666, 59.6, 57.45, 59.71666666666667, 66.84166666666667, 65.675, 62.09166666666667, 65.725, 63.891666666666666, 69.10833333333333, 68.05, 64.96666666666667, 65.33333333333333, 66.73333333333333, 68.08333333333333, 62.391666666666666, 62.916666666666664, 69.69166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

1612.7146980762482
[1.766453742980957, 3.3237855434417725, 4.894162893295288, 6.433196544647217, 7.976104259490967, 9.52530813217163, 11.098428010940552, 12.660261392593384, 14.21766471862793, 15.773924589157104, 17.33973240852356, 18.894376277923584, 20.44609832763672, 22.00562357902527, 23.557750463485718, 25.106810092926025, 26.68271279335022, 28.241912364959717, 29.818881511688232, 31.366480112075806, 32.92719864845276, 34.483033657073975, 36.035685777664185, 37.51300597190857, 39.080201625823975, 40.73284602165222, 42.30519485473633, 43.87139344215393, 45.446006059646606, 47.02426362037659, 48.54108262062073, 50.08647108078003, 51.59811472892761, 53.17396402359009, 54.717458963394165, 56.21957492828369, 57.706756830215454, 59.26474618911743, 60.77346587181091, 62.3183913230896, 63.84968876838684, 65.35892128944397, 66.91025686264038, 68.46230125427246, 69.98563766479492, 71.52010703086853, 73.06506896018982, 74.60699582099915, 76.08191156387329, 77.6138391494751, 79.1552197933197, 80.69885635375977, 82.24753642082214, 83.81789636611938, 85.37503170967102, 86.94592046737671, 88.5120313167572, 90.08214282989502, 91.64184641838074, 93.210134267807, 94.77116966247559, 96.34403538703918, 97.91906833648682, 99.49106454849243, 101.05645775794983, 102.62542533874512, 104.18536686897278, 105.74407982826233, 107.31945776939392, 108.88808751106262, 110.45612502098083, 112.0139696598053, 113.57734751701355, 115.1405303478241, 116.70569038391113, 118.26459383964539, 119.82765436172485, 121.39128828048706, 122.94132113456726, 124.70966601371765, 126.36496639251709, 128.09126782417297, 129.6749768257141, 131.24525237083435, 132.84333896636963, 134.63534474372864, 136.2717752456665, 137.74192023277283, 139.15077805519104, 140.54663610458374, 141.93283033370972, 143.34817171096802, 144.73994088172913, 146.141459941864, 147.53596711158752, 148.94160556793213, 150.3559844493866, 151.74800038337708, 153.14989638328552, 154.5625445842743, 156.84493494033813]
[13.008333333333333, 14.783333333333333, 16.475, 13.941666666666666, 12.916666666666666, 9.925, 8.4, 8.483333333333333, 11.816666666666666, 11.816666666666666, 11.441666666666666, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.201, Test loss: 0.335, Test accuracy: 87.17
Average accuracy final 10 rounds: 86.84833333333333
1338.309240579605
[2.019369125366211, 3.7540252208709717, 5.474609375, 7.212928771972656, 8.946325778961182, 10.664479732513428, 12.298943281173706, 13.931092023849487, 15.656965970993042, 17.313971042633057, 19.081378698349, 20.791154861450195, 22.514766693115234, 24.23301672935486, 25.949283123016357, 27.66043734550476, 29.374175310134888, 31.101011276245117, 32.80352759361267, 34.51260542869568, 36.230117082595825, 37.95482015609741, 39.66375470161438, 41.37774181365967, 43.103564500808716, 44.8358588218689, 46.53474807739258, 48.25927019119263, 49.975656509399414, 51.75009608268738, 53.46504783630371, 55.18289017677307, 56.88493871688843, 58.60750341415405, 60.322752237319946, 62.026878118515015, 63.744802713394165, 65.45852994918823, 67.18060541152954, 68.89681005477905, 70.61864614486694, 72.30339288711548, 74.02107095718384, 75.71081519126892, 77.4131178855896, 79.09705305099487, 80.79058384895325, 82.48868203163147, 84.09591269493103, 85.64640951156616, 87.22334885597229, 88.79239583015442, 90.35811281204224, 91.920649766922, 93.47694802284241, 95.05386304855347, 96.659099817276, 98.22665476799011, 99.84599447250366, 101.41139650344849, 102.9769868850708, 104.55153918266296, 106.10391283035278, 107.66304278373718, 109.2320556640625, 110.81372165679932, 112.38961267471313, 113.95685648918152, 115.50958156585693, 117.06454277038574, 118.64541029930115, 120.21020030975342, 121.78040909767151, 123.34897208213806, 124.9260265827179, 126.50224041938782, 128.04717135429382, 129.6066699028015, 131.1570599079132, 132.73746252059937, 134.2991542816162, 135.85319542884827, 137.41820430755615, 139.0016167163849, 140.58020544052124, 142.1317446231842, 143.6980357170105, 145.26594400405884, 146.82805824279785, 148.3805215358734, 149.94930458068848, 151.52684593200684, 153.10990715026855, 154.66679859161377, 156.21285700798035, 157.7697856426239, 159.34993505477905, 160.9297034740448, 162.50003337860107, 164.04677057266235, 166.09982252120972]
[24.725, 41.858333333333334, 49.108333333333334, 53.95, 57.958333333333336, 63.325, 62.266666666666666, 63.516666666666666, 65.49166666666666, 73.65833333333333, 72.325, 71.93333333333334, 72.15, 76.78333333333333, 77.09166666666667, 78.66666666666667, 78.65, 78.575, 79.15, 79.44166666666666, 80.05833333333334, 80.46666666666667, 80.80833333333334, 81.26666666666667, 81.7, 81.76666666666667, 81.8, 81.90833333333333, 82.34166666666667, 82.90833333333333, 82.59166666666667, 82.63333333333334, 82.95833333333333, 82.89166666666667, 83.05833333333334, 83.05833333333334, 83.75, 83.68333333333334, 83.40833333333333, 83.35833333333333, 83.71666666666667, 83.975, 84.31666666666666, 84.15833333333333, 84.3, 84.14166666666667, 84.2, 84.49166666666666, 84.45, 84.71666666666667, 84.925, 84.73333333333333, 84.89166666666667, 85.00833333333334, 85.025, 85.28333333333333, 85.29166666666667, 85.425, 85.15833333333333, 85.61666666666666, 85.74166666666666, 85.725, 86.075, 85.725, 85.50833333333334, 86.025, 85.96666666666667, 85.84166666666667, 85.68333333333334, 85.58333333333333, 85.24166666666666, 85.99166666666666, 86.10833333333333, 85.94166666666666, 86.2, 86.08333333333333, 86.2, 86.38333333333334, 86.2, 86.20833333333333, 86.49166666666666, 86.55833333333334, 86.09166666666667, 86.35, 86.35833333333333, 86.41666666666667, 86.59166666666667, 86.68333333333334, 86.86666666666666, 86.625, 86.55833333333334, 86.85, 86.78333333333333, 86.74166666666666, 86.79166666666667, 87.1, 86.70833333333333, 87.05, 86.975, 86.925, 87.16666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.198, Test loss: 0.333, Test accuracy: 86.89
Average accuracy final 10 rounds: 86.74583333333332
1655.3847670555115
[2.1932783126831055, 4.386556625366211, 6.125100135803223, 7.863643646240234, 9.6170015335083, 11.370359420776367, 13.130180597305298, 14.890001773834229, 16.649778842926025, 18.409555912017822, 20.15326166152954, 21.89696741104126, 23.637258291244507, 25.377549171447754, 27.12372088432312, 28.869892597198486, 30.609524250030518, 32.34915590286255, 34.0791802406311, 35.80920457839966, 37.57452964782715, 39.33985471725464, 41.088067293167114, 42.83627986907959, 44.57629036903381, 46.31630086898804, 48.055222511291504, 49.79414415359497, 51.550965309143066, 53.30778646469116, 55.046391010284424, 56.784995555877686, 58.53268766403198, 60.28037977218628, 62.03247952461243, 63.784579277038574, 65.54060792922974, 67.2966365814209, 69.06140398979187, 70.82617139816284, 72.56510972976685, 74.30404806137085, 76.06458854675293, 77.82512903213501, 79.59565234184265, 81.3661756515503, 83.11832642555237, 84.87047719955444, 86.61771631240845, 88.36495542526245, 90.1239984035492, 91.88304138183594, 93.62956142425537, 95.3760814666748, 97.15228819847107, 98.92849493026733, 100.70857000350952, 102.48864507675171, 104.24847364425659, 106.00830221176147, 107.7640290260315, 109.51975584030151, 111.28056526184082, 113.04137468338013, 114.79312705993652, 116.54487943649292, 118.29989266395569, 120.05490589141846, 121.80211544036865, 123.54932498931885, 125.3027913570404, 127.05625772476196, 128.8159408569336, 130.57562398910522, 132.33238410949707, 134.08914422988892, 135.84693384170532, 137.60472345352173, 139.24699306488037, 140.889262676239, 142.67669200897217, 144.46412134170532, 146.19066762924194, 147.91721391677856, 149.66124534606934, 151.4052767753601, 153.1209201812744, 154.83656358718872, 156.62567162513733, 158.41477966308594, 160.12709546089172, 161.8394112586975, 163.55697679519653, 165.27454233169556, 167.01178550720215, 168.74902868270874, 170.46304965019226, 172.17707061767578, 173.83700680732727, 175.49694299697876, 177.17626190185547, 178.85558080673218, 180.56086611747742, 182.26615142822266, 184.00505709648132, 185.74396276474, 187.4604115486145, 189.176860332489, 190.87759685516357, 192.57833337783813, 194.23315262794495, 195.88797187805176, 197.5466525554657, 199.20533323287964, 200.87601137161255, 202.54668951034546, 204.2186737060547, 205.89065790176392, 207.5309488773346, 209.17123985290527, 210.79077005386353, 212.41030025482178, 213.98905444145203, 215.56780862808228, 217.23246932029724, 218.8971300125122, 220.5458686351776, 222.19460725784302, 223.80886268615723, 225.42311811447144, 227.0206594467163, 228.61820077896118, 230.24285197257996, 231.86750316619873, 233.5165560245514, 235.16560888290405, 236.64191341400146, 238.11821794509888, 239.7315080165863, 241.34479808807373, 242.96123552322388, 244.57767295837402, 246.18690419197083, 247.79613542556763, 249.40063524246216, 251.0051350593567, 252.6140217781067, 254.2229084968567, 255.83428168296814, 257.4456548690796, 259.0571279525757, 260.6686010360718, 262.2693204879761, 263.87003993988037, 265.53688311576843, 267.2037262916565, 268.8064019680023, 270.40907764434814, 272.023451089859, 273.6378245353699, 275.2484495639801, 276.85907459259033, 278.44058656692505, 280.02209854125977, 281.6097209453583, 283.1973433494568, 284.7863745689392, 286.37540578842163, 287.9700357913971, 289.56466579437256, 291.10345673561096, 292.64224767684937, 294.22218227386475, 295.8021168708801, 297.3946189880371, 298.9871211051941, 300.5829737186432, 302.1788263320923, 303.77170586586, 305.3645853996277, 306.9504249095917, 308.53626441955566, 310.12717175483704, 311.7180790901184, 313.25296092033386, 314.7878427505493, 316.39983081817627, 318.0118188858032, 319.6064622402191, 321.201105594635, 322.80620884895325, 324.4113121032715, 326.0255546569824, 327.63979721069336, 329.24163341522217, 330.843469619751, 332.44561553001404, 334.0477614402771, 335.65524792671204, 337.262734413147, 339.3337857723236, 341.40483713150024]
[23.5, 23.5, 45.28333333333333, 45.28333333333333, 43.80833333333333, 43.80833333333333, 53.2, 53.2, 58.93333333333333, 58.93333333333333, 66.56666666666666, 66.56666666666666, 70.63333333333334, 70.63333333333334, 73.13333333333334, 73.13333333333334, 73.7, 73.7, 73.94166666666666, 73.94166666666666, 73.925, 73.925, 77.43333333333334, 77.43333333333334, 78.3, 78.3, 78.03333333333333, 78.03333333333333, 78.74166666666666, 78.74166666666666, 79.275, 79.275, 79.58333333333333, 79.58333333333333, 80.08333333333333, 80.08333333333333, 80.21666666666667, 80.21666666666667, 80.86666666666666, 80.86666666666666, 80.74166666666666, 80.74166666666666, 80.99166666666666, 80.99166666666666, 80.90833333333333, 80.90833333333333, 81.2, 81.2, 81.55, 81.55, 82.29166666666667, 82.29166666666667, 82.56666666666666, 82.56666666666666, 81.36666666666666, 81.36666666666666, 82.23333333333333, 82.23333333333333, 82.275, 82.275, 82.50833333333334, 82.50833333333334, 82.9, 82.9, 83.44166666666666, 83.44166666666666, 82.95833333333333, 82.95833333333333, 83.2, 83.2, 82.775, 82.775, 82.88333333333334, 82.88333333333334, 83.66666666666667, 83.66666666666667, 84.09166666666667, 84.09166666666667, 84.1, 84.1, 84.33333333333333, 84.33333333333333, 84.2, 84.2, 84.39166666666667, 84.39166666666667, 84.7, 84.7, 84.51666666666667, 84.51666666666667, 84.675, 84.675, 84.41666666666667, 84.41666666666667, 84.76666666666667, 84.76666666666667, 84.975, 84.975, 84.975, 84.975, 84.45, 84.45, 85.19166666666666, 85.19166666666666, 85.125, 85.125, 84.76666666666667, 84.76666666666667, 85.35, 85.35, 85.425, 85.425, 85.36666666666666, 85.36666666666666, 84.875, 84.875, 85.775, 85.775, 85.66666666666667, 85.66666666666667, 85.60833333333333, 85.60833333333333, 85.675, 85.675, 85.44166666666666, 85.44166666666666, 85.94166666666666, 85.94166666666666, 85.85, 85.85, 85.85833333333333, 85.85833333333333, 85.78333333333333, 85.78333333333333, 85.95833333333333, 85.95833333333333, 86.10833333333333, 86.10833333333333, 85.79166666666667, 85.79166666666667, 85.95833333333333, 85.95833333333333, 85.86666666666666, 85.86666666666666, 86.40833333333333, 86.40833333333333, 86.04166666666667, 86.04166666666667, 86.43333333333334, 86.43333333333334, 86.39166666666667, 86.39166666666667, 86.51666666666667, 86.51666666666667, 86.41666666666667, 86.41666666666667, 86.68333333333334, 86.68333333333334, 86.26666666666667, 86.26666666666667, 86.25833333333334, 86.25833333333334, 86.775, 86.775, 86.68333333333334, 86.68333333333334, 86.45, 86.45, 86.73333333333333, 86.73333333333333, 86.6, 86.6, 86.86666666666666, 86.86666666666666, 86.83333333333333, 86.83333333333333, 86.53333333333333, 86.53333333333333, 86.4, 86.4, 86.34166666666667, 86.34166666666667, 86.525, 86.525, 86.86666666666666, 86.86666666666666, 87.03333333333333, 87.03333333333333, 86.68333333333334, 86.68333333333334, 86.775, 86.775, 86.74166666666666, 86.74166666666666, 87.18333333333334, 87.18333333333334, 86.6, 86.6, 86.70833333333333, 86.70833333333333, 86.89166666666667, 86.89166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.047, Test loss: 0.915, Test accuracy: 82.37
Final Round, Global train loss: 0.047, Global test loss: 2.095, Global test accuracy: 27.62
Average accuracy final 10 rounds: 82.78333333333333 

Average global accuracy final 10 rounds: 28.461666666666666 

1859.5182900428772
[1.6533730030059814, 3.306746006011963, 4.694247722625732, 6.081749439239502, 7.532066345214844, 8.982383251190186, 10.500253677368164, 12.018124103546143, 13.445971012115479, 14.873817920684814, 16.245370626449585, 17.616923332214355, 19.093000411987305, 20.569077491760254, 22.05255103111267, 23.536024570465088, 24.920055627822876, 26.304086685180664, 27.71846914291382, 29.132851600646973, 30.646048545837402, 32.15924549102783, 33.427796363830566, 34.6963472366333, 35.993932008743286, 37.29151678085327, 38.66762089729309, 40.04372501373291, 41.30836796760559, 42.57301092147827, 43.89804816246033, 45.22308540344238, 46.60000491142273, 47.976924419403076, 49.30010652542114, 50.62328863143921, 51.95119333267212, 53.27909803390503, 54.59000205993652, 55.90090608596802, 57.214131355285645, 58.52735662460327, 59.854700803756714, 61.182044982910156, 62.4902184009552, 63.798391819000244, 65.11768627166748, 66.43698072433472, 67.72429275512695, 69.01160478591919, 70.29982447624207, 71.58804416656494, 72.87355184555054, 74.15905952453613, 75.45315051078796, 76.7472414970398, 78.05707836151123, 79.36691522598267, 80.6668028831482, 81.96669054031372, 83.27597165107727, 84.58525276184082, 85.9585473537445, 87.3318419456482, 88.62173676490784, 89.91163158416748, 91.25512290000916, 92.59861421585083, 93.89483308792114, 95.19105195999146, 96.4742579460144, 97.75746393203735, 99.05046200752258, 100.34346008300781, 101.64728260040283, 102.95110511779785, 104.24148201942444, 105.53185892105103, 106.82239699363708, 108.11293506622314, 109.44248342514038, 110.77203178405762, 112.0720624923706, 113.3720932006836, 114.66519498825073, 115.95829677581787, 117.24319672584534, 118.5280966758728, 119.82674193382263, 121.12538719177246, 122.43612313270569, 123.74685907363892, 125.0604658126831, 126.3740725517273, 127.6587905883789, 128.94350862503052, 130.2496955394745, 131.55588245391846, 132.8576855659485, 134.15948867797852, 135.44987320899963, 136.74025774002075, 138.03241777420044, 139.32457780838013, 140.62869501113892, 141.9328122138977, 143.24062180519104, 144.54843139648438, 145.828022480011, 147.1076135635376, 148.47393417358398, 149.84025478363037, 151.14715003967285, 152.45404529571533, 153.716894865036, 154.9797444343567, 156.3503556251526, 157.7209668159485, 159.07133531570435, 160.4217038154602, 161.71404337882996, 163.0063829421997, 164.39735627174377, 165.78832960128784, 167.14098453521729, 168.49363946914673, 169.79784989356995, 171.10206031799316, 172.42055869102478, 173.7390570640564, 175.08263421058655, 176.4262113571167, 177.74401664733887, 179.06182193756104, 180.34596633911133, 181.63011074066162, 182.9541916847229, 184.27827262878418, 185.60468816757202, 186.93110370635986, 188.23118257522583, 189.5312614440918, 190.83628129959106, 192.14130115509033, 193.47098398208618, 194.80066680908203, 196.121258020401, 197.44184923171997, 198.75658202171326, 200.07131481170654, 201.391051530838, 202.71078824996948, 204.02838826179504, 205.3459882736206, 206.6439950466156, 207.9420018196106, 209.25071477890015, 210.5594277381897, 211.88468647003174, 213.20994520187378, 214.52606010437012, 215.84217500686646, 217.12930130958557, 218.4164276123047, 219.7765097618103, 221.13659191131592, 222.51434922218323, 223.89210653305054, 225.15565943717957, 226.4192123413086, 227.74566912651062, 229.07212591171265, 230.4402129650116, 231.80830001831055, 233.08110785484314, 234.35391569137573, 235.65142011642456, 236.9489245414734, 238.3282928466797, 239.707661151886, 241.00585317611694, 242.3040452003479, 243.6032989025116, 244.9025526046753, 246.23306727409363, 247.56358194351196, 248.92883038520813, 250.2940788269043, 251.57840943336487, 252.86274003982544, 254.18417763710022, 255.505615234375, 256.86782360076904, 258.2300319671631, 259.49356269836426, 260.75709342956543, 262.05986189842224, 263.36263036727905, 264.73790526390076, 266.11318016052246, 268.4184353351593, 270.72369050979614]
[25.933333333333334, 25.933333333333334, 46.166666666666664, 46.166666666666664, 57.00833333333333, 57.00833333333333, 57.38333333333333, 57.38333333333333, 62.233333333333334, 62.233333333333334, 65.025, 65.025, 65.625, 65.625, 69.625, 69.625, 71.99166666666666, 71.99166666666666, 72.96666666666667, 72.96666666666667, 72.15833333333333, 72.15833333333333, 74.025, 74.025, 73.3, 73.3, 74.04166666666667, 74.04166666666667, 74.9, 74.9, 77.80833333333334, 77.80833333333334, 78.65, 78.65, 78.84166666666667, 78.84166666666667, 79.15833333333333, 79.15833333333333, 79.03333333333333, 79.03333333333333, 78.8, 78.8, 78.18333333333334, 78.18333333333334, 78.40833333333333, 78.40833333333333, 78.725, 78.725, 79.26666666666667, 79.26666666666667, 79.18333333333334, 79.18333333333334, 79.75, 79.75, 79.64166666666667, 79.64166666666667, 80.03333333333333, 80.03333333333333, 80.81666666666666, 80.81666666666666, 80.66666666666667, 80.66666666666667, 80.79166666666667, 80.79166666666667, 80.88333333333334, 80.88333333333334, 80.875, 80.875, 81.20833333333333, 81.20833333333333, 80.81666666666666, 80.81666666666666, 81.18333333333334, 81.18333333333334, 80.64166666666667, 80.64166666666667, 81.25833333333334, 81.25833333333334, 81.24166666666666, 81.24166666666666, 81.4, 81.4, 81.38333333333334, 81.38333333333334, 81.53333333333333, 81.53333333333333, 81.45833333333333, 81.45833333333333, 81.925, 81.925, 81.54166666666667, 81.54166666666667, 81.26666666666667, 81.26666666666667, 81.45, 81.45, 81.925, 81.925, 81.83333333333333, 81.83333333333333, 81.675, 81.675, 81.475, 81.475, 81.06666666666666, 81.06666666666666, 81.225, 81.225, 81.6, 81.6, 81.89166666666667, 81.89166666666667, 81.65833333333333, 81.65833333333333, 81.675, 81.675, 82.325, 82.325, 81.96666666666667, 81.96666666666667, 82.15, 82.15, 82.56666666666666, 82.56666666666666, 82.50833333333334, 82.50833333333334, 82.275, 82.275, 82.275, 82.275, 82.34166666666667, 82.34166666666667, 82.125, 82.125, 81.88333333333334, 81.88333333333334, 81.94166666666666, 81.94166666666666, 82.33333333333333, 82.33333333333333, 82.24166666666666, 82.24166666666666, 82.4, 82.4, 82.375, 82.375, 82.5, 82.5, 82.05833333333334, 82.05833333333334, 82.25833333333334, 82.25833333333334, 81.58333333333333, 81.58333333333333, 81.85, 81.85, 81.95, 81.95, 82.425, 82.425, 82.35833333333333, 82.35833333333333, 82.20833333333333, 82.20833333333333, 82.39166666666667, 82.39166666666667, 82.44166666666666, 82.44166666666666, 82.45, 82.45, 82.55, 82.55, 82.45, 82.45, 82.64166666666667, 82.64166666666667, 82.70833333333333, 82.70833333333333, 82.55833333333334, 82.55833333333334, 82.35, 82.35, 82.35833333333333, 82.35833333333333, 82.525, 82.525, 82.91666666666667, 82.91666666666667, 82.84166666666667, 82.84166666666667, 82.875, 82.875, 83.075, 83.075, 83.15, 83.15, 82.9, 82.9, 82.84166666666667, 82.84166666666667, 82.36666666666666, 82.36666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.136, Test loss: 0.471, Test accuracy: 86.79
Final Round, Global train loss: 0.136, Global test loss: 1.091, Global test accuracy: 64.70
Average accuracy final 10 rounds: 86.33749999999999 

Average global accuracy final 10 rounds: 62.095 

2078.4057388305664
[1.5504639148712158, 3.1009278297424316, 4.410255193710327, 5.719582557678223, 7.017308473587036, 8.31503438949585, 9.622627973556519, 10.930221557617188, 12.270838499069214, 13.61145544052124, 14.881822347640991, 16.152189254760742, 17.439539670944214, 18.726890087127686, 20.142757654190063, 21.55862522125244, 22.98221182823181, 24.40579843521118, 25.87887716293335, 27.351955890655518, 28.75284767150879, 30.15373945236206, 31.561180591583252, 32.96862173080444, 34.37904739379883, 35.78947305679321, 37.24758553504944, 38.705698013305664, 40.17967224121094, 41.65364646911621, 43.06772422790527, 44.481801986694336, 45.8927116394043, 47.30362129211426, 48.77852535247803, 50.2534294128418, 51.72251081466675, 53.1915922164917, 54.636536836624146, 56.08148145675659, 57.52325963973999, 58.96503782272339, 60.31263470649719, 61.660231590270996, 63.00467085838318, 64.34911012649536, 65.62116122245789, 66.89321231842041, 68.17336559295654, 69.45351886749268, 70.73494791984558, 72.01637697219849, 73.49122643470764, 74.9660758972168, 76.42233610153198, 77.87859630584717, 79.27254033088684, 80.66648435592651, 82.10403251647949, 83.54158067703247, 84.81441235542297, 86.08724403381348, 87.35742378234863, 88.62760353088379, 89.94253540039062, 91.25746726989746, 92.53591465950012, 93.81436204910278, 95.1164140701294, 96.418466091156, 97.71517944335938, 99.01189279556274, 100.2920310497284, 101.57216930389404, 102.89128565788269, 104.21040201187134, 105.55238771438599, 106.89437341690063, 108.18789005279541, 109.48140668869019, 110.9035975933075, 112.3257884979248, 113.81566739082336, 115.30554628372192, 116.62906289100647, 117.95257949829102, 119.36543917655945, 120.77829885482788, 122.20921730995178, 123.64013576507568, 125.03805637359619, 126.4359769821167, 127.79027986526489, 129.1445827484131, 130.45929169654846, 131.77400064468384, 133.14534378051758, 134.51668691635132, 135.81429648399353, 137.11190605163574, 138.47328925132751, 139.8346724510193, 141.23997330665588, 142.64527416229248, 144.051011800766, 145.4567494392395, 146.85049295425415, 148.2442364692688, 149.85865378379822, 151.47307109832764, 153.17878675460815, 154.88450241088867, 156.55414605140686, 158.22378969192505, 159.81873059272766, 161.41367149353027, 163.04291486740112, 164.67215824127197, 166.1981987953186, 167.72423934936523, 169.19719791412354, 170.67015647888184, 172.16693663597107, 173.6637167930603, 175.2338263988495, 176.80393600463867, 178.39854907989502, 179.99316215515137, 181.6343550682068, 183.2755479812622, 184.85898303985596, 186.4424180984497, 188.08473992347717, 189.72706174850464, 191.36827993392944, 193.00949811935425, 194.49375104904175, 195.97800397872925, 197.45593976974487, 198.9338755607605, 200.49125337600708, 202.04863119125366, 203.52298998832703, 204.9973487854004, 206.48291039466858, 207.96847200393677, 209.45803141593933, 210.9475908279419, 212.5717625617981, 214.1959342956543, 215.7020604610443, 217.20818662643433, 218.76911449432373, 220.33004236221313, 221.91699266433716, 223.50394296646118, 224.9644968509674, 226.42505073547363, 227.99704933166504, 229.56904792785645, 231.11168909072876, 232.65433025360107, 234.22041845321655, 235.78650665283203, 237.2793881893158, 238.77226972579956, 240.38886976242065, 242.00546979904175, 243.663414478302, 245.32135915756226, 246.91368532180786, 248.50601148605347, 250.14484763145447, 251.78368377685547, 253.31170296669006, 254.83972215652466, 256.3574924468994, 257.87526273727417, 259.3579795360565, 260.84069633483887, 262.3665175437927, 263.8923387527466, 265.391459941864, 266.89058113098145, 268.4393846988678, 269.98818826675415, 271.53300380706787, 273.0778193473816, 274.654531955719, 276.2312445640564, 277.6712384223938, 279.1112322807312, 280.62746143341064, 282.1436905860901, 283.67025423049927, 285.19681787490845, 286.67900466918945, 288.16119146347046, 289.68203377723694, 291.2028760910034, 293.75923132896423, 296.31558656692505]
[28.025, 28.025, 39.05, 39.05, 44.141666666666666, 44.141666666666666, 55.725, 55.725, 63.81666666666667, 63.81666666666667, 68.33333333333333, 68.33333333333333, 71.025, 71.025, 71.9, 71.9, 76.025, 76.025, 76.70833333333333, 76.70833333333333, 77.225, 77.225, 78.325, 78.325, 78.375, 78.375, 78.46666666666667, 78.46666666666667, 79.05833333333334, 79.05833333333334, 79.03333333333333, 79.03333333333333, 79.6, 79.6, 79.41666666666667, 79.41666666666667, 80.325, 80.325, 80.90833333333333, 80.90833333333333, 80.75833333333334, 80.75833333333334, 80.475, 80.475, 80.33333333333333, 80.33333333333333, 81.24166666666666, 81.24166666666666, 80.99166666666666, 80.99166666666666, 81.49166666666666, 81.49166666666666, 81.59166666666667, 81.59166666666667, 81.875, 81.875, 82.71666666666667, 82.71666666666667, 82.74166666666666, 82.74166666666666, 82.74166666666666, 82.74166666666666, 83.79166666666667, 83.79166666666667, 84.0, 84.0, 84.075, 84.075, 84.00833333333334, 84.00833333333334, 84.29166666666667, 84.29166666666667, 83.98333333333333, 83.98333333333333, 84.125, 84.125, 84.33333333333333, 84.33333333333333, 84.45833333333333, 84.45833333333333, 84.2, 84.2, 84.45833333333333, 84.45833333333333, 84.53333333333333, 84.53333333333333, 84.14166666666667, 84.14166666666667, 84.20833333333333, 84.20833333333333, 84.575, 84.575, 84.275, 84.275, 84.53333333333333, 84.53333333333333, 85.09166666666667, 85.09166666666667, 84.96666666666667, 84.96666666666667, 85.01666666666667, 85.01666666666667, 85.19166666666666, 85.19166666666666, 85.83333333333333, 85.83333333333333, 85.775, 85.775, 85.05833333333334, 85.05833333333334, 85.20833333333333, 85.20833333333333, 85.31666666666666, 85.31666666666666, 85.5, 85.5, 85.36666666666666, 85.36666666666666, 85.40833333333333, 85.40833333333333, 85.61666666666666, 85.61666666666666, 85.69166666666666, 85.69166666666666, 85.2, 85.2, 85.41666666666667, 85.41666666666667, 85.60833333333333, 85.60833333333333, 85.84166666666667, 85.84166666666667, 85.86666666666666, 85.86666666666666, 86.14166666666667, 86.14166666666667, 85.86666666666666, 85.86666666666666, 85.60833333333333, 85.60833333333333, 85.49166666666666, 85.49166666666666, 85.35, 85.35, 85.84166666666667, 85.84166666666667, 85.69166666666666, 85.69166666666666, 85.66666666666667, 85.66666666666667, 85.25, 85.25, 85.33333333333333, 85.33333333333333, 85.45, 85.45, 85.55833333333334, 85.55833333333334, 85.50833333333334, 85.50833333333334, 86.05, 86.05, 85.84166666666667, 85.84166666666667, 86.40833333333333, 86.40833333333333, 86.48333333333333, 86.48333333333333, 86.075, 86.075, 85.99166666666666, 85.99166666666666, 85.925, 85.925, 85.63333333333334, 85.63333333333334, 85.70833333333333, 85.70833333333333, 86.05, 86.05, 86.05, 86.05, 86.31666666666666, 86.31666666666666, 86.59166666666667, 86.59166666666667, 86.70833333333333, 86.70833333333333, 86.60833333333333, 86.60833333333333, 86.33333333333333, 86.33333333333333, 86.11666666666666, 86.11666666666666, 85.83333333333333, 85.83333333333333, 86.35, 86.35, 86.46666666666667, 86.46666666666667, 86.79166666666667, 86.79166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.183, Test loss: 0.353, Test accuracy: 87.22
Average accuracy final 10 rounds: 87.0975 

1742.1363413333893
[1.6923465728759766, 3.384693145751953, 4.809221982955933, 6.233750820159912, 7.69359016418457, 9.153429508209229, 10.524348258972168, 11.895267009735107, 13.237996578216553, 14.580726146697998, 16.00988507270813, 17.43904399871826, 18.781818389892578, 20.124592781066895, 21.540921211242676, 22.957249641418457, 24.376848936080933, 25.796448230743408, 27.20103144645691, 28.60561466217041, 30.034430980682373, 31.463247299194336, 32.871224880218506, 34.279202461242676, 35.68860054016113, 37.09799861907959, 38.442665338516235, 39.78733205795288, 41.14598035812378, 42.50462865829468, 43.8698844909668, 45.235140323638916, 46.61770963668823, 48.00027894973755, 49.37644839286804, 50.752617835998535, 52.14097023010254, 53.52932262420654, 54.920809745788574, 56.312296867370605, 57.773396015167236, 59.23449516296387, 60.58490037918091, 61.93530559539795, 63.27741003036499, 64.61951446533203, 65.97629523277283, 67.33307600021362, 68.71692228317261, 70.10076856613159, 71.4699125289917, 72.8390564918518, 74.23667430877686, 75.6342921257019, 77.02899742126465, 78.42370271682739, 79.83071637153625, 81.23773002624512, 82.72984552383423, 84.22196102142334, 85.68278551101685, 87.14361000061035, 88.57659196853638, 90.0095739364624, 91.48983550071716, 92.97009706497192, 94.40170001983643, 95.83330297470093, 97.24596333503723, 98.65862369537354, 100.080983877182, 101.50334405899048, 102.94080567359924, 104.37826728820801, 105.81211614608765, 107.24596500396729, 108.69321084022522, 110.14045667648315, 111.57003879547119, 112.99962091445923, 114.4387936592102, 115.87796640396118, 117.2696385383606, 118.66131067276001, 120.05768823623657, 121.45406579971313, 122.90306496620178, 124.35206413269043, 125.8321328163147, 127.31220149993896, 128.8028905391693, 130.29357957839966, 131.76206135749817, 133.23054313659668, 134.72917556762695, 136.22780799865723, 137.74398732185364, 139.26016664505005, 140.66667437553406, 142.07318210601807, 143.52971506118774, 144.98624801635742, 146.4267053604126, 147.86716270446777, 149.3066782951355, 150.74619388580322, 152.15313696861267, 153.56008005142212, 155.01952743530273, 156.47897481918335, 157.97561955451965, 159.47226428985596, 160.97948932647705, 162.48671436309814, 163.92593502998352, 165.3651556968689, 166.8012068271637, 168.2372579574585, 169.7043433189392, 171.17142868041992, 172.57079648971558, 173.97016429901123, 175.36971282958984, 176.76926136016846, 178.16874957084656, 179.56823778152466, 180.99343132972717, 182.4186248779297, 183.84350991249084, 185.268394947052, 186.74890232086182, 188.22940969467163, 189.66250944137573, 191.09560918807983, 192.57929134368896, 194.0629734992981, 195.55514550209045, 197.0473175048828, 198.53685784339905, 200.02639818191528, 201.4902741909027, 202.95415019989014, 204.43522191047668, 205.91629362106323, 207.34273147583008, 208.76916933059692, 210.19412779808044, 211.61908626556396, 213.07408690452576, 214.52908754348755, 215.92922353744507, 217.3293595314026, 218.84314131736755, 220.35692310333252, 221.85379362106323, 223.35066413879395, 224.8171546459198, 226.28364515304565, 227.7040114402771, 229.12437772750854, 230.48203349113464, 231.83968925476074, 233.25881481170654, 234.67794036865234, 236.18852353096008, 237.69910669326782, 239.18222284317017, 240.6653389930725, 242.13884949684143, 243.61236000061035, 245.1297254562378, 246.64709091186523, 248.16973996162415, 249.69238901138306, 251.17958998680115, 252.66679096221924, 253.9928023815155, 255.31881380081177, 256.6608510017395, 258.00288820266724, 259.38063621520996, 260.7583842277527, 262.1605975627899, 263.56281089782715, 264.991331577301, 266.4198522567749, 267.94486951828003, 269.46988677978516, 270.9138021469116, 272.3577175140381, 273.81110882759094, 275.2645001411438, 276.6971263885498, 278.1297526359558, 279.5310196876526, 280.93228673934937, 282.33232975006104, 283.7323727607727, 285.1043791770935, 286.4763855934143, 288.70161652565, 290.92684745788574]
[19.941666666666666, 19.941666666666666, 40.141666666666666, 40.141666666666666, 39.8, 39.8, 49.958333333333336, 49.958333333333336, 57.09166666666667, 57.09166666666667, 64.925, 64.925, 70.33333333333333, 70.33333333333333, 72.79166666666667, 72.79166666666667, 72.98333333333333, 72.98333333333333, 74.70833333333333, 74.70833333333333, 75.83333333333333, 75.83333333333333, 75.69166666666666, 75.69166666666666, 77.425, 77.425, 77.28333333333333, 77.28333333333333, 77.55, 77.55, 78.0, 78.0, 78.70833333333333, 78.70833333333333, 79.275, 79.275, 80.06666666666666, 80.06666666666666, 80.425, 80.425, 80.99166666666666, 80.99166666666666, 81.24166666666666, 81.24166666666666, 81.81666666666666, 81.81666666666666, 81.65, 81.65, 81.93333333333334, 81.93333333333334, 81.73333333333333, 81.73333333333333, 82.15, 82.15, 82.775, 82.775, 82.9, 82.9, 82.61666666666666, 82.61666666666666, 82.875, 82.875, 82.99166666666666, 82.99166666666666, 83.01666666666667, 83.01666666666667, 83.5, 83.5, 83.41666666666667, 83.41666666666667, 84.16666666666667, 84.16666666666667, 83.9, 83.9, 83.625, 83.625, 83.91666666666667, 83.91666666666667, 83.8, 83.8, 83.775, 83.775, 83.975, 83.975, 83.74166666666666, 83.74166666666666, 84.325, 84.325, 84.45833333333333, 84.45833333333333, 84.61666666666666, 84.61666666666666, 84.39166666666667, 84.39166666666667, 84.39166666666667, 84.39166666666667, 84.76666666666667, 84.76666666666667, 84.83333333333333, 84.83333333333333, 84.725, 84.725, 84.53333333333333, 84.53333333333333, 84.76666666666667, 84.76666666666667, 84.99166666666666, 84.99166666666666, 84.45833333333333, 84.45833333333333, 85.59166666666667, 85.59166666666667, 85.675, 85.675, 85.53333333333333, 85.53333333333333, 85.60833333333333, 85.60833333333333, 85.525, 85.525, 85.525, 85.525, 85.49166666666666, 85.49166666666666, 85.71666666666667, 85.71666666666667, 85.50833333333334, 85.50833333333334, 85.65833333333333, 85.65833333333333, 85.60833333333333, 85.60833333333333, 86.33333333333333, 86.33333333333333, 86.19166666666666, 86.19166666666666, 86.35833333333333, 86.35833333333333, 86.40833333333333, 86.40833333333333, 85.80833333333334, 85.80833333333334, 86.20833333333333, 86.20833333333333, 86.275, 86.275, 86.03333333333333, 86.03333333333333, 85.98333333333333, 85.98333333333333, 86.375, 86.375, 86.3, 86.3, 86.25833333333334, 86.25833333333334, 86.65, 86.65, 86.71666666666667, 86.71666666666667, 86.39166666666667, 86.39166666666667, 86.73333333333333, 86.73333333333333, 86.76666666666667, 86.76666666666667, 86.60833333333333, 86.60833333333333, 86.80833333333334, 86.80833333333334, 86.775, 86.775, 86.85833333333333, 86.85833333333333, 86.86666666666666, 86.86666666666666, 86.78333333333333, 86.78333333333333, 86.71666666666667, 86.71666666666667, 86.95, 86.95, 87.00833333333334, 87.00833333333334, 87.03333333333333, 87.03333333333333, 86.93333333333334, 86.93333333333334, 87.18333333333334, 87.18333333333334, 87.35833333333333, 87.35833333333333, 87.24166666666666, 87.24166666666666, 87.125, 87.125, 87.18333333333334, 87.18333333333334, 86.95833333333333, 86.95833333333333, 87.225, 87.225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.105, Test loss: 0.403, Test accuracy: 87.82
Average accuracy final 10 rounds: 87.61722222222224 

2652.314126968384
[2.5389552116394043, 5.077910423278809, 7.4844982624053955, 9.891086101531982, 12.256566286087036, 14.62204647064209, 16.848803758621216, 19.075561046600342, 21.54745650291443, 24.019351959228516, 26.331441640853882, 28.643531322479248, 31.04185652732849, 33.440181732177734, 35.758474826812744, 38.076767921447754, 40.45880103111267, 42.84083414077759, 45.21838665008545, 47.59593915939331, 49.97186064720154, 52.347782135009766, 54.81387734413147, 57.279972553253174, 59.46297073364258, 61.64596891403198, 64.11325001716614, 66.5805311203003, 69.01787519454956, 71.45521926879883, 73.69626760482788, 75.93731594085693, 78.2008810043335, 80.46444606781006, 82.63949465751648, 84.8145432472229, 87.01689386367798, 89.21924448013306, 91.4407103061676, 93.66217613220215, 95.86177515983582, 98.06137418746948, 100.23783016204834, 102.4142861366272, 104.63009524345398, 106.84590435028076, 109.05045056343079, 111.25499677658081, 113.49679851531982, 115.73860025405884, 117.94504594802856, 120.15149164199829, 122.34358429908752, 124.53567695617676, 126.93479943275452, 129.33392190933228, 131.53387188911438, 133.73382186889648, 136.03578901290894, 138.3377561569214, 140.5738296508789, 142.80990314483643, 145.02595567703247, 147.24200820922852, 149.4841091632843, 151.7262101173401, 154.01746702194214, 156.3087239265442, 158.55592846870422, 160.80313301086426, 163.05129194259644, 165.2994508743286, 167.46891856193542, 169.63838624954224, 171.92896676063538, 174.21954727172852, 176.4244146347046, 178.62928199768066, 180.8284080028534, 183.02753400802612, 185.224937915802, 187.42234182357788, 189.66905641555786, 191.91577100753784, 194.1360559463501, 196.35634088516235, 198.59222602844238, 200.8281111717224, 203.04363560676575, 205.25916004180908, 207.47614789009094, 209.6931357383728, 211.970716714859, 214.24829769134521, 216.4969356060028, 218.7455735206604, 221.23349380493164, 223.72141408920288, 226.07115149497986, 228.42088890075684, 230.80458784103394, 233.18828678131104, 235.6120104789734, 238.03573417663574, 240.3948609828949, 242.75398778915405, 245.18443703651428, 247.6148862838745, 250.04279613494873, 252.47070598602295, 254.69549989700317, 256.9202938079834, 259.0873260498047, 261.254358291626, 263.50120544433594, 265.7480525970459, 267.942862033844, 270.1376714706421, 272.510573387146, 274.8834753036499, 277.27287769317627, 279.66228008270264, 281.9995148181915, 284.3367495536804, 286.6621685028076, 288.9875874519348, 291.26598381996155, 293.5443801879883, 295.85247564315796, 298.16057109832764, 300.4653367996216, 302.7701025009155, 305.18819546699524, 307.60628843307495, 309.9429497718811, 312.27961111068726, 314.63886427879333, 316.9981174468994, 319.2239909172058, 321.4498643875122, 323.6434190273285, 325.8369736671448, 327.97737169265747, 330.11776971817017, 332.3298716545105, 334.54197359085083, 336.80778646469116, 339.0735993385315, 341.2447052001953, 343.41581106185913, 345.59815788269043, 347.78050470352173, 350.00050711631775, 352.22050952911377, 354.3577980995178, 356.4950866699219, 358.71847009658813, 360.9418535232544, 363.16072058677673, 365.3795876502991, 367.65607237815857, 369.93255710601807, 372.16966938972473, 374.4067816734314, 376.5953781604767, 378.783974647522, 380.94014072418213, 383.0963068008423, 385.28591203689575, 387.4755172729492, 389.6322236061096, 391.78892993927, 393.9008128643036, 396.01269578933716, 398.18873381614685, 400.36477184295654, 402.60243797302246, 404.8401041030884, 407.09991908073425, 409.3597340583801, 411.53225469589233, 413.70477533340454, 415.84265899658203, 417.9805426597595, 420.1416435241699, 422.3027443885803, 424.4619390964508, 426.6211338043213, 428.7698748111725, 430.9186158180237, 433.05998253822327, 435.20134925842285, 437.3973424434662, 439.5933356285095, 441.8068470954895, 444.0203585624695, 446.31505489349365, 448.6097512245178, 450.93804025650024, 453.26632928848267, 455.56543946266174, 457.8645496368408]
[31.333333333333332, 31.333333333333332, 40.516666666666666, 40.516666666666666, 49.81111111111111, 49.81111111111111, 49.32222222222222, 49.32222222222222, 63.766666666666666, 63.766666666666666, 63.63333333333333, 63.63333333333333, 72.36666666666666, 72.36666666666666, 73.15, 73.15, 71.1, 71.1, 71.7388888888889, 71.7388888888889, 77.56666666666666, 77.56666666666666, 77.47222222222223, 77.47222222222223, 78.9, 78.9, 79.9888888888889, 79.9888888888889, 81.93888888888888, 81.93888888888888, 82.06111111111112, 82.06111111111112, 81.56666666666666, 81.56666666666666, 82.08888888888889, 82.08888888888889, 83.40555555555555, 83.40555555555555, 83.39444444444445, 83.39444444444445, 83.82777777777778, 83.82777777777778, 83.54444444444445, 83.54444444444445, 84.7, 84.7, 84.66666666666667, 84.66666666666667, 84.68333333333334, 84.68333333333334, 84.41666666666667, 84.41666666666667, 84.50555555555556, 84.50555555555556, 84.97777777777777, 84.97777777777777, 84.88888888888889, 84.88888888888889, 85.37777777777778, 85.37777777777778, 85.51666666666667, 85.51666666666667, 85.05555555555556, 85.05555555555556, 85.32222222222222, 85.32222222222222, 84.7, 84.7, 86.01666666666667, 86.01666666666667, 85.8, 85.8, 85.91666666666667, 85.91666666666667, 86.29444444444445, 86.29444444444445, 86.35, 86.35, 86.5, 86.5, 86.58888888888889, 86.58888888888889, 85.81111111111112, 85.81111111111112, 86.25555555555556, 86.25555555555556, 86.58333333333333, 86.58333333333333, 86.7, 86.7, 86.24444444444444, 86.24444444444444, 86.86666666666666, 86.86666666666666, 86.83888888888889, 86.83888888888889, 87.07777777777778, 87.07777777777778, 87.17777777777778, 87.17777777777778, 87.22222222222223, 87.22222222222223, 87.24444444444444, 87.24444444444444, 87.12222222222222, 87.12222222222222, 86.68888888888888, 86.68888888888888, 87.19444444444444, 87.19444444444444, 87.07222222222222, 87.07222222222222, 87.17777777777778, 87.17777777777778, 86.79444444444445, 86.79444444444445, 87.07777777777778, 87.07777777777778, 87.39444444444445, 87.39444444444445, 87.13888888888889, 87.13888888888889, 87.35, 87.35, 87.2, 87.2, 87.38333333333334, 87.38333333333334, 87.46111111111111, 87.46111111111111, 87.75555555555556, 87.75555555555556, 87.4888888888889, 87.4888888888889, 87.69444444444444, 87.69444444444444, 87.82222222222222, 87.82222222222222, 87.07222222222222, 87.07222222222222, 87.2, 87.2, 87.22222222222223, 87.22222222222223, 87.78333333333333, 87.78333333333333, 87.7, 87.7, 87.46666666666667, 87.46666666666667, 87.91666666666667, 87.91666666666667, 87.65, 87.65, 87.65555555555555, 87.65555555555555, 87.51666666666667, 87.51666666666667, 87.86666666666666, 87.86666666666666, 87.46111111111111, 87.46111111111111, 87.82777777777778, 87.82777777777778, 87.61666666666666, 87.61666666666666, 87.71666666666667, 87.71666666666667, 87.77222222222223, 87.77222222222223, 88.05, 88.05, 87.65, 87.65, 87.83888888888889, 87.83888888888889, 87.75, 87.75, 87.66666666666667, 87.66666666666667, 87.82777777777778, 87.82777777777778, 87.43333333333334, 87.43333333333334, 87.7388888888889, 87.7388888888889, 87.4, 87.4, 87.6, 87.6, 87.87777777777778, 87.87777777777778, 87.33333333333333, 87.33333333333333, 87.7611111111111, 87.7611111111111, 87.53888888888889, 87.53888888888889, 87.66111111111111, 87.66111111111111, 87.82222222222222, 87.82222222222222]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.031, Test loss: 0.889, Test accuracy: 84.19
Average accuracy final 10 rounds: 83.5888888888889 

2616.7433698177338
[2.6634366512298584, 5.326873302459717, 7.726099014282227, 10.125324726104736, 12.409938097000122, 14.694551467895508, 17.008958339691162, 19.323365211486816, 21.41378092765808, 23.504196643829346, 25.647238969802856, 27.790281295776367, 29.947460412979126, 32.104639530181885, 34.35231566429138, 36.59999179840088, 38.7301127910614, 40.860233783721924, 43.03100609779358, 45.201778411865234, 47.39944505691528, 49.59711170196533, 51.807398080825806, 54.01768445968628, 56.196139097213745, 58.37459373474121, 60.564253091812134, 62.75391244888306, 64.96029663085938, 67.1666808128357, 69.47557806968689, 71.78447532653809, 73.99409413337708, 76.20371294021606, 78.40036201477051, 80.59701108932495, 82.82664561271667, 85.0562801361084, 87.25579905509949, 89.45531797409058, 91.59807920455933, 93.74084043502808, 95.89003992080688, 98.0392394065857, 100.42707419395447, 102.81490898132324, 105.00988292694092, 107.2048568725586, 109.39567255973816, 111.58648824691772, 113.72033143043518, 115.85417461395264, 118.01457118988037, 120.1749677658081, 122.38011384010315, 124.5852599143982, 126.74678564071655, 128.9083113670349, 131.0687472820282, 133.22918319702148, 135.34636235237122, 137.46354150772095, 139.64016556739807, 141.8167896270752, 144.0493884086609, 146.28198719024658, 148.40623784065247, 150.53048849105835, 152.664235830307, 154.79798316955566, 156.93546772003174, 159.0729522705078, 161.28603625297546, 163.49912023544312, 165.64358162879944, 167.78804302215576, 169.96156692504883, 172.1350908279419, 174.27965545654297, 176.42422008514404, 178.5751953125, 180.72617053985596, 182.9097957611084, 185.09342098236084, 187.23105263710022, 189.3686842918396, 191.4829659461975, 193.59724760055542, 195.74734687805176, 197.8974461555481, 200.00821113586426, 202.11897611618042, 204.23880887031555, 206.35864162445068, 208.4904556274414, 210.62226963043213, 212.8144233226776, 215.0065770149231, 217.3400800228119, 219.67358303070068, 221.9790322780609, 224.28448152542114, 226.57211422920227, 228.8597469329834, 231.07516479492188, 233.29058265686035, 235.48712515830994, 237.68366765975952, 239.90656208992004, 242.12945652008057, 244.33786749839783, 246.5462784767151, 248.8659110069275, 251.1855435371399, 253.4181089401245, 255.65067434310913, 257.87830805778503, 260.10594177246094, 262.27249026298523, 264.4390387535095, 266.7239291667938, 269.0088195800781, 271.3262162208557, 273.6436128616333, 275.9357259273529, 278.2278389930725, 280.50534653663635, 282.7828540802002, 285.0763065814972, 287.3697590827942, 289.60140776634216, 291.83305644989014, 294.13692450523376, 296.4407925605774, 298.6581025123596, 300.87541246414185, 303.1596302986145, 305.44384813308716, 307.60167241096497, 309.7594966888428, 312.0617253780365, 314.3639540672302, 316.65579199790955, 318.94762992858887, 321.2807877063751, 323.6139454841614, 325.94578099250793, 328.2776165008545, 330.53254795074463, 332.78747940063477, 335.0307958126068, 337.27411222457886, 339.46121978759766, 341.64832735061646, 343.82228899002075, 345.99625062942505, 348.27297711372375, 350.54970359802246, 352.8421230316162, 355.13454246520996, 357.4265298843384, 359.7185173034668, 361.99436831474304, 364.2702193260193, 366.52078461647034, 368.7713499069214, 371.0046184062958, 373.23788690567017, 375.51168298721313, 377.7854790687561, 380.0989534854889, 382.4124279022217, 384.646409034729, 386.8803901672363, 389.1315610408783, 391.38273191452026, 393.670529127121, 395.9583263397217, 398.2291667461395, 400.5000071525574, 402.7979025840759, 405.0957980155945, 407.39422035217285, 409.6926426887512, 411.95255970954895, 414.2124767303467, 416.4829652309418, 418.75345373153687, 421.0312831401825, 423.3091125488281, 425.6499786376953, 427.9908447265625, 430.1255028247833, 432.26016092300415, 434.57624435424805, 436.89232778549194, 439.1916129589081, 441.4908981323242, 443.72984313964844, 445.96878814697266, 448.44617652893066, 450.9235649108887]
[30.72222222222222, 30.72222222222222, 45.03888888888889, 45.03888888888889, 45.51111111111111, 45.51111111111111, 60.63333333333333, 60.63333333333333, 69.06666666666666, 69.06666666666666, 66.47222222222223, 66.47222222222223, 69.64444444444445, 69.64444444444445, 74.25555555555556, 74.25555555555556, 73.28888888888889, 73.28888888888889, 74.61111111111111, 74.61111111111111, 75.52222222222223, 75.52222222222223, 77.04444444444445, 77.04444444444445, 77.5111111111111, 77.5111111111111, 77.09444444444445, 77.09444444444445, 76.65555555555555, 76.65555555555555, 77.62222222222222, 77.62222222222222, 78.61111111111111, 78.61111111111111, 79.42222222222222, 79.42222222222222, 79.5, 79.5, 80.17777777777778, 80.17777777777778, 80.25555555555556, 80.25555555555556, 79.46666666666667, 79.46666666666667, 80.13333333333334, 80.13333333333334, 80.37777777777778, 80.37777777777778, 80.63333333333334, 80.63333333333334, 80.64444444444445, 80.64444444444445, 80.98333333333333, 80.98333333333333, 80.97222222222223, 80.97222222222223, 81.07777777777778, 81.07777777777778, 81.20555555555555, 81.20555555555555, 81.40555555555555, 81.40555555555555, 81.49444444444444, 81.49444444444444, 81.40555555555555, 81.40555555555555, 81.62222222222222, 81.62222222222222, 81.3, 81.3, 81.47777777777777, 81.47777777777777, 81.72777777777777, 81.72777777777777, 81.50555555555556, 81.50555555555556, 81.37222222222222, 81.37222222222222, 81.77777777777777, 81.77777777777777, 81.8, 81.8, 82.4888888888889, 82.4888888888889, 82.85555555555555, 82.85555555555555, 82.79444444444445, 82.79444444444445, 82.62777777777778, 82.62777777777778, 82.43888888888888, 82.43888888888888, 82.87777777777778, 82.87777777777778, 82.5111111111111, 82.5111111111111, 82.2, 82.2, 81.78333333333333, 81.78333333333333, 81.75555555555556, 81.75555555555556, 82.46111111111111, 82.46111111111111, 82.91666666666667, 82.91666666666667, 82.62777777777778, 82.62777777777778, 82.58333333333333, 82.58333333333333, 82.81666666666666, 82.81666666666666, 82.96666666666667, 82.96666666666667, 82.77222222222223, 82.77222222222223, 82.84444444444445, 82.84444444444445, 82.79444444444445, 82.79444444444445, 82.79444444444445, 82.79444444444445, 82.48333333333333, 82.48333333333333, 82.61666666666666, 82.61666666666666, 82.97222222222223, 82.97222222222223, 83.15, 83.15, 83.20555555555555, 83.20555555555555, 83.27222222222223, 83.27222222222223, 83.36111111111111, 83.36111111111111, 83.36666666666666, 83.36666666666666, 83.7388888888889, 83.7388888888889, 83.60555555555555, 83.60555555555555, 83.57777777777778, 83.57777777777778, 83.37222222222222, 83.37222222222222, 83.65, 83.65, 83.53888888888889, 83.53888888888889, 83.66666666666667, 83.66666666666667, 83.53888888888889, 83.53888888888889, 83.30555555555556, 83.30555555555556, 83.43888888888888, 83.43888888888888, 83.60555555555555, 83.60555555555555, 83.6, 83.6, 83.41111111111111, 83.41111111111111, 83.3, 83.3, 83.71666666666667, 83.71666666666667, 83.67222222222222, 83.67222222222222, 83.53333333333333, 83.53333333333333, 83.78333333333333, 83.78333333333333, 83.72222222222223, 83.72222222222223, 83.62222222222222, 83.62222222222222, 83.77222222222223, 83.77222222222223, 83.53333333333333, 83.53333333333333, 83.62222222222222, 83.62222222222222, 83.63888888888889, 83.63888888888889, 83.54444444444445, 83.54444444444445, 83.46666666666667, 83.46666666666667, 83.56666666666666, 83.56666666666666, 83.59444444444445, 83.59444444444445, 83.52777777777777, 83.52777777777777, 83.62222222222222, 83.62222222222222, 83.77222222222223, 83.77222222222223, 84.18888888888888, 84.18888888888888]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 438, in train
    batch_loss.append(loss.item())
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.557, Test loss: 0.552, Test accuracy: 77.80
Average accuracy final 10 rounds: 78.73833333333333
Average global accuracy final 10 rounds: 78.73833333333333
1717.3389010429382
[]
[21.316666666666666, 28.425, 39.608333333333334, 56.541666666666664, 59.475, 60.18333333333333, 61.925, 61.99166666666667, 64.31666666666666, 66.29166666666667, 67.50833333333334, 67.64166666666667, 67.25, 67.3, 66.15833333333333, 67.9, 68.10833333333333, 68.88333333333334, 67.79166666666667, 69.09166666666667, 70.475, 71.03333333333333, 70.36666666666666, 70.48333333333333, 72.6, 74.98333333333333, 75.06666666666666, 75.05, 74.85, 74.86666666666666, 75.75833333333334, 76.26666666666667, 75.74166666666666, 76.36666666666666, 75.825, 75.475, 75.31666666666666, 76.6, 76.0, 76.525, 76.95, 77.19166666666666, 76.85833333333333, 76.93333333333334, 76.48333333333333, 77.18333333333334, 77.06666666666666, 76.85833333333333, 76.80833333333334, 76.55, 76.875, 76.94166666666666, 77.1, 76.9, 77.63333333333334, 77.275, 76.99166666666666, 77.20833333333333, 76.8, 77.03333333333333, 76.78333333333333, 77.575, 77.33333333333333, 77.95, 77.575, 77.525, 78.03333333333333, 77.94166666666666, 77.51666666666667, 77.03333333333333, 76.825, 76.94166666666666, 76.9, 76.99166666666666, 77.475, 77.86666666666666, 77.25, 77.4, 77.53333333333333, 78.45833333333333, 78.325, 78.46666666666667, 78.25, 78.36666666666666, 78.44166666666666, 77.65, 78.45833333333333, 78.73333333333333, 78.85, 78.60833333333333, 79.06666666666666, 78.61666666666666, 78.40833333333333, 78.45833333333333, 78.55833333333334, 78.475, 79.075, 79.11666666666666, 78.73333333333333, 78.875, 77.8]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.293, Test loss: 0.728, Test accuracy: 79.38
Average accuracy final 10 rounds: 79.03725
8271.66266989708
[14.155335426330566, 27.896233558654785, 41.92708730697632, 55.59601664543152, 69.43136787414551, 83.07526540756226, 96.6687490940094, 110.48524069786072, 123.53528237342834, 136.87540650367737, 150.04490447044373, 163.3306200504303, 176.67564296722412, 189.78634452819824, 202.9784450531006, 216.37519693374634, 229.9110677242279, 243.10019516944885, 256.1255123615265, 269.51245760917664, 282.63939023017883, 295.3223948478699, 307.9065635204315, 319.83183789253235, 331.3196527957916, 342.83364820480347, 354.2120382785797, 365.5198073387146, 376.9566812515259, 388.4287507534027, 399.90116000175476, 411.3363182544708, 422.7699842453003, 434.2434341907501, 445.75218892097473, 457.26228380203247, 468.6200706958771, 479.9949562549591, 491.4627192020416, 502.9336268901825, 514.505069732666, 525.9997787475586, 537.5621519088745, 549.075412273407, 560.6064727306366, 572.0462701320648, 583.5341548919678, 594.9895868301392, 606.5558574199677, 618.1519193649292, 629.7556796073914, 641.2261898517609, 652.7575354576111, 664.2967011928558, 675.736439704895, 687.1856126785278, 698.6601116657257, 710.1244027614594, 721.5881531238556, 732.9627623558044, 744.3543558120728, 755.795072555542, 767.2322027683258, 778.622921705246, 790.0142333507538, 801.4299812316895, 812.8693153858185, 824.2842953205109, 836.2432105541229, 847.661007642746, 859.112292766571, 870.4834740161896, 881.9051222801208, 893.3048949241638, 904.6662819385529, 916.0017294883728, 927.3505492210388, 938.7939457893372, 950.1488881111145, 961.4707698822021, 972.917129278183, 984.3725414276123, 995.8330047130585, 1007.3296756744385, 1018.75630402565, 1030.0813891887665, 1041.4479162693024, 1052.7342138290405, 1063.9904959201813, 1075.2321195602417, 1086.4448971748352, 1097.7810695171356, 1109.102732181549, 1120.290462255478, 1131.5800940990448, 1142.8751909732819, 1154.29554438591, 1165.7538447380066, 1177.1338124275208, 1188.6379978656769, 1191.866622209549]
[38.8, 46.84, 51.385, 54.7575, 58.065, 60.7625, 62.42, 64.0325, 64.6425, 67.1175, 68.1625, 68.5425, 69.735, 69.9975, 70.945, 71.3125, 71.615, 71.9225, 72.645, 73.23, 73.8475, 73.305, 73.9125, 74.2925, 74.2825, 74.4175, 74.825, 74.67, 75.3975, 75.005, 75.4075, 75.51, 75.2475, 75.6175, 75.5275, 75.9975, 76.115, 76.05, 76.37, 75.81, 76.26, 76.7375, 76.4875, 76.4875, 76.8925, 76.455, 77.38, 76.835, 77.215, 76.72, 76.92, 76.3825, 77.13, 77.285, 77.46, 77.1925, 77.3875, 77.4675, 77.37, 77.535, 77.9875, 77.74, 77.365, 77.8125, 77.7, 77.8975, 77.6175, 77.88, 78.3225, 77.7, 77.86, 77.83, 77.665, 78.08, 78.2575, 78.4125, 78.1325, 78.19, 78.5, 78.6725, 78.3175, 78.8075, 78.685, 78.315, 78.86, 78.9075, 78.6, 78.84, 79.02, 78.9475, 78.7875, 78.9725, 79.305, 78.9525, 79.32, 79.45, 78.8075, 79.0825, 78.88, 78.815, 79.375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.425, Test loss: 0.699, Test accuracy: 77.31
Average accuracy final 10 rounds: 76.7445
4574.325084209442
[6.106100559234619, 11.924298763275146, 17.735737562179565, 23.540982484817505, 29.36302423477173, 35.2764208316803, 41.15225839614868, 46.958351612091064, 52.71220564842224, 58.485058307647705, 64.33705544471741, 70.17611336708069, 75.93017911911011, 81.7356505393982, 87.5322015285492, 93.27962470054626, 99.04709267616272, 104.82454037666321, 110.62706398963928, 116.42829084396362, 122.19586110115051, 127.93595790863037, 133.73464608192444, 139.51793026924133, 145.26255631446838, 151.018807888031, 156.82076334953308, 162.59149074554443, 168.39490365982056, 174.19923973083496, 179.95458149909973, 185.67792439460754, 191.4766993522644, 197.3126835823059, 203.10940408706665, 208.9363133907318, 214.80114817619324, 220.58319568634033, 226.29820775985718, 232.04925894737244, 237.81642532348633, 243.5753412246704, 249.1422083377838, 254.9133324623108, 260.63602471351624, 266.36262369155884, 272.1179766654968, 277.7250702381134, 283.4778745174408, 289.24202513694763, 294.9884078502655, 300.7353045940399, 306.4672272205353, 312.2380714416504, 317.9782381057739, 323.711305141449, 329.43375158309937, 335.18867325782776, 341.0020046234131, 346.8095541000366, 352.60330033302307, 358.1906633377075, 363.8174433708191, 369.4466903209686, 375.04577827453613, 380.7741696834564, 386.52717661857605, 392.3070533275604, 398.12048172950745, 403.9423990249634, 409.6999464035034, 415.4398009777069, 421.1902503967285, 426.9435646533966, 432.6601514816284, 438.46537351608276, 444.2739510536194, 450.08239364624023, 455.86788988113403, 461.6909427642822, 467.4917104244232, 473.25107741355896, 478.9867641925812, 484.7710907459259, 490.5937469005585, 496.41994404792786, 502.17131447792053, 507.93754839897156, 513.5218994617462, 519.08522772789, 524.8701717853546, 530.6644809246063, 536.5184533596039, 542.3318519592285, 548.0812952518463, 554.0206789970398, 559.88223695755, 565.6506326198578, 571.4591310024261, 577.2330696582794, 579.3990585803986]
[33.39, 41.9075, 45.895, 49.6175, 52.3025, 54.235, 56.7275, 58.4275, 59.8, 61.2275, 62.0375, 62.5675, 64.905, 65.6225, 66.4, 67.0575, 67.8725, 68.565, 69.145, 69.705, 69.8475, 70.0475, 70.645, 70.7, 71.5375, 71.5875, 71.675, 71.915, 72.445, 72.595, 72.5125, 73.0825, 72.785, 72.345, 73.1175, 73.03, 73.4975, 73.48, 73.5375, 73.975, 74.2875, 74.8325, 74.7975, 74.61, 74.81, 75.105, 75.1175, 75.26, 75.2225, 75.345, 75.3075, 74.87, 75.2525, 75.4225, 75.1525, 75.295, 75.5, 75.3375, 75.0775, 75.425, 75.4, 75.76, 75.5625, 75.12, 75.69, 75.66, 75.9075, 75.9775, 76.445, 75.7625, 75.72, 76.2325, 75.9625, 76.1725, 76.1725, 76.255, 76.4925, 76.7, 76.6675, 76.6775, 76.665, 75.8525, 76.32, 76.1175, 76.1725, 76.1475, 76.4075, 76.6875, 76.685, 76.6275, 76.5475, 76.105, 76.9, 76.4375, 76.485, 76.9225, 76.83, 76.665, 77.2675, 77.285, 77.315]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.292, Test loss: 0.458, Test accuracy: 81.97
Average accuracy final 10 rounds: 81.73666666666666
1661.2663679122925
[2.1237618923187256, 4.247523784637451, 5.944533824920654, 7.641543865203857, 9.306119441986084, 10.97069501876831, 12.625237226486206, 14.279779434204102, 15.925904989242554, 17.572030544281006, 19.223551511764526, 20.875072479248047, 22.512227773666382, 24.149383068084717, 25.794439554214478, 27.43949604034424, 29.099955081939697, 30.760414123535156, 32.42420554161072, 34.08799695968628, 35.73188042640686, 37.37576389312744, 39.09145259857178, 40.80714130401611, 42.56043267250061, 44.31372404098511, 46.00378489494324, 47.69384574890137, 49.42524838447571, 51.15665102005005, 52.90000915527344, 54.643367290496826, 56.342177867889404, 58.04098844528198, 59.7537739276886, 61.466559410095215, 63.20077729225159, 64.93499517440796, 66.65030884742737, 68.36562252044678, 70.08693313598633, 71.80824375152588, 73.55083107948303, 75.29341840744019, 76.98858213424683, 78.68374586105347, 80.42024087905884, 82.15673589706421, 83.8837411403656, 85.61074638366699, 87.30784583091736, 89.00494527816772, 90.74160766601562, 92.47827005386353, 94.1926007270813, 95.90693140029907, 97.61856698989868, 99.33020257949829, 101.01749300956726, 102.70478343963623, 104.24646043777466, 105.78813743591309, 107.308758020401, 108.82937860488892, 110.36250710487366, 111.8956356048584, 113.45985054969788, 115.02406549453735, 116.56227421760559, 118.10048294067383, 119.64048218727112, 121.18048143386841, 122.73638010025024, 124.29227876663208, 125.84879112243652, 127.40530347824097, 128.96899223327637, 130.53268098831177, 132.07427740097046, 133.61587381362915, 135.1847939491272, 136.75371408462524, 138.31318283081055, 139.87265157699585, 141.40264415740967, 142.9326367378235, 144.48037099838257, 146.02810525894165, 147.56427454948425, 149.10044384002686, 150.6410267353058, 152.18160963058472, 153.71861219406128, 155.25561475753784, 156.79909873008728, 158.34258270263672, 159.90723490715027, 161.47188711166382, 163.00419187545776, 164.5364966392517, 166.0845787525177, 167.6326608657837, 169.20217895507812, 170.77169704437256, 172.3039152622223, 173.83613348007202, 175.37762093544006, 176.9191083908081, 178.46084094047546, 180.00257349014282, 181.54197645187378, 183.08137941360474, 184.60903930664062, 186.1366991996765, 187.67171025276184, 189.20672130584717, 190.76647639274597, 192.32623147964478, 193.8527853488922, 195.37933921813965, 196.94245839118958, 198.5055775642395, 200.07183861732483, 201.63809967041016, 203.1673698425293, 204.69664001464844, 206.24849200248718, 207.80034399032593, 209.33957314491272, 210.8788022994995, 212.42658853530884, 213.97437477111816, 215.49353909492493, 217.0127034187317, 218.5547752380371, 220.09684705734253, 221.645122051239, 223.1933970451355, 224.71122360229492, 226.22905015945435, 227.7850251197815, 229.34100008010864, 230.91874933242798, 232.49649858474731, 234.03129720687866, 235.56609582901, 237.1104166507721, 238.65473747253418, 240.1955668926239, 241.73639631271362, 243.28321313858032, 244.83002996444702, 246.36087584495544, 247.89172172546387, 249.42759490013123, 250.96346807479858, 252.5147213935852, 254.06597471237183, 255.5947597026825, 257.12354469299316, 258.6532473564148, 260.1829500198364, 261.73085713386536, 263.2787642478943, 264.81873774528503, 266.3587112426758, 267.88678908348083, 269.4148669242859, 270.98144125938416, 272.5480155944824, 274.07959246635437, 275.6111693382263, 277.11021161079407, 278.6092538833618, 280.1183669567108, 281.6274800300598, 283.1376760005951, 284.64787197113037, 286.1480519771576, 287.6482319831848, 289.1637682914734, 290.67930459976196, 292.188884973526, 293.69846534729004, 295.22643852233887, 296.7544116973877, 298.24791264533997, 299.74141359329224, 301.25527143478394, 302.76912927627563, 304.279159784317, 305.7891902923584, 311.58624291419983, 317.38329553604126, 319.05541253089905, 320.72752952575684, 322.333411693573, 323.93929386138916, 325.5222370624542, 327.1051802635193, 329.1902506351471, 331.2753210067749]
[22.375, 22.375, 41.325, 41.325, 50.641666666666666, 50.641666666666666, 57.05833333333333, 57.05833333333333, 57.75833333333333, 57.75833333333333, 60.3, 60.3, 64.075, 64.075, 64.86666666666666, 64.86666666666666, 67.73333333333333, 67.73333333333333, 68.16666666666667, 68.16666666666667, 68.71666666666667, 68.71666666666667, 70.2, 70.2, 70.4, 70.4, 72.025, 72.025, 72.18333333333334, 72.18333333333334, 72.68333333333334, 72.68333333333334, 72.75833333333334, 72.75833333333334, 73.55833333333334, 73.55833333333334, 74.08333333333333, 74.08333333333333, 74.41666666666667, 74.41666666666667, 74.95833333333333, 74.95833333333333, 74.9, 74.9, 75.29166666666667, 75.29166666666667, 75.55833333333334, 75.55833333333334, 75.95833333333333, 75.95833333333333, 75.48333333333333, 75.48333333333333, 75.9, 75.9, 76.375, 76.375, 76.925, 76.925, 77.475, 77.475, 77.58333333333333, 77.58333333333333, 77.14166666666667, 77.14166666666667, 77.675, 77.675, 78.18333333333334, 78.18333333333334, 78.55833333333334, 78.55833333333334, 78.58333333333333, 78.58333333333333, 78.71666666666667, 78.71666666666667, 78.33333333333333, 78.33333333333333, 78.65, 78.65, 78.8, 78.8, 78.78333333333333, 78.78333333333333, 78.79166666666667, 78.79166666666667, 78.9, 78.9, 78.86666666666666, 78.86666666666666, 79.25, 79.25, 79.63333333333334, 79.63333333333334, 79.65833333333333, 79.65833333333333, 79.96666666666667, 79.96666666666667, 79.7, 79.7, 79.78333333333333, 79.78333333333333, 80.2, 80.2, 80.04166666666667, 80.04166666666667, 80.16666666666667, 80.16666666666667, 79.99166666666666, 79.99166666666666, 79.675, 79.675, 79.88333333333334, 79.88333333333334, 80.45833333333333, 80.45833333333333, 80.26666666666667, 80.26666666666667, 80.475, 80.475, 80.825, 80.825, 80.44166666666666, 80.44166666666666, 80.2, 80.2, 80.73333333333333, 80.73333333333333, 80.55, 80.55, 80.86666666666666, 80.86666666666666, 80.88333333333334, 80.88333333333334, 81.20833333333333, 81.20833333333333, 81.575, 81.575, 81.3, 81.3, 81.325, 81.325, 81.45833333333333, 81.45833333333333, 80.88333333333334, 80.88333333333334, 81.375, 81.375, 81.475, 81.475, 81.55833333333334, 81.55833333333334, 81.53333333333333, 81.53333333333333, 81.64166666666667, 81.64166666666667, 81.31666666666666, 81.31666666666666, 81.625, 81.625, 81.96666666666667, 81.96666666666667, 81.70833333333333, 81.70833333333333, 81.95, 81.95, 81.975, 81.975, 81.5, 81.5, 81.8, 81.8, 81.93333333333334, 81.93333333333334, 81.95833333333333, 81.95833333333333, 81.99166666666666, 81.99166666666666, 82.10833333333333, 82.10833333333333, 82.1, 82.1, 81.93333333333334, 81.93333333333334, 82.025, 82.025, 82.00833333333334, 82.00833333333334, 81.98333333333333, 81.98333333333333, 81.69166666666666, 81.69166666666666, 81.48333333333333, 81.48333333333333, 81.04166666666667, 81.04166666666667, 81.89166666666667, 81.89166666666667, 82.00833333333334, 82.00833333333334, 81.3, 81.3, 81.975, 81.975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.050, Test loss: 0.961, Test accuracy: 81.97
Final Round, Global train loss: 0.050, Global test loss: 2.334, Global test accuracy: 17.37
Average accuracy final 10 rounds: 80.57333333333332 

Average global accuracy final 10 rounds: 14.279166666666665 

1941.7790706157684
[1.6704425811767578, 3.3408851623535156, 4.80021595954895, 6.259546756744385, 7.711470127105713, 9.163393497467041, 10.639723300933838, 12.116053104400635, 13.580100297927856, 15.044147491455078, 16.52588725090027, 18.00762701034546, 19.47560214996338, 20.9435772895813, 22.401695251464844, 23.85981321334839, 25.327406883239746, 26.795000553131104, 28.266990184783936, 29.738979816436768, 31.213582515716553, 32.68818521499634, 34.157493352890015, 35.62680149078369, 37.089303970336914, 38.55180644989014, 40.024195432662964, 41.49658441543579, 42.95353603363037, 44.41048765182495, 45.874526023864746, 47.33856439590454, 48.8051278591156, 50.27169132232666, 51.74390530586243, 53.21611928939819, 54.68936800956726, 56.16261672973633, 57.63237190246582, 59.10212707519531, 60.57437491416931, 62.04662275314331, 63.50267219543457, 64.95872163772583, 66.41681742668152, 67.8749132156372, 69.34858536720276, 70.82225751876831, 72.27576398849487, 73.72927045822144, 75.17541456222534, 76.62155866622925, 78.08705711364746, 79.55255556106567, 81.01270914077759, 82.4728627204895, 83.920254945755, 85.36764717102051, 86.84465861320496, 88.3216700553894, 89.7795729637146, 91.2374758720398, 92.68804669380188, 94.13861751556396, 95.5871376991272, 97.03565788269043, 98.509117603302, 99.98257732391357, 101.43735098838806, 102.89212465286255, 104.33243823051453, 105.7727518081665, 107.24549078941345, 108.7182297706604, 110.19445013999939, 111.67067050933838, 113.1196346282959, 114.56859874725342, 116.02066993713379, 117.47274112701416, 118.9430410861969, 120.41334104537964, 121.87029433250427, 123.3272476196289, 124.78403759002686, 126.2408275604248, 127.701828956604, 129.1628303527832, 130.62139010429382, 132.07994985580444, 133.52482628822327, 134.9697027206421, 136.45798110961914, 137.9462594985962, 139.4151713848114, 140.8840832710266, 142.33517384529114, 143.78626441955566, 145.25412273406982, 146.72198104858398, 148.19217443466187, 149.66236782073975, 151.13171362876892, 152.6010594367981, 154.0681893825531, 155.5353193283081, 157.00048208236694, 158.46564483642578, 159.93656516075134, 161.4074854850769, 162.8822045326233, 164.35692358016968, 165.81751918792725, 167.27811479568481, 168.75948286056519, 170.24085092544556, 171.69936609268188, 173.1578812599182, 174.6225461959839, 176.08721113204956, 177.55191898345947, 179.01662683486938, 180.48188304901123, 181.94713926315308, 183.40740442276, 184.86766958236694, 186.33208203315735, 187.79649448394775, 189.27221274375916, 190.74793100357056, 192.21530604362488, 193.6826810836792, 195.1464672088623, 196.6102533340454, 198.07653069496155, 199.54280805587769, 201.02949690818787, 202.51618576049805, 203.9691891670227, 205.42219257354736, 206.88852047920227, 208.35484838485718, 209.7300844192505, 211.1053204536438, 212.48359036445618, 213.86186027526855, 215.2336778640747, 216.60549545288086, 218.00125741958618, 219.3970193862915, 220.83904910087585, 222.2810788154602, 223.72849941253662, 225.17592000961304, 226.64397621154785, 228.11203241348267, 229.58416557312012, 231.05629873275757, 232.51420521736145, 233.97211170196533, 235.444349527359, 236.91658735275269, 238.40864896774292, 239.90071058273315, 241.352303981781, 242.80389738082886, 244.18373894691467, 245.5635805130005, 246.9483826160431, 248.3331847190857, 249.72750854492188, 251.12183237075806, 252.49810886383057, 253.87438535690308, 255.2671127319336, 256.6598401069641, 258.0602743625641, 259.46070861816406, 260.913950920105, 262.3671932220459, 263.82526111602783, 265.28332901000977, 266.7785189151764, 268.273708820343, 269.7411193847656, 271.20852994918823, 272.6589689254761, 274.1094079017639, 275.5759468078613, 277.04248571395874, 278.519508600235, 279.99653148651123, 281.4616286754608, 282.9267258644104, 284.3863625526428, 285.84599924087524, 287.3173451423645, 288.78869104385376, 290.26649594306946, 291.74430084228516, 294.17885088920593, 296.6134009361267]
[26.858333333333334, 26.858333333333334, 38.65, 38.65, 44.416666666666664, 44.416666666666664, 55.725, 55.725, 66.85, 66.85, 71.84166666666667, 71.84166666666667, 73.21666666666667, 73.21666666666667, 73.73333333333333, 73.73333333333333, 73.9, 73.9, 74.70833333333333, 74.70833333333333, 74.31666666666666, 74.31666666666666, 75.4, 75.4, 75.475, 75.475, 74.975, 74.975, 75.91666666666667, 75.91666666666667, 75.375, 75.375, 76.09166666666667, 76.09166666666667, 77.0, 77.0, 76.925, 76.925, 77.325, 77.325, 77.65833333333333, 77.65833333333333, 77.81666666666666, 77.81666666666666, 77.30833333333334, 77.30833333333334, 78.25833333333334, 78.25833333333334, 78.00833333333334, 78.00833333333334, 78.53333333333333, 78.53333333333333, 78.89166666666667, 78.89166666666667, 79.3, 79.3, 78.65, 78.65, 79.09166666666667, 79.09166666666667, 79.65833333333333, 79.65833333333333, 79.75833333333334, 79.75833333333334, 80.05, 80.05, 79.53333333333333, 79.53333333333333, 79.83333333333333, 79.83333333333333, 80.35833333333333, 80.35833333333333, 80.23333333333333, 80.23333333333333, 79.85, 79.85, 80.05833333333334, 80.05833333333334, 79.84166666666667, 79.84166666666667, 79.58333333333333, 79.58333333333333, 79.43333333333334, 79.43333333333334, 79.85, 79.85, 79.70833333333333, 79.70833333333333, 79.74166666666666, 79.74166666666666, 79.84166666666667, 79.84166666666667, 80.20833333333333, 80.20833333333333, 80.28333333333333, 80.28333333333333, 80.56666666666666, 80.56666666666666, 80.375, 80.375, 80.59166666666667, 80.59166666666667, 80.78333333333333, 80.78333333333333, 80.75, 80.75, 80.55833333333334, 80.55833333333334, 80.26666666666667, 80.26666666666667, 80.14166666666667, 80.14166666666667, 80.75, 80.75, 80.76666666666667, 80.76666666666667, 80.725, 80.725, 80.46666666666667, 80.46666666666667, 80.45833333333333, 80.45833333333333, 80.45, 80.45, 80.46666666666667, 80.46666666666667, 80.53333333333333, 80.53333333333333, 80.63333333333334, 80.63333333333334, 80.38333333333334, 80.38333333333334, 80.75833333333334, 80.75833333333334, 80.40833333333333, 80.40833333333333, 80.58333333333333, 80.58333333333333, 80.93333333333334, 80.93333333333334, 81.08333333333333, 81.08333333333333, 80.89166666666667, 80.89166666666667, 80.70833333333333, 80.70833333333333, 80.38333333333334, 80.38333333333334, 80.69166666666666, 80.69166666666666, 80.8, 80.8, 81.06666666666666, 81.06666666666666, 81.00833333333334, 81.00833333333334, 80.90833333333333, 80.90833333333333, 81.00833333333334, 81.00833333333334, 81.125, 81.125, 80.875, 80.875, 80.8, 80.8, 80.91666666666667, 80.91666666666667, 80.98333333333333, 80.98333333333333, 80.89166666666667, 80.89166666666667, 81.15, 81.15, 81.15833333333333, 81.15833333333333, 80.91666666666667, 80.91666666666667, 80.88333333333334, 80.88333333333334, 80.76666666666667, 80.76666666666667, 80.45, 80.45, 80.78333333333333, 80.78333333333333, 81.05, 81.05, 80.55, 80.55, 80.54166666666667, 80.54166666666667, 80.74166666666666, 80.74166666666666, 80.45833333333333, 80.45833333333333, 80.15833333333333, 80.15833333333333, 80.23333333333333, 80.23333333333333, 81.975, 81.975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.180, Test loss: 0.572, Test accuracy: 83.69
Final Round, Global train loss: 0.180, Global test loss: 3.173, Global test accuracy: 16.29
Average accuracy final 10 rounds: 84.06666666666669 

Average global accuracy final 10 rounds: 18.425833333333333 

1906.1862008571625
[1.702063798904419, 3.404127597808838, 4.862179517745972, 6.3202314376831055, 7.759759426116943, 9.199287414550781, 10.64822769165039, 12.09716796875, 13.55291199684143, 15.008656024932861, 16.476887702941895, 17.945119380950928, 19.390848875045776, 20.836578369140625, 22.300297498703003, 23.76401662826538, 25.212987899780273, 26.661959171295166, 28.11226773262024, 29.562576293945312, 31.012278079986572, 32.46197986602783, 33.91139602661133, 35.360812187194824, 36.82230353355408, 38.28379487991333, 39.74451279640198, 41.205230712890625, 42.668248653411865, 44.131266593933105, 45.57956886291504, 47.02787113189697, 48.48627209663391, 49.94467306137085, 51.39756226539612, 52.85045146942139, 54.29216241836548, 55.73387336730957, 57.193299531936646, 58.65272569656372, 60.119645833969116, 61.58656597137451, 63.03914141654968, 64.49171686172485, 65.94021677970886, 67.38871669769287, 68.85759043693542, 70.32646417617798, 71.77798986434937, 73.22951555252075, 74.66865086555481, 76.10778617858887, 77.569096326828, 79.03040647506714, 80.48267412185669, 81.93494176864624, 83.37270092964172, 84.8104600906372, 86.27535462379456, 87.7402491569519, 89.20752143859863, 90.67479372024536, 92.11222696304321, 93.54966020584106, 94.99542713165283, 96.4411940574646, 97.90875768661499, 99.37632131576538, 100.821932554245, 102.26754379272461, 103.71400594711304, 105.16046810150146, 106.61082482337952, 108.06118154525757, 109.50793981552124, 110.95469808578491, 112.3915843963623, 113.8284707069397, 115.2668571472168, 116.7052435874939, 118.16457915306091, 119.62391471862793, 121.08592486381531, 122.54793500900269, 123.99978756904602, 125.45164012908936, 126.90918922424316, 128.36673831939697, 129.8216531276703, 131.2765679359436, 132.71410942077637, 134.15165090560913, 135.6145257949829, 137.0774006843567, 138.53685307502747, 139.99630546569824, 141.43265342712402, 142.8690013885498, 144.2827868461609, 145.69657230377197, 147.09488773345947, 148.49320316314697, 149.882661819458, 151.27212047576904, 152.65668725967407, 154.0412540435791, 155.44039154052734, 156.8395290374756, 158.29618620872498, 159.75284337997437, 161.21089100837708, 162.66893863677979, 164.11861515045166, 165.56829166412354, 167.0333971977234, 168.49850273132324, 169.95341634750366, 171.40832996368408, 172.86880135536194, 174.3292727470398, 175.77096319198608, 177.21265363693237, 178.67003345489502, 180.12741327285767, 181.58443570137024, 183.0414581298828, 184.4869532585144, 185.932448387146, 187.26595187187195, 188.5994553565979, 189.85592126846313, 191.11238718032837, 192.36552262306213, 193.6186580657959, 194.87722253799438, 196.13578701019287, 197.40002155303955, 198.66425609588623, 199.9282741546631, 201.19229221343994, 202.43705296516418, 203.68181371688843, 204.94795989990234, 206.21410608291626, 207.4853048324585, 208.75650358200073, 210.03056263923645, 211.30462169647217, 212.56947135925293, 213.8343210220337, 215.3033356666565, 216.7723503112793, 218.23340487480164, 219.69445943832397, 221.15415024757385, 222.61384105682373, 224.0613350868225, 225.5088291168213, 226.95494771003723, 228.40106630325317, 229.8453586101532, 231.28965091705322, 232.73732447624207, 234.1849980354309, 235.63503432273865, 237.0850706100464, 238.53334259986877, 239.98161458969116, 241.44379711151123, 242.9059796333313, 244.35473942756653, 245.80349922180176, 247.06266117095947, 248.3218231201172, 249.56251859664917, 250.80321407318115, 252.07422852516174, 253.34524297714233, 254.61350846290588, 255.88177394866943, 257.13833451271057, 258.3948950767517, 259.6662709712982, 260.9376468658447, 262.20869731903076, 263.4797477722168, 264.7416481971741, 266.00354862213135, 267.2556064128876, 268.5076642036438, 269.77041697502136, 271.0331697463989, 272.28006649017334, 273.52696323394775, 274.77575278282166, 276.02454233169556, 277.29176020622253, 278.5589780807495, 279.83729553222656, 281.1156129837036, 283.22250175476074, 285.32939052581787]
[30.691666666666666, 30.691666666666666, 39.791666666666664, 39.791666666666664, 49.59166666666667, 49.59166666666667, 58.03333333333333, 58.03333333333333, 60.675, 60.675, 61.71666666666667, 61.71666666666667, 64.6, 64.6, 65.8, 65.8, 67.21666666666667, 67.21666666666667, 67.41666666666667, 67.41666666666667, 74.90833333333333, 74.90833333333333, 75.50833333333334, 75.50833333333334, 75.39166666666667, 75.39166666666667, 75.09166666666667, 75.09166666666667, 75.54166666666667, 75.54166666666667, 76.35833333333333, 76.35833333333333, 76.25833333333334, 76.25833333333334, 76.99166666666666, 76.99166666666666, 77.11666666666666, 77.11666666666666, 77.44166666666666, 77.44166666666666, 78.25, 78.25, 78.79166666666667, 78.79166666666667, 78.875, 78.875, 79.0, 79.0, 79.10833333333333, 79.10833333333333, 79.425, 79.425, 79.375, 79.375, 78.90833333333333, 78.90833333333333, 78.20833333333333, 78.20833333333333, 78.475, 78.475, 79.04166666666667, 79.04166666666667, 78.9, 78.9, 80.85, 80.85, 81.025, 81.025, 80.45, 80.45, 80.95, 80.95, 81.11666666666666, 81.11666666666666, 81.2, 81.2, 82.14166666666667, 82.14166666666667, 81.35, 81.35, 81.19166666666666, 81.19166666666666, 81.76666666666667, 81.76666666666667, 81.625, 81.625, 81.85, 81.85, 81.70833333333333, 81.70833333333333, 82.475, 82.475, 82.25, 82.25, 82.25, 82.25, 81.525, 81.525, 81.425, 81.425, 81.54166666666667, 81.54166666666667, 82.05833333333334, 82.05833333333334, 81.80833333333334, 81.80833333333334, 81.79166666666667, 81.79166666666667, 82.45, 82.45, 82.73333333333333, 82.73333333333333, 82.86666666666666, 82.86666666666666, 82.475, 82.475, 82.63333333333334, 82.63333333333334, 82.45833333333333, 82.45833333333333, 82.325, 82.325, 82.04166666666667, 82.04166666666667, 82.55, 82.55, 82.35, 82.35, 81.69166666666666, 81.69166666666666, 82.225, 82.225, 82.63333333333334, 82.63333333333334, 82.69166666666666, 82.69166666666666, 82.7, 82.7, 82.53333333333333, 82.53333333333333, 82.85, 82.85, 82.51666666666667, 82.51666666666667, 82.55833333333334, 82.55833333333334, 82.53333333333333, 82.53333333333333, 83.0, 83.0, 83.21666666666667, 83.21666666666667, 83.275, 83.275, 83.29166666666667, 83.29166666666667, 83.46666666666667, 83.46666666666667, 83.75, 83.75, 83.61666666666666, 83.61666666666666, 83.65833333333333, 83.65833333333333, 83.475, 83.475, 83.46666666666667, 83.46666666666667, 83.63333333333334, 83.63333333333334, 83.81666666666666, 83.81666666666666, 83.74166666666666, 83.74166666666666, 83.86666666666666, 83.86666666666666, 83.475, 83.475, 83.95, 83.95, 84.00833333333334, 84.00833333333334, 83.48333333333333, 83.48333333333333, 84.16666666666667, 84.16666666666667, 84.1, 84.1, 83.89166666666667, 83.89166666666667, 83.975, 83.975, 84.20833333333333, 84.20833333333333, 84.225, 84.225, 84.15833333333333, 84.15833333333333, 84.45, 84.45, 83.69166666666666, 83.69166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.200, Test loss: 0.370, Test accuracy: 86.42
Average accuracy final 10 rounds: 86.03083333333333 

1528.8333513736725
[1.634042501449585, 3.26808500289917, 4.606025695800781, 5.943966388702393, 7.28431510925293, 8.624663829803467, 9.98981523513794, 11.354966640472412, 12.715526819229126, 14.07608699798584, 15.425177812576294, 16.774268627166748, 18.1361985206604, 19.498128414154053, 20.8574857711792, 22.216843128204346, 23.574788808822632, 24.932734489440918, 26.28117847442627, 27.62962245941162, 28.979259252548218, 30.328896045684814, 31.68607997894287, 33.04326391220093, 34.40656304359436, 35.76986217498779, 37.1154408454895, 38.46101951599121, 39.80344486236572, 41.145870208740234, 42.50082850456238, 43.85578680038452, 45.21909832954407, 46.58240985870361, 47.9301598072052, 49.27790975570679, 50.610695362091064, 51.94348096847534, 53.29849410057068, 54.653507232666016, 56.01852989196777, 57.38355255126953, 58.73631954193115, 60.08908653259277, 61.436283349990845, 62.783480167388916, 64.13698482513428, 65.49048948287964, 66.83861637115479, 68.18674325942993, 69.56857085227966, 70.9503984451294, 72.29350018501282, 73.63660192489624, 74.98992419242859, 76.34324645996094, 77.71094274520874, 79.07863903045654, 80.42319679260254, 81.76775455474854, 83.11254954338074, 84.45734453201294, 85.81254148483276, 87.16773843765259, 88.52383255958557, 89.87992668151855, 91.27679228782654, 92.67365789413452, 94.02549743652344, 95.37733697891235, 96.7284369468689, 98.07953691482544, 99.44353938102722, 100.807541847229, 102.16812586784363, 103.52870988845825, 104.87474131584167, 106.2207727432251, 107.56538534164429, 108.90999794006348, 110.26346588134766, 111.61693382263184, 112.97252440452576, 114.32811498641968, 115.66446685791016, 117.00081872940063, 118.34287452697754, 119.68493032455444, 121.04099822044373, 122.39706611633301, 123.745041847229, 125.093017578125, 126.43776988983154, 127.78252220153809, 129.1270387172699, 130.4715552330017, 131.83054327964783, 133.18953132629395, 134.5516722202301, 135.91381311416626, 137.2642924785614, 138.61477184295654, 139.9638533592224, 141.31293487548828, 142.66490697860718, 144.01687908172607, 145.37904405593872, 146.74120903015137, 148.09680223464966, 149.45239543914795, 150.79741549491882, 152.1424355506897, 153.4856927394867, 154.8289499282837, 156.19374227523804, 157.55853462219238, 158.90693974494934, 160.2553448677063, 161.59597730636597, 162.93660974502563, 164.28308844566345, 165.62956714630127, 166.9774169921875, 168.32526683807373, 169.67298936843872, 171.0207118988037, 172.3570110797882, 173.6933102607727, 174.9513714313507, 176.2094326019287, 177.4647810459137, 178.72012948989868, 179.9551718235016, 181.1902141571045, 182.4184386730194, 183.64666318893433, 184.8719675540924, 186.0972719192505, 187.34235882759094, 188.5874457359314, 189.82559466362, 191.0637435913086, 192.29398894309998, 193.52423429489136, 194.73379588127136, 195.94335746765137, 197.17136430740356, 198.39937114715576, 199.64196968078613, 200.8845682144165, 202.12299489974976, 203.361421585083, 204.695702791214, 206.02998399734497, 207.24750232696533, 208.4650206565857, 209.71207118034363, 210.95912170410156, 212.1869523525238, 213.41478300094604, 214.86515545845032, 216.3155279159546, 217.63792252540588, 218.96031713485718, 220.23860788345337, 221.51689863204956, 222.79204392433167, 224.06718921661377, 225.31758308410645, 226.56797695159912, 227.8255832195282, 229.08318948745728, 230.3599398136139, 231.6366901397705, 232.92538022994995, 234.2140703201294, 235.507075548172, 236.8000807762146, 238.04958271980286, 239.2990846633911, 240.5623869895935, 241.8256893157959, 243.10971188545227, 244.39373445510864, 245.67140769958496, 246.94908094406128, 248.21589541435242, 249.48270988464355, 250.73350310325623, 251.9842963218689, 253.27138948440552, 254.55848264694214, 255.82443261146545, 257.09038257598877, 258.33683013916016, 259.58327770233154, 260.8516848087311, 262.1200919151306, 263.3862998485565, 264.6525077819824, 266.6574773788452, 268.662446975708]
[23.266666666666666, 23.266666666666666, 32.05833333333333, 32.05833333333333, 38.00833333333333, 38.00833333333333, 49.5, 49.5, 54.75833333333333, 54.75833333333333, 58.333333333333336, 58.333333333333336, 59.233333333333334, 59.233333333333334, 61.775, 61.775, 64.95833333333333, 64.95833333333333, 71.50833333333334, 71.50833333333334, 73.83333333333333, 73.83333333333333, 73.6, 73.6, 73.81666666666666, 73.81666666666666, 75.54166666666667, 75.54166666666667, 76.68333333333334, 76.68333333333334, 77.15, 77.15, 76.85833333333333, 76.85833333333333, 77.81666666666666, 77.81666666666666, 78.45, 78.45, 78.93333333333334, 78.93333333333334, 78.85, 78.85, 78.925, 78.925, 78.4, 78.4, 79.05833333333334, 79.05833333333334, 79.66666666666667, 79.66666666666667, 79.60833333333333, 79.60833333333333, 80.18333333333334, 80.18333333333334, 80.54166666666667, 80.54166666666667, 80.6, 80.6, 81.29166666666667, 81.29166666666667, 80.7, 80.7, 81.55, 81.55, 81.925, 81.925, 82.5, 82.5, 82.5, 82.5, 82.325, 82.325, 82.86666666666666, 82.86666666666666, 82.35833333333333, 82.35833333333333, 82.93333333333334, 82.93333333333334, 82.775, 82.775, 83.11666666666666, 83.11666666666666, 83.60833333333333, 83.60833333333333, 82.75, 82.75, 83.175, 83.175, 82.88333333333334, 82.88333333333334, 83.04166666666667, 83.04166666666667, 83.10833333333333, 83.10833333333333, 83.975, 83.975, 84.15833333333333, 84.15833333333333, 83.525, 83.525, 83.89166666666667, 83.89166666666667, 84.10833333333333, 84.10833333333333, 84.025, 84.025, 84.1, 84.1, 84.45, 84.45, 84.38333333333334, 84.38333333333334, 84.56666666666666, 84.56666666666666, 84.49166666666666, 84.49166666666666, 84.69166666666666, 84.69166666666666, 84.89166666666667, 84.89166666666667, 84.45, 84.45, 84.59166666666667, 84.59166666666667, 84.25833333333334, 84.25833333333334, 84.45833333333333, 84.45833333333333, 85.19166666666666, 85.19166666666666, 84.98333333333333, 84.98333333333333, 84.75833333333334, 84.75833333333334, 85.05, 85.05, 84.85833333333333, 84.85833333333333, 85.275, 85.275, 85.23333333333333, 85.23333333333333, 85.20833333333333, 85.20833333333333, 85.625, 85.625, 85.59166666666667, 85.59166666666667, 85.73333333333333, 85.73333333333333, 85.70833333333333, 85.70833333333333, 85.54166666666667, 85.54166666666667, 85.625, 85.625, 85.725, 85.725, 85.75, 85.75, 85.90833333333333, 85.90833333333333, 85.49166666666666, 85.49166666666666, 85.99166666666666, 85.99166666666666, 85.95, 85.95, 86.06666666666666, 86.06666666666666, 85.23333333333333, 85.23333333333333, 85.475, 85.475, 85.71666666666667, 85.71666666666667, 85.65833333333333, 85.65833333333333, 85.75, 85.75, 86.3, 86.3, 86.025, 86.025, 86.20833333333333, 86.20833333333333, 86.15, 86.15, 85.95833333333333, 85.95833333333333, 85.64166666666667, 85.64166666666667, 85.975, 85.975, 86.04166666666667, 86.04166666666667, 86.0, 86.0, 86.00833333333334, 86.00833333333334, 86.425, 86.425]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.134, Test loss: 0.415, Test accuracy: 86.66
Average accuracy final 10 rounds: 85.65916666666666 

1654.897988319397
[1.9619495868682861, 3.9238991737365723, 5.501491546630859, 7.0790839195251465, 8.633984804153442, 10.188885688781738, 11.745627164840698, 13.302368640899658, 14.871382236480713, 16.440395832061768, 18.022592544555664, 19.60478925704956, 21.150906085968018, 22.697022914886475, 24.247681617736816, 25.798340320587158, 27.373828887939453, 28.949317455291748, 30.52605891227722, 32.102800369262695, 33.658207654953, 35.21361494064331, 36.75472164154053, 38.295828342437744, 39.708006858825684, 41.12018537521362, 42.65701198577881, 44.193838596343994, 45.749757289886475, 47.305675983428955, 48.86955261230469, 50.43342924118042, 52.005359411239624, 53.57728958129883, 55.14506697654724, 56.712844371795654, 58.27519941329956, 59.83755445480347, 61.3999297618866, 62.96230506896973, 64.54418444633484, 66.12606382369995, 67.68281126022339, 69.23955869674683, 70.78399848937988, 72.32843828201294, 73.88320732116699, 75.43797636032104, 77.00997853279114, 78.58198070526123, 80.12616968154907, 81.67035865783691, 83.21597957611084, 84.76160049438477, 86.33906149864197, 87.91652250289917, 89.50264263153076, 91.08876276016235, 92.64608955383301, 94.20341634750366, 95.75911092758179, 97.31480550765991, 98.88970923423767, 100.46461296081543, 102.04212927818298, 103.61964559555054, 105.17538714408875, 106.73112869262695, 108.27784085273743, 109.8245530128479, 111.40858507156372, 112.99261713027954, 114.5656955242157, 116.13877391815186, 117.68852806091309, 119.23828220367432, 120.8044490814209, 122.37061595916748, 123.95553755760193, 125.54045915603638, 127.12361431121826, 128.70676946640015, 130.26054763793945, 131.81432580947876, 133.38111782073975, 134.94790983200073, 136.50900983810425, 138.07010984420776, 139.62976670265198, 141.1894235610962, 142.58206343650818, 143.97470331192017, 145.51656317710876, 147.05842304229736, 148.64587450027466, 150.23332595825195, 151.78723311424255, 153.34114027023315, 154.90415811538696, 156.46717596054077, 158.03546404838562, 159.60375213623047, 161.15236473083496, 162.70097732543945, 164.24825811386108, 165.79553890228271, 167.3512830734253, 168.90702724456787, 170.4943299293518, 172.08163261413574, 173.64989018440247, 175.2181477546692, 176.7145459651947, 178.21094417572021, 179.68922591209412, 181.16750764846802, 182.64631605148315, 184.1251244544983, 185.59973621368408, 187.07434797286987, 188.55252146720886, 190.03069496154785, 191.5185046195984, 193.00631427764893, 194.48087668418884, 195.95543909072876, 197.43915843963623, 198.9228777885437, 200.3989109992981, 201.8749442100525, 203.34929490089417, 204.82364559173584, 206.34390234947205, 207.86415910720825, 209.33416986465454, 210.80418062210083, 212.27100944519043, 213.73783826828003, 215.2110743522644, 216.68431043624878, 218.15338945388794, 219.6224684715271, 221.09276032447815, 222.5630521774292, 224.03419470787048, 225.50533723831177, 226.97952914237976, 228.45372104644775, 229.92863845825195, 231.40355587005615, 232.8798589706421, 234.35616207122803, 235.84135627746582, 237.3265504837036, 238.80192947387695, 240.2773084640503, 241.75316047668457, 243.22901248931885, 244.71103954315186, 246.19306659698486, 247.6678867340088, 249.14270687103271, 250.62134265899658, 252.09997844696045, 253.58717036247253, 255.07436227798462, 256.56038999557495, 258.0464177131653, 259.51590275764465, 260.985387802124, 262.47509765625, 263.964807510376, 265.4523708820343, 266.9399342536926, 268.4192190170288, 269.898503780365, 271.3731026649475, 272.84770154953003, 274.3170123100281, 275.7863230705261, 277.25852727890015, 278.73073148727417, 280.19955587387085, 281.66838026046753, 283.1409032344818, 284.6134262084961, 286.09124755859375, 287.5690689086914, 289.048223733902, 290.52737855911255, 291.9967019557953, 293.466025352478, 294.9510841369629, 296.43614292144775, 297.92204427719116, 299.40794563293457, 300.8907964229584, 302.3736472129822, 303.85260248184204, 305.3315577507019, 307.3521490097046, 309.3727402687073]
[13.775, 13.775, 28.658333333333335, 28.658333333333335, 32.891666666666666, 32.891666666666666, 41.733333333333334, 41.733333333333334, 49.94166666666667, 49.94166666666667, 54.68333333333333, 54.68333333333333, 58.99166666666667, 58.99166666666667, 63.05833333333333, 63.05833333333333, 66.975, 66.975, 68.99166666666666, 68.99166666666666, 69.4, 69.4, 74.025, 74.025, 75.46666666666667, 75.46666666666667, 75.80833333333334, 75.80833333333334, 77.84166666666667, 77.84166666666667, 77.64166666666667, 77.64166666666667, 78.59166666666667, 78.59166666666667, 79.15833333333333, 79.15833333333333, 78.24166666666666, 78.24166666666666, 79.18333333333334, 79.18333333333334, 79.34166666666667, 79.34166666666667, 79.99166666666666, 79.99166666666666, 80.90833333333333, 80.90833333333333, 81.04166666666667, 81.04166666666667, 80.48333333333333, 80.48333333333333, 81.6, 81.6, 81.475, 81.475, 81.98333333333333, 81.98333333333333, 81.35, 81.35, 81.9, 81.9, 82.94166666666666, 82.94166666666666, 82.74166666666666, 82.74166666666666, 82.14166666666667, 82.14166666666667, 81.9, 81.9, 83.25833333333334, 83.25833333333334, 83.36666666666666, 83.36666666666666, 82.725, 82.725, 83.2, 83.2, 82.775, 82.775, 83.35833333333333, 83.35833333333333, 83.675, 83.675, 83.39166666666667, 83.39166666666667, 83.65, 83.65, 83.59166666666667, 83.59166666666667, 84.38333333333334, 84.38333333333334, 84.35833333333333, 84.35833333333333, 84.69166666666666, 84.69166666666666, 84.175, 84.175, 84.54166666666667, 84.54166666666667, 84.46666666666667, 84.46666666666667, 84.34166666666667, 84.34166666666667, 84.65, 84.65, 84.28333333333333, 84.28333333333333, 84.85, 84.85, 84.69166666666666, 84.69166666666666, 84.65833333333333, 84.65833333333333, 84.7, 84.7, 84.84166666666667, 84.84166666666667, 84.24166666666666, 84.24166666666666, 85.09166666666667, 85.09166666666667, 84.78333333333333, 84.78333333333333, 84.99166666666666, 84.99166666666666, 85.39166666666667, 85.39166666666667, 84.86666666666666, 84.86666666666666, 85.5, 85.5, 85.38333333333334, 85.38333333333334, 85.19166666666666, 85.19166666666666, 85.21666666666667, 85.21666666666667, 85.28333333333333, 85.28333333333333, 85.16666666666667, 85.16666666666667, 85.575, 85.575, 85.13333333333334, 85.13333333333334, 85.45833333333333, 85.45833333333333, 85.64166666666667, 85.64166666666667, 85.975, 85.975, 85.825, 85.825, 85.51666666666667, 85.51666666666667, 85.175, 85.175, 85.35, 85.35, 85.875, 85.875, 85.73333333333333, 85.73333333333333, 85.99166666666666, 85.99166666666666, 85.3, 85.3, 86.0, 86.0, 85.95, 85.95, 85.44166666666666, 85.44166666666666, 85.575, 85.575, 85.66666666666667, 85.66666666666667, 85.73333333333333, 85.73333333333333, 85.725, 85.725, 85.54166666666667, 85.54166666666667, 85.95, 85.95, 85.68333333333334, 85.68333333333334, 85.64166666666667, 85.64166666666667, 85.50833333333334, 85.50833333333334, 85.94166666666666, 85.94166666666666, 85.5, 85.5, 85.89166666666667, 85.89166666666667, 85.25833333333334, 85.25833333333334, 85.675, 85.675, 86.65833333333333, 86.65833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.044, Test loss: 0.947, Test accuracy: 82.42
Average accuracy final 10 rounds: 81.61999999999999 

1579.6279578208923
[1.698671579360962, 3.397343158721924, 4.9346702098846436, 6.471997261047363, 7.97927451133728, 9.486551761627197, 10.99276876449585, 12.498985767364502, 13.997092723846436, 15.49519968032837, 16.988056898117065, 18.48091411590576, 19.974703311920166, 21.46849250793457, 22.968594551086426, 24.46869659423828, 25.959442853927612, 27.450189113616943, 28.973374605178833, 30.496560096740723, 32.01310992240906, 33.52965974807739, 35.03617238998413, 36.54268503189087, 38.04706072807312, 39.55143642425537, 41.058332204818726, 42.56522798538208, 44.064844608306885, 45.56446123123169, 47.06060481071472, 48.556748390197754, 50.0634663105011, 51.57018423080444, 53.08130216598511, 54.59242010116577, 56.0949764251709, 57.597532749176025, 59.09990382194519, 60.602274894714355, 62.10272765159607, 63.60318040847778, 65.10913324356079, 66.6150860786438, 68.12749075889587, 69.63989543914795, 71.1450846195221, 72.65027379989624, 74.15191555023193, 75.65355730056763, 77.15762090682983, 78.66168451309204, 80.17134857177734, 81.68101263046265, 83.18257236480713, 84.68413209915161, 86.16742897033691, 87.65072584152222, 89.15975332260132, 90.66878080368042, 92.17258667945862, 93.67639255523682, 95.17870283126831, 96.6810131072998, 98.18119525909424, 99.68137741088867, 101.17065477371216, 102.65993213653564, 104.16892695426941, 105.67792177200317, 107.18723130226135, 108.69654083251953, 110.01992774009705, 111.34331464767456, 112.66328954696655, 113.98326444625854, 115.31247425079346, 116.64168405532837, 117.95691275596619, 119.272141456604, 120.59875893592834, 121.92537641525269, 123.23420548439026, 124.54303455352783, 125.86514091491699, 127.18724727630615, 128.51161575317383, 129.8359842300415, 131.1619417667389, 132.48789930343628, 133.80172753334045, 135.11555576324463, 136.4241590499878, 137.73276233673096, 139.03719854354858, 140.3416347503662, 141.65451526641846, 142.9673957824707, 144.28629541397095, 145.6051950454712, 146.93223524093628, 148.25927543640137, 149.59273195266724, 150.9261884689331, 152.253511428833, 153.5808343887329, 154.90363836288452, 156.22644233703613, 157.5554656982422, 158.88448905944824, 160.1970932483673, 161.50969743728638, 162.83189940452576, 164.15410137176514, 165.4822497367859, 166.81039810180664, 168.13644790649414, 169.46249771118164, 170.78858423233032, 172.114670753479, 173.41912651062012, 174.72358226776123, 176.10683250427246, 177.4900827407837, 178.81945037841797, 180.14881801605225, 181.47065782546997, 182.7924976348877, 184.11881160736084, 185.44512557983398, 186.81355595588684, 188.1819863319397, 189.5419840812683, 190.90198183059692, 192.2139813899994, 193.52598094940186, 194.8792314529419, 196.23248195648193, 197.5559799671173, 198.87947797775269, 200.2108278274536, 201.54217767715454, 202.85899114608765, 204.17580461502075, 205.48721551895142, 206.79862642288208, 208.11918377876282, 209.43974113464355, 210.7635452747345, 212.08734941482544, 213.40964818000793, 214.73194694519043, 216.04485392570496, 217.35776090621948, 218.66536045074463, 219.97295999526978, 221.29218006134033, 222.6114001274109, 223.92902088165283, 225.24664163589478, 226.55242657661438, 227.85821151733398, 229.16693449020386, 230.47565746307373, 231.79723691940308, 233.11881637573242, 234.43941235542297, 235.76000833511353, 237.07591581344604, 238.39182329177856, 239.71608543395996, 241.04034757614136, 242.36285519599915, 243.68536281585693, 245.00169563293457, 246.3180284500122, 247.64107060432434, 248.96411275863647, 250.28545093536377, 251.60678911209106, 252.92121839523315, 254.23564767837524, 255.56115412712097, 256.8866605758667, 258.20014572143555, 259.5136308670044, 260.83340096473694, 262.1531710624695, 263.4729104042053, 264.79264974594116, 266.1111285686493, 267.4296073913574, 268.7536642551422, 270.077721118927, 271.39139771461487, 272.70507431030273, 274.0294985771179, 275.3539228439331, 276.6715738773346, 277.9892249107361, 280.1239244937897, 282.25862407684326]
[16.575, 16.575, 30.841666666666665, 30.841666666666665, 36.916666666666664, 36.916666666666664, 40.71666666666667, 40.71666666666667, 60.18333333333333, 60.18333333333333, 60.733333333333334, 60.733333333333334, 58.391666666666666, 58.391666666666666, 65.66666666666667, 65.66666666666667, 70.61666666666666, 70.61666666666666, 70.55, 70.55, 71.04166666666667, 71.04166666666667, 71.975, 71.975, 72.61666666666666, 72.61666666666666, 73.11666666666666, 73.11666666666666, 73.94166666666666, 73.94166666666666, 74.60833333333333, 74.60833333333333, 74.70833333333333, 74.70833333333333, 75.69166666666666, 75.69166666666666, 76.525, 76.525, 76.51666666666667, 76.51666666666667, 76.775, 76.775, 76.975, 76.975, 76.76666666666667, 76.76666666666667, 77.24166666666666, 77.24166666666666, 78.03333333333333, 78.03333333333333, 77.90833333333333, 77.90833333333333, 77.875, 77.875, 79.21666666666667, 79.21666666666667, 79.39166666666667, 79.39166666666667, 79.53333333333333, 79.53333333333333, 79.50833333333334, 79.50833333333334, 79.2, 79.2, 79.70833333333333, 79.70833333333333, 79.40833333333333, 79.40833333333333, 79.6, 79.6, 79.6, 79.6, 79.975, 79.975, 80.19166666666666, 80.19166666666666, 79.89166666666667, 79.89166666666667, 80.00833333333334, 80.00833333333334, 80.28333333333333, 80.28333333333333, 80.2, 80.2, 80.16666666666667, 80.16666666666667, 80.48333333333333, 80.48333333333333, 80.91666666666667, 80.91666666666667, 81.25833333333334, 81.25833333333334, 81.64166666666667, 81.64166666666667, 81.55833333333334, 81.55833333333334, 81.125, 81.125, 81.25, 81.25, 81.2, 81.2, 81.14166666666667, 81.14166666666667, 81.275, 81.275, 80.925, 80.925, 80.89166666666667, 80.89166666666667, 80.675, 80.675, 81.01666666666667, 81.01666666666667, 81.325, 81.325, 81.0, 81.0, 81.09166666666667, 81.09166666666667, 80.925, 80.925, 80.98333333333333, 80.98333333333333, 81.33333333333333, 81.33333333333333, 81.575, 81.575, 81.56666666666666, 81.56666666666666, 81.525, 81.525, 81.48333333333333, 81.48333333333333, 81.54166666666667, 81.54166666666667, 81.50833333333334, 81.50833333333334, 81.74166666666666, 81.74166666666666, 81.54166666666667, 81.54166666666667, 81.075, 81.075, 81.05833333333334, 81.05833333333334, 81.16666666666667, 81.16666666666667, 80.65, 80.65, 80.225, 80.225, 81.00833333333334, 81.00833333333334, 80.89166666666667, 80.89166666666667, 80.88333333333334, 80.88333333333334, 81.00833333333334, 81.00833333333334, 81.78333333333333, 81.78333333333333, 81.75833333333334, 81.75833333333334, 81.71666666666667, 81.71666666666667, 81.65833333333333, 81.65833333333333, 81.75, 81.75, 81.775, 81.775, 81.3, 81.3, 81.025, 81.025, 81.35, 81.35, 81.65, 81.65, 81.80833333333334, 81.80833333333334, 81.775, 81.775, 81.525, 81.525, 81.575, 81.575, 81.61666666666666, 81.61666666666666, 81.56666666666666, 81.56666666666666, 81.65833333333333, 81.65833333333333, 81.69166666666666, 81.69166666666666, 81.63333333333334, 81.63333333333334, 81.35, 81.35, 82.425, 82.425]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Final Round, Train loss: 0.073, Test loss: 0.788, Test accuracy: 66.58
Average accuracy final 10 rounds: 66.91166666666666
1893.5923430919647
[]
[16.016666666666666, 25.925, 29.208333333333332, 38.825, 40.891666666666666, 44.0, 47.36666666666667, 51.108333333333334, 55.233333333333334, 55.84166666666667, 55.983333333333334, 56.75833333333333, 57.175, 58.45, 58.525, 60.3, 60.61666666666667, 60.25, 59.483333333333334, 61.78333333333333, 62.358333333333334, 61.791666666666664, 62.05, 62.19166666666667, 62.425, 62.45, 63.625, 62.35, 63.05, 63.15833333333333, 63.84166666666667, 63.208333333333336, 64.56666666666666, 64.56666666666666, 65.33333333333333, 66.43333333333334, 65.03333333333333, 65.25, 65.31666666666666, 65.625, 64.10833333333333, 63.975, 64.30833333333334, 63.483333333333334, 64.19166666666666, 65.0, 65.475, 64.60833333333333, 66.00833333333334, 67.35, 66.3, 65.04166666666667, 64.43333333333334, 64.45, 65.34166666666667, 67.00833333333334, 66.94166666666666, 67.36666666666666, 67.075, 67.68333333333334, 67.075, 66.79166666666667, 67.15, 67.55833333333334, 68.86666666666666, 69.26666666666667, 69.36666666666666, 68.66666666666667, 67.9, 66.78333333333333, 67.86666666666666, 67.925, 67.875, 69.39166666666667, 68.86666666666666, 68.04166666666667, 68.28333333333333, 69.45833333333333, 69.34166666666667, 69.125, 69.25833333333334, 68.90833333333333, 67.66666666666667, 66.65833333333333, 65.18333333333334, 65.725, 65.74166666666666, 65.0, 65.53333333333333, 66.03333333333333, 66.53333333333333, 67.075, 67.0, 67.525, 67.5, 66.83333333333333, 65.95833333333333, 67.14166666666667, 66.71666666666667, 66.83333333333333, 66.575]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.615, Test loss: 0.561, Test accuracy: 77.20
Average accuracy final 10 rounds: 77.2725
Average global accuracy final 10 rounds: 77.2725
1415.8955476284027
[]
[25.975, 33.93333333333333, 39.541666666666664, 47.125, 59.916666666666664, 60.06666666666667, 62.291666666666664, 60.425, 62.6, 62.8, 62.15, 63.95, 65.025, 66.33333333333333, 67.18333333333334, 67.475, 68.35, 68.0, 68.81666666666666, 70.39166666666667, 70.85, 71.025, 71.0, 71.1, 71.80833333333334, 72.04166666666667, 72.60833333333333, 72.775, 72.81666666666666, 72.3, 72.775, 73.06666666666666, 73.51666666666667, 73.45, 73.64166666666667, 74.20833333333333, 73.91666666666667, 73.90833333333333, 74.14166666666667, 74.26666666666667, 74.21666666666667, 73.86666666666666, 74.275, 74.30833333333334, 73.94166666666666, 75.15833333333333, 75.29166666666667, 74.25833333333334, 74.53333333333333, 75.06666666666666, 74.58333333333333, 74.3, 74.84166666666667, 74.43333333333334, 74.65, 75.8, 76.2, 76.14166666666667, 76.18333333333334, 75.275, 75.94166666666666, 75.56666666666666, 76.375, 76.21666666666667, 76.44166666666666, 76.20833333333333, 76.76666666666667, 76.60833333333333, 76.40833333333333, 76.35833333333333, 76.45, 76.175, 76.03333333333333, 75.8, 76.15, 75.975, 76.26666666666667, 76.325, 76.25833333333334, 76.30833333333334, 76.425, 76.2, 76.175, 75.875, 75.625, 75.9, 76.86666666666666, 76.975, 76.875, 76.80833333333334, 77.2, 77.63333333333334, 77.48333333333333, 77.58333333333333, 77.75833333333334, 77.40833333333333, 76.64166666666667, 77.0, 76.775, 77.24166666666666, 77.2]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.77
Final Round, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.52
Average accuracy final 10 rounds: 10.64725 

Average global accuracy final 10 rounds: 10.515750000000002 

5103.164774894714
[5.595698833465576, 10.904415130615234, 16.270564794540405, 21.633708477020264, 26.923089027404785, 32.28213024139404, 37.644813537597656, 43.02292013168335, 48.36277508735657, 53.04651165008545, 57.68609356880188, 62.318777084350586, 66.96153283119202, 71.64692616462708, 76.31996870040894, 80.98509693145752, 85.70097494125366, 90.38658046722412, 95.12540030479431, 99.8864517211914, 104.61517667770386, 109.23854231834412, 113.96099805831909, 118.68950176239014, 123.31507587432861, 127.98411726951599, 132.65900683403015, 137.28365111351013, 141.89605593681335, 146.52810311317444, 151.18856644630432, 155.8392903804779, 160.4958519935608, 165.10301089286804, 169.71344327926636, 174.34666848182678, 179.01277327537537, 183.6475384235382, 188.25551629066467, 192.84173727035522, 197.43730282783508, 202.02936553955078, 206.65352869033813, 211.28164148330688, 215.8895239830017, 220.4972870349884, 225.10782408714294, 229.73842859268188, 234.37314677238464, 238.9632658958435, 243.5468053817749, 248.1335699558258, 252.7252037525177, 257.32347798347473, 261.92975783348083, 266.54738545417786, 271.1577422618866, 275.7616868019104, 280.36405539512634, 284.98314929008484, 289.5953688621521, 294.1866829395294, 298.79348826408386, 303.3923935890198, 307.9866497516632, 312.5862138271332, 317.1928861141205, 321.82333278656006, 326.44106221199036, 331.06822967529297, 335.6755037307739, 340.29720544815063, 344.9300057888031, 349.53870153427124, 354.172367811203, 358.76132917404175, 363.37277388572693, 367.99259185791016, 372.6066343784332, 377.2234842777252, 381.8467333316803, 386.4715361595154, 391.1021258831024, 395.72643303871155, 400.37580704689026, 405.0260922908783, 409.66056632995605, 414.2595045566559, 418.8688578605652, 423.4729678630829, 428.06330013275146, 432.6864788532257, 437.3070333003998, 441.92649030685425, 446.5576186180115, 451.18684339523315, 455.8231873512268, 460.470591545105, 465.1197702884674, 469.7231249809265, 472.03471994400024]
[10.015, 10.05, 10.01, 10.015, 10.0525, 10.12, 10.09, 10.06, 10.075, 10.0725, 10.0925, 10.075, 10.11, 10.1075, 10.0575, 10.0575, 10.085, 10.0625, 10.0425, 10.095, 10.13, 10.145, 10.1025, 10.08, 10.115, 10.125, 10.0925, 9.98, 10.1275, 10.165, 10.0875, 10.2125, 10.22, 10.285, 10.3075, 10.3825, 10.3675, 10.33, 10.22, 10.3175, 10.3925, 10.4525, 10.475, 10.3825, 10.415, 10.615, 10.635, 10.495, 10.48, 10.5025, 10.4375, 10.525, 10.6075, 10.47, 10.56, 10.49, 10.555, 10.6325, 10.605, 10.5275, 10.74, 10.6175, 10.75, 10.8675, 10.78, 10.6425, 10.585, 10.7275, 10.7025, 10.6825, 10.6525, 10.5225, 10.51, 10.61, 10.645, 10.625, 10.665, 10.63, 10.6025, 10.655, 10.6475, 10.75, 10.7625, 10.88, 10.875, 10.9025, 10.645, 10.62, 10.4975, 10.57, 10.605, 10.555, 10.7875, 10.77, 10.6825, 10.48, 10.5275, 10.5025, 10.715, 10.8475, 10.765]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.422, Test loss: 2.463, Test accuracy: 17.27
Average accuracy final 10 rounds: 14.710749999999999
8124.5711488723755
[12.635523557662964, 25.20308756828308, 37.75546741485596, 50.27739453315735, 62.806732416152954, 75.35084176063538, 86.60041952133179, 97.84343695640564, 109.21174240112305, 121.70275115966797, 134.2412874698639, 146.71025156974792, 159.1464557647705, 171.66247630119324, 184.0736494064331, 196.59522366523743, 209.23449110984802, 221.04278349876404, 233.60367035865784, 244.70985412597656, 255.92326259613037, 267.1558825969696, 278.40305972099304, 289.6268525123596, 300.88584303855896, 312.20214653015137, 323.3599615097046, 334.6995840072632, 345.9617409706116, 357.3177888393402, 369.75452065467834, 382.27533292770386, 394.7650799751282, 407.12554454803467, 418.4369795322418, 430.94389390945435, 442.16550493240356, 453.6536548137665, 465.0167021751404, 476.2178134918213, 487.4815266132355, 498.85613489151, 510.1042432785034, 521.3757259845734, 533.7234942913055, 544.9566552639008, 556.1300497055054, 568.7645001411438, 581.3962621688843, 594.145592212677, 606.738128900528, 619.3233952522278, 631.9804935455322, 644.6281027793884, 657.302116394043, 669.913666009903, 682.5549728870392, 695.2059977054596, 707.830593585968, 720.4975528717041, 733.1190528869629, 745.6496176719666, 758.0617311000824, 770.4337010383606, 782.9519503116608, 795.4360632896423, 806.8909471035004, 818.1799354553223, 829.4628720283508, 840.7892799377441, 852.1216921806335, 863.339953660965, 874.6310021877289, 885.8552451133728, 897.0944082736969, 908.4792070388794, 919.8232936859131, 931.1358168125153, 942.3257040977478, 953.5377368927002, 964.8105401992798, 976.1849336624146, 987.3004457950592, 998.5094339847565, 1009.6997649669647, 1020.843759059906, 1032.0475206375122, 1043.0984723567963, 1054.263510942459, 1065.3593327999115, 1076.4705884456635, 1087.748919725418, 1098.8907580375671, 1110.0336813926697, 1121.274062871933, 1132.475422859192, 1143.7909543514252, 1154.9373078346252, 1166.2086329460144, 1177.4031326770782, 1180.2679734230042]
[11.6775, 12.7625, 13.35, 12.8225, 12.8575, 13.325, 14.02, 13.1825, 14.395, 14.1575, 14.2275, 14.09, 14.39, 14.265, 13.785, 14.2875, 15.46, 16.0175, 15.13, 14.45, 13.3275, 14.2775, 13.0375, 14.1325, 14.7625, 13.815, 15.0575, 14.865, 16.025, 14.185, 14.525, 15.0575, 13.945, 15.1675, 14.625, 15.865, 12.5725, 15.905, 15.7375, 13.99, 15.46, 15.45, 15.175, 14.9925, 13.765, 14.54, 15.4525, 15.33, 14.9, 14.6525, 13.595, 15.225, 16.78, 14.725, 16.2375, 12.695, 14.6625, 14.0575, 15.0475, 14.6125, 16.32, 13.57, 13.9825, 15.6875, 16.7525, 15.0325, 16.235, 15.185, 14.355, 15.74, 14.05, 13.6725, 16.4675, 15.37, 14.49, 14.9325, 14.2575, 15.7725, 14.015, 14.8525, 15.99, 15.7025, 15.0325, 16.8375, 12.965, 14.87, 14.8075, 15.895, 15.765, 15.115, 14.1625, 15.3375, 15.25, 13.905, 15.1075, 16.0225, 14.9675, 14.7025, 13.93, 13.7225, 17.2725]
python: can't open file 'main_fedpac_k.py': [Errno 2] No such file or directory
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.406, Test loss: 0.714, Test accuracy: 77.18
Average accuracy final 10 rounds: 76.74800000000002
5743.036155462265
[6.1608264446258545, 12.321652889251709, 18.037980556488037, 23.754308223724365, 29.521003484725952, 35.28769874572754, 41.075003147125244, 46.86230754852295, 52.632609605789185, 58.40291166305542, 64.06246519088745, 69.72201871871948, 75.35609102249146, 80.99016332626343, 86.68643069267273, 92.38269805908203, 98.06855654716492, 103.7544150352478, 109.46927118301392, 115.18412733078003, 120.90778946876526, 126.63145160675049, 132.3285892009735, 138.02572679519653, 143.79491996765137, 149.5641131401062, 155.19062042236328, 160.81712770462036, 166.55439233779907, 172.29165697097778, 178.0429949760437, 183.79433298110962, 189.564466714859, 195.3346004486084, 201.06556105613708, 206.79652166366577, 212.38144063949585, 217.96635961532593, 223.6372890472412, 229.3082184791565, 234.89785432815552, 240.48749017715454, 246.24975085258484, 252.01201152801514, 257.6896560192108, 263.3673005104065, 268.9736614227295, 274.5800223350525, 280.34882521629333, 286.1176280975342, 291.8494565486908, 297.5812849998474, 303.2688248157501, 308.95636463165283, 314.6580493450165, 320.3597340583801, 326.0852723121643, 331.8108105659485, 337.5580151081085, 343.30521965026855, 349.0343282222748, 354.763436794281, 360.3747878074646, 365.9861388206482, 371.73054003715515, 377.4749412536621, 383.2427225112915, 389.0105037689209, 394.90096497535706, 400.7914261817932, 406.63347578048706, 412.4755253791809, 418.31060695648193, 424.14568853378296, 429.93417096138, 435.72265338897705, 441.5136568546295, 447.304660320282, 453.1929395198822, 459.0812187194824, 464.8996124267578, 470.7180061340332, 476.5932893753052, 482.46857261657715, 488.2576560974121, 494.04673957824707, 499.8417458534241, 505.6367521286011, 511.4190514087677, 517.2013506889343, 522.9315042495728, 528.6616578102112, 534.4337768554688, 540.2058959007263, 545.9719240665436, 551.7379522323608, 557.0303838253021, 562.3228154182434, 567.8766481876373, 573.4304809570312, 579.0140135288239, 584.5975461006165, 590.2965672016144, 595.9955883026123, 601.6497390270233, 607.3038897514343, 612.8988978862762, 618.4939060211182, 624.250349521637, 630.0067930221558, 635.671884059906, 641.3369750976562, 647.0481691360474, 652.7593631744385, 658.5035321712494, 664.2477011680603, 669.9582197666168, 675.6687383651733, 681.413923740387, 687.1591091156006, 692.9719576835632, 698.7848062515259, 704.4368214607239, 710.0888366699219, 715.6567180156708, 721.2245993614197, 726.864461183548, 732.5043230056763, 738.336124420166, 744.1679258346558, 750.0215456485748, 755.8751654624939, 761.7079784870148, 767.5407915115356, 773.394280910492, 779.2477703094482, 784.9573411941528, 790.6669120788574, 796.6314196586609, 802.5959272384644, 808.2779493331909, 813.9599714279175, 819.827223777771, 825.6944761276245, 831.3856081962585, 837.0767402648926, 842.8131504058838, 848.549560546875, 854.2586102485657, 859.9676599502563, 865.6614217758179, 871.3551836013794, 877.2925109863281, 883.2298383712769, 889.1889252662659, 895.1480121612549, 900.9883313179016, 906.8286504745483, 912.6827387809753, 918.5368270874023, 924.5466551780701, 930.5564832687378, 936.389524936676, 942.2225666046143, 948.2659783363342, 954.3093900680542, 960.2526803016663, 966.1959705352783, 972.100029706955, 978.0040888786316, 983.9308731555939, 989.8576574325562, 995.8673739433289, 1001.8770904541016, 1007.9700524806976, 1014.0630145072937, 1019.807776927948, 1025.5525393486023, 1031.5231187343597, 1037.4936981201172, 1043.7585406303406, 1050.023383140564, 1056.2169518470764, 1062.4105205535889, 1068.6114733219147, 1074.8124260902405, 1081.191656589508, 1087.5708870887756, 1093.91037774086, 1100.2498683929443, 1106.2894129753113, 1112.3289575576782, 1118.5692522525787, 1124.8095469474792, 1130.9969592094421, 1137.184371471405, 1143.357561826706, 1149.5307521820068, 1155.7063417434692, 1161.8819313049316, 1164.1748106479645, 1166.4676899909973]
[11.6775, 11.6775, 17.77, 17.77, 25.8975, 25.8975, 30.73, 30.73, 38.34, 38.34, 43.875, 43.875, 45.1725, 45.1725, 49.175, 49.175, 52.945, 52.945, 53.32, 53.32, 54.5825, 54.5825, 55.4025, 55.4025, 57.4, 57.4, 61.9325, 61.9325, 62.6375, 62.6375, 63.875, 63.875, 64.2225, 64.2225, 64.645, 64.645, 68.7825, 68.7825, 68.97, 68.97, 69.645, 69.645, 69.795, 69.795, 69.9725, 69.9725, 71.2925, 71.2925, 71.15, 71.15, 71.8025, 71.8025, 72.165, 72.165, 72.2125, 72.2125, 72.6825, 72.6825, 72.97, 72.97, 72.76, 72.76, 72.965, 72.965, 73.515, 73.515, 73.47, 73.47, 73.8425, 73.8425, 73.895, 73.895, 73.5975, 73.5975, 73.705, 73.705, 74.1425, 74.1425, 74.3125, 74.3125, 74.67, 74.67, 74.8, 74.8, 74.4675, 74.4675, 75.11, 75.11, 74.7325, 74.7325, 74.795, 74.795, 75.2, 75.2, 74.8125, 74.8125, 75.2825, 75.2825, 75.2625, 75.2625, 74.965, 74.965, 75.2975, 75.2975, 75.6775, 75.6775, 75.47, 75.47, 75.7275, 75.7275, 75.5775, 75.5775, 75.6525, 75.6525, 75.88, 75.88, 75.3975, 75.3975, 75.36, 75.36, 75.2975, 75.2975, 76.07, 76.07, 75.8425, 75.8425, 76.4425, 76.4425, 76.375, 76.375, 76.1275, 76.1275, 75.86, 75.86, 75.8725, 75.8725, 75.88, 75.88, 76.5725, 76.5725, 76.415, 76.415, 76.5025, 76.5025, 76.4225, 76.4225, 76.3925, 76.3925, 76.0625, 76.0625, 76.0075, 76.0075, 76.365, 76.365, 76.3975, 76.3975, 76.5675, 76.5675, 76.495, 76.495, 76.3825, 76.3825, 76.3675, 76.3675, 76.85, 76.85, 76.7, 76.7, 76.6725, 76.6725, 76.7125, 76.7125, 76.56, 76.56, 76.04, 76.04, 76.7075, 76.7075, 76.57, 76.57, 76.9925, 76.9925, 76.9325, 76.9325, 76.6925, 76.6925, 76.45, 76.45, 76.575, 76.575, 76.7, 76.7, 76.555, 76.555, 76.385, 76.385, 77.1625, 77.1625, 77.035, 77.035, 77.1775, 77.1775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.293, Test loss: 2.265, Test accuracy: 44.15
Round   0, Global train loss: 2.293, Global test loss: 2.265, Global test accuracy: 45.60
Round   1, Train loss: 1.990, Test loss: 1.889, Test accuracy: 65.02
Round   1, Global train loss: 1.990, Global test loss: 1.767, Global test accuracy: 72.48
Round   2, Train loss: 1.837, Test loss: 1.783, Test accuracy: 73.18
Round   2, Global train loss: 1.837, Global test loss: 1.717, Global test accuracy: 76.61
Round   3, Train loss: 1.663, Test loss: 1.733, Test accuracy: 76.94
Round   3, Global train loss: 1.663, Global test loss: 1.650, Global test accuracy: 82.37
Round   4, Train loss: 1.610, Test loss: 1.707, Test accuracy: 79.31
Round   4, Global train loss: 1.610, Global test loss: 1.611, Global test accuracy: 85.94
Round   5, Train loss: 1.678, Test loss: 1.678, Test accuracy: 80.67
Round   5, Global train loss: 1.678, Global test loss: 1.650, Global test accuracy: 82.14
Round   6, Train loss: 1.596, Test loss: 1.669, Test accuracy: 81.50
Round   6, Global train loss: 1.596, Global test loss: 1.628, Global test accuracy: 83.19
Round   7, Train loss: 1.586, Test loss: 1.660, Test accuracy: 82.22
Round   7, Global train loss: 1.586, Global test loss: 1.629, Global test accuracy: 83.30
Round   8, Train loss: 1.627, Test loss: 1.632, Test accuracy: 83.94
Round   8, Global train loss: 1.627, Global test loss: 1.624, Global test accuracy: 84.23
Round   9, Train loss: 1.570, Test loss: 1.626, Test accuracy: 84.42
Round   9, Global train loss: 1.570, Global test loss: 1.598, Global test accuracy: 87.31
Round  10, Train loss: 1.586, Test loss: 1.621, Test accuracy: 84.66
Round  10, Global train loss: 1.586, Global test loss: 1.630, Global test accuracy: 83.50
Round  11, Train loss: 1.550, Test loss: 1.621, Test accuracy: 84.64
Round  11, Global train loss: 1.550, Global test loss: 1.623, Global test accuracy: 83.75
Round  12, Train loss: 1.535, Test loss: 1.616, Test accuracy: 85.09
Round  12, Global train loss: 1.535, Global test loss: 1.573, Global test accuracy: 89.37
Round  13, Train loss: 1.569, Test loss: 1.613, Test accuracy: 85.38
Round  13, Global train loss: 1.569, Global test loss: 1.629, Global test accuracy: 83.33
Round  14, Train loss: 1.535, Test loss: 1.611, Test accuracy: 85.54
Round  14, Global train loss: 1.535, Global test loss: 1.588, Global test accuracy: 88.09
Round  15, Train loss: 1.532, Test loss: 1.600, Test accuracy: 86.72
Round  15, Global train loss: 1.532, Global test loss: 1.561, Global test accuracy: 90.80
Round  16, Train loss: 1.569, Test loss: 1.595, Test accuracy: 87.16
Round  16, Global train loss: 1.569, Global test loss: 1.629, Global test accuracy: 83.55
Round  17, Train loss: 1.535, Test loss: 1.593, Test accuracy: 87.34
Round  17, Global train loss: 1.535, Global test loss: 1.588, Global test accuracy: 87.88
Round  18, Train loss: 1.523, Test loss: 1.590, Test accuracy: 87.67
Round  18, Global train loss: 1.523, Global test loss: 1.569, Global test accuracy: 89.81
Round  19, Train loss: 1.521, Test loss: 1.586, Test accuracy: 88.09
Round  19, Global train loss: 1.521, Global test loss: 1.575, Global test accuracy: 89.16
Round  20, Train loss: 1.527, Test loss: 1.586, Test accuracy: 88.05
Round  20, Global train loss: 1.527, Global test loss: 1.604, Global test accuracy: 85.93
Round  21, Train loss: 1.510, Test loss: 1.585, Test accuracy: 88.09
Round  21, Global train loss: 1.510, Global test loss: 1.564, Global test accuracy: 90.27
Round  22, Train loss: 1.511, Test loss: 1.584, Test accuracy: 88.22
Round  22, Global train loss: 1.511, Global test loss: 1.565, Global test accuracy: 89.92
Round  23, Train loss: 1.511, Test loss: 1.582, Test accuracy: 88.36
Round  23, Global train loss: 1.511, Global test loss: 1.565, Global test accuracy: 90.18
Round  24, Train loss: 1.511, Test loss: 1.582, Test accuracy: 88.37
Round  24, Global train loss: 1.511, Global test loss: 1.569, Global test accuracy: 89.71
Round  25, Train loss: 1.522, Test loss: 1.582, Test accuracy: 88.36
Round  25, Global train loss: 1.522, Global test loss: 1.596, Global test accuracy: 86.82
Round  26, Train loss: 1.524, Test loss: 1.581, Test accuracy: 88.44
Round  26, Global train loss: 1.524, Global test loss: 1.593, Global test accuracy: 87.23
Round  27, Train loss: 1.506, Test loss: 1.581, Test accuracy: 88.45
Round  27, Global train loss: 1.506, Global test loss: 1.567, Global test accuracy: 89.94
Round  28, Train loss: 1.520, Test loss: 1.581, Test accuracy: 88.46
Round  28, Global train loss: 1.520, Global test loss: 1.588, Global test accuracy: 87.67
Round  29, Train loss: 1.509, Test loss: 1.580, Test accuracy: 88.50
Round  29, Global train loss: 1.509, Global test loss: 1.562, Global test accuracy: 90.28
Round  30, Train loss: 1.521, Test loss: 1.580, Test accuracy: 88.46
Round  30, Global train loss: 1.521, Global test loss: 1.593, Global test accuracy: 87.33
Round  31, Train loss: 1.505, Test loss: 1.579, Test accuracy: 88.53
Round  31, Global train loss: 1.505, Global test loss: 1.562, Global test accuracy: 90.50
Round  32, Train loss: 1.490, Test loss: 1.579, Test accuracy: 88.57
Round  32, Global train loss: 1.490, Global test loss: 1.552, Global test accuracy: 91.20
Round  33, Train loss: 1.502, Test loss: 1.579, Test accuracy: 88.58
Round  33, Global train loss: 1.502, Global test loss: 1.561, Global test accuracy: 90.50
Round  34, Train loss: 1.487, Test loss: 1.579, Test accuracy: 88.63
Round  34, Global train loss: 1.487, Global test loss: 1.551, Global test accuracy: 91.47
Round  35, Train loss: 1.490, Test loss: 1.579, Test accuracy: 88.58
Round  35, Global train loss: 1.490, Global test loss: 1.551, Global test accuracy: 91.38
Round  36, Train loss: 1.488, Test loss: 1.578, Test accuracy: 88.61
Round  36, Global train loss: 1.488, Global test loss: 1.552, Global test accuracy: 91.24
Round  37, Train loss: 1.505, Test loss: 1.578, Test accuracy: 88.66
Round  37, Global train loss: 1.505, Global test loss: 1.559, Global test accuracy: 90.69
Round  38, Train loss: 1.504, Test loss: 1.578, Test accuracy: 88.63
Round  38, Global train loss: 1.504, Global test loss: 1.562, Global test accuracy: 90.33
Round  39, Train loss: 1.485, Test loss: 1.578, Test accuracy: 88.66
Round  39, Global train loss: 1.485, Global test loss: 1.548, Global test accuracy: 91.53
Round  40, Train loss: 1.486, Test loss: 1.578, Test accuracy: 88.64
Round  40, Global train loss: 1.486, Global test loss: 1.547, Global test accuracy: 91.69
Round  41, Train loss: 1.532, Test loss: 1.578, Test accuracy: 88.65
Round  41, Global train loss: 1.532, Global test loss: 1.614, Global test accuracy: 84.51
Round  42, Train loss: 1.499, Test loss: 1.578, Test accuracy: 88.63
Round  42, Global train loss: 1.499, Global test loss: 1.557, Global test accuracy: 90.92
Round  43, Train loss: 1.485, Test loss: 1.578, Test accuracy: 88.63
Round  43, Global train loss: 1.485, Global test loss: 1.549, Global test accuracy: 91.67
Round  44, Train loss: 1.516, Test loss: 1.578, Test accuracy: 88.65
Round  44, Global train loss: 1.516, Global test loss: 1.579, Global test accuracy: 88.53
Round  45, Train loss: 1.502, Test loss: 1.578, Test accuracy: 88.63
Round  45, Global train loss: 1.502, Global test loss: 1.554, Global test accuracy: 90.97
Round  46, Train loss: 1.486, Test loss: 1.578, Test accuracy: 88.61
Round  46, Global train loss: 1.486, Global test loss: 1.551, Global test accuracy: 91.25
Round  47, Train loss: 1.498, Test loss: 1.578, Test accuracy: 88.60
Round  47, Global train loss: 1.498, Global test loss: 1.555, Global test accuracy: 91.00
Round  48, Train loss: 1.485, Test loss: 1.578, Test accuracy: 88.61
Round  48, Global train loss: 1.485, Global test loss: 1.548, Global test accuracy: 91.71
Round  49, Train loss: 1.493, Test loss: 1.575, Test accuracy: 88.90
Round  49, Global train loss: 1.493, Global test loss: 1.551, Global test accuracy: 91.29
Round  50, Train loss: 1.482, Test loss: 1.575, Test accuracy: 88.94
Round  50, Global train loss: 1.482, Global test loss: 1.547, Global test accuracy: 91.65
Round  51, Train loss: 1.484, Test loss: 1.575, Test accuracy: 88.97
Round  51, Global train loss: 1.484, Global test loss: 1.551, Global test accuracy: 91.31
Round  52, Train loss: 1.500, Test loss: 1.573, Test accuracy: 89.07
Round  52, Global train loss: 1.500, Global test loss: 1.562, Global test accuracy: 90.30
Round  53, Train loss: 1.486, Test loss: 1.573, Test accuracy: 89.11
Round  53, Global train loss: 1.486, Global test loss: 1.550, Global test accuracy: 91.44
Round  54, Train loss: 1.513, Test loss: 1.572, Test accuracy: 89.28
Round  54, Global train loss: 1.513, Global test loss: 1.565, Global test accuracy: 90.01
Round  55, Train loss: 1.485, Test loss: 1.571, Test accuracy: 89.30
Round  55, Global train loss: 1.485, Global test loss: 1.550, Global test accuracy: 91.43
Round  56, Train loss: 1.481, Test loss: 1.571, Test accuracy: 89.30
Round  56, Global train loss: 1.481, Global test loss: 1.546, Global test accuracy: 91.77
Round  57, Train loss: 1.488, Test loss: 1.570, Test accuracy: 89.43
Round  57, Global train loss: 1.488, Global test loss: 1.549, Global test accuracy: 91.55
Round  58, Train loss: 1.502, Test loss: 1.570, Test accuracy: 89.42
Round  58, Global train loss: 1.502, Global test loss: 1.560, Global test accuracy: 90.66
Round  59, Train loss: 1.482, Test loss: 1.570, Test accuracy: 89.42
Round  59, Global train loss: 1.482, Global test loss: 1.549, Global test accuracy: 91.60
Round  60, Train loss: 1.484, Test loss: 1.569, Test accuracy: 89.44
Round  60, Global train loss: 1.484, Global test loss: 1.548, Global test accuracy: 91.53
Round  61, Train loss: 1.501, Test loss: 1.569, Test accuracy: 89.45
Round  61, Global train loss: 1.501, Global test loss: 1.565, Global test accuracy: 89.93
Round  62, Train loss: 1.499, Test loss: 1.569, Test accuracy: 89.49
Round  62, Global train loss: 1.499, Global test loss: 1.558, Global test accuracy: 90.57
Round  63, Train loss: 1.483, Test loss: 1.569, Test accuracy: 89.52
Round  63, Global train loss: 1.483, Global test loss: 1.546, Global test accuracy: 91.68
Round  64, Train loss: 1.485, Test loss: 1.569, Test accuracy: 89.53
Round  64, Global train loss: 1.485, Global test loss: 1.546, Global test accuracy: 91.76
Round  65, Train loss: 1.499, Test loss: 1.569, Test accuracy: 89.53
Round  65, Global train loss: 1.499, Global test loss: 1.558, Global test accuracy: 90.58
Round  66, Train loss: 1.480, Test loss: 1.569, Test accuracy: 89.53
Round  66, Global train loss: 1.480, Global test loss: 1.546, Global test accuracy: 91.78
Round  67, Train loss: 1.483, Test loss: 1.569, Test accuracy: 89.54
Round  67, Global train loss: 1.483, Global test loss: 1.548, Global test accuracy: 91.61
Round  68, Train loss: 1.485, Test loss: 1.569, Test accuracy: 89.56
Round  68, Global train loss: 1.485, Global test loss: 1.544, Global test accuracy: 91.97
Round  69, Train loss: 1.485, Test loss: 1.569, Test accuracy: 89.58
Round  69, Global train loss: 1.485, Global test loss: 1.548, Global test accuracy: 91.47
Round  70, Train loss: 1.482, Test loss: 1.569, Test accuracy: 89.55
Round  70, Global train loss: 1.482, Global test loss: 1.550, Global test accuracy: 91.33
Round  71, Train loss: 1.497, Test loss: 1.569, Test accuracy: 89.53
Round  71, Global train loss: 1.497, Global test loss: 1.553, Global test accuracy: 91.10
Round  72, Train loss: 1.483, Test loss: 1.569, Test accuracy: 89.55
Round  72, Global train loss: 1.483, Global test loss: 1.547, Global test accuracy: 91.57
Round  73, Train loss: 1.481, Test loss: 1.568, Test accuracy: 89.56
Round  73, Global train loss: 1.481, Global test loss: 1.550, Global test accuracy: 91.32
Round  74, Train loss: 1.500, Test loss: 1.568, Test accuracy: 89.56
Round  74, Global train loss: 1.500, Global test loss: 1.550, Global test accuracy: 91.45
Round  75, Train loss: 1.484, Test loss: 1.568, Test accuracy: 89.55
Round  75, Global train loss: 1.484, Global test loss: 1.548, Global test accuracy: 91.75
Round  76, Train loss: 1.484, Test loss: 1.568, Test accuracy: 89.53
Round  76, Global train loss: 1.484, Global test loss: 1.551, Global test accuracy: 91.30
Round  77, Train loss: 1.486, Test loss: 1.568, Test accuracy: 89.56
Round  77, Global train loss: 1.486, Global test loss: 1.549, Global test accuracy: 91.51
Round  78, Train loss: 1.484, Test loss: 1.568, Test accuracy: 89.55
Round  78, Global train loss: 1.484, Global test loss: 1.544, Global test accuracy: 91.89
Round  79, Train loss: 1.489, Test loss: 1.565, Test accuracy: 89.94
Round  79, Global train loss: 1.489, Global test loss: 1.547, Global test accuracy: 91.61
Round  80, Train loss: 1.487, Test loss: 1.565, Test accuracy: 89.95
Round  80, Global train loss: 1.487, Global test loss: 1.547, Global test accuracy: 91.74
Round  81, Train loss: 1.481, Test loss: 1.565, Test accuracy: 89.94
Round  81, Global train loss: 1.481, Global test loss: 1.546, Global test accuracy: 91.93
Round  82, Train loss: 1.481, Test loss: 1.565, Test accuracy: 89.95
Round  82, Global train loss: 1.481, Global test loss: 1.544, Global test accuracy: 92.04
Round  83, Train loss: 1.485, Test loss: 1.565, Test accuracy: 89.95
Round  83, Global train loss: 1.485, Global test loss: 1.552, Global test accuracy: 91.27
Round  84, Train loss: 1.480, Test loss: 1.565, Test accuracy: 89.96
Round  84, Global train loss: 1.480, Global test loss: 1.550, Global test accuracy: 91.45
Round  85, Train loss: 1.480, Test loss: 1.565, Test accuracy: 89.97
Round  85, Global train loss: 1.480, Global test loss: 1.546, Global test accuracy: 91.91
Round  86, Train loss: 1.485, Test loss: 1.565, Test accuracy: 89.96
Round  86, Global train loss: 1.485, Global test loss: 1.552, Global test accuracy: 91.22
Round  87, Train loss: 1.482, Test loss: 1.565, Test accuracy: 89.99
Round  87, Global train loss: 1.482, Global test loss: 1.546, Global test accuracy: 91.78
Round  88, Train loss: 1.482, Test loss: 1.564, Test accuracy: 90.00
Round  88, Global train loss: 1.482, Global test loss: 1.549, Global test accuracy: 91.48
Round  89, Train loss: 1.485, Test loss: 1.564, Test accuracy: 90.00
Round  89, Global train loss: 1.485, Global test loss: 1.548, Global test accuracy: 91.49
Round  90, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.97
Round  90, Global train loss: 1.483, Global test loss: 1.548, Global test accuracy: 91.57
Round  91, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.97
Round  91, Global train loss: 1.483, Global test loss: 1.548, Global test accuracy: 91.64
Round  92, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.97
Round  92, Global train loss: 1.481, Global test loss: 1.553, Global test accuracy: 91.07
Round  93, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.97
Round  93, Global train loss: 1.481, Global test loss: 1.544, Global test accuracy: 91.98
Round  94, Train loss: 1.482, Test loss: 1.564, Test accuracy: 90.00
Round  94, Global train loss: 1.482, Global test loss: 1.543, Global test accuracy: 92.07
Round  95, Train loss: 1.485, Test loss: 1.564, Test accuracy: 89.99
Round  95, Global train loss: 1.485, Global test loss: 1.548, Global test accuracy: 91.43/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.480, Test loss: 1.564, Test accuracy: 89.97
Round  96, Global train loss: 1.480, Global test loss: 1.547, Global test accuracy: 91.68
Round  97, Train loss: 1.482, Test loss: 1.564, Test accuracy: 89.96
Round  97, Global train loss: 1.482, Global test loss: 1.545, Global test accuracy: 91.92
Round  98, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.98
Round  98, Global train loss: 1.483, Global test loss: 1.549, Global test accuracy: 91.74
Round  99, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.95
Round  99, Global train loss: 1.481, Global test loss: 1.544, Global test accuracy: 92.03
Final Round, Train loss: 1.483, Test loss: 1.565, Test accuracy: 89.85
Final Round, Global train loss: 1.483, Global test loss: 1.544, Global test accuracy: 92.03
Average accuracy final 10 rounds: 89.97375 

Average global accuracy final 10 rounds: 91.71325 

5476.526216745377
[3.6140453815460205, 7.228090763092041, 10.947640180587769, 14.667189598083496, 18.442952632904053, 22.21871566772461, 25.32017469406128, 28.42163372039795, 31.53436589241028, 34.64709806442261, 37.72979402542114, 40.81248998641968, 44.019447565078735, 47.22640514373779, 50.387640953063965, 53.54887676239014, 56.907352685928345, 60.26582860946655, 63.4013888835907, 66.53694915771484, 69.49372839927673, 72.45050764083862, 75.43419647216797, 78.41788530349731, 81.53585743904114, 84.65382957458496, 87.77538442611694, 90.89693927764893, 94.13759422302246, 97.378249168396, 100.58589768409729, 103.79354619979858, 106.84836459159851, 109.90318298339844, 112.94393730163574, 115.98469161987305, 119.10553979873657, 122.2263879776001, 125.34958004951477, 128.47277212142944, 131.63442540168762, 134.7960786819458, 137.94677352905273, 141.09746837615967, 144.18997192382812, 147.28247547149658, 150.29182815551758, 153.30118083953857, 156.36104774475098, 159.42091464996338, 162.49654626846313, 165.5721778869629, 168.8057346343994, 172.03929138183594, 175.26838970184326, 178.4974880218506, 181.6929485797882, 184.88840913772583, 188.00806212425232, 191.1277151107788, 194.37682914733887, 197.62594318389893, 200.79838490486145, 203.97082662582397, 207.1648154258728, 210.35880422592163, 213.45565009117126, 216.5524959564209, 219.74645113945007, 222.94040632247925, 226.0849301815033, 229.22945404052734, 232.45439267158508, 235.67933130264282, 238.9217448234558, 242.1641583442688, 245.3162453174591, 248.4683322906494, 251.5275754928589, 254.58681869506836, 257.62425327301025, 260.66168785095215, 263.87167859077454, 267.0816693305969, 270.3618993759155, 273.64212942123413, 277.10259437561035, 280.5630593299866, 283.9736111164093, 287.38416290283203, 290.69381976127625, 294.00347661972046, 297.1695833206177, 300.3356900215149, 303.7923684120178, 307.24904680252075, 310.5848116874695, 313.9205765724182, 317.1924743652344, 320.46437215805054, 323.7904827594757, 327.1165933609009, 330.4416391849518, 333.7666850090027, 337.07795333862305, 340.3892216682434, 343.70067620277405, 347.0121307373047, 350.51166439056396, 354.01119804382324, 357.49717354774475, 360.98314905166626, 364.3855218887329, 367.78789472579956, 371.2218482494354, 374.6558017730713, 378.00514245033264, 381.354483127594, 384.93575954437256, 388.5170359611511, 392.0028359889984, 395.4886360168457, 398.9559063911438, 402.4231767654419, 405.7730224132538, 409.1228680610657, 412.3722314834595, 415.62159490585327, 419.06449460983276, 422.50739431381226, 425.9913680553436, 429.475341796875, 433.05517315864563, 436.63500452041626, 440.11045002937317, 443.5858955383301, 446.8397512435913, 450.09360694885254, 453.1800260543823, 456.2664451599121, 459.3350121974945, 462.4035792350769, 465.758944272995, 469.1143093109131, 472.55592012405396, 475.9975309371948, 479.3535375595093, 482.70954418182373, 486.0234351158142, 489.3373260498047, 492.5972309112549, 495.8571357727051, 499.21841168403625, 502.57968759536743, 505.895437002182, 509.2111864089966, 512.5974953174591, 515.9838042259216, 519.3934025764465, 522.8030009269714, 526.1453998088837, 529.4877986907959, 532.8225071430206, 536.1572155952454, 539.4515759944916, 542.7459363937378, 546.170982837677, 549.5960292816162, 552.8624670505524, 556.1289048194885, 559.5221357345581, 562.9153666496277, 566.2290945053101, 569.5428223609924, 572.7538223266602, 575.9648222923279, 579.2194745540619, 582.4741268157959, 585.9082410335541, 589.3423552513123, 592.7495856285095, 596.1568160057068, 599.6395883560181, 603.1223607063293, 606.5253117084503, 609.9282627105713, 613.1604180335999, 616.3925733566284, 619.7100870609283, 623.0276007652283, 626.364824295044, 629.7020478248596, 632.9157099723816, 636.1293721199036, 639.3081078529358, 642.486843585968, 645.7280418872833, 648.9692401885986, 652.1757090091705, 655.3821778297424, 657.1210646629333, 658.8599514961243]
[44.1475, 44.1475, 65.015, 65.015, 73.18, 73.18, 76.945, 76.945, 79.305, 79.305, 80.6675, 80.6675, 81.495, 81.495, 82.215, 82.215, 83.935, 83.935, 84.4175, 84.4175, 84.66, 84.66, 84.645, 84.645, 85.0875, 85.0875, 85.38, 85.38, 85.5425, 85.5425, 86.7225, 86.7225, 87.1625, 87.1625, 87.345, 87.345, 87.6725, 87.6725, 88.0925, 88.0925, 88.0525, 88.0525, 88.0875, 88.0875, 88.2225, 88.2225, 88.355, 88.355, 88.37, 88.37, 88.3575, 88.3575, 88.4375, 88.4375, 88.45, 88.45, 88.4575, 88.4575, 88.5, 88.5, 88.4625, 88.4625, 88.5275, 88.5275, 88.5675, 88.5675, 88.5825, 88.5825, 88.63, 88.63, 88.58, 88.58, 88.605, 88.605, 88.66, 88.66, 88.6325, 88.6325, 88.6575, 88.6575, 88.6375, 88.6375, 88.6525, 88.6525, 88.6275, 88.6275, 88.6325, 88.6325, 88.6525, 88.6525, 88.6275, 88.6275, 88.61, 88.61, 88.5975, 88.5975, 88.605, 88.605, 88.9, 88.9, 88.9375, 88.9375, 88.97, 88.97, 89.0675, 89.0675, 89.115, 89.115, 89.285, 89.285, 89.3025, 89.3025, 89.2975, 89.2975, 89.4325, 89.4325, 89.425, 89.425, 89.4175, 89.4175, 89.4375, 89.4375, 89.4475, 89.4475, 89.4875, 89.4875, 89.515, 89.515, 89.5275, 89.5275, 89.5275, 89.5275, 89.53, 89.53, 89.5425, 89.5425, 89.56, 89.56, 89.5825, 89.5825, 89.545, 89.545, 89.5325, 89.5325, 89.545, 89.545, 89.565, 89.565, 89.5625, 89.5625, 89.545, 89.545, 89.535, 89.535, 89.5575, 89.5575, 89.5525, 89.5525, 89.9375, 89.9375, 89.9475, 89.9475, 89.935, 89.935, 89.9475, 89.9475, 89.9525, 89.9525, 89.96, 89.96, 89.97, 89.97, 89.9575, 89.9575, 89.9925, 89.9925, 89.995, 89.995, 89.9975, 89.9975, 89.975, 89.975, 89.965, 89.965, 89.9675, 89.9675, 89.975, 89.975, 89.995, 89.995, 89.99, 89.99, 89.975, 89.975, 89.9625, 89.9625, 89.9775, 89.9775, 89.955, 89.955, 89.8525, 89.8525]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.270, Test loss: 2.146, Test accuracy: 31.06
Round   0, Global train loss: 2.270, Global test loss: 2.147, Global test accuracy: 30.95
Round   1, Train loss: 1.896, Test loss: 1.784, Test accuracy: 70.49
Round   1, Global train loss: 1.896, Global test loss: 1.691, Global test accuracy: 80.63
Round   2, Train loss: 1.658, Test loss: 1.757, Test accuracy: 71.67
Round   2, Global train loss: 1.658, Global test loss: 1.641, Global test accuracy: 83.03
Round   3, Train loss: 1.630, Test loss: 1.699, Test accuracy: 77.13
Round   3, Global train loss: 1.630, Global test loss: 1.626, Global test accuracy: 84.07
Round   4, Train loss: 1.617, Test loss: 1.668, Test accuracy: 80.28
Round   4, Global train loss: 1.617, Global test loss: 1.620, Global test accuracy: 84.60
Round   5, Train loss: 1.609, Test loss: 1.634, Test accuracy: 83.52
Round   5, Global train loss: 1.609, Global test loss: 1.615, Global test accuracy: 84.92
Round   6, Train loss: 1.606, Test loss: 1.628, Test accuracy: 83.93
Round   6, Global train loss: 1.606, Global test loss: 1.611, Global test accuracy: 85.25
Round   7, Train loss: 1.598, Test loss: 1.625, Test accuracy: 84.18
Round   7, Global train loss: 1.598, Global test loss: 1.608, Global test accuracy: 85.54
Round   8, Train loss: 1.579, Test loss: 1.602, Test accuracy: 86.59
Round   8, Global train loss: 1.579, Global test loss: 1.558, Global test accuracy: 91.12
Round   9, Train loss: 1.532, Test loss: 1.586, Test accuracy: 88.10
Round   9, Global train loss: 1.532, Global test loss: 1.542, Global test accuracy: 92.52
Round  10, Train loss: 1.524, Test loss: 1.571, Test accuracy: 89.51
Round  10, Global train loss: 1.524, Global test loss: 1.536, Global test accuracy: 93.01
Round  11, Train loss: 1.516, Test loss: 1.560, Test accuracy: 90.63
Round  11, Global train loss: 1.516, Global test loss: 1.530, Global test accuracy: 93.52
Round  12, Train loss: 1.509, Test loss: 1.552, Test accuracy: 91.33
Round  12, Global train loss: 1.509, Global test loss: 1.526, Global test accuracy: 93.91
Round  13, Train loss: 1.510, Test loss: 1.541, Test accuracy: 92.48
Round  13, Global train loss: 1.510, Global test loss: 1.523, Global test accuracy: 94.31
Round  14, Train loss: 1.500, Test loss: 1.539, Test accuracy: 92.64
Round  14, Global train loss: 1.500, Global test loss: 1.521, Global test accuracy: 94.33
Round  15, Train loss: 1.504, Test loss: 1.535, Test accuracy: 92.96
Round  15, Global train loss: 1.504, Global test loss: 1.518, Global test accuracy: 94.75
Round  16, Train loss: 1.500, Test loss: 1.527, Test accuracy: 93.75
Round  16, Global train loss: 1.500, Global test loss: 1.516, Global test accuracy: 94.86
Round  17, Train loss: 1.497, Test loss: 1.524, Test accuracy: 94.05
Round  17, Global train loss: 1.497, Global test loss: 1.514, Global test accuracy: 95.03
Round  18, Train loss: 1.492, Test loss: 1.523, Test accuracy: 94.14
Round  18, Global train loss: 1.492, Global test loss: 1.514, Global test accuracy: 95.04
Round  19, Train loss: 1.495, Test loss: 1.521, Test accuracy: 94.25
Round  19, Global train loss: 1.495, Global test loss: 1.512, Global test accuracy: 95.21
Round  20, Train loss: 1.491, Test loss: 1.519, Test accuracy: 94.53
Round  20, Global train loss: 1.491, Global test loss: 1.510, Global test accuracy: 95.33
Round  21, Train loss: 1.490, Test loss: 1.517, Test accuracy: 94.73
Round  21, Global train loss: 1.490, Global test loss: 1.509, Global test accuracy: 95.45
Round  22, Train loss: 1.489, Test loss: 1.515, Test accuracy: 94.90
Round  22, Global train loss: 1.489, Global test loss: 1.509, Global test accuracy: 95.39
Round  23, Train loss: 1.487, Test loss: 1.514, Test accuracy: 95.00
Round  23, Global train loss: 1.487, Global test loss: 1.507, Global test accuracy: 95.69
Round  24, Train loss: 1.485, Test loss: 1.513, Test accuracy: 95.11
Round  24, Global train loss: 1.485, Global test loss: 1.507, Global test accuracy: 95.63
Round  25, Train loss: 1.484, Test loss: 1.512, Test accuracy: 95.22
Round  25, Global train loss: 1.484, Global test loss: 1.506, Global test accuracy: 95.74
Round  26, Train loss: 1.484, Test loss: 1.511, Test accuracy: 95.26
Round  26, Global train loss: 1.484, Global test loss: 1.505, Global test accuracy: 95.87
Round  27, Train loss: 1.483, Test loss: 1.510, Test accuracy: 95.32
Round  27, Global train loss: 1.483, Global test loss: 1.506, Global test accuracy: 95.77
Round  28, Train loss: 1.483, Test loss: 1.509, Test accuracy: 95.42
Round  28, Global train loss: 1.483, Global test loss: 1.504, Global test accuracy: 95.95
Round  29, Train loss: 1.481, Test loss: 1.508, Test accuracy: 95.48
Round  29, Global train loss: 1.481, Global test loss: 1.503, Global test accuracy: 96.01
Round  30, Train loss: 1.480, Test loss: 1.508, Test accuracy: 95.57
Round  30, Global train loss: 1.480, Global test loss: 1.502, Global test accuracy: 96.16
Round  31, Train loss: 1.480, Test loss: 1.507, Test accuracy: 95.65
Round  31, Global train loss: 1.480, Global test loss: 1.502, Global test accuracy: 96.15
Round  32, Train loss: 1.479, Test loss: 1.506, Test accuracy: 95.72
Round  32, Global train loss: 1.479, Global test loss: 1.501, Global test accuracy: 96.26
Round  33, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.83
Round  33, Global train loss: 1.479, Global test loss: 1.501, Global test accuracy: 96.23
Round  34, Train loss: 1.482, Test loss: 1.505, Test accuracy: 95.91
Round  34, Global train loss: 1.482, Global test loss: 1.501, Global test accuracy: 96.21
Round  35, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.88
Round  35, Global train loss: 1.479, Global test loss: 1.501, Global test accuracy: 96.16
Round  36, Train loss: 1.478, Test loss: 1.504, Test accuracy: 95.94
Round  36, Global train loss: 1.478, Global test loss: 1.500, Global test accuracy: 96.28
Round  37, Train loss: 1.477, Test loss: 1.503, Test accuracy: 96.02
Round  37, Global train loss: 1.477, Global test loss: 1.499, Global test accuracy: 96.42
Round  38, Train loss: 1.475, Test loss: 1.503, Test accuracy: 96.05
Round  38, Global train loss: 1.475, Global test loss: 1.499, Global test accuracy: 96.42
Round  39, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.12
Round  39, Global train loss: 1.475, Global test loss: 1.499, Global test accuracy: 96.42
Round  40, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.19
Round  40, Global train loss: 1.475, Global test loss: 1.498, Global test accuracy: 96.50
Round  41, Train loss: 1.476, Test loss: 1.501, Test accuracy: 96.30
Round  41, Global train loss: 1.476, Global test loss: 1.498, Global test accuracy: 96.65
Round  42, Train loss: 1.477, Test loss: 1.500, Test accuracy: 96.32
Round  42, Global train loss: 1.477, Global test loss: 1.497, Global test accuracy: 96.58
Round  43, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.35
Round  43, Global train loss: 1.474, Global test loss: 1.497, Global test accuracy: 96.66
Round  44, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.39
Round  44, Global train loss: 1.473, Global test loss: 1.497, Global test accuracy: 96.75
Round  45, Train loss: 1.475, Test loss: 1.500, Test accuracy: 96.39
Round  45, Global train loss: 1.475, Global test loss: 1.496, Global test accuracy: 96.67
Round  46, Train loss: 1.475, Test loss: 1.500, Test accuracy: 96.39
Round  46, Global train loss: 1.475, Global test loss: 1.496, Global test accuracy: 96.75
Round  47, Train loss: 1.475, Test loss: 1.499, Test accuracy: 96.41
Round  47, Global train loss: 1.475, Global test loss: 1.496, Global test accuracy: 96.80
Round  48, Train loss: 1.474, Test loss: 1.499, Test accuracy: 96.42
Round  48, Global train loss: 1.474, Global test loss: 1.496, Global test accuracy: 96.76
Round  49, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.45
Round  49, Global train loss: 1.472, Global test loss: 1.496, Global test accuracy: 96.75
Round  50, Train loss: 1.474, Test loss: 1.498, Test accuracy: 96.56
Round  50, Global train loss: 1.474, Global test loss: 1.495, Global test accuracy: 96.91
Round  51, Train loss: 1.472, Test loss: 1.498, Test accuracy: 96.57
Round  51, Global train loss: 1.472, Global test loss: 1.495, Global test accuracy: 96.89
Round  52, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.64
Round  52, Global train loss: 1.471, Global test loss: 1.495, Global test accuracy: 96.87
Round  53, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.62
Round  53, Global train loss: 1.472, Global test loss: 1.495, Global test accuracy: 96.88
Round  54, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.65
Round  54, Global train loss: 1.471, Global test loss: 1.495, Global test accuracy: 96.91
Round  55, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.65
Round  55, Global train loss: 1.472, Global test loss: 1.495, Global test accuracy: 96.88
Round  56, Train loss: 1.473, Test loss: 1.497, Test accuracy: 96.70
Round  56, Global train loss: 1.473, Global test loss: 1.495, Global test accuracy: 96.89
Round  57, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.69
Round  57, Global train loss: 1.472, Global test loss: 1.494, Global test accuracy: 97.02
Round  58, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.68
Round  58, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.91
Round  59, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.72
Round  59, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.97
Round  60, Train loss: 1.471, Test loss: 1.496, Test accuracy: 96.74
Round  60, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 97.08
Round  61, Train loss: 1.471, Test loss: 1.496, Test accuracy: 96.75
Round  61, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.95
Round  62, Train loss: 1.471, Test loss: 1.496, Test accuracy: 96.77
Round  62, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.97
Round  63, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.77
Round  63, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 97.00
Round  64, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.80
Round  64, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 97.02
Round  65, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.87
Round  65, Global train loss: 1.471, Global test loss: 1.493, Global test accuracy: 97.05
Round  66, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.87
Round  66, Global train loss: 1.472, Global test loss: 1.493, Global test accuracy: 96.97
Round  67, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.83
Round  67, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 97.00
Round  68, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.83
Round  68, Global train loss: 1.471, Global test loss: 1.493, Global test accuracy: 96.99
Round  69, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.84
Round  69, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 96.97
Round  70, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.85
Round  70, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 96.98
Round  71, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.86
Round  71, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 96.92
Round  72, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.88
Round  72, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 96.98
Round  73, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.87
Round  73, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 96.97
Round  74, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.89
Round  74, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 97.02
Round  75, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.90
Round  75, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 97.04
Round  76, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.92
Round  76, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 97.06
Round  77, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.94
Round  77, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 97.05
Round  78, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.97
Round  78, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.02
Round  79, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.97
Round  79, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 97.01
Round  80, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.97
Round  80, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.03
Round  81, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.97
Round  81, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 97.01
Round  82, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.97
Round  82, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.11
Round  83, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.97
Round  83, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.07
Round  84, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.01
Round  84, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.06
Round  85, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.00
Round  85, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.02
Round  86, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.01
Round  86, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.01
Round  87, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.01
Round  87, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.06
Round  88, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.99
Round  88, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.95
Round  89, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.98
Round  89, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.92
Round  90, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.98
Round  90, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.06
Round  91, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.97
Round  91, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.08
Round  92, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.00
Round  92, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.10
Round  93, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.03
Round  93, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.15
Round  94, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.04
Round  94, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.12
Round  95, Train loss: 1.467, Test loss: 1.493, Test accuracy: 97.04
Round  95, Global train loss: 1.467, Global test loss: 1.492, Global test accuracy: 97.06/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.03
Round  96, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.11
Round  97, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.05
Round  97, Global train loss: 1.468, Global test loss: 1.491, Global test accuracy: 97.05
Round  98, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.06
Round  98, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.03
Round  99, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.07
Round  99, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.15
Final Round, Train loss: 1.467, Test loss: 1.493, Test accuracy: 97.01
Final Round, Global train loss: 1.467, Global test loss: 1.492, Global test accuracy: 97.15
Average accuracy final 10 rounds: 97.02575 

Average global accuracy final 10 rounds: 97.08975 

5017.805799484253
[4.039471626281738, 8.078943252563477, 11.59777283668518, 15.116602420806885, 18.502187490463257, 21.88777256011963, 25.081029653549194, 28.27428674697876, 31.67644429206848, 35.0786018371582, 38.44091272354126, 41.803223609924316, 45.15665078163147, 48.51007795333862, 52.035231590270996, 55.56038522720337, 59.08343291282654, 62.60648059844971, 66.5643584728241, 70.52223634719849, 74.17868232727051, 77.83512830734253, 81.4588029384613, 85.08247756958008, 88.72721409797668, 92.37195062637329, 95.9835114479065, 99.5950722694397, 102.9259192943573, 106.2567663192749, 109.91867256164551, 113.58057880401611, 117.53958797454834, 121.49859714508057, 124.94019198417664, 128.3817868232727, 131.80055260658264, 135.21931838989258, 138.39442801475525, 141.56953763961792, 144.82019209861755, 148.0708465576172, 151.21695852279663, 154.36307048797607, 157.54370665550232, 160.72434282302856, 163.92570281028748, 167.1270627975464, 170.1383764743805, 173.1496901512146, 176.35082244873047, 179.55195474624634, 182.8028905391693, 186.05382633209229, 189.28209924697876, 192.51037216186523, 195.42117285728455, 198.33197355270386, 201.05159258842468, 203.7712116241455, 206.65269351005554, 209.53417539596558, 212.36059713363647, 215.18701887130737, 218.11015844345093, 221.03329801559448, 224.20009803771973, 227.36689805984497, 230.4445765018463, 233.52225494384766, 236.5741777420044, 239.62610054016113, 242.69401144981384, 245.76192235946655, 248.85723066329956, 251.95253896713257, 255.080801486969, 258.2090640068054, 261.37239503860474, 264.53572607040405, 267.6455202102661, 270.7553143501282, 273.89477944374084, 277.0342445373535, 280.0944814682007, 283.15471839904785, 286.27476835250854, 289.39481830596924, 292.59385538101196, 295.7928924560547, 298.92944264411926, 302.06599283218384, 305.17652773857117, 308.2870626449585, 311.4704315662384, 314.6538004875183, 317.78729271888733, 320.92078495025635, 324.09489011764526, 327.2689952850342, 330.3385679721832, 333.4081406593323, 336.7864394187927, 340.1647381782532, 343.610586643219, 347.0564351081848, 350.4709298610687, 353.88542461395264, 357.30874466896057, 360.7320647239685, 364.0807671546936, 367.4294695854187, 370.76697611808777, 374.10448265075684, 377.523738861084, 380.94299507141113, 384.3271586894989, 387.71132230758667, 390.84014320373535, 393.96896409988403, 397.03988575935364, 400.11080741882324, 403.16443729400635, 406.21806716918945, 409.30384969711304, 412.3896322250366, 415.53169798851013, 418.67376375198364, 421.8047273159027, 424.9356908798218, 427.9681694507599, 431.000648021698, 434.12103819847107, 437.24142837524414, 440.69896268844604, 444.15649700164795, 447.5598928928375, 450.9632887840271, 454.14801001548767, 457.33273124694824, 460.43892669677734, 463.54512214660645, 466.67339038848877, 469.8016586303711, 472.8911440372467, 475.9806294441223, 479.1156187057495, 482.2506079673767, 485.45034074783325, 488.6500735282898, 491.76272463798523, 494.87537574768066, 497.98171162605286, 501.08804750442505, 504.15985083580017, 507.2316541671753, 510.26511001586914, 513.298565864563, 516.396744966507, 519.4949240684509, 522.6659643650055, 525.8370046615601, 528.9425451755524, 532.0480856895447, 535.201464176178, 538.3548426628113, 541.4155955314636, 544.476348400116, 547.696713924408, 550.9170794487, 553.9804770946503, 557.0438747406006, 560.1339485645294, 563.2240223884583, 566.5771913528442, 569.9303603172302, 572.9930930137634, 576.0558257102966, 579.1596503257751, 582.2634749412537, 585.2977468967438, 588.3320188522339, 591.5504264831543, 594.7688341140747, 597.9363257884979, 601.1038174629211, 604.2537839412689, 607.4037504196167, 610.81738114357, 614.2310118675232, 617.4995563030243, 620.7681007385254, 623.8567504882812, 626.9454002380371, 630.0206046104431, 633.0958089828491, 635.9156022071838, 638.7353954315186, 642.018084526062, 645.3007736206055, 647.0158877372742, 648.7310018539429]
[31.06, 31.06, 70.4875, 70.4875, 71.675, 71.675, 77.13, 77.13, 80.2825, 80.2825, 83.515, 83.515, 83.9275, 83.9275, 84.1825, 84.1825, 86.595, 86.595, 88.1025, 88.1025, 89.51, 89.51, 90.6325, 90.6325, 91.3325, 91.3325, 92.4775, 92.4775, 92.6375, 92.6375, 92.9575, 92.9575, 93.7475, 93.7475, 94.05, 94.05, 94.14, 94.14, 94.255, 94.255, 94.53, 94.53, 94.7275, 94.7275, 94.9025, 94.9025, 94.995, 94.995, 95.105, 95.105, 95.22, 95.22, 95.2625, 95.2625, 95.32, 95.32, 95.4225, 95.4225, 95.48, 95.48, 95.57, 95.57, 95.6475, 95.6475, 95.7225, 95.7225, 95.8275, 95.8275, 95.91, 95.91, 95.875, 95.875, 95.9375, 95.9375, 96.0225, 96.0225, 96.0525, 96.0525, 96.12, 96.12, 96.1925, 96.1925, 96.3025, 96.3025, 96.32, 96.32, 96.35, 96.35, 96.385, 96.385, 96.395, 96.395, 96.3925, 96.3925, 96.405, 96.405, 96.42, 96.42, 96.455, 96.455, 96.565, 96.565, 96.5675, 96.5675, 96.64, 96.64, 96.625, 96.625, 96.65, 96.65, 96.6525, 96.6525, 96.705, 96.705, 96.685, 96.685, 96.68, 96.68, 96.715, 96.715, 96.7425, 96.7425, 96.7525, 96.7525, 96.7675, 96.7675, 96.765, 96.765, 96.8, 96.8, 96.8725, 96.8725, 96.87, 96.87, 96.835, 96.835, 96.8325, 96.8325, 96.84, 96.84, 96.8475, 96.8475, 96.86, 96.86, 96.875, 96.875, 96.8675, 96.8675, 96.8875, 96.8875, 96.8975, 96.8975, 96.92, 96.92, 96.9375, 96.9375, 96.965, 96.965, 96.965, 96.965, 96.9675, 96.9675, 96.9725, 96.9725, 96.9725, 96.9725, 96.97, 96.97, 97.0125, 97.0125, 97.005, 97.005, 97.0125, 97.0125, 97.0125, 97.0125, 96.9925, 96.9925, 96.9825, 96.9825, 96.9775, 96.9775, 96.965, 96.965, 97.0, 97.0, 97.0275, 97.0275, 97.0375, 97.0375, 97.0375, 97.0375, 97.0325, 97.0325, 97.0525, 97.0525, 97.055, 97.055, 97.0725, 97.0725, 97.0075, 97.0075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.293, Test accuracy: 28.23
Round   1, Train loss: 2.282, Test loss: 2.235, Test accuracy: 52.86
Round   2, Train loss: 2.066, Test loss: 1.867, Test accuracy: 67.82
Round   3, Train loss: 1.749, Test loss: 1.716, Test accuracy: 78.02
Round   4, Train loss: 1.669, Test loss: 1.671, Test accuracy: 80.98
Round   5, Train loss: 1.646, Test loss: 1.643, Test accuracy: 82.93
Round   6, Train loss: 1.634, Test loss: 1.635, Test accuracy: 83.32
Round   7, Train loss: 1.627, Test loss: 1.630, Test accuracy: 83.70
Round   8, Train loss: 1.618, Test loss: 1.627, Test accuracy: 84.03
Round   9, Train loss: 1.613, Test loss: 1.625, Test accuracy: 84.06
Round  10, Train loss: 1.612, Test loss: 1.623, Test accuracy: 84.28
Round  11, Train loss: 1.609, Test loss: 1.620, Test accuracy: 84.48
Round  12, Train loss: 1.607, Test loss: 1.618, Test accuracy: 84.69
Round  13, Train loss: 1.602, Test loss: 1.616, Test accuracy: 84.93
Round  14, Train loss: 1.600, Test loss: 1.615, Test accuracy: 85.06
Round  15, Train loss: 1.601, Test loss: 1.608, Test accuracy: 85.69
Round  16, Train loss: 1.590, Test loss: 1.605, Test accuracy: 85.91
Round  17, Train loss: 1.592, Test loss: 1.604, Test accuracy: 86.03
Round  18, Train loss: 1.593, Test loss: 1.603, Test accuracy: 86.19
Round  19, Train loss: 1.589, Test loss: 1.602, Test accuracy: 86.25
Round  20, Train loss: 1.591, Test loss: 1.601, Test accuracy: 86.45
Round  21, Train loss: 1.594, Test loss: 1.599, Test accuracy: 86.44
Round  22, Train loss: 1.585, Test loss: 1.599, Test accuracy: 86.49
Round  23, Train loss: 1.587, Test loss: 1.598, Test accuracy: 86.58
Round  24, Train loss: 1.572, Test loss: 1.595, Test accuracy: 86.86
Round  25, Train loss: 1.566, Test loss: 1.595, Test accuracy: 86.86
Round  26, Train loss: 1.580, Test loss: 1.594, Test accuracy: 86.94
Round  27, Train loss: 1.582, Test loss: 1.594, Test accuracy: 86.97
Round  28, Train loss: 1.570, Test loss: 1.593, Test accuracy: 86.98
Round  29, Train loss: 1.582, Test loss: 1.593, Test accuracy: 87.01
Round  30, Train loss: 1.571, Test loss: 1.592, Test accuracy: 87.09
Round  31, Train loss: 1.578, Test loss: 1.591, Test accuracy: 87.22
Round  32, Train loss: 1.580, Test loss: 1.591, Test accuracy: 87.19
Round  33, Train loss: 1.570, Test loss: 1.588, Test accuracy: 87.43
Round  34, Train loss: 1.568, Test loss: 1.585, Test accuracy: 87.76
Round  35, Train loss: 1.556, Test loss: 1.585, Test accuracy: 87.75
Round  36, Train loss: 1.546, Test loss: 1.581, Test accuracy: 88.09
Round  37, Train loss: 1.557, Test loss: 1.581, Test accuracy: 88.14
Round  38, Train loss: 1.576, Test loss: 1.581, Test accuracy: 88.19
Round  39, Train loss: 1.564, Test loss: 1.580, Test accuracy: 88.24
Round  40, Train loss: 1.576, Test loss: 1.580, Test accuracy: 88.22
Round  41, Train loss: 1.555, Test loss: 1.576, Test accuracy: 88.67
Round  42, Train loss: 1.550, Test loss: 1.575, Test accuracy: 88.77
Round  43, Train loss: 1.552, Test loss: 1.574, Test accuracy: 88.83
Round  44, Train loss: 1.550, Test loss: 1.574, Test accuracy: 88.80
Round  45, Train loss: 1.549, Test loss: 1.574, Test accuracy: 88.86
Round  46, Train loss: 1.536, Test loss: 1.573, Test accuracy: 88.95
Round  47, Train loss: 1.572, Test loss: 1.573, Test accuracy: 88.99
Round  48, Train loss: 1.550, Test loss: 1.570, Test accuracy: 89.34
Round  49, Train loss: 1.546, Test loss: 1.569, Test accuracy: 89.31
Round  50, Train loss: 1.546, Test loss: 1.569, Test accuracy: 89.34
Round  51, Train loss: 1.546, Test loss: 1.565, Test accuracy: 89.75
Round  52, Train loss: 1.541, Test loss: 1.563, Test accuracy: 90.05
Round  53, Train loss: 1.534, Test loss: 1.558, Test accuracy: 90.48
Round  54, Train loss: 1.533, Test loss: 1.556, Test accuracy: 90.80
Round  55, Train loss: 1.504, Test loss: 1.543, Test accuracy: 92.07
Round  56, Train loss: 1.531, Test loss: 1.542, Test accuracy: 92.17
Round  57, Train loss: 1.529, Test loss: 1.542, Test accuracy: 92.15
Round  58, Train loss: 1.519, Test loss: 1.541, Test accuracy: 92.25
Round  59, Train loss: 1.501, Test loss: 1.541, Test accuracy: 92.28
Round  60, Train loss: 1.516, Test loss: 1.540, Test accuracy: 92.39
Round  61, Train loss: 1.514, Test loss: 1.539, Test accuracy: 92.36
Round  62, Train loss: 1.508, Test loss: 1.537, Test accuracy: 92.66
Round  63, Train loss: 1.511, Test loss: 1.528, Test accuracy: 93.55
Round  64, Train loss: 1.506, Test loss: 1.528, Test accuracy: 93.58
Round  65, Train loss: 1.501, Test loss: 1.528, Test accuracy: 93.72
Round  66, Train loss: 1.487, Test loss: 1.525, Test accuracy: 93.96
Round  67, Train loss: 1.486, Test loss: 1.522, Test accuracy: 94.17
Round  68, Train loss: 1.489, Test loss: 1.516, Test accuracy: 94.77
Round  69, Train loss: 1.487, Test loss: 1.513, Test accuracy: 95.02
Round  70, Train loss: 1.484, Test loss: 1.513, Test accuracy: 95.07
Round  71, Train loss: 1.484, Test loss: 1.513, Test accuracy: 95.06
Round  72, Train loss: 1.480, Test loss: 1.513, Test accuracy: 95.08
Round  73, Train loss: 1.485, Test loss: 1.513, Test accuracy: 95.09
Round  74, Train loss: 1.482, Test loss: 1.513, Test accuracy: 95.05
Round  75, Train loss: 1.483, Test loss: 1.508, Test accuracy: 95.53
Round  76, Train loss: 1.484, Test loss: 1.506, Test accuracy: 95.75
Round  77, Train loss: 1.481, Test loss: 1.505, Test accuracy: 95.80
Round  78, Train loss: 1.481, Test loss: 1.505, Test accuracy: 95.84
Round  79, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.84
Round  80, Train loss: 1.482, Test loss: 1.505, Test accuracy: 95.83
Round  81, Train loss: 1.480, Test loss: 1.505, Test accuracy: 95.84
Round  82, Train loss: 1.480, Test loss: 1.504, Test accuracy: 95.95
Round  83, Train loss: 1.480, Test loss: 1.504, Test accuracy: 95.93
Round  84, Train loss: 1.478, Test loss: 1.504, Test accuracy: 96.00
Round  85, Train loss: 1.479, Test loss: 1.504, Test accuracy: 96.02
Round  86, Train loss: 1.479, Test loss: 1.503, Test accuracy: 96.05
Round  87, Train loss: 1.477, Test loss: 1.503, Test accuracy: 96.08
Round  88, Train loss: 1.479, Test loss: 1.503, Test accuracy: 96.06
Round  89, Train loss: 1.476, Test loss: 1.503, Test accuracy: 96.05
Round  90, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.07
Round  91, Train loss: 1.477, Test loss: 1.502, Test accuracy: 96.09
Round  92, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.12
Round  93, Train loss: 1.478, Test loss: 1.502, Test accuracy: 96.11
Round  94, Train loss: 1.477, Test loss: 1.502, Test accuracy: 96.17
Round  95, Train loss: 1.477, Test loss: 1.502, Test accuracy: 96.13
Round  96, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.13
Round  97, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.14
Round  98, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.19/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.22
Final Round, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.22
Average accuracy final 10 rounds: 96.13725 

4151.85293841362
[3.1042380332946777, 6.2084760665893555, 9.349327564239502, 12.490179061889648, 15.623443365097046, 18.756707668304443, 22.07085084915161, 25.38499402999878, 28.77542996406555, 32.165865898132324, 35.22525119781494, 38.28463649749756, 41.37298083305359, 44.46132516860962, 47.49956202507019, 50.53779888153076, 53.37718915939331, 56.21657943725586, 59.14655947685242, 62.076539516448975, 65.34645366668701, 68.61636781692505, 71.5545003414154, 74.49263286590576, 77.31330442428589, 80.13397598266602, 83.2376537322998, 86.3413314819336, 89.45941996574402, 92.57750844955444, 95.48842525482178, 98.39934206008911, 101.2754533290863, 104.1515645980835, 107.119473695755, 110.08738279342651, 112.95307731628418, 115.81877183914185, 118.74939703941345, 121.68002223968506, 124.72419714927673, 127.76837205886841, 130.62505531311035, 133.4817385673523, 136.31843423843384, 139.15512990951538, 141.96765542030334, 144.7801809310913, 147.62422561645508, 150.46827030181885, 153.51809215545654, 156.56791400909424, 159.44025778770447, 162.3126015663147, 165.14459657669067, 167.97659158706665, 170.9715461730957, 173.96650075912476, 176.810142993927, 179.65378522872925, 182.5426471233368, 185.43150901794434, 188.49578547477722, 191.5600619316101, 194.44353461265564, 197.32700729370117, 200.26075387001038, 203.19450044631958, 206.2203619480133, 209.24622344970703, 212.2079451084137, 215.16966676712036, 218.20435643196106, 221.23904609680176, 224.02401638031006, 226.80898666381836, 229.62987232208252, 232.45075798034668, 235.3981328010559, 238.34550762176514, 241.2088804244995, 244.0722532272339, 247.05392956733704, 250.03560590744019, 252.95128631591797, 255.86696672439575, 258.76720118522644, 261.66743564605713, 264.75261330604553, 267.83779096603394, 270.86401772499084, 273.89024448394775, 276.7353210449219, 279.580397605896, 282.77487349510193, 285.96934938430786, 289.2393310070038, 292.5093126296997, 295.6472237110138, 298.7851347923279, 302.1609539985657, 305.53677320480347, 308.7219145298004, 311.90705585479736, 315.19189047813416, 318.47672510147095, 322.18639183044434, 325.8960585594177, 329.29644751548767, 332.6968364715576, 335.79886174201965, 338.9008870124817, 342.04723358154297, 345.19358015060425, 348.89017510414124, 352.5867700576782, 355.9421067237854, 359.2974433898926, 362.4965789318085, 365.69571447372437, 368.83918595314026, 371.98265743255615, 375.31040716171265, 378.63815689086914, 381.8152160644531, 384.9922752380371, 388.278400182724, 391.5645251274109, 395.193039894104, 398.8215546607971, 401.87005376815796, 404.9185528755188, 408.11958837509155, 411.3206238746643, 414.5484776496887, 417.77633142471313, 421.02941703796387, 424.2825026512146, 427.5134584903717, 430.7444143295288, 434.0519645214081, 437.35951471328735, 440.47885298728943, 443.5981912612915, 447.0236167907715, 450.44904232025146, 453.9180746078491, 457.3871068954468, 460.7611355781555, 464.13516426086426, 467.4810588359833, 470.8269534111023, 474.4649169445038, 478.1028804779053, 481.5798308849335, 485.05678129196167, 488.39726972579956, 491.73775815963745, 495.190452337265, 498.6431465148926, 501.727942943573, 504.8127393722534, 508.10736107826233, 511.40198278427124, 514.7874002456665, 518.1728177070618, 521.5583703517914, 524.943922996521, 528.3904986381531, 531.8370742797852, 535.0580582618713, 538.2790422439575, 541.5934369564056, 544.9078316688538, 548.130619764328, 551.3534078598022, 554.6345436573029, 557.9156794548035, 561.1633565425873, 564.4110336303711, 567.6323547363281, 570.8536758422852, 574.363698720932, 577.8737215995789, 581.3926067352295, 584.9114918708801, 588.1121609210968, 591.3128299713135, 594.56613945961, 597.8194489479065, 601.080518245697, 604.3415875434875, 607.5096232891083, 610.677659034729, 613.9250936508179, 617.1725282669067, 620.6301651000977, 624.0878019332886, 627.2337212562561, 630.3796405792236, 631.9146206378937, 633.4496006965637]
[28.235, 28.235, 52.8575, 52.8575, 67.82, 67.82, 78.02, 78.02, 80.9775, 80.9775, 82.9275, 82.9275, 83.3175, 83.3175, 83.7, 83.7, 84.035, 84.035, 84.065, 84.065, 84.285, 84.285, 84.48, 84.48, 84.685, 84.685, 84.93, 84.93, 85.06, 85.06, 85.685, 85.685, 85.9075, 85.9075, 86.0275, 86.0275, 86.195, 86.195, 86.2525, 86.2525, 86.4475, 86.4475, 86.435, 86.435, 86.49, 86.49, 86.585, 86.585, 86.86, 86.86, 86.855, 86.855, 86.9425, 86.9425, 86.975, 86.975, 86.9825, 86.9825, 87.01, 87.01, 87.0875, 87.0875, 87.215, 87.215, 87.1875, 87.1875, 87.4275, 87.4275, 87.7625, 87.7625, 87.75, 87.75, 88.09, 88.09, 88.145, 88.145, 88.185, 88.185, 88.2425, 88.2425, 88.2225, 88.2225, 88.67, 88.67, 88.7725, 88.7725, 88.8325, 88.8325, 88.8, 88.8, 88.865, 88.865, 88.9475, 88.9475, 88.9925, 88.9925, 89.34, 89.34, 89.3125, 89.3125, 89.345, 89.345, 89.7475, 89.7475, 90.0475, 90.0475, 90.48, 90.48, 90.8025, 90.8025, 92.0675, 92.0675, 92.165, 92.165, 92.1525, 92.1525, 92.25, 92.25, 92.285, 92.285, 92.3875, 92.3875, 92.36, 92.36, 92.6625, 92.6625, 93.5525, 93.5525, 93.585, 93.585, 93.725, 93.725, 93.9575, 93.9575, 94.175, 94.175, 94.765, 94.765, 95.0225, 95.0225, 95.0725, 95.0725, 95.0625, 95.0625, 95.085, 95.085, 95.095, 95.095, 95.0525, 95.0525, 95.5275, 95.5275, 95.75, 95.75, 95.8, 95.8, 95.8375, 95.8375, 95.8425, 95.8425, 95.83, 95.83, 95.8425, 95.8425, 95.955, 95.955, 95.9325, 95.9325, 96.0, 96.0, 96.015, 96.015, 96.0475, 96.0475, 96.0775, 96.0775, 96.055, 96.055, 96.0525, 96.0525, 96.0725, 96.0725, 96.09, 96.09, 96.125, 96.125, 96.105, 96.105, 96.17, 96.17, 96.1275, 96.1275, 96.1325, 96.1325, 96.1375, 96.1375, 96.1925, 96.1925, 96.22, 96.22, 96.22, 96.22]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.298, Test accuracy: 25.76
Round   1, Train loss: 2.294, Test loss: 2.289, Test accuracy: 39.89
Round   2, Train loss: 2.271, Test loss: 2.240, Test accuracy: 36.57
Round   3, Train loss: 2.151, Test loss: 2.076, Test accuracy: 51.33
Round   4, Train loss: 1.945, Test loss: 1.884, Test accuracy: 65.84
Round   5, Train loss: 1.793, Test loss: 1.751, Test accuracy: 75.72
Round   6, Train loss: 1.708, Test loss: 1.698, Test accuracy: 79.43
Round   7, Train loss: 1.672, Test loss: 1.678, Test accuracy: 80.67
Round   8, Train loss: 1.645, Test loss: 1.662, Test accuracy: 81.77
Round   9, Train loss: 1.628, Test loss: 1.655, Test accuracy: 82.10
Round  10, Train loss: 1.631, Test loss: 1.645, Test accuracy: 82.77
Round  11, Train loss: 1.622, Test loss: 1.640, Test accuracy: 82.97
Round  12, Train loss: 1.633, Test loss: 1.635, Test accuracy: 83.39
Round  13, Train loss: 1.615, Test loss: 1.632, Test accuracy: 83.54
Round  14, Train loss: 1.609, Test loss: 1.630, Test accuracy: 83.81
Round  15, Train loss: 1.612, Test loss: 1.628, Test accuracy: 84.12
Round  16, Train loss: 1.601, Test loss: 1.627, Test accuracy: 84.05
Round  17, Train loss: 1.614, Test loss: 1.622, Test accuracy: 84.39
Round  18, Train loss: 1.611, Test loss: 1.621, Test accuracy: 84.48
Round  19, Train loss: 1.595, Test loss: 1.619, Test accuracy: 84.54
Round  20, Train loss: 1.604, Test loss: 1.618, Test accuracy: 84.61
Round  21, Train loss: 1.598, Test loss: 1.618, Test accuracy: 84.60
Round  22, Train loss: 1.601, Test loss: 1.618, Test accuracy: 84.65
Round  23, Train loss: 1.607, Test loss: 1.617, Test accuracy: 84.67
Round  24, Train loss: 1.592, Test loss: 1.616, Test accuracy: 84.78
Round  25, Train loss: 1.598, Test loss: 1.615, Test accuracy: 84.88
Round  26, Train loss: 1.595, Test loss: 1.615, Test accuracy: 84.87
Round  27, Train loss: 1.592, Test loss: 1.615, Test accuracy: 84.97
Round  28, Train loss: 1.591, Test loss: 1.615, Test accuracy: 84.98
Round  29, Train loss: 1.594, Test loss: 1.615, Test accuracy: 84.97
Round  30, Train loss: 1.599, Test loss: 1.614, Test accuracy: 85.08
Round  31, Train loss: 1.586, Test loss: 1.613, Test accuracy: 85.05
Round  32, Train loss: 1.594, Test loss: 1.614, Test accuracy: 84.94
Round  33, Train loss: 1.578, Test loss: 1.613, Test accuracy: 85.02
Round  34, Train loss: 1.585, Test loss: 1.613, Test accuracy: 85.08
Round  35, Train loss: 1.582, Test loss: 1.613, Test accuracy: 85.02
Round  36, Train loss: 1.588, Test loss: 1.612, Test accuracy: 85.12
Round  37, Train loss: 1.586, Test loss: 1.611, Test accuracy: 85.20
Round  38, Train loss: 1.584, Test loss: 1.611, Test accuracy: 85.15
Round  39, Train loss: 1.585, Test loss: 1.612, Test accuracy: 85.12
Round  40, Train loss: 1.582, Test loss: 1.612, Test accuracy: 85.10
Round  41, Train loss: 1.582, Test loss: 1.610, Test accuracy: 85.24
Round  42, Train loss: 1.576, Test loss: 1.611, Test accuracy: 85.10
Round  43, Train loss: 1.577, Test loss: 1.611, Test accuracy: 85.18
Round  44, Train loss: 1.586, Test loss: 1.611, Test accuracy: 85.22
Round  45, Train loss: 1.573, Test loss: 1.611, Test accuracy: 85.08
Round  46, Train loss: 1.588, Test loss: 1.610, Test accuracy: 85.27
Round  47, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.26
Round  48, Train loss: 1.584, Test loss: 1.610, Test accuracy: 85.32
Round  49, Train loss: 1.578, Test loss: 1.610, Test accuracy: 85.33
Round  50, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.36
Round  51, Train loss: 1.571, Test loss: 1.609, Test accuracy: 85.32
Round  52, Train loss: 1.570, Test loss: 1.609, Test accuracy: 85.27
Round  53, Train loss: 1.583, Test loss: 1.609, Test accuracy: 85.38
Round  54, Train loss: 1.578, Test loss: 1.609, Test accuracy: 85.34
Round  55, Train loss: 1.579, Test loss: 1.609, Test accuracy: 85.37
Round  56, Train loss: 1.570, Test loss: 1.608, Test accuracy: 85.43
Round  57, Train loss: 1.578, Test loss: 1.609, Test accuracy: 85.30
Round  58, Train loss: 1.576, Test loss: 1.609, Test accuracy: 85.29
Round  59, Train loss: 1.573, Test loss: 1.608, Test accuracy: 85.37
Round  60, Train loss: 1.580, Test loss: 1.609, Test accuracy: 85.36
Round  61, Train loss: 1.574, Test loss: 1.608, Test accuracy: 85.46
Round  62, Train loss: 1.575, Test loss: 1.608, Test accuracy: 85.34
Round  63, Train loss: 1.581, Test loss: 1.608, Test accuracy: 85.40
Round  64, Train loss: 1.581, Test loss: 1.608, Test accuracy: 85.42
Round  65, Train loss: 1.574, Test loss: 1.608, Test accuracy: 85.42
Round  66, Train loss: 1.572, Test loss: 1.608, Test accuracy: 85.46
Round  67, Train loss: 1.569, Test loss: 1.607, Test accuracy: 85.51
Round  68, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.47
Round  69, Train loss: 1.569, Test loss: 1.607, Test accuracy: 85.47
Round  70, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.53
Round  71, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.54
Round  72, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.51
Round  73, Train loss: 1.569, Test loss: 1.607, Test accuracy: 85.55
Round  74, Train loss: 1.581, Test loss: 1.607, Test accuracy: 85.55
Round  75, Train loss: 1.576, Test loss: 1.607, Test accuracy: 85.52
Round  76, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.65
Round  77, Train loss: 1.575, Test loss: 1.607, Test accuracy: 85.58
Round  78, Train loss: 1.578, Test loss: 1.606, Test accuracy: 85.65
Round  79, Train loss: 1.572, Test loss: 1.606, Test accuracy: 85.67
Round  80, Train loss: 1.567, Test loss: 1.606, Test accuracy: 85.67
Round  81, Train loss: 1.570, Test loss: 1.607, Test accuracy: 85.57
Round  82, Train loss: 1.571, Test loss: 1.606, Test accuracy: 85.62
Round  83, Train loss: 1.569, Test loss: 1.606, Test accuracy: 85.62
Round  84, Train loss: 1.575, Test loss: 1.606, Test accuracy: 85.62
Round  85, Train loss: 1.566, Test loss: 1.606, Test accuracy: 85.68
Round  86, Train loss: 1.571, Test loss: 1.606, Test accuracy: 85.69
Round  87, Train loss: 1.566, Test loss: 1.606, Test accuracy: 85.73
Round  88, Train loss: 1.567, Test loss: 1.605, Test accuracy: 85.73
Round  89, Train loss: 1.565, Test loss: 1.605, Test accuracy: 85.79
Round  90, Train loss: 1.578, Test loss: 1.605, Test accuracy: 85.82
Round  91, Train loss: 1.571, Test loss: 1.605, Test accuracy: 85.84
Round  92, Train loss: 1.569, Test loss: 1.605, Test accuracy: 85.80
Round  93, Train loss: 1.575, Test loss: 1.605, Test accuracy: 85.77
Round  94, Train loss: 1.565, Test loss: 1.605, Test accuracy: 85.83
Round  95, Train loss: 1.566, Test loss: 1.605, Test accuracy: 85.78
Round  96, Train loss: 1.574, Test loss: 1.605, Test accuracy: 85.74
Round  97, Train loss: 1.576, Test loss: 1.605, Test accuracy: 85.76
Round  98, Train loss: 1.568, Test loss: 1.605, Test accuracy: 85.79/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.564, Test loss: 1.605, Test accuracy: 85.76
Final Round, Train loss: 1.569, Test loss: 1.606, Test accuracy: 85.69
Average accuracy final 10 rounds: 85.78833333333334 

1362.093878030777
[1.2968358993530273, 2.5936717987060547, 3.7971203327178955, 5.000568866729736, 6.162575006484985, 7.324581146240234, 8.53959035873413, 9.754599571228027, 10.893835306167603, 12.033071041107178, 13.16342544555664, 14.293779850006104, 15.457037925720215, 16.620296001434326, 17.783292531967163, 18.9462890625, 20.10463571548462, 21.26298236846924, 22.435977935791016, 23.608973503112793, 24.78184199333191, 25.954710483551025, 27.11422348022461, 28.273736476898193, 29.41520929336548, 30.556682109832764, 31.71173334121704, 32.86678457260132, 33.94534778594971, 35.023910999298096, 36.140852212905884, 37.25779342651367, 38.44614815711975, 39.63450288772583, 40.629584312438965, 41.6246657371521, 42.803592920303345, 43.98252010345459, 45.113187074661255, 46.24385404586792, 47.29950523376465, 48.35515642166138, 49.43514895439148, 50.51514148712158, 51.63157796859741, 52.74801445007324, 53.93882703781128, 55.129639625549316, 56.2643985748291, 57.39915752410889, 58.535508155822754, 59.67185878753662, 60.87051439285278, 62.069169998168945, 63.18367624282837, 64.2981824874878, 65.32146048545837, 66.34473848342896, 67.39444422721863, 68.4441499710083, 69.596186876297, 70.7482237815857, 71.95440912246704, 73.16059446334839, 74.30522274971008, 75.44985103607178, 76.64668917655945, 77.84352731704712, 78.94210243225098, 80.04067754745483, 81.23005938529968, 82.41944122314453, 83.54257273674011, 84.6657042503357, 85.79845976829529, 86.93121528625488, 88.080815076828, 89.23041486740112, 90.35200572013855, 91.47359657287598, 92.64962100982666, 93.82564544677734, 95.01398086547852, 96.20231628417969, 97.3385910987854, 98.47486591339111, 99.64751744270325, 100.82016897201538, 101.99730110168457, 103.17443323135376, 104.2735230922699, 105.37261295318604, 106.52548694610596, 107.67836093902588, 108.60417699813843, 109.52999305725098, 110.54015731811523, 111.55032157897949, 112.56993818283081, 113.58955478668213, 114.52308869361877, 115.45662260055542, 116.44097590446472, 117.42532920837402, 118.43769764900208, 119.45006608963013, 120.42757034301758, 121.40507459640503, 122.43908309936523, 123.47309160232544, 124.4534101486206, 125.43372869491577, 126.41366052627563, 127.3935923576355, 128.49886345863342, 129.60413455963135, 130.61199498176575, 131.61985540390015, 132.63328981399536, 133.64672422409058, 134.66956496238708, 135.6924057006836, 136.67328310012817, 137.65416049957275, 138.64659070968628, 139.6390209197998, 140.67856097221375, 141.71810102462769, 142.765766620636, 143.8134322166443, 144.75786304473877, 145.70229387283325, 146.67919635772705, 147.65609884262085, 148.6484673023224, 149.64083576202393, 150.6653027534485, 151.68976974487305, 152.6882824897766, 153.68679523468018, 154.62469696998596, 155.56259870529175, 156.51182579994202, 157.46105289459229, 158.51858925819397, 159.57612562179565, 160.58666920661926, 161.59721279144287, 162.52538418769836, 163.45355558395386, 164.43078541755676, 165.40801525115967, 166.40611720085144, 167.4042191505432, 168.4088592529297, 169.41349935531616, 170.44815683364868, 171.4828143119812, 172.43670010566711, 173.39058589935303, 174.40191006660461, 175.4132342338562, 176.4481611251831, 177.48308801651, 178.4668300151825, 179.45057201385498, 180.50062656402588, 181.55068111419678, 182.53335976600647, 183.51603841781616, 184.50952672958374, 185.50301504135132, 186.52125525474548, 187.53949546813965, 188.56641602516174, 189.59333658218384, 190.56103348731995, 191.52873039245605, 192.5064902305603, 193.48425006866455, 194.46873927116394, 195.45322847366333, 196.4406509399414, 197.42807340621948, 198.44486451148987, 199.46165561676025, 200.4380693435669, 201.41448307037354, 202.37968254089355, 203.34488201141357, 204.3423933982849, 205.33990478515625, 206.3665611743927, 207.39321756362915, 208.3992269039154, 209.40523624420166, 210.45186972618103, 211.4985032081604, 212.48080968856812, 213.46311616897583, 215.1834421157837, 216.90376806259155]
[25.758333333333333, 25.758333333333333, 39.891666666666666, 39.891666666666666, 36.56666666666667, 36.56666666666667, 51.325, 51.325, 65.84166666666667, 65.84166666666667, 75.71666666666667, 75.71666666666667, 79.43333333333334, 79.43333333333334, 80.66666666666667, 80.66666666666667, 81.76666666666667, 81.76666666666667, 82.1, 82.1, 82.76666666666667, 82.76666666666667, 82.975, 82.975, 83.39166666666667, 83.39166666666667, 83.54166666666667, 83.54166666666667, 83.80833333333334, 83.80833333333334, 84.125, 84.125, 84.05, 84.05, 84.39166666666667, 84.39166666666667, 84.48333333333333, 84.48333333333333, 84.54166666666667, 84.54166666666667, 84.60833333333333, 84.60833333333333, 84.6, 84.6, 84.65, 84.65, 84.675, 84.675, 84.775, 84.775, 84.88333333333334, 84.88333333333334, 84.86666666666666, 84.86666666666666, 84.975, 84.975, 84.98333333333333, 84.98333333333333, 84.96666666666667, 84.96666666666667, 85.075, 85.075, 85.05, 85.05, 84.94166666666666, 84.94166666666666, 85.01666666666667, 85.01666666666667, 85.075, 85.075, 85.01666666666667, 85.01666666666667, 85.125, 85.125, 85.2, 85.2, 85.15, 85.15, 85.125, 85.125, 85.1, 85.1, 85.24166666666666, 85.24166666666666, 85.1, 85.1, 85.18333333333334, 85.18333333333334, 85.225, 85.225, 85.075, 85.075, 85.26666666666667, 85.26666666666667, 85.25833333333334, 85.25833333333334, 85.31666666666666, 85.31666666666666, 85.33333333333333, 85.33333333333333, 85.35833333333333, 85.35833333333333, 85.31666666666666, 85.31666666666666, 85.26666666666667, 85.26666666666667, 85.375, 85.375, 85.34166666666667, 85.34166666666667, 85.36666666666666, 85.36666666666666, 85.43333333333334, 85.43333333333334, 85.3, 85.3, 85.29166666666667, 85.29166666666667, 85.36666666666666, 85.36666666666666, 85.35833333333333, 85.35833333333333, 85.45833333333333, 85.45833333333333, 85.34166666666667, 85.34166666666667, 85.4, 85.4, 85.425, 85.425, 85.425, 85.425, 85.45833333333333, 85.45833333333333, 85.50833333333334, 85.50833333333334, 85.46666666666667, 85.46666666666667, 85.46666666666667, 85.46666666666667, 85.53333333333333, 85.53333333333333, 85.54166666666667, 85.54166666666667, 85.50833333333334, 85.50833333333334, 85.55, 85.55, 85.55, 85.55, 85.51666666666667, 85.51666666666667, 85.65, 85.65, 85.575, 85.575, 85.65, 85.65, 85.66666666666667, 85.66666666666667, 85.675, 85.675, 85.56666666666666, 85.56666666666666, 85.625, 85.625, 85.625, 85.625, 85.61666666666666, 85.61666666666666, 85.68333333333334, 85.68333333333334, 85.69166666666666, 85.69166666666666, 85.73333333333333, 85.73333333333333, 85.73333333333333, 85.73333333333333, 85.79166666666667, 85.79166666666667, 85.81666666666666, 85.81666666666666, 85.84166666666667, 85.84166666666667, 85.8, 85.8, 85.76666666666667, 85.76666666666667, 85.825, 85.825, 85.78333333333333, 85.78333333333333, 85.74166666666666, 85.74166666666666, 85.75833333333334, 85.75833333333334, 85.79166666666667, 85.79166666666667, 85.75833333333334, 85.75833333333334, 85.69166666666666, 85.69166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.289, Test loss: 2.237, Test accuracy: 30.20
Round   1, Train loss: 1.953, Test loss: 1.824, Test accuracy: 70.58
Round   2, Train loss: 1.642, Test loss: 1.704, Test accuracy: 82.47
Round   3, Train loss: 1.590, Test loss: 1.661, Test accuracy: 83.97
Round   4, Train loss: 1.552, Test loss: 1.646, Test accuracy: 84.89
Round   5, Train loss: 1.559, Test loss: 1.616, Test accuracy: 86.32
Round   6, Train loss: 1.546, Test loss: 1.594, Test accuracy: 87.88
Round   7, Train loss: 1.527, Test loss: 1.589, Test accuracy: 88.39
Round   8, Train loss: 1.532, Test loss: 1.570, Test accuracy: 89.66
Round   9, Train loss: 1.519, Test loss: 1.568, Test accuracy: 89.71
Round  10, Train loss: 1.510, Test loss: 1.566, Test accuracy: 89.93
Round  11, Train loss: 1.514, Test loss: 1.563, Test accuracy: 90.26
Round  12, Train loss: 1.500, Test loss: 1.562, Test accuracy: 90.36
Round  13, Train loss: 1.499, Test loss: 1.561, Test accuracy: 90.36
Round  14, Train loss: 1.501, Test loss: 1.561, Test accuracy: 90.43
Round  15, Train loss: 1.494, Test loss: 1.561, Test accuracy: 90.42
Round  16, Train loss: 1.495, Test loss: 1.560, Test accuracy: 90.47
Round  17, Train loss: 1.493, Test loss: 1.560, Test accuracy: 90.47
Round  18, Train loss: 1.495, Test loss: 1.558, Test accuracy: 90.55
Round  19, Train loss: 1.491, Test loss: 1.558, Test accuracy: 90.60
Round  20, Train loss: 1.497, Test loss: 1.558, Test accuracy: 90.58
Round  21, Train loss: 1.492, Test loss: 1.558, Test accuracy: 90.58
Round  22, Train loss: 1.499, Test loss: 1.557, Test accuracy: 90.68
Round  23, Train loss: 1.492, Test loss: 1.557, Test accuracy: 90.69
Round  24, Train loss: 1.490, Test loss: 1.556, Test accuracy: 90.73
Round  25, Train loss: 1.492, Test loss: 1.556, Test accuracy: 90.77
Round  26, Train loss: 1.488, Test loss: 1.556, Test accuracy: 90.77
Round  27, Train loss: 1.491, Test loss: 1.555, Test accuracy: 90.78
Round  28, Train loss: 1.491, Test loss: 1.555, Test accuracy: 90.83
Round  29, Train loss: 1.490, Test loss: 1.555, Test accuracy: 90.80
Round  30, Train loss: 1.488, Test loss: 1.555, Test accuracy: 90.86
Round  31, Train loss: 1.492, Test loss: 1.555, Test accuracy: 90.83
Round  32, Train loss: 1.489, Test loss: 1.555, Test accuracy: 90.81
Round  33, Train loss: 1.493, Test loss: 1.555, Test accuracy: 90.82
Round  34, Train loss: 1.488, Test loss: 1.555, Test accuracy: 90.83
Round  35, Train loss: 1.487, Test loss: 1.555, Test accuracy: 90.84
Round  36, Train loss: 1.486, Test loss: 1.555, Test accuracy: 90.86
Round  37, Train loss: 1.491, Test loss: 1.554, Test accuracy: 90.85
Round  38, Train loss: 1.492, Test loss: 1.554, Test accuracy: 90.85
Round  39, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.84
Round  40, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.86
Round  41, Train loss: 1.492, Test loss: 1.554, Test accuracy: 90.84
Round  42, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.86
Round  43, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.89
Round  44, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.89
Round  45, Train loss: 1.490, Test loss: 1.554, Test accuracy: 90.89
Round  46, Train loss: 1.492, Test loss: 1.554, Test accuracy: 90.90
Round  47, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.91
Round  48, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.92
Round  49, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.94
Round  50, Train loss: 1.488, Test loss: 1.554, Test accuracy: 90.93
Round  51, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.95
Round  52, Train loss: 1.488, Test loss: 1.554, Test accuracy: 90.92
Round  53, Train loss: 1.490, Test loss: 1.554, Test accuracy: 90.91
Round  54, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.94
Round  55, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.94
Round  56, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.94
Round  57, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.95
Round  58, Train loss: 1.488, Test loss: 1.554, Test accuracy: 90.92
Round  59, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.94
Round  60, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.95
Round  61, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.94
Round  62, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.92
Round  63, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.94
Round  64, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.94
Round  65, Train loss: 1.488, Test loss: 1.554, Test accuracy: 90.95
Round  66, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.97
Round  67, Train loss: 1.491, Test loss: 1.554, Test accuracy: 90.97
Round  68, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.95
Round  69, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.95
Round  70, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.95
Round  71, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.95
Round  72, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.96
Round  73, Train loss: 1.489, Test loss: 1.553, Test accuracy: 90.96
Round  74, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.95
Round  75, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.93
Round  76, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.97
Round  77, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.96
Round  78, Train loss: 1.487, Test loss: 1.553, Test accuracy: 90.98
Round  79, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.98
Round  80, Train loss: 1.485, Test loss: 1.553, Test accuracy: 90.99
Round  81, Train loss: 1.487, Test loss: 1.553, Test accuracy: 91.00
Round  82, Train loss: 1.488, Test loss: 1.553, Test accuracy: 91.00
Round  83, Train loss: 1.486, Test loss: 1.553, Test accuracy: 90.99
Round  84, Train loss: 1.490, Test loss: 1.553, Test accuracy: 91.00
Round  85, Train loss: 1.487, Test loss: 1.553, Test accuracy: 91.00
Round  86, Train loss: 1.485, Test loss: 1.553, Test accuracy: 90.98
Round  87, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.99
Round  88, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.99
Round  89, Train loss: 1.485, Test loss: 1.553, Test accuracy: 91.00
Round  90, Train loss: 1.482, Test loss: 1.553, Test accuracy: 91.01
Round  91, Train loss: 1.483, Test loss: 1.553, Test accuracy: 91.02
Round  92, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.99
Round  93, Train loss: 1.485, Test loss: 1.553, Test accuracy: 90.98
Round  94, Train loss: 1.486, Test loss: 1.553, Test accuracy: 90.98
Round  95, Train loss: 1.482, Test loss: 1.553, Test accuracy: 90.98
Round  96, Train loss: 1.484, Test loss: 1.553, Test accuracy: 91.00
Round  97, Train loss: 1.487, Test loss: 1.553, Test accuracy: 91.00
Round  98, Train loss: 1.485, Test loss: 1.553, Test accuracy: 90.98
Round  99, Train loss: 1.486, Test loss: 1.553, Test accuracy: 90.98/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.98
Average accuracy final 10 rounds: 90.99175 

4282.736147642136
[2.5665767192840576, 5.133153438568115, 7.7849390506744385, 10.436724662780762, 13.021049737930298, 15.605374813079834, 18.227288246154785, 20.849201679229736, 23.365391492843628, 25.88158130645752, 28.38517189025879, 30.88876247406006, 33.7054717540741, 36.522181034088135, 39.41102075576782, 42.29986047744751, 45.253742933273315, 48.20762538909912, 51.246575593948364, 54.28552579879761, 57.19935345649719, 60.11318111419678, 63.248175621032715, 66.38317012786865, 69.55757808685303, 72.7319860458374, 75.9301815032959, 79.1283769607544, 82.81270933151245, 86.49704170227051, 89.92322778701782, 93.34941387176514, 96.51136326789856, 99.67331266403198, 103.1108410358429, 106.54836940765381, 109.9453067779541, 113.3422441482544, 116.52050018310547, 119.69875621795654, 123.00830101966858, 126.31784582138062, 129.66406750679016, 133.0102891921997, 136.32869338989258, 139.64709758758545, 143.15394163131714, 146.66078567504883, 150.0623073577881, 153.46382904052734, 156.77379417419434, 160.08375930786133, 163.41743302345276, 166.7511067390442, 170.06626653671265, 173.3814263343811, 176.76961755752563, 180.15780878067017, 183.6155002117157, 187.07319164276123, 190.32452988624573, 193.57586812973022, 196.7484414577484, 199.9210147857666, 203.18318557739258, 206.44535636901855, 209.76532316207886, 213.08528995513916, 216.4792492389679, 219.87320852279663, 223.52814269065857, 227.1830768585205, 230.58315706253052, 233.98323726654053, 237.36666631698608, 240.75009536743164, 244.14242005348206, 247.53474473953247, 250.86126041412354, 254.1877760887146, 257.4853880405426, 260.7829999923706, 264.1864798069, 267.58995962142944, 270.84940338134766, 274.10884714126587, 277.40533542633057, 280.70182371139526, 284.08152532577515, 287.46122694015503, 290.75016355514526, 294.0391001701355, 297.2102301120758, 300.3813600540161, 304.0063388347626, 307.63131761550903, 311.4306900501251, 315.2300624847412, 318.4853916168213, 321.74072074890137, 325.38952803611755, 329.03833532333374, 332.67973017692566, 336.3211250305176, 339.6713788509369, 343.0216326713562, 346.5695741176605, 350.11751556396484, 353.74088501930237, 357.3642544746399, 361.24071168899536, 365.11716890335083, 368.7512195110321, 372.3852701187134, 376.22021555900574, 380.0551609992981, 383.6987202167511, 387.3422794342041, 390.7350058555603, 394.1277322769165, 397.5788609981537, 401.02998971939087, 404.5716087818146, 408.1132278442383, 411.4869601726532, 414.8606925010681, 418.2200870513916, 421.5794816017151, 425.099689245224, 428.6198968887329, 432.0894374847412, 435.5589780807495, 439.0501925945282, 442.5414071083069, 445.95774102211, 449.3740749359131, 452.7448139190674, 456.1155529022217, 459.5776951313019, 463.0398373603821, 466.4631607532501, 469.88648414611816, 473.3106346130371, 476.73478507995605, 480.0584292411804, 483.3820734024048, 486.8849995136261, 490.3879256248474, 493.8390805721283, 497.2902355194092, 500.6958384513855, 504.1014413833618, 507.47617864608765, 510.8509159088135, 514.2869343757629, 517.7229528427124, 521.1032269001007, 524.483500957489, 527.8771917819977, 531.2708826065063, 534.7062084674835, 538.1415343284607, 541.6223275661469, 545.103120803833, 548.6838405132294, 552.2645602226257, 555.752911567688, 559.2412629127502, 562.510808467865, 565.7803540229797, 569.3975734710693, 573.0147929191589, 576.5160932540894, 580.0173935890198, 583.3672318458557, 586.7170701026917, 590.0850851535797, 593.4531002044678, 597.0220687389374, 600.591037273407, 603.8629951477051, 607.1349530220032, 610.5285840034485, 613.9222149848938, 617.4533972740173, 620.9845795631409, 624.3476333618164, 627.710687160492, 630.9757013320923, 634.2407155036926, 637.8420970439911, 641.4434785842896, 645.0221807956696, 648.6008830070496, 651.7637372016907, 654.9265913963318, 658.5133860111237, 662.1001806259155, 665.6296539306641, 669.1591272354126, 670.9433901309967, 672.7276530265808]
[30.1975, 30.1975, 70.58, 70.58, 82.4725, 82.4725, 83.97, 83.97, 84.885, 84.885, 86.3175, 86.3175, 87.8775, 87.8775, 88.3875, 88.3875, 89.66, 89.66, 89.71, 89.71, 89.93, 89.93, 90.2625, 90.2625, 90.36, 90.36, 90.3575, 90.3575, 90.43, 90.43, 90.4175, 90.4175, 90.475, 90.475, 90.465, 90.465, 90.5475, 90.5475, 90.5975, 90.5975, 90.58, 90.58, 90.575, 90.575, 90.68, 90.68, 90.6875, 90.6875, 90.73, 90.73, 90.77, 90.77, 90.765, 90.765, 90.785, 90.785, 90.8275, 90.8275, 90.7975, 90.7975, 90.86, 90.86, 90.835, 90.835, 90.815, 90.815, 90.82, 90.82, 90.8275, 90.8275, 90.8375, 90.8375, 90.86, 90.86, 90.85, 90.85, 90.8475, 90.8475, 90.845, 90.845, 90.865, 90.865, 90.8425, 90.8425, 90.86, 90.86, 90.89, 90.89, 90.895, 90.895, 90.8925, 90.8925, 90.9, 90.9, 90.9125, 90.9125, 90.925, 90.925, 90.945, 90.945, 90.9325, 90.9325, 90.9475, 90.9475, 90.925, 90.925, 90.9125, 90.9125, 90.9375, 90.9375, 90.9425, 90.9425, 90.9425, 90.9425, 90.955, 90.955, 90.9175, 90.9175, 90.94, 90.94, 90.9475, 90.9475, 90.935, 90.935, 90.92, 90.92, 90.94, 90.94, 90.945, 90.945, 90.9475, 90.9475, 90.965, 90.965, 90.9675, 90.9675, 90.955, 90.955, 90.955, 90.955, 90.9475, 90.9475, 90.95, 90.95, 90.96, 90.96, 90.9625, 90.9625, 90.95, 90.95, 90.93, 90.93, 90.975, 90.975, 90.9625, 90.9625, 90.9775, 90.9775, 90.98, 90.98, 90.9875, 90.9875, 91.005, 91.005, 91.0, 91.0, 90.9925, 90.9925, 90.995, 90.995, 91.0025, 91.0025, 90.98, 90.98, 90.9875, 90.9875, 90.99, 90.99, 90.995, 90.995, 91.0075, 91.0075, 91.0175, 91.0175, 90.99, 90.99, 90.985, 90.985, 90.98, 90.98, 90.98, 90.98, 90.9975, 90.9975, 91.0, 91.0, 90.9775, 90.9775, 90.9825, 90.9825, 90.98, 90.98]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.518, Test loss: 1.923, Test accuracy: 65.89
Round   1, Train loss: 1.270, Test loss: 1.726, Test accuracy: 80.40
Round   2, Train loss: 1.220, Test loss: 1.696, Test accuracy: 81.54
Round   3, Train loss: 1.217, Test loss: 1.682, Test accuracy: 82.47
Round   4, Train loss: 1.202, Test loss: 1.676, Test accuracy: 82.77
Round   5, Train loss: 1.205, Test loss: 1.675, Test accuracy: 82.69
Round   6, Train loss: 1.194, Test loss: 1.673, Test accuracy: 82.82
Round   7, Train loss: 1.189, Test loss: 1.671, Test accuracy: 83.28
Round   8, Train loss: 1.197, Test loss: 1.669, Test accuracy: 83.56
Round   9, Train loss: 1.185, Test loss: 1.665, Test accuracy: 84.06
Round  10, Train loss: 1.192, Test loss: 1.664, Test accuracy: 84.19
Round  11, Train loss: 1.186, Test loss: 1.665, Test accuracy: 84.03
Round  12, Train loss: 1.180, Test loss: 1.664, Test accuracy: 84.02
Round  13, Train loss: 1.185, Test loss: 1.664, Test accuracy: 83.97
Round  14, Train loss: 1.178, Test loss: 1.665, Test accuracy: 83.93
Round  15, Train loss: 1.180, Test loss: 1.665, Test accuracy: 83.83
Round  16, Train loss: 1.181, Test loss: 1.665, Test accuracy: 83.84
Round  17, Train loss: 1.184, Test loss: 1.666, Test accuracy: 83.75
Round  18, Train loss: 1.183, Test loss: 1.668, Test accuracy: 83.61
Round  19, Train loss: 1.183, Test loss: 1.668, Test accuracy: 83.62
Round  20, Train loss: 1.179, Test loss: 1.668, Test accuracy: 83.58
Round  21, Train loss: 1.179, Test loss: 1.670, Test accuracy: 83.50
Round  22, Train loss: 1.177, Test loss: 1.670, Test accuracy: 83.44
Round  23, Train loss: 1.177, Test loss: 1.671, Test accuracy: 83.48
Round  24, Train loss: 1.179, Test loss: 1.671, Test accuracy: 83.31
Round  25, Train loss: 1.178, Test loss: 1.672, Test accuracy: 83.29
Round  26, Train loss: 1.178, Test loss: 1.674, Test accuracy: 83.19
Round  27, Train loss: 1.173, Test loss: 1.674, Test accuracy: 83.08
Round  28, Train loss: 1.176, Test loss: 1.675, Test accuracy: 83.00
Round  29, Train loss: 1.172, Test loss: 1.676, Test accuracy: 82.94
Round  30, Train loss: 1.174, Test loss: 1.677, Test accuracy: 82.89
Round  31, Train loss: 1.170, Test loss: 1.678, Test accuracy: 82.85
Round  32, Train loss: 1.177, Test loss: 1.679, Test accuracy: 82.74
Round  33, Train loss: 1.178, Test loss: 1.679, Test accuracy: 82.74
Round  34, Train loss: 1.176, Test loss: 1.680, Test accuracy: 82.71
Round  35, Train loss: 1.175, Test loss: 1.681, Test accuracy: 82.56
Round  36, Train loss: 1.174, Test loss: 1.681, Test accuracy: 82.56
Round  37, Train loss: 1.172, Test loss: 1.682, Test accuracy: 82.45
Round  38, Train loss: 1.174, Test loss: 1.684, Test accuracy: 82.35
Round  39, Train loss: 1.172, Test loss: 1.685, Test accuracy: 82.22
Round  40, Train loss: 1.176, Test loss: 1.686, Test accuracy: 82.20
Round  41, Train loss: 1.175, Test loss: 1.686, Test accuracy: 82.11
Round  42, Train loss: 1.172, Test loss: 1.687, Test accuracy: 82.05
Round  43, Train loss: 1.175, Test loss: 1.688, Test accuracy: 82.01
Round  44, Train loss: 1.172, Test loss: 1.689, Test accuracy: 81.89
Round  45, Train loss: 1.174, Test loss: 1.691, Test accuracy: 81.81
Round  46, Train loss: 1.172, Test loss: 1.692, Test accuracy: 81.73
Round  47, Train loss: 1.172, Test loss: 1.693, Test accuracy: 81.60
Round  48, Train loss: 1.174, Test loss: 1.694, Test accuracy: 81.60
Round  49, Train loss: 1.172, Test loss: 1.695, Test accuracy: 81.56
Round  50, Train loss: 1.173, Test loss: 1.695, Test accuracy: 81.49
Round  51, Train loss: 1.174, Test loss: 1.697, Test accuracy: 81.38
Round  52, Train loss: 1.171, Test loss: 1.697, Test accuracy: 81.39
Round  53, Train loss: 1.147, Test loss: 1.690, Test accuracy: 83.03
Round  54, Train loss: 1.126, Test loss: 1.683, Test accuracy: 83.89
Round  55, Train loss: 1.135, Test loss: 1.676, Test accuracy: 84.98
Round  56, Train loss: 1.108, Test loss: 1.674, Test accuracy: 85.06
Round  57, Train loss: 1.115, Test loss: 1.671, Test accuracy: 85.58
Round  58, Train loss: 1.118, Test loss: 1.665, Test accuracy: 86.31
Round  59, Train loss: 1.114, Test loss: 1.663, Test accuracy: 87.00
Round  60, Train loss: 1.110, Test loss: 1.663, Test accuracy: 87.36
Round  61, Train loss: 1.107, Test loss: 1.663, Test accuracy: 87.38
Round  62, Train loss: 1.106, Test loss: 1.664, Test accuracy: 87.35
Round  63, Train loss: 1.105, Test loss: 1.662, Test accuracy: 87.47
Round  64, Train loss: 1.107, Test loss: 1.663, Test accuracy: 87.36
Round  65, Train loss: 1.106, Test loss: 1.662, Test accuracy: 87.42
Round  66, Train loss: 1.105, Test loss: 1.663, Test accuracy: 87.33
Round  67, Train loss: 1.105, Test loss: 1.663, Test accuracy: 87.22
Round  68, Train loss: 1.105, Test loss: 1.664, Test accuracy: 87.17
Round  69, Train loss: 1.105, Test loss: 1.665, Test accuracy: 87.12
Round  70, Train loss: 1.105, Test loss: 1.666, Test accuracy: 86.97
Round  71, Train loss: 1.104, Test loss: 1.666, Test accuracy: 86.92
Round  72, Train loss: 1.104, Test loss: 1.666, Test accuracy: 86.85
Round  73, Train loss: 1.103, Test loss: 1.668, Test accuracy: 86.71
Round  74, Train loss: 1.102, Test loss: 1.668, Test accuracy: 86.61
Round  75, Train loss: 1.104, Test loss: 1.669, Test accuracy: 86.59
Round  76, Train loss: 1.104, Test loss: 1.669, Test accuracy: 86.53
Round  77, Train loss: 1.102, Test loss: 1.670, Test accuracy: 86.48
Round  78, Train loss: 1.103, Test loss: 1.670, Test accuracy: 86.51
Round  79, Train loss: 1.103, Test loss: 1.670, Test accuracy: 86.50
Round  80, Train loss: 1.103, Test loss: 1.670, Test accuracy: 86.51
Round  81, Train loss: 1.103, Test loss: 1.670, Test accuracy: 86.57
Round  82, Train loss: 1.102, Test loss: 1.671, Test accuracy: 86.50
Round  83, Train loss: 1.103, Test loss: 1.671, Test accuracy: 86.47
Round  84, Train loss: 1.103, Test loss: 1.672, Test accuracy: 86.34
Round  85, Train loss: 1.104, Test loss: 1.672, Test accuracy: 86.32
Round  86, Train loss: 1.103, Test loss: 1.673, Test accuracy: 86.23
Round  87, Train loss: 1.103, Test loss: 1.674, Test accuracy: 86.17
Round  88, Train loss: 1.102, Test loss: 1.674, Test accuracy: 86.18
Round  89, Train loss: 1.103, Test loss: 1.675, Test accuracy: 86.06
Round  90, Train loss: 1.102, Test loss: 1.676, Test accuracy: 85.94
Round  91, Train loss: 1.102, Test loss: 1.676, Test accuracy: 85.96
Round  92, Train loss: 1.103, Test loss: 1.676, Test accuracy: 85.85
Round  93, Train loss: 1.102, Test loss: 1.677, Test accuracy: 85.72
Round  94, Train loss: 1.103, Test loss: 1.678, Test accuracy: 85.67
Round  95, Train loss: 1.103, Test loss: 1.679, Test accuracy: 85.59
Round  96, Train loss: 1.102, Test loss: 1.678, Test accuracy: 85.64
Round  97, Train loss: 1.103, Test loss: 1.679, Test accuracy: 85.59
Round  98, Train loss: 1.103, Test loss: 1.680, Test accuracy: 85.43
Round  99, Train loss: 1.103, Test loss: 1.680, Test accuracy: 85.44
Final Round, Train loss: 1.102, Test loss: 1.683, Test accuracy: 85.32
Average accuracy final 10 rounds: 85.68275
5555.044305801392
[]
[65.89, 80.4025, 81.5375, 82.4725, 82.7725, 82.695, 82.8175, 83.285, 83.565, 84.065, 84.1875, 84.035, 84.015, 83.975, 83.9275, 83.825, 83.8375, 83.755, 83.615, 83.6175, 83.5775, 83.495, 83.445, 83.48, 83.3075, 83.2925, 83.1875, 83.08, 83.005, 82.9425, 82.885, 82.8525, 82.7375, 82.7375, 82.7125, 82.5575, 82.56, 82.45, 82.3525, 82.2175, 82.2, 82.11, 82.05, 82.0125, 81.8875, 81.8125, 81.73, 81.6025, 81.5975, 81.5575, 81.4875, 81.3825, 81.3875, 83.0275, 83.885, 84.9775, 85.06, 85.575, 86.3125, 87.005, 87.365, 87.3825, 87.35, 87.475, 87.365, 87.4175, 87.33, 87.2225, 87.1725, 87.1175, 86.97, 86.9225, 86.8475, 86.71, 86.61, 86.5875, 86.535, 86.485, 86.51, 86.5025, 86.5125, 86.5725, 86.505, 86.465, 86.34, 86.3175, 86.2275, 86.17, 86.18, 86.055, 85.945, 85.9575, 85.8525, 85.715, 85.665, 85.5925, 85.64, 85.5875, 85.4325, 85.44, 85.3225]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.293, Test loss: 2.292, Test accuracy: 23.37
Round   1, Train loss: 2.202, Test loss: 2.201, Test accuracy: 41.34
Round   2, Train loss: 1.973, Test loss: 2.103, Test accuracy: 50.65
Round   3, Train loss: 1.990, Test loss: 2.089, Test accuracy: 48.02
Round   4, Train loss: 1.822, Test loss: 2.082, Test accuracy: 44.56
Round   5, Train loss: 1.694, Test loss: 2.022, Test accuracy: 47.61
Round   6, Train loss: 1.623, Test loss: 1.989, Test accuracy: 48.75
Round   7, Train loss: 0.704, Test loss: 1.883, Test accuracy: 59.52
Round   8, Train loss: 1.362, Test loss: 1.836, Test accuracy: 64.96
Round   9, Train loss: 1.572, Test loss: 1.812, Test accuracy: 68.15
Round  10, Train loss: 1.771, Test loss: 1.792, Test accuracy: 70.09
Round  11, Train loss: 0.964, Test loss: 1.759, Test accuracy: 72.93
Round  12, Train loss: 1.623, Test loss: 1.757, Test accuracy: 73.19
Round  13, Train loss: 1.646, Test loss: 1.740, Test accuracy: 74.56
Round  14, Train loss: 1.411, Test loss: 1.733, Test accuracy: 74.59
Round  15, Train loss: 1.377, Test loss: 1.741, Test accuracy: 73.87
Round  16, Train loss: 1.499, Test loss: 1.721, Test accuracy: 75.79
Round  17, Train loss: 1.292, Test loss: 1.711, Test accuracy: 76.26
Round  18, Train loss: 1.583, Test loss: 1.711, Test accuracy: 76.59
Round  19, Train loss: 1.452, Test loss: 1.690, Test accuracy: 78.47
Round  20, Train loss: 1.364, Test loss: 1.687, Test accuracy: 78.76
Round  21, Train loss: 1.494, Test loss: 1.677, Test accuracy: 79.60
Round  22, Train loss: 1.353, Test loss: 1.676, Test accuracy: 79.74
Round  23, Train loss: 1.246, Test loss: 1.674, Test accuracy: 79.83
Round  24, Train loss: 1.279, Test loss: 1.668, Test accuracy: 80.38
Round  25, Train loss: 1.514, Test loss: 1.663, Test accuracy: 80.41
Round  26, Train loss: 1.173, Test loss: 1.662, Test accuracy: 80.49
Round  27, Train loss: 1.242, Test loss: 1.662, Test accuracy: 80.51
Round  28, Train loss: 1.276, Test loss: 1.655, Test accuracy: 81.21
Round  29, Train loss: 1.208, Test loss: 1.655, Test accuracy: 81.27
Round  30, Train loss: 1.073, Test loss: 1.648, Test accuracy: 81.89
Round  31, Train loss: 1.255, Test loss: 1.646, Test accuracy: 82.12
Round  32, Train loss: 1.147, Test loss: 1.642, Test accuracy: 82.51
Round  33, Train loss: 1.226, Test loss: 1.628, Test accuracy: 84.08
Round  34, Train loss: 1.180, Test loss: 1.619, Test accuracy: 85.01
Round  35, Train loss: 1.156, Test loss: 1.621, Test accuracy: 84.69
Round  36, Train loss: 1.115, Test loss: 1.625, Test accuracy: 84.24
Round  37, Train loss: 1.195, Test loss: 1.631, Test accuracy: 83.71
Round  38, Train loss: 1.181, Test loss: 1.639, Test accuracy: 82.63
Round  39, Train loss: 1.066, Test loss: 1.631, Test accuracy: 83.51
Round  40, Train loss: 1.034, Test loss: 1.635, Test accuracy: 83.07
Round  41, Train loss: 1.019, Test loss: 1.623, Test accuracy: 84.23
Round  42, Train loss: 0.934, Test loss: 1.619, Test accuracy: 84.60
Round  43, Train loss: 0.876, Test loss: 1.614, Test accuracy: 85.12
Round  44, Train loss: 1.030, Test loss: 1.613, Test accuracy: 85.18
Round  45, Train loss: 0.818, Test loss: 1.610, Test accuracy: 85.44
Round  46, Train loss: 0.743, Test loss: 1.609, Test accuracy: 85.54
Round  47, Train loss: 0.985, Test loss: 1.613, Test accuracy: 85.06
Round  48, Train loss: 1.029, Test loss: 1.609, Test accuracy: 85.55
Round  49, Train loss: 0.962, Test loss: 1.615, Test accuracy: 84.90
Round  50, Train loss: 0.988, Test loss: 1.625, Test accuracy: 83.94
Round  51, Train loss: 0.770, Test loss: 1.608, Test accuracy: 85.62
Round  52, Train loss: 0.885, Test loss: 1.596, Test accuracy: 86.76
Round  53, Train loss: 0.739, Test loss: 1.591, Test accuracy: 87.32
Round  54, Train loss: 0.963, Test loss: 1.594, Test accuracy: 87.00
Round  55, Train loss: 1.048, Test loss: 1.597, Test accuracy: 86.81
Round  56, Train loss: 0.803, Test loss: 1.595, Test accuracy: 87.07
Round  57, Train loss: 0.854, Test loss: 1.589, Test accuracy: 87.67
Round  58, Train loss: 0.765, Test loss: 1.585, Test accuracy: 88.08
Round  59, Train loss: 0.808, Test loss: 1.583, Test accuracy: 88.24
Round  60, Train loss: 1.000, Test loss: 1.583, Test accuracy: 88.32
Round  61, Train loss: 0.735, Test loss: 1.582, Test accuracy: 88.45
Round  62, Train loss: 0.786, Test loss: 1.584, Test accuracy: 88.09
Round  63, Train loss: 0.752, Test loss: 1.581, Test accuracy: 88.30
Round  64, Train loss: 0.814, Test loss: 1.579, Test accuracy: 88.57
Round  65, Train loss: 0.617, Test loss: 1.573, Test accuracy: 89.17
Round  66, Train loss: 0.641, Test loss: 1.573, Test accuracy: 89.20
Round  67, Train loss: 0.711, Test loss: 1.572, Test accuracy: 89.30
Round  68, Train loss: 0.900, Test loss: 1.574, Test accuracy: 89.19
Round  69, Train loss: 0.605, Test loss: 1.571, Test accuracy: 89.37
Round  70, Train loss: 0.654, Test loss: 1.569, Test accuracy: 89.58
Round  71, Train loss: 0.831, Test loss: 1.568, Test accuracy: 89.67
Round  72, Train loss: 0.784, Test loss: 1.569, Test accuracy: 89.58
Round  73, Train loss: 0.666, Test loss: 1.571, Test accuracy: 89.36
Round  74, Train loss: 0.735, Test loss: 1.568, Test accuracy: 89.74
Round  75, Train loss: 0.815, Test loss: 1.568, Test accuracy: 89.70
Round  76, Train loss: 0.739, Test loss: 1.568, Test accuracy: 89.65
Round  77, Train loss: 0.704, Test loss: 1.570, Test accuracy: 89.49
Round  78, Train loss: 0.720, Test loss: 1.570, Test accuracy: 89.37
Round  79, Train loss: 0.682, Test loss: 1.568, Test accuracy: 89.59
Round  80, Train loss: 0.652, Test loss: 1.567, Test accuracy: 89.74
Round  81, Train loss: 0.603, Test loss: 1.566, Test accuracy: 89.84
Round  82, Train loss: 0.818, Test loss: 1.567, Test accuracy: 89.75
Round  83, Train loss: 0.560, Test loss: 1.566, Test accuracy: 89.86
Round  84, Train loss: 0.580, Test loss: 1.566, Test accuracy: 89.86
Round  85, Train loss: 0.743, Test loss: 1.565, Test accuracy: 89.98
Round  86, Train loss: 0.680, Test loss: 1.566, Test accuracy: 89.86
Round  87, Train loss: 0.733, Test loss: 1.566, Test accuracy: 89.82
Round  88, Train loss: 0.738, Test loss: 1.566, Test accuracy: 89.91
Round  89, Train loss: 0.685, Test loss: 1.566, Test accuracy: 89.93
Round  90, Train loss: 0.623, Test loss: 1.566, Test accuracy: 89.90
Round  91, Train loss: 0.637, Test loss: 1.564, Test accuracy: 90.03
Round  92, Train loss: 0.732, Test loss: 1.565, Test accuracy: 89.93
Round  93, Train loss: 0.724, Test loss: 1.566, Test accuracy: 89.92
Round  94, Train loss: 0.691, Test loss: 1.563, Test accuracy: 90.22
Round  95, Train loss: 0.838, Test loss: 1.563, Test accuracy: 90.20
Round  96, Train loss: 0.588, Test loss: 1.564, Test accuracy: 90.09
Round  97, Train loss: 0.731, Test loss: 1.564, Test accuracy: 90.06
Round  98, Train loss: 0.718, Test loss: 1.565, Test accuracy: 89.96
Round  99, Train loss: 0.635, Test loss: 1.565, Test accuracy: 90.03
Final Round, Train loss: 1.507, Test loss: 1.550, Test accuracy: 91.66
Average accuracy final 10 rounds: 90.03305555555555
Average global accuracy final 10 rounds: 90.03305555555555
3829.7804656028748
[]
[23.372222222222224, 41.33611111111111, 50.65, 48.019444444444446, 44.56388888888889, 47.605555555555554, 48.74722222222222, 59.522222222222226, 64.95833333333333, 68.15277777777777, 70.08611111111111, 72.92777777777778, 73.18611111111112, 74.55555555555556, 74.58611111111111, 73.86944444444444, 75.78611111111111, 76.25555555555556, 76.59166666666667, 78.46666666666667, 78.76388888888889, 79.60277777777777, 79.74166666666666, 79.83055555555555, 80.375, 80.41111111111111, 80.49444444444444, 80.50833333333334, 81.21111111111111, 81.27222222222223, 81.88888888888889, 82.11944444444444, 82.5111111111111, 84.07777777777778, 85.0111111111111, 84.69166666666666, 84.24444444444444, 83.71111111111111, 82.63333333333334, 83.50555555555556, 83.06666666666666, 84.23333333333333, 84.6, 85.11944444444444, 85.18055555555556, 85.44444444444444, 85.53611111111111, 85.05833333333334, 85.55, 84.9, 83.94444444444444, 85.625, 86.76388888888889, 87.31944444444444, 87.0, 86.80833333333334, 87.06666666666666, 87.67222222222222, 88.07777777777778, 88.24444444444444, 88.31944444444444, 88.44722222222222, 88.09444444444445, 88.3, 88.56944444444444, 89.16944444444445, 89.19722222222222, 89.29722222222222, 89.18888888888888, 89.36666666666666, 89.575, 89.675, 89.58333333333333, 89.36111111111111, 89.74166666666666, 89.7, 89.65, 89.49444444444444, 89.36666666666666, 89.59444444444445, 89.74444444444444, 89.83611111111111, 89.75, 89.85555555555555, 89.8638888888889, 89.98333333333333, 89.86111111111111, 89.82222222222222, 89.90833333333333, 89.93333333333334, 89.9, 90.02777777777777, 89.93055555555556, 89.92222222222222, 90.21666666666667, 90.2, 90.09444444444445, 90.05833333333334, 89.95555555555555, 90.025, 91.65833333333333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.22
Round   0, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.22
Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.22
Round   1, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.22
Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.23
Round   2, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.28
Round   3, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.27
Round   3, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.27
Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.27
Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.26
Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.27
Round   5, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.30
Round   6, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.30
Round   6, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.32
Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.34
Round   7, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.31
Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.34
Round   8, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.33
Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.36
Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.39
Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.39
Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.41
Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.43
Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.50
Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.47
Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.52
Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.49
Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.59
Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.56
Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.68
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.61
Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.69
Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.66
Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.77
Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.70
Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.77
Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.75
Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.80
Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.90
Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.84
Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.94
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88
Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.02
Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.95
Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.08
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.01
Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.09
Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.05
Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.15
Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.10
Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.20
Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.17
Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.27
Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.19
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.31
Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.24
Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.34
Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.30
Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.46
Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.34
Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.49
Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.42
Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.52
Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.45
Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.52
Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.52
Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.60
Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.57
Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.63
Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.64
Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.71
Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.67
Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.75
Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.71
Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.79
Round  38, Train loss: 2.301, Test loss: 2.302, Test accuracy: 13.74
Round  38, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 13.81
Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.77
Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.82
Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.79
Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.86
Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.83
Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.92
Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.85
Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.91
Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.89
Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.98
Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.93
Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.99
Round  45, Train loss: 2.301, Test loss: 2.302, Test accuracy: 13.96
Round  45, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.03
Round  46, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.02
Round  46, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.06
Round  47, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.04
Round  47, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.09
Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.10
Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.16
Round  49, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.12
Round  49, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.16
Round  50, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.14
Round  50, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.16
Round  51, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.16
Round  51, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.25
Round  52, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.21
Round  52, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.28
Round  53, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.24
Round  53, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.35
Round  54, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.26
Round  54, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.36
Round  55, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.34
Round  55, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.47
Round  56, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.38
Round  56, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.47
Round  57, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.43
Round  57, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.50
Round  58, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.43
Round  58, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.53
Round  59, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.46
Round  59, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.54
Round  60, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.49
Round  60, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.58
Round  61, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.51
Round  61, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.57
Round  62, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.54
Round  62, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.60
Round  63, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.58
Round  63, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.71
Round  64, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.62
Round  64, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.67
Round  65, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.65
Round  65, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.68
Round  66, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.70
Round  66, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.74
Round  67, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.74
Round  67, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.84
Round  68, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.79
Round  68, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.93
Round  69, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.83
Round  69, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.96
Round  70, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.87
Round  70, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.96
Round  71, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.90
Round  71, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.04
Round  72, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.93
Round  72, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.05
Round  73, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.98
Round  73, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.11
Round  74, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.06
Round  74, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.20
Round  75, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.12
Round  75, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.23
Round  76, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.21
Round  76, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.29
Round  77, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.27
Round  77, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.37
Round  78, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.31
Round  78, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.51
Round  79, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.38
Round  79, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.54
Round  80, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.43
Round  80, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.57
Round  81, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.52
Round  81, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.69
Round  82, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.56
Round  82, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.70
Round  83, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.62
Round  83, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.74
Round  84, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.66
Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.75
Round  85, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.70
Round  85, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.80
Round  86, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.73
Round  86, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.82
Round  87, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.77
Round  87, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.90
Round  88, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.81
Round  88, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.91
Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.85
Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.96
Round  90, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.89
Round  90, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.97
Round  91, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.93
Round  91, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.98
Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.96
Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.01
Round  93, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.96
Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.01
Round  94, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.98
Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.09
Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.99
Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.11
Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.08
Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.16/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.14
Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.23
Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.17
Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.29
Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.20
Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.38
Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.45
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.38
Average accuracy final 10 rounds: 16.03025 

Average global accuracy final 10 rounds: 16.12175 

5094.493855953217
[4.508702039718628, 8.581531763076782, 13.067278861999512, 17.27228546142578, 21.58528447151184, 25.75183129310608, 29.976293802261353, 34.3009672164917, 38.68493700027466, 42.94258403778076, 47.2479989528656, 51.52560877799988, 55.85429239273071, 60.164915800094604, 64.58789658546448, 68.80404853820801, 72.85351490974426, 76.94397473335266, 81.38966369628906, 85.81671261787415, 90.34549260139465, 94.91104745864868, 99.00733804702759, 103.09248042106628, 107.18189787864685, 111.14309310913086, 115.24141788482666, 119.64856743812561, 123.79462933540344, 127.95480966567993, 132.18062233924866, 136.1995813846588, 140.3780345916748, 144.70395708084106, 148.81972765922546, 152.8419497013092, 156.9302704334259, 161.1497049331665, 165.4647204875946, 169.26802945137024, 173.00830125808716, 176.90457344055176, 180.82667136192322, 184.56640195846558, 188.29823899269104, 192.27606415748596, 196.21294236183167, 199.95066142082214, 204.03905487060547, 208.11991715431213, 212.0632824897766, 215.80443334579468, 219.89651083946228, 223.9928011894226, 227.9270372390747, 231.6399130821228, 235.52163743972778, 239.46585512161255, 243.2317762374878, 246.97271966934204, 250.8281409740448, 254.69478249549866, 258.2542419433594, 262.00812315940857, 265.7014639377594, 269.69799065589905, 273.82115292549133, 278.1881754398346, 282.56011724472046, 286.5347583293915, 290.68982338905334, 294.9829041957855, 299.3552203178406, 303.7272837162018, 308.0758521556854, 312.3375415802002, 316.65093636512756, 320.8980255126953, 325.2164657115936, 329.5521056652069, 334.0722541809082, 338.5145502090454, 342.8997564315796, 347.1471815109253, 351.3618128299713, 355.80442070961, 360.24254393577576, 364.70861625671387, 369.109002828598, 373.3504271507263, 377.7874619960785, 382.25958824157715, 386.6551821231842, 391.09562969207764, 395.42405891418457, 399.85498237609863, 404.33044505119324, 408.8184254169464, 413.2162654399872, 417.5727093219757, 419.82385754585266]
[12.225, 12.2175, 12.2325, 12.27, 12.2675, 12.2675, 12.3, 12.335, 12.34, 12.3625, 12.395, 12.4325, 12.4675, 12.4925, 12.565, 12.605, 12.66, 12.7, 12.7475, 12.8, 12.84, 12.8825, 12.95, 13.0125, 13.0475, 13.1, 13.1675, 13.1925, 13.2375, 13.2975, 13.34, 13.4175, 13.45, 13.5175, 13.57, 13.6425, 13.67, 13.71, 13.74, 13.77, 13.795, 13.8275, 13.8525, 13.8875, 13.93, 13.955, 14.0175, 14.0425, 14.095, 14.12, 14.14, 14.1625, 14.215, 14.2375, 14.26, 14.34, 14.3825, 14.4325, 14.4275, 14.46, 14.4925, 14.51, 14.54, 14.58, 14.62, 14.65, 14.6975, 14.7425, 14.795, 14.8275, 14.87, 14.9025, 14.93, 14.9825, 15.065, 15.12, 15.2125, 15.27, 15.3125, 15.3825, 15.4325, 15.52, 15.565, 15.625, 15.66, 15.6975, 15.7275, 15.77, 15.81, 15.8525, 15.8875, 15.93, 15.965, 15.9625, 15.9825, 15.99, 16.0775, 16.1375, 16.1675, 16.2025, 16.4475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.220, Test accuracy: 42.68
Round   1, Train loss: 2.199, Test loss: 1.751, Test accuracy: 73.08
Round   2, Train loss: 1.724, Test loss: 1.598, Test accuracy: 88.33
Round   3, Train loss: 1.620, Test loss: 1.568, Test accuracy: 90.23
Round   4, Train loss: 1.591, Test loss: 1.555, Test accuracy: 91.16
Round   5, Train loss: 1.564, Test loss: 1.546, Test accuracy: 92.02
Round   6, Train loss: 1.555, Test loss: 1.540, Test accuracy: 92.47
Round   7, Train loss: 1.541, Test loss: 1.537, Test accuracy: 92.91
Round   8, Train loss: 1.534, Test loss: 1.532, Test accuracy: 93.20
Round   9, Train loss: 1.533, Test loss: 1.528, Test accuracy: 93.52
Round  10, Train loss: 1.527, Test loss: 1.526, Test accuracy: 93.64
Round  11, Train loss: 1.519, Test loss: 1.523, Test accuracy: 93.96
Round  12, Train loss: 1.514, Test loss: 1.522, Test accuracy: 94.11
Round  13, Train loss: 1.513, Test loss: 1.520, Test accuracy: 94.34
Round  14, Train loss: 1.507, Test loss: 1.519, Test accuracy: 94.31
Round  15, Train loss: 1.507, Test loss: 1.518, Test accuracy: 94.64
Round  16, Train loss: 1.505, Test loss: 1.516, Test accuracy: 94.73
Round  17, Train loss: 1.510, Test loss: 1.513, Test accuracy: 95.02
Round  18, Train loss: 1.500, Test loss: 1.513, Test accuracy: 95.01
Round  19, Train loss: 1.500, Test loss: 1.512, Test accuracy: 95.11
Round  20, Train loss: 1.498, Test loss: 1.509, Test accuracy: 95.39
Round  21, Train loss: 1.496, Test loss: 1.508, Test accuracy: 95.33
Round  22, Train loss: 1.492, Test loss: 1.508, Test accuracy: 95.47
Round  23, Train loss: 1.492, Test loss: 1.507, Test accuracy: 95.53
Round  24, Train loss: 1.492, Test loss: 1.506, Test accuracy: 95.54
Round  25, Train loss: 1.488, Test loss: 1.506, Test accuracy: 95.58
Round  26, Train loss: 1.489, Test loss: 1.505, Test accuracy: 95.73
Round  27, Train loss: 1.488, Test loss: 1.504, Test accuracy: 95.89
Round  28, Train loss: 1.489, Test loss: 1.503, Test accuracy: 95.96
Round  29, Train loss: 1.487, Test loss: 1.503, Test accuracy: 96.12
Round  30, Train loss: 1.487, Test loss: 1.502, Test accuracy: 96.06
Round  31, Train loss: 1.485, Test loss: 1.502, Test accuracy: 96.05
Round  32, Train loss: 1.486, Test loss: 1.502, Test accuracy: 96.09
Round  33, Train loss: 1.481, Test loss: 1.502, Test accuracy: 96.15
Round  34, Train loss: 1.482, Test loss: 1.501, Test accuracy: 96.22
Round  35, Train loss: 1.480, Test loss: 1.501, Test accuracy: 96.19
Round  36, Train loss: 1.479, Test loss: 1.500, Test accuracy: 96.22
Round  37, Train loss: 1.479, Test loss: 1.500, Test accuracy: 96.29
Round  38, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.13
Round  39, Train loss: 1.477, Test loss: 1.500, Test accuracy: 96.23
Round  40, Train loss: 1.478, Test loss: 1.500, Test accuracy: 96.29
Round  41, Train loss: 1.481, Test loss: 1.499, Test accuracy: 96.29
Round  42, Train loss: 1.476, Test loss: 1.499, Test accuracy: 96.36
Round  43, Train loss: 1.477, Test loss: 1.499, Test accuracy: 96.25
Round  44, Train loss: 1.478, Test loss: 1.499, Test accuracy: 96.39
Round  45, Train loss: 1.477, Test loss: 1.498, Test accuracy: 96.40
Round  46, Train loss: 1.476, Test loss: 1.498, Test accuracy: 96.44
Round  47, Train loss: 1.478, Test loss: 1.498, Test accuracy: 96.54
Round  48, Train loss: 1.476, Test loss: 1.497, Test accuracy: 96.53
Round  49, Train loss: 1.477, Test loss: 1.497, Test accuracy: 96.62
Round  50, Train loss: 1.474, Test loss: 1.497, Test accuracy: 96.55
Round  51, Train loss: 1.475, Test loss: 1.497, Test accuracy: 96.58
Round  52, Train loss: 1.475, Test loss: 1.496, Test accuracy: 96.69
Round  53, Train loss: 1.473, Test loss: 1.496, Test accuracy: 96.74
Round  54, Train loss: 1.473, Test loss: 1.496, Test accuracy: 96.73
Round  55, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.70
Round  56, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.72
Round  57, Train loss: 1.473, Test loss: 1.496, Test accuracy: 96.69
Round  58, Train loss: 1.472, Test loss: 1.496, Test accuracy: 96.62
Round  59, Train loss: 1.471, Test loss: 1.496, Test accuracy: 96.65
Round  60, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.73
Round  61, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.74
Round  62, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.67
Round  63, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.78
Round  64, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.80
Round  65, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.71
Round  66, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.83
Round  67, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.84
Round  68, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.83
Round  69, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.90
Round  70, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.84
Round  71, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.86
Round  72, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.82
Round  73, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.76
Round  74, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.81
Round  75, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.78
Round  76, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.81
Round  77, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.81
Round  78, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.83
Round  79, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.77
Round  80, Train loss: 1.471, Test loss: 1.494, Test accuracy: 96.85
Round  81, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.89
Round  82, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.87
Round  83, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.87
Round  84, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.73
Round  85, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.87
Round  86, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.91
Round  87, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.89
Round  88, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.91
Round  89, Train loss: 1.471, Test loss: 1.494, Test accuracy: 96.93
Round  90, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.85
Round  91, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.88
Round  92, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.92
Round  93, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.95
Round  94, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.92
Round  95, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.98
Round  96, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.89
Round  97, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.85
Round  98, Train loss: 1.471, Test loss: 1.493, Test accuracy: 96.97
Round  99, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.95
Final Round, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.99
Average accuracy final 10 rounds: 96.91599999999998
7568.788421154022
[10.400645732879639, 21.033329725265503, 31.60121726989746, 42.01387143135071, 52.216978311538696, 62.703754901885986, 72.99411201477051, 83.4039409160614, 93.90336441993713, 104.32038068771362, 114.65614771842957, 125.17689943313599, 135.53233003616333, 145.90452766418457, 156.3507022857666, 166.75912761688232, 177.24327397346497, 188.05440163612366, 198.82538533210754, 209.84946513175964, 220.93943548202515, 231.96623516082764, 243.14920473098755, 254.17529845237732, 265.07750177383423, 276.03759360313416, 287.0703778266907, 298.03928685188293, 308.7881922721863, 319.5102586746216, 330.32980370521545, 341.2198407649994, 352.28675270080566, 363.1371765136719, 373.97079157829285, 384.8643329143524, 395.8770225048065, 406.83095622062683, 417.6471893787384, 428.7022051811218, 439.7367510795593, 450.4644477367401, 461.4314908981323, 472.4893436431885, 483.32436752319336, 494.2890899181366, 505.2567648887634, 516.1287045478821, 527.0915229320526, 538.0130705833435, 548.9820890426636, 559.8215565681458, 570.7973005771637, 581.8587362766266, 592.7048270702362, 603.7256481647491, 614.7435595989227, 625.8616857528687, 636.8058354854584, 647.5802092552185, 658.36354804039, 668.8659517765045, 679.3173944950104, 689.0, 698.8709044456482, 708.8293833732605, 718.8767304420471, 729.003714799881, 738.900191783905, 748.8432967662811, 758.8224036693573, 768.2897372245789, 778.024665594101, 787.7561030387878, 797.4743659496307, 807.0979735851288, 816.7138469219208, 826.3242802619934, 835.9268169403076, 845.5569915771484, 855.1402578353882, 864.8057811260223, 874.4786920547485, 884.0901472568512, 893.7070627212524, 903.5717403888702, 913.101461648941, 922.7585093975067, 932.5826890468597, 942.3229472637177, 952.0033786296844, 961.7735786437988, 971.4976208209991, 981.1675295829773, 990.9303476810455, 1000.4617223739624, 1010.0297510623932, 1019.5969262123108, 1029.0900444984436, 1038.6316142082214, 1041.0613713264465]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[42.68, 73.085, 88.3325, 90.2325, 91.1575, 92.0225, 92.47, 92.9075, 93.205, 93.5225, 93.645, 93.96, 94.1075, 94.3375, 94.31, 94.635, 94.7325, 95.0225, 95.0075, 95.105, 95.3925, 95.325, 95.4675, 95.535, 95.5425, 95.575, 95.7275, 95.8925, 95.9575, 96.1175, 96.0575, 96.045, 96.0875, 96.1525, 96.225, 96.1875, 96.2175, 96.2875, 96.1325, 96.2275, 96.2875, 96.29, 96.3575, 96.25, 96.395, 96.3975, 96.445, 96.54, 96.53, 96.6175, 96.55, 96.5825, 96.685, 96.7425, 96.735, 96.7, 96.725, 96.6875, 96.625, 96.6525, 96.73, 96.7425, 96.6725, 96.7775, 96.795, 96.7075, 96.8275, 96.84, 96.83, 96.8975, 96.845, 96.855, 96.8225, 96.7575, 96.8125, 96.7825, 96.81, 96.815, 96.8325, 96.7725, 96.85, 96.89, 96.8725, 96.8725, 96.735, 96.8675, 96.905, 96.8875, 96.91, 96.9325, 96.8525, 96.88, 96.92, 96.9475, 96.925, 96.98, 96.885, 96.8475, 96.9675, 96.955, 96.99]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.302, Test accuracy: 9.42
Round   1, Train loss: 2.301, Test loss: 2.300, Test accuracy: 9.47
Round   2, Train loss: 2.300, Test loss: 2.299, Test accuracy: 9.57
Round   3, Train loss: 2.296, Test loss: 2.296, Test accuracy: 9.90
Round   4, Train loss: 2.295, Test loss: 2.293, Test accuracy: 10.27
Round   5, Train loss: 2.290, Test loss: 2.288, Test accuracy: 10.42
Round   6, Train loss: 2.280, Test loss: 2.276, Test accuracy: 10.43
Round   7, Train loss: 2.258, Test loss: 2.257, Test accuracy: 13.57
Round   8, Train loss: 2.239, Test loss: 2.245, Test accuracy: 27.52
Round   9, Train loss: 2.234, Test loss: 2.235, Test accuracy: 38.55
Round  10, Train loss: 2.222, Test loss: 2.225, Test accuracy: 46.30
Round  11, Train loss: 2.214, Test loss: 2.214, Test accuracy: 51.52
Round  12, Train loss: 2.205, Test loss: 2.198, Test accuracy: 55.35
Round  13, Train loss: 2.180, Test loss: 2.175, Test accuracy: 58.42
Round  14, Train loss: 2.135, Test loss: 2.123, Test accuracy: 56.93
Round  15, Train loss: 2.066, Test loss: 2.051, Test accuracy: 57.65
Round  16, Train loss: 1.996, Test loss: 1.996, Test accuracy: 59.63
Round  17, Train loss: 1.961, Test loss: 1.946, Test accuracy: 62.00
Round  18, Train loss: 1.919, Test loss: 1.910, Test accuracy: 63.97
Round  19, Train loss: 1.881, Test loss: 1.880, Test accuracy: 66.40
Round  20, Train loss: 1.858, Test loss: 1.851, Test accuracy: 68.82
Round  21, Train loss: 1.826, Test loss: 1.828, Test accuracy: 70.38
Round  22, Train loss: 1.816, Test loss: 1.807, Test accuracy: 71.88
Round  23, Train loss: 1.812, Test loss: 1.788, Test accuracy: 72.88
Round  24, Train loss: 1.769, Test loss: 1.774, Test accuracy: 74.25
Round  25, Train loss: 1.776, Test loss: 1.761, Test accuracy: 75.62
Round  26, Train loss: 1.752, Test loss: 1.751, Test accuracy: 76.82
Round  27, Train loss: 1.752, Test loss: 1.738, Test accuracy: 78.18
Round  28, Train loss: 1.723, Test loss: 1.728, Test accuracy: 79.17
Round  29, Train loss: 1.721, Test loss: 1.721, Test accuracy: 80.27
Round  30, Train loss: 1.725, Test loss: 1.712, Test accuracy: 80.53
Round  31, Train loss: 1.696, Test loss: 1.706, Test accuracy: 80.77
Round  32, Train loss: 1.686, Test loss: 1.700, Test accuracy: 81.20
Round  33, Train loss: 1.689, Test loss: 1.694, Test accuracy: 81.57
Round  34, Train loss: 1.684, Test loss: 1.690, Test accuracy: 81.80
Round  35, Train loss: 1.690, Test loss: 1.685, Test accuracy: 82.07
Round  36, Train loss: 1.659, Test loss: 1.681, Test accuracy: 82.40
Round  37, Train loss: 1.660, Test loss: 1.677, Test accuracy: 82.78
Round  38, Train loss: 1.655, Test loss: 1.669, Test accuracy: 83.87
Round  39, Train loss: 1.654, Test loss: 1.662, Test accuracy: 84.85
Round  40, Train loss: 1.648, Test loss: 1.654, Test accuracy: 85.73
Round  41, Train loss: 1.632, Test loss: 1.645, Test accuracy: 86.60
Round  42, Train loss: 1.616, Test loss: 1.641, Test accuracy: 87.03
Round  43, Train loss: 1.611, Test loss: 1.631, Test accuracy: 88.57
Round  44, Train loss: 1.607, Test loss: 1.625, Test accuracy: 88.98
Round  45, Train loss: 1.606, Test loss: 1.617, Test accuracy: 89.82
Round  46, Train loss: 1.595, Test loss: 1.612, Test accuracy: 90.08
Round  47, Train loss: 1.592, Test loss: 1.608, Test accuracy: 90.23
Round  48, Train loss: 1.585, Test loss: 1.605, Test accuracy: 90.30
Round  49, Train loss: 1.585, Test loss: 1.600, Test accuracy: 90.63
Round  50, Train loss: 1.576, Test loss: 1.597, Test accuracy: 90.80
Round  51, Train loss: 1.573, Test loss: 1.595, Test accuracy: 90.83
Round  52, Train loss: 1.585, Test loss: 1.591, Test accuracy: 91.12
Round  53, Train loss: 1.577, Test loss: 1.589, Test accuracy: 91.27
Round  54, Train loss: 1.575, Test loss: 1.587, Test accuracy: 91.25
Round  55, Train loss: 1.559, Test loss: 1.585, Test accuracy: 91.33
Round  56, Train loss: 1.562, Test loss: 1.583, Test accuracy: 91.58
Round  57, Train loss: 1.566, Test loss: 1.582, Test accuracy: 91.82
Round  58, Train loss: 1.553, Test loss: 1.581, Test accuracy: 91.77
Round  59, Train loss: 1.555, Test loss: 1.579, Test accuracy: 91.85
Round  60, Train loss: 1.553, Test loss: 1.577, Test accuracy: 91.85
Round  61, Train loss: 1.549, Test loss: 1.575, Test accuracy: 91.92
Round  62, Train loss: 1.555, Test loss: 1.574, Test accuracy: 92.17
Round  63, Train loss: 1.545, Test loss: 1.572, Test accuracy: 92.20
Round  64, Train loss: 1.555, Test loss: 1.571, Test accuracy: 92.28
Round  65, Train loss: 1.539, Test loss: 1.570, Test accuracy: 92.37
Round  66, Train loss: 1.542, Test loss: 1.569, Test accuracy: 92.57
Round  67, Train loss: 1.541, Test loss: 1.568, Test accuracy: 92.48
Round  68, Train loss: 1.533, Test loss: 1.567, Test accuracy: 92.65
Round  69, Train loss: 1.541, Test loss: 1.567, Test accuracy: 92.65
Round  70, Train loss: 1.539, Test loss: 1.566, Test accuracy: 92.70
Round  71, Train loss: 1.533, Test loss: 1.564, Test accuracy: 92.82
Round  72, Train loss: 1.543, Test loss: 1.563, Test accuracy: 92.93
Round  73, Train loss: 1.537, Test loss: 1.563, Test accuracy: 92.88
Round  74, Train loss: 1.532, Test loss: 1.562, Test accuracy: 92.93
Round  75, Train loss: 1.524, Test loss: 1.562, Test accuracy: 92.98
Round  76, Train loss: 1.528, Test loss: 1.560, Test accuracy: 93.03
Round  77, Train loss: 1.528, Test loss: 1.560, Test accuracy: 92.97
Round  78, Train loss: 1.532, Test loss: 1.559, Test accuracy: 93.07
Round  79, Train loss: 1.518, Test loss: 1.559, Test accuracy: 93.00
Round  80, Train loss: 1.524, Test loss: 1.557, Test accuracy: 93.30
Round  81, Train loss: 1.518, Test loss: 1.556, Test accuracy: 93.20
Round  82, Train loss: 1.530, Test loss: 1.556, Test accuracy: 93.32
Round  83, Train loss: 1.517, Test loss: 1.555, Test accuracy: 93.33
Round  84, Train loss: 1.522, Test loss: 1.554, Test accuracy: 93.37
Round  85, Train loss: 1.515, Test loss: 1.555, Test accuracy: 93.28
Round  86, Train loss: 1.521, Test loss: 1.554, Test accuracy: 93.47
Round  87, Train loss: 1.516, Test loss: 1.553, Test accuracy: 93.48
Round  88, Train loss: 1.521, Test loss: 1.552, Test accuracy: 93.45
Round  89, Train loss: 1.523, Test loss: 1.551, Test accuracy: 93.48
Round  90, Train loss: 1.523, Test loss: 1.551, Test accuracy: 93.48
Round  91, Train loss: 1.513, Test loss: 1.551, Test accuracy: 93.45
Round  92, Train loss: 1.517, Test loss: 1.550, Test accuracy: 93.45
Round  93, Train loss: 1.512, Test loss: 1.549, Test accuracy: 93.60
Round  94, Train loss: 1.515, Test loss: 1.549, Test accuracy: 93.52/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.504, Test loss: 1.548, Test accuracy: 93.63
Round  96, Train loss: 1.514, Test loss: 1.548, Test accuracy: 93.77
Round  97, Train loss: 1.513, Test loss: 1.547, Test accuracy: 93.77
Round  98, Train loss: 1.513, Test loss: 1.547, Test accuracy: 93.83
Round  99, Train loss: 1.516, Test loss: 1.547, Test accuracy: 93.83
Final Round, Train loss: 1.499, Test loss: 1.545, Test accuracy: 93.78
Average accuracy final 10 rounds: 93.63333333333333
676.8124577999115
[0.9251487255096436, 1.741847276687622, 2.527006149291992, 3.3106729984283447, 4.083233594894409, 4.91316294670105, 5.703510761260986, 6.46515154838562, 7.252104759216309, 8.02939248085022, 8.79038119316101, 9.546809911727905, 10.270069599151611, 11.020633935928345, 11.776048183441162, 12.533546924591064, 13.294961929321289, 14.053802490234375, 14.839960098266602, 15.597003698348999, 16.364869594573975, 17.129474878311157, 17.896178483963013, 18.65776753425598, 19.433483839035034, 20.18592143058777, 20.91315770149231, 21.665785312652588, 22.426446199417114, 23.183331727981567, 23.948986768722534, 24.723422288894653, 25.48666763305664, 26.247647762298584, 26.999661684036255, 27.75613808631897, 28.490603923797607, 29.233840703964233, 29.97525668144226, 30.737850427627563, 31.51589035987854, 32.27764582633972, 33.03347373008728, 33.8036527633667, 34.55300283432007, 35.317158699035645, 36.089603662490845, 36.84874153137207, 37.60018849372864, 38.37174940109253, 39.110379695892334, 39.84114098548889, 40.56570053100586, 41.31802201271057, 42.07544255256653, 42.84369087219238, 43.61069869995117, 44.348705768585205, 45.09496545791626, 45.83591032028198, 46.58703017234802, 47.333370208740234, 48.07592296600342, 48.845561265945435, 49.619220495224, 50.345619916915894, 51.08383536338806, 51.822569608688354, 52.58634376525879, 53.34138321876526, 54.107178926467896, 54.873284339904785, 55.623088121414185, 56.367921113967896, 57.12542724609375, 57.863396644592285, 58.60878300666809, 59.36707019805908, 60.112465381622314, 60.8577663898468, 61.609044313430786, 62.36127591133118, 63.10583806037903, 63.81950497627258, 64.54864573478699, 65.29154086112976, 66.03714036941528, 66.7812249660492, 67.46146988868713, 68.16230797767639, 68.85974335670471, 69.56741166114807, 70.2491238117218, 70.95781540870667, 71.6785397529602, 72.39885973930359, 73.10851573944092, 73.7930600643158, 74.49125266075134, 75.19414854049683, 76.29013276100159]
[9.416666666666666, 9.466666666666667, 9.566666666666666, 9.9, 10.266666666666667, 10.416666666666666, 10.433333333333334, 13.566666666666666, 27.516666666666666, 38.55, 46.3, 51.516666666666666, 55.35, 58.416666666666664, 56.93333333333333, 57.65, 59.63333333333333, 62.0, 63.96666666666667, 66.4, 68.81666666666666, 70.38333333333334, 71.88333333333334, 72.88333333333334, 74.25, 75.61666666666666, 76.81666666666666, 78.18333333333334, 79.16666666666667, 80.26666666666667, 80.53333333333333, 80.76666666666667, 81.2, 81.56666666666666, 81.8, 82.06666666666666, 82.4, 82.78333333333333, 83.86666666666666, 84.85, 85.73333333333333, 86.6, 87.03333333333333, 88.56666666666666, 88.98333333333333, 89.81666666666666, 90.08333333333333, 90.23333333333333, 90.3, 90.63333333333334, 90.8, 90.83333333333333, 91.11666666666666, 91.26666666666667, 91.25, 91.33333333333333, 91.58333333333333, 91.81666666666666, 91.76666666666667, 91.85, 91.85, 91.91666666666667, 92.16666666666667, 92.2, 92.28333333333333, 92.36666666666666, 92.56666666666666, 92.48333333333333, 92.65, 92.65, 92.7, 92.81666666666666, 92.93333333333334, 92.88333333333334, 92.93333333333334, 92.98333333333333, 93.03333333333333, 92.96666666666667, 93.06666666666666, 93.0, 93.3, 93.2, 93.31666666666666, 93.33333333333333, 93.36666666666666, 93.28333333333333, 93.46666666666667, 93.48333333333333, 93.45, 93.48333333333333, 93.48333333333333, 93.45, 93.45, 93.6, 93.51666666666667, 93.63333333333334, 93.76666666666667, 93.76666666666667, 93.83333333333333, 93.83333333333333, 93.78333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.322, Test loss: 2.301, Test accuracy: 12.83
Round   1, Train loss: 2.318, Test loss: 2.299, Test accuracy: 18.13
Round   2, Train loss: 2.313, Test loss: 2.296, Test accuracy: 20.57
Round   3, Train loss: 2.307, Test loss: 2.291, Test accuracy: 18.18
Round   4, Train loss: 2.300, Test loss: 2.285, Test accuracy: 21.82
Round   5, Train loss: 2.291, Test loss: 2.275, Test accuracy: 25.78
Round   6, Train loss: 2.273, Test loss: 2.255, Test accuracy: 27.32
Round   7, Train loss: 2.256, Test loss: 2.229, Test accuracy: 39.85
Round   8, Train loss: 2.220, Test loss: 2.186, Test accuracy: 44.65
Round   9, Train loss: 2.160, Test loss: 2.125, Test accuracy: 46.52
Round  10, Train loss: 2.089, Test loss: 2.080, Test accuracy: 52.83
Round  11, Train loss: 2.078, Test loss: 2.039, Test accuracy: 56.82
Round  12, Train loss: 2.038, Test loss: 2.007, Test accuracy: 61.00
Round  13, Train loss: 2.019, Test loss: 1.973, Test accuracy: 63.15
Round  14, Train loss: 1.962, Test loss: 1.945, Test accuracy: 65.48
Round  15, Train loss: 1.946, Test loss: 1.919, Test accuracy: 66.55
Round  16, Train loss: 1.922, Test loss: 1.897, Test accuracy: 67.68
Round  17, Train loss: 1.927, Test loss: 1.882, Test accuracy: 68.13
Round  18, Train loss: 1.902, Test loss: 1.864, Test accuracy: 69.52
Round  19, Train loss: 1.884, Test loss: 1.853, Test accuracy: 70.15
Round  20, Train loss: 1.861, Test loss: 1.840, Test accuracy: 71.45
Round  21, Train loss: 1.860, Test loss: 1.835, Test accuracy: 72.10
Round  22, Train loss: 1.860, Test loss: 1.821, Test accuracy: 73.02
Round  23, Train loss: 1.835, Test loss: 1.811, Test accuracy: 73.52
Round  24, Train loss: 1.827, Test loss: 1.805, Test accuracy: 74.45
Round  25, Train loss: 1.813, Test loss: 1.798, Test accuracy: 74.70
Round  26, Train loss: 1.819, Test loss: 1.785, Test accuracy: 75.68
Round  27, Train loss: 1.812, Test loss: 1.776, Test accuracy: 76.67
Round  28, Train loss: 1.778, Test loss: 1.769, Test accuracy: 78.23
Round  29, Train loss: 1.772, Test loss: 1.760, Test accuracy: 79.38
Round  30, Train loss: 1.783, Test loss: 1.750, Test accuracy: 80.20
Round  31, Train loss: 1.760, Test loss: 1.746, Test accuracy: 80.83
Round  32, Train loss: 1.752, Test loss: 1.739, Test accuracy: 81.18
Round  33, Train loss: 1.763, Test loss: 1.732, Test accuracy: 81.72
Round  34, Train loss: 1.738, Test loss: 1.728, Test accuracy: 82.15
Round  35, Train loss: 1.738, Test loss: 1.722, Test accuracy: 82.50
Round  36, Train loss: 1.739, Test loss: 1.717, Test accuracy: 82.72
Round  37, Train loss: 1.729, Test loss: 1.715, Test accuracy: 83.02
Round  38, Train loss: 1.727, Test loss: 1.711, Test accuracy: 83.17
Round  39, Train loss: 1.709, Test loss: 1.706, Test accuracy: 83.37
Round  40, Train loss: 1.709, Test loss: 1.704, Test accuracy: 83.45
Round  41, Train loss: 1.713, Test loss: 1.701, Test accuracy: 83.68
Round  42, Train loss: 1.713, Test loss: 1.694, Test accuracy: 83.73
Round  43, Train loss: 1.718, Test loss: 1.692, Test accuracy: 83.78
Round  44, Train loss: 1.722, Test loss: 1.684, Test accuracy: 83.88
Round  45, Train loss: 1.694, Test loss: 1.685, Test accuracy: 84.15
Round  46, Train loss: 1.727, Test loss: 1.677, Test accuracy: 84.33
Round  47, Train loss: 1.679, Test loss: 1.681, Test accuracy: 84.37
Round  48, Train loss: 1.698, Test loss: 1.676, Test accuracy: 84.42
Round  49, Train loss: 1.701, Test loss: 1.671, Test accuracy: 84.47
Round  50, Train loss: 1.681, Test loss: 1.670, Test accuracy: 84.65
Round  51, Train loss: 1.682, Test loss: 1.669, Test accuracy: 84.57
Round  52, Train loss: 1.674, Test loss: 1.668, Test accuracy: 84.72
Round  53, Train loss: 1.670, Test loss: 1.667, Test accuracy: 84.80
Round  54, Train loss: 1.689, Test loss: 1.662, Test accuracy: 85.23
Round  55, Train loss: 1.676, Test loss: 1.659, Test accuracy: 85.27
Round  56, Train loss: 1.683, Test loss: 1.655, Test accuracy: 86.12
Round  57, Train loss: 1.662, Test loss: 1.653, Test accuracy: 86.78
Round  58, Train loss: 1.655, Test loss: 1.649, Test accuracy: 87.83
Round  59, Train loss: 1.654, Test loss: 1.646, Test accuracy: 88.97
Round  60, Train loss: 1.648, Test loss: 1.637, Test accuracy: 89.97
Round  61, Train loss: 1.636, Test loss: 1.631, Test accuracy: 90.50
Round  62, Train loss: 1.640, Test loss: 1.626, Test accuracy: 91.10
Round  63, Train loss: 1.627, Test loss: 1.620, Test accuracy: 91.43
Round  64, Train loss: 1.612, Test loss: 1.617, Test accuracy: 91.78
Round  65, Train loss: 1.620, Test loss: 1.613, Test accuracy: 91.83
Round  66, Train loss: 1.616, Test loss: 1.607, Test accuracy: 92.12
Round  67, Train loss: 1.612, Test loss: 1.604, Test accuracy: 92.40
Round  68, Train loss: 1.607, Test loss: 1.601, Test accuracy: 92.43
Round  69, Train loss: 1.599, Test loss: 1.599, Test accuracy: 92.57
Round  70, Train loss: 1.600, Test loss: 1.596, Test accuracy: 92.65
Round  71, Train loss: 1.597, Test loss: 1.595, Test accuracy: 92.62
Round  72, Train loss: 1.588, Test loss: 1.595, Test accuracy: 92.82
Round  73, Train loss: 1.592, Test loss: 1.592, Test accuracy: 92.98
Round  74, Train loss: 1.595, Test loss: 1.590, Test accuracy: 93.07
Round  75, Train loss: 1.586, Test loss: 1.588, Test accuracy: 93.02
Round  76, Train loss: 1.578, Test loss: 1.589, Test accuracy: 93.12
Round  77, Train loss: 1.591, Test loss: 1.586, Test accuracy: 93.10
Round  78, Train loss: 1.589, Test loss: 1.585, Test accuracy: 93.15
Round  79, Train loss: 1.582, Test loss: 1.584, Test accuracy: 93.25
Round  80, Train loss: 1.579, Test loss: 1.583, Test accuracy: 93.37
Round  81, Train loss: 1.578, Test loss: 1.581, Test accuracy: 93.40
Round  82, Train loss: 1.577, Test loss: 1.580, Test accuracy: 93.43
Round  83, Train loss: 1.568, Test loss: 1.580, Test accuracy: 93.43
Round  84, Train loss: 1.574, Test loss: 1.580, Test accuracy: 93.50
Round  85, Train loss: 1.565, Test loss: 1.580, Test accuracy: 93.48
Round  86, Train loss: 1.570, Test loss: 1.578, Test accuracy: 93.43
Round  87, Train loss: 1.569, Test loss: 1.576, Test accuracy: 93.70
Round  88, Train loss: 1.565, Test loss: 1.577, Test accuracy: 93.73
Round  89, Train loss: 1.560, Test loss: 1.577, Test accuracy: 93.68
Round  90, Train loss: 1.567, Test loss: 1.574, Test accuracy: 93.63
Round  91, Train loss: 1.564, Test loss: 1.574, Test accuracy: 93.67
Round  92, Train loss: 1.562, Test loss: 1.572, Test accuracy: 93.70
Round  93, Train loss: 1.565, Test loss: 1.573, Test accuracy: 93.85/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.561, Test loss: 1.571, Test accuracy: 93.78
Round  95, Train loss: 1.555, Test loss: 1.571, Test accuracy: 93.93
Round  96, Train loss: 1.555, Test loss: 1.570, Test accuracy: 93.95
Round  97, Train loss: 1.561, Test loss: 1.570, Test accuracy: 94.13
Round  98, Train loss: 1.553, Test loss: 1.569, Test accuracy: 94.08
Round  99, Train loss: 1.557, Test loss: 1.568, Test accuracy: 94.08
Final Round, Train loss: 1.518, Test loss: 1.562, Test accuracy: 94.13
Average accuracy final 10 rounds: 93.88166666666666
858.0433385372162
[0.8737804889678955, 1.747560977935791, 2.4758996963500977, 3.2042384147644043, 3.963881731033325, 4.723525047302246, 5.5050365924835205, 6.286548137664795, 7.059857606887817, 7.83316707611084, 8.589112281799316, 9.345057487487793, 10.103520631790161, 10.86198377609253, 11.591235399246216, 12.320487022399902, 13.087521076202393, 13.854555130004883, 14.642961263656616, 15.43136739730835, 16.226981163024902, 17.022594928741455, 17.76557755470276, 18.508560180664062, 19.241358041763306, 19.97415590286255, 20.72872304916382, 21.483290195465088, 22.26089096069336, 23.03849172592163, 23.81898522377014, 24.599478721618652, 25.378936052322388, 26.158393383026123, 26.926507472991943, 27.694621562957764, 28.46446180343628, 29.234302043914795, 30.012076139450073, 30.78985023498535, 31.571584224700928, 32.353318214416504, 33.14650201797485, 33.9396858215332, 34.69946503639221, 35.45924425125122, 36.25295376777649, 37.04666328430176, 37.843743324279785, 38.64082336425781, 39.445083141326904, 40.249342918395996, 41.030147075653076, 41.810951232910156, 42.573774576187134, 43.33659791946411, 44.09491419792175, 44.853230476379395, 45.63233542442322, 46.41144037246704, 47.20237684249878, 47.99331331253052, 48.7888925075531, 49.584471702575684, 50.37432289123535, 51.16417407989502, 51.94194579124451, 52.719717502593994, 53.46889686584473, 54.21807622909546, 54.93921685218811, 55.66035747528076, 56.38332557678223, 57.10629367828369, 57.83198666572571, 58.557679653167725, 59.27602767944336, 59.994375705718994, 60.7154335975647, 61.4364914894104, 62.16130352020264, 62.88611555099487, 63.58380055427551, 64.28148555755615, 64.98097681999207, 65.68046808242798, 66.39361381530762, 67.10675954818726, 67.84094333648682, 68.57512712478638, 69.29901075363159, 70.0228943824768, 70.75365424156189, 71.48441410064697, 72.22306394577026, 72.96171379089355, 73.71229338645935, 74.46287298202515, 75.20677423477173, 75.95067548751831, 76.67313742637634, 77.39559936523438, 78.11536478996277, 78.83513021469116, 79.53046131134033, 80.2257924079895, 80.96438026428223, 81.70296812057495, 82.4795892238617, 83.25621032714844, 84.01735758781433, 84.77850484848022, 85.50463008880615, 86.23075532913208, 86.94307923316956, 87.65540313720703, 88.37753510475159, 89.09966707229614, 89.82535409927368, 90.55104112625122, 91.28803396224976, 92.02502679824829, 92.7734215259552, 93.52181625366211, 94.22274947166443, 94.92368268966675, 95.62598299980164, 96.32828330993652, 97.0617516040802, 97.79521989822388, 98.50951600074768, 99.22381210327148, 99.93380331993103, 100.64379453659058, 101.33266282081604, 102.0215311050415, 102.72445273399353, 103.42737436294556, 104.1389045715332, 104.85043478012085, 105.56707763671875, 106.28372049331665, 107.01989555358887, 107.75607061386108, 108.46524357795715, 109.17441654205322, 109.8530786037445, 110.53174066543579, 111.22314405441284, 111.91454744338989, 112.6114604473114, 113.30837345123291, 114.02728199958801, 114.74619054794312, 115.49472737312317, 116.24326419830322, 116.98592448234558, 117.72858476638794, 118.43555641174316, 119.14252805709839, 119.83635568618774, 120.5301833152771, 121.2167649269104, 121.9033465385437, 122.61492681503296, 123.32650709152222, 124.03275060653687, 124.73899412155151, 125.44723796844482, 126.15548181533813, 126.87783455848694, 127.60018730163574, 128.30365753173828, 129.00712776184082, 129.71974062919617, 130.4323534965515, 131.1389467716217, 131.8455400466919, 132.57454705238342, 133.30355405807495, 134.0115773677826, 134.71960067749023, 135.4273874759674, 136.13517427444458, 136.8380024433136, 137.54083061218262, 138.26712083816528, 138.99341106414795, 139.70379042625427, 140.4141697883606, 141.1364095211029, 141.85864925384521, 142.54941606521606, 143.2401828765869, 143.95094513893127, 144.66170740127563, 145.3133225440979, 145.96493768692017, 146.6865119934082, 147.40808629989624, 148.50809240341187, 149.6080985069275]
[12.833333333333334, 12.833333333333334, 18.133333333333333, 18.133333333333333, 20.566666666666666, 20.566666666666666, 18.183333333333334, 18.183333333333334, 21.816666666666666, 21.816666666666666, 25.783333333333335, 25.783333333333335, 27.316666666666666, 27.316666666666666, 39.85, 39.85, 44.65, 44.65, 46.516666666666666, 46.516666666666666, 52.833333333333336, 52.833333333333336, 56.81666666666667, 56.81666666666667, 61.0, 61.0, 63.15, 63.15, 65.48333333333333, 65.48333333333333, 66.55, 66.55, 67.68333333333334, 67.68333333333334, 68.13333333333334, 68.13333333333334, 69.51666666666667, 69.51666666666667, 70.15, 70.15, 71.45, 71.45, 72.1, 72.1, 73.01666666666667, 73.01666666666667, 73.51666666666667, 73.51666666666667, 74.45, 74.45, 74.7, 74.7, 75.68333333333334, 75.68333333333334, 76.66666666666667, 76.66666666666667, 78.23333333333333, 78.23333333333333, 79.38333333333334, 79.38333333333334, 80.2, 80.2, 80.83333333333333, 80.83333333333333, 81.18333333333334, 81.18333333333334, 81.71666666666667, 81.71666666666667, 82.15, 82.15, 82.5, 82.5, 82.71666666666667, 82.71666666666667, 83.01666666666667, 83.01666666666667, 83.16666666666667, 83.16666666666667, 83.36666666666666, 83.36666666666666, 83.45, 83.45, 83.68333333333334, 83.68333333333334, 83.73333333333333, 83.73333333333333, 83.78333333333333, 83.78333333333333, 83.88333333333334, 83.88333333333334, 84.15, 84.15, 84.33333333333333, 84.33333333333333, 84.36666666666666, 84.36666666666666, 84.41666666666667, 84.41666666666667, 84.46666666666667, 84.46666666666667, 84.65, 84.65, 84.56666666666666, 84.56666666666666, 84.71666666666667, 84.71666666666667, 84.8, 84.8, 85.23333333333333, 85.23333333333333, 85.26666666666667, 85.26666666666667, 86.11666666666666, 86.11666666666666, 86.78333333333333, 86.78333333333333, 87.83333333333333, 87.83333333333333, 88.96666666666667, 88.96666666666667, 89.96666666666667, 89.96666666666667, 90.5, 90.5, 91.1, 91.1, 91.43333333333334, 91.43333333333334, 91.78333333333333, 91.78333333333333, 91.83333333333333, 91.83333333333333, 92.11666666666666, 92.11666666666666, 92.4, 92.4, 92.43333333333334, 92.43333333333334, 92.56666666666666, 92.56666666666666, 92.65, 92.65, 92.61666666666666, 92.61666666666666, 92.81666666666666, 92.81666666666666, 92.98333333333333, 92.98333333333333, 93.06666666666666, 93.06666666666666, 93.01666666666667, 93.01666666666667, 93.11666666666666, 93.11666666666666, 93.1, 93.1, 93.15, 93.15, 93.25, 93.25, 93.36666666666666, 93.36666666666666, 93.4, 93.4, 93.43333333333334, 93.43333333333334, 93.43333333333334, 93.43333333333334, 93.5, 93.5, 93.48333333333333, 93.48333333333333, 93.43333333333334, 93.43333333333334, 93.7, 93.7, 93.73333333333333, 93.73333333333333, 93.68333333333334, 93.68333333333334, 93.63333333333334, 93.63333333333334, 93.66666666666667, 93.66666666666667, 93.7, 93.7, 93.85, 93.85, 93.78333333333333, 93.78333333333333, 93.93333333333334, 93.93333333333334, 93.95, 93.95, 94.13333333333334, 94.13333333333334, 94.08333333333333, 94.08333333333333, 94.08333333333333, 94.08333333333333, 94.13333333333334, 94.13333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.186, Test loss: 2.171, Test accuracy: 28.50
Round   0, Global train loss: 2.186, Global test loss: 2.274, Global test accuracy: 20.52
Round   1, Train loss: 1.686, Test loss: 1.939, Test accuracy: 55.91
Round   1, Global train loss: 1.686, Global test loss: 2.167, Global test accuracy: 33.37
Round   2, Train loss: 1.567, Test loss: 1.857, Test accuracy: 61.40
Round   2, Global train loss: 1.567, Global test loss: 2.178, Global test accuracy: 30.66
Round   3, Train loss: 1.552, Test loss: 1.740, Test accuracy: 73.11
Round   3, Global train loss: 1.552, Global test loss: 2.114, Global test accuracy: 34.84
Round   4, Train loss: 1.525, Test loss: 1.693, Test accuracy: 76.99
Round   4, Global train loss: 1.525, Global test loss: 2.144, Global test accuracy: 28.48
Round   5, Train loss: 1.593, Test loss: 1.616, Test accuracy: 85.71
Round   5, Global train loss: 1.593, Global test loss: 2.088, Global test accuracy: 39.85
Round   6, Train loss: 1.511, Test loss: 1.585, Test accuracy: 88.67
Round   6, Global train loss: 1.511, Global test loss: 2.100, Global test accuracy: 38.59
Round   7, Train loss: 1.488, Test loss: 1.559, Test accuracy: 90.92
Round   7, Global train loss: 1.488, Global test loss: 2.021, Global test accuracy: 49.27
Round   8, Train loss: 1.481, Test loss: 1.547, Test accuracy: 92.28
Round   8, Global train loss: 1.481, Global test loss: 2.108, Global test accuracy: 35.34
Round   9, Train loss: 1.477, Test loss: 1.552, Test accuracy: 92.20
Round   9, Global train loss: 1.477, Global test loss: 2.090, Global test accuracy: 43.85
Round  10, Train loss: 1.474, Test loss: 1.560, Test accuracy: 90.30
Round  10, Global train loss: 1.474, Global test loss: 2.145, Global test accuracy: 34.37
Round  11, Train loss: 1.510, Test loss: 1.520, Test accuracy: 94.57
Round  11, Global train loss: 1.510, Global test loss: 2.038, Global test accuracy: 44.80
Round  12, Train loss: 1.478, Test loss: 1.520, Test accuracy: 94.62
Round  12, Global train loss: 1.478, Global test loss: 2.136, Global test accuracy: 31.29
Round  13, Train loss: 1.478, Test loss: 1.518, Test accuracy: 94.72
Round  13, Global train loss: 1.478, Global test loss: 2.156, Global test accuracy: 32.10
Round  14, Train loss: 1.522, Test loss: 1.510, Test accuracy: 95.88
Round  14, Global train loss: 1.522, Global test loss: 2.071, Global test accuracy: 38.77
Round  15, Train loss: 1.484, Test loss: 1.503, Test accuracy: 96.30
Round  15, Global train loss: 1.484, Global test loss: 2.203, Global test accuracy: 23.44
Round  16, Train loss: 1.470, Test loss: 1.503, Test accuracy: 96.28
Round  16, Global train loss: 1.470, Global test loss: 2.068, Global test accuracy: 41.11
Round  17, Train loss: 1.471, Test loss: 1.502, Test accuracy: 96.35
Round  17, Global train loss: 1.471, Global test loss: 2.140, Global test accuracy: 27.43
Round  18, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.37
Round  18, Global train loss: 1.476, Global test loss: 2.069, Global test accuracy: 41.37
Round  19, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.44
Round  19, Global train loss: 1.474, Global test loss: 2.132, Global test accuracy: 34.66
Round  20, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.47
Round  20, Global train loss: 1.468, Global test loss: 2.140, Global test accuracy: 30.47
Round  21, Train loss: 1.468, Test loss: 1.500, Test accuracy: 96.52
Round  21, Global train loss: 1.468, Global test loss: 2.111, Global test accuracy: 37.48
Round  22, Train loss: 1.467, Test loss: 1.500, Test accuracy: 96.53
Round  22, Global train loss: 1.467, Global test loss: 2.020, Global test accuracy: 48.24
Round  23, Train loss: 1.469, Test loss: 1.500, Test accuracy: 96.50
Round  23, Global train loss: 1.469, Global test loss: 2.028, Global test accuracy: 46.79
Round  24, Train loss: 1.468, Test loss: 1.500, Test accuracy: 96.50
Round  24, Global train loss: 1.468, Global test loss: 2.028, Global test accuracy: 46.95
Round  25, Train loss: 1.466, Test loss: 1.499, Test accuracy: 96.54
Round  25, Global train loss: 1.466, Global test loss: 2.079, Global test accuracy: 37.54
Round  26, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.56
Round  26, Global train loss: 1.472, Global test loss: 2.096, Global test accuracy: 33.13
Round  27, Train loss: 1.468, Test loss: 1.499, Test accuracy: 96.57
Round  27, Global train loss: 1.468, Global test loss: 2.053, Global test accuracy: 44.39
Round  28, Train loss: 1.468, Test loss: 1.499, Test accuracy: 96.58
Round  28, Global train loss: 1.468, Global test loss: 2.053, Global test accuracy: 39.73
Round  29, Train loss: 1.467, Test loss: 1.499, Test accuracy: 96.58
Round  29, Global train loss: 1.467, Global test loss: 2.077, Global test accuracy: 40.88
Round  30, Train loss: 1.468, Test loss: 1.499, Test accuracy: 96.58
Round  30, Global train loss: 1.468, Global test loss: 2.144, Global test accuracy: 30.67
Round  31, Train loss: 1.469, Test loss: 1.499, Test accuracy: 96.56
Round  31, Global train loss: 1.469, Global test loss: 2.156, Global test accuracy: 30.18
Round  32, Train loss: 1.470, Test loss: 1.499, Test accuracy: 96.58
Round  32, Global train loss: 1.470, Global test loss: 2.108, Global test accuracy: 33.59
Round  33, Train loss: 1.467, Test loss: 1.499, Test accuracy: 96.56
Round  33, Global train loss: 1.467, Global test loss: 2.102, Global test accuracy: 33.41
Round  34, Train loss: 1.469, Test loss: 1.499, Test accuracy: 96.53
Round  34, Global train loss: 1.469, Global test loss: 2.136, Global test accuracy: 29.73
Round  35, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.55
Round  35, Global train loss: 1.469, Global test loss: 2.122, Global test accuracy: 37.06
Round  36, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.55
Round  36, Global train loss: 1.468, Global test loss: 2.064, Global test accuracy: 45.28
Round  37, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.54
Round  37, Global train loss: 1.468, Global test loss: 2.073, Global test accuracy: 36.53
Round  38, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.55
Round  38, Global train loss: 1.467, Global test loss: 2.113, Global test accuracy: 36.00
Round  39, Train loss: 1.465, Test loss: 1.498, Test accuracy: 96.57
Round  39, Global train loss: 1.465, Global test loss: 2.142, Global test accuracy: 30.05
Round  40, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.58
Round  40, Global train loss: 1.467, Global test loss: 2.126, Global test accuracy: 28.94
Round  41, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.58
Round  41, Global train loss: 1.469, Global test loss: 2.096, Global test accuracy: 55.67
Round  42, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.57
Round  42, Global train loss: 1.468, Global test loss: 2.075, Global test accuracy: 38.70
Round  43, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.57
Round  43, Global train loss: 1.466, Global test loss: 2.125, Global test accuracy: 32.57
Round  44, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.57
Round  44, Global train loss: 1.468, Global test loss: 2.161, Global test accuracy: 27.22
Round  45, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.58
Round  45, Global train loss: 1.469, Global test loss: 2.128, Global test accuracy: 37.83
Round  46, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.58
Round  46, Global train loss: 1.468, Global test loss: 2.064, Global test accuracy: 39.18
Round  47, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.61
Round  47, Global train loss: 1.470, Global test loss: 2.034, Global test accuracy: 43.96
Round  48, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.61
Round  48, Global train loss: 1.471, Global test loss: 2.184, Global test accuracy: 26.53
Round  49, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.62
Round  49, Global train loss: 1.467, Global test loss: 2.150, Global test accuracy: 29.63
Round  50, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.60
Round  50, Global train loss: 1.469, Global test loss: 2.117, Global test accuracy: 34.22
Round  51, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.60
Round  51, Global train loss: 1.469, Global test loss: 2.219, Global test accuracy: 20.49
Round  52, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.61
Round  52, Global train loss: 1.467, Global test loss: 2.070, Global test accuracy: 38.01
Round  53, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.61
Round  53, Global train loss: 1.466, Global test loss: 2.100, Global test accuracy: 36.23
Round  54, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.58
Round  54, Global train loss: 1.467, Global test loss: 2.055, Global test accuracy: 45.15
Round  55, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.58
Round  55, Global train loss: 1.466, Global test loss: 2.077, Global test accuracy: 39.21
Round  56, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.58
Round  56, Global train loss: 1.466, Global test loss: 2.091, Global test accuracy: 37.77
Round  57, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.58
Round  57, Global train loss: 1.468, Global test loss: 2.067, Global test accuracy: 38.52
Round  58, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.58
Round  58, Global train loss: 1.467, Global test loss: 2.134, Global test accuracy: 30.12
Round  59, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.58
Round  59, Global train loss: 1.467, Global test loss: 2.119, Global test accuracy: 32.23
Round  60, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.59
Round  60, Global train loss: 1.469, Global test loss: 2.130, Global test accuracy: 30.82
Round  61, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.59
Round  61, Global train loss: 1.465, Global test loss: 2.109, Global test accuracy: 37.59
Round  62, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.58
Round  62, Global train loss: 1.466, Global test loss: 2.166, Global test accuracy: 27.90
Round  63, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.59
Round  63, Global train loss: 1.467, Global test loss: 2.197, Global test accuracy: 22.32
Round  64, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.59
Round  64, Global train loss: 1.469, Global test loss: 2.064, Global test accuracy: 39.97
Round  65, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.61
Round  65, Global train loss: 1.467, Global test loss: 2.136, Global test accuracy: 33.14
Round  66, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  66, Global train loss: 1.469, Global test loss: 2.092, Global test accuracy: 39.20
Round  67, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  67, Global train loss: 1.469, Global test loss: 2.079, Global test accuracy: 35.61
Round  68, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.61
Round  68, Global train loss: 1.465, Global test loss: 2.078, Global test accuracy: 42.75
Round  69, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.61
Round  69, Global train loss: 1.467, Global test loss: 2.056, Global test accuracy: 40.88
Round  70, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.61
Round  70, Global train loss: 1.466, Global test loss: 2.056, Global test accuracy: 38.12
Round  71, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.61
Round  71, Global train loss: 1.467, Global test loss: 2.017, Global test accuracy: 44.27
Round  72, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  72, Global train loss: 1.469, Global test loss: 2.099, Global test accuracy: 35.87
Round  73, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.61
Round  73, Global train loss: 1.466, Global test loss: 2.161, Global test accuracy: 27.11
Round  74, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.62
Round  74, Global train loss: 1.467, Global test loss: 2.009, Global test accuracy: 43.82
Round  75, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.62
Round  75, Global train loss: 1.466, Global test loss: 2.076, Global test accuracy: 53.15
Round  76, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.61
Round  76, Global train loss: 1.465, Global test loss: 2.188, Global test accuracy: 25.25
Round  77, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.61
Round  77, Global train loss: 1.470, Global test loss: 2.052, Global test accuracy: 39.21
Round  78, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.61
Round  78, Global train loss: 1.464, Global test loss: 2.085, Global test accuracy: 38.79
Round  79, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  79, Global train loss: 1.469, Global test loss: 2.116, Global test accuracy: 31.32
Round  80, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.61
Round  80, Global train loss: 1.468, Global test loss: 2.082, Global test accuracy: 43.28
Round  81, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.61
Round  81, Global train loss: 1.467, Global test loss: 2.048, Global test accuracy: 49.05
Round  82, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.62
Round  82, Global train loss: 1.468, Global test loss: 2.114, Global test accuracy: 30.50
Round  83, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.62
Round  83, Global train loss: 1.466, Global test loss: 2.143, Global test accuracy: 31.82
Round  84, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.62
Round  84, Global train loss: 1.468, Global test loss: 2.048, Global test accuracy: 43.21
Round  85, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.61
Round  85, Global train loss: 1.467, Global test loss: 2.002, Global test accuracy: 46.85
Round  86, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.62
Round  86, Global train loss: 1.467, Global test loss: 2.098, Global test accuracy: 38.83
Round  87, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.61
Round  87, Global train loss: 1.468, Global test loss: 2.168, Global test accuracy: 24.28
Round  88, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.62
Round  88, Global train loss: 1.465, Global test loss: 2.046, Global test accuracy: 41.49
Round  89, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.62
Round  89, Global train loss: 1.467, Global test loss: 2.076, Global test accuracy: 42.26
Round  90, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.62
Round  90, Global train loss: 1.468, Global test loss: 2.038, Global test accuracy: 44.98
Round  91, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.59
Round  91, Global train loss: 1.469, Global test loss: 2.168, Global test accuracy: 26.40
Round  92, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.59
Round  92, Global train loss: 1.468, Global test loss: 2.133, Global test accuracy: 29.19
Round  93, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.61
Round  93, Global train loss: 1.464, Global test loss: 2.206, Global test accuracy: 21.81
Round  94, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.61
Round  94, Global train loss: 1.465, Global test loss: 2.048, Global test accuracy: 41.97
Round  95, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.62
Round  95, Global train loss: 1.469, Global test loss: 2.093, Global test accuracy: 36.51/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.63
Round  96, Global train loss: 1.466, Global test loss: 2.179, Global test accuracy: 23.06
Round  97, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.63
Round  97, Global train loss: 1.467, Global test loss: 2.057, Global test accuracy: 56.13
Round  98, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.63
Round  98, Global train loss: 1.467, Global test loss: 1.982, Global test accuracy: 55.11
Round  99, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.63
Round  99, Global train loss: 1.468, Global test loss: 2.154, Global test accuracy: 29.73
Final Round, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.64
Final Round, Global train loss: 1.467, Global test loss: 2.154, Global test accuracy: 29.73
Average accuracy final 10 rounds: 96.61666666666665 

Average global accuracy final 10 rounds: 36.48916666666667 

1862.498660326004
[1.2731776237487793, 2.5463552474975586, 3.6265242099761963, 4.706693172454834, 5.7435760498046875, 6.780458927154541, 7.869782209396362, 8.959105491638184, 10.022645473480225, 11.086185455322266, 12.0753173828125, 13.064449310302734, 14.141101837158203, 15.217754364013672, 16.272948026657104, 17.328141689300537, 18.368265628814697, 19.408389568328857, 20.45296025276184, 21.497530937194824, 23.160943269729614, 24.824355602264404, 25.908575773239136, 26.992795944213867, 28.01714539527893, 29.041494846343994, 30.04773473739624, 31.053974628448486, 32.10659146308899, 33.15920829772949, 34.18903422355652, 35.218860149383545, 36.273696422576904, 37.328532695770264, 38.394959688186646, 39.46138668060303, 40.50674486160278, 41.55210304260254, 42.66410994529724, 43.77611684799194, 44.85891795158386, 45.94171905517578, 46.991960525512695, 48.04220199584961, 49.104514598846436, 50.16682720184326, 51.205541133880615, 52.24425506591797, 53.31991124153137, 54.395567417144775, 55.43202567100525, 56.46848392486572, 57.50941729545593, 58.55035066604614, 59.62363409996033, 60.69691753387451, 61.71460819244385, 62.732298851013184, 63.7828574180603, 64.83341598510742, 65.87073230743408, 66.90804862976074, 67.9386260509491, 68.96920347213745, 70.0154287815094, 71.06165409088135, 72.11850500106812, 73.17535591125488, 74.21435546875, 75.25335502624512, 76.31404399871826, 77.3747329711914, 78.42935848236084, 79.48398399353027, 80.55907773971558, 81.63417148590088, 82.69682145118713, 83.75947141647339, 84.79937028884888, 85.83926916122437, 86.90406608581543, 87.9688630104065, 88.98909378051758, 90.00932455062866, 91.03452372550964, 92.05972290039062, 93.12180829048157, 94.18389368057251, 95.23370385169983, 96.28351402282715, 97.34971141815186, 98.41590881347656, 99.49414396286011, 100.57237911224365, 101.62661838531494, 102.68085765838623, 103.71662998199463, 104.75240230560303, 105.81059718132019, 106.86879205703735, 107.9140031337738, 108.95921421051025, 110.01270294189453, 111.06619167327881, 112.13883137702942, 113.21147108078003, 114.28766584396362, 115.36386060714722, 116.45475268363953, 117.54564476013184, 118.60183382034302, 119.6580228805542, 120.72657513618469, 121.79512739181519, 122.91285014152527, 124.03057289123535, 125.10563445091248, 126.1806960105896, 127.25581693649292, 128.33093786239624, 129.41991639137268, 130.50889492034912, 131.5868682861328, 132.6648416519165, 133.72473645210266, 134.78463125228882, 135.87845587730408, 136.97228050231934, 138.0232493877411, 139.07421827316284, 140.16582250595093, 141.257426738739, 142.34604287147522, 143.43465900421143, 144.52586817741394, 145.61707735061646, 146.70865941047668, 147.8002414703369, 148.86920714378357, 149.93817281723022, 151.03160786628723, 152.12504291534424, 153.21489572525024, 154.30474853515625, 155.38749980926514, 156.47025108337402, 157.52809381484985, 158.58593654632568, 159.67493534088135, 160.763934135437, 161.85696291923523, 162.94999170303345, 164.0684232711792, 165.18685483932495, 166.29336786270142, 167.39988088607788, 168.47748398780823, 169.55508708953857, 170.643230676651, 171.73137426376343, 172.82767391204834, 173.92397356033325, 174.99775862693787, 176.07154369354248, 177.14504170417786, 178.21853971481323, 179.31453204154968, 180.41052436828613, 181.49681544303894, 182.58310651779175, 183.65961360931396, 184.73612070083618, 185.80525851249695, 186.87439632415771, 187.92562079429626, 188.97684526443481, 190.11715745925903, 191.25746965408325, 192.34103345870972, 193.42459726333618, 194.53015208244324, 195.6357069015503, 196.77565336227417, 197.91559982299805, 199.00360536575317, 200.0916109085083, 201.18523955345154, 202.27886819839478, 203.3795268535614, 204.48018550872803, 205.58980679512024, 206.69942808151245, 207.77513360977173, 208.850839138031, 209.94897556304932, 211.04711198806763, 212.15760207176208, 213.26809215545654, 214.401113986969, 215.53413581848145, 217.35505294799805, 219.17597007751465]
[28.5, 28.5, 55.90833333333333, 55.90833333333333, 61.4, 61.4, 73.10833333333333, 73.10833333333333, 76.99166666666666, 76.99166666666666, 85.70833333333333, 85.70833333333333, 88.675, 88.675, 90.91666666666667, 90.91666666666667, 92.275, 92.275, 92.2, 92.2, 90.3, 90.3, 94.56666666666666, 94.56666666666666, 94.625, 94.625, 94.71666666666667, 94.71666666666667, 95.88333333333334, 95.88333333333334, 96.3, 96.3, 96.275, 96.275, 96.35, 96.35, 96.36666666666666, 96.36666666666666, 96.44166666666666, 96.44166666666666, 96.46666666666667, 96.46666666666667, 96.51666666666667, 96.51666666666667, 96.525, 96.525, 96.5, 96.5, 96.5, 96.5, 96.54166666666667, 96.54166666666667, 96.55833333333334, 96.55833333333334, 96.56666666666666, 96.56666666666666, 96.575, 96.575, 96.575, 96.575, 96.58333333333333, 96.58333333333333, 96.55833333333334, 96.55833333333334, 96.58333333333333, 96.58333333333333, 96.55833333333334, 96.55833333333334, 96.53333333333333, 96.53333333333333, 96.55, 96.55, 96.55, 96.55, 96.54166666666667, 96.54166666666667, 96.55, 96.55, 96.56666666666666, 96.56666666666666, 96.575, 96.575, 96.575, 96.575, 96.56666666666666, 96.56666666666666, 96.56666666666666, 96.56666666666666, 96.56666666666666, 96.56666666666666, 96.575, 96.575, 96.58333333333333, 96.58333333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.6, 96.6, 96.6, 96.6, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.58333333333333, 96.58333333333333, 96.58333333333333, 96.58333333333333, 96.575, 96.575, 96.575, 96.575, 96.58333333333333, 96.58333333333333, 96.58333333333333, 96.58333333333333, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.58333333333333, 96.58333333333333, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.64166666666667, 96.64166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.299, Test accuracy: 22.01
Round   0, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 21.83
Round   1, Train loss: 2.296, Test loss: 2.294, Test accuracy: 42.60
Round   1, Global train loss: 2.296, Global test loss: 2.292, Global test accuracy: 49.89
Round   2, Train loss: 2.285, Test loss: 2.280, Test accuracy: 43.50
Round   2, Global train loss: 2.285, Global test loss: 2.273, Global test accuracy: 49.40
Round   3, Train loss: 2.222, Test loss: 2.194, Test accuracy: 39.80
Round   3, Global train loss: 2.222, Global test loss: 2.122, Global test accuracy: 40.95
Round   4, Train loss: 2.009, Test loss: 2.051, Test accuracy: 53.07
Round   4, Global train loss: 2.009, Global test loss: 1.882, Global test accuracy: 67.25
Round   5, Train loss: 1.821, Test loss: 1.959, Test accuracy: 61.17
Round   5, Global train loss: 1.821, Global test loss: 1.764, Global test accuracy: 73.92
Round   6, Train loss: 1.755, Test loss: 1.883, Test accuracy: 65.56
Round   6, Global train loss: 1.755, Global test loss: 1.726, Global test accuracy: 75.52
Round   7, Train loss: 1.708, Test loss: 1.842, Test accuracy: 67.71
Round   7, Global train loss: 1.708, Global test loss: 1.690, Global test accuracy: 79.54
Round   8, Train loss: 1.673, Test loss: 1.791, Test accuracy: 70.91
Round   8, Global train loss: 1.673, Global test loss: 1.666, Global test accuracy: 81.30
Round   9, Train loss: 1.646, Test loss: 1.747, Test accuracy: 74.20
Round   9, Global train loss: 1.646, Global test loss: 1.656, Global test accuracy: 81.71
Round  10, Train loss: 1.639, Test loss: 1.740, Test accuracy: 74.90
Round  10, Global train loss: 1.639, Global test loss: 1.649, Global test accuracy: 82.22
Round  11, Train loss: 1.630, Test loss: 1.709, Test accuracy: 77.43
Round  11, Global train loss: 1.630, Global test loss: 1.643, Global test accuracy: 82.50
Round  12, Train loss: 1.628, Test loss: 1.691, Test accuracy: 78.56
Round  12, Global train loss: 1.628, Global test loss: 1.639, Global test accuracy: 82.67
Round  13, Train loss: 1.629, Test loss: 1.686, Test accuracy: 78.83
Round  13, Global train loss: 1.629, Global test loss: 1.636, Global test accuracy: 82.92
Round  14, Train loss: 1.617, Test loss: 1.683, Test accuracy: 79.04
Round  14, Global train loss: 1.617, Global test loss: 1.634, Global test accuracy: 83.12
Round  15, Train loss: 1.612, Test loss: 1.679, Test accuracy: 79.31
Round  15, Global train loss: 1.612, Global test loss: 1.631, Global test accuracy: 83.39
Round  16, Train loss: 1.616, Test loss: 1.674, Test accuracy: 79.65
Round  16, Global train loss: 1.616, Global test loss: 1.629, Global test accuracy: 83.57
Round  17, Train loss: 1.605, Test loss: 1.673, Test accuracy: 79.86
Round  17, Global train loss: 1.605, Global test loss: 1.627, Global test accuracy: 83.74
Round  18, Train loss: 1.614, Test loss: 1.668, Test accuracy: 80.22
Round  18, Global train loss: 1.614, Global test loss: 1.625, Global test accuracy: 83.89
Round  19, Train loss: 1.602, Test loss: 1.668, Test accuracy: 80.32
Round  19, Global train loss: 1.602, Global test loss: 1.624, Global test accuracy: 84.01
Round  20, Train loss: 1.599, Test loss: 1.665, Test accuracy: 80.57
Round  20, Global train loss: 1.599, Global test loss: 1.623, Global test accuracy: 84.04
Round  21, Train loss: 1.602, Test loss: 1.664, Test accuracy: 80.76
Round  21, Global train loss: 1.602, Global test loss: 1.622, Global test accuracy: 84.11
Round  22, Train loss: 1.597, Test loss: 1.628, Test accuracy: 83.79
Round  22, Global train loss: 1.597, Global test loss: 1.620, Global test accuracy: 84.33
Round  23, Train loss: 1.600, Test loss: 1.627, Test accuracy: 83.82
Round  23, Global train loss: 1.600, Global test loss: 1.619, Global test accuracy: 84.53
Round  24, Train loss: 1.589, Test loss: 1.627, Test accuracy: 83.79
Round  24, Global train loss: 1.589, Global test loss: 1.618, Global test accuracy: 84.60
Round  25, Train loss: 1.589, Test loss: 1.626, Test accuracy: 83.82
Round  25, Global train loss: 1.589, Global test loss: 1.618, Global test accuracy: 84.42
Round  26, Train loss: 1.602, Test loss: 1.624, Test accuracy: 83.93
Round  26, Global train loss: 1.602, Global test loss: 1.619, Global test accuracy: 84.40
Round  27, Train loss: 1.593, Test loss: 1.622, Test accuracy: 84.09
Round  27, Global train loss: 1.593, Global test loss: 1.617, Global test accuracy: 84.69
Round  28, Train loss: 1.604, Test loss: 1.622, Test accuracy: 84.18
Round  28, Global train loss: 1.604, Global test loss: 1.617, Global test accuracy: 84.55
Round  29, Train loss: 1.591, Test loss: 1.621, Test accuracy: 84.23
Round  29, Global train loss: 1.591, Global test loss: 1.615, Global test accuracy: 84.89
Round  30, Train loss: 1.594, Test loss: 1.621, Test accuracy: 84.25
Round  30, Global train loss: 1.594, Global test loss: 1.616, Global test accuracy: 84.81
Round  31, Train loss: 1.589, Test loss: 1.620, Test accuracy: 84.24
Round  31, Global train loss: 1.589, Global test loss: 1.615, Global test accuracy: 84.76
Round  32, Train loss: 1.590, Test loss: 1.619, Test accuracy: 84.36
Round  32, Global train loss: 1.590, Global test loss: 1.614, Global test accuracy: 84.78
Round  33, Train loss: 1.587, Test loss: 1.619, Test accuracy: 84.47
Round  33, Global train loss: 1.587, Global test loss: 1.613, Global test accuracy: 84.84
Round  34, Train loss: 1.591, Test loss: 1.618, Test accuracy: 84.48
Round  34, Global train loss: 1.591, Global test loss: 1.614, Global test accuracy: 84.88
Round  35, Train loss: 1.584, Test loss: 1.618, Test accuracy: 84.56
Round  35, Global train loss: 1.584, Global test loss: 1.613, Global test accuracy: 84.97
Round  36, Train loss: 1.588, Test loss: 1.618, Test accuracy: 84.47
Round  36, Global train loss: 1.588, Global test loss: 1.613, Global test accuracy: 84.87
Round  37, Train loss: 1.586, Test loss: 1.617, Test accuracy: 84.56
Round  37, Global train loss: 1.586, Global test loss: 1.612, Global test accuracy: 84.94
Round  38, Train loss: 1.594, Test loss: 1.616, Test accuracy: 84.72
Round  38, Global train loss: 1.594, Global test loss: 1.612, Global test accuracy: 84.96
Round  39, Train loss: 1.580, Test loss: 1.615, Test accuracy: 84.81
Round  39, Global train loss: 1.580, Global test loss: 1.612, Global test accuracy: 85.03
Round  40, Train loss: 1.585, Test loss: 1.615, Test accuracy: 84.80
Round  40, Global train loss: 1.585, Global test loss: 1.612, Global test accuracy: 85.09
Round  41, Train loss: 1.587, Test loss: 1.615, Test accuracy: 84.82
Round  41, Global train loss: 1.587, Global test loss: 1.611, Global test accuracy: 85.17
Round  42, Train loss: 1.584, Test loss: 1.614, Test accuracy: 84.76
Round  42, Global train loss: 1.584, Global test loss: 1.611, Global test accuracy: 85.07
Round  43, Train loss: 1.583, Test loss: 1.614, Test accuracy: 84.80
Round  43, Global train loss: 1.583, Global test loss: 1.610, Global test accuracy: 85.19
Round  44, Train loss: 1.588, Test loss: 1.614, Test accuracy: 84.77
Round  44, Global train loss: 1.588, Global test loss: 1.611, Global test accuracy: 85.14
Round  45, Train loss: 1.578, Test loss: 1.614, Test accuracy: 84.73
Round  45, Global train loss: 1.578, Global test loss: 1.610, Global test accuracy: 85.21
Round  46, Train loss: 1.583, Test loss: 1.613, Test accuracy: 84.80
Round  46, Global train loss: 1.583, Global test loss: 1.609, Global test accuracy: 85.38
Round  47, Train loss: 1.583, Test loss: 1.613, Test accuracy: 84.86
Round  47, Global train loss: 1.583, Global test loss: 1.609, Global test accuracy: 85.27
Round  48, Train loss: 1.589, Test loss: 1.612, Test accuracy: 84.89
Round  48, Global train loss: 1.589, Global test loss: 1.609, Global test accuracy: 85.23
Round  49, Train loss: 1.583, Test loss: 1.612, Test accuracy: 84.94
Round  49, Global train loss: 1.583, Global test loss: 1.609, Global test accuracy: 85.22
Round  50, Train loss: 1.579, Test loss: 1.612, Test accuracy: 85.02
Round  50, Global train loss: 1.579, Global test loss: 1.608, Global test accuracy: 85.37
Round  51, Train loss: 1.580, Test loss: 1.612, Test accuracy: 84.99
Round  51, Global train loss: 1.580, Global test loss: 1.608, Global test accuracy: 85.29
Round  52, Train loss: 1.586, Test loss: 1.611, Test accuracy: 85.02
Round  52, Global train loss: 1.586, Global test loss: 1.608, Global test accuracy: 85.53
Round  53, Train loss: 1.581, Test loss: 1.611, Test accuracy: 85.04
Round  53, Global train loss: 1.581, Global test loss: 1.608, Global test accuracy: 85.51
Round  54, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.02
Round  54, Global train loss: 1.580, Global test loss: 1.608, Global test accuracy: 85.36
Round  55, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.03
Round  55, Global train loss: 1.580, Global test loss: 1.607, Global test accuracy: 85.50
Round  56, Train loss: 1.576, Test loss: 1.611, Test accuracy: 85.03
Round  56, Global train loss: 1.576, Global test loss: 1.607, Global test accuracy: 85.47
Round  57, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.02
Round  57, Global train loss: 1.579, Global test loss: 1.607, Global test accuracy: 85.42
Round  58, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.12
Round  58, Global train loss: 1.579, Global test loss: 1.607, Global test accuracy: 85.47
Round  59, Train loss: 1.586, Test loss: 1.610, Test accuracy: 85.12
Round  59, Global train loss: 1.586, Global test loss: 1.607, Global test accuracy: 85.51
Round  60, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.09
Round  60, Global train loss: 1.575, Global test loss: 1.606, Global test accuracy: 85.56
Round  61, Train loss: 1.576, Test loss: 1.610, Test accuracy: 85.17
Round  61, Global train loss: 1.576, Global test loss: 1.606, Global test accuracy: 85.58
Round  62, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.22
Round  62, Global train loss: 1.574, Global test loss: 1.606, Global test accuracy: 85.59
Round  63, Train loss: 1.576, Test loss: 1.609, Test accuracy: 85.27
Round  63, Global train loss: 1.576, Global test loss: 1.606, Global test accuracy: 85.42
Round  64, Train loss: 1.576, Test loss: 1.609, Test accuracy: 85.24
Round  64, Global train loss: 1.576, Global test loss: 1.606, Global test accuracy: 85.52
Round  65, Train loss: 1.583, Test loss: 1.609, Test accuracy: 85.29
Round  65, Global train loss: 1.583, Global test loss: 1.607, Global test accuracy: 85.47
Round  66, Train loss: 1.580, Test loss: 1.609, Test accuracy: 85.30
Round  66, Global train loss: 1.580, Global test loss: 1.606, Global test accuracy: 85.39
Round  67, Train loss: 1.584, Test loss: 1.609, Test accuracy: 85.30
Round  67, Global train loss: 1.584, Global test loss: 1.606, Global test accuracy: 85.56
Round  68, Train loss: 1.575, Test loss: 1.608, Test accuracy: 85.28
Round  68, Global train loss: 1.575, Global test loss: 1.605, Global test accuracy: 85.66
Round  69, Train loss: 1.574, Test loss: 1.608, Test accuracy: 85.32
Round  69, Global train loss: 1.574, Global test loss: 1.605, Global test accuracy: 85.68
Round  70, Train loss: 1.578, Test loss: 1.608, Test accuracy: 85.36
Round  70, Global train loss: 1.578, Global test loss: 1.605, Global test accuracy: 85.72
Round  71, Train loss: 1.571, Test loss: 1.608, Test accuracy: 85.43
Round  71, Global train loss: 1.571, Global test loss: 1.605, Global test accuracy: 85.74
Round  72, Train loss: 1.583, Test loss: 1.607, Test accuracy: 85.46
Round  72, Global train loss: 1.583, Global test loss: 1.605, Global test accuracy: 85.56
Round  73, Train loss: 1.572, Test loss: 1.608, Test accuracy: 85.48
Round  73, Global train loss: 1.572, Global test loss: 1.605, Global test accuracy: 85.62
Round  74, Train loss: 1.571, Test loss: 1.607, Test accuracy: 85.49
Round  74, Global train loss: 1.571, Global test loss: 1.605, Global test accuracy: 85.76
Round  75, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.48
Round  75, Global train loss: 1.572, Global test loss: 1.604, Global test accuracy: 85.78
Round  76, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.50
Round  76, Global train loss: 1.574, Global test loss: 1.604, Global test accuracy: 85.72
Round  77, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.53
Round  77, Global train loss: 1.573, Global test loss: 1.605, Global test accuracy: 85.63
Round  78, Train loss: 1.578, Test loss: 1.607, Test accuracy: 85.54
Round  78, Global train loss: 1.578, Global test loss: 1.605, Global test accuracy: 85.63
Round  79, Train loss: 1.580, Test loss: 1.607, Test accuracy: 85.55
Round  79, Global train loss: 1.580, Global test loss: 1.605, Global test accuracy: 85.68
Round  80, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.55
Round  80, Global train loss: 1.573, Global test loss: 1.604, Global test accuracy: 85.78
Round  81, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.50
Round  81, Global train loss: 1.574, Global test loss: 1.604, Global test accuracy: 85.76
Round  82, Train loss: 1.578, Test loss: 1.607, Test accuracy: 85.49
Round  82, Global train loss: 1.578, Global test loss: 1.604, Global test accuracy: 85.78
Round  83, Train loss: 1.579, Test loss: 1.607, Test accuracy: 85.46
Round  83, Global train loss: 1.579, Global test loss: 1.604, Global test accuracy: 85.78
Round  84, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.50
Round  84, Global train loss: 1.572, Global test loss: 1.604, Global test accuracy: 85.77
Round  85, Train loss: 1.574, Test loss: 1.606, Test accuracy: 85.58
Round  85, Global train loss: 1.574, Global test loss: 1.604, Global test accuracy: 85.64
Round  86, Train loss: 1.576, Test loss: 1.606, Test accuracy: 85.58
Round  86, Global train loss: 1.576, Global test loss: 1.603, Global test accuracy: 85.83
Round  87, Train loss: 1.579, Test loss: 1.606, Test accuracy: 85.59
Round  87, Global train loss: 1.579, Global test loss: 1.603, Global test accuracy: 85.84
Round  88, Train loss: 1.575, Test loss: 1.606, Test accuracy: 85.62
Round  88, Global train loss: 1.575, Global test loss: 1.603, Global test accuracy: 85.84
Round  89, Train loss: 1.580, Test loss: 1.606, Test accuracy: 85.62
Round  89, Global train loss: 1.580, Global test loss: 1.603, Global test accuracy: 85.74
Round  90, Train loss: 1.577, Test loss: 1.606, Test accuracy: 85.60
Round  90, Global train loss: 1.577, Global test loss: 1.603, Global test accuracy: 85.87
Round  91, Train loss: 1.568, Test loss: 1.606, Test accuracy: 85.59
Round  91, Global train loss: 1.568, Global test loss: 1.603, Global test accuracy: 85.96
Round  92, Train loss: 1.572, Test loss: 1.605, Test accuracy: 85.61
Round  92, Global train loss: 1.572, Global test loss: 1.603, Global test accuracy: 85.85
Round  93, Train loss: 1.577, Test loss: 1.605, Test accuracy: 85.64
Round  93, Global train loss: 1.577, Global test loss: 1.603, Global test accuracy: 85.84
Round  94, Train loss: 1.575, Test loss: 1.605, Test accuracy: 85.68
Round  94, Global train loss: 1.575, Global test loss: 1.602, Global test accuracy: 85.80
Round  95, Train loss: 1.576, Test loss: 1.605, Test accuracy: 85.69
Round  95, Global train loss: 1.576, Global test loss: 1.603, Global test accuracy: 85.94/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.569, Test loss: 1.605, Test accuracy: 85.66
Round  96, Global train loss: 1.569, Global test loss: 1.602, Global test accuracy: 85.97
Round  97, Train loss: 1.572, Test loss: 1.605, Test accuracy: 85.68
Round  97, Global train loss: 1.572, Global test loss: 1.602, Global test accuracy: 85.89
Round  98, Train loss: 1.571, Test loss: 1.604, Test accuracy: 85.73
Round  98, Global train loss: 1.571, Global test loss: 1.602, Global test accuracy: 86.00
Round  99, Train loss: 1.576, Test loss: 1.604, Test accuracy: 85.72
Round  99, Global train loss: 1.576, Global test loss: 1.602, Global test accuracy: 85.95
Final Round, Train loss: 1.572, Test loss: 1.604, Test accuracy: 85.77
Final Round, Global train loss: 1.572, Global test loss: 1.602, Global test accuracy: 85.95
Average accuracy final 10 rounds: 85.66083333333334 

Average global accuracy final 10 rounds: 85.9075 

1899.6647062301636
[1.3997843265533447, 2.7995686531066895, 4.078166484832764, 5.356764316558838, 6.647262334823608, 7.937760353088379, 9.223896026611328, 10.510031700134277, 11.790722370147705, 13.071413040161133, 14.366656303405762, 15.66189956665039, 16.923475980758667, 18.185052394866943, 19.44253420829773, 20.700016021728516, 21.968594551086426, 23.237173080444336, 24.522416591644287, 25.80766010284424, 27.07195258140564, 28.33624505996704, 29.590100288391113, 30.843955516815186, 32.08951926231384, 33.3350830078125, 34.6443395614624, 35.953596115112305, 37.22858023643494, 38.50356435775757, 39.72669553756714, 40.94982671737671, 42.108619928359985, 43.26741313934326, 44.41824913024902, 45.569085121154785, 46.78674817085266, 48.00441122055054, 49.20047664642334, 50.39654207229614, 51.58549356460571, 52.77444505691528, 53.97318387031555, 55.17192268371582, 56.374797344207764, 57.57767200469971, 58.67144298553467, 59.76521396636963, 60.83178424835205, 61.89835453033447, 62.98613166809082, 64.07390880584717, 65.14314723014832, 66.21238565444946, 67.27391862869263, 68.33545160293579, 69.39421844482422, 70.45298528671265, 71.5799617767334, 72.70693826675415, 73.87639331817627, 75.04584836959839, 76.1234712600708, 77.20109415054321, 78.30524492263794, 79.40939569473267, 80.5735695362091, 81.73774337768555, 82.84361982345581, 83.94949626922607, 85.03286552429199, 86.11623477935791, 87.232248544693, 88.34826231002808, 89.46368956565857, 90.57911682128906, 91.67516541481018, 92.7712140083313, 93.84969234466553, 94.92817068099976, 96.06908416748047, 97.20999765396118, 98.33426570892334, 99.4585337638855, 100.50737357139587, 101.55621337890625, 102.61652874946594, 103.67684412002563, 104.77345252037048, 105.87006092071533, 106.94466543197632, 108.0192699432373, 109.05517339706421, 110.09107685089111, 111.13225817680359, 112.17343950271606, 113.26523327827454, 114.35702705383301, 115.3896279335022, 116.42222881317139, 117.46993851661682, 118.51764822006226, 119.60506868362427, 120.69248914718628, 121.78161406517029, 122.8707389831543, 123.9604880809784, 125.05023717880249, 126.13986444473267, 127.22949171066284, 128.34596371650696, 129.46243572235107, 130.56559205055237, 131.66874837875366, 132.76367807388306, 133.85860776901245, 134.9755802154541, 136.09255266189575, 137.1980242729187, 138.30349588394165, 139.3939356803894, 140.48437547683716, 141.56481432914734, 142.64525318145752, 143.72861123085022, 144.81196928024292, 145.89414882659912, 146.97632837295532, 148.04801964759827, 149.1197109222412, 150.22703313827515, 151.33435535430908, 152.3886260986328, 153.44289684295654, 154.5544741153717, 155.66605138778687, 156.72964811325073, 157.7932448387146, 158.87886762619019, 159.96449041366577, 161.12713384628296, 162.28977727890015, 163.3697154521942, 164.44965362548828, 165.51646041870117, 166.58326721191406, 167.62405943870544, 168.66485166549683, 169.75821447372437, 170.8515772819519, 171.96046328544617, 173.06934928894043, 174.12685537338257, 175.1843614578247, 176.2303478717804, 177.27633428573608, 178.44302129745483, 179.60970830917358, 180.71451210975647, 181.81931591033936, 182.88326001167297, 183.9472041130066, 185.058087348938, 186.16897058486938, 187.26177406311035, 188.35457754135132, 189.48152589797974, 190.60847425460815, 191.62453508377075, 192.64059591293335, 193.66813373565674, 194.69567155838013, 195.7932686805725, 196.8908658027649, 197.9028000831604, 198.9147343635559, 199.97420048713684, 201.03366661071777, 202.10778331756592, 203.18190002441406, 204.27355861663818, 205.3652172088623, 206.43902492523193, 207.51283264160156, 208.56706762313843, 209.6213026046753, 210.7593433856964, 211.89738416671753, 212.96214747428894, 214.02691078186035, 215.05260705947876, 216.07830333709717, 217.11234784126282, 218.14639234542847, 219.17155647277832, 220.19672060012817, 221.22957229614258, 222.26242399215698, 223.32209038734436, 224.38175678253174, 226.24252557754517, 228.1032943725586]
[22.008333333333333, 22.008333333333333, 42.6, 42.6, 43.5, 43.5, 39.8, 39.8, 53.06666666666667, 53.06666666666667, 61.175, 61.175, 65.55833333333334, 65.55833333333334, 67.70833333333333, 67.70833333333333, 70.90833333333333, 70.90833333333333, 74.2, 74.2, 74.9, 74.9, 77.43333333333334, 77.43333333333334, 78.55833333333334, 78.55833333333334, 78.825, 78.825, 79.04166666666667, 79.04166666666667, 79.30833333333334, 79.30833333333334, 79.65, 79.65, 79.85833333333333, 79.85833333333333, 80.21666666666667, 80.21666666666667, 80.31666666666666, 80.31666666666666, 80.56666666666666, 80.56666666666666, 80.75833333333334, 80.75833333333334, 83.79166666666667, 83.79166666666667, 83.81666666666666, 83.81666666666666, 83.79166666666667, 83.79166666666667, 83.81666666666666, 83.81666666666666, 83.93333333333334, 83.93333333333334, 84.09166666666667, 84.09166666666667, 84.18333333333334, 84.18333333333334, 84.23333333333333, 84.23333333333333, 84.25, 84.25, 84.24166666666666, 84.24166666666666, 84.35833333333333, 84.35833333333333, 84.46666666666667, 84.46666666666667, 84.48333333333333, 84.48333333333333, 84.55833333333334, 84.55833333333334, 84.475, 84.475, 84.55833333333334, 84.55833333333334, 84.725, 84.725, 84.80833333333334, 84.80833333333334, 84.8, 84.8, 84.81666666666666, 84.81666666666666, 84.75833333333334, 84.75833333333334, 84.8, 84.8, 84.76666666666667, 84.76666666666667, 84.73333333333333, 84.73333333333333, 84.8, 84.8, 84.85833333333333, 84.85833333333333, 84.89166666666667, 84.89166666666667, 84.94166666666666, 84.94166666666666, 85.01666666666667, 85.01666666666667, 84.99166666666666, 84.99166666666666, 85.01666666666667, 85.01666666666667, 85.04166666666667, 85.04166666666667, 85.01666666666667, 85.01666666666667, 85.025, 85.025, 85.025, 85.025, 85.01666666666667, 85.01666666666667, 85.125, 85.125, 85.125, 85.125, 85.09166666666667, 85.09166666666667, 85.175, 85.175, 85.21666666666667, 85.21666666666667, 85.26666666666667, 85.26666666666667, 85.24166666666666, 85.24166666666666, 85.29166666666667, 85.29166666666667, 85.3, 85.3, 85.3, 85.3, 85.275, 85.275, 85.31666666666666, 85.31666666666666, 85.35833333333333, 85.35833333333333, 85.43333333333334, 85.43333333333334, 85.45833333333333, 85.45833333333333, 85.48333333333333, 85.48333333333333, 85.49166666666666, 85.49166666666666, 85.48333333333333, 85.48333333333333, 85.5, 85.5, 85.525, 85.525, 85.54166666666667, 85.54166666666667, 85.55, 85.55, 85.55, 85.55, 85.5, 85.5, 85.49166666666666, 85.49166666666666, 85.45833333333333, 85.45833333333333, 85.5, 85.5, 85.575, 85.575, 85.58333333333333, 85.58333333333333, 85.59166666666667, 85.59166666666667, 85.625, 85.625, 85.625, 85.625, 85.6, 85.6, 85.59166666666667, 85.59166666666667, 85.60833333333333, 85.60833333333333, 85.64166666666667, 85.64166666666667, 85.68333333333334, 85.68333333333334, 85.69166666666666, 85.69166666666666, 85.65833333333333, 85.65833333333333, 85.68333333333334, 85.68333333333334, 85.73333333333333, 85.73333333333333, 85.71666666666667, 85.71666666666667, 85.76666666666667, 85.76666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 21.42
Round   1, Train loss: 2.300, Test loss: 2.299, Test accuracy: 23.47
Round   2, Train loss: 2.298, Test loss: 2.296, Test accuracy: 23.76
Round   3, Train loss: 2.295, Test loss: 2.291, Test accuracy: 25.66
Round   4, Train loss: 2.289, Test loss: 2.284, Test accuracy: 30.09
Round   5, Train loss: 2.277, Test loss: 2.266, Test accuracy: 30.27
Round   6, Train loss: 2.246, Test loss: 2.217, Test accuracy: 38.92
Round   7, Train loss: 2.144, Test loss: 2.097, Test accuracy: 41.83
Round   8, Train loss: 2.057, Test loss: 2.034, Test accuracy: 47.32
Round   9, Train loss: 1.985, Test loss: 1.970, Test accuracy: 51.38
Round  10, Train loss: 1.901, Test loss: 1.904, Test accuracy: 58.65
Round  11, Train loss: 1.829, Test loss: 1.831, Test accuracy: 67.96
Round  12, Train loss: 1.750, Test loss: 1.769, Test accuracy: 73.88
Round  13, Train loss: 1.715, Test loss: 1.730, Test accuracy: 76.97
Round  14, Train loss: 1.698, Test loss: 1.704, Test accuracy: 78.96
Round  15, Train loss: 1.673, Test loss: 1.691, Test accuracy: 79.37
Round  16, Train loss: 1.670, Test loss: 1.675, Test accuracy: 80.58
Round  17, Train loss: 1.655, Test loss: 1.660, Test accuracy: 81.92
Round  18, Train loss: 1.634, Test loss: 1.647, Test accuracy: 83.24
Round  19, Train loss: 1.624, Test loss: 1.634, Test accuracy: 84.23
Round  20, Train loss: 1.610, Test loss: 1.625, Test accuracy: 85.04
Round  21, Train loss: 1.597, Test loss: 1.616, Test accuracy: 86.30
Round  22, Train loss: 1.590, Test loss: 1.610, Test accuracy: 86.52
Round  23, Train loss: 1.586, Test loss: 1.602, Test accuracy: 87.39
Round  24, Train loss: 1.577, Test loss: 1.598, Test accuracy: 87.57
Round  25, Train loss: 1.567, Test loss: 1.591, Test accuracy: 88.25
Round  26, Train loss: 1.569, Test loss: 1.588, Test accuracy: 88.38
Round  27, Train loss: 1.564, Test loss: 1.584, Test accuracy: 88.79
Round  28, Train loss: 1.560, Test loss: 1.581, Test accuracy: 89.09
Round  29, Train loss: 1.548, Test loss: 1.576, Test accuracy: 89.62
Round  30, Train loss: 1.554, Test loss: 1.575, Test accuracy: 89.50
Round  31, Train loss: 1.550, Test loss: 1.572, Test accuracy: 90.04
Round  32, Train loss: 1.549, Test loss: 1.570, Test accuracy: 90.13
Round  33, Train loss: 1.540, Test loss: 1.572, Test accuracy: 89.88
Round  34, Train loss: 1.543, Test loss: 1.569, Test accuracy: 90.09
Round  35, Train loss: 1.540, Test loss: 1.569, Test accuracy: 89.96
Round  36, Train loss: 1.533, Test loss: 1.568, Test accuracy: 90.08
Round  37, Train loss: 1.535, Test loss: 1.565, Test accuracy: 90.32
Round  38, Train loss: 1.532, Test loss: 1.564, Test accuracy: 90.35
Round  39, Train loss: 1.524, Test loss: 1.566, Test accuracy: 90.19
Round  40, Train loss: 1.527, Test loss: 1.564, Test accuracy: 90.19
Round  41, Train loss: 1.523, Test loss: 1.563, Test accuracy: 90.27
Round  42, Train loss: 1.521, Test loss: 1.563, Test accuracy: 90.32
Round  43, Train loss: 1.522, Test loss: 1.563, Test accuracy: 90.33
Round  44, Train loss: 1.521, Test loss: 1.562, Test accuracy: 90.37
Round  45, Train loss: 1.521, Test loss: 1.562, Test accuracy: 90.38
Round  46, Train loss: 1.527, Test loss: 1.561, Test accuracy: 90.56
Round  47, Train loss: 1.526, Test loss: 1.560, Test accuracy: 90.65
Round  48, Train loss: 1.520, Test loss: 1.559, Test accuracy: 90.76
Round  49, Train loss: 1.524, Test loss: 1.558, Test accuracy: 90.76
Round  50, Train loss: 1.515, Test loss: 1.558, Test accuracy: 90.87
Round  51, Train loss: 1.520, Test loss: 1.557, Test accuracy: 90.70
Round  52, Train loss: 1.514, Test loss: 1.556, Test accuracy: 90.95
Round  53, Train loss: 1.508, Test loss: 1.558, Test accuracy: 90.82
Round  54, Train loss: 1.516, Test loss: 1.555, Test accuracy: 91.07
Round  55, Train loss: 1.517, Test loss: 1.556, Test accuracy: 91.01
Round  56, Train loss: 1.507, Test loss: 1.555, Test accuracy: 91.00
Round  57, Train loss: 1.514, Test loss: 1.555, Test accuracy: 91.04
Round  58, Train loss: 1.513, Test loss: 1.554, Test accuracy: 91.17
Round  59, Train loss: 1.508, Test loss: 1.553, Test accuracy: 91.22
Round  60, Train loss: 1.511, Test loss: 1.552, Test accuracy: 91.28
Round  61, Train loss: 1.508, Test loss: 1.553, Test accuracy: 91.25
Round  62, Train loss: 1.510, Test loss: 1.553, Test accuracy: 91.26
Round  63, Train loss: 1.504, Test loss: 1.552, Test accuracy: 91.23
Round  64, Train loss: 1.507, Test loss: 1.551, Test accuracy: 91.28
Round  65, Train loss: 1.508, Test loss: 1.552, Test accuracy: 91.33
Round  66, Train loss: 1.500, Test loss: 1.552, Test accuracy: 91.06
Round  67, Train loss: 1.506, Test loss: 1.552, Test accuracy: 91.18
Round  68, Train loss: 1.511, Test loss: 1.552, Test accuracy: 91.19
Round  69, Train loss: 1.500, Test loss: 1.551, Test accuracy: 91.24
Round  70, Train loss: 1.500, Test loss: 1.551, Test accuracy: 91.35
Round  71, Train loss: 1.500, Test loss: 1.550, Test accuracy: 91.43
Round  72, Train loss: 1.502, Test loss: 1.549, Test accuracy: 91.42
Round  73, Train loss: 1.509, Test loss: 1.549, Test accuracy: 91.44
Round  74, Train loss: 1.500, Test loss: 1.549, Test accuracy: 91.47
Round  75, Train loss: 1.497, Test loss: 1.548, Test accuracy: 91.53
Round  76, Train loss: 1.497, Test loss: 1.548, Test accuracy: 91.57
Round  77, Train loss: 1.505, Test loss: 1.549, Test accuracy: 91.52
Round  78, Train loss: 1.501, Test loss: 1.549, Test accuracy: 91.56
Round  79, Train loss: 1.501, Test loss: 1.549, Test accuracy: 91.53
Round  80, Train loss: 1.500, Test loss: 1.549, Test accuracy: 91.43
Round  81, Train loss: 1.499, Test loss: 1.548, Test accuracy: 91.49
Round  82, Train loss: 1.502, Test loss: 1.548, Test accuracy: 91.46
Round  83, Train loss: 1.496, Test loss: 1.549, Test accuracy: 91.44
Round  84, Train loss: 1.500, Test loss: 1.549, Test accuracy: 91.48
Round  85, Train loss: 1.494, Test loss: 1.549, Test accuracy: 91.29
Round  86, Train loss: 1.499, Test loss: 1.548, Test accuracy: 91.47
Round  87, Train loss: 1.496, Test loss: 1.548, Test accuracy: 91.46
Round  88, Train loss: 1.498, Test loss: 1.548, Test accuracy: 91.55
Round  89, Train loss: 1.499, Test loss: 1.547, Test accuracy: 91.52
Round  90, Train loss: 1.500, Test loss: 1.546, Test accuracy: 91.57
Round  91, Train loss: 1.496, Test loss: 1.546, Test accuracy: 91.58
Round  92, Train loss: 1.498, Test loss: 1.546, Test accuracy: 91.68
Round  93, Train loss: 1.497, Test loss: 1.546, Test accuracy: 91.65
Round  94, Train loss: 1.495, Test loss: 1.546, Test accuracy: 91.71
Round  95, Train loss: 1.496, Test loss: 1.546, Test accuracy: 91.78
Round  96, Train loss: 1.495, Test loss: 1.546, Test accuracy: 91.73
Round  97, Train loss: 1.492, Test loss: 1.546, Test accuracy: 91.78
Round  98, Train loss: 1.497, Test loss: 1.546, Test accuracy: 91.72/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.502, Test loss: 1.546, Test accuracy: 91.87
Final Round, Train loss: 1.497, Test loss: 1.546, Test accuracy: 91.70
Average accuracy final 10 rounds: 91.7075 

1464.5953578948975
[1.144723892211914, 2.289447784423828, 3.3081023693084717, 4.326756954193115, 5.445935487747192, 6.5651140213012695, 7.720194339752197, 8.875274658203125, 10.05892562866211, 11.242576599121094, 12.420680284500122, 13.59878396987915, 14.712285041809082, 15.825786113739014, 16.952881813049316, 18.07997751235962, 19.28310775756836, 20.4862380027771, 21.632781267166138, 22.779324531555176, 23.887059688568115, 24.994794845581055, 26.124056100845337, 27.25331735610962, 28.421566486358643, 29.589815616607666, 30.74164652824402, 31.89347743988037, 33.016931772232056, 34.14038610458374, 35.254517793655396, 36.36864948272705, 37.41401505470276, 38.45938062667847, 39.57668662071228, 40.693992614746094, 41.79447269439697, 42.89495277404785, 43.932961225509644, 44.970969676971436, 46.0489981174469, 47.12702655792236, 48.276286125183105, 49.42554569244385, 50.561041831970215, 51.69653797149658, 52.85284423828125, 54.00915050506592, 55.16408967971802, 56.31902885437012, 57.490570306777954, 58.66211175918579, 59.82073211669922, 60.97935247421265, 62.15676188468933, 63.334171295166016, 64.49282932281494, 65.65148735046387, 66.79626607894897, 67.94104480743408, 69.10910940170288, 70.27717399597168, 71.4371166229248, 72.59705924987793, 73.75546264648438, 74.91386604309082, 76.06718301773071, 77.2204999923706, 78.3610463142395, 79.5015926361084, 80.59068155288696, 81.67977046966553, 82.8526656627655, 84.02556085586548, 85.22487902641296, 86.42419719696045, 87.56280970573425, 88.70142221450806, 89.81303715705872, 90.92465209960938, 92.07775044441223, 93.23084878921509, 94.38200759887695, 95.53316640853882, 96.70578813552856, 97.87840986251831, 98.9691915512085, 100.05997323989868, 101.19873452186584, 102.33749580383301, 103.48353815078735, 104.6295804977417, 105.80602383613586, 106.98246717453003, 108.0846152305603, 109.18676328659058, 110.29982995986938, 111.4128966331482, 112.60057663917542, 113.78825664520264, 114.95594787597656, 116.12363910675049, 117.25680804252625, 118.389976978302, 119.50959038734436, 120.62920379638672, 121.7886266708374, 122.94804954528809, 124.06759858131409, 125.18714761734009, 126.33539485931396, 127.48364210128784, 128.62186217308044, 129.76008224487305, 130.9347333908081, 132.10938453674316, 133.26536226272583, 134.4213399887085, 135.58927583694458, 136.75721168518066, 137.85521817207336, 138.95322465896606, 139.96452641487122, 140.97582817077637, 141.98647022247314, 142.99711227416992, 144.01028680801392, 145.0234613418579, 146.02653527259827, 147.02960920333862, 148.06215023994446, 149.0946912765503, 150.12450218200684, 151.15431308746338, 152.10769057273865, 153.06106805801392, 154.0494978427887, 155.03792762756348, 156.08959555625916, 157.14126348495483, 158.19318866729736, 159.2451138496399, 160.22088646888733, 161.19665908813477, 162.2035539150238, 163.21044874191284, 164.24955010414124, 165.28865146636963, 166.32256412506104, 167.35647678375244, 168.37090373039246, 169.38533067703247, 170.41133522987366, 171.43733978271484, 172.47037076950073, 173.50340175628662, 174.50695490837097, 175.51050806045532, 176.53478837013245, 177.55906867980957, 178.57216453552246, 179.58526039123535, 180.5986042022705, 181.61194801330566, 182.59685134887695, 183.58175468444824, 184.62473917007446, 185.66772365570068, 186.72880601882935, 187.789888381958, 188.7810537815094, 189.7722191810608, 190.7741162776947, 191.7760133743286, 192.82000517845154, 193.86399698257446, 194.88237690925598, 195.9007568359375, 196.904687166214, 197.90861749649048, 198.98842644691467, 200.06823539733887, 201.109699010849, 202.15116262435913, 203.15910935401917, 204.1670560836792, 205.20375037193298, 206.24044466018677, 207.28501415252686, 208.32958364486694, 209.3722152709961, 210.41484689712524, 211.43106770515442, 212.4472885131836, 213.46608233451843, 214.48487615585327, 215.45026874542236, 216.41566133499146, 217.43808674812317, 218.46051216125488, 220.1197099685669, 221.7789077758789]
[21.416666666666668, 21.416666666666668, 23.466666666666665, 23.466666666666665, 23.758333333333333, 23.758333333333333, 25.658333333333335, 25.658333333333335, 30.091666666666665, 30.091666666666665, 30.275, 30.275, 38.916666666666664, 38.916666666666664, 41.825, 41.825, 47.31666666666667, 47.31666666666667, 51.38333333333333, 51.38333333333333, 58.65, 58.65, 67.95833333333333, 67.95833333333333, 73.875, 73.875, 76.96666666666667, 76.96666666666667, 78.95833333333333, 78.95833333333333, 79.36666666666666, 79.36666666666666, 80.575, 80.575, 81.91666666666667, 81.91666666666667, 83.24166666666666, 83.24166666666666, 84.23333333333333, 84.23333333333333, 85.04166666666667, 85.04166666666667, 86.3, 86.3, 86.51666666666667, 86.51666666666667, 87.39166666666667, 87.39166666666667, 87.56666666666666, 87.56666666666666, 88.25, 88.25, 88.375, 88.375, 88.79166666666667, 88.79166666666667, 89.09166666666667, 89.09166666666667, 89.61666666666666, 89.61666666666666, 89.5, 89.5, 90.04166666666667, 90.04166666666667, 90.13333333333334, 90.13333333333334, 89.875, 89.875, 90.09166666666667, 90.09166666666667, 89.95833333333333, 89.95833333333333, 90.075, 90.075, 90.31666666666666, 90.31666666666666, 90.35, 90.35, 90.19166666666666, 90.19166666666666, 90.19166666666666, 90.19166666666666, 90.26666666666667, 90.26666666666667, 90.31666666666666, 90.31666666666666, 90.325, 90.325, 90.36666666666666, 90.36666666666666, 90.38333333333334, 90.38333333333334, 90.55833333333334, 90.55833333333334, 90.65, 90.65, 90.75833333333334, 90.75833333333334, 90.75833333333334, 90.75833333333334, 90.86666666666666, 90.86666666666666, 90.7, 90.7, 90.95, 90.95, 90.81666666666666, 90.81666666666666, 91.06666666666666, 91.06666666666666, 91.00833333333334, 91.00833333333334, 91.0, 91.0, 91.04166666666667, 91.04166666666667, 91.16666666666667, 91.16666666666667, 91.21666666666667, 91.21666666666667, 91.275, 91.275, 91.25, 91.25, 91.25833333333334, 91.25833333333334, 91.23333333333333, 91.23333333333333, 91.275, 91.275, 91.325, 91.325, 91.05833333333334, 91.05833333333334, 91.18333333333334, 91.18333333333334, 91.19166666666666, 91.19166666666666, 91.24166666666666, 91.24166666666666, 91.35, 91.35, 91.43333333333334, 91.43333333333334, 91.41666666666667, 91.41666666666667, 91.44166666666666, 91.44166666666666, 91.46666666666667, 91.46666666666667, 91.525, 91.525, 91.56666666666666, 91.56666666666666, 91.51666666666667, 91.51666666666667, 91.55833333333334, 91.55833333333334, 91.525, 91.525, 91.43333333333334, 91.43333333333334, 91.49166666666666, 91.49166666666666, 91.45833333333333, 91.45833333333333, 91.44166666666666, 91.44166666666666, 91.48333333333333, 91.48333333333333, 91.29166666666667, 91.29166666666667, 91.46666666666667, 91.46666666666667, 91.45833333333333, 91.45833333333333, 91.55, 91.55, 91.51666666666667, 91.51666666666667, 91.56666666666666, 91.56666666666666, 91.58333333333333, 91.58333333333333, 91.68333333333334, 91.68333333333334, 91.65, 91.65, 91.70833333333333, 91.70833333333333, 91.78333333333333, 91.78333333333333, 91.73333333333333, 91.73333333333333, 91.775, 91.775, 91.725, 91.725, 91.86666666666666, 91.86666666666666, 91.7, 91.7]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.298, Test accuracy: 22.16
Round   1, Train loss: 2.293, Test loss: 2.286, Test accuracy: 20.54
Round   2, Train loss: 2.255, Test loss: 2.218, Test accuracy: 25.68
Round   3, Train loss: 2.166, Test loss: 2.125, Test accuracy: 45.29
Round   4, Train loss: 2.017, Test loss: 1.943, Test accuracy: 59.74
Round   5, Train loss: 1.835, Test loss: 1.837, Test accuracy: 66.17
Round   6, Train loss: 1.757, Test loss: 1.765, Test accuracy: 72.83
Round   7, Train loss: 1.684, Test loss: 1.735, Test accuracy: 74.88
Round   8, Train loss: 1.665, Test loss: 1.712, Test accuracy: 76.79
Round   9, Train loss: 1.661, Test loss: 1.680, Test accuracy: 79.67
Round  10, Train loss: 1.644, Test loss: 1.666, Test accuracy: 81.05
Round  11, Train loss: 1.630, Test loss: 1.649, Test accuracy: 82.38
Round  12, Train loss: 1.621, Test loss: 1.645, Test accuracy: 82.54
Round  13, Train loss: 1.617, Test loss: 1.637, Test accuracy: 83.33
Round  14, Train loss: 1.615, Test loss: 1.624, Test accuracy: 84.55
Round  15, Train loss: 1.597, Test loss: 1.617, Test accuracy: 85.22
Round  16, Train loss: 1.582, Test loss: 1.610, Test accuracy: 86.05
Round  17, Train loss: 1.569, Test loss: 1.602, Test accuracy: 86.60
Round  18, Train loss: 1.572, Test loss: 1.599, Test accuracy: 86.89
Round  19, Train loss: 1.549, Test loss: 1.592, Test accuracy: 87.58
Round  20, Train loss: 1.541, Test loss: 1.587, Test accuracy: 88.19
Round  21, Train loss: 1.535, Test loss: 1.582, Test accuracy: 88.46
Round  22, Train loss: 1.537, Test loss: 1.575, Test accuracy: 89.36
Round  23, Train loss: 1.527, Test loss: 1.570, Test accuracy: 89.70
Round  24, Train loss: 1.532, Test loss: 1.565, Test accuracy: 90.20
Round  25, Train loss: 1.525, Test loss: 1.558, Test accuracy: 91.02
Round  26, Train loss: 1.519, Test loss: 1.556, Test accuracy: 91.08
Round  27, Train loss: 1.516, Test loss: 1.556, Test accuracy: 91.09
Round  28, Train loss: 1.515, Test loss: 1.553, Test accuracy: 91.28
Round  29, Train loss: 1.514, Test loss: 1.552, Test accuracy: 91.43
Round  30, Train loss: 1.512, Test loss: 1.551, Test accuracy: 91.53
Round  31, Train loss: 1.513, Test loss: 1.551, Test accuracy: 91.56
Round  32, Train loss: 1.506, Test loss: 1.550, Test accuracy: 91.64
Round  33, Train loss: 1.510, Test loss: 1.550, Test accuracy: 91.58
Round  34, Train loss: 1.509, Test loss: 1.549, Test accuracy: 91.59
Round  35, Train loss: 1.507, Test loss: 1.548, Test accuracy: 91.77
Round  36, Train loss: 1.501, Test loss: 1.548, Test accuracy: 91.71
Round  37, Train loss: 1.502, Test loss: 1.548, Test accuracy: 91.70
Round  38, Train loss: 1.498, Test loss: 1.546, Test accuracy: 91.93
Round  39, Train loss: 1.503, Test loss: 1.546, Test accuracy: 91.87
Round  40, Train loss: 1.504, Test loss: 1.546, Test accuracy: 91.78
Round  41, Train loss: 1.498, Test loss: 1.545, Test accuracy: 91.97
Round  42, Train loss: 1.501, Test loss: 1.544, Test accuracy: 91.90
Round  43, Train loss: 1.495, Test loss: 1.543, Test accuracy: 92.12
Round  44, Train loss: 1.502, Test loss: 1.543, Test accuracy: 92.08
Round  45, Train loss: 1.498, Test loss: 1.543, Test accuracy: 92.00
Round  46, Train loss: 1.495, Test loss: 1.543, Test accuracy: 92.01
Round  47, Train loss: 1.497, Test loss: 1.542, Test accuracy: 92.12
Round  48, Train loss: 1.498, Test loss: 1.542, Test accuracy: 92.17
Round  49, Train loss: 1.495, Test loss: 1.541, Test accuracy: 92.25
Round  50, Train loss: 1.495, Test loss: 1.541, Test accuracy: 92.27
Round  51, Train loss: 1.496, Test loss: 1.540, Test accuracy: 92.37
Round  52, Train loss: 1.497, Test loss: 1.539, Test accuracy: 92.32
Round  53, Train loss: 1.492, Test loss: 1.539, Test accuracy: 92.43
Round  54, Train loss: 1.497, Test loss: 1.539, Test accuracy: 92.47
Round  55, Train loss: 1.492, Test loss: 1.539, Test accuracy: 92.38
Round  56, Train loss: 1.492, Test loss: 1.539, Test accuracy: 92.40
Round  57, Train loss: 1.494, Test loss: 1.539, Test accuracy: 92.38
Round  58, Train loss: 1.489, Test loss: 1.538, Test accuracy: 92.57
Round  59, Train loss: 1.494, Test loss: 1.538, Test accuracy: 92.41
Round  60, Train loss: 1.490, Test loss: 1.538, Test accuracy: 92.46
Round  61, Train loss: 1.490, Test loss: 1.538, Test accuracy: 92.53
Round  62, Train loss: 1.490, Test loss: 1.538, Test accuracy: 92.41
Round  63, Train loss: 1.490, Test loss: 1.538, Test accuracy: 92.39
Round  64, Train loss: 1.489, Test loss: 1.538, Test accuracy: 92.53
Round  65, Train loss: 1.488, Test loss: 1.538, Test accuracy: 92.51
Round  66, Train loss: 1.489, Test loss: 1.538, Test accuracy: 92.52
Round  67, Train loss: 1.489, Test loss: 1.537, Test accuracy: 92.62
Round  68, Train loss: 1.492, Test loss: 1.536, Test accuracy: 92.70
Round  69, Train loss: 1.486, Test loss: 1.537, Test accuracy: 92.62
Round  70, Train loss: 1.490, Test loss: 1.536, Test accuracy: 92.72
Round  71, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.62
Round  72, Train loss: 1.485, Test loss: 1.537, Test accuracy: 92.63
Round  73, Train loss: 1.489, Test loss: 1.536, Test accuracy: 92.76
Round  74, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.66
Round  75, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.67
Round  76, Train loss: 1.490, Test loss: 1.536, Test accuracy: 92.78
Round  77, Train loss: 1.488, Test loss: 1.535, Test accuracy: 92.76
Round  78, Train loss: 1.491, Test loss: 1.535, Test accuracy: 92.81
Round  79, Train loss: 1.483, Test loss: 1.535, Test accuracy: 92.82
Round  80, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.87
Round  81, Train loss: 1.488, Test loss: 1.535, Test accuracy: 92.81
Round  82, Train loss: 1.486, Test loss: 1.535, Test accuracy: 92.78
Round  83, Train loss: 1.486, Test loss: 1.535, Test accuracy: 92.73
Round  84, Train loss: 1.486, Test loss: 1.535, Test accuracy: 92.78
Round  85, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.81
Round  86, Train loss: 1.490, Test loss: 1.535, Test accuracy: 92.79
Round  87, Train loss: 1.488, Test loss: 1.535, Test accuracy: 92.77
Round  88, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.78
Round  89, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.79
Round  90, Train loss: 1.487, Test loss: 1.534, Test accuracy: 92.84
Round  91, Train loss: 1.487, Test loss: 1.534, Test accuracy: 92.92
Round  92, Train loss: 1.489, Test loss: 1.534, Test accuracy: 92.83
Round  93, Train loss: 1.487, Test loss: 1.534, Test accuracy: 92.91
Round  94, Train loss: 1.488, Test loss: 1.534, Test accuracy: 92.90
Round  95, Train loss: 1.483, Test loss: 1.534, Test accuracy: 92.99
Round  96, Train loss: 1.488, Test loss: 1.534, Test accuracy: 92.92
Round  97, Train loss: 1.486, Test loss: 1.534, Test accuracy: 92.94
Round  98, Train loss: 1.483, Test loss: 1.534, Test accuracy: 93.01/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.484, Test loss: 1.534, Test accuracy: 92.96
Final Round, Train loss: 1.486, Test loss: 1.534, Test accuracy: 92.89
Average accuracy final 10 rounds: 92.92333333333333 

1491.2249071598053
[1.3443148136138916, 2.688629627227783, 3.8894925117492676, 5.090355396270752, 6.249100208282471, 7.4078450202941895, 8.605440378189087, 9.803035736083984, 10.993014335632324, 12.182992935180664, 13.414763927459717, 14.64653491973877, 15.851973056793213, 17.057411193847656, 18.24536633491516, 19.433321475982666, 20.60528826713562, 21.777255058288574, 22.99303650856018, 24.208817958831787, 25.42117166519165, 26.633525371551514, 27.80776596069336, 28.982006549835205, 30.14248561859131, 31.302964687347412, 32.49832892417908, 33.69369316101074, 34.908724308013916, 36.12375545501709, 37.31027388572693, 38.49679231643677, 39.68870234489441, 40.88061237335205, 42.067941188812256, 43.25527000427246, 44.47039437294006, 45.685518741607666, 46.88672232627869, 48.08792591094971, 49.2689471244812, 50.449968338012695, 51.63508152961731, 52.820194721221924, 54.03812885284424, 55.25606298446655, 56.473700284957886, 57.69133758544922, 58.83600115776062, 59.98066473007202, 61.13870310783386, 62.2967414855957, 63.48384237289429, 64.67094326019287, 65.84691214561462, 67.02288103103638, 68.22229671478271, 69.42171239852905, 70.60147714614868, 71.78124189376831, 72.9421124458313, 74.10298299789429, 75.2818021774292, 76.46062135696411, 77.69117879867554, 78.92173624038696, 80.07823491096497, 81.23473358154297, 82.42070436477661, 83.60667514801025, 84.83949971199036, 86.07232427597046, 87.25502276420593, 88.4377212524414, 89.60122847557068, 90.76473569869995, 91.93470549583435, 93.10467529296875, 94.27113318443298, 95.43759107589722, 96.63506507873535, 97.83253908157349, 99.05959177017212, 100.28664445877075, 101.45957469940186, 102.63250494003296, 103.77371788024902, 104.91493082046509, 106.11994695663452, 107.32496309280396, 108.55307745933533, 109.7811918258667, 110.84539294242859, 111.90959405899048, 112.97755026817322, 114.04550647735596, 115.11630201339722, 116.18709754943848, 117.27152585983276, 118.35595417022705, 119.41882562637329, 120.48169708251953, 121.56508994102478, 122.64848279953003, 123.6924901008606, 124.73649740219116, 125.83233571052551, 126.92817401885986, 128.0235025882721, 129.11883115768433, 130.16481137275696, 131.2107915878296, 132.26126146316528, 133.31173133850098, 134.3878800868988, 135.46402883529663, 136.58850359916687, 137.7129783630371, 138.7981207370758, 139.8832631111145, 140.91729307174683, 141.95132303237915, 143.01395320892334, 144.07658338546753, 145.11492109298706, 146.1532588005066, 147.2461395263672, 148.33902025222778, 149.41713070869446, 150.49524116516113, 151.56496739387512, 152.6346936225891, 153.6909317970276, 154.74716997146606, 155.89556241035461, 157.04395484924316, 158.16431283950806, 159.28467082977295, 160.34376430511475, 161.40285778045654, 162.47888016700745, 163.55490255355835, 164.59923362731934, 165.64356470108032, 166.70203375816345, 167.76050281524658, 168.8390245437622, 169.91754627227783, 170.99162125587463, 172.06569623947144, 173.10919451713562, 174.1526927947998, 175.22723722457886, 176.3017816543579, 177.44604301452637, 178.59030437469482, 179.69181871414185, 180.79333305358887, 181.8516447544098, 182.9099564552307, 184.057865858078, 185.2057752609253, 186.357563495636, 187.50935173034668, 188.59563899040222, 189.68192625045776, 190.7562437057495, 191.83056116104126, 192.95873999595642, 194.08691883087158, 195.18698143959045, 196.28704404830933, 197.38276886940002, 198.47849369049072, 199.54216122627258, 200.60582876205444, 201.6928195953369, 202.77981042861938, 203.8462302684784, 204.9126501083374, 205.99974942207336, 207.08684873580933, 208.2069833278656, 209.32711791992188, 210.34652161598206, 211.36592531204224, 212.3908851146698, 213.41584491729736, 214.51078009605408, 215.6057152748108, 216.69311928749084, 217.7805233001709, 218.86588788032532, 219.95125246047974, 221.02565002441406, 222.1000475883484, 223.16756296157837, 224.23507833480835, 225.33034920692444, 226.42562007904053, 228.14054322242737, 229.8554663658142]
[22.158333333333335, 22.158333333333335, 20.541666666666668, 20.541666666666668, 25.683333333333334, 25.683333333333334, 45.291666666666664, 45.291666666666664, 59.74166666666667, 59.74166666666667, 66.175, 66.175, 72.83333333333333, 72.83333333333333, 74.88333333333334, 74.88333333333334, 76.79166666666667, 76.79166666666667, 79.675, 79.675, 81.05, 81.05, 82.38333333333334, 82.38333333333334, 82.54166666666667, 82.54166666666667, 83.33333333333333, 83.33333333333333, 84.55, 84.55, 85.21666666666667, 85.21666666666667, 86.05, 86.05, 86.6, 86.6, 86.89166666666667, 86.89166666666667, 87.58333333333333, 87.58333333333333, 88.19166666666666, 88.19166666666666, 88.45833333333333, 88.45833333333333, 89.35833333333333, 89.35833333333333, 89.7, 89.7, 90.2, 90.2, 91.01666666666667, 91.01666666666667, 91.075, 91.075, 91.09166666666667, 91.09166666666667, 91.28333333333333, 91.28333333333333, 91.43333333333334, 91.43333333333334, 91.53333333333333, 91.53333333333333, 91.55833333333334, 91.55833333333334, 91.64166666666667, 91.64166666666667, 91.58333333333333, 91.58333333333333, 91.59166666666667, 91.59166666666667, 91.76666666666667, 91.76666666666667, 91.70833333333333, 91.70833333333333, 91.7, 91.7, 91.93333333333334, 91.93333333333334, 91.86666666666666, 91.86666666666666, 91.78333333333333, 91.78333333333333, 91.975, 91.975, 91.9, 91.9, 92.125, 92.125, 92.075, 92.075, 92.0, 92.0, 92.00833333333334, 92.00833333333334, 92.11666666666666, 92.11666666666666, 92.175, 92.175, 92.25, 92.25, 92.26666666666667, 92.26666666666667, 92.36666666666666, 92.36666666666666, 92.31666666666666, 92.31666666666666, 92.43333333333334, 92.43333333333334, 92.46666666666667, 92.46666666666667, 92.375, 92.375, 92.4, 92.4, 92.38333333333334, 92.38333333333334, 92.56666666666666, 92.56666666666666, 92.40833333333333, 92.40833333333333, 92.45833333333333, 92.45833333333333, 92.525, 92.525, 92.40833333333333, 92.40833333333333, 92.39166666666667, 92.39166666666667, 92.525, 92.525, 92.50833333333334, 92.50833333333334, 92.51666666666667, 92.51666666666667, 92.625, 92.625, 92.7, 92.7, 92.61666666666666, 92.61666666666666, 92.71666666666667, 92.71666666666667, 92.61666666666666, 92.61666666666666, 92.63333333333334, 92.63333333333334, 92.75833333333334, 92.75833333333334, 92.65833333333333, 92.65833333333333, 92.675, 92.675, 92.775, 92.775, 92.75833333333334, 92.75833333333334, 92.80833333333334, 92.80833333333334, 92.81666666666666, 92.81666666666666, 92.86666666666666, 92.86666666666666, 92.80833333333334, 92.80833333333334, 92.78333333333333, 92.78333333333333, 92.73333333333333, 92.73333333333333, 92.775, 92.775, 92.80833333333334, 92.80833333333334, 92.79166666666667, 92.79166666666667, 92.76666666666667, 92.76666666666667, 92.775, 92.775, 92.79166666666667, 92.79166666666667, 92.84166666666667, 92.84166666666667, 92.925, 92.925, 92.83333333333333, 92.83333333333333, 92.90833333333333, 92.90833333333333, 92.9, 92.9, 92.99166666666666, 92.99166666666666, 92.925, 92.925, 92.94166666666666, 92.94166666666666, 93.00833333333334, 93.00833333333334, 92.95833333333333, 92.95833333333333, 92.89166666666667, 92.89166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.298, Test accuracy: 16.53
Round   1, Train loss: 2.293, Test loss: 2.290, Test accuracy: 30.00
Round   2, Train loss: 2.276, Test loss: 2.265, Test accuracy: 30.90
Round   3, Train loss: 2.187, Test loss: 2.179, Test accuracy: 40.58
Round   4, Train loss: 2.046, Test loss: 2.077, Test accuracy: 50.13
Round   5, Train loss: 1.942, Test loss: 1.998, Test accuracy: 55.10
Round   6, Train loss: 1.892, Test loss: 1.930, Test accuracy: 60.12
Round   7, Train loss: 1.818, Test loss: 1.863, Test accuracy: 66.06
Round   8, Train loss: 1.740, Test loss: 1.808, Test accuracy: 71.77
Round   9, Train loss: 1.688, Test loss: 1.782, Test accuracy: 73.68
Round  10, Train loss: 1.689, Test loss: 1.736, Test accuracy: 76.57
Round  11, Train loss: 1.636, Test loss: 1.725, Test accuracy: 77.41
Round  12, Train loss: 1.612, Test loss: 1.717, Test accuracy: 77.61
Round  13, Train loss: 1.618, Test loss: 1.710, Test accuracy: 78.13
Round  14, Train loss: 1.618, Test loss: 1.705, Test accuracy: 78.33
Round  15, Train loss: 1.627, Test loss: 1.683, Test accuracy: 79.31
Round  16, Train loss: 1.597, Test loss: 1.680, Test accuracy: 79.47
Round  17, Train loss: 1.609, Test loss: 1.677, Test accuracy: 79.64
Round  18, Train loss: 1.581, Test loss: 1.676, Test accuracy: 79.77
Round  19, Train loss: 1.579, Test loss: 1.675, Test accuracy: 79.72
Round  20, Train loss: 1.579, Test loss: 1.674, Test accuracy: 79.77
Round  21, Train loss: 1.583, Test loss: 1.673, Test accuracy: 79.80
Round  22, Train loss: 1.587, Test loss: 1.672, Test accuracy: 79.88
Round  23, Train loss: 1.583, Test loss: 1.671, Test accuracy: 79.86
Round  24, Train loss: 1.587, Test loss: 1.670, Test accuracy: 80.01
Round  25, Train loss: 1.578, Test loss: 1.669, Test accuracy: 80.01
Round  26, Train loss: 1.579, Test loss: 1.669, Test accuracy: 79.94
Round  27, Train loss: 1.577, Test loss: 1.668, Test accuracy: 79.86
Round  28, Train loss: 1.583, Test loss: 1.668, Test accuracy: 79.94
Round  29, Train loss: 1.571, Test loss: 1.667, Test accuracy: 80.02
Round  30, Train loss: 1.583, Test loss: 1.667, Test accuracy: 79.99
Round  31, Train loss: 1.576, Test loss: 1.667, Test accuracy: 80.02
Round  32, Train loss: 1.578, Test loss: 1.666, Test accuracy: 80.02
Round  33, Train loss: 1.574, Test loss: 1.666, Test accuracy: 80.00
Round  34, Train loss: 1.567, Test loss: 1.666, Test accuracy: 80.11
Round  35, Train loss: 1.573, Test loss: 1.666, Test accuracy: 80.04
Round  36, Train loss: 1.573, Test loss: 1.666, Test accuracy: 80.03
Round  37, Train loss: 1.571, Test loss: 1.666, Test accuracy: 80.02
Round  38, Train loss: 1.566, Test loss: 1.666, Test accuracy: 79.98
Round  39, Train loss: 1.562, Test loss: 1.664, Test accuracy: 80.06
Round  40, Train loss: 1.519, Test loss: 1.628, Test accuracy: 84.47
Round  41, Train loss: 1.509, Test loss: 1.622, Test accuracy: 84.93
Round  42, Train loss: 1.497, Test loss: 1.618, Test accuracy: 85.33
Round  43, Train loss: 1.490, Test loss: 1.616, Test accuracy: 85.50
Round  44, Train loss: 1.495, Test loss: 1.615, Test accuracy: 85.53
Round  45, Train loss: 1.490, Test loss: 1.613, Test accuracy: 85.67
Round  46, Train loss: 1.485, Test loss: 1.611, Test accuracy: 85.92
Round  47, Train loss: 1.490, Test loss: 1.609, Test accuracy: 86.06
Round  48, Train loss: 1.483, Test loss: 1.608, Test accuracy: 86.12
Round  49, Train loss: 1.484, Test loss: 1.607, Test accuracy: 86.17
Round  50, Train loss: 1.483, Test loss: 1.606, Test accuracy: 86.28
Round  51, Train loss: 1.485, Test loss: 1.605, Test accuracy: 86.31
Round  52, Train loss: 1.488, Test loss: 1.604, Test accuracy: 86.28
Round  53, Train loss: 1.481, Test loss: 1.604, Test accuracy: 86.33
Round  54, Train loss: 1.487, Test loss: 1.604, Test accuracy: 86.32
Round  55, Train loss: 1.481, Test loss: 1.603, Test accuracy: 86.42
Round  56, Train loss: 1.484, Test loss: 1.603, Test accuracy: 86.48
Round  57, Train loss: 1.480, Test loss: 1.603, Test accuracy: 86.44
Round  58, Train loss: 1.481, Test loss: 1.602, Test accuracy: 86.43
Round  59, Train loss: 1.482, Test loss: 1.602, Test accuracy: 86.50
Round  60, Train loss: 1.481, Test loss: 1.602, Test accuracy: 86.48
Round  61, Train loss: 1.476, Test loss: 1.602, Test accuracy: 86.45
Round  62, Train loss: 1.482, Test loss: 1.602, Test accuracy: 86.46
Round  63, Train loss: 1.479, Test loss: 1.602, Test accuracy: 86.54
Round  64, Train loss: 1.480, Test loss: 1.601, Test accuracy: 86.54
Round  65, Train loss: 1.482, Test loss: 1.601, Test accuracy: 86.54
Round  66, Train loss: 1.480, Test loss: 1.601, Test accuracy: 86.58
Round  67, Train loss: 1.481, Test loss: 1.601, Test accuracy: 86.60
Round  68, Train loss: 1.477, Test loss: 1.601, Test accuracy: 86.60
Round  69, Train loss: 1.482, Test loss: 1.601, Test accuracy: 86.58
Round  70, Train loss: 1.477, Test loss: 1.600, Test accuracy: 86.66
Round  71, Train loss: 1.481, Test loss: 1.600, Test accuracy: 86.69
Round  72, Train loss: 1.479, Test loss: 1.600, Test accuracy: 86.71
Round  73, Train loss: 1.481, Test loss: 1.600, Test accuracy: 86.62
Round  74, Train loss: 1.480, Test loss: 1.600, Test accuracy: 86.69
Round  75, Train loss: 1.480, Test loss: 1.600, Test accuracy: 86.70
Round  76, Train loss: 1.482, Test loss: 1.600, Test accuracy: 86.70
Round  77, Train loss: 1.477, Test loss: 1.600, Test accuracy: 86.71
Round  78, Train loss: 1.476, Test loss: 1.599, Test accuracy: 86.67
Round  79, Train loss: 1.478, Test loss: 1.599, Test accuracy: 86.73
Round  80, Train loss: 1.480, Test loss: 1.599, Test accuracy: 86.75
Round  81, Train loss: 1.478, Test loss: 1.599, Test accuracy: 86.72
Round  82, Train loss: 1.481, Test loss: 1.599, Test accuracy: 86.68
Round  83, Train loss: 1.479, Test loss: 1.599, Test accuracy: 86.67
Round  84, Train loss: 1.482, Test loss: 1.599, Test accuracy: 86.70
Round  85, Train loss: 1.478, Test loss: 1.599, Test accuracy: 86.70
Round  86, Train loss: 1.475, Test loss: 1.599, Test accuracy: 86.72
Round  87, Train loss: 1.478, Test loss: 1.599, Test accuracy: 86.67
Round  88, Train loss: 1.479, Test loss: 1.599, Test accuracy: 86.67
Round  89, Train loss: 1.479, Test loss: 1.599, Test accuracy: 86.67
Round  90, Train loss: 1.479, Test loss: 1.599, Test accuracy: 86.67
Round  91, Train loss: 1.480, Test loss: 1.599, Test accuracy: 86.69
Round  92, Train loss: 1.482, Test loss: 1.599, Test accuracy: 86.72
Round  93, Train loss: 1.479, Test loss: 1.599, Test accuracy: 86.75
Round  94, Train loss: 1.480, Test loss: 1.599, Test accuracy: 86.78
Round  95, Train loss: 1.477, Test loss: 1.598, Test accuracy: 86.72
Round  96, Train loss: 1.478, Test loss: 1.599, Test accuracy: 86.72
Round  97, Train loss: 1.478, Test loss: 1.598, Test accuracy: 86.75
Round  98, Train loss: 1.476, Test loss: 1.598, Test accuracy: 86.76
Round  99, Train loss: 1.478, Test loss: 1.598, Test accuracy: 86.82/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.479, Test loss: 1.598, Test accuracy: 86.85
Average accuracy final 10 rounds: 86.73916666666668 

1511.661565065384
[1.2590851783752441, 2.5181703567504883, 3.749950647354126, 4.981730937957764, 6.23357081413269, 7.485410690307617, 8.72631049156189, 9.967210292816162, 11.18976092338562, 12.412311553955078, 13.699485778808594, 14.98666000366211, 16.246419191360474, 17.506178379058838, 18.750125885009766, 19.994073390960693, 21.22144365310669, 22.448813915252686, 23.736154794692993, 25.0234956741333, 26.322089195251465, 27.62068271636963, 28.845895767211914, 30.0711088180542, 31.307135105133057, 32.543161392211914, 33.79792404174805, 35.05268669128418, 36.29404282569885, 37.535398960113525, 38.753132343292236, 39.97086572647095, 41.188639640808105, 42.406413555145264, 43.62606930732727, 44.84572505950928, 46.10637855529785, 47.367032051086426, 48.61286997795105, 49.858707904815674, 51.13662314414978, 52.41453838348389, 53.67260456085205, 54.930670738220215, 56.203245878219604, 57.475821018218994, 58.54754304885864, 59.61926507949829, 60.62166738510132, 61.624069690704346, 62.68571352958679, 63.74735736846924, 64.8573853969574, 65.96741342544556, 67.05446982383728, 68.141526222229, 69.21654319763184, 70.29156017303467, 71.34274387359619, 72.39392757415771, 73.44090723991394, 74.48788690567017, 75.58726572990417, 76.68664455413818, 77.8269259929657, 78.96720743179321, 80.04328489303589, 81.11936235427856, 82.13454794883728, 83.149733543396, 84.1731026172638, 85.19647169113159, 86.29347133636475, 87.3904709815979, 88.48995065689087, 89.58943033218384, 90.6772871017456, 91.76514387130737, 92.82678842544556, 93.88843297958374, 94.96717953681946, 96.04592609405518, 97.11131834983826, 98.17671060562134, 99.27359199523926, 100.37047338485718, 101.43255162239075, 102.49462985992432, 103.51609230041504, 104.53755474090576, 105.60922479629517, 106.68089485168457, 107.73069262504578, 108.78049039840698, 109.83841896057129, 110.8963475227356, 111.94350862503052, 112.99066972732544, 114.06483936309814, 115.13900899887085, 116.2063536643982, 117.27369832992554, 118.32803058624268, 119.38236284255981, 120.45455813407898, 121.52675342559814, 122.55539298057556, 123.58403253555298, 124.64935731887817, 125.71468210220337, 126.79568934440613, 127.87669658660889, 128.93500661849976, 129.99331665039062, 131.0057442188263, 132.01817178726196, 133.0301878452301, 134.04220390319824, 135.15259432792664, 136.26298475265503, 137.3521101474762, 138.44123554229736, 139.5102195739746, 140.57920360565186, 141.6596794128418, 142.74015522003174, 143.8266725540161, 144.9131898880005, 145.98462843894958, 147.05606698989868, 148.11397695541382, 149.17188692092896, 150.21895813941956, 151.26602935791016, 152.29137992858887, 153.31673049926758, 154.37634563446045, 155.43596076965332, 156.53782773017883, 157.63969469070435, 158.66682314872742, 159.6939516067505, 160.71633458137512, 161.73871755599976, 162.86172318458557, 163.9847288131714, 165.2354953289032, 166.486261844635, 167.76356410980225, 169.04086637496948, 170.3064308166504, 171.5719952583313, 172.8314425945282, 174.0908899307251, 175.41488075256348, 176.73887157440186, 177.9900631904602, 179.24125480651855, 180.48264527320862, 181.72403573989868, 182.98170924186707, 184.23938274383545, 185.55578923225403, 186.8721957206726, 188.18350315093994, 189.49481058120728, 190.79868483543396, 192.10255908966064, 193.3720874786377, 194.64161586761475, 195.86710023880005, 197.09258460998535, 198.3176941871643, 199.54280376434326, 200.73438453674316, 201.92596530914307, 203.07708764076233, 204.2282099723816, 205.4437940120697, 206.6593780517578, 207.82888555526733, 208.99839305877686, 210.20419836044312, 211.41000366210938, 212.61991906166077, 213.82983446121216, 215.04535269737244, 216.26087093353271, 217.514328956604, 218.7677869796753, 220.0141339302063, 221.2604808807373, 222.4832112789154, 223.7059416770935, 224.92058205604553, 226.13522243499756, 227.37787222862244, 228.62052202224731, 229.864155292511, 231.10778856277466, 232.98306918144226, 234.85834980010986]
[16.533333333333335, 16.533333333333335, 30.0, 30.0, 30.9, 30.9, 40.575, 40.575, 50.13333333333333, 50.13333333333333, 55.1, 55.1, 60.125, 60.125, 66.05833333333334, 66.05833333333334, 71.76666666666667, 71.76666666666667, 73.68333333333334, 73.68333333333334, 76.56666666666666, 76.56666666666666, 77.40833333333333, 77.40833333333333, 77.60833333333333, 77.60833333333333, 78.13333333333334, 78.13333333333334, 78.325, 78.325, 79.30833333333334, 79.30833333333334, 79.46666666666667, 79.46666666666667, 79.64166666666667, 79.64166666666667, 79.76666666666667, 79.76666666666667, 79.725, 79.725, 79.76666666666667, 79.76666666666667, 79.8, 79.8, 79.88333333333334, 79.88333333333334, 79.85833333333333, 79.85833333333333, 80.00833333333334, 80.00833333333334, 80.00833333333334, 80.00833333333334, 79.94166666666666, 79.94166666666666, 79.85833333333333, 79.85833333333333, 79.94166666666666, 79.94166666666666, 80.01666666666667, 80.01666666666667, 79.99166666666666, 79.99166666666666, 80.01666666666667, 80.01666666666667, 80.01666666666667, 80.01666666666667, 80.0, 80.0, 80.10833333333333, 80.10833333333333, 80.04166666666667, 80.04166666666667, 80.03333333333333, 80.03333333333333, 80.01666666666667, 80.01666666666667, 79.98333333333333, 79.98333333333333, 80.05833333333334, 80.05833333333334, 84.46666666666667, 84.46666666666667, 84.93333333333334, 84.93333333333334, 85.325, 85.325, 85.5, 85.5, 85.525, 85.525, 85.675, 85.675, 85.91666666666667, 85.91666666666667, 86.05833333333334, 86.05833333333334, 86.11666666666666, 86.11666666666666, 86.175, 86.175, 86.275, 86.275, 86.30833333333334, 86.30833333333334, 86.275, 86.275, 86.33333333333333, 86.33333333333333, 86.31666666666666, 86.31666666666666, 86.41666666666667, 86.41666666666667, 86.48333333333333, 86.48333333333333, 86.44166666666666, 86.44166666666666, 86.43333333333334, 86.43333333333334, 86.5, 86.5, 86.48333333333333, 86.48333333333333, 86.45, 86.45, 86.45833333333333, 86.45833333333333, 86.54166666666667, 86.54166666666667, 86.54166666666667, 86.54166666666667, 86.54166666666667, 86.54166666666667, 86.58333333333333, 86.58333333333333, 86.6, 86.6, 86.6, 86.6, 86.58333333333333, 86.58333333333333, 86.65833333333333, 86.65833333333333, 86.69166666666666, 86.69166666666666, 86.70833333333333, 86.70833333333333, 86.625, 86.625, 86.69166666666666, 86.69166666666666, 86.7, 86.7, 86.7, 86.7, 86.70833333333333, 86.70833333333333, 86.675, 86.675, 86.73333333333333, 86.73333333333333, 86.75, 86.75, 86.71666666666667, 86.71666666666667, 86.68333333333334, 86.68333333333334, 86.66666666666667, 86.66666666666667, 86.7, 86.7, 86.7, 86.7, 86.725, 86.725, 86.675, 86.675, 86.675, 86.675, 86.675, 86.675, 86.675, 86.675, 86.69166666666666, 86.69166666666666, 86.725, 86.725, 86.75, 86.75, 86.78333333333333, 86.78333333333333, 86.725, 86.725, 86.71666666666667, 86.71666666666667, 86.75, 86.75, 86.75833333333334, 86.75833333333334, 86.81666666666666, 86.81666666666666, 86.85, 86.85]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.695, Test loss: 2.288, Test accuracy: 30.42
Round   1, Train loss: 1.560, Test loss: 2.218, Test accuracy: 54.69
Round   2, Train loss: 1.387, Test loss: 2.076, Test accuracy: 62.42
Round   3, Train loss: 1.347, Test loss: 2.001, Test accuracy: 64.28
Round   4, Train loss: 1.329, Test loss: 1.978, Test accuracy: 64.90
Round   5, Train loss: 1.337, Test loss: 1.968, Test accuracy: 65.66
Round   6, Train loss: 1.308, Test loss: 1.960, Test accuracy: 66.37
Round   7, Train loss: 1.279, Test loss: 1.944, Test accuracy: 68.65
Round   8, Train loss: 1.273, Test loss: 1.935, Test accuracy: 69.59
Round   9, Train loss: 1.269, Test loss: 1.924, Test accuracy: 70.62
Round  10, Train loss: 1.267, Test loss: 1.916, Test accuracy: 72.09
Round  11, Train loss: 1.265, Test loss: 1.912, Test accuracy: 72.62
Round  12, Train loss: 1.260, Test loss: 1.909, Test accuracy: 73.03
Round  13, Train loss: 1.250, Test loss: 1.906, Test accuracy: 72.92
Round  14, Train loss: 1.250, Test loss: 1.905, Test accuracy: 72.79
Round  15, Train loss: 1.246, Test loss: 1.903, Test accuracy: 72.54
Round  16, Train loss: 1.254, Test loss: 1.901, Test accuracy: 72.35
Round  17, Train loss: 1.247, Test loss: 1.899, Test accuracy: 72.77
Round  18, Train loss: 1.249, Test loss: 1.899, Test accuracy: 72.61
Round  19, Train loss: 1.251, Test loss: 1.900, Test accuracy: 72.50
Round  20, Train loss: 1.243, Test loss: 1.899, Test accuracy: 72.21
Round  21, Train loss: 1.247, Test loss: 1.898, Test accuracy: 72.09
Round  22, Train loss: 1.244, Test loss: 1.898, Test accuracy: 72.12
Round  23, Train loss: 1.246, Test loss: 1.898, Test accuracy: 72.06
Round  24, Train loss: 1.244, Test loss: 1.899, Test accuracy: 72.00
Round  25, Train loss: 1.239, Test loss: 1.901, Test accuracy: 72.14
Round  26, Train loss: 1.207, Test loss: 1.898, Test accuracy: 72.88
Round  27, Train loss: 1.206, Test loss: 1.894, Test accuracy: 73.24
Round  28, Train loss: 1.197, Test loss: 1.887, Test accuracy: 74.68
Round  29, Train loss: 1.194, Test loss: 1.887, Test accuracy: 75.69
Round  30, Train loss: 1.181, Test loss: 1.885, Test accuracy: 75.60
Round  31, Train loss: 1.190, Test loss: 1.884, Test accuracy: 76.12
Round  32, Train loss: 1.184, Test loss: 1.883, Test accuracy: 76.31
Round  33, Train loss: 1.186, Test loss: 1.882, Test accuracy: 76.68
Round  34, Train loss: 1.190, Test loss: 1.880, Test accuracy: 76.75
Round  35, Train loss: 1.182, Test loss: 1.879, Test accuracy: 76.62
Round  36, Train loss: 1.184, Test loss: 1.878, Test accuracy: 76.46
Round  37, Train loss: 1.183, Test loss: 1.878, Test accuracy: 76.22
Round  38, Train loss: 1.183, Test loss: 1.877, Test accuracy: 76.11
Round  39, Train loss: 1.180, Test loss: 1.879, Test accuracy: 75.97
Round  40, Train loss: 1.179, Test loss: 1.879, Test accuracy: 75.92
Round  41, Train loss: 1.179, Test loss: 1.879, Test accuracy: 75.52
Round  42, Train loss: 1.183, Test loss: 1.878, Test accuracy: 75.47
Round  43, Train loss: 1.176, Test loss: 1.879, Test accuracy: 75.30
Round  44, Train loss: 1.178, Test loss: 1.880, Test accuracy: 75.25
Round  45, Train loss: 1.176, Test loss: 1.879, Test accuracy: 75.08
Round  46, Train loss: 1.174, Test loss: 1.881, Test accuracy: 74.74
Round  47, Train loss: 1.175, Test loss: 1.881, Test accuracy: 74.54
Round  48, Train loss: 1.175, Test loss: 1.881, Test accuracy: 74.36
Round  49, Train loss: 1.180, Test loss: 1.880, Test accuracy: 74.16
Round  50, Train loss: 1.182, Test loss: 1.881, Test accuracy: 74.14
Round  51, Train loss: 1.182, Test loss: 1.880, Test accuracy: 74.04
Round  52, Train loss: 1.180, Test loss: 1.880, Test accuracy: 73.94
Round  53, Train loss: 1.182, Test loss: 1.881, Test accuracy: 73.78
Round  54, Train loss: 1.176, Test loss: 1.881, Test accuracy: 73.61
Round  55, Train loss: 1.178, Test loss: 1.882, Test accuracy: 73.44
Round  56, Train loss: 1.176, Test loss: 1.882, Test accuracy: 73.38
Round  57, Train loss: 1.172, Test loss: 1.882, Test accuracy: 73.04
Round  58, Train loss: 1.169, Test loss: 1.883, Test accuracy: 73.07
Round  59, Train loss: 1.179, Test loss: 1.883, Test accuracy: 73.08
Round  60, Train loss: 1.172, Test loss: 1.884, Test accuracy: 72.82
Round  61, Train loss: 1.175, Test loss: 1.884, Test accuracy: 72.62
Round  62, Train loss: 1.178, Test loss: 1.884, Test accuracy: 72.39
Round  63, Train loss: 1.177, Test loss: 1.884, Test accuracy: 72.33
Round  64, Train loss: 1.175, Test loss: 1.885, Test accuracy: 72.11
Round  65, Train loss: 1.179, Test loss: 1.886, Test accuracy: 72.09
Round  66, Train loss: 1.175, Test loss: 1.887, Test accuracy: 71.84
Round  67, Train loss: 1.173, Test loss: 1.887, Test accuracy: 71.64
Round  68, Train loss: 1.174, Test loss: 1.888, Test accuracy: 71.55
Round  69, Train loss: 1.175, Test loss: 1.889, Test accuracy: 71.39
Round  70, Train loss: 1.177, Test loss: 1.888, Test accuracy: 71.47
Round  71, Train loss: 1.173, Test loss: 1.889, Test accuracy: 71.37
Round  72, Train loss: 1.177, Test loss: 1.889, Test accuracy: 71.20
Round  73, Train loss: 1.176, Test loss: 1.890, Test accuracy: 71.05
Round  74, Train loss: 1.175, Test loss: 1.891, Test accuracy: 70.95
Round  75, Train loss: 1.171, Test loss: 1.891, Test accuracy: 70.79
Round  76, Train loss: 1.180, Test loss: 1.891, Test accuracy: 70.61
Round  77, Train loss: 1.176, Test loss: 1.892, Test accuracy: 70.61
Round  78, Train loss: 1.172, Test loss: 1.893, Test accuracy: 70.62
Round  79, Train loss: 1.175, Test loss: 1.893, Test accuracy: 70.49
Round  80, Train loss: 1.175, Test loss: 1.893, Test accuracy: 70.38
Round  81, Train loss: 1.178, Test loss: 1.894, Test accuracy: 70.28
Round  82, Train loss: 1.177, Test loss: 1.894, Test accuracy: 70.17
Round  83, Train loss: 1.176, Test loss: 1.895, Test accuracy: 70.13
Round  84, Train loss: 1.174, Test loss: 1.894, Test accuracy: 70.20
Round  85, Train loss: 1.174, Test loss: 1.895, Test accuracy: 70.08
Round  86, Train loss: 1.163, Test loss: 1.897, Test accuracy: 70.02
Round  87, Train loss: 1.171, Test loss: 1.896, Test accuracy: 69.99
Round  88, Train loss: 1.170, Test loss: 1.897, Test accuracy: 69.92
Round  89, Train loss: 1.176, Test loss: 1.896, Test accuracy: 69.84
Round  90, Train loss: 1.174, Test loss: 1.897, Test accuracy: 69.84
Round  91, Train loss: 1.176, Test loss: 1.897, Test accuracy: 69.80
Round  92, Train loss: 1.173, Test loss: 1.897, Test accuracy: 69.72
Round  93, Train loss: 1.173, Test loss: 1.897, Test accuracy: 69.77
Round  94, Train loss: 1.175, Test loss: 1.897, Test accuracy: 69.85
Round  95, Train loss: 1.175, Test loss: 1.898, Test accuracy: 69.72
Round  96, Train loss: 1.170, Test loss: 1.898, Test accuracy: 69.54
Round  97, Train loss: 1.167, Test loss: 1.899, Test accuracy: 69.42
Round  98, Train loss: 1.170, Test loss: 1.899, Test accuracy: 69.41
Round  99, Train loss: 1.174, Test loss: 1.899, Test accuracy: 69.32
Final Round, Train loss: 1.173, Test loss: 1.902, Test accuracy: 68.94
Average accuracy final 10 rounds: 69.63916666666667
1656.6209440231323
[]
[30.416666666666668, 54.69166666666667, 62.416666666666664, 64.275, 64.9, 65.65833333333333, 66.36666666666666, 68.65, 69.59166666666667, 70.61666666666666, 72.09166666666667, 72.625, 73.03333333333333, 72.925, 72.79166666666667, 72.54166666666667, 72.35, 72.76666666666667, 72.60833333333333, 72.5, 72.20833333333333, 72.09166666666667, 72.11666666666666, 72.05833333333334, 72.0, 72.14166666666667, 72.88333333333334, 73.24166666666666, 74.68333333333334, 75.69166666666666, 75.6, 76.11666666666666, 76.30833333333334, 76.68333333333334, 76.75, 76.61666666666666, 76.45833333333333, 76.21666666666667, 76.10833333333333, 75.975, 75.925, 75.51666666666667, 75.46666666666667, 75.3, 75.25, 75.08333333333333, 74.74166666666666, 74.54166666666667, 74.35833333333333, 74.15833333333333, 74.14166666666667, 74.04166666666667, 73.94166666666666, 73.775, 73.60833333333333, 73.44166666666666, 73.375, 73.04166666666667, 73.06666666666666, 73.075, 72.81666666666666, 72.625, 72.39166666666667, 72.325, 72.10833333333333, 72.09166666666667, 71.84166666666667, 71.64166666666667, 71.55, 71.39166666666667, 71.475, 71.36666666666666, 71.2, 71.05, 70.95, 70.79166666666667, 70.60833333333333, 70.60833333333333, 70.61666666666666, 70.49166666666666, 70.38333333333334, 70.28333333333333, 70.16666666666667, 70.13333333333334, 70.2, 70.08333333333333, 70.01666666666667, 69.99166666666666, 69.925, 69.84166666666667, 69.84166666666667, 69.8, 69.725, 69.76666666666667, 69.85, 69.71666666666667, 69.54166666666667, 69.425, 69.40833333333333, 69.31666666666666, 68.94166666666666]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.286, Test loss: 2.274, Test accuracy: 18.32
Round   1, Train loss: 2.106, Test loss: 2.149, Test accuracy: 38.18
Round   2, Train loss: 1.937, Test loss: 2.098, Test accuracy: 42.48
Round   3, Train loss: 1.339, Test loss: 1.930, Test accuracy: 61.62
Round   4, Train loss: 1.745, Test loss: 1.893, Test accuracy: 61.59
Round   5, Train loss: 1.507, Test loss: 1.826, Test accuracy: 66.18
Round   6, Train loss: 1.643, Test loss: 1.807, Test accuracy: 68.97
Round   7, Train loss: 1.584, Test loss: 1.777, Test accuracy: 71.16
Round   8, Train loss: 1.600, Test loss: 1.761, Test accuracy: 71.37
Round   9, Train loss: 1.709, Test loss: 1.766, Test accuracy: 70.98
Round  10, Train loss: 1.589, Test loss: 1.748, Test accuracy: 72.55
Round  11, Train loss: 1.483, Test loss: 1.746, Test accuracy: 72.48
Round  12, Train loss: 1.457, Test loss: 1.737, Test accuracy: 73.33
Round  13, Train loss: 1.528, Test loss: 1.726, Test accuracy: 74.49
Round  14, Train loss: 1.618, Test loss: 1.689, Test accuracy: 78.52
Round  15, Train loss: 1.412, Test loss: 1.670, Test accuracy: 80.33
Round  16, Train loss: 1.434, Test loss: 1.673, Test accuracy: 80.04
Round  17, Train loss: 1.265, Test loss: 1.668, Test accuracy: 80.44
Round  18, Train loss: 1.280, Test loss: 1.649, Test accuracy: 82.25
Round  19, Train loss: 1.306, Test loss: 1.640, Test accuracy: 82.97
Round  20, Train loss: 1.349, Test loss: 1.628, Test accuracy: 84.36
Round  21, Train loss: 1.451, Test loss: 1.624, Test accuracy: 84.68
Round  22, Train loss: 1.211, Test loss: 1.612, Test accuracy: 85.73
Round  23, Train loss: 1.225, Test loss: 1.602, Test accuracy: 86.64
Round  24, Train loss: 1.141, Test loss: 1.598, Test accuracy: 86.95
Round  25, Train loss: 1.235, Test loss: 1.614, Test accuracy: 85.30
Round  26, Train loss: 1.026, Test loss: 1.597, Test accuracy: 87.03
Round  27, Train loss: 1.174, Test loss: 1.607, Test accuracy: 86.02
Round  28, Train loss: 1.193, Test loss: 1.607, Test accuracy: 85.89
Round  29, Train loss: 0.951, Test loss: 1.607, Test accuracy: 85.88
Round  30, Train loss: 1.193, Test loss: 1.615, Test accuracy: 85.01
Round  31, Train loss: 1.193, Test loss: 1.608, Test accuracy: 85.69
Round  32, Train loss: 0.994, Test loss: 1.607, Test accuracy: 85.77
Round  33, Train loss: 0.996, Test loss: 1.599, Test accuracy: 86.53
Round  34, Train loss: 1.103, Test loss: 1.598, Test accuracy: 86.60
Round  35, Train loss: 0.979, Test loss: 1.598, Test accuracy: 86.59
Round  36, Train loss: 0.856, Test loss: 1.597, Test accuracy: 86.67
Round  37, Train loss: 0.962, Test loss: 1.600, Test accuracy: 86.40
Round  38, Train loss: 0.811, Test loss: 1.593, Test accuracy: 87.14
Round  39, Train loss: 0.855, Test loss: 1.592, Test accuracy: 87.39
Round  40, Train loss: 0.697, Test loss: 1.579, Test accuracy: 88.72
Round  41, Train loss: 0.732, Test loss: 1.570, Test accuracy: 89.50
Round  42, Train loss: 0.832, Test loss: 1.565, Test accuracy: 90.00
Round  43, Train loss: 0.872, Test loss: 1.560, Test accuracy: 90.55
Round  44, Train loss: 0.886, Test loss: 1.559, Test accuracy: 90.53
Round  45, Train loss: 0.923, Test loss: 1.559, Test accuracy: 90.45
Round  46, Train loss: 0.844, Test loss: 1.559, Test accuracy: 90.49
Round  47, Train loss: 0.707, Test loss: 1.560, Test accuracy: 90.44
Round  48, Train loss: 0.969, Test loss: 1.560, Test accuracy: 90.48
Round  49, Train loss: 0.708, Test loss: 1.559, Test accuracy: 90.62
Round  50, Train loss: 0.680, Test loss: 1.557, Test accuracy: 90.78
Round  51, Train loss: 0.808, Test loss: 1.559, Test accuracy: 90.54
Round  52, Train loss: 0.822, Test loss: 1.558, Test accuracy: 90.65
Round  53, Train loss: 0.650, Test loss: 1.558, Test accuracy: 90.64
Round  54, Train loss: 0.926, Test loss: 1.558, Test accuracy: 90.62
Round  55, Train loss: 0.698, Test loss: 1.557, Test accuracy: 90.73
Round  56, Train loss: 0.885, Test loss: 1.557, Test accuracy: 90.75
Round  57, Train loss: 0.937, Test loss: 1.554, Test accuracy: 91.00
Round  58, Train loss: 0.549, Test loss: 1.554, Test accuracy: 91.04
Round  59, Train loss: 0.721, Test loss: 1.555, Test accuracy: 90.93
Round  60, Train loss: 0.744, Test loss: 1.554, Test accuracy: 90.95
Round  61, Train loss: 0.843, Test loss: 1.555, Test accuracy: 90.84
Round  62, Train loss: 0.747, Test loss: 1.556, Test accuracy: 90.81
Round  63, Train loss: 0.755, Test loss: 1.557, Test accuracy: 90.64
Round  64, Train loss: 0.750, Test loss: 1.556, Test accuracy: 90.77
Round  65, Train loss: 0.728, Test loss: 1.556, Test accuracy: 90.79
Round  66, Train loss: 0.684, Test loss: 1.555, Test accuracy: 90.82
Round  67, Train loss: 0.661, Test loss: 1.555, Test accuracy: 90.83
Round  68, Train loss: 0.653, Test loss: 1.555, Test accuracy: 90.82
Round  69, Train loss: 0.629, Test loss: 1.557, Test accuracy: 90.62
Round  70, Train loss: 0.871, Test loss: 1.555, Test accuracy: 90.78
Round  71, Train loss: 0.584, Test loss: 1.554, Test accuracy: 90.97
Round  72, Train loss: 0.499, Test loss: 1.555, Test accuracy: 90.83
Round  73, Train loss: 0.668, Test loss: 1.555, Test accuracy: 90.83
Round  74, Train loss: 0.709, Test loss: 1.556, Test accuracy: 90.71
Round  75, Train loss: 0.763, Test loss: 1.555, Test accuracy: 90.80
Round  76, Train loss: 0.741, Test loss: 1.555, Test accuracy: 90.83
Round  77, Train loss: 0.662, Test loss: 1.552, Test accuracy: 91.13
Round  78, Train loss: 0.768, Test loss: 1.553, Test accuracy: 91.00
Round  79, Train loss: 0.802, Test loss: 1.552, Test accuracy: 91.10
Round  80, Train loss: 0.716, Test loss: 1.551, Test accuracy: 91.16
Round  81, Train loss: 0.708, Test loss: 1.552, Test accuracy: 91.15
Round  82, Train loss: 0.573, Test loss: 1.552, Test accuracy: 91.10
Round  83, Train loss: 0.790, Test loss: 1.552, Test accuracy: 91.10
Round  84, Train loss: 0.702, Test loss: 1.551, Test accuracy: 91.19
Round  85, Train loss: 0.804, Test loss: 1.550, Test accuracy: 91.30
Round  86, Train loss: 0.793, Test loss: 1.551, Test accuracy: 91.19
Round  87, Train loss: 0.857, Test loss: 1.552, Test accuracy: 91.20
Round  88, Train loss: 0.633, Test loss: 1.551, Test accuracy: 91.22
Round  89, Train loss: 0.786, Test loss: 1.549, Test accuracy: 91.39
Round  90, Train loss: 0.781, Test loss: 1.550, Test accuracy: 91.30
Round  91, Train loss: 0.668, Test loss: 1.550, Test accuracy: 91.27
Round  92, Train loss: 0.742, Test loss: 1.549, Test accuracy: 91.42
Round  93, Train loss: 0.692, Test loss: 1.549, Test accuracy: 91.36
Round  94, Train loss: 0.649, Test loss: 1.550, Test accuracy: 91.26
Round  95, Train loss: 0.812, Test loss: 1.548, Test accuracy: 91.48
Round  96, Train loss: 0.676, Test loss: 1.547, Test accuracy: 91.54
Round  97, Train loss: 0.633, Test loss: 1.547, Test accuracy: 91.65
Round  98, Train loss: 0.724, Test loss: 1.548, Test accuracy: 91.53
Round  99, Train loss: 0.749, Test loss: 1.547, Test accuracy: 91.53
Final Round, Train loss: 1.503, Test loss: 1.535, Test accuracy: 92.94
Average accuracy final 10 rounds: 91.43475
Average global accuracy final 10 rounds: 91.43475
4146.6141221523285
[]
[18.3225, 38.18, 42.48, 61.6175, 61.595, 66.1825, 68.9675, 71.155, 71.3675, 70.9775, 72.5525, 72.48, 73.3325, 74.4925, 78.52, 80.325, 80.0425, 80.44, 82.245, 82.9725, 84.3625, 84.6825, 85.7325, 86.635, 86.9525, 85.2975, 87.025, 86.0225, 85.895, 85.88, 85.01, 85.685, 85.7675, 86.535, 86.5975, 86.595, 86.675, 86.3975, 87.135, 87.395, 88.7225, 89.4975, 90.005, 90.5525, 90.535, 90.4525, 90.4925, 90.4425, 90.485, 90.6225, 90.785, 90.54, 90.6475, 90.645, 90.625, 90.735, 90.755, 91.0, 91.0375, 90.9325, 90.9475, 90.845, 90.8125, 90.635, 90.77, 90.79, 90.8225, 90.83, 90.8175, 90.62, 90.7825, 90.9725, 90.825, 90.8325, 90.7125, 90.795, 90.835, 91.1275, 90.9975, 91.1, 91.1625, 91.1525, 91.1, 91.0975, 91.195, 91.2975, 91.1875, 91.1975, 91.215, 91.395, 91.2975, 91.265, 91.42, 91.36, 91.2625, 91.4825, 91.5425, 91.6475, 91.535, 91.535, 92.9425]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.80
Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 7.78
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.80
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 7.77
Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.88
Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 7.92
Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.88
Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 7.88
Round   4, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.92
Round   4, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 7.98
Round   5, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.94
Round   5, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.03
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.96
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.07
Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.02
Round   7, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.14
Round   8, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.06
Round   8, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.06
Round   9, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.07
Round   9, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.06
Round  10, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.05
Round  10, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.14
Round  11, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.10
Round  11, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.26
Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.12
Round  12, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.20
Round  13, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.19
Round  13, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.25
Round  14, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.27
Round  14, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.28
Round  15, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.29
Round  15, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.30
Round  16, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.28
Round  16, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.34
Round  17, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.24
Round  17, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.36
Round  18, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.25
Round  18, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.37
Round  19, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.31
Round  19, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.43
Round  20, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.37
Round  20, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.44
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.46
Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.50
Round  22, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.41
Round  22, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.53
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.44
Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.54
Round  24, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.48
Round  24, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.53
Round  25, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.53
Round  25, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.55
Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.59
Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.62
Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.66
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.69
Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.66
Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.75
Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.72
Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.72
Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.74
Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.77
Round  31, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.78
Round  31, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.73
Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.72
Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.82
Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.72
Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.83
Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.75
Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.88
Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.74
Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.88
Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.79
Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.91
Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.83
Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.93
Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.89
Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.95
Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.88
Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.97
Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.88
Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.99
Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.96
Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.04
Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.02
Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08
Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.03
Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.11
Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.05
Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.10
Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.12
Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.14
Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.18
Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.23
Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.19
Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.28
Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.20
Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.32
Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.29
Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.38
Round  50, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.34
Round  50, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.44
Round  51, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.35
Round  51, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.47
Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.43
Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  53, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.47
Round  53, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.62
Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.54
Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.67
Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.60
Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.73
Round  56, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.63
Round  56, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.72
Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.68
Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.80
Round  58, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.77
Round  58, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.83
Round  59, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.82
Round  59, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.83
Round  60, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.83
Round  60, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.95
Round  61, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.88
Round  61, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.00
Round  62, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.91
Round  62, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.09
Round  63, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.95
Round  63, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12
Round  64, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.04
Round  64, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.13
Round  65, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.07
Round  65, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.19
Round  66, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.10
Round  66, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.24
Round  67, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.15
Round  67, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.26
Round  68, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.16
Round  68, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.26
Round  69, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.18
Round  69, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.31
Round  70, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.21
Round  70, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.34
Round  71, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.23
Round  71, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.32
Round  72, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.27
Round  72, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.35
Round  73, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.32
Round  73, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.38
Round  74, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.38
Round  74, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.44
Round  75, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.45
Round  75, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.47
Round  76, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.49
Round  76, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.59
Round  77, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.51
Round  77, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.62
Round  78, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.50
Round  78, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.63
Round  79, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.54
Round  79, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.74
Round  80, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.62
Round  80, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.73
Round  81, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.70
Round  81, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 10.78
Round  82, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.76
Round  82, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.76
Round  83, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.75
Round  83, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.84
Round  84, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.81
Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.97
Round  85, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.90
Round  85, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.06
Round  86, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.92
Round  86, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.21
Round  87, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.11
Round  87, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.19
Round  88, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.13
Round  88, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.27
Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.21
Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.28
Round  90, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.26
Round  90, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.42
Round  91, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.37
Round  91, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.55
Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.44
Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.57
Round  93, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.60
Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.72
Round  94, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.66
Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.77
Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.65
Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.73
Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.68
Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.88
Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.77/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.95
Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.85
Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.20
Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.99
Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.12
Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.24
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.12
Average accuracy final 10 rounds: 11.625833333333334 

Average global accuracy final 10 rounds: 11.790000000000001 

1538.9132664203644
[1.3532044887542725, 2.6342616081237793, 3.966350793838501, 5.289721488952637, 6.5362420082092285, 7.800758361816406, 9.039989948272705, 10.341791152954102, 11.58493971824646, 12.847169637680054, 14.217298984527588, 15.58506464958191, 16.838356494903564, 18.108829736709595, 19.44683837890625, 20.80109214782715, 22.085822343826294, 23.352311849594116, 24.66079306602478, 25.999967575073242, 27.32211661338806, 28.58534550666809, 29.861334323883057, 31.233317613601685, 32.54749298095703, 33.783979177474976, 34.96440505981445, 36.21650171279907, 37.4117648601532, 38.66688394546509, 39.89256024360657, 41.19761323928833, 42.544851303100586, 43.832486391067505, 45.09209060668945, 46.292338371276855, 47.670711040496826, 48.93832564353943, 50.19951510429382, 51.465052127838135, 52.79139709472656, 54.07193660736084, 55.34259343147278, 56.60689067840576, 57.90203666687012, 59.19431281089783, 60.480560302734375, 61.74802923202515, 63.03634452819824, 64.39605236053467, 65.72396087646484, 66.97266507148743, 68.21669316291809, 69.54377126693726, 70.85295915603638, 72.13944482803345, 73.40432810783386, 74.70284628868103, 76.0421929359436, 77.35075235366821, 78.59107971191406, 79.88983249664307, 81.2093095779419, 82.50297689437866, 83.75930643081665, 84.99848294258118, 86.27882528305054, 87.64666318893433, 88.94084048271179, 90.17843341827393, 91.44748163223267, 92.74330568313599, 94.07249307632446, 95.32809543609619, 96.59898138046265, 97.90640950202942, 99.19781446456909, 100.4785327911377, 101.78397870063782, 103.04479217529297, 104.32854747772217, 105.68084120750427, 106.95814204216003, 108.22892427444458, 109.51749396324158, 110.81140065193176, 112.0928840637207, 113.3735363483429, 114.67956471443176, 115.986243724823, 117.29441690444946, 118.59514331817627, 119.87928771972656, 121.13443398475647, 122.46401500701904, 123.80354952812195, 125.12195134162903, 126.38397312164307, 127.69129538536072, 129.02139019966125, 131.23050141334534]
[7.8, 7.8, 7.883333333333334, 7.875, 7.925, 7.941666666666666, 7.958333333333333, 8.016666666666667, 8.058333333333334, 8.066666666666666, 8.05, 8.1, 8.125, 8.191666666666666, 8.266666666666667, 8.291666666666666, 8.275, 8.241666666666667, 8.25, 8.308333333333334, 8.366666666666667, 8.458333333333334, 8.408333333333333, 8.441666666666666, 8.483333333333333, 8.525, 8.591666666666667, 8.658333333333333, 8.658333333333333, 8.725, 8.741666666666667, 8.775, 8.725, 8.716666666666667, 8.75, 8.741666666666667, 8.791666666666666, 8.833333333333334, 8.891666666666667, 8.875, 8.883333333333333, 8.958333333333334, 9.016666666666667, 9.033333333333333, 9.05, 9.125, 9.183333333333334, 9.191666666666666, 9.2, 9.291666666666666, 9.341666666666667, 9.35, 9.433333333333334, 9.475, 9.541666666666666, 9.6, 9.633333333333333, 9.683333333333334, 9.766666666666667, 9.816666666666666, 9.833333333333334, 9.875, 9.908333333333333, 9.95, 10.041666666666666, 10.075, 10.1, 10.15, 10.158333333333333, 10.175, 10.208333333333334, 10.233333333333333, 10.266666666666667, 10.325, 10.375, 10.45, 10.491666666666667, 10.508333333333333, 10.5, 10.541666666666666, 10.625, 10.7, 10.758333333333333, 10.75, 10.808333333333334, 10.9, 10.916666666666666, 11.108333333333333, 11.133333333333333, 11.208333333333334, 11.258333333333333, 11.366666666666667, 11.441666666666666, 11.6, 11.658333333333333, 11.65, 11.675, 11.766666666666667, 11.85, 11.991666666666667, 12.241666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.300, Test accuracy: 25.89
Round   1, Train loss: 2.300, Test loss: 2.296, Test accuracy: 35.52
Round   2, Train loss: 2.296, Test loss: 2.286, Test accuracy: 47.89
Round   3, Train loss: 2.282, Test loss: 2.207, Test accuracy: 35.88
Round   4, Train loss: 2.194, Test loss: 2.000, Test accuracy: 50.96
Round   5, Train loss: 2.011, Test loss: 1.851, Test accuracy: 63.63
Round   6, Train loss: 1.908, Test loss: 1.770, Test accuracy: 72.58
Round   7, Train loss: 1.788, Test loss: 1.744, Test accuracy: 73.24
Round   8, Train loss: 1.763, Test loss: 1.734, Test accuracy: 73.46
Round   9, Train loss: 1.748, Test loss: 1.727, Test accuracy: 74.12
Round  10, Train loss: 1.731, Test loss: 1.725, Test accuracy: 73.97
Round  11, Train loss: 1.723, Test loss: 1.720, Test accuracy: 74.47
Round  12, Train loss: 1.719, Test loss: 1.718, Test accuracy: 74.66
Round  13, Train loss: 1.718, Test loss: 1.717, Test accuracy: 74.53
Round  14, Train loss: 1.702, Test loss: 1.714, Test accuracy: 74.85
Round  15, Train loss: 1.706, Test loss: 1.713, Test accuracy: 75.02
Round  16, Train loss: 1.701, Test loss: 1.712, Test accuracy: 75.08
Round  17, Train loss: 1.689, Test loss: 1.712, Test accuracy: 75.12
Round  18, Train loss: 1.697, Test loss: 1.709, Test accuracy: 75.47
Round  19, Train loss: 1.693, Test loss: 1.708, Test accuracy: 75.52
Round  20, Train loss: 1.693, Test loss: 1.708, Test accuracy: 75.30
Round  21, Train loss: 1.688, Test loss: 1.694, Test accuracy: 77.06
Round  22, Train loss: 1.673, Test loss: 1.666, Test accuracy: 80.32
Round  23, Train loss: 1.655, Test loss: 1.655, Test accuracy: 81.43
Round  24, Train loss: 1.645, Test loss: 1.649, Test accuracy: 82.02
Round  25, Train loss: 1.628, Test loss: 1.646, Test accuracy: 82.14
Round  26, Train loss: 1.630, Test loss: 1.643, Test accuracy: 82.46
Round  27, Train loss: 1.619, Test loss: 1.642, Test accuracy: 82.42
Round  28, Train loss: 1.610, Test loss: 1.641, Test accuracy: 82.63
Round  29, Train loss: 1.614, Test loss: 1.638, Test accuracy: 82.67
Round  30, Train loss: 1.607, Test loss: 1.638, Test accuracy: 82.69
Round  31, Train loss: 1.612, Test loss: 1.637, Test accuracy: 82.97
Round  32, Train loss: 1.604, Test loss: 1.636, Test accuracy: 82.78
Round  33, Train loss: 1.612, Test loss: 1.635, Test accuracy: 82.88
Round  34, Train loss: 1.596, Test loss: 1.634, Test accuracy: 83.06
Round  35, Train loss: 1.597, Test loss: 1.631, Test accuracy: 83.24
Round  36, Train loss: 1.603, Test loss: 1.630, Test accuracy: 83.33
Round  37, Train loss: 1.590, Test loss: 1.630, Test accuracy: 83.54
Round  38, Train loss: 1.598, Test loss: 1.629, Test accuracy: 83.45
Round  39, Train loss: 1.596, Test loss: 1.629, Test accuracy: 83.30
Round  40, Train loss: 1.592, Test loss: 1.628, Test accuracy: 83.67
Round  41, Train loss: 1.592, Test loss: 1.628, Test accuracy: 83.63
Round  42, Train loss: 1.591, Test loss: 1.627, Test accuracy: 83.53
Round  43, Train loss: 1.597, Test loss: 1.626, Test accuracy: 83.62
Round  44, Train loss: 1.590, Test loss: 1.626, Test accuracy: 83.54
Round  45, Train loss: 1.591, Test loss: 1.625, Test accuracy: 83.79
Round  46, Train loss: 1.589, Test loss: 1.625, Test accuracy: 83.68
Round  47, Train loss: 1.593, Test loss: 1.624, Test accuracy: 83.88
Round  48, Train loss: 1.592, Test loss: 1.625, Test accuracy: 83.88
Round  49, Train loss: 1.584, Test loss: 1.623, Test accuracy: 84.00
Round  50, Train loss: 1.596, Test loss: 1.623, Test accuracy: 83.96
Round  51, Train loss: 1.594, Test loss: 1.623, Test accuracy: 83.88
Round  52, Train loss: 1.583, Test loss: 1.622, Test accuracy: 83.99
Round  53, Train loss: 1.585, Test loss: 1.622, Test accuracy: 83.89
Round  54, Train loss: 1.584, Test loss: 1.622, Test accuracy: 84.11
Round  55, Train loss: 1.585, Test loss: 1.620, Test accuracy: 84.23
Round  56, Train loss: 1.590, Test loss: 1.621, Test accuracy: 84.17
Round  57, Train loss: 1.581, Test loss: 1.620, Test accuracy: 84.15
Round  58, Train loss: 1.580, Test loss: 1.620, Test accuracy: 84.13
Round  59, Train loss: 1.581, Test loss: 1.620, Test accuracy: 84.23
Round  60, Train loss: 1.583, Test loss: 1.619, Test accuracy: 84.26
Round  61, Train loss: 1.581, Test loss: 1.619, Test accuracy: 84.38
Round  62, Train loss: 1.578, Test loss: 1.619, Test accuracy: 84.42
Round  63, Train loss: 1.583, Test loss: 1.618, Test accuracy: 84.38
Round  64, Train loss: 1.581, Test loss: 1.619, Test accuracy: 84.39
Round  65, Train loss: 1.581, Test loss: 1.619, Test accuracy: 84.52
Round  66, Train loss: 1.583, Test loss: 1.618, Test accuracy: 84.44
Round  67, Train loss: 1.579, Test loss: 1.618, Test accuracy: 84.52
Round  68, Train loss: 1.579, Test loss: 1.617, Test accuracy: 84.62
Round  69, Train loss: 1.584, Test loss: 1.617, Test accuracy: 84.58
Round  70, Train loss: 1.581, Test loss: 1.618, Test accuracy: 84.56
Round  71, Train loss: 1.584, Test loss: 1.617, Test accuracy: 84.50
Round  72, Train loss: 1.584, Test loss: 1.617, Test accuracy: 84.48
Round  73, Train loss: 1.581, Test loss: 1.617, Test accuracy: 84.48
Round  74, Train loss: 1.581, Test loss: 1.616, Test accuracy: 84.65
Round  75, Train loss: 1.576, Test loss: 1.616, Test accuracy: 84.73
Round  76, Train loss: 1.579, Test loss: 1.616, Test accuracy: 84.53
Round  77, Train loss: 1.578, Test loss: 1.615, Test accuracy: 84.62
Round  78, Train loss: 1.573, Test loss: 1.615, Test accuracy: 84.62
Round  79, Train loss: 1.579, Test loss: 1.616, Test accuracy: 84.62
Round  80, Train loss: 1.583, Test loss: 1.615, Test accuracy: 84.80
Round  81, Train loss: 1.572, Test loss: 1.615, Test accuracy: 84.82
Round  82, Train loss: 1.577, Test loss: 1.615, Test accuracy: 84.80
Round  83, Train loss: 1.581, Test loss: 1.615, Test accuracy: 84.85
Round  84, Train loss: 1.576, Test loss: 1.614, Test accuracy: 84.88
Round  85, Train loss: 1.574, Test loss: 1.614, Test accuracy: 84.87
Round  86, Train loss: 1.576, Test loss: 1.614, Test accuracy: 84.93
Round  87, Train loss: 1.568, Test loss: 1.613, Test accuracy: 84.95
Round  88, Train loss: 1.575, Test loss: 1.613, Test accuracy: 84.95
Round  89, Train loss: 1.581, Test loss: 1.613, Test accuracy: 84.97
Round  90, Train loss: 1.570, Test loss: 1.613, Test accuracy: 84.89
Round  91, Train loss: 1.570, Test loss: 1.613, Test accuracy: 84.87
Round  92, Train loss: 1.573, Test loss: 1.612, Test accuracy: 85.04
Round  93, Train loss: 1.578, Test loss: 1.612, Test accuracy: 85.00
Round  94, Train loss: 1.576, Test loss: 1.613, Test accuracy: 84.91
Round  95, Train loss: 1.580, Test loss: 1.613, Test accuracy: 84.97
Round  96, Train loss: 1.575, Test loss: 1.612, Test accuracy: 85.04
Round  97, Train loss: 1.579, Test loss: 1.612, Test accuracy: 85.10
Round  98, Train loss: 1.580, Test loss: 1.612, Test accuracy: 85.05
Round  99, Train loss: 1.578, Test loss: 1.612, Test accuracy: 85.01
Final Round, Train loss: 1.572, Test loss: 1.611, Test accuracy: 85.17
Average accuracy final 10 rounds: 84.98833333333333
2174.3934288024902
[3.0905416011810303, 6.038847208023071, 9.004701852798462, 12.033514499664307, 15.031401872634888, 18.107611656188965, 21.27397871017456, 24.385071992874146, 27.43814516067505, 30.545291423797607, 33.60923218727112, 36.74669885635376, 39.85393238067627, 42.9967155456543, 46.05895948410034, 49.26123023033142, 52.30790090560913, 55.48307657241821, 58.62778663635254, 61.66920709609985, 64.6961464881897, 67.85885238647461, 70.97501587867737, 74.01054263114929, 77.06857991218567, 80.13679194450378, 83.24382758140564, 86.28459692001343, 89.32371187210083, 92.37146401405334, 95.43280458450317, 98.39618372917175, 101.45525312423706, 104.49402976036072, 107.55524921417236, 110.60716915130615, 113.5685887336731, 116.59005522727966, 119.75631523132324, 122.78288912773132, 125.80948448181152, 128.83035564422607, 131.83376932144165, 134.85160040855408, 137.77121710777283, 140.79603910446167, 143.7754249572754, 146.75932550430298, 149.78141713142395, 152.8279106616974, 155.8202896118164, 158.85327243804932, 161.86774253845215, 164.88926529884338, 167.92511200904846, 170.92537999153137, 173.92109060287476, 176.9567379951477, 179.9721760749817, 183.0562641620636, 186.1002631187439, 189.1028344631195, 192.1449899673462, 195.1188428401947, 198.06340289115906, 201.0257682800293, 203.8925564289093, 206.78248643875122, 209.7088224887848, 212.59160661697388, 215.50279116630554, 218.48990988731384, 221.48558592796326, 224.48046231269836, 227.4965169429779, 230.46849083900452, 233.416184425354, 236.43118953704834, 239.36440229415894, 242.3262038230896, 245.30935549736023, 248.20755529403687, 251.13196325302124, 254.17998909950256, 257.05002880096436, 259.9824047088623, 262.9679961204529, 265.8868079185486, 268.85461926460266, 271.8653128147125, 274.83133578300476, 277.8183000087738, 280.8103492259979, 283.66939997673035, 286.38616490364075, 289.1345157623291, 291.8745141029358, 294.6381883621216, 297.38285875320435, 300.1408460140228, 302.4407842159271]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[25.891666666666666, 35.525, 47.891666666666666, 35.875, 50.958333333333336, 63.63333333333333, 72.575, 73.24166666666666, 73.45833333333333, 74.11666666666666, 73.975, 74.46666666666667, 74.65833333333333, 74.525, 74.85, 75.01666666666667, 75.075, 75.11666666666666, 75.46666666666667, 75.51666666666667, 75.3, 77.05833333333334, 80.31666666666666, 81.43333333333334, 82.01666666666667, 82.14166666666667, 82.45833333333333, 82.41666666666667, 82.63333333333334, 82.675, 82.69166666666666, 82.975, 82.775, 82.875, 83.05833333333334, 83.24166666666666, 83.325, 83.54166666666667, 83.45, 83.3, 83.675, 83.63333333333334, 83.525, 83.61666666666666, 83.54166666666667, 83.79166666666667, 83.68333333333334, 83.875, 83.88333333333334, 84.0, 83.95833333333333, 83.875, 83.99166666666666, 83.89166666666667, 84.10833333333333, 84.23333333333333, 84.175, 84.15, 84.13333333333334, 84.23333333333333, 84.25833333333334, 84.375, 84.41666666666667, 84.375, 84.39166666666667, 84.51666666666667, 84.44166666666666, 84.51666666666667, 84.625, 84.58333333333333, 84.55833333333334, 84.5, 84.48333333333333, 84.48333333333333, 84.65, 84.73333333333333, 84.53333333333333, 84.625, 84.625, 84.625, 84.8, 84.81666666666666, 84.8, 84.85, 84.875, 84.86666666666666, 84.93333333333334, 84.95, 84.95, 84.975, 84.89166666666667, 84.86666666666666, 85.04166666666667, 85.0, 84.90833333333333, 84.975, 85.04166666666667, 85.1, 85.05, 85.00833333333334, 85.175]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.320, Test loss: 2.300, Test accuracy: 19.48
Round   1, Train loss: 2.299, Test loss: 2.296, Test accuracy: 31.46
Round   2, Train loss: 2.294, Test loss: 2.290, Test accuracy: 38.45
Round   3, Train loss: 2.287, Test loss: 2.279, Test accuracy: 42.38
Round   4, Train loss: 2.269, Test loss: 2.249, Test accuracy: 39.57
Round   5, Train loss: 2.225, Test loss: 2.200, Test accuracy: 47.45
Round   6, Train loss: 2.175, Test loss: 2.123, Test accuracy: 52.12
Round   7, Train loss: 2.065, Test loss: 2.031, Test accuracy: 56.74
Round   8, Train loss: 1.985, Test loss: 1.949, Test accuracy: 67.58
Round   9, Train loss: 1.879, Test loss: 1.856, Test accuracy: 76.03
Round  10, Train loss: 1.815, Test loss: 1.779, Test accuracy: 80.83
Round  11, Train loss: 1.741, Test loss: 1.735, Test accuracy: 84.38
Round  12, Train loss: 1.720, Test loss: 1.698, Test accuracy: 86.02
Round  13, Train loss: 1.685, Test loss: 1.677, Test accuracy: 87.22
Round  14, Train loss: 1.661, Test loss: 1.657, Test accuracy: 88.04
Round  15, Train loss: 1.647, Test loss: 1.644, Test accuracy: 88.87
Round  16, Train loss: 1.652, Test loss: 1.629, Test accuracy: 89.58
Round  17, Train loss: 1.627, Test loss: 1.618, Test accuracy: 90.05
Round  18, Train loss: 1.632, Test loss: 1.605, Test accuracy: 90.50
Round  19, Train loss: 1.614, Test loss: 1.599, Test accuracy: 90.79
Round  20, Train loss: 1.607, Test loss: 1.594, Test accuracy: 91.12
Round  21, Train loss: 1.588, Test loss: 1.589, Test accuracy: 91.44
Round  22, Train loss: 1.589, Test loss: 1.584, Test accuracy: 91.73
Round  23, Train loss: 1.583, Test loss: 1.580, Test accuracy: 91.90
Round  24, Train loss: 1.577, Test loss: 1.577, Test accuracy: 92.14
Round  25, Train loss: 1.577, Test loss: 1.573, Test accuracy: 92.21
Round  26, Train loss: 1.573, Test loss: 1.568, Test accuracy: 92.49
Round  27, Train loss: 1.567, Test loss: 1.565, Test accuracy: 92.63
Round  28, Train loss: 1.570, Test loss: 1.562, Test accuracy: 92.69
Round  29, Train loss: 1.561, Test loss: 1.560, Test accuracy: 92.83
Round  30, Train loss: 1.559, Test loss: 1.558, Test accuracy: 93.03
Round  31, Train loss: 1.553, Test loss: 1.556, Test accuracy: 93.17
Round  32, Train loss: 1.551, Test loss: 1.553, Test accuracy: 93.42
Round  33, Train loss: 1.543, Test loss: 1.552, Test accuracy: 93.49
Round  34, Train loss: 1.546, Test loss: 1.549, Test accuracy: 93.77
Round  35, Train loss: 1.544, Test loss: 1.548, Test accuracy: 93.76
Round  36, Train loss: 1.538, Test loss: 1.548, Test accuracy: 93.73
Round  37, Train loss: 1.537, Test loss: 1.546, Test accuracy: 93.92
Round  38, Train loss: 1.539, Test loss: 1.544, Test accuracy: 94.08
Round  39, Train loss: 1.530, Test loss: 1.542, Test accuracy: 94.20
Round  40, Train loss: 1.528, Test loss: 1.542, Test accuracy: 94.22
Round  41, Train loss: 1.532, Test loss: 1.540, Test accuracy: 94.29
Round  42, Train loss: 1.526, Test loss: 1.538, Test accuracy: 94.47
Round  43, Train loss: 1.527, Test loss: 1.537, Test accuracy: 94.58
Round  44, Train loss: 1.520, Test loss: 1.536, Test accuracy: 94.56
Round  45, Train loss: 1.520, Test loss: 1.536, Test accuracy: 94.70
Round  46, Train loss: 1.520, Test loss: 1.536, Test accuracy: 94.58
Round  47, Train loss: 1.522, Test loss: 1.534, Test accuracy: 94.61
Round  48, Train loss: 1.515, Test loss: 1.534, Test accuracy: 94.69
Round  49, Train loss: 1.516, Test loss: 1.532, Test accuracy: 94.90
Round  50, Train loss: 1.517, Test loss: 1.532, Test accuracy: 94.88
Round  51, Train loss: 1.514, Test loss: 1.531, Test accuracy: 94.92
Round  52, Train loss: 1.512, Test loss: 1.530, Test accuracy: 95.01
Round  53, Train loss: 1.514, Test loss: 1.529, Test accuracy: 94.97
Round  54, Train loss: 1.508, Test loss: 1.529, Test accuracy: 94.97
Round  55, Train loss: 1.509, Test loss: 1.528, Test accuracy: 95.09
Round  56, Train loss: 1.511, Test loss: 1.527, Test accuracy: 95.08
Round  57, Train loss: 1.506, Test loss: 1.527, Test accuracy: 95.06
Round  58, Train loss: 1.506, Test loss: 1.526, Test accuracy: 95.17
Round  59, Train loss: 1.506, Test loss: 1.526, Test accuracy: 95.16
Round  60, Train loss: 1.504, Test loss: 1.525, Test accuracy: 95.22
Round  61, Train loss: 1.505, Test loss: 1.525, Test accuracy: 95.22
Round  62, Train loss: 1.502, Test loss: 1.525, Test accuracy: 95.26
Round  63, Train loss: 1.503, Test loss: 1.524, Test accuracy: 95.35
Round  64, Train loss: 1.497, Test loss: 1.524, Test accuracy: 95.33
Round  65, Train loss: 1.500, Test loss: 1.523, Test accuracy: 95.38
Round  66, Train loss: 1.495, Test loss: 1.523, Test accuracy: 95.37
Round  67, Train loss: 1.497, Test loss: 1.522, Test accuracy: 95.38
Round  68, Train loss: 1.501, Test loss: 1.522, Test accuracy: 95.53
Round  69, Train loss: 1.497, Test loss: 1.521, Test accuracy: 95.58
Round  70, Train loss: 1.501, Test loss: 1.521, Test accuracy: 95.68
Round  71, Train loss: 1.494, Test loss: 1.521, Test accuracy: 95.63
Round  72, Train loss: 1.497, Test loss: 1.521, Test accuracy: 95.62
Round  73, Train loss: 1.494, Test loss: 1.520, Test accuracy: 95.72
Round  74, Train loss: 1.492, Test loss: 1.520, Test accuracy: 95.63
Round  75, Train loss: 1.495, Test loss: 1.520, Test accuracy: 95.74
Round  76, Train loss: 1.492, Test loss: 1.519, Test accuracy: 95.73
Round  77, Train loss: 1.493, Test loss: 1.519, Test accuracy: 95.69
Round  78, Train loss: 1.488, Test loss: 1.518, Test accuracy: 95.78
Round  79, Train loss: 1.489, Test loss: 1.518, Test accuracy: 95.75
Round  80, Train loss: 1.491, Test loss: 1.518, Test accuracy: 95.78
Round  81, Train loss: 1.493, Test loss: 1.518, Test accuracy: 95.88
Round  82, Train loss: 1.490, Test loss: 1.517, Test accuracy: 95.82
Round  83, Train loss: 1.488, Test loss: 1.517, Test accuracy: 95.87
Round  84, Train loss: 1.488, Test loss: 1.517, Test accuracy: 95.83
Round  85, Train loss: 1.492, Test loss: 1.516, Test accuracy: 95.92
Round  86, Train loss: 1.488, Test loss: 1.516, Test accuracy: 95.83
Round  87, Train loss: 1.488, Test loss: 1.516, Test accuracy: 95.92
Round  88, Train loss: 1.485, Test loss: 1.516, Test accuracy: 95.94
Round  89, Train loss: 1.485, Test loss: 1.515, Test accuracy: 95.95
Round  90, Train loss: 1.485, Test loss: 1.515, Test accuracy: 96.03
Round  91, Train loss: 1.486, Test loss: 1.515, Test accuracy: 95.97
Round  92, Train loss: 1.485, Test loss: 1.515, Test accuracy: 96.01
Round  93, Train loss: 1.485, Test loss: 1.514, Test accuracy: 96.06
Round  94, Train loss: 1.486, Test loss: 1.514, Test accuracy: 96.09/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.486, Test loss: 1.513, Test accuracy: 96.05
Round  96, Train loss: 1.485, Test loss: 1.513, Test accuracy: 96.02
Round  97, Train loss: 1.485, Test loss: 1.513, Test accuracy: 96.15
Round  98, Train loss: 1.483, Test loss: 1.513, Test accuracy: 96.11
Round  99, Train loss: 1.483, Test loss: 1.513, Test accuracy: 96.07
Final Round, Train loss: 1.477, Test loss: 1.512, Test accuracy: 96.14
Average accuracy final 10 rounds: 96.05499999999999
1216.4130160808563
[1.5616660118103027, 2.9976208209991455, 4.422950267791748, 5.832774639129639, 7.26511812210083, 8.65614938735962, 10.034319639205933, 11.480600595474243, 12.93125057220459, 14.417518854141235, 15.95972228050232, 17.507853507995605, 18.95095705986023, 20.452418088912964, 21.941532135009766, 23.42617678642273, 24.944435596466064, 26.42912244796753, 27.994996547698975, 29.432262897491455, 30.933004140853882, 32.43524193763733, 33.97403907775879, 35.43221068382263, 36.96679759025574, 38.51637101173401, 39.938631772994995, 41.42620134353638, 42.90643000602722, 44.352497816085815, 45.823662757873535, 47.2185332775116, 48.59868144989014, 49.96057653427124, 51.37681698799133, 52.70527768135071, 54.10187363624573, 55.517937421798706, 56.83785271644592, 58.244627952575684, 59.59470796585083, 60.93804860115051, 62.28361105918884, 63.64550757408142, 65.0086030960083, 66.35198378562927, 67.75703001022339, 69.10706233978271, 70.4492871761322, 71.8622407913208, 73.2295229434967, 74.58330535888672, 75.96228408813477, 77.28579545021057, 78.63868260383606, 79.96991443634033, 81.34730815887451, 82.66382145881653, 84.03848218917847, 85.3993661403656, 86.74427366256714, 88.13070154190063, 89.44153451919556, 90.76828503608704, 92.15775632858276, 93.46962428092957, 94.80487203598022, 96.13516306877136, 97.44780683517456, 98.77128171920776, 100.09659004211426, 101.4139244556427, 102.7498242855072, 104.09499073028564, 105.37013220787048, 106.69917368888855, 107.99281787872314, 109.34023642539978, 110.69120168685913, 111.97214865684509, 113.30903339385986, 114.60619354248047, 115.93333792686462, 117.23308420181274, 118.5830454826355, 119.89860606193542, 121.2481677532196, 122.6004467010498, 123.92069292068481, 125.25162029266357, 126.60668706893921, 127.97803974151611, 129.3195526599884, 130.6839199066162, 132.01995992660522, 133.4218339920044, 134.79097318649292, 136.14210629463196, 137.48057079315186, 138.85200119018555, 140.60088396072388]
[19.475, 31.458333333333332, 38.45, 42.38333333333333, 39.56666666666667, 47.45, 52.125, 56.74166666666667, 67.58333333333333, 76.025, 80.825, 84.38333333333334, 86.01666666666667, 87.225, 88.04166666666667, 88.86666666666666, 89.575, 90.05, 90.5, 90.79166666666667, 91.11666666666666, 91.44166666666666, 91.73333333333333, 91.9, 92.14166666666667, 92.20833333333333, 92.49166666666666, 92.63333333333334, 92.69166666666666, 92.825, 93.025, 93.16666666666667, 93.425, 93.49166666666666, 93.76666666666667, 93.75833333333334, 93.73333333333333, 93.91666666666667, 94.08333333333333, 94.2, 94.225, 94.29166666666667, 94.475, 94.575, 94.55833333333334, 94.7, 94.58333333333333, 94.60833333333333, 94.69166666666666, 94.9, 94.875, 94.925, 95.00833333333334, 94.975, 94.96666666666667, 95.09166666666667, 95.075, 95.05833333333334, 95.16666666666667, 95.15833333333333, 95.21666666666667, 95.21666666666667, 95.25833333333334, 95.35, 95.325, 95.38333333333334, 95.36666666666666, 95.375, 95.53333333333333, 95.58333333333333, 95.68333333333334, 95.63333333333334, 95.61666666666666, 95.725, 95.63333333333334, 95.74166666666666, 95.73333333333333, 95.69166666666666, 95.78333333333333, 95.75, 95.775, 95.875, 95.81666666666666, 95.86666666666666, 95.83333333333333, 95.925, 95.83333333333333, 95.91666666666667, 95.94166666666666, 95.95, 96.03333333333333, 95.96666666666667, 96.00833333333334, 96.05833333333334, 96.09166666666667, 96.05, 96.01666666666667, 96.15, 96.10833333333333, 96.06666666666666, 96.14166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.300, Test accuracy: 13.29
Round   1, Train loss: 2.314, Test loss: 2.294, Test accuracy: 19.18
Round   2, Train loss: 2.301, Test loss: 2.282, Test accuracy: 19.35
Round   3, Train loss: 2.281, Test loss: 2.252, Test accuracy: 25.27
Round   4, Train loss: 2.230, Test loss: 2.176, Test accuracy: 41.88
Round   5, Train loss: 2.136, Test loss: 2.070, Test accuracy: 55.72
Round   6, Train loss: 2.031, Test loss: 1.991, Test accuracy: 56.77
Round   7, Train loss: 1.984, Test loss: 1.959, Test accuracy: 57.81
Round   8, Train loss: 1.960, Test loss: 1.931, Test accuracy: 59.58
Round   9, Train loss: 1.930, Test loss: 1.906, Test accuracy: 63.62
Round  10, Train loss: 1.931, Test loss: 1.876, Test accuracy: 65.13
Round  11, Train loss: 1.873, Test loss: 1.862, Test accuracy: 66.32
Round  12, Train loss: 1.869, Test loss: 1.845, Test accuracy: 68.51
Round  13, Train loss: 1.856, Test loss: 1.819, Test accuracy: 71.42
Round  14, Train loss: 1.815, Test loss: 1.800, Test accuracy: 73.61
Round  15, Train loss: 1.812, Test loss: 1.783, Test accuracy: 74.17
Round  16, Train loss: 1.793, Test loss: 1.772, Test accuracy: 75.27
Round  17, Train loss: 1.792, Test loss: 1.757, Test accuracy: 76.76
Round  18, Train loss: 1.776, Test loss: 1.736, Test accuracy: 79.17
Round  19, Train loss: 1.749, Test loss: 1.723, Test accuracy: 80.43
Round  20, Train loss: 1.726, Test loss: 1.715, Test accuracy: 81.30
Round  21, Train loss: 1.745, Test loss: 1.700, Test accuracy: 82.64
Round  22, Train loss: 1.710, Test loss: 1.692, Test accuracy: 85.30
Round  23, Train loss: 1.728, Test loss: 1.663, Test accuracy: 87.83
Round  24, Train loss: 1.688, Test loss: 1.643, Test accuracy: 89.29
Round  25, Train loss: 1.665, Test loss: 1.635, Test accuracy: 90.35
Round  26, Train loss: 1.651, Test loss: 1.628, Test accuracy: 90.91
Round  27, Train loss: 1.646, Test loss: 1.622, Test accuracy: 91.33
Round  28, Train loss: 1.639, Test loss: 1.614, Test accuracy: 91.80
Round  29, Train loss: 1.635, Test loss: 1.606, Test accuracy: 91.96
Round  30, Train loss: 1.636, Test loss: 1.600, Test accuracy: 92.28
Round  31, Train loss: 1.614, Test loss: 1.598, Test accuracy: 92.26
Round  32, Train loss: 1.617, Test loss: 1.594, Test accuracy: 92.61
Round  33, Train loss: 1.616, Test loss: 1.589, Test accuracy: 92.90
Round  34, Train loss: 1.607, Test loss: 1.588, Test accuracy: 93.07
Round  35, Train loss: 1.598, Test loss: 1.587, Test accuracy: 93.19
Round  36, Train loss: 1.594, Test loss: 1.584, Test accuracy: 93.27
Round  37, Train loss: 1.588, Test loss: 1.583, Test accuracy: 93.37
Round  38, Train loss: 1.603, Test loss: 1.577, Test accuracy: 93.47
Round  39, Train loss: 1.585, Test loss: 1.578, Test accuracy: 93.49
Round  40, Train loss: 1.603, Test loss: 1.569, Test accuracy: 93.67
Round  41, Train loss: 1.573, Test loss: 1.573, Test accuracy: 93.98
Round  42, Train loss: 1.575, Test loss: 1.572, Test accuracy: 94.05
Round  43, Train loss: 1.581, Test loss: 1.567, Test accuracy: 94.03
Round  44, Train loss: 1.579, Test loss: 1.565, Test accuracy: 94.22
Round  45, Train loss: 1.572, Test loss: 1.565, Test accuracy: 94.28
Round  46, Train loss: 1.568, Test loss: 1.564, Test accuracy: 94.25
Round  47, Train loss: 1.568, Test loss: 1.563, Test accuracy: 94.40
Round  48, Train loss: 1.570, Test loss: 1.561, Test accuracy: 94.47
Round  49, Train loss: 1.559, Test loss: 1.560, Test accuracy: 94.53
Round  50, Train loss: 1.562, Test loss: 1.558, Test accuracy: 94.49
Round  51, Train loss: 1.567, Test loss: 1.554, Test accuracy: 94.56
Round  52, Train loss: 1.564, Test loss: 1.553, Test accuracy: 94.64
Round  53, Train loss: 1.552, Test loss: 1.554, Test accuracy: 94.67
Round  54, Train loss: 1.555, Test loss: 1.552, Test accuracy: 94.88
Round  55, Train loss: 1.552, Test loss: 1.552, Test accuracy: 94.86
Round  56, Train loss: 1.548, Test loss: 1.552, Test accuracy: 95.04
Round  57, Train loss: 1.551, Test loss: 1.550, Test accuracy: 94.91
Round  58, Train loss: 1.548, Test loss: 1.549, Test accuracy: 94.99
Round  59, Train loss: 1.547, Test loss: 1.547, Test accuracy: 95.17
Round  60, Train loss: 1.547, Test loss: 1.547, Test accuracy: 95.12
Round  61, Train loss: 1.544, Test loss: 1.546, Test accuracy: 95.28
Round  62, Train loss: 1.541, Test loss: 1.545, Test accuracy: 95.31
Round  63, Train loss: 1.541, Test loss: 1.544, Test accuracy: 95.29
Round  64, Train loss: 1.541, Test loss: 1.543, Test accuracy: 95.42
Round  65, Train loss: 1.534, Test loss: 1.545, Test accuracy: 95.48
Round  66, Train loss: 1.540, Test loss: 1.542, Test accuracy: 95.42
Round  67, Train loss: 1.539, Test loss: 1.541, Test accuracy: 95.58
Round  68, Train loss: 1.532, Test loss: 1.543, Test accuracy: 95.55
Round  69, Train loss: 1.534, Test loss: 1.541, Test accuracy: 95.53
Round  70, Train loss: 1.536, Test loss: 1.539, Test accuracy: 95.62
Round  71, Train loss: 1.530, Test loss: 1.541, Test accuracy: 95.73
Round  72, Train loss: 1.531, Test loss: 1.539, Test accuracy: 95.65
Round  73, Train loss: 1.530, Test loss: 1.539, Test accuracy: 95.79
Round  74, Train loss: 1.533, Test loss: 1.538, Test accuracy: 95.76
Round  75, Train loss: 1.529, Test loss: 1.538, Test accuracy: 95.71
Round  76, Train loss: 1.530, Test loss: 1.537, Test accuracy: 95.80
Round  77, Train loss: 1.528, Test loss: 1.536, Test accuracy: 95.78
Round  78, Train loss: 1.528, Test loss: 1.535, Test accuracy: 95.81
Round  79, Train loss: 1.529, Test loss: 1.534, Test accuracy: 95.83
Round  80, Train loss: 1.520, Test loss: 1.536, Test accuracy: 95.73
Round  81, Train loss: 1.524, Test loss: 1.535, Test accuracy: 95.87
Round  82, Train loss: 1.524, Test loss: 1.534, Test accuracy: 95.84
Round  83, Train loss: 1.518, Test loss: 1.536, Test accuracy: 95.87
Round  84, Train loss: 1.525, Test loss: 1.533, Test accuracy: 95.92
Round  85, Train loss: 1.518, Test loss: 1.534, Test accuracy: 95.96
Round  86, Train loss: 1.524, Test loss: 1.532, Test accuracy: 95.98
Round  87, Train loss: 1.522, Test loss: 1.531, Test accuracy: 95.99
Round  88, Train loss: 1.514, Test loss: 1.534, Test accuracy: 96.02
Round  89, Train loss: 1.521, Test loss: 1.532, Test accuracy: 96.03
Round  90, Train loss: 1.518, Test loss: 1.531, Test accuracy: 96.07
Round  91, Train loss: 1.519, Test loss: 1.531, Test accuracy: 96.07
Round  92, Train loss: 1.518, Test loss: 1.530, Test accuracy: 96.03
Round  93, Train loss: 1.517, Test loss: 1.530, Test accuracy: 96.11/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.516, Test loss: 1.530, Test accuracy: 96.20
Round  95, Train loss: 1.517, Test loss: 1.529, Test accuracy: 96.14
Round  96, Train loss: 1.513, Test loss: 1.530, Test accuracy: 96.13
Round  97, Train loss: 1.512, Test loss: 1.530, Test accuracy: 96.16
Round  98, Train loss: 1.511, Test loss: 1.529, Test accuracy: 96.09
Round  99, Train loss: 1.513, Test loss: 1.529, Test accuracy: 96.04
Final Round, Train loss: 1.489, Test loss: 1.525, Test accuracy: 96.12
Average accuracy final 10 rounds: 96.10333333333332
1657.7910447120667
[1.6413383483886719, 3.2826766967773438, 4.853394269943237, 6.424111843109131, 7.976399183273315, 9.5286865234375, 11.041675806045532, 12.554665088653564, 14.059593439102173, 15.564521789550781, 17.05356001853943, 18.542598247528076, 20.099026679992676, 21.655455112457275, 23.140681266784668, 24.62590742111206, 26.125620365142822, 27.625333309173584, 29.17519974708557, 30.72506618499756, 32.22988295555115, 33.734699726104736, 35.273417234420776, 36.812134742736816, 38.37763953208923, 39.94314432144165, 41.44765567779541, 42.95216703414917, 44.4875066280365, 46.02284622192383, 47.577388286590576, 49.131930351257324, 50.689733266830444, 52.247536182403564, 53.80824899673462, 55.368961811065674, 56.92311882972717, 58.47727584838867, 59.97357487678528, 61.469873905181885, 62.96384334564209, 64.4578127861023, 65.96571564674377, 67.47361850738525, 69.00837779045105, 70.54313707351685, 72.08306932449341, 73.62300157546997, 75.11857318878174, 76.6141448020935, 78.0969169139862, 79.5796890258789, 81.12470126152039, 82.66971349716187, 84.22912096977234, 85.78852844238281, 87.34730076789856, 88.9060730934143, 90.44462418556213, 91.98317527770996, 93.53441596031189, 95.08565664291382, 96.61204433441162, 98.13843202590942, 99.660893201828, 101.18335437774658, 102.72568225860596, 104.26801013946533, 105.81917452812195, 107.37033891677856, 108.92912578582764, 110.48791265487671, 112.05880355834961, 113.62969446182251, 115.14258217811584, 116.65546989440918, 118.17429804801941, 119.69312620162964, 121.2671890258789, 122.84125185012817, 124.41765904426575, 125.99406623840332, 127.52352547645569, 129.05298471450806, 130.58209323883057, 132.11120176315308, 133.64564442634583, 135.18008708953857, 136.73544096946716, 138.29079484939575, 139.8843879699707, 141.47798109054565, 142.98579859733582, 144.49361610412598, 146.09329676628113, 147.69297742843628, 149.26559901237488, 150.83822059631348, 152.38598489761353, 153.93374919891357, 155.4303638935089, 156.92697858810425, 158.3485701084137, 159.77016162872314, 161.2493965625763, 162.72863149642944, 164.2048544883728, 165.68107748031616, 167.17322087287903, 168.6653642654419, 170.14166474342346, 171.61796522140503, 173.14477968215942, 174.67159414291382, 176.16501307487488, 177.65843200683594, 179.17878413200378, 180.69913625717163, 182.26809215545654, 183.83704805374146, 185.32697129249573, 186.81689453125, 188.3150451183319, 189.81319570541382, 191.32190775871277, 192.83061981201172, 194.33784127235413, 195.84506273269653, 197.3151834011078, 198.78530406951904, 200.24228048324585, 201.69925689697266, 203.14999556541443, 204.6007342338562, 206.1180145740509, 207.6352949142456, 209.1652476787567, 210.69520044326782, 212.2544765472412, 213.8137526512146, 215.30612587928772, 216.79849910736084, 218.3198528289795, 219.84120655059814, 221.34722328186035, 222.85324001312256, 224.37118577957153, 225.8891315460205, 227.39204573631287, 228.89495992660522, 230.3565456867218, 231.81813144683838, 233.31483459472656, 234.81153774261475, 236.3131446838379, 237.81475162506104, 239.25744915008545, 240.70014667510986, 242.15165543556213, 243.6031641960144, 245.0945975780487, 246.586030960083, 248.04013204574585, 249.4942331314087, 250.960631608963, 252.42703008651733, 253.92390370368958, 255.42077732086182, 256.89638662338257, 258.3719959259033, 259.9009029865265, 261.42981004714966, 262.91330575942993, 264.3968014717102, 265.8507242202759, 267.30464696884155, 268.73868465423584, 270.1727223396301, 271.6064395904541, 273.0401568412781, 274.5006160736084, 275.9610753059387, 277.37591457366943, 278.79075384140015, 280.27036905288696, 281.7499842643738, 283.23322105407715, 284.7164578437805, 286.1805303096771, 287.64460277557373, 289.0993502140045, 290.5540976524353, 291.983078956604, 293.4120602607727, 294.9102737903595, 296.4084873199463, 297.8441162109375, 299.2797451019287, 300.7088186740875, 302.13789224624634, 304.01370429992676, 305.8895163536072]
[13.291666666666666, 13.291666666666666, 19.175, 19.175, 19.35, 19.35, 25.266666666666666, 25.266666666666666, 41.88333333333333, 41.88333333333333, 55.71666666666667, 55.71666666666667, 56.775, 56.775, 57.80833333333333, 57.80833333333333, 59.583333333333336, 59.583333333333336, 63.61666666666667, 63.61666666666667, 65.13333333333334, 65.13333333333334, 66.31666666666666, 66.31666666666666, 68.50833333333334, 68.50833333333334, 71.425, 71.425, 73.60833333333333, 73.60833333333333, 74.16666666666667, 74.16666666666667, 75.26666666666667, 75.26666666666667, 76.75833333333334, 76.75833333333334, 79.175, 79.175, 80.43333333333334, 80.43333333333334, 81.3, 81.3, 82.64166666666667, 82.64166666666667, 85.3, 85.3, 87.825, 87.825, 89.29166666666667, 89.29166666666667, 90.35, 90.35, 90.90833333333333, 90.90833333333333, 91.325, 91.325, 91.8, 91.8, 91.95833333333333, 91.95833333333333, 92.28333333333333, 92.28333333333333, 92.25833333333334, 92.25833333333334, 92.60833333333333, 92.60833333333333, 92.9, 92.9, 93.06666666666666, 93.06666666666666, 93.19166666666666, 93.19166666666666, 93.26666666666667, 93.26666666666667, 93.36666666666666, 93.36666666666666, 93.46666666666667, 93.46666666666667, 93.49166666666666, 93.49166666666666, 93.66666666666667, 93.66666666666667, 93.98333333333333, 93.98333333333333, 94.05, 94.05, 94.025, 94.025, 94.21666666666667, 94.21666666666667, 94.275, 94.275, 94.25, 94.25, 94.4, 94.4, 94.46666666666667, 94.46666666666667, 94.53333333333333, 94.53333333333333, 94.49166666666666, 94.49166666666666, 94.55833333333334, 94.55833333333334, 94.64166666666667, 94.64166666666667, 94.675, 94.675, 94.88333333333334, 94.88333333333334, 94.85833333333333, 94.85833333333333, 95.04166666666667, 95.04166666666667, 94.90833333333333, 94.90833333333333, 94.99166666666666, 94.99166666666666, 95.16666666666667, 95.16666666666667, 95.11666666666666, 95.11666666666666, 95.28333333333333, 95.28333333333333, 95.30833333333334, 95.30833333333334, 95.29166666666667, 95.29166666666667, 95.425, 95.425, 95.48333333333333, 95.48333333333333, 95.425, 95.425, 95.58333333333333, 95.58333333333333, 95.55, 95.55, 95.525, 95.525, 95.61666666666666, 95.61666666666666, 95.73333333333333, 95.73333333333333, 95.65, 95.65, 95.79166666666667, 95.79166666666667, 95.75833333333334, 95.75833333333334, 95.70833333333333, 95.70833333333333, 95.8, 95.8, 95.775, 95.775, 95.80833333333334, 95.80833333333334, 95.83333333333333, 95.83333333333333, 95.73333333333333, 95.73333333333333, 95.86666666666666, 95.86666666666666, 95.84166666666667, 95.84166666666667, 95.86666666666666, 95.86666666666666, 95.91666666666667, 95.91666666666667, 95.95833333333333, 95.95833333333333, 95.98333333333333, 95.98333333333333, 95.99166666666666, 95.99166666666666, 96.01666666666667, 96.01666666666667, 96.025, 96.025, 96.06666666666666, 96.06666666666666, 96.06666666666666, 96.06666666666666, 96.025, 96.025, 96.10833333333333, 96.10833333333333, 96.2, 96.2, 96.14166666666667, 96.14166666666667, 96.13333333333334, 96.13333333333334, 96.15833333333333, 96.15833333333333, 96.09166666666667, 96.09166666666667, 96.04166666666667, 96.04166666666667, 96.11666666666666, 96.11666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.230, Test loss: 2.221, Test accuracy: 19.56
Round   0, Global train loss: 2.230, Global test loss: 2.300, Global test accuracy: 12.67
Round   1, Train loss: 1.924, Test loss: 2.076, Test accuracy: 39.98
Round   1, Global train loss: 1.924, Global test loss: 2.283, Global test accuracy: 15.00
Round   2, Train loss: 1.832, Test loss: 1.951, Test accuracy: 52.38
Round   2, Global train loss: 1.832, Global test loss: 2.266, Global test accuracy: 18.34
Round   3, Train loss: 1.713, Test loss: 1.878, Test accuracy: 58.78
Round   3, Global train loss: 1.713, Global test loss: 2.290, Global test accuracy: 11.77
Round   4, Train loss: 1.716, Test loss: 1.793, Test accuracy: 67.94
Round   4, Global train loss: 1.716, Global test loss: 2.267, Global test accuracy: 16.79
Round   5, Train loss: 1.763, Test loss: 1.724, Test accuracy: 76.24
Round   5, Global train loss: 1.763, Global test loss: 2.251, Global test accuracy: 18.97
Round   6, Train loss: 1.645, Test loss: 1.720, Test accuracy: 76.34
Round   6, Global train loss: 1.645, Global test loss: 2.296, Global test accuracy: 11.24
Round   7, Train loss: 1.652, Test loss: 1.659, Test accuracy: 81.14
Round   7, Global train loss: 1.652, Global test loss: 2.286, Global test accuracy: 16.45
Round   8, Train loss: 1.705, Test loss: 1.655, Test accuracy: 81.28
Round   8, Global train loss: 1.705, Global test loss: 2.261, Global test accuracy: 18.73
Round   9, Train loss: 1.637, Test loss: 1.653, Test accuracy: 81.32
Round   9, Global train loss: 1.637, Global test loss: 2.259, Global test accuracy: 20.31
Round  10, Train loss: 1.571, Test loss: 1.637, Test accuracy: 83.03
Round  10, Global train loss: 1.571, Global test loss: 2.265, Global test accuracy: 16.53
Round  11, Train loss: 1.583, Test loss: 1.635, Test accuracy: 83.08
Round  11, Global train loss: 1.583, Global test loss: 2.262, Global test accuracy: 17.81
Round  12, Train loss: 1.530, Test loss: 1.634, Test accuracy: 83.20
Round  12, Global train loss: 1.530, Global test loss: 2.272, Global test accuracy: 15.16
Round  13, Train loss: 1.581, Test loss: 1.632, Test accuracy: 83.27
Round  13, Global train loss: 1.581, Global test loss: 2.247, Global test accuracy: 19.38
Round  14, Train loss: 1.616, Test loss: 1.603, Test accuracy: 86.32
Round  14, Global train loss: 1.616, Global test loss: 2.260, Global test accuracy: 16.66
Round  15, Train loss: 1.693, Test loss: 1.603, Test accuracy: 86.33
Round  15, Global train loss: 1.693, Global test loss: 2.268, Global test accuracy: 19.02
Round  16, Train loss: 1.527, Test loss: 1.602, Test accuracy: 86.37
Round  16, Global train loss: 1.527, Global test loss: 2.238, Global test accuracy: 19.95
Round  17, Train loss: 1.580, Test loss: 1.601, Test accuracy: 86.40
Round  17, Global train loss: 1.580, Global test loss: 2.233, Global test accuracy: 24.70
Round  18, Train loss: 1.525, Test loss: 1.601, Test accuracy: 86.41
Round  18, Global train loss: 1.525, Global test loss: 2.260, Global test accuracy: 16.77
Round  19, Train loss: 1.523, Test loss: 1.601, Test accuracy: 86.40
Round  19, Global train loss: 1.523, Global test loss: 2.246, Global test accuracy: 17.91
Round  20, Train loss: 1.523, Test loss: 1.601, Test accuracy: 86.42
Round  20, Global train loss: 1.523, Global test loss: 2.239, Global test accuracy: 19.64
Round  21, Train loss: 1.474, Test loss: 1.600, Test accuracy: 86.44
Round  21, Global train loss: 1.474, Global test loss: 2.266, Global test accuracy: 19.57
Round  22, Train loss: 1.523, Test loss: 1.600, Test accuracy: 86.44
Round  22, Global train loss: 1.523, Global test loss: 2.248, Global test accuracy: 19.48
Round  23, Train loss: 1.470, Test loss: 1.600, Test accuracy: 86.42
Round  23, Global train loss: 1.470, Global test loss: 2.279, Global test accuracy: 15.15
Round  24, Train loss: 1.679, Test loss: 1.589, Test accuracy: 87.72
Round  24, Global train loss: 1.679, Global test loss: 2.266, Global test accuracy: 17.52
Round  25, Train loss: 1.524, Test loss: 1.589, Test accuracy: 87.70
Round  25, Global train loss: 1.524, Global test loss: 2.272, Global test accuracy: 14.93
Round  26, Train loss: 1.584, Test loss: 1.586, Test accuracy: 87.87
Round  26, Global train loss: 1.584, Global test loss: 2.261, Global test accuracy: 17.00
Round  27, Train loss: 1.468, Test loss: 1.585, Test accuracy: 87.92
Round  27, Global train loss: 1.468, Global test loss: 2.279, Global test accuracy: 13.76
Round  28, Train loss: 1.616, Test loss: 1.573, Test accuracy: 89.27
Round  28, Global train loss: 1.616, Global test loss: 2.278, Global test accuracy: 14.32
Round  29, Train loss: 1.468, Test loss: 1.573, Test accuracy: 89.26
Round  29, Global train loss: 1.468, Global test loss: 2.262, Global test accuracy: 18.48
Round  30, Train loss: 1.588, Test loss: 1.571, Test accuracy: 89.33
Round  30, Global train loss: 1.588, Global test loss: 2.274, Global test accuracy: 18.19
Round  31, Train loss: 1.547, Test loss: 1.559, Test accuracy: 90.65
Round  31, Global train loss: 1.547, Global test loss: 2.241, Global test accuracy: 22.21
Round  32, Train loss: 1.528, Test loss: 1.557, Test accuracy: 90.78
Round  32, Global train loss: 1.528, Global test loss: 2.268, Global test accuracy: 17.38
Round  33, Train loss: 1.528, Test loss: 1.557, Test accuracy: 90.79
Round  33, Global train loss: 1.528, Global test loss: 2.283, Global test accuracy: 14.40
Round  34, Train loss: 1.471, Test loss: 1.557, Test accuracy: 90.77
Round  34, Global train loss: 1.471, Global test loss: 2.278, Global test accuracy: 15.50
Round  35, Train loss: 1.525, Test loss: 1.557, Test accuracy: 90.79
Round  35, Global train loss: 1.525, Global test loss: 2.240, Global test accuracy: 20.54
Round  36, Train loss: 1.578, Test loss: 1.556, Test accuracy: 90.81
Round  36, Global train loss: 1.578, Global test loss: 2.309, Global test accuracy: 12.83
Round  37, Train loss: 1.468, Test loss: 1.556, Test accuracy: 90.83
Round  37, Global train loss: 1.468, Global test loss: 2.268, Global test accuracy: 16.82
Round  38, Train loss: 1.524, Test loss: 1.556, Test accuracy: 90.83
Round  38, Global train loss: 1.524, Global test loss: 2.293, Global test accuracy: 13.84
Round  39, Train loss: 1.468, Test loss: 1.556, Test accuracy: 90.83
Round  39, Global train loss: 1.468, Global test loss: 2.244, Global test accuracy: 22.67
Round  40, Train loss: 1.578, Test loss: 1.556, Test accuracy: 90.82
Round  40, Global train loss: 1.578, Global test loss: 2.288, Global test accuracy: 15.07
Round  41, Train loss: 1.466, Test loss: 1.555, Test accuracy: 90.90
Round  41, Global train loss: 1.466, Global test loss: 2.229, Global test accuracy: 22.87
Round  42, Train loss: 1.520, Test loss: 1.555, Test accuracy: 90.88
Round  42, Global train loss: 1.520, Global test loss: 2.235, Global test accuracy: 22.38
Round  43, Train loss: 1.577, Test loss: 1.555, Test accuracy: 90.89
Round  43, Global train loss: 1.577, Global test loss: 2.262, Global test accuracy: 16.03
Round  44, Train loss: 1.577, Test loss: 1.555, Test accuracy: 90.87
Round  44, Global train loss: 1.577, Global test loss: 2.262, Global test accuracy: 18.58
Round  45, Train loss: 1.469, Test loss: 1.555, Test accuracy: 90.88
Round  45, Global train loss: 1.469, Global test loss: 2.266, Global test accuracy: 16.70
Round  46, Train loss: 1.520, Test loss: 1.555, Test accuracy: 90.89
Round  46, Global train loss: 1.520, Global test loss: 2.258, Global test accuracy: 22.84
Round  47, Train loss: 1.470, Test loss: 1.555, Test accuracy: 90.93
Round  47, Global train loss: 1.470, Global test loss: 2.291, Global test accuracy: 14.17
Round  48, Train loss: 1.575, Test loss: 1.555, Test accuracy: 90.90
Round  48, Global train loss: 1.575, Global test loss: 2.247, Global test accuracy: 20.82
Round  49, Train loss: 1.521, Test loss: 1.555, Test accuracy: 90.90
Round  49, Global train loss: 1.521, Global test loss: 2.281, Global test accuracy: 13.79
Round  50, Train loss: 1.577, Test loss: 1.555, Test accuracy: 90.92
Round  50, Global train loss: 1.577, Global test loss: 2.274, Global test accuracy: 16.89
Round  51, Train loss: 1.522, Test loss: 1.555, Test accuracy: 90.92
Round  51, Global train loss: 1.522, Global test loss: 2.320, Global test accuracy: 10.01
Round  52, Train loss: 1.467, Test loss: 1.555, Test accuracy: 90.92
Round  52, Global train loss: 1.467, Global test loss: 2.307, Global test accuracy: 11.59
Round  53, Train loss: 1.576, Test loss: 1.555, Test accuracy: 90.91
Round  53, Global train loss: 1.576, Global test loss: 2.307, Global test accuracy: 12.36
Round  54, Train loss: 1.522, Test loss: 1.555, Test accuracy: 90.88
Round  54, Global train loss: 1.522, Global test loss: 2.291, Global test accuracy: 12.65
Round  55, Train loss: 1.520, Test loss: 1.555, Test accuracy: 90.87
Round  55, Global train loss: 1.520, Global test loss: 2.241, Global test accuracy: 21.59
Round  56, Train loss: 1.523, Test loss: 1.555, Test accuracy: 90.88
Round  56, Global train loss: 1.523, Global test loss: 2.292, Global test accuracy: 13.88
Round  57, Train loss: 1.466, Test loss: 1.554, Test accuracy: 90.86
Round  57, Global train loss: 1.466, Global test loss: 2.259, Global test accuracy: 17.31
Round  58, Train loss: 1.466, Test loss: 1.554, Test accuracy: 90.86
Round  58, Global train loss: 1.466, Global test loss: 2.262, Global test accuracy: 16.84
Round  59, Train loss: 1.465, Test loss: 1.555, Test accuracy: 90.84
Round  59, Global train loss: 1.465, Global test loss: 2.272, Global test accuracy: 15.04
Round  60, Train loss: 1.576, Test loss: 1.555, Test accuracy: 90.88
Round  60, Global train loss: 1.576, Global test loss: 2.295, Global test accuracy: 10.79
Round  61, Train loss: 1.522, Test loss: 1.555, Test accuracy: 90.88
Round  61, Global train loss: 1.522, Global test loss: 2.287, Global test accuracy: 13.62
Round  62, Train loss: 1.521, Test loss: 1.555, Test accuracy: 90.87
Round  62, Global train loss: 1.521, Global test loss: 2.249, Global test accuracy: 20.51
Round  63, Train loss: 1.575, Test loss: 1.554, Test accuracy: 90.85
Round  63, Global train loss: 1.575, Global test loss: 2.314, Global test accuracy: 13.31
Round  64, Train loss: 1.573, Test loss: 1.554, Test accuracy: 90.86
Round  64, Global train loss: 1.573, Global test loss: 2.291, Global test accuracy: 14.28
Round  65, Train loss: 1.468, Test loss: 1.554, Test accuracy: 90.88
Round  65, Global train loss: 1.468, Global test loss: 2.283, Global test accuracy: 17.01
Round  66, Train loss: 1.469, Test loss: 1.554, Test accuracy: 90.88
Round  66, Global train loss: 1.469, Global test loss: 2.324, Global test accuracy: 10.09
Round  67, Train loss: 1.464, Test loss: 1.554, Test accuracy: 90.88
Round  67, Global train loss: 1.464, Global test loss: 2.251, Global test accuracy: 20.71
Round  68, Train loss: 1.518, Test loss: 1.554, Test accuracy: 90.90
Round  68, Global train loss: 1.518, Global test loss: 2.256, Global test accuracy: 19.98
Round  69, Train loss: 1.576, Test loss: 1.554, Test accuracy: 90.90
Round  69, Global train loss: 1.576, Global test loss: 2.272, Global test accuracy: 15.41
Round  70, Train loss: 1.576, Test loss: 1.554, Test accuracy: 90.90
Round  70, Global train loss: 1.576, Global test loss: 2.241, Global test accuracy: 21.56
Round  71, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.90
Round  71, Global train loss: 1.520, Global test loss: 2.269, Global test accuracy: 17.33
Round  72, Train loss: 1.521, Test loss: 1.554, Test accuracy: 90.92
Round  72, Global train loss: 1.521, Global test loss: 2.245, Global test accuracy: 21.92
Round  73, Train loss: 1.469, Test loss: 1.554, Test accuracy: 90.92
Round  73, Global train loss: 1.469, Global test loss: 2.282, Global test accuracy: 15.83
Round  74, Train loss: 1.521, Test loss: 1.554, Test accuracy: 90.93
Round  74, Global train loss: 1.521, Global test loss: 2.323, Global test accuracy: 10.22
Round  75, Train loss: 1.467, Test loss: 1.554, Test accuracy: 90.92
Round  75, Global train loss: 1.467, Global test loss: 2.304, Global test accuracy: 14.98
Round  76, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.91
Round  76, Global train loss: 1.520, Global test loss: 2.244, Global test accuracy: 20.88
Round  77, Train loss: 1.576, Test loss: 1.554, Test accuracy: 90.91
Round  77, Global train loss: 1.576, Global test loss: 2.274, Global test accuracy: 16.41
Round  78, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.92
Round  78, Global train loss: 1.520, Global test loss: 2.228, Global test accuracy: 24.08
Round  79, Train loss: 1.519, Test loss: 1.554, Test accuracy: 90.92
Round  79, Global train loss: 1.519, Global test loss: 2.227, Global test accuracy: 22.99
Round  80, Train loss: 1.522, Test loss: 1.554, Test accuracy: 90.92
Round  80, Global train loss: 1.522, Global test loss: 2.286, Global test accuracy: 14.48
Round  81, Train loss: 1.465, Test loss: 1.554, Test accuracy: 90.92
Round  81, Global train loss: 1.465, Global test loss: 2.256, Global test accuracy: 20.10
Round  82, Train loss: 1.518, Test loss: 1.554, Test accuracy: 90.91
Round  82, Global train loss: 1.518, Global test loss: 2.280, Global test accuracy: 15.94
Round  83, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.92
Round  83, Global train loss: 1.520, Global test loss: 2.286, Global test accuracy: 15.25
Round  84, Train loss: 1.466, Test loss: 1.554, Test accuracy: 90.92
Round  84, Global train loss: 1.466, Global test loss: 2.261, Global test accuracy: 16.98
Round  85, Train loss: 1.578, Test loss: 1.554, Test accuracy: 90.92
Round  85, Global train loss: 1.578, Global test loss: 2.273, Global test accuracy: 16.29
Round  86, Train loss: 1.464, Test loss: 1.554, Test accuracy: 90.94
Round  86, Global train loss: 1.464, Global test loss: 2.249, Global test accuracy: 19.37
Round  87, Train loss: 1.573, Test loss: 1.554, Test accuracy: 90.94
Round  87, Global train loss: 1.573, Global test loss: 2.276, Global test accuracy: 15.23
Round  88, Train loss: 1.577, Test loss: 1.554, Test accuracy: 90.94
Round  88, Global train loss: 1.577, Global test loss: 2.284, Global test accuracy: 14.94
Round  89, Train loss: 1.521, Test loss: 1.554, Test accuracy: 90.95
Round  89, Global train loss: 1.521, Global test loss: 2.298, Global test accuracy: 10.67
Round  90, Train loss: 1.465, Test loss: 1.554, Test accuracy: 90.95
Round  90, Global train loss: 1.465, Global test loss: 2.267, Global test accuracy: 16.57
Round  91, Train loss: 1.519, Test loss: 1.554, Test accuracy: 90.95
Round  91, Global train loss: 1.519, Global test loss: 2.283, Global test accuracy: 14.12
Round  92, Train loss: 1.467, Test loss: 1.554, Test accuracy: 90.94
Round  92, Global train loss: 1.467, Global test loss: 2.253, Global test accuracy: 17.53
Round  93, Train loss: 1.522, Test loss: 1.554, Test accuracy: 90.94
Round  93, Global train loss: 1.522, Global test loss: 2.294, Global test accuracy: 14.11
Round  94, Train loss: 1.573, Test loss: 1.554, Test accuracy: 90.94
Round  94, Global train loss: 1.573, Global test loss: 2.278, Global test accuracy: 15.53
Round  95, Train loss: 1.465, Test loss: 1.554, Test accuracy: 90.93
Round  95, Global train loss: 1.465, Global test loss: 2.259, Global test accuracy: 18.36/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.94
Round  96, Global train loss: 1.520, Global test loss: 2.294, Global test accuracy: 13.21
Round  97, Train loss: 1.521, Test loss: 1.552, Test accuracy: 91.04
Round  97, Global train loss: 1.521, Global test loss: 2.289, Global test accuracy: 12.89
Round  98, Train loss: 1.520, Test loss: 1.552, Test accuracy: 91.04
Round  98, Global train loss: 1.520, Global test loss: 2.233, Global test accuracy: 21.27
Round  99, Train loss: 1.522, Test loss: 1.552, Test accuracy: 91.04
Round  99, Global train loss: 1.522, Global test loss: 2.261, Global test accuracy: 19.27
Final Round, Train loss: 1.502, Test loss: 1.540, Test accuracy: 92.36
Final Round, Global train loss: 1.502, Global test loss: 2.261, Global test accuracy: 19.27
Average accuracy final 10 rounds: 90.97250000000001 

Average global accuracy final 10 rounds: 16.285833333333333 

1801.404032945633
[1.202974557876587, 2.405949115753174, 3.5640451908111572, 4.722141265869141, 5.947144985198975, 7.172148704528809, 8.408240795135498, 9.644332885742188, 10.85632848739624, 12.068324089050293, 13.24002742767334, 14.411730766296387, 15.65399718284607, 16.896263599395752, 18.114308834075928, 19.332354068756104, 20.50759720802307, 21.68284034729004, 22.90056610107422, 24.1182918548584, 25.35043168067932, 26.582571506500244, 27.759640216827393, 28.93670892715454, 30.119654417037964, 31.302599906921387, 32.52321147918701, 33.74382305145264, 34.98256874084473, 36.221314430236816, 37.422566175460815, 38.623817920684814, 39.845356941223145, 41.066895961761475, 42.26981735229492, 43.47273874282837, 44.67629528045654, 45.87985181808472, 47.122785329818726, 48.365718841552734, 49.630249977111816, 50.8947811126709, 52.1160204410553, 53.3372597694397, 54.475598096847534, 55.61393642425537, 56.803555488586426, 57.99317455291748, 59.16562509536743, 60.33807563781738, 61.50686287879944, 62.675650119781494, 63.831395626068115, 64.98714113235474, 66.27583527565002, 67.56452941894531, 68.781809091568, 69.99908876419067, 71.17025232315063, 72.3414158821106, 73.53971982002258, 74.73802375793457, 75.92772436141968, 77.11742496490479, 78.2515480518341, 79.38567113876343, 80.55656933784485, 81.72746753692627, 82.9206211566925, 84.11377477645874, 85.288747549057, 86.46372032165527, 87.61806750297546, 88.77241468429565, 89.90862154960632, 91.04482841491699, 92.260178565979, 93.47552871704102, 94.63362693786621, 95.7917251586914, 96.95106744766235, 98.1104097366333, 99.32282853126526, 100.53524732589722, 101.71024370193481, 102.88524007797241, 103.88538360595703, 104.88552713394165, 106.05201506614685, 107.21850299835205, 108.37666535377502, 109.534827709198, 110.70044565200806, 111.86606359481812, 113.05040740966797, 114.23475122451782, 115.42209601402283, 116.60944080352783, 117.82506442070007, 119.04068803787231, 120.18178176879883, 121.32287549972534, 122.4629898071289, 123.60310411453247, 124.83839058876038, 126.07367706298828, 127.23813796043396, 128.40259885787964, 129.58421087265015, 130.76582288742065, 131.9867241382599, 133.20762538909912, 134.39084577560425, 135.57406616210938, 136.75962567329407, 137.94518518447876, 139.15305519104004, 140.36092519760132, 141.59630131721497, 142.8316774368286, 144.05815649032593, 145.28463554382324, 146.4719524383545, 147.65926933288574, 148.87217712402344, 150.08508491516113, 151.30055236816406, 152.516019821167, 153.69047379493713, 154.86492776870728, 156.03936314582825, 157.21379852294922, 158.4103581905365, 159.60691785812378, 160.82915592193604, 162.0513939857483, 163.217209815979, 164.38302564620972, 165.55000925064087, 166.71699285507202, 167.8670403957367, 169.01708793640137, 170.1622326374054, 171.30737733840942, 172.43615865707397, 173.56493997573853, 174.77116322517395, 175.97738647460938, 177.19971823692322, 178.42204999923706, 179.56673002243042, 180.71141004562378, 181.8573977947235, 183.00338554382324, 184.16593289375305, 185.32848024368286, 186.47112607955933, 187.6137719154358, 188.79717636108398, 189.98058080673218, 191.16113257408142, 192.34168434143066, 193.4669976234436, 194.59231090545654, 195.7334485054016, 196.87458610534668, 198.01002597808838, 199.14546585083008, 200.33109617233276, 201.51672649383545, 202.6541452407837, 203.79156398773193, 204.9309446811676, 206.07032537460327, 207.29072499275208, 208.51112461090088, 209.69191980361938, 210.8727149963379, 212.06181240081787, 213.25090980529785, 214.43434953689575, 215.61778926849365, 216.73513531684875, 217.85248136520386, 218.9941508769989, 220.13582038879395, 221.26612877845764, 222.39643716812134, 223.5663845539093, 224.73633193969727, 225.87589120864868, 227.0154504776001, 228.16839909553528, 229.32134771347046, 230.5105278491974, 231.69970798492432, 232.81796026229858, 233.93621253967285, 235.09271025657654, 236.24920797348022, 238.18091988563538, 240.11263179779053]
[19.558333333333334, 19.558333333333334, 39.975, 39.975, 52.375, 52.375, 58.78333333333333, 58.78333333333333, 67.94166666666666, 67.94166666666666, 76.24166666666666, 76.24166666666666, 76.34166666666667, 76.34166666666667, 81.14166666666667, 81.14166666666667, 81.275, 81.275, 81.31666666666666, 81.31666666666666, 83.03333333333333, 83.03333333333333, 83.075, 83.075, 83.2, 83.2, 83.26666666666667, 83.26666666666667, 86.31666666666666, 86.31666666666666, 86.33333333333333, 86.33333333333333, 86.36666666666666, 86.36666666666666, 86.4, 86.4, 86.40833333333333, 86.40833333333333, 86.4, 86.4, 86.425, 86.425, 86.44166666666666, 86.44166666666666, 86.44166666666666, 86.44166666666666, 86.425, 86.425, 87.725, 87.725, 87.7, 87.7, 87.86666666666666, 87.86666666666666, 87.925, 87.925, 89.26666666666667, 89.26666666666667, 89.25833333333334, 89.25833333333334, 89.33333333333333, 89.33333333333333, 90.65, 90.65, 90.775, 90.775, 90.79166666666667, 90.79166666666667, 90.76666666666667, 90.76666666666667, 90.79166666666667, 90.79166666666667, 90.80833333333334, 90.80833333333334, 90.825, 90.825, 90.83333333333333, 90.83333333333333, 90.83333333333333, 90.83333333333333, 90.81666666666666, 90.81666666666666, 90.9, 90.9, 90.875, 90.875, 90.89166666666667, 90.89166666666667, 90.86666666666666, 90.86666666666666, 90.88333333333334, 90.88333333333334, 90.89166666666667, 90.89166666666667, 90.93333333333334, 90.93333333333334, 90.9, 90.9, 90.9, 90.9, 90.925, 90.925, 90.91666666666667, 90.91666666666667, 90.91666666666667, 90.91666666666667, 90.90833333333333, 90.90833333333333, 90.875, 90.875, 90.86666666666666, 90.86666666666666, 90.875, 90.875, 90.85833333333333, 90.85833333333333, 90.85833333333333, 90.85833333333333, 90.84166666666667, 90.84166666666667, 90.88333333333334, 90.88333333333334, 90.875, 90.875, 90.86666666666666, 90.86666666666666, 90.85, 90.85, 90.85833333333333, 90.85833333333333, 90.88333333333334, 90.88333333333334, 90.875, 90.875, 90.88333333333334, 90.88333333333334, 90.9, 90.9, 90.9, 90.9, 90.9, 90.9, 90.9, 90.9, 90.925, 90.925, 90.925, 90.925, 90.93333333333334, 90.93333333333334, 90.91666666666667, 90.91666666666667, 90.90833333333333, 90.90833333333333, 90.90833333333333, 90.90833333333333, 90.925, 90.925, 90.925, 90.925, 90.91666666666667, 90.91666666666667, 90.91666666666667, 90.91666666666667, 90.90833333333333, 90.90833333333333, 90.91666666666667, 90.91666666666667, 90.925, 90.925, 90.925, 90.925, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.95, 90.95, 90.95, 90.95, 90.95, 90.95, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.93333333333334, 90.93333333333334, 90.94166666666666, 90.94166666666666, 91.04166666666667, 91.04166666666667, 91.04166666666667, 91.04166666666667, 91.04166666666667, 91.04166666666667, 92.35833333333333, 92.35833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.300, Test accuracy: 16.57
Round   0, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.38
Round   1, Train loss: 2.294, Test loss: 2.293, Test accuracy: 18.71
Round   1, Global train loss: 2.294, Global test loss: 2.301, Global test accuracy: 11.80
Round   2, Train loss: 2.278, Test loss: 2.275, Test accuracy: 23.35
Round   2, Global train loss: 2.278, Global test loss: 2.300, Global test accuracy: 11.57
Round   3, Train loss: 2.218, Test loss: 2.231, Test accuracy: 27.83
Round   3, Global train loss: 2.218, Global test loss: 2.296, Global test accuracy: 11.72
Round   4, Train loss: 2.099, Test loss: 2.180, Test accuracy: 30.98
Round   4, Global train loss: 2.099, Global test loss: 2.302, Global test accuracy: 12.43
Round   5, Train loss: 2.034, Test loss: 2.135, Test accuracy: 35.83
Round   5, Global train loss: 2.034, Global test loss: 2.307, Global test accuracy: 12.59
Round   6, Train loss: 2.089, Test loss: 2.092, Test accuracy: 39.30
Round   6, Global train loss: 2.089, Global test loss: 2.305, Global test accuracy: 12.67
Round   7, Train loss: 1.979, Test loss: 2.065, Test accuracy: 41.39
Round   7, Global train loss: 1.979, Global test loss: 2.308, Global test accuracy: 12.87
Round   8, Train loss: 2.123, Test loss: 2.042, Test accuracy: 41.97
Round   8, Global train loss: 2.123, Global test loss: 2.302, Global test accuracy: 12.59
Round   9, Train loss: 2.049, Test loss: 2.037, Test accuracy: 42.72
Round   9, Global train loss: 2.049, Global test loss: 2.304, Global test accuracy: 13.10
Round  10, Train loss: 2.061, Test loss: 2.029, Test accuracy: 43.60
Round  10, Global train loss: 2.061, Global test loss: 2.304, Global test accuracy: 13.31
Round  11, Train loss: 1.992, Test loss: 2.017, Test accuracy: 44.91
Round  11, Global train loss: 1.992, Global test loss: 2.295, Global test accuracy: 14.28
Round  12, Train loss: 2.052, Test loss: 2.007, Test accuracy: 45.97
Round  12, Global train loss: 2.052, Global test loss: 2.295, Global test accuracy: 14.08
Round  13, Train loss: 2.032, Test loss: 1.995, Test accuracy: 46.81
Round  13, Global train loss: 2.032, Global test loss: 2.293, Global test accuracy: 13.96
Round  14, Train loss: 2.025, Test loss: 1.987, Test accuracy: 47.58
Round  14, Global train loss: 2.025, Global test loss: 2.293, Global test accuracy: 14.67
Round  15, Train loss: 1.982, Test loss: 1.982, Test accuracy: 48.11
Round  15, Global train loss: 1.982, Global test loss: 2.294, Global test accuracy: 13.96
Round  16, Train loss: 1.994, Test loss: 1.985, Test accuracy: 47.54
Round  16, Global train loss: 1.994, Global test loss: 2.297, Global test accuracy: 14.03
Round  17, Train loss: 1.987, Test loss: 1.980, Test accuracy: 48.14
Round  17, Global train loss: 1.987, Global test loss: 2.298, Global test accuracy: 14.13
Round  18, Train loss: 1.995, Test loss: 1.974, Test accuracy: 48.43
Round  18, Global train loss: 1.995, Global test loss: 2.294, Global test accuracy: 14.89
Round  19, Train loss: 1.963, Test loss: 1.971, Test accuracy: 48.81
Round  19, Global train loss: 1.963, Global test loss: 2.295, Global test accuracy: 14.41
Round  20, Train loss: 1.980, Test loss: 1.968, Test accuracy: 49.24
Round  20, Global train loss: 1.980, Global test loss: 2.297, Global test accuracy: 13.57
Round  21, Train loss: 1.993, Test loss: 1.953, Test accuracy: 50.69
Round  21, Global train loss: 1.993, Global test loss: 2.286, Global test accuracy: 15.27
Round  22, Train loss: 2.002, Test loss: 1.960, Test accuracy: 50.01
Round  22, Global train loss: 2.002, Global test loss: 2.295, Global test accuracy: 13.65
Round  23, Train loss: 1.948, Test loss: 1.955, Test accuracy: 50.51
Round  23, Global train loss: 1.948, Global test loss: 2.289, Global test accuracy: 15.02
Round  24, Train loss: 1.954, Test loss: 1.948, Test accuracy: 51.26
Round  24, Global train loss: 1.954, Global test loss: 2.288, Global test accuracy: 14.61
Round  25, Train loss: 1.929, Test loss: 1.940, Test accuracy: 52.07
Round  25, Global train loss: 1.929, Global test loss: 2.286, Global test accuracy: 15.04
Round  26, Train loss: 1.950, Test loss: 1.937, Test accuracy: 52.31
Round  26, Global train loss: 1.950, Global test loss: 2.293, Global test accuracy: 13.97
Round  27, Train loss: 1.974, Test loss: 1.934, Test accuracy: 52.68
Round  27, Global train loss: 1.974, Global test loss: 2.285, Global test accuracy: 15.52
Round  28, Train loss: 1.943, Test loss: 1.938, Test accuracy: 52.16
Round  28, Global train loss: 1.943, Global test loss: 2.290, Global test accuracy: 14.92
Round  29, Train loss: 1.986, Test loss: 1.943, Test accuracy: 51.67
Round  29, Global train loss: 1.986, Global test loss: 2.300, Global test accuracy: 14.19
Round  30, Train loss: 1.964, Test loss: 1.932, Test accuracy: 52.86
Round  30, Global train loss: 1.964, Global test loss: 2.287, Global test accuracy: 15.20
Round  31, Train loss: 1.916, Test loss: 1.922, Test accuracy: 53.87
Round  31, Global train loss: 1.916, Global test loss: 2.292, Global test accuracy: 13.80
Round  32, Train loss: 1.890, Test loss: 1.923, Test accuracy: 53.66
Round  32, Global train loss: 1.890, Global test loss: 2.290, Global test accuracy: 15.06
Round  33, Train loss: 1.849, Test loss: 1.909, Test accuracy: 55.06
Round  33, Global train loss: 1.849, Global test loss: 2.292, Global test accuracy: 14.42
Round  34, Train loss: 1.914, Test loss: 1.905, Test accuracy: 55.49
Round  34, Global train loss: 1.914, Global test loss: 2.287, Global test accuracy: 15.31
Round  35, Train loss: 1.919, Test loss: 1.898, Test accuracy: 56.23
Round  35, Global train loss: 1.919, Global test loss: 2.290, Global test accuracy: 14.79
Round  36, Train loss: 1.972, Test loss: 1.906, Test accuracy: 55.28
Round  36, Global train loss: 1.972, Global test loss: 2.281, Global test accuracy: 15.58
Round  37, Train loss: 1.896, Test loss: 1.901, Test accuracy: 55.83
Round  37, Global train loss: 1.896, Global test loss: 2.289, Global test accuracy: 15.19
Round  38, Train loss: 1.919, Test loss: 1.900, Test accuracy: 55.92
Round  38, Global train loss: 1.919, Global test loss: 2.286, Global test accuracy: 14.77
Round  39, Train loss: 2.022, Test loss: 1.896, Test accuracy: 56.38
Round  39, Global train loss: 2.022, Global test loss: 2.284, Global test accuracy: 15.16
Round  40, Train loss: 1.955, Test loss: 1.890, Test accuracy: 57.02
Round  40, Global train loss: 1.955, Global test loss: 2.289, Global test accuracy: 14.43
Round  41, Train loss: 1.884, Test loss: 1.905, Test accuracy: 55.41
Round  41, Global train loss: 1.884, Global test loss: 2.287, Global test accuracy: 14.94
Round  42, Train loss: 1.874, Test loss: 1.902, Test accuracy: 55.71
Round  42, Global train loss: 1.874, Global test loss: 2.285, Global test accuracy: 15.63
Round  43, Train loss: 1.927, Test loss: 1.902, Test accuracy: 55.67
Round  43, Global train loss: 1.927, Global test loss: 2.284, Global test accuracy: 15.37
Round  44, Train loss: 1.885, Test loss: 1.893, Test accuracy: 56.54
Round  44, Global train loss: 1.885, Global test loss: 2.292, Global test accuracy: 14.36
Round  45, Train loss: 1.932, Test loss: 1.892, Test accuracy: 56.61
Round  45, Global train loss: 1.932, Global test loss: 2.278, Global test accuracy: 16.53
Round  46, Train loss: 1.894, Test loss: 1.896, Test accuracy: 56.23
Round  46, Global train loss: 1.894, Global test loss: 2.284, Global test accuracy: 15.51
Round  47, Train loss: 1.872, Test loss: 1.905, Test accuracy: 55.26
Round  47, Global train loss: 1.872, Global test loss: 2.289, Global test accuracy: 14.19
Round  48, Train loss: 1.935, Test loss: 1.900, Test accuracy: 55.78
Round  48, Global train loss: 1.935, Global test loss: 2.282, Global test accuracy: 16.05
Round  49, Train loss: 1.864, Test loss: 1.902, Test accuracy: 55.67
Round  49, Global train loss: 1.864, Global test loss: 2.289, Global test accuracy: 14.38
Round  50, Train loss: 1.925, Test loss: 1.894, Test accuracy: 56.50
Round  50, Global train loss: 1.925, Global test loss: 2.285, Global test accuracy: 15.20
Round  51, Train loss: 1.852, Test loss: 1.888, Test accuracy: 57.06
Round  51, Global train loss: 1.852, Global test loss: 2.288, Global test accuracy: 15.35
Round  52, Train loss: 1.956, Test loss: 1.883, Test accuracy: 57.69
Round  52, Global train loss: 1.956, Global test loss: 2.279, Global test accuracy: 16.19
Round  53, Train loss: 1.908, Test loss: 1.867, Test accuracy: 59.40
Round  53, Global train loss: 1.908, Global test loss: 2.283, Global test accuracy: 15.36
Round  54, Train loss: 1.817, Test loss: 1.854, Test accuracy: 60.87
Round  54, Global train loss: 1.817, Global test loss: 2.287, Global test accuracy: 14.99
Round  55, Train loss: 1.817, Test loss: 1.842, Test accuracy: 62.21
Round  55, Global train loss: 1.817, Global test loss: 2.305, Global test accuracy: 13.02
Round  56, Train loss: 1.791, Test loss: 1.841, Test accuracy: 62.24
Round  56, Global train loss: 1.791, Global test loss: 2.294, Global test accuracy: 14.39
Round  57, Train loss: 1.932, Test loss: 1.849, Test accuracy: 61.24
Round  57, Global train loss: 1.932, Global test loss: 2.304, Global test accuracy: 12.77
Round  58, Train loss: 1.779, Test loss: 1.842, Test accuracy: 62.06
Round  58, Global train loss: 1.779, Global test loss: 2.292, Global test accuracy: 14.12
Round  59, Train loss: 1.858, Test loss: 1.855, Test accuracy: 60.61
Round  59, Global train loss: 1.858, Global test loss: 2.298, Global test accuracy: 13.73
Round  60, Train loss: 1.895, Test loss: 1.872, Test accuracy: 58.57
Round  60, Global train loss: 1.895, Global test loss: 2.298, Global test accuracy: 13.76
Round  61, Train loss: 1.865, Test loss: 1.869, Test accuracy: 58.99
Round  61, Global train loss: 1.865, Global test loss: 2.286, Global test accuracy: 15.28
Round  62, Train loss: 1.889, Test loss: 1.864, Test accuracy: 59.47
Round  62, Global train loss: 1.889, Global test loss: 2.302, Global test accuracy: 13.78
Round  63, Train loss: 1.940, Test loss: 1.859, Test accuracy: 60.09
Round  63, Global train loss: 1.940, Global test loss: 2.294, Global test accuracy: 13.84
Round  64, Train loss: 1.853, Test loss: 1.862, Test accuracy: 59.66
Round  64, Global train loss: 1.853, Global test loss: 2.297, Global test accuracy: 13.79
Round  65, Train loss: 1.830, Test loss: 1.869, Test accuracy: 58.91
Round  65, Global train loss: 1.830, Global test loss: 2.281, Global test accuracy: 15.88
Round  66, Train loss: 1.837, Test loss: 1.867, Test accuracy: 59.15
Round  66, Global train loss: 1.837, Global test loss: 2.285, Global test accuracy: 15.02
Round  67, Train loss: 1.796, Test loss: 1.857, Test accuracy: 60.07
Round  67, Global train loss: 1.796, Global test loss: 2.295, Global test accuracy: 14.60
Round  68, Train loss: 1.823, Test loss: 1.865, Test accuracy: 59.22
Round  68, Global train loss: 1.823, Global test loss: 2.295, Global test accuracy: 13.82
Round  69, Train loss: 1.903, Test loss: 1.869, Test accuracy: 58.79
Round  69, Global train loss: 1.903, Global test loss: 2.288, Global test accuracy: 15.33
Round  70, Train loss: 1.932, Test loss: 1.860, Test accuracy: 59.79
Round  70, Global train loss: 1.932, Global test loss: 2.282, Global test accuracy: 15.72
Round  71, Train loss: 1.838, Test loss: 1.846, Test accuracy: 61.29
Round  71, Global train loss: 1.838, Global test loss: 2.285, Global test accuracy: 14.89
Round  72, Train loss: 1.762, Test loss: 1.842, Test accuracy: 61.56
Round  72, Global train loss: 1.762, Global test loss: 2.280, Global test accuracy: 15.57
Round  73, Train loss: 1.874, Test loss: 1.850, Test accuracy: 60.94
Round  73, Global train loss: 1.874, Global test loss: 2.280, Global test accuracy: 16.11
Round  74, Train loss: 1.865, Test loss: 1.829, Test accuracy: 63.06
Round  74, Global train loss: 1.865, Global test loss: 2.288, Global test accuracy: 14.63
Round  75, Train loss: 1.825, Test loss: 1.824, Test accuracy: 63.68
Round  75, Global train loss: 1.825, Global test loss: 2.281, Global test accuracy: 15.92
Round  76, Train loss: 1.806, Test loss: 1.821, Test accuracy: 63.98
Round  76, Global train loss: 1.806, Global test loss: 2.283, Global test accuracy: 15.09
Round  77, Train loss: 1.810, Test loss: 1.820, Test accuracy: 64.09
Round  77, Global train loss: 1.810, Global test loss: 2.289, Global test accuracy: 14.56
Round  78, Train loss: 1.804, Test loss: 1.818, Test accuracy: 64.23
Round  78, Global train loss: 1.804, Global test loss: 2.282, Global test accuracy: 15.56
Round  79, Train loss: 1.820, Test loss: 1.820, Test accuracy: 64.12
Round  79, Global train loss: 1.820, Global test loss: 2.277, Global test accuracy: 16.20
Round  80, Train loss: 1.852, Test loss: 1.819, Test accuracy: 64.11
Round  80, Global train loss: 1.852, Global test loss: 2.279, Global test accuracy: 16.33
Round  81, Train loss: 1.776, Test loss: 1.809, Test accuracy: 65.14
Round  81, Global train loss: 1.776, Global test loss: 2.279, Global test accuracy: 16.12
Round  82, Train loss: 1.804, Test loss: 1.813, Test accuracy: 64.69
Round  82, Global train loss: 1.804, Global test loss: 2.294, Global test accuracy: 13.88
Round  83, Train loss: 1.829, Test loss: 1.821, Test accuracy: 63.90
Round  83, Global train loss: 1.829, Global test loss: 2.288, Global test accuracy: 14.99
Round  84, Train loss: 1.808, Test loss: 1.811, Test accuracy: 64.94
Round  84, Global train loss: 1.808, Global test loss: 2.294, Global test accuracy: 14.51
Round  85, Train loss: 1.806, Test loss: 1.807, Test accuracy: 65.42
Round  85, Global train loss: 1.806, Global test loss: 2.292, Global test accuracy: 14.34
Round  86, Train loss: 1.843, Test loss: 1.802, Test accuracy: 65.84
Round  86, Global train loss: 1.843, Global test loss: 2.300, Global test accuracy: 13.73
Round  87, Train loss: 1.805, Test loss: 1.814, Test accuracy: 64.61
Round  87, Global train loss: 1.805, Global test loss: 2.286, Global test accuracy: 14.84
Round  88, Train loss: 1.790, Test loss: 1.810, Test accuracy: 65.07
Round  88, Global train loss: 1.790, Global test loss: 2.287, Global test accuracy: 14.98
Round  89, Train loss: 1.832, Test loss: 1.804, Test accuracy: 65.54
Round  89, Global train loss: 1.832, Global test loss: 2.289, Global test accuracy: 15.11
Round  90, Train loss: 1.860, Test loss: 1.805, Test accuracy: 65.53
Round  90, Global train loss: 1.860, Global test loss: 2.283, Global test accuracy: 15.28
Round  91, Train loss: 1.783, Test loss: 1.798, Test accuracy: 66.22
Round  91, Global train loss: 1.783, Global test loss: 2.279, Global test accuracy: 16.17
Round  92, Train loss: 1.796, Test loss: 1.800, Test accuracy: 66.08
Round  92, Global train loss: 1.796, Global test loss: 2.283, Global test accuracy: 15.31
Round  93, Train loss: 1.753, Test loss: 1.797, Test accuracy: 66.40
Round  93, Global train loss: 1.753, Global test loss: 2.281, Global test accuracy: 15.19
Round  94, Train loss: 1.779, Test loss: 1.785, Test accuracy: 67.66
Round  94, Global train loss: 1.779, Global test loss: 2.283, Global test accuracy: 15.26
Round  95, Train loss: 1.765, Test loss: 1.779, Test accuracy: 68.23
Round  95, Global train loss: 1.765, Global test loss: 2.286, Global test accuracy: 14.88
Round  96, Train loss: 1.710, Test loss: 1.779, Test accuracy: 68.34
Round  96, Global train loss: 1.710, Global test loss: 2.279, Global test accuracy: 15.91
Round  97, Train loss: 1.772, Test loss: 1.776, Test accuracy: 68.64
Round  97, Global train loss: 1.772, Global test loss: 2.287, Global test accuracy: 14.86
Round  98, Train loss: 1.812, Test loss: 1.770, Test accuracy: 69.33
Round  98, Global train loss: 1.812, Global test loss: 2.282, Global test accuracy: 15.16
Round  99, Train loss: 1.736, Test loss: 1.770, Test accuracy: 69.27
Round  99, Global train loss: 1.736, Global test loss: 2.286, Global test accuracy: 15.06
Final Round, Train loss: 1.725, Test loss: 1.767, Test accuracy: 69.61
Final Round, Global train loss: 1.725, Global test loss: 2.286, Global test accuracy: 15.06
Average accuracy final 10 rounds: 67.57 

Average global accuracy final 10 rounds: 15.30777777777778 

2613.2193512916565
[1.7365078926086426, 3.473015785217285, 5.123746633529663, 6.774477481842041, 8.411771059036255, 10.049064636230469, 11.780347108840942, 13.511629581451416, 15.194716453552246, 16.877803325653076, 18.5593364238739, 20.240869522094727, 21.92493486404419, 23.609000205993652, 25.29995822906494, 26.99091625213623, 28.650312423706055, 30.30970859527588, 31.976306676864624, 33.64290475845337, 35.2991988658905, 36.95549297332764, 38.61063265800476, 40.265772342681885, 41.921950340270996, 43.57812833786011, 45.26247525215149, 46.94682216644287, 48.61033773422241, 50.27385330200195, 52.00873875617981, 53.743624210357666, 55.38912892341614, 57.03463363647461, 58.66936445236206, 60.30409526824951, 61.99014639854431, 63.67619752883911, 65.28466939926147, 66.89314126968384, 68.60535931587219, 70.31757736206055, 71.90882182121277, 73.50006628036499, 75.1413266658783, 76.7825870513916, 78.4463369846344, 80.1100869178772, 81.6937415599823, 83.2773962020874, 85.05024838447571, 86.82310056686401, 88.44569206237793, 90.06828355789185, 91.77232670783997, 93.47636985778809, 95.16027593612671, 96.84418201446533, 98.54970002174377, 100.25521802902222, 101.99908757209778, 103.74295711517334, 105.38899159431458, 107.03502607345581, 108.81931948661804, 110.60361289978027, 112.27465987205505, 113.94570684432983, 115.75737953186035, 117.56905221939087, 119.35337781906128, 121.13770341873169, 122.83042335510254, 124.52314329147339, 126.21079874038696, 127.89845418930054, 129.58161091804504, 131.26476764678955, 132.97317457199097, 134.68158149719238, 136.39855074882507, 138.11552000045776, 139.77727723121643, 141.4390344619751, 143.12541699409485, 144.8117995262146, 146.48306822776794, 148.1543369293213, 149.8046474456787, 151.45495796203613, 153.13137674331665, 154.80779552459717, 156.4464933872223, 158.0851912498474, 159.79347562789917, 161.50176000595093, 163.1137535572052, 164.72574710845947, 166.40324091911316, 168.08073472976685, 169.74921870231628, 171.41770267486572, 173.06168627738953, 174.70566987991333, 176.46702814102173, 178.22838640213013, 179.9269859790802, 181.62558555603027, 183.31475114822388, 185.00391674041748, 186.7025363445282, 188.40115594863892, 190.11351895332336, 191.8258819580078, 193.51667428016663, 195.20746660232544, 196.976459980011, 198.74545335769653, 200.4303696155548, 202.1152858734131, 203.8013792037964, 205.4874725341797, 207.1654508113861, 208.84342908859253, 210.52811431884766, 212.21279954910278, 213.91799235343933, 215.62318515777588, 217.2733941078186, 218.92360305786133, 220.63256168365479, 222.34152030944824, 223.92746877670288, 225.51341724395752, 227.2004678249359, 228.8875184059143, 230.52179431915283, 232.15607023239136, 233.76443004608154, 235.37278985977173, 237.03011012077332, 238.6874303817749, 240.2613205909729, 241.8352108001709, 243.5921630859375, 245.3491153717041, 246.99085116386414, 248.63258695602417, 250.31594014167786, 251.99929332733154, 253.70525598526, 255.41121864318848, 257.0897870063782, 258.76835536956787, 260.53243494033813, 262.2965145111084, 263.97334480285645, 265.6501750946045, 267.3214089870453, 268.9926428794861, 270.724161863327, 272.45568084716797, 274.1973400115967, 275.9389991760254, 277.692182302475, 279.44536542892456, 281.1542341709137, 282.86310291290283, 284.4872844219208, 286.1114659309387, 287.51434230804443, 288.91721868515015, 290.30932569503784, 291.70143270492554, 293.22133708000183, 294.7412414550781, 296.1336135864258, 297.52598571777344, 299.22707200050354, 300.92815828323364, 302.5228669643402, 304.1175756454468, 305.6741864681244, 307.230797290802, 308.87333703041077, 310.51587677001953, 312.10780024528503, 313.69972372055054, 315.3788650035858, 317.0580062866211, 318.6896941661835, 320.32138204574585, 322.0149266719818, 323.7084712982178, 325.35819125175476, 327.00791120529175, 328.6419909000397, 330.2760705947876, 331.9506151676178, 333.625159740448, 335.57940578460693, 337.53365182876587]
[16.572222222222223, 16.572222222222223, 18.705555555555556, 18.705555555555556, 23.35, 23.35, 27.833333333333332, 27.833333333333332, 30.977777777777778, 30.977777777777778, 35.827777777777776, 35.827777777777776, 39.3, 39.3, 41.394444444444446, 41.394444444444446, 41.97222222222222, 41.97222222222222, 42.72222222222222, 42.72222222222222, 43.6, 43.6, 44.90555555555556, 44.90555555555556, 45.97222222222222, 45.97222222222222, 46.80555555555556, 46.80555555555556, 47.583333333333336, 47.583333333333336, 48.105555555555554, 48.105555555555554, 47.544444444444444, 47.544444444444444, 48.144444444444446, 48.144444444444446, 48.43333333333333, 48.43333333333333, 48.80555555555556, 48.80555555555556, 49.23888888888889, 49.23888888888889, 50.68888888888889, 50.68888888888889, 50.00555555555555, 50.00555555555555, 50.50555555555555, 50.50555555555555, 51.25555555555555, 51.25555555555555, 52.07222222222222, 52.07222222222222, 52.31111111111111, 52.31111111111111, 52.68333333333333, 52.68333333333333, 52.16111111111111, 52.16111111111111, 51.666666666666664, 51.666666666666664, 52.855555555555554, 52.855555555555554, 53.87222222222222, 53.87222222222222, 53.66111111111111, 53.66111111111111, 55.06111111111111, 55.06111111111111, 55.49444444444445, 55.49444444444445, 56.233333333333334, 56.233333333333334, 55.28333333333333, 55.28333333333333, 55.833333333333336, 55.833333333333336, 55.922222222222224, 55.922222222222224, 56.37777777777778, 56.37777777777778, 57.022222222222226, 57.022222222222226, 55.40555555555556, 55.40555555555556, 55.705555555555556, 55.705555555555556, 55.672222222222224, 55.672222222222224, 56.544444444444444, 56.544444444444444, 56.611111111111114, 56.611111111111114, 56.233333333333334, 56.233333333333334, 55.25555555555555, 55.25555555555555, 55.77777777777778, 55.77777777777778, 55.672222222222224, 55.672222222222224, 56.5, 56.5, 57.05555555555556, 57.05555555555556, 57.69444444444444, 57.69444444444444, 59.4, 59.4, 60.87222222222222, 60.87222222222222, 62.205555555555556, 62.205555555555556, 62.23888888888889, 62.23888888888889, 61.24444444444445, 61.24444444444445, 62.06111111111111, 62.06111111111111, 60.605555555555554, 60.605555555555554, 58.56666666666667, 58.56666666666667, 58.98888888888889, 58.98888888888889, 59.46666666666667, 59.46666666666667, 60.08888888888889, 60.08888888888889, 59.66111111111111, 59.66111111111111, 58.90555555555556, 58.90555555555556, 59.15, 59.15, 60.07222222222222, 60.07222222222222, 59.21666666666667, 59.21666666666667, 58.794444444444444, 58.794444444444444, 59.78888888888889, 59.78888888888889, 61.28888888888889, 61.28888888888889, 61.56111111111111, 61.56111111111111, 60.93888888888889, 60.93888888888889, 63.05555555555556, 63.05555555555556, 63.67777777777778, 63.67777777777778, 63.983333333333334, 63.983333333333334, 64.08888888888889, 64.08888888888889, 64.23333333333333, 64.23333333333333, 64.12222222222222, 64.12222222222222, 64.11111111111111, 64.11111111111111, 65.14444444444445, 65.14444444444445, 64.68888888888888, 64.68888888888888, 63.9, 63.9, 64.93888888888888, 64.93888888888888, 65.42222222222222, 65.42222222222222, 65.84444444444445, 65.84444444444445, 64.61111111111111, 64.61111111111111, 65.06666666666666, 65.06666666666666, 65.54444444444445, 65.54444444444445, 65.53333333333333, 65.53333333333333, 66.21666666666667, 66.21666666666667, 66.07777777777778, 66.07777777777778, 66.4, 66.4, 67.65555555555555, 67.65555555555555, 68.23333333333333, 68.23333333333333, 68.34444444444445, 68.34444444444445, 68.63888888888889, 68.63888888888889, 69.33333333333333, 69.33333333333333, 69.26666666666667, 69.26666666666667, 69.61111111111111, 69.61111111111111]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.09
Round   1, Train loss: 2.300, Test loss: 2.302, Test accuracy: 11.12
Round   2, Train loss: 2.299, Test loss: 2.301, Test accuracy: 11.77
Round   3, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.28
Round   4, Train loss: 2.297, Test loss: 2.300, Test accuracy: 12.21
Round   5, Train loss: 2.295, Test loss: 2.298, Test accuracy: 11.20
Round   6, Train loss: 2.288, Test loss: 2.295, Test accuracy: 12.44
Round   7, Train loss: 2.269, Test loss: 2.284, Test accuracy: 16.66
Round   8, Train loss: 2.220, Test loss: 2.260, Test accuracy: 18.06
Round   9, Train loss: 2.156, Test loss: 2.220, Test accuracy: 22.69
Round  10, Train loss: 2.104, Test loss: 2.186, Test accuracy: 26.13
Round  11, Train loss: 2.087, Test loss: 2.134, Test accuracy: 32.55
Round  12, Train loss: 2.032, Test loss: 2.112, Test accuracy: 34.85
Round  13, Train loss: 1.998, Test loss: 2.084, Test accuracy: 37.69
Round  14, Train loss: 2.031, Test loss: 2.053, Test accuracy: 40.98
Round  15, Train loss: 1.992, Test loss: 2.041, Test accuracy: 41.91
Round  16, Train loss: 1.946, Test loss: 2.011, Test accuracy: 45.25
Round  17, Train loss: 1.990, Test loss: 1.996, Test accuracy: 46.82
Round  18, Train loss: 2.002, Test loss: 1.990, Test accuracy: 47.38
Round  19, Train loss: 1.923, Test loss: 1.987, Test accuracy: 47.57
Round  20, Train loss: 1.948, Test loss: 1.981, Test accuracy: 48.09
Round  21, Train loss: 1.996, Test loss: 1.979, Test accuracy: 48.29
Round  22, Train loss: 1.936, Test loss: 1.967, Test accuracy: 49.45
Round  23, Train loss: 1.944, Test loss: 1.965, Test accuracy: 49.58
Round  24, Train loss: 1.956, Test loss: 1.964, Test accuracy: 49.68
Round  25, Train loss: 1.851, Test loss: 1.946, Test accuracy: 51.56
Round  26, Train loss: 1.975, Test loss: 1.944, Test accuracy: 51.75
Round  27, Train loss: 1.940, Test loss: 1.925, Test accuracy: 53.94
Round  28, Train loss: 1.889, Test loss: 1.922, Test accuracy: 54.28
Round  29, Train loss: 1.891, Test loss: 1.915, Test accuracy: 54.93
Round  30, Train loss: 1.887, Test loss: 1.913, Test accuracy: 55.06
Round  31, Train loss: 1.884, Test loss: 1.911, Test accuracy: 55.11
Round  32, Train loss: 1.858, Test loss: 1.909, Test accuracy: 55.51
Round  33, Train loss: 1.843, Test loss: 1.905, Test accuracy: 55.89
Round  34, Train loss: 1.874, Test loss: 1.902, Test accuracy: 55.97
Round  35, Train loss: 1.871, Test loss: 1.901, Test accuracy: 56.14
Round  36, Train loss: 1.849, Test loss: 1.900, Test accuracy: 56.19
Round  37, Train loss: 1.842, Test loss: 1.899, Test accuracy: 56.27
Round  38, Train loss: 1.880, Test loss: 1.894, Test accuracy: 56.72
Round  39, Train loss: 1.840, Test loss: 1.889, Test accuracy: 57.33
Round  40, Train loss: 1.902, Test loss: 1.885, Test accuracy: 57.67
Round  41, Train loss: 1.829, Test loss: 1.883, Test accuracy: 57.79
Round  42, Train loss: 1.881, Test loss: 1.883, Test accuracy: 57.82
Round  43, Train loss: 1.852, Test loss: 1.879, Test accuracy: 58.07
Round  44, Train loss: 1.848, Test loss: 1.876, Test accuracy: 58.39
Round  45, Train loss: 1.856, Test loss: 1.875, Test accuracy: 58.54
Round  46, Train loss: 1.876, Test loss: 1.874, Test accuracy: 58.66
Round  47, Train loss: 1.848, Test loss: 1.873, Test accuracy: 58.59
Round  48, Train loss: 1.830, Test loss: 1.873, Test accuracy: 58.63
Round  49, Train loss: 1.835, Test loss: 1.873, Test accuracy: 58.67
Round  50, Train loss: 1.797, Test loss: 1.873, Test accuracy: 58.76
Round  51, Train loss: 1.870, Test loss: 1.872, Test accuracy: 58.82
Round  52, Train loss: 1.825, Test loss: 1.872, Test accuracy: 58.74
Round  53, Train loss: 1.836, Test loss: 1.872, Test accuracy: 58.78
Round  54, Train loss: 1.835, Test loss: 1.872, Test accuracy: 58.73
Round  55, Train loss: 1.849, Test loss: 1.870, Test accuracy: 58.93
Round  56, Train loss: 1.846, Test loss: 1.869, Test accuracy: 58.99
Round  57, Train loss: 1.844, Test loss: 1.864, Test accuracy: 59.48
Round  58, Train loss: 1.861, Test loss: 1.864, Test accuracy: 59.56
Round  59, Train loss: 1.877, Test loss: 1.862, Test accuracy: 59.71
Round  60, Train loss: 1.855, Test loss: 1.857, Test accuracy: 60.27
Round  61, Train loss: 1.860, Test loss: 1.857, Test accuracy: 60.34
Round  62, Train loss: 1.791, Test loss: 1.856, Test accuracy: 60.34
Round  63, Train loss: 1.889, Test loss: 1.855, Test accuracy: 60.49
Round  64, Train loss: 1.837, Test loss: 1.855, Test accuracy: 60.44
Round  65, Train loss: 1.815, Test loss: 1.854, Test accuracy: 60.49
Round  66, Train loss: 1.790, Test loss: 1.854, Test accuracy: 60.55
Round  67, Train loss: 1.808, Test loss: 1.853, Test accuracy: 60.59
Round  68, Train loss: 1.824, Test loss: 1.850, Test accuracy: 60.91
Round  69, Train loss: 1.796, Test loss: 1.849, Test accuracy: 60.98
Round  70, Train loss: 1.805, Test loss: 1.849, Test accuracy: 61.03
Round  71, Train loss: 1.808, Test loss: 1.850, Test accuracy: 60.85
Round  72, Train loss: 1.848, Test loss: 1.848, Test accuracy: 61.07
Round  73, Train loss: 1.825, Test loss: 1.848, Test accuracy: 61.03
Round  74, Train loss: 1.779, Test loss: 1.847, Test accuracy: 61.09
Round  75, Train loss: 1.847, Test loss: 1.847, Test accuracy: 61.24
Round  76, Train loss: 1.810, Test loss: 1.847, Test accuracy: 61.19
Round  77, Train loss: 1.855, Test loss: 1.847, Test accuracy: 61.11
Round  78, Train loss: 1.803, Test loss: 1.847, Test accuracy: 61.12
Round  79, Train loss: 1.819, Test loss: 1.847, Test accuracy: 61.14
Round  80, Train loss: 1.847, Test loss: 1.846, Test accuracy: 61.18
Round  81, Train loss: 1.792, Test loss: 1.846, Test accuracy: 61.23
Round  82, Train loss: 1.835, Test loss: 1.842, Test accuracy: 61.66
Round  83, Train loss: 1.845, Test loss: 1.837, Test accuracy: 62.21
Round  84, Train loss: 1.797, Test loss: 1.837, Test accuracy: 62.23
Round  85, Train loss: 1.804, Test loss: 1.836, Test accuracy: 62.19
Round  86, Train loss: 1.809, Test loss: 1.835, Test accuracy: 62.28
Round  87, Train loss: 1.796, Test loss: 1.835, Test accuracy: 62.35
Round  88, Train loss: 1.823, Test loss: 1.835, Test accuracy: 62.38
Round  89, Train loss: 1.785, Test loss: 1.835, Test accuracy: 62.37
Round  90, Train loss: 1.816, Test loss: 1.835, Test accuracy: 62.36
Round  91, Train loss: 1.811, Test loss: 1.835, Test accuracy: 62.37
Round  92, Train loss: 1.831, Test loss: 1.834, Test accuracy: 62.38
Round  93, Train loss: 1.777, Test loss: 1.834, Test accuracy: 62.39
Round  94, Train loss: 1.792, Test loss: 1.834, Test accuracy: 62.46
Round  95, Train loss: 1.813, Test loss: 1.834, Test accuracy: 62.48
Round  96, Train loss: 1.803, Test loss: 1.833, Test accuracy: 62.42
Round  97, Train loss: 1.774, Test loss: 1.833, Test accuracy: 62.47
Round  98, Train loss: 1.795, Test loss: 1.833, Test accuracy: 62.52/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.794, Test loss: 1.833, Test accuracy: 62.56
Final Round, Train loss: 1.803, Test loss: 1.830, Test accuracy: 62.84
Average accuracy final 10 rounds: 62.44111111111111 

1996.5081324577332
[1.532517910003662, 3.065035820007324, 4.710226774215698, 6.355417728424072, 7.955010890960693, 9.554604053497314, 11.112279415130615, 12.669954776763916, 14.259313106536865, 15.848671436309814, 17.49692177772522, 19.145172119140625, 20.71688461303711, 22.288597106933594, 23.913411378860474, 25.538225650787354, 27.172008275985718, 28.805790901184082, 30.29602885246277, 31.786266803741455, 33.42288112640381, 35.05949544906616, 36.69851303100586, 38.33753061294556, 39.90210270881653, 41.4666748046875, 43.033796072006226, 44.60091733932495, 46.25863242149353, 47.91634750366211, 49.45176076889038, 50.98717403411865, 52.63157677650452, 54.27597951889038, 55.82721304893494, 57.37844657897949, 58.90436148643494, 60.43027639389038, 62.05902981758118, 63.68778324127197, 65.24448251724243, 66.80118179321289, 68.37840032577515, 69.9556188583374, 71.55957841873169, 73.16353797912598, 74.7385003566742, 76.31346273422241, 77.8212399482727, 79.329017162323, 80.93741822242737, 82.54581928253174, 84.11327528953552, 85.6807312965393, 87.21297240257263, 88.74521350860596, 90.34744811058044, 91.94968271255493, 93.53686141967773, 95.12404012680054, 96.66513228416443, 98.20622444152832, 99.76985478401184, 101.33348512649536, 102.89582395553589, 104.45816278457642, 106.05814361572266, 107.6581244468689, 109.29174327850342, 110.92536211013794, 112.492680311203, 114.05999851226807, 115.66589641571045, 117.27179431915283, 118.89377760887146, 120.51576089859009, 122.06078147888184, 123.60580205917358, 125.17674446105957, 126.74768686294556, 128.35694479942322, 129.96620273590088, 131.50198888778687, 133.03777503967285, 134.60439085960388, 136.1710066795349, 137.7950839996338, 139.41916131973267, 140.9647581577301, 142.51035499572754, 144.0603699684143, 145.61038494110107, 147.2516074180603, 148.89282989501953, 150.4982945919037, 152.10375928878784, 153.65637969970703, 155.20900011062622, 156.7946846485138, 158.38036918640137, 159.98622369766235, 161.59207820892334, 163.19791960716248, 164.8037610054016, 166.40581965446472, 168.00787830352783, 169.56745100021362, 171.1270236968994, 172.7081983089447, 174.28937292099, 175.89252042770386, 177.49566793441772, 179.0784113407135, 180.66115474700928, 182.24736142158508, 183.8335680961609, 185.43349146842957, 187.03341484069824, 188.63524389266968, 190.2370729446411, 191.80151271820068, 193.36595249176025, 194.9812891483307, 196.59662580490112, 198.20663404464722, 199.8166422843933, 201.36657619476318, 202.91651010513306, 204.5191490650177, 206.12178802490234, 207.7474868297577, 209.37318563461304, 210.95205330848694, 212.53092098236084, 214.10832285881042, 215.68572473526, 217.35336709022522, 219.02100944519043, 220.59333038330078, 222.16565132141113, 223.71853637695312, 225.27142143249512, 226.9114637374878, 228.55150604248047, 230.16790866851807, 231.78431129455566, 233.35238337516785, 234.92045545578003, 236.55601286888123, 238.19157028198242, 239.7982633113861, 241.4049563407898, 243.02086091041565, 244.6367654800415, 246.23674654960632, 247.83672761917114, 249.4079954624176, 250.97926330566406, 252.5550434589386, 254.13082361221313, 255.74146056175232, 257.3520975112915, 258.824449300766, 260.2968010902405, 261.72572112083435, 263.1546411514282, 264.6198196411133, 266.08499813079834, 267.426230430603, 268.7674627304077, 270.1306266784668, 271.4937906265259, 272.9933273792267, 274.4928641319275, 275.9293076992035, 277.3657512664795, 278.7274694442749, 280.0891876220703, 281.57769322395325, 283.0661988258362, 284.46568274497986, 285.86516666412354, 287.1873617172241, 288.5095567703247, 290.0241687297821, 291.5387806892395, 292.9612810611725, 294.38378143310547, 295.6995711326599, 297.01536083221436, 298.49177861213684, 299.9681963920593, 301.3633954524994, 302.75859451293945, 304.0737419128418, 305.38888931274414, 306.8824954032898, 308.37610149383545, 309.8270752429962, 311.278048992157, 312.8382053375244, 314.39836168289185]
[10.094444444444445, 10.094444444444445, 11.116666666666667, 11.116666666666667, 11.766666666666667, 11.766666666666667, 12.283333333333333, 12.283333333333333, 12.21111111111111, 12.21111111111111, 11.2, 11.2, 12.444444444444445, 12.444444444444445, 16.66111111111111, 16.66111111111111, 18.06111111111111, 18.06111111111111, 22.68888888888889, 22.68888888888889, 26.133333333333333, 26.133333333333333, 32.55, 32.55, 34.85, 34.85, 37.68888888888889, 37.68888888888889, 40.983333333333334, 40.983333333333334, 41.91111111111111, 41.91111111111111, 45.25, 45.25, 46.82222222222222, 46.82222222222222, 47.38333333333333, 47.38333333333333, 47.56666666666667, 47.56666666666667, 48.08888888888889, 48.08888888888889, 48.28888888888889, 48.28888888888889, 49.45, 49.45, 49.577777777777776, 49.577777777777776, 49.67777777777778, 49.67777777777778, 51.55555555555556, 51.55555555555556, 51.75, 51.75, 53.93888888888889, 53.93888888888889, 54.27777777777778, 54.27777777777778, 54.93333333333333, 54.93333333333333, 55.06111111111111, 55.06111111111111, 55.111111111111114, 55.111111111111114, 55.50555555555555, 55.50555555555555, 55.894444444444446, 55.894444444444446, 55.97222222222222, 55.97222222222222, 56.138888888888886, 56.138888888888886, 56.19444444444444, 56.19444444444444, 56.272222222222226, 56.272222222222226, 56.72222222222222, 56.72222222222222, 57.327777777777776, 57.327777777777776, 57.666666666666664, 57.666666666666664, 57.78888888888889, 57.78888888888889, 57.81666666666667, 57.81666666666667, 58.07222222222222, 58.07222222222222, 58.394444444444446, 58.394444444444446, 58.53888888888889, 58.53888888888889, 58.65555555555556, 58.65555555555556, 58.58888888888889, 58.58888888888889, 58.63333333333333, 58.63333333333333, 58.672222222222224, 58.672222222222224, 58.76111111111111, 58.76111111111111, 58.81666666666667, 58.81666666666667, 58.74444444444445, 58.74444444444445, 58.78333333333333, 58.78333333333333, 58.733333333333334, 58.733333333333334, 58.92777777777778, 58.92777777777778, 58.98888888888889, 58.98888888888889, 59.483333333333334, 59.483333333333334, 59.56111111111111, 59.56111111111111, 59.71111111111111, 59.71111111111111, 60.272222222222226, 60.272222222222226, 60.33888888888889, 60.33888888888889, 60.33888888888889, 60.33888888888889, 60.49444444444445, 60.49444444444445, 60.43888888888889, 60.43888888888889, 60.49444444444445, 60.49444444444445, 60.55, 60.55, 60.59444444444444, 60.59444444444444, 60.90555555555556, 60.90555555555556, 60.983333333333334, 60.983333333333334, 61.02777777777778, 61.02777777777778, 60.85, 60.85, 61.06666666666667, 61.06666666666667, 61.03333333333333, 61.03333333333333, 61.08888888888889, 61.08888888888889, 61.23888888888889, 61.23888888888889, 61.18888888888889, 61.18888888888889, 61.111111111111114, 61.111111111111114, 61.11666666666667, 61.11666666666667, 61.138888888888886, 61.138888888888886, 61.17777777777778, 61.17777777777778, 61.227777777777774, 61.227777777777774, 61.65555555555556, 61.65555555555556, 62.205555555555556, 62.205555555555556, 62.233333333333334, 62.233333333333334, 62.19444444444444, 62.19444444444444, 62.28333333333333, 62.28333333333333, 62.35, 62.35, 62.37777777777778, 62.37777777777778, 62.36666666666667, 62.36666666666667, 62.355555555555554, 62.355555555555554, 62.37222222222222, 62.37222222222222, 62.38333333333333, 62.38333333333333, 62.394444444444446, 62.394444444444446, 62.46111111111111, 62.46111111111111, 62.477777777777774, 62.477777777777774, 62.422222222222224, 62.422222222222224, 62.46666666666667, 62.46666666666667, 62.516666666666666, 62.516666666666666, 62.56111111111111, 62.56111111111111, 62.83888888888889, 62.83888888888889]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.302, Test accuracy: 10.22
Round   1, Train loss: 2.289, Test loss: 2.300, Test accuracy: 11.50
Round   2, Train loss: 2.285, Test loss: 2.295, Test accuracy: 12.71
Round   3, Train loss: 2.227, Test loss: 2.280, Test accuracy: 17.46
Round   4, Train loss: 2.115, Test loss: 2.237, Test accuracy: 23.34
Round   5, Train loss: 1.989, Test loss: 2.178, Test accuracy: 27.94
Round   6, Train loss: 2.093, Test loss: 2.117, Test accuracy: 35.67
Round   7, Train loss: 1.935, Test loss: 2.074, Test accuracy: 39.65
Round   8, Train loss: 1.975, Test loss: 2.041, Test accuracy: 43.34
Round   9, Train loss: 1.973, Test loss: 2.022, Test accuracy: 45.01
Round  10, Train loss: 1.940, Test loss: 1.996, Test accuracy: 47.31
Round  11, Train loss: 1.895, Test loss: 1.970, Test accuracy: 50.14
Round  12, Train loss: 1.909, Test loss: 1.960, Test accuracy: 51.03
Round  13, Train loss: 1.875, Test loss: 1.950, Test accuracy: 51.89
Round  14, Train loss: 1.865, Test loss: 1.939, Test accuracy: 52.78
Round  15, Train loss: 1.852, Test loss: 1.921, Test accuracy: 54.57
Round  16, Train loss: 1.870, Test loss: 1.906, Test accuracy: 56.07
Round  17, Train loss: 1.908, Test loss: 1.897, Test accuracy: 56.85
Round  18, Train loss: 1.821, Test loss: 1.892, Test accuracy: 57.23
Round  19, Train loss: 1.838, Test loss: 1.887, Test accuracy: 57.57
Round  20, Train loss: 1.866, Test loss: 1.880, Test accuracy: 58.24
Round  21, Train loss: 1.755, Test loss: 1.878, Test accuracy: 58.37
Round  22, Train loss: 1.800, Test loss: 1.876, Test accuracy: 58.51
Round  23, Train loss: 1.821, Test loss: 1.872, Test accuracy: 58.94
Round  24, Train loss: 1.822, Test loss: 1.861, Test accuracy: 60.07
Round  25, Train loss: 1.837, Test loss: 1.860, Test accuracy: 60.20
Round  26, Train loss: 1.830, Test loss: 1.858, Test accuracy: 60.32
Round  27, Train loss: 1.901, Test loss: 1.854, Test accuracy: 60.96
Round  28, Train loss: 1.835, Test loss: 1.851, Test accuracy: 61.08
Round  29, Train loss: 1.858, Test loss: 1.849, Test accuracy: 61.22
Round  30, Train loss: 1.783, Test loss: 1.844, Test accuracy: 61.68
Round  31, Train loss: 1.738, Test loss: 1.841, Test accuracy: 62.04
Round  32, Train loss: 1.863, Test loss: 1.841, Test accuracy: 62.04
Round  33, Train loss: 1.805, Test loss: 1.841, Test accuracy: 61.92
Round  34, Train loss: 1.770, Test loss: 1.834, Test accuracy: 62.72
Round  35, Train loss: 1.819, Test loss: 1.833, Test accuracy: 62.72
Round  36, Train loss: 1.830, Test loss: 1.831, Test accuracy: 62.96
Round  37, Train loss: 1.820, Test loss: 1.826, Test accuracy: 63.51
Round  38, Train loss: 1.809, Test loss: 1.824, Test accuracy: 63.64
Round  39, Train loss: 1.805, Test loss: 1.824, Test accuracy: 63.75
Round  40, Train loss: 1.831, Test loss: 1.823, Test accuracy: 63.78
Round  41, Train loss: 1.769, Test loss: 1.822, Test accuracy: 63.83
Round  42, Train loss: 1.729, Test loss: 1.822, Test accuracy: 63.86
Round  43, Train loss: 1.768, Test loss: 1.821, Test accuracy: 63.83
Round  44, Train loss: 1.799, Test loss: 1.821, Test accuracy: 63.84
Round  45, Train loss: 1.725, Test loss: 1.819, Test accuracy: 64.07
Round  46, Train loss: 1.715, Test loss: 1.818, Test accuracy: 64.09
Round  47, Train loss: 1.730, Test loss: 1.818, Test accuracy: 64.19
Round  48, Train loss: 1.834, Test loss: 1.817, Test accuracy: 64.19
Round  49, Train loss: 1.858, Test loss: 1.817, Test accuracy: 64.23
Round  50, Train loss: 1.798, Test loss: 1.817, Test accuracy: 64.28
Round  51, Train loss: 1.794, Test loss: 1.814, Test accuracy: 64.48
Round  52, Train loss: 1.809, Test loss: 1.814, Test accuracy: 64.49
Round  53, Train loss: 1.801, Test loss: 1.814, Test accuracy: 64.53
Round  54, Train loss: 1.709, Test loss: 1.811, Test accuracy: 64.74
Round  55, Train loss: 1.787, Test loss: 1.810, Test accuracy: 64.91
Round  56, Train loss: 1.817, Test loss: 1.804, Test accuracy: 65.40
Round  57, Train loss: 1.787, Test loss: 1.805, Test accuracy: 65.36
Round  58, Train loss: 1.720, Test loss: 1.804, Test accuracy: 65.51
Round  59, Train loss: 1.759, Test loss: 1.801, Test accuracy: 65.84
Round  60, Train loss: 1.730, Test loss: 1.801, Test accuracy: 65.77
Round  61, Train loss: 1.748, Test loss: 1.800, Test accuracy: 65.92
Round  62, Train loss: 1.723, Test loss: 1.800, Test accuracy: 65.82
Round  63, Train loss: 1.761, Test loss: 1.800, Test accuracy: 65.84
Round  64, Train loss: 1.820, Test loss: 1.799, Test accuracy: 66.01
Round  65, Train loss: 1.747, Test loss: 1.796, Test accuracy: 66.43
Round  66, Train loss: 1.715, Test loss: 1.795, Test accuracy: 66.44
Round  67, Train loss: 1.856, Test loss: 1.795, Test accuracy: 66.36
Round  68, Train loss: 1.786, Test loss: 1.795, Test accuracy: 66.46
Round  69, Train loss: 1.709, Test loss: 1.791, Test accuracy: 66.91
Round  70, Train loss: 1.783, Test loss: 1.789, Test accuracy: 67.02
Round  71, Train loss: 1.810, Test loss: 1.789, Test accuracy: 67.06
Round  72, Train loss: 1.733, Test loss: 1.788, Test accuracy: 67.06
Round  73, Train loss: 1.822, Test loss: 1.788, Test accuracy: 67.06
Round  74, Train loss: 1.754, Test loss: 1.789, Test accuracy: 67.00
Round  75, Train loss: 1.796, Test loss: 1.788, Test accuracy: 67.06
Round  76, Train loss: 1.777, Test loss: 1.788, Test accuracy: 67.03
Round  77, Train loss: 1.694, Test loss: 1.787, Test accuracy: 67.13
Round  78, Train loss: 1.810, Test loss: 1.787, Test accuracy: 67.16
Round  79, Train loss: 1.752, Test loss: 1.784, Test accuracy: 67.47
Round  80, Train loss: 1.774, Test loss: 1.785, Test accuracy: 67.42
Round  81, Train loss: 1.755, Test loss: 1.782, Test accuracy: 67.68
Round  82, Train loss: 1.716, Test loss: 1.775, Test accuracy: 68.39
Round  83, Train loss: 1.787, Test loss: 1.773, Test accuracy: 68.59
Round  84, Train loss: 1.786, Test loss: 1.770, Test accuracy: 68.93
Round  85, Train loss: 1.804, Test loss: 1.769, Test accuracy: 69.07
Round  86, Train loss: 1.763, Test loss: 1.768, Test accuracy: 69.05
Round  87, Train loss: 1.772, Test loss: 1.766, Test accuracy: 69.38
Round  88, Train loss: 1.663, Test loss: 1.765, Test accuracy: 69.50
Round  89, Train loss: 1.722, Test loss: 1.764, Test accuracy: 69.58
Round  90, Train loss: 1.712, Test loss: 1.763, Test accuracy: 69.52
Round  91, Train loss: 1.813, Test loss: 1.763, Test accuracy: 69.58
Round  92, Train loss: 1.750, Test loss: 1.763, Test accuracy: 69.63
Round  93, Train loss: 1.679, Test loss: 1.763, Test accuracy: 69.54
Round  94, Train loss: 1.753, Test loss: 1.762, Test accuracy: 69.61
Round  95, Train loss: 1.662, Test loss: 1.762, Test accuracy: 69.61
Round  96, Train loss: 1.728, Test loss: 1.762, Test accuracy: 69.62
Round  97, Train loss: 1.696, Test loss: 1.759, Test accuracy: 70.03
Round  98, Train loss: 1.758, Test loss: 1.759, Test accuracy: 70.02/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.769, Test loss: 1.758, Test accuracy: 70.04
Final Round, Train loss: 1.736, Test loss: 1.755, Test accuracy: 70.40
Average accuracy final 10 rounds: 69.7188888888889 

2005.1992001533508
[1.7495274543762207, 3.4990549087524414, 5.1457836627960205, 6.7925124168396, 8.508836507797241, 10.225160598754883, 11.881489992141724, 13.537819385528564, 15.175763845443726, 16.813708305358887, 18.523821115493774, 20.233933925628662, 21.885016441345215, 23.536098957061768, 25.23034381866455, 26.924588680267334, 28.614607095718384, 30.304625511169434, 31.947171449661255, 33.589717388153076, 35.28760838508606, 36.98549938201904, 38.64709115028381, 40.308682918548584, 41.94399881362915, 43.57931470870972, 45.2782301902771, 46.97714567184448, 48.6943576335907, 50.411569595336914, 52.100030183792114, 53.788490772247314, 55.542877435684204, 57.297264099121094, 58.77155041694641, 60.24583673477173, 61.903977394104004, 63.56211805343628, 65.02871751785278, 66.49531698226929, 68.02135252952576, 69.54738807678223, 71.09105086326599, 72.63471364974976, 74.14933180809021, 75.66394996643066, 77.07873630523682, 78.49352264404297, 79.98600912094116, 81.47849559783936, 83.01462244987488, 84.5507493019104, 85.94069242477417, 87.33063554763794, 88.73993372917175, 90.14923191070557, 91.65404391288757, 93.15885591506958, 94.5818281173706, 96.00480031967163, 97.45072746276855, 98.89665460586548, 100.35667943954468, 101.81670427322388, 103.22691583633423, 104.63712739944458, 106.06062722206116, 107.48412704467773, 108.93420076370239, 110.38427448272705, 111.84076142311096, 113.29724836349487, 114.7320671081543, 116.16688585281372, 117.60261845588684, 119.03835105895996, 120.5434160232544, 122.04848098754883, 123.52537560462952, 125.0022702217102, 126.42350172996521, 127.84473323822021, 129.343763589859, 130.8427939414978, 132.34633564949036, 133.8498773574829, 135.26033425331116, 136.6707911491394, 138.18813061714172, 139.70547008514404, 141.20606017112732, 142.7066502571106, 144.14260578155518, 145.57856130599976, 147.00165629386902, 148.42475128173828, 149.92074155807495, 151.41673183441162, 152.8720006942749, 154.32726955413818, 155.96408605575562, 157.60090255737305, 159.29330468177795, 160.98570680618286, 162.70694637298584, 164.42818593978882, 166.07543420791626, 167.7226824760437, 169.40114426612854, 171.07960605621338, 172.73741388320923, 174.39522171020508, 176.06586146354675, 177.73650121688843, 179.4016604423523, 181.06681966781616, 182.73085045814514, 184.39488124847412, 186.0668613910675, 187.7388415336609, 189.450914144516, 191.1629867553711, 192.86918902397156, 194.57539129257202, 196.2175064086914, 197.8596215248108, 199.49626922607422, 201.13291692733765, 202.827538728714, 204.52216053009033, 206.19649362564087, 207.8708267211914, 209.50305104255676, 211.13527536392212, 212.8573660850525, 214.57945680618286, 216.30095791816711, 218.02245903015137, 219.68733716011047, 221.35221529006958, 222.96485233306885, 224.57748937606812, 226.31025075912476, 228.0430121421814, 229.75325083732605, 231.4634895324707, 233.1310691833496, 234.79864883422852, 236.42182111740112, 238.04499340057373, 239.72997045516968, 241.41494750976562, 243.07220482826233, 244.72946214675903, 246.32329058647156, 247.91711902618408, 249.5633099079132, 251.20950078964233, 252.8971962928772, 254.58489179611206, 256.2297012805939, 257.8745107650757, 259.4283649921417, 260.98221921920776, 262.62767481803894, 264.2731304168701, 265.9422254562378, 267.61132049560547, 269.2303743362427, 270.8494281768799, 272.4968967437744, 274.14436531066895, 275.62482047080994, 277.1052756309509, 278.53424763679504, 279.96321964263916, 281.4500825405121, 282.936945438385, 284.3986186981201, 285.8602919578552, 287.32367062568665, 288.78704929351807, 290.1774318218231, 291.5678143501282, 292.974347114563, 294.3808798789978, 295.8501477241516, 297.3194155693054, 298.8418393135071, 300.36426305770874, 301.7854402065277, 303.2066173553467, 304.67388010025024, 306.1411428451538, 307.65838980674744, 309.17563676834106, 310.5844638347626, 311.9932909011841, 313.478880405426, 314.96446990966797, 316.4809560775757, 317.9974422454834]
[10.222222222222221, 10.222222222222221, 11.5, 11.5, 12.705555555555556, 12.705555555555556, 17.455555555555556, 17.455555555555556, 23.344444444444445, 23.344444444444445, 27.944444444444443, 27.944444444444443, 35.672222222222224, 35.672222222222224, 39.65, 39.65, 43.34444444444444, 43.34444444444444, 45.01111111111111, 45.01111111111111, 47.30555555555556, 47.30555555555556, 50.144444444444446, 50.144444444444446, 51.02777777777778, 51.02777777777778, 51.894444444444446, 51.894444444444446, 52.78333333333333, 52.78333333333333, 54.57222222222222, 54.57222222222222, 56.06666666666667, 56.06666666666667, 56.85, 56.85, 57.233333333333334, 57.233333333333334, 57.56666666666667, 57.56666666666667, 58.24444444444445, 58.24444444444445, 58.37222222222222, 58.37222222222222, 58.51111111111111, 58.51111111111111, 58.94444444444444, 58.94444444444444, 60.07222222222222, 60.07222222222222, 60.2, 60.2, 60.32222222222222, 60.32222222222222, 60.96111111111111, 60.96111111111111, 61.077777777777776, 61.077777777777776, 61.22222222222222, 61.22222222222222, 61.67777777777778, 61.67777777777778, 62.044444444444444, 62.044444444444444, 62.03888888888889, 62.03888888888889, 61.916666666666664, 61.916666666666664, 62.72222222222222, 62.72222222222222, 62.72222222222222, 62.72222222222222, 62.96111111111111, 62.96111111111111, 63.51111111111111, 63.51111111111111, 63.644444444444446, 63.644444444444446, 63.75, 63.75, 63.77777777777778, 63.77777777777778, 63.827777777777776, 63.827777777777776, 63.855555555555554, 63.855555555555554, 63.833333333333336, 63.833333333333336, 63.84444444444444, 63.84444444444444, 64.06666666666666, 64.06666666666666, 64.08888888888889, 64.08888888888889, 64.19444444444444, 64.19444444444444, 64.19444444444444, 64.19444444444444, 64.23333333333333, 64.23333333333333, 64.28333333333333, 64.28333333333333, 64.47777777777777, 64.47777777777777, 64.49444444444444, 64.49444444444444, 64.53333333333333, 64.53333333333333, 64.74444444444444, 64.74444444444444, 64.90555555555555, 64.90555555555555, 65.4, 65.4, 65.36111111111111, 65.36111111111111, 65.50555555555556, 65.50555555555556, 65.84444444444445, 65.84444444444445, 65.77222222222223, 65.77222222222223, 65.91666666666667, 65.91666666666667, 65.81666666666666, 65.81666666666666, 65.83888888888889, 65.83888888888889, 66.0111111111111, 66.0111111111111, 66.43333333333334, 66.43333333333334, 66.44444444444444, 66.44444444444444, 66.36111111111111, 66.36111111111111, 66.45555555555555, 66.45555555555555, 66.90555555555555, 66.90555555555555, 67.02222222222223, 67.02222222222223, 67.05555555555556, 67.05555555555556, 67.05555555555556, 67.05555555555556, 67.05555555555556, 67.05555555555556, 67.0, 67.0, 67.06111111111112, 67.06111111111112, 67.02777777777777, 67.02777777777777, 67.13333333333334, 67.13333333333334, 67.15555555555555, 67.15555555555555, 67.46666666666667, 67.46666666666667, 67.42222222222222, 67.42222222222222, 67.68333333333334, 67.68333333333334, 68.38888888888889, 68.38888888888889, 68.59444444444445, 68.59444444444445, 68.92777777777778, 68.92777777777778, 69.06666666666666, 69.06666666666666, 69.05, 69.05, 69.38333333333334, 69.38333333333334, 69.5, 69.5, 69.58333333333333, 69.58333333333333, 69.52222222222223, 69.52222222222223, 69.58333333333333, 69.58333333333333, 69.62777777777778, 69.62777777777778, 69.54444444444445, 69.54444444444445, 69.60555555555555, 69.60555555555555, 69.60555555555555, 69.60555555555555, 69.61666666666666, 69.61666666666666, 70.02777777777777, 70.02777777777777, 70.01666666666667, 70.01666666666667, 70.03888888888889, 70.03888888888889, 70.4, 70.4]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.50
Round   1, Train loss: 2.300, Test loss: 2.301, Test accuracy: 18.48
Round   2, Train loss: 2.298, Test loss: 2.299, Test accuracy: 22.28
Round   3, Train loss: 2.293, Test loss: 2.296, Test accuracy: 25.73
Round   4, Train loss: 2.282, Test loss: 2.291, Test accuracy: 26.53
Round   5, Train loss: 2.261, Test loss: 2.271, Test accuracy: 26.02
Round   6, Train loss: 2.148, Test loss: 2.218, Test accuracy: 29.34
Round   7, Train loss: 2.122, Test loss: 2.170, Test accuracy: 34.12
Round   8, Train loss: 2.005, Test loss: 2.104, Test accuracy: 41.03
Round   9, Train loss: 2.007, Test loss: 2.035, Test accuracy: 46.46
Round  10, Train loss: 1.903, Test loss: 1.962, Test accuracy: 53.23
Round  11, Train loss: 1.798, Test loss: 1.904, Test accuracy: 58.81
Round  12, Train loss: 1.716, Test loss: 1.867, Test accuracy: 61.84
Round  13, Train loss: 1.718, Test loss: 1.829, Test accuracy: 65.42
Round  14, Train loss: 1.686, Test loss: 1.812, Test accuracy: 66.82
Round  15, Train loss: 1.629, Test loss: 1.797, Test accuracy: 68.07
Round  16, Train loss: 1.667, Test loss: 1.783, Test accuracy: 69.29
Round  17, Train loss: 1.656, Test loss: 1.773, Test accuracy: 70.16
Round  18, Train loss: 1.684, Test loss: 1.744, Test accuracy: 73.02
Round  19, Train loss: 1.605, Test loss: 1.739, Test accuracy: 73.28
Round  20, Train loss: 1.635, Test loss: 1.736, Test accuracy: 73.63
Round  21, Train loss: 1.615, Test loss: 1.729, Test accuracy: 74.20
Round  22, Train loss: 1.580, Test loss: 1.725, Test accuracy: 74.71
Round  23, Train loss: 1.589, Test loss: 1.721, Test accuracy: 75.03
Round  24, Train loss: 1.587, Test loss: 1.715, Test accuracy: 75.65
Round  25, Train loss: 1.543, Test loss: 1.701, Test accuracy: 77.40
Round  26, Train loss: 1.560, Test loss: 1.692, Test accuracy: 78.17
Round  27, Train loss: 1.580, Test loss: 1.682, Test accuracy: 79.27
Round  28, Train loss: 1.506, Test loss: 1.674, Test accuracy: 80.19
Round  29, Train loss: 1.551, Test loss: 1.666, Test accuracy: 80.61
Round  30, Train loss: 1.535, Test loss: 1.665, Test accuracy: 80.69
Round  31, Train loss: 1.512, Test loss: 1.657, Test accuracy: 81.57
Round  32, Train loss: 1.529, Test loss: 1.655, Test accuracy: 81.82
Round  33, Train loss: 1.503, Test loss: 1.653, Test accuracy: 81.94
Round  34, Train loss: 1.500, Test loss: 1.647, Test accuracy: 82.40
Round  35, Train loss: 1.489, Test loss: 1.645, Test accuracy: 82.58
Round  36, Train loss: 1.488, Test loss: 1.643, Test accuracy: 82.64
Round  37, Train loss: 1.484, Test loss: 1.642, Test accuracy: 82.69
Round  38, Train loss: 1.489, Test loss: 1.641, Test accuracy: 82.78
Round  39, Train loss: 1.501, Test loss: 1.640, Test accuracy: 82.86
Round  40, Train loss: 1.542, Test loss: 1.640, Test accuracy: 82.89
Round  41, Train loss: 1.540, Test loss: 1.639, Test accuracy: 82.94
Round  42, Train loss: 1.489, Test loss: 1.639, Test accuracy: 82.99
Round  43, Train loss: 1.520, Test loss: 1.638, Test accuracy: 82.99
Round  44, Train loss: 1.496, Test loss: 1.638, Test accuracy: 83.01
Round  45, Train loss: 1.485, Test loss: 1.637, Test accuracy: 83.03
Round  46, Train loss: 1.494, Test loss: 1.637, Test accuracy: 83.12
Round  47, Train loss: 1.493, Test loss: 1.636, Test accuracy: 83.15
Round  48, Train loss: 1.493, Test loss: 1.636, Test accuracy: 83.13
Round  49, Train loss: 1.500, Test loss: 1.636, Test accuracy: 83.13
Round  50, Train loss: 1.522, Test loss: 1.635, Test accuracy: 83.10
Round  51, Train loss: 1.550, Test loss: 1.631, Test accuracy: 83.49
Round  52, Train loss: 1.485, Test loss: 1.631, Test accuracy: 83.58
Round  53, Train loss: 1.493, Test loss: 1.631, Test accuracy: 83.54
Round  54, Train loss: 1.498, Test loss: 1.630, Test accuracy: 83.62
Round  55, Train loss: 1.495, Test loss: 1.630, Test accuracy: 83.60
Round  56, Train loss: 1.489, Test loss: 1.630, Test accuracy: 83.59
Round  57, Train loss: 1.485, Test loss: 1.630, Test accuracy: 83.59
Round  58, Train loss: 1.480, Test loss: 1.630, Test accuracy: 83.64
Round  59, Train loss: 1.483, Test loss: 1.630, Test accuracy: 83.62
Round  60, Train loss: 1.495, Test loss: 1.629, Test accuracy: 83.77
Round  61, Train loss: 1.496, Test loss: 1.629, Test accuracy: 83.69
Round  62, Train loss: 1.493, Test loss: 1.629, Test accuracy: 83.72
Round  63, Train loss: 1.494, Test loss: 1.629, Test accuracy: 83.76
Round  64, Train loss: 1.495, Test loss: 1.629, Test accuracy: 83.69
Round  65, Train loss: 1.480, Test loss: 1.629, Test accuracy: 83.74
Round  66, Train loss: 1.491, Test loss: 1.628, Test accuracy: 83.81
Round  67, Train loss: 1.481, Test loss: 1.628, Test accuracy: 83.78
Round  68, Train loss: 1.514, Test loss: 1.628, Test accuracy: 83.78
Round  69, Train loss: 1.480, Test loss: 1.628, Test accuracy: 83.88
Round  70, Train loss: 1.480, Test loss: 1.628, Test accuracy: 83.87
Round  71, Train loss: 1.510, Test loss: 1.628, Test accuracy: 83.84
Round  72, Train loss: 1.494, Test loss: 1.628, Test accuracy: 83.85
Round  73, Train loss: 1.493, Test loss: 1.628, Test accuracy: 83.86
Round  74, Train loss: 1.494, Test loss: 1.627, Test accuracy: 83.91
Round  75, Train loss: 1.482, Test loss: 1.627, Test accuracy: 83.88
Round  76, Train loss: 1.524, Test loss: 1.627, Test accuracy: 83.86
Round  77, Train loss: 1.493, Test loss: 1.627, Test accuracy: 83.87
Round  78, Train loss: 1.510, Test loss: 1.627, Test accuracy: 83.88
Round  79, Train loss: 1.511, Test loss: 1.627, Test accuracy: 83.87
Round  80, Train loss: 1.491, Test loss: 1.627, Test accuracy: 83.87
Round  81, Train loss: 1.489, Test loss: 1.627, Test accuracy: 83.88
Round  82, Train loss: 1.513, Test loss: 1.627, Test accuracy: 83.88
Round  83, Train loss: 1.493, Test loss: 1.627, Test accuracy: 83.86
Round  84, Train loss: 1.507, Test loss: 1.627, Test accuracy: 83.83
Round  85, Train loss: 1.497, Test loss: 1.627, Test accuracy: 83.80
Round  86, Train loss: 1.510, Test loss: 1.627, Test accuracy: 83.81
Round  87, Train loss: 1.483, Test loss: 1.627, Test accuracy: 83.80
Round  88, Train loss: 1.488, Test loss: 1.626, Test accuracy: 83.80
Round  89, Train loss: 1.480, Test loss: 1.626, Test accuracy: 83.84
Round  90, Train loss: 1.477, Test loss: 1.626, Test accuracy: 83.90
Round  91, Train loss: 1.494, Test loss: 1.626, Test accuracy: 83.85
Round  92, Train loss: 1.525, Test loss: 1.626, Test accuracy: 83.89
Round  93, Train loss: 1.518, Test loss: 1.626, Test accuracy: 83.81
Round  94, Train loss: 1.488, Test loss: 1.626, Test accuracy: 83.83
Round  95, Train loss: 1.488, Test loss: 1.626, Test accuracy: 83.84
Round  96, Train loss: 1.525, Test loss: 1.626, Test accuracy: 83.87
Round  97, Train loss: 1.480, Test loss: 1.626, Test accuracy: 83.87
Round  98, Train loss: 1.479, Test loss: 1.626, Test accuracy: 83.86
Round  99, Train loss: 1.509, Test loss: 1.626, Test accuracy: 83.88/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.496, Test loss: 1.625, Test accuracy: 83.87
Average accuracy final 10 rounds: 83.85916666666668 

1323.5252902507782
[1.2428152561187744, 2.485630512237549, 3.674614667892456, 4.863598823547363, 5.951647758483887, 7.03969669342041, 8.031782150268555, 9.0238676071167, 9.950754880905151, 10.877642154693604, 11.871899127960205, 12.866156101226807, 13.814558267593384, 14.762960433959961, 15.704482078552246, 16.64600372314453, 17.65276575088501, 18.65952777862549, 19.635381937026978, 20.611236095428467, 21.561065435409546, 22.510894775390625, 23.44971227645874, 24.388529777526855, 25.37814998626709, 26.367770195007324, 27.282697916030884, 28.197625637054443, 29.24933099746704, 30.30103635787964, 31.26936388015747, 32.2376914024353, 33.24158763885498, 34.24548387527466, 35.23578882217407, 36.226093769073486, 37.2728796005249, 38.31966543197632, 39.26609659194946, 40.21252775192261, 41.19581198692322, 42.17909622192383, 43.202001094818115, 44.2249059677124, 45.227733850479126, 46.23056173324585, 47.236327171325684, 48.24209260940552, 49.21373534202576, 50.185378074645996, 51.19362831115723, 52.20187854766846, 53.15974473953247, 54.117610931396484, 55.13282585144043, 56.148040771484375, 57.10110306739807, 58.05416536331177, 59.07205557823181, 60.089945793151855, 61.09996271133423, 62.1099796295166, 63.1006076335907, 64.0912356376648, 65.07982540130615, 66.06841516494751, 67.05407166481018, 68.03972816467285, 69.03331351280212, 70.0268988609314, 71.04307126998901, 72.05924367904663, 73.02909803390503, 73.99895238876343, 74.96218848228455, 75.92542457580566, 76.91726756095886, 77.90911054611206, 78.91689705848694, 79.92468357086182, 80.85902976989746, 81.7933759689331, 82.76002717018127, 83.72667837142944, 84.71797108650208, 85.7092638015747, 86.67402720451355, 87.63879060745239, 88.64473795890808, 89.65068531036377, 90.59057283401489, 91.53046035766602, 92.50526475906372, 93.48006916046143, 94.43105292320251, 95.3820366859436, 96.34675788879395, 97.31147909164429, 98.27467060089111, 99.23786211013794, 100.18422198295593, 101.13058185577393, 102.15475153923035, 103.17892122268677, 104.1482446193695, 105.11756801605225, 106.11125659942627, 107.1049451828003, 108.11101460456848, 109.11708402633667, 110.11308598518372, 111.10908794403076, 112.0298011302948, 112.95051431655884, 113.9792070388794, 115.00789976119995, 115.99346256256104, 116.97902536392212, 117.94618582725525, 118.91334629058838, 120.00433492660522, 121.09532356262207, 122.18656325340271, 123.27780294418335, 124.36713767051697, 125.45647239685059, 126.55319476127625, 127.6499171257019, 128.72283387184143, 129.79575061798096, 130.85472345352173, 131.9136962890625, 133.00045323371887, 134.08721017837524, 135.1576371192932, 136.22806406021118, 137.31336212158203, 138.39866018295288, 139.4808897972107, 140.5631194114685, 141.5876269340515, 142.61213445663452, 143.69178438186646, 144.7714343070984, 145.7197618484497, 146.66808938980103, 147.6565203666687, 148.64495134353638, 149.62870383262634, 150.6124563217163, 151.5716917514801, 152.5309271812439, 153.52763891220093, 154.52435064315796, 155.52093720436096, 156.51752376556396, 157.45296216011047, 158.38840055465698, 159.36808061599731, 160.34776067733765, 161.32506561279297, 162.3023705482483, 163.25132608413696, 164.20028162002563, 165.1948254108429, 166.18936920166016, 167.1984887123108, 168.20760822296143, 169.131591796875, 170.05557537078857, 171.05552697181702, 172.05547857284546, 173.0646688938141, 174.07385921478271, 175.01452374458313, 175.95518827438354, 176.93616223335266, 177.91713619232178, 178.88964939117432, 179.86216259002686, 180.8451657295227, 181.82816886901855, 182.82690024375916, 183.82563161849976, 184.79740285873413, 185.7691740989685, 186.75147414207458, 187.73377418518066, 188.69042420387268, 189.6470742225647, 190.62420868873596, 191.60134315490723, 192.54062914848328, 193.47991514205933, 194.48062109947205, 195.48132705688477, 196.51539015769958, 197.5494532585144, 198.5161440372467, 199.482834815979, 201.1753568649292, 202.8678789138794]
[14.5, 14.5, 18.483333333333334, 18.483333333333334, 22.283333333333335, 22.283333333333335, 25.733333333333334, 25.733333333333334, 26.533333333333335, 26.533333333333335, 26.025, 26.025, 29.341666666666665, 29.341666666666665, 34.125, 34.125, 41.03333333333333, 41.03333333333333, 46.458333333333336, 46.458333333333336, 53.233333333333334, 53.233333333333334, 58.80833333333333, 58.80833333333333, 61.84166666666667, 61.84166666666667, 65.425, 65.425, 66.81666666666666, 66.81666666666666, 68.06666666666666, 68.06666666666666, 69.29166666666667, 69.29166666666667, 70.15833333333333, 70.15833333333333, 73.01666666666667, 73.01666666666667, 73.275, 73.275, 73.63333333333334, 73.63333333333334, 74.2, 74.2, 74.70833333333333, 74.70833333333333, 75.03333333333333, 75.03333333333333, 75.65, 75.65, 77.4, 77.4, 78.175, 78.175, 79.26666666666667, 79.26666666666667, 80.19166666666666, 80.19166666666666, 80.60833333333333, 80.60833333333333, 80.69166666666666, 80.69166666666666, 81.56666666666666, 81.56666666666666, 81.81666666666666, 81.81666666666666, 81.94166666666666, 81.94166666666666, 82.4, 82.4, 82.575, 82.575, 82.64166666666667, 82.64166666666667, 82.69166666666666, 82.69166666666666, 82.775, 82.775, 82.85833333333333, 82.85833333333333, 82.89166666666667, 82.89166666666667, 82.94166666666666, 82.94166666666666, 82.99166666666666, 82.99166666666666, 82.99166666666666, 82.99166666666666, 83.00833333333334, 83.00833333333334, 83.025, 83.025, 83.11666666666666, 83.11666666666666, 83.15, 83.15, 83.13333333333334, 83.13333333333334, 83.13333333333334, 83.13333333333334, 83.1, 83.1, 83.49166666666666, 83.49166666666666, 83.575, 83.575, 83.54166666666667, 83.54166666666667, 83.61666666666666, 83.61666666666666, 83.6, 83.6, 83.59166666666667, 83.59166666666667, 83.59166666666667, 83.59166666666667, 83.64166666666667, 83.64166666666667, 83.625, 83.625, 83.76666666666667, 83.76666666666667, 83.69166666666666, 83.69166666666666, 83.725, 83.725, 83.75833333333334, 83.75833333333334, 83.69166666666666, 83.69166666666666, 83.74166666666666, 83.74166666666666, 83.80833333333334, 83.80833333333334, 83.78333333333333, 83.78333333333333, 83.78333333333333, 83.78333333333333, 83.875, 83.875, 83.86666666666666, 83.86666666666666, 83.84166666666667, 83.84166666666667, 83.85, 83.85, 83.85833333333333, 83.85833333333333, 83.90833333333333, 83.90833333333333, 83.875, 83.875, 83.85833333333333, 83.85833333333333, 83.86666666666666, 83.86666666666666, 83.875, 83.875, 83.86666666666666, 83.86666666666666, 83.86666666666666, 83.86666666666666, 83.875, 83.875, 83.875, 83.875, 83.85833333333333, 83.85833333333333, 83.83333333333333, 83.83333333333333, 83.8, 83.8, 83.80833333333334, 83.80833333333334, 83.8, 83.8, 83.8, 83.8, 83.84166666666667, 83.84166666666667, 83.9, 83.9, 83.85, 83.85, 83.89166666666667, 83.89166666666667, 83.80833333333334, 83.80833333333334, 83.825, 83.825, 83.84166666666667, 83.84166666666667, 83.86666666666666, 83.86666666666666, 83.86666666666666, 83.86666666666666, 83.85833333333333, 83.85833333333333, 83.88333333333334, 83.88333333333334, 83.86666666666666, 83.86666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.706, Test loss: 2.299, Test accuracy: 15.86
Round   1, Train loss: 1.651, Test loss: 2.288, Test accuracy: 25.18
Round   2, Train loss: 1.567, Test loss: 2.266, Test accuracy: 34.78
Round   3, Train loss: 1.484, Test loss: 2.236, Test accuracy: 39.83
Round   4, Train loss: 1.473, Test loss: 2.201, Test accuracy: 44.77
Round   5, Train loss: 1.437, Test loss: 2.162, Test accuracy: 49.69
Round   6, Train loss: 1.416, Test loss: 2.130, Test accuracy: 52.59
Round   7, Train loss: 1.398, Test loss: 2.100, Test accuracy: 52.94
Round   8, Train loss: 1.337, Test loss: 2.086, Test accuracy: 52.53
Round   9, Train loss: 1.363, Test loss: 2.073, Test accuracy: 52.93
Round  10, Train loss: 1.333, Test loss: 2.062, Test accuracy: 52.41
Round  11, Train loss: 1.371, Test loss: 2.054, Test accuracy: 51.68
Round  12, Train loss: 1.374, Test loss: 2.043, Test accuracy: 52.32
Round  13, Train loss: 1.367, Test loss: 2.035, Test accuracy: 52.18
Round  14, Train loss: 1.391, Test loss: 2.031, Test accuracy: 52.42
Round  15, Train loss: 1.408, Test loss: 2.023, Test accuracy: 52.52
Round  16, Train loss: 1.303, Test loss: 2.020, Test accuracy: 52.63
Round  17, Train loss: 1.327, Test loss: 2.014, Test accuracy: 53.31
Round  18, Train loss: 1.396, Test loss: 2.007, Test accuracy: 53.76
Round  19, Train loss: 1.335, Test loss: 2.006, Test accuracy: 53.49
Round  20, Train loss: 1.356, Test loss: 2.003, Test accuracy: 53.83
Round  21, Train loss: 1.330, Test loss: 2.001, Test accuracy: 53.78
Round  22, Train loss: 1.406, Test loss: 1.998, Test accuracy: 53.58
Round  23, Train loss: 1.298, Test loss: 1.996, Test accuracy: 53.61
Round  24, Train loss: 1.323, Test loss: 1.992, Test accuracy: 53.67
Round  25, Train loss: 1.323, Test loss: 1.991, Test accuracy: 53.73
Round  26, Train loss: 1.376, Test loss: 1.989, Test accuracy: 53.68
Round  27, Train loss: 1.276, Test loss: 1.990, Test accuracy: 53.40
Round  28, Train loss: 1.329, Test loss: 1.990, Test accuracy: 53.21
Round  29, Train loss: 1.337, Test loss: 1.991, Test accuracy: 53.23
Round  30, Train loss: 1.337, Test loss: 1.993, Test accuracy: 52.46
Round  31, Train loss: 1.274, Test loss: 1.994, Test accuracy: 52.47
Round  32, Train loss: 1.318, Test loss: 1.993, Test accuracy: 52.23
Round  33, Train loss: 1.327, Test loss: 1.990, Test accuracy: 52.32
Round  34, Train loss: 1.371, Test loss: 1.989, Test accuracy: 52.09
Round  35, Train loss: 1.330, Test loss: 1.987, Test accuracy: 52.18
Round  36, Train loss: 1.306, Test loss: 1.987, Test accuracy: 51.99
Round  37, Train loss: 1.352, Test loss: 1.987, Test accuracy: 51.97
Round  38, Train loss: 1.293, Test loss: 1.986, Test accuracy: 52.06
Round  39, Train loss: 1.306, Test loss: 1.986, Test accuracy: 52.06
Round  40, Train loss: 1.278, Test loss: 1.988, Test accuracy: 51.57
Round  41, Train loss: 1.349, Test loss: 1.988, Test accuracy: 51.68
Round  42, Train loss: 1.381, Test loss: 1.988, Test accuracy: 51.52
Round  43, Train loss: 1.296, Test loss: 1.989, Test accuracy: 51.23
Round  44, Train loss: 1.316, Test loss: 1.989, Test accuracy: 51.10
Round  45, Train loss: 1.321, Test loss: 1.990, Test accuracy: 51.11
Round  46, Train loss: 1.358, Test loss: 1.990, Test accuracy: 50.89
Round  47, Train loss: 1.305, Test loss: 1.988, Test accuracy: 51.08
Round  48, Train loss: 1.301, Test loss: 1.989, Test accuracy: 50.85
Round  49, Train loss: 1.305, Test loss: 1.990, Test accuracy: 50.67
Round  50, Train loss: 1.307, Test loss: 1.985, Test accuracy: 50.97
Round  51, Train loss: 1.310, Test loss: 1.981, Test accuracy: 51.62
Round  52, Train loss: 1.252, Test loss: 1.979, Test accuracy: 51.69
Round  53, Train loss: 1.324, Test loss: 1.986, Test accuracy: 50.54
Round  54, Train loss: 1.304, Test loss: 1.988, Test accuracy: 50.27
Round  55, Train loss: 1.289, Test loss: 1.986, Test accuracy: 50.43
Round  56, Train loss: 1.290, Test loss: 1.986, Test accuracy: 50.33
Round  57, Train loss: 1.304, Test loss: 1.983, Test accuracy: 50.86
Round  58, Train loss: 1.326, Test loss: 1.984, Test accuracy: 50.76
Round  59, Train loss: 1.307, Test loss: 1.982, Test accuracy: 50.79
Round  60, Train loss: 1.280, Test loss: 1.986, Test accuracy: 49.98
Round  61, Train loss: 1.316, Test loss: 1.986, Test accuracy: 49.79
Round  62, Train loss: 1.242, Test loss: 1.988, Test accuracy: 49.46
Round  63, Train loss: 1.282, Test loss: 1.987, Test accuracy: 49.77
Round  64, Train loss: 1.314, Test loss: 1.987, Test accuracy: 49.83
Round  65, Train loss: 1.214, Test loss: 1.983, Test accuracy: 50.42
Round  66, Train loss: 1.267, Test loss: 1.988, Test accuracy: 49.95
Round  67, Train loss: 1.338, Test loss: 1.987, Test accuracy: 50.00
Round  68, Train loss: 1.262, Test loss: 1.989, Test accuracy: 49.92
Round  69, Train loss: 1.352, Test loss: 1.992, Test accuracy: 49.55
Round  70, Train loss: 1.309, Test loss: 1.994, Test accuracy: 49.39
Round  71, Train loss: 1.259, Test loss: 1.992, Test accuracy: 49.53
Round  72, Train loss: 1.270, Test loss: 1.992, Test accuracy: 49.63
Round  73, Train loss: 1.275, Test loss: 1.986, Test accuracy: 50.47
Round  74, Train loss: 1.299, Test loss: 1.987, Test accuracy: 50.26
Round  75, Train loss: 1.305, Test loss: 1.991, Test accuracy: 49.62
Round  76, Train loss: 1.287, Test loss: 1.990, Test accuracy: 49.61
Round  77, Train loss: 1.265, Test loss: 1.995, Test accuracy: 48.86
Round  78, Train loss: 1.312, Test loss: 1.991, Test accuracy: 49.52
Round  79, Train loss: 1.257, Test loss: 1.989, Test accuracy: 49.54
Round  80, Train loss: 1.259, Test loss: 1.996, Test accuracy: 48.43
Round  81, Train loss: 1.251, Test loss: 1.991, Test accuracy: 49.15
Round  82, Train loss: 1.314, Test loss: 1.994, Test accuracy: 48.98
Round  83, Train loss: 1.305, Test loss: 1.994, Test accuracy: 48.71
Round  84, Train loss: 1.283, Test loss: 1.996, Test accuracy: 48.53
Round  85, Train loss: 1.298, Test loss: 2.000, Test accuracy: 48.03
Round  86, Train loss: 1.288, Test loss: 2.000, Test accuracy: 48.08
Round  87, Train loss: 1.304, Test loss: 1.998, Test accuracy: 48.23
Round  88, Train loss: 1.276, Test loss: 2.000, Test accuracy: 47.80
Round  89, Train loss: 1.259, Test loss: 2.002, Test accuracy: 47.59
Round  90, Train loss: 1.273, Test loss: 2.001, Test accuracy: 47.77
Round  91, Train loss: 1.292, Test loss: 2.005, Test accuracy: 47.35
Round  92, Train loss: 1.310, Test loss: 2.007, Test accuracy: 47.08
Round  93, Train loss: 1.277, Test loss: 2.004, Test accuracy: 47.34
Round  94, Train loss: 1.332, Test loss: 2.004, Test accuracy: 47.42
Round  95, Train loss: 1.297, Test loss: 2.003, Test accuracy: 47.53
Round  96, Train loss: 1.275, Test loss: 2.005, Test accuracy: 47.47
Round  97, Train loss: 1.275, Test loss: 2.006, Test accuracy: 47.37
Round  98, Train loss: 1.215, Test loss: 2.005, Test accuracy: 47.31
Round  99, Train loss: 1.241, Test loss: 2.005, Test accuracy: 47.39
Final Round, Train loss: 1.286, Test loss: 2.011, Test accuracy: 46.43
Average accuracy final 10 rounds: 47.401666666666664
1542.4355726242065
[]
[15.858333333333333, 25.183333333333334, 34.78333333333333, 39.833333333333336, 44.766666666666666, 49.69166666666667, 52.59166666666667, 52.94166666666667, 52.53333333333333, 52.93333333333333, 52.40833333333333, 51.68333333333333, 52.31666666666667, 52.18333333333333, 52.416666666666664, 52.525, 52.63333333333333, 53.30833333333333, 53.75833333333333, 53.49166666666667, 53.825, 53.78333333333333, 53.583333333333336, 53.608333333333334, 53.675, 53.725, 53.68333333333333, 53.4, 53.208333333333336, 53.225, 52.458333333333336, 52.46666666666667, 52.233333333333334, 52.31666666666667, 52.09166666666667, 52.18333333333333, 51.99166666666667, 51.96666666666667, 52.05833333333333, 52.05833333333333, 51.56666666666667, 51.68333333333333, 51.525, 51.233333333333334, 51.1, 51.108333333333334, 50.891666666666666, 51.075, 50.85, 50.666666666666664, 50.96666666666667, 51.61666666666667, 51.69166666666667, 50.541666666666664, 50.275, 50.43333333333333, 50.333333333333336, 50.858333333333334, 50.75833333333333, 50.791666666666664, 49.983333333333334, 49.791666666666664, 49.458333333333336, 49.766666666666666, 49.825, 50.425, 49.95, 50.0, 49.916666666666664, 49.55, 49.391666666666666, 49.53333333333333, 49.63333333333333, 50.46666666666667, 50.25833333333333, 49.625, 49.608333333333334, 48.858333333333334, 49.525, 49.541666666666664, 48.43333333333333, 49.15, 48.983333333333334, 48.708333333333336, 48.53333333333333, 48.03333333333333, 48.075, 48.233333333333334, 47.8, 47.59166666666667, 47.766666666666666, 47.35, 47.075, 47.34166666666667, 47.416666666666664, 47.53333333333333, 47.46666666666667, 47.36666666666667, 47.30833333333333, 47.391666666666666, 46.43333333333333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.287, Test loss: 2.281, Test accuracy: 16.81
Round   1, Train loss: 2.246, Test loss: 2.252, Test accuracy: 19.09
Round   2, Train loss: 2.197, Test loss: 2.185, Test accuracy: 27.29
Round   3, Train loss: 2.019, Test loss: 2.121, Test accuracy: 34.70
Round   4, Train loss: 1.985, Test loss: 2.099, Test accuracy: 37.36
Round   5, Train loss: 1.940, Test loss: 2.082, Test accuracy: 37.02
Round   6, Train loss: 1.696, Test loss: 2.095, Test accuracy: 36.31
Round   7, Train loss: 1.724, Test loss: 1.975, Test accuracy: 50.12
Round   8, Train loss: 1.314, Test loss: 1.970, Test accuracy: 50.58
Round   9, Train loss: 1.660, Test loss: 2.026, Test accuracy: 44.93
Round  10, Train loss: 1.067, Test loss: 1.948, Test accuracy: 53.20
Round  11, Train loss: 0.980, Test loss: 1.930, Test accuracy: 54.15
Round  12, Train loss: 1.365, Test loss: 1.926, Test accuracy: 54.48
Round  13, Train loss: 0.910, Test loss: 1.867, Test accuracy: 61.09
Round  14, Train loss: 0.308, Test loss: 1.827, Test accuracy: 64.56
Round  15, Train loss: 0.731, Test loss: 1.801, Test accuracy: 66.94
Round  16, Train loss: 0.534, Test loss: 1.790, Test accuracy: 67.81
Round  17, Train loss: 0.742, Test loss: 1.800, Test accuracy: 66.83
Round  18, Train loss: 0.589, Test loss: 1.779, Test accuracy: 69.06
Round  19, Train loss: 0.228, Test loss: 1.776, Test accuracy: 69.06
Round  20, Train loss: 0.272, Test loss: 1.769, Test accuracy: 69.48
Round  21, Train loss: -0.215, Test loss: 1.748, Test accuracy: 71.31
Round  22, Train loss: -0.850, Test loss: 1.728, Test accuracy: 73.33
Round  23, Train loss: -0.643, Test loss: 1.726, Test accuracy: 73.55
Round  24, Train loss: 0.251, Test loss: 1.723, Test accuracy: 73.86
Round  25, Train loss: -0.021, Test loss: 1.732, Test accuracy: 72.90
Round  26, Train loss: 0.169, Test loss: 1.724, Test accuracy: 73.71
Round  27, Train loss: -0.161, Test loss: 1.731, Test accuracy: 72.94
Round  28, Train loss: -0.904, Test loss: 1.736, Test accuracy: 72.50
Round  29, Train loss: -0.848, Test loss: 1.738, Test accuracy: 72.27
Round  30, Train loss: -0.936, Test loss: 1.733, Test accuracy: 72.77
Round  31, Train loss: -0.833, Test loss: 1.702, Test accuracy: 75.85
Round  32, Train loss: -0.658, Test loss: 1.721, Test accuracy: 73.88
Round  33, Train loss: -1.156, Test loss: 1.717, Test accuracy: 74.37
Round  34, Train loss: -0.759, Test loss: 1.700, Test accuracy: 76.03
Round  35, Train loss: -0.705, Test loss: 1.698, Test accuracy: 76.22
Round  36, Train loss: -1.235, Test loss: 1.689, Test accuracy: 77.09
Round  37, Train loss: -1.561, Test loss: 1.677, Test accuracy: 78.29
Round  38, Train loss: -0.490, Test loss: 1.712, Test accuracy: 74.84
Round  39, Train loss: -0.764, Test loss: 1.704, Test accuracy: 75.61
Round  40, Train loss: -1.165, Test loss: 1.697, Test accuracy: 76.30
Round  41, Train loss: -1.769, Test loss: 1.697, Test accuracy: 76.34
Round  42, Train loss: -1.127, Test loss: 1.695, Test accuracy: 76.53
Round  43, Train loss: -1.079, Test loss: 1.711, Test accuracy: 74.97
Round  44, Train loss: -1.623, Test loss: 1.706, Test accuracy: 75.44
Round  45, Train loss: -0.804, Test loss: 1.684, Test accuracy: 77.71
Round  46, Train loss: -1.649, Test loss: 1.655, Test accuracy: 80.64
Round  47, Train loss: -1.074, Test loss: 1.681, Test accuracy: 77.94
Round  48, Train loss: -1.148, Test loss: 1.676, Test accuracy: 78.45
Round  49, Train loss: -1.230, Test loss: 1.704, Test accuracy: 75.66
Round  50, Train loss: -0.973, Test loss: 1.683, Test accuracy: 77.77
Round  51, Train loss: -1.632, Test loss: 1.679, Test accuracy: 78.17
Round  52, Train loss: -1.086, Test loss: 1.650, Test accuracy: 81.04
Round  53, Train loss: -0.631, Test loss: 1.684, Test accuracy: 77.67
Round  54, Train loss: -0.847, Test loss: 1.685, Test accuracy: 77.54
Round  55, Train loss: -1.455, Test loss: 1.696, Test accuracy: 76.53
Round  56, Train loss: -1.970, Test loss: 1.688, Test accuracy: 77.28
Round  57, Train loss: -1.101, Test loss: 1.683, Test accuracy: 77.78
Round  58, Train loss: -1.407, Test loss: 1.675, Test accuracy: 78.58
Round  59, Train loss: -1.223, Test loss: 1.663, Test accuracy: 79.76
Round  60, Train loss: -1.198, Test loss: 1.663, Test accuracy: 79.74
Round  61, Train loss: -1.425, Test loss: 1.633, Test accuracy: 82.80
Round  62, Train loss: -1.685, Test loss: 1.655, Test accuracy: 80.62
Round  63, Train loss: -1.435, Test loss: 1.659, Test accuracy: 80.20
Round  64, Train loss: -1.288, Test loss: 1.663, Test accuracy: 79.78
Round  65, Train loss: -1.374, Test loss: 1.685, Test accuracy: 77.55
Round  66, Train loss: -1.338, Test loss: 1.676, Test accuracy: 78.42
Round  67, Train loss: -1.260, Test loss: 1.678, Test accuracy: 78.26
Round  68, Train loss: -1.465, Test loss: 1.680, Test accuracy: 78.03
Round  69, Train loss: -1.574, Test loss: 1.676, Test accuracy: 78.37
Round  70, Train loss: -1.034, Test loss: 1.652, Test accuracy: 80.86
Round  71, Train loss: -1.518, Test loss: 1.664, Test accuracy: 79.69
Round  72, Train loss: -1.460, Test loss: 1.650, Test accuracy: 81.13
Round  73, Train loss: -1.042, Test loss: 1.650, Test accuracy: 81.10
Round  74, Train loss: -1.485, Test loss: 1.648, Test accuracy: 81.32
Round  75, Train loss: -1.495, Test loss: 1.647, Test accuracy: 81.36
Round  76, Train loss: -1.797, Test loss: 1.617, Test accuracy: 84.34
Round  77, Train loss: -1.419, Test loss: 1.666, Test accuracy: 79.40
Round  78, Train loss: -1.628, Test loss: 1.637, Test accuracy: 82.34
Round  79, Train loss: -1.449, Test loss: 1.658, Test accuracy: 80.19
Round  80, Train loss: -1.575, Test loss: 1.654, Test accuracy: 80.63
Round  81, Train loss: -1.596, Test loss: 1.644, Test accuracy: 81.67
Round  82, Train loss: -1.309, Test loss: 1.663, Test accuracy: 79.78
Round  83, Train loss: -1.776, Test loss: 1.666, Test accuracy: 79.52
Round  84, Train loss: -1.923, Test loss: 1.649, Test accuracy: 81.20
Round  85, Train loss: -1.148, Test loss: 1.645, Test accuracy: 81.53
Round  86, Train loss: -1.253, Test loss: 1.615, Test accuracy: 84.56
Round  87, Train loss: -1.282, Test loss: 1.643, Test accuracy: 81.70
Round  88, Train loss: -1.363, Test loss: 1.634, Test accuracy: 82.61
Round  89, Train loss: -1.211, Test loss: 1.606, Test accuracy: 85.48
Round  90, Train loss: -1.340, Test loss: 1.629, Test accuracy: 83.14
Round  91, Train loss: -1.153, Test loss: 1.664, Test accuracy: 79.62
Round  92, Train loss: -1.607, Test loss: 1.635, Test accuracy: 82.51
Round  93, Train loss: -1.275, Test loss: 1.626, Test accuracy: 83.40
Round  94, Train loss: -1.270, Test loss: 1.611, Test accuracy: 84.95
Round  95, Train loss: -1.273, Test loss: 1.641, Test accuracy: 81.95
Round  96, Train loss: -1.521, Test loss: 1.642, Test accuracy: 81.91
Round  97, Train loss: -1.454, Test loss: 1.642, Test accuracy: 81.90
Round  98, Train loss: -1.513, Test loss: 1.619, Test accuracy: 84.15
Round  99, Train loss: -1.466, Test loss: 1.633, Test accuracy: 82.74
Final Round, Train loss: 1.767, Test loss: 1.730, Test accuracy: 73.33
Average accuracy final 10 rounds: 82.62700000000001
Average global accuracy final 10 rounds: 82.62700000000001
3894.9198553562164
[]
[16.8075, 19.0925, 27.2875, 34.7025, 37.3625, 37.025, 36.3125, 50.1175, 50.5825, 44.9325, 53.2, 54.1525, 54.475, 61.095, 64.565, 66.9375, 67.81, 66.835, 69.055, 69.065, 69.4775, 71.3125, 73.33, 73.5525, 73.8625, 72.9025, 73.71, 72.9425, 72.4975, 72.27, 72.7675, 75.85, 73.875, 74.37, 76.0275, 76.22, 77.0925, 78.29, 74.84, 75.61, 76.2975, 76.345, 76.5275, 74.975, 75.4425, 77.7125, 80.64, 77.935, 78.455, 75.655, 77.765, 78.165, 81.0375, 77.6725, 77.54, 76.53, 77.2775, 77.775, 78.575, 79.76, 79.7375, 82.795, 80.6225, 80.2025, 79.775, 77.5525, 78.4175, 78.26, 78.035, 78.3675, 80.86, 79.6925, 81.1275, 81.0975, 81.3225, 81.355, 84.34, 79.4025, 82.34, 80.19, 80.63, 81.675, 79.7825, 79.5225, 81.2025, 81.525, 84.565, 81.7, 82.615, 85.4825, 83.1375, 79.6225, 82.5125, 83.4025, 84.95, 81.95, 81.9075, 81.8975, 84.1525, 82.7375, 73.3275]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.71
Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.66
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.71
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.67
Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.72
Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.66
Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.73
Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.66
Round   4, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.70
Round   4, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.64
Round   5, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.70
Round   5, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.67
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.66
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.66
Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.67
Round   7, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.67
Round   8, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.69
Round   8, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.67
Round   9, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.70
Round   9, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.67
Round  10, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.72
Round  10, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.66
Round  11, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.71
Round  11, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.69
Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.68
Round  12, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.69
Round  13, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.72
Round  13, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.71
Round  14, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.76
Round  14, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.71
Round  15, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.77
Round  15, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.73
Round  16, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  16, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.74
Round  17, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.78
Round  17, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  18, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.78
Round  18, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.70
Round  19, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  19, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.73
Round  20, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.78
Round  20, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  21, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  21, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  22, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.81
Round  22, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  23, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.81
Round  23, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  24, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  24, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  25, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.81
Round  25, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  26, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.80
Round  26, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.74
Round  27, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  27, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  28, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  28, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  29, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  29, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  30, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  30, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  31, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  31, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  32, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  32, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  33, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.82
Round  33, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.76
Round  34, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  34, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  35, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.82
Round  35, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  36, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.82
Round  36, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  37, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.82
Round  37, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  38, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  38, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  39, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  39, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  40, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  40, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  41, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  41, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  42, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.85
Round  42, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.76
Round  43, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  43, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  44, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.85
Round  44, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.76
Round  45, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  45, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  46, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  46, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  47, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  47, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  48, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  48, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  49, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  49, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  50, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  50, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  51, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.80
Round  51, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.74
Round  52, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  52, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  53, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  53, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  54, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  54, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.73
Round  55, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  55, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  56, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  56, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  57, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  57, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  58, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  58, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  59, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  59, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  60, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  60, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  61, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  61, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  62, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  62, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  63, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.82
Round  63, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.81
Round  64, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  64, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  65, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  65, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  66, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  66, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.81
Round  67, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.87
Round  67, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.81
Round  68, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  68, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.81
Round  69, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  69, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.81
Round  70, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.86
Round  70, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.80
Round  71, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  71, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  72, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  72, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  73, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  73, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  74, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  74, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  75, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  75, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  76, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  76, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  77, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.82
Round  77, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.74
Round  78, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  78, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  79, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.81
Round  79, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  80, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.81
Round  80, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.77
Round  81, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  81, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  82, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.81
Round  82, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  83, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.82
Round  83, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.79
Round  84, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  84, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  85, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.84
Round  85, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.78
Round  86, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.83
Round  86, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.79
Round  87, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.80
Round  87, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.77
Round  88, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  88, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  89, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  89, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.82
Round  90, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.88
Round  90, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  91, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.88
Round  91, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  92, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  92, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  93, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  93, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.82
Round  94, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  94, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  95, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  95, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  96, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  96, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  97, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  97, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.84/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.88
Round  98, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  99, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.87
Round  99, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.84
Final Round, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.94
Final Round, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.84
Average accuracy final 10 rounds: 9.863750000000001 

Average global accuracy final 10 rounds: 9.8305 

4465.310877323151
[3.7355759143829346, 7.1133873462677, 10.592016220092773, 14.038311243057251, 17.601195573806763, 21.21653151512146, 24.701862573623657, 28.08051586151123, 31.573898315429688, 35.07039546966553, 38.596328020095825, 42.06899642944336, 45.652220010757446, 49.15260100364685, 52.70014953613281, 56.25193667411804, 59.79173040390015, 63.20315146446228, 66.86389923095703, 70.57476425170898, 74.28189754486084, 77.87626218795776, 81.42658853530884, 84.87735271453857, 88.42195105552673, 92.1154215335846, 95.74990916252136, 99.42687392234802, 102.90167140960693, 106.44490504264832, 110.0309431552887, 113.5478286743164, 117.10482573509216, 120.65081596374512, 124.21660995483398, 127.7170205116272, 131.34080576896667, 135.42671179771423, 139.48385906219482, 143.4028513431549, 147.39509105682373, 151.37320923805237, 155.3565480709076, 159.1502480506897, 162.6561884880066, 166.5829746723175, 170.51562452316284, 174.52663946151733, 178.52798295021057, 182.47801232337952, 186.47666335105896, 190.433251619339, 193.98694276809692, 197.45097613334656, 200.9965717792511, 205.0062358379364, 209.00410652160645, 212.99027395248413, 216.7874834537506, 220.2811279296875, 223.72194170951843, 227.30285024642944, 230.8995759487152, 234.42065525054932, 237.98866415023804, 241.62026476860046, 245.27041387557983, 248.90881156921387, 252.37017560005188, 255.98933458328247, 259.63824439048767, 263.2212038040161, 266.74303936958313, 270.4774343967438, 274.0428168773651, 277.60140323638916, 281.5068621635437, 285.45269799232483, 289.41906213760376, 293.3561670780182, 297.31923842430115, 301.3811318874359, 305.4079189300537, 308.98169016838074, 312.5488302707672, 316.10341143608093, 319.66718912124634, 323.1507785320282, 326.646116733551, 330.3103404045105, 333.95027899742126, 337.51392793655396, 341.3083965778351, 345.1650640964508, 348.99357891082764, 352.5656521320343, 356.1251232624054, 359.80872988700867, 363.74071431159973, 367.6947491168976, 369.76402258872986]
[9.71, 9.71, 9.7225, 9.73, 9.695, 9.6975, 9.66, 9.67, 9.6925, 9.7025, 9.72, 9.7125, 9.675, 9.7175, 9.755, 9.77, 9.79, 9.7775, 9.78, 9.7975, 9.7825, 9.7875, 9.8125, 9.815, 9.8, 9.8125, 9.805, 9.8025, 9.7925, 9.7975, 9.785, 9.795, 9.7875, 9.8225, 9.805, 9.8175, 9.825, 9.8225, 9.84, 9.865, 9.85, 9.86, 9.8525, 9.8475, 9.8525, 9.84, 9.8475, 9.8575, 9.83, 9.85, 9.83, 9.805, 9.835, 9.87, 9.8625, 9.8625, 9.8625, 9.8325, 9.84, 9.8675, 9.8525, 9.83, 9.8375, 9.82, 9.8425, 9.86, 9.865, 9.865, 9.87, 9.855, 9.855, 9.8625, 9.8525, 9.8375, 9.835, 9.8, 9.79, 9.82, 9.8275, 9.81, 9.81, 9.8025, 9.81, 9.8175, 9.83, 9.8375, 9.8275, 9.805, 9.8025, 9.8675, 9.88, 9.88, 9.855, 9.85, 9.8525, 9.8625, 9.85, 9.865, 9.875, 9.8675, 9.9375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.300, Test accuracy: 12.62
Round   1, Train loss: 2.285, Test loss: 2.285, Test accuracy: 14.61
Round   2, Train loss: 2.107, Test loss: 2.287, Test accuracy: 14.80
Round   3, Train loss: 2.014, Test loss: 2.284, Test accuracy: 15.78
Round   4, Train loss: 2.114, Test loss: 2.287, Test accuracy: 14.69
Round   5, Train loss: 1.985, Test loss: 2.293, Test accuracy: 13.91
Round   6, Train loss: 1.963, Test loss: 2.286, Test accuracy: 14.79
Round   7, Train loss: 2.067, Test loss: 2.286, Test accuracy: 15.20
Round   8, Train loss: 2.079, Test loss: 2.282, Test accuracy: 15.36
Round   9, Train loss: 1.946, Test loss: 2.290, Test accuracy: 14.81
Round  10, Train loss: 1.910, Test loss: 2.272, Test accuracy: 16.59
Round  11, Train loss: 1.904, Test loss: 2.276, Test accuracy: 16.35
Round  12, Train loss: 1.923, Test loss: 2.273, Test accuracy: 16.79
Round  13, Train loss: 1.849, Test loss: 2.269, Test accuracy: 17.21
Round  14, Train loss: 1.895, Test loss: 2.273, Test accuracy: 16.69
Round  15, Train loss: 1.882, Test loss: 2.267, Test accuracy: 17.80
Round  16, Train loss: 1.847, Test loss: 2.269, Test accuracy: 17.83
Round  17, Train loss: 1.860, Test loss: 2.273, Test accuracy: 16.88
Round  18, Train loss: 1.803, Test loss: 2.286, Test accuracy: 15.76
Round  19, Train loss: 1.900, Test loss: 2.271, Test accuracy: 16.99
Round  20, Train loss: 1.863, Test loss: 2.279, Test accuracy: 16.16
Round  21, Train loss: 1.809, Test loss: 2.268, Test accuracy: 17.76
Round  22, Train loss: 1.793, Test loss: 2.273, Test accuracy: 17.45
Round  23, Train loss: 1.892, Test loss: 2.279, Test accuracy: 16.38
Round  24, Train loss: 1.851, Test loss: 2.282, Test accuracy: 16.52
Round  25, Train loss: 1.814, Test loss: 2.285, Test accuracy: 15.90
Round  26, Train loss: 1.846, Test loss: 2.280, Test accuracy: 15.77
Round  27, Train loss: 1.802, Test loss: 2.269, Test accuracy: 17.01
Round  28, Train loss: 1.780, Test loss: 2.271, Test accuracy: 16.79
Round  29, Train loss: 1.863, Test loss: 2.279, Test accuracy: 16.34
Round  30, Train loss: 1.848, Test loss: 2.277, Test accuracy: 15.91
Round  31, Train loss: 1.766, Test loss: 2.275, Test accuracy: 16.34
Round  32, Train loss: 1.735, Test loss: 2.269, Test accuracy: 17.46
Round  33, Train loss: 1.803, Test loss: 2.265, Test accuracy: 17.63
Round  34, Train loss: 1.770, Test loss: 2.275, Test accuracy: 15.97
Round  35, Train loss: 1.771, Test loss: 2.276, Test accuracy: 15.82
Round  36, Train loss: 1.768, Test loss: 2.274, Test accuracy: 16.16
Round  37, Train loss: 1.741, Test loss: 2.266, Test accuracy: 17.59
Round  38, Train loss: 1.754, Test loss: 2.268, Test accuracy: 16.96
Round  39, Train loss: 1.746, Test loss: 2.260, Test accuracy: 18.31
Round  40, Train loss: 1.690, Test loss: 2.276, Test accuracy: 16.30
Round  41, Train loss: 1.731, Test loss: 2.277, Test accuracy: 16.12
Round  42, Train loss: 1.711, Test loss: 2.275, Test accuracy: 16.21
Round  43, Train loss: 1.779, Test loss: 2.264, Test accuracy: 17.93
Round  44, Train loss: 1.729, Test loss: 2.263, Test accuracy: 18.13
Round  45, Train loss: 1.728, Test loss: 2.259, Test accuracy: 18.35
Round  46, Train loss: 1.748, Test loss: 2.274, Test accuracy: 16.30
Round  47, Train loss: 1.674, Test loss: 2.268, Test accuracy: 17.56
Round  48, Train loss: 1.695, Test loss: 2.273, Test accuracy: 16.52
Round  49, Train loss: 1.711, Test loss: 2.268, Test accuracy: 17.26
Round  50, Train loss: 1.734, Test loss: 2.267, Test accuracy: 18.09
Round  51, Train loss: 1.736, Test loss: 2.268, Test accuracy: 17.59
Round  52, Train loss: 1.706, Test loss: 2.264, Test accuracy: 17.94
Round  53, Train loss: 1.718, Test loss: 2.264, Test accuracy: 17.81
Round  54, Train loss: 1.740, Test loss: 2.272, Test accuracy: 16.59
Round  55, Train loss: 1.691, Test loss: 2.268, Test accuracy: 17.11
Round  56, Train loss: 1.691, Test loss: 2.266, Test accuracy: 17.93
Round  57, Train loss: 1.758, Test loss: 2.279, Test accuracy: 15.89
Round  58, Train loss: 1.696, Test loss: 2.274, Test accuracy: 16.71
Round  59, Train loss: 1.698, Test loss: 2.269, Test accuracy: 16.82
Round  60, Train loss: 1.733, Test loss: 2.272, Test accuracy: 16.57
Round  61, Train loss: 1.721, Test loss: 2.271, Test accuracy: 16.80
Round  62, Train loss: 1.664, Test loss: 2.280, Test accuracy: 15.77
Round  63, Train loss: 1.662, Test loss: 2.273, Test accuracy: 16.86
Round  64, Train loss: 1.735, Test loss: 2.273, Test accuracy: 16.96
Round  65, Train loss: 1.688, Test loss: 2.292, Test accuracy: 14.14
Round  66, Train loss: 1.718, Test loss: 2.274, Test accuracy: 17.00
Round  67, Train loss: 1.686, Test loss: 2.285, Test accuracy: 15.71
Round  68, Train loss: 1.687, Test loss: 2.260, Test accuracy: 17.87
Round  69, Train loss: 1.720, Test loss: 2.287, Test accuracy: 15.21
Round  70, Train loss: 1.680, Test loss: 2.283, Test accuracy: 15.15
Round  71, Train loss: 1.693, Test loss: 2.284, Test accuracy: 15.06
Round  72, Train loss: 1.673, Test loss: 2.287, Test accuracy: 14.84
Round  73, Train loss: 1.701, Test loss: 2.296, Test accuracy: 14.43
Round  74, Train loss: 1.644, Test loss: 2.271, Test accuracy: 17.18
Round  75, Train loss: 1.629, Test loss: 2.283, Test accuracy: 15.62
Round  76, Train loss: 1.671, Test loss: 2.274, Test accuracy: 16.51
Round  77, Train loss: 1.634, Test loss: 2.268, Test accuracy: 16.89
Round  78, Train loss: 1.683, Test loss: 2.271, Test accuracy: 16.80
Round  79, Train loss: 1.663, Test loss: 2.273, Test accuracy: 16.59
Round  80, Train loss: 1.641, Test loss: 2.274, Test accuracy: 16.36
Round  81, Train loss: 1.627, Test loss: 2.270, Test accuracy: 16.89
Round  82, Train loss: 1.667, Test loss: 2.272, Test accuracy: 16.66
Round  83, Train loss: 1.640, Test loss: 2.263, Test accuracy: 17.71
Round  84, Train loss: 1.638, Test loss: 2.275, Test accuracy: 16.72
Round  85, Train loss: 1.645, Test loss: 2.276, Test accuracy: 15.89
Round  86, Train loss: 1.618, Test loss: 2.260, Test accuracy: 18.11
Round  87, Train loss: 1.648, Test loss: 2.278, Test accuracy: 15.79
Round  88, Train loss: 1.603, Test loss: 2.279, Test accuracy: 15.85
Round  89, Train loss: 1.642, Test loss: 2.258, Test accuracy: 18.59
Round  90, Train loss: 1.639, Test loss: 2.270, Test accuracy: 17.20
Round  91, Train loss: 1.600, Test loss: 2.274, Test accuracy: 16.75
Round  92, Train loss: 1.634, Test loss: 2.256, Test accuracy: 18.44
Round  93, Train loss: 1.626, Test loss: 2.255, Test accuracy: 18.81
Round  94, Train loss: 1.648, Test loss: 2.266, Test accuracy: 16.87
Round  95, Train loss: 1.585, Test loss: 2.268, Test accuracy: 17.56
Round  96, Train loss: 1.650, Test loss: 2.262, Test accuracy: 18.05
Round  97, Train loss: 1.614, Test loss: 2.282, Test accuracy: 15.28
Round  98, Train loss: 1.641, Test loss: 2.278, Test accuracy: 16.12
Round  99, Train loss: 1.611, Test loss: 2.283, Test accuracy: 14.94
Final Round, Train loss: 1.610, Test loss: 2.261, Test accuracy: 17.14
Average accuracy final 10 rounds: 17.001250000000002
6402.290692090988
[9.443022727966309, 18.769291877746582, 27.64563512802124, 36.82928967475891, 46.08959627151489, 55.43185353279114, 64.21048069000244, 72.97492480278015, 81.59699964523315, 90.24593496322632, 99.25161051750183, 108.7015266418457, 117.5609438419342, 126.31704711914062, 135.48368048667908, 144.18780827522278, 152.85577750205994, 161.66883873939514, 170.80730056762695, 179.83060765266418, 188.49919176101685, 197.16386485099792, 205.82643055915833, 214.49473929405212, 223.4998230934143, 232.36059308052063, 241.33184480667114, 250.65128111839294, 259.66386246681213, 268.58256340026855, 277.3477551937103, 286.2125976085663, 294.861572265625, 303.4476315975189, 312.60261392593384, 321.85309958457947, 331.2332019805908, 340.2269947528839, 349.1727318763733, 358.2691798210144, 367.7283401489258, 376.48869013786316, 385.14084458351135, 393.89677262306213, 402.7223472595215, 411.7989480495453, 421.15080094337463, 430.58408212661743, 439.9488220214844, 449.22978138923645, 458.6980128288269, 468.2635476589203, 477.74551343917847, 487.2196536064148, 496.80332922935486, 506.34213304519653, 515.7436044216156, 525.1414546966553, 534.5257215499878, 543.9504261016846, 553.4641711711884, 562.975133895874, 571.7686409950256, 580.6968607902527, 589.909188747406, 598.8967461585999, 607.6593804359436, 616.5589628219604, 625.3291909694672, 634.094024181366, 642.7673556804657, 651.399002790451, 660.0294773578644, 668.6109747886658, 677.232824087143, 685.9703950881958, 694.6034998893738, 703.1933174133301, 711.8531959056854, 720.5329632759094, 729.0695593357086, 737.907559633255, 746.7086930274963, 755.2967472076416, 763.971841096878, 772.8079042434692, 781.6159389019012, 790.1886646747589, 798.7548477649689, 807.4130759239197, 816.0236632823944, 824.578711271286, 833.2753260135651, 841.8969044685364, 850.4879651069641, 859.0259635448456, 867.8426196575165, 876.4069395065308, 885.0872271060944, 893.70077085495, 895.8846538066864]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[12.625, 14.605, 14.805, 15.7775, 14.685, 13.91, 14.795, 15.2025, 15.3575, 14.8075, 16.5925, 16.3525, 16.7925, 17.215, 16.6875, 17.805, 17.83, 16.8825, 15.76, 16.99, 16.16, 17.7625, 17.4525, 16.3775, 16.52, 15.8975, 15.77, 17.0075, 16.7875, 16.3375, 15.9075, 16.335, 17.4625, 17.6275, 15.9675, 15.8225, 16.155, 17.59, 16.9575, 18.3125, 16.3, 16.1175, 16.21, 17.9325, 18.13, 18.3525, 16.2975, 17.5575, 16.5175, 17.2575, 18.085, 17.5925, 17.9375, 17.8125, 16.595, 17.1075, 17.9325, 15.895, 16.7125, 16.8175, 16.5725, 16.8025, 15.765, 16.86, 16.9575, 14.1375, 16.995, 15.7075, 17.8675, 15.205, 15.155, 15.0575, 14.8375, 14.425, 17.185, 15.625, 16.5075, 16.885, 16.795, 16.5925, 16.3625, 16.895, 16.66, 17.7125, 16.72, 15.89, 18.105, 15.795, 15.845, 18.585, 17.195, 16.7525, 18.4375, 18.8075, 16.87, 17.56, 18.055, 15.2775, 16.1175, 14.94, 17.135]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.311, Test loss: 2.301, Test accuracy: 10.03
Round   1, Train loss: 2.280, Test loss: 2.293, Test accuracy: 13.54
Round   2, Train loss: 2.231, Test loss: 2.275, Test accuracy: 15.32
Round   3, Train loss: 2.165, Test loss: 2.233, Test accuracy: 22.03
Round   4, Train loss: 2.123, Test loss: 2.199, Test accuracy: 27.94
Round   5, Train loss: 2.064, Test loss: 2.157, Test accuracy: 32.77
Round   6, Train loss: 2.008, Test loss: 2.104, Test accuracy: 38.38
Round   7, Train loss: 2.040, Test loss: 2.070, Test accuracy: 44.35
Round   8, Train loss: 1.939, Test loss: 2.027, Test accuracy: 47.97
Round   9, Train loss: 1.936, Test loss: 1.993, Test accuracy: 51.70
Round  10, Train loss: 1.919, Test loss: 1.965, Test accuracy: 53.88
Round  11, Train loss: 1.858, Test loss: 1.917, Test accuracy: 58.62
Round  12, Train loss: 1.856, Test loss: 1.917, Test accuracy: 60.04
Round  13, Train loss: 1.845, Test loss: 1.888, Test accuracy: 62.37
Round  14, Train loss: 1.855, Test loss: 1.887, Test accuracy: 62.58
Round  15, Train loss: 1.786, Test loss: 1.840, Test accuracy: 65.61
Round  16, Train loss: 1.823, Test loss: 1.825, Test accuracy: 67.96
Round  17, Train loss: 1.795, Test loss: 1.809, Test accuracy: 69.01
Round  18, Train loss: 1.759, Test loss: 1.798, Test accuracy: 70.45
Round  19, Train loss: 1.744, Test loss: 1.775, Test accuracy: 72.66
Round  20, Train loss: 1.723, Test loss: 1.765, Test accuracy: 74.02
Round  21, Train loss: 1.768, Test loss: 1.752, Test accuracy: 75.98
Round  22, Train loss: 1.671, Test loss: 1.736, Test accuracy: 76.84
Round  23, Train loss: 1.693, Test loss: 1.735, Test accuracy: 77.48
Round  24, Train loss: 1.704, Test loss: 1.716, Test accuracy: 78.29
Round  25, Train loss: 1.679, Test loss: 1.710, Test accuracy: 78.80
Round  26, Train loss: 1.695, Test loss: 1.700, Test accuracy: 79.66
Round  27, Train loss: 1.688, Test loss: 1.704, Test accuracy: 80.19
Round  28, Train loss: 1.662, Test loss: 1.684, Test accuracy: 81.60
Round  29, Train loss: 1.641, Test loss: 1.665, Test accuracy: 82.71
Round  30, Train loss: 1.619, Test loss: 1.660, Test accuracy: 82.94
Round  31, Train loss: 1.634, Test loss: 1.660, Test accuracy: 83.76
Round  32, Train loss: 1.628, Test loss: 1.642, Test accuracy: 84.81
Round  33, Train loss: 1.621, Test loss: 1.649, Test accuracy: 84.78
Round  34, Train loss: 1.642, Test loss: 1.643, Test accuracy: 84.91
Round  35, Train loss: 1.601, Test loss: 1.642, Test accuracy: 84.92
Round  36, Train loss: 1.626, Test loss: 1.643, Test accuracy: 84.94
Round  37, Train loss: 1.613, Test loss: 1.637, Test accuracy: 85.06
Round  38, Train loss: 1.610, Test loss: 1.634, Test accuracy: 85.03
Round  39, Train loss: 1.645, Test loss: 1.637, Test accuracy: 85.16
Round  40, Train loss: 1.632, Test loss: 1.638, Test accuracy: 85.10
Round  41, Train loss: 1.614, Test loss: 1.633, Test accuracy: 85.19
Round  42, Train loss: 1.615, Test loss: 1.629, Test accuracy: 85.21
Round  43, Train loss: 1.623, Test loss: 1.630, Test accuracy: 85.18
Round  44, Train loss: 1.612, Test loss: 1.630, Test accuracy: 85.69
Round  45, Train loss: 1.637, Test loss: 1.630, Test accuracy: 85.73
Round  46, Train loss: 1.621, Test loss: 1.629, Test accuracy: 85.69
Round  47, Train loss: 1.619, Test loss: 1.626, Test accuracy: 85.81
Round  48, Train loss: 1.599, Test loss: 1.621, Test accuracy: 86.16
Round  49, Train loss: 1.588, Test loss: 1.620, Test accuracy: 86.32
Round  50, Train loss: 1.598, Test loss: 1.619, Test accuracy: 86.47
Round  51, Train loss: 1.581, Test loss: 1.611, Test accuracy: 86.88
Round  52, Train loss: 1.583, Test loss: 1.611, Test accuracy: 86.90
Round  53, Train loss: 1.579, Test loss: 1.603, Test accuracy: 87.51
Round  54, Train loss: 1.614, Test loss: 1.609, Test accuracy: 87.56
Round  55, Train loss: 1.590, Test loss: 1.607, Test accuracy: 87.61
Round  56, Train loss: 1.574, Test loss: 1.605, Test accuracy: 87.60
Round  57, Train loss: 1.582, Test loss: 1.603, Test accuracy: 87.60
Round  58, Train loss: 1.562, Test loss: 1.604, Test accuracy: 87.67
Round  59, Train loss: 1.551, Test loss: 1.594, Test accuracy: 88.35
Round  60, Train loss: 1.610, Test loss: 1.599, Test accuracy: 88.40
Round  61, Train loss: 1.574, Test loss: 1.593, Test accuracy: 88.81
Round  62, Train loss: 1.564, Test loss: 1.584, Test accuracy: 89.26
Round  63, Train loss: 1.538, Test loss: 1.586, Test accuracy: 89.40
Round  64, Train loss: 1.551, Test loss: 1.578, Test accuracy: 90.06
Round  65, Train loss: 1.587, Test loss: 1.580, Test accuracy: 90.44
Round  66, Train loss: 1.567, Test loss: 1.583, Test accuracy: 90.49
Round  67, Train loss: 1.552, Test loss: 1.573, Test accuracy: 91.22
Round  68, Train loss: 1.563, Test loss: 1.563, Test accuracy: 91.86
Round  69, Train loss: 1.527, Test loss: 1.561, Test accuracy: 92.27
Round  70, Train loss: 1.551, Test loss: 1.561, Test accuracy: 92.32
Round  71, Train loss: 1.537, Test loss: 1.559, Test accuracy: 92.39
Round  72, Train loss: 1.549, Test loss: 1.561, Test accuracy: 92.35
Round  73, Train loss: 1.579, Test loss: 1.567, Test accuracy: 92.34
Round  74, Train loss: 1.534, Test loss: 1.563, Test accuracy: 92.30
Round  75, Train loss: 1.542, Test loss: 1.557, Test accuracy: 92.39
Round  76, Train loss: 1.510, Test loss: 1.550, Test accuracy: 92.96
Round  77, Train loss: 1.548, Test loss: 1.558, Test accuracy: 92.90
Round  78, Train loss: 1.547, Test loss: 1.558, Test accuracy: 92.89
Round  79, Train loss: 1.523, Test loss: 1.554, Test accuracy: 92.88
Round  80, Train loss: 1.520, Test loss: 1.549, Test accuracy: 92.90
Round  81, Train loss: 1.546, Test loss: 1.545, Test accuracy: 93.44
Round  82, Train loss: 1.542, Test loss: 1.551, Test accuracy: 93.43
Round  83, Train loss: 1.531, Test loss: 1.550, Test accuracy: 93.36
Round  84, Train loss: 1.518, Test loss: 1.544, Test accuracy: 93.81
Round  85, Train loss: 1.511, Test loss: 1.540, Test accuracy: 93.90
Round  86, Train loss: 1.538, Test loss: 1.543, Test accuracy: 93.80
Round  87, Train loss: 1.536, Test loss: 1.541, Test accuracy: 93.91
Round  88, Train loss: 1.527, Test loss: 1.542, Test accuracy: 93.89
Round  89, Train loss: 1.514, Test loss: 1.539, Test accuracy: 93.91
Round  90, Train loss: 1.520, Test loss: 1.539, Test accuracy: 93.90
Round  91, Train loss: 1.558, Test loss: 1.543, Test accuracy: 93.89
Round  92, Train loss: 1.504, Test loss: 1.537, Test accuracy: 93.91
Round  93, Train loss: 1.501, Test loss: 1.535, Test accuracy: 94.06
Round  94, Train loss: 1.511, Test loss: 1.537, Test accuracy: 94.07/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.524, Test loss: 1.536, Test accuracy: 94.36
Round  96, Train loss: 1.527, Test loss: 1.536, Test accuracy: 94.38
Round  97, Train loss: 1.541, Test loss: 1.538, Test accuracy: 94.44
Round  98, Train loss: 1.515, Test loss: 1.535, Test accuracy: 94.45
Round  99, Train loss: 1.518, Test loss: 1.532, Test accuracy: 94.85
Final Round, Train loss: 1.501, Test loss: 1.528, Test accuracy: 94.89
Average accuracy final 10 rounds: 94.231
3680.3744916915894
[4.855016231536865, 9.50922417640686, 13.858511209487915, 18.38762402534485, 22.80555534362793, 26.8503155708313, 31.108685731887817, 35.319308042526245, 39.397510051727295, 43.84643626213074, 48.46121168136597, 52.39828944206238, 56.59237837791443, 60.78569221496582, 64.98848819732666, 69.1585705280304, 73.3620913028717, 77.70531058311462, 81.77349734306335, 86.01404762268066, 90.35544514656067, 94.51781797409058, 98.78074908256531, 102.9572446346283, 107.09020924568176, 111.37251496315002, 115.68226408958435, 120.01844096183777, 124.22416234016418, 128.63992953300476, 132.93323016166687, 137.2161979675293, 141.7015597820282, 146.01344060897827, 150.30629229545593, 154.7228090763092, 159.1669511795044, 163.71879529953003, 168.32299423217773, 172.7797920703888, 177.08861923217773, 181.34652280807495, 185.7420575618744, 190.09791898727417, 194.47214317321777, 198.781152009964, 203.08232831954956, 207.39822459220886, 211.87364172935486, 216.1770679950714, 220.39410710334778, 224.81759762763977, 229.12716460227966, 233.4200575351715, 237.71836185455322, 242.0615954399109, 246.46120977401733, 250.70907759666443, 255.1816794872284, 259.5808787345886, 263.75805044174194, 267.945604801178, 272.19784784317017, 276.67970275878906, 281.06790113449097, 285.3759922981262, 289.80947184562683, 294.0860815048218, 298.3654041290283, 302.9991190433502, 307.3970205783844, 311.7611734867096, 316.23725485801697, 320.5597331523895, 324.9403829574585, 329.1568281650543, 333.4055986404419, 337.54112482070923, 341.676796913147, 345.89807772636414, 350.13438272476196, 354.6596345901489, 359.06733441352844, 363.575279712677, 368.06501603126526, 372.44269704818726, 376.85029554367065, 381.1964817047119, 385.4936418533325, 389.9456901550293, 394.1194167137146, 398.4366776943207, 402.7079350948334, 407.1525390148163, 411.44028973579407, 416.0153844356537, 420.4364836215973, 424.8203115463257, 429.2973475456238, 433.66221928596497, 435.43223690986633]
[10.025, 13.535, 15.3225, 22.0275, 27.9425, 32.7675, 38.3775, 44.35, 47.965, 51.7025, 53.88, 58.625, 60.0375, 62.3675, 62.5825, 65.615, 67.96, 69.0075, 70.4525, 72.6625, 74.015, 75.985, 76.84, 77.48, 78.2925, 78.8, 79.6625, 80.195, 81.5975, 82.7125, 82.9375, 83.76, 84.81, 84.785, 84.91, 84.9225, 84.945, 85.06, 85.035, 85.155, 85.1, 85.1875, 85.2125, 85.1825, 85.685, 85.73, 85.69, 85.815, 86.155, 86.32, 86.465, 86.8775, 86.9025, 87.51, 87.5625, 87.6075, 87.5975, 87.5975, 87.675, 88.3475, 88.4025, 88.805, 89.26, 89.4, 90.0625, 90.4425, 90.4875, 91.215, 91.865, 92.27, 92.3225, 92.39, 92.35, 92.34, 92.2975, 92.385, 92.9575, 92.9025, 92.89, 92.8775, 92.9, 93.4375, 93.43, 93.3625, 93.8125, 93.9, 93.8025, 93.905, 93.8925, 93.91, 93.9025, 93.89, 93.905, 94.0625, 94.0725, 94.3625, 94.3825, 94.435, 94.4475, 94.85, 94.885]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.316, Test loss: 2.302, Test accuracy: 10.90
Round   1, Train loss: 2.305, Test loss: 2.301, Test accuracy: 12.32
Round   2, Train loss: 2.293, Test loss: 2.297, Test accuracy: 12.50
Round   3, Train loss: 2.261, Test loss: 2.286, Test accuracy: 13.16
Round   4, Train loss: 2.262, Test loss: 2.270, Test accuracy: 15.49
Round   5, Train loss: 2.217, Test loss: 2.241, Test accuracy: 19.73
Round   6, Train loss: 2.183, Test loss: 2.224, Test accuracy: 22.94
Round   7, Train loss: 2.137, Test loss: 2.205, Test accuracy: 25.66
Round   8, Train loss: 2.108, Test loss: 2.179, Test accuracy: 28.84
Round   9, Train loss: 2.088, Test loss: 2.152, Test accuracy: 32.56
Round  10, Train loss: 2.032, Test loss: 2.130, Test accuracy: 35.08
Round  11, Train loss: 2.022, Test loss: 2.106, Test accuracy: 37.66
Round  12, Train loss: 1.997, Test loss: 2.084, Test accuracy: 41.07
Round  13, Train loss: 2.006, Test loss: 2.070, Test accuracy: 43.23
Round  14, Train loss: 2.023, Test loss: 2.062, Test accuracy: 44.23
Round  15, Train loss: 2.007, Test loss: 2.039, Test accuracy: 46.64
Round  16, Train loss: 1.953, Test loss: 2.019, Test accuracy: 49.09
Round  17, Train loss: 1.928, Test loss: 1.996, Test accuracy: 51.17
Round  18, Train loss: 1.960, Test loss: 1.991, Test accuracy: 52.04
Round  19, Train loss: 1.957, Test loss: 1.973, Test accuracy: 54.27
Round  20, Train loss: 1.913, Test loss: 1.952, Test accuracy: 57.06
Round  21, Train loss: 1.881, Test loss: 1.919, Test accuracy: 59.51
Round  22, Train loss: 1.831, Test loss: 1.894, Test accuracy: 61.27
Round  23, Train loss: 1.881, Test loss: 1.875, Test accuracy: 64.46
Round  24, Train loss: 1.813, Test loss: 1.867, Test accuracy: 64.64
Round  25, Train loss: 1.857, Test loss: 1.847, Test accuracy: 66.15
Round  26, Train loss: 1.840, Test loss: 1.833, Test accuracy: 67.96
Round  27, Train loss: 1.778, Test loss: 1.819, Test accuracy: 69.25
Round  28, Train loss: 1.764, Test loss: 1.798, Test accuracy: 71.16
Round  29, Train loss: 1.753, Test loss: 1.798, Test accuracy: 71.87
Round  30, Train loss: 1.775, Test loss: 1.793, Test accuracy: 72.37
Round  31, Train loss: 1.708, Test loss: 1.769, Test accuracy: 73.26
Round  32, Train loss: 1.780, Test loss: 1.761, Test accuracy: 74.33
Round  33, Train loss: 1.673, Test loss: 1.743, Test accuracy: 75.55
Round  34, Train loss: 1.797, Test loss: 1.752, Test accuracy: 76.09
Round  35, Train loss: 1.691, Test loss: 1.741, Test accuracy: 76.15
Round  36, Train loss: 1.742, Test loss: 1.730, Test accuracy: 76.83
Round  37, Train loss: 1.724, Test loss: 1.734, Test accuracy: 76.98
Round  38, Train loss: 1.689, Test loss: 1.717, Test accuracy: 77.49
Round  39, Train loss: 1.652, Test loss: 1.716, Test accuracy: 77.62
Round  40, Train loss: 1.750, Test loss: 1.720, Test accuracy: 78.29
Round  41, Train loss: 1.672, Test loss: 1.715, Test accuracy: 78.24
Round  42, Train loss: 1.700, Test loss: 1.712, Test accuracy: 78.70
Round  43, Train loss: 1.640, Test loss: 1.705, Test accuracy: 78.85
Round  44, Train loss: 1.721, Test loss: 1.702, Test accuracy: 79.85
Round  45, Train loss: 1.695, Test loss: 1.698, Test accuracy: 79.96
Round  46, Train loss: 1.683, Test loss: 1.691, Test accuracy: 80.44
Round  47, Train loss: 1.686, Test loss: 1.684, Test accuracy: 81.41
Round  48, Train loss: 1.668, Test loss: 1.674, Test accuracy: 81.88
Round  49, Train loss: 1.659, Test loss: 1.675, Test accuracy: 82.03
Round  50, Train loss: 1.640, Test loss: 1.673, Test accuracy: 82.20
Round  51, Train loss: 1.677, Test loss: 1.654, Test accuracy: 83.82
Round  52, Train loss: 1.663, Test loss: 1.658, Test accuracy: 84.16
Round  53, Train loss: 1.637, Test loss: 1.648, Test accuracy: 84.89
Round  54, Train loss: 1.664, Test loss: 1.656, Test accuracy: 84.90
Round  55, Train loss: 1.612, Test loss: 1.642, Test accuracy: 85.40
Round  56, Train loss: 1.618, Test loss: 1.640, Test accuracy: 85.48
Round  57, Train loss: 1.621, Test loss: 1.638, Test accuracy: 85.45
Round  58, Train loss: 1.583, Test loss: 1.633, Test accuracy: 85.63
Round  59, Train loss: 1.629, Test loss: 1.631, Test accuracy: 85.88
Round  60, Train loss: 1.634, Test loss: 1.635, Test accuracy: 85.83
Round  61, Train loss: 1.620, Test loss: 1.631, Test accuracy: 85.78
Round  62, Train loss: 1.623, Test loss: 1.626, Test accuracy: 86.46
Round  63, Train loss: 1.587, Test loss: 1.624, Test accuracy: 86.42
Round  64, Train loss: 1.626, Test loss: 1.623, Test accuracy: 86.75
Round  65, Train loss: 1.607, Test loss: 1.620, Test accuracy: 86.80
Round  66, Train loss: 1.564, Test loss: 1.614, Test accuracy: 87.22
Round  67, Train loss: 1.600, Test loss: 1.618, Test accuracy: 87.17
Round  68, Train loss: 1.609, Test loss: 1.620, Test accuracy: 87.15
Round  69, Train loss: 1.614, Test loss: 1.616, Test accuracy: 87.34
Round  70, Train loss: 1.618, Test loss: 1.608, Test accuracy: 88.08
Round  71, Train loss: 1.588, Test loss: 1.606, Test accuracy: 88.50
Round  72, Train loss: 1.599, Test loss: 1.606, Test accuracy: 88.60
Round  73, Train loss: 1.614, Test loss: 1.604, Test accuracy: 88.61
Round  74, Train loss: 1.560, Test loss: 1.596, Test accuracy: 88.90
Round  75, Train loss: 1.572, Test loss: 1.595, Test accuracy: 89.38
Round  76, Train loss: 1.564, Test loss: 1.591, Test accuracy: 89.86
Round  77, Train loss: 1.552, Test loss: 1.590, Test accuracy: 89.78
Round  78, Train loss: 1.545, Test loss: 1.586, Test accuracy: 90.20
Round  79, Train loss: 1.555, Test loss: 1.588, Test accuracy: 90.59
Round  80, Train loss: 1.543, Test loss: 1.582, Test accuracy: 90.76
Round  81, Train loss: 1.553, Test loss: 1.582, Test accuracy: 90.80
Round  82, Train loss: 1.562, Test loss: 1.575, Test accuracy: 91.25
Round  83, Train loss: 1.589, Test loss: 1.575, Test accuracy: 91.69
Round  84, Train loss: 1.550, Test loss: 1.568, Test accuracy: 92.20
Round  85, Train loss: 1.584, Test loss: 1.572, Test accuracy: 92.28
Round  86, Train loss: 1.527, Test loss: 1.562, Test accuracy: 92.32
Round  87, Train loss: 1.549, Test loss: 1.567, Test accuracy: 92.37
Round  88, Train loss: 1.546, Test loss: 1.565, Test accuracy: 92.40
Round  89, Train loss: 1.551, Test loss: 1.562, Test accuracy: 92.60
Round  90, Train loss: 1.541, Test loss: 1.559, Test accuracy: 92.60
Round  91, Train loss: 1.541, Test loss: 1.557, Test accuracy: 92.90
Round  92, Train loss: 1.521, Test loss: 1.556, Test accuracy: 92.99
Round  93, Train loss: 1.522, Test loss: 1.553, Test accuracy: 93.04/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.527, Test loss: 1.550, Test accuracy: 93.50
Round  95, Train loss: 1.537, Test loss: 1.547, Test accuracy: 94.03
Round  96, Train loss: 1.554, Test loss: 1.555, Test accuracy: 94.05
Round  97, Train loss: 1.549, Test loss: 1.549, Test accuracy: 94.02
Round  98, Train loss: 1.528, Test loss: 1.551, Test accuracy: 94.16
Round  99, Train loss: 1.540, Test loss: 1.548, Test accuracy: 94.15
Final Round, Train loss: 1.507, Test loss: 1.540, Test accuracy: 94.65
Average accuracy final 10 rounds: 93.54458333333335
2957.5960595607758
[2.748178005218506, 5.496356010437012, 8.258110046386719, 11.019864082336426, 13.72285008430481, 16.425836086273193, 19.178837299346924, 21.931838512420654, 24.509331703186035, 27.086824893951416, 29.554579496383667, 32.02233409881592, 34.64414834976196, 37.26596260070801, 39.90442967414856, 42.54289674758911, 45.077417612075806, 47.6119384765625, 50.20532178878784, 52.798705101013184, 55.40433073043823, 58.00995635986328, 60.52179455757141, 63.03363275527954, 65.50744724273682, 67.98126173019409, 70.4332070350647, 72.8851523399353, 75.32728385925293, 77.76941537857056, 80.29854846000671, 82.82768154144287, 85.4224419593811, 88.01720237731934, 90.51030445098877, 93.0034065246582, 95.5995659828186, 98.195725440979, 100.97304964065552, 103.75037384033203, 106.43334317207336, 109.1163125038147, 111.73095726966858, 114.34560203552246, 117.06763505935669, 119.78966808319092, 122.51768708229065, 125.24570608139038, 128.0013885498047, 130.757071018219, 133.48774528503418, 136.21841955184937, 138.69590735435486, 141.17339515686035, 143.71184420585632, 146.2502932548523, 148.89666056632996, 151.54302787780762, 154.1807336807251, 156.81843948364258, 159.58113312721252, 162.34382677078247, 165.0264790058136, 167.70913124084473, 170.42078852653503, 173.13244581222534, 175.7103807926178, 178.28831577301025, 180.84063005447388, 183.3929443359375, 186.08163166046143, 188.77031898498535, 191.35307598114014, 193.93583297729492, 196.6055727005005, 199.27531242370605, 201.85816621780396, 204.44102001190186, 206.9860475063324, 209.53107500076294, 212.12444615364075, 214.71781730651855, 217.30578756332397, 219.8937578201294, 222.45530128479004, 225.01684474945068, 227.67141795158386, 230.32599115371704, 233.01429986953735, 235.70260858535767, 238.50204396247864, 241.3014793395996, 244.04798579216003, 246.79449224472046, 249.41350388526917, 252.03251552581787, 254.7661416530609, 257.49976778030396, 260.11040329933167, 262.7210388183594, 265.3372263908386, 267.95341396331787, 270.58564138412476, 273.21786880493164, 275.84940576553345, 278.48094272613525, 281.1050956249237, 283.72924852371216, 286.46016931533813, 289.1910901069641, 291.85549426078796, 294.5198984146118, 297.02721309661865, 299.5345277786255, 302.21845507621765, 304.9023823738098, 307.5050437450409, 310.107705116272, 312.78489875793457, 315.46209239959717, 318.13279247283936, 320.80349254608154, 323.43904185295105, 326.07459115982056, 328.7302062511444, 331.38582134246826, 334.02531933784485, 336.66481733322144, 339.39050102233887, 342.1161847114563, 344.7641146183014, 347.4120445251465, 350.17163491249084, 352.9312252998352, 355.7378406524658, 358.54445600509644, 361.3133325576782, 364.08220911026, 366.79530239105225, 369.5083956718445, 372.13824939727783, 374.7681031227112, 377.4029700756073, 380.0378370285034, 382.6552608013153, 385.2726845741272, 387.9660804271698, 390.6594762802124, 393.2467408180237, 395.83400535583496, 398.6041250228882, 401.3742446899414, 404.1853632926941, 406.9964818954468, 409.71504044532776, 412.43359899520874, 415.1379840373993, 417.84236907958984, 420.3060233592987, 422.76967763900757, 425.37905502319336, 427.98843240737915, 430.65456438064575, 433.32069635391235, 436.0827639102936, 438.8448314666748, 441.4123342037201, 443.9798369407654, 446.5854322910309, 449.1910276412964, 451.7642869949341, 454.3375463485718, 456.8237030506134, 459.30985975265503, 461.96539759635925, 464.6209354400635, 467.3301589488983, 470.03938245773315, 472.7086284160614, 475.37787437438965, 478.0110981464386, 480.64432191848755, 483.25225734710693, 485.8601927757263, 488.46111130714417, 491.062029838562, 493.6702527999878, 496.2784757614136, 498.9078047275543, 501.53713369369507, 504.1994593143463, 506.86178493499756, 509.51315426826477, 512.164523601532, 514.7935440540314, 517.4225645065308, 520.0030851364136, 522.5836057662964, 525.1809096336365, 527.7782135009766, 529.4799108505249, 531.1816082000732]
[10.895833333333334, 10.895833333333334, 12.320833333333333, 12.320833333333333, 12.5, 12.5, 13.1625, 13.1625, 15.4875, 15.4875, 19.733333333333334, 19.733333333333334, 22.9375, 22.9375, 25.6625, 25.6625, 28.841666666666665, 28.841666666666665, 32.5625, 32.5625, 35.079166666666666, 35.079166666666666, 37.65833333333333, 37.65833333333333, 41.06666666666667, 41.06666666666667, 43.225, 43.225, 44.233333333333334, 44.233333333333334, 46.641666666666666, 46.641666666666666, 49.09166666666667, 49.09166666666667, 51.170833333333334, 51.170833333333334, 52.041666666666664, 52.041666666666664, 54.270833333333336, 54.270833333333336, 57.0625, 57.0625, 59.50833333333333, 59.50833333333333, 61.270833333333336, 61.270833333333336, 64.45833333333333, 64.45833333333333, 64.6375, 64.6375, 66.15416666666667, 66.15416666666667, 67.9625, 67.9625, 69.25416666666666, 69.25416666666666, 71.1625, 71.1625, 71.86666666666666, 71.86666666666666, 72.36666666666666, 72.36666666666666, 73.2625, 73.2625, 74.325, 74.325, 75.55416666666666, 75.55416666666666, 76.0875, 76.0875, 76.14583333333333, 76.14583333333333, 76.82916666666667, 76.82916666666667, 76.97916666666667, 76.97916666666667, 77.4875, 77.4875, 77.625, 77.625, 78.2875, 78.2875, 78.24166666666666, 78.24166666666666, 78.69583333333334, 78.69583333333334, 78.85416666666667, 78.85416666666667, 79.85, 79.85, 79.9625, 79.9625, 80.4375, 80.4375, 81.4125, 81.4125, 81.88333333333334, 81.88333333333334, 82.025, 82.025, 82.19583333333334, 82.19583333333334, 83.82083333333334, 83.82083333333334, 84.1625, 84.1625, 84.8875, 84.8875, 84.89583333333333, 84.89583333333333, 85.39583333333333, 85.39583333333333, 85.48333333333333, 85.48333333333333, 85.44583333333334, 85.44583333333334, 85.62916666666666, 85.62916666666666, 85.875, 85.875, 85.825, 85.825, 85.78333333333333, 85.78333333333333, 86.45833333333333, 86.45833333333333, 86.41666666666667, 86.41666666666667, 86.74583333333334, 86.74583333333334, 86.80416666666666, 86.80416666666666, 87.225, 87.225, 87.17083333333333, 87.17083333333333, 87.15, 87.15, 87.34166666666667, 87.34166666666667, 88.08333333333333, 88.08333333333333, 88.50416666666666, 88.50416666666666, 88.6, 88.6, 88.60833333333333, 88.60833333333333, 88.90416666666667, 88.90416666666667, 89.375, 89.375, 89.85833333333333, 89.85833333333333, 89.78333333333333, 89.78333333333333, 90.2, 90.2, 90.59166666666667, 90.59166666666667, 90.75833333333334, 90.75833333333334, 90.79583333333333, 90.79583333333333, 91.25416666666666, 91.25416666666666, 91.69166666666666, 91.69166666666666, 92.20416666666667, 92.20416666666667, 92.27916666666667, 92.27916666666667, 92.32083333333334, 92.32083333333334, 92.36666666666666, 92.36666666666666, 92.4, 92.4, 92.6, 92.6, 92.60416666666667, 92.60416666666667, 92.9, 92.9, 92.99166666666666, 92.99166666666666, 93.04166666666667, 93.04166666666667, 93.5, 93.5, 94.02916666666667, 94.02916666666667, 94.05, 94.05, 94.02083333333333, 94.02083333333333, 94.15833333333333, 94.15833333333333, 94.15, 94.15, 94.65416666666667, 94.65416666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.154, Test loss: 2.163, Test accuracy: 30.73
Round   0, Global train loss: 2.154, Global test loss: 2.295, Global test accuracy: 15.00
Round   1, Train loss: 1.863, Test loss: 1.996, Test accuracy: 46.19
Round   1, Global train loss: 1.863, Global test loss: 2.287, Global test accuracy: 15.49
Round   2, Train loss: 1.589, Test loss: 1.874, Test accuracy: 59.67
Round   2, Global train loss: 1.589, Global test loss: 2.293, Global test accuracy: 12.93
Round   3, Train loss: 1.595, Test loss: 1.795, Test accuracy: 65.47
Round   3, Global train loss: 1.595, Global test loss: 2.285, Global test accuracy: 13.38
Round   4, Train loss: 1.696, Test loss: 1.714, Test accuracy: 74.96
Round   4, Global train loss: 1.696, Global test loss: 2.290, Global test accuracy: 14.93
Round   5, Train loss: 1.604, Test loss: 1.665, Test accuracy: 80.70
Round   5, Global train loss: 1.604, Global test loss: 2.285, Global test accuracy: 11.51
Round   6, Train loss: 1.654, Test loss: 1.641, Test accuracy: 83.47
Round   6, Global train loss: 1.654, Global test loss: 2.269, Global test accuracy: 17.96
Round   7, Train loss: 1.605, Test loss: 1.633, Test accuracy: 83.76
Round   7, Global train loss: 1.605, Global test loss: 2.268, Global test accuracy: 18.29
Round   8, Train loss: 1.551, Test loss: 1.608, Test accuracy: 85.63
Round   8, Global train loss: 1.551, Global test loss: 2.310, Global test accuracy: 9.58
Round   9, Train loss: 1.637, Test loss: 1.601, Test accuracy: 86.80
Round   9, Global train loss: 1.637, Global test loss: 2.269, Global test accuracy: 17.45
Round  10, Train loss: 1.641, Test loss: 1.598, Test accuracy: 86.92
Round  10, Global train loss: 1.641, Global test loss: 2.270, Global test accuracy: 18.04
Round  11, Train loss: 1.596, Test loss: 1.582, Test accuracy: 88.43
Round  11, Global train loss: 1.596, Global test loss: 2.268, Global test accuracy: 17.63
Round  12, Train loss: 1.526, Test loss: 1.580, Test accuracy: 88.50
Round  12, Global train loss: 1.526, Global test loss: 2.281, Global test accuracy: 16.14
Round  13, Train loss: 1.578, Test loss: 1.580, Test accuracy: 88.54
Round  13, Global train loss: 1.578, Global test loss: 2.281, Global test accuracy: 14.78
Round  14, Train loss: 1.584, Test loss: 1.580, Test accuracy: 88.51
Round  14, Global train loss: 1.584, Global test loss: 2.268, Global test accuracy: 18.10
Round  15, Train loss: 1.631, Test loss: 1.579, Test accuracy: 88.54
Round  15, Global train loss: 1.631, Global test loss: 2.292, Global test accuracy: 14.93
Round  16, Train loss: 1.475, Test loss: 1.579, Test accuracy: 88.51
Round  16, Global train loss: 1.475, Global test loss: 2.273, Global test accuracy: 17.13
Round  17, Train loss: 1.570, Test loss: 1.567, Test accuracy: 89.99
Round  17, Global train loss: 1.570, Global test loss: 2.264, Global test accuracy: 18.07
Round  18, Train loss: 1.470, Test loss: 1.567, Test accuracy: 90.01
Round  18, Global train loss: 1.470, Global test loss: 2.278, Global test accuracy: 15.13
Round  19, Train loss: 1.542, Test loss: 1.551, Test accuracy: 91.64
Round  19, Global train loss: 1.542, Global test loss: 2.272, Global test accuracy: 16.85
Round  20, Train loss: 1.523, Test loss: 1.551, Test accuracy: 91.64
Round  20, Global train loss: 1.523, Global test loss: 2.282, Global test accuracy: 13.97
Round  21, Train loss: 1.580, Test loss: 1.550, Test accuracy: 91.63
Round  21, Global train loss: 1.580, Global test loss: 2.254, Global test accuracy: 19.38
Round  22, Train loss: 1.505, Test loss: 1.534, Test accuracy: 93.23
Round  22, Global train loss: 1.505, Global test loss: 2.281, Global test accuracy: 14.87
Round  23, Train loss: 1.524, Test loss: 1.534, Test accuracy: 93.18
Round  23, Global train loss: 1.524, Global test loss: 2.281, Global test accuracy: 15.11
Round  24, Train loss: 1.466, Test loss: 1.534, Test accuracy: 93.21
Round  24, Global train loss: 1.466, Global test loss: 2.278, Global test accuracy: 15.28
Round  25, Train loss: 1.468, Test loss: 1.532, Test accuracy: 93.22
Round  25, Global train loss: 1.468, Global test loss: 2.270, Global test accuracy: 16.84
Round  26, Train loss: 1.467, Test loss: 1.532, Test accuracy: 93.24
Round  26, Global train loss: 1.467, Global test loss: 2.301, Global test accuracy: 11.97
Round  27, Train loss: 1.523, Test loss: 1.532, Test accuracy: 93.27
Round  27, Global train loss: 1.523, Global test loss: 2.287, Global test accuracy: 13.07
Round  28, Train loss: 1.471, Test loss: 1.532, Test accuracy: 93.27
Round  28, Global train loss: 1.471, Global test loss: 2.283, Global test accuracy: 15.86
Round  29, Train loss: 1.468, Test loss: 1.532, Test accuracy: 93.25
Round  29, Global train loss: 1.468, Global test loss: 2.296, Global test accuracy: 12.01
Round  30, Train loss: 1.468, Test loss: 1.532, Test accuracy: 93.23
Round  30, Global train loss: 1.468, Global test loss: 2.317, Global test accuracy: 10.57
Round  31, Train loss: 1.495, Test loss: 1.520, Test accuracy: 94.57
Round  31, Global train loss: 1.495, Global test loss: 2.310, Global test accuracy: 10.53
Round  32, Train loss: 1.525, Test loss: 1.515, Test accuracy: 95.08
Round  32, Global train loss: 1.525, Global test loss: 2.284, Global test accuracy: 15.29
Round  33, Train loss: 1.488, Test loss: 1.506, Test accuracy: 95.90
Round  33, Global train loss: 1.488, Global test loss: 2.272, Global test accuracy: 15.88
Round  34, Train loss: 1.470, Test loss: 1.506, Test accuracy: 95.90
Round  34, Global train loss: 1.470, Global test loss: 2.272, Global test accuracy: 16.87
Round  35, Train loss: 1.473, Test loss: 1.504, Test accuracy: 96.02
Round  35, Global train loss: 1.473, Global test loss: 2.284, Global test accuracy: 13.62
Round  36, Train loss: 1.471, Test loss: 1.504, Test accuracy: 96.02
Round  36, Global train loss: 1.471, Global test loss: 2.268, Global test accuracy: 17.96
Round  37, Train loss: 1.473, Test loss: 1.504, Test accuracy: 96.04
Round  37, Global train loss: 1.473, Global test loss: 2.269, Global test accuracy: 16.80
Round  38, Train loss: 1.465, Test loss: 1.504, Test accuracy: 96.06
Round  38, Global train loss: 1.465, Global test loss: 2.300, Global test accuracy: 15.08
Round  39, Train loss: 1.470, Test loss: 1.504, Test accuracy: 96.05
Round  39, Global train loss: 1.470, Global test loss: 2.280, Global test accuracy: 16.42
Round  40, Train loss: 1.464, Test loss: 1.504, Test accuracy: 96.04
Round  40, Global train loss: 1.464, Global test loss: 2.267, Global test accuracy: 19.52
Round  41, Train loss: 1.470, Test loss: 1.504, Test accuracy: 95.96
Round  41, Global train loss: 1.470, Global test loss: 2.280, Global test accuracy: 15.67
Round  42, Train loss: 1.465, Test loss: 1.504, Test accuracy: 95.96
Round  42, Global train loss: 1.465, Global test loss: 2.275, Global test accuracy: 15.67
Round  43, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.06
Round  43, Global train loss: 1.467, Global test loss: 2.287, Global test accuracy: 15.24
Round  44, Train loss: 1.471, Test loss: 1.503, Test accuracy: 96.06
Round  44, Global train loss: 1.471, Global test loss: 2.262, Global test accuracy: 18.17
Round  45, Train loss: 1.468, Test loss: 1.503, Test accuracy: 96.06
Round  45, Global train loss: 1.468, Global test loss: 2.281, Global test accuracy: 17.59
Round  46, Train loss: 1.465, Test loss: 1.503, Test accuracy: 96.14
Round  46, Global train loss: 1.465, Global test loss: 2.259, Global test accuracy: 18.34
Round  47, Train loss: 1.471, Test loss: 1.503, Test accuracy: 96.09
Round  47, Global train loss: 1.471, Global test loss: 2.280, Global test accuracy: 15.28
Round  48, Train loss: 1.470, Test loss: 1.503, Test accuracy: 96.10
Round  48, Global train loss: 1.470, Global test loss: 2.279, Global test accuracy: 15.82
Round  49, Train loss: 1.464, Test loss: 1.503, Test accuracy: 95.99
Round  49, Global train loss: 1.464, Global test loss: 2.285, Global test accuracy: 16.30
Round  50, Train loss: 1.465, Test loss: 1.503, Test accuracy: 96.05
Round  50, Global train loss: 1.465, Global test loss: 2.279, Global test accuracy: 15.82
Round  51, Train loss: 1.470, Test loss: 1.503, Test accuracy: 96.04
Round  51, Global train loss: 1.470, Global test loss: 2.270, Global test accuracy: 18.44
Round  52, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.10
Round  52, Global train loss: 1.467, Global test loss: 2.278, Global test accuracy: 16.63
Round  53, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.08
Round  53, Global train loss: 1.466, Global test loss: 2.263, Global test accuracy: 17.13
Round  54, Train loss: 1.464, Test loss: 1.503, Test accuracy: 96.08
Round  54, Global train loss: 1.464, Global test loss: 2.292, Global test accuracy: 15.34
Round  55, Train loss: 1.463, Test loss: 1.503, Test accuracy: 96.08
Round  55, Global train loss: 1.463, Global test loss: 2.283, Global test accuracy: 13.07
Round  56, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.08
Round  56, Global train loss: 1.467, Global test loss: 2.287, Global test accuracy: 13.86
Round  57, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.08
Round  57, Global train loss: 1.468, Global test loss: 2.275, Global test accuracy: 15.68
Round  58, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.08
Round  58, Global train loss: 1.468, Global test loss: 2.315, Global test accuracy: 12.26
Round  59, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.09
Round  59, Global train loss: 1.467, Global test loss: 2.273, Global test accuracy: 17.29
Round  60, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.09
Round  60, Global train loss: 1.468, Global test loss: 2.286, Global test accuracy: 16.29
Round  61, Train loss: 1.469, Test loss: 1.502, Test accuracy: 96.09
Round  61, Global train loss: 1.469, Global test loss: 2.275, Global test accuracy: 16.44
Round  62, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.08
Round  62, Global train loss: 1.466, Global test loss: 2.290, Global test accuracy: 14.33
Round  63, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.08
Round  63, Global train loss: 1.467, Global test loss: 2.287, Global test accuracy: 13.76
Round  64, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.08
Round  64, Global train loss: 1.468, Global test loss: 2.283, Global test accuracy: 15.40
Round  65, Train loss: 1.465, Test loss: 1.502, Test accuracy: 96.06
Round  65, Global train loss: 1.465, Global test loss: 2.284, Global test accuracy: 15.85
Round  66, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.08
Round  66, Global train loss: 1.467, Global test loss: 2.330, Global test accuracy: 11.67
Round  67, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.08
Round  67, Global train loss: 1.466, Global test loss: 2.270, Global test accuracy: 17.52
Round  68, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.08
Round  68, Global train loss: 1.467, Global test loss: 2.272, Global test accuracy: 15.86
Round  69, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.08
Round  69, Global train loss: 1.468, Global test loss: 2.281, Global test accuracy: 15.43
Round  70, Train loss: 1.463, Test loss: 1.502, Test accuracy: 96.08
Round  70, Global train loss: 1.463, Global test loss: 2.275, Global test accuracy: 17.20
Round  71, Train loss: 1.465, Test loss: 1.502, Test accuracy: 96.08
Round  71, Global train loss: 1.465, Global test loss: 2.295, Global test accuracy: 13.95
Round  72, Train loss: 1.465, Test loss: 1.502, Test accuracy: 96.07
Round  72, Global train loss: 1.465, Global test loss: 2.278, Global test accuracy: 15.93
Round  73, Train loss: 1.464, Test loss: 1.502, Test accuracy: 96.07
Round  73, Global train loss: 1.464, Global test loss: 2.263, Global test accuracy: 19.20
Round  74, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.04
Round  74, Global train loss: 1.466, Global test loss: 2.269, Global test accuracy: 18.71
Round  75, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.06
Round  75, Global train loss: 1.467, Global test loss: 2.271, Global test accuracy: 16.27
Round  76, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.05
Round  76, Global train loss: 1.467, Global test loss: 2.289, Global test accuracy: 11.93
Round  77, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.06
Round  77, Global train loss: 1.468, Global test loss: 2.294, Global test accuracy: 10.11
Round  78, Train loss: 1.464, Test loss: 1.502, Test accuracy: 96.08
Round  78, Global train loss: 1.464, Global test loss: 2.276, Global test accuracy: 16.86
Round  79, Train loss: 1.469, Test loss: 1.502, Test accuracy: 96.06
Round  79, Global train loss: 1.469, Global test loss: 2.263, Global test accuracy: 18.18
Round  80, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.08
Round  80, Global train loss: 1.467, Global test loss: 2.286, Global test accuracy: 14.83
Round  81, Train loss: 1.465, Test loss: 1.502, Test accuracy: 96.08
Round  81, Global train loss: 1.465, Global test loss: 2.289, Global test accuracy: 14.55
Round  82, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.10
Round  82, Global train loss: 1.466, Global test loss: 2.291, Global test accuracy: 14.40
Round  83, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.11
Round  83, Global train loss: 1.466, Global test loss: 2.272, Global test accuracy: 16.55
Round  84, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.12
Round  84, Global train loss: 1.467, Global test loss: 2.261, Global test accuracy: 18.20
Round  85, Train loss: 1.464, Test loss: 1.502, Test accuracy: 96.10
Round  85, Global train loss: 1.464, Global test loss: 2.272, Global test accuracy: 15.93
Round  86, Train loss: 1.464, Test loss: 1.502, Test accuracy: 96.11
Round  86, Global train loss: 1.464, Global test loss: 2.284, Global test accuracy: 13.13
Round  87, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.10
Round  87, Global train loss: 1.466, Global test loss: 2.292, Global test accuracy: 10.87
Round  88, Train loss: 1.464, Test loss: 1.502, Test accuracy: 96.11
Round  88, Global train loss: 1.464, Global test loss: 2.253, Global test accuracy: 18.24
Round  89, Train loss: 1.465, Test loss: 1.502, Test accuracy: 96.10
Round  89, Global train loss: 1.465, Global test loss: 2.254, Global test accuracy: 18.31
Round  90, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.10
Round  90, Global train loss: 1.467, Global test loss: 2.285, Global test accuracy: 14.62
Round  91, Train loss: 1.465, Test loss: 1.502, Test accuracy: 96.10
Round  91, Global train loss: 1.465, Global test loss: 2.262, Global test accuracy: 18.35
Round  92, Train loss: 1.464, Test loss: 1.502, Test accuracy: 96.10
Round  92, Global train loss: 1.464, Global test loss: 2.289, Global test accuracy: 15.64
Round  93, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.09
Round  93, Global train loss: 1.467, Global test loss: 2.291, Global test accuracy: 13.29
Round  94, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.09
Round  94, Global train loss: 1.467, Global test loss: 2.266, Global test accuracy: 18.07
Round  95, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.09
Round  95, Global train loss: 1.466, Global test loss: 2.272, Global test accuracy: 16.54/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.465, Test loss: 1.502, Test accuracy: 96.09
Round  96, Global train loss: 1.465, Global test loss: 2.296, Global test accuracy: 13.32
Round  97, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.08
Round  97, Global train loss: 1.466, Global test loss: 2.262, Global test accuracy: 18.77
Round  98, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.09
Round  98, Global train loss: 1.466, Global test loss: 2.295, Global test accuracy: 14.18
Round  99, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.10
Round  99, Global train loss: 1.466, Global test loss: 2.290, Global test accuracy: 14.67
Final Round, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.12
Final Round, Global train loss: 1.466, Global test loss: 2.290, Global test accuracy: 14.67
Average accuracy final 10 rounds: 96.09416666666667 

Average global accuracy final 10 rounds: 15.744166666666668 

1656.5740733146667
[1.108271598815918, 2.216543197631836, 3.281813621520996, 4.347084045410156, 5.447920083999634, 6.548756122589111, 7.6115758419036865, 8.674395561218262, 9.73290205001831, 10.79140853881836, 11.855372905731201, 12.919337272644043, 13.975811243057251, 15.032285213470459, 16.130727767944336, 17.229170322418213, 18.298083543777466, 19.36699676513672, 20.473603010177612, 21.580209255218506, 22.647200107574463, 23.71419095993042, 24.810977458953857, 25.907763957977295, 26.9679172039032, 28.0280704498291, 29.12576913833618, 30.22346782684326, 31.308275938034058, 32.39308404922485, 33.46794247627258, 34.54280090332031, 35.58377385139465, 36.624746799468994, 37.69669723510742, 38.76864767074585, 39.86423420906067, 40.95982074737549, 42.01001739501953, 43.060214042663574, 44.158772706985474, 45.25733137130737, 46.30390954017639, 47.35048770904541, 48.4431529045105, 49.535818099975586, 50.61902046203613, 51.70222282409668, 52.781171560287476, 53.86012029647827, 54.95766520500183, 56.05521011352539, 57.099634408950806, 58.14405870437622, 59.21509337425232, 60.28612804412842, 61.36328411102295, 62.44044017791748, 63.532368421554565, 64.62429666519165, 65.71367335319519, 66.80305004119873, 67.88370180130005, 68.96435356140137, 70.02532911300659, 71.08630466461182, 72.17871332168579, 73.27112197875977, 74.37141418457031, 75.47170639038086, 76.57937908172607, 77.68705177307129, 78.75663781166077, 79.82622385025024, 80.86984610557556, 81.91346836090088, 82.99196577072144, 84.07046318054199, 85.14822912216187, 86.22599506378174, 87.27252268791199, 88.31905031204224, 89.39993834495544, 90.48082637786865, 91.56574273109436, 92.65065908432007, 93.70411777496338, 94.75757646560669, 95.85881900787354, 96.96006155014038, 98.03314852714539, 99.10623550415039, 100.18191409111023, 101.25759267807007, 102.35330009460449, 103.44900751113892, 104.50200366973877, 105.55499982833862, 106.63876676559448, 107.72253370285034, 108.8069531917572, 109.89137268066406, 110.96599340438843, 112.0406141281128, 113.1353611946106, 114.2301082611084, 115.29309225082397, 116.35607624053955, 117.39384007453918, 118.43160390853882, 119.50372266769409, 120.57584142684937, 121.63293218612671, 122.69002294540405, 123.73647117614746, 124.78291940689087, 125.80618023872375, 126.82944107055664, 127.78771948814392, 128.7459979057312, 129.7993450164795, 130.85269212722778, 131.94503903388977, 133.03738594055176, 134.0905637741089, 135.14374160766602, 136.21352362632751, 137.283305644989, 138.36202120780945, 139.44073677062988, 140.37436962127686, 141.30800247192383, 142.23324847221375, 143.15849447250366, 144.09325075149536, 145.02800703048706, 145.9566867351532, 146.88536643981934, 147.8230652809143, 148.76076412200928, 149.732816696167, 150.7048692703247, 151.63753938674927, 152.57020950317383, 153.50940585136414, 154.44860219955444, 155.38062930107117, 156.3126564025879, 157.24869966506958, 158.18474292755127, 159.10427618026733, 160.0238094329834, 160.98145127296448, 161.93909311294556, 162.87133073806763, 163.8035683631897, 164.75037384033203, 165.69717931747437, 166.66744184494019, 167.637704372406, 168.57590889930725, 169.5141134262085, 170.4574704170227, 171.4008274078369, 172.33256649971008, 173.26430559158325, 174.19789123535156, 175.13147687911987, 176.05980706214905, 176.98813724517822, 177.96182298660278, 178.93550872802734, 179.87050533294678, 180.8055019378662, 181.77527236938477, 182.74504280090332, 183.68177032470703, 184.61849784851074, 185.50675296783447, 186.3950080871582, 187.31696248054504, 188.23891687393188, 189.10114312171936, 189.96336936950684, 190.90220761299133, 191.84104585647583, 192.76985836029053, 193.69867086410522, 194.66590857505798, 195.63314628601074, 196.55240082740784, 197.47165536880493, 198.43148040771484, 199.39130544662476, 200.30383396148682, 201.21636247634888, 202.15517234802246, 203.09398221969604, 204.072114944458, 205.05024766921997, 206.63457441329956, 208.21890115737915]
[30.733333333333334, 30.733333333333334, 46.19166666666667, 46.19166666666667, 59.666666666666664, 59.666666666666664, 65.46666666666667, 65.46666666666667, 74.95833333333333, 74.95833333333333, 80.7, 80.7, 83.46666666666667, 83.46666666666667, 83.75833333333334, 83.75833333333334, 85.63333333333334, 85.63333333333334, 86.8, 86.8, 86.91666666666667, 86.91666666666667, 88.43333333333334, 88.43333333333334, 88.5, 88.5, 88.54166666666667, 88.54166666666667, 88.50833333333334, 88.50833333333334, 88.54166666666667, 88.54166666666667, 88.50833333333334, 88.50833333333334, 89.99166666666666, 89.99166666666666, 90.00833333333334, 90.00833333333334, 91.64166666666667, 91.64166666666667, 91.64166666666667, 91.64166666666667, 91.63333333333334, 91.63333333333334, 93.23333333333333, 93.23333333333333, 93.18333333333334, 93.18333333333334, 93.20833333333333, 93.20833333333333, 93.225, 93.225, 93.24166666666666, 93.24166666666666, 93.26666666666667, 93.26666666666667, 93.26666666666667, 93.26666666666667, 93.25, 93.25, 93.23333333333333, 93.23333333333333, 94.56666666666666, 94.56666666666666, 95.08333333333333, 95.08333333333333, 95.9, 95.9, 95.9, 95.9, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.04166666666667, 96.04166666666667, 96.05833333333334, 96.05833333333334, 96.05, 96.05, 96.04166666666667, 96.04166666666667, 95.95833333333333, 95.95833333333333, 95.95833333333333, 95.95833333333333, 96.05833333333334, 96.05833333333334, 96.05833333333334, 96.05833333333334, 96.05833333333334, 96.05833333333334, 96.14166666666667, 96.14166666666667, 96.09166666666667, 96.09166666666667, 96.1, 96.1, 95.99166666666666, 95.99166666666666, 96.05, 96.05, 96.04166666666667, 96.04166666666667, 96.1, 96.1, 96.075, 96.075, 96.08333333333333, 96.08333333333333, 96.08333333333333, 96.08333333333333, 96.08333333333333, 96.08333333333333, 96.075, 96.075, 96.08333333333333, 96.08333333333333, 96.09166666666667, 96.09166666666667, 96.09166666666667, 96.09166666666667, 96.09166666666667, 96.09166666666667, 96.075, 96.075, 96.075, 96.075, 96.075, 96.075, 96.05833333333334, 96.05833333333334, 96.08333333333333, 96.08333333333333, 96.08333333333333, 96.08333333333333, 96.075, 96.075, 96.075, 96.075, 96.08333333333333, 96.08333333333333, 96.075, 96.075, 96.06666666666666, 96.06666666666666, 96.06666666666666, 96.06666666666666, 96.04166666666667, 96.04166666666667, 96.05833333333334, 96.05833333333334, 96.05, 96.05, 96.05833333333334, 96.05833333333334, 96.075, 96.075, 96.05833333333334, 96.05833333333334, 96.075, 96.075, 96.08333333333333, 96.08333333333333, 96.1, 96.1, 96.10833333333333, 96.10833333333333, 96.11666666666666, 96.11666666666666, 96.1, 96.1, 96.10833333333333, 96.10833333333333, 96.1, 96.1, 96.10833333333333, 96.10833333333333, 96.1, 96.1, 96.1, 96.1, 96.1, 96.1, 96.1, 96.1, 96.09166666666667, 96.09166666666667, 96.09166666666667, 96.09166666666667, 96.09166666666667, 96.09166666666667, 96.09166666666667, 96.09166666666667, 96.08333333333333, 96.08333333333333, 96.09166666666667, 96.09166666666667, 96.1, 96.1, 96.11666666666666, 96.11666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.145, Test loss: 2.155, Test accuracy: 30.32
Round   0, Global train loss: 2.145, Global test loss: 2.281, Global test accuracy: 18.98
Round   1, Train loss: 1.791, Test loss: 2.005, Test accuracy: 44.01
Round   1, Global train loss: 1.791, Global test loss: 2.266, Global test accuracy: 17.92
Round   2, Train loss: 1.691, Test loss: 1.992, Test accuracy: 45.26
Round   2, Global train loss: 1.691, Global test loss: 2.299, Global test accuracy: 13.12
Round   3, Train loss: 1.885, Test loss: 1.888, Test accuracy: 57.40
Round   3, Global train loss: 1.885, Global test loss: 2.243, Global test accuracy: 20.45
Round   4, Train loss: 1.766, Test loss: 1.839, Test accuracy: 62.39
Round   4, Global train loss: 1.766, Global test loss: 2.255, Global test accuracy: 18.15
Round   5, Train loss: 1.815, Test loss: 1.800, Test accuracy: 66.53
Round   5, Global train loss: 1.815, Global test loss: 2.246, Global test accuracy: 19.53
Round   6, Train loss: 1.719, Test loss: 1.801, Test accuracy: 66.50
Round   6, Global train loss: 1.719, Global test loss: 2.276, Global test accuracy: 16.28
Round   7, Train loss: 1.842, Test loss: 1.741, Test accuracy: 72.36
Round   7, Global train loss: 1.842, Global test loss: 2.218, Global test accuracy: 22.66
Round   8, Train loss: 1.734, Test loss: 1.728, Test accuracy: 73.86
Round   8, Global train loss: 1.734, Global test loss: 2.236, Global test accuracy: 20.81
Round   9, Train loss: 1.740, Test loss: 1.755, Test accuracy: 70.79
Round   9, Global train loss: 1.740, Global test loss: 2.234, Global test accuracy: 21.02
Round  10, Train loss: 1.813, Test loss: 1.770, Test accuracy: 69.15
Round  10, Global train loss: 1.813, Global test loss: 2.237, Global test accuracy: 20.67
Round  11, Train loss: 1.891, Test loss: 1.765, Test accuracy: 69.45
Round  11, Global train loss: 1.891, Global test loss: 2.235, Global test accuracy: 20.00
Round  12, Train loss: 1.865, Test loss: 1.736, Test accuracy: 72.58
Round  12, Global train loss: 1.865, Global test loss: 2.235, Global test accuracy: 21.07
Round  13, Train loss: 1.875, Test loss: 1.752, Test accuracy: 70.82
Round  13, Global train loss: 1.875, Global test loss: 2.229, Global test accuracy: 21.60
Round  14, Train loss: 1.722, Test loss: 1.751, Test accuracy: 70.95
Round  14, Global train loss: 1.722, Global test loss: 2.263, Global test accuracy: 17.57
Round  15, Train loss: 1.781, Test loss: 1.753, Test accuracy: 70.76
Round  15, Global train loss: 1.781, Global test loss: 2.227, Global test accuracy: 21.73
Round  16, Train loss: 1.863, Test loss: 1.753, Test accuracy: 70.81
Round  16, Global train loss: 1.863, Global test loss: 2.246, Global test accuracy: 18.84
Round  17, Train loss: 1.693, Test loss: 1.751, Test accuracy: 70.83
Round  17, Global train loss: 1.693, Global test loss: 2.220, Global test accuracy: 21.77
Round  18, Train loss: 1.780, Test loss: 1.721, Test accuracy: 74.03
Round  18, Global train loss: 1.780, Global test loss: 2.222, Global test accuracy: 21.98
Round  19, Train loss: 1.713, Test loss: 1.720, Test accuracy: 74.01
Round  19, Global train loss: 1.713, Global test loss: 2.228, Global test accuracy: 21.56
Round  20, Train loss: 1.769, Test loss: 1.720, Test accuracy: 73.97
Round  20, Global train loss: 1.769, Global test loss: 2.235, Global test accuracy: 21.09
Round  21, Train loss: 1.615, Test loss: 1.719, Test accuracy: 74.02
Round  21, Global train loss: 1.615, Global test loss: 2.208, Global test accuracy: 23.85
Round  22, Train loss: 1.774, Test loss: 1.703, Test accuracy: 75.76
Round  22, Global train loss: 1.774, Global test loss: 2.230, Global test accuracy: 20.92
Round  23, Train loss: 1.667, Test loss: 1.703, Test accuracy: 75.83
Round  23, Global train loss: 1.667, Global test loss: 2.216, Global test accuracy: 22.82
Round  24, Train loss: 1.764, Test loss: 1.690, Test accuracy: 77.12
Round  24, Global train loss: 1.764, Global test loss: 2.225, Global test accuracy: 21.94
Round  25, Train loss: 1.836, Test loss: 1.689, Test accuracy: 77.15
Round  25, Global train loss: 1.836, Global test loss: 2.244, Global test accuracy: 19.29
Round  26, Train loss: 1.712, Test loss: 1.689, Test accuracy: 77.16
Round  26, Global train loss: 1.712, Global test loss: 2.206, Global test accuracy: 23.58
Round  27, Train loss: 1.782, Test loss: 1.687, Test accuracy: 77.40
Round  27, Global train loss: 1.782, Global test loss: 2.205, Global test accuracy: 24.40
Round  28, Train loss: 1.624, Test loss: 1.687, Test accuracy: 77.42
Round  28, Global train loss: 1.624, Global test loss: 2.204, Global test accuracy: 24.57
Round  29, Train loss: 1.704, Test loss: 1.688, Test accuracy: 77.28
Round  29, Global train loss: 1.704, Global test loss: 2.198, Global test accuracy: 25.10
Round  30, Train loss: 1.654, Test loss: 1.687, Test accuracy: 77.27
Round  30, Global train loss: 1.654, Global test loss: 2.192, Global test accuracy: 26.00
Round  31, Train loss: 1.678, Test loss: 1.675, Test accuracy: 78.54
Round  31, Global train loss: 1.678, Global test loss: 2.206, Global test accuracy: 23.81
Round  32, Train loss: 1.655, Test loss: 1.658, Test accuracy: 80.26
Round  32, Global train loss: 1.655, Global test loss: 2.233, Global test accuracy: 20.68
Round  33, Train loss: 1.735, Test loss: 1.658, Test accuracy: 80.31
Round  33, Global train loss: 1.735, Global test loss: 2.232, Global test accuracy: 20.61
Round  34, Train loss: 1.813, Test loss: 1.658, Test accuracy: 80.32
Round  34, Global train loss: 1.813, Global test loss: 2.231, Global test accuracy: 21.61
Round  35, Train loss: 1.708, Test loss: 1.658, Test accuracy: 80.32
Round  35, Global train loss: 1.708, Global test loss: 2.264, Global test accuracy: 17.38
Round  36, Train loss: 1.688, Test loss: 1.658, Test accuracy: 80.24
Round  36, Global train loss: 1.688, Global test loss: 2.212, Global test accuracy: 22.73
Round  37, Train loss: 1.590, Test loss: 1.659, Test accuracy: 80.19
Round  37, Global train loss: 1.590, Global test loss: 2.221, Global test accuracy: 22.34
Round  38, Train loss: 1.602, Test loss: 1.642, Test accuracy: 81.92
Round  38, Global train loss: 1.602, Global test loss: 2.228, Global test accuracy: 21.53
Round  39, Train loss: 1.770, Test loss: 1.613, Test accuracy: 84.96
Round  39, Global train loss: 1.770, Global test loss: 2.243, Global test accuracy: 18.38
Round  40, Train loss: 1.685, Test loss: 1.612, Test accuracy: 85.07
Round  40, Global train loss: 1.685, Global test loss: 2.220, Global test accuracy: 22.38
Round  41, Train loss: 1.678, Test loss: 1.612, Test accuracy: 84.97
Round  41, Global train loss: 1.678, Global test loss: 2.224, Global test accuracy: 21.40
Round  42, Train loss: 1.556, Test loss: 1.612, Test accuracy: 85.02
Round  42, Global train loss: 1.556, Global test loss: 2.214, Global test accuracy: 22.72
Round  43, Train loss: 1.556, Test loss: 1.611, Test accuracy: 85.04
Round  43, Global train loss: 1.556, Global test loss: 2.212, Global test accuracy: 23.87
Round  44, Train loss: 1.618, Test loss: 1.619, Test accuracy: 84.37
Round  44, Global train loss: 1.618, Global test loss: 2.219, Global test accuracy: 22.80
Round  45, Train loss: 1.619, Test loss: 1.618, Test accuracy: 84.38
Round  45, Global train loss: 1.619, Global test loss: 2.226, Global test accuracy: 21.62
Round  46, Train loss: 1.567, Test loss: 1.595, Test accuracy: 86.70
Round  46, Global train loss: 1.567, Global test loss: 2.236, Global test accuracy: 20.50
Round  47, Train loss: 1.667, Test loss: 1.596, Test accuracy: 86.59
Round  47, Global train loss: 1.667, Global test loss: 2.211, Global test accuracy: 23.58
Round  48, Train loss: 1.614, Test loss: 1.596, Test accuracy: 86.58
Round  48, Global train loss: 1.614, Global test loss: 2.218, Global test accuracy: 22.59
Round  49, Train loss: 1.693, Test loss: 1.596, Test accuracy: 86.54
Round  49, Global train loss: 1.693, Global test loss: 2.211, Global test accuracy: 23.27
Round  50, Train loss: 1.689, Test loss: 1.597, Test accuracy: 86.56
Round  50, Global train loss: 1.689, Global test loss: 2.218, Global test accuracy: 21.73
Round  51, Train loss: 1.601, Test loss: 1.595, Test accuracy: 86.74
Round  51, Global train loss: 1.601, Global test loss: 2.248, Global test accuracy: 19.29
Round  52, Train loss: 1.597, Test loss: 1.610, Test accuracy: 85.12
Round  52, Global train loss: 1.597, Global test loss: 2.223, Global test accuracy: 21.82
Round  53, Train loss: 1.572, Test loss: 1.610, Test accuracy: 85.12
Round  53, Global train loss: 1.572, Global test loss: 2.202, Global test accuracy: 24.31
Round  54, Train loss: 1.649, Test loss: 1.625, Test accuracy: 83.55
Round  54, Global train loss: 1.649, Global test loss: 2.195, Global test accuracy: 25.10
Round  55, Train loss: 1.625, Test loss: 1.609, Test accuracy: 85.23
Round  55, Global train loss: 1.625, Global test loss: 2.215, Global test accuracy: 23.93
Round  56, Train loss: 1.619, Test loss: 1.609, Test accuracy: 85.25
Round  56, Global train loss: 1.619, Global test loss: 2.202, Global test accuracy: 24.42
Round  57, Train loss: 1.702, Test loss: 1.594, Test accuracy: 86.88
Round  57, Global train loss: 1.702, Global test loss: 2.215, Global test accuracy: 23.10
Round  58, Train loss: 1.575, Test loss: 1.594, Test accuracy: 86.93
Round  58, Global train loss: 1.575, Global test loss: 2.198, Global test accuracy: 25.29
Round  59, Train loss: 1.488, Test loss: 1.593, Test accuracy: 86.93
Round  59, Global train loss: 1.488, Global test loss: 2.218, Global test accuracy: 22.73
Round  60, Train loss: 1.502, Test loss: 1.592, Test accuracy: 86.97
Round  60, Global train loss: 1.502, Global test loss: 2.211, Global test accuracy: 23.84
Round  61, Train loss: 1.603, Test loss: 1.592, Test accuracy: 86.92
Round  61, Global train loss: 1.603, Global test loss: 2.228, Global test accuracy: 21.10
Round  62, Train loss: 1.524, Test loss: 1.592, Test accuracy: 86.88
Round  62, Global train loss: 1.524, Global test loss: 2.200, Global test accuracy: 25.35
Round  63, Train loss: 1.721, Test loss: 1.593, Test accuracy: 86.80
Round  63, Global train loss: 1.721, Global test loss: 2.199, Global test accuracy: 24.62
Round  64, Train loss: 1.596, Test loss: 1.593, Test accuracy: 86.85
Round  64, Global train loss: 1.596, Global test loss: 2.228, Global test accuracy: 21.44
Round  65, Train loss: 1.567, Test loss: 1.593, Test accuracy: 86.87
Round  65, Global train loss: 1.567, Global test loss: 2.199, Global test accuracy: 24.63
Round  66, Train loss: 1.590, Test loss: 1.593, Test accuracy: 86.85
Round  66, Global train loss: 1.590, Global test loss: 2.218, Global test accuracy: 22.38
Round  67, Train loss: 1.556, Test loss: 1.595, Test accuracy: 86.71
Round  67, Global train loss: 1.556, Global test loss: 2.205, Global test accuracy: 24.05
Round  68, Train loss: 1.593, Test loss: 1.595, Test accuracy: 86.68
Round  68, Global train loss: 1.593, Global test loss: 2.236, Global test accuracy: 20.72
Round  69, Train loss: 1.720, Test loss: 1.595, Test accuracy: 86.63
Round  69, Global train loss: 1.720, Global test loss: 2.216, Global test accuracy: 22.57
Round  70, Train loss: 1.609, Test loss: 1.595, Test accuracy: 86.62
Round  70, Global train loss: 1.609, Global test loss: 2.228, Global test accuracy: 21.49
Round  71, Train loss: 1.544, Test loss: 1.595, Test accuracy: 86.64
Round  71, Global train loss: 1.544, Global test loss: 2.213, Global test accuracy: 23.74
Round  72, Train loss: 1.610, Test loss: 1.595, Test accuracy: 86.62
Round  72, Global train loss: 1.610, Global test loss: 2.236, Global test accuracy: 19.98
Round  73, Train loss: 1.578, Test loss: 1.595, Test accuracy: 86.62
Round  73, Global train loss: 1.578, Global test loss: 2.235, Global test accuracy: 21.23
Round  74, Train loss: 1.597, Test loss: 1.607, Test accuracy: 85.23
Round  74, Global train loss: 1.597, Global test loss: 2.217, Global test accuracy: 23.08
Round  75, Train loss: 1.622, Test loss: 1.592, Test accuracy: 86.94
Round  75, Global train loss: 1.622, Global test loss: 2.212, Global test accuracy: 22.98
Round  76, Train loss: 1.595, Test loss: 1.593, Test accuracy: 86.90
Round  76, Global train loss: 1.595, Global test loss: 2.219, Global test accuracy: 22.30
Round  77, Train loss: 1.593, Test loss: 1.593, Test accuracy: 86.87
Round  77, Global train loss: 1.593, Global test loss: 2.205, Global test accuracy: 23.95
Round  78, Train loss: 1.663, Test loss: 1.593, Test accuracy: 86.88
Round  78, Global train loss: 1.663, Global test loss: 2.207, Global test accuracy: 24.30
Round  79, Train loss: 1.585, Test loss: 1.593, Test accuracy: 86.83
Round  79, Global train loss: 1.585, Global test loss: 2.226, Global test accuracy: 22.48
Round  80, Train loss: 1.651, Test loss: 1.593, Test accuracy: 86.87
Round  80, Global train loss: 1.651, Global test loss: 2.239, Global test accuracy: 20.26
Round  81, Train loss: 1.585, Test loss: 1.593, Test accuracy: 86.88
Round  81, Global train loss: 1.585, Global test loss: 2.226, Global test accuracy: 22.46
Round  82, Train loss: 1.627, Test loss: 1.592, Test accuracy: 86.92
Round  82, Global train loss: 1.627, Global test loss: 2.197, Global test accuracy: 25.24
Round  83, Train loss: 1.649, Test loss: 1.593, Test accuracy: 86.85
Round  83, Global train loss: 1.649, Global test loss: 2.202, Global test accuracy: 24.84
Round  84, Train loss: 1.594, Test loss: 1.592, Test accuracy: 86.92
Round  84, Global train loss: 1.594, Global test loss: 2.226, Global test accuracy: 21.17
Round  85, Train loss: 1.501, Test loss: 1.591, Test accuracy: 87.03
Round  85, Global train loss: 1.501, Global test loss: 2.207, Global test accuracy: 24.21
Round  86, Train loss: 1.537, Test loss: 1.591, Test accuracy: 87.09
Round  86, Global train loss: 1.537, Global test loss: 2.229, Global test accuracy: 20.92
Round  87, Train loss: 1.656, Test loss: 1.591, Test accuracy: 87.04
Round  87, Global train loss: 1.656, Global test loss: 2.192, Global test accuracy: 25.71
Round  88, Train loss: 1.539, Test loss: 1.591, Test accuracy: 86.97
Round  88, Global train loss: 1.539, Global test loss: 2.229, Global test accuracy: 20.81
Round  89, Train loss: 1.601, Test loss: 1.592, Test accuracy: 86.90
Round  89, Global train loss: 1.601, Global test loss: 2.214, Global test accuracy: 23.03
Round  90, Train loss: 1.581, Test loss: 1.591, Test accuracy: 86.95
Round  90, Global train loss: 1.581, Global test loss: 2.203, Global test accuracy: 24.54
Round  91, Train loss: 1.604, Test loss: 1.591, Test accuracy: 86.99
Round  91, Global train loss: 1.604, Global test loss: 2.211, Global test accuracy: 23.87
Round  92, Train loss: 1.612, Test loss: 1.590, Test accuracy: 87.10
Round  92, Global train loss: 1.612, Global test loss: 2.207, Global test accuracy: 23.01
Round  93, Train loss: 1.532, Test loss: 1.590, Test accuracy: 87.12
Round  93, Global train loss: 1.532, Global test loss: 2.203, Global test accuracy: 24.78
Round  94, Train loss: 1.673, Test loss: 1.591, Test accuracy: 87.08
Round  94, Global train loss: 1.673, Global test loss: 2.194, Global test accuracy: 25.05
Round  95, Train loss: 1.596, Test loss: 1.590, Test accuracy: 87.17
Round  95, Global train loss: 1.596, Global test loss: 2.229, Global test accuracy: 21.44/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.508, Test loss: 1.591, Test accuracy: 87.06
Round  96, Global train loss: 1.508, Global test loss: 2.207, Global test accuracy: 24.38
Round  97, Train loss: 1.585, Test loss: 1.592, Test accuracy: 86.97
Round  97, Global train loss: 1.585, Global test loss: 2.203, Global test accuracy: 24.83
Round  98, Train loss: 1.542, Test loss: 1.592, Test accuracy: 86.94
Round  98, Global train loss: 1.542, Global test loss: 2.211, Global test accuracy: 22.73
Round  99, Train loss: 1.655, Test loss: 1.592, Test accuracy: 86.98
Round  99, Global train loss: 1.655, Global test loss: 2.206, Global test accuracy: 23.98
Final Round, Train loss: 1.579, Test loss: 1.589, Test accuracy: 87.13
Final Round, Global train loss: 1.579, Global test loss: 2.206, Global test accuracy: 23.98
Average accuracy final 10 rounds: 87.0375 

Average global accuracy final 10 rounds: 23.861666666666668 

1642.3518421649933
[1.1266567707061768, 2.2533135414123535, 3.3152506351470947, 4.377187728881836, 5.478592157363892, 6.579996585845947, 7.659963130950928, 8.739929676055908, 9.801960706710815, 10.863991737365723, 11.95275092124939, 13.041510105133057, 14.111669301986694, 15.181828498840332, 16.270224571228027, 17.358620643615723, 18.437516689300537, 19.51641273498535, 20.598660469055176, 21.680908203125, 22.76315689086914, 23.84540557861328, 24.918378353118896, 25.99135112762451, 27.062025785446167, 28.132700443267822, 29.216565370559692, 30.300430297851562, 31.392964601516724, 32.485498905181885, 33.54625940322876, 34.607019901275635, 35.71135354042053, 36.81568717956543, 37.90536069869995, 38.99503421783447, 40.10441589355469, 41.2137975692749, 42.3387405872345, 43.46368360519409, 44.53085517883301, 45.598026752471924, 46.72192668914795, 47.845826625823975, 48.9387640953064, 50.03170156478882, 51.121432304382324, 52.21116304397583, 53.288658618927, 54.366154193878174, 55.41080331802368, 56.45545244216919, 57.466320276260376, 58.47718811035156, 59.46328043937683, 60.4493727684021, 61.43505930900574, 62.420745849609375, 63.39065194129944, 64.3605580329895, 65.31261801719666, 66.26467800140381, 67.23678350448608, 68.20888900756836, 69.13188004493713, 70.05487108230591, 71.0282609462738, 72.0016508102417, 72.93989086151123, 73.87813091278076, 74.83093500137329, 75.78373908996582, 76.72816920280457, 77.67259931564331, 78.6440749168396, 79.61555051803589, 80.5841794013977, 81.55280828475952, 82.51890659332275, 83.48500490188599, 84.38687229156494, 85.2887396812439, 86.1802442073822, 87.07174873352051, 88.01076531410217, 88.94978189468384, 89.88989186286926, 90.83000183105469, 91.79790377616882, 92.76580572128296, 93.69843196868896, 94.63105821609497, 95.50930595397949, 96.38755369186401, 97.32171607017517, 98.25587844848633, 99.196462392807, 100.13704633712769, 101.11558198928833, 102.09411764144897, 103.1243405342102, 104.15456342697144, 105.20325016975403, 106.25193691253662, 107.30927920341492, 108.36662149429321, 109.44172167778015, 110.51682186126709, 111.57331037521362, 112.62979888916016, 113.66985416412354, 114.70990943908691, 115.80191898345947, 116.89392852783203, 117.98641228675842, 119.07889604568481, 120.13682293891907, 121.19474983215332, 122.29061198234558, 123.38647413253784, 124.46909642219543, 125.55171871185303, 126.63601303100586, 127.72030735015869, 128.81374549865723, 129.90718364715576, 130.97348976135254, 132.03979587554932, 133.12575960159302, 134.21172332763672, 135.30579495429993, 136.39986658096313, 137.47971057891846, 138.55955457687378, 139.64905190467834, 140.7385492324829, 141.81097507476807, 142.88340091705322, 143.954491853714, 145.02558279037476, 146.11824703216553, 147.2109112739563, 148.27299094200134, 149.3350706100464, 150.3827464580536, 151.4304223060608, 152.5031032562256, 153.57578420639038, 154.64690041542053, 155.71801662445068, 156.76610779762268, 157.81419897079468, 158.8728895187378, 159.9315800666809, 161.00215792655945, 162.072735786438, 163.1123824119568, 164.1520290374756, 165.20285177230835, 166.2536745071411, 167.3174831867218, 168.3812918663025, 169.43562150001526, 170.48995113372803, 171.55328702926636, 172.6166229248047, 173.6614489555359, 174.7062749862671, 175.64154696464539, 176.57681894302368, 177.58822202682495, 178.59962511062622, 179.62022137641907, 180.6408176422119, 181.69108700752258, 182.74135637283325, 183.80273938179016, 184.86412239074707, 185.77346467971802, 186.68280696868896, 187.60608339309692, 188.52935981750488, 189.4768168926239, 190.42427396774292, 191.32937097549438, 192.23446798324585, 193.15392065048218, 194.0733733177185, 194.98381567001343, 195.89425802230835, 196.7958595752716, 197.69746112823486, 198.61864185333252, 199.53982257843018, 200.43770742416382, 201.33559226989746, 202.25041818618774, 203.16524410247803, 204.05221152305603, 204.93917894363403, 206.46798849105835, 207.99679803848267]
[30.316666666666666, 30.316666666666666, 44.00833333333333, 44.00833333333333, 45.25833333333333, 45.25833333333333, 57.4, 57.4, 62.391666666666666, 62.391666666666666, 66.525, 66.525, 66.5, 66.5, 72.35833333333333, 72.35833333333333, 73.85833333333333, 73.85833333333333, 70.79166666666667, 70.79166666666667, 69.15, 69.15, 69.45, 69.45, 72.58333333333333, 72.58333333333333, 70.81666666666666, 70.81666666666666, 70.95, 70.95, 70.75833333333334, 70.75833333333334, 70.80833333333334, 70.80833333333334, 70.83333333333333, 70.83333333333333, 74.025, 74.025, 74.00833333333334, 74.00833333333334, 73.975, 73.975, 74.01666666666667, 74.01666666666667, 75.75833333333334, 75.75833333333334, 75.83333333333333, 75.83333333333333, 77.11666666666666, 77.11666666666666, 77.15, 77.15, 77.15833333333333, 77.15833333333333, 77.4, 77.4, 77.425, 77.425, 77.28333333333333, 77.28333333333333, 77.26666666666667, 77.26666666666667, 78.54166666666667, 78.54166666666667, 80.25833333333334, 80.25833333333334, 80.30833333333334, 80.30833333333334, 80.31666666666666, 80.31666666666666, 80.31666666666666, 80.31666666666666, 80.24166666666666, 80.24166666666666, 80.19166666666666, 80.19166666666666, 81.925, 81.925, 84.95833333333333, 84.95833333333333, 85.06666666666666, 85.06666666666666, 84.96666666666667, 84.96666666666667, 85.01666666666667, 85.01666666666667, 85.04166666666667, 85.04166666666667, 84.36666666666666, 84.36666666666666, 84.38333333333334, 84.38333333333334, 86.7, 86.7, 86.59166666666667, 86.59166666666667, 86.58333333333333, 86.58333333333333, 86.54166666666667, 86.54166666666667, 86.55833333333334, 86.55833333333334, 86.74166666666666, 86.74166666666666, 85.11666666666666, 85.11666666666666, 85.125, 85.125, 83.55, 83.55, 85.23333333333333, 85.23333333333333, 85.25, 85.25, 86.88333333333334, 86.88333333333334, 86.93333333333334, 86.93333333333334, 86.93333333333334, 86.93333333333334, 86.96666666666667, 86.96666666666667, 86.91666666666667, 86.91666666666667, 86.875, 86.875, 86.8, 86.8, 86.85, 86.85, 86.86666666666666, 86.86666666666666, 86.85, 86.85, 86.70833333333333, 86.70833333333333, 86.68333333333334, 86.68333333333334, 86.63333333333334, 86.63333333333334, 86.625, 86.625, 86.64166666666667, 86.64166666666667, 86.625, 86.625, 86.61666666666666, 86.61666666666666, 85.23333333333333, 85.23333333333333, 86.94166666666666, 86.94166666666666, 86.9, 86.9, 86.86666666666666, 86.86666666666666, 86.88333333333334, 86.88333333333334, 86.83333333333333, 86.83333333333333, 86.86666666666666, 86.86666666666666, 86.88333333333334, 86.88333333333334, 86.91666666666667, 86.91666666666667, 86.85, 86.85, 86.925, 86.925, 87.025, 87.025, 87.09166666666667, 87.09166666666667, 87.04166666666667, 87.04166666666667, 86.96666666666667, 86.96666666666667, 86.9, 86.9, 86.95, 86.95, 86.99166666666666, 86.99166666666666, 87.1, 87.1, 87.125, 87.125, 87.08333333333333, 87.08333333333333, 87.175, 87.175, 87.05833333333334, 87.05833333333334, 86.96666666666667, 86.96666666666667, 86.94166666666666, 86.94166666666666, 86.98333333333333, 86.98333333333333, 87.13333333333334, 87.13333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.23
Round   1, Train loss: 2.300, Test loss: 2.302, Test accuracy: 10.88
Round   2, Train loss: 2.298, Test loss: 2.301, Test accuracy: 11.54
Round   3, Train loss: 2.290, Test loss: 2.298, Test accuracy: 11.99
Round   4, Train loss: 2.262, Test loss: 2.287, Test accuracy: 14.60
Round   5, Train loss: 2.208, Test loss: 2.253, Test accuracy: 17.97
Round   6, Train loss: 2.091, Test loss: 2.202, Test accuracy: 24.26
Round   7, Train loss: 1.992, Test loss: 2.131, Test accuracy: 31.99
Round   8, Train loss: 2.006, Test loss: 2.108, Test accuracy: 34.25
Round   9, Train loss: 2.006, Test loss: 2.071, Test accuracy: 38.63
Round  10, Train loss: 1.938, Test loss: 2.032, Test accuracy: 43.04
Round  11, Train loss: 1.944, Test loss: 2.012, Test accuracy: 45.17
Round  12, Train loss: 1.945, Test loss: 1.997, Test accuracy: 46.58
Round  13, Train loss: 1.963, Test loss: 1.991, Test accuracy: 47.10
Round  14, Train loss: 2.006, Test loss: 1.989, Test accuracy: 47.10
Round  15, Train loss: 1.959, Test loss: 1.986, Test accuracy: 47.42
Round  16, Train loss: 1.903, Test loss: 1.981, Test accuracy: 47.85
Round  17, Train loss: 1.941, Test loss: 1.973, Test accuracy: 48.63
Round  18, Train loss: 1.921, Test loss: 1.965, Test accuracy: 49.44
Round  19, Train loss: 1.947, Test loss: 1.958, Test accuracy: 50.18
Round  20, Train loss: 1.939, Test loss: 1.957, Test accuracy: 50.29
Round  21, Train loss: 1.932, Test loss: 1.953, Test accuracy: 50.69
Round  22, Train loss: 1.933, Test loss: 1.947, Test accuracy: 51.37
Round  23, Train loss: 1.956, Test loss: 1.945, Test accuracy: 51.60
Round  24, Train loss: 1.902, Test loss: 1.945, Test accuracy: 51.50
Round  25, Train loss: 1.928, Test loss: 1.939, Test accuracy: 52.17
Round  26, Train loss: 1.890, Test loss: 1.937, Test accuracy: 52.26
Round  27, Train loss: 1.967, Test loss: 1.936, Test accuracy: 52.32
Round  28, Train loss: 1.938, Test loss: 1.935, Test accuracy: 52.49
Round  29, Train loss: 1.880, Test loss: 1.927, Test accuracy: 53.24
Round  30, Train loss: 1.925, Test loss: 1.927, Test accuracy: 53.26
Round  31, Train loss: 1.946, Test loss: 1.927, Test accuracy: 53.24
Round  32, Train loss: 1.910, Test loss: 1.923, Test accuracy: 53.74
Round  33, Train loss: 1.941, Test loss: 1.921, Test accuracy: 53.89
Round  34, Train loss: 1.872, Test loss: 1.917, Test accuracy: 54.37
Round  35, Train loss: 1.882, Test loss: 1.913, Test accuracy: 54.65
Round  36, Train loss: 1.889, Test loss: 1.912, Test accuracy: 54.71
Round  37, Train loss: 1.880, Test loss: 1.912, Test accuracy: 54.66
Round  38, Train loss: 1.840, Test loss: 1.904, Test accuracy: 55.51
Round  39, Train loss: 1.827, Test loss: 1.904, Test accuracy: 55.61
Round  40, Train loss: 1.919, Test loss: 1.903, Test accuracy: 55.69
Round  41, Train loss: 1.930, Test loss: 1.900, Test accuracy: 55.86
Round  42, Train loss: 1.860, Test loss: 1.896, Test accuracy: 56.40
Round  43, Train loss: 1.892, Test loss: 1.895, Test accuracy: 56.43
Round  44, Train loss: 1.899, Test loss: 1.894, Test accuracy: 56.54
Round  45, Train loss: 1.842, Test loss: 1.894, Test accuracy: 56.56
Round  46, Train loss: 1.903, Test loss: 1.890, Test accuracy: 56.95
Round  47, Train loss: 1.853, Test loss: 1.889, Test accuracy: 57.00
Round  48, Train loss: 1.865, Test loss: 1.886, Test accuracy: 57.34
Round  49, Train loss: 1.857, Test loss: 1.885, Test accuracy: 57.41
Round  50, Train loss: 1.859, Test loss: 1.881, Test accuracy: 57.85
Round  51, Train loss: 1.867, Test loss: 1.881, Test accuracy: 57.89
Round  52, Train loss: 1.890, Test loss: 1.881, Test accuracy: 57.88
Round  53, Train loss: 1.878, Test loss: 1.880, Test accuracy: 57.94
Round  54, Train loss: 1.811, Test loss: 1.879, Test accuracy: 57.99
Round  55, Train loss: 1.851, Test loss: 1.879, Test accuracy: 58.02
Round  56, Train loss: 1.837, Test loss: 1.878, Test accuracy: 58.06
Round  57, Train loss: 1.872, Test loss: 1.875, Test accuracy: 58.45
Round  58, Train loss: 1.857, Test loss: 1.873, Test accuracy: 58.62
Round  59, Train loss: 1.818, Test loss: 1.873, Test accuracy: 58.59
Round  60, Train loss: 1.816, Test loss: 1.873, Test accuracy: 58.63
Round  61, Train loss: 1.846, Test loss: 1.872, Test accuracy: 58.63
Round  62, Train loss: 1.899, Test loss: 1.872, Test accuracy: 58.67
Round  63, Train loss: 1.883, Test loss: 1.868, Test accuracy: 59.11
Round  64, Train loss: 1.853, Test loss: 1.864, Test accuracy: 59.55
Round  65, Train loss: 1.853, Test loss: 1.861, Test accuracy: 59.91
Round  66, Train loss: 1.868, Test loss: 1.859, Test accuracy: 60.04
Round  67, Train loss: 1.819, Test loss: 1.859, Test accuracy: 59.99
Round  68, Train loss: 1.812, Test loss: 1.859, Test accuracy: 60.02
Round  69, Train loss: 1.835, Test loss: 1.859, Test accuracy: 60.03
Round  70, Train loss: 1.835, Test loss: 1.858, Test accuracy: 60.06
Round  71, Train loss: 1.811, Test loss: 1.857, Test accuracy: 60.10
Round  72, Train loss: 1.794, Test loss: 1.857, Test accuracy: 60.11
Round  73, Train loss: 1.783, Test loss: 1.857, Test accuracy: 60.12
Round  74, Train loss: 1.821, Test loss: 1.857, Test accuracy: 60.15
Round  75, Train loss: 1.865, Test loss: 1.856, Test accuracy: 60.18
Round  76, Train loss: 1.898, Test loss: 1.855, Test accuracy: 60.27
Round  77, Train loss: 1.830, Test loss: 1.855, Test accuracy: 60.32
Round  78, Train loss: 1.837, Test loss: 1.855, Test accuracy: 60.28
Round  79, Train loss: 1.815, Test loss: 1.855, Test accuracy: 60.26
Round  80, Train loss: 1.769, Test loss: 1.854, Test accuracy: 60.31
Round  81, Train loss: 1.822, Test loss: 1.855, Test accuracy: 60.24
Round  82, Train loss: 1.852, Test loss: 1.854, Test accuracy: 60.39
Round  83, Train loss: 1.780, Test loss: 1.854, Test accuracy: 60.37
Round  84, Train loss: 1.790, Test loss: 1.853, Test accuracy: 60.40
Round  85, Train loss: 1.824, Test loss: 1.853, Test accuracy: 60.41
Round  86, Train loss: 1.801, Test loss: 1.853, Test accuracy: 60.39
Round  87, Train loss: 1.845, Test loss: 1.853, Test accuracy: 60.46
Round  88, Train loss: 1.850, Test loss: 1.852, Test accuracy: 60.47
Round  89, Train loss: 1.836, Test loss: 1.852, Test accuracy: 60.52
Round  90, Train loss: 1.796, Test loss: 1.852, Test accuracy: 60.47
Round  91, Train loss: 1.785, Test loss: 1.852, Test accuracy: 60.47
Round  92, Train loss: 1.861, Test loss: 1.852, Test accuracy: 60.47
Round  93, Train loss: 1.814, Test loss: 1.852, Test accuracy: 60.50
Round  94, Train loss: 1.821, Test loss: 1.852, Test accuracy: 60.46
Round  95, Train loss: 1.793, Test loss: 1.851, Test accuracy: 60.50
Round  96, Train loss: 1.857, Test loss: 1.851, Test accuracy: 60.56
Round  97, Train loss: 1.870, Test loss: 1.851, Test accuracy: 60.56
Round  98, Train loss: 1.809, Test loss: 1.847, Test accuracy: 60.99/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.822, Test loss: 1.847, Test accuracy: 60.99
Final Round, Train loss: 1.818, Test loss: 1.844, Test accuracy: 61.34
Average accuracy final 10 rounds: 60.598 

2998.7795639038086
[2.427969217300415, 4.85593843460083, 7.182157516479492, 9.508376598358154, 11.914167404174805, 14.319958209991455, 16.573652267456055, 18.827346324920654, 21.174022674560547, 23.52069902420044, 25.864395141601562, 28.208091259002686, 30.475794553756714, 32.74349784851074, 35.087074518203735, 37.43065118789673, 39.711387157440186, 41.99212312698364, 44.279855728149414, 46.567588329315186, 48.906111001968384, 51.24463367462158, 53.51448655128479, 55.784339427948, 58.14170169830322, 60.49906396865845, 62.85284399986267, 65.2066240310669, 67.61152124404907, 70.01641845703125, 72.40200710296631, 74.78759574890137, 77.18552494049072, 79.58345413208008, 81.7268590927124, 83.87026405334473, 86.09862804412842, 88.32699203491211, 90.56164193153381, 92.79629182815552, 95.0382661819458, 97.28024053573608, 99.408451795578, 101.53666305541992, 103.82398629188538, 106.11130952835083, 108.28830027580261, 110.4652910232544, 112.67835831642151, 114.89142560958862, 117.17674446105957, 119.46206331253052, 121.66099977493286, 123.8599362373352, 126.05727028846741, 128.2546043395996, 130.4968626499176, 132.7391209602356, 134.90896487236023, 137.07880878448486, 139.28339457511902, 141.48798036575317, 143.74323391914368, 145.99848747253418, 148.14935731887817, 150.30022716522217, 152.58069777488708, 154.861168384552, 157.01806807518005, 159.1749677658081, 161.3827567100525, 163.59054565429688, 165.8003706932068, 168.0101957321167, 170.29563879966736, 172.58108186721802, 174.74446773529053, 176.90785360336304, 179.0661633014679, 181.22447299957275, 183.37721228599548, 185.5299515724182, 187.78423762321472, 190.03852367401123, 192.25379252433777, 194.4690613746643, 196.76749110221863, 199.06592082977295, 201.18755960464478, 203.3091983795166, 205.61695337295532, 207.92470836639404, 210.16209435462952, 212.399480342865, 214.5984239578247, 216.79736757278442, 218.9802052974701, 221.16304302215576, 223.43532061576843, 225.7075982093811, 227.88145995140076, 230.0553216934204, 232.3154218196869, 234.57552194595337, 236.81729173660278, 239.0590615272522, 241.28102779388428, 243.50299406051636, 245.84730553627014, 248.19161701202393, 250.44853377342224, 252.70545053482056, 254.83062362670898, 256.9557967185974, 259.2147879600525, 261.47377920150757, 263.69050693511963, 265.9072346687317, 268.09996366500854, 270.2926926612854, 272.4971511363983, 274.70160961151123, 276.923996925354, 279.1463842391968, 281.2844421863556, 283.4225001335144, 285.7714846134186, 288.12046909332275, 290.3237843513489, 292.527099609375, 294.80749917030334, 297.0878987312317, 299.30157136917114, 301.5152440071106, 303.8552165031433, 306.195188999176, 308.32556343078613, 310.45593786239624, 312.69755935668945, 314.93918085098267, 317.21046328544617, 319.48174571990967, 321.7258298397064, 323.9699139595032, 326.2159252166748, 328.46193647384644, 330.70315408706665, 332.94437170028687, 335.10712909698486, 337.26988649368286, 339.811874628067, 342.3538627624512, 344.7860746383667, 347.2182865142822, 349.72481989860535, 352.23135328292847, 354.6993260383606, 357.1672987937927, 359.62603545188904, 362.08477210998535, 364.4646317958832, 366.844491481781, 369.3104066848755, 371.77632188796997, 374.2273783683777, 376.6784348487854, 379.1008565425873, 381.52327823638916, 384.0050265789032, 386.48677492141724, 388.91427183151245, 391.34176874160767, 393.6934552192688, 396.04514169692993, 398.4574863910675, 400.8698310852051, 403.30160093307495, 405.7333707809448, 408.0645270347595, 410.3956832885742, 412.8513379096985, 415.30699253082275, 417.71704053878784, 420.12708854675293, 422.5924139022827, 425.0577392578125, 427.22749638557434, 429.3972535133362, 431.70405554771423, 434.0108575820923, 436.1664402484894, 438.3220229148865, 440.5216417312622, 442.72126054763794, 444.96035742759705, 447.19945430755615, 449.40548062324524, 451.6115069389343, 453.8241684436798, 456.0368299484253, 457.5174415111542, 458.99805307388306]
[10.23, 10.23, 10.883333333333333, 10.883333333333333, 11.543333333333333, 11.543333333333333, 11.99, 11.99, 14.603333333333333, 14.603333333333333, 17.97, 17.97, 24.256666666666668, 24.256666666666668, 31.99, 31.99, 34.25333333333333, 34.25333333333333, 38.63, 38.63, 43.04, 43.04, 45.17333333333333, 45.17333333333333, 46.58, 46.58, 47.10333333333333, 47.10333333333333, 47.096666666666664, 47.096666666666664, 47.416666666666664, 47.416666666666664, 47.85, 47.85, 48.63333333333333, 48.63333333333333, 49.443333333333335, 49.443333333333335, 50.18, 50.18, 50.29333333333334, 50.29333333333334, 50.68666666666667, 50.68666666666667, 51.37, 51.37, 51.60333333333333, 51.60333333333333, 51.50333333333333, 51.50333333333333, 52.166666666666664, 52.166666666666664, 52.263333333333335, 52.263333333333335, 52.31666666666667, 52.31666666666667, 52.49333333333333, 52.49333333333333, 53.24333333333333, 53.24333333333333, 53.263333333333335, 53.263333333333335, 53.24, 53.24, 53.74333333333333, 53.74333333333333, 53.88666666666666, 53.88666666666666, 54.373333333333335, 54.373333333333335, 54.65, 54.65, 54.70666666666666, 54.70666666666666, 54.66, 54.66, 55.513333333333335, 55.513333333333335, 55.61333333333334, 55.61333333333334, 55.693333333333335, 55.693333333333335, 55.85666666666667, 55.85666666666667, 56.403333333333336, 56.403333333333336, 56.43333333333333, 56.43333333333333, 56.54333333333334, 56.54333333333334, 56.556666666666665, 56.556666666666665, 56.95333333333333, 56.95333333333333, 57.00333333333333, 57.00333333333333, 57.34, 57.34, 57.406666666666666, 57.406666666666666, 57.85, 57.85, 57.88666666666666, 57.88666666666666, 57.876666666666665, 57.876666666666665, 57.94, 57.94, 57.986666666666665, 57.986666666666665, 58.016666666666666, 58.016666666666666, 58.06, 58.06, 58.45333333333333, 58.45333333333333, 58.623333333333335, 58.623333333333335, 58.593333333333334, 58.593333333333334, 58.63333333333333, 58.63333333333333, 58.63, 58.63, 58.666666666666664, 58.666666666666664, 59.11333333333334, 59.11333333333334, 59.54666666666667, 59.54666666666667, 59.913333333333334, 59.913333333333334, 60.04333333333334, 60.04333333333334, 59.99, 59.99, 60.02333333333333, 60.02333333333333, 60.03333333333333, 60.03333333333333, 60.056666666666665, 60.056666666666665, 60.096666666666664, 60.096666666666664, 60.11333333333334, 60.11333333333334, 60.123333333333335, 60.123333333333335, 60.153333333333336, 60.153333333333336, 60.18333333333333, 60.18333333333333, 60.27333333333333, 60.27333333333333, 60.31666666666667, 60.31666666666667, 60.276666666666664, 60.276666666666664, 60.25666666666667, 60.25666666666667, 60.31333333333333, 60.31333333333333, 60.24333333333333, 60.24333333333333, 60.39, 60.39, 60.373333333333335, 60.373333333333335, 60.403333333333336, 60.403333333333336, 60.413333333333334, 60.413333333333334, 60.39, 60.39, 60.45666666666666, 60.45666666666666, 60.47, 60.47, 60.52333333333333, 60.52333333333333, 60.47, 60.47, 60.46666666666667, 60.46666666666667, 60.473333333333336, 60.473333333333336, 60.50333333333333, 60.50333333333333, 60.46333333333333, 60.46333333333333, 60.50333333333333, 60.50333333333333, 60.556666666666665, 60.556666666666665, 60.56333333333333, 60.56333333333333, 60.986666666666665, 60.986666666666665, 60.99333333333333, 60.99333333333333, 61.34, 61.34]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.284, Test loss: 2.300, Test accuracy: 11.34
Round   1, Train loss: 2.102, Test loss: 2.280, Test accuracy: 15.15
Round   2, Train loss: 1.861, Test loss: 2.224, Test accuracy: 22.43
Round   3, Train loss: 1.903, Test loss: 2.189, Test accuracy: 26.34
Round   4, Train loss: 1.941, Test loss: 2.135, Test accuracy: 32.65
Round   5, Train loss: 1.835, Test loss: 2.075, Test accuracy: 38.99
Round   6, Train loss: 1.819, Test loss: 2.059, Test accuracy: 39.99
Round   7, Train loss: 1.843, Test loss: 1.986, Test accuracy: 48.60
Round   8, Train loss: 1.883, Test loss: 1.953, Test accuracy: 52.08
Round   9, Train loss: 1.892, Test loss: 1.930, Test accuracy: 54.18
Round  10, Train loss: 1.747, Test loss: 1.902, Test accuracy: 56.75
Round  11, Train loss: 1.774, Test loss: 1.890, Test accuracy: 57.92
Round  12, Train loss: 1.749, Test loss: 1.876, Test accuracy: 59.10
Round  13, Train loss: 1.884, Test loss: 1.866, Test accuracy: 60.23
Round  14, Train loss: 1.809, Test loss: 1.848, Test accuracy: 61.73
Round  15, Train loss: 1.767, Test loss: 1.840, Test accuracy: 62.38
Round  16, Train loss: 1.877, Test loss: 1.829, Test accuracy: 63.36
Round  17, Train loss: 1.833, Test loss: 1.834, Test accuracy: 62.73
Round  18, Train loss: 1.779, Test loss: 1.818, Test accuracy: 64.37
Round  19, Train loss: 1.780, Test loss: 1.811, Test accuracy: 64.95
Round  20, Train loss: 1.718, Test loss: 1.806, Test accuracy: 65.44
Round  21, Train loss: 1.719, Test loss: 1.807, Test accuracy: 65.33
Round  22, Train loss: 1.735, Test loss: 1.799, Test accuracy: 66.06
Round  23, Train loss: 1.717, Test loss: 1.796, Test accuracy: 66.47
Round  24, Train loss: 1.807, Test loss: 1.795, Test accuracy: 66.59
Round  25, Train loss: 1.746, Test loss: 1.790, Test accuracy: 67.08
Round  26, Train loss: 1.828, Test loss: 1.789, Test accuracy: 67.14
Round  27, Train loss: 1.681, Test loss: 1.787, Test accuracy: 67.32
Round  28, Train loss: 1.766, Test loss: 1.783, Test accuracy: 67.67
Round  29, Train loss: 1.798, Test loss: 1.774, Test accuracy: 68.65
Round  30, Train loss: 1.694, Test loss: 1.770, Test accuracy: 69.03
Round  31, Train loss: 1.691, Test loss: 1.769, Test accuracy: 69.02
Round  32, Train loss: 1.744, Test loss: 1.768, Test accuracy: 69.06
Round  33, Train loss: 1.730, Test loss: 1.756, Test accuracy: 70.36
Round  34, Train loss: 1.697, Test loss: 1.756, Test accuracy: 70.37
Round  35, Train loss: 1.663, Test loss: 1.755, Test accuracy: 70.44
Round  36, Train loss: 1.707, Test loss: 1.753, Test accuracy: 70.70
Round  37, Train loss: 1.778, Test loss: 1.748, Test accuracy: 71.21
Round  38, Train loss: 1.769, Test loss: 1.748, Test accuracy: 71.17
Round  39, Train loss: 1.754, Test loss: 1.747, Test accuracy: 71.24
Round  40, Train loss: 1.773, Test loss: 1.746, Test accuracy: 71.26
Round  41, Train loss: 1.673, Test loss: 1.741, Test accuracy: 71.69
Round  42, Train loss: 1.640, Test loss: 1.740, Test accuracy: 71.77
Round  43, Train loss: 1.625, Test loss: 1.740, Test accuracy: 71.80
Round  44, Train loss: 1.695, Test loss: 1.740, Test accuracy: 71.74
Round  45, Train loss: 1.676, Test loss: 1.739, Test accuracy: 71.90
Round  46, Train loss: 1.740, Test loss: 1.736, Test accuracy: 72.16
Round  47, Train loss: 1.794, Test loss: 1.733, Test accuracy: 72.48
Round  48, Train loss: 1.693, Test loss: 1.732, Test accuracy: 72.52
Round  49, Train loss: 1.782, Test loss: 1.731, Test accuracy: 72.64
Round  50, Train loss: 1.653, Test loss: 1.727, Test accuracy: 73.06
Round  51, Train loss: 1.767, Test loss: 1.727, Test accuracy: 73.09
Round  52, Train loss: 1.727, Test loss: 1.726, Test accuracy: 73.17
Round  53, Train loss: 1.629, Test loss: 1.725, Test accuracy: 73.24
Round  54, Train loss: 1.707, Test loss: 1.724, Test accuracy: 73.28
Round  55, Train loss: 1.609, Test loss: 1.724, Test accuracy: 73.27
Round  56, Train loss: 1.695, Test loss: 1.721, Test accuracy: 73.57
Round  57, Train loss: 1.677, Test loss: 1.722, Test accuracy: 73.53
Round  58, Train loss: 1.666, Test loss: 1.716, Test accuracy: 74.15
Round  59, Train loss: 1.645, Test loss: 1.714, Test accuracy: 74.33
Round  60, Train loss: 1.751, Test loss: 1.712, Test accuracy: 74.66
Round  61, Train loss: 1.574, Test loss: 1.709, Test accuracy: 74.85
Round  62, Train loss: 1.704, Test loss: 1.701, Test accuracy: 75.67
Round  63, Train loss: 1.668, Test loss: 1.694, Test accuracy: 76.49
Round  64, Train loss: 1.589, Test loss: 1.694, Test accuracy: 76.51
Round  65, Train loss: 1.692, Test loss: 1.693, Test accuracy: 76.54
Round  66, Train loss: 1.592, Test loss: 1.691, Test accuracy: 76.68
Round  67, Train loss: 1.673, Test loss: 1.690, Test accuracy: 76.81
Round  68, Train loss: 1.715, Test loss: 1.688, Test accuracy: 76.98
Round  69, Train loss: 1.688, Test loss: 1.688, Test accuracy: 76.97
Round  70, Train loss: 1.667, Test loss: 1.685, Test accuracy: 77.39
Round  71, Train loss: 1.634, Test loss: 1.684, Test accuracy: 77.44
Round  72, Train loss: 1.581, Test loss: 1.684, Test accuracy: 77.44
Round  73, Train loss: 1.657, Test loss: 1.683, Test accuracy: 77.48
Round  74, Train loss: 1.647, Test loss: 1.680, Test accuracy: 77.89
Round  75, Train loss: 1.690, Test loss: 1.680, Test accuracy: 77.91
Round  76, Train loss: 1.637, Test loss: 1.680, Test accuracy: 77.92
Round  77, Train loss: 1.703, Test loss: 1.680, Test accuracy: 77.86
Round  78, Train loss: 1.665, Test loss: 1.675, Test accuracy: 78.44
Round  79, Train loss: 1.603, Test loss: 1.671, Test accuracy: 78.78
Round  80, Train loss: 1.718, Test loss: 1.671, Test accuracy: 78.86
Round  81, Train loss: 1.656, Test loss: 1.670, Test accuracy: 78.94
Round  82, Train loss: 1.678, Test loss: 1.667, Test accuracy: 79.21
Round  83, Train loss: 1.588, Test loss: 1.663, Test accuracy: 79.79
Round  84, Train loss: 1.635, Test loss: 1.663, Test accuracy: 79.70
Round  85, Train loss: 1.670, Test loss: 1.663, Test accuracy: 79.70
Round  86, Train loss: 1.691, Test loss: 1.663, Test accuracy: 79.68
Round  87, Train loss: 1.669, Test loss: 1.662, Test accuracy: 79.70
Round  88, Train loss: 1.627, Test loss: 1.661, Test accuracy: 79.84
Round  89, Train loss: 1.626, Test loss: 1.659, Test accuracy: 80.04
Round  90, Train loss: 1.639, Test loss: 1.657, Test accuracy: 80.08
Round  91, Train loss: 1.629, Test loss: 1.657, Test accuracy: 80.12
Round  92, Train loss: 1.659, Test loss: 1.657, Test accuracy: 80.16
Round  93, Train loss: 1.675, Test loss: 1.656, Test accuracy: 80.19
Round  94, Train loss: 1.637, Test loss: 1.656, Test accuracy: 80.22
Round  95, Train loss: 1.687, Test loss: 1.658, Test accuracy: 80.13
Round  96, Train loss: 1.623, Test loss: 1.656, Test accuracy: 80.21
Round  97, Train loss: 1.624, Test loss: 1.656, Test accuracy: 80.24
Round  98, Train loss: 1.629, Test loss: 1.656, Test accuracy: 80.20/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.621, Test loss: 1.656, Test accuracy: 80.22
Final Round, Train loss: 1.632, Test loss: 1.655, Test accuracy: 80.30
Average accuracy final 10 rounds: 80.17774999999999 

4012.5860726833344
[3.5255918502807617, 7.051183700561523, 10.107309341430664, 13.163434982299805, 16.284217834472656, 19.405000686645508, 22.399104833602905, 25.393208980560303, 28.376196146011353, 31.359183311462402, 34.361398458480835, 37.36361360549927, 40.9063503742218, 44.449087142944336, 47.533019065856934, 50.61695098876953, 53.66695046424866, 56.71694993972778, 59.79188299179077, 62.86681604385376, 65.86520314216614, 68.86359024047852, 71.84839177131653, 74.83319330215454, 77.92339754104614, 81.01360177993774, 83.98384833335876, 86.95409488677979, 90.0783531665802, 93.20261144638062, 96.42734694480896, 99.6520824432373, 102.73412418365479, 105.81616592407227, 108.8738226890564, 111.93147945404053, 115.08243680000305, 118.23339414596558, 121.28816270828247, 124.34293127059937, 127.31422638893127, 130.28552150726318, 133.43991613388062, 136.59431076049805, 139.6274995803833, 142.66068840026855, 145.68507862091064, 148.70946884155273, 151.9611575603485, 155.2128462791443, 158.26065587997437, 161.30846548080444, 164.22217965126038, 167.1358938217163, 170.2769637107849, 173.41803359985352, 176.392813205719, 179.36759281158447, 182.32929039001465, 185.29098796844482, 188.38644123077393, 191.48189449310303, 194.46523022651672, 197.44856595993042, 200.44392848014832, 203.4392910003662, 206.51922917366028, 209.59916734695435, 212.7134187221527, 215.82767009735107, 218.88987398147583, 221.9520778656006, 225.03250288963318, 228.11292791366577, 231.24497938156128, 234.3770308494568, 237.8577663898468, 241.33850193023682, 244.82829594612122, 248.31808996200562, 251.79605841636658, 255.27402687072754, 258.81555247306824, 262.35707807540894, 265.8225874900818, 269.28809690475464, 272.8276717662811, 276.3672466278076, 279.4050838947296, 282.4429211616516, 285.54383993148804, 288.64475870132446, 291.7697169780731, 294.8946752548218, 297.83040976524353, 300.7661442756653, 303.86183285713196, 306.95752143859863, 309.9590756893158, 312.96062994003296, 315.8395917415619, 318.7185535430908, 321.78711128234863, 324.85566902160645, 327.9729151725769, 331.09016132354736, 334.04842472076416, 337.00668811798096, 340.23004484176636, 343.45340156555176, 346.4595522880554, 349.4657030105591, 352.46318006515503, 355.460657119751, 358.494371175766, 361.528085231781, 364.5307891368866, 367.5334930419922, 370.6408431529999, 373.74819326400757, 376.74860978126526, 379.74902629852295, 382.8047318458557, 385.8604373931885, 389.02412843704224, 392.187819480896, 395.1276111602783, 398.06740283966064, 401.0333888530731, 403.9993748664856, 407.20080518722534, 410.4022355079651, 413.2971956729889, 416.1921558380127, 419.2641990184784, 422.3362421989441, 425.52681851387024, 428.7173948287964, 431.678759098053, 434.64012336730957, 437.8225426673889, 441.00496196746826, 444.58675384521484, 448.1685457229614, 451.70133900642395, 455.2341322898865, 458.7577884197235, 462.28144454956055, 465.8388831615448, 469.39632177352905, 472.84562277793884, 476.29492378234863, 479.70707631111145, 483.11922883987427, 486.2104787826538, 489.30172872543335, 492.30792784690857, 495.3141269683838, 498.23679876327515, 501.1594705581665, 504.19951915740967, 507.23956775665283, 510.22417974472046, 513.2087917327881, 516.3670608997345, 519.5253300666809, 522.6410615444183, 525.7567930221558, 528.6401703357697, 531.5235476493835, 534.7086906433105, 537.8938336372375, 540.8902931213379, 543.8867526054382, 546.8829116821289, 549.8790707588196, 553.0289633274078, 556.1788558959961, 559.2782351970673, 562.3776144981384, 565.424845457077, 568.4720764160156, 571.6289708614349, 574.7858653068542, 577.7659847736359, 580.7461042404175, 583.8928735256195, 587.0396428108215, 590.1699583530426, 593.3002738952637, 596.2184338569641, 599.1365938186646, 602.2899866104126, 605.4433794021606, 608.5229601860046, 611.6025409698486, 614.5177829265594, 617.4330248832703, 620.5320191383362, 623.6310133934021, 625.103643655777, 626.5762739181519]
[11.3375, 11.3375, 15.1525, 15.1525, 22.4275, 22.4275, 26.3425, 26.3425, 32.65, 32.65, 38.9875, 38.9875, 39.995, 39.995, 48.6025, 48.6025, 52.0775, 52.0775, 54.1775, 54.1775, 56.75, 56.75, 57.925, 57.925, 59.0975, 59.0975, 60.2275, 60.2275, 61.7275, 61.7275, 62.3825, 62.3825, 63.3575, 63.3575, 62.7275, 62.7275, 64.37, 64.37, 64.9525, 64.9525, 65.44, 65.44, 65.335, 65.335, 66.065, 66.065, 66.465, 66.465, 66.5925, 66.5925, 67.075, 67.075, 67.1375, 67.1375, 67.32, 67.32, 67.6725, 67.6725, 68.6475, 68.6475, 69.03, 69.03, 69.0175, 69.0175, 69.06, 69.06, 70.355, 70.355, 70.3725, 70.3725, 70.435, 70.435, 70.7, 70.7, 71.21, 71.21, 71.165, 71.165, 71.24, 71.24, 71.2625, 71.2625, 71.69, 71.69, 71.765, 71.765, 71.795, 71.795, 71.74, 71.74, 71.8975, 71.8975, 72.1575, 72.1575, 72.4775, 72.4775, 72.5175, 72.5175, 72.64, 72.64, 73.0575, 73.0575, 73.0875, 73.0875, 73.165, 73.165, 73.24, 73.24, 73.275, 73.275, 73.27, 73.27, 73.5725, 73.5725, 73.535, 73.535, 74.1525, 74.1525, 74.325, 74.325, 74.6625, 74.6625, 74.8525, 74.8525, 75.675, 75.675, 76.4875, 76.4875, 76.5075, 76.5075, 76.5375, 76.5375, 76.6825, 76.6825, 76.8125, 76.8125, 76.98, 76.98, 76.965, 76.965, 77.395, 77.395, 77.435, 77.435, 77.44, 77.44, 77.48, 77.48, 77.89, 77.89, 77.9075, 77.9075, 77.92, 77.92, 77.855, 77.855, 78.44, 78.44, 78.7825, 78.7825, 78.8575, 78.8575, 78.94, 78.94, 79.2125, 79.2125, 79.7925, 79.7925, 79.7, 79.7, 79.705, 79.705, 79.68, 79.68, 79.705, 79.705, 79.8375, 79.8375, 80.0425, 80.0425, 80.0775, 80.0775, 80.1175, 80.1175, 80.16, 80.16, 80.19, 80.19, 80.225, 80.225, 80.13, 80.13, 80.2125, 80.2125, 80.2375, 80.2375, 80.2025, 80.2025, 80.225, 80.225, 80.3, 80.3]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.274, Test loss: 2.280, Test accuracy: 17.68
Round   1, Train loss: 1.981, Test loss: 2.162, Test accuracy: 31.77
Round   2, Train loss: 1.796, Test loss: 2.068, Test accuracy: 38.29
Round   3, Train loss: 1.940, Test loss: 1.963, Test accuracy: 50.22
Round   4, Train loss: 1.734, Test loss: 1.912, Test accuracy: 54.72
Round   5, Train loss: 1.817, Test loss: 1.840, Test accuracy: 62.59
Round   6, Train loss: 1.751, Test loss: 1.807, Test accuracy: 66.13
Round   7, Train loss: 1.696, Test loss: 1.784, Test accuracy: 68.33
Round   8, Train loss: 1.658, Test loss: 1.759, Test accuracy: 70.72
Round   9, Train loss: 1.644, Test loss: 1.734, Test accuracy: 73.14
Round  10, Train loss: 1.671, Test loss: 1.725, Test accuracy: 73.97
Round  11, Train loss: 1.732, Test loss: 1.715, Test accuracy: 74.84
Round  12, Train loss: 1.688, Test loss: 1.710, Test accuracy: 75.30
Round  13, Train loss: 1.638, Test loss: 1.707, Test accuracy: 75.59
Round  14, Train loss: 1.660, Test loss: 1.693, Test accuracy: 77.06
Round  15, Train loss: 1.628, Test loss: 1.688, Test accuracy: 77.54
Round  16, Train loss: 1.658, Test loss: 1.683, Test accuracy: 77.98
Round  17, Train loss: 1.594, Test loss: 1.675, Test accuracy: 78.80
Round  18, Train loss: 1.605, Test loss: 1.671, Test accuracy: 79.25
Round  19, Train loss: 1.585, Test loss: 1.669, Test accuracy: 79.42
Round  20, Train loss: 1.558, Test loss: 1.665, Test accuracy: 79.75
Round  21, Train loss: 1.573, Test loss: 1.658, Test accuracy: 80.53
Round  22, Train loss: 1.612, Test loss: 1.657, Test accuracy: 80.64
Round  23, Train loss: 1.519, Test loss: 1.655, Test accuracy: 80.74
Round  24, Train loss: 1.574, Test loss: 1.651, Test accuracy: 81.17
Round  25, Train loss: 1.555, Test loss: 1.647, Test accuracy: 81.56
Round  26, Train loss: 1.579, Test loss: 1.646, Test accuracy: 81.67
Round  27, Train loss: 1.545, Test loss: 1.645, Test accuracy: 81.72
Round  28, Train loss: 1.542, Test loss: 1.645, Test accuracy: 81.72
Round  29, Train loss: 1.518, Test loss: 1.644, Test accuracy: 81.86
Round  30, Train loss: 1.593, Test loss: 1.641, Test accuracy: 82.16
Round  31, Train loss: 1.575, Test loss: 1.641, Test accuracy: 82.16
Round  32, Train loss: 1.577, Test loss: 1.641, Test accuracy: 82.20
Round  33, Train loss: 1.575, Test loss: 1.640, Test accuracy: 82.19
Round  34, Train loss: 1.580, Test loss: 1.639, Test accuracy: 82.26
Round  35, Train loss: 1.498, Test loss: 1.639, Test accuracy: 82.26
Round  36, Train loss: 1.563, Test loss: 1.639, Test accuracy: 82.25
Round  37, Train loss: 1.560, Test loss: 1.636, Test accuracy: 82.62
Round  38, Train loss: 1.618, Test loss: 1.636, Test accuracy: 82.65
Round  39, Train loss: 1.549, Test loss: 1.636, Test accuracy: 82.62
Round  40, Train loss: 1.532, Test loss: 1.636, Test accuracy: 82.60
Round  41, Train loss: 1.586, Test loss: 1.631, Test accuracy: 83.13
Round  42, Train loss: 1.588, Test loss: 1.631, Test accuracy: 83.13
Round  43, Train loss: 1.562, Test loss: 1.631, Test accuracy: 83.18
Round  44, Train loss: 1.542, Test loss: 1.631, Test accuracy: 83.18
Round  45, Train loss: 1.557, Test loss: 1.630, Test accuracy: 83.17
Round  46, Train loss: 1.560, Test loss: 1.631, Test accuracy: 83.16
Round  47, Train loss: 1.525, Test loss: 1.630, Test accuracy: 83.16
Round  48, Train loss: 1.573, Test loss: 1.630, Test accuracy: 83.19
Round  49, Train loss: 1.572, Test loss: 1.630, Test accuracy: 83.16
Round  50, Train loss: 1.561, Test loss: 1.630, Test accuracy: 83.17
Round  51, Train loss: 1.525, Test loss: 1.630, Test accuracy: 83.20
Round  52, Train loss: 1.539, Test loss: 1.630, Test accuracy: 83.19
Round  53, Train loss: 1.528, Test loss: 1.630, Test accuracy: 83.19
Round  54, Train loss: 1.592, Test loss: 1.630, Test accuracy: 83.24
Round  55, Train loss: 1.523, Test loss: 1.630, Test accuracy: 83.27
Round  56, Train loss: 1.616, Test loss: 1.627, Test accuracy: 83.59
Round  57, Train loss: 1.522, Test loss: 1.627, Test accuracy: 83.56
Round  58, Train loss: 1.556, Test loss: 1.626, Test accuracy: 83.58
Round  59, Train loss: 1.613, Test loss: 1.626, Test accuracy: 83.61
Round  60, Train loss: 1.554, Test loss: 1.626, Test accuracy: 83.61
Round  61, Train loss: 1.524, Test loss: 1.626, Test accuracy: 83.61
Round  62, Train loss: 1.537, Test loss: 1.626, Test accuracy: 83.61
Round  63, Train loss: 1.613, Test loss: 1.626, Test accuracy: 83.61
Round  64, Train loss: 1.527, Test loss: 1.626, Test accuracy: 83.62
Round  65, Train loss: 1.574, Test loss: 1.626, Test accuracy: 83.63
Round  66, Train loss: 1.533, Test loss: 1.625, Test accuracy: 83.70
Round  67, Train loss: 1.552, Test loss: 1.625, Test accuracy: 83.70
Round  68, Train loss: 1.560, Test loss: 1.625, Test accuracy: 83.69
Round  69, Train loss: 1.541, Test loss: 1.625, Test accuracy: 83.71
Round  70, Train loss: 1.563, Test loss: 1.617, Test accuracy: 84.45
Round  71, Train loss: 1.493, Test loss: 1.617, Test accuracy: 84.51
Round  72, Train loss: 1.512, Test loss: 1.617, Test accuracy: 84.50
Round  73, Train loss: 1.589, Test loss: 1.617, Test accuracy: 84.50
Round  74, Train loss: 1.570, Test loss: 1.616, Test accuracy: 84.61
Round  75, Train loss: 1.542, Test loss: 1.616, Test accuracy: 84.64
Round  76, Train loss: 1.571, Test loss: 1.615, Test accuracy: 84.67
Round  77, Train loss: 1.526, Test loss: 1.615, Test accuracy: 84.64
Round  78, Train loss: 1.516, Test loss: 1.612, Test accuracy: 84.99
Round  79, Train loss: 1.519, Test loss: 1.612, Test accuracy: 85.02
Round  80, Train loss: 1.529, Test loss: 1.611, Test accuracy: 85.12
Round  81, Train loss: 1.529, Test loss: 1.606, Test accuracy: 85.62
Round  82, Train loss: 1.512, Test loss: 1.606, Test accuracy: 85.61
Round  83, Train loss: 1.521, Test loss: 1.606, Test accuracy: 85.62
Round  84, Train loss: 1.535, Test loss: 1.606, Test accuracy: 85.59
Round  85, Train loss: 1.508, Test loss: 1.606, Test accuracy: 85.64
Round  86, Train loss: 1.565, Test loss: 1.605, Test accuracy: 85.69
Round  87, Train loss: 1.540, Test loss: 1.605, Test accuracy: 85.66
Round  88, Train loss: 1.536, Test loss: 1.605, Test accuracy: 85.68
Round  89, Train loss: 1.520, Test loss: 1.601, Test accuracy: 86.17
Round  90, Train loss: 1.492, Test loss: 1.600, Test accuracy: 86.26
Round  91, Train loss: 1.528, Test loss: 1.600, Test accuracy: 86.28
Round  92, Train loss: 1.494, Test loss: 1.600, Test accuracy: 86.31
Round  93, Train loss: 1.494, Test loss: 1.600, Test accuracy: 86.32
Round  94, Train loss: 1.508, Test loss: 1.600, Test accuracy: 86.33
Round  95, Train loss: 1.555, Test loss: 1.599, Test accuracy: 86.34
Round  96, Train loss: 1.550, Test loss: 1.597, Test accuracy: 86.58
Round  97, Train loss: 1.550, Test loss: 1.594, Test accuracy: 86.90
Round  98, Train loss: 1.523, Test loss: 1.594, Test accuracy: 86.91
Round  99, Train loss: 1.521, Test loss: 1.593, Test accuracy: 87.05/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.512, Test loss: 1.585, Test accuracy: 87.87
Average accuracy final 10 rounds: 86.52875 

4103.781829833984
[3.49830961227417, 6.99661922454834, 10.542116403579712, 14.087613582611084, 17.63265824317932, 21.17770290374756, 24.641693830490112, 28.105684757232666, 31.646252155303955, 35.186819553375244, 38.74271368980408, 42.29860782623291, 45.73123288154602, 49.16385793685913, 52.711132764816284, 56.25840759277344, 59.81048607826233, 63.36256456375122, 66.83862352371216, 70.3146824836731, 73.70007729530334, 77.0854721069336, 80.9433240890503, 84.80117607116699, 88.4419813156128, 92.0827865600586, 95.61887407302856, 99.15496158599854, 102.73444867134094, 106.31393575668335, 109.9744086265564, 113.63488149642944, 117.28327560424805, 120.93166971206665, 124.39476728439331, 127.85786485671997, 131.39706683158875, 134.93626880645752, 138.07341623306274, 141.21056365966797, 144.88895440101624, 148.5673451423645, 151.97925305366516, 155.39116096496582, 159.0392439365387, 162.68732690811157, 166.4679491519928, 170.24857139587402, 173.80264687538147, 177.35672235488892, 180.84846138954163, 184.34020042419434, 187.61649894714355, 190.89279747009277, 194.1931028366089, 197.493408203125, 200.41064858436584, 203.3278889656067, 206.6168885231018, 209.90588808059692, 213.1511869430542, 216.39648580551147, 219.3780484199524, 222.3596110343933, 225.64873051643372, 228.93784999847412, 232.08836817741394, 235.23888635635376, 238.50487327575684, 241.7708601951599, 245.01450371742249, 248.25814723968506, 251.30835437774658, 254.3585615158081, 257.5360996723175, 260.7136378288269, 263.98585987091064, 267.2580819129944, 270.2221612930298, 273.1862406730652, 276.52163457870483, 279.8570284843445, 283.05405735969543, 286.2510862350464, 289.27023220062256, 292.28937816619873, 295.5047302246094, 298.72008228302, 301.75082516670227, 304.7815680503845, 307.8606216907501, 310.9396753311157, 314.0816240310669, 317.22357273101807, 320.2566657066345, 323.289758682251, 326.4116458892822, 329.5335330963135, 332.72819209098816, 335.92285108566284, 338.77931571006775, 341.63578033447266, 344.9186854362488, 348.2015905380249, 351.20917201042175, 354.2167534828186, 357.3019390106201, 360.38712453842163, 363.7095191478729, 367.0319137573242, 370.28776717185974, 373.54362058639526, 376.6114628314972, 379.6793050765991, 382.85348081588745, 386.0276565551758, 389.0876896381378, 392.14772272109985, 395.422926902771, 398.69813108444214, 401.9507837295532, 405.2034363746643, 408.35686349868774, 411.5102906227112, 414.5785222053528, 417.6467537879944, 420.8310377597809, 424.0153217315674, 427.15427827835083, 430.2932348251343, 433.586874961853, 436.8805150985718, 440.5366322994232, 444.19274950027466, 447.6973354816437, 451.2019214630127, 454.8316934108734, 458.46146535873413, 461.7771224975586, 465.09277963638306, 467.9613652229309, 470.82995080947876, 474.2043216228485, 477.57869243621826, 480.9312164783478, 484.2837405204773, 487.33631134033203, 490.38888216018677, 493.5734417438507, 496.75800132751465, 500.0248725414276, 503.2917437553406, 506.2385160923004, 509.18528842926025, 512.3998546600342, 515.6144208908081, 518.8846004009247, 522.1547799110413, 525.2404873371124, 528.3261947631836, 531.54434633255, 534.7624979019165, 537.9695301055908, 541.1765623092651, 544.1919853687286, 547.2074084281921, 550.5014629364014, 553.7955174446106, 557.0729148387909, 560.3503122329712, 563.4951546192169, 566.6399970054626, 569.6500868797302, 572.6601767539978, 575.7833750247955, 578.9065732955933, 581.9566764831543, 585.0067796707153, 587.9659900665283, 590.9252004623413, 594.0394577980042, 597.153715133667, 600.2207386493683, 603.2877621650696, 606.4412786960602, 609.5947952270508, 612.7861347198486, 615.9774742126465, 619.0073714256287, 622.0372686386108, 625.1849663257599, 628.3326640129089, 631.571519613266, 634.810375213623, 637.7885868549347, 640.7667984962463, 643.8959772586823, 647.0251560211182, 650.1469626426697, 653.2687692642212, 654.7802686691284, 656.2917680740356]
[17.685, 17.685, 31.7725, 31.7725, 38.29, 38.29, 50.2225, 50.2225, 54.7175, 54.7175, 62.59, 62.59, 66.1275, 66.1275, 68.335, 68.335, 70.72, 70.72, 73.1375, 73.1375, 73.965, 73.965, 74.845, 74.845, 75.295, 75.295, 75.59, 75.59, 77.0575, 77.0575, 77.5425, 77.5425, 77.9825, 77.9825, 78.8, 78.8, 79.255, 79.255, 79.415, 79.415, 79.745, 79.745, 80.5325, 80.5325, 80.6375, 80.6375, 80.74, 80.74, 81.175, 81.175, 81.5575, 81.5575, 81.6675, 81.6675, 81.7225, 81.7225, 81.7225, 81.7225, 81.8575, 81.8575, 82.155, 82.155, 82.1575, 82.1575, 82.205, 82.205, 82.1925, 82.1925, 82.2575, 82.2575, 82.26, 82.26, 82.245, 82.245, 82.62, 82.62, 82.6525, 82.6525, 82.6175, 82.6175, 82.6025, 82.6025, 83.1325, 83.1325, 83.13, 83.13, 83.1775, 83.1775, 83.18, 83.18, 83.1725, 83.1725, 83.155, 83.155, 83.1625, 83.1625, 83.185, 83.185, 83.1575, 83.1575, 83.17, 83.17, 83.2, 83.2, 83.1875, 83.1875, 83.1875, 83.1875, 83.2425, 83.2425, 83.2725, 83.2725, 83.5875, 83.5875, 83.56, 83.56, 83.58, 83.58, 83.605, 83.605, 83.605, 83.605, 83.6125, 83.6125, 83.61, 83.61, 83.6125, 83.6125, 83.6175, 83.6175, 83.6275, 83.6275, 83.7025, 83.7025, 83.7, 83.7, 83.69, 83.69, 83.71, 83.71, 84.4525, 84.4525, 84.5125, 84.5125, 84.4975, 84.4975, 84.495, 84.495, 84.6125, 84.6125, 84.6425, 84.6425, 84.67, 84.67, 84.64, 84.64, 84.9925, 84.9925, 85.02, 85.02, 85.1225, 85.1225, 85.6225, 85.6225, 85.615, 85.615, 85.6175, 85.6175, 85.5875, 85.5875, 85.635, 85.635, 85.685, 85.685, 85.655, 85.655, 85.68, 85.68, 86.165, 86.165, 86.2575, 86.2575, 86.285, 86.285, 86.31, 86.31, 86.3175, 86.3175, 86.325, 86.325, 86.3425, 86.3425, 86.585, 86.585, 86.9025, 86.9025, 86.91, 86.91, 87.0525, 87.0525, 87.8675, 87.8675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.519, Test loss: 2.232, Test accuracy: 25.63
Round   1, Train loss: 1.470, Test loss: 2.158, Test accuracy: 37.55
Round   2, Train loss: 1.412, Test loss: 2.110, Test accuracy: 42.30
Round   3, Train loss: 1.376, Test loss: 2.059, Test accuracy: 46.72
Round   4, Train loss: 1.416, Test loss: 2.034, Test accuracy: 47.97
Round   5, Train loss: 1.346, Test loss: 2.005, Test accuracy: 50.21
Round   6, Train loss: 1.420, Test loss: 1.977, Test accuracy: 52.95
Round   7, Train loss: 1.387, Test loss: 1.952, Test accuracy: 55.26
Round   8, Train loss: 1.436, Test loss: 1.932, Test accuracy: 57.22
Round   9, Train loss: 1.376, Test loss: 1.926, Test accuracy: 57.41
Round  10, Train loss: 1.404, Test loss: 1.926, Test accuracy: 57.16
Round  11, Train loss: 1.414, Test loss: 1.897, Test accuracy: 60.01
Round  12, Train loss: 1.335, Test loss: 1.886, Test accuracy: 61.07
Round  13, Train loss: 1.314, Test loss: 1.876, Test accuracy: 62.02
Round  14, Train loss: 1.403, Test loss: 1.871, Test accuracy: 62.63
Round  15, Train loss: 1.254, Test loss: 1.866, Test accuracy: 62.98
Round  16, Train loss: 1.281, Test loss: 1.862, Test accuracy: 63.06
Round  17, Train loss: 1.329, Test loss: 1.860, Test accuracy: 63.01
Round  18, Train loss: 1.263, Test loss: 1.855, Test accuracy: 63.55
Round  19, Train loss: 1.249, Test loss: 1.850, Test accuracy: 63.64
Round  20, Train loss: 1.345, Test loss: 1.849, Test accuracy: 63.70
Round  21, Train loss: 1.270, Test loss: 1.842, Test accuracy: 64.28
Round  22, Train loss: 1.257, Test loss: 1.838, Test accuracy: 64.44
Round  23, Train loss: 1.191, Test loss: 1.839, Test accuracy: 64.34
Round  24, Train loss: 1.308, Test loss: 1.835, Test accuracy: 64.62
Round  25, Train loss: 1.317, Test loss: 1.832, Test accuracy: 64.91
Round  26, Train loss: 1.307, Test loss: 1.830, Test accuracy: 64.95
Round  27, Train loss: 1.325, Test loss: 1.830, Test accuracy: 64.97
Round  28, Train loss: 1.260, Test loss: 1.833, Test accuracy: 64.66
Round  29, Train loss: 1.336, Test loss: 1.829, Test accuracy: 65.13
Round  30, Train loss: 1.245, Test loss: 1.830, Test accuracy: 64.75
Round  31, Train loss: 1.208, Test loss: 1.826, Test accuracy: 65.03
Round  32, Train loss: 1.255, Test loss: 1.830, Test accuracy: 64.50
Round  33, Train loss: 1.281, Test loss: 1.828, Test accuracy: 64.74
Round  34, Train loss: 1.261, Test loss: 1.829, Test accuracy: 64.58
Round  35, Train loss: 1.288, Test loss: 1.830, Test accuracy: 64.40
Round  36, Train loss: 1.279, Test loss: 1.831, Test accuracy: 64.34
Round  37, Train loss: 1.269, Test loss: 1.832, Test accuracy: 64.33
Round  38, Train loss: 1.206, Test loss: 1.832, Test accuracy: 64.22
Round  39, Train loss: 1.247, Test loss: 1.831, Test accuracy: 64.26
Round  40, Train loss: 1.247, Test loss: 1.829, Test accuracy: 64.66
Round  41, Train loss: 1.197, Test loss: 1.833, Test accuracy: 64.15
Round  42, Train loss: 1.230, Test loss: 1.834, Test accuracy: 63.90
Round  43, Train loss: 1.289, Test loss: 1.836, Test accuracy: 63.62
Round  44, Train loss: 1.293, Test loss: 1.836, Test accuracy: 63.64
Round  45, Train loss: 1.225, Test loss: 1.838, Test accuracy: 63.48
Round  46, Train loss: 1.242, Test loss: 1.837, Test accuracy: 63.54
Round  47, Train loss: 1.277, Test loss: 1.836, Test accuracy: 63.56
Round  48, Train loss: 1.285, Test loss: 1.832, Test accuracy: 63.99
Round  49, Train loss: 1.248, Test loss: 1.832, Test accuracy: 64.02
Round  50, Train loss: 1.280, Test loss: 1.833, Test accuracy: 63.85
Round  51, Train loss: 1.236, Test loss: 1.836, Test accuracy: 63.44
Round  52, Train loss: 1.184, Test loss: 1.833, Test accuracy: 63.76
Round  53, Train loss: 1.190, Test loss: 1.835, Test accuracy: 63.45
Round  54, Train loss: 1.283, Test loss: 1.837, Test accuracy: 63.29
Round  55, Train loss: 1.267, Test loss: 1.838, Test accuracy: 63.11
Round  56, Train loss: 1.244, Test loss: 1.842, Test accuracy: 62.77
Round  57, Train loss: 1.209, Test loss: 1.841, Test accuracy: 62.76
Round  58, Train loss: 1.266, Test loss: 1.842, Test accuracy: 62.75
Round  59, Train loss: 1.239, Test loss: 1.845, Test accuracy: 62.41
Round  60, Train loss: 1.228, Test loss: 1.845, Test accuracy: 62.20
Round  61, Train loss: 1.302, Test loss: 1.847, Test accuracy: 61.96
Round  62, Train loss: 1.257, Test loss: 1.842, Test accuracy: 62.49
Round  63, Train loss: 1.241, Test loss: 1.843, Test accuracy: 62.31
Round  64, Train loss: 1.206, Test loss: 1.841, Test accuracy: 62.63
Round  65, Train loss: 1.242, Test loss: 1.841, Test accuracy: 62.55
Round  66, Train loss: 1.200, Test loss: 1.844, Test accuracy: 62.27
Round  67, Train loss: 1.253, Test loss: 1.845, Test accuracy: 62.12
Round  68, Train loss: 1.255, Test loss: 1.846, Test accuracy: 62.08
Round  69, Train loss: 1.233, Test loss: 1.847, Test accuracy: 61.88
Round  70, Train loss: 1.218, Test loss: 1.850, Test accuracy: 61.51
Round  71, Train loss: 1.180, Test loss: 1.847, Test accuracy: 61.76
Round  72, Train loss: 1.325, Test loss: 1.853, Test accuracy: 61.23
Round  73, Train loss: 1.247, Test loss: 1.856, Test accuracy: 60.87
Round  74, Train loss: 1.236, Test loss: 1.854, Test accuracy: 61.04
Round  75, Train loss: 1.278, Test loss: 1.856, Test accuracy: 60.81
Round  76, Train loss: 1.279, Test loss: 1.856, Test accuracy: 60.88
Round  77, Train loss: 1.266, Test loss: 1.857, Test accuracy: 60.76
Round  78, Train loss: 1.232, Test loss: 1.856, Test accuracy: 60.85
Round  79, Train loss: 1.251, Test loss: 1.854, Test accuracy: 61.03
Round  80, Train loss: 1.257, Test loss: 1.861, Test accuracy: 60.38
Round  81, Train loss: 1.239, Test loss: 1.860, Test accuracy: 60.49
Round  82, Train loss: 1.193, Test loss: 1.860, Test accuracy: 60.44
Round  83, Train loss: 1.220, Test loss: 1.858, Test accuracy: 60.67
Round  84, Train loss: 1.293, Test loss: 1.861, Test accuracy: 60.28
Round  85, Train loss: 1.201, Test loss: 1.858, Test accuracy: 60.58
Round  86, Train loss: 1.216, Test loss: 1.860, Test accuracy: 60.46
Round  87, Train loss: 1.203, Test loss: 1.859, Test accuracy: 60.53
Round  88, Train loss: 1.248, Test loss: 1.858, Test accuracy: 60.69
Round  89, Train loss: 1.228, Test loss: 1.857, Test accuracy: 60.73
Round  90, Train loss: 1.228, Test loss: 1.861, Test accuracy: 60.28
Round  91, Train loss: 1.215, Test loss: 1.860, Test accuracy: 60.31
Round  92, Train loss: 1.208, Test loss: 1.859, Test accuracy: 60.43
Round  93, Train loss: 1.179, Test loss: 1.861, Test accuracy: 60.30
Round  94, Train loss: 1.264, Test loss: 1.855, Test accuracy: 60.85
Round  95, Train loss: 1.227, Test loss: 1.853, Test accuracy: 61.22
Round  96, Train loss: 1.263, Test loss: 1.855, Test accuracy: 60.93
Round  97, Train loss: 1.294, Test loss: 1.854, Test accuracy: 61.10
Round  98, Train loss: 1.239, Test loss: 1.863, Test accuracy: 60.12
Round  99, Train loss: 1.295, Test loss: 1.862, Test accuracy: 60.21
Final Round, Train loss: 1.230, Test loss: 1.858, Test accuracy: 60.51
Average accuracy final 10 rounds: 60.5755
5025.353011846542
[]
[25.6325, 37.5525, 42.305, 46.7225, 47.9725, 50.21, 52.9525, 55.2575, 57.22, 57.415, 57.1625, 60.0125, 61.07, 62.02, 62.6275, 62.9825, 63.065, 63.005, 63.555, 63.6425, 63.7, 64.2825, 64.4425, 64.3425, 64.6175, 64.9075, 64.9475, 64.9725, 64.6575, 65.1325, 64.75, 65.03, 64.505, 64.7425, 64.585, 64.4025, 64.345, 64.33, 64.225, 64.2625, 64.655, 64.15, 63.9, 63.6175, 63.64, 63.475, 63.5425, 63.5625, 63.995, 64.015, 63.85, 63.4375, 63.7625, 63.45, 63.2875, 63.1125, 62.7725, 62.7625, 62.7525, 62.405, 62.2, 61.9575, 62.495, 62.3125, 62.6325, 62.5475, 62.2725, 62.115, 62.0825, 61.875, 61.51, 61.76, 61.2275, 60.8675, 61.0425, 60.8125, 60.88, 60.755, 60.855, 61.0275, 60.3825, 60.4875, 60.4375, 60.67, 60.285, 60.5825, 60.4625, 60.53, 60.6925, 60.73, 60.285, 60.3125, 60.43, 60.2975, 60.855, 61.215, 60.9325, 61.0975, 60.12, 60.21, 60.5075]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.282, Test loss: 2.274, Test accuracy: 15.51
Round   1, Train loss: 2.207, Test loss: 2.209, Test accuracy: 23.59
Round   2, Train loss: 2.166, Test loss: 2.186, Test accuracy: 26.00
Round   3, Train loss: 1.925, Test loss: 2.129, Test accuracy: 32.14
Round   4, Train loss: 2.040, Test loss: 2.086, Test accuracy: 37.20
Round   5, Train loss: 2.038, Test loss: 2.141, Test accuracy: 31.95
Round   6, Train loss: 1.266, Test loss: 2.035, Test accuracy: 42.89
Round   7, Train loss: 1.495, Test loss: 2.023, Test accuracy: 44.22
Round   8, Train loss: 1.625, Test loss: 2.014, Test accuracy: 45.60
Round   9, Train loss: 1.357, Test loss: 2.012, Test accuracy: 45.30
Round  10, Train loss: 1.410, Test loss: 1.974, Test accuracy: 48.59
Round  11, Train loss: 1.334, Test loss: 1.983, Test accuracy: 47.66
Round  12, Train loss: 1.054, Test loss: 1.974, Test accuracy: 49.31
Round  13, Train loss: 1.239, Test loss: 1.953, Test accuracy: 52.31
Round  14, Train loss: 0.968, Test loss: 1.914, Test accuracy: 56.80
Round  15, Train loss: 0.628, Test loss: 1.844, Test accuracy: 62.95
Round  16, Train loss: 0.492, Test loss: 1.824, Test accuracy: 64.91
Round  17, Train loss: 0.107, Test loss: 1.814, Test accuracy: 65.93
Round  18, Train loss: -0.158, Test loss: 1.815, Test accuracy: 65.90
Round  19, Train loss: 0.571, Test loss: 1.788, Test accuracy: 68.75
Round  20, Train loss: 0.334, Test loss: 1.814, Test accuracy: 66.50
Round  21, Train loss: -0.024, Test loss: 1.767, Test accuracy: 70.25
Round  22, Train loss: -0.220, Test loss: 1.779, Test accuracy: 69.15
Round  23, Train loss: -0.457, Test loss: 1.746, Test accuracy: 71.80
Round  24, Train loss: -1.120, Test loss: 1.751, Test accuracy: 71.26
Round  25, Train loss: -0.565, Test loss: 1.744, Test accuracy: 71.89
Round  26, Train loss: -1.279, Test loss: 1.747, Test accuracy: 71.64
Round  27, Train loss: -0.864, Test loss: 1.748, Test accuracy: 71.52
Round  28, Train loss: -1.258, Test loss: 1.730, Test accuracy: 73.17
Round  29, Train loss: -1.466, Test loss: 1.727, Test accuracy: 73.54
Round  30, Train loss: -1.266, Test loss: 1.709, Test accuracy: 75.17
Round  31, Train loss: -1.339, Test loss: 1.697, Test accuracy: 76.41
Round  32, Train loss: -1.792, Test loss: 1.707, Test accuracy: 75.44
Round  33, Train loss: -1.748, Test loss: 1.717, Test accuracy: 74.34
Round  34, Train loss: -1.638, Test loss: 1.711, Test accuracy: 75.01
Round  35, Train loss: -1.814, Test loss: 1.704, Test accuracy: 75.64
Round  36, Train loss: -1.655, Test loss: 1.699, Test accuracy: 76.22
Round  37, Train loss: -2.445, Test loss: 1.705, Test accuracy: 75.60
Round  38, Train loss: -2.157, Test loss: 1.711, Test accuracy: 74.94
Round  39, Train loss: -1.503, Test loss: 1.706, Test accuracy: 75.41
Round  40, Train loss: -2.374, Test loss: 1.675, Test accuracy: 78.52
Round  41, Train loss: -2.728, Test loss: 1.694, Test accuracy: 76.62
Round  42, Train loss: -2.692, Test loss: 1.669, Test accuracy: 79.14
Round  43, Train loss: -3.032, Test loss: 1.664, Test accuracy: 79.64
Round  44, Train loss: -2.552, Test loss: 1.653, Test accuracy: 80.76
Round  45, Train loss: -3.323, Test loss: 1.661, Test accuracy: 79.97
Round  46, Train loss: -3.635, Test loss: 1.657, Test accuracy: 80.34
Round  47, Train loss: -3.784, Test loss: 1.659, Test accuracy: 80.20
Round  48, Train loss: -3.372, Test loss: 1.654, Test accuracy: 80.69
Round  49, Train loss: -3.127, Test loss: 1.662, Test accuracy: 79.92
Round  50, Train loss: -1.844, Test loss: 1.678, Test accuracy: 78.29
Round  51, Train loss: -2.613, Test loss: 1.660, Test accuracy: 80.01
Round  52, Train loss: -2.768, Test loss: 1.642, Test accuracy: 81.83
Round  53, Train loss: -2.885, Test loss: 1.642, Test accuracy: 81.88
Round  54, Train loss: -3.103, Test loss: 1.658, Test accuracy: 80.25
Round  55, Train loss: -3.311, Test loss: 1.651, Test accuracy: 80.98
Round  56, Train loss: -3.604, Test loss: 1.655, Test accuracy: 80.60
Round  57, Train loss: -2.435, Test loss: 1.663, Test accuracy: 79.81
Round  58, Train loss: -3.175, Test loss: 1.657, Test accuracy: 80.34
Round  59, Train loss: -2.880, Test loss: 1.667, Test accuracy: 79.39
Round  60, Train loss: -3.048, Test loss: 1.657, Test accuracy: 80.42
Round  61, Train loss: -2.989, Test loss: 1.667, Test accuracy: 79.34
Round  62, Train loss: -2.489, Test loss: 1.643, Test accuracy: 81.84
Round  63, Train loss: -2.831, Test loss: 1.638, Test accuracy: 82.22
Round  64, Train loss: -3.085, Test loss: 1.641, Test accuracy: 82.03
Round  65, Train loss: -3.780, Test loss: 1.652, Test accuracy: 80.88
Round  66, Train loss: -2.846, Test loss: 1.633, Test accuracy: 82.76
Round  67, Train loss: -2.268, Test loss: 1.639, Test accuracy: 82.14
Round  68, Train loss: -1.861, Test loss: 1.630, Test accuracy: 83.08
Round  69, Train loss: -3.231, Test loss: 1.633, Test accuracy: 82.72
Round  70, Train loss: -2.922, Test loss: 1.631, Test accuracy: 83.00
Round  71, Train loss: -3.374, Test loss: 1.638, Test accuracy: 82.22
Round  72, Train loss: -2.765, Test loss: 1.620, Test accuracy: 84.06
Round  73, Train loss: -2.139, Test loss: 1.617, Test accuracy: 84.33
Round  74, Train loss: -2.832, Test loss: 1.628, Test accuracy: 83.22
Round  75, Train loss: -3.803, Test loss: 1.628, Test accuracy: 83.30
Round  76, Train loss: -3.126, Test loss: 1.628, Test accuracy: 83.22
Round  77, Train loss: -3.660, Test loss: 1.627, Test accuracy: 83.35
Round  78, Train loss: -2.664, Test loss: 1.625, Test accuracy: 83.55
Round  79, Train loss: -2.389, Test loss: 1.601, Test accuracy: 85.96
Round  80, Train loss: -2.977, Test loss: 1.613, Test accuracy: 84.78
Round  81, Train loss: -2.875, Test loss: 1.611, Test accuracy: 85.00
Round  82, Train loss: -1.965, Test loss: 1.631, Test accuracy: 83.00
Round  83, Train loss: -2.530, Test loss: 1.633, Test accuracy: 82.69
Round  84, Train loss: -2.612, Test loss: 1.617, Test accuracy: 84.33
Round  85, Train loss: -2.353, Test loss: 1.622, Test accuracy: 83.84
Round  86, Train loss: -1.953, Test loss: 1.623, Test accuracy: 83.82
Round  87, Train loss: -3.139, Test loss: 1.611, Test accuracy: 84.98
Round  88, Train loss: -3.006, Test loss: 1.603, Test accuracy: 85.68
Round  89, Train loss: -3.051, Test loss: 1.618, Test accuracy: 84.20
Round  90, Train loss: -2.399, Test loss: 1.593, Test accuracy: 86.73
Round  91, Train loss: -2.591, Test loss: 1.600, Test accuracy: 86.06
Round  92, Train loss: -2.535, Test loss: 1.618, Test accuracy: 84.21
Round  93, Train loss: -2.872, Test loss: 1.608, Test accuracy: 85.28
Round  94, Train loss: -2.838, Test loss: 1.617, Test accuracy: 84.40
Round  95, Train loss: -2.354, Test loss: 1.619, Test accuracy: 84.18
Round  96, Train loss: -2.972, Test loss: 1.613, Test accuracy: 84.78
Round  97, Train loss: -2.644, Test loss: 1.610, Test accuracy: 85.09
Round  98, Train loss: -2.458, Test loss: 1.596, Test accuracy: 86.48
Round  99, Train loss: -3.201, Test loss: 1.600, Test accuracy: 86.07
Final Round, Train loss: 1.779, Test loss: 1.744, Test accuracy: 71.95
Average accuracy final 10 rounds: 85.327
Average global accuracy final 10 rounds: 85.327
3960.210889816284
[]
[15.5125, 23.5925, 25.9975, 32.1375, 37.195, 31.95, 42.89, 44.22, 45.5975, 45.305, 48.59, 47.665, 49.3075, 52.31, 56.8, 62.9475, 64.91, 65.9325, 65.9, 68.7525, 66.5025, 70.25, 69.1525, 71.7975, 71.26, 71.89, 71.645, 71.5175, 73.1725, 73.5375, 75.1675, 76.4075, 75.4425, 74.3425, 75.0075, 75.635, 76.225, 75.6025, 74.9375, 75.405, 78.52, 76.6175, 79.1425, 79.6375, 80.76, 79.975, 80.3425, 80.1975, 80.6925, 79.915, 78.2925, 80.0125, 81.8325, 81.875, 80.255, 80.98, 80.6, 79.815, 80.3375, 79.3925, 80.42, 79.3375, 81.8375, 82.22, 82.025, 80.88, 82.7575, 82.135, 83.085, 82.7225, 82.995, 82.2225, 84.06, 84.325, 83.225, 83.3, 83.2175, 83.35, 83.5475, 85.96, 84.7825, 85.0, 83.0, 82.69, 84.325, 83.8425, 83.8175, 84.9775, 85.6825, 84.2025, 86.735, 86.055, 84.2075, 85.285, 84.4, 84.1775, 84.775, 85.0875, 86.4775, 86.07, 71.9525]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.29
Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.30
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.29
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.30
Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.30
Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.30
Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.30
Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.29
Round   4, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.32
Round   4, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.29
Round   5, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.31
Round   5, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.28
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.32
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.29
Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.33
Round   7, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.29
Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.32
Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.29
Round   9, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.30
Round   9, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.29
Round  10, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.31
Round  10, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.29
Round  11, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.32
Round  11, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.30
Round  12, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.34
Round  12, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.34
Round  13, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.34
Round  13, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.34
Round  14, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.35
Round  14, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.35
Round  15, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.33
Round  15, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.33
Round  16, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.33
Round  16, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.34
Round  17, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.31
Round  17, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.34
Round  18, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.34
Round  18, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.34
Round  19, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.35
Round  19, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.36
Round  20, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.34
Round  20, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.34
Round  21, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.36
Round  21, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.33
Round  22, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.37
Round  22, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.33
Round  23, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.38
Round  23, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.34
Round  24, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.41
Round  24, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.36
Round  25, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.40
Round  25, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.37
Round  26, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.40
Round  26, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.37
Round  27, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.40
Round  27, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.38
Round  28, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.39
Round  28, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.38
Round  29, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.40
Round  29, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.38
Round  30, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.40
Round  30, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.38
Round  31, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.41
Round  31, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.40
Round  32, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.39
Round  32, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.41
Round  33, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.38
Round  33, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.43
Round  34, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.39
Round  34, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.42
Round  35, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.40
Round  35, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.42
Round  36, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.39
Round  36, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.40
Round  37, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.42
Round  37, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.40
Round  38, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.43
Round  38, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.41
Round  39, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.43
Round  39, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.41
Round  40, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.43
Round  40, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.41
Round  41, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.43
Round  41, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.40
Round  42, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.43
Round  42, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.40
Round  43, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.46
Round  43, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.43
Round  44, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.47
Round  44, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.44
Round  45, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.49
Round  45, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.46
Round  46, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.48
Round  46, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.44
Round  47, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.50
Round  47, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.51
Round  48, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.51
Round  48, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.51
Round  49, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.54
Round  49, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.51
Round  50, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.51
Round  50, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.52
Round  51, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.54
Round  51, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.52
Round  52, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.53
Round  52, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.53
Round  53, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.50
Round  53, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.53
Round  54, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.53
Round  54, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.56
Round  55, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.53
Round  55, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.56
Round  56, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.54
Round  56, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.56
Round  57, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.54
Round  57, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round  58, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.55
Round  58, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.59
Round  59, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.54
Round  59, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.56
Round  60, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.57
Round  60, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.56
Round  61, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.60
Round  61, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.54
Round  62, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.59
Round  62, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.55
Round  63, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.60
Round  63, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.55
Round  64, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.59
Round  64, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  65, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.61
Round  65, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.59
Round  66, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  66, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  67, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  67, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round  68, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  68, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  69, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  69, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  70, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.61
Round  70, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.54
Round  71, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.61
Round  71, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.57
Round  72, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.60
Round  72, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.56
Round  73, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.60
Round  73, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  74, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  74, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  75, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  75, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  76, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  76, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round  77, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  77, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Round  78, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.62
Round  78, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.61
Round  79, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.62
Round  79, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60
Round  80, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  80, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.61
Round  81, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  81, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  82, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  82, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  83, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.63
Round  83, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  84, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  84, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Round  85, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.62
Round  85, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.61
Round  86, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.62
Round  86, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.62
Round  87, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round  87, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.61
Round  88, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  88, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  89, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  89, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.69
Round  90, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  90, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Round  91, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round  91, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.67
Round  92, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  92, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  93, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  93, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  94, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  94, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  95, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  95, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  96, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.66
Round  96, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.68/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  97, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.69
Round  98, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.66
Round  98, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.67
Round  99, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  99, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Final Round, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.73
Final Round, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Average accuracy final 10 rounds: 10.65575 

Average global accuracy final 10 rounds: 10.66275 

4298.89391875267
[3.8820550441741943, 7.417370796203613, 10.934468746185303, 14.43026065826416, 18.09445095062256, 21.631858110427856, 25.223612070083618, 28.80903196334839, 32.34335207939148, 35.98638582229614, 39.592565059661865, 43.0699565410614, 46.586639404296875, 50.0409369468689, 53.43537783622742, 56.89012289047241, 60.35920286178589, 63.66270732879639, 67.1752724647522, 70.73445081710815, 74.2653706073761, 77.87012147903442, 81.46801614761353, 84.87819409370422, 88.34678554534912, 91.84233975410461, 95.28384613990784, 98.69724774360657, 102.13825678825378, 105.52614569664001, 109.00728392601013, 112.4573917388916, 117.40450191497803, 121.01136708259583, 124.61063814163208, 128.10612511634827, 131.57351803779602, 134.99148392677307, 138.4311158657074, 142.27575659751892, 146.0385720729828, 149.4963665008545, 152.90873622894287, 156.36018586158752, 159.9036545753479, 163.4613332748413, 167.04978275299072, 170.4721121788025, 173.93351340293884, 177.40527415275574, 180.83073115348816, 184.28637075424194, 187.70296001434326, 191.20739841461182, 194.89949440956116, 198.3714621067047, 201.78986644744873, 205.51477575302124, 208.9219422340393, 212.37628769874573, 215.83476281166077, 219.2987036705017, 222.6726746559143, 226.00356698036194, 229.4478418827057, 233.05974340438843, 236.60471844673157, 240.13116264343262, 243.61886548995972, 246.93421697616577, 250.34121894836426, 253.81441402435303, 257.41693115234375, 260.95539712905884, 264.4715702533722, 268.0664162635803, 271.5694968700409, 275.0600287914276, 278.51079535484314, 282.0236291885376, 285.5962176322937, 289.1457860469818, 292.6911277770996, 296.12134289741516, 299.61868619918823, 303.20585083961487, 306.78651547431946, 310.3383209705353, 313.83026027679443, 317.3554701805115, 320.8556673526764, 324.3287856578827, 327.82600355148315, 331.40504717826843, 335.0005407333374, 338.61271262168884, 342.14575815200806, 345.5484857559204, 349.0684473514557, 352.6005594730377, 354.38118600845337]
[10.2925, 10.29, 10.3025, 10.3, 10.3175, 10.315, 10.3175, 10.3325, 10.32, 10.305, 10.3075, 10.3225, 10.3375, 10.34, 10.3525, 10.33, 10.3275, 10.315, 10.3375, 10.3525, 10.3425, 10.36, 10.3725, 10.3825, 10.41, 10.4025, 10.3975, 10.4, 10.39, 10.3975, 10.405, 10.41, 10.3925, 10.3825, 10.39, 10.405, 10.395, 10.42, 10.425, 10.425, 10.425, 10.425, 10.43, 10.4575, 10.4725, 10.495, 10.4825, 10.4975, 10.51, 10.5375, 10.5125, 10.535, 10.525, 10.5025, 10.53, 10.5275, 10.54, 10.5375, 10.5475, 10.54, 10.57, 10.6, 10.5925, 10.595, 10.5875, 10.61, 10.6125, 10.605, 10.61, 10.605, 10.6125, 10.61, 10.6, 10.5975, 10.6075, 10.62, 10.61, 10.605, 10.615, 10.615, 10.6275, 10.6225, 10.6325, 10.6275, 10.6175, 10.62, 10.62, 10.6425, 10.635, 10.66, 10.65, 10.645, 10.6375, 10.6475, 10.66, 10.6725, 10.6625, 10.6675, 10.6575, 10.6575, 10.7325]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.17
Round   1, Train loss: 2.269, Test loss: 2.289, Test accuracy: 13.94
Round   2, Train loss: 2.080, Test loss: 2.282, Test accuracy: 15.52
Round   3, Train loss: 1.929, Test loss: 2.283, Test accuracy: 15.52
Round   4, Train loss: 2.061, Test loss: 2.279, Test accuracy: 16.34
Round   5, Train loss: 1.919, Test loss: 2.286, Test accuracy: 15.82
Round   6, Train loss: 1.985, Test loss: 2.289, Test accuracy: 15.51
Round   7, Train loss: 1.961, Test loss: 2.280, Test accuracy: 16.61
Round   8, Train loss: 1.966, Test loss: 2.284, Test accuracy: 15.38
Round   9, Train loss: 1.926, Test loss: 2.277, Test accuracy: 16.15
Round  10, Train loss: 1.963, Test loss: 2.279, Test accuracy: 16.14
Round  11, Train loss: 1.965, Test loss: 2.281, Test accuracy: 15.93
Round  12, Train loss: 1.927, Test loss: 2.280, Test accuracy: 16.23
Round  13, Train loss: 1.906, Test loss: 2.279, Test accuracy: 16.05
Round  14, Train loss: 1.939, Test loss: 2.276, Test accuracy: 16.16
Round  15, Train loss: 1.897, Test loss: 2.269, Test accuracy: 17.22
Round  16, Train loss: 1.915, Test loss: 2.278, Test accuracy: 16.48
Round  17, Train loss: 1.903, Test loss: 2.278, Test accuracy: 16.02
Round  18, Train loss: 1.855, Test loss: 2.283, Test accuracy: 15.91
Round  19, Train loss: 1.924, Test loss: 2.269, Test accuracy: 17.24
Round  20, Train loss: 1.780, Test loss: 2.273, Test accuracy: 17.04
Round  21, Train loss: 1.908, Test loss: 2.273, Test accuracy: 16.32
Round  22, Train loss: 1.880, Test loss: 2.263, Test accuracy: 18.20
Round  23, Train loss: 1.880, Test loss: 2.267, Test accuracy: 17.40
Round  24, Train loss: 1.849, Test loss: 2.279, Test accuracy: 16.32
Round  25, Train loss: 1.787, Test loss: 2.269, Test accuracy: 16.96
Round  26, Train loss: 1.818, Test loss: 2.267, Test accuracy: 17.69
Round  27, Train loss: 1.843, Test loss: 2.266, Test accuracy: 17.51
Round  28, Train loss: 1.738, Test loss: 2.271, Test accuracy: 16.62
Round  29, Train loss: 1.855, Test loss: 2.269, Test accuracy: 17.31
Round  30, Train loss: 1.756, Test loss: 2.282, Test accuracy: 16.00
Round  31, Train loss: 1.806, Test loss: 2.278, Test accuracy: 15.95
Round  32, Train loss: 1.866, Test loss: 2.279, Test accuracy: 15.99
Round  33, Train loss: 1.842, Test loss: 2.270, Test accuracy: 17.20
Round  34, Train loss: 1.765, Test loss: 2.271, Test accuracy: 16.71
Round  35, Train loss: 1.774, Test loss: 2.263, Test accuracy: 17.73
Round  36, Train loss: 1.802, Test loss: 2.272, Test accuracy: 16.51
Round  37, Train loss: 1.803, Test loss: 2.277, Test accuracy: 15.49
Round  38, Train loss: 1.691, Test loss: 2.267, Test accuracy: 17.17
Round  39, Train loss: 1.715, Test loss: 2.272, Test accuracy: 16.34
Round  40, Train loss: 1.742, Test loss: 2.271, Test accuracy: 16.32
Round  41, Train loss: 1.729, Test loss: 2.271, Test accuracy: 16.91
Round  42, Train loss: 1.768, Test loss: 2.263, Test accuracy: 18.32
Round  43, Train loss: 1.752, Test loss: 2.281, Test accuracy: 15.63
Round  44, Train loss: 1.738, Test loss: 2.266, Test accuracy: 17.29
Round  45, Train loss: 1.715, Test loss: 2.264, Test accuracy: 18.20
Round  46, Train loss: 1.709, Test loss: 2.266, Test accuracy: 17.28
Round  47, Train loss: 1.701, Test loss: 2.266, Test accuracy: 17.69
Round  48, Train loss: 1.711, Test loss: 2.294, Test accuracy: 14.81
Round  49, Train loss: 1.755, Test loss: 2.267, Test accuracy: 17.45
Round  50, Train loss: 1.688, Test loss: 2.278, Test accuracy: 15.71
Round  51, Train loss: 1.695, Test loss: 2.280, Test accuracy: 15.94
Round  52, Train loss: 1.780, Test loss: 2.267, Test accuracy: 17.62
Round  53, Train loss: 1.742, Test loss: 2.284, Test accuracy: 15.18
Round  54, Train loss: 1.694, Test loss: 2.281, Test accuracy: 15.92
Round  55, Train loss: 1.666, Test loss: 2.274, Test accuracy: 16.87
Round  56, Train loss: 1.697, Test loss: 2.272, Test accuracy: 16.42
Round  57, Train loss: 1.650, Test loss: 2.289, Test accuracy: 14.66
Round  58, Train loss: 1.685, Test loss: 2.293, Test accuracy: 14.51
Round  59, Train loss: 1.640, Test loss: 2.275, Test accuracy: 16.44
Round  60, Train loss: 1.735, Test loss: 2.262, Test accuracy: 17.82
Round  61, Train loss: 1.680, Test loss: 2.272, Test accuracy: 16.72
Round  62, Train loss: 1.649, Test loss: 2.276, Test accuracy: 16.30
Round  63, Train loss: 1.613, Test loss: 2.276, Test accuracy: 16.39
Round  64, Train loss: 1.613, Test loss: 2.276, Test accuracy: 16.45
Round  65, Train loss: 1.692, Test loss: 2.284, Test accuracy: 15.33
Round  66, Train loss: 1.630, Test loss: 2.275, Test accuracy: 16.48
Round  67, Train loss: 1.696, Test loss: 2.267, Test accuracy: 17.47
Round  68, Train loss: 1.665, Test loss: 2.266, Test accuracy: 17.54
Round  69, Train loss: 1.606, Test loss: 2.272, Test accuracy: 16.82
Round  70, Train loss: 1.630, Test loss: 2.270, Test accuracy: 16.94
Round  71, Train loss: 1.692, Test loss: 2.279, Test accuracy: 16.32
Round  72, Train loss: 1.645, Test loss: 2.280, Test accuracy: 15.86
Round  73, Train loss: 1.608, Test loss: 2.279, Test accuracy: 16.14
Round  74, Train loss: 1.673, Test loss: 2.272, Test accuracy: 17.23
Round  75, Train loss: 1.631, Test loss: 2.287, Test accuracy: 15.15
Round  76, Train loss: 1.738, Test loss: 2.280, Test accuracy: 16.01
Round  77, Train loss: 1.658, Test loss: 2.269, Test accuracy: 17.23
Round  78, Train loss: 1.603, Test loss: 2.271, Test accuracy: 16.61
Round  79, Train loss: 1.689, Test loss: 2.272, Test accuracy: 16.30
Round  80, Train loss: 1.584, Test loss: 2.276, Test accuracy: 16.43
Round  81, Train loss: 1.664, Test loss: 2.274, Test accuracy: 16.77
Round  82, Train loss: 1.647, Test loss: 2.272, Test accuracy: 16.12
Round  83, Train loss: 1.641, Test loss: 2.282, Test accuracy: 15.16
Round  84, Train loss: 1.595, Test loss: 2.270, Test accuracy: 16.93
Round  85, Train loss: 1.711, Test loss: 2.279, Test accuracy: 15.24
Round  86, Train loss: 1.579, Test loss: 2.280, Test accuracy: 16.07
Round  87, Train loss: 1.603, Test loss: 2.274, Test accuracy: 16.55
Round  88, Train loss: 1.626, Test loss: 2.272, Test accuracy: 16.47
Round  89, Train loss: 1.582, Test loss: 2.280, Test accuracy: 16.27
Round  90, Train loss: 1.602, Test loss: 2.276, Test accuracy: 16.78
Round  91, Train loss: 1.610, Test loss: 2.280, Test accuracy: 15.63
Round  92, Train loss: 1.654, Test loss: 2.273, Test accuracy: 16.58
Round  93, Train loss: 1.649, Test loss: 2.291, Test accuracy: 14.35
Round  94, Train loss: 1.626, Test loss: 2.273, Test accuracy: 16.57
Round  95, Train loss: 1.561, Test loss: 2.285, Test accuracy: 15.30
Round  96, Train loss: 1.597, Test loss: 2.284, Test accuracy: 15.40
Round  97, Train loss: 1.620, Test loss: 2.275, Test accuracy: 16.11
Round  98, Train loss: 1.553, Test loss: 2.274, Test accuracy: 16.53
Round  99, Train loss: 1.589, Test loss: 2.271, Test accuracy: 17.13
Final Round, Train loss: 1.595, Test loss: 2.259, Test accuracy: 17.74
Average accuracy final 10 rounds: 16.037000000000003
6274.204290866852
[9.056325435638428, 17.523892402648926, 25.889090061187744, 34.461453437805176, 42.98763394355774, 51.47302174568176, 59.984009742736816, 68.49750018119812, 77.00524187088013, 85.4728615283966, 93.94301104545593, 102.39742136001587, 110.96870565414429, 119.36098980903625, 127.80660605430603, 136.2099642753601, 144.62805366516113, 153.03312134742737, 161.52613520622253, 169.94939303398132, 178.3159065246582, 186.79414463043213, 195.29147243499756, 203.70344805717468, 212.3650918006897, 220.8073284626007, 229.33160519599915, 237.9530839920044, 246.6044955253601, 255.3236162662506, 263.93690943717957, 272.7297332286835, 281.50843620300293, 290.2059600353241, 298.8141119480133, 307.4013214111328, 316.01392459869385, 324.6275360584259, 334.079717874527, 342.6612329483032, 351.194198846817, 359.8223457336426, 368.4634976387024, 377.02197909355164, 385.6036117076874, 394.1434864997864, 402.7839424610138, 411.38150906562805, 419.9228928089142, 428.395227432251, 436.9235260486603, 445.5793967247009, 454.23610973358154, 462.99860286712646, 471.7083511352539, 480.33644914627075, 488.86434841156006, 497.4695291519165, 506.1366503238678, 514.7090561389923, 523.3194091320038, 531.9712898731232, 540.670640707016, 549.3447020053864, 558.0667309761047, 566.7080836296082, 575.3296275138855, 583.9498064517975, 592.6449687480927, 601.3122589588165, 609.9684362411499, 618.6497511863708, 627.293025970459, 635.8758571147919, 644.3608410358429, 652.9399967193604, 661.5698962211609, 670.0758852958679, 678.5833923816681, 687.1168172359467, 695.690643787384, 704.1925146579742, 712.7288155555725, 721.3093621730804, 729.8361263275146, 738.3812685012817, 746.8897433280945, 755.4070153236389, 763.9310402870178, 772.4431664943695, 780.8760335445404, 789.5824964046478, 798.1696708202362, 806.7356414794922, 815.3998029232025, 824.0128676891327, 832.6891527175903, 841.3337082862854, 850.0530307292938, 858.7747645378113, 860.9413039684296]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[11.1675, 13.9425, 15.52, 15.5175, 16.345, 15.8225, 15.5125, 16.6125, 15.375, 16.1475, 16.14, 15.9325, 16.235, 16.05, 16.16, 17.2175, 16.475, 16.025, 15.9075, 17.24, 17.0425, 16.315, 18.1975, 17.4025, 16.3225, 16.9575, 17.6925, 17.5125, 16.6175, 17.31, 16.0, 15.95, 15.99, 17.2, 16.71, 17.7325, 16.5075, 15.4925, 17.1675, 16.34, 16.3225, 16.9125, 18.32, 15.6275, 17.29, 18.1975, 17.2825, 17.6925, 14.8075, 17.455, 15.7075, 15.94, 17.625, 15.18, 15.9225, 16.8675, 16.42, 14.66, 14.5075, 16.4425, 17.82, 16.72, 16.2975, 16.395, 16.45, 15.3325, 16.4775, 17.47, 17.5425, 16.82, 16.9425, 16.315, 15.8625, 16.14, 17.225, 15.1525, 16.01, 17.225, 16.61, 16.3, 16.435, 16.7675, 16.1175, 15.16, 16.935, 15.245, 16.065, 16.5475, 16.4675, 16.27, 16.7775, 15.63, 16.5775, 14.345, 16.575, 15.3, 15.4, 16.105, 16.5325, 17.1275, 17.74]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.312, Test loss: 2.302, Test accuracy: 10.41
Round   1, Train loss: 2.287, Test loss: 2.298, Test accuracy: 13.01
Round   2, Train loss: 2.229, Test loss: 2.281, Test accuracy: 15.73
Round   3, Train loss: 2.166, Test loss: 2.248, Test accuracy: 20.46
Round   4, Train loss: 2.111, Test loss: 2.195, Test accuracy: 27.16
Round   5, Train loss: 2.034, Test loss: 2.158, Test accuracy: 31.48
Round   6, Train loss: 2.011, Test loss: 2.108, Test accuracy: 37.88
Round   7, Train loss: 1.994, Test loss: 2.065, Test accuracy: 41.45
Round   8, Train loss: 1.937, Test loss: 2.024, Test accuracy: 47.02
Round   9, Train loss: 1.923, Test loss: 2.001, Test accuracy: 49.52
Round  10, Train loss: 1.919, Test loss: 1.952, Test accuracy: 55.25
Round  11, Train loss: 1.925, Test loss: 1.950, Test accuracy: 57.32
Round  12, Train loss: 1.870, Test loss: 1.916, Test accuracy: 59.00
Round  13, Train loss: 1.875, Test loss: 1.902, Test accuracy: 60.19
Round  14, Train loss: 1.820, Test loss: 1.877, Test accuracy: 61.55
Round  15, Train loss: 1.821, Test loss: 1.867, Test accuracy: 63.51
Round  16, Train loss: 1.818, Test loss: 1.857, Test accuracy: 64.28
Round  17, Train loss: 1.821, Test loss: 1.841, Test accuracy: 66.40
Round  18, Train loss: 1.826, Test loss: 1.820, Test accuracy: 67.62
Round  19, Train loss: 1.800, Test loss: 1.816, Test accuracy: 69.05
Round  20, Train loss: 1.784, Test loss: 1.810, Test accuracy: 69.88
Round  21, Train loss: 1.743, Test loss: 1.795, Test accuracy: 70.83
Round  22, Train loss: 1.744, Test loss: 1.794, Test accuracy: 70.74
Round  23, Train loss: 1.761, Test loss: 1.765, Test accuracy: 72.65
Round  24, Train loss: 1.780, Test loss: 1.760, Test accuracy: 74.45
Round  25, Train loss: 1.759, Test loss: 1.745, Test accuracy: 75.49
Round  26, Train loss: 1.698, Test loss: 1.727, Test accuracy: 77.04
Round  27, Train loss: 1.688, Test loss: 1.724, Test accuracy: 77.44
Round  28, Train loss: 1.658, Test loss: 1.712, Test accuracy: 78.04
Round  29, Train loss: 1.695, Test loss: 1.702, Test accuracy: 78.94
Round  30, Train loss: 1.684, Test loss: 1.692, Test accuracy: 79.85
Round  31, Train loss: 1.665, Test loss: 1.695, Test accuracy: 79.96
Round  32, Train loss: 1.664, Test loss: 1.690, Test accuracy: 79.94
Round  33, Train loss: 1.646, Test loss: 1.676, Test accuracy: 80.74
Round  34, Train loss: 1.650, Test loss: 1.679, Test accuracy: 80.76
Round  35, Train loss: 1.673, Test loss: 1.676, Test accuracy: 81.25
Round  36, Train loss: 1.643, Test loss: 1.660, Test accuracy: 82.11
Round  37, Train loss: 1.634, Test loss: 1.663, Test accuracy: 82.22
Round  38, Train loss: 1.682, Test loss: 1.662, Test accuracy: 82.23
Round  39, Train loss: 1.613, Test loss: 1.654, Test accuracy: 82.76
Round  40, Train loss: 1.617, Test loss: 1.636, Test accuracy: 84.50
Round  41, Train loss: 1.580, Test loss: 1.636, Test accuracy: 84.48
Round  42, Train loss: 1.636, Test loss: 1.644, Test accuracy: 84.69
Round  43, Train loss: 1.568, Test loss: 1.630, Test accuracy: 85.47
Round  44, Train loss: 1.620, Test loss: 1.631, Test accuracy: 85.56
Round  45, Train loss: 1.597, Test loss: 1.629, Test accuracy: 85.59
Round  46, Train loss: 1.613, Test loss: 1.632, Test accuracy: 85.58
Round  47, Train loss: 1.622, Test loss: 1.623, Test accuracy: 86.01
Round  48, Train loss: 1.591, Test loss: 1.619, Test accuracy: 86.00
Round  49, Train loss: 1.550, Test loss: 1.613, Test accuracy: 86.48
Round  50, Train loss: 1.571, Test loss: 1.609, Test accuracy: 86.98
Round  51, Train loss: 1.605, Test loss: 1.610, Test accuracy: 87.37
Round  52, Train loss: 1.618, Test loss: 1.610, Test accuracy: 87.72
Round  53, Train loss: 1.610, Test loss: 1.606, Test accuracy: 88.14
Round  54, Train loss: 1.529, Test loss: 1.600, Test accuracy: 88.25
Round  55, Train loss: 1.572, Test loss: 1.591, Test accuracy: 88.69
Round  56, Train loss: 1.645, Test loss: 1.601, Test accuracy: 88.64
Round  57, Train loss: 1.559, Test loss: 1.595, Test accuracy: 88.71
Round  58, Train loss: 1.570, Test loss: 1.595, Test accuracy: 88.84
Round  59, Train loss: 1.543, Test loss: 1.591, Test accuracy: 88.85
Round  60, Train loss: 1.522, Test loss: 1.589, Test accuracy: 88.87
Round  61, Train loss: 1.584, Test loss: 1.592, Test accuracy: 88.83
Round  62, Train loss: 1.580, Test loss: 1.592, Test accuracy: 88.83
Round  63, Train loss: 1.581, Test loss: 1.581, Test accuracy: 89.73
Round  64, Train loss: 1.602, Test loss: 1.577, Test accuracy: 90.24
Round  65, Train loss: 1.527, Test loss: 1.569, Test accuracy: 90.72
Round  66, Train loss: 1.562, Test loss: 1.575, Test accuracy: 90.63
Round  67, Train loss: 1.562, Test loss: 1.575, Test accuracy: 90.69
Round  68, Train loss: 1.574, Test loss: 1.569, Test accuracy: 91.08
Round  69, Train loss: 1.549, Test loss: 1.570, Test accuracy: 91.16
Round  70, Train loss: 1.534, Test loss: 1.568, Test accuracy: 91.19
Round  71, Train loss: 1.529, Test loss: 1.563, Test accuracy: 91.65
Round  72, Train loss: 1.521, Test loss: 1.560, Test accuracy: 91.70
Round  73, Train loss: 1.542, Test loss: 1.557, Test accuracy: 92.18
Round  74, Train loss: 1.555, Test loss: 1.559, Test accuracy: 92.48
Round  75, Train loss: 1.548, Test loss: 1.565, Test accuracy: 92.47
Round  76, Train loss: 1.533, Test loss: 1.564, Test accuracy: 92.34
Round  77, Train loss: 1.550, Test loss: 1.560, Test accuracy: 92.48
Round  78, Train loss: 1.564, Test loss: 1.559, Test accuracy: 92.53
Round  79, Train loss: 1.522, Test loss: 1.550, Test accuracy: 92.64
Round  80, Train loss: 1.516, Test loss: 1.550, Test accuracy: 92.69
Round  81, Train loss: 1.517, Test loss: 1.551, Test accuracy: 92.67
Round  82, Train loss: 1.534, Test loss: 1.551, Test accuracy: 92.63
Round  83, Train loss: 1.544, Test loss: 1.553, Test accuracy: 92.72
Round  84, Train loss: 1.546, Test loss: 1.554, Test accuracy: 92.68
Round  85, Train loss: 1.503, Test loss: 1.548, Test accuracy: 92.76
Round  86, Train loss: 1.526, Test loss: 1.545, Test accuracy: 93.26
Round  87, Train loss: 1.487, Test loss: 1.545, Test accuracy: 93.29
Round  88, Train loss: 1.560, Test loss: 1.551, Test accuracy: 93.14
Round  89, Train loss: 1.559, Test loss: 1.550, Test accuracy: 93.28
Round  90, Train loss: 1.556, Test loss: 1.552, Test accuracy: 93.12
Round  91, Train loss: 1.516, Test loss: 1.546, Test accuracy: 93.25
Round  92, Train loss: 1.537, Test loss: 1.537, Test accuracy: 93.83
Round  93, Train loss: 1.488, Test loss: 1.537, Test accuracy: 93.86
Round  94, Train loss: 1.498, Test loss: 1.539, Test accuracy: 93.78/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.516, Test loss: 1.537, Test accuracy: 94.28
Round  96, Train loss: 1.532, Test loss: 1.536, Test accuracy: 94.35
Round  97, Train loss: 1.507, Test loss: 1.531, Test accuracy: 94.91
Round  98, Train loss: 1.498, Test loss: 1.532, Test accuracy: 94.86
Round  99, Train loss: 1.517, Test loss: 1.528, Test accuracy: 94.88
Final Round, Train loss: 1.499, Test loss: 1.523, Test accuracy: 95.36
Average accuracy final 10 rounds: 94.11250000000001
3676.816881418228
[4.7715983390808105, 9.435208797454834, 14.145880937576294, 18.85870862007141, 23.47604465484619, 28.107845783233643, 32.80754089355469, 37.47147512435913, 42.088751554489136, 46.83540153503418, 51.59242033958435, 55.97285270690918, 60.21773815155029, 64.5461356639862, 68.72265028953552, 73.01861715316772, 77.35904335975647, 81.58294725418091, 85.85294342041016, 90.12585878372192, 94.40671300888062, 98.66565990447998, 102.93940949440002, 107.29213809967041, 111.55040431022644, 115.96067643165588, 120.22866988182068, 124.4841980934143, 128.81861877441406, 133.08947348594666, 137.35492730140686, 141.7340443134308, 146.1232500076294, 150.35608077049255, 154.65561866760254, 158.99641942977905, 163.2275674343109, 167.4839572906494, 171.83199763298035, 176.21882677078247, 180.42385578155518, 184.80433583259583, 189.2202754020691, 193.4535322189331, 197.7046935558319, 202.12247371673584, 206.3711941242218, 210.7282431125641, 214.9011628627777, 219.07529211044312, 223.18314266204834, 227.32308149337769, 231.52916550636292, 235.6424639225006, 239.82380962371826, 244.06792902946472, 248.13934135437012, 252.65805339813232, 257.2150192260742, 261.73565006256104, 266.1110508441925, 270.6921761035919, 275.2916944026947, 279.81852197647095, 284.2745735645294, 288.66406655311584, 293.00263571739197, 297.0600743293762, 301.38363814353943, 305.82990765571594, 310.3627600669861, 314.64882707595825, 318.6583938598633, 322.8891360759735, 327.0190508365631, 330.9906997680664, 335.26024293899536, 339.43503761291504, 343.3895511627197, 347.5871148109436, 351.725909948349, 355.8291857242584, 359.9396095275879, 364.0079565048218, 367.98814821243286, 372.1871795654297, 376.24307227134705, 380.3346092700958, 384.4864330291748, 388.5990867614746, 393.92502188682556, 397.96492433547974, 402.1377866268158, 406.2435050010681, 410.519380569458, 414.6742124557495, 418.73402428627014, 422.9792492389679, 427.170859336853, 431.2390503883362, 432.8377318382263]
[10.4125, 13.0125, 15.7275, 20.46, 27.165, 31.4825, 37.885, 41.45, 47.02, 49.515, 55.2475, 57.32, 59.0, 60.185, 61.555, 63.5075, 64.2775, 66.4025, 67.6225, 69.05, 69.88, 70.8325, 70.74, 72.6525, 74.4475, 75.49, 77.04, 77.4375, 78.0425, 78.945, 79.85, 79.9575, 79.94, 80.7375, 80.76, 81.255, 82.115, 82.2225, 82.235, 82.7575, 84.5025, 84.48, 84.6925, 85.4675, 85.5575, 85.5925, 85.5825, 86.0125, 86.0025, 86.4825, 86.98, 87.37, 87.715, 88.1425, 88.2475, 88.69, 88.6375, 88.7125, 88.8375, 88.8525, 88.8675, 88.825, 88.835, 89.735, 90.2425, 90.72, 90.6325, 90.6875, 91.08, 91.16, 91.19, 91.6525, 91.6975, 92.18, 92.48, 92.465, 92.3425, 92.485, 92.5275, 92.645, 92.69, 92.6675, 92.6325, 92.7225, 92.6825, 92.7625, 93.26, 93.29, 93.1375, 93.275, 93.1175, 93.2525, 93.8325, 93.855, 93.785, 94.2825, 94.3525, 94.9075, 94.86, 94.88, 95.3575]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.319, Test loss: 2.303, Test accuracy: 9.36
Round   1, Train loss: 2.315, Test loss: 2.302, Test accuracy: 10.60
Round   2, Train loss: 2.310, Test loss: 2.302, Test accuracy: 9.81
Round   3, Train loss: 2.307, Test loss: 2.302, Test accuracy: 10.45
Round   4, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.12
Round   5, Train loss: 2.303, Test loss: 2.301, Test accuracy: 12.91
Round   6, Train loss: 2.297, Test loss: 2.299, Test accuracy: 11.32
Round   7, Train loss: 2.297, Test loss: 2.298, Test accuracy: 11.29
Round   8, Train loss: 2.290, Test loss: 2.294, Test accuracy: 13.10
Round   9, Train loss: 2.281, Test loss: 2.288, Test accuracy: 13.78
Round  10, Train loss: 2.269, Test loss: 2.282, Test accuracy: 15.67
Round  11, Train loss: 2.250, Test loss: 2.271, Test accuracy: 16.88
Round  12, Train loss: 2.244, Test loss: 2.263, Test accuracy: 18.85
Round  13, Train loss: 2.203, Test loss: 2.246, Test accuracy: 19.80
Round  14, Train loss: 2.212, Test loss: 2.237, Test accuracy: 21.63
Round  15, Train loss: 2.205, Test loss: 2.222, Test accuracy: 23.12
Round  16, Train loss: 2.195, Test loss: 2.209, Test accuracy: 24.78
Round  17, Train loss: 2.179, Test loss: 2.195, Test accuracy: 27.07
Round  18, Train loss: 2.158, Test loss: 2.179, Test accuracy: 29.50
Round  19, Train loss: 2.140, Test loss: 2.162, Test accuracy: 31.34
Round  20, Train loss: 2.129, Test loss: 2.150, Test accuracy: 32.29
Round  21, Train loss: 2.099, Test loss: 2.137, Test accuracy: 33.83
Round  22, Train loss: 2.111, Test loss: 2.123, Test accuracy: 34.95
Round  23, Train loss: 2.096, Test loss: 2.103, Test accuracy: 37.97
Round  24, Train loss: 2.033, Test loss: 2.085, Test accuracy: 40.09
Round  25, Train loss: 2.031, Test loss: 2.077, Test accuracy: 41.12
Round  26, Train loss: 2.006, Test loss: 2.065, Test accuracy: 42.37
Round  27, Train loss: 2.024, Test loss: 2.057, Test accuracy: 43.35
Round  28, Train loss: 2.012, Test loss: 2.044, Test accuracy: 44.42
Round  29, Train loss: 2.063, Test loss: 2.037, Test accuracy: 46.33
Round  30, Train loss: 2.031, Test loss: 2.029, Test accuracy: 47.27
Round  31, Train loss: 2.040, Test loss: 2.019, Test accuracy: 48.41
Round  32, Train loss: 2.027, Test loss: 2.005, Test accuracy: 49.70
Round  33, Train loss: 1.974, Test loss: 1.997, Test accuracy: 50.22
Round  34, Train loss: 2.009, Test loss: 1.990, Test accuracy: 50.77
Round  35, Train loss: 1.929, Test loss: 1.984, Test accuracy: 51.12
Round  36, Train loss: 1.969, Test loss: 1.965, Test accuracy: 52.93
Round  37, Train loss: 1.908, Test loss: 1.963, Test accuracy: 53.72
Round  38, Train loss: 1.937, Test loss: 1.953, Test accuracy: 54.73
Round  39, Train loss: 1.900, Test loss: 1.944, Test accuracy: 55.01
Round  40, Train loss: 1.928, Test loss: 1.929, Test accuracy: 56.81
Round  41, Train loss: 1.889, Test loss: 1.925, Test accuracy: 57.47
Round  42, Train loss: 1.928, Test loss: 1.912, Test accuracy: 59.19
Round  43, Train loss: 1.904, Test loss: 1.893, Test accuracy: 61.35
Round  44, Train loss: 1.868, Test loss: 1.886, Test accuracy: 62.50
Round  45, Train loss: 1.829, Test loss: 1.877, Test accuracy: 63.27
Round  46, Train loss: 1.890, Test loss: 1.871, Test accuracy: 64.31
Round  47, Train loss: 1.858, Test loss: 1.859, Test accuracy: 65.30
Round  48, Train loss: 1.798, Test loss: 1.857, Test accuracy: 65.38
Round  49, Train loss: 1.824, Test loss: 1.843, Test accuracy: 66.35
Round  50, Train loss: 1.861, Test loss: 1.836, Test accuracy: 66.75
Round  51, Train loss: 1.815, Test loss: 1.830, Test accuracy: 67.48
Round  52, Train loss: 1.841, Test loss: 1.822, Test accuracy: 68.58
Round  53, Train loss: 1.775, Test loss: 1.817, Test accuracy: 69.36
Round  54, Train loss: 1.798, Test loss: 1.812, Test accuracy: 69.55
Round  55, Train loss: 1.798, Test loss: 1.805, Test accuracy: 69.89
Round  56, Train loss: 1.829, Test loss: 1.801, Test accuracy: 70.70
Round  57, Train loss: 1.790, Test loss: 1.789, Test accuracy: 71.56
Round  58, Train loss: 1.746, Test loss: 1.790, Test accuracy: 72.02
Round  59, Train loss: 1.790, Test loss: 1.788, Test accuracy: 72.02
Round  60, Train loss: 1.743, Test loss: 1.782, Test accuracy: 72.29
Round  61, Train loss: 1.759, Test loss: 1.780, Test accuracy: 72.38
Round  62, Train loss: 1.753, Test loss: 1.769, Test accuracy: 72.84
Round  63, Train loss: 1.759, Test loss: 1.769, Test accuracy: 72.96
Round  64, Train loss: 1.741, Test loss: 1.764, Test accuracy: 73.42
Round  65, Train loss: 1.708, Test loss: 1.754, Test accuracy: 74.50
Round  66, Train loss: 1.735, Test loss: 1.759, Test accuracy: 74.38
Round  67, Train loss: 1.701, Test loss: 1.749, Test accuracy: 74.59
Round  68, Train loss: 1.710, Test loss: 1.741, Test accuracy: 75.51
Round  69, Train loss: 1.738, Test loss: 1.744, Test accuracy: 75.64
Round  70, Train loss: 1.746, Test loss: 1.744, Test accuracy: 75.82
Round  71, Train loss: 1.724, Test loss: 1.735, Test accuracy: 75.91
Round  72, Train loss: 1.706, Test loss: 1.737, Test accuracy: 75.93
Round  73, Train loss: 1.712, Test loss: 1.737, Test accuracy: 75.96
Round  74, Train loss: 1.707, Test loss: 1.732, Test accuracy: 76.49
Round  75, Train loss: 1.663, Test loss: 1.729, Test accuracy: 76.49
Round  76, Train loss: 1.733, Test loss: 1.728, Test accuracy: 76.75
Round  77, Train loss: 1.704, Test loss: 1.729, Test accuracy: 76.69
Round  78, Train loss: 1.689, Test loss: 1.725, Test accuracy: 76.92
Round  79, Train loss: 1.714, Test loss: 1.722, Test accuracy: 76.84
Round  80, Train loss: 1.642, Test loss: 1.714, Test accuracy: 77.10
Round  81, Train loss: 1.622, Test loss: 1.711, Test accuracy: 77.60
Round  82, Train loss: 1.690, Test loss: 1.712, Test accuracy: 78.05
Round  83, Train loss: 1.658, Test loss: 1.707, Test accuracy: 78.26
Round  84, Train loss: 1.635, Test loss: 1.707, Test accuracy: 78.30
Round  85, Train loss: 1.673, Test loss: 1.706, Test accuracy: 78.46
Round  86, Train loss: 1.675, Test loss: 1.699, Test accuracy: 79.44
Round  87, Train loss: 1.696, Test loss: 1.691, Test accuracy: 80.27
Round  88, Train loss: 1.661, Test loss: 1.691, Test accuracy: 80.57
Round  89, Train loss: 1.686, Test loss: 1.686, Test accuracy: 80.80
Round  90, Train loss: 1.697, Test loss: 1.688, Test accuracy: 80.85
Round  91, Train loss: 1.646, Test loss: 1.677, Test accuracy: 81.62
Round  92, Train loss: 1.711, Test loss: 1.684, Test accuracy: 82.00
Round  93, Train loss: 1.623, Test loss: 1.676, Test accuracy: 81.96/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.646, Test loss: 1.675, Test accuracy: 82.19
Round  95, Train loss: 1.665, Test loss: 1.676, Test accuracy: 82.21
Round  96, Train loss: 1.681, Test loss: 1.666, Test accuracy: 82.94
Round  97, Train loss: 1.620, Test loss: 1.658, Test accuracy: 83.75
Round  98, Train loss: 1.613, Test loss: 1.655, Test accuracy: 84.01
Round  99, Train loss: 1.622, Test loss: 1.648, Test accuracy: 84.83
Final Round, Train loss: 1.597, Test loss: 1.630, Test accuracy: 85.62
Average accuracy final 10 rounds: 82.63583333333334
1501.2684144973755
[1.4984002113342285, 2.996800422668457, 4.311079740524292, 5.625359058380127, 6.906175851821899, 8.186992645263672, 9.527042150497437, 10.867091655731201, 12.180260419845581, 13.493429183959961, 14.779253244400024, 16.065077304840088, 17.405630588531494, 18.7461838722229, 20.069289684295654, 21.392395496368408, 22.65808606147766, 23.923776626586914, 25.209038257598877, 26.49429988861084, 27.784826278686523, 29.075352668762207, 30.419838428497314, 31.764324188232422, 33.08835315704346, 34.41238212585449, 35.73799753189087, 37.063612937927246, 38.42529273033142, 39.786972522735596, 41.124621629714966, 42.462270736694336, 43.76563000679016, 45.068989276885986, 46.43130040168762, 47.79361152648926, 49.16987729072571, 50.54614305496216, 51.87463426589966, 53.20312547683716, 54.56037163734436, 55.91761779785156, 57.289454221725464, 58.661290645599365, 59.953031063079834, 61.2447714805603, 62.553338050842285, 63.86190462112427, 65.13889169692993, 66.4158787727356, 67.7978982925415, 69.17991781234741, 70.4887204170227, 71.797523021698, 73.1898181438446, 74.58211326599121, 75.94362473487854, 77.30513620376587, 78.58498692512512, 79.86483764648438, 81.21394968032837, 82.56306171417236, 83.96135091781616, 85.35964012145996, 86.73781371116638, 88.1159873008728, 89.40634298324585, 90.6966986656189, 92.09115505218506, 93.48561143875122, 94.84585857391357, 96.20610570907593, 97.51058721542358, 98.81506872177124, 100.15494632720947, 101.4948239326477, 102.89319348335266, 104.29156303405762, 105.63980197906494, 106.98804092407227, 108.25566792488098, 109.5232949256897, 110.8995954990387, 112.2758960723877, 113.6338939666748, 114.99189186096191, 116.31431770324707, 117.63674354553223, 119.02173638343811, 120.406729221344, 121.78403091430664, 123.16133260726929, 124.47498679161072, 125.78864097595215, 127.07752513885498, 128.3664093017578, 129.70589685440063, 131.04538440704346, 132.37638092041016, 133.70737743377686, 135.0008544921875, 136.29433155059814, 137.5938265323639, 138.89332151412964, 140.30871987342834, 141.72411823272705, 143.0391881465912, 144.35425806045532, 145.66574025154114, 146.97722244262695, 148.32647442817688, 149.6757264137268, 151.0586643218994, 152.44160223007202, 153.75267124176025, 155.0637402534485, 156.33136248588562, 157.59898471832275, 158.96617794036865, 160.33337116241455, 161.6833565235138, 163.03334188461304, 164.3120059967041, 165.59067010879517, 166.9702501296997, 168.34983015060425, 169.7459900379181, 171.14214992523193, 172.42200541496277, 173.7018609046936, 174.9905195236206, 176.2791781425476, 177.64569783210754, 179.01221752166748, 180.34133529663086, 181.67045307159424, 182.96463561058044, 184.25881814956665, 185.64334774017334, 187.02787733078003, 188.42866826057434, 189.82945919036865, 191.11033582687378, 192.3912124633789, 193.67365956306458, 194.95610666275024, 196.29535341262817, 197.6346001625061, 199.01809239387512, 200.40158462524414, 201.7133309841156, 203.02507734298706, 204.38777661323547, 205.7504758834839, 207.10007333755493, 208.44967079162598, 209.73356866836548, 211.01746654510498, 212.32824730873108, 213.63902807235718, 214.9928970336914, 216.34676599502563, 217.7281665802002, 219.10956716537476, 220.41290068626404, 221.71623420715332, 223.03039407730103, 224.34455394744873, 225.7002112865448, 227.05586862564087, 228.38249039649963, 229.7091121673584, 231.00530362129211, 232.30149507522583, 233.6364607810974, 234.971426486969, 236.3394651412964, 237.70750379562378, 238.9981107711792, 240.28871774673462, 241.61626410484314, 242.94381046295166, 244.30548119544983, 245.667151927948, 247.01325225830078, 248.35935258865356, 249.63378047943115, 250.90820837020874, 252.25380444526672, 253.5994005203247, 254.96431612968445, 256.3292317390442, 257.6644833087921, 258.99973487854004, 260.36544275283813, 261.73115062713623, 263.0907254219055, 264.4503002166748, 265.7215995788574, 266.99289894104004, 268.7008173465729, 270.4087357521057]
[9.358333333333333, 9.358333333333333, 10.6, 10.6, 9.808333333333334, 9.808333333333334, 10.45, 10.45, 12.125, 12.125, 12.908333333333333, 12.908333333333333, 11.325, 11.325, 11.291666666666666, 11.291666666666666, 13.1, 13.1, 13.775, 13.775, 15.666666666666666, 15.666666666666666, 16.875, 16.875, 18.85, 18.85, 19.8, 19.8, 21.633333333333333, 21.633333333333333, 23.116666666666667, 23.116666666666667, 24.783333333333335, 24.783333333333335, 27.075, 27.075, 29.5, 29.5, 31.341666666666665, 31.341666666666665, 32.291666666666664, 32.291666666666664, 33.833333333333336, 33.833333333333336, 34.95, 34.95, 37.96666666666667, 37.96666666666667, 40.09166666666667, 40.09166666666667, 41.125, 41.125, 42.36666666666667, 42.36666666666667, 43.35, 43.35, 44.425, 44.425, 46.325, 46.325, 47.275, 47.275, 48.40833333333333, 48.40833333333333, 49.7, 49.7, 50.21666666666667, 50.21666666666667, 50.775, 50.775, 51.11666666666667, 51.11666666666667, 52.93333333333333, 52.93333333333333, 53.71666666666667, 53.71666666666667, 54.733333333333334, 54.733333333333334, 55.00833333333333, 55.00833333333333, 56.80833333333333, 56.80833333333333, 57.46666666666667, 57.46666666666667, 59.19166666666667, 59.19166666666667, 61.35, 61.35, 62.5, 62.5, 63.266666666666666, 63.266666666666666, 64.30833333333334, 64.30833333333334, 65.3, 65.3, 65.375, 65.375, 66.35, 66.35, 66.75, 66.75, 67.48333333333333, 67.48333333333333, 68.575, 68.575, 69.35833333333333, 69.35833333333333, 69.55, 69.55, 69.89166666666667, 69.89166666666667, 70.7, 70.7, 71.55833333333334, 71.55833333333334, 72.01666666666667, 72.01666666666667, 72.01666666666667, 72.01666666666667, 72.29166666666667, 72.29166666666667, 72.375, 72.375, 72.84166666666667, 72.84166666666667, 72.95833333333333, 72.95833333333333, 73.425, 73.425, 74.5, 74.5, 74.38333333333334, 74.38333333333334, 74.59166666666667, 74.59166666666667, 75.50833333333334, 75.50833333333334, 75.64166666666667, 75.64166666666667, 75.81666666666666, 75.81666666666666, 75.90833333333333, 75.90833333333333, 75.93333333333334, 75.93333333333334, 75.95833333333333, 75.95833333333333, 76.49166666666666, 76.49166666666666, 76.49166666666666, 76.49166666666666, 76.75, 76.75, 76.69166666666666, 76.69166666666666, 76.925, 76.925, 76.84166666666667, 76.84166666666667, 77.1, 77.1, 77.6, 77.6, 78.05, 78.05, 78.25833333333334, 78.25833333333334, 78.3, 78.3, 78.45833333333333, 78.45833333333333, 79.44166666666666, 79.44166666666666, 80.26666666666667, 80.26666666666667, 80.56666666666666, 80.56666666666666, 80.8, 80.8, 80.85, 80.85, 81.61666666666666, 81.61666666666666, 82.0, 82.0, 81.95833333333333, 81.95833333333333, 82.19166666666666, 82.19166666666666, 82.20833333333333, 82.20833333333333, 82.94166666666666, 82.94166666666666, 83.75, 83.75, 84.00833333333334, 84.00833333333334, 84.83333333333333, 84.83333333333333, 85.625, 85.625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.985, Test loss: 1.712, Test accuracy: 39.01
Round   0, Global train loss: 1.985, Global test loss: 1.688, Global test accuracy: 40.56
Round   1, Train loss: 1.671, Test loss: 1.569, Test accuracy: 42.96
Round   1, Global train loss: 1.671, Global test loss: 1.471, Global test accuracy: 46.86
Round   2, Train loss: 1.547, Test loss: 1.477, Test accuracy: 46.64
Round   2, Global train loss: 1.547, Global test loss: 1.344, Global test accuracy: 51.84
Round   3, Train loss: 1.433, Test loss: 1.450, Test accuracy: 48.23
Round   3, Global train loss: 1.433, Global test loss: 1.260, Global test accuracy: 55.97
Round   4, Train loss: 1.347, Test loss: 1.408, Test accuracy: 49.42
Round   4, Global train loss: 1.347, Global test loss: 1.179, Global test accuracy: 58.62
Round   5, Train loss: 1.274, Test loss: 1.363, Test accuracy: 51.15
Round   5, Global train loss: 1.274, Global test loss: 1.110, Global test accuracy: 61.97
Round   6, Train loss: 1.201, Test loss: 1.313, Test accuracy: 53.35
Round   6, Global train loss: 1.201, Global test loss: 1.080, Global test accuracy: 62.46
Round   7, Train loss: 1.149, Test loss: 1.284, Test accuracy: 54.48
Round   7, Global train loss: 1.149, Global test loss: 1.014, Global test accuracy: 64.78
Round   8, Train loss: 1.102, Test loss: 1.241, Test accuracy: 56.01
Round   8, Global train loss: 1.102, Global test loss: 0.980, Global test accuracy: 66.05
Round   9, Train loss: 1.058, Test loss: 1.237, Test accuracy: 55.98
Round   9, Global train loss: 1.058, Global test loss: 0.960, Global test accuracy: 66.33
Round  10, Train loss: 1.011, Test loss: 1.189, Test accuracy: 58.03
Round  10, Global train loss: 1.011, Global test loss: 0.918, Global test accuracy: 68.47
Round  11, Train loss: 0.990, Test loss: 1.168, Test accuracy: 59.20
Round  11, Global train loss: 0.990, Global test loss: 0.910, Global test accuracy: 68.58
Round  12, Train loss: 0.966, Test loss: 1.131, Test accuracy: 60.74
Round  12, Global train loss: 0.966, Global test loss: 0.874, Global test accuracy: 70.06
Round  13, Train loss: 0.918, Test loss: 1.091, Test accuracy: 62.18
Round  13, Global train loss: 0.918, Global test loss: 0.860, Global test accuracy: 70.50
Round  14, Train loss: 0.881, Test loss: 1.056, Test accuracy: 63.77
Round  14, Global train loss: 0.881, Global test loss: 0.853, Global test accuracy: 70.73
Round  15, Train loss: 0.885, Test loss: 1.034, Test accuracy: 64.72
Round  15, Global train loss: 0.885, Global test loss: 0.840, Global test accuracy: 71.43
Round  16, Train loss: 0.856, Test loss: 1.023, Test accuracy: 65.33
Round  16, Global train loss: 0.856, Global test loss: 0.815, Global test accuracy: 72.54
Round  17, Train loss: 0.843, Test loss: 1.020, Test accuracy: 65.53
Round  17, Global train loss: 0.843, Global test loss: 0.812, Global test accuracy: 71.89
Round  18, Train loss: 0.819, Test loss: 1.007, Test accuracy: 65.89
Round  18, Global train loss: 0.819, Global test loss: 0.792, Global test accuracy: 72.95
Round  19, Train loss: 0.794, Test loss: 0.999, Test accuracy: 66.25
Round  19, Global train loss: 0.794, Global test loss: 0.779, Global test accuracy: 73.34
Round  20, Train loss: 0.768, Test loss: 0.990, Test accuracy: 66.94
Round  20, Global train loss: 0.768, Global test loss: 0.788, Global test accuracy: 73.22
Round  21, Train loss: 0.772, Test loss: 0.995, Test accuracy: 67.18
Round  21, Global train loss: 0.772, Global test loss: 0.776, Global test accuracy: 73.86
Round  22, Train loss: 0.745, Test loss: 0.985, Test accuracy: 67.54
Round  22, Global train loss: 0.745, Global test loss: 0.765, Global test accuracy: 73.83
Round  23, Train loss: 0.734, Test loss: 0.977, Test accuracy: 67.82
Round  23, Global train loss: 0.734, Global test loss: 0.763, Global test accuracy: 74.44
Round  24, Train loss: 0.720, Test loss: 0.988, Test accuracy: 67.64
Round  24, Global train loss: 0.720, Global test loss: 0.762, Global test accuracy: 74.23
Round  25, Train loss: 0.699, Test loss: 0.978, Test accuracy: 68.02
Round  25, Global train loss: 0.699, Global test loss: 0.758, Global test accuracy: 74.44
Round  26, Train loss: 0.696, Test loss: 0.986, Test accuracy: 68.00
Round  26, Global train loss: 0.696, Global test loss: 0.756, Global test accuracy: 74.35
Round  27, Train loss: 0.696, Test loss: 0.964, Test accuracy: 68.64
Round  27, Global train loss: 0.696, Global test loss: 0.739, Global test accuracy: 74.80
Round  28, Train loss: 0.699, Test loss: 0.950, Test accuracy: 68.98
Round  28, Global train loss: 0.699, Global test loss: 0.737, Global test accuracy: 74.90
Round  29, Train loss: 0.648, Test loss: 0.941, Test accuracy: 69.42
Round  29, Global train loss: 0.648, Global test loss: 0.736, Global test accuracy: 75.42
Round  30, Train loss: 0.647, Test loss: 0.940, Test accuracy: 69.58
Round  30, Global train loss: 0.647, Global test loss: 0.725, Global test accuracy: 75.48
Round  31, Train loss: 0.653, Test loss: 0.937, Test accuracy: 69.77
Round  31, Global train loss: 0.653, Global test loss: 0.729, Global test accuracy: 75.72
Round  32, Train loss: 0.632, Test loss: 0.940, Test accuracy: 69.92
Round  32, Global train loss: 0.632, Global test loss: 0.723, Global test accuracy: 75.89
Round  33, Train loss: 0.638, Test loss: 0.933, Test accuracy: 70.15
Round  33, Global train loss: 0.638, Global test loss: 0.716, Global test accuracy: 75.83
Round  34, Train loss: 0.629, Test loss: 0.932, Test accuracy: 70.41
Round  34, Global train loss: 0.629, Global test loss: 0.718, Global test accuracy: 76.10
Round  35, Train loss: 0.604, Test loss: 0.941, Test accuracy: 70.38
Round  35, Global train loss: 0.604, Global test loss: 0.715, Global test accuracy: 76.57
Round  36, Train loss: 0.569, Test loss: 0.943, Test accuracy: 70.39
Round  36, Global train loss: 0.569, Global test loss: 0.724, Global test accuracy: 76.14
Round  37, Train loss: 0.583, Test loss: 0.938, Test accuracy: 70.65
Round  37, Global train loss: 0.583, Global test loss: 0.720, Global test accuracy: 76.33
Round  38, Train loss: 0.594, Test loss: 0.938, Test accuracy: 70.85
Round  38, Global train loss: 0.594, Global test loss: 0.718, Global test accuracy: 76.56
Round  39, Train loss: 0.603, Test loss: 0.936, Test accuracy: 70.73
Round  39, Global train loss: 0.603, Global test loss: 0.714, Global test accuracy: 76.20
Round  40, Train loss: 0.581, Test loss: 0.932, Test accuracy: 71.00
Round  40, Global train loss: 0.581, Global test loss: 0.727, Global test accuracy: 76.27
Round  41, Train loss: 0.592, Test loss: 0.936, Test accuracy: 70.95
Round  41, Global train loss: 0.592, Global test loss: 0.719, Global test accuracy: 76.03
Round  42, Train loss: 0.568, Test loss: 0.928, Test accuracy: 71.15
Round  42, Global train loss: 0.568, Global test loss: 0.705, Global test accuracy: 76.65
Round  43, Train loss: 0.568, Test loss: 0.922, Test accuracy: 71.46
Round  43, Global train loss: 0.568, Global test loss: 0.706, Global test accuracy: 77.08
Round  44, Train loss: 0.544, Test loss: 0.921, Test accuracy: 71.55
Round  44, Global train loss: 0.544, Global test loss: 0.715, Global test accuracy: 77.06
Round  45, Train loss: 0.547, Test loss: 0.923, Test accuracy: 71.61
Round  45, Global train loss: 0.547, Global test loss: 0.721, Global test accuracy: 76.50
Round  46, Train loss: 0.521, Test loss: 0.930, Test accuracy: 71.33
Round  46, Global train loss: 0.521, Global test loss: 0.727, Global test accuracy: 76.76
Round  47, Train loss: 0.548, Test loss: 0.927, Test accuracy: 71.54
Round  47, Global train loss: 0.548, Global test loss: 0.719, Global test accuracy: 76.80
Round  48, Train loss: 0.555, Test loss: 0.919, Test accuracy: 72.02
Round  48, Global train loss: 0.555, Global test loss: 0.712, Global test accuracy: 76.88
Round  49, Train loss: 0.573, Test loss: 0.917, Test accuracy: 71.98
Round  49, Global train loss: 0.573, Global test loss: 0.692, Global test accuracy: 77.14
Round  50, Train loss: 0.508, Test loss: 0.923, Test accuracy: 71.99
Round  50, Global train loss: 0.508, Global test loss: 0.713, Global test accuracy: 77.14
Round  51, Train loss: 0.537, Test loss: 0.930, Test accuracy: 72.06
Round  51, Global train loss: 0.537, Global test loss: 0.699, Global test accuracy: 77.58
Round  52, Train loss: 0.504, Test loss: 0.918, Test accuracy: 72.11
Round  52, Global train loss: 0.504, Global test loss: 0.700, Global test accuracy: 77.20
Round  53, Train loss: 0.527, Test loss: 0.919, Test accuracy: 72.03
Round  53, Global train loss: 0.527, Global test loss: 0.700, Global test accuracy: 77.51
Round  54, Train loss: 0.510, Test loss: 0.918, Test accuracy: 72.14
Round  54, Global train loss: 0.510, Global test loss: 0.698, Global test accuracy: 77.30
Round  55, Train loss: 0.520, Test loss: 0.928, Test accuracy: 72.16
Round  55, Global train loss: 0.520, Global test loss: 0.699, Global test accuracy: 77.07
Round  56, Train loss: 0.473, Test loss: 0.924, Test accuracy: 72.32
Round  56, Global train loss: 0.473, Global test loss: 0.709, Global test accuracy: 77.31
Round  57, Train loss: 0.487, Test loss: 0.927, Test accuracy: 72.42
Round  57, Global train loss: 0.487, Global test loss: 0.710, Global test accuracy: 77.50
Round  58, Train loss: 0.501, Test loss: 0.926, Test accuracy: 72.58
Round  58, Global train loss: 0.501, Global test loss: 0.696, Global test accuracy: 78.05
Round  59, Train loss: 0.485, Test loss: 0.936, Test accuracy: 72.41
Round  59, Global train loss: 0.485, Global test loss: 0.704, Global test accuracy: 77.85
Round  60, Train loss: 0.474, Test loss: 0.934, Test accuracy: 72.67
Round  60, Global train loss: 0.474, Global test loss: 0.726, Global test accuracy: 77.18
Round  61, Train loss: 0.491, Test loss: 0.919, Test accuracy: 73.00
Round  61, Global train loss: 0.491, Global test loss: 0.705, Global test accuracy: 77.50
Round  62, Train loss: 0.470, Test loss: 0.929, Test accuracy: 72.61
Round  62, Global train loss: 0.470, Global test loss: 0.707, Global test accuracy: 77.76
Round  63, Train loss: 0.450, Test loss: 0.933, Test accuracy: 72.42
Round  63, Global train loss: 0.450, Global test loss: 0.717, Global test accuracy: 77.50
Round  64, Train loss: 0.485, Test loss: 0.935, Test accuracy: 72.74
Round  64, Global train loss: 0.485, Global test loss: 0.704, Global test accuracy: 77.63
Round  65, Train loss: 0.450, Test loss: 0.929, Test accuracy: 72.84
Round  65, Global train loss: 0.450, Global test loss: 0.711, Global test accuracy: 78.19
Round  66, Train loss: 0.477, Test loss: 0.931, Test accuracy: 72.87
Round  66, Global train loss: 0.477, Global test loss: 0.702, Global test accuracy: 78.31
Round  67, Train loss: 0.464, Test loss: 0.927, Test accuracy: 72.95
Round  67, Global train loss: 0.464, Global test loss: 0.704, Global test accuracy: 77.83
Round  68, Train loss: 0.439, Test loss: 0.925, Test accuracy: 72.92
Round  68, Global train loss: 0.439, Global test loss: 0.727, Global test accuracy: 77.51
Round  69, Train loss: 0.482, Test loss: 0.930, Test accuracy: 72.88
Round  69, Global train loss: 0.482, Global test loss: 0.709, Global test accuracy: 77.74
Round  70, Train loss: 0.484, Test loss: 0.931, Test accuracy: 72.71
Round  70, Global train loss: 0.484, Global test loss: 0.692, Global test accuracy: 78.12
Round  71, Train loss: 0.437, Test loss: 0.912, Test accuracy: 73.07
Round  71, Global train loss: 0.437, Global test loss: 0.712, Global test accuracy: 77.71
Round  72, Train loss: 0.432, Test loss: 0.903, Test accuracy: 73.20
Round  72, Global train loss: 0.432, Global test loss: 0.710, Global test accuracy: 78.33
Round  73, Train loss: 0.452, Test loss: 0.914, Test accuracy: 73.09
Round  73, Global train loss: 0.452, Global test loss: 0.715, Global test accuracy: 78.15
Round  74, Train loss: 0.463, Test loss: 0.929, Test accuracy: 73.08
Round  74, Global train loss: 0.463, Global test loss: 0.704, Global test accuracy: 78.09
Round  75, Train loss: 0.460, Test loss: 0.944, Test accuracy: 72.91
Round  75, Global train loss: 0.460, Global test loss: 0.710, Global test accuracy: 77.67
Round  76, Train loss: 0.436, Test loss: 0.937, Test accuracy: 73.16
Round  76, Global train loss: 0.436, Global test loss: 0.714, Global test accuracy: 78.08
Round  77, Train loss: 0.454, Test loss: 0.938, Test accuracy: 73.12
Round  77, Global train loss: 0.454, Global test loss: 0.715, Global test accuracy: 77.72
Round  78, Train loss: 0.446, Test loss: 0.933, Test accuracy: 73.38
Round  78, Global train loss: 0.446, Global test loss: 0.709, Global test accuracy: 78.29
Round  79, Train loss: 0.410, Test loss: 0.931, Test accuracy: 73.45
Round  79, Global train loss: 0.410, Global test loss: 0.718, Global test accuracy: 78.38
Round  80, Train loss: 0.448, Test loss: 0.932, Test accuracy: 73.49
Round  80, Global train loss: 0.448, Global test loss: 0.709, Global test accuracy: 78.36
Round  81, Train loss: 0.407, Test loss: 0.922, Test accuracy: 73.56
Round  81, Global train loss: 0.407, Global test loss: 0.710, Global test accuracy: 78.53
Round  82, Train loss: 0.449, Test loss: 0.922, Test accuracy: 73.54
Round  82, Global train loss: 0.449, Global test loss: 0.712, Global test accuracy: 78.36
Round  83, Train loss: 0.416, Test loss: 0.931, Test accuracy: 73.62
Round  83, Global train loss: 0.416, Global test loss: 0.726, Global test accuracy: 78.09
Round  84, Train loss: 0.468, Test loss: 0.936, Test accuracy: 73.54
Round  84, Global train loss: 0.468, Global test loss: 0.695, Global test accuracy: 78.26
Round  85, Train loss: 0.409, Test loss: 0.936, Test accuracy: 73.53
Round  85, Global train loss: 0.409, Global test loss: 0.713, Global test accuracy: 78.60
Round  86, Train loss: 0.442, Test loss: 0.930, Test accuracy: 73.63
Round  86, Global train loss: 0.442, Global test loss: 0.690, Global test accuracy: 78.93
Round  87, Train loss: 0.402, Test loss: 0.932, Test accuracy: 73.66
Round  87, Global train loss: 0.402, Global test loss: 0.729, Global test accuracy: 78.32
Round  88, Train loss: 0.404, Test loss: 0.935, Test accuracy: 73.85
Round  88, Global train loss: 0.404, Global test loss: 0.728, Global test accuracy: 78.34
Round  89, Train loss: 0.419, Test loss: 0.935, Test accuracy: 73.94
Round  89, Global train loss: 0.419, Global test loss: 0.713, Global test accuracy: 78.51
Round  90, Train loss: 0.416, Test loss: 0.929, Test accuracy: 74.00
Round  90, Global train loss: 0.416, Global test loss: 0.710, Global test accuracy: 78.83
Round  91, Train loss: 0.452, Test loss: 0.932, Test accuracy: 74.07
Round  91, Global train loss: 0.452, Global test loss: 0.708, Global test accuracy: 78.45
Round  92, Train loss: 0.394, Test loss: 0.932, Test accuracy: 73.95
Round  92, Global train loss: 0.394, Global test loss: 0.723, Global test accuracy: 78.19
Round  93, Train loss: 0.408, Test loss: 0.932, Test accuracy: 73.85
Round  93, Global train loss: 0.408, Global test loss: 0.716, Global test accuracy: 78.75
Round  94, Train loss: 0.375, Test loss: 0.940, Test accuracy: 73.74
Round  94, Global train loss: 0.375, Global test loss: 0.736, Global test accuracy: 78.77
Round  95, Train loss: 0.408, Test loss: 0.940, Test accuracy: 73.82
Round  95, Global train loss: 0.408, Global test loss: 0.729, Global test accuracy: 78.72
Round  96, Train loss: 0.389, Test loss: 0.953, Test accuracy: 73.81
Round  96, Global train loss: 0.389, Global test loss: 0.744, Global test accuracy: 78.25
Round  97, Train loss: 0.434, Test loss: 0.956, Test accuracy: 73.82
Round  97, Global train loss: 0.434, Global test loss: 0.716, Global test accuracy: 78.66
Round  98, Train loss: 0.389, Test loss: 0.945, Test accuracy: 74.08
Round  98, Global train loss: 0.389, Global test loss: 0.724, Global test accuracy: 78.65
Round  99, Train loss: 0.430, Test loss: 0.947, Test accuracy: 74.18
Round  99, Global train loss: 0.430, Global test loss: 0.707, Global test accuracy: 78.62
Final Round, Train loss: 0.243, Test loss: 1.022, Test accuracy: 74.54
Final Round, Global train loss: 0.243, Global test loss: 0.707, Global test accuracy: 78.62
Average accuracy final 10 rounds: 73.93249999999999 

Average global accuracy final 10 rounds: 78.58924999999999 

6136.373747587204
[4.898418426513672, 9.796836853027344, 14.726511716842651, 19.65618658065796, 24.57734227180481, 29.49849796295166, 34.351754903793335, 39.20501184463501, 44.033432960510254, 48.8618540763855, 53.763832807540894, 58.66581153869629, 63.643022298812866, 68.62023305892944, 73.57624197006226, 78.53225088119507, 83.4554796218872, 88.37870836257935, 93.30574584007263, 98.23278331756592, 103.13033723831177, 108.02789115905762, 112.91646313667297, 117.80503511428833, 122.66849827766418, 127.53196144104004, 132.3842580318451, 137.23655462265015, 141.9931936264038, 146.74983263015747, 151.6027934551239, 156.45575428009033, 161.31851959228516, 166.18128490447998, 171.01200366020203, 175.84272241592407, 180.6682550907135, 185.49378776550293, 190.36273503303528, 195.23168230056763, 200.09878587722778, 204.96588945388794, 209.88393020629883, 214.80197095870972, 219.7045464515686, 224.6071219444275, 229.53969049453735, 234.47225904464722, 239.4099476337433, 244.34763622283936, 249.1956820487976, 254.04372787475586, 258.90058422088623, 263.7574405670166, 268.6394832134247, 273.52152585983276, 278.39266872406006, 283.26381158828735, 288.16662883758545, 293.06944608688354, 297.97201895713806, 302.8745918273926, 307.73641753196716, 312.59824323654175, 317.5396263599396, 322.4810094833374, 327.35295701026917, 332.2249045372009, 337.0756347179413, 341.92636489868164, 346.79266023635864, 351.65895557403564, 356.51342582702637, 361.3678960800171, 366.2466621398926, 371.12542819976807, 376.017231464386, 380.9090347290039, 385.7634177207947, 390.61780071258545, 395.08506894111633, 399.5523371696472, 403.92057943344116, 408.2888216972351, 412.5658657550812, 416.84290981292725, 421.20195388793945, 425.56099796295166, 429.78332710266113, 434.0056562423706, 438.2645447254181, 442.5234332084656, 446.77352380752563, 451.0236144065857, 455.28898549079895, 459.5543565750122, 463.8187608718872, 468.0831651687622, 472.3406307697296, 476.598096370697, 480.84538078308105, 485.0926651954651, 489.3316054344177, 493.57054567337036, 497.8366639614105, 502.1027822494507, 506.3674097061157, 510.63203716278076, 514.9119803905487, 519.1919236183167, 523.4542636871338, 527.7166037559509, 531.9689409732819, 536.2212781906128, 540.4661767482758, 544.7110753059387, 548.9797298908234, 553.248384475708, 557.5543806552887, 561.8603768348694, 566.1333103179932, 570.406243801117, 574.6540315151215, 578.901819229126, 583.1473734378815, 587.392927646637, 591.6506087779999, 595.9082899093628, 600.1887226104736, 604.4691553115845, 608.715478181839, 612.9618010520935, 617.2240781784058, 621.486355304718, 625.7583038806915, 630.030252456665, 634.3074729442596, 638.5846934318542, 642.8298625946045, 647.0750317573547, 651.353777885437, 655.6325240135193, 659.9110670089722, 664.189610004425, 668.4279668331146, 672.6663236618042, 676.9229099750519, 681.1794962882996, 685.4365427494049, 689.6935892105103, 693.935399055481, 698.1772089004517, 702.4049007892609, 706.6325926780701, 710.8787577152252, 715.1249227523804, 719.375364780426, 723.6258068084717, 727.8857619762421, 732.1457171440125, 736.3700883388519, 740.5944595336914, 744.8217527866364, 749.0490460395813, 753.3006479740143, 757.5522499084473, 761.765305519104, 765.9783611297607, 770.2058091163635, 774.4332571029663, 778.6610317230225, 782.8888063430786, 787.1530508995056, 791.4172954559326, 795.6351237297058, 799.852952003479, 804.1008710861206, 808.3487901687622, 812.6005139350891, 816.852237701416, 821.0946471691132, 825.3370566368103, 829.5525808334351, 833.7681050300598, 838.0515985488892, 842.3350920677185, 846.5795905590057, 850.824089050293, 855.0476372241974, 859.2711853981018, 863.5134708881378, 867.7557563781738, 872.0112090110779, 876.2666616439819, 880.4870202541351, 884.7073788642883, 888.9217138290405, 893.1360487937927, 897.3701546192169, 901.6042604446411, 903.7478342056274, 905.8914079666138]
[39.0125, 39.0125, 42.9575, 42.9575, 46.6375, 46.6375, 48.2325, 48.2325, 49.42, 49.42, 51.1525, 51.1525, 53.3525, 53.3525, 54.48, 54.48, 56.0075, 56.0075, 55.985, 55.985, 58.0325, 58.0325, 59.1975, 59.1975, 60.745, 60.745, 62.1775, 62.1775, 63.77, 63.77, 64.72, 64.72, 65.335, 65.335, 65.5325, 65.5325, 65.8925, 65.8925, 66.25, 66.25, 66.935, 66.935, 67.18, 67.18, 67.54, 67.54, 67.82, 67.82, 67.64, 67.64, 68.0225, 68.0225, 67.9975, 67.9975, 68.635, 68.635, 68.98, 68.98, 69.425, 69.425, 69.585, 69.585, 69.77, 69.77, 69.9175, 69.9175, 70.1525, 70.1525, 70.4125, 70.4125, 70.3825, 70.3825, 70.3875, 70.3875, 70.65, 70.65, 70.8475, 70.8475, 70.735, 70.735, 71.0025, 71.0025, 70.9475, 70.9475, 71.1525, 71.1525, 71.4625, 71.4625, 71.5475, 71.5475, 71.61, 71.61, 71.3325, 71.3325, 71.5425, 71.5425, 72.02, 72.02, 71.985, 71.985, 71.9925, 71.9925, 72.065, 72.065, 72.1125, 72.1125, 72.025, 72.025, 72.14, 72.14, 72.155, 72.155, 72.32, 72.32, 72.4175, 72.4175, 72.575, 72.575, 72.41, 72.41, 72.6675, 72.6675, 72.9975, 72.9975, 72.6075, 72.6075, 72.425, 72.425, 72.7375, 72.7375, 72.8375, 72.8375, 72.87, 72.87, 72.955, 72.955, 72.925, 72.925, 72.8825, 72.8825, 72.7125, 72.7125, 73.0675, 73.0675, 73.2025, 73.2025, 73.0925, 73.0925, 73.085, 73.085, 72.905, 72.905, 73.155, 73.155, 73.125, 73.125, 73.3825, 73.3825, 73.4525, 73.4525, 73.4925, 73.4925, 73.56, 73.56, 73.5375, 73.5375, 73.625, 73.625, 73.54, 73.54, 73.525, 73.525, 73.6275, 73.6275, 73.6625, 73.6625, 73.85, 73.85, 73.9425, 73.9425, 74.0, 74.0, 74.0725, 74.0725, 73.9525, 73.9525, 73.8475, 73.8475, 73.7375, 73.7375, 73.8175, 73.8175, 73.81, 73.81, 73.8225, 73.8225, 74.0825, 74.0825, 74.1825, 74.1825, 74.54, 74.54]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.251, Test loss: 2.761, Test accuracy: 19.94
Round   1, Train loss: 0.919, Test loss: 1.912, Test accuracy: 35.90
Round   2, Train loss: 0.824, Test loss: 1.268, Test accuracy: 47.29
Round   3, Train loss: 0.741, Test loss: 1.348, Test accuracy: 55.40
Round   4, Train loss: 0.677, Test loss: 1.285, Test accuracy: 58.84
Round   5, Train loss: 0.684, Test loss: 0.994, Test accuracy: 61.34
Round   6, Train loss: 0.618, Test loss: 0.839, Test accuracy: 67.87
Round   7, Train loss: 0.610, Test loss: 0.767, Test accuracy: 70.02
Round   8, Train loss: 0.519, Test loss: 0.746, Test accuracy: 71.58
Round   9, Train loss: 0.599, Test loss: 0.656, Test accuracy: 74.22
Round  10, Train loss: 0.591, Test loss: 0.679, Test accuracy: 74.72
Round  11, Train loss: 0.531, Test loss: 0.509, Test accuracy: 79.27
Round  12, Train loss: 0.493, Test loss: 0.492, Test accuracy: 80.10
Round  13, Train loss: 0.451, Test loss: 0.478, Test accuracy: 80.62
Round  14, Train loss: 0.536, Test loss: 0.470, Test accuracy: 81.33
Round  15, Train loss: 0.476, Test loss: 0.451, Test accuracy: 81.98
Round  16, Train loss: 0.456, Test loss: 0.446, Test accuracy: 82.14
Round  17, Train loss: 0.460, Test loss: 0.446, Test accuracy: 81.94
Round  18, Train loss: 0.519, Test loss: 0.437, Test accuracy: 83.06
Round  19, Train loss: 0.390, Test loss: 0.427, Test accuracy: 82.93
Round  20, Train loss: 0.391, Test loss: 0.416, Test accuracy: 83.57
Round  21, Train loss: 0.386, Test loss: 0.404, Test accuracy: 83.63
Round  22, Train loss: 0.465, Test loss: 0.390, Test accuracy: 84.40
Round  23, Train loss: 0.356, Test loss: 0.390, Test accuracy: 84.14
Round  24, Train loss: 0.409, Test loss: 0.385, Test accuracy: 84.79
Round  25, Train loss: 0.377, Test loss: 0.387, Test accuracy: 84.14
Round  26, Train loss: 0.349, Test loss: 0.377, Test accuracy: 84.74
Round  27, Train loss: 0.309, Test loss: 0.379, Test accuracy: 84.47
Round  28, Train loss: 0.376, Test loss: 0.378, Test accuracy: 84.67
Round  29, Train loss: 0.435, Test loss: 0.380, Test accuracy: 84.53
Round  30, Train loss: 0.374, Test loss: 0.381, Test accuracy: 84.58
Round  31, Train loss: 0.319, Test loss: 0.370, Test accuracy: 85.24
Round  32, Train loss: 0.411, Test loss: 0.363, Test accuracy: 85.26
Round  33, Train loss: 0.408, Test loss: 0.355, Test accuracy: 85.88
Round  34, Train loss: 0.390, Test loss: 0.353, Test accuracy: 86.02
Round  35, Train loss: 0.297, Test loss: 0.350, Test accuracy: 86.12
Round  36, Train loss: 0.316, Test loss: 0.347, Test accuracy: 86.16
Round  37, Train loss: 0.357, Test loss: 0.347, Test accuracy: 86.30
Round  38, Train loss: 0.352, Test loss: 0.346, Test accuracy: 86.34
Round  39, Train loss: 0.382, Test loss: 0.340, Test accuracy: 86.65
Round  40, Train loss: 0.377, Test loss: 0.337, Test accuracy: 86.77
Round  41, Train loss: 0.327, Test loss: 0.337, Test accuracy: 86.73
Round  42, Train loss: 0.333, Test loss: 0.340, Test accuracy: 86.47
Round  43, Train loss: 0.366, Test loss: 0.337, Test accuracy: 86.67
Round  44, Train loss: 0.300, Test loss: 0.337, Test accuracy: 86.83
Round  45, Train loss: 0.263, Test loss: 0.338, Test accuracy: 86.65
Round  46, Train loss: 0.309, Test loss: 0.334, Test accuracy: 86.71
Round  47, Train loss: 0.259, Test loss: 0.333, Test accuracy: 86.72
Round  48, Train loss: 0.264, Test loss: 0.339, Test accuracy: 86.57
Round  49, Train loss: 0.331, Test loss: 0.331, Test accuracy: 87.05
Round  50, Train loss: 0.255, Test loss: 0.332, Test accuracy: 87.02
Round  51, Train loss: 0.326, Test loss: 0.332, Test accuracy: 87.08
Round  52, Train loss: 0.345, Test loss: 0.330, Test accuracy: 87.25
Round  53, Train loss: 0.262, Test loss: 0.329, Test accuracy: 87.12
Round  54, Train loss: 0.283, Test loss: 0.328, Test accuracy: 87.15
Round  55, Train loss: 0.284, Test loss: 0.324, Test accuracy: 87.25
Round  56, Train loss: 0.318, Test loss: 0.323, Test accuracy: 87.36
Round  57, Train loss: 0.303, Test loss: 0.325, Test accuracy: 87.15
Round  58, Train loss: 0.258, Test loss: 0.328, Test accuracy: 87.25
Round  59, Train loss: 0.321, Test loss: 0.323, Test accuracy: 87.42
Round  60, Train loss: 0.281, Test loss: 0.326, Test accuracy: 87.27
Round  61, Train loss: 0.300, Test loss: 0.326, Test accuracy: 87.38
Round  62, Train loss: 0.309, Test loss: 0.319, Test accuracy: 87.68
Round  63, Train loss: 0.295, Test loss: 0.329, Test accuracy: 87.10
Round  64, Train loss: 0.241, Test loss: 0.321, Test accuracy: 87.69
Round  65, Train loss: 0.196, Test loss: 0.326, Test accuracy: 87.51
Round  66, Train loss: 0.243, Test loss: 0.321, Test accuracy: 87.63
Round  67, Train loss: 0.305, Test loss: 0.315, Test accuracy: 87.91
Round  68, Train loss: 0.250, Test loss: 0.315, Test accuracy: 88.06
Round  69, Train loss: 0.280, Test loss: 0.317, Test accuracy: 87.88
Round  70, Train loss: 0.281, Test loss: 0.325, Test accuracy: 87.68
Round  71, Train loss: 0.284, Test loss: 0.317, Test accuracy: 87.73
Round  72, Train loss: 0.267, Test loss: 0.325, Test accuracy: 87.66
Round  73, Train loss: 0.249, Test loss: 0.321, Test accuracy: 87.71
Round  74, Train loss: 0.218, Test loss: 0.327, Test accuracy: 87.60
Round  75, Train loss: 0.236, Test loss: 0.326, Test accuracy: 87.58
Round  76, Train loss: 0.254, Test loss: 0.338, Test accuracy: 87.21
Round  77, Train loss: 0.273, Test loss: 0.325, Test accuracy: 87.51
Round  78, Train loss: 0.221, Test loss: 0.328, Test accuracy: 87.67
Round  79, Train loss: 0.240, Test loss: 0.329, Test accuracy: 87.65
Round  80, Train loss: 0.238, Test loss: 0.329, Test accuracy: 87.63
Round  81, Train loss: 0.198, Test loss: 0.335, Test accuracy: 87.46
Round  82, Train loss: 0.207, Test loss: 0.330, Test accuracy: 87.46
Round  83, Train loss: 0.275, Test loss: 0.330, Test accuracy: 87.68
Round  84, Train loss: 0.208, Test loss: 0.334, Test accuracy: 87.39
Round  85, Train loss: 0.245, Test loss: 0.320, Test accuracy: 87.97
Round  86, Train loss: 0.236, Test loss: 0.319, Test accuracy: 88.01
Round  87, Train loss: 0.277, Test loss: 0.316, Test accuracy: 88.15
Round  88, Train loss: 0.257, Test loss: 0.318, Test accuracy: 87.96
Round  89, Train loss: 0.209, Test loss: 0.319, Test accuracy: 88.03
Round  90, Train loss: 0.237, Test loss: 0.318, Test accuracy: 87.95
Round  91, Train loss: 0.239, Test loss: 0.316, Test accuracy: 88.15
Round  92, Train loss: 0.233, Test loss: 0.319, Test accuracy: 87.89
Round  93, Train loss: 0.232, Test loss: 0.320, Test accuracy: 88.04
Round  94, Train loss: 0.219, Test loss: 0.324, Test accuracy: 87.95
Round  95, Train loss: 0.237, Test loss: 0.320, Test accuracy: 88.06
Round  96, Train loss: 0.216, Test loss: 0.318, Test accuracy: 88.06
Round  97, Train loss: 0.202, Test loss: 0.321, Test accuracy: 88.01
Round  98, Train loss: 0.218, Test loss: 0.317, Test accuracy: 88.17
Round  99, Train loss: 0.209, Test loss: 0.320, Test accuracy: 88.07
Final Round, Train loss: 0.181, Test loss: 0.321, Test accuracy: 88.10
Average accuracy final 10 rounds: 88.03527777777778 

4237.565317869186
[4.116043329238892, 8.232086658477783, 12.264410257339478, 16.296733856201172, 20.343615531921387, 24.3904972076416, 28.445138454437256, 32.49977970123291, 36.53491544723511, 40.570051193237305, 44.598573446273804, 48.6270956993103, 52.58143854141235, 56.535781383514404, 60.53112554550171, 64.52646970748901, 68.51379013061523, 72.50111055374146, 76.4691002368927, 80.43708992004395, 84.33836078643799, 88.23963165283203, 92.22051644325256, 96.2014012336731, 100.18709707260132, 104.17279291152954, 107.81971478462219, 111.46663665771484, 115.07869124412537, 118.69074583053589, 122.31689691543579, 125.9430480003357, 129.58474373817444, 133.22643947601318, 136.85004425048828, 140.47364902496338, 144.07810258865356, 147.68255615234375, 151.30304431915283, 154.9235324859619, 158.58506631851196, 162.246600151062, 165.88419365882874, 169.52178716659546, 173.16446924209595, 176.80715131759644, 180.47364807128906, 184.1401448249817, 187.79557061195374, 191.45099639892578, 195.0524022579193, 198.65380811691284, 202.3306369781494, 206.007465839386, 209.7094156742096, 213.4113655090332, 217.0316927433014, 220.65201997756958, 224.29348158836365, 227.93494319915771, 231.55202722549438, 235.16911125183105, 238.77537488937378, 242.3816385269165, 246.0097939968109, 249.63794946670532, 253.23265433311462, 256.8273591995239, 260.43100094795227, 264.0346426963806, 267.6640064716339, 271.2933702468872, 274.9130437374115, 278.5327172279358, 282.1486716270447, 285.76462602615356, 289.40773034095764, 293.0508346557617, 296.6778314113617, 300.30482816696167, 303.9082450866699, 307.5116620063782, 311.1502754688263, 314.7888889312744, 318.37524485588074, 321.96160078048706, 325.53434324264526, 329.10708570480347, 332.7253634929657, 336.34364128112793, 339.9153552055359, 343.48706912994385, 347.08941197395325, 350.69175481796265, 354.2880747318268, 357.8843946456909, 361.4644548892975, 365.04451513290405, 368.66924953460693, 372.2939839363098, 375.9180085659027, 379.5420331954956, 383.10816717147827, 386.67430114746094, 390.26839089393616, 393.8624806404114, 397.4637734889984, 401.06506633758545, 404.62675499916077, 408.1884436607361, 411.7503833770752, 415.3123230934143, 418.92494010925293, 422.53755712509155, 426.11541080474854, 429.6932644844055, 433.2950382232666, 436.8968119621277, 440.48357105255127, 444.07033014297485, 447.64749813079834, 451.2246661186218, 454.8205692768097, 458.41647243499756, 462.00749254226685, 465.59851264953613, 469.17235493659973, 472.74619722366333, 476.3227665424347, 479.89933586120605, 483.47658467292786, 487.05383348464966, 490.62617921829224, 494.1985249519348, 497.80406379699707, 501.4096026420593, 504.976811170578, 508.5440196990967, 512.1205623149872, 515.6971049308777, 519.2982258796692, 522.8993468284607, 526.4886088371277, 530.0778708457947, 533.6435143947601, 537.2091579437256, 540.8181388378143, 544.4271197319031, 548.0298552513123, 551.6325907707214, 555.1801970005035, 558.7278032302856, 562.3198163509369, 565.9118294715881, 569.5051693916321, 573.098509311676, 576.6455566883087, 580.1926040649414, 583.7673120498657, 587.34202003479, 590.9195156097412, 594.4970111846924, 598.0436148643494, 601.5902185440063, 605.1909675598145, 608.7917165756226, 612.3895440101624, 615.9873714447021, 619.546532869339, 623.1056942939758, 626.6560168266296, 630.2063393592834, 633.8167154788971, 637.4270915985107, 641.0172061920166, 644.6073207855225, 648.1878383159637, 651.768355846405, 655.3826079368591, 658.9968600273132, 662.610830783844, 666.2248015403748, 669.7953288555145, 673.3658561706543, 676.9805092811584, 680.5951623916626, 684.1902527809143, 687.785343170166, 691.3760647773743, 694.9667863845825, 698.5304110050201, 702.0940356254578, 705.6831531524658, 709.2722706794739, 712.8644104003906, 716.4565501213074, 720.0195233821869, 723.5824966430664, 727.1626513004303, 730.7428059577942, 732.6514995098114, 734.5601930618286]
[19.93888888888889, 19.93888888888889, 35.9, 35.9, 47.294444444444444, 47.294444444444444, 55.4, 55.4, 58.84166666666667, 58.84166666666667, 61.33888888888889, 61.33888888888889, 67.87222222222222, 67.87222222222222, 70.02222222222223, 70.02222222222223, 71.575, 71.575, 74.22222222222223, 74.22222222222223, 74.72222222222223, 74.72222222222223, 79.26944444444445, 79.26944444444445, 80.1, 80.1, 80.61944444444444, 80.61944444444444, 81.33333333333333, 81.33333333333333, 81.97777777777777, 81.97777777777777, 82.14444444444445, 82.14444444444445, 81.94166666666666, 81.94166666666666, 83.05555555555556, 83.05555555555556, 82.93055555555556, 82.93055555555556, 83.56944444444444, 83.56944444444444, 83.63333333333334, 83.63333333333334, 84.40277777777777, 84.40277777777777, 84.14166666666667, 84.14166666666667, 84.79444444444445, 84.79444444444445, 84.13888888888889, 84.13888888888889, 84.74166666666666, 84.74166666666666, 84.46944444444445, 84.46944444444445, 84.67222222222222, 84.67222222222222, 84.53055555555555, 84.53055555555555, 84.575, 84.575, 85.24444444444444, 85.24444444444444, 85.25555555555556, 85.25555555555556, 85.88333333333334, 85.88333333333334, 86.01944444444445, 86.01944444444445, 86.11944444444444, 86.11944444444444, 86.15833333333333, 86.15833333333333, 86.30277777777778, 86.30277777777778, 86.34166666666667, 86.34166666666667, 86.64722222222223, 86.64722222222223, 86.76944444444445, 86.76944444444445, 86.73055555555555, 86.73055555555555, 86.475, 86.475, 86.66944444444445, 86.66944444444445, 86.83333333333333, 86.83333333333333, 86.65277777777777, 86.65277777777777, 86.71388888888889, 86.71388888888889, 86.71666666666667, 86.71666666666667, 86.57222222222222, 86.57222222222222, 87.05277777777778, 87.05277777777778, 87.01666666666667, 87.01666666666667, 87.08055555555555, 87.08055555555555, 87.25, 87.25, 87.125, 87.125, 87.14722222222223, 87.14722222222223, 87.25, 87.25, 87.35833333333333, 87.35833333333333, 87.15, 87.15, 87.25, 87.25, 87.41666666666667, 87.41666666666667, 87.27222222222223, 87.27222222222223, 87.37777777777778, 87.37777777777778, 87.68055555555556, 87.68055555555556, 87.09722222222223, 87.09722222222223, 87.69166666666666, 87.69166666666666, 87.5111111111111, 87.5111111111111, 87.63333333333334, 87.63333333333334, 87.91388888888889, 87.91388888888889, 88.06111111111112, 88.06111111111112, 87.88333333333334, 87.88333333333334, 87.67777777777778, 87.67777777777778, 87.73055555555555, 87.73055555555555, 87.66388888888889, 87.66388888888889, 87.71388888888889, 87.71388888888889, 87.59722222222223, 87.59722222222223, 87.575, 87.575, 87.20833333333333, 87.20833333333333, 87.5111111111111, 87.5111111111111, 87.66944444444445, 87.66944444444445, 87.65, 87.65, 87.63333333333334, 87.63333333333334, 87.45555555555555, 87.45555555555555, 87.45555555555555, 87.45555555555555, 87.67777777777778, 87.67777777777778, 87.39166666666667, 87.39166666666667, 87.97222222222223, 87.97222222222223, 88.00833333333334, 88.00833333333334, 88.14722222222223, 88.14722222222223, 87.96388888888889, 87.96388888888889, 88.02777777777777, 88.02777777777777, 87.95277777777778, 87.95277777777778, 88.15, 88.15, 87.8861111111111, 87.8861111111111, 88.04166666666667, 88.04166666666667, 87.95277777777778, 87.95277777777778, 88.06388888888888, 88.06388888888888, 88.06111111111112, 88.06111111111112, 88.0111111111111, 88.0111111111111, 88.16666666666667, 88.16666666666667, 88.06666666666666, 88.06666666666666, 88.10277777777777, 88.10277777777777]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.211, Test loss: 1.911, Test accuracy: 34.00
Round   1, Train loss: 1.859, Test loss: 1.626, Test accuracy: 42.49
Round   2, Train loss: 1.680, Test loss: 1.518, Test accuracy: 46.77
Round   3, Train loss: 1.568, Test loss: 1.460, Test accuracy: 50.51
Round   4, Train loss: 1.519, Test loss: 1.364, Test accuracy: 53.13
Round   5, Train loss: 1.424, Test loss: 1.341, Test accuracy: 54.50
Round   6, Train loss: 1.374, Test loss: 1.244, Test accuracy: 57.70
Round   7, Train loss: 1.334, Test loss: 1.194, Test accuracy: 59.53
Round   8, Train loss: 1.273, Test loss: 1.136, Test accuracy: 61.15
Round   9, Train loss: 1.229, Test loss: 1.115, Test accuracy: 62.87
Round  10, Train loss: 1.184, Test loss: 1.085, Test accuracy: 63.78
Round  11, Train loss: 1.136, Test loss: 1.071, Test accuracy: 64.17
Round  12, Train loss: 1.131, Test loss: 1.051, Test accuracy: 64.57
Round  13, Train loss: 1.090, Test loss: 1.019, Test accuracy: 65.55
Round  14, Train loss: 1.056, Test loss: 1.023, Test accuracy: 65.58
Round  15, Train loss: 1.032, Test loss: 0.985, Test accuracy: 66.77
Round  16, Train loss: 1.030, Test loss: 0.948, Test accuracy: 67.65
Round  17, Train loss: 0.992, Test loss: 0.945, Test accuracy: 68.18
Round  18, Train loss: 0.966, Test loss: 0.951, Test accuracy: 68.35
Round  19, Train loss: 0.960, Test loss: 0.936, Test accuracy: 68.42
Round  20, Train loss: 0.932, Test loss: 0.911, Test accuracy: 69.48
Round  21, Train loss: 0.932, Test loss: 0.894, Test accuracy: 69.88
Round  22, Train loss: 0.929, Test loss: 0.871, Test accuracy: 70.72
Round  23, Train loss: 0.905, Test loss: 0.848, Test accuracy: 71.65
Round  24, Train loss: 0.881, Test loss: 0.838, Test accuracy: 71.72
Round  25, Train loss: 0.879, Test loss: 0.842, Test accuracy: 71.70
Round  26, Train loss: 0.860, Test loss: 0.829, Test accuracy: 72.18
Round  27, Train loss: 0.828, Test loss: 0.821, Test accuracy: 72.60
Round  28, Train loss: 0.839, Test loss: 0.817, Test accuracy: 72.64
Round  29, Train loss: 0.815, Test loss: 0.810, Test accuracy: 72.88
Round  30, Train loss: 0.793, Test loss: 0.805, Test accuracy: 72.99
Round  31, Train loss: 0.805, Test loss: 0.792, Test accuracy: 73.48
Round  32, Train loss: 0.793, Test loss: 0.790, Test accuracy: 73.50
Round  33, Train loss: 0.781, Test loss: 0.786, Test accuracy: 73.38
Round  34, Train loss: 0.771, Test loss: 0.774, Test accuracy: 73.61
Round  35, Train loss: 0.769, Test loss: 0.759, Test accuracy: 74.07
Round  36, Train loss: 0.746, Test loss: 0.768, Test accuracy: 73.98
Round  37, Train loss: 0.740, Test loss: 0.768, Test accuracy: 74.06
Round  38, Train loss: 0.748, Test loss: 0.761, Test accuracy: 74.19
Round  39, Train loss: 0.713, Test loss: 0.756, Test accuracy: 74.22
Round  40, Train loss: 0.698, Test loss: 0.763, Test accuracy: 74.53
Round  41, Train loss: 0.679, Test loss: 0.760, Test accuracy: 74.18
Round  42, Train loss: 0.731, Test loss: 0.769, Test accuracy: 74.16
Round  43, Train loss: 0.692, Test loss: 0.756, Test accuracy: 74.48
Round  44, Train loss: 0.739, Test loss: 0.741, Test accuracy: 74.80
Round  45, Train loss: 0.671, Test loss: 0.746, Test accuracy: 74.94
Round  46, Train loss: 0.691, Test loss: 0.741, Test accuracy: 74.80
Round  47, Train loss: 0.685, Test loss: 0.734, Test accuracy: 75.25
Round  48, Train loss: 0.682, Test loss: 0.733, Test accuracy: 74.95
Round  49, Train loss: 0.677, Test loss: 0.727, Test accuracy: 75.11
Round  50, Train loss: 0.678, Test loss: 0.730, Test accuracy: 75.01
Round  51, Train loss: 0.678, Test loss: 0.725, Test accuracy: 75.47
Round  52, Train loss: 0.662, Test loss: 0.715, Test accuracy: 75.75
Round  53, Train loss: 0.649, Test loss: 0.723, Test accuracy: 75.72
Round  54, Train loss: 0.644, Test loss: 0.712, Test accuracy: 76.25
Round  55, Train loss: 0.624, Test loss: 0.724, Test accuracy: 75.84
Round  56, Train loss: 0.639, Test loss: 0.719, Test accuracy: 75.84
Round  57, Train loss: 0.648, Test loss: 0.711, Test accuracy: 76.17
Round  58, Train loss: 0.608, Test loss: 0.717, Test accuracy: 75.63
Round  59, Train loss: 0.622, Test loss: 0.717, Test accuracy: 75.51
Round  60, Train loss: 0.616, Test loss: 0.711, Test accuracy: 75.94
Round  61, Train loss: 0.621, Test loss: 0.699, Test accuracy: 76.36
Round  62, Train loss: 0.608, Test loss: 0.713, Test accuracy: 76.12
Round  63, Train loss: 0.639, Test loss: 0.701, Test accuracy: 76.33
Round  64, Train loss: 0.615, Test loss: 0.704, Test accuracy: 76.12
Round  65, Train loss: 0.641, Test loss: 0.712, Test accuracy: 76.26
Round  66, Train loss: 0.600, Test loss: 0.700, Test accuracy: 76.39
Round  67, Train loss: 0.613, Test loss: 0.709, Test accuracy: 76.28
Round  68, Train loss: 0.596, Test loss: 0.709, Test accuracy: 76.10
Round  69, Train loss: 0.597, Test loss: 0.698, Test accuracy: 76.39
Round  70, Train loss: 0.581, Test loss: 0.719, Test accuracy: 76.02
Round  71, Train loss: 0.588, Test loss: 0.705, Test accuracy: 76.33
Round  72, Train loss: 0.588, Test loss: 0.707, Test accuracy: 75.98
Round  73, Train loss: 0.572, Test loss: 0.707, Test accuracy: 76.31
Round  74, Train loss: 0.558, Test loss: 0.699, Test accuracy: 76.47
Round  75, Train loss: 0.577, Test loss: 0.693, Test accuracy: 76.53
Round  76, Train loss: 0.561, Test loss: 0.687, Test accuracy: 76.80
Round  77, Train loss: 0.558, Test loss: 0.690, Test accuracy: 76.94
Round  78, Train loss: 0.559, Test loss: 0.704, Test accuracy: 76.55
Round  79, Train loss: 0.593, Test loss: 0.696, Test accuracy: 76.69
Round  80, Train loss: 0.550, Test loss: 0.693, Test accuracy: 76.68
Round  81, Train loss: 0.554, Test loss: 0.702, Test accuracy: 76.59
Round  82, Train loss: 0.593, Test loss: 0.698, Test accuracy: 76.75
Round  83, Train loss: 0.543, Test loss: 0.695, Test accuracy: 76.84
Round  84, Train loss: 0.538, Test loss: 0.699, Test accuracy: 76.73
Round  85, Train loss: 0.567, Test loss: 0.689, Test accuracy: 76.95
Round  86, Train loss: 0.561, Test loss: 0.694, Test accuracy: 76.76
Round  87, Train loss: 0.550, Test loss: 0.689, Test accuracy: 77.12
Round  88, Train loss: 0.546, Test loss: 0.694, Test accuracy: 76.90
Round  89, Train loss: 0.531, Test loss: 0.695, Test accuracy: 76.86
Round  90, Train loss: 0.544, Test loss: 0.695, Test accuracy: 76.82
Round  91, Train loss: 0.543, Test loss: 0.691, Test accuracy: 76.86
Round  92, Train loss: 0.523, Test loss: 0.699, Test accuracy: 76.94
Round  93, Train loss: 0.516, Test loss: 0.698, Test accuracy: 77.03
Round  94, Train loss: 0.513, Test loss: 0.709, Test accuracy: 76.66
Round  95, Train loss: 0.519, Test loss: 0.701, Test accuracy: 76.76
Round  96, Train loss: 0.524, Test loss: 0.694, Test accuracy: 76.88
Round  97, Train loss: 0.523, Test loss: 0.705, Test accuracy: 76.78
Round  98, Train loss: 0.518, Test loss: 0.705, Test accuracy: 76.53
Round  99, Train loss: 0.507, Test loss: 0.707, Test accuracy: 76.53
Final Round, Train loss: 0.430, Test loss: 0.698, Test accuracy: 76.84
Average accuracy final 10 rounds: 76.77749999999999
5948.342165708542
[6.0848472118377686, 12.169694423675537, 17.97329568862915, 23.776896953582764, 29.556857109069824, 35.336817264556885, 41.15411901473999, 46.971420764923096, 52.791706800460815, 58.611992835998535, 64.35427832603455, 70.09656381607056, 75.9588611125946, 81.82115840911865, 87.63186740875244, 93.44257640838623, 99.26261472702026, 105.0826530456543, 110.92342066764832, 116.76418828964233, 122.62014698982239, 128.47610569000244, 134.32199501991272, 140.167884349823, 146.03167700767517, 151.89546966552734, 157.73051691055298, 163.5655641555786, 169.39700746536255, 175.22845077514648, 181.06035375595093, 186.89225673675537, 192.73916029930115, 198.58606386184692, 204.43122935295105, 210.27639484405518, 216.14425420761108, 222.012113571167, 227.9397156238556, 233.8673176765442, 239.79335618019104, 245.7193946838379, 251.62583589553833, 257.53227710723877, 263.4175136089325, 269.3027501106262, 275.2279658317566, 281.15318155288696, 287.04618406295776, 292.93918657302856, 298.8748679161072, 304.8105492591858, 310.7252390384674, 316.639928817749, 322.5771985054016, 328.5144681930542, 334.43520975112915, 340.3559513092041, 346.2967381477356, 352.2375249862671, 358.1523108482361, 364.0670967102051, 370.028315782547, 375.9895348548889, 381.9310746192932, 387.8726143836975, 393.8318018913269, 399.7909893989563, 405.7183265686035, 411.64566373825073, 417.61199736595154, 423.57833099365234, 429.55022263526917, 435.522114276886, 441.4620292186737, 447.4019441604614, 453.3364703655243, 459.27099657058716, 465.2060935497284, 471.14119052886963, 477.06868863105774, 482.99618673324585, 488.93531107902527, 494.8744354248047, 500.7960591316223, 506.71768283843994, 512.6398077011108, 518.5619325637817, 524.4718227386475, 530.3817129135132, 536.3105871677399, 542.2394614219666, 548.1959331035614, 554.1524047851562, 560.1172695159912, 566.0821342468262, 572.0647513866425, 578.0473685264587, 584.03440117836, 590.0214338302612, 595.9865798950195, 601.9517259597778, 607.9270715713501, 613.9024171829224, 619.8260934352875, 625.7497696876526, 631.9032821655273, 638.0567946434021, 644.1640174388885, 650.271240234375, 656.3838367462158, 662.4964332580566, 668.6275684833527, 674.7587037086487, 681.3764405250549, 687.9941773414612, 694.1302559375763, 700.2663345336914, 706.3600206375122, 712.453706741333, 718.5708999633789, 724.6880931854248, 730.8206133842468, 736.9531335830688, 743.1290278434753, 749.3049221038818, 755.4226024150848, 761.5402827262878, 767.6577961444855, 773.7753095626831, 779.9173982143402, 786.0594868659973, 792.1775953769684, 798.2957038879395, 804.4588057994843, 810.621907711029, 816.8362774848938, 823.0506472587585, 829.2572193145752, 835.4637913703918, 841.6359980106354, 847.8082046508789, 853.9868495464325, 860.1654944419861, 866.3950715065002, 872.6246485710144, 878.8149924278259, 885.0053362846375, 891.1690139770508, 897.3326916694641, 903.460687160492, 909.5886826515198, 915.7603240013123, 921.9319653511047, 928.0820472240448, 934.2321290969849, 940.4080286026001, 946.5839281082153, 952.7939820289612, 959.004035949707, 965.1878998279572, 971.3717637062073, 977.5623440742493, 983.7529244422913, 989.932466506958, 996.1120085716248, 1002.2739050388336, 1008.4358015060425, 1014.6559212207794, 1020.8760409355164, 1027.0398588180542, 1033.203676700592, 1039.3646812438965, 1045.525685787201, 1051.7522008419037, 1057.9787158966064, 1064.1515719890594, 1070.3244280815125, 1076.4054310321808, 1082.4864339828491, 1088.5534989833832, 1094.6205639839172, 1100.7254962921143, 1106.8304286003113, 1112.9019904136658, 1118.9735522270203, 1125.0066094398499, 1131.0396666526794, 1137.2449944019318, 1143.450322151184, 1149.4885890483856, 1155.5268559455872, 1161.5540707111359, 1167.5812854766846, 1173.6069564819336, 1179.6326274871826, 1185.745878458023, 1191.8591294288635, 1198.0306804180145, 1204.2022314071655, 1206.485414981842, 1208.7685985565186]
[34.0025, 34.0025, 42.495, 42.495, 46.7725, 46.7725, 50.5075, 50.5075, 53.1275, 53.1275, 54.4975, 54.4975, 57.705, 57.705, 59.53, 59.53, 61.1475, 61.1475, 62.865, 62.865, 63.785, 63.785, 64.165, 64.165, 64.57, 64.57, 65.5475, 65.5475, 65.5775, 65.5775, 66.7675, 66.7675, 67.6525, 67.6525, 68.1775, 68.1775, 68.3475, 68.3475, 68.425, 68.425, 69.48, 69.48, 69.88, 69.88, 70.715, 70.715, 71.6475, 71.6475, 71.72, 71.72, 71.7, 71.7, 72.18, 72.18, 72.6025, 72.6025, 72.635, 72.635, 72.875, 72.875, 72.9925, 72.9925, 73.4775, 73.4775, 73.4975, 73.4975, 73.38, 73.38, 73.61, 73.61, 74.0725, 74.0725, 73.98, 73.98, 74.065, 74.065, 74.1925, 74.1925, 74.2225, 74.2225, 74.5325, 74.5325, 74.1775, 74.1775, 74.16, 74.16, 74.4825, 74.4825, 74.8025, 74.8025, 74.94, 74.94, 74.8, 74.8, 75.2475, 75.2475, 74.955, 74.955, 75.1125, 75.1125, 75.0125, 75.0125, 75.4725, 75.4725, 75.745, 75.745, 75.72, 75.72, 76.2475, 76.2475, 75.8425, 75.8425, 75.8375, 75.8375, 76.1725, 76.1725, 75.6275, 75.6275, 75.5075, 75.5075, 75.94, 75.94, 76.355, 76.355, 76.12, 76.12, 76.33, 76.33, 76.125, 76.125, 76.2575, 76.2575, 76.3875, 76.3875, 76.28, 76.28, 76.0975, 76.0975, 76.395, 76.395, 76.0175, 76.0175, 76.33, 76.33, 75.9825, 75.9825, 76.3075, 76.3075, 76.465, 76.465, 76.5325, 76.5325, 76.7975, 76.7975, 76.94, 76.94, 76.5475, 76.5475, 76.69, 76.69, 76.68, 76.68, 76.5875, 76.5875, 76.7475, 76.7475, 76.8425, 76.8425, 76.735, 76.735, 76.9475, 76.9475, 76.7575, 76.7575, 77.1175, 77.1175, 76.8975, 76.8975, 76.86, 76.86, 76.82, 76.82, 76.86, 76.86, 76.9375, 76.9375, 77.0275, 77.0275, 76.6625, 76.6625, 76.7575, 76.7575, 76.875, 76.875, 76.775, 76.775, 76.525, 76.525, 76.535, 76.535, 76.84, 76.84]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.183, Test loss: 1.758, Test accuracy: 35.08
Round   1, Train loss: 1.813, Test loss: 1.484, Test accuracy: 45.73
Round   2, Train loss: 1.611, Test loss: 1.361, Test accuracy: 51.09
Round   3, Train loss: 1.506, Test loss: 1.269, Test accuracy: 55.19
Round   4, Train loss: 1.403, Test loss: 1.189, Test accuracy: 58.88
Round   5, Train loss: 1.341, Test loss: 1.127, Test accuracy: 60.81
Round   6, Train loss: 1.245, Test loss: 1.078, Test accuracy: 62.95
Round   7, Train loss: 1.194, Test loss: 1.040, Test accuracy: 64.04
Round   8, Train loss: 1.150, Test loss: 0.991, Test accuracy: 65.82
Round   9, Train loss: 1.100, Test loss: 0.957, Test accuracy: 66.60
Round  10, Train loss: 1.067, Test loss: 0.932, Test accuracy: 67.72
Round  11, Train loss: 1.040, Test loss: 0.912, Test accuracy: 68.93
Round  12, Train loss: 0.994, Test loss: 0.896, Test accuracy: 69.50
Round  13, Train loss: 0.986, Test loss: 0.856, Test accuracy: 70.43
Round  14, Train loss: 0.943, Test loss: 0.843, Test accuracy: 70.65
Round  15, Train loss: 0.907, Test loss: 0.840, Test accuracy: 71.11
Round  16, Train loss: 0.866, Test loss: 0.821, Test accuracy: 71.97
Round  17, Train loss: 0.897, Test loss: 0.799, Test accuracy: 72.30
Round  18, Train loss: 0.847, Test loss: 0.796, Test accuracy: 72.80
Round  19, Train loss: 0.793, Test loss: 0.780, Test accuracy: 73.36
Round  20, Train loss: 0.800, Test loss: 0.779, Test accuracy: 73.72
Round  21, Train loss: 0.797, Test loss: 0.766, Test accuracy: 74.19
Round  22, Train loss: 0.787, Test loss: 0.757, Test accuracy: 74.20
Round  23, Train loss: 0.762, Test loss: 0.755, Test accuracy: 74.00
Round  24, Train loss: 0.728, Test loss: 0.751, Test accuracy: 74.20
Round  25, Train loss: 0.696, Test loss: 0.758, Test accuracy: 74.80
Round  26, Train loss: 0.733, Test loss: 0.743, Test accuracy: 74.66
Round  27, Train loss: 0.725, Test loss: 0.733, Test accuracy: 75.22
Round  28, Train loss: 0.685, Test loss: 0.731, Test accuracy: 75.36
Round  29, Train loss: 0.668, Test loss: 0.729, Test accuracy: 75.67
Round  30, Train loss: 0.683, Test loss: 0.732, Test accuracy: 75.57
Round  31, Train loss: 0.686, Test loss: 0.727, Test accuracy: 75.78
Round  32, Train loss: 0.654, Test loss: 0.723, Test accuracy: 75.83
Round  33, Train loss: 0.656, Test loss: 0.706, Test accuracy: 76.44
Round  34, Train loss: 0.627, Test loss: 0.726, Test accuracy: 75.41
Round  35, Train loss: 0.609, Test loss: 0.717, Test accuracy: 76.19
Round  36, Train loss: 0.626, Test loss: 0.715, Test accuracy: 76.28
Round  37, Train loss: 0.576, Test loss: 0.729, Test accuracy: 75.50
Round  38, Train loss: 0.600, Test loss: 0.714, Test accuracy: 76.36
Round  39, Train loss: 0.600, Test loss: 0.709, Test accuracy: 76.53
Round  40, Train loss: 0.562, Test loss: 0.726, Test accuracy: 76.43
Round  41, Train loss: 0.597, Test loss: 0.711, Test accuracy: 76.75
Round  42, Train loss: 0.590, Test loss: 0.709, Test accuracy: 76.90
Round  43, Train loss: 0.583, Test loss: 0.694, Test accuracy: 77.04
Round  44, Train loss: 0.565, Test loss: 0.705, Test accuracy: 76.84
Round  45, Train loss: 0.582, Test loss: 0.703, Test accuracy: 77.08
Round  46, Train loss: 0.586, Test loss: 0.696, Test accuracy: 76.83
Round  47, Train loss: 0.545, Test loss: 0.696, Test accuracy: 77.09
Round  48, Train loss: 0.553, Test loss: 0.703, Test accuracy: 77.60
Round  49, Train loss: 0.525, Test loss: 0.702, Test accuracy: 77.80
Round  50, Train loss: 0.563, Test loss: 0.696, Test accuracy: 77.74
Round  51, Train loss: 0.508, Test loss: 0.719, Test accuracy: 77.40
Round  52, Train loss: 0.549, Test loss: 0.695, Test accuracy: 77.47
Round  53, Train loss: 0.491, Test loss: 0.708, Test accuracy: 77.32
Round  54, Train loss: 0.510, Test loss: 0.711, Test accuracy: 76.91
Round  55, Train loss: 0.534, Test loss: 0.699, Test accuracy: 77.43
Round  56, Train loss: 0.538, Test loss: 0.689, Test accuracy: 78.22
Round  57, Train loss: 0.495, Test loss: 0.690, Test accuracy: 77.94
Round  58, Train loss: 0.523, Test loss: 0.690, Test accuracy: 78.04
Round  59, Train loss: 0.488, Test loss: 0.702, Test accuracy: 77.81
Round  60, Train loss: 0.500, Test loss: 0.686, Test accuracy: 78.15
Round  61, Train loss: 0.513, Test loss: 0.685, Test accuracy: 78.74
Round  62, Train loss: 0.472, Test loss: 0.700, Test accuracy: 78.04
Round  63, Train loss: 0.474, Test loss: 0.694, Test accuracy: 78.70
Round  64, Train loss: 0.513, Test loss: 0.690, Test accuracy: 78.06
Round  65, Train loss: 0.438, Test loss: 0.702, Test accuracy: 78.42
Round  66, Train loss: 0.494, Test loss: 0.686, Test accuracy: 78.48
Round  67, Train loss: 0.461, Test loss: 0.695, Test accuracy: 78.56
Round  68, Train loss: 0.467, Test loss: 0.707, Test accuracy: 78.08
Round  69, Train loss: 0.479, Test loss: 0.699, Test accuracy: 78.35
Round  70, Train loss: 0.467, Test loss: 0.706, Test accuracy: 78.02
Round  71, Train loss: 0.462, Test loss: 0.690, Test accuracy: 78.30
Round  72, Train loss: 0.438, Test loss: 0.697, Test accuracy: 78.67
Round  73, Train loss: 0.463, Test loss: 0.713, Test accuracy: 78.33
Round  74, Train loss: 0.403, Test loss: 0.711, Test accuracy: 78.64
Round  75, Train loss: 0.404, Test loss: 0.715, Test accuracy: 78.51
Round  76, Train loss: 0.390, Test loss: 0.730, Test accuracy: 78.35
Round  77, Train loss: 0.442, Test loss: 0.723, Test accuracy: 78.30
Round  78, Train loss: 0.450, Test loss: 0.713, Test accuracy: 78.07
Round  79, Train loss: 0.472, Test loss: 0.704, Test accuracy: 78.38
Round  80, Train loss: 0.458, Test loss: 0.700, Test accuracy: 78.62
Round  81, Train loss: 0.421, Test loss: 0.708, Test accuracy: 78.55
Round  82, Train loss: 0.430, Test loss: 0.703, Test accuracy: 78.77
Round  83, Train loss: 0.396, Test loss: 0.715, Test accuracy: 78.62
Round  84, Train loss: 0.458, Test loss: 0.709, Test accuracy: 78.52
Round  85, Train loss: 0.412, Test loss: 0.713, Test accuracy: 78.83
Round  86, Train loss: 0.415, Test loss: 0.714, Test accuracy: 78.80
Round  87, Train loss: 0.447, Test loss: 0.704, Test accuracy: 78.72
Round  88, Train loss: 0.410, Test loss: 0.714, Test accuracy: 78.92
Round  89, Train loss: 0.425, Test loss: 0.704, Test accuracy: 79.18
Round  90, Train loss: 0.401, Test loss: 0.718, Test accuracy: 78.78
Round  91, Train loss: 0.392, Test loss: 0.717, Test accuracy: 78.50
Round  92, Train loss: 0.440, Test loss: 0.704, Test accuracy: 78.46
Round  93, Train loss: 0.385, Test loss: 0.715, Test accuracy: 78.84
Round  94, Train loss: 0.416, Test loss: 0.707, Test accuracy: 78.90
Round  95, Train loss: 0.415, Test loss: 0.707, Test accuracy: 78.92
Round  96, Train loss: 0.407, Test loss: 0.706, Test accuracy: 78.90
Round  97, Train loss: 0.395, Test loss: 0.726, Test accuracy: 78.98
Round  98, Train loss: 0.398, Test loss: 0.712, Test accuracy: 79.33
Round  99, Train loss: 0.393, Test loss: 0.713, Test accuracy: 79.02
Final Round, Train loss: 0.312, Test loss: 0.721, Test accuracy: 79.38
Average accuracy final 10 rounds: 78.86374999999998
8329.506166696548
[13.42375898361206, 26.860618829727173, 40.17368245124817, 53.39473605155945, 66.65336632728577, 79.94672131538391, 93.51236510276794, 106.70756793022156, 120.0006012916565, 133.7318570613861, 147.07814049720764, 160.41828322410583, 173.65158343315125, 188.26072788238525, 202.1369411945343, 220.64664673805237, 234.17125368118286, 249.78452491760254, 263.2720425128937, 276.67060351371765, 289.95497608184814, 303.11655354499817, 314.46813321113586, 326.6472716331482, 337.97935819625854, 349.2051820755005, 360.56728172302246, 372.1763164997101, 383.66024446487427, 395.0153555870056, 406.32678151130676, 417.61222100257874, 429.14998745918274, 440.6230080127716, 452.01955342292786, 464.43701457977295, 475.9777047634125, 487.5119080543518, 498.983610868454, 510.3952841758728, 521.7910635471344, 533.2678837776184, 544.7525289058685, 556.150799036026, 567.6512861251831, 579.0929806232452, 590.4861135482788, 601.8791878223419, 613.3450536727905, 624.7821598052979, 636.899566411972, 648.3406717777252, 659.8062806129456, 671.4076495170593, 683.0039584636688, 697.5736389160156, 709.1921007633209, 721.2856669425964, 734.265367269516, 745.9239709377289, 757.4764320850372, 769.217175245285, 780.7297487258911, 792.3729386329651, 804.0856051445007, 815.7737319469452, 827.3915848731995, 839.0710391998291, 850.6938934326172, 862.2831032276154, 873.900817155838, 885.5687670707703, 897.2631640434265, 908.9640200138092, 920.6747295856476, 932.3474617004395, 943.8921585083008, 955.47656416893, 967.1064615249634, 978.8083693981171, 990.4868216514587, 1002.2301008701324, 1013.9765033721924, 1025.6479275226593, 1037.464281320572, 1049.2131383419037, 1061.054827928543, 1072.7946331501007, 1084.5705440044403, 1096.3350942134857, 1108.2271974086761, 1119.9376244544983, 1131.621991634369, 1143.1711463928223, 1154.8311669826508, 1166.498708486557, 1178.2604653835297, 1189.9586684703827, 1201.6525876522064, 1213.4126052856445, 1216.379724740982]
[35.0825, 45.73, 51.0925, 55.1925, 58.8825, 60.81, 62.945, 64.04, 65.8225, 66.6025, 67.725, 68.9325, 69.505, 70.43, 70.65, 71.1075, 71.975, 72.3, 72.8, 73.355, 73.715, 74.185, 74.1975, 74.005, 74.1975, 74.8025, 74.655, 75.22, 75.355, 75.665, 75.5675, 75.785, 75.8325, 76.445, 75.4075, 76.1875, 76.28, 75.5, 76.3625, 76.53, 76.4275, 76.745, 76.9, 77.0425, 76.8375, 77.0775, 76.8275, 77.09, 77.5975, 77.8, 77.74, 77.4, 77.4675, 77.3225, 76.905, 77.43, 78.2225, 77.94, 78.04, 77.81, 78.1525, 78.7375, 78.0375, 78.705, 78.065, 78.42, 78.48, 78.565, 78.085, 78.3525, 78.0225, 78.3, 78.6675, 78.33, 78.64, 78.51, 78.3475, 78.3, 78.0675, 78.375, 78.62, 78.5525, 78.77, 78.6225, 78.52, 78.835, 78.795, 78.72, 78.925, 79.1775, 78.78, 78.5, 78.4625, 78.84, 78.8975, 78.9225, 78.9025, 78.985, 79.33, 79.0175, 79.3825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.306, Test loss: 2.305, Test accuracy: 9.94
Round   0, Global train loss: 2.306, Global test loss: 2.305, Global test accuracy: 9.97
Round   1, Train loss: 2.305, Test loss: 2.305, Test accuracy: 9.92
Round   1, Global train loss: 2.305, Global test loss: 2.305, Global test accuracy: 9.96
Round   2, Train loss: 2.305, Test loss: 2.304, Test accuracy: 9.93
Round   2, Global train loss: 2.305, Global test loss: 2.304, Global test accuracy: 9.92
Round   3, Train loss: 2.305, Test loss: 2.304, Test accuracy: 9.89
Round   3, Global train loss: 2.305, Global test loss: 2.304, Global test accuracy: 9.92
Round   4, Train loss: 2.305, Test loss: 2.304, Test accuracy: 9.91
Round   4, Global train loss: 2.305, Global test loss: 2.303, Global test accuracy: 9.98
Round   5, Train loss: 2.304, Test loss: 2.304, Test accuracy: 9.94
Round   5, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 9.97
Round   6, Train loss: 2.304, Test loss: 2.303, Test accuracy: 9.94
Round   6, Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 9.95
Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.95
Round   7, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 9.96
Round   8, Train loss: 2.303, Test loss: 2.302, Test accuracy: 9.95
Round   8, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 9.96
Round   9, Train loss: 2.303, Test loss: 2.302, Test accuracy: 9.95
Round   9, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 9.97
Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.96
Round  10, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 9.97
Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.96
Round  11, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 9.97
Round  12, Train loss: 2.302, Test loss: 2.301, Test accuracy: 9.97
Round  12, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 9.97
Round  13, Train loss: 2.302, Test loss: 2.301, Test accuracy: 9.98
Round  13, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 9.97
Round  14, Train loss: 2.301, Test loss: 2.300, Test accuracy: 9.98
Round  14, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 9.97
Round  15, Train loss: 2.301, Test loss: 2.300, Test accuracy: 9.98
Round  15, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 9.97
Round  16, Train loss: 2.300, Test loss: 2.300, Test accuracy: 9.99
Round  16, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 9.97
Round  17, Train loss: 2.300, Test loss: 2.299, Test accuracy: 9.99
Round  17, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 9.99
Round  18, Train loss: 2.300, Test loss: 2.299, Test accuracy: 9.99
Round  18, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 9.99
Round  19, Train loss: 2.300, Test loss: 2.299, Test accuracy: 9.99
Round  19, Global train loss: 2.300, Global test loss: 2.297, Global test accuracy: 10.00
Round  20, Train loss: 2.299, Test loss: 2.298, Test accuracy: 9.99
Round  20, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.00
Round  21, Train loss: 2.299, Test loss: 2.298, Test accuracy: 9.99
Round  21, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.00
Round  22, Train loss: 2.298, Test loss: 2.297, Test accuracy: 9.99
Round  22, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 10.00
Round  23, Train loss: 2.298, Test loss: 2.297, Test accuracy: 9.99
Round  23, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 10.00
Round  24, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.00
Round  24, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.00
Round  25, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.00
Round  25, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.01
Round  26, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.01
Round  26, Global train loss: 2.297, Global test loss: 2.294, Global test accuracy: 10.01
Round  27, Train loss: 2.296, Test loss: 2.295, Test accuracy: 10.04
Round  27, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 10.04
Round  28, Train loss: 2.295, Test loss: 2.294, Test accuracy: 10.08
Round  28, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 10.15
Round  29, Train loss: 2.296, Test loss: 2.294, Test accuracy: 10.18
Round  29, Global train loss: 2.296, Global test loss: 2.292, Global test accuracy: 10.26
Round  30, Train loss: 2.294, Test loss: 2.293, Test accuracy: 10.27
Round  30, Global train loss: 2.294, Global test loss: 2.292, Global test accuracy: 10.41
Round  31, Train loss: 2.294, Test loss: 2.293, Test accuracy: 10.38
Round  31, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 10.61
Round  32, Train loss: 2.294, Test loss: 2.292, Test accuracy: 10.48
Round  32, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 10.49
Round  33, Train loss: 2.293, Test loss: 2.291, Test accuracy: 10.82
Round  33, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 11.30
Round  34, Train loss: 2.293, Test loss: 2.291, Test accuracy: 11.06
Round  34, Global train loss: 2.293, Global test loss: 2.289, Global test accuracy: 10.91
Round  35, Train loss: 2.292, Test loss: 2.290, Test accuracy: 10.96
Round  35, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 10.73
Round  36, Train loss: 2.292, Test loss: 2.290, Test accuracy: 11.11
Round  36, Global train loss: 2.292, Global test loss: 2.288, Global test accuracy: 11.28
Round  37, Train loss: 2.291, Test loss: 2.289, Test accuracy: 11.44
Round  37, Global train loss: 2.291, Global test loss: 2.287, Global test accuracy: 11.65
Round  38, Train loss: 2.290, Test loss: 2.289, Test accuracy: 11.60
Round  38, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 12.07
Round  39, Train loss: 2.290, Test loss: 2.288, Test accuracy: 11.81
Round  39, Global train loss: 2.290, Global test loss: 2.286, Global test accuracy: 12.24
Round  40, Train loss: 2.290, Test loss: 2.287, Test accuracy: 11.91
Round  40, Global train loss: 2.290, Global test loss: 2.285, Global test accuracy: 11.69
Round  41, Train loss: 2.290, Test loss: 2.286, Test accuracy: 12.10
Round  41, Global train loss: 2.290, Global test loss: 2.285, Global test accuracy: 11.90
Round  42, Train loss: 2.289, Test loss: 2.285, Test accuracy: 12.38
Round  42, Global train loss: 2.289, Global test loss: 2.284, Global test accuracy: 12.98
Round  43, Train loss: 2.289, Test loss: 2.285, Test accuracy: 12.65
Round  43, Global train loss: 2.289, Global test loss: 2.283, Global test accuracy: 13.32
Round  44, Train loss: 2.288, Test loss: 2.284, Test accuracy: 13.28
Round  44, Global train loss: 2.288, Global test loss: 2.282, Global test accuracy: 13.99
Round  45, Train loss: 2.287, Test loss: 2.283, Test accuracy: 13.40
Round  45, Global train loss: 2.287, Global test loss: 2.281, Global test accuracy: 13.43
Round  46, Train loss: 2.287, Test loss: 2.282, Test accuracy: 13.68
Round  46, Global train loss: 2.287, Global test loss: 2.281, Global test accuracy: 13.85
Round  47, Train loss: 2.287, Test loss: 2.282, Test accuracy: 13.90
Round  47, Global train loss: 2.287, Global test loss: 2.280, Global test accuracy: 14.13
Round  48, Train loss: 2.286, Test loss: 2.281, Test accuracy: 13.95
Round  48, Global train loss: 2.286, Global test loss: 2.279, Global test accuracy: 13.67
Round  49, Train loss: 2.285, Test loss: 2.280, Test accuracy: 13.97
Round  49, Global train loss: 2.285, Global test loss: 2.278, Global test accuracy: 14.12
Round  50, Train loss: 2.284, Test loss: 2.279, Test accuracy: 14.23
Round  50, Global train loss: 2.284, Global test loss: 2.277, Global test accuracy: 14.45
Round  51, Train loss: 2.283, Test loss: 2.279, Test accuracy: 14.25
Round  51, Global train loss: 2.283, Global test loss: 2.277, Global test accuracy: 14.01
Round  52, Train loss: 2.283, Test loss: 2.278, Test accuracy: 14.11
Round  52, Global train loss: 2.283, Global test loss: 2.276, Global test accuracy: 13.97
Round  53, Train loss: 2.282, Test loss: 2.277, Test accuracy: 14.27
Round  53, Global train loss: 2.282, Global test loss: 2.275, Global test accuracy: 14.19
Round  54, Train loss: 2.283, Test loss: 2.276, Test accuracy: 14.49
Round  54, Global train loss: 2.283, Global test loss: 2.274, Global test accuracy: 14.85
Round  55, Train loss: 2.282, Test loss: 2.275, Test accuracy: 14.68
Round  55, Global train loss: 2.282, Global test loss: 2.273, Global test accuracy: 14.71
Round  56, Train loss: 2.281, Test loss: 2.274, Test accuracy: 14.70
Round  56, Global train loss: 2.281, Global test loss: 2.272, Global test accuracy: 15.04
Round  57, Train loss: 2.280, Test loss: 2.273, Test accuracy: 14.78
Round  57, Global train loss: 2.280, Global test loss: 2.271, Global test accuracy: 15.05
Round  58, Train loss: 2.279, Test loss: 2.272, Test accuracy: 15.14
Round  58, Global train loss: 2.279, Global test loss: 2.270, Global test accuracy: 15.42
Round  59, Train loss: 2.279, Test loss: 2.272, Test accuracy: 15.27
Round  59, Global train loss: 2.279, Global test loss: 2.269, Global test accuracy: 15.55
Round  60, Train loss: 2.278, Test loss: 2.271, Test accuracy: 15.11
Round  60, Global train loss: 2.278, Global test loss: 2.268, Global test accuracy: 15.68
Round  61, Train loss: 2.278, Test loss: 2.269, Test accuracy: 15.35
Round  61, Global train loss: 2.278, Global test loss: 2.268, Global test accuracy: 15.59
Round  62, Train loss: 2.276, Test loss: 2.269, Test accuracy: 15.44
Round  62, Global train loss: 2.276, Global test loss: 2.267, Global test accuracy: 15.56
Round  63, Train loss: 2.274, Test loss: 2.268, Test accuracy: 15.71
Round  63, Global train loss: 2.274, Global test loss: 2.265, Global test accuracy: 15.97
Round  64, Train loss: 2.274, Test loss: 2.267, Test accuracy: 15.87
Round  64, Global train loss: 2.274, Global test loss: 2.264, Global test accuracy: 16.20
Round  65, Train loss: 2.274, Test loss: 2.265, Test accuracy: 16.18
Round  65, Global train loss: 2.274, Global test loss: 2.262, Global test accuracy: 16.23
Round  66, Train loss: 2.273, Test loss: 2.264, Test accuracy: 16.04
Round  66, Global train loss: 2.273, Global test loss: 2.261, Global test accuracy: 16.23
Round  67, Train loss: 2.272, Test loss: 2.263, Test accuracy: 15.95
Round  67, Global train loss: 2.272, Global test loss: 2.259, Global test accuracy: 16.24
Round  68, Train loss: 2.271, Test loss: 2.261, Test accuracy: 16.09
Round  68, Global train loss: 2.271, Global test loss: 2.258, Global test accuracy: 16.42
Round  69, Train loss: 2.270, Test loss: 2.260, Test accuracy: 16.09
Round  69, Global train loss: 2.270, Global test loss: 2.257, Global test accuracy: 16.71
Round  70, Train loss: 2.270, Test loss: 2.259, Test accuracy: 16.20
Round  70, Global train loss: 2.270, Global test loss: 2.255, Global test accuracy: 16.89
Round  71, Train loss: 2.270, Test loss: 2.257, Test accuracy: 16.65
Round  71, Global train loss: 2.270, Global test loss: 2.254, Global test accuracy: 17.16
Round  72, Train loss: 2.269, Test loss: 2.256, Test accuracy: 16.93
Round  72, Global train loss: 2.269, Global test loss: 2.253, Global test accuracy: 17.32
Round  73, Train loss: 2.268, Test loss: 2.254, Test accuracy: 17.22
Round  73, Global train loss: 2.268, Global test loss: 2.251, Global test accuracy: 17.78
Round  74, Train loss: 2.269, Test loss: 2.253, Test accuracy: 17.69
Round  74, Global train loss: 2.269, Global test loss: 2.251, Global test accuracy: 18.41
Round  75, Train loss: 2.267, Test loss: 2.252, Test accuracy: 17.86
Round  75, Global train loss: 2.267, Global test loss: 2.249, Global test accuracy: 18.89
Round  76, Train loss: 2.267, Test loss: 2.252, Test accuracy: 17.93
Round  76, Global train loss: 2.267, Global test loss: 2.249, Global test accuracy: 18.96
Round  77, Train loss: 2.266, Test loss: 2.250, Test accuracy: 18.52
Round  77, Global train loss: 2.266, Global test loss: 2.247, Global test accuracy: 19.20
Round  78, Train loss: 2.265, Test loss: 2.248, Test accuracy: 18.79
Round  78, Global train loss: 2.265, Global test loss: 2.245, Global test accuracy: 19.28
Round  79, Train loss: 2.266, Test loss: 2.247, Test accuracy: 19.13
Round  79, Global train loss: 2.266, Global test loss: 2.243, Global test accuracy: 19.31
Round  80, Train loss: 2.264, Test loss: 2.246, Test accuracy: 19.31
Round  80, Global train loss: 2.264, Global test loss: 2.242, Global test accuracy: 19.60
Round  81, Train loss: 2.264, Test loss: 2.245, Test accuracy: 19.28
Round  81, Global train loss: 2.264, Global test loss: 2.241, Global test accuracy: 19.90
Round  82, Train loss: 2.262, Test loss: 2.244, Test accuracy: 19.18
Round  82, Global train loss: 2.262, Global test loss: 2.239, Global test accuracy: 19.74
Round  83, Train loss: 2.263, Test loss: 2.242, Test accuracy: 19.40
Round  83, Global train loss: 2.263, Global test loss: 2.238, Global test accuracy: 20.09
Round  84, Train loss: 2.261, Test loss: 2.241, Test accuracy: 19.34
Round  84, Global train loss: 2.261, Global test loss: 2.237, Global test accuracy: 20.24
Round  85, Train loss: 2.260, Test loss: 2.239, Test accuracy: 19.44
Round  85, Global train loss: 2.260, Global test loss: 2.235, Global test accuracy: 19.32
Round  86, Train loss: 2.257, Test loss: 2.237, Test accuracy: 19.61
Round  86, Global train loss: 2.257, Global test loss: 2.234, Global test accuracy: 19.83
Round  87, Train loss: 2.255, Test loss: 2.236, Test accuracy: 19.57
Round  87, Global train loss: 2.255, Global test loss: 2.233, Global test accuracy: 19.50
Round  88, Train loss: 2.256, Test loss: 2.235, Test accuracy: 19.67
Round  88, Global train loss: 2.256, Global test loss: 2.232, Global test accuracy: 19.81
Round  89, Train loss: 2.252, Test loss: 2.234, Test accuracy: 19.58
Round  89, Global train loss: 2.252, Global test loss: 2.231, Global test accuracy: 20.10
Round  90, Train loss: 2.255, Test loss: 2.234, Test accuracy: 19.70
Round  90, Global train loss: 2.255, Global test loss: 2.231, Global test accuracy: 20.13
Round  91, Train loss: 2.253, Test loss: 2.233, Test accuracy: 19.61
Round  91, Global train loss: 2.253, Global test loss: 2.230, Global test accuracy: 20.11
Round  92, Train loss: 2.251, Test loss: 2.232, Test accuracy: 19.89
Round  92, Global train loss: 2.251, Global test loss: 2.229, Global test accuracy: 20.52
Round  93, Train loss: 2.251, Test loss: 2.231, Test accuracy: 20.11
Round  93, Global train loss: 2.251, Global test loss: 2.227, Global test accuracy: 20.73
Round  94, Train loss: 2.249, Test loss: 2.230, Test accuracy: 20.32
Round  94, Global train loss: 2.249, Global test loss: 2.227, Global test accuracy: 21.16
Round  95, Train loss: 2.249, Test loss: 2.228, Test accuracy: 20.57
Round  95, Global train loss: 2.249, Global test loss: 2.225, Global test accuracy: 21.26
Round  96, Train loss: 2.245, Test loss: 2.227, Test accuracy: 20.54
Round  96, Global train loss: 2.245, Global test loss: 2.224, Global test accuracy: 20.04
Round  97, Train loss: 2.245, Test loss: 2.226, Test accuracy: 20.58
Round  97, Global train loss: 2.245, Global test loss: 2.223, Global test accuracy: 20.80
Round  98, Train loss: 2.244, Test loss: 2.226, Test accuracy: 20.36
Round  98, Global train loss: 2.244, Global test loss: 2.222, Global test accuracy: 20.92
Round  99, Train loss: 2.245, Test loss: 2.225, Test accuracy: 20.32
Round  99, Global train loss: 2.245, Global test loss: 2.222, Global test accuracy: 20.02
Final Round, Train loss: 2.239, Test loss: 2.219, Test accuracy: 19.91
Final Round, Global train loss: 2.239, Global test loss: 2.222, Global test accuracy: 20.02
Average accuracy final 10 rounds: 20.1985 

Average global accuracy final 10 rounds: 20.5695 

5375.75567483902
[5.356572389602661, 10.616850137710571, 15.854620218276978, 21.131537675857544, 26.43030881881714, 31.69947838783264, 37.02443814277649, 42.33320951461792, 47.79660439491272, 53.11915636062622, 58.41279435157776, 63.72678327560425, 68.9987428188324, 74.27186417579651, 79.54709124565125, 84.83438348770142, 90.08797812461853, 95.33877038955688, 100.62687635421753, 105.89912056922913, 111.12969517707825, 116.37235426902771, 121.64825296401978, 126.93040704727173, 131.60947346687317, 136.25839066505432, 140.87612986564636, 145.5269968509674, 150.1661515235901, 154.8485813140869, 159.5245487689972, 164.1830382347107, 168.8277509212494, 173.4694061279297, 178.1244568824768, 182.75901246070862, 187.40571355819702, 192.04793906211853, 196.72299766540527, 201.3919496536255, 206.07617568969727, 210.67909383773804, 215.33096861839294, 219.98947310447693, 224.65148830413818, 230.41101098060608, 236.1671769618988, 241.9022092819214, 247.21733570098877, 252.11054253578186, 257.00471091270447, 261.9567527770996, 266.85933327674866, 271.76617336273193, 276.67197251319885, 281.6104459762573, 286.84656620025635, 292.2623019218445, 297.73539209365845, 303.07692408561707, 308.4570779800415, 313.8503828048706, 319.22968435287476, 324.53450298309326, 329.7302715778351, 335.13339257240295, 340.5096266269684, 345.846506357193, 351.0250566005707, 356.22209548950195, 361.4253399372101, 366.5937044620514, 371.48660159111023, 376.28478622436523, 381.1290695667267, 386.01709175109863, 390.7988905906677, 395.63013076782227, 400.55033922195435, 405.3569920063019, 410.15403175354004, 414.9675712585449, 419.7601592540741, 424.5309057235718, 429.31333112716675, 434.1019866466522, 438.92051553726196, 443.6906747817993, 448.4928650856018, 453.2916603088379, 458.05795526504517, 462.82544016838074, 467.61808729171753, 472.4102375507355, 477.22861528396606, 482.0537586212158, 486.85029768943787, 491.7170515060425, 496.54861664772034, 501.36013770103455, 503.79965925216675]
[9.9425, 9.92, 9.9275, 9.89, 9.91, 9.9375, 9.9375, 9.9525, 9.945, 9.9525, 9.9575, 9.965, 9.975, 9.985, 9.985, 9.985, 9.9925, 9.9925, 9.9925, 9.9875, 9.9875, 9.9925, 9.995, 9.9925, 10.0, 10.0, 10.0125, 10.04, 10.0775, 10.18, 10.27, 10.385, 10.4825, 10.8175, 11.065, 10.965, 11.105, 11.4375, 11.595, 11.81, 11.915, 12.1, 12.38, 12.6475, 13.2825, 13.4, 13.6775, 13.8975, 13.945, 13.9675, 14.2325, 14.2525, 14.105, 14.265, 14.4925, 14.675, 14.7025, 14.78, 15.1425, 15.2675, 15.1125, 15.345, 15.4375, 15.705, 15.87, 16.185, 16.04, 15.9525, 16.0925, 16.0925, 16.2, 16.6525, 16.93, 17.22, 17.6875, 17.865, 17.925, 18.515, 18.79, 19.1325, 19.31, 19.28, 19.175, 19.4, 19.3425, 19.4425, 19.6125, 19.5675, 19.6675, 19.5825, 19.7, 19.6075, 19.885, 20.1125, 20.32, 20.565, 20.535, 20.5825, 20.355, 20.3225, 19.905]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.009, Test loss: 1.783, Test accuracy: 34.06
Round   0, Global train loss: 2.009, Global test loss: 1.799, Global test accuracy: 33.26
Round   1, Train loss: 1.706, Test loss: 1.572, Test accuracy: 42.18
Round   1, Global train loss: 1.706, Global test loss: 1.560, Global test accuracy: 43.27
Round   2, Train loss: 1.556, Test loss: 1.482, Test accuracy: 45.37
Round   2, Global train loss: 1.556, Global test loss: 1.440, Global test accuracy: 46.50
Round   3, Train loss: 1.434, Test loss: 1.442, Test accuracy: 48.38
Round   3, Global train loss: 1.434, Global test loss: 1.375, Global test accuracy: 51.49
Round   4, Train loss: 1.348, Test loss: 1.366, Test accuracy: 50.57
Round   4, Global train loss: 1.348, Global test loss: 1.293, Global test accuracy: 52.12
Round   5, Train loss: 1.267, Test loss: 1.324, Test accuracy: 52.42
Round   5, Global train loss: 1.267, Global test loss: 1.222, Global test accuracy: 56.05
Round   6, Train loss: 1.195, Test loss: 1.293, Test accuracy: 53.74
Round   6, Global train loss: 1.195, Global test loss: 1.187, Global test accuracy: 57.66
Round   7, Train loss: 1.153, Test loss: 1.240, Test accuracy: 56.05
Round   7, Global train loss: 1.153, Global test loss: 1.142, Global test accuracy: 59.63
Round   8, Train loss: 1.094, Test loss: 1.238, Test accuracy: 56.68
Round   8, Global train loss: 1.094, Global test loss: 1.117, Global test accuracy: 60.86
Round   9, Train loss: 1.065, Test loss: 1.177, Test accuracy: 59.05
Round   9, Global train loss: 1.065, Global test loss: 1.077, Global test accuracy: 61.99
Round  10, Train loss: 1.015, Test loss: 1.167, Test accuracy: 59.62
Round  10, Global train loss: 1.015, Global test loss: 1.064, Global test accuracy: 60.37
Round  11, Train loss: 0.973, Test loss: 1.138, Test accuracy: 60.69
Round  11, Global train loss: 0.973, Global test loss: 1.047, Global test accuracy: 60.94
Round  12, Train loss: 0.958, Test loss: 1.109, Test accuracy: 62.09
Round  12, Global train loss: 0.958, Global test loss: 1.085, Global test accuracy: 65.09
Round  13, Train loss: 0.907, Test loss: 1.093, Test accuracy: 62.77
Round  13, Global train loss: 0.907, Global test loss: 1.012, Global test accuracy: 64.97
Round  14, Train loss: 0.900, Test loss: 1.052, Test accuracy: 64.08
Round  14, Global train loss: 0.900, Global test loss: 0.994, Global test accuracy: 62.50
Round  15, Train loss: 0.887, Test loss: 1.041, Test accuracy: 64.66
Round  15, Global train loss: 0.887, Global test loss: 0.999, Global test accuracy: 62.92
Round  16, Train loss: 0.850, Test loss: 1.028, Test accuracy: 65.34
Round  16, Global train loss: 0.850, Global test loss: 0.982, Global test accuracy: 63.19
Round  17, Train loss: 0.818, Test loss: 1.020, Test accuracy: 65.77
Round  17, Global train loss: 0.818, Global test loss: 1.177, Global test accuracy: 66.70
Round  18, Train loss: 0.795, Test loss: 1.017, Test accuracy: 66.01
Round  18, Global train loss: 0.795, Global test loss: 0.976, Global test accuracy: 64.16
Round  19, Train loss: 0.799, Test loss: 1.010, Test accuracy: 66.50
Round  19, Global train loss: 0.799, Global test loss: 0.957, Global test accuracy: 67.18
Round  20, Train loss: 0.763, Test loss: 1.003, Test accuracy: 66.95
Round  20, Global train loss: 0.763, Global test loss: 0.944, Global test accuracy: 64.34
Round  21, Train loss: 0.753, Test loss: 0.989, Test accuracy: 67.33
Round  21, Global train loss: 0.753, Global test loss: 1.040, Global test accuracy: 68.54
Round  22, Train loss: 0.741, Test loss: 0.977, Test accuracy: 67.87
Round  22, Global train loss: 0.741, Global test loss: 0.947, Global test accuracy: 68.14
Round  23, Train loss: 0.731, Test loss: 0.981, Test accuracy: 67.93
Round  23, Global train loss: 0.731, Global test loss: 0.925, Global test accuracy: 65.35
Round  24, Train loss: 0.750, Test loss: 0.982, Test accuracy: 68.05
Round  24, Global train loss: 0.750, Global test loss: 0.915, Global test accuracy: 65.66
Round  25, Train loss: 0.705, Test loss: 0.976, Test accuracy: 68.22
Round  25, Global train loss: 0.705, Global test loss: 1.214, Global test accuracy: 68.57
Round  26, Train loss: 0.695, Test loss: 0.972, Test accuracy: 68.48
Round  26, Global train loss: 0.695, Global test loss: 0.913, Global test accuracy: 66.11
Round  27, Train loss: 0.689, Test loss: 0.976, Test accuracy: 68.56
Round  27, Global train loss: 0.689, Global test loss: 0.932, Global test accuracy: 68.89
Round  28, Train loss: 0.673, Test loss: 0.976, Test accuracy: 68.72
Round  28, Global train loss: 0.673, Global test loss: 1.045, Global test accuracy: 70.06
Round  29, Train loss: 0.648, Test loss: 0.979, Test accuracy: 68.84
Round  29, Global train loss: 0.648, Global test loss: 1.064, Global test accuracy: 69.84
Round  30, Train loss: 0.664, Test loss: 0.971, Test accuracy: 69.11
Round  30, Global train loss: 0.664, Global test loss: 0.918, Global test accuracy: 66.44
Round  31, Train loss: 0.645, Test loss: 0.973, Test accuracy: 69.30
Round  31, Global train loss: 0.645, Global test loss: 1.047, Global test accuracy: 70.19
Round  32, Train loss: 0.609, Test loss: 0.965, Test accuracy: 69.61
Round  32, Global train loss: 0.609, Global test loss: 1.109, Global test accuracy: 70.08
Round  33, Train loss: 0.635, Test loss: 0.955, Test accuracy: 70.10
Round  33, Global train loss: 0.635, Global test loss: 0.917, Global test accuracy: 66.80
Round  34, Train loss: 0.630, Test loss: 0.952, Test accuracy: 70.14
Round  34, Global train loss: 0.630, Global test loss: 0.929, Global test accuracy: 70.60
Round  35, Train loss: 0.624, Test loss: 0.944, Test accuracy: 70.10
Round  35, Global train loss: 0.624, Global test loss: 1.053, Global test accuracy: 70.37
Round  36, Train loss: 0.600, Test loss: 0.945, Test accuracy: 70.27
Round  36, Global train loss: 0.600, Global test loss: 1.093, Global test accuracy: 66.03
Round  37, Train loss: 0.609, Test loss: 0.940, Test accuracy: 70.58
Round  37, Global train loss: 0.609, Global test loss: 0.889, Global test accuracy: 67.37
Round  38, Train loss: 0.585, Test loss: 0.929, Test accuracy: 70.86
Round  38, Global train loss: 0.585, Global test loss: 1.070, Global test accuracy: 70.91
Round  39, Train loss: 0.595, Test loss: 0.934, Test accuracy: 70.92
Round  39, Global train loss: 0.595, Global test loss: 0.941, Global test accuracy: 69.83
Round  40, Train loss: 0.546, Test loss: 0.947, Test accuracy: 70.92
Round  40, Global train loss: 0.546, Global test loss: 0.927, Global test accuracy: 70.70
Round  41, Train loss: 0.570, Test loss: 0.958, Test accuracy: 70.55
Round  41, Global train loss: 0.570, Global test loss: 0.912, Global test accuracy: 70.70
Round  42, Train loss: 0.597, Test loss: 0.951, Test accuracy: 70.76
Round  42, Global train loss: 0.597, Global test loss: 0.910, Global test accuracy: 70.03
Round  43, Train loss: 0.578, Test loss: 0.927, Test accuracy: 71.40
Round  43, Global train loss: 0.578, Global test loss: 0.909, Global test accuracy: 70.54
Round  44, Train loss: 0.560, Test loss: 0.931, Test accuracy: 71.64
Round  44, Global train loss: 0.560, Global test loss: 0.932, Global test accuracy: 70.34
Round  45, Train loss: 0.553, Test loss: 0.927, Test accuracy: 71.80
Round  45, Global train loss: 0.553, Global test loss: 0.891, Global test accuracy: 68.25
Round  46, Train loss: 0.530, Test loss: 0.934, Test accuracy: 71.83
Round  46, Global train loss: 0.530, Global test loss: 0.937, Global test accuracy: 71.18
Round  47, Train loss: 0.563, Test loss: 0.934, Test accuracy: 71.83
Round  47, Global train loss: 0.563, Global test loss: 0.911, Global test accuracy: 70.92
Round  48, Train loss: 0.529, Test loss: 0.936, Test accuracy: 71.91
Round  48, Global train loss: 0.529, Global test loss: 1.440, Global test accuracy: 66.81
Round  49, Train loss: 0.530, Test loss: 0.928, Test accuracy: 72.17
Round  49, Global train loss: 0.530, Global test loss: 0.882, Global test accuracy: 68.06
Round  50, Train loss: 0.549, Test loss: 0.936, Test accuracy: 72.06
Round  50, Global train loss: 0.549, Global test loss: 0.893, Global test accuracy: 68.07
Round  51, Train loss: 0.516, Test loss: 0.929, Test accuracy: 72.12
Round  51, Global train loss: 0.516, Global test loss: 1.084, Global test accuracy: 71.49
Round  52, Train loss: 0.508, Test loss: 0.937, Test accuracy: 72.07
Round  52, Global train loss: 0.508, Global test loss: 1.108, Global test accuracy: 71.55
Round  53, Train loss: 0.495, Test loss: 0.949, Test accuracy: 71.97
Round  53, Global train loss: 0.495, Global test loss: 0.942, Global test accuracy: 70.96
Round  54, Train loss: 0.502, Test loss: 0.950, Test accuracy: 71.96
Round  54, Global train loss: 0.502, Global test loss: 0.951, Global test accuracy: 70.57
Round  55, Train loss: 0.486, Test loss: 0.940, Test accuracy: 72.17
Round  55, Global train loss: 0.486, Global test loss: 0.958, Global test accuracy: 70.90
Round  56, Train loss: 0.510, Test loss: 0.956, Test accuracy: 71.89
Round  56, Global train loss: 0.510, Global test loss: 0.948, Global test accuracy: 70.55
Round  57, Train loss: 0.517, Test loss: 0.960, Test accuracy: 71.85
Round  57, Global train loss: 0.517, Global test loss: 0.958, Global test accuracy: 71.41
Round  58, Train loss: 0.492, Test loss: 0.953, Test accuracy: 71.76
Round  58, Global train loss: 0.492, Global test loss: 0.954, Global test accuracy: 71.56
Round  59, Train loss: 0.490, Test loss: 0.940, Test accuracy: 72.19
Round  59, Global train loss: 0.490, Global test loss: 0.931, Global test accuracy: 71.28
Round  60, Train loss: 0.501, Test loss: 0.937, Test accuracy: 72.52
Round  60, Global train loss: 0.501, Global test loss: 0.911, Global test accuracy: 68.16
Round  61, Train loss: 0.467, Test loss: 0.944, Test accuracy: 72.25
Round  61, Global train loss: 0.467, Global test loss: 1.348, Global test accuracy: 71.70
Round  62, Train loss: 0.473, Test loss: 0.952, Test accuracy: 72.27
Round  62, Global train loss: 0.473, Global test loss: 0.945, Global test accuracy: 71.71
Round  63, Train loss: 0.508, Test loss: 0.965, Test accuracy: 72.09
Round  63, Global train loss: 0.508, Global test loss: 0.902, Global test accuracy: 68.39
Round  64, Train loss: 0.464, Test loss: 0.967, Test accuracy: 72.29
Round  64, Global train loss: 0.464, Global test loss: 1.344, Global test accuracy: 71.69
Round  65, Train loss: 0.475, Test loss: 0.957, Test accuracy: 72.33
Round  65, Global train loss: 0.475, Global test loss: 0.949, Global test accuracy: 71.58
Round  66, Train loss: 0.477, Test loss: 0.946, Test accuracy: 72.61
Round  66, Global train loss: 0.477, Global test loss: 0.953, Global test accuracy: 71.39
Round  67, Train loss: 0.444, Test loss: 0.953, Test accuracy: 72.61
Round  67, Global train loss: 0.444, Global test loss: 0.982, Global test accuracy: 71.46
Round  68, Train loss: 0.443, Test loss: 0.964, Test accuracy: 72.42
Round  68, Global train loss: 0.443, Global test loss: 1.363, Global test accuracy: 71.67
Round  69, Train loss: 0.445, Test loss: 0.961, Test accuracy: 72.63
Round  69, Global train loss: 0.445, Global test loss: 0.986, Global test accuracy: 71.14
Round  70, Train loss: 0.468, Test loss: 0.958, Test accuracy: 72.73
Round  70, Global train loss: 0.468, Global test loss: 0.895, Global test accuracy: 68.85
Round  71, Train loss: 0.465, Test loss: 0.950, Test accuracy: 72.80
Round  71, Global train loss: 0.465, Global test loss: 0.891, Global test accuracy: 68.60
Round  72, Train loss: 0.448, Test loss: 0.943, Test accuracy: 72.78
Round  72, Global train loss: 0.448, Global test loss: 0.973, Global test accuracy: 71.25
Round  73, Train loss: 0.452, Test loss: 0.950, Test accuracy: 72.97
Round  73, Global train loss: 0.452, Global test loss: 1.129, Global test accuracy: 71.93
Round  74, Train loss: 0.486, Test loss: 0.951, Test accuracy: 73.09
Round  74, Global train loss: 0.486, Global test loss: 0.906, Global test accuracy: 68.14
Round  75, Train loss: 0.440, Test loss: 0.945, Test accuracy: 73.26
Round  75, Global train loss: 0.440, Global test loss: 1.088, Global test accuracy: 72.03
Round  76, Train loss: 0.446, Test loss: 0.949, Test accuracy: 73.43
Round  76, Global train loss: 0.446, Global test loss: 0.971, Global test accuracy: 71.26
Round  77, Train loss: 0.399, Test loss: 0.949, Test accuracy: 73.38
Round  77, Global train loss: 0.399, Global test loss: 1.207, Global test accuracy: 71.99
Round  78, Train loss: 0.465, Test loss: 0.957, Test accuracy: 73.25
Round  78, Global train loss: 0.465, Global test loss: 1.247, Global test accuracy: 67.19
Round  79, Train loss: 0.419, Test loss: 0.965, Test accuracy: 73.19
Round  79, Global train loss: 0.419, Global test loss: 0.982, Global test accuracy: 71.64
Round  80, Train loss: 0.416, Test loss: 0.963, Test accuracy: 73.04
Round  80, Global train loss: 0.416, Global test loss: 0.971, Global test accuracy: 71.72
Round  81, Train loss: 0.421, Test loss: 0.970, Test accuracy: 72.79
Round  81, Global train loss: 0.421, Global test loss: 0.952, Global test accuracy: 71.91
Round  82, Train loss: 0.422, Test loss: 0.958, Test accuracy: 73.05
Round  82, Global train loss: 0.422, Global test loss: 0.957, Global test accuracy: 72.22
Round  83, Train loss: 0.423, Test loss: 0.966, Test accuracy: 72.83
Round  83, Global train loss: 0.423, Global test loss: 1.152, Global test accuracy: 72.22
Round  84, Train loss: 0.431, Test loss: 0.969, Test accuracy: 72.97
Round  84, Global train loss: 0.431, Global test loss: 0.896, Global test accuracy: 69.81
Round  85, Train loss: 0.410, Test loss: 0.962, Test accuracy: 73.10
Round  85, Global train loss: 0.410, Global test loss: 0.981, Global test accuracy: 71.56
Round  86, Train loss: 0.429, Test loss: 0.950, Test accuracy: 73.27
Round  86, Global train loss: 0.429, Global test loss: 0.907, Global test accuracy: 68.92
Round  87, Train loss: 0.428, Test loss: 0.951, Test accuracy: 73.45
Round  87, Global train loss: 0.428, Global test loss: 0.976, Global test accuracy: 71.94
Round  88, Train loss: 0.427, Test loss: 0.951, Test accuracy: 73.46
Round  88, Global train loss: 0.427, Global test loss: 0.998, Global test accuracy: 71.58
Round  89, Train loss: 0.420, Test loss: 0.951, Test accuracy: 73.56
Round  89, Global train loss: 0.420, Global test loss: 0.899, Global test accuracy: 69.23
Round  90, Train loss: 0.422, Test loss: 0.952, Test accuracy: 73.47
Round  90, Global train loss: 0.422, Global test loss: 0.983, Global test accuracy: 72.02
Round  91, Train loss: 0.431, Test loss: 0.956, Test accuracy: 73.58
Round  91, Global train loss: 0.431, Global test loss: 0.899, Global test accuracy: 68.92
Round  92, Train loss: 0.423, Test loss: 0.953, Test accuracy: 73.44
Round  92, Global train loss: 0.423, Global test loss: 1.115, Global test accuracy: 72.55
Round  93, Train loss: 0.413, Test loss: 0.943, Test accuracy: 73.68
Round  93, Global train loss: 0.413, Global test loss: 0.944, Global test accuracy: 71.94
Round  94, Train loss: 0.412, Test loss: 0.948, Test accuracy: 73.62
Round  94, Global train loss: 0.412, Global test loss: 0.915, Global test accuracy: 69.14
Round  95, Train loss: 0.399, Test loss: 0.949, Test accuracy: 73.59
Round  95, Global train loss: 0.399, Global test loss: 0.934, Global test accuracy: 68.68
Round  96, Train loss: 0.394, Test loss: 0.954, Test accuracy: 73.57
Round  96, Global train loss: 0.394, Global test loss: 0.941, Global test accuracy: 72.20
Round  97, Train loss: 0.394, Test loss: 0.946, Test accuracy: 73.71
Round  97, Global train loss: 0.394, Global test loss: 1.164, Global test accuracy: 72.42
Round  98, Train loss: 0.413, Test loss: 0.934, Test accuracy: 74.00
Round  98, Global train loss: 0.413, Global test loss: 1.148, Global test accuracy: 67.49
Round  99, Train loss: 0.388, Test loss: 0.950, Test accuracy: 73.80
Round  99, Global train loss: 0.388, Global test loss: 0.920, Global test accuracy: 69.25
Final Round, Train loss: 0.242, Test loss: 1.030, Test accuracy: 74.54
Final Round, Global train loss: 0.242, Global test loss: 0.920, Global test accuracy: 69.25
Average accuracy final 10 rounds: 73.64750000000001 

Average global accuracy final 10 rounds: 70.45925 

6576.225626945496
[5.35716700553894, 10.71433401107788, 15.736302614212036, 20.75827121734619, 25.542513608932495, 30.3267560005188, 35.515116691589355, 40.70347738265991, 45.77158236503601, 50.83968734741211, 55.74213695526123, 60.64458656311035, 65.6031391620636, 70.56169176101685, 75.66977953910828, 80.7778673171997, 86.16628551483154, 91.55470371246338, 97.05826687812805, 102.56183004379272, 107.79777264595032, 113.03371524810791, 118.274747133255, 123.5157790184021, 128.71269297599792, 133.90960693359375, 139.16942524909973, 144.4292435646057, 149.66466689109802, 154.90009021759033, 160.11529850959778, 165.33050680160522, 170.53456115722656, 175.7386155128479, 181.001944065094, 186.2652726173401, 191.5263147354126, 196.7873568534851, 202.00907707214355, 207.230797290802, 212.31590151786804, 217.40100574493408, 222.47186303138733, 227.54272031784058, 232.54804182052612, 237.55336332321167, 242.4844765663147, 247.41558980941772, 252.4537558555603, 257.4919219017029, 262.6365466117859, 267.7811713218689, 272.91924500465393, 278.05731868743896, 283.09281969070435, 288.1283206939697, 293.3292410373688, 298.5301613807678, 303.55971908569336, 308.5892767906189, 313.60907196998596, 318.628867149353, 323.74914598464966, 328.8694248199463, 333.8300259113312, 338.79062700271606, 344.09852290153503, 349.406418800354, 354.6957848072052, 359.9851508140564, 365.23019647598267, 370.47524213790894, 375.7197778224945, 380.9643135070801, 386.25711941719055, 391.549925327301, 396.8627667427063, 402.1756081581116, 407.32301592826843, 412.4704236984253, 417.6503562927246, 422.8302888870239, 428.0550048351288, 433.27972078323364, 438.4893445968628, 443.69896841049194, 448.9618573188782, 454.2247462272644, 459.3725264072418, 464.52030658721924, 469.72215247154236, 474.9239983558655, 479.90248823165894, 484.8809781074524, 490.058226108551, 495.23547410964966, 500.27264428138733, 505.309814453125, 510.30882596969604, 515.3078374862671, 520.296306848526, 525.2847762107849, 530.4253888130188, 535.5660014152527, 540.7520191669464, 545.9380369186401, 550.9620106220245, 555.9859843254089, 561.0220293998718, 566.0580744743347, 571.2536461353302, 576.4492177963257, 581.6604144573212, 586.8716111183167, 591.8810381889343, 596.890465259552, 601.9391443729401, 606.9878234863281, 612.2131226062775, 617.4384217262268, 622.7111582756042, 627.9838948249817, 633.2771165370941, 638.5703382492065, 643.548326253891, 648.5263142585754, 653.6221299171448, 658.7179455757141, 663.7439985275269, 668.7700514793396, 673.900737285614, 679.0314230918884, 684.1336879730225, 689.2359528541565, 694.3308284282684, 699.4257040023804, 704.5383155345917, 709.650927066803, 714.6601164340973, 719.6693058013916, 724.8937532901764, 730.1182007789612, 735.415198802948, 740.7121968269348, 746.0310423374176, 751.3498878479004, 756.6117796897888, 761.8736715316772, 767.166695356369, 772.4597191810608, 777.7749056816101, 783.0900921821594, 788.2903964519501, 793.4907007217407, 798.6759598255157, 803.8612189292908, 809.0806081295013, 814.2999973297119, 819.4927787780762, 824.6855602264404, 829.8717167377472, 835.057873249054, 840.1767609119415, 845.2956485748291, 850.3609690666199, 855.4262895584106, 860.5566458702087, 865.6870021820068, 870.8365588188171, 875.9861154556274, 880.9026381969452, 885.8191609382629, 890.7727179527283, 895.7262749671936, 900.8348422050476, 905.9434094429016, 910.8184959888458, 915.69358253479, 920.011519908905, 924.32945728302, 928.6342301368713, 932.9390029907227, 937.2430856227875, 941.5471682548523, 945.8750507831573, 950.2029333114624, 954.5335469245911, 958.8641605377197, 963.1651222705841, 967.4660840034485, 971.7553384304047, 976.0445928573608, 980.3211624622345, 984.5977320671082, 988.8924603462219, 993.1871886253357, 997.5112726688385, 1001.8353567123413, 1006.1654913425446, 1010.4956259727478, 1012.659903049469, 1014.8241801261902]
[34.065, 34.065, 42.1825, 42.1825, 45.37, 45.37, 48.38, 48.38, 50.5725, 50.5725, 52.42, 52.42, 53.745, 53.745, 56.055, 56.055, 56.6825, 56.6825, 59.0525, 59.0525, 59.625, 59.625, 60.685, 60.685, 62.095, 62.095, 62.7675, 62.7675, 64.085, 64.085, 64.6625, 64.6625, 65.3425, 65.3425, 65.7675, 65.7675, 66.0125, 66.0125, 66.505, 66.505, 66.9525, 66.9525, 67.3325, 67.3325, 67.8675, 67.8675, 67.9325, 67.9325, 68.0475, 68.0475, 68.215, 68.215, 68.48, 68.48, 68.5625, 68.5625, 68.7225, 68.7225, 68.845, 68.845, 69.11, 69.11, 69.3, 69.3, 69.6075, 69.6075, 70.1, 70.1, 70.14, 70.14, 70.1, 70.1, 70.27, 70.27, 70.5775, 70.5775, 70.8625, 70.8625, 70.915, 70.915, 70.9225, 70.9225, 70.5525, 70.5525, 70.76, 70.76, 71.4, 71.4, 71.635, 71.635, 71.8, 71.8, 71.83, 71.83, 71.835, 71.835, 71.905, 71.905, 72.1675, 72.1675, 72.055, 72.055, 72.1175, 72.1175, 72.0725, 72.0725, 71.9725, 71.9725, 71.96, 71.96, 72.165, 72.165, 71.8925, 71.8925, 71.85, 71.85, 71.76, 71.76, 72.1925, 72.1925, 72.52, 72.52, 72.2475, 72.2475, 72.27, 72.27, 72.0875, 72.0875, 72.29, 72.29, 72.3325, 72.3325, 72.6075, 72.6075, 72.605, 72.605, 72.415, 72.415, 72.6325, 72.6325, 72.735, 72.735, 72.8025, 72.8025, 72.775, 72.775, 72.97, 72.97, 73.0875, 73.0875, 73.2625, 73.2625, 73.43, 73.43, 73.3775, 73.3775, 73.25, 73.25, 73.195, 73.195, 73.04, 73.04, 72.7875, 72.7875, 73.0475, 73.0475, 72.83, 72.83, 72.9725, 72.9725, 73.0975, 73.0975, 73.27, 73.27, 73.45, 73.45, 73.46, 73.46, 73.56, 73.56, 73.4725, 73.4725, 73.5825, 73.5825, 73.445, 73.445, 73.68, 73.68, 73.62, 73.62, 73.5925, 73.5925, 73.5675, 73.5675, 73.71, 73.71, 74.0025, 74.0025, 73.8025, 73.8025, 74.54, 74.54]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.171, Test loss: 1.911, Test accuracy: 29.59
Round   1, Train loss: 1.841, Test loss: 1.679, Test accuracy: 36.61
Round   2, Train loss: 1.667, Test loss: 1.563, Test accuracy: 43.01
Round   3, Train loss: 1.581, Test loss: 1.504, Test accuracy: 45.77
Round   4, Train loss: 1.502, Test loss: 1.420, Test accuracy: 49.94
Round   5, Train loss: 1.429, Test loss: 1.358, Test accuracy: 51.70
Round   6, Train loss: 1.381, Test loss: 1.282, Test accuracy: 55.59
Round   7, Train loss: 1.314, Test loss: 1.249, Test accuracy: 56.66
Round   8, Train loss: 1.270, Test loss: 1.218, Test accuracy: 57.48
Round   9, Train loss: 1.234, Test loss: 1.171, Test accuracy: 59.04
Round  10, Train loss: 1.188, Test loss: 1.154, Test accuracy: 59.98
Round  11, Train loss: 1.137, Test loss: 1.140, Test accuracy: 60.52
Round  12, Train loss: 1.133, Test loss: 1.059, Test accuracy: 62.58
Round  13, Train loss: 1.073, Test loss: 1.050, Test accuracy: 63.31
Round  14, Train loss: 1.053, Test loss: 1.011, Test accuracy: 65.12
Round  15, Train loss: 1.013, Test loss: 0.999, Test accuracy: 65.50
Round  16, Train loss: 1.032, Test loss: 0.937, Test accuracy: 67.45
Round  17, Train loss: 0.968, Test loss: 0.935, Test accuracy: 67.89
Round  18, Train loss: 0.980, Test loss: 0.914, Test accuracy: 68.50
Round  19, Train loss: 0.943, Test loss: 0.894, Test accuracy: 69.47
Round  20, Train loss: 0.925, Test loss: 0.888, Test accuracy: 69.59
Round  21, Train loss: 0.902, Test loss: 0.872, Test accuracy: 69.97
Round  22, Train loss: 0.876, Test loss: 0.876, Test accuracy: 69.81
Round  23, Train loss: 0.874, Test loss: 0.870, Test accuracy: 70.21
Round  24, Train loss: 0.890, Test loss: 0.863, Test accuracy: 70.78
Round  25, Train loss: 0.844, Test loss: 0.861, Test accuracy: 70.96
Round  26, Train loss: 0.825, Test loss: 0.840, Test accuracy: 71.36
Round  27, Train loss: 0.813, Test loss: 0.838, Test accuracy: 71.67
Round  28, Train loss: 0.807, Test loss: 0.842, Test accuracy: 71.75
Round  29, Train loss: 0.804, Test loss: 0.827, Test accuracy: 72.14
Round  30, Train loss: 0.798, Test loss: 0.819, Test accuracy: 72.44
Round  31, Train loss: 0.786, Test loss: 0.816, Test accuracy: 72.20
Round  32, Train loss: 0.775, Test loss: 0.821, Test accuracy: 72.34
Round  33, Train loss: 0.759, Test loss: 0.814, Test accuracy: 72.50
Round  34, Train loss: 0.742, Test loss: 0.812, Test accuracy: 72.62
Round  35, Train loss: 0.724, Test loss: 0.804, Test accuracy: 72.88
Round  36, Train loss: 0.715, Test loss: 0.806, Test accuracy: 73.02
Round  37, Train loss: 0.722, Test loss: 0.804, Test accuracy: 72.95
Round  38, Train loss: 0.716, Test loss: 0.785, Test accuracy: 73.73
Round  39, Train loss: 0.692, Test loss: 0.777, Test accuracy: 73.84
Round  40, Train loss: 0.695, Test loss: 0.786, Test accuracy: 73.53
Round  41, Train loss: 0.692, Test loss: 0.783, Test accuracy: 73.72
Round  42, Train loss: 0.686, Test loss: 0.775, Test accuracy: 73.73
Round  43, Train loss: 0.705, Test loss: 0.771, Test accuracy: 73.85
Round  44, Train loss: 0.662, Test loss: 0.787, Test accuracy: 73.87
Round  45, Train loss: 0.688, Test loss: 0.798, Test accuracy: 73.50
Round  46, Train loss: 0.654, Test loss: 0.807, Test accuracy: 73.50
Round  47, Train loss: 0.649, Test loss: 0.783, Test accuracy: 74.00
Round  48, Train loss: 0.644, Test loss: 0.781, Test accuracy: 74.39
Round  49, Train loss: 0.648, Test loss: 0.779, Test accuracy: 74.12
Round  50, Train loss: 0.643, Test loss: 0.791, Test accuracy: 74.11
Round  51, Train loss: 0.628, Test loss: 0.778, Test accuracy: 74.16
Round  52, Train loss: 0.623, Test loss: 0.785, Test accuracy: 73.92
Round  53, Train loss: 0.614, Test loss: 0.781, Test accuracy: 73.94
Round  54, Train loss: 0.583, Test loss: 0.789, Test accuracy: 73.92
Round  55, Train loss: 0.645, Test loss: 0.780, Test accuracy: 74.48
Round  56, Train loss: 0.595, Test loss: 0.792, Test accuracy: 74.23
Round  57, Train loss: 0.623, Test loss: 0.786, Test accuracy: 74.61
Round  58, Train loss: 0.597, Test loss: 0.772, Test accuracy: 74.69
Round  59, Train loss: 0.583, Test loss: 0.763, Test accuracy: 75.00
Round  60, Train loss: 0.582, Test loss: 0.773, Test accuracy: 74.98
Round  61, Train loss: 0.626, Test loss: 0.771, Test accuracy: 75.26
Round  62, Train loss: 0.565, Test loss: 0.763, Test accuracy: 75.32
Round  63, Train loss: 0.604, Test loss: 0.767, Test accuracy: 75.10
Round  64, Train loss: 0.578, Test loss: 0.782, Test accuracy: 74.82
Round  65, Train loss: 0.576, Test loss: 0.781, Test accuracy: 74.98
Round  66, Train loss: 0.593, Test loss: 0.772, Test accuracy: 75.30
Round  67, Train loss: 0.542, Test loss: 0.780, Test accuracy: 75.17
Round  68, Train loss: 0.549, Test loss: 0.765, Test accuracy: 75.35
Round  69, Train loss: 0.574, Test loss: 0.763, Test accuracy: 75.51
Round  70, Train loss: 0.535, Test loss: 0.762, Test accuracy: 75.53
Round  71, Train loss: 0.554, Test loss: 0.770, Test accuracy: 75.37
Round  72, Train loss: 0.555, Test loss: 0.765, Test accuracy: 75.54
Round  73, Train loss: 0.544, Test loss: 0.778, Test accuracy: 75.25
Round  74, Train loss: 0.497, Test loss: 0.770, Test accuracy: 75.42
Round  75, Train loss: 0.545, Test loss: 0.766, Test accuracy: 75.84
Round  76, Train loss: 0.561, Test loss: 0.756, Test accuracy: 75.81
Round  77, Train loss: 0.516, Test loss: 0.766, Test accuracy: 75.41
Round  78, Train loss: 0.569, Test loss: 0.760, Test accuracy: 75.71
Round  79, Train loss: 0.537, Test loss: 0.749, Test accuracy: 76.02
Round  80, Train loss: 0.524, Test loss: 0.757, Test accuracy: 75.86
Round  81, Train loss: 0.525, Test loss: 0.759, Test accuracy: 76.27
Round  82, Train loss: 0.516, Test loss: 0.768, Test accuracy: 75.98
Round  83, Train loss: 0.522, Test loss: 0.762, Test accuracy: 76.12
Round  84, Train loss: 0.516, Test loss: 0.764, Test accuracy: 76.08
Round  85, Train loss: 0.516, Test loss: 0.760, Test accuracy: 76.33
Round  86, Train loss: 0.508, Test loss: 0.770, Test accuracy: 75.87
Round  87, Train loss: 0.516, Test loss: 0.767, Test accuracy: 76.00
Round  88, Train loss: 0.509, Test loss: 0.754, Test accuracy: 76.18
Round  89, Train loss: 0.524, Test loss: 0.784, Test accuracy: 75.75
Round  90, Train loss: 0.527, Test loss: 0.780, Test accuracy: 75.41
Round  91, Train loss: 0.506, Test loss: 0.778, Test accuracy: 75.69
Round  92, Train loss: 0.478, Test loss: 0.801, Test accuracy: 75.08
Round  93, Train loss: 0.515, Test loss: 0.789, Test accuracy: 75.64
Round  94, Train loss: 0.502, Test loss: 0.775, Test accuracy: 75.44
Round  95, Train loss: 0.482, Test loss: 0.776, Test accuracy: 76.01
Round  96, Train loss: 0.488, Test loss: 0.784, Test accuracy: 75.63
Round  97, Train loss: 0.515, Test loss: 0.782, Test accuracy: 75.81
Round  98, Train loss: 0.490, Test loss: 0.790, Test accuracy: 75.93
Round  99, Train loss: 0.503, Test loss: 0.789, Test accuracy: 75.85
Final Round, Train loss: 0.407, Test loss: 0.771, Test accuracy: 76.34
Average accuracy final 10 rounds: 75.64874999999999 

4800.60418176651
[4.752617359161377, 9.505234718322754, 14.120551824569702, 18.73586893081665, 23.187482118606567, 27.639095306396484, 32.109219551086426, 36.57934379577637, 40.94068384170532, 45.30202388763428, 49.6802191734314, 54.058414459228516, 58.48730683326721, 62.91619920730591, 67.25147891044617, 71.58675861358643, 75.9398946762085, 80.29303073883057, 84.71176052093506, 89.13049030303955, 93.56681299209595, 98.00313568115234, 102.36597943305969, 106.72882318496704, 111.17640829086304, 115.62399339675903, 120.07047271728516, 124.51695203781128, 128.93232440948486, 133.34769678115845, 137.8054256439209, 142.26315450668335, 146.7337453365326, 151.20433616638184, 155.7015471458435, 160.19875812530518, 164.60523223876953, 169.0117063522339, 173.36439204216003, 177.71707773208618, 182.06831169128418, 186.41954565048218, 190.77232384681702, 195.12510204315186, 199.4720973968506, 203.81909275054932, 208.2011263370514, 212.58315992355347, 216.89544773101807, 221.20773553848267, 225.5663321018219, 229.92492866516113, 234.32813930511475, 238.73134994506836, 243.060040473938, 247.38873100280762, 251.72059679031372, 256.0524625778198, 260.48297023773193, 264.91347789764404, 269.26953530311584, 273.62559270858765, 278.0506730079651, 282.47575330734253, 286.9240736961365, 291.3723940849304, 295.7047908306122, 300.03718757629395, 304.3338747024536, 308.6305618286133, 312.9927508831024, 317.35493993759155, 321.6955723762512, 326.0362048149109, 330.38021087646484, 334.7242169380188, 339.15219044685364, 343.5801639556885, 347.9945480823517, 352.4089322090149, 356.78192925453186, 361.1549263000488, 365.4834864139557, 369.81204652786255, 374.1346414089203, 378.457236289978, 382.89540791511536, 387.3335795402527, 391.7423679828644, 396.1511564254761, 400.5052137374878, 404.8592710494995, 409.29166173934937, 413.7240524291992, 418.13008403778076, 422.5361156463623, 426.90137577056885, 431.2666358947754, 435.6735243797302, 440.08041286468506, 444.38952684402466, 448.69864082336426, 453.077210187912, 457.4557795524597, 461.8257110118866, 466.1956424713135, 470.6321339607239, 475.0686254501343, 479.5046155452728, 483.9406056404114, 488.39096879959106, 492.84133195877075, 497.2806713581085, 501.7200107574463, 506.1553111076355, 510.5906114578247, 515.0391008853912, 519.4875903129578, 523.7543411254883, 528.0210919380188, 532.1266436576843, 536.2321953773499, 540.2764937877655, 544.3207921981812, 548.3831741809845, 552.4455561637878, 556.5338428020477, 560.6221294403076, 564.6480238437653, 568.6739182472229, 572.7398746013641, 576.8058309555054, 580.8689744472504, 584.9321179389954, 589.0186235904694, 593.1051292419434, 597.1996428966522, 601.2941565513611, 605.4061884880066, 609.5182204246521, 613.624089717865, 617.7299590110779, 621.7744739055634, 625.8189888000488, 629.8906593322754, 633.962329864502, 638.0455577373505, 642.128785610199, 646.1984012126923, 650.2680168151855, 654.3224792480469, 658.3769416809082, 662.500687122345, 666.6244325637817, 670.747974395752, 674.8715162277222, 678.9146981239319, 682.9578800201416, 687.0313732624054, 691.1048665046692, 695.2096476554871, 699.3144288063049, 703.3004205226898, 707.2864122390747, 711.2447636127472, 715.2031149864197, 719.1825406551361, 723.1619663238525, 727.5086941719055, 731.8554220199585, 736.22412276268, 740.5928235054016, 744.9673757553101, 749.3419280052185, 753.723237991333, 758.1045479774475, 762.5021870136261, 766.8998260498047, 770.8753976821899, 774.8509693145752, 778.7823243141174, 782.7136793136597, 786.6972584724426, 790.6808376312256, 794.619215965271, 798.5575942993164, 802.4852182865143, 806.4128422737122, 810.3967835903168, 814.3807249069214, 818.3505036830902, 822.320282459259, 826.2590527534485, 830.1978230476379, 834.1642937660217, 838.1307644844055, 842.0875508785248, 846.044337272644, 849.9974091053009, 853.9504809379578, 855.8468725681305, 857.7432641983032]
[29.595, 29.595, 36.61, 36.61, 43.01, 43.01, 45.765, 45.765, 49.9375, 49.9375, 51.6975, 51.6975, 55.595, 55.595, 56.655, 56.655, 57.48, 57.48, 59.04, 59.04, 59.985, 59.985, 60.515, 60.515, 62.58, 62.58, 63.3125, 63.3125, 65.12, 65.12, 65.5, 65.5, 67.4475, 67.4475, 67.89, 67.89, 68.505, 68.505, 69.4725, 69.4725, 69.595, 69.595, 69.9675, 69.9675, 69.8075, 69.8075, 70.2125, 70.2125, 70.775, 70.775, 70.9625, 70.9625, 71.365, 71.365, 71.67, 71.67, 71.75, 71.75, 72.1375, 72.1375, 72.435, 72.435, 72.2025, 72.2025, 72.34, 72.34, 72.5, 72.5, 72.6175, 72.6175, 72.88, 72.88, 73.0225, 73.0225, 72.9525, 72.9525, 73.73, 73.73, 73.84, 73.84, 73.525, 73.525, 73.72, 73.72, 73.7325, 73.7325, 73.85, 73.85, 73.8675, 73.8675, 73.505, 73.505, 73.4975, 73.4975, 73.9975, 73.9975, 74.385, 74.385, 74.125, 74.125, 74.1075, 74.1075, 74.16, 74.16, 73.9175, 73.9175, 73.9375, 73.9375, 73.9225, 73.9225, 74.485, 74.485, 74.23, 74.23, 74.615, 74.615, 74.69, 74.69, 75.005, 75.005, 74.985, 74.985, 75.26, 75.26, 75.3225, 75.3225, 75.1025, 75.1025, 74.82, 74.82, 74.98, 74.98, 75.2975, 75.2975, 75.165, 75.165, 75.35, 75.35, 75.5125, 75.5125, 75.5325, 75.5325, 75.37, 75.37, 75.54, 75.54, 75.2525, 75.2525, 75.4225, 75.4225, 75.845, 75.845, 75.8125, 75.8125, 75.4075, 75.4075, 75.71, 75.71, 76.0225, 76.0225, 75.855, 75.855, 76.2675, 76.2675, 75.9825, 75.9825, 76.12, 76.12, 76.0825, 76.0825, 76.325, 76.325, 75.8725, 75.8725, 76.0, 76.0, 76.1775, 76.1775, 75.75, 75.75, 75.4075, 75.4075, 75.6925, 75.6925, 75.0825, 75.0825, 75.64, 75.64, 75.4425, 75.4425, 76.0075, 76.0075, 75.6275, 75.6275, 75.81, 75.81, 75.9275, 75.9275, 75.85, 75.85, 76.3375, 76.3375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.215, Test loss: 1.997, Test accuracy: 29.27
Round   1, Train loss: 1.896, Test loss: 1.704, Test accuracy: 38.72
Round   2, Train loss: 1.686, Test loss: 1.625, Test accuracy: 43.02
Round   3, Train loss: 1.590, Test loss: 1.513, Test accuracy: 47.63
Round   4, Train loss: 1.514, Test loss: 1.437, Test accuracy: 51.30
Round   5, Train loss: 1.443, Test loss: 1.369, Test accuracy: 53.82
Round   6, Train loss: 1.397, Test loss: 1.309, Test accuracy: 56.00
Round   7, Train loss: 1.317, Test loss: 1.288, Test accuracy: 57.75
Round   8, Train loss: 1.279, Test loss: 1.238, Test accuracy: 59.00
Round   9, Train loss: 1.227, Test loss: 1.202, Test accuracy: 60.57
Round  10, Train loss: 1.240, Test loss: 1.103, Test accuracy: 62.61
Round  11, Train loss: 1.154, Test loss: 1.086, Test accuracy: 63.78
Round  12, Train loss: 1.130, Test loss: 1.061, Test accuracy: 64.40
Round  13, Train loss: 1.097, Test loss: 1.042, Test accuracy: 65.19
Round  14, Train loss: 1.073, Test loss: 1.036, Test accuracy: 65.22
Round  15, Train loss: 1.037, Test loss: 1.026, Test accuracy: 66.50
Round  16, Train loss: 1.079, Test loss: 0.930, Test accuracy: 68.72
Round  17, Train loss: 1.014, Test loss: 0.933, Test accuracy: 68.58
Round  18, Train loss: 0.988, Test loss: 0.926, Test accuracy: 68.89
Round  19, Train loss: 0.974, Test loss: 0.907, Test accuracy: 69.41
Round  20, Train loss: 0.949, Test loss: 0.884, Test accuracy: 70.39
Round  21, Train loss: 0.942, Test loss: 0.858, Test accuracy: 71.49
Round  22, Train loss: 0.903, Test loss: 0.848, Test accuracy: 71.60
Round  23, Train loss: 0.904, Test loss: 0.844, Test accuracy: 71.76
Round  24, Train loss: 0.880, Test loss: 0.834, Test accuracy: 72.31
Round  25, Train loss: 0.867, Test loss: 0.814, Test accuracy: 72.78
Round  26, Train loss: 0.844, Test loss: 0.817, Test accuracy: 73.07
Round  27, Train loss: 0.870, Test loss: 0.803, Test accuracy: 72.91
Round  28, Train loss: 0.830, Test loss: 0.799, Test accuracy: 73.29
Round  29, Train loss: 0.832, Test loss: 0.791, Test accuracy: 73.40
Round  30, Train loss: 0.804, Test loss: 0.786, Test accuracy: 73.78
Round  31, Train loss: 0.795, Test loss: 0.781, Test accuracy: 73.72
Round  32, Train loss: 0.769, Test loss: 0.784, Test accuracy: 73.53
Round  33, Train loss: 0.778, Test loss: 0.775, Test accuracy: 74.12
Round  34, Train loss: 0.775, Test loss: 0.760, Test accuracy: 74.61
Round  35, Train loss: 0.770, Test loss: 0.758, Test accuracy: 74.63
Round  36, Train loss: 0.734, Test loss: 0.760, Test accuracy: 74.58
Round  37, Train loss: 0.732, Test loss: 0.770, Test accuracy: 74.64
Round  38, Train loss: 0.739, Test loss: 0.757, Test accuracy: 74.89
Round  39, Train loss: 0.726, Test loss: 0.763, Test accuracy: 75.03
Round  40, Train loss: 0.706, Test loss: 0.772, Test accuracy: 74.23
Round  41, Train loss: 0.770, Test loss: 0.749, Test accuracy: 75.06
Round  42, Train loss: 0.713, Test loss: 0.738, Test accuracy: 75.43
Round  43, Train loss: 0.702, Test loss: 0.730, Test accuracy: 75.77
Round  44, Train loss: 0.678, Test loss: 0.725, Test accuracy: 75.84
Round  45, Train loss: 0.701, Test loss: 0.719, Test accuracy: 75.66
Round  46, Train loss: 0.712, Test loss: 0.724, Test accuracy: 75.69
Round  47, Train loss: 0.649, Test loss: 0.718, Test accuracy: 75.82
Round  48, Train loss: 0.712, Test loss: 0.711, Test accuracy: 76.43
Round  49, Train loss: 0.658, Test loss: 0.727, Test accuracy: 76.02
Round  50, Train loss: 0.693, Test loss: 0.726, Test accuracy: 75.78
Round  51, Train loss: 0.671, Test loss: 0.719, Test accuracy: 76.13
Round  52, Train loss: 0.668, Test loss: 0.709, Test accuracy: 76.21
Round  53, Train loss: 0.665, Test loss: 0.709, Test accuracy: 76.50
Round  54, Train loss: 0.662, Test loss: 0.705, Test accuracy: 76.69
Round  55, Train loss: 0.658, Test loss: 0.705, Test accuracy: 76.26
Round  56, Train loss: 0.642, Test loss: 0.705, Test accuracy: 76.50
Round  57, Train loss: 0.653, Test loss: 0.706, Test accuracy: 76.70
Round  58, Train loss: 0.639, Test loss: 0.706, Test accuracy: 76.42
Round  59, Train loss: 0.654, Test loss: 0.704, Test accuracy: 76.26
Round  60, Train loss: 0.619, Test loss: 0.703, Test accuracy: 76.43
Round  61, Train loss: 0.630, Test loss: 0.712, Test accuracy: 76.02
Round  62, Train loss: 0.609, Test loss: 0.694, Test accuracy: 76.82
Round  63, Train loss: 0.617, Test loss: 0.691, Test accuracy: 76.91
Round  64, Train loss: 0.635, Test loss: 0.689, Test accuracy: 77.09
Round  65, Train loss: 0.608, Test loss: 0.686, Test accuracy: 77.09
Round  66, Train loss: 0.606, Test loss: 0.696, Test accuracy: 76.94
Round  67, Train loss: 0.579, Test loss: 0.695, Test accuracy: 76.82
Round  68, Train loss: 0.587, Test loss: 0.697, Test accuracy: 76.76
Round  69, Train loss: 0.578, Test loss: 0.697, Test accuracy: 76.75
Round  70, Train loss: 0.599, Test loss: 0.693, Test accuracy: 76.71
Round  71, Train loss: 0.600, Test loss: 0.702, Test accuracy: 76.46
Round  72, Train loss: 0.550, Test loss: 0.702, Test accuracy: 76.78
Round  73, Train loss: 0.576, Test loss: 0.685, Test accuracy: 77.34
Round  74, Train loss: 0.567, Test loss: 0.687, Test accuracy: 77.11
Round  75, Train loss: 0.579, Test loss: 0.685, Test accuracy: 77.16
Round  76, Train loss: 0.576, Test loss: 0.692, Test accuracy: 76.95
Round  77, Train loss: 0.581, Test loss: 0.687, Test accuracy: 77.18
Round  78, Train loss: 0.567, Test loss: 0.690, Test accuracy: 77.16
Round  79, Train loss: 0.588, Test loss: 0.688, Test accuracy: 77.19
Round  80, Train loss: 0.560, Test loss: 0.692, Test accuracy: 77.36
Round  81, Train loss: 0.561, Test loss: 0.689, Test accuracy: 77.41
Round  82, Train loss: 0.543, Test loss: 0.689, Test accuracy: 77.25
Round  83, Train loss: 0.557, Test loss: 0.687, Test accuracy: 77.52
Round  84, Train loss: 0.576, Test loss: 0.679, Test accuracy: 77.69
Round  85, Train loss: 0.556, Test loss: 0.689, Test accuracy: 77.27
Round  86, Train loss: 0.551, Test loss: 0.685, Test accuracy: 77.47
Round  87, Train loss: 0.528, Test loss: 0.685, Test accuracy: 77.48
Round  88, Train loss: 0.548, Test loss: 0.682, Test accuracy: 77.72
Round  89, Train loss: 0.566, Test loss: 0.692, Test accuracy: 77.28
Round  90, Train loss: 0.550, Test loss: 0.681, Test accuracy: 77.60
Round  91, Train loss: 0.530, Test loss: 0.690, Test accuracy: 77.41
Round  92, Train loss: 0.528, Test loss: 0.687, Test accuracy: 77.40
Round  93, Train loss: 0.517, Test loss: 0.694, Test accuracy: 77.41
Round  94, Train loss: 0.528, Test loss: 0.700, Test accuracy: 77.38
Round  95, Train loss: 0.517, Test loss: 0.689, Test accuracy: 77.47
Round  96, Train loss: 0.513, Test loss: 0.690, Test accuracy: 77.66
Round  97, Train loss: 0.534, Test loss: 0.683, Test accuracy: 77.72
Round  98, Train loss: 0.540, Test loss: 0.681, Test accuracy: 77.55
Round  99, Train loss: 0.497, Test loss: 0.680, Test accuracy: 77.99
Final Round, Train loss: 0.422, Test loss: 0.680, Test accuracy: 78.00
Average accuracy final 10 rounds: 77.55799999999999
5344.035399675369
[6.026882886886597, 12.053765773773193, 17.72864842414856, 23.403531074523926, 29.118530750274658, 34.83353042602539, 40.537012577056885, 46.24049472808838, 51.96116399765015, 57.681833267211914, 63.4057354927063, 69.12963771820068, 74.85578036308289, 80.58192300796509, 86.36496305465698, 92.14800310134888, 97.85304307937622, 103.55808305740356, 109.29969358444214, 115.04130411148071, 120.79227995872498, 126.54325580596924, 132.25804615020752, 137.9728364944458, 143.6447570323944, 149.31667757034302, 155.01905131340027, 160.72142505645752, 166.42303800582886, 172.1246509552002, 177.5578441619873, 182.9910373687744, 188.67775011062622, 194.36446285247803, 200.02048254013062, 205.6765022277832, 211.3578543663025, 217.03920650482178, 222.7107150554657, 228.38222360610962, 234.07158088684082, 239.76093816757202, 245.43771982192993, 251.11450147628784, 256.817462682724, 262.52042388916016, 268.1641550064087, 273.8078861236572, 279.52306175231934, 285.23823738098145, 290.9548304080963, 296.6714234352112, 302.3546447753906, 308.03786611557007, 313.7018747329712, 319.3658833503723, 325.12695240974426, 330.8880214691162, 336.512966632843, 342.1379117965698, 347.75143933296204, 353.36496686935425, 359.03682351112366, 364.70868015289307, 370.3432765007019, 375.97787284851074, 381.59285378456116, 387.2078347206116, 392.81869626045227, 398.42955780029297, 404.09030079841614, 409.7510437965393, 415.38557505607605, 421.0201063156128, 426.68154096603394, 432.3429756164551, 437.92872428894043, 443.5144729614258, 449.12342858314514, 454.7323842048645, 460.3485896587372, 465.96479511260986, 471.6017179489136, 477.2386407852173, 482.86336040496826, 488.48808002471924, 494.087420463562, 499.6867609024048, 505.34662652015686, 511.00649213790894, 516.6055309772491, 522.2045698165894, 527.3069429397583, 532.4093160629272, 537.5476458072662, 542.6859755516052, 547.8316352367401, 552.977294921875, 558.1195030212402, 563.2617111206055, 568.3908553123474, 573.5199995040894, 578.6363289356232, 583.752658367157, 588.8696069717407, 593.9865555763245, 599.0753536224365, 604.1641516685486, 609.2897469997406, 614.4153423309326, 619.5575630664825, 624.6997838020325, 629.8594393730164, 635.0190949440002, 640.1601717472076, 645.301248550415, 650.4139268398285, 655.526605129242, 660.6175365447998, 665.7084679603577, 670.8620982170105, 676.0157284736633, 681.1552340984344, 686.2947397232056, 691.4272313117981, 696.5597229003906, 701.6836023330688, 706.8074817657471, 711.9585807323456, 717.1096796989441, 722.2019317150116, 727.2941837310791, 732.4155042171478, 737.5368247032166, 742.6283977031708, 747.719970703125, 752.8727996349335, 758.025628566742, 763.165363073349, 768.305097579956, 773.4484457969666, 778.591794013977, 783.7254536151886, 788.8591132164001, 793.9038670063019, 798.9486207962036, 804.0858428478241, 809.2230648994446, 814.3404335975647, 819.4578022956848, 824.576043844223, 829.6942853927612, 834.8245666027069, 839.9548478126526, 845.1669838428497, 850.3791198730469, 855.5163056850433, 860.6534914970398, 865.731654882431, 870.8098182678223, 875.9586391448975, 881.1074600219727, 886.2344951629639, 891.3615303039551, 896.510636806488, 901.659743309021, 906.8301331996918, 912.0005230903625, 917.165709733963, 922.3308963775635, 927.4658601284027, 932.600823879242, 937.6916177272797, 942.7824115753174, 947.9452376365662, 953.1080636978149, 958.2315719127655, 963.3550801277161, 968.5355155467987, 973.7159509658813, 978.8388216495514, 983.9616923332214, 989.0750663280487, 994.188440322876, 999.2720897197723, 1004.3557391166687, 1009.48513007164, 1014.6145210266113, 1019.8794746398926, 1025.1444282531738, 1030.3240356445312, 1035.5036430358887, 1040.6911673545837, 1045.8786916732788, 1051.0471470355988, 1056.2156023979187, 1061.3588802814484, 1066.502158164978, 1071.6170015335083, 1076.7318449020386, 1078.7770745754242, 1080.8223042488098]
[29.275, 29.275, 38.7225, 38.7225, 43.02, 43.02, 47.6325, 47.6325, 51.2975, 51.2975, 53.8175, 53.8175, 56.0025, 56.0025, 57.7525, 57.7525, 59.0025, 59.0025, 60.5725, 60.5725, 62.6125, 62.6125, 63.7825, 63.7825, 64.4025, 64.4025, 65.185, 65.185, 65.2175, 65.2175, 66.505, 66.505, 68.72, 68.72, 68.5825, 68.5825, 68.8875, 68.8875, 69.41, 69.41, 70.39, 70.39, 71.4925, 71.4925, 71.6025, 71.6025, 71.76, 71.76, 72.305, 72.305, 72.7775, 72.7775, 73.0675, 73.0675, 72.905, 72.905, 73.29, 73.29, 73.4025, 73.4025, 73.7825, 73.7825, 73.725, 73.725, 73.5275, 73.5275, 74.12, 74.12, 74.6075, 74.6075, 74.63, 74.63, 74.5775, 74.5775, 74.6425, 74.6425, 74.895, 74.895, 75.0275, 75.0275, 74.235, 74.235, 75.06, 75.06, 75.4325, 75.4325, 75.7675, 75.7675, 75.8425, 75.8425, 75.6625, 75.6625, 75.685, 75.685, 75.82, 75.82, 76.4275, 76.4275, 76.0175, 76.0175, 75.7825, 75.7825, 76.13, 76.13, 76.21, 76.21, 76.4975, 76.4975, 76.6925, 76.6925, 76.26, 76.26, 76.505, 76.505, 76.6975, 76.6975, 76.42, 76.42, 76.26, 76.26, 76.4275, 76.4275, 76.0225, 76.0225, 76.8225, 76.8225, 76.91, 76.91, 77.0875, 77.0875, 77.0925, 77.0925, 76.9375, 76.9375, 76.8225, 76.8225, 76.7575, 76.7575, 76.7525, 76.7525, 76.71, 76.71, 76.46, 76.46, 76.7775, 76.7775, 77.345, 77.345, 77.1075, 77.1075, 77.155, 77.155, 76.955, 76.955, 77.1825, 77.1825, 77.1575, 77.1575, 77.1925, 77.1925, 77.3575, 77.3575, 77.405, 77.405, 77.25, 77.25, 77.52, 77.52, 77.6875, 77.6875, 77.2725, 77.2725, 77.475, 77.475, 77.4825, 77.4825, 77.7225, 77.7225, 77.28, 77.28, 77.6, 77.6, 77.405, 77.405, 77.3975, 77.3975, 77.41, 77.41, 77.38, 77.38, 77.4675, 77.4675, 77.66, 77.66, 77.7175, 77.7175, 77.5525, 77.5525, 77.99, 77.99, 78.0025, 78.0025]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.191, Test loss: 1.851, Test accuracy: 35.10
Round   1, Train loss: 1.824, Test loss: 1.587, Test accuracy: 41.52
Round   2, Train loss: 1.644, Test loss: 1.498, Test accuracy: 44.66
Round   3, Train loss: 1.536, Test loss: 1.400, Test accuracy: 48.68
Round   4, Train loss: 1.421, Test loss: 1.313, Test accuracy: 52.03
Round   5, Train loss: 1.340, Test loss: 1.392, Test accuracy: 53.17
Round   6, Train loss: 1.277, Test loss: 1.194, Test accuracy: 57.30
Round   7, Train loss: 1.217, Test loss: 1.223, Test accuracy: 58.83
Round   8, Train loss: 1.192, Test loss: 1.144, Test accuracy: 59.49
Round   9, Train loss: 1.106, Test loss: 1.096, Test accuracy: 61.26
Round  10, Train loss: 1.078, Test loss: 1.096, Test accuracy: 61.40
Round  11, Train loss: 1.023, Test loss: 1.147, Test accuracy: 62.73
Round  12, Train loss: 1.041, Test loss: 1.049, Test accuracy: 61.13
Round  13, Train loss: 1.001, Test loss: 1.019, Test accuracy: 62.34
Round  14, Train loss: 0.962, Test loss: 1.029, Test accuracy: 64.19
Round  15, Train loss: 0.950, Test loss: 0.994, Test accuracy: 64.91
Round  16, Train loss: 0.900, Test loss: 1.100, Test accuracy: 66.17
Round  17, Train loss: 0.889, Test loss: 0.975, Test accuracy: 63.85
Round  18, Train loss: 0.900, Test loss: 0.968, Test accuracy: 63.95
Round  19, Train loss: 0.856, Test loss: 0.975, Test accuracy: 66.46
Round  20, Train loss: 0.846, Test loss: 1.080, Test accuracy: 67.27
Round  21, Train loss: 0.799, Test loss: 1.079, Test accuracy: 67.28
Round  22, Train loss: 0.792, Test loss: 0.977, Test accuracy: 67.12
Round  23, Train loss: 0.765, Test loss: 1.112, Test accuracy: 66.99
Round  24, Train loss: 0.755, Test loss: 0.969, Test accuracy: 67.19
Round  25, Train loss: 0.772, Test loss: 0.954, Test accuracy: 67.83
Round  26, Train loss: 0.728, Test loss: 1.233, Test accuracy: 68.14
Round  27, Train loss: 0.766, Test loss: 1.040, Test accuracy: 65.42
Round  28, Train loss: 0.742, Test loss: 0.930, Test accuracy: 66.11
Round  29, Train loss: 0.715, Test loss: 0.919, Test accuracy: 66.17
Round  30, Train loss: 0.694, Test loss: 0.904, Test accuracy: 66.37
Round  31, Train loss: 0.669, Test loss: 1.077, Test accuracy: 68.62
Round  32, Train loss: 0.688, Test loss: 1.043, Test accuracy: 66.10
Round  33, Train loss: 0.684, Test loss: 0.947, Test accuracy: 68.50
Round  34, Train loss: 0.652, Test loss: 1.087, Test accuracy: 69.57
Round  35, Train loss: 0.652, Test loss: 0.951, Test accuracy: 68.86
Round  36, Train loss: 0.640, Test loss: 0.909, Test accuracy: 67.29
Round  37, Train loss: 0.681, Test loss: 1.306, Test accuracy: 66.52
Round  38, Train loss: 0.656, Test loss: 1.103, Test accuracy: 69.53
Round  39, Train loss: 0.636, Test loss: 0.945, Test accuracy: 69.92
Round  40, Train loss: 0.599, Test loss: 1.283, Test accuracy: 69.58
Round  41, Train loss: 0.624, Test loss: 1.051, Test accuracy: 66.86
Round  42, Train loss: 0.632, Test loss: 1.087, Test accuracy: 70.21
Round  43, Train loss: 0.574, Test loss: 0.882, Test accuracy: 67.75
Round  44, Train loss: 0.615, Test loss: 1.122, Test accuracy: 70.40
Round  45, Train loss: 0.589, Test loss: 1.126, Test accuracy: 70.00
Round  46, Train loss: 0.611, Test loss: 1.054, Test accuracy: 67.05
Round  47, Train loss: 0.580, Test loss: 0.951, Test accuracy: 70.13
Round  48, Train loss: 0.586, Test loss: 0.964, Test accuracy: 69.73
Round  49, Train loss: 0.543, Test loss: 0.918, Test accuracy: 67.56
Round  50, Train loss: 0.528, Test loss: 0.961, Test accuracy: 70.19
Round  51, Train loss: 0.587, Test loss: 0.947, Test accuracy: 70.30
Round  52, Train loss: 0.547, Test loss: 0.892, Test accuracy: 67.80
Round  53, Train loss: 0.546, Test loss: 1.096, Test accuracy: 67.46
Round  54, Train loss: 0.578, Test loss: 0.890, Test accuracy: 68.34
Round  55, Train loss: 0.550, Test loss: 0.889, Test accuracy: 68.15
Round  56, Train loss: 0.556, Test loss: 0.883, Test accuracy: 68.71
Round  57, Train loss: 0.521, Test loss: 0.968, Test accuracy: 70.18
Round  58, Train loss: 0.552, Test loss: 0.926, Test accuracy: 70.02
Round  59, Train loss: 0.520, Test loss: 0.952, Test accuracy: 70.52
Round  60, Train loss: 0.534, Test loss: 0.958, Test accuracy: 70.27
Round  61, Train loss: 0.481, Test loss: 0.894, Test accuracy: 68.58
Round  62, Train loss: 0.515, Test loss: 0.961, Test accuracy: 70.61
Round  63, Train loss: 0.492, Test loss: 0.958, Test accuracy: 70.31
Round  64, Train loss: 0.511, Test loss: 1.136, Test accuracy: 70.97
Round  65, Train loss: 0.513, Test loss: 0.902, Test accuracy: 67.98
Round  66, Train loss: 0.530, Test loss: 0.955, Test accuracy: 70.68
Round  67, Train loss: 0.480, Test loss: 1.154, Test accuracy: 71.56
Round  68, Train loss: 0.509, Test loss: 0.904, Test accuracy: 68.44
Round  69, Train loss: 0.496, Test loss: 1.168, Test accuracy: 71.15
Round  70, Train loss: 0.496, Test loss: 0.982, Test accuracy: 70.92
Round  71, Train loss: 0.440, Test loss: 1.174, Test accuracy: 71.42
Round  72, Train loss: 0.473, Test loss: 1.142, Test accuracy: 71.10
Round  73, Train loss: 0.460, Test loss: 0.890, Test accuracy: 69.34
Round  74, Train loss: 0.493, Test loss: 0.966, Test accuracy: 70.89
Round  75, Train loss: 0.475, Test loss: 0.971, Test accuracy: 70.75
Round  76, Train loss: 0.447, Test loss: 1.144, Test accuracy: 71.25
Round  77, Train loss: 0.425, Test loss: 1.430, Test accuracy: 71.25
Round  78, Train loss: 0.486, Test loss: 1.468, Test accuracy: 68.09
Round  79, Train loss: 0.489, Test loss: 0.907, Test accuracy: 68.30
Round  80, Train loss: 0.467, Test loss: 1.180, Test accuracy: 71.66
Round  81, Train loss: 0.463, Test loss: 0.953, Test accuracy: 71.28
Round  82, Train loss: 0.431, Test loss: 1.434, Test accuracy: 71.62
Round  83, Train loss: 0.444, Test loss: 1.009, Test accuracy: 71.27
Round  84, Train loss: 0.407, Test loss: 1.448, Test accuracy: 70.87
Round  85, Train loss: 0.496, Test loss: 1.063, Test accuracy: 67.95
Round  86, Train loss: 0.431, Test loss: 1.227, Test accuracy: 71.72
Round  87, Train loss: 0.449, Test loss: 0.908, Test accuracy: 69.02
Round  88, Train loss: 0.442, Test loss: 1.130, Test accuracy: 67.97
Round  89, Train loss: 0.416, Test loss: 1.036, Test accuracy: 71.56
Round  90, Train loss: 0.425, Test loss: 0.938, Test accuracy: 68.41
Round  91, Train loss: 0.454, Test loss: 1.222, Test accuracy: 71.25
Round  92, Train loss: 0.427, Test loss: 1.204, Test accuracy: 68.46
Round  93, Train loss: 0.425, Test loss: 0.901, Test accuracy: 69.03
Round  94, Train loss: 0.376, Test loss: 1.210, Test accuracy: 71.94
Round  95, Train loss: 0.425, Test loss: 0.996, Test accuracy: 71.38
Round  96, Train loss: 0.408, Test loss: 0.937, Test accuracy: 68.97
Round  97, Train loss: 0.441, Test loss: 0.958, Test accuracy: 71.97
Round  98, Train loss: 0.397, Test loss: 1.436, Test accuracy: 71.75
Round  99, Train loss: 0.427, Test loss: 1.175, Test accuracy: 68.26
Final Round, Train loss: 0.332, Test loss: 0.922, Test accuracy: 71.94
Average accuracy final 10 rounds: 70.14075000000001
8726.585144042969
[13.241506099700928, 26.11647391319275, 39.107260942459106, 51.92325496673584, 64.88166761398315, 77.79188632965088, 90.73203420639038, 103.69590973854065, 116.62110304832458, 129.50643110275269, 142.35699319839478, 155.2232313156128, 168.0766544342041, 181.0035219192505, 193.917498588562, 206.92675256729126, 219.84698009490967, 232.8032157421112, 245.71680688858032, 258.6328229904175, 271.59096598625183, 284.61824131011963, 297.5524260997772, 310.47993421554565, 323.40919280052185, 336.3166217803955, 349.2914798259735, 362.2364995479584, 375.0408465862274, 387.93163299560547, 400.733416557312, 413.68119502067566, 426.5259532928467, 439.401983499527, 452.3200821876526, 465.2434322834015, 478.18132615089417, 491.1411786079407, 504.06041073799133, 516.9749944210052, 529.8987073898315, 542.7969267368317, 555.7123529911041, 568.6071574687958, 581.5409433841705, 594.4273979663849, 607.3314392566681, 620.2012453079224, 633.039085149765, 645.8885681629181, 658.6825611591339, 671.442168712616, 684.3428964614868, 697.164370059967, 709.9852695465088, 722.6995363235474, 735.2943768501282, 748.0018935203552, 760.723105430603, 773.3947565555573, 785.9833462238312, 798.6741573810577, 811.2926032543182, 824.0401248931885, 836.7738325595856, 849.594434261322, 862.4045653343201, 875.0747606754303, 887.8946058750153, 900.7249507904053, 913.5473935604095, 926.4196815490723, 939.4084873199463, 952.371444940567, 965.3595387935638, 978.3085069656372, 991.2658088207245, 1004.2267985343933, 1017.1945354938507, 1030.1719796657562, 1043.1487214565277, 1056.097586631775, 1069.0781919956207, 1082.0371572971344, 1094.8693025112152, 1107.629497051239, 1120.433923959732, 1133.339504957199, 1146.0591163635254, 1158.8143033981323, 1172.2568027973175, 1185.015294790268, 1197.730233669281, 1210.4034507274628, 1223.2294702529907, 1234.0778295993805, 1244.9357583522797, 1255.7931501865387, 1266.6717405319214, 1277.4906213283539, 1280.2329297065735]
[35.0975, 41.525, 44.665, 48.6825, 52.0275, 53.1725, 57.295, 58.8275, 59.4925, 61.2575, 61.395, 62.725, 61.1275, 62.3425, 64.185, 64.91, 66.17, 63.8525, 63.9525, 66.4575, 67.265, 67.275, 67.125, 66.9875, 67.1875, 67.825, 68.135, 65.415, 66.11, 66.1675, 66.3675, 68.62, 66.1, 68.495, 69.5675, 68.86, 67.29, 66.5175, 69.535, 69.925, 69.5775, 66.86, 70.2075, 67.7475, 70.3975, 69.995, 67.0475, 70.13, 69.7325, 67.56, 70.19, 70.3025, 67.795, 67.4575, 68.34, 68.15, 68.7125, 70.1775, 70.02, 70.5225, 70.2675, 68.58, 70.6125, 70.305, 70.9725, 67.9825, 70.6775, 71.555, 68.44, 71.15, 70.9225, 71.425, 71.1025, 69.3425, 70.89, 70.7475, 71.245, 71.2475, 68.0925, 68.2975, 71.6625, 71.2825, 71.62, 71.265, 70.87, 67.9525, 71.725, 69.0225, 67.965, 71.555, 68.4075, 71.255, 68.46, 69.0275, 71.935, 71.375, 68.975, 71.965, 71.7475, 68.26, 71.945]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.277, Test loss: 2.303, Test accuracy: 6.71
Round   0, Global train loss: 2.277, Global test loss: 2.306, Global test accuracy: 6.67
Round   1, Train loss: 2.264, Test loss: 2.301, Test accuracy: 6.98
Round   1, Global train loss: 2.264, Global test loss: 2.305, Global test accuracy: 6.67
Round   2, Train loss: 2.270, Test loss: 2.300, Test accuracy: 6.99
Round   2, Global train loss: 2.270, Global test loss: 2.305, Global test accuracy: 6.67
Round   3, Train loss: 2.258, Test loss: 2.298, Test accuracy: 7.59
Round   3, Global train loss: 2.258, Global test loss: 2.304, Global test accuracy: 6.83
Round   4, Train loss: 2.260, Test loss: 2.296, Test accuracy: 7.96
Round   4, Global train loss: 2.260, Global test loss: 2.303, Global test accuracy: 6.99
Round   5, Train loss: 2.281, Test loss: 2.297, Test accuracy: 8.85
Round   5, Global train loss: 2.281, Global test loss: 2.304, Global test accuracy: 7.55
Round   6, Train loss: 2.311, Test loss: 2.307, Test accuracy: 8.24
Round   6, Global train loss: 2.311, Global test loss: 2.305, Global test accuracy: 9.31
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 7.78
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 6.71
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 6.51
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 7.00
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 8.50
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 10.05
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 10.05
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

1498.8139863014221
[1.89534330368042, 3.531271457672119, 5.161009073257446, 6.806232929229736, 8.458988666534424, 10.099047660827637, 11.464618682861328, 12.820544481277466, 14.18913459777832, 15.5513334274292, 16.909931182861328, 18.270455837249756, 19.63108801841736, 20.99682879447937, 22.354626893997192, 23.707630395889282, 25.07410216331482, 26.435524940490723, 27.79666519165039, 29.16121768951416, 30.5274600982666, 31.888389110565186, 33.253329277038574, 34.61459755897522, 35.97658061981201, 37.34013080596924, 38.69893503189087, 40.061081886291504, 41.43702054023743, 42.81780505180359, 44.17942714691162, 45.54134964942932, 46.89651131629944, 48.26198077201843, 49.62369203567505, 50.98018479347229, 52.34237837791443, 53.70499539375305, 55.06285619735718, 56.422343254089355, 57.78311634063721, 59.141305685043335, 60.49918222427368, 61.857481718063354, 63.220486640930176, 64.58547520637512, 65.95443773269653, 67.312002658844, 68.67392611503601, 70.03951907157898, 71.39162516593933, 72.75554466247559, 74.12286806106567, 75.47871160507202, 76.83875679969788, 78.20040369033813, 79.56124877929688, 80.92017769813538, 82.27367162704468, 83.63744282722473, 84.99793028831482, 86.35805130004883, 87.71131157875061, 89.07373332977295, 90.43652486801147, 91.80005598068237, 93.15922141075134, 94.52516078948975, 95.88226342201233, 97.23918795585632, 98.59826064109802, 99.96311116218567, 101.32065844535828, 102.675044298172, 104.03108406066895, 105.41144514083862, 106.77198338508606, 108.12857460975647, 109.49456477165222, 110.85723233222961, 112.21625685691833, 113.57280969619751, 114.93589806556702, 116.29946160316467, 117.6573576927185, 119.01356315612793, 120.38068175315857, 121.73879289627075, 123.10021424293518, 124.4650707244873, 125.82249283790588, 127.18544793128967, 128.54128885269165, 129.89941263198853, 131.26303219795227, 132.61052536964417, 133.9600648880005, 135.3165259361267, 136.67804265022278, 138.0324935913086, 140.29871559143066]
[6.708333333333333, 6.983333333333333, 6.991666666666666, 7.591666666666667, 7.958333333333333, 8.85, 8.241666666666667, 7.783333333333333, 6.708333333333333, 6.508333333333334, 7.0, 8.5, 10.05, 10.05, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.995, Test loss: 1.935, Test accuracy: 32.50
Round   0, Global train loss: 1.995, Global test loss: 1.996, Global test accuracy: 31.49
Round   1, Train loss: 1.713, Test loss: 1.762, Test accuracy: 38.21
Round   1, Global train loss: 1.713, Global test loss: 1.869, Global test accuracy: 37.63
Round   2, Train loss: 1.577, Test loss: 1.669, Test accuracy: 41.49
Round   2, Global train loss: 1.577, Global test loss: 1.778, Global test accuracy: 42.02
Round   3, Train loss: 1.477, Test loss: 1.566, Test accuracy: 44.36
Round   3, Global train loss: 1.477, Global test loss: 1.717, Global test accuracy: 44.11
Round   4, Train loss: 1.394, Test loss: 1.508, Test accuracy: 47.03
Round   4, Global train loss: 1.394, Global test loss: 1.689, Global test accuracy: 45.91
Round   5, Train loss: 1.315, Test loss: 1.431, Test accuracy: 49.02
Round   5, Global train loss: 1.315, Global test loss: 1.579, Global test accuracy: 46.83
Round   6, Train loss: 1.264, Test loss: 1.368, Test accuracy: 51.43
Round   6, Global train loss: 1.264, Global test loss: 1.625, Global test accuracy: 47.40
Round   7, Train loss: 1.226, Test loss: 1.326, Test accuracy: 53.02
Round   7, Global train loss: 1.226, Global test loss: 1.607, Global test accuracy: 48.92
Round   8, Train loss: 1.170, Test loss: 1.275, Test accuracy: 54.73
Round   8, Global train loss: 1.170, Global test loss: 1.623, Global test accuracy: 49.89
Round   9, Train loss: 1.113, Test loss: 1.234, Test accuracy: 56.39
Round   9, Global train loss: 1.113, Global test loss: 1.591, Global test accuracy: 50.08
Round  10, Train loss: 1.080, Test loss: 1.206, Test accuracy: 57.51
Round  10, Global train loss: 1.080, Global test loss: 1.503, Global test accuracy: 50.70
Round  11, Train loss: 1.042, Test loss: 1.193, Test accuracy: 58.27
Round  11, Global train loss: 1.042, Global test loss: 1.671, Global test accuracy: 51.50
Round  12, Train loss: 1.010, Test loss: 1.183, Test accuracy: 58.73
Round  12, Global train loss: 1.010, Global test loss: 1.485, Global test accuracy: 51.17
Round  13, Train loss: 0.980, Test loss: 1.166, Test accuracy: 59.68
Round  13, Global train loss: 0.980, Global test loss: 1.455, Global test accuracy: 52.84
Round  14, Train loss: 0.955, Test loss: 1.137, Test accuracy: 60.84
Round  14, Global train loss: 0.955, Global test loss: 1.552, Global test accuracy: 52.82
Round  15, Train loss: 0.959, Test loss: 1.104, Test accuracy: 62.01
Round  15, Global train loss: 0.959, Global test loss: 1.471, Global test accuracy: 54.21
Round  16, Train loss: 0.921, Test loss: 1.103, Test accuracy: 62.30
Round  16, Global train loss: 0.921, Global test loss: 1.428, Global test accuracy: 53.63
Round  17, Train loss: 0.879, Test loss: 1.084, Test accuracy: 62.98
Round  17, Global train loss: 0.879, Global test loss: 1.524, Global test accuracy: 54.81
Round  18, Train loss: 0.885, Test loss: 1.054, Test accuracy: 64.22
Round  18, Global train loss: 0.885, Global test loss: 1.434, Global test accuracy: 53.67
Round  19, Train loss: 0.849, Test loss: 1.037, Test accuracy: 65.02
Round  19, Global train loss: 0.849, Global test loss: 1.388, Global test accuracy: 54.63
Round  20, Train loss: 0.834, Test loss: 1.028, Test accuracy: 65.61
Round  20, Global train loss: 0.834, Global test loss: 1.466, Global test accuracy: 54.51
Round  21, Train loss: 0.806, Test loss: 1.025, Test accuracy: 65.74
Round  21, Global train loss: 0.806, Global test loss: 1.393, Global test accuracy: 54.62
Round  22, Train loss: 0.803, Test loss: 1.005, Test accuracy: 66.48
Round  22, Global train loss: 0.803, Global test loss: 1.457, Global test accuracy: 55.12
Round  23, Train loss: 0.783, Test loss: 1.007, Test accuracy: 66.57
Round  23, Global train loss: 0.783, Global test loss: 1.419, Global test accuracy: 54.53
Round  24, Train loss: 0.762, Test loss: 1.007, Test accuracy: 66.66
Round  24, Global train loss: 0.762, Global test loss: 1.563, Global test accuracy: 54.67
Round  25, Train loss: 0.757, Test loss: 1.018, Test accuracy: 66.66
Round  25, Global train loss: 0.757, Global test loss: 1.515, Global test accuracy: 55.78
Round  26, Train loss: 0.761, Test loss: 1.013, Test accuracy: 67.01
Round  26, Global train loss: 0.761, Global test loss: 1.509, Global test accuracy: 56.63
Round  27, Train loss: 0.728, Test loss: 0.999, Test accuracy: 67.61
Round  27, Global train loss: 0.728, Global test loss: 1.478, Global test accuracy: 55.05
Round  28, Train loss: 0.722, Test loss: 0.981, Test accuracy: 68.29
Round  28, Global train loss: 0.722, Global test loss: 1.552, Global test accuracy: 56.50
Round  29, Train loss: 0.697, Test loss: 0.977, Test accuracy: 68.51
Round  29, Global train loss: 0.697, Global test loss: 1.530, Global test accuracy: 55.95
Round  30, Train loss: 0.696, Test loss: 0.983, Test accuracy: 68.53
Round  30, Global train loss: 0.696, Global test loss: 1.398, Global test accuracy: 55.67
Round  31, Train loss: 0.697, Test loss: 0.957, Test accuracy: 69.08
Round  31, Global train loss: 0.697, Global test loss: 1.488, Global test accuracy: 56.12
Round  32, Train loss: 0.656, Test loss: 0.969, Test accuracy: 68.86
Round  32, Global train loss: 0.656, Global test loss: 1.438, Global test accuracy: 56.26
Round  33, Train loss: 0.648, Test loss: 0.991, Test accuracy: 68.53
Round  33, Global train loss: 0.648, Global test loss: 1.440, Global test accuracy: 56.26
Round  34, Train loss: 0.695, Test loss: 0.987, Test accuracy: 68.66
Round  34, Global train loss: 0.695, Global test loss: 1.471, Global test accuracy: 57.27
Round  35, Train loss: 0.649, Test loss: 0.975, Test accuracy: 69.14
Round  35, Global train loss: 0.649, Global test loss: 1.431, Global test accuracy: 55.85
Round  36, Train loss: 0.629, Test loss: 0.970, Test accuracy: 69.44
Round  36, Global train loss: 0.629, Global test loss: 1.537, Global test accuracy: 56.41
Round  37, Train loss: 0.619, Test loss: 0.966, Test accuracy: 69.60
Round  37, Global train loss: 0.619, Global test loss: 1.495, Global test accuracy: 56.72
Round  38, Train loss: 0.626, Test loss: 0.978, Test accuracy: 69.42
Round  38, Global train loss: 0.626, Global test loss: 1.582, Global test accuracy: 57.32
Round  39, Train loss: 0.625, Test loss: 0.970, Test accuracy: 69.43
Round  39, Global train loss: 0.625, Global test loss: 1.407, Global test accuracy: 55.78
Round  40, Train loss: 0.642, Test loss: 0.970, Test accuracy: 69.62
Round  40, Global train loss: 0.642, Global test loss: 1.455, Global test accuracy: 56.22
Round  41, Train loss: 0.590, Test loss: 0.960, Test accuracy: 70.21
Round  41, Global train loss: 0.590, Global test loss: 1.436, Global test accuracy: 57.48
Round  42, Train loss: 0.608, Test loss: 0.971, Test accuracy: 70.05
Round  42, Global train loss: 0.608, Global test loss: 1.647, Global test accuracy: 56.25
Round  43, Train loss: 0.593, Test loss: 0.968, Test accuracy: 70.27
Round  43, Global train loss: 0.593, Global test loss: 1.502, Global test accuracy: 56.51
Round  44, Train loss: 0.574, Test loss: 0.975, Test accuracy: 70.16
Round  44, Global train loss: 0.574, Global test loss: 1.727, Global test accuracy: 58.32
Round  45, Train loss: 0.581, Test loss: 0.965, Test accuracy: 70.40
Round  45, Global train loss: 0.581, Global test loss: 1.434, Global test accuracy: 57.48
Round  46, Train loss: 0.565, Test loss: 0.961, Test accuracy: 70.64
Round  46, Global train loss: 0.565, Global test loss: 1.502, Global test accuracy: 55.82
Round  47, Train loss: 0.584, Test loss: 0.959, Test accuracy: 70.77
Round  47, Global train loss: 0.584, Global test loss: 1.454, Global test accuracy: 55.60
Round  48, Train loss: 0.570, Test loss: 0.966, Test accuracy: 70.47
Round  48, Global train loss: 0.570, Global test loss: 1.529, Global test accuracy: 57.95
Round  49, Train loss: 0.566, Test loss: 0.959, Test accuracy: 70.78
Round  49, Global train loss: 0.566, Global test loss: 1.411, Global test accuracy: 57.15
Round  50, Train loss: 0.552, Test loss: 0.943, Test accuracy: 71.26
Round  50, Global train loss: 0.552, Global test loss: 1.428, Global test accuracy: 57.49
Round  51, Train loss: 0.578, Test loss: 0.937, Test accuracy: 71.41
Round  51, Global train loss: 0.578, Global test loss: 1.452, Global test accuracy: 55.99
Round  52, Train loss: 0.542, Test loss: 0.952, Test accuracy: 71.46
Round  52, Global train loss: 0.542, Global test loss: 1.619, Global test accuracy: 58.37
Round  53, Train loss: 0.529, Test loss: 0.945, Test accuracy: 71.47
Round  53, Global train loss: 0.529, Global test loss: 1.595, Global test accuracy: 58.88
Round  54, Train loss: 0.544, Test loss: 0.947, Test accuracy: 71.38
Round  54, Global train loss: 0.544, Global test loss: 1.513, Global test accuracy: 56.77
Round  55, Train loss: 0.551, Test loss: 0.950, Test accuracy: 71.36
Round  55, Global train loss: 0.551, Global test loss: 1.422, Global test accuracy: 56.65
Round  56, Train loss: 0.534, Test loss: 0.947, Test accuracy: 71.61
Round  56, Global train loss: 0.534, Global test loss: 1.640, Global test accuracy: 56.53
Round  57, Train loss: 0.542, Test loss: 0.945, Test accuracy: 71.75
Round  57, Global train loss: 0.542, Global test loss: 1.588, Global test accuracy: 57.77
Round  58, Train loss: 0.526, Test loss: 0.941, Test accuracy: 71.96
Round  58, Global train loss: 0.526, Global test loss: 1.543, Global test accuracy: 57.38
Round  59, Train loss: 0.513, Test loss: 0.955, Test accuracy: 71.75
Round  59, Global train loss: 0.513, Global test loss: 1.654, Global test accuracy: 57.12
Round  60, Train loss: 0.479, Test loss: 0.959, Test accuracy: 71.73
Round  60, Global train loss: 0.479, Global test loss: 1.466, Global test accuracy: 56.79
Round  61, Train loss: 0.504, Test loss: 0.963, Test accuracy: 71.89
Round  61, Global train loss: 0.504, Global test loss: 1.640, Global test accuracy: 56.95
Round  62, Train loss: 0.523, Test loss: 0.970, Test accuracy: 71.85
Round  62, Global train loss: 0.523, Global test loss: 1.448, Global test accuracy: 57.30
Round  63, Train loss: 0.519, Test loss: 0.968, Test accuracy: 71.90
Round  63, Global train loss: 0.519, Global test loss: 1.570, Global test accuracy: 57.44
Round  64, Train loss: 0.503, Test loss: 0.965, Test accuracy: 71.87
Round  64, Global train loss: 0.503, Global test loss: 1.590, Global test accuracy: 57.54
Round  65, Train loss: 0.512, Test loss: 0.965, Test accuracy: 72.01
Round  65, Global train loss: 0.512, Global test loss: 1.623, Global test accuracy: 59.13
Round  66, Train loss: 0.463, Test loss: 0.970, Test accuracy: 71.97
Round  66, Global train loss: 0.463, Global test loss: 1.512, Global test accuracy: 57.53
Round  67, Train loss: 0.509, Test loss: 0.969, Test accuracy: 72.16
Round  67, Global train loss: 0.509, Global test loss: 1.725, Global test accuracy: 58.97
Round  68, Train loss: 0.492, Test loss: 0.977, Test accuracy: 72.11
Round  68, Global train loss: 0.492, Global test loss: 1.431, Global test accuracy: 57.78
Round  69, Train loss: 0.478, Test loss: 0.972, Test accuracy: 72.23
Round  69, Global train loss: 0.478, Global test loss: 1.503, Global test accuracy: 57.14
Round  70, Train loss: 0.494, Test loss: 0.977, Test accuracy: 72.33
Round  70, Global train loss: 0.494, Global test loss: 1.614, Global test accuracy: 57.26
Round  71, Train loss: 0.467, Test loss: 0.980, Test accuracy: 72.46
Round  71, Global train loss: 0.467, Global test loss: 1.719, Global test accuracy: 58.07
Round  72, Train loss: 0.464, Test loss: 0.974, Test accuracy: 72.41
Round  72, Global train loss: 0.464, Global test loss: 1.705, Global test accuracy: 56.38
Round  73, Train loss: 0.477, Test loss: 0.972, Test accuracy: 72.30
Round  73, Global train loss: 0.477, Global test loss: 1.626, Global test accuracy: 57.74
Round  74, Train loss: 0.447, Test loss: 0.980, Test accuracy: 72.29
Round  74, Global train loss: 0.447, Global test loss: 1.451, Global test accuracy: 58.12
Round  75, Train loss: 0.450, Test loss: 0.971, Test accuracy: 72.47
Round  75, Global train loss: 0.450, Global test loss: 1.629, Global test accuracy: 57.37
Round  76, Train loss: 0.493, Test loss: 0.963, Test accuracy: 72.70
Round  76, Global train loss: 0.493, Global test loss: 1.479, Global test accuracy: 57.80
Round  77, Train loss: 0.443, Test loss: 0.960, Test accuracy: 72.75
Round  77, Global train loss: 0.443, Global test loss: 1.709, Global test accuracy: 58.04
Round  78, Train loss: 0.454, Test loss: 0.964, Test accuracy: 72.68
Round  78, Global train loss: 0.454, Global test loss: 1.557, Global test accuracy: 57.62
Round  79, Train loss: 0.465, Test loss: 0.965, Test accuracy: 72.72
Round  79, Global train loss: 0.465, Global test loss: 1.621, Global test accuracy: 57.77
Round  80, Train loss: 0.455, Test loss: 0.979, Test accuracy: 72.66
Round  80, Global train loss: 0.455, Global test loss: 1.493, Global test accuracy: 58.17
Round  81, Train loss: 0.443, Test loss: 0.989, Test accuracy: 72.41
Round  81, Global train loss: 0.443, Global test loss: 1.545, Global test accuracy: 57.10
Round  82, Train loss: 0.437, Test loss: 0.987, Test accuracy: 72.57
Round  82, Global train loss: 0.437, Global test loss: 1.809, Global test accuracy: 58.05
Round  83, Train loss: 0.470, Test loss: 0.987, Test accuracy: 72.56
Round  83, Global train loss: 0.470, Global test loss: 1.780, Global test accuracy: 59.11
Round  84, Train loss: 0.471, Test loss: 0.987, Test accuracy: 72.57
Round  84, Global train loss: 0.471, Global test loss: 1.488, Global test accuracy: 56.59
Round  85, Train loss: 0.451, Test loss: 0.989, Test accuracy: 72.60
Round  85, Global train loss: 0.451, Global test loss: 1.617, Global test accuracy: 56.26
Round  86, Train loss: 0.438, Test loss: 0.983, Test accuracy: 72.83
Round  86, Global train loss: 0.438, Global test loss: 1.673, Global test accuracy: 58.56
Round  87, Train loss: 0.429, Test loss: 0.982, Test accuracy: 72.89
Round  87, Global train loss: 0.429, Global test loss: 1.489, Global test accuracy: 57.56
Round  88, Train loss: 0.448, Test loss: 0.980, Test accuracy: 72.80
Round  88, Global train loss: 0.448, Global test loss: 1.704, Global test accuracy: 58.43
Round  89, Train loss: 0.395, Test loss: 0.990, Test accuracy: 72.88
Round  89, Global train loss: 0.395, Global test loss: 1.671, Global test accuracy: 58.14
Round  90, Train loss: 0.409, Test loss: 0.996, Test accuracy: 72.74
Round  90, Global train loss: 0.409, Global test loss: 1.715, Global test accuracy: 59.18
Round  91, Train loss: 0.402, Test loss: 0.997, Test accuracy: 72.61
Round  91, Global train loss: 0.402, Global test loss: 1.735, Global test accuracy: 58.48
Round  92, Train loss: 0.457, Test loss: 0.990, Test accuracy: 72.88
Round  92, Global train loss: 0.457, Global test loss: 1.609, Global test accuracy: 57.56
Round  93, Train loss: 0.429, Test loss: 0.986, Test accuracy: 72.81
Round  93, Global train loss: 0.429, Global test loss: 1.467, Global test accuracy: 57.61
Round  94, Train loss: 0.429, Test loss: 0.972, Test accuracy: 73.07
Round  94, Global train loss: 0.429, Global test loss: 1.419, Global test accuracy: 58.04
Round  95, Train loss: 0.434, Test loss: 0.978, Test accuracy: 73.12
Round  95, Global train loss: 0.434, Global test loss: 1.638, Global test accuracy: 57.36
Round  96, Train loss: 0.390, Test loss: 0.972, Test accuracy: 73.31
Round  96, Global train loss: 0.390, Global test loss: 1.602, Global test accuracy: 58.42
Round  97, Train loss: 0.427, Test loss: 0.991, Test accuracy: 73.03
Round  97, Global train loss: 0.427, Global test loss: 1.559, Global test accuracy: 57.97
Round  98, Train loss: 0.417, Test loss: 1.000, Test accuracy: 73.01
Round  98, Global train loss: 0.417, Global test loss: 1.633, Global test accuracy: 58.64
Round  99, Train loss: 0.419, Test loss: 0.993, Test accuracy: 73.17
Round  99, Global train loss: 0.419, Global test loss: 1.788, Global test accuracy: 58.35
Final Round, Train loss: 0.277, Test loss: 1.085, Test accuracy: 73.72
Final Round, Global train loss: 0.277, Global test loss: 1.788, Global test accuracy: 58.35
Average accuracy final 10 rounds: 72.97524999999999 

Average global accuracy final 10 rounds: 58.161500000000004 

5884.528660058975
[5.302435874938965, 10.60487174987793, 15.454488754272461, 20.304105758666992, 25.200372219085693, 30.096638679504395, 35.032827615737915, 39.969016551971436, 44.88524913787842, 49.8014817237854, 54.696613073349, 59.5917444229126, 64.49788451194763, 69.40402460098267, 74.3132758140564, 79.22252702713013, 84.12845969200134, 89.03439235687256, 93.94330883026123, 98.8522253036499, 103.74693727493286, 108.64164924621582, 113.5272901058197, 118.41293096542358, 123.26660346984863, 128.12027597427368, 133.0434696674347, 137.9666633605957, 142.88100004196167, 147.79533672332764, 152.71647119522095, 157.63760566711426, 162.5486285686493, 167.45965147018433, 172.37038159370422, 177.28111171722412, 182.19631600379944, 187.11152029037476, 192.03596830368042, 196.96041631698608, 201.9006164073944, 206.84081649780273, 211.74920082092285, 216.65758514404297, 220.88872146606445, 225.11985778808594, 229.35771703720093, 233.59557628631592, 237.83734726905823, 242.07911825180054, 246.3163959980011, 250.55367374420166, 254.78274297714233, 259.011812210083, 263.24261450767517, 267.47341680526733, 271.7098069190979, 275.94619703292847, 280.1833782196045, 284.4205594062805, 288.6194221973419, 292.8182849884033, 296.9709675312042, 301.1236500740051, 305.304527759552, 309.4854054450989, 313.6610915660858, 317.83677768707275, 322.0258252620697, 326.21487283706665, 330.41809582710266, 334.6213188171387, 338.82883882522583, 343.036358833313, 347.21103048324585, 351.3857021331787, 355.58546447753906, 359.7852268218994, 364.0257124900818, 368.26619815826416, 372.51352643966675, 376.76085472106934, 380.9993317127228, 385.2378087043762, 389.46774911880493, 393.69768953323364, 397.9410583972931, 402.18442726135254, 406.4242811203003, 410.66413497924805, 414.898521900177, 419.13290882110596, 423.3655335903168, 427.5981583595276, 431.83311581611633, 436.0680732727051, 440.29800844192505, 444.527943611145, 448.7646198272705, 453.001296043396, 457.2200701236725, 461.438844203949, 465.652551651001, 469.866259098053, 474.0631477832794, 478.26003646850586, 482.4614007472992, 486.66276502609253, 490.8668882846832, 495.0710115432739, 499.28229999542236, 503.4935884475708, 507.7090337276459, 511.92447900772095, 516.1476013660431, 520.3707237243652, 524.6112670898438, 528.8518104553223, 533.0908169746399, 537.3298234939575, 541.5756721496582, 545.8215208053589, 550.0509185791016, 554.2803163528442, 558.5200953483582, 562.7598743438721, 566.9839317798615, 571.2079892158508, 575.439563035965, 579.6711368560791, 583.9047436714172, 588.1383504867554, 592.3628351688385, 596.5873198509216, 600.8138899803162, 605.0404601097107, 609.2855501174927, 613.5306401252747, 617.7505264282227, 621.9704127311707, 626.1846778392792, 630.3989429473877, 634.6380400657654, 638.8771371841431, 643.0769591331482, 647.2767810821533, 651.4833846092224, 655.6899881362915, 659.891753911972, 664.0935196876526, 668.305810213089, 672.5181007385254, 676.7189786434174, 680.9198565483093, 685.1518223285675, 689.3837881088257, 693.5933179855347, 697.8028478622437, 702.016664981842, 706.2304821014404, 710.480628490448, 714.7307748794556, 718.9389362335205, 723.1470975875854, 727.380001783371, 731.6129059791565, 735.8514471054077, 740.0899882316589, 744.3159704208374, 748.5419526100159, 752.7616736888885, 756.9813947677612, 761.192768573761, 765.4041423797607, 769.6314611434937, 773.8587799072266, 778.0901856422424, 782.3215913772583, 786.5538272857666, 790.7860631942749, 794.998242855072, 799.2104225158691, 803.4075789451599, 807.6047353744507, 811.8684868812561, 816.1322383880615, 820.4027698040009, 824.6733012199402, 828.927832365036, 833.1823635101318, 837.454870223999, 841.7273769378662, 845.9951887130737, 850.2630004882812, 854.5284731388092, 858.7939457893372, 863.0491805076599, 867.3044152259827, 871.5913577079773, 875.8783001899719, 878.0348091125488, 880.1913180351257]
[32.5, 32.5, 38.21, 38.21, 41.49, 41.49, 44.3625, 44.3625, 47.0275, 47.0275, 49.02, 49.02, 51.4325, 51.4325, 53.02, 53.02, 54.7325, 54.7325, 56.39, 56.39, 57.505, 57.505, 58.2675, 58.2675, 58.735, 58.735, 59.6825, 59.6825, 60.84, 60.84, 62.01, 62.01, 62.305, 62.305, 62.9825, 62.9825, 64.2175, 64.2175, 65.015, 65.015, 65.605, 65.605, 65.7425, 65.7425, 66.4825, 66.4825, 66.5725, 66.5725, 66.6575, 66.6575, 66.655, 66.655, 67.01, 67.01, 67.605, 67.605, 68.29, 68.29, 68.5125, 68.5125, 68.53, 68.53, 69.08, 69.08, 68.86, 68.86, 68.5275, 68.5275, 68.6625, 68.6625, 69.135, 69.135, 69.435, 69.435, 69.6, 69.6, 69.425, 69.425, 69.4325, 69.4325, 69.6175, 69.6175, 70.2125, 70.2125, 70.045, 70.045, 70.2675, 70.2675, 70.16, 70.16, 70.3975, 70.3975, 70.6375, 70.6375, 70.7725, 70.7725, 70.475, 70.475, 70.78, 70.78, 71.2575, 71.2575, 71.4125, 71.4125, 71.46, 71.46, 71.475, 71.475, 71.3825, 71.3825, 71.3625, 71.3625, 71.615, 71.615, 71.755, 71.755, 71.96, 71.96, 71.7525, 71.7525, 71.73, 71.73, 71.895, 71.895, 71.85, 71.85, 71.9, 71.9, 71.8675, 71.8675, 72.0075, 72.0075, 71.965, 71.965, 72.155, 72.155, 72.115, 72.115, 72.2325, 72.2325, 72.33, 72.33, 72.4575, 72.4575, 72.41, 72.41, 72.3025, 72.3025, 72.2925, 72.2925, 72.4675, 72.4675, 72.7, 72.7, 72.755, 72.755, 72.68, 72.68, 72.7175, 72.7175, 72.66, 72.66, 72.405, 72.405, 72.5675, 72.5675, 72.555, 72.555, 72.5725, 72.5725, 72.6, 72.6, 72.8275, 72.8275, 72.895, 72.895, 72.8, 72.8, 72.875, 72.875, 72.7425, 72.7425, 72.61, 72.61, 72.88, 72.88, 72.8125, 72.8125, 73.0675, 73.0675, 73.12, 73.12, 73.3075, 73.3075, 73.035, 73.035, 73.0075, 73.0075, 73.17, 73.17, 73.725, 73.725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.476, Test loss: 2.628, Test accuracy: 17.57
Round   1, Train loss: 1.041, Test loss: 2.507, Test accuracy: 25.26
Round   2, Train loss: 0.935, Test loss: 1.642, Test accuracy: 40.26
Round   3, Train loss: 0.894, Test loss: 1.431, Test accuracy: 49.17
Round   4, Train loss: 0.742, Test loss: 1.052, Test accuracy: 58.98
Round   5, Train loss: 0.773, Test loss: 1.213, Test accuracy: 56.62
Round   6, Train loss: 0.641, Test loss: 0.947, Test accuracy: 63.01
Round   7, Train loss: 0.728, Test loss: 0.795, Test accuracy: 67.12
Round   8, Train loss: 0.619, Test loss: 0.843, Test accuracy: 66.44
Round   9, Train loss: 0.636, Test loss: 0.793, Test accuracy: 69.11
Round  10, Train loss: 0.732, Test loss: 0.802, Test accuracy: 68.63
Round  11, Train loss: 0.629, Test loss: 0.809, Test accuracy: 69.12
Round  12, Train loss: 0.620, Test loss: 0.718, Test accuracy: 71.57
Round  13, Train loss: 0.684, Test loss: 0.829, Test accuracy: 70.52
Round  14, Train loss: 0.565, Test loss: 0.739, Test accuracy: 72.63
Round  15, Train loss: 0.626, Test loss: 0.805, Test accuracy: 72.25
Round  16, Train loss: 0.556, Test loss: 0.637, Test accuracy: 74.33
Round  17, Train loss: 0.606, Test loss: 0.757, Test accuracy: 72.76
Round  18, Train loss: 0.583, Test loss: 0.529, Test accuracy: 77.49
Round  19, Train loss: 0.613, Test loss: 0.529, Test accuracy: 77.50
Round  20, Train loss: 0.527, Test loss: 0.524, Test accuracy: 78.19
Round  21, Train loss: 0.524, Test loss: 0.525, Test accuracy: 77.87
Round  22, Train loss: 0.472, Test loss: 0.513, Test accuracy: 78.72
Round  23, Train loss: 0.631, Test loss: 0.510, Test accuracy: 78.88
Round  24, Train loss: 0.514, Test loss: 0.507, Test accuracy: 78.62
Round  25, Train loss: 0.512, Test loss: 0.508, Test accuracy: 78.90
Round  26, Train loss: 0.415, Test loss: 0.512, Test accuracy: 78.38
Round  27, Train loss: 0.444, Test loss: 0.497, Test accuracy: 79.08
Round  28, Train loss: 0.455, Test loss: 0.490, Test accuracy: 79.46
Round  29, Train loss: 0.422, Test loss: 0.480, Test accuracy: 79.72
Round  30, Train loss: 0.425, Test loss: 0.476, Test accuracy: 79.98
Round  31, Train loss: 0.540, Test loss: 0.483, Test accuracy: 80.02
Round  32, Train loss: 0.490, Test loss: 0.484, Test accuracy: 79.73
Round  33, Train loss: 0.494, Test loss: 0.470, Test accuracy: 80.39
Round  34, Train loss: 0.495, Test loss: 0.463, Test accuracy: 80.70
Round  35, Train loss: 0.398, Test loss: 0.462, Test accuracy: 80.79
Round  36, Train loss: 0.520, Test loss: 0.460, Test accuracy: 81.17
Round  37, Train loss: 0.398, Test loss: 0.459, Test accuracy: 81.12
Round  38, Train loss: 0.420, Test loss: 0.446, Test accuracy: 81.61
Round  39, Train loss: 0.473, Test loss: 0.442, Test accuracy: 81.89
Round  40, Train loss: 0.423, Test loss: 0.452, Test accuracy: 81.43
Round  41, Train loss: 0.487, Test loss: 0.447, Test accuracy: 81.38
Round  42, Train loss: 0.452, Test loss: 0.475, Test accuracy: 80.60
Round  43, Train loss: 0.386, Test loss: 0.447, Test accuracy: 81.67
Round  44, Train loss: 0.424, Test loss: 0.456, Test accuracy: 81.27
Round  45, Train loss: 0.395, Test loss: 0.432, Test accuracy: 82.01
Round  46, Train loss: 0.413, Test loss: 0.445, Test accuracy: 81.66
Round  47, Train loss: 0.411, Test loss: 0.433, Test accuracy: 82.18
Round  48, Train loss: 0.327, Test loss: 0.425, Test accuracy: 82.64
Round  49, Train loss: 0.381, Test loss: 0.430, Test accuracy: 82.24
Round  50, Train loss: 0.369, Test loss: 0.427, Test accuracy: 82.33
Round  51, Train loss: 0.396, Test loss: 0.428, Test accuracy: 82.46
Round  52, Train loss: 0.381, Test loss: 0.425, Test accuracy: 82.56
Round  53, Train loss: 0.336, Test loss: 0.418, Test accuracy: 82.83
Round  54, Train loss: 0.372, Test loss: 0.422, Test accuracy: 82.89
Round  55, Train loss: 0.387, Test loss: 0.418, Test accuracy: 82.81
Round  56, Train loss: 0.323, Test loss: 0.420, Test accuracy: 82.94
Round  57, Train loss: 0.352, Test loss: 0.417, Test accuracy: 83.33
Round  58, Train loss: 0.318, Test loss: 0.426, Test accuracy: 83.01
Round  59, Train loss: 0.408, Test loss: 0.423, Test accuracy: 83.31
Round  60, Train loss: 0.303, Test loss: 0.427, Test accuracy: 82.73
Round  61, Train loss: 0.299, Test loss: 0.431, Test accuracy: 82.56
Round  62, Train loss: 0.292, Test loss: 0.420, Test accuracy: 83.05
Round  63, Train loss: 0.290, Test loss: 0.413, Test accuracy: 83.20
Round  64, Train loss: 0.373, Test loss: 0.411, Test accuracy: 83.24
Round  65, Train loss: 0.305, Test loss: 0.409, Test accuracy: 83.62
Round  66, Train loss: 0.375, Test loss: 0.416, Test accuracy: 83.30
Round  67, Train loss: 0.309, Test loss: 0.422, Test accuracy: 83.26
Round  68, Train loss: 0.366, Test loss: 0.425, Test accuracy: 83.43
Round  69, Train loss: 0.348, Test loss: 0.418, Test accuracy: 83.31
Round  70, Train loss: 0.370, Test loss: 0.419, Test accuracy: 83.48
Round  71, Train loss: 0.312, Test loss: 0.417, Test accuracy: 83.72
Round  72, Train loss: 0.357, Test loss: 0.413, Test accuracy: 83.85
Round  73, Train loss: 0.291, Test loss: 0.414, Test accuracy: 83.83
Round  74, Train loss: 0.289, Test loss: 0.405, Test accuracy: 83.86
Round  75, Train loss: 0.217, Test loss: 0.405, Test accuracy: 84.12
Round  76, Train loss: 0.272, Test loss: 0.413, Test accuracy: 83.96
Round  77, Train loss: 0.337, Test loss: 0.430, Test accuracy: 83.48
Round  78, Train loss: 0.268, Test loss: 0.421, Test accuracy: 83.55
Round  79, Train loss: 0.271, Test loss: 0.415, Test accuracy: 84.19
Round  80, Train loss: 0.344, Test loss: 0.405, Test accuracy: 84.12
Round  81, Train loss: 0.246, Test loss: 0.403, Test accuracy: 84.33
Round  82, Train loss: 0.346, Test loss: 0.403, Test accuracy: 84.32
Round  83, Train loss: 0.234, Test loss: 0.416, Test accuracy: 83.67
Round  84, Train loss: 0.248, Test loss: 0.418, Test accuracy: 83.87
Round  85, Train loss: 0.255, Test loss: 0.426, Test accuracy: 83.94
Round  86, Train loss: 0.283, Test loss: 0.402, Test accuracy: 84.42
Round  87, Train loss: 0.286, Test loss: 0.411, Test accuracy: 84.45
Round  88, Train loss: 0.313, Test loss: 0.416, Test accuracy: 84.22
Round  89, Train loss: 0.281, Test loss: 0.406, Test accuracy: 84.58
Round  90, Train loss: 0.265, Test loss: 0.411, Test accuracy: 84.24
Round  91, Train loss: 0.231, Test loss: 0.409, Test accuracy: 84.29
Round  92, Train loss: 0.252, Test loss: 0.409, Test accuracy: 84.40
Round  93, Train loss: 0.229, Test loss: 0.423, Test accuracy: 84.33
Round  94, Train loss: 0.261, Test loss: 0.421, Test accuracy: 84.15
Round  95, Train loss: 0.248, Test loss: 0.417, Test accuracy: 84.13
Round  96, Train loss: 0.253, Test loss: 0.425, Test accuracy: 84.18
Round  97, Train loss: 0.290, Test loss: 0.412, Test accuracy: 84.72
Round  98, Train loss: 0.189, Test loss: 0.423, Test accuracy: 84.30
Round  99, Train loss: 0.258, Test loss: 0.412, Test accuracy: 84.81
Final Round, Train loss: 0.221, Test loss: 0.405, Test accuracy: 85.26
Average accuracy final 10 rounds: 84.35583333333334 

1396.2391874790192
[1.5611884593963623, 3.1223769187927246, 4.371246099472046, 5.620115280151367, 6.876126766204834, 8.1321382522583, 9.404194593429565, 10.67625093460083, 11.968669176101685, 13.261087417602539, 14.54732632637024, 15.83356523513794, 17.10653257369995, 18.379499912261963, 19.68887209892273, 20.998244285583496, 22.304609775543213, 23.61097526550293, 24.90005612373352, 26.18913698196411, 27.485854864120483, 28.782572746276855, 30.080669403076172, 31.37876605987549, 32.654810190200806, 33.93085432052612, 35.21432828903198, 36.49780225753784, 37.782055377960205, 39.06630849838257, 40.36220932006836, 41.65811014175415, 42.960880756378174, 44.2636513710022, 45.55200171470642, 46.840352058410645, 48.148099184036255, 49.455846309661865, 50.75808620452881, 52.06032609939575, 53.43516421318054, 54.81000232696533, 56.110716819763184, 57.411431312561035, 58.69870948791504, 59.98598766326904, 61.289506912231445, 62.59302616119385, 63.90662741661072, 65.22022867202759, 66.5235321521759, 67.82683563232422, 69.12012791633606, 70.4134202003479, 71.71239304542542, 73.01136589050293, 74.30392527580261, 75.5964846611023, 76.90279746055603, 78.20911026000977, 79.50032877922058, 80.7915472984314, 82.09306764602661, 83.39458799362183, 84.70221304893494, 86.00983810424805, 87.30140447616577, 88.5929708480835, 89.88282632827759, 91.17268180847168, 92.46279788017273, 93.75291395187378, 95.05928945541382, 96.36566495895386, 97.66998314857483, 98.9743013381958, 100.2620918750763, 101.54988241195679, 102.7092113494873, 103.86854028701782, 105.03825545310974, 106.20797061920166, 107.38703155517578, 108.5660924911499, 109.72451114654541, 110.88292980194092, 112.04492020606995, 113.20691061019897, 114.44957685470581, 115.69224309921265, 116.94008684158325, 118.18793058395386, 119.46758508682251, 120.74723958969116, 122.01990962028503, 123.2925796508789, 124.57219576835632, 125.85181188583374, 127.13562440872192, 128.4194369316101, 129.70711088180542, 130.99478483200073, 132.28298354148865, 133.57118225097656, 134.85025691986084, 136.12933158874512, 137.41389536857605, 138.69845914840698, 139.98983526229858, 141.28121137619019, 142.56453275680542, 143.84785413742065, 145.13174152374268, 146.4156289100647, 147.6940722465515, 148.97251558303833, 150.16246271133423, 151.35240983963013, 152.54368233680725, 153.73495483398438, 154.91399145126343, 156.09302806854248, 157.28332114219666, 158.47361421585083, 159.6519021987915, 160.83019018173218, 162.02253198623657, 163.21487379074097, 164.38662219047546, 165.55837059020996, 166.7261927127838, 167.89401483535767, 169.04577159881592, 170.19752836227417, 171.3643844127655, 172.53124046325684, 173.69247174263, 174.85370302200317, 176.01452898979187, 177.17535495758057, 178.36430883407593, 179.5532627105713, 180.72112607955933, 181.88898944854736, 183.06269073486328, 184.2363920211792, 185.4005627632141, 186.56473350524902, 187.73225331306458, 188.89977312088013, 190.0711967945099, 191.24262046813965, 192.40223145484924, 193.56184244155884, 194.71692156791687, 195.8720006942749, 197.0429928302765, 198.21398496627808, 199.37839126586914, 200.5427975654602, 201.7099187374115, 202.8770399093628, 204.05027604103088, 205.22351217269897, 206.38412880897522, 207.54474544525146, 208.69737815856934, 209.8500108718872, 211.018132686615, 212.18625450134277, 213.3464322090149, 214.506609916687, 215.668860912323, 216.83111190795898, 217.99340295791626, 219.15569400787354, 220.32219576835632, 221.4886975288391, 222.65168261528015, 223.8146677017212, 224.97355699539185, 226.1324462890625, 227.30047607421875, 228.468505859375, 229.63403868675232, 230.79957151412964, 231.96372079849243, 233.12787008285522, 234.27545762062073, 235.42304515838623, 236.59304976463318, 237.76305437088013, 238.9300196170807, 240.09698486328125, 241.259028673172, 242.42107248306274, 243.5861256122589, 244.75117874145508, 245.92123460769653, 247.091290473938, 248.9685914516449, 250.8458924293518]
[17.566666666666666, 17.566666666666666, 25.258333333333333, 25.258333333333333, 40.25833333333333, 40.25833333333333, 49.166666666666664, 49.166666666666664, 58.983333333333334, 58.983333333333334, 56.61666666666667, 56.61666666666667, 63.00833333333333, 63.00833333333333, 67.125, 67.125, 66.44166666666666, 66.44166666666666, 69.10833333333333, 69.10833333333333, 68.63333333333334, 68.63333333333334, 69.125, 69.125, 71.56666666666666, 71.56666666666666, 70.51666666666667, 70.51666666666667, 72.63333333333334, 72.63333333333334, 72.25, 72.25, 74.325, 74.325, 72.75833333333334, 72.75833333333334, 77.49166666666666, 77.49166666666666, 77.5, 77.5, 78.19166666666666, 78.19166666666666, 77.86666666666666, 77.86666666666666, 78.71666666666667, 78.71666666666667, 78.88333333333334, 78.88333333333334, 78.625, 78.625, 78.9, 78.9, 78.375, 78.375, 79.08333333333333, 79.08333333333333, 79.45833333333333, 79.45833333333333, 79.71666666666667, 79.71666666666667, 79.98333333333333, 79.98333333333333, 80.01666666666667, 80.01666666666667, 79.73333333333333, 79.73333333333333, 80.39166666666667, 80.39166666666667, 80.7, 80.7, 80.79166666666667, 80.79166666666667, 81.16666666666667, 81.16666666666667, 81.125, 81.125, 81.60833333333333, 81.60833333333333, 81.89166666666667, 81.89166666666667, 81.43333333333334, 81.43333333333334, 81.375, 81.375, 80.6, 80.6, 81.675, 81.675, 81.26666666666667, 81.26666666666667, 82.00833333333334, 82.00833333333334, 81.65833333333333, 81.65833333333333, 82.18333333333334, 82.18333333333334, 82.64166666666667, 82.64166666666667, 82.24166666666666, 82.24166666666666, 82.33333333333333, 82.33333333333333, 82.45833333333333, 82.45833333333333, 82.55833333333334, 82.55833333333334, 82.825, 82.825, 82.89166666666667, 82.89166666666667, 82.80833333333334, 82.80833333333334, 82.94166666666666, 82.94166666666666, 83.33333333333333, 83.33333333333333, 83.00833333333334, 83.00833333333334, 83.30833333333334, 83.30833333333334, 82.73333333333333, 82.73333333333333, 82.55833333333334, 82.55833333333334, 83.05, 83.05, 83.2, 83.2, 83.24166666666666, 83.24166666666666, 83.61666666666666, 83.61666666666666, 83.3, 83.3, 83.25833333333334, 83.25833333333334, 83.43333333333334, 83.43333333333334, 83.30833333333334, 83.30833333333334, 83.48333333333333, 83.48333333333333, 83.725, 83.725, 83.85, 83.85, 83.83333333333333, 83.83333333333333, 83.85833333333333, 83.85833333333333, 84.11666666666666, 84.11666666666666, 83.95833333333333, 83.95833333333333, 83.48333333333333, 83.48333333333333, 83.55, 83.55, 84.19166666666666, 84.19166666666666, 84.125, 84.125, 84.325, 84.325, 84.31666666666666, 84.31666666666666, 83.66666666666667, 83.66666666666667, 83.86666666666666, 83.86666666666666, 83.94166666666666, 83.94166666666666, 84.41666666666667, 84.41666666666667, 84.45, 84.45, 84.225, 84.225, 84.575, 84.575, 84.24166666666666, 84.24166666666666, 84.29166666666667, 84.29166666666667, 84.4, 84.4, 84.33333333333333, 84.33333333333333, 84.15, 84.15, 84.13333333333334, 84.13333333333334, 84.18333333333334, 84.18333333333334, 84.71666666666667, 84.71666666666667, 84.3, 84.3, 84.80833333333334, 84.80833333333334, 85.25833333333334, 85.25833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.536, Test loss: 2.092, Test accuracy: 20.00
Round   1, Train loss: 1.008, Test loss: 1.648, Test accuracy: 41.35
Round   2, Train loss: 0.878, Test loss: 1.447, Test accuracy: 43.23
Round   3, Train loss: 0.808, Test loss: 1.165, Test accuracy: 55.70
Round   4, Train loss: 0.735, Test loss: 1.175, Test accuracy: 56.78
Round   5, Train loss: 0.858, Test loss: 0.841, Test accuracy: 67.04
Round   6, Train loss: 0.796, Test loss: 0.769, Test accuracy: 69.88
Round   7, Train loss: 0.690, Test loss: 0.689, Test accuracy: 71.81
Round   8, Train loss: 0.569, Test loss: 0.694, Test accuracy: 72.02
Round   9, Train loss: 0.555, Test loss: 0.660, Test accuracy: 72.96
Round  10, Train loss: 0.723, Test loss: 0.653, Test accuracy: 74.27
Round  11, Train loss: 0.676, Test loss: 0.640, Test accuracy: 74.65
Round  12, Train loss: 0.557, Test loss: 0.608, Test accuracy: 74.87
Round  13, Train loss: 0.533, Test loss: 0.605, Test accuracy: 74.80
Round  14, Train loss: 0.608, Test loss: 0.590, Test accuracy: 75.54
Round  15, Train loss: 0.556, Test loss: 0.587, Test accuracy: 75.47
Round  16, Train loss: 0.521, Test loss: 0.588, Test accuracy: 76.42
Round  17, Train loss: 0.584, Test loss: 0.567, Test accuracy: 76.76
Round  18, Train loss: 0.596, Test loss: 0.577, Test accuracy: 76.90
Round  19, Train loss: 0.583, Test loss: 0.558, Test accuracy: 77.12
Round  20, Train loss: 0.529, Test loss: 0.557, Test accuracy: 77.69
Round  21, Train loss: 0.534, Test loss: 0.529, Test accuracy: 78.24
Round  22, Train loss: 0.535, Test loss: 0.540, Test accuracy: 78.47
Round  23, Train loss: 0.498, Test loss: 0.526, Test accuracy: 78.97
Round  24, Train loss: 0.513, Test loss: 0.519, Test accuracy: 79.24
Round  25, Train loss: 0.590, Test loss: 0.501, Test accuracy: 79.62
Round  26, Train loss: 0.587, Test loss: 0.505, Test accuracy: 79.56
Round  27, Train loss: 0.566, Test loss: 0.507, Test accuracy: 79.90
Round  28, Train loss: 0.422, Test loss: 0.493, Test accuracy: 79.99
Round  29, Train loss: 0.435, Test loss: 0.502, Test accuracy: 80.07
Round  30, Train loss: 0.533, Test loss: 0.478, Test accuracy: 80.74
Round  31, Train loss: 0.449, Test loss: 0.478, Test accuracy: 80.47
Round  32, Train loss: 0.520, Test loss: 0.486, Test accuracy: 80.73
Round  33, Train loss: 0.477, Test loss: 0.470, Test accuracy: 81.29
Round  34, Train loss: 0.519, Test loss: 0.475, Test accuracy: 81.33
Round  35, Train loss: 0.467, Test loss: 0.464, Test accuracy: 81.42
Round  36, Train loss: 0.491, Test loss: 0.469, Test accuracy: 81.33
Round  37, Train loss: 0.433, Test loss: 0.460, Test accuracy: 81.29
Round  38, Train loss: 0.398, Test loss: 0.454, Test accuracy: 81.78
Round  39, Train loss: 0.436, Test loss: 0.451, Test accuracy: 81.83
Round  40, Train loss: 0.451, Test loss: 0.448, Test accuracy: 81.75
Round  41, Train loss: 0.417, Test loss: 0.451, Test accuracy: 81.67
Round  42, Train loss: 0.451, Test loss: 0.442, Test accuracy: 82.13
Round  43, Train loss: 0.441, Test loss: 0.434, Test accuracy: 82.51
Round  44, Train loss: 0.464, Test loss: 0.440, Test accuracy: 82.64
Round  45, Train loss: 0.369, Test loss: 0.434, Test accuracy: 82.74
Round  46, Train loss: 0.450, Test loss: 0.442, Test accuracy: 82.56
Round  47, Train loss: 0.346, Test loss: 0.431, Test accuracy: 82.34
Round  48, Train loss: 0.334, Test loss: 0.435, Test accuracy: 82.44
Round  49, Train loss: 0.304, Test loss: 0.428, Test accuracy: 82.94
Round  50, Train loss: 0.393, Test loss: 0.424, Test accuracy: 82.72
Round  51, Train loss: 0.350, Test loss: 0.430, Test accuracy: 82.81
Round  52, Train loss: 0.361, Test loss: 0.423, Test accuracy: 82.86
Round  53, Train loss: 0.346, Test loss: 0.421, Test accuracy: 82.94
Round  54, Train loss: 0.356, Test loss: 0.419, Test accuracy: 83.08
Round  55, Train loss: 0.357, Test loss: 0.420, Test accuracy: 83.11
Round  56, Train loss: 0.410, Test loss: 0.422, Test accuracy: 83.00
Round  57, Train loss: 0.430, Test loss: 0.411, Test accuracy: 83.49
Round  58, Train loss: 0.379, Test loss: 0.416, Test accuracy: 83.17
Round  59, Train loss: 0.346, Test loss: 0.419, Test accuracy: 83.31
Round  60, Train loss: 0.366, Test loss: 0.406, Test accuracy: 83.91
Round  61, Train loss: 0.362, Test loss: 0.400, Test accuracy: 83.88
Round  62, Train loss: 0.369, Test loss: 0.400, Test accuracy: 83.87
Round  63, Train loss: 0.354, Test loss: 0.405, Test accuracy: 83.83
Round  64, Train loss: 0.380, Test loss: 0.408, Test accuracy: 83.76
Round  65, Train loss: 0.373, Test loss: 0.412, Test accuracy: 83.53
Round  66, Train loss: 0.314, Test loss: 0.403, Test accuracy: 84.07
Round  67, Train loss: 0.308, Test loss: 0.408, Test accuracy: 83.76
Round  68, Train loss: 0.331, Test loss: 0.400, Test accuracy: 84.12
Round  69, Train loss: 0.357, Test loss: 0.392, Test accuracy: 84.42
Round  70, Train loss: 0.286, Test loss: 0.394, Test accuracy: 84.23
Round  71, Train loss: 0.335, Test loss: 0.387, Test accuracy: 84.74
Round  72, Train loss: 0.372, Test loss: 0.392, Test accuracy: 84.58
Round  73, Train loss: 0.280, Test loss: 0.395, Test accuracy: 84.33
Round  74, Train loss: 0.303, Test loss: 0.389, Test accuracy: 84.74
Round  75, Train loss: 0.310, Test loss: 0.386, Test accuracy: 84.67
Round  76, Train loss: 0.348, Test loss: 0.397, Test accuracy: 84.42
Round  77, Train loss: 0.325, Test loss: 0.395, Test accuracy: 84.28
Round  78, Train loss: 0.286, Test loss: 0.394, Test accuracy: 84.21
Round  79, Train loss: 0.283, Test loss: 0.392, Test accuracy: 84.08
Round  80, Train loss: 0.293, Test loss: 0.385, Test accuracy: 84.72
Round  81, Train loss: 0.298, Test loss: 0.378, Test accuracy: 85.06
Round  82, Train loss: 0.297, Test loss: 0.381, Test accuracy: 85.13
Round  83, Train loss: 0.300, Test loss: 0.381, Test accuracy: 85.08
Round  84, Train loss: 0.311, Test loss: 0.378, Test accuracy: 85.03
Round  85, Train loss: 0.291, Test loss: 0.375, Test accuracy: 85.33
Round  86, Train loss: 0.248, Test loss: 0.378, Test accuracy: 85.13
Round  87, Train loss: 0.278, Test loss: 0.382, Test accuracy: 85.22
Round  88, Train loss: 0.251, Test loss: 0.381, Test accuracy: 85.12
Round  89, Train loss: 0.326, Test loss: 0.381, Test accuracy: 84.83
Round  90, Train loss: 0.335, Test loss: 0.376, Test accuracy: 85.17
Round  91, Train loss: 0.275, Test loss: 0.372, Test accuracy: 85.46
Round  92, Train loss: 0.244, Test loss: 0.370, Test accuracy: 85.30
Round  93, Train loss: 0.302, Test loss: 0.373, Test accuracy: 85.35
Round  94, Train loss: 0.217, Test loss: 0.370, Test accuracy: 85.61
Round  95, Train loss: 0.287, Test loss: 0.369, Test accuracy: 85.78
Round  96, Train loss: 0.316, Test loss: 0.378, Test accuracy: 85.22
Round  97, Train loss: 0.276, Test loss: 0.384, Test accuracy: 85.42
Round  98, Train loss: 0.225, Test loss: 0.381, Test accuracy: 85.08
Round  99, Train loss: 0.282, Test loss: 0.374, Test accuracy: 85.31
Final Round, Train loss: 0.218, Test loss: 0.371, Test accuracy: 85.64
Average accuracy final 10 rounds: 85.3675
1620.1393299102783
[1.923767328262329, 3.847534656524658, 5.530678987503052, 7.213823318481445, 8.802806615829468, 10.39178991317749, 11.973029136657715, 13.55426836013794, 15.13659930229187, 16.7189302444458, 18.30419087409973, 19.889451503753662, 21.462432622909546, 23.03541374206543, 24.684670448303223, 26.333927154541016, 27.978492259979248, 29.62305736541748, 31.2860324382782, 32.949007511138916, 34.59143257141113, 36.23385763168335, 37.89630889892578, 39.55876016616821, 41.22629189491272, 42.89382362365723, 44.55658316612244, 46.21934270858765, 47.885822772979736, 49.552302837371826, 51.225921630859375, 52.899540424346924, 54.579588651657104, 56.259636878967285, 57.93622279167175, 59.61280870437622, 61.28354549407959, 62.95428228378296, 64.63135004043579, 66.30841779708862, 67.97779965400696, 69.6471815109253, 71.32075929641724, 72.99433708190918, 74.65356707572937, 76.31279706954956, 77.97582197189331, 79.63884687423706, 81.31299018859863, 82.9871335029602, 84.6214075088501, 86.25568151473999, 87.91716146469116, 89.57864141464233, 91.24521684646606, 92.9117922782898, 94.55370807647705, 96.1956238746643, 97.81384611129761, 99.43206834793091, 101.04600620269775, 102.6599440574646, 104.29249787330627, 105.92505168914795, 107.56167078018188, 109.19828987121582, 110.83931684494019, 112.48034381866455, 114.12864780426025, 115.77695178985596, 117.4410629272461, 119.10517406463623, 120.7730302810669, 122.44088649749756, 124.0924334526062, 125.74398040771484, 127.40266942977905, 129.06135845184326, 130.69442868232727, 132.32749891281128, 133.9923882484436, 135.65727758407593, 137.3243567943573, 138.99143600463867, 140.64255547523499, 142.2936749458313, 143.97764325141907, 145.66161155700684, 147.33396553993225, 149.00631952285767, 150.69096422195435, 152.37560892105103, 154.06167650222778, 155.74774408340454, 157.43066334724426, 159.11358261108398, 160.79629015922546, 162.47899770736694, 164.129891872406, 165.78078603744507, 167.4313645362854, 169.08194303512573, 170.72794699668884, 172.37395095825195, 174.0403847694397, 175.70681858062744, 177.40983080863953, 179.1128430366516, 180.7584307193756, 182.4040184020996, 183.90337920188904, 185.40274000167847, 186.88524770736694, 188.36775541305542, 189.8709774017334, 191.37419939041138, 192.88407564163208, 194.39395189285278, 195.89246773719788, 197.39098358154297, 198.90406918525696, 200.41715478897095, 201.92367506027222, 203.4301953315735, 204.94304871559143, 206.45590209960938, 207.95626139640808, 209.4566206932068, 210.98314905166626, 212.50967741012573, 214.01996397972107, 215.5302505493164, 217.04307413101196, 218.55589771270752, 220.04987812042236, 221.5438585281372, 223.06190252304077, 224.57994651794434, 226.0892686843872, 227.59859085083008, 229.107159614563, 230.6157283782959, 232.12037992477417, 233.62503147125244, 235.1635661125183, 236.70210075378418, 238.20106601715088, 239.70003128051758, 241.20803880691528, 242.716046333313, 244.2226688861847, 245.7292914390564, 247.24783205986023, 248.76637268066406, 250.26991081237793, 251.7734489440918, 253.2862422466278, 254.79903554916382, 256.30819058418274, 257.81734561920166, 259.34017181396484, 260.862998008728, 262.3776788711548, 263.89235973358154, 265.4068911075592, 266.92142248153687, 268.4383137226105, 269.9552049636841, 271.45192885398865, 272.9486527442932, 274.4452884197235, 275.9419240951538, 277.44852018356323, 278.95511627197266, 280.4953374862671, 282.0355587005615, 283.5704493522644, 285.1053400039673, 286.62871193885803, 288.1520838737488, 289.7051031589508, 291.25812244415283, 292.79722690582275, 294.3363313674927, 295.8633556365967, 297.3903799057007, 298.9316842556, 300.47298860549927, 302.00801134109497, 303.5430340766907, 305.06422328948975, 306.5854125022888, 308.137255191803, 309.68909788131714, 311.2249205112457, 312.7607431411743, 314.2980682849884, 315.8353934288025, 317.3688142299652, 318.90223503112793, 320.92896914482117, 322.9557032585144]
[20.0, 20.0, 41.35, 41.35, 43.233333333333334, 43.233333333333334, 55.7, 55.7, 56.78333333333333, 56.78333333333333, 67.04166666666667, 67.04166666666667, 69.88333333333334, 69.88333333333334, 71.80833333333334, 71.80833333333334, 72.01666666666667, 72.01666666666667, 72.95833333333333, 72.95833333333333, 74.26666666666667, 74.26666666666667, 74.65, 74.65, 74.86666666666666, 74.86666666666666, 74.8, 74.8, 75.54166666666667, 75.54166666666667, 75.475, 75.475, 76.41666666666667, 76.41666666666667, 76.75833333333334, 76.75833333333334, 76.9, 76.9, 77.11666666666666, 77.11666666666666, 77.69166666666666, 77.69166666666666, 78.24166666666666, 78.24166666666666, 78.475, 78.475, 78.96666666666667, 78.96666666666667, 79.24166666666666, 79.24166666666666, 79.625, 79.625, 79.55833333333334, 79.55833333333334, 79.9, 79.9, 79.99166666666666, 79.99166666666666, 80.06666666666666, 80.06666666666666, 80.74166666666666, 80.74166666666666, 80.475, 80.475, 80.73333333333333, 80.73333333333333, 81.29166666666667, 81.29166666666667, 81.325, 81.325, 81.41666666666667, 81.41666666666667, 81.33333333333333, 81.33333333333333, 81.29166666666667, 81.29166666666667, 81.775, 81.775, 81.83333333333333, 81.83333333333333, 81.75, 81.75, 81.66666666666667, 81.66666666666667, 82.13333333333334, 82.13333333333334, 82.50833333333334, 82.50833333333334, 82.64166666666667, 82.64166666666667, 82.74166666666666, 82.74166666666666, 82.55833333333334, 82.55833333333334, 82.34166666666667, 82.34166666666667, 82.44166666666666, 82.44166666666666, 82.94166666666666, 82.94166666666666, 82.725, 82.725, 82.80833333333334, 82.80833333333334, 82.85833333333333, 82.85833333333333, 82.94166666666666, 82.94166666666666, 83.075, 83.075, 83.10833333333333, 83.10833333333333, 83.0, 83.0, 83.49166666666666, 83.49166666666666, 83.16666666666667, 83.16666666666667, 83.30833333333334, 83.30833333333334, 83.90833333333333, 83.90833333333333, 83.875, 83.875, 83.86666666666666, 83.86666666666666, 83.83333333333333, 83.83333333333333, 83.75833333333334, 83.75833333333334, 83.525, 83.525, 84.06666666666666, 84.06666666666666, 83.75833333333334, 83.75833333333334, 84.125, 84.125, 84.425, 84.425, 84.23333333333333, 84.23333333333333, 84.74166666666666, 84.74166666666666, 84.58333333333333, 84.58333333333333, 84.33333333333333, 84.33333333333333, 84.74166666666666, 84.74166666666666, 84.675, 84.675, 84.425, 84.425, 84.28333333333333, 84.28333333333333, 84.20833333333333, 84.20833333333333, 84.075, 84.075, 84.71666666666667, 84.71666666666667, 85.05833333333334, 85.05833333333334, 85.13333333333334, 85.13333333333334, 85.075, 85.075, 85.025, 85.025, 85.33333333333333, 85.33333333333333, 85.13333333333334, 85.13333333333334, 85.21666666666667, 85.21666666666667, 85.11666666666666, 85.11666666666666, 84.83333333333333, 84.83333333333333, 85.16666666666667, 85.16666666666667, 85.45833333333333, 85.45833333333333, 85.3, 85.3, 85.35, 85.35, 85.60833333333333, 85.60833333333333, 85.775, 85.775, 85.21666666666667, 85.21666666666667, 85.41666666666667, 85.41666666666667, 85.075, 85.075, 85.30833333333334, 85.30833333333334, 85.64166666666667, 85.64166666666667]
