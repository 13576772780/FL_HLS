nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.148, Test loss: 2.829, Test accuracy: 56.01
Final Round, Global train loss: 0.148, Global test loss: 1.278, Global test accuracy: 57.76
Average accuracy final 10 rounds: 55.86999999999999 

Average global accuracy final 10 rounds: 58.88699999999999 

6120.565320491791
[4.359459400177002, 8.718918800354004, 12.738895893096924, 16.758872985839844, 21.355366945266724, 25.951860904693604, 30.418733596801758, 34.88560628890991, 38.985806941986084, 43.086007595062256, 48.2911114692688, 53.49621534347534, 58.64113092422485, 63.786046504974365, 68.94348526000977, 74.10092401504517, 79.16656494140625, 84.23220586776733, 89.38104200363159, 94.52987813949585, 99.63354825973511, 104.73721837997437, 110.03837871551514, 115.33953905105591, 120.54706811904907, 125.75459718704224, 130.22351837158203, 134.69243955612183, 139.04897117614746, 143.4055027961731, 147.78437304496765, 152.1632432937622, 156.5509147644043, 160.9385862350464, 165.3150405883789, 169.69149494171143, 174.04103255271912, 178.3905701637268, 182.7310175895691, 187.07146501541138, 191.38056755065918, 195.68967008590698, 200.02093029022217, 204.35219049453735, 208.70774841308594, 213.06330633163452, 217.4295551776886, 221.79580402374268, 226.11359000205994, 230.4313759803772, 234.79906964302063, 239.16676330566406, 243.54628443717957, 247.92580556869507, 252.30716633796692, 256.68852710723877, 261.0646708011627, 265.44081449508667, 269.7514169216156, 274.06201934814453, 278.4470043182373, 282.8319892883301, 287.2468616962433, 291.6617341041565, 296.1098704338074, 300.55800676345825, 304.9259498119354, 309.2938928604126, 313.64004492759705, 317.9861969947815, 322.33534240722656, 326.68448781967163, 331.04424929618835, 335.4040107727051, 339.79327368736267, 344.18253660202026, 348.5033223628998, 352.8241081237793, 357.1890766620636, 361.5540452003479, 365.91806387901306, 370.2820825576782, 374.63504672050476, 378.9880108833313, 383.40219831466675, 387.8163857460022, 392.2220296859741, 396.62767362594604, 401.5333435535431, 406.43901348114014, 412.0222198963165, 417.6054263114929, 422.52097630500793, 427.43652629852295, 431.90410327911377, 436.3716802597046, 440.86436223983765, 445.3570442199707, 450.01376128196716, 454.6704783439636, 459.0807156562805, 463.4909529685974, 468.0792541503906, 472.66755533218384, 477.2071170806885, 481.7466788291931, 486.282071352005, 490.8174638748169, 495.33455061912537, 499.85163736343384, 504.259281873703, 508.66692638397217, 513.5680801868439, 518.4692339897156, 522.9324672222137, 527.3957004547119, 531.8937358856201, 536.3917713165283, 541.1266806125641, 545.8615899085999, 550.2722527980804, 554.682915687561, 559.0685513019562, 563.4541869163513, 567.8622627258301, 572.2703385353088, 576.6009533405304, 580.931568145752, 585.3237104415894, 589.7158527374268, 594.1109828948975, 598.5061130523682, 602.8978645801544, 607.2896161079407, 611.6242916584015, 615.9589672088623, 620.3382420539856, 624.7175168991089, 629.0960011482239, 633.4744853973389, 637.8785717487335, 642.2826581001282, 646.5977900028229, 650.9129219055176, 655.3295874595642, 659.7462530136108, 664.1971659660339, 668.648078918457, 673.1151595115662, 677.5822401046753, 682.4870295524597, 687.3918190002441, 692.1978380680084, 697.0038571357727, 701.9010679721832, 706.7982788085938, 711.1963446140289, 715.5944104194641, 719.9716844558716, 724.348958492279, 728.779951095581, 733.210943698883, 738.126473903656, 743.042004108429, 747.9454164505005, 752.848828792572, 757.957923412323, 763.067018032074, 767.9241285324097, 772.7812390327454, 778.25958776474, 783.7379364967346, 789.0288825035095, 794.3198285102844, 799.179701089859, 804.0395736694336, 808.9261019229889, 813.8126301765442, 818.4228241443634, 823.0330181121826, 827.4629316329956, 831.8928451538086, 836.3459475040436, 840.7990498542786, 845.7533147335052, 850.7075796127319, 855.565267086029, 860.4229545593262, 865.3722279071808, 870.3215012550354, 875.1541562080383, 879.9868111610413, 884.2915015220642, 888.5961918830872, 893.0022618770599, 897.4083318710327, 901.8473432064056, 906.2863545417786, 910.6143913269043, 914.94242811203, 917.1267502307892, 919.3110723495483]
[37.8025, 37.8025, 42.4575, 42.4575, 44.63, 44.63, 45.7825, 45.7825, 46.6425, 46.6425, 48.155, 48.155, 48.3575, 48.3575, 49.39, 49.39, 50.0725, 50.0725, 50.4975, 50.4975, 50.6325, 50.6325, 51.0725, 51.0725, 52.04, 52.04, 52.4025, 52.4025, 52.8825, 52.8825, 53.115, 53.115, 53.75, 53.75, 53.8675, 53.8675, 54.0675, 54.0675, 53.975, 53.975, 54.13, 54.13, 54.435, 54.435, 54.355, 54.355, 54.4625, 54.4625, 54.7575, 54.7575, 55.0925, 55.0925, 55.2125, 55.2125, 55.0275, 55.0275, 54.97, 54.97, 54.8025, 54.8025, 54.8, 54.8, 55.0825, 55.0825, 54.98, 54.98, 55.14, 55.14, 55.3475, 55.3475, 55.3825, 55.3825, 55.745, 55.745, 55.6125, 55.6125, 55.3475, 55.3475, 55.39, 55.39, 55.3275, 55.3275, 55.42, 55.42, 55.35, 55.35, 55.4175, 55.4175, 55.045, 55.045, 54.89, 54.89, 55.1175, 55.1175, 55.2, 55.2, 55.24, 55.24, 55.0875, 55.0875, 55.205, 55.205, 55.0075, 55.0075, 55.1, 55.1, 54.93, 54.93, 54.965, 54.965, 55.19, 55.19, 55.39, 55.39, 55.24, 55.24, 55.2025, 55.2025, 55.4625, 55.4625, 55.7425, 55.7425, 55.72, 55.72, 55.6225, 55.6225, 55.575, 55.575, 55.5325, 55.5325, 55.4125, 55.4125, 55.52, 55.52, 55.8425, 55.8425, 55.37, 55.37, 55.57, 55.57, 55.7725, 55.7725, 55.85, 55.85, 55.635, 55.635, 55.2875, 55.2875, 55.2525, 55.2525, 55.3975, 55.3975, 55.7075, 55.7075, 56.1125, 56.1125, 55.7875, 55.7875, 55.755, 55.755, 55.95, 55.95, 55.9225, 55.9225, 56.0475, 56.0475, 56.08, 56.08, 55.915, 55.915, 55.8975, 55.8975, 55.92, 55.92, 56.01, 56.01, 56.0175, 56.0175, 56.0775, 56.0775, 56.0375, 56.0375, 55.865, 55.865, 55.775, 55.775, 55.7125, 55.7125, 55.725, 55.725, 55.83, 55.83, 56.145, 56.145, 56.07, 56.07, 55.815, 55.815, 55.725, 55.725, 56.01, 56.01]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.149, Test loss: 0.485, Test accuracy: 85.73
Final Round, Global train loss: 0.149, Global test loss: 1.141, Global test accuracy: 64.77
Average accuracy final 10 rounds: 85.32833333333335 

Average global accuracy final 10 rounds: 63.2825 

1887.54816365242
[1.658266305923462, 3.316532611846924, 4.702439308166504, 6.088346004486084, 7.45285701751709, 8.817368030548096, 10.245531558990479, 11.673695087432861, 13.04361891746521, 14.413542747497559, 15.774561166763306, 17.135579586029053, 18.511882543563843, 19.888185501098633, 21.276273012161255, 22.664360523223877, 24.038406372070312, 25.412452220916748, 26.819921731948853, 28.227391242980957, 29.62997269630432, 31.032554149627686, 32.40773391723633, 33.78291368484497, 35.18319749832153, 36.583481311798096, 37.9827401638031, 39.381999015808105, 40.7051465511322, 42.0282940864563, 43.34870624542236, 44.66911840438843, 45.96249079704285, 47.255863189697266, 48.530866622924805, 49.805870056152344, 51.097692251205444, 52.389514446258545, 53.70807480812073, 55.02663516998291, 56.47720432281494, 57.92777347564697, 59.32890462875366, 60.73003578186035, 62.1725378036499, 63.61503982543945, 64.91448783874512, 66.21393585205078, 67.52333688735962, 68.83273792266846, 70.11592245101929, 71.39910697937012, 72.85809516906738, 74.31708335876465, 75.76099967956543, 77.20491600036621, 78.65593791007996, 80.1069598197937, 81.57109260559082, 83.03522539138794, 84.4160041809082, 85.79678297042847, 87.1028344631195, 88.40888595581055, 89.7513837814331, 91.09388160705566, 92.40048694610596, 93.70709228515625, 95.03455829620361, 96.36202430725098, 97.69263815879822, 99.02325201034546, 100.37803292274475, 101.73281383514404, 103.10589694976807, 104.47898006439209, 105.79511594772339, 107.11125183105469, 108.42756795883179, 109.74388408660889, 111.0691180229187, 112.39435195922852, 113.71374273300171, 115.0331335067749, 116.3397171497345, 117.64630079269409, 118.97092962265015, 120.2955584526062, 121.61316752433777, 122.93077659606934, 124.2299394607544, 125.52910232543945, 126.83301091194153, 128.1369194984436, 129.42879605293274, 130.72067260742188, 132.0175621509552, 133.31445169448853, 134.77861380577087, 136.24277591705322, 137.69056367874146, 139.1383514404297, 140.5811848640442, 142.0240182876587, 143.4962387084961, 144.9684591293335, 146.33866620063782, 147.70887327194214, 149.1485879421234, 150.5883026123047, 152.0263273715973, 153.4643521308899, 154.88813281059265, 156.3119134902954, 157.73874044418335, 159.1655673980713, 160.61321377754211, 162.06086015701294, 163.5120677947998, 164.96327543258667, 166.42130589485168, 167.8793363571167, 169.33049964904785, 170.781662940979, 172.28182864189148, 173.78199434280396, 175.23255395889282, 176.6831135749817, 178.15845823287964, 179.6338028907776, 181.07723593711853, 182.52066898345947, 183.96655559539795, 185.41244220733643, 186.80572748184204, 188.19901275634766, 189.6330063343048, 191.06699991226196, 192.45473861694336, 193.84247732162476, 195.25153017044067, 196.6605830192566, 198.10963201522827, 199.55868101119995, 200.94778156280518, 202.3368821144104, 203.79866194725037, 205.26044178009033, 206.69190192222595, 208.12336206436157, 209.59122443199158, 211.05908679962158, 212.51291418075562, 213.96674156188965, 215.40447092056274, 216.84220027923584, 218.29829907417297, 219.7543978691101, 221.2009632587433, 222.64752864837646, 224.08628153800964, 225.52503442764282, 226.92036199569702, 228.31568956375122, 229.74225330352783, 231.16881704330444, 232.60605645179749, 234.04329586029053, 235.49136447906494, 236.93943309783936, 238.31193327903748, 239.6844334602356, 241.06320023536682, 242.44196701049805, 243.83334183692932, 245.2247166633606, 246.56257963180542, 247.90044260025024, 249.33962965011597, 250.7788166999817, 252.15870714187622, 253.53859758377075, 254.95623087882996, 256.37386417388916, 257.76793575286865, 259.16200733184814, 260.548326253891, 261.93464517593384, 263.31553316116333, 264.6964211463928, 266.08719778060913, 267.47797441482544, 268.86774158477783, 270.2575087547302, 271.6458806991577, 273.0342526435852, 274.42569041252136, 275.8171281814575, 277.240106344223, 278.6630845069885, 281.04016613960266, 283.4172477722168]
[29.283333333333335, 29.283333333333335, 40.708333333333336, 40.708333333333336, 52.325, 52.325, 60.38333333333333, 60.38333333333333, 61.983333333333334, 61.983333333333334, 64.0, 64.0, 67.35833333333333, 67.35833333333333, 66.04166666666667, 66.04166666666667, 67.66666666666667, 67.66666666666667, 70.875, 70.875, 72.78333333333333, 72.78333333333333, 74.65833333333333, 74.65833333333333, 74.0, 74.0, 74.00833333333334, 74.00833333333334, 74.56666666666666, 74.56666666666666, 74.75833333333334, 74.75833333333334, 74.56666666666666, 74.56666666666666, 75.65, 75.65, 76.70833333333333, 76.70833333333333, 77.79166666666667, 77.79166666666667, 78.48333333333333, 78.48333333333333, 78.09166666666667, 78.09166666666667, 78.28333333333333, 78.28333333333333, 78.01666666666667, 78.01666666666667, 78.14166666666667, 78.14166666666667, 79.125, 79.125, 80.075, 80.075, 80.675, 80.675, 81.14166666666667, 81.14166666666667, 81.30833333333334, 81.30833333333334, 81.4, 81.4, 80.46666666666667, 80.46666666666667, 81.89166666666667, 81.89166666666667, 81.93333333333334, 81.93333333333334, 81.4, 81.4, 81.89166666666667, 81.89166666666667, 81.58333333333333, 81.58333333333333, 82.125, 82.125, 82.13333333333334, 82.13333333333334, 82.3, 82.3, 82.63333333333334, 82.63333333333334, 82.55, 82.55, 82.65, 82.65, 82.5, 82.5, 82.73333333333333, 82.73333333333333, 82.16666666666667, 82.16666666666667, 82.45833333333333, 82.45833333333333, 82.475, 82.475, 82.96666666666667, 82.96666666666667, 82.89166666666667, 82.89166666666667, 84.10833333333333, 84.10833333333333, 83.85833333333333, 83.85833333333333, 83.58333333333333, 83.58333333333333, 83.56666666666666, 83.56666666666666, 83.44166666666666, 83.44166666666666, 83.025, 83.025, 82.86666666666666, 82.86666666666666, 83.225, 83.225, 83.35, 83.35, 83.3, 83.3, 83.70833333333333, 83.70833333333333, 83.51666666666667, 83.51666666666667, 84.075, 84.075, 84.65, 84.65, 84.49166666666666, 84.49166666666666, 84.98333333333333, 84.98333333333333, 84.69166666666666, 84.69166666666666, 84.03333333333333, 84.03333333333333, 83.98333333333333, 83.98333333333333, 84.26666666666667, 84.26666666666667, 84.275, 84.275, 84.775, 84.775, 84.78333333333333, 84.78333333333333, 85.05, 85.05, 84.59166666666667, 84.59166666666667, 84.31666666666666, 84.31666666666666, 84.325, 84.325, 84.38333333333334, 84.38333333333334, 84.68333333333334, 84.68333333333334, 84.64166666666667, 84.64166666666667, 84.7, 84.7, 84.84166666666667, 84.84166666666667, 85.01666666666667, 85.01666666666667, 85.14166666666667, 85.14166666666667, 85.08333333333333, 85.08333333333333, 84.925, 84.925, 84.90833333333333, 84.90833333333333, 84.93333333333334, 84.93333333333334, 85.05, 85.05, 85.33333333333333, 85.33333333333333, 85.425, 85.425, 85.45, 85.45, 85.43333333333334, 85.43333333333334, 85.6, 85.6, 85.425, 85.425, 85.30833333333334, 85.30833333333334, 84.98333333333333, 84.98333333333333, 85.15, 85.15, 85.24166666666666, 85.24166666666666, 85.26666666666667, 85.26666666666667, 85.73333333333333, 85.73333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.206, Test loss: 0.371, Test accuracy: 86.33
Average accuracy final 10 rounds: 85.48083333333335 

1461.2493076324463
[1.6189014911651611, 3.2378029823303223, 4.555332660675049, 5.872862339019775, 7.217505216598511, 8.562148094177246, 9.885499715805054, 11.208851337432861, 12.559505939483643, 13.910160541534424, 15.257428169250488, 16.604695796966553, 17.96135687828064, 19.318017959594727, 20.62562584877014, 21.933233737945557, 23.273985862731934, 24.61473798751831, 25.9570472240448, 27.29935646057129, 28.62447953224182, 29.949602603912354, 31.243125200271606, 32.53664779663086, 33.84381079673767, 35.15097379684448, 36.4836859703064, 37.81639814376831, 39.103628635406494, 40.39085912704468, 41.76149272918701, 43.132126331329346, 44.4644980430603, 45.79686975479126, 47.07120943069458, 48.3455491065979, 49.67646551132202, 51.00738191604614, 52.36697959899902, 53.726577281951904, 55.0731897354126, 56.41980218887329, 57.744449615478516, 59.06909704208374, 60.446982860565186, 61.82486867904663, 63.15502738952637, 64.4851861000061, 65.77196431159973, 67.05874252319336, 68.32835245132446, 69.59796237945557, 70.83807039260864, 72.07817840576172, 73.3808069229126, 74.68343544006348, 76.00529646873474, 77.327157497406, 78.62224841117859, 79.91733932495117, 81.19602131843567, 82.47470331192017, 83.74262022972107, 85.01053714752197, 86.23027420043945, 87.45001125335693, 88.67727303504944, 89.90453481674194, 91.11649370193481, 92.32845258712769, 93.54439687728882, 94.76034116744995, 95.98234939575195, 97.20435762405396, 98.43484210968018, 99.6653265953064, 100.87245535850525, 102.0795841217041, 103.30541515350342, 104.53124618530273, 105.78185033798218, 107.03245449066162, 108.27114772796631, 109.509840965271, 110.72844290733337, 111.94704484939575, 113.19027304649353, 114.43350124359131, 115.66411471366882, 116.89472818374634, 118.10345602035522, 119.31218385696411, 120.53212356567383, 121.75206327438354, 122.96848344802856, 124.18490362167358, 125.4115583896637, 126.63821315765381, 127.87030673027039, 129.10240030288696, 130.33195090293884, 131.56150150299072, 132.79084968566895, 134.02019786834717, 135.2346019744873, 136.44900608062744, 137.6686544418335, 138.88830280303955, 140.21279001235962, 141.5372772216797, 142.84531450271606, 144.15335178375244, 145.43755316734314, 146.72175455093384, 148.03427815437317, 149.3468017578125, 150.64858555793762, 151.95036935806274, 153.24896907806396, 154.54756879806519, 155.85064244270325, 157.1537160873413, 158.45760345458984, 159.76149082183838, 160.98714089393616, 162.21279096603394, 163.43456935882568, 164.65634775161743, 165.89711570739746, 167.1378836631775, 168.4359061717987, 169.73392868041992, 171.04330921173096, 172.352689743042, 173.66999316215515, 174.9872965812683, 176.32072043418884, 177.65414428710938, 178.98027920722961, 180.30641412734985, 181.60345149040222, 182.9004888534546, 184.22644329071045, 185.5523977279663, 186.8724570274353, 188.1925163269043, 189.4933180809021, 190.7941198348999, 192.07918429374695, 193.364248752594, 194.66614270210266, 195.96803665161133, 197.3180763721466, 198.66811609268188, 199.96243047714233, 201.25674486160278, 202.49742770195007, 203.73811054229736, 204.96290969848633, 206.1877088546753, 207.41093516349792, 208.63416147232056, 209.86312294006348, 211.0920844078064, 212.31420302391052, 213.53632164001465, 214.76426672935486, 215.99221181869507, 217.2018759250641, 218.4115400314331, 219.6351580619812, 220.8587760925293, 222.0842320919037, 223.30968809127808, 224.520733833313, 225.7317795753479, 226.94824719429016, 228.16471481323242, 229.4404137134552, 230.71611261367798, 231.94646286964417, 233.17681312561035, 234.38955283164978, 235.6022925376892, 236.8184356689453, 238.03457880020142, 239.25257658958435, 240.47057437896729, 241.7183437347412, 242.96611309051514, 244.1610140800476, 245.35591506958008, 246.57174229621887, 247.78756952285767, 249.01408433914185, 250.24059915542603, 251.47654128074646, 252.7124834060669, 253.90333104133606, 255.09417867660522, 257.0201404094696, 258.946102142334]
[24.316666666666666, 24.316666666666666, 32.225, 32.225, 42.825, 42.825, 49.733333333333334, 49.733333333333334, 53.083333333333336, 53.083333333333336, 57.083333333333336, 57.083333333333336, 58.608333333333334, 58.608333333333334, 64.54166666666667, 64.54166666666667, 66.325, 66.325, 67.51666666666667, 67.51666666666667, 71.73333333333333, 71.73333333333333, 71.38333333333334, 71.38333333333334, 72.90833333333333, 72.90833333333333, 75.525, 75.525, 75.9, 75.9, 75.96666666666667, 75.96666666666667, 76.74166666666666, 76.74166666666666, 76.98333333333333, 76.98333333333333, 77.03333333333333, 77.03333333333333, 77.36666666666666, 77.36666666666666, 78.23333333333333, 78.23333333333333, 78.08333333333333, 78.08333333333333, 78.3, 78.3, 78.275, 78.275, 79.18333333333334, 79.18333333333334, 79.7, 79.7, 80.00833333333334, 80.00833333333334, 79.6, 79.6, 79.5, 79.5, 80.01666666666667, 80.01666666666667, 80.33333333333333, 80.33333333333333, 80.30833333333334, 80.30833333333334, 80.525, 80.525, 80.83333333333333, 80.83333333333333, 81.21666666666667, 81.21666666666667, 81.25833333333334, 81.25833333333334, 81.18333333333334, 81.18333333333334, 81.33333333333333, 81.33333333333333, 81.65833333333333, 81.65833333333333, 81.725, 81.725, 82.225, 82.225, 82.44166666666666, 82.44166666666666, 82.35, 82.35, 82.30833333333334, 82.30833333333334, 82.35833333333333, 82.35833333333333, 82.3, 82.3, 82.30833333333334, 82.30833333333334, 82.46666666666667, 82.46666666666667, 82.25, 82.25, 82.61666666666666, 82.61666666666666, 82.71666666666667, 82.71666666666667, 82.91666666666667, 82.91666666666667, 83.35833333333333, 83.35833333333333, 83.38333333333334, 83.38333333333334, 83.16666666666667, 83.16666666666667, 83.075, 83.075, 83.3, 83.3, 83.38333333333334, 83.38333333333334, 83.49166666666666, 83.49166666666666, 83.93333333333334, 83.93333333333334, 83.91666666666667, 83.91666666666667, 83.23333333333333, 83.23333333333333, 83.36666666666666, 83.36666666666666, 83.60833333333333, 83.60833333333333, 83.89166666666667, 83.89166666666667, 83.93333333333334, 83.93333333333334, 84.33333333333333, 84.33333333333333, 83.88333333333334, 83.88333333333334, 83.8, 83.8, 83.65833333333333, 83.65833333333333, 84.00833333333334, 84.00833333333334, 84.13333333333334, 84.13333333333334, 84.21666666666667, 84.21666666666667, 84.66666666666667, 84.66666666666667, 84.8, 84.8, 84.53333333333333, 84.53333333333333, 84.54166666666667, 84.54166666666667, 84.80833333333334, 84.80833333333334, 84.56666666666666, 84.56666666666666, 84.61666666666666, 84.61666666666666, 84.51666666666667, 84.51666666666667, 84.85833333333333, 84.85833333333333, 85.15833333333333, 85.15833333333333, 84.93333333333334, 84.93333333333334, 84.99166666666666, 84.99166666666666, 84.98333333333333, 84.98333333333333, 85.24166666666666, 85.24166666666666, 85.30833333333334, 85.30833333333334, 85.40833333333333, 85.40833333333333, 85.64166666666667, 85.64166666666667, 85.66666666666667, 85.66666666666667, 85.325, 85.325, 85.61666666666666, 85.61666666666666, 85.09166666666667, 85.09166666666667, 85.45833333333333, 85.45833333333333, 85.59166666666667, 85.59166666666667, 85.43333333333334, 85.43333333333334, 85.26666666666667, 85.26666666666667, 85.48333333333333, 85.48333333333333, 85.875, 85.875, 86.33333333333333, 86.33333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.132, Test loss: 0.398, Test accuracy: 86.78
Average accuracy final 10 rounds: 85.71083333333334 

1449.7867629528046
[1.6854722499847412, 3.3709444999694824, 4.714811325073242, 6.058678150177002, 7.478066921234131, 8.89745569229126, 10.22470235824585, 11.55194902420044, 12.934211730957031, 14.316474437713623, 15.783438920974731, 17.25040340423584, 18.676037788391113, 20.101672172546387, 21.520442008972168, 22.93921184539795, 24.414520263671875, 25.8898286819458, 27.27459144592285, 28.659354209899902, 30.035486698150635, 31.411619186401367, 32.834994077682495, 34.25836896896362, 35.637847900390625, 37.01732683181763, 38.368165016174316, 39.719003200531006, 41.04007029533386, 42.36113739013672, 43.73379373550415, 45.10645008087158, 46.42357420921326, 47.74069833755493, 48.99320340156555, 50.24570846557617, 51.500051736831665, 52.75439500808716, 54.01965141296387, 55.284907817840576, 56.53670573234558, 57.788503646850586, 59.05277466773987, 60.31704568862915, 61.59370970726013, 62.87037372589111, 64.13595175743103, 65.40152978897095, 66.6793270111084, 67.95712423324585, 69.23823428153992, 70.51934432983398, 71.78121733665466, 73.04309034347534, 74.30197048187256, 75.56085062026978, 76.81664252281189, 78.072434425354, 79.36626720428467, 80.66009998321533, 81.91435527801514, 83.16861057281494, 84.42357683181763, 85.67854309082031, 86.95629572868347, 88.23404836654663, 89.48123550415039, 90.72842264175415, 91.99372816085815, 93.25903367996216, 94.53147768974304, 95.80392169952393, 97.07230854034424, 98.34069538116455, 99.60363602638245, 100.86657667160034, 102.11610913276672, 103.3656415939331, 104.63621973991394, 105.90679788589478, 107.19679164886475, 108.48678541183472, 109.74934720993042, 111.01190900802612, 112.28275966644287, 113.55361032485962, 114.80597162246704, 116.05833292007446, 117.34263348579407, 118.62693405151367, 119.93351006507874, 121.2400860786438, 122.5048897266388, 123.76969337463379, 125.06832432746887, 126.36695528030396, 127.63973259925842, 128.9125099182129, 130.17211079597473, 131.43171167373657, 132.68623280525208, 133.94075393676758, 135.2198827266693, 136.49901151657104, 137.75961184501648, 139.0202121734619, 140.30375051498413, 141.58728885650635, 142.85989785194397, 144.1325068473816, 145.4101083278656, 146.6877098083496, 147.9491901397705, 149.2106704711914, 150.49478840827942, 151.77890634536743, 153.03990149497986, 154.30089664459229, 155.55960822105408, 156.81831979751587, 158.08139657974243, 159.344473361969, 160.59552836418152, 161.84658336639404, 163.09260249137878, 164.33862161636353, 165.62163162231445, 166.90464162826538, 168.19230246543884, 169.4799633026123, 170.72904896736145, 171.9781346321106, 173.25680303573608, 174.53547143936157, 175.8315827846527, 177.12769412994385, 178.42410683631897, 179.7205195426941, 181.00658059120178, 182.29264163970947, 183.5865535736084, 184.88046550750732, 186.18539810180664, 187.49033069610596, 188.7366738319397, 189.98301696777344, 191.27604484558105, 192.56907272338867, 193.86092162132263, 195.1527705192566, 196.43457961082458, 197.71638870239258, 198.96722793579102, 200.21806716918945, 201.49573135375977, 202.77339553833008, 204.062237739563, 205.3510799407959, 206.59336400032043, 207.83564805984497, 209.11219143867493, 210.38873481750488, 211.66133546829224, 212.9339361190796, 214.20386576652527, 215.47379541397095, 216.73216032981873, 217.9905252456665, 219.2697982788086, 220.54907131195068, 221.8189868927002, 223.0889024734497, 224.34539771080017, 225.60189294815063, 226.86060881614685, 228.11932468414307, 229.40076112747192, 230.68219757080078, 232.00380325317383, 233.32540893554688, 234.5909297466278, 235.85645055770874, 237.11922192573547, 238.3819932937622, 239.6602373123169, 240.93848133087158, 242.22081351280212, 243.50314569473267, 244.9015998840332, 246.30005407333374, 247.73320698738098, 249.16635990142822, 250.60274481773376, 252.0391297340393, 253.44116377830505, 254.8431978225708, 256.2388060092926, 257.6344141960144, 258.99273347854614, 260.3510527610779, 262.2740750312805, 264.19709730148315]
[25.808333333333334, 25.808333333333334, 32.90833333333333, 32.90833333333333, 40.7, 40.7, 57.40833333333333, 57.40833333333333, 59.80833333333333, 59.80833333333333, 63.28333333333333, 63.28333333333333, 67.275, 67.275, 68.43333333333334, 68.43333333333334, 73.50833333333334, 73.50833333333334, 73.63333333333334, 73.63333333333334, 74.34166666666667, 74.34166666666667, 72.96666666666667, 72.96666666666667, 75.16666666666667, 75.16666666666667, 76.25833333333334, 76.25833333333334, 75.8, 75.8, 77.29166666666667, 77.29166666666667, 77.4, 77.4, 77.30833333333334, 77.30833333333334, 78.46666666666667, 78.46666666666667, 78.40833333333333, 78.40833333333333, 79.51666666666667, 79.51666666666667, 79.9, 79.9, 80.38333333333334, 80.38333333333334, 80.375, 80.375, 80.875, 80.875, 80.53333333333333, 80.53333333333333, 81.55, 81.55, 81.31666666666666, 81.31666666666666, 81.33333333333333, 81.33333333333333, 81.59166666666667, 81.59166666666667, 81.8, 81.8, 82.01666666666667, 82.01666666666667, 82.54166666666667, 82.54166666666667, 81.96666666666667, 81.96666666666667, 82.43333333333334, 82.43333333333334, 82.525, 82.525, 82.08333333333333, 82.08333333333333, 82.4, 82.4, 82.41666666666667, 82.41666666666667, 83.18333333333334, 83.18333333333334, 83.34166666666667, 83.34166666666667, 83.29166666666667, 83.29166666666667, 83.55, 83.55, 83.75, 83.75, 83.70833333333333, 83.70833333333333, 83.175, 83.175, 83.64166666666667, 83.64166666666667, 84.23333333333333, 84.23333333333333, 84.29166666666667, 84.29166666666667, 83.34166666666667, 83.34166666666667, 83.48333333333333, 83.48333333333333, 84.31666666666666, 84.31666666666666, 84.84166666666667, 84.84166666666667, 84.25833333333334, 84.25833333333334, 84.525, 84.525, 84.41666666666667, 84.41666666666667, 84.24166666666666, 84.24166666666666, 84.15833333333333, 84.15833333333333, 84.33333333333333, 84.33333333333333, 84.74166666666666, 84.74166666666666, 84.28333333333333, 84.28333333333333, 84.81666666666666, 84.81666666666666, 84.83333333333333, 84.83333333333333, 84.99166666666666, 84.99166666666666, 84.9, 84.9, 84.74166666666666, 84.74166666666666, 84.9, 84.9, 84.85833333333333, 84.85833333333333, 84.75833333333334, 84.75833333333334, 85.08333333333333, 85.08333333333333, 85.21666666666667, 85.21666666666667, 85.19166666666666, 85.19166666666666, 85.31666666666666, 85.31666666666666, 85.15, 85.15, 85.89166666666667, 85.89166666666667, 85.59166666666667, 85.59166666666667, 85.25833333333334, 85.25833333333334, 85.50833333333334, 85.50833333333334, 85.45833333333333, 85.45833333333333, 85.61666666666666, 85.61666666666666, 85.13333333333334, 85.13333333333334, 85.35, 85.35, 85.70833333333333, 85.70833333333333, 85.31666666666666, 85.31666666666666, 85.41666666666667, 85.41666666666667, 85.25833333333334, 85.25833333333334, 85.34166666666667, 85.34166666666667, 85.30833333333334, 85.30833333333334, 85.5, 85.5, 85.39166666666667, 85.39166666666667, 85.4, 85.4, 85.68333333333334, 85.68333333333334, 85.65, 85.65, 85.95, 85.95, 85.64166666666667, 85.64166666666667, 85.63333333333334, 85.63333333333334, 85.75, 85.75, 86.03333333333333, 86.03333333333333, 85.56666666666666, 85.56666666666666, 85.8, 85.8, 86.78333333333333, 86.78333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.038, Test loss: 0.977, Test accuracy: 81.47
Average accuracy final 10 rounds: 81.19083333333333 

1459.739337682724
[1.6091408729553223, 3.2182817459106445, 4.606249094009399, 5.994216442108154, 7.386534690856934, 8.778852939605713, 10.138027906417847, 11.49720287322998, 12.854233026504517, 14.211263179779053, 15.61318302154541, 17.015102863311768, 18.39157462120056, 19.768046379089355, 21.147654056549072, 22.52726173400879, 23.87855815887451, 25.229854583740234, 26.51396679878235, 27.798079013824463, 29.038511276245117, 30.27894353866577, 31.50681495666504, 32.73468637466431, 34.01202583312988, 35.28936529159546, 36.54839611053467, 37.80742692947388, 39.035974740982056, 40.264522552490234, 41.53060746192932, 42.79669237136841, 44.07342767715454, 45.350162982940674, 46.59254336357117, 47.83492374420166, 49.06786918640137, 50.300814628601074, 51.5571403503418, 52.81346607208252, 54.0928852558136, 55.37230443954468, 56.623188734054565, 57.87407302856445, 59.26377201080322, 60.65347099304199, 62.03183579444885, 63.41020059585571, 64.82780456542969, 66.24540853500366, 67.6354591846466, 69.02550983428955, 70.44088125228882, 71.85625267028809, 73.27284932136536, 74.68944597244263, 76.08547282218933, 77.48149967193604, 78.8878562450409, 80.29421281814575, 81.71568703651428, 83.13716125488281, 84.55169582366943, 85.96623039245605, 87.37948489189148, 88.7927393913269, 90.23700714111328, 91.68127489089966, 93.08202695846558, 94.4827790260315, 95.84279918670654, 97.20281934738159, 98.63162136077881, 100.06042337417603, 101.42672991752625, 102.79303646087646, 104.14947867393494, 105.50592088699341, 106.93833661079407, 108.37075233459473, 109.62839007377625, 110.88602781295776, 112.13539862632751, 113.38476943969727, 114.6532974243164, 115.92182540893555, 117.1847312450409, 118.44763708114624, 119.70854806900024, 120.96945905685425, 122.24449181556702, 123.51952457427979, 124.8075213432312, 126.09551811218262, 127.36114001274109, 128.62676191329956, 129.8880581855774, 131.14935445785522, 132.43008661270142, 133.7108187675476, 134.97099614143372, 136.23117351531982, 137.47252535820007, 138.71387720108032, 140.16456055641174, 141.61524391174316, 142.9839859008789, 144.35272789001465, 145.81413412094116, 147.27554035186768, 148.66759061813354, 150.0596408843994, 151.47060704231262, 152.88157320022583, 154.33398699760437, 155.7864007949829, 157.12935709953308, 158.47231340408325, 159.75962495803833, 161.0469365119934, 162.5569405555725, 164.0669445991516, 165.48954343795776, 166.91214227676392, 168.30398035049438, 169.69581842422485, 171.0057816505432, 172.31574487686157, 173.63839387893677, 174.96104288101196, 176.29514408111572, 177.62924528121948, 179.02929854393005, 180.42935180664062, 181.82291531562805, 183.21647882461548, 184.46108603477478, 185.70569324493408, 186.9732882976532, 188.24088335037231, 189.51040887832642, 190.77993440628052, 192.02258920669556, 193.2652440071106, 194.5517234802246, 195.83820295333862, 197.09462904930115, 198.35105514526367, 199.62136054039001, 200.89166593551636, 202.15514826774597, 203.4186305999756, 204.6576874256134, 205.89674425125122, 207.17258048057556, 208.4484167098999, 209.71193623542786, 210.9754557609558, 212.23557710647583, 213.49569845199585, 214.76411604881287, 216.03253364562988, 217.32924723625183, 218.62596082687378, 219.88711810112, 221.1482753753662, 222.4156937599182, 223.68311214447021, 224.93789219856262, 226.19267225265503, 227.47953152656555, 228.76639080047607, 229.996319770813, 231.2262487411499, 232.47965812683105, 233.7330675125122, 234.9998071193695, 236.2665467262268, 237.54285264015198, 238.81915855407715, 240.0638861656189, 241.30861377716064, 242.56017756462097, 243.8117413520813, 245.11963057518005, 246.4275197982788, 247.68649291992188, 248.94546604156494, 250.21140718460083, 251.47734832763672, 252.72565817832947, 253.97396802902222, 255.2204031944275, 256.46683835983276, 257.75631642341614, 259.0457944869995, 260.29476284980774, 261.54373121261597, 262.83413577079773, 264.1245403289795, 266.4278070926666, 268.73107385635376]
[24.4, 24.4, 34.15833333333333, 34.15833333333333, 49.958333333333336, 49.958333333333336, 50.05833333333333, 50.05833333333333, 51.86666666666667, 51.86666666666667, 52.358333333333334, 52.358333333333334, 66.425, 66.425, 68.675, 68.675, 71.36666666666666, 71.36666666666666, 70.0, 70.0, 71.81666666666666, 71.81666666666666, 72.90833333333333, 72.90833333333333, 73.18333333333334, 73.18333333333334, 73.95, 73.95, 73.2, 73.2, 73.30833333333334, 73.30833333333334, 75.025, 75.025, 74.725, 74.725, 75.775, 75.775, 76.09166666666667, 76.09166666666667, 77.275, 77.275, 77.40833333333333, 77.40833333333333, 76.475, 76.475, 77.44166666666666, 77.44166666666666, 77.85833333333333, 77.85833333333333, 77.88333333333334, 77.88333333333334, 77.275, 77.275, 78.33333333333333, 78.33333333333333, 78.6, 78.6, 78.26666666666667, 78.26666666666667, 78.78333333333333, 78.78333333333333, 79.05833333333334, 79.05833333333334, 79.24166666666666, 79.24166666666666, 78.6, 78.6, 78.875, 78.875, 79.31666666666666, 79.31666666666666, 79.25, 79.25, 79.25833333333334, 79.25833333333334, 78.775, 78.775, 78.75833333333334, 78.75833333333334, 79.75, 79.75, 79.68333333333334, 79.68333333333334, 80.13333333333334, 80.13333333333334, 80.33333333333333, 80.33333333333333, 80.11666666666666, 80.11666666666666, 80.26666666666667, 80.26666666666667, 80.09166666666667, 80.09166666666667, 80.36666666666666, 80.36666666666666, 80.9, 80.9, 80.76666666666667, 80.76666666666667, 80.59166666666667, 80.59166666666667, 80.625, 80.625, 80.64166666666667, 80.64166666666667, 80.525, 80.525, 80.91666666666667, 80.91666666666667, 80.83333333333333, 80.83333333333333, 80.23333333333333, 80.23333333333333, 80.18333333333334, 80.18333333333334, 80.59166666666667, 80.59166666666667, 80.54166666666667, 80.54166666666667, 80.79166666666667, 80.79166666666667, 80.84166666666667, 80.84166666666667, 80.8, 80.8, 80.39166666666667, 80.39166666666667, 80.275, 80.275, 80.55, 80.55, 80.91666666666667, 80.91666666666667, 80.89166666666667, 80.89166666666667, 80.575, 80.575, 80.875, 80.875, 80.83333333333333, 80.83333333333333, 80.95, 80.95, 81.125, 81.125, 80.975, 80.975, 80.925, 80.925, 80.9, 80.9, 80.85, 80.85, 80.88333333333334, 80.88333333333334, 80.70833333333333, 80.70833333333333, 80.3, 80.3, 80.76666666666667, 80.76666666666667, 80.96666666666667, 80.96666666666667, 81.19166666666666, 81.19166666666666, 81.075, 81.075, 80.9, 80.9, 81.05833333333334, 81.05833333333334, 81.21666666666667, 81.21666666666667, 81.15833333333333, 81.15833333333333, 81.06666666666666, 81.06666666666666, 81.05, 81.05, 81.09166666666667, 81.09166666666667, 81.10833333333333, 81.10833333333333, 81.05833333333334, 81.05833333333334, 80.98333333333333, 80.98333333333333, 81.06666666666666, 81.06666666666666, 81.00833333333334, 81.00833333333334, 81.25, 81.25, 81.44166666666666, 81.44166666666666, 81.45, 81.45, 81.45, 81.45, 81.475, 81.475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Final Round, Train loss: 0.060, Test loss: 0.713, Test accuracy: 70.24
Average accuracy final 10 rounds: 69.58083333333333
1895.272759437561
[]
[19.475, 34.25, 34.141666666666666, 38.775, 42.75833333333333, 41.80833333333333, 43.983333333333334, 47.11666666666667, 50.041666666666664, 50.45, 51.55, 52.61666666666667, 55.4, 53.666666666666664, 55.108333333333334, 54.05, 56.68333333333333, 58.03333333333333, 57.34166666666667, 59.25833333333333, 60.78333333333333, 59.625, 58.25833333333333, 59.2, 59.19166666666667, 58.791666666666664, 60.25833333333333, 60.61666666666667, 61.208333333333336, 62.05833333333333, 62.2, 62.11666666666667, 62.34166666666667, 61.99166666666667, 63.71666666666667, 63.7, 64.90833333333333, 64.43333333333334, 66.08333333333333, 65.05, 65.05, 64.43333333333334, 65.59166666666667, 65.19166666666666, 65.59166666666667, 65.58333333333333, 66.3, 66.81666666666666, 68.15, 68.25833333333334, 69.05833333333334, 68.09166666666667, 68.2, 69.03333333333333, 69.26666666666667, 69.04166666666667, 69.48333333333333, 68.725, 68.975, 68.525, 67.28333333333333, 67.56666666666666, 67.85, 67.98333333333333, 67.7, 68.81666666666666, 68.9, 68.95833333333333, 69.34166666666667, 69.16666666666667, 68.56666666666666, 70.46666666666667, 71.50833333333334, 70.0, 70.76666666666667, 70.75833333333334, 71.25833333333334, 71.10833333333333, 69.54166666666667, 69.325, 68.6, 69.01666666666667, 68.65833333333333, 68.39166666666667, 69.55, 69.68333333333334, 69.375, 69.69166666666666, 69.425, 69.675, 70.35833333333333, 69.9, 69.35833333333333, 69.59166666666667, 68.7, 68.51666666666667, 68.6, 69.64166666666667, 70.65833333333333, 70.48333333333333, 70.24166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.570, Test loss: 0.564, Test accuracy: 76.72
Average accuracy final 10 rounds: 76.75416666666666
Average global accuracy final 10 rounds: 76.75416666666666
1362.8424985408783
[]
[26.15, 42.63333333333333, 49.93333333333333, 55.225, 60.93333333333333, 64.08333333333333, 60.541666666666664, 62.06666666666667, 64.10833333333333, 64.21666666666667, 64.85833333333333, 64.26666666666667, 66.425, 67.55, 68.30833333333334, 70.21666666666667, 70.91666666666667, 71.34166666666667, 71.35833333333333, 70.8, 70.64166666666667, 70.625, 70.75, 70.74166666666666, 71.73333333333333, 71.04166666666667, 72.03333333333333, 73.03333333333333, 72.54166666666667, 73.28333333333333, 73.04166666666667, 73.23333333333333, 72.88333333333334, 72.84166666666667, 73.05, 73.25, 73.4, 73.11666666666666, 73.89166666666667, 74.2, 74.71666666666667, 74.625, 75.40833333333333, 75.31666666666666, 75.25833333333334, 75.11666666666666, 75.16666666666667, 74.76666666666667, 74.78333333333333, 75.13333333333334, 74.575, 74.825, 75.59166666666667, 74.98333333333333, 75.03333333333333, 74.86666666666666, 74.44166666666666, 73.96666666666667, 73.90833333333333, 73.71666666666667, 73.89166666666667, 73.875, 74.98333333333333, 75.55, 75.475, 75.20833333333333, 76.14166666666667, 75.88333333333334, 75.44166666666666, 75.55, 74.925, 74.96666666666667, 75.00833333333334, 76.01666666666667, 75.58333333333333, 74.975, 75.58333333333333, 74.7, 75.81666666666666, 74.94166666666666, 75.10833333333333, 75.30833333333334, 75.79166666666667, 76.075, 76.25, 75.85833333333333, 75.85, 75.51666666666667, 75.775, 76.05, 76.59166666666667, 76.10833333333333, 76.60833333333333, 76.98333333333333, 76.925, 77.39166666666667, 77.075, 76.73333333333333, 76.38333333333334, 76.74166666666666, 76.725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.155, Test loss: 0.964, Test accuracy: 69.69
Average accuracy final 10 rounds: 65.72
2447.4591653347015
[3.8644943237304688, 7.488119125366211, 11.154989004135132, 14.741856098175049, 18.3591525554657, 21.953367471694946, 25.538928747177124, 29.150879859924316, 32.76177382469177, 36.346662759780884, 39.95781755447388, 43.556289196014404, 47.1662323474884, 50.803494691848755, 54.45432424545288, 58.105247259140015, 61.74561142921448, 65.41852712631226, 69.04208827018738, 72.70627927780151, 76.34647035598755, 79.9730167388916, 83.63514447212219, 87.28495693206787, 90.94180727005005, 94.60454964637756, 98.26420593261719, 101.90854454040527, 105.55574798583984, 109.18428158760071, 112.81820940971375, 116.46355295181274, 120.10664558410645, 123.76938796043396, 127.42789769172668, 131.0773491859436, 134.70920038223267, 138.33400440216064, 141.9610104560852, 145.61343622207642, 149.2510805130005, 152.8903033733368, 156.51804161071777, 160.15584254264832, 163.79457807540894, 167.45203137397766, 171.10771489143372, 174.74120259284973, 178.39453172683716, 182.04065871238708, 185.6900293827057, 189.34883737564087, 192.98670625686646, 196.64803767204285, 200.2984220981598, 203.93010807037354, 207.5625729560852, 211.18706727027893, 214.8436417579651, 218.44509506225586, 221.83219623565674, 225.1144359111786, 228.35147666931152, 231.63203287124634, 234.88214635849, 238.13133478164673, 241.43884921073914, 244.6886179447174, 247.97748708724976, 251.24485278129578, 254.50376272201538, 257.77961325645447, 261.03246307373047, 264.28071188926697, 267.5514883995056, 270.79471945762634, 274.0654196739197, 277.3123688697815, 280.5641551017761, 283.83007311820984, 287.0799446105957, 290.32497692108154, 293.59170722961426, 296.8561372756958, 300.13222551345825, 303.3980858325958, 306.669748544693, 309.90555357933044, 313.1398103237152, 316.40080523490906, 319.6272768974304, 322.8868567943573, 326.51477098464966, 329.93162727355957, 333.2697899341583, 336.5392162799835, 339.8443179130554, 343.1349446773529, 346.3990705013275, 349.69568943977356, 352.46370697021484]
[14.166666666666666, 20.675, 28.483333333333334, 37.03333333333333, 37.19166666666667, 36.46666666666667, 37.53333333333333, 37.94166666666667, 45.825, 38.075, 43.13333333333333, 45.666666666666664, 45.45, 45.583333333333336, 46.56666666666667, 46.25833333333333, 46.80833333333333, 55.31666666666667, 51.40833333333333, 53.88333333333333, 51.766666666666666, 51.675, 52.916666666666664, 57.525, 57.4, 54.458333333333336, 57.95, 56.9, 59.233333333333334, 58.875, 51.90833333333333, 52.13333333333333, 54.583333333333336, 52.025, 57.458333333333336, 57.3, 56.641666666666666, 62.15833333333333, 54.333333333333336, 57.35, 59.666666666666664, 50.525, 55.725, 60.65, 59.11666666666667, 58.891666666666666, 52.041666666666664, 61.325, 58.15, 63.75, 66.10833333333333, 62.916666666666664, 47.94166666666667, 59.525, 61.475, 55.425, 61.31666666666667, 63.166666666666664, 61.541666666666664, 63.975, 66.425, 65.775, 62.65, 61.9, 60.00833333333333, 63.38333333333333, 65.23333333333333, 62.325, 66.225, 63.516666666666666, 64.96666666666667, 63.125, 66.525, 65.7, 64.95833333333333, 67.26666666666667, 66.63333333333334, 63.625, 58.225, 60.99166666666667, 63.541666666666664, 63.483333333333334, 64.75, 63.891666666666666, 59.6, 57.45, 59.71666666666667, 66.84166666666667, 65.675, 62.09166666666667, 65.725, 63.891666666666666, 69.10833333333333, 68.05, 64.96666666666667, 65.33333333333333, 66.73333333333333, 68.08333333333333, 62.391666666666666, 62.916666666666664, 69.69166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

1612.7146980762482
[1.766453742980957, 3.3237855434417725, 4.894162893295288, 6.433196544647217, 7.976104259490967, 9.52530813217163, 11.098428010940552, 12.660261392593384, 14.21766471862793, 15.773924589157104, 17.33973240852356, 18.894376277923584, 20.44609832763672, 22.00562357902527, 23.557750463485718, 25.106810092926025, 26.68271279335022, 28.241912364959717, 29.818881511688232, 31.366480112075806, 32.92719864845276, 34.483033657073975, 36.035685777664185, 37.51300597190857, 39.080201625823975, 40.73284602165222, 42.30519485473633, 43.87139344215393, 45.446006059646606, 47.02426362037659, 48.54108262062073, 50.08647108078003, 51.59811472892761, 53.17396402359009, 54.717458963394165, 56.21957492828369, 57.706756830215454, 59.26474618911743, 60.77346587181091, 62.3183913230896, 63.84968876838684, 65.35892128944397, 66.91025686264038, 68.46230125427246, 69.98563766479492, 71.52010703086853, 73.06506896018982, 74.60699582099915, 76.08191156387329, 77.6138391494751, 79.1552197933197, 80.69885635375977, 82.24753642082214, 83.81789636611938, 85.37503170967102, 86.94592046737671, 88.5120313167572, 90.08214282989502, 91.64184641838074, 93.210134267807, 94.77116966247559, 96.34403538703918, 97.91906833648682, 99.49106454849243, 101.05645775794983, 102.62542533874512, 104.18536686897278, 105.74407982826233, 107.31945776939392, 108.88808751106262, 110.45612502098083, 112.0139696598053, 113.57734751701355, 115.1405303478241, 116.70569038391113, 118.26459383964539, 119.82765436172485, 121.39128828048706, 122.94132113456726, 124.70966601371765, 126.36496639251709, 128.09126782417297, 129.6749768257141, 131.24525237083435, 132.84333896636963, 134.63534474372864, 136.2717752456665, 137.74192023277283, 139.15077805519104, 140.54663610458374, 141.93283033370972, 143.34817171096802, 144.73994088172913, 146.141459941864, 147.53596711158752, 148.94160556793213, 150.3559844493866, 151.74800038337708, 153.14989638328552, 154.5625445842743, 156.84493494033813]
[13.008333333333333, 14.783333333333333, 16.475, 13.941666666666666, 12.916666666666666, 9.925, 8.4, 8.483333333333333, 11.816666666666666, 11.816666666666666, 11.441666666666666, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.201, Test loss: 0.335, Test accuracy: 87.17
Average accuracy final 10 rounds: 86.84833333333333
1338.309240579605
[2.019369125366211, 3.7540252208709717, 5.474609375, 7.212928771972656, 8.946325778961182, 10.664479732513428, 12.298943281173706, 13.931092023849487, 15.656965970993042, 17.313971042633057, 19.081378698349, 20.791154861450195, 22.514766693115234, 24.23301672935486, 25.949283123016357, 27.66043734550476, 29.374175310134888, 31.101011276245117, 32.80352759361267, 34.51260542869568, 36.230117082595825, 37.95482015609741, 39.66375470161438, 41.37774181365967, 43.103564500808716, 44.8358588218689, 46.53474807739258, 48.25927019119263, 49.975656509399414, 51.75009608268738, 53.46504783630371, 55.18289017677307, 56.88493871688843, 58.60750341415405, 60.322752237319946, 62.026878118515015, 63.744802713394165, 65.45852994918823, 67.18060541152954, 68.89681005477905, 70.61864614486694, 72.30339288711548, 74.02107095718384, 75.71081519126892, 77.4131178855896, 79.09705305099487, 80.79058384895325, 82.48868203163147, 84.09591269493103, 85.64640951156616, 87.22334885597229, 88.79239583015442, 90.35811281204224, 91.920649766922, 93.47694802284241, 95.05386304855347, 96.659099817276, 98.22665476799011, 99.84599447250366, 101.41139650344849, 102.9769868850708, 104.55153918266296, 106.10391283035278, 107.66304278373718, 109.2320556640625, 110.81372165679932, 112.38961267471313, 113.95685648918152, 115.50958156585693, 117.06454277038574, 118.64541029930115, 120.21020030975342, 121.78040909767151, 123.34897208213806, 124.9260265827179, 126.50224041938782, 128.04717135429382, 129.6066699028015, 131.1570599079132, 132.73746252059937, 134.2991542816162, 135.85319542884827, 137.41820430755615, 139.0016167163849, 140.58020544052124, 142.1317446231842, 143.6980357170105, 145.26594400405884, 146.82805824279785, 148.3805215358734, 149.94930458068848, 151.52684593200684, 153.10990715026855, 154.66679859161377, 156.21285700798035, 157.7697856426239, 159.34993505477905, 160.9297034740448, 162.50003337860107, 164.04677057266235, 166.09982252120972]
[24.725, 41.858333333333334, 49.108333333333334, 53.95, 57.958333333333336, 63.325, 62.266666666666666, 63.516666666666666, 65.49166666666666, 73.65833333333333, 72.325, 71.93333333333334, 72.15, 76.78333333333333, 77.09166666666667, 78.66666666666667, 78.65, 78.575, 79.15, 79.44166666666666, 80.05833333333334, 80.46666666666667, 80.80833333333334, 81.26666666666667, 81.7, 81.76666666666667, 81.8, 81.90833333333333, 82.34166666666667, 82.90833333333333, 82.59166666666667, 82.63333333333334, 82.95833333333333, 82.89166666666667, 83.05833333333334, 83.05833333333334, 83.75, 83.68333333333334, 83.40833333333333, 83.35833333333333, 83.71666666666667, 83.975, 84.31666666666666, 84.15833333333333, 84.3, 84.14166666666667, 84.2, 84.49166666666666, 84.45, 84.71666666666667, 84.925, 84.73333333333333, 84.89166666666667, 85.00833333333334, 85.025, 85.28333333333333, 85.29166666666667, 85.425, 85.15833333333333, 85.61666666666666, 85.74166666666666, 85.725, 86.075, 85.725, 85.50833333333334, 86.025, 85.96666666666667, 85.84166666666667, 85.68333333333334, 85.58333333333333, 85.24166666666666, 85.99166666666666, 86.10833333333333, 85.94166666666666, 86.2, 86.08333333333333, 86.2, 86.38333333333334, 86.2, 86.20833333333333, 86.49166666666666, 86.55833333333334, 86.09166666666667, 86.35, 86.35833333333333, 86.41666666666667, 86.59166666666667, 86.68333333333334, 86.86666666666666, 86.625, 86.55833333333334, 86.85, 86.78333333333333, 86.74166666666666, 86.79166666666667, 87.1, 86.70833333333333, 87.05, 86.975, 86.925, 87.16666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.198, Test loss: 0.333, Test accuracy: 86.89
Average accuracy final 10 rounds: 86.74583333333332
1655.3847670555115
[2.1932783126831055, 4.386556625366211, 6.125100135803223, 7.863643646240234, 9.6170015335083, 11.370359420776367, 13.130180597305298, 14.890001773834229, 16.649778842926025, 18.409555912017822, 20.15326166152954, 21.89696741104126, 23.637258291244507, 25.377549171447754, 27.12372088432312, 28.869892597198486, 30.609524250030518, 32.34915590286255, 34.0791802406311, 35.80920457839966, 37.57452964782715, 39.33985471725464, 41.088067293167114, 42.83627986907959, 44.57629036903381, 46.31630086898804, 48.055222511291504, 49.79414415359497, 51.550965309143066, 53.30778646469116, 55.046391010284424, 56.784995555877686, 58.53268766403198, 60.28037977218628, 62.03247952461243, 63.784579277038574, 65.54060792922974, 67.2966365814209, 69.06140398979187, 70.82617139816284, 72.56510972976685, 74.30404806137085, 76.06458854675293, 77.82512903213501, 79.59565234184265, 81.3661756515503, 83.11832642555237, 84.87047719955444, 86.61771631240845, 88.36495542526245, 90.1239984035492, 91.88304138183594, 93.62956142425537, 95.3760814666748, 97.15228819847107, 98.92849493026733, 100.70857000350952, 102.48864507675171, 104.24847364425659, 106.00830221176147, 107.7640290260315, 109.51975584030151, 111.28056526184082, 113.04137468338013, 114.79312705993652, 116.54487943649292, 118.29989266395569, 120.05490589141846, 121.80211544036865, 123.54932498931885, 125.3027913570404, 127.05625772476196, 128.8159408569336, 130.57562398910522, 132.33238410949707, 134.08914422988892, 135.84693384170532, 137.60472345352173, 139.24699306488037, 140.889262676239, 142.67669200897217, 144.46412134170532, 146.19066762924194, 147.91721391677856, 149.66124534606934, 151.4052767753601, 153.1209201812744, 154.83656358718872, 156.62567162513733, 158.41477966308594, 160.12709546089172, 161.8394112586975, 163.55697679519653, 165.27454233169556, 167.01178550720215, 168.74902868270874, 170.46304965019226, 172.17707061767578, 173.83700680732727, 175.49694299697876, 177.17626190185547, 178.85558080673218, 180.56086611747742, 182.26615142822266, 184.00505709648132, 185.74396276474, 187.4604115486145, 189.176860332489, 190.87759685516357, 192.57833337783813, 194.23315262794495, 195.88797187805176, 197.5466525554657, 199.20533323287964, 200.87601137161255, 202.54668951034546, 204.2186737060547, 205.89065790176392, 207.5309488773346, 209.17123985290527, 210.79077005386353, 212.41030025482178, 213.98905444145203, 215.56780862808228, 217.23246932029724, 218.8971300125122, 220.5458686351776, 222.19460725784302, 223.80886268615723, 225.42311811447144, 227.0206594467163, 228.61820077896118, 230.24285197257996, 231.86750316619873, 233.5165560245514, 235.16560888290405, 236.64191341400146, 238.11821794509888, 239.7315080165863, 241.34479808807373, 242.96123552322388, 244.57767295837402, 246.18690419197083, 247.79613542556763, 249.40063524246216, 251.0051350593567, 252.6140217781067, 254.2229084968567, 255.83428168296814, 257.4456548690796, 259.0571279525757, 260.6686010360718, 262.2693204879761, 263.87003993988037, 265.53688311576843, 267.2037262916565, 268.8064019680023, 270.40907764434814, 272.023451089859, 273.6378245353699, 275.2484495639801, 276.85907459259033, 278.44058656692505, 280.02209854125977, 281.6097209453583, 283.1973433494568, 284.7863745689392, 286.37540578842163, 287.9700357913971, 289.56466579437256, 291.10345673561096, 292.64224767684937, 294.22218227386475, 295.8021168708801, 297.3946189880371, 298.9871211051941, 300.5829737186432, 302.1788263320923, 303.77170586586, 305.3645853996277, 306.9504249095917, 308.53626441955566, 310.12717175483704, 311.7180790901184, 313.25296092033386, 314.7878427505493, 316.39983081817627, 318.0118188858032, 319.6064622402191, 321.201105594635, 322.80620884895325, 324.4113121032715, 326.0255546569824, 327.63979721069336, 329.24163341522217, 330.843469619751, 332.44561553001404, 334.0477614402771, 335.65524792671204, 337.262734413147, 339.3337857723236, 341.40483713150024]
[23.5, 23.5, 45.28333333333333, 45.28333333333333, 43.80833333333333, 43.80833333333333, 53.2, 53.2, 58.93333333333333, 58.93333333333333, 66.56666666666666, 66.56666666666666, 70.63333333333334, 70.63333333333334, 73.13333333333334, 73.13333333333334, 73.7, 73.7, 73.94166666666666, 73.94166666666666, 73.925, 73.925, 77.43333333333334, 77.43333333333334, 78.3, 78.3, 78.03333333333333, 78.03333333333333, 78.74166666666666, 78.74166666666666, 79.275, 79.275, 79.58333333333333, 79.58333333333333, 80.08333333333333, 80.08333333333333, 80.21666666666667, 80.21666666666667, 80.86666666666666, 80.86666666666666, 80.74166666666666, 80.74166666666666, 80.99166666666666, 80.99166666666666, 80.90833333333333, 80.90833333333333, 81.2, 81.2, 81.55, 81.55, 82.29166666666667, 82.29166666666667, 82.56666666666666, 82.56666666666666, 81.36666666666666, 81.36666666666666, 82.23333333333333, 82.23333333333333, 82.275, 82.275, 82.50833333333334, 82.50833333333334, 82.9, 82.9, 83.44166666666666, 83.44166666666666, 82.95833333333333, 82.95833333333333, 83.2, 83.2, 82.775, 82.775, 82.88333333333334, 82.88333333333334, 83.66666666666667, 83.66666666666667, 84.09166666666667, 84.09166666666667, 84.1, 84.1, 84.33333333333333, 84.33333333333333, 84.2, 84.2, 84.39166666666667, 84.39166666666667, 84.7, 84.7, 84.51666666666667, 84.51666666666667, 84.675, 84.675, 84.41666666666667, 84.41666666666667, 84.76666666666667, 84.76666666666667, 84.975, 84.975, 84.975, 84.975, 84.45, 84.45, 85.19166666666666, 85.19166666666666, 85.125, 85.125, 84.76666666666667, 84.76666666666667, 85.35, 85.35, 85.425, 85.425, 85.36666666666666, 85.36666666666666, 84.875, 84.875, 85.775, 85.775, 85.66666666666667, 85.66666666666667, 85.60833333333333, 85.60833333333333, 85.675, 85.675, 85.44166666666666, 85.44166666666666, 85.94166666666666, 85.94166666666666, 85.85, 85.85, 85.85833333333333, 85.85833333333333, 85.78333333333333, 85.78333333333333, 85.95833333333333, 85.95833333333333, 86.10833333333333, 86.10833333333333, 85.79166666666667, 85.79166666666667, 85.95833333333333, 85.95833333333333, 85.86666666666666, 85.86666666666666, 86.40833333333333, 86.40833333333333, 86.04166666666667, 86.04166666666667, 86.43333333333334, 86.43333333333334, 86.39166666666667, 86.39166666666667, 86.51666666666667, 86.51666666666667, 86.41666666666667, 86.41666666666667, 86.68333333333334, 86.68333333333334, 86.26666666666667, 86.26666666666667, 86.25833333333334, 86.25833333333334, 86.775, 86.775, 86.68333333333334, 86.68333333333334, 86.45, 86.45, 86.73333333333333, 86.73333333333333, 86.6, 86.6, 86.86666666666666, 86.86666666666666, 86.83333333333333, 86.83333333333333, 86.53333333333333, 86.53333333333333, 86.4, 86.4, 86.34166666666667, 86.34166666666667, 86.525, 86.525, 86.86666666666666, 86.86666666666666, 87.03333333333333, 87.03333333333333, 86.68333333333334, 86.68333333333334, 86.775, 86.775, 86.74166666666666, 86.74166666666666, 87.18333333333334, 87.18333333333334, 86.6, 86.6, 86.70833333333333, 86.70833333333333, 86.89166666666667, 86.89166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.047, Test loss: 0.915, Test accuracy: 82.37
Final Round, Global train loss: 0.047, Global test loss: 2.095, Global test accuracy: 27.62
Average accuracy final 10 rounds: 82.78333333333333 

Average global accuracy final 10 rounds: 28.461666666666666 

1859.5182900428772
[1.6533730030059814, 3.306746006011963, 4.694247722625732, 6.081749439239502, 7.532066345214844, 8.982383251190186, 10.500253677368164, 12.018124103546143, 13.445971012115479, 14.873817920684814, 16.245370626449585, 17.616923332214355, 19.093000411987305, 20.569077491760254, 22.05255103111267, 23.536024570465088, 24.920055627822876, 26.304086685180664, 27.71846914291382, 29.132851600646973, 30.646048545837402, 32.15924549102783, 33.427796363830566, 34.6963472366333, 35.993932008743286, 37.29151678085327, 38.66762089729309, 40.04372501373291, 41.30836796760559, 42.57301092147827, 43.89804816246033, 45.22308540344238, 46.60000491142273, 47.976924419403076, 49.30010652542114, 50.62328863143921, 51.95119333267212, 53.27909803390503, 54.59000205993652, 55.90090608596802, 57.214131355285645, 58.52735662460327, 59.854700803756714, 61.182044982910156, 62.4902184009552, 63.798391819000244, 65.11768627166748, 66.43698072433472, 67.72429275512695, 69.01160478591919, 70.29982447624207, 71.58804416656494, 72.87355184555054, 74.15905952453613, 75.45315051078796, 76.7472414970398, 78.05707836151123, 79.36691522598267, 80.6668028831482, 81.96669054031372, 83.27597165107727, 84.58525276184082, 85.9585473537445, 87.3318419456482, 88.62173676490784, 89.91163158416748, 91.25512290000916, 92.59861421585083, 93.89483308792114, 95.19105195999146, 96.4742579460144, 97.75746393203735, 99.05046200752258, 100.34346008300781, 101.64728260040283, 102.95110511779785, 104.24148201942444, 105.53185892105103, 106.82239699363708, 108.11293506622314, 109.44248342514038, 110.77203178405762, 112.0720624923706, 113.3720932006836, 114.66519498825073, 115.95829677581787, 117.24319672584534, 118.5280966758728, 119.82674193382263, 121.12538719177246, 122.43612313270569, 123.74685907363892, 125.0604658126831, 126.3740725517273, 127.6587905883789, 128.94350862503052, 130.2496955394745, 131.55588245391846, 132.8576855659485, 134.15948867797852, 135.44987320899963, 136.74025774002075, 138.03241777420044, 139.32457780838013, 140.62869501113892, 141.9328122138977, 143.24062180519104, 144.54843139648438, 145.828022480011, 147.1076135635376, 148.47393417358398, 149.84025478363037, 151.14715003967285, 152.45404529571533, 153.716894865036, 154.9797444343567, 156.3503556251526, 157.7209668159485, 159.07133531570435, 160.4217038154602, 161.71404337882996, 163.0063829421997, 164.39735627174377, 165.78832960128784, 167.14098453521729, 168.49363946914673, 169.79784989356995, 171.10206031799316, 172.42055869102478, 173.7390570640564, 175.08263421058655, 176.4262113571167, 177.74401664733887, 179.06182193756104, 180.34596633911133, 181.63011074066162, 182.9541916847229, 184.27827262878418, 185.60468816757202, 186.93110370635986, 188.23118257522583, 189.5312614440918, 190.83628129959106, 192.14130115509033, 193.47098398208618, 194.80066680908203, 196.121258020401, 197.44184923171997, 198.75658202171326, 200.07131481170654, 201.391051530838, 202.71078824996948, 204.02838826179504, 205.3459882736206, 206.6439950466156, 207.9420018196106, 209.25071477890015, 210.5594277381897, 211.88468647003174, 213.20994520187378, 214.52606010437012, 215.84217500686646, 217.12930130958557, 218.4164276123047, 219.7765097618103, 221.13659191131592, 222.51434922218323, 223.89210653305054, 225.15565943717957, 226.4192123413086, 227.74566912651062, 229.07212591171265, 230.4402129650116, 231.80830001831055, 233.08110785484314, 234.35391569137573, 235.65142011642456, 236.9489245414734, 238.3282928466797, 239.707661151886, 241.00585317611694, 242.3040452003479, 243.6032989025116, 244.9025526046753, 246.23306727409363, 247.56358194351196, 248.92883038520813, 250.2940788269043, 251.57840943336487, 252.86274003982544, 254.18417763710022, 255.505615234375, 256.86782360076904, 258.2300319671631, 259.49356269836426, 260.75709342956543, 262.05986189842224, 263.36263036727905, 264.73790526390076, 266.11318016052246, 268.4184353351593, 270.72369050979614]
[25.933333333333334, 25.933333333333334, 46.166666666666664, 46.166666666666664, 57.00833333333333, 57.00833333333333, 57.38333333333333, 57.38333333333333, 62.233333333333334, 62.233333333333334, 65.025, 65.025, 65.625, 65.625, 69.625, 69.625, 71.99166666666666, 71.99166666666666, 72.96666666666667, 72.96666666666667, 72.15833333333333, 72.15833333333333, 74.025, 74.025, 73.3, 73.3, 74.04166666666667, 74.04166666666667, 74.9, 74.9, 77.80833333333334, 77.80833333333334, 78.65, 78.65, 78.84166666666667, 78.84166666666667, 79.15833333333333, 79.15833333333333, 79.03333333333333, 79.03333333333333, 78.8, 78.8, 78.18333333333334, 78.18333333333334, 78.40833333333333, 78.40833333333333, 78.725, 78.725, 79.26666666666667, 79.26666666666667, 79.18333333333334, 79.18333333333334, 79.75, 79.75, 79.64166666666667, 79.64166666666667, 80.03333333333333, 80.03333333333333, 80.81666666666666, 80.81666666666666, 80.66666666666667, 80.66666666666667, 80.79166666666667, 80.79166666666667, 80.88333333333334, 80.88333333333334, 80.875, 80.875, 81.20833333333333, 81.20833333333333, 80.81666666666666, 80.81666666666666, 81.18333333333334, 81.18333333333334, 80.64166666666667, 80.64166666666667, 81.25833333333334, 81.25833333333334, 81.24166666666666, 81.24166666666666, 81.4, 81.4, 81.38333333333334, 81.38333333333334, 81.53333333333333, 81.53333333333333, 81.45833333333333, 81.45833333333333, 81.925, 81.925, 81.54166666666667, 81.54166666666667, 81.26666666666667, 81.26666666666667, 81.45, 81.45, 81.925, 81.925, 81.83333333333333, 81.83333333333333, 81.675, 81.675, 81.475, 81.475, 81.06666666666666, 81.06666666666666, 81.225, 81.225, 81.6, 81.6, 81.89166666666667, 81.89166666666667, 81.65833333333333, 81.65833333333333, 81.675, 81.675, 82.325, 82.325, 81.96666666666667, 81.96666666666667, 82.15, 82.15, 82.56666666666666, 82.56666666666666, 82.50833333333334, 82.50833333333334, 82.275, 82.275, 82.275, 82.275, 82.34166666666667, 82.34166666666667, 82.125, 82.125, 81.88333333333334, 81.88333333333334, 81.94166666666666, 81.94166666666666, 82.33333333333333, 82.33333333333333, 82.24166666666666, 82.24166666666666, 82.4, 82.4, 82.375, 82.375, 82.5, 82.5, 82.05833333333334, 82.05833333333334, 82.25833333333334, 82.25833333333334, 81.58333333333333, 81.58333333333333, 81.85, 81.85, 81.95, 81.95, 82.425, 82.425, 82.35833333333333, 82.35833333333333, 82.20833333333333, 82.20833333333333, 82.39166666666667, 82.39166666666667, 82.44166666666666, 82.44166666666666, 82.45, 82.45, 82.55, 82.55, 82.45, 82.45, 82.64166666666667, 82.64166666666667, 82.70833333333333, 82.70833333333333, 82.55833333333334, 82.55833333333334, 82.35, 82.35, 82.35833333333333, 82.35833333333333, 82.525, 82.525, 82.91666666666667, 82.91666666666667, 82.84166666666667, 82.84166666666667, 82.875, 82.875, 83.075, 83.075, 83.15, 83.15, 82.9, 82.9, 82.84166666666667, 82.84166666666667, 82.36666666666666, 82.36666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.136, Test loss: 0.471, Test accuracy: 86.79
Final Round, Global train loss: 0.136, Global test loss: 1.091, Global test accuracy: 64.70
Average accuracy final 10 rounds: 86.33749999999999 

Average global accuracy final 10 rounds: 62.095 

2078.4057388305664
[1.5504639148712158, 3.1009278297424316, 4.410255193710327, 5.719582557678223, 7.017308473587036, 8.31503438949585, 9.622627973556519, 10.930221557617188, 12.270838499069214, 13.61145544052124, 14.881822347640991, 16.152189254760742, 17.439539670944214, 18.726890087127686, 20.142757654190063, 21.55862522125244, 22.98221182823181, 24.40579843521118, 25.87887716293335, 27.351955890655518, 28.75284767150879, 30.15373945236206, 31.561180591583252, 32.96862173080444, 34.37904739379883, 35.78947305679321, 37.24758553504944, 38.705698013305664, 40.17967224121094, 41.65364646911621, 43.06772422790527, 44.481801986694336, 45.8927116394043, 47.30362129211426, 48.77852535247803, 50.2534294128418, 51.72251081466675, 53.1915922164917, 54.636536836624146, 56.08148145675659, 57.52325963973999, 58.96503782272339, 60.31263470649719, 61.660231590270996, 63.00467085838318, 64.34911012649536, 65.62116122245789, 66.89321231842041, 68.17336559295654, 69.45351886749268, 70.73494791984558, 72.01637697219849, 73.49122643470764, 74.9660758972168, 76.42233610153198, 77.87859630584717, 79.27254033088684, 80.66648435592651, 82.10403251647949, 83.54158067703247, 84.81441235542297, 86.08724403381348, 87.35742378234863, 88.62760353088379, 89.94253540039062, 91.25746726989746, 92.53591465950012, 93.81436204910278, 95.1164140701294, 96.418466091156, 97.71517944335938, 99.01189279556274, 100.2920310497284, 101.57216930389404, 102.89128565788269, 104.21040201187134, 105.55238771438599, 106.89437341690063, 108.18789005279541, 109.48140668869019, 110.9035975933075, 112.3257884979248, 113.81566739082336, 115.30554628372192, 116.62906289100647, 117.95257949829102, 119.36543917655945, 120.77829885482788, 122.20921730995178, 123.64013576507568, 125.03805637359619, 126.4359769821167, 127.79027986526489, 129.1445827484131, 130.45929169654846, 131.77400064468384, 133.14534378051758, 134.51668691635132, 135.81429648399353, 137.11190605163574, 138.47328925132751, 139.8346724510193, 141.23997330665588, 142.64527416229248, 144.051011800766, 145.4567494392395, 146.85049295425415, 148.2442364692688, 149.85865378379822, 151.47307109832764, 153.17878675460815, 154.88450241088867, 156.55414605140686, 158.22378969192505, 159.81873059272766, 161.41367149353027, 163.04291486740112, 164.67215824127197, 166.1981987953186, 167.72423934936523, 169.19719791412354, 170.67015647888184, 172.16693663597107, 173.6637167930603, 175.2338263988495, 176.80393600463867, 178.39854907989502, 179.99316215515137, 181.6343550682068, 183.2755479812622, 184.85898303985596, 186.4424180984497, 188.08473992347717, 189.72706174850464, 191.36827993392944, 193.00949811935425, 194.49375104904175, 195.97800397872925, 197.45593976974487, 198.9338755607605, 200.49125337600708, 202.04863119125366, 203.52298998832703, 204.9973487854004, 206.48291039466858, 207.96847200393677, 209.45803141593933, 210.9475908279419, 212.5717625617981, 214.1959342956543, 215.7020604610443, 217.20818662643433, 218.76911449432373, 220.33004236221313, 221.91699266433716, 223.50394296646118, 224.9644968509674, 226.42505073547363, 227.99704933166504, 229.56904792785645, 231.11168909072876, 232.65433025360107, 234.22041845321655, 235.78650665283203, 237.2793881893158, 238.77226972579956, 240.38886976242065, 242.00546979904175, 243.663414478302, 245.32135915756226, 246.91368532180786, 248.50601148605347, 250.14484763145447, 251.78368377685547, 253.31170296669006, 254.83972215652466, 256.3574924468994, 257.87526273727417, 259.3579795360565, 260.84069633483887, 262.3665175437927, 263.8923387527466, 265.391459941864, 266.89058113098145, 268.4393846988678, 269.98818826675415, 271.53300380706787, 273.0778193473816, 274.654531955719, 276.2312445640564, 277.6712384223938, 279.1112322807312, 280.62746143341064, 282.1436905860901, 283.67025423049927, 285.19681787490845, 286.67900466918945, 288.16119146347046, 289.68203377723694, 291.2028760910034, 293.75923132896423, 296.31558656692505]
[28.025, 28.025, 39.05, 39.05, 44.141666666666666, 44.141666666666666, 55.725, 55.725, 63.81666666666667, 63.81666666666667, 68.33333333333333, 68.33333333333333, 71.025, 71.025, 71.9, 71.9, 76.025, 76.025, 76.70833333333333, 76.70833333333333, 77.225, 77.225, 78.325, 78.325, 78.375, 78.375, 78.46666666666667, 78.46666666666667, 79.05833333333334, 79.05833333333334, 79.03333333333333, 79.03333333333333, 79.6, 79.6, 79.41666666666667, 79.41666666666667, 80.325, 80.325, 80.90833333333333, 80.90833333333333, 80.75833333333334, 80.75833333333334, 80.475, 80.475, 80.33333333333333, 80.33333333333333, 81.24166666666666, 81.24166666666666, 80.99166666666666, 80.99166666666666, 81.49166666666666, 81.49166666666666, 81.59166666666667, 81.59166666666667, 81.875, 81.875, 82.71666666666667, 82.71666666666667, 82.74166666666666, 82.74166666666666, 82.74166666666666, 82.74166666666666, 83.79166666666667, 83.79166666666667, 84.0, 84.0, 84.075, 84.075, 84.00833333333334, 84.00833333333334, 84.29166666666667, 84.29166666666667, 83.98333333333333, 83.98333333333333, 84.125, 84.125, 84.33333333333333, 84.33333333333333, 84.45833333333333, 84.45833333333333, 84.2, 84.2, 84.45833333333333, 84.45833333333333, 84.53333333333333, 84.53333333333333, 84.14166666666667, 84.14166666666667, 84.20833333333333, 84.20833333333333, 84.575, 84.575, 84.275, 84.275, 84.53333333333333, 84.53333333333333, 85.09166666666667, 85.09166666666667, 84.96666666666667, 84.96666666666667, 85.01666666666667, 85.01666666666667, 85.19166666666666, 85.19166666666666, 85.83333333333333, 85.83333333333333, 85.775, 85.775, 85.05833333333334, 85.05833333333334, 85.20833333333333, 85.20833333333333, 85.31666666666666, 85.31666666666666, 85.5, 85.5, 85.36666666666666, 85.36666666666666, 85.40833333333333, 85.40833333333333, 85.61666666666666, 85.61666666666666, 85.69166666666666, 85.69166666666666, 85.2, 85.2, 85.41666666666667, 85.41666666666667, 85.60833333333333, 85.60833333333333, 85.84166666666667, 85.84166666666667, 85.86666666666666, 85.86666666666666, 86.14166666666667, 86.14166666666667, 85.86666666666666, 85.86666666666666, 85.60833333333333, 85.60833333333333, 85.49166666666666, 85.49166666666666, 85.35, 85.35, 85.84166666666667, 85.84166666666667, 85.69166666666666, 85.69166666666666, 85.66666666666667, 85.66666666666667, 85.25, 85.25, 85.33333333333333, 85.33333333333333, 85.45, 85.45, 85.55833333333334, 85.55833333333334, 85.50833333333334, 85.50833333333334, 86.05, 86.05, 85.84166666666667, 85.84166666666667, 86.40833333333333, 86.40833333333333, 86.48333333333333, 86.48333333333333, 86.075, 86.075, 85.99166666666666, 85.99166666666666, 85.925, 85.925, 85.63333333333334, 85.63333333333334, 85.70833333333333, 85.70833333333333, 86.05, 86.05, 86.05, 86.05, 86.31666666666666, 86.31666666666666, 86.59166666666667, 86.59166666666667, 86.70833333333333, 86.70833333333333, 86.60833333333333, 86.60833333333333, 86.33333333333333, 86.33333333333333, 86.11666666666666, 86.11666666666666, 85.83333333333333, 85.83333333333333, 86.35, 86.35, 86.46666666666667, 86.46666666666667, 86.79166666666667, 86.79166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.183, Test loss: 0.353, Test accuracy: 87.22
Average accuracy final 10 rounds: 87.0975 

1742.1363413333893
[1.6923465728759766, 3.384693145751953, 4.809221982955933, 6.233750820159912, 7.69359016418457, 9.153429508209229, 10.524348258972168, 11.895267009735107, 13.237996578216553, 14.580726146697998, 16.00988507270813, 17.43904399871826, 18.781818389892578, 20.124592781066895, 21.540921211242676, 22.957249641418457, 24.376848936080933, 25.796448230743408, 27.20103144645691, 28.60561466217041, 30.034430980682373, 31.463247299194336, 32.871224880218506, 34.279202461242676, 35.68860054016113, 37.09799861907959, 38.442665338516235, 39.78733205795288, 41.14598035812378, 42.50462865829468, 43.8698844909668, 45.235140323638916, 46.61770963668823, 48.00027894973755, 49.37644839286804, 50.752617835998535, 52.14097023010254, 53.52932262420654, 54.920809745788574, 56.312296867370605, 57.773396015167236, 59.23449516296387, 60.58490037918091, 61.93530559539795, 63.27741003036499, 64.61951446533203, 65.97629523277283, 67.33307600021362, 68.71692228317261, 70.10076856613159, 71.4699125289917, 72.8390564918518, 74.23667430877686, 75.6342921257019, 77.02899742126465, 78.42370271682739, 79.83071637153625, 81.23773002624512, 82.72984552383423, 84.22196102142334, 85.68278551101685, 87.14361000061035, 88.57659196853638, 90.0095739364624, 91.48983550071716, 92.97009706497192, 94.40170001983643, 95.83330297470093, 97.24596333503723, 98.65862369537354, 100.080983877182, 101.50334405899048, 102.94080567359924, 104.37826728820801, 105.81211614608765, 107.24596500396729, 108.69321084022522, 110.14045667648315, 111.57003879547119, 112.99962091445923, 114.4387936592102, 115.87796640396118, 117.2696385383606, 118.66131067276001, 120.05768823623657, 121.45406579971313, 122.90306496620178, 124.35206413269043, 125.8321328163147, 127.31220149993896, 128.8028905391693, 130.29357957839966, 131.76206135749817, 133.23054313659668, 134.72917556762695, 136.22780799865723, 137.74398732185364, 139.26016664505005, 140.66667437553406, 142.07318210601807, 143.52971506118774, 144.98624801635742, 146.4267053604126, 147.86716270446777, 149.3066782951355, 150.74619388580322, 152.15313696861267, 153.56008005142212, 155.01952743530273, 156.47897481918335, 157.97561955451965, 159.47226428985596, 160.97948932647705, 162.48671436309814, 163.92593502998352, 165.3651556968689, 166.8012068271637, 168.2372579574585, 169.7043433189392, 171.17142868041992, 172.57079648971558, 173.97016429901123, 175.36971282958984, 176.76926136016846, 178.16874957084656, 179.56823778152466, 180.99343132972717, 182.4186248779297, 183.84350991249084, 185.268394947052, 186.74890232086182, 188.22940969467163, 189.66250944137573, 191.09560918807983, 192.57929134368896, 194.0629734992981, 195.55514550209045, 197.0473175048828, 198.53685784339905, 200.02639818191528, 201.4902741909027, 202.95415019989014, 204.43522191047668, 205.91629362106323, 207.34273147583008, 208.76916933059692, 210.19412779808044, 211.61908626556396, 213.07408690452576, 214.52908754348755, 215.92922353744507, 217.3293595314026, 218.84314131736755, 220.35692310333252, 221.85379362106323, 223.35066413879395, 224.8171546459198, 226.28364515304565, 227.7040114402771, 229.12437772750854, 230.48203349113464, 231.83968925476074, 233.25881481170654, 234.67794036865234, 236.18852353096008, 237.69910669326782, 239.18222284317017, 240.6653389930725, 242.13884949684143, 243.61236000061035, 245.1297254562378, 246.64709091186523, 248.16973996162415, 249.69238901138306, 251.17958998680115, 252.66679096221924, 253.9928023815155, 255.31881380081177, 256.6608510017395, 258.00288820266724, 259.38063621520996, 260.7583842277527, 262.1605975627899, 263.56281089782715, 264.991331577301, 266.4198522567749, 267.94486951828003, 269.46988677978516, 270.9138021469116, 272.3577175140381, 273.81110882759094, 275.2645001411438, 276.6971263885498, 278.1297526359558, 279.5310196876526, 280.93228673934937, 282.33232975006104, 283.7323727607727, 285.1043791770935, 286.4763855934143, 288.70161652565, 290.92684745788574]
[19.941666666666666, 19.941666666666666, 40.141666666666666, 40.141666666666666, 39.8, 39.8, 49.958333333333336, 49.958333333333336, 57.09166666666667, 57.09166666666667, 64.925, 64.925, 70.33333333333333, 70.33333333333333, 72.79166666666667, 72.79166666666667, 72.98333333333333, 72.98333333333333, 74.70833333333333, 74.70833333333333, 75.83333333333333, 75.83333333333333, 75.69166666666666, 75.69166666666666, 77.425, 77.425, 77.28333333333333, 77.28333333333333, 77.55, 77.55, 78.0, 78.0, 78.70833333333333, 78.70833333333333, 79.275, 79.275, 80.06666666666666, 80.06666666666666, 80.425, 80.425, 80.99166666666666, 80.99166666666666, 81.24166666666666, 81.24166666666666, 81.81666666666666, 81.81666666666666, 81.65, 81.65, 81.93333333333334, 81.93333333333334, 81.73333333333333, 81.73333333333333, 82.15, 82.15, 82.775, 82.775, 82.9, 82.9, 82.61666666666666, 82.61666666666666, 82.875, 82.875, 82.99166666666666, 82.99166666666666, 83.01666666666667, 83.01666666666667, 83.5, 83.5, 83.41666666666667, 83.41666666666667, 84.16666666666667, 84.16666666666667, 83.9, 83.9, 83.625, 83.625, 83.91666666666667, 83.91666666666667, 83.8, 83.8, 83.775, 83.775, 83.975, 83.975, 83.74166666666666, 83.74166666666666, 84.325, 84.325, 84.45833333333333, 84.45833333333333, 84.61666666666666, 84.61666666666666, 84.39166666666667, 84.39166666666667, 84.39166666666667, 84.39166666666667, 84.76666666666667, 84.76666666666667, 84.83333333333333, 84.83333333333333, 84.725, 84.725, 84.53333333333333, 84.53333333333333, 84.76666666666667, 84.76666666666667, 84.99166666666666, 84.99166666666666, 84.45833333333333, 84.45833333333333, 85.59166666666667, 85.59166666666667, 85.675, 85.675, 85.53333333333333, 85.53333333333333, 85.60833333333333, 85.60833333333333, 85.525, 85.525, 85.525, 85.525, 85.49166666666666, 85.49166666666666, 85.71666666666667, 85.71666666666667, 85.50833333333334, 85.50833333333334, 85.65833333333333, 85.65833333333333, 85.60833333333333, 85.60833333333333, 86.33333333333333, 86.33333333333333, 86.19166666666666, 86.19166666666666, 86.35833333333333, 86.35833333333333, 86.40833333333333, 86.40833333333333, 85.80833333333334, 85.80833333333334, 86.20833333333333, 86.20833333333333, 86.275, 86.275, 86.03333333333333, 86.03333333333333, 85.98333333333333, 85.98333333333333, 86.375, 86.375, 86.3, 86.3, 86.25833333333334, 86.25833333333334, 86.65, 86.65, 86.71666666666667, 86.71666666666667, 86.39166666666667, 86.39166666666667, 86.73333333333333, 86.73333333333333, 86.76666666666667, 86.76666666666667, 86.60833333333333, 86.60833333333333, 86.80833333333334, 86.80833333333334, 86.775, 86.775, 86.85833333333333, 86.85833333333333, 86.86666666666666, 86.86666666666666, 86.78333333333333, 86.78333333333333, 86.71666666666667, 86.71666666666667, 86.95, 86.95, 87.00833333333334, 87.00833333333334, 87.03333333333333, 87.03333333333333, 86.93333333333334, 86.93333333333334, 87.18333333333334, 87.18333333333334, 87.35833333333333, 87.35833333333333, 87.24166666666666, 87.24166666666666, 87.125, 87.125, 87.18333333333334, 87.18333333333334, 86.95833333333333, 86.95833333333333, 87.225, 87.225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.105, Test loss: 0.403, Test accuracy: 87.82
Average accuracy final 10 rounds: 87.61722222222224 

2652.314126968384
[2.5389552116394043, 5.077910423278809, 7.4844982624053955, 9.891086101531982, 12.256566286087036, 14.62204647064209, 16.848803758621216, 19.075561046600342, 21.54745650291443, 24.019351959228516, 26.331441640853882, 28.643531322479248, 31.04185652732849, 33.440181732177734, 35.758474826812744, 38.076767921447754, 40.45880103111267, 42.84083414077759, 45.21838665008545, 47.59593915939331, 49.97186064720154, 52.347782135009766, 54.81387734413147, 57.279972553253174, 59.46297073364258, 61.64596891403198, 64.11325001716614, 66.5805311203003, 69.01787519454956, 71.45521926879883, 73.69626760482788, 75.93731594085693, 78.2008810043335, 80.46444606781006, 82.63949465751648, 84.8145432472229, 87.01689386367798, 89.21924448013306, 91.4407103061676, 93.66217613220215, 95.86177515983582, 98.06137418746948, 100.23783016204834, 102.4142861366272, 104.63009524345398, 106.84590435028076, 109.05045056343079, 111.25499677658081, 113.49679851531982, 115.73860025405884, 117.94504594802856, 120.15149164199829, 122.34358429908752, 124.53567695617676, 126.93479943275452, 129.33392190933228, 131.53387188911438, 133.73382186889648, 136.03578901290894, 138.3377561569214, 140.5738296508789, 142.80990314483643, 145.02595567703247, 147.24200820922852, 149.4841091632843, 151.7262101173401, 154.01746702194214, 156.3087239265442, 158.55592846870422, 160.80313301086426, 163.05129194259644, 165.2994508743286, 167.46891856193542, 169.63838624954224, 171.92896676063538, 174.21954727172852, 176.4244146347046, 178.62928199768066, 180.8284080028534, 183.02753400802612, 185.224937915802, 187.42234182357788, 189.66905641555786, 191.91577100753784, 194.1360559463501, 196.35634088516235, 198.59222602844238, 200.8281111717224, 203.04363560676575, 205.25916004180908, 207.47614789009094, 209.6931357383728, 211.970716714859, 214.24829769134521, 216.4969356060028, 218.7455735206604, 221.23349380493164, 223.72141408920288, 226.07115149497986, 228.42088890075684, 230.80458784103394, 233.18828678131104, 235.6120104789734, 238.03573417663574, 240.3948609828949, 242.75398778915405, 245.18443703651428, 247.6148862838745, 250.04279613494873, 252.47070598602295, 254.69549989700317, 256.9202938079834, 259.0873260498047, 261.254358291626, 263.50120544433594, 265.7480525970459, 267.942862033844, 270.1376714706421, 272.510573387146, 274.8834753036499, 277.27287769317627, 279.66228008270264, 281.9995148181915, 284.3367495536804, 286.6621685028076, 288.9875874519348, 291.26598381996155, 293.5443801879883, 295.85247564315796, 298.16057109832764, 300.4653367996216, 302.7701025009155, 305.18819546699524, 307.60628843307495, 309.9429497718811, 312.27961111068726, 314.63886427879333, 316.9981174468994, 319.2239909172058, 321.4498643875122, 323.6434190273285, 325.8369736671448, 327.97737169265747, 330.11776971817017, 332.3298716545105, 334.54197359085083, 336.80778646469116, 339.0735993385315, 341.2447052001953, 343.41581106185913, 345.59815788269043, 347.78050470352173, 350.00050711631775, 352.22050952911377, 354.3577980995178, 356.4950866699219, 358.71847009658813, 360.9418535232544, 363.16072058677673, 365.3795876502991, 367.65607237815857, 369.93255710601807, 372.16966938972473, 374.4067816734314, 376.5953781604767, 378.783974647522, 380.94014072418213, 383.0963068008423, 385.28591203689575, 387.4755172729492, 389.6322236061096, 391.78892993927, 393.9008128643036, 396.01269578933716, 398.18873381614685, 400.36477184295654, 402.60243797302246, 404.8401041030884, 407.09991908073425, 409.3597340583801, 411.53225469589233, 413.70477533340454, 415.84265899658203, 417.9805426597595, 420.1416435241699, 422.3027443885803, 424.4619390964508, 426.6211338043213, 428.7698748111725, 430.9186158180237, 433.05998253822327, 435.20134925842285, 437.3973424434662, 439.5933356285095, 441.8068470954895, 444.0203585624695, 446.31505489349365, 448.6097512245178, 450.93804025650024, 453.26632928848267, 455.56543946266174, 457.8645496368408]
[31.333333333333332, 31.333333333333332, 40.516666666666666, 40.516666666666666, 49.81111111111111, 49.81111111111111, 49.32222222222222, 49.32222222222222, 63.766666666666666, 63.766666666666666, 63.63333333333333, 63.63333333333333, 72.36666666666666, 72.36666666666666, 73.15, 73.15, 71.1, 71.1, 71.7388888888889, 71.7388888888889, 77.56666666666666, 77.56666666666666, 77.47222222222223, 77.47222222222223, 78.9, 78.9, 79.9888888888889, 79.9888888888889, 81.93888888888888, 81.93888888888888, 82.06111111111112, 82.06111111111112, 81.56666666666666, 81.56666666666666, 82.08888888888889, 82.08888888888889, 83.40555555555555, 83.40555555555555, 83.39444444444445, 83.39444444444445, 83.82777777777778, 83.82777777777778, 83.54444444444445, 83.54444444444445, 84.7, 84.7, 84.66666666666667, 84.66666666666667, 84.68333333333334, 84.68333333333334, 84.41666666666667, 84.41666666666667, 84.50555555555556, 84.50555555555556, 84.97777777777777, 84.97777777777777, 84.88888888888889, 84.88888888888889, 85.37777777777778, 85.37777777777778, 85.51666666666667, 85.51666666666667, 85.05555555555556, 85.05555555555556, 85.32222222222222, 85.32222222222222, 84.7, 84.7, 86.01666666666667, 86.01666666666667, 85.8, 85.8, 85.91666666666667, 85.91666666666667, 86.29444444444445, 86.29444444444445, 86.35, 86.35, 86.5, 86.5, 86.58888888888889, 86.58888888888889, 85.81111111111112, 85.81111111111112, 86.25555555555556, 86.25555555555556, 86.58333333333333, 86.58333333333333, 86.7, 86.7, 86.24444444444444, 86.24444444444444, 86.86666666666666, 86.86666666666666, 86.83888888888889, 86.83888888888889, 87.07777777777778, 87.07777777777778, 87.17777777777778, 87.17777777777778, 87.22222222222223, 87.22222222222223, 87.24444444444444, 87.24444444444444, 87.12222222222222, 87.12222222222222, 86.68888888888888, 86.68888888888888, 87.19444444444444, 87.19444444444444, 87.07222222222222, 87.07222222222222, 87.17777777777778, 87.17777777777778, 86.79444444444445, 86.79444444444445, 87.07777777777778, 87.07777777777778, 87.39444444444445, 87.39444444444445, 87.13888888888889, 87.13888888888889, 87.35, 87.35, 87.2, 87.2, 87.38333333333334, 87.38333333333334, 87.46111111111111, 87.46111111111111, 87.75555555555556, 87.75555555555556, 87.4888888888889, 87.4888888888889, 87.69444444444444, 87.69444444444444, 87.82222222222222, 87.82222222222222, 87.07222222222222, 87.07222222222222, 87.2, 87.2, 87.22222222222223, 87.22222222222223, 87.78333333333333, 87.78333333333333, 87.7, 87.7, 87.46666666666667, 87.46666666666667, 87.91666666666667, 87.91666666666667, 87.65, 87.65, 87.65555555555555, 87.65555555555555, 87.51666666666667, 87.51666666666667, 87.86666666666666, 87.86666666666666, 87.46111111111111, 87.46111111111111, 87.82777777777778, 87.82777777777778, 87.61666666666666, 87.61666666666666, 87.71666666666667, 87.71666666666667, 87.77222222222223, 87.77222222222223, 88.05, 88.05, 87.65, 87.65, 87.83888888888889, 87.83888888888889, 87.75, 87.75, 87.66666666666667, 87.66666666666667, 87.82777777777778, 87.82777777777778, 87.43333333333334, 87.43333333333334, 87.7388888888889, 87.7388888888889, 87.4, 87.4, 87.6, 87.6, 87.87777777777778, 87.87777777777778, 87.33333333333333, 87.33333333333333, 87.7611111111111, 87.7611111111111, 87.53888888888889, 87.53888888888889, 87.66111111111111, 87.66111111111111, 87.82222222222222, 87.82222222222222]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.031, Test loss: 0.889, Test accuracy: 84.19
Average accuracy final 10 rounds: 83.5888888888889 

2616.7433698177338
[2.6634366512298584, 5.326873302459717, 7.726099014282227, 10.125324726104736, 12.409938097000122, 14.694551467895508, 17.008958339691162, 19.323365211486816, 21.41378092765808, 23.504196643829346, 25.647238969802856, 27.790281295776367, 29.947460412979126, 32.104639530181885, 34.35231566429138, 36.59999179840088, 38.7301127910614, 40.860233783721924, 43.03100609779358, 45.201778411865234, 47.39944505691528, 49.59711170196533, 51.807398080825806, 54.01768445968628, 56.196139097213745, 58.37459373474121, 60.564253091812134, 62.75391244888306, 64.96029663085938, 67.1666808128357, 69.47557806968689, 71.78447532653809, 73.99409413337708, 76.20371294021606, 78.40036201477051, 80.59701108932495, 82.82664561271667, 85.0562801361084, 87.25579905509949, 89.45531797409058, 91.59807920455933, 93.74084043502808, 95.89003992080688, 98.0392394065857, 100.42707419395447, 102.81490898132324, 105.00988292694092, 107.2048568725586, 109.39567255973816, 111.58648824691772, 113.72033143043518, 115.85417461395264, 118.01457118988037, 120.1749677658081, 122.38011384010315, 124.5852599143982, 126.74678564071655, 128.9083113670349, 131.0687472820282, 133.22918319702148, 135.34636235237122, 137.46354150772095, 139.64016556739807, 141.8167896270752, 144.0493884086609, 146.28198719024658, 148.40623784065247, 150.53048849105835, 152.664235830307, 154.79798316955566, 156.93546772003174, 159.0729522705078, 161.28603625297546, 163.49912023544312, 165.64358162879944, 167.78804302215576, 169.96156692504883, 172.1350908279419, 174.27965545654297, 176.42422008514404, 178.5751953125, 180.72617053985596, 182.9097957611084, 185.09342098236084, 187.23105263710022, 189.3686842918396, 191.4829659461975, 193.59724760055542, 195.74734687805176, 197.8974461555481, 200.00821113586426, 202.11897611618042, 204.23880887031555, 206.35864162445068, 208.4904556274414, 210.62226963043213, 212.8144233226776, 215.0065770149231, 217.3400800228119, 219.67358303070068, 221.9790322780609, 224.28448152542114, 226.57211422920227, 228.8597469329834, 231.07516479492188, 233.29058265686035, 235.48712515830994, 237.68366765975952, 239.90656208992004, 242.12945652008057, 244.33786749839783, 246.5462784767151, 248.8659110069275, 251.1855435371399, 253.4181089401245, 255.65067434310913, 257.87830805778503, 260.10594177246094, 262.27249026298523, 264.4390387535095, 266.7239291667938, 269.0088195800781, 271.3262162208557, 273.6436128616333, 275.9357259273529, 278.2278389930725, 280.50534653663635, 282.7828540802002, 285.0763065814972, 287.3697590827942, 289.60140776634216, 291.83305644989014, 294.13692450523376, 296.4407925605774, 298.6581025123596, 300.87541246414185, 303.1596302986145, 305.44384813308716, 307.60167241096497, 309.7594966888428, 312.0617253780365, 314.3639540672302, 316.65579199790955, 318.94762992858887, 321.2807877063751, 323.6139454841614, 325.94578099250793, 328.2776165008545, 330.53254795074463, 332.78747940063477, 335.0307958126068, 337.27411222457886, 339.46121978759766, 341.64832735061646, 343.82228899002075, 345.99625062942505, 348.27297711372375, 350.54970359802246, 352.8421230316162, 355.13454246520996, 357.4265298843384, 359.7185173034668, 361.99436831474304, 364.2702193260193, 366.52078461647034, 368.7713499069214, 371.0046184062958, 373.23788690567017, 375.51168298721313, 377.7854790687561, 380.0989534854889, 382.4124279022217, 384.646409034729, 386.8803901672363, 389.1315610408783, 391.38273191452026, 393.670529127121, 395.9583263397217, 398.2291667461395, 400.5000071525574, 402.7979025840759, 405.0957980155945, 407.39422035217285, 409.6926426887512, 411.95255970954895, 414.2124767303467, 416.4829652309418, 418.75345373153687, 421.0312831401825, 423.3091125488281, 425.6499786376953, 427.9908447265625, 430.1255028247833, 432.26016092300415, 434.57624435424805, 436.89232778549194, 439.1916129589081, 441.4908981323242, 443.72984313964844, 445.96878814697266, 448.44617652893066, 450.9235649108887]
[30.72222222222222, 30.72222222222222, 45.03888888888889, 45.03888888888889, 45.51111111111111, 45.51111111111111, 60.63333333333333, 60.63333333333333, 69.06666666666666, 69.06666666666666, 66.47222222222223, 66.47222222222223, 69.64444444444445, 69.64444444444445, 74.25555555555556, 74.25555555555556, 73.28888888888889, 73.28888888888889, 74.61111111111111, 74.61111111111111, 75.52222222222223, 75.52222222222223, 77.04444444444445, 77.04444444444445, 77.5111111111111, 77.5111111111111, 77.09444444444445, 77.09444444444445, 76.65555555555555, 76.65555555555555, 77.62222222222222, 77.62222222222222, 78.61111111111111, 78.61111111111111, 79.42222222222222, 79.42222222222222, 79.5, 79.5, 80.17777777777778, 80.17777777777778, 80.25555555555556, 80.25555555555556, 79.46666666666667, 79.46666666666667, 80.13333333333334, 80.13333333333334, 80.37777777777778, 80.37777777777778, 80.63333333333334, 80.63333333333334, 80.64444444444445, 80.64444444444445, 80.98333333333333, 80.98333333333333, 80.97222222222223, 80.97222222222223, 81.07777777777778, 81.07777777777778, 81.20555555555555, 81.20555555555555, 81.40555555555555, 81.40555555555555, 81.49444444444444, 81.49444444444444, 81.40555555555555, 81.40555555555555, 81.62222222222222, 81.62222222222222, 81.3, 81.3, 81.47777777777777, 81.47777777777777, 81.72777777777777, 81.72777777777777, 81.50555555555556, 81.50555555555556, 81.37222222222222, 81.37222222222222, 81.77777777777777, 81.77777777777777, 81.8, 81.8, 82.4888888888889, 82.4888888888889, 82.85555555555555, 82.85555555555555, 82.79444444444445, 82.79444444444445, 82.62777777777778, 82.62777777777778, 82.43888888888888, 82.43888888888888, 82.87777777777778, 82.87777777777778, 82.5111111111111, 82.5111111111111, 82.2, 82.2, 81.78333333333333, 81.78333333333333, 81.75555555555556, 81.75555555555556, 82.46111111111111, 82.46111111111111, 82.91666666666667, 82.91666666666667, 82.62777777777778, 82.62777777777778, 82.58333333333333, 82.58333333333333, 82.81666666666666, 82.81666666666666, 82.96666666666667, 82.96666666666667, 82.77222222222223, 82.77222222222223, 82.84444444444445, 82.84444444444445, 82.79444444444445, 82.79444444444445, 82.79444444444445, 82.79444444444445, 82.48333333333333, 82.48333333333333, 82.61666666666666, 82.61666666666666, 82.97222222222223, 82.97222222222223, 83.15, 83.15, 83.20555555555555, 83.20555555555555, 83.27222222222223, 83.27222222222223, 83.36111111111111, 83.36111111111111, 83.36666666666666, 83.36666666666666, 83.7388888888889, 83.7388888888889, 83.60555555555555, 83.60555555555555, 83.57777777777778, 83.57777777777778, 83.37222222222222, 83.37222222222222, 83.65, 83.65, 83.53888888888889, 83.53888888888889, 83.66666666666667, 83.66666666666667, 83.53888888888889, 83.53888888888889, 83.30555555555556, 83.30555555555556, 83.43888888888888, 83.43888888888888, 83.60555555555555, 83.60555555555555, 83.6, 83.6, 83.41111111111111, 83.41111111111111, 83.3, 83.3, 83.71666666666667, 83.71666666666667, 83.67222222222222, 83.67222222222222, 83.53333333333333, 83.53333333333333, 83.78333333333333, 83.78333333333333, 83.72222222222223, 83.72222222222223, 83.62222222222222, 83.62222222222222, 83.77222222222223, 83.77222222222223, 83.53333333333333, 83.53333333333333, 83.62222222222222, 83.62222222222222, 83.63888888888889, 83.63888888888889, 83.54444444444445, 83.54444444444445, 83.46666666666667, 83.46666666666667, 83.56666666666666, 83.56666666666666, 83.59444444444445, 83.59444444444445, 83.52777777777777, 83.52777777777777, 83.62222222222222, 83.62222222222222, 83.77222222222223, 83.77222222222223, 84.18888888888888, 84.18888888888888]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 438, in train
    batch_loss.append(loss.item())
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.557, Test loss: 0.552, Test accuracy: 77.80
Average accuracy final 10 rounds: 78.73833333333333
Average global accuracy final 10 rounds: 78.73833333333333
1717.3389010429382
[]
[21.316666666666666, 28.425, 39.608333333333334, 56.541666666666664, 59.475, 60.18333333333333, 61.925, 61.99166666666667, 64.31666666666666, 66.29166666666667, 67.50833333333334, 67.64166666666667, 67.25, 67.3, 66.15833333333333, 67.9, 68.10833333333333, 68.88333333333334, 67.79166666666667, 69.09166666666667, 70.475, 71.03333333333333, 70.36666666666666, 70.48333333333333, 72.6, 74.98333333333333, 75.06666666666666, 75.05, 74.85, 74.86666666666666, 75.75833333333334, 76.26666666666667, 75.74166666666666, 76.36666666666666, 75.825, 75.475, 75.31666666666666, 76.6, 76.0, 76.525, 76.95, 77.19166666666666, 76.85833333333333, 76.93333333333334, 76.48333333333333, 77.18333333333334, 77.06666666666666, 76.85833333333333, 76.80833333333334, 76.55, 76.875, 76.94166666666666, 77.1, 76.9, 77.63333333333334, 77.275, 76.99166666666666, 77.20833333333333, 76.8, 77.03333333333333, 76.78333333333333, 77.575, 77.33333333333333, 77.95, 77.575, 77.525, 78.03333333333333, 77.94166666666666, 77.51666666666667, 77.03333333333333, 76.825, 76.94166666666666, 76.9, 76.99166666666666, 77.475, 77.86666666666666, 77.25, 77.4, 77.53333333333333, 78.45833333333333, 78.325, 78.46666666666667, 78.25, 78.36666666666666, 78.44166666666666, 77.65, 78.45833333333333, 78.73333333333333, 78.85, 78.60833333333333, 79.06666666666666, 78.61666666666666, 78.40833333333333, 78.45833333333333, 78.55833333333334, 78.475, 79.075, 79.11666666666666, 78.73333333333333, 78.875, 77.8]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.293, Test loss: 0.728, Test accuracy: 79.38
Average accuracy final 10 rounds: 79.03725
8271.66266989708
[14.155335426330566, 27.896233558654785, 41.92708730697632, 55.59601664543152, 69.43136787414551, 83.07526540756226, 96.6687490940094, 110.48524069786072, 123.53528237342834, 136.87540650367737, 150.04490447044373, 163.3306200504303, 176.67564296722412, 189.78634452819824, 202.9784450531006, 216.37519693374634, 229.9110677242279, 243.10019516944885, 256.1255123615265, 269.51245760917664, 282.63939023017883, 295.3223948478699, 307.9065635204315, 319.83183789253235, 331.3196527957916, 342.83364820480347, 354.2120382785797, 365.5198073387146, 376.9566812515259, 388.4287507534027, 399.90116000175476, 411.3363182544708, 422.7699842453003, 434.2434341907501, 445.75218892097473, 457.26228380203247, 468.6200706958771, 479.9949562549591, 491.4627192020416, 502.9336268901825, 514.505069732666, 525.9997787475586, 537.5621519088745, 549.075412273407, 560.6064727306366, 572.0462701320648, 583.5341548919678, 594.9895868301392, 606.5558574199677, 618.1519193649292, 629.7556796073914, 641.2261898517609, 652.7575354576111, 664.2967011928558, 675.736439704895, 687.1856126785278, 698.6601116657257, 710.1244027614594, 721.5881531238556, 732.9627623558044, 744.3543558120728, 755.795072555542, 767.2322027683258, 778.622921705246, 790.0142333507538, 801.4299812316895, 812.8693153858185, 824.2842953205109, 836.2432105541229, 847.661007642746, 859.112292766571, 870.4834740161896, 881.9051222801208, 893.3048949241638, 904.6662819385529, 916.0017294883728, 927.3505492210388, 938.7939457893372, 950.1488881111145, 961.4707698822021, 972.917129278183, 984.3725414276123, 995.8330047130585, 1007.3296756744385, 1018.75630402565, 1030.0813891887665, 1041.4479162693024, 1052.7342138290405, 1063.9904959201813, 1075.2321195602417, 1086.4448971748352, 1097.7810695171356, 1109.102732181549, 1120.290462255478, 1131.5800940990448, 1142.8751909732819, 1154.29554438591, 1165.7538447380066, 1177.1338124275208, 1188.6379978656769, 1191.866622209549]
[38.8, 46.84, 51.385, 54.7575, 58.065, 60.7625, 62.42, 64.0325, 64.6425, 67.1175, 68.1625, 68.5425, 69.735, 69.9975, 70.945, 71.3125, 71.615, 71.9225, 72.645, 73.23, 73.8475, 73.305, 73.9125, 74.2925, 74.2825, 74.4175, 74.825, 74.67, 75.3975, 75.005, 75.4075, 75.51, 75.2475, 75.6175, 75.5275, 75.9975, 76.115, 76.05, 76.37, 75.81, 76.26, 76.7375, 76.4875, 76.4875, 76.8925, 76.455, 77.38, 76.835, 77.215, 76.72, 76.92, 76.3825, 77.13, 77.285, 77.46, 77.1925, 77.3875, 77.4675, 77.37, 77.535, 77.9875, 77.74, 77.365, 77.8125, 77.7, 77.8975, 77.6175, 77.88, 78.3225, 77.7, 77.86, 77.83, 77.665, 78.08, 78.2575, 78.4125, 78.1325, 78.19, 78.5, 78.6725, 78.3175, 78.8075, 78.685, 78.315, 78.86, 78.9075, 78.6, 78.84, 79.02, 78.9475, 78.7875, 78.9725, 79.305, 78.9525, 79.32, 79.45, 78.8075, 79.0825, 78.88, 78.815, 79.375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.425, Test loss: 0.699, Test accuracy: 77.31
Average accuracy final 10 rounds: 76.7445
4574.325084209442
[6.106100559234619, 11.924298763275146, 17.735737562179565, 23.540982484817505, 29.36302423477173, 35.2764208316803, 41.15225839614868, 46.958351612091064, 52.71220564842224, 58.485058307647705, 64.33705544471741, 70.17611336708069, 75.93017911911011, 81.7356505393982, 87.5322015285492, 93.27962470054626, 99.04709267616272, 104.82454037666321, 110.62706398963928, 116.42829084396362, 122.19586110115051, 127.93595790863037, 133.73464608192444, 139.51793026924133, 145.26255631446838, 151.018807888031, 156.82076334953308, 162.59149074554443, 168.39490365982056, 174.19923973083496, 179.95458149909973, 185.67792439460754, 191.4766993522644, 197.3126835823059, 203.10940408706665, 208.9363133907318, 214.80114817619324, 220.58319568634033, 226.29820775985718, 232.04925894737244, 237.81642532348633, 243.5753412246704, 249.1422083377838, 254.9133324623108, 260.63602471351624, 266.36262369155884, 272.1179766654968, 277.7250702381134, 283.4778745174408, 289.24202513694763, 294.9884078502655, 300.7353045940399, 306.4672272205353, 312.2380714416504, 317.9782381057739, 323.711305141449, 329.43375158309937, 335.18867325782776, 341.0020046234131, 346.8095541000366, 352.60330033302307, 358.1906633377075, 363.8174433708191, 369.4466903209686, 375.04577827453613, 380.7741696834564, 386.52717661857605, 392.3070533275604, 398.12048172950745, 403.9423990249634, 409.6999464035034, 415.4398009777069, 421.1902503967285, 426.9435646533966, 432.6601514816284, 438.46537351608276, 444.2739510536194, 450.08239364624023, 455.86788988113403, 461.6909427642822, 467.4917104244232, 473.25107741355896, 478.9867641925812, 484.7710907459259, 490.5937469005585, 496.41994404792786, 502.17131447792053, 507.93754839897156, 513.5218994617462, 519.08522772789, 524.8701717853546, 530.6644809246063, 536.5184533596039, 542.3318519592285, 548.0812952518463, 554.0206789970398, 559.88223695755, 565.6506326198578, 571.4591310024261, 577.2330696582794, 579.3990585803986]
[33.39, 41.9075, 45.895, 49.6175, 52.3025, 54.235, 56.7275, 58.4275, 59.8, 61.2275, 62.0375, 62.5675, 64.905, 65.6225, 66.4, 67.0575, 67.8725, 68.565, 69.145, 69.705, 69.8475, 70.0475, 70.645, 70.7, 71.5375, 71.5875, 71.675, 71.915, 72.445, 72.595, 72.5125, 73.0825, 72.785, 72.345, 73.1175, 73.03, 73.4975, 73.48, 73.5375, 73.975, 74.2875, 74.8325, 74.7975, 74.61, 74.81, 75.105, 75.1175, 75.26, 75.2225, 75.345, 75.3075, 74.87, 75.2525, 75.4225, 75.1525, 75.295, 75.5, 75.3375, 75.0775, 75.425, 75.4, 75.76, 75.5625, 75.12, 75.69, 75.66, 75.9075, 75.9775, 76.445, 75.7625, 75.72, 76.2325, 75.9625, 76.1725, 76.1725, 76.255, 76.4925, 76.7, 76.6675, 76.6775, 76.665, 75.8525, 76.32, 76.1175, 76.1725, 76.1475, 76.4075, 76.6875, 76.685, 76.6275, 76.5475, 76.105, 76.9, 76.4375, 76.485, 76.9225, 76.83, 76.665, 77.2675, 77.285, 77.315]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.292, Test loss: 0.458, Test accuracy: 81.97
Average accuracy final 10 rounds: 81.73666666666666
1661.2663679122925
[2.1237618923187256, 4.247523784637451, 5.944533824920654, 7.641543865203857, 9.306119441986084, 10.97069501876831, 12.625237226486206, 14.279779434204102, 15.925904989242554, 17.572030544281006, 19.223551511764526, 20.875072479248047, 22.512227773666382, 24.149383068084717, 25.794439554214478, 27.43949604034424, 29.099955081939697, 30.760414123535156, 32.42420554161072, 34.08799695968628, 35.73188042640686, 37.37576389312744, 39.09145259857178, 40.80714130401611, 42.56043267250061, 44.31372404098511, 46.00378489494324, 47.69384574890137, 49.42524838447571, 51.15665102005005, 52.90000915527344, 54.643367290496826, 56.342177867889404, 58.04098844528198, 59.7537739276886, 61.466559410095215, 63.20077729225159, 64.93499517440796, 66.65030884742737, 68.36562252044678, 70.08693313598633, 71.80824375152588, 73.55083107948303, 75.29341840744019, 76.98858213424683, 78.68374586105347, 80.42024087905884, 82.15673589706421, 83.8837411403656, 85.61074638366699, 87.30784583091736, 89.00494527816772, 90.74160766601562, 92.47827005386353, 94.1926007270813, 95.90693140029907, 97.61856698989868, 99.33020257949829, 101.01749300956726, 102.70478343963623, 104.24646043777466, 105.78813743591309, 107.308758020401, 108.82937860488892, 110.36250710487366, 111.8956356048584, 113.45985054969788, 115.02406549453735, 116.56227421760559, 118.10048294067383, 119.64048218727112, 121.18048143386841, 122.73638010025024, 124.29227876663208, 125.84879112243652, 127.40530347824097, 128.96899223327637, 130.53268098831177, 132.07427740097046, 133.61587381362915, 135.1847939491272, 136.75371408462524, 138.31318283081055, 139.87265157699585, 141.40264415740967, 142.9326367378235, 144.48037099838257, 146.02810525894165, 147.56427454948425, 149.10044384002686, 150.6410267353058, 152.18160963058472, 153.71861219406128, 155.25561475753784, 156.79909873008728, 158.34258270263672, 159.90723490715027, 161.47188711166382, 163.00419187545776, 164.5364966392517, 166.0845787525177, 167.6326608657837, 169.20217895507812, 170.77169704437256, 172.3039152622223, 173.83613348007202, 175.37762093544006, 176.9191083908081, 178.46084094047546, 180.00257349014282, 181.54197645187378, 183.08137941360474, 184.60903930664062, 186.1366991996765, 187.67171025276184, 189.20672130584717, 190.76647639274597, 192.32623147964478, 193.8527853488922, 195.37933921813965, 196.94245839118958, 198.5055775642395, 200.07183861732483, 201.63809967041016, 203.1673698425293, 204.69664001464844, 206.24849200248718, 207.80034399032593, 209.33957314491272, 210.8788022994995, 212.42658853530884, 213.97437477111816, 215.49353909492493, 217.0127034187317, 218.5547752380371, 220.09684705734253, 221.645122051239, 223.1933970451355, 224.71122360229492, 226.22905015945435, 227.7850251197815, 229.34100008010864, 230.91874933242798, 232.49649858474731, 234.03129720687866, 235.56609582901, 237.1104166507721, 238.65473747253418, 240.1955668926239, 241.73639631271362, 243.28321313858032, 244.83002996444702, 246.36087584495544, 247.89172172546387, 249.42759490013123, 250.96346807479858, 252.5147213935852, 254.06597471237183, 255.5947597026825, 257.12354469299316, 258.6532473564148, 260.1829500198364, 261.73085713386536, 263.2787642478943, 264.81873774528503, 266.3587112426758, 267.88678908348083, 269.4148669242859, 270.98144125938416, 272.5480155944824, 274.07959246635437, 275.6111693382263, 277.11021161079407, 278.6092538833618, 280.1183669567108, 281.6274800300598, 283.1376760005951, 284.64787197113037, 286.1480519771576, 287.6482319831848, 289.1637682914734, 290.67930459976196, 292.188884973526, 293.69846534729004, 295.22643852233887, 296.7544116973877, 298.24791264533997, 299.74141359329224, 301.25527143478394, 302.76912927627563, 304.279159784317, 305.7891902923584, 311.58624291419983, 317.38329553604126, 319.05541253089905, 320.72752952575684, 322.333411693573, 323.93929386138916, 325.5222370624542, 327.1051802635193, 329.1902506351471, 331.2753210067749]
[22.375, 22.375, 41.325, 41.325, 50.641666666666666, 50.641666666666666, 57.05833333333333, 57.05833333333333, 57.75833333333333, 57.75833333333333, 60.3, 60.3, 64.075, 64.075, 64.86666666666666, 64.86666666666666, 67.73333333333333, 67.73333333333333, 68.16666666666667, 68.16666666666667, 68.71666666666667, 68.71666666666667, 70.2, 70.2, 70.4, 70.4, 72.025, 72.025, 72.18333333333334, 72.18333333333334, 72.68333333333334, 72.68333333333334, 72.75833333333334, 72.75833333333334, 73.55833333333334, 73.55833333333334, 74.08333333333333, 74.08333333333333, 74.41666666666667, 74.41666666666667, 74.95833333333333, 74.95833333333333, 74.9, 74.9, 75.29166666666667, 75.29166666666667, 75.55833333333334, 75.55833333333334, 75.95833333333333, 75.95833333333333, 75.48333333333333, 75.48333333333333, 75.9, 75.9, 76.375, 76.375, 76.925, 76.925, 77.475, 77.475, 77.58333333333333, 77.58333333333333, 77.14166666666667, 77.14166666666667, 77.675, 77.675, 78.18333333333334, 78.18333333333334, 78.55833333333334, 78.55833333333334, 78.58333333333333, 78.58333333333333, 78.71666666666667, 78.71666666666667, 78.33333333333333, 78.33333333333333, 78.65, 78.65, 78.8, 78.8, 78.78333333333333, 78.78333333333333, 78.79166666666667, 78.79166666666667, 78.9, 78.9, 78.86666666666666, 78.86666666666666, 79.25, 79.25, 79.63333333333334, 79.63333333333334, 79.65833333333333, 79.65833333333333, 79.96666666666667, 79.96666666666667, 79.7, 79.7, 79.78333333333333, 79.78333333333333, 80.2, 80.2, 80.04166666666667, 80.04166666666667, 80.16666666666667, 80.16666666666667, 79.99166666666666, 79.99166666666666, 79.675, 79.675, 79.88333333333334, 79.88333333333334, 80.45833333333333, 80.45833333333333, 80.26666666666667, 80.26666666666667, 80.475, 80.475, 80.825, 80.825, 80.44166666666666, 80.44166666666666, 80.2, 80.2, 80.73333333333333, 80.73333333333333, 80.55, 80.55, 80.86666666666666, 80.86666666666666, 80.88333333333334, 80.88333333333334, 81.20833333333333, 81.20833333333333, 81.575, 81.575, 81.3, 81.3, 81.325, 81.325, 81.45833333333333, 81.45833333333333, 80.88333333333334, 80.88333333333334, 81.375, 81.375, 81.475, 81.475, 81.55833333333334, 81.55833333333334, 81.53333333333333, 81.53333333333333, 81.64166666666667, 81.64166666666667, 81.31666666666666, 81.31666666666666, 81.625, 81.625, 81.96666666666667, 81.96666666666667, 81.70833333333333, 81.70833333333333, 81.95, 81.95, 81.975, 81.975, 81.5, 81.5, 81.8, 81.8, 81.93333333333334, 81.93333333333334, 81.95833333333333, 81.95833333333333, 81.99166666666666, 81.99166666666666, 82.10833333333333, 82.10833333333333, 82.1, 82.1, 81.93333333333334, 81.93333333333334, 82.025, 82.025, 82.00833333333334, 82.00833333333334, 81.98333333333333, 81.98333333333333, 81.69166666666666, 81.69166666666666, 81.48333333333333, 81.48333333333333, 81.04166666666667, 81.04166666666667, 81.89166666666667, 81.89166666666667, 82.00833333333334, 82.00833333333334, 81.3, 81.3, 81.975, 81.975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.050, Test loss: 0.961, Test accuracy: 81.97
Final Round, Global train loss: 0.050, Global test loss: 2.334, Global test accuracy: 17.37
Average accuracy final 10 rounds: 80.57333333333332 

Average global accuracy final 10 rounds: 14.279166666666665 

1941.7790706157684
[1.6704425811767578, 3.3408851623535156, 4.80021595954895, 6.259546756744385, 7.711470127105713, 9.163393497467041, 10.639723300933838, 12.116053104400635, 13.580100297927856, 15.044147491455078, 16.52588725090027, 18.00762701034546, 19.47560214996338, 20.9435772895813, 22.401695251464844, 23.85981321334839, 25.327406883239746, 26.795000553131104, 28.266990184783936, 29.738979816436768, 31.213582515716553, 32.68818521499634, 34.157493352890015, 35.62680149078369, 37.089303970336914, 38.55180644989014, 40.024195432662964, 41.49658441543579, 42.95353603363037, 44.41048765182495, 45.874526023864746, 47.33856439590454, 48.8051278591156, 50.27169132232666, 51.74390530586243, 53.21611928939819, 54.68936800956726, 56.16261672973633, 57.63237190246582, 59.10212707519531, 60.57437491416931, 62.04662275314331, 63.50267219543457, 64.95872163772583, 66.41681742668152, 67.8749132156372, 69.34858536720276, 70.82225751876831, 72.27576398849487, 73.72927045822144, 75.17541456222534, 76.62155866622925, 78.08705711364746, 79.55255556106567, 81.01270914077759, 82.4728627204895, 83.920254945755, 85.36764717102051, 86.84465861320496, 88.3216700553894, 89.7795729637146, 91.2374758720398, 92.68804669380188, 94.13861751556396, 95.5871376991272, 97.03565788269043, 98.509117603302, 99.98257732391357, 101.43735098838806, 102.89212465286255, 104.33243823051453, 105.7727518081665, 107.24549078941345, 108.7182297706604, 110.19445013999939, 111.67067050933838, 113.1196346282959, 114.56859874725342, 116.02066993713379, 117.47274112701416, 118.9430410861969, 120.41334104537964, 121.87029433250427, 123.3272476196289, 124.78403759002686, 126.2408275604248, 127.701828956604, 129.1628303527832, 130.62139010429382, 132.07994985580444, 133.52482628822327, 134.9697027206421, 136.45798110961914, 137.9462594985962, 139.4151713848114, 140.8840832710266, 142.33517384529114, 143.78626441955566, 145.25412273406982, 146.72198104858398, 148.19217443466187, 149.66236782073975, 151.13171362876892, 152.6010594367981, 154.0681893825531, 155.5353193283081, 157.00048208236694, 158.46564483642578, 159.93656516075134, 161.4074854850769, 162.8822045326233, 164.35692358016968, 165.81751918792725, 167.27811479568481, 168.75948286056519, 170.24085092544556, 171.69936609268188, 173.1578812599182, 174.6225461959839, 176.08721113204956, 177.55191898345947, 179.01662683486938, 180.48188304901123, 181.94713926315308, 183.40740442276, 184.86766958236694, 186.33208203315735, 187.79649448394775, 189.27221274375916, 190.74793100357056, 192.21530604362488, 193.6826810836792, 195.1464672088623, 196.6102533340454, 198.07653069496155, 199.54280805587769, 201.02949690818787, 202.51618576049805, 203.9691891670227, 205.42219257354736, 206.88852047920227, 208.35484838485718, 209.7300844192505, 211.1053204536438, 212.48359036445618, 213.86186027526855, 215.2336778640747, 216.60549545288086, 218.00125741958618, 219.3970193862915, 220.83904910087585, 222.2810788154602, 223.72849941253662, 225.17592000961304, 226.64397621154785, 228.11203241348267, 229.58416557312012, 231.05629873275757, 232.51420521736145, 233.97211170196533, 235.444349527359, 236.91658735275269, 238.40864896774292, 239.90071058273315, 241.352303981781, 242.80389738082886, 244.18373894691467, 245.5635805130005, 246.9483826160431, 248.3331847190857, 249.72750854492188, 251.12183237075806, 252.49810886383057, 253.87438535690308, 255.2671127319336, 256.6598401069641, 258.0602743625641, 259.46070861816406, 260.913950920105, 262.3671932220459, 263.82526111602783, 265.28332901000977, 266.7785189151764, 268.273708820343, 269.7411193847656, 271.20852994918823, 272.6589689254761, 274.1094079017639, 275.5759468078613, 277.04248571395874, 278.519508600235, 279.99653148651123, 281.4616286754608, 282.9267258644104, 284.3863625526428, 285.84599924087524, 287.3173451423645, 288.78869104385376, 290.26649594306946, 291.74430084228516, 294.17885088920593, 296.6134009361267]
[26.858333333333334, 26.858333333333334, 38.65, 38.65, 44.416666666666664, 44.416666666666664, 55.725, 55.725, 66.85, 66.85, 71.84166666666667, 71.84166666666667, 73.21666666666667, 73.21666666666667, 73.73333333333333, 73.73333333333333, 73.9, 73.9, 74.70833333333333, 74.70833333333333, 74.31666666666666, 74.31666666666666, 75.4, 75.4, 75.475, 75.475, 74.975, 74.975, 75.91666666666667, 75.91666666666667, 75.375, 75.375, 76.09166666666667, 76.09166666666667, 77.0, 77.0, 76.925, 76.925, 77.325, 77.325, 77.65833333333333, 77.65833333333333, 77.81666666666666, 77.81666666666666, 77.30833333333334, 77.30833333333334, 78.25833333333334, 78.25833333333334, 78.00833333333334, 78.00833333333334, 78.53333333333333, 78.53333333333333, 78.89166666666667, 78.89166666666667, 79.3, 79.3, 78.65, 78.65, 79.09166666666667, 79.09166666666667, 79.65833333333333, 79.65833333333333, 79.75833333333334, 79.75833333333334, 80.05, 80.05, 79.53333333333333, 79.53333333333333, 79.83333333333333, 79.83333333333333, 80.35833333333333, 80.35833333333333, 80.23333333333333, 80.23333333333333, 79.85, 79.85, 80.05833333333334, 80.05833333333334, 79.84166666666667, 79.84166666666667, 79.58333333333333, 79.58333333333333, 79.43333333333334, 79.43333333333334, 79.85, 79.85, 79.70833333333333, 79.70833333333333, 79.74166666666666, 79.74166666666666, 79.84166666666667, 79.84166666666667, 80.20833333333333, 80.20833333333333, 80.28333333333333, 80.28333333333333, 80.56666666666666, 80.56666666666666, 80.375, 80.375, 80.59166666666667, 80.59166666666667, 80.78333333333333, 80.78333333333333, 80.75, 80.75, 80.55833333333334, 80.55833333333334, 80.26666666666667, 80.26666666666667, 80.14166666666667, 80.14166666666667, 80.75, 80.75, 80.76666666666667, 80.76666666666667, 80.725, 80.725, 80.46666666666667, 80.46666666666667, 80.45833333333333, 80.45833333333333, 80.45, 80.45, 80.46666666666667, 80.46666666666667, 80.53333333333333, 80.53333333333333, 80.63333333333334, 80.63333333333334, 80.38333333333334, 80.38333333333334, 80.75833333333334, 80.75833333333334, 80.40833333333333, 80.40833333333333, 80.58333333333333, 80.58333333333333, 80.93333333333334, 80.93333333333334, 81.08333333333333, 81.08333333333333, 80.89166666666667, 80.89166666666667, 80.70833333333333, 80.70833333333333, 80.38333333333334, 80.38333333333334, 80.69166666666666, 80.69166666666666, 80.8, 80.8, 81.06666666666666, 81.06666666666666, 81.00833333333334, 81.00833333333334, 80.90833333333333, 80.90833333333333, 81.00833333333334, 81.00833333333334, 81.125, 81.125, 80.875, 80.875, 80.8, 80.8, 80.91666666666667, 80.91666666666667, 80.98333333333333, 80.98333333333333, 80.89166666666667, 80.89166666666667, 81.15, 81.15, 81.15833333333333, 81.15833333333333, 80.91666666666667, 80.91666666666667, 80.88333333333334, 80.88333333333334, 80.76666666666667, 80.76666666666667, 80.45, 80.45, 80.78333333333333, 80.78333333333333, 81.05, 81.05, 80.55, 80.55, 80.54166666666667, 80.54166666666667, 80.74166666666666, 80.74166666666666, 80.45833333333333, 80.45833333333333, 80.15833333333333, 80.15833333333333, 80.23333333333333, 80.23333333333333, 81.975, 81.975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.180, Test loss: 0.572, Test accuracy: 83.69
Final Round, Global train loss: 0.180, Global test loss: 3.173, Global test accuracy: 16.29
Average accuracy final 10 rounds: 84.06666666666669 

Average global accuracy final 10 rounds: 18.425833333333333 

1906.1862008571625
[1.702063798904419, 3.404127597808838, 4.862179517745972, 6.3202314376831055, 7.759759426116943, 9.199287414550781, 10.64822769165039, 12.09716796875, 13.55291199684143, 15.008656024932861, 16.476887702941895, 17.945119380950928, 19.390848875045776, 20.836578369140625, 22.300297498703003, 23.76401662826538, 25.212987899780273, 26.661959171295166, 28.11226773262024, 29.562576293945312, 31.012278079986572, 32.46197986602783, 33.91139602661133, 35.360812187194824, 36.82230353355408, 38.28379487991333, 39.74451279640198, 41.205230712890625, 42.668248653411865, 44.131266593933105, 45.57956886291504, 47.02787113189697, 48.48627209663391, 49.94467306137085, 51.39756226539612, 52.85045146942139, 54.29216241836548, 55.73387336730957, 57.193299531936646, 58.65272569656372, 60.119645833969116, 61.58656597137451, 63.03914141654968, 64.49171686172485, 65.94021677970886, 67.38871669769287, 68.85759043693542, 70.32646417617798, 71.77798986434937, 73.22951555252075, 74.66865086555481, 76.10778617858887, 77.569096326828, 79.03040647506714, 80.48267412185669, 81.93494176864624, 83.37270092964172, 84.8104600906372, 86.27535462379456, 87.7402491569519, 89.20752143859863, 90.67479372024536, 92.11222696304321, 93.54966020584106, 94.99542713165283, 96.4411940574646, 97.90875768661499, 99.37632131576538, 100.821932554245, 102.26754379272461, 103.71400594711304, 105.16046810150146, 106.61082482337952, 108.06118154525757, 109.50793981552124, 110.95469808578491, 112.3915843963623, 113.8284707069397, 115.2668571472168, 116.7052435874939, 118.16457915306091, 119.62391471862793, 121.08592486381531, 122.54793500900269, 123.99978756904602, 125.45164012908936, 126.90918922424316, 128.36673831939697, 129.8216531276703, 131.2765679359436, 132.71410942077637, 134.15165090560913, 135.6145257949829, 137.0774006843567, 138.53685307502747, 139.99630546569824, 141.43265342712402, 142.8690013885498, 144.2827868461609, 145.69657230377197, 147.09488773345947, 148.49320316314697, 149.882661819458, 151.27212047576904, 152.65668725967407, 154.0412540435791, 155.44039154052734, 156.8395290374756, 158.29618620872498, 159.75284337997437, 161.21089100837708, 162.66893863677979, 164.11861515045166, 165.56829166412354, 167.0333971977234, 168.49850273132324, 169.95341634750366, 171.40832996368408, 172.86880135536194, 174.3292727470398, 175.77096319198608, 177.21265363693237, 178.67003345489502, 180.12741327285767, 181.58443570137024, 183.0414581298828, 184.4869532585144, 185.932448387146, 187.26595187187195, 188.5994553565979, 189.85592126846313, 191.11238718032837, 192.36552262306213, 193.6186580657959, 194.87722253799438, 196.13578701019287, 197.40002155303955, 198.66425609588623, 199.9282741546631, 201.19229221343994, 202.43705296516418, 203.68181371688843, 204.94795989990234, 206.21410608291626, 207.4853048324585, 208.75650358200073, 210.03056263923645, 211.30462169647217, 212.56947135925293, 213.8343210220337, 215.3033356666565, 216.7723503112793, 218.23340487480164, 219.69445943832397, 221.15415024757385, 222.61384105682373, 224.0613350868225, 225.5088291168213, 226.95494771003723, 228.40106630325317, 229.8453586101532, 231.28965091705322, 232.73732447624207, 234.1849980354309, 235.63503432273865, 237.0850706100464, 238.53334259986877, 239.98161458969116, 241.44379711151123, 242.9059796333313, 244.35473942756653, 245.80349922180176, 247.06266117095947, 248.3218231201172, 249.56251859664917, 250.80321407318115, 252.07422852516174, 253.34524297714233, 254.61350846290588, 255.88177394866943, 257.13833451271057, 258.3948950767517, 259.6662709712982, 260.9376468658447, 262.20869731903076, 263.4797477722168, 264.7416481971741, 266.00354862213135, 267.2556064128876, 268.5076642036438, 269.77041697502136, 271.0331697463989, 272.28006649017334, 273.52696323394775, 274.77575278282166, 276.02454233169556, 277.29176020622253, 278.5589780807495, 279.83729553222656, 281.1156129837036, 283.22250175476074, 285.32939052581787]
[30.691666666666666, 30.691666666666666, 39.791666666666664, 39.791666666666664, 49.59166666666667, 49.59166666666667, 58.03333333333333, 58.03333333333333, 60.675, 60.675, 61.71666666666667, 61.71666666666667, 64.6, 64.6, 65.8, 65.8, 67.21666666666667, 67.21666666666667, 67.41666666666667, 67.41666666666667, 74.90833333333333, 74.90833333333333, 75.50833333333334, 75.50833333333334, 75.39166666666667, 75.39166666666667, 75.09166666666667, 75.09166666666667, 75.54166666666667, 75.54166666666667, 76.35833333333333, 76.35833333333333, 76.25833333333334, 76.25833333333334, 76.99166666666666, 76.99166666666666, 77.11666666666666, 77.11666666666666, 77.44166666666666, 77.44166666666666, 78.25, 78.25, 78.79166666666667, 78.79166666666667, 78.875, 78.875, 79.0, 79.0, 79.10833333333333, 79.10833333333333, 79.425, 79.425, 79.375, 79.375, 78.90833333333333, 78.90833333333333, 78.20833333333333, 78.20833333333333, 78.475, 78.475, 79.04166666666667, 79.04166666666667, 78.9, 78.9, 80.85, 80.85, 81.025, 81.025, 80.45, 80.45, 80.95, 80.95, 81.11666666666666, 81.11666666666666, 81.2, 81.2, 82.14166666666667, 82.14166666666667, 81.35, 81.35, 81.19166666666666, 81.19166666666666, 81.76666666666667, 81.76666666666667, 81.625, 81.625, 81.85, 81.85, 81.70833333333333, 81.70833333333333, 82.475, 82.475, 82.25, 82.25, 82.25, 82.25, 81.525, 81.525, 81.425, 81.425, 81.54166666666667, 81.54166666666667, 82.05833333333334, 82.05833333333334, 81.80833333333334, 81.80833333333334, 81.79166666666667, 81.79166666666667, 82.45, 82.45, 82.73333333333333, 82.73333333333333, 82.86666666666666, 82.86666666666666, 82.475, 82.475, 82.63333333333334, 82.63333333333334, 82.45833333333333, 82.45833333333333, 82.325, 82.325, 82.04166666666667, 82.04166666666667, 82.55, 82.55, 82.35, 82.35, 81.69166666666666, 81.69166666666666, 82.225, 82.225, 82.63333333333334, 82.63333333333334, 82.69166666666666, 82.69166666666666, 82.7, 82.7, 82.53333333333333, 82.53333333333333, 82.85, 82.85, 82.51666666666667, 82.51666666666667, 82.55833333333334, 82.55833333333334, 82.53333333333333, 82.53333333333333, 83.0, 83.0, 83.21666666666667, 83.21666666666667, 83.275, 83.275, 83.29166666666667, 83.29166666666667, 83.46666666666667, 83.46666666666667, 83.75, 83.75, 83.61666666666666, 83.61666666666666, 83.65833333333333, 83.65833333333333, 83.475, 83.475, 83.46666666666667, 83.46666666666667, 83.63333333333334, 83.63333333333334, 83.81666666666666, 83.81666666666666, 83.74166666666666, 83.74166666666666, 83.86666666666666, 83.86666666666666, 83.475, 83.475, 83.95, 83.95, 84.00833333333334, 84.00833333333334, 83.48333333333333, 83.48333333333333, 84.16666666666667, 84.16666666666667, 84.1, 84.1, 83.89166666666667, 83.89166666666667, 83.975, 83.975, 84.20833333333333, 84.20833333333333, 84.225, 84.225, 84.15833333333333, 84.15833333333333, 84.45, 84.45, 83.69166666666666, 83.69166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.200, Test loss: 0.370, Test accuracy: 86.42
Average accuracy final 10 rounds: 86.03083333333333 

1528.8333513736725
[1.634042501449585, 3.26808500289917, 4.606025695800781, 5.943966388702393, 7.28431510925293, 8.624663829803467, 9.98981523513794, 11.354966640472412, 12.715526819229126, 14.07608699798584, 15.425177812576294, 16.774268627166748, 18.1361985206604, 19.498128414154053, 20.8574857711792, 22.216843128204346, 23.574788808822632, 24.932734489440918, 26.28117847442627, 27.62962245941162, 28.979259252548218, 30.328896045684814, 31.68607997894287, 33.04326391220093, 34.40656304359436, 35.76986217498779, 37.1154408454895, 38.46101951599121, 39.80344486236572, 41.145870208740234, 42.50082850456238, 43.85578680038452, 45.21909832954407, 46.58240985870361, 47.9301598072052, 49.27790975570679, 50.610695362091064, 51.94348096847534, 53.29849410057068, 54.653507232666016, 56.01852989196777, 57.38355255126953, 58.73631954193115, 60.08908653259277, 61.436283349990845, 62.783480167388916, 64.13698482513428, 65.49048948287964, 66.83861637115479, 68.18674325942993, 69.56857085227966, 70.9503984451294, 72.29350018501282, 73.63660192489624, 74.98992419242859, 76.34324645996094, 77.71094274520874, 79.07863903045654, 80.42319679260254, 81.76775455474854, 83.11254954338074, 84.45734453201294, 85.81254148483276, 87.16773843765259, 88.52383255958557, 89.87992668151855, 91.27679228782654, 92.67365789413452, 94.02549743652344, 95.37733697891235, 96.7284369468689, 98.07953691482544, 99.44353938102722, 100.807541847229, 102.16812586784363, 103.52870988845825, 104.87474131584167, 106.2207727432251, 107.56538534164429, 108.90999794006348, 110.26346588134766, 111.61693382263184, 112.97252440452576, 114.32811498641968, 115.66446685791016, 117.00081872940063, 118.34287452697754, 119.68493032455444, 121.04099822044373, 122.39706611633301, 123.745041847229, 125.093017578125, 126.43776988983154, 127.78252220153809, 129.1270387172699, 130.4715552330017, 131.83054327964783, 133.18953132629395, 134.5516722202301, 135.91381311416626, 137.2642924785614, 138.61477184295654, 139.9638533592224, 141.31293487548828, 142.66490697860718, 144.01687908172607, 145.37904405593872, 146.74120903015137, 148.09680223464966, 149.45239543914795, 150.79741549491882, 152.1424355506897, 153.4856927394867, 154.8289499282837, 156.19374227523804, 157.55853462219238, 158.90693974494934, 160.2553448677063, 161.59597730636597, 162.93660974502563, 164.28308844566345, 165.62956714630127, 166.9774169921875, 168.32526683807373, 169.67298936843872, 171.0207118988037, 172.3570110797882, 173.6933102607727, 174.9513714313507, 176.2094326019287, 177.4647810459137, 178.72012948989868, 179.9551718235016, 181.1902141571045, 182.4184386730194, 183.64666318893433, 184.8719675540924, 186.0972719192505, 187.34235882759094, 188.5874457359314, 189.82559466362, 191.0637435913086, 192.29398894309998, 193.52423429489136, 194.73379588127136, 195.94335746765137, 197.17136430740356, 198.39937114715576, 199.64196968078613, 200.8845682144165, 202.12299489974976, 203.361421585083, 204.695702791214, 206.02998399734497, 207.24750232696533, 208.4650206565857, 209.71207118034363, 210.95912170410156, 212.1869523525238, 213.41478300094604, 214.86515545845032, 216.3155279159546, 217.63792252540588, 218.96031713485718, 220.23860788345337, 221.51689863204956, 222.79204392433167, 224.06718921661377, 225.31758308410645, 226.56797695159912, 227.8255832195282, 229.08318948745728, 230.3599398136139, 231.6366901397705, 232.92538022994995, 234.2140703201294, 235.507075548172, 236.8000807762146, 238.04958271980286, 239.2990846633911, 240.5623869895935, 241.8256893157959, 243.10971188545227, 244.39373445510864, 245.67140769958496, 246.94908094406128, 248.21589541435242, 249.48270988464355, 250.73350310325623, 251.9842963218689, 253.27138948440552, 254.55848264694214, 255.82443261146545, 257.09038257598877, 258.33683013916016, 259.58327770233154, 260.8516848087311, 262.1200919151306, 263.3862998485565, 264.6525077819824, 266.6574773788452, 268.662446975708]
[23.266666666666666, 23.266666666666666, 32.05833333333333, 32.05833333333333, 38.00833333333333, 38.00833333333333, 49.5, 49.5, 54.75833333333333, 54.75833333333333, 58.333333333333336, 58.333333333333336, 59.233333333333334, 59.233333333333334, 61.775, 61.775, 64.95833333333333, 64.95833333333333, 71.50833333333334, 71.50833333333334, 73.83333333333333, 73.83333333333333, 73.6, 73.6, 73.81666666666666, 73.81666666666666, 75.54166666666667, 75.54166666666667, 76.68333333333334, 76.68333333333334, 77.15, 77.15, 76.85833333333333, 76.85833333333333, 77.81666666666666, 77.81666666666666, 78.45, 78.45, 78.93333333333334, 78.93333333333334, 78.85, 78.85, 78.925, 78.925, 78.4, 78.4, 79.05833333333334, 79.05833333333334, 79.66666666666667, 79.66666666666667, 79.60833333333333, 79.60833333333333, 80.18333333333334, 80.18333333333334, 80.54166666666667, 80.54166666666667, 80.6, 80.6, 81.29166666666667, 81.29166666666667, 80.7, 80.7, 81.55, 81.55, 81.925, 81.925, 82.5, 82.5, 82.5, 82.5, 82.325, 82.325, 82.86666666666666, 82.86666666666666, 82.35833333333333, 82.35833333333333, 82.93333333333334, 82.93333333333334, 82.775, 82.775, 83.11666666666666, 83.11666666666666, 83.60833333333333, 83.60833333333333, 82.75, 82.75, 83.175, 83.175, 82.88333333333334, 82.88333333333334, 83.04166666666667, 83.04166666666667, 83.10833333333333, 83.10833333333333, 83.975, 83.975, 84.15833333333333, 84.15833333333333, 83.525, 83.525, 83.89166666666667, 83.89166666666667, 84.10833333333333, 84.10833333333333, 84.025, 84.025, 84.1, 84.1, 84.45, 84.45, 84.38333333333334, 84.38333333333334, 84.56666666666666, 84.56666666666666, 84.49166666666666, 84.49166666666666, 84.69166666666666, 84.69166666666666, 84.89166666666667, 84.89166666666667, 84.45, 84.45, 84.59166666666667, 84.59166666666667, 84.25833333333334, 84.25833333333334, 84.45833333333333, 84.45833333333333, 85.19166666666666, 85.19166666666666, 84.98333333333333, 84.98333333333333, 84.75833333333334, 84.75833333333334, 85.05, 85.05, 84.85833333333333, 84.85833333333333, 85.275, 85.275, 85.23333333333333, 85.23333333333333, 85.20833333333333, 85.20833333333333, 85.625, 85.625, 85.59166666666667, 85.59166666666667, 85.73333333333333, 85.73333333333333, 85.70833333333333, 85.70833333333333, 85.54166666666667, 85.54166666666667, 85.625, 85.625, 85.725, 85.725, 85.75, 85.75, 85.90833333333333, 85.90833333333333, 85.49166666666666, 85.49166666666666, 85.99166666666666, 85.99166666666666, 85.95, 85.95, 86.06666666666666, 86.06666666666666, 85.23333333333333, 85.23333333333333, 85.475, 85.475, 85.71666666666667, 85.71666666666667, 85.65833333333333, 85.65833333333333, 85.75, 85.75, 86.3, 86.3, 86.025, 86.025, 86.20833333333333, 86.20833333333333, 86.15, 86.15, 85.95833333333333, 85.95833333333333, 85.64166666666667, 85.64166666666667, 85.975, 85.975, 86.04166666666667, 86.04166666666667, 86.0, 86.0, 86.00833333333334, 86.00833333333334, 86.425, 86.425]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.134, Test loss: 0.415, Test accuracy: 86.66
Average accuracy final 10 rounds: 85.65916666666666 

1654.897988319397
[1.9619495868682861, 3.9238991737365723, 5.501491546630859, 7.0790839195251465, 8.633984804153442, 10.188885688781738, 11.745627164840698, 13.302368640899658, 14.871382236480713, 16.440395832061768, 18.022592544555664, 19.60478925704956, 21.150906085968018, 22.697022914886475, 24.247681617736816, 25.798340320587158, 27.373828887939453, 28.949317455291748, 30.52605891227722, 32.102800369262695, 33.658207654953, 35.21361494064331, 36.75472164154053, 38.295828342437744, 39.708006858825684, 41.12018537521362, 42.65701198577881, 44.193838596343994, 45.749757289886475, 47.305675983428955, 48.86955261230469, 50.43342924118042, 52.005359411239624, 53.57728958129883, 55.14506697654724, 56.712844371795654, 58.27519941329956, 59.83755445480347, 61.3999297618866, 62.96230506896973, 64.54418444633484, 66.12606382369995, 67.68281126022339, 69.23955869674683, 70.78399848937988, 72.32843828201294, 73.88320732116699, 75.43797636032104, 77.00997853279114, 78.58198070526123, 80.12616968154907, 81.67035865783691, 83.21597957611084, 84.76160049438477, 86.33906149864197, 87.91652250289917, 89.50264263153076, 91.08876276016235, 92.64608955383301, 94.20341634750366, 95.75911092758179, 97.31480550765991, 98.88970923423767, 100.46461296081543, 102.04212927818298, 103.61964559555054, 105.17538714408875, 106.73112869262695, 108.27784085273743, 109.8245530128479, 111.40858507156372, 112.99261713027954, 114.5656955242157, 116.13877391815186, 117.68852806091309, 119.23828220367432, 120.8044490814209, 122.37061595916748, 123.95553755760193, 125.54045915603638, 127.12361431121826, 128.70676946640015, 130.26054763793945, 131.81432580947876, 133.38111782073975, 134.94790983200073, 136.50900983810425, 138.07010984420776, 139.62976670265198, 141.1894235610962, 142.58206343650818, 143.97470331192017, 145.51656317710876, 147.05842304229736, 148.64587450027466, 150.23332595825195, 151.78723311424255, 153.34114027023315, 154.90415811538696, 156.46717596054077, 158.03546404838562, 159.60375213623047, 161.15236473083496, 162.70097732543945, 164.24825811386108, 165.79553890228271, 167.3512830734253, 168.90702724456787, 170.4943299293518, 172.08163261413574, 173.64989018440247, 175.2181477546692, 176.7145459651947, 178.21094417572021, 179.68922591209412, 181.16750764846802, 182.64631605148315, 184.1251244544983, 185.59973621368408, 187.07434797286987, 188.55252146720886, 190.03069496154785, 191.5185046195984, 193.00631427764893, 194.48087668418884, 195.95543909072876, 197.43915843963623, 198.9228777885437, 200.3989109992981, 201.8749442100525, 203.34929490089417, 204.82364559173584, 206.34390234947205, 207.86415910720825, 209.33416986465454, 210.80418062210083, 212.27100944519043, 213.73783826828003, 215.2110743522644, 216.68431043624878, 218.15338945388794, 219.6224684715271, 221.09276032447815, 222.5630521774292, 224.03419470787048, 225.50533723831177, 226.97952914237976, 228.45372104644775, 229.92863845825195, 231.40355587005615, 232.8798589706421, 234.35616207122803, 235.84135627746582, 237.3265504837036, 238.80192947387695, 240.2773084640503, 241.75316047668457, 243.22901248931885, 244.71103954315186, 246.19306659698486, 247.6678867340088, 249.14270687103271, 250.62134265899658, 252.09997844696045, 253.58717036247253, 255.07436227798462, 256.56038999557495, 258.0464177131653, 259.51590275764465, 260.985387802124, 262.47509765625, 263.964807510376, 265.4523708820343, 266.9399342536926, 268.4192190170288, 269.898503780365, 271.3731026649475, 272.84770154953003, 274.3170123100281, 275.7863230705261, 277.25852727890015, 278.73073148727417, 280.19955587387085, 281.66838026046753, 283.1409032344818, 284.6134262084961, 286.09124755859375, 287.5690689086914, 289.048223733902, 290.52737855911255, 291.9967019557953, 293.466025352478, 294.9510841369629, 296.43614292144775, 297.92204427719116, 299.40794563293457, 300.8907964229584, 302.3736472129822, 303.85260248184204, 305.3315577507019, 307.3521490097046, 309.3727402687073]
[13.775, 13.775, 28.658333333333335, 28.658333333333335, 32.891666666666666, 32.891666666666666, 41.733333333333334, 41.733333333333334, 49.94166666666667, 49.94166666666667, 54.68333333333333, 54.68333333333333, 58.99166666666667, 58.99166666666667, 63.05833333333333, 63.05833333333333, 66.975, 66.975, 68.99166666666666, 68.99166666666666, 69.4, 69.4, 74.025, 74.025, 75.46666666666667, 75.46666666666667, 75.80833333333334, 75.80833333333334, 77.84166666666667, 77.84166666666667, 77.64166666666667, 77.64166666666667, 78.59166666666667, 78.59166666666667, 79.15833333333333, 79.15833333333333, 78.24166666666666, 78.24166666666666, 79.18333333333334, 79.18333333333334, 79.34166666666667, 79.34166666666667, 79.99166666666666, 79.99166666666666, 80.90833333333333, 80.90833333333333, 81.04166666666667, 81.04166666666667, 80.48333333333333, 80.48333333333333, 81.6, 81.6, 81.475, 81.475, 81.98333333333333, 81.98333333333333, 81.35, 81.35, 81.9, 81.9, 82.94166666666666, 82.94166666666666, 82.74166666666666, 82.74166666666666, 82.14166666666667, 82.14166666666667, 81.9, 81.9, 83.25833333333334, 83.25833333333334, 83.36666666666666, 83.36666666666666, 82.725, 82.725, 83.2, 83.2, 82.775, 82.775, 83.35833333333333, 83.35833333333333, 83.675, 83.675, 83.39166666666667, 83.39166666666667, 83.65, 83.65, 83.59166666666667, 83.59166666666667, 84.38333333333334, 84.38333333333334, 84.35833333333333, 84.35833333333333, 84.69166666666666, 84.69166666666666, 84.175, 84.175, 84.54166666666667, 84.54166666666667, 84.46666666666667, 84.46666666666667, 84.34166666666667, 84.34166666666667, 84.65, 84.65, 84.28333333333333, 84.28333333333333, 84.85, 84.85, 84.69166666666666, 84.69166666666666, 84.65833333333333, 84.65833333333333, 84.7, 84.7, 84.84166666666667, 84.84166666666667, 84.24166666666666, 84.24166666666666, 85.09166666666667, 85.09166666666667, 84.78333333333333, 84.78333333333333, 84.99166666666666, 84.99166666666666, 85.39166666666667, 85.39166666666667, 84.86666666666666, 84.86666666666666, 85.5, 85.5, 85.38333333333334, 85.38333333333334, 85.19166666666666, 85.19166666666666, 85.21666666666667, 85.21666666666667, 85.28333333333333, 85.28333333333333, 85.16666666666667, 85.16666666666667, 85.575, 85.575, 85.13333333333334, 85.13333333333334, 85.45833333333333, 85.45833333333333, 85.64166666666667, 85.64166666666667, 85.975, 85.975, 85.825, 85.825, 85.51666666666667, 85.51666666666667, 85.175, 85.175, 85.35, 85.35, 85.875, 85.875, 85.73333333333333, 85.73333333333333, 85.99166666666666, 85.99166666666666, 85.3, 85.3, 86.0, 86.0, 85.95, 85.95, 85.44166666666666, 85.44166666666666, 85.575, 85.575, 85.66666666666667, 85.66666666666667, 85.73333333333333, 85.73333333333333, 85.725, 85.725, 85.54166666666667, 85.54166666666667, 85.95, 85.95, 85.68333333333334, 85.68333333333334, 85.64166666666667, 85.64166666666667, 85.50833333333334, 85.50833333333334, 85.94166666666666, 85.94166666666666, 85.5, 85.5, 85.89166666666667, 85.89166666666667, 85.25833333333334, 85.25833333333334, 85.675, 85.675, 86.65833333333333, 86.65833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.044, Test loss: 0.947, Test accuracy: 82.42
Average accuracy final 10 rounds: 81.61999999999999 

1579.6279578208923
[1.698671579360962, 3.397343158721924, 4.9346702098846436, 6.471997261047363, 7.97927451133728, 9.486551761627197, 10.99276876449585, 12.498985767364502, 13.997092723846436, 15.49519968032837, 16.988056898117065, 18.48091411590576, 19.974703311920166, 21.46849250793457, 22.968594551086426, 24.46869659423828, 25.959442853927612, 27.450189113616943, 28.973374605178833, 30.496560096740723, 32.01310992240906, 33.52965974807739, 35.03617238998413, 36.54268503189087, 38.04706072807312, 39.55143642425537, 41.058332204818726, 42.56522798538208, 44.064844608306885, 45.56446123123169, 47.06060481071472, 48.556748390197754, 50.0634663105011, 51.57018423080444, 53.08130216598511, 54.59242010116577, 56.0949764251709, 57.597532749176025, 59.09990382194519, 60.602274894714355, 62.10272765159607, 63.60318040847778, 65.10913324356079, 66.6150860786438, 68.12749075889587, 69.63989543914795, 71.1450846195221, 72.65027379989624, 74.15191555023193, 75.65355730056763, 77.15762090682983, 78.66168451309204, 80.17134857177734, 81.68101263046265, 83.18257236480713, 84.68413209915161, 86.16742897033691, 87.65072584152222, 89.15975332260132, 90.66878080368042, 92.17258667945862, 93.67639255523682, 95.17870283126831, 96.6810131072998, 98.18119525909424, 99.68137741088867, 101.17065477371216, 102.65993213653564, 104.16892695426941, 105.67792177200317, 107.18723130226135, 108.69654083251953, 110.01992774009705, 111.34331464767456, 112.66328954696655, 113.98326444625854, 115.31247425079346, 116.64168405532837, 117.95691275596619, 119.272141456604, 120.59875893592834, 121.92537641525269, 123.23420548439026, 124.54303455352783, 125.86514091491699, 127.18724727630615, 128.51161575317383, 129.8359842300415, 131.1619417667389, 132.48789930343628, 133.80172753334045, 135.11555576324463, 136.4241590499878, 137.73276233673096, 139.03719854354858, 140.3416347503662, 141.65451526641846, 142.9673957824707, 144.28629541397095, 145.6051950454712, 146.93223524093628, 148.25927543640137, 149.59273195266724, 150.9261884689331, 152.253511428833, 153.5808343887329, 154.90363836288452, 156.22644233703613, 157.5554656982422, 158.88448905944824, 160.1970932483673, 161.50969743728638, 162.83189940452576, 164.15410137176514, 165.4822497367859, 166.81039810180664, 168.13644790649414, 169.46249771118164, 170.78858423233032, 172.114670753479, 173.41912651062012, 174.72358226776123, 176.10683250427246, 177.4900827407837, 178.81945037841797, 180.14881801605225, 181.47065782546997, 182.7924976348877, 184.11881160736084, 185.44512557983398, 186.81355595588684, 188.1819863319397, 189.5419840812683, 190.90198183059692, 192.2139813899994, 193.52598094940186, 194.8792314529419, 196.23248195648193, 197.5559799671173, 198.87947797775269, 200.2108278274536, 201.54217767715454, 202.85899114608765, 204.17580461502075, 205.48721551895142, 206.79862642288208, 208.11918377876282, 209.43974113464355, 210.7635452747345, 212.08734941482544, 213.40964818000793, 214.73194694519043, 216.04485392570496, 217.35776090621948, 218.66536045074463, 219.97295999526978, 221.29218006134033, 222.6114001274109, 223.92902088165283, 225.24664163589478, 226.55242657661438, 227.85821151733398, 229.16693449020386, 230.47565746307373, 231.79723691940308, 233.11881637573242, 234.43941235542297, 235.76000833511353, 237.07591581344604, 238.39182329177856, 239.71608543395996, 241.04034757614136, 242.36285519599915, 243.68536281585693, 245.00169563293457, 246.3180284500122, 247.64107060432434, 248.96411275863647, 250.28545093536377, 251.60678911209106, 252.92121839523315, 254.23564767837524, 255.56115412712097, 256.8866605758667, 258.20014572143555, 259.5136308670044, 260.83340096473694, 262.1531710624695, 263.4729104042053, 264.79264974594116, 266.1111285686493, 267.4296073913574, 268.7536642551422, 270.077721118927, 271.39139771461487, 272.70507431030273, 274.0294985771179, 275.3539228439331, 276.6715738773346, 277.9892249107361, 280.1239244937897, 282.25862407684326]
[16.575, 16.575, 30.841666666666665, 30.841666666666665, 36.916666666666664, 36.916666666666664, 40.71666666666667, 40.71666666666667, 60.18333333333333, 60.18333333333333, 60.733333333333334, 60.733333333333334, 58.391666666666666, 58.391666666666666, 65.66666666666667, 65.66666666666667, 70.61666666666666, 70.61666666666666, 70.55, 70.55, 71.04166666666667, 71.04166666666667, 71.975, 71.975, 72.61666666666666, 72.61666666666666, 73.11666666666666, 73.11666666666666, 73.94166666666666, 73.94166666666666, 74.60833333333333, 74.60833333333333, 74.70833333333333, 74.70833333333333, 75.69166666666666, 75.69166666666666, 76.525, 76.525, 76.51666666666667, 76.51666666666667, 76.775, 76.775, 76.975, 76.975, 76.76666666666667, 76.76666666666667, 77.24166666666666, 77.24166666666666, 78.03333333333333, 78.03333333333333, 77.90833333333333, 77.90833333333333, 77.875, 77.875, 79.21666666666667, 79.21666666666667, 79.39166666666667, 79.39166666666667, 79.53333333333333, 79.53333333333333, 79.50833333333334, 79.50833333333334, 79.2, 79.2, 79.70833333333333, 79.70833333333333, 79.40833333333333, 79.40833333333333, 79.6, 79.6, 79.6, 79.6, 79.975, 79.975, 80.19166666666666, 80.19166666666666, 79.89166666666667, 79.89166666666667, 80.00833333333334, 80.00833333333334, 80.28333333333333, 80.28333333333333, 80.2, 80.2, 80.16666666666667, 80.16666666666667, 80.48333333333333, 80.48333333333333, 80.91666666666667, 80.91666666666667, 81.25833333333334, 81.25833333333334, 81.64166666666667, 81.64166666666667, 81.55833333333334, 81.55833333333334, 81.125, 81.125, 81.25, 81.25, 81.2, 81.2, 81.14166666666667, 81.14166666666667, 81.275, 81.275, 80.925, 80.925, 80.89166666666667, 80.89166666666667, 80.675, 80.675, 81.01666666666667, 81.01666666666667, 81.325, 81.325, 81.0, 81.0, 81.09166666666667, 81.09166666666667, 80.925, 80.925, 80.98333333333333, 80.98333333333333, 81.33333333333333, 81.33333333333333, 81.575, 81.575, 81.56666666666666, 81.56666666666666, 81.525, 81.525, 81.48333333333333, 81.48333333333333, 81.54166666666667, 81.54166666666667, 81.50833333333334, 81.50833333333334, 81.74166666666666, 81.74166666666666, 81.54166666666667, 81.54166666666667, 81.075, 81.075, 81.05833333333334, 81.05833333333334, 81.16666666666667, 81.16666666666667, 80.65, 80.65, 80.225, 80.225, 81.00833333333334, 81.00833333333334, 80.89166666666667, 80.89166666666667, 80.88333333333334, 80.88333333333334, 81.00833333333334, 81.00833333333334, 81.78333333333333, 81.78333333333333, 81.75833333333334, 81.75833333333334, 81.71666666666667, 81.71666666666667, 81.65833333333333, 81.65833333333333, 81.75, 81.75, 81.775, 81.775, 81.3, 81.3, 81.025, 81.025, 81.35, 81.35, 81.65, 81.65, 81.80833333333334, 81.80833333333334, 81.775, 81.775, 81.525, 81.525, 81.575, 81.575, 81.61666666666666, 81.61666666666666, 81.56666666666666, 81.56666666666666, 81.65833333333333, 81.65833333333333, 81.69166666666666, 81.69166666666666, 81.63333333333334, 81.63333333333334, 81.35, 81.35, 82.425, 82.425]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Final Round, Train loss: 0.073, Test loss: 0.788, Test accuracy: 66.58
Average accuracy final 10 rounds: 66.91166666666666
1893.5923430919647
[]
[16.016666666666666, 25.925, 29.208333333333332, 38.825, 40.891666666666666, 44.0, 47.36666666666667, 51.108333333333334, 55.233333333333334, 55.84166666666667, 55.983333333333334, 56.75833333333333, 57.175, 58.45, 58.525, 60.3, 60.61666666666667, 60.25, 59.483333333333334, 61.78333333333333, 62.358333333333334, 61.791666666666664, 62.05, 62.19166666666667, 62.425, 62.45, 63.625, 62.35, 63.05, 63.15833333333333, 63.84166666666667, 63.208333333333336, 64.56666666666666, 64.56666666666666, 65.33333333333333, 66.43333333333334, 65.03333333333333, 65.25, 65.31666666666666, 65.625, 64.10833333333333, 63.975, 64.30833333333334, 63.483333333333334, 64.19166666666666, 65.0, 65.475, 64.60833333333333, 66.00833333333334, 67.35, 66.3, 65.04166666666667, 64.43333333333334, 64.45, 65.34166666666667, 67.00833333333334, 66.94166666666666, 67.36666666666666, 67.075, 67.68333333333334, 67.075, 66.79166666666667, 67.15, 67.55833333333334, 68.86666666666666, 69.26666666666667, 69.36666666666666, 68.66666666666667, 67.9, 66.78333333333333, 67.86666666666666, 67.925, 67.875, 69.39166666666667, 68.86666666666666, 68.04166666666667, 68.28333333333333, 69.45833333333333, 69.34166666666667, 69.125, 69.25833333333334, 68.90833333333333, 67.66666666666667, 66.65833333333333, 65.18333333333334, 65.725, 65.74166666666666, 65.0, 65.53333333333333, 66.03333333333333, 66.53333333333333, 67.075, 67.0, 67.525, 67.5, 66.83333333333333, 65.95833333333333, 67.14166666666667, 66.71666666666667, 66.83333333333333, 66.575]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.615, Test loss: 0.561, Test accuracy: 77.20
Average accuracy final 10 rounds: 77.2725
Average global accuracy final 10 rounds: 77.2725
1415.8955476284027
[]
[25.975, 33.93333333333333, 39.541666666666664, 47.125, 59.916666666666664, 60.06666666666667, 62.291666666666664, 60.425, 62.6, 62.8, 62.15, 63.95, 65.025, 66.33333333333333, 67.18333333333334, 67.475, 68.35, 68.0, 68.81666666666666, 70.39166666666667, 70.85, 71.025, 71.0, 71.1, 71.80833333333334, 72.04166666666667, 72.60833333333333, 72.775, 72.81666666666666, 72.3, 72.775, 73.06666666666666, 73.51666666666667, 73.45, 73.64166666666667, 74.20833333333333, 73.91666666666667, 73.90833333333333, 74.14166666666667, 74.26666666666667, 74.21666666666667, 73.86666666666666, 74.275, 74.30833333333334, 73.94166666666666, 75.15833333333333, 75.29166666666667, 74.25833333333334, 74.53333333333333, 75.06666666666666, 74.58333333333333, 74.3, 74.84166666666667, 74.43333333333334, 74.65, 75.8, 76.2, 76.14166666666667, 76.18333333333334, 75.275, 75.94166666666666, 75.56666666666666, 76.375, 76.21666666666667, 76.44166666666666, 76.20833333333333, 76.76666666666667, 76.60833333333333, 76.40833333333333, 76.35833333333333, 76.45, 76.175, 76.03333333333333, 75.8, 76.15, 75.975, 76.26666666666667, 76.325, 76.25833333333334, 76.30833333333334, 76.425, 76.2, 76.175, 75.875, 75.625, 75.9, 76.86666666666666, 76.975, 76.875, 76.80833333333334, 77.2, 77.63333333333334, 77.48333333333333, 77.58333333333333, 77.75833333333334, 77.40833333333333, 76.64166666666667, 77.0, 76.775, 77.24166666666666, 77.2]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.77
Final Round, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.52
Average accuracy final 10 rounds: 10.64725 

Average global accuracy final 10 rounds: 10.515750000000002 

5103.164774894714
[5.595698833465576, 10.904415130615234, 16.270564794540405, 21.633708477020264, 26.923089027404785, 32.28213024139404, 37.644813537597656, 43.02292013168335, 48.36277508735657, 53.04651165008545, 57.68609356880188, 62.318777084350586, 66.96153283119202, 71.64692616462708, 76.31996870040894, 80.98509693145752, 85.70097494125366, 90.38658046722412, 95.12540030479431, 99.8864517211914, 104.61517667770386, 109.23854231834412, 113.96099805831909, 118.68950176239014, 123.31507587432861, 127.98411726951599, 132.65900683403015, 137.28365111351013, 141.89605593681335, 146.52810311317444, 151.18856644630432, 155.8392903804779, 160.4958519935608, 165.10301089286804, 169.71344327926636, 174.34666848182678, 179.01277327537537, 183.6475384235382, 188.25551629066467, 192.84173727035522, 197.43730282783508, 202.02936553955078, 206.65352869033813, 211.28164148330688, 215.8895239830017, 220.4972870349884, 225.10782408714294, 229.73842859268188, 234.37314677238464, 238.9632658958435, 243.5468053817749, 248.1335699558258, 252.7252037525177, 257.32347798347473, 261.92975783348083, 266.54738545417786, 271.1577422618866, 275.7616868019104, 280.36405539512634, 284.98314929008484, 289.5953688621521, 294.1866829395294, 298.79348826408386, 303.3923935890198, 307.9866497516632, 312.5862138271332, 317.1928861141205, 321.82333278656006, 326.44106221199036, 331.06822967529297, 335.6755037307739, 340.29720544815063, 344.9300057888031, 349.53870153427124, 354.172367811203, 358.76132917404175, 363.37277388572693, 367.99259185791016, 372.6066343784332, 377.2234842777252, 381.8467333316803, 386.4715361595154, 391.1021258831024, 395.72643303871155, 400.37580704689026, 405.0260922908783, 409.66056632995605, 414.2595045566559, 418.8688578605652, 423.4729678630829, 428.06330013275146, 432.6864788532257, 437.3070333003998, 441.92649030685425, 446.5576186180115, 451.18684339523315, 455.8231873512268, 460.470591545105, 465.1197702884674, 469.7231249809265, 472.03471994400024]
[10.015, 10.05, 10.01, 10.015, 10.0525, 10.12, 10.09, 10.06, 10.075, 10.0725, 10.0925, 10.075, 10.11, 10.1075, 10.0575, 10.0575, 10.085, 10.0625, 10.0425, 10.095, 10.13, 10.145, 10.1025, 10.08, 10.115, 10.125, 10.0925, 9.98, 10.1275, 10.165, 10.0875, 10.2125, 10.22, 10.285, 10.3075, 10.3825, 10.3675, 10.33, 10.22, 10.3175, 10.3925, 10.4525, 10.475, 10.3825, 10.415, 10.615, 10.635, 10.495, 10.48, 10.5025, 10.4375, 10.525, 10.6075, 10.47, 10.56, 10.49, 10.555, 10.6325, 10.605, 10.5275, 10.74, 10.6175, 10.75, 10.8675, 10.78, 10.6425, 10.585, 10.7275, 10.7025, 10.6825, 10.6525, 10.5225, 10.51, 10.61, 10.645, 10.625, 10.665, 10.63, 10.6025, 10.655, 10.6475, 10.75, 10.7625, 10.88, 10.875, 10.9025, 10.645, 10.62, 10.4975, 10.57, 10.605, 10.555, 10.7875, 10.77, 10.6825, 10.48, 10.5275, 10.5025, 10.715, 10.8475, 10.765]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.422, Test loss: 2.463, Test accuracy: 17.27
Average accuracy final 10 rounds: 14.710749999999999
8124.5711488723755
[12.635523557662964, 25.20308756828308, 37.75546741485596, 50.27739453315735, 62.806732416152954, 75.35084176063538, 86.60041952133179, 97.84343695640564, 109.21174240112305, 121.70275115966797, 134.2412874698639, 146.71025156974792, 159.1464557647705, 171.66247630119324, 184.0736494064331, 196.59522366523743, 209.23449110984802, 221.04278349876404, 233.60367035865784, 244.70985412597656, 255.92326259613037, 267.1558825969696, 278.40305972099304, 289.6268525123596, 300.88584303855896, 312.20214653015137, 323.3599615097046, 334.6995840072632, 345.9617409706116, 357.3177888393402, 369.75452065467834, 382.27533292770386, 394.7650799751282, 407.12554454803467, 418.4369795322418, 430.94389390945435, 442.16550493240356, 453.6536548137665, 465.0167021751404, 476.2178134918213, 487.4815266132355, 498.85613489151, 510.1042432785034, 521.3757259845734, 533.7234942913055, 544.9566552639008, 556.1300497055054, 568.7645001411438, 581.3962621688843, 594.145592212677, 606.738128900528, 619.3233952522278, 631.9804935455322, 644.6281027793884, 657.302116394043, 669.913666009903, 682.5549728870392, 695.2059977054596, 707.830593585968, 720.4975528717041, 733.1190528869629, 745.6496176719666, 758.0617311000824, 770.4337010383606, 782.9519503116608, 795.4360632896423, 806.8909471035004, 818.1799354553223, 829.4628720283508, 840.7892799377441, 852.1216921806335, 863.339953660965, 874.6310021877289, 885.8552451133728, 897.0944082736969, 908.4792070388794, 919.8232936859131, 931.1358168125153, 942.3257040977478, 953.5377368927002, 964.8105401992798, 976.1849336624146, 987.3004457950592, 998.5094339847565, 1009.6997649669647, 1020.843759059906, 1032.0475206375122, 1043.0984723567963, 1054.263510942459, 1065.3593327999115, 1076.4705884456635, 1087.748919725418, 1098.8907580375671, 1110.0336813926697, 1121.274062871933, 1132.475422859192, 1143.7909543514252, 1154.9373078346252, 1166.2086329460144, 1177.4031326770782, 1180.2679734230042]
[11.6775, 12.7625, 13.35, 12.8225, 12.8575, 13.325, 14.02, 13.1825, 14.395, 14.1575, 14.2275, 14.09, 14.39, 14.265, 13.785, 14.2875, 15.46, 16.0175, 15.13, 14.45, 13.3275, 14.2775, 13.0375, 14.1325, 14.7625, 13.815, 15.0575, 14.865, 16.025, 14.185, 14.525, 15.0575, 13.945, 15.1675, 14.625, 15.865, 12.5725, 15.905, 15.7375, 13.99, 15.46, 15.45, 15.175, 14.9925, 13.765, 14.54, 15.4525, 15.33, 14.9, 14.6525, 13.595, 15.225, 16.78, 14.725, 16.2375, 12.695, 14.6625, 14.0575, 15.0475, 14.6125, 16.32, 13.57, 13.9825, 15.6875, 16.7525, 15.0325, 16.235, 15.185, 14.355, 15.74, 14.05, 13.6725, 16.4675, 15.37, 14.49, 14.9325, 14.2575, 15.7725, 14.015, 14.8525, 15.99, 15.7025, 15.0325, 16.8375, 12.965, 14.87, 14.8075, 15.895, 15.765, 15.115, 14.1625, 15.3375, 15.25, 13.905, 15.1075, 16.0225, 14.9675, 14.7025, 13.93, 13.7225, 17.2725]
python: can't open file 'main_fedpac_k.py': [Errno 2] No such file or directory
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.406, Test loss: 0.714, Test accuracy: 77.18
Average accuracy final 10 rounds: 76.74800000000002
5743.036155462265
[6.1608264446258545, 12.321652889251709, 18.037980556488037, 23.754308223724365, 29.521003484725952, 35.28769874572754, 41.075003147125244, 46.86230754852295, 52.632609605789185, 58.40291166305542, 64.06246519088745, 69.72201871871948, 75.35609102249146, 80.99016332626343, 86.68643069267273, 92.38269805908203, 98.06855654716492, 103.7544150352478, 109.46927118301392, 115.18412733078003, 120.90778946876526, 126.63145160675049, 132.3285892009735, 138.02572679519653, 143.79491996765137, 149.5641131401062, 155.19062042236328, 160.81712770462036, 166.55439233779907, 172.29165697097778, 178.0429949760437, 183.79433298110962, 189.564466714859, 195.3346004486084, 201.06556105613708, 206.79652166366577, 212.38144063949585, 217.96635961532593, 223.6372890472412, 229.3082184791565, 234.89785432815552, 240.48749017715454, 246.24975085258484, 252.01201152801514, 257.6896560192108, 263.3673005104065, 268.9736614227295, 274.5800223350525, 280.34882521629333, 286.1176280975342, 291.8494565486908, 297.5812849998474, 303.2688248157501, 308.95636463165283, 314.6580493450165, 320.3597340583801, 326.0852723121643, 331.8108105659485, 337.5580151081085, 343.30521965026855, 349.0343282222748, 354.763436794281, 360.3747878074646, 365.9861388206482, 371.73054003715515, 377.4749412536621, 383.2427225112915, 389.0105037689209, 394.90096497535706, 400.7914261817932, 406.63347578048706, 412.4755253791809, 418.31060695648193, 424.14568853378296, 429.93417096138, 435.72265338897705, 441.5136568546295, 447.304660320282, 453.1929395198822, 459.0812187194824, 464.8996124267578, 470.7180061340332, 476.5932893753052, 482.46857261657715, 488.2576560974121, 494.04673957824707, 499.8417458534241, 505.6367521286011, 511.4190514087677, 517.2013506889343, 522.9315042495728, 528.6616578102112, 534.4337768554688, 540.2058959007263, 545.9719240665436, 551.7379522323608, 557.0303838253021, 562.3228154182434, 567.8766481876373, 573.4304809570312, 579.0140135288239, 584.5975461006165, 590.2965672016144, 595.9955883026123, 601.6497390270233, 607.3038897514343, 612.8988978862762, 618.4939060211182, 624.250349521637, 630.0067930221558, 635.671884059906, 641.3369750976562, 647.0481691360474, 652.7593631744385, 658.5035321712494, 664.2477011680603, 669.9582197666168, 675.6687383651733, 681.413923740387, 687.1591091156006, 692.9719576835632, 698.7848062515259, 704.4368214607239, 710.0888366699219, 715.6567180156708, 721.2245993614197, 726.864461183548, 732.5043230056763, 738.336124420166, 744.1679258346558, 750.0215456485748, 755.8751654624939, 761.7079784870148, 767.5407915115356, 773.394280910492, 779.2477703094482, 784.9573411941528, 790.6669120788574, 796.6314196586609, 802.5959272384644, 808.2779493331909, 813.9599714279175, 819.827223777771, 825.6944761276245, 831.3856081962585, 837.0767402648926, 842.8131504058838, 848.549560546875, 854.2586102485657, 859.9676599502563, 865.6614217758179, 871.3551836013794, 877.2925109863281, 883.2298383712769, 889.1889252662659, 895.1480121612549, 900.9883313179016, 906.8286504745483, 912.6827387809753, 918.5368270874023, 924.5466551780701, 930.5564832687378, 936.389524936676, 942.2225666046143, 948.2659783363342, 954.3093900680542, 960.2526803016663, 966.1959705352783, 972.100029706955, 978.0040888786316, 983.9308731555939, 989.8576574325562, 995.8673739433289, 1001.8770904541016, 1007.9700524806976, 1014.0630145072937, 1019.807776927948, 1025.5525393486023, 1031.5231187343597, 1037.4936981201172, 1043.7585406303406, 1050.023383140564, 1056.2169518470764, 1062.4105205535889, 1068.6114733219147, 1074.8124260902405, 1081.191656589508, 1087.5708870887756, 1093.91037774086, 1100.2498683929443, 1106.2894129753113, 1112.3289575576782, 1118.5692522525787, 1124.8095469474792, 1130.9969592094421, 1137.184371471405, 1143.357561826706, 1149.5307521820068, 1155.7063417434692, 1161.8819313049316, 1164.1748106479645, 1166.4676899909973]
[11.6775, 11.6775, 17.77, 17.77, 25.8975, 25.8975, 30.73, 30.73, 38.34, 38.34, 43.875, 43.875, 45.1725, 45.1725, 49.175, 49.175, 52.945, 52.945, 53.32, 53.32, 54.5825, 54.5825, 55.4025, 55.4025, 57.4, 57.4, 61.9325, 61.9325, 62.6375, 62.6375, 63.875, 63.875, 64.2225, 64.2225, 64.645, 64.645, 68.7825, 68.7825, 68.97, 68.97, 69.645, 69.645, 69.795, 69.795, 69.9725, 69.9725, 71.2925, 71.2925, 71.15, 71.15, 71.8025, 71.8025, 72.165, 72.165, 72.2125, 72.2125, 72.6825, 72.6825, 72.97, 72.97, 72.76, 72.76, 72.965, 72.965, 73.515, 73.515, 73.47, 73.47, 73.8425, 73.8425, 73.895, 73.895, 73.5975, 73.5975, 73.705, 73.705, 74.1425, 74.1425, 74.3125, 74.3125, 74.67, 74.67, 74.8, 74.8, 74.4675, 74.4675, 75.11, 75.11, 74.7325, 74.7325, 74.795, 74.795, 75.2, 75.2, 74.8125, 74.8125, 75.2825, 75.2825, 75.2625, 75.2625, 74.965, 74.965, 75.2975, 75.2975, 75.6775, 75.6775, 75.47, 75.47, 75.7275, 75.7275, 75.5775, 75.5775, 75.6525, 75.6525, 75.88, 75.88, 75.3975, 75.3975, 75.36, 75.36, 75.2975, 75.2975, 76.07, 76.07, 75.8425, 75.8425, 76.4425, 76.4425, 76.375, 76.375, 76.1275, 76.1275, 75.86, 75.86, 75.8725, 75.8725, 75.88, 75.88, 76.5725, 76.5725, 76.415, 76.415, 76.5025, 76.5025, 76.4225, 76.4225, 76.3925, 76.3925, 76.0625, 76.0625, 76.0075, 76.0075, 76.365, 76.365, 76.3975, 76.3975, 76.5675, 76.5675, 76.495, 76.495, 76.3825, 76.3825, 76.3675, 76.3675, 76.85, 76.85, 76.7, 76.7, 76.6725, 76.6725, 76.7125, 76.7125, 76.56, 76.56, 76.04, 76.04, 76.7075, 76.7075, 76.57, 76.57, 76.9925, 76.9925, 76.9325, 76.9325, 76.6925, 76.6925, 76.45, 76.45, 76.575, 76.575, 76.7, 76.7, 76.555, 76.555, 76.385, 76.385, 77.1625, 77.1625, 77.035, 77.035, 77.1775, 77.1775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.293, Test loss: 2.265, Test accuracy: 44.15
Round   0, Global train loss: 2.293, Global test loss: 2.265, Global test accuracy: 45.60
Round   1, Train loss: 1.990, Test loss: 1.889, Test accuracy: 65.02
Round   1, Global train loss: 1.990, Global test loss: 1.767, Global test accuracy: 72.48
Round   2, Train loss: 1.837, Test loss: 1.783, Test accuracy: 73.18
Round   2, Global train loss: 1.837, Global test loss: 1.717, Global test accuracy: 76.61
Round   3, Train loss: 1.663, Test loss: 1.733, Test accuracy: 76.94
Round   3, Global train loss: 1.663, Global test loss: 1.650, Global test accuracy: 82.37
Round   4, Train loss: 1.610, Test loss: 1.707, Test accuracy: 79.31
Round   4, Global train loss: 1.610, Global test loss: 1.611, Global test accuracy: 85.94
Round   5, Train loss: 1.678, Test loss: 1.678, Test accuracy: 80.67
Round   5, Global train loss: 1.678, Global test loss: 1.650, Global test accuracy: 82.14
Round   6, Train loss: 1.596, Test loss: 1.669, Test accuracy: 81.50
Round   6, Global train loss: 1.596, Global test loss: 1.628, Global test accuracy: 83.19
Round   7, Train loss: 1.586, Test loss: 1.660, Test accuracy: 82.22
Round   7, Global train loss: 1.586, Global test loss: 1.629, Global test accuracy: 83.30
Round   8, Train loss: 1.627, Test loss: 1.632, Test accuracy: 83.94
Round   8, Global train loss: 1.627, Global test loss: 1.624, Global test accuracy: 84.23
Round   9, Train loss: 1.570, Test loss: 1.626, Test accuracy: 84.42
Round   9, Global train loss: 1.570, Global test loss: 1.598, Global test accuracy: 87.31
Round  10, Train loss: 1.586, Test loss: 1.621, Test accuracy: 84.66
Round  10, Global train loss: 1.586, Global test loss: 1.630, Global test accuracy: 83.50
Round  11, Train loss: 1.550, Test loss: 1.621, Test accuracy: 84.64
Round  11, Global train loss: 1.550, Global test loss: 1.623, Global test accuracy: 83.75
Round  12, Train loss: 1.535, Test loss: 1.616, Test accuracy: 85.09
Round  12, Global train loss: 1.535, Global test loss: 1.573, Global test accuracy: 89.37
Round  13, Train loss: 1.569, Test loss: 1.613, Test accuracy: 85.38
Round  13, Global train loss: 1.569, Global test loss: 1.629, Global test accuracy: 83.33
Round  14, Train loss: 1.535, Test loss: 1.611, Test accuracy: 85.54
Round  14, Global train loss: 1.535, Global test loss: 1.588, Global test accuracy: 88.09
Round  15, Train loss: 1.532, Test loss: 1.600, Test accuracy: 86.72
Round  15, Global train loss: 1.532, Global test loss: 1.561, Global test accuracy: 90.80
Round  16, Train loss: 1.569, Test loss: 1.595, Test accuracy: 87.16
Round  16, Global train loss: 1.569, Global test loss: 1.629, Global test accuracy: 83.55
Round  17, Train loss: 1.535, Test loss: 1.593, Test accuracy: 87.34
Round  17, Global train loss: 1.535, Global test loss: 1.588, Global test accuracy: 87.88
Round  18, Train loss: 1.523, Test loss: 1.590, Test accuracy: 87.67
Round  18, Global train loss: 1.523, Global test loss: 1.569, Global test accuracy: 89.81
Round  19, Train loss: 1.521, Test loss: 1.586, Test accuracy: 88.09
Round  19, Global train loss: 1.521, Global test loss: 1.575, Global test accuracy: 89.16
Round  20, Train loss: 1.527, Test loss: 1.586, Test accuracy: 88.05
Round  20, Global train loss: 1.527, Global test loss: 1.604, Global test accuracy: 85.93
Round  21, Train loss: 1.510, Test loss: 1.585, Test accuracy: 88.09
Round  21, Global train loss: 1.510, Global test loss: 1.564, Global test accuracy: 90.27
Round  22, Train loss: 1.511, Test loss: 1.584, Test accuracy: 88.22
Round  22, Global train loss: 1.511, Global test loss: 1.565, Global test accuracy: 89.92
Round  23, Train loss: 1.511, Test loss: 1.582, Test accuracy: 88.36
Round  23, Global train loss: 1.511, Global test loss: 1.565, Global test accuracy: 90.18
Round  24, Train loss: 1.511, Test loss: 1.582, Test accuracy: 88.37
Round  24, Global train loss: 1.511, Global test loss: 1.569, Global test accuracy: 89.71
Round  25, Train loss: 1.522, Test loss: 1.582, Test accuracy: 88.36
Round  25, Global train loss: 1.522, Global test loss: 1.596, Global test accuracy: 86.82
Round  26, Train loss: 1.524, Test loss: 1.581, Test accuracy: 88.44
Round  26, Global train loss: 1.524, Global test loss: 1.593, Global test accuracy: 87.23
Round  27, Train loss: 1.506, Test loss: 1.581, Test accuracy: 88.45
Round  27, Global train loss: 1.506, Global test loss: 1.567, Global test accuracy: 89.94
Round  28, Train loss: 1.520, Test loss: 1.581, Test accuracy: 88.46
Round  28, Global train loss: 1.520, Global test loss: 1.588, Global test accuracy: 87.67
Round  29, Train loss: 1.509, Test loss: 1.580, Test accuracy: 88.50
Round  29, Global train loss: 1.509, Global test loss: 1.562, Global test accuracy: 90.28
Round  30, Train loss: 1.521, Test loss: 1.580, Test accuracy: 88.46
Round  30, Global train loss: 1.521, Global test loss: 1.593, Global test accuracy: 87.33
Round  31, Train loss: 1.505, Test loss: 1.579, Test accuracy: 88.53
Round  31, Global train loss: 1.505, Global test loss: 1.562, Global test accuracy: 90.50
Round  32, Train loss: 1.490, Test loss: 1.579, Test accuracy: 88.57
Round  32, Global train loss: 1.490, Global test loss: 1.552, Global test accuracy: 91.20
Round  33, Train loss: 1.502, Test loss: 1.579, Test accuracy: 88.58
Round  33, Global train loss: 1.502, Global test loss: 1.561, Global test accuracy: 90.50
Round  34, Train loss: 1.487, Test loss: 1.579, Test accuracy: 88.63
Round  34, Global train loss: 1.487, Global test loss: 1.551, Global test accuracy: 91.47
Round  35, Train loss: 1.490, Test loss: 1.579, Test accuracy: 88.58
Round  35, Global train loss: 1.490, Global test loss: 1.551, Global test accuracy: 91.38
Round  36, Train loss: 1.488, Test loss: 1.578, Test accuracy: 88.61
Round  36, Global train loss: 1.488, Global test loss: 1.552, Global test accuracy: 91.24
Round  37, Train loss: 1.505, Test loss: 1.578, Test accuracy: 88.66
Round  37, Global train loss: 1.505, Global test loss: 1.559, Global test accuracy: 90.69
Round  38, Train loss: 1.504, Test loss: 1.578, Test accuracy: 88.63
Round  38, Global train loss: 1.504, Global test loss: 1.562, Global test accuracy: 90.33
Round  39, Train loss: 1.485, Test loss: 1.578, Test accuracy: 88.66
Round  39, Global train loss: 1.485, Global test loss: 1.548, Global test accuracy: 91.53
Round  40, Train loss: 1.486, Test loss: 1.578, Test accuracy: 88.64
Round  40, Global train loss: 1.486, Global test loss: 1.547, Global test accuracy: 91.69
Round  41, Train loss: 1.532, Test loss: 1.578, Test accuracy: 88.65
Round  41, Global train loss: 1.532, Global test loss: 1.614, Global test accuracy: 84.51
Round  42, Train loss: 1.499, Test loss: 1.578, Test accuracy: 88.63
Round  42, Global train loss: 1.499, Global test loss: 1.557, Global test accuracy: 90.92
Round  43, Train loss: 1.485, Test loss: 1.578, Test accuracy: 88.63
Round  43, Global train loss: 1.485, Global test loss: 1.549, Global test accuracy: 91.67
Round  44, Train loss: 1.516, Test loss: 1.578, Test accuracy: 88.65
Round  44, Global train loss: 1.516, Global test loss: 1.579, Global test accuracy: 88.53
Round  45, Train loss: 1.502, Test loss: 1.578, Test accuracy: 88.63
Round  45, Global train loss: 1.502, Global test loss: 1.554, Global test accuracy: 90.97
Round  46, Train loss: 1.486, Test loss: 1.578, Test accuracy: 88.61
Round  46, Global train loss: 1.486, Global test loss: 1.551, Global test accuracy: 91.25
Round  47, Train loss: 1.498, Test loss: 1.578, Test accuracy: 88.60
Round  47, Global train loss: 1.498, Global test loss: 1.555, Global test accuracy: 91.00
Round  48, Train loss: 1.485, Test loss: 1.578, Test accuracy: 88.61
Round  48, Global train loss: 1.485, Global test loss: 1.548, Global test accuracy: 91.71
Round  49, Train loss: 1.493, Test loss: 1.575, Test accuracy: 88.90
Round  49, Global train loss: 1.493, Global test loss: 1.551, Global test accuracy: 91.29
Round  50, Train loss: 1.482, Test loss: 1.575, Test accuracy: 88.94
Round  50, Global train loss: 1.482, Global test loss: 1.547, Global test accuracy: 91.65
Round  51, Train loss: 1.484, Test loss: 1.575, Test accuracy: 88.97
Round  51, Global train loss: 1.484, Global test loss: 1.551, Global test accuracy: 91.31
Round  52, Train loss: 1.500, Test loss: 1.573, Test accuracy: 89.07
Round  52, Global train loss: 1.500, Global test loss: 1.562, Global test accuracy: 90.30
Round  53, Train loss: 1.486, Test loss: 1.573, Test accuracy: 89.11
Round  53, Global train loss: 1.486, Global test loss: 1.550, Global test accuracy: 91.44
Round  54, Train loss: 1.513, Test loss: 1.572, Test accuracy: 89.28
Round  54, Global train loss: 1.513, Global test loss: 1.565, Global test accuracy: 90.01
Round  55, Train loss: 1.485, Test loss: 1.571, Test accuracy: 89.30
Round  55, Global train loss: 1.485, Global test loss: 1.550, Global test accuracy: 91.43
Round  56, Train loss: 1.481, Test loss: 1.571, Test accuracy: 89.30
Round  56, Global train loss: 1.481, Global test loss: 1.546, Global test accuracy: 91.77
Round  57, Train loss: 1.488, Test loss: 1.570, Test accuracy: 89.43
Round  57, Global train loss: 1.488, Global test loss: 1.549, Global test accuracy: 91.55
Round  58, Train loss: 1.502, Test loss: 1.570, Test accuracy: 89.42
Round  58, Global train loss: 1.502, Global test loss: 1.560, Global test accuracy: 90.66
Round  59, Train loss: 1.482, Test loss: 1.570, Test accuracy: 89.42
Round  59, Global train loss: 1.482, Global test loss: 1.549, Global test accuracy: 91.60
Round  60, Train loss: 1.484, Test loss: 1.569, Test accuracy: 89.44
Round  60, Global train loss: 1.484, Global test loss: 1.548, Global test accuracy: 91.53
Round  61, Train loss: 1.501, Test loss: 1.569, Test accuracy: 89.45
Round  61, Global train loss: 1.501, Global test loss: 1.565, Global test accuracy: 89.93
Round  62, Train loss: 1.499, Test loss: 1.569, Test accuracy: 89.49
Round  62, Global train loss: 1.499, Global test loss: 1.558, Global test accuracy: 90.57
Round  63, Train loss: 1.483, Test loss: 1.569, Test accuracy: 89.52
Round  63, Global train loss: 1.483, Global test loss: 1.546, Global test accuracy: 91.68
Round  64, Train loss: 1.485, Test loss: 1.569, Test accuracy: 89.53
Round  64, Global train loss: 1.485, Global test loss: 1.546, Global test accuracy: 91.76
Round  65, Train loss: 1.499, Test loss: 1.569, Test accuracy: 89.53
Round  65, Global train loss: 1.499, Global test loss: 1.558, Global test accuracy: 90.58
Round  66, Train loss: 1.480, Test loss: 1.569, Test accuracy: 89.53
Round  66, Global train loss: 1.480, Global test loss: 1.546, Global test accuracy: 91.78
Round  67, Train loss: 1.483, Test loss: 1.569, Test accuracy: 89.54
Round  67, Global train loss: 1.483, Global test loss: 1.548, Global test accuracy: 91.61
Round  68, Train loss: 1.485, Test loss: 1.569, Test accuracy: 89.56
Round  68, Global train loss: 1.485, Global test loss: 1.544, Global test accuracy: 91.97
Round  69, Train loss: 1.485, Test loss: 1.569, Test accuracy: 89.58
Round  69, Global train loss: 1.485, Global test loss: 1.548, Global test accuracy: 91.47
Round  70, Train loss: 1.482, Test loss: 1.569, Test accuracy: 89.55
Round  70, Global train loss: 1.482, Global test loss: 1.550, Global test accuracy: 91.33
Round  71, Train loss: 1.497, Test loss: 1.569, Test accuracy: 89.53
Round  71, Global train loss: 1.497, Global test loss: 1.553, Global test accuracy: 91.10
Round  72, Train loss: 1.483, Test loss: 1.569, Test accuracy: 89.55
Round  72, Global train loss: 1.483, Global test loss: 1.547, Global test accuracy: 91.57
Round  73, Train loss: 1.481, Test loss: 1.568, Test accuracy: 89.56
Round  73, Global train loss: 1.481, Global test loss: 1.550, Global test accuracy: 91.32
Round  74, Train loss: 1.500, Test loss: 1.568, Test accuracy: 89.56
Round  74, Global train loss: 1.500, Global test loss: 1.550, Global test accuracy: 91.45
Round  75, Train loss: 1.484, Test loss: 1.568, Test accuracy: 89.55
Round  75, Global train loss: 1.484, Global test loss: 1.548, Global test accuracy: 91.75
Round  76, Train loss: 1.484, Test loss: 1.568, Test accuracy: 89.53
Round  76, Global train loss: 1.484, Global test loss: 1.551, Global test accuracy: 91.30
Round  77, Train loss: 1.486, Test loss: 1.568, Test accuracy: 89.56
Round  77, Global train loss: 1.486, Global test loss: 1.549, Global test accuracy: 91.51
Round  78, Train loss: 1.484, Test loss: 1.568, Test accuracy: 89.55
Round  78, Global train loss: 1.484, Global test loss: 1.544, Global test accuracy: 91.89
Round  79, Train loss: 1.489, Test loss: 1.565, Test accuracy: 89.94
Round  79, Global train loss: 1.489, Global test loss: 1.547, Global test accuracy: 91.61
Round  80, Train loss: 1.487, Test loss: 1.565, Test accuracy: 89.95
Round  80, Global train loss: 1.487, Global test loss: 1.547, Global test accuracy: 91.74
Round  81, Train loss: 1.481, Test loss: 1.565, Test accuracy: 89.94
Round  81, Global train loss: 1.481, Global test loss: 1.546, Global test accuracy: 91.93
Round  82, Train loss: 1.481, Test loss: 1.565, Test accuracy: 89.95
Round  82, Global train loss: 1.481, Global test loss: 1.544, Global test accuracy: 92.04
Round  83, Train loss: 1.485, Test loss: 1.565, Test accuracy: 89.95
Round  83, Global train loss: 1.485, Global test loss: 1.552, Global test accuracy: 91.27
Round  84, Train loss: 1.480, Test loss: 1.565, Test accuracy: 89.96
Round  84, Global train loss: 1.480, Global test loss: 1.550, Global test accuracy: 91.45
Round  85, Train loss: 1.480, Test loss: 1.565, Test accuracy: 89.97
Round  85, Global train loss: 1.480, Global test loss: 1.546, Global test accuracy: 91.91
Round  86, Train loss: 1.485, Test loss: 1.565, Test accuracy: 89.96
Round  86, Global train loss: 1.485, Global test loss: 1.552, Global test accuracy: 91.22
Round  87, Train loss: 1.482, Test loss: 1.565, Test accuracy: 89.99
Round  87, Global train loss: 1.482, Global test loss: 1.546, Global test accuracy: 91.78
Round  88, Train loss: 1.482, Test loss: 1.564, Test accuracy: 90.00
Round  88, Global train loss: 1.482, Global test loss: 1.549, Global test accuracy: 91.48
Round  89, Train loss: 1.485, Test loss: 1.564, Test accuracy: 90.00
Round  89, Global train loss: 1.485, Global test loss: 1.548, Global test accuracy: 91.49
Round  90, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.97
Round  90, Global train loss: 1.483, Global test loss: 1.548, Global test accuracy: 91.57
Round  91, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.97
Round  91, Global train loss: 1.483, Global test loss: 1.548, Global test accuracy: 91.64
Round  92, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.97
Round  92, Global train loss: 1.481, Global test loss: 1.553, Global test accuracy: 91.07
Round  93, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.97
Round  93, Global train loss: 1.481, Global test loss: 1.544, Global test accuracy: 91.98
Round  94, Train loss: 1.482, Test loss: 1.564, Test accuracy: 90.00
Round  94, Global train loss: 1.482, Global test loss: 1.543, Global test accuracy: 92.07
Round  95, Train loss: 1.485, Test loss: 1.564, Test accuracy: 89.99
Round  95, Global train loss: 1.485, Global test loss: 1.548, Global test accuracy: 91.43/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.480, Test loss: 1.564, Test accuracy: 89.97
Round  96, Global train loss: 1.480, Global test loss: 1.547, Global test accuracy: 91.68
Round  97, Train loss: 1.482, Test loss: 1.564, Test accuracy: 89.96
Round  97, Global train loss: 1.482, Global test loss: 1.545, Global test accuracy: 91.92
Round  98, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.98
Round  98, Global train loss: 1.483, Global test loss: 1.549, Global test accuracy: 91.74
Round  99, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.95
Round  99, Global train loss: 1.481, Global test loss: 1.544, Global test accuracy: 92.03
Final Round, Train loss: 1.483, Test loss: 1.565, Test accuracy: 89.85
Final Round, Global train loss: 1.483, Global test loss: 1.544, Global test accuracy: 92.03
Average accuracy final 10 rounds: 89.97375 

Average global accuracy final 10 rounds: 91.71325 

5476.526216745377
[3.6140453815460205, 7.228090763092041, 10.947640180587769, 14.667189598083496, 18.442952632904053, 22.21871566772461, 25.32017469406128, 28.42163372039795, 31.53436589241028, 34.64709806442261, 37.72979402542114, 40.81248998641968, 44.019447565078735, 47.22640514373779, 50.387640953063965, 53.54887676239014, 56.907352685928345, 60.26582860946655, 63.4013888835907, 66.53694915771484, 69.49372839927673, 72.45050764083862, 75.43419647216797, 78.41788530349731, 81.53585743904114, 84.65382957458496, 87.77538442611694, 90.89693927764893, 94.13759422302246, 97.378249168396, 100.58589768409729, 103.79354619979858, 106.84836459159851, 109.90318298339844, 112.94393730163574, 115.98469161987305, 119.10553979873657, 122.2263879776001, 125.34958004951477, 128.47277212142944, 131.63442540168762, 134.7960786819458, 137.94677352905273, 141.09746837615967, 144.18997192382812, 147.28247547149658, 150.29182815551758, 153.30118083953857, 156.36104774475098, 159.42091464996338, 162.49654626846313, 165.5721778869629, 168.8057346343994, 172.03929138183594, 175.26838970184326, 178.4974880218506, 181.6929485797882, 184.88840913772583, 188.00806212425232, 191.1277151107788, 194.37682914733887, 197.62594318389893, 200.79838490486145, 203.97082662582397, 207.1648154258728, 210.35880422592163, 213.45565009117126, 216.5524959564209, 219.74645113945007, 222.94040632247925, 226.0849301815033, 229.22945404052734, 232.45439267158508, 235.67933130264282, 238.9217448234558, 242.1641583442688, 245.3162453174591, 248.4683322906494, 251.5275754928589, 254.58681869506836, 257.62425327301025, 260.66168785095215, 263.87167859077454, 267.0816693305969, 270.3618993759155, 273.64212942123413, 277.10259437561035, 280.5630593299866, 283.9736111164093, 287.38416290283203, 290.69381976127625, 294.00347661972046, 297.1695833206177, 300.3356900215149, 303.7923684120178, 307.24904680252075, 310.5848116874695, 313.9205765724182, 317.1924743652344, 320.46437215805054, 323.7904827594757, 327.1165933609009, 330.4416391849518, 333.7666850090027, 337.07795333862305, 340.3892216682434, 343.70067620277405, 347.0121307373047, 350.51166439056396, 354.01119804382324, 357.49717354774475, 360.98314905166626, 364.3855218887329, 367.78789472579956, 371.2218482494354, 374.6558017730713, 378.00514245033264, 381.354483127594, 384.93575954437256, 388.5170359611511, 392.0028359889984, 395.4886360168457, 398.9559063911438, 402.4231767654419, 405.7730224132538, 409.1228680610657, 412.3722314834595, 415.62159490585327, 419.06449460983276, 422.50739431381226, 425.9913680553436, 429.475341796875, 433.05517315864563, 436.63500452041626, 440.11045002937317, 443.5858955383301, 446.8397512435913, 450.09360694885254, 453.1800260543823, 456.2664451599121, 459.3350121974945, 462.4035792350769, 465.758944272995, 469.1143093109131, 472.55592012405396, 475.9975309371948, 479.3535375595093, 482.70954418182373, 486.0234351158142, 489.3373260498047, 492.5972309112549, 495.8571357727051, 499.21841168403625, 502.57968759536743, 505.895437002182, 509.2111864089966, 512.5974953174591, 515.9838042259216, 519.3934025764465, 522.8030009269714, 526.1453998088837, 529.4877986907959, 532.8225071430206, 536.1572155952454, 539.4515759944916, 542.7459363937378, 546.170982837677, 549.5960292816162, 552.8624670505524, 556.1289048194885, 559.5221357345581, 562.9153666496277, 566.2290945053101, 569.5428223609924, 572.7538223266602, 575.9648222923279, 579.2194745540619, 582.4741268157959, 585.9082410335541, 589.3423552513123, 592.7495856285095, 596.1568160057068, 599.6395883560181, 603.1223607063293, 606.5253117084503, 609.9282627105713, 613.1604180335999, 616.3925733566284, 619.7100870609283, 623.0276007652283, 626.364824295044, 629.7020478248596, 632.9157099723816, 636.1293721199036, 639.3081078529358, 642.486843585968, 645.7280418872833, 648.9692401885986, 652.1757090091705, 655.3821778297424, 657.1210646629333, 658.8599514961243]
[44.1475, 44.1475, 65.015, 65.015, 73.18, 73.18, 76.945, 76.945, 79.305, 79.305, 80.6675, 80.6675, 81.495, 81.495, 82.215, 82.215, 83.935, 83.935, 84.4175, 84.4175, 84.66, 84.66, 84.645, 84.645, 85.0875, 85.0875, 85.38, 85.38, 85.5425, 85.5425, 86.7225, 86.7225, 87.1625, 87.1625, 87.345, 87.345, 87.6725, 87.6725, 88.0925, 88.0925, 88.0525, 88.0525, 88.0875, 88.0875, 88.2225, 88.2225, 88.355, 88.355, 88.37, 88.37, 88.3575, 88.3575, 88.4375, 88.4375, 88.45, 88.45, 88.4575, 88.4575, 88.5, 88.5, 88.4625, 88.4625, 88.5275, 88.5275, 88.5675, 88.5675, 88.5825, 88.5825, 88.63, 88.63, 88.58, 88.58, 88.605, 88.605, 88.66, 88.66, 88.6325, 88.6325, 88.6575, 88.6575, 88.6375, 88.6375, 88.6525, 88.6525, 88.6275, 88.6275, 88.6325, 88.6325, 88.6525, 88.6525, 88.6275, 88.6275, 88.61, 88.61, 88.5975, 88.5975, 88.605, 88.605, 88.9, 88.9, 88.9375, 88.9375, 88.97, 88.97, 89.0675, 89.0675, 89.115, 89.115, 89.285, 89.285, 89.3025, 89.3025, 89.2975, 89.2975, 89.4325, 89.4325, 89.425, 89.425, 89.4175, 89.4175, 89.4375, 89.4375, 89.4475, 89.4475, 89.4875, 89.4875, 89.515, 89.515, 89.5275, 89.5275, 89.5275, 89.5275, 89.53, 89.53, 89.5425, 89.5425, 89.56, 89.56, 89.5825, 89.5825, 89.545, 89.545, 89.5325, 89.5325, 89.545, 89.545, 89.565, 89.565, 89.5625, 89.5625, 89.545, 89.545, 89.535, 89.535, 89.5575, 89.5575, 89.5525, 89.5525, 89.9375, 89.9375, 89.9475, 89.9475, 89.935, 89.935, 89.9475, 89.9475, 89.9525, 89.9525, 89.96, 89.96, 89.97, 89.97, 89.9575, 89.9575, 89.9925, 89.9925, 89.995, 89.995, 89.9975, 89.9975, 89.975, 89.975, 89.965, 89.965, 89.9675, 89.9675, 89.975, 89.975, 89.995, 89.995, 89.99, 89.99, 89.975, 89.975, 89.9625, 89.9625, 89.9775, 89.9775, 89.955, 89.955, 89.8525, 89.8525]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.270, Test loss: 2.146, Test accuracy: 31.06
Round   0, Global train loss: 2.270, Global test loss: 2.147, Global test accuracy: 30.95
Round   1, Train loss: 1.896, Test loss: 1.784, Test accuracy: 70.49
Round   1, Global train loss: 1.896, Global test loss: 1.691, Global test accuracy: 80.63
Round   2, Train loss: 1.658, Test loss: 1.757, Test accuracy: 71.67
Round   2, Global train loss: 1.658, Global test loss: 1.641, Global test accuracy: 83.03
Round   3, Train loss: 1.630, Test loss: 1.699, Test accuracy: 77.13
Round   3, Global train loss: 1.630, Global test loss: 1.626, Global test accuracy: 84.07
Round   4, Train loss: 1.617, Test loss: 1.668, Test accuracy: 80.28
Round   4, Global train loss: 1.617, Global test loss: 1.620, Global test accuracy: 84.60
Round   5, Train loss: 1.609, Test loss: 1.634, Test accuracy: 83.52
Round   5, Global train loss: 1.609, Global test loss: 1.615, Global test accuracy: 84.92
Round   6, Train loss: 1.606, Test loss: 1.628, Test accuracy: 83.93
Round   6, Global train loss: 1.606, Global test loss: 1.611, Global test accuracy: 85.25
Round   7, Train loss: 1.598, Test loss: 1.625, Test accuracy: 84.18
Round   7, Global train loss: 1.598, Global test loss: 1.608, Global test accuracy: 85.54
Round   8, Train loss: 1.579, Test loss: 1.602, Test accuracy: 86.59
Round   8, Global train loss: 1.579, Global test loss: 1.558, Global test accuracy: 91.12
Round   9, Train loss: 1.532, Test loss: 1.586, Test accuracy: 88.10
Round   9, Global train loss: 1.532, Global test loss: 1.542, Global test accuracy: 92.52
Round  10, Train loss: 1.524, Test loss: 1.571, Test accuracy: 89.51
Round  10, Global train loss: 1.524, Global test loss: 1.536, Global test accuracy: 93.01
Round  11, Train loss: 1.516, Test loss: 1.560, Test accuracy: 90.63
Round  11, Global train loss: 1.516, Global test loss: 1.530, Global test accuracy: 93.52
Round  12, Train loss: 1.509, Test loss: 1.552, Test accuracy: 91.33
Round  12, Global train loss: 1.509, Global test loss: 1.526, Global test accuracy: 93.91
Round  13, Train loss: 1.510, Test loss: 1.541, Test accuracy: 92.48
Round  13, Global train loss: 1.510, Global test loss: 1.523, Global test accuracy: 94.31
Round  14, Train loss: 1.500, Test loss: 1.539, Test accuracy: 92.64
Round  14, Global train loss: 1.500, Global test loss: 1.521, Global test accuracy: 94.33
Round  15, Train loss: 1.504, Test loss: 1.535, Test accuracy: 92.96
Round  15, Global train loss: 1.504, Global test loss: 1.518, Global test accuracy: 94.75
Round  16, Train loss: 1.500, Test loss: 1.527, Test accuracy: 93.75
Round  16, Global train loss: 1.500, Global test loss: 1.516, Global test accuracy: 94.86
Round  17, Train loss: 1.497, Test loss: 1.524, Test accuracy: 94.05
Round  17, Global train loss: 1.497, Global test loss: 1.514, Global test accuracy: 95.03
Round  18, Train loss: 1.492, Test loss: 1.523, Test accuracy: 94.14
Round  18, Global train loss: 1.492, Global test loss: 1.514, Global test accuracy: 95.04
Round  19, Train loss: 1.495, Test loss: 1.521, Test accuracy: 94.25
Round  19, Global train loss: 1.495, Global test loss: 1.512, Global test accuracy: 95.21
Round  20, Train loss: 1.491, Test loss: 1.519, Test accuracy: 94.53
Round  20, Global train loss: 1.491, Global test loss: 1.510, Global test accuracy: 95.33
Round  21, Train loss: 1.490, Test loss: 1.517, Test accuracy: 94.73
Round  21, Global train loss: 1.490, Global test loss: 1.509, Global test accuracy: 95.45
Round  22, Train loss: 1.489, Test loss: 1.515, Test accuracy: 94.90
Round  22, Global train loss: 1.489, Global test loss: 1.509, Global test accuracy: 95.39
Round  23, Train loss: 1.487, Test loss: 1.514, Test accuracy: 95.00
Round  23, Global train loss: 1.487, Global test loss: 1.507, Global test accuracy: 95.69
Round  24, Train loss: 1.485, Test loss: 1.513, Test accuracy: 95.11
Round  24, Global train loss: 1.485, Global test loss: 1.507, Global test accuracy: 95.63
Round  25, Train loss: 1.484, Test loss: 1.512, Test accuracy: 95.22
Round  25, Global train loss: 1.484, Global test loss: 1.506, Global test accuracy: 95.74
Round  26, Train loss: 1.484, Test loss: 1.511, Test accuracy: 95.26
Round  26, Global train loss: 1.484, Global test loss: 1.505, Global test accuracy: 95.87
Round  27, Train loss: 1.483, Test loss: 1.510, Test accuracy: 95.32
Round  27, Global train loss: 1.483, Global test loss: 1.506, Global test accuracy: 95.77
Round  28, Train loss: 1.483, Test loss: 1.509, Test accuracy: 95.42
Round  28, Global train loss: 1.483, Global test loss: 1.504, Global test accuracy: 95.95
Round  29, Train loss: 1.481, Test loss: 1.508, Test accuracy: 95.48
Round  29, Global train loss: 1.481, Global test loss: 1.503, Global test accuracy: 96.01
Round  30, Train loss: 1.480, Test loss: 1.508, Test accuracy: 95.57
Round  30, Global train loss: 1.480, Global test loss: 1.502, Global test accuracy: 96.16
Round  31, Train loss: 1.480, Test loss: 1.507, Test accuracy: 95.65
Round  31, Global train loss: 1.480, Global test loss: 1.502, Global test accuracy: 96.15
Round  32, Train loss: 1.479, Test loss: 1.506, Test accuracy: 95.72
Round  32, Global train loss: 1.479, Global test loss: 1.501, Global test accuracy: 96.26
Round  33, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.83
Round  33, Global train loss: 1.479, Global test loss: 1.501, Global test accuracy: 96.23
Round  34, Train loss: 1.482, Test loss: 1.505, Test accuracy: 95.91
Round  34, Global train loss: 1.482, Global test loss: 1.501, Global test accuracy: 96.21
Round  35, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.88
Round  35, Global train loss: 1.479, Global test loss: 1.501, Global test accuracy: 96.16
Round  36, Train loss: 1.478, Test loss: 1.504, Test accuracy: 95.94
Round  36, Global train loss: 1.478, Global test loss: 1.500, Global test accuracy: 96.28
Round  37, Train loss: 1.477, Test loss: 1.503, Test accuracy: 96.02
Round  37, Global train loss: 1.477, Global test loss: 1.499, Global test accuracy: 96.42
Round  38, Train loss: 1.475, Test loss: 1.503, Test accuracy: 96.05
Round  38, Global train loss: 1.475, Global test loss: 1.499, Global test accuracy: 96.42
Round  39, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.12
Round  39, Global train loss: 1.475, Global test loss: 1.499, Global test accuracy: 96.42
Round  40, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.19
Round  40, Global train loss: 1.475, Global test loss: 1.498, Global test accuracy: 96.50
Round  41, Train loss: 1.476, Test loss: 1.501, Test accuracy: 96.30
Round  41, Global train loss: 1.476, Global test loss: 1.498, Global test accuracy: 96.65
Round  42, Train loss: 1.477, Test loss: 1.500, Test accuracy: 96.32
Round  42, Global train loss: 1.477, Global test loss: 1.497, Global test accuracy: 96.58
Round  43, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.35
Round  43, Global train loss: 1.474, Global test loss: 1.497, Global test accuracy: 96.66
Round  44, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.39
Round  44, Global train loss: 1.473, Global test loss: 1.497, Global test accuracy: 96.75
Round  45, Train loss: 1.475, Test loss: 1.500, Test accuracy: 96.39
Round  45, Global train loss: 1.475, Global test loss: 1.496, Global test accuracy: 96.67
Round  46, Train loss: 1.475, Test loss: 1.500, Test accuracy: 96.39
Round  46, Global train loss: 1.475, Global test loss: 1.496, Global test accuracy: 96.75
Round  47, Train loss: 1.475, Test loss: 1.499, Test accuracy: 96.41
Round  47, Global train loss: 1.475, Global test loss: 1.496, Global test accuracy: 96.80
Round  48, Train loss: 1.474, Test loss: 1.499, Test accuracy: 96.42
Round  48, Global train loss: 1.474, Global test loss: 1.496, Global test accuracy: 96.76
Round  49, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.45
Round  49, Global train loss: 1.472, Global test loss: 1.496, Global test accuracy: 96.75
Round  50, Train loss: 1.474, Test loss: 1.498, Test accuracy: 96.56
Round  50, Global train loss: 1.474, Global test loss: 1.495, Global test accuracy: 96.91
Round  51, Train loss: 1.472, Test loss: 1.498, Test accuracy: 96.57
Round  51, Global train loss: 1.472, Global test loss: 1.495, Global test accuracy: 96.89
Round  52, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.64
Round  52, Global train loss: 1.471, Global test loss: 1.495, Global test accuracy: 96.87
Round  53, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.62
Round  53, Global train loss: 1.472, Global test loss: 1.495, Global test accuracy: 96.88
Round  54, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.65
Round  54, Global train loss: 1.471, Global test loss: 1.495, Global test accuracy: 96.91
Round  55, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.65
Round  55, Global train loss: 1.472, Global test loss: 1.495, Global test accuracy: 96.88
Round  56, Train loss: 1.473, Test loss: 1.497, Test accuracy: 96.70
Round  56, Global train loss: 1.473, Global test loss: 1.495, Global test accuracy: 96.89
Round  57, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.69
Round  57, Global train loss: 1.472, Global test loss: 1.494, Global test accuracy: 97.02
Round  58, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.68
Round  58, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.91
Round  59, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.72
Round  59, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.97
Round  60, Train loss: 1.471, Test loss: 1.496, Test accuracy: 96.74
Round  60, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 97.08
Round  61, Train loss: 1.471, Test loss: 1.496, Test accuracy: 96.75
Round  61, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.95
Round  62, Train loss: 1.471, Test loss: 1.496, Test accuracy: 96.77
Round  62, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.97
Round  63, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.77
Round  63, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 97.00
Round  64, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.80
Round  64, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 97.02
Round  65, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.87
Round  65, Global train loss: 1.471, Global test loss: 1.493, Global test accuracy: 97.05
Round  66, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.87
Round  66, Global train loss: 1.472, Global test loss: 1.493, Global test accuracy: 96.97
Round  67, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.83
Round  67, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 97.00
Round  68, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.83
Round  68, Global train loss: 1.471, Global test loss: 1.493, Global test accuracy: 96.99
Round  69, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.84
Round  69, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 96.97
Round  70, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.85
Round  70, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 96.98
Round  71, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.86
Round  71, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 96.92
Round  72, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.88
Round  72, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 96.98
Round  73, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.87
Round  73, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 96.97
Round  74, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.89
Round  74, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 97.02
Round  75, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.90
Round  75, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 97.04
Round  76, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.92
Round  76, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 97.06
Round  77, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.94
Round  77, Global train loss: 1.470, Global test loss: 1.493, Global test accuracy: 97.05
Round  78, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.97
Round  78, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.02
Round  79, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.97
Round  79, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 97.01
Round  80, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.97
Round  80, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.03
Round  81, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.97
Round  81, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 97.01
Round  82, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.97
Round  82, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.11
Round  83, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.97
Round  83, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.07
Round  84, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.01
Round  84, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.06
Round  85, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.00
Round  85, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.02
Round  86, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.01
Round  86, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.01
Round  87, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.01
Round  87, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.06
Round  88, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.99
Round  88, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.95
Round  89, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.98
Round  89, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.92
Round  90, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.98
Round  90, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.06
Round  91, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.97
Round  91, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.08
Round  92, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.00
Round  92, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.10
Round  93, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.03
Round  93, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.15
Round  94, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.04
Round  94, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.12
Round  95, Train loss: 1.467, Test loss: 1.493, Test accuracy: 97.04
Round  95, Global train loss: 1.467, Global test loss: 1.492, Global test accuracy: 97.06/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.03
Round  96, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.11
Round  97, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.05
Round  97, Global train loss: 1.468, Global test loss: 1.491, Global test accuracy: 97.05
Round  98, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.06
Round  98, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.03
Round  99, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.07
Round  99, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.15
Final Round, Train loss: 1.467, Test loss: 1.493, Test accuracy: 97.01
Final Round, Global train loss: 1.467, Global test loss: 1.492, Global test accuracy: 97.15
Average accuracy final 10 rounds: 97.02575 

Average global accuracy final 10 rounds: 97.08975 

5017.805799484253
[4.039471626281738, 8.078943252563477, 11.59777283668518, 15.116602420806885, 18.502187490463257, 21.88777256011963, 25.081029653549194, 28.27428674697876, 31.67644429206848, 35.0786018371582, 38.44091272354126, 41.803223609924316, 45.15665078163147, 48.51007795333862, 52.035231590270996, 55.56038522720337, 59.08343291282654, 62.60648059844971, 66.5643584728241, 70.52223634719849, 74.17868232727051, 77.83512830734253, 81.4588029384613, 85.08247756958008, 88.72721409797668, 92.37195062637329, 95.9835114479065, 99.5950722694397, 102.9259192943573, 106.2567663192749, 109.91867256164551, 113.58057880401611, 117.53958797454834, 121.49859714508057, 124.94019198417664, 128.3817868232727, 131.80055260658264, 135.21931838989258, 138.39442801475525, 141.56953763961792, 144.82019209861755, 148.0708465576172, 151.21695852279663, 154.36307048797607, 157.54370665550232, 160.72434282302856, 163.92570281028748, 167.1270627975464, 170.1383764743805, 173.1496901512146, 176.35082244873047, 179.55195474624634, 182.8028905391693, 186.05382633209229, 189.28209924697876, 192.51037216186523, 195.42117285728455, 198.33197355270386, 201.05159258842468, 203.7712116241455, 206.65269351005554, 209.53417539596558, 212.36059713363647, 215.18701887130737, 218.11015844345093, 221.03329801559448, 224.20009803771973, 227.36689805984497, 230.4445765018463, 233.52225494384766, 236.5741777420044, 239.62610054016113, 242.69401144981384, 245.76192235946655, 248.85723066329956, 251.95253896713257, 255.080801486969, 258.2090640068054, 261.37239503860474, 264.53572607040405, 267.6455202102661, 270.7553143501282, 273.89477944374084, 277.0342445373535, 280.0944814682007, 283.15471839904785, 286.27476835250854, 289.39481830596924, 292.59385538101196, 295.7928924560547, 298.92944264411926, 302.06599283218384, 305.17652773857117, 308.2870626449585, 311.4704315662384, 314.6538004875183, 317.78729271888733, 320.92078495025635, 324.09489011764526, 327.2689952850342, 330.3385679721832, 333.4081406593323, 336.7864394187927, 340.1647381782532, 343.610586643219, 347.0564351081848, 350.4709298610687, 353.88542461395264, 357.30874466896057, 360.7320647239685, 364.0807671546936, 367.4294695854187, 370.76697611808777, 374.10448265075684, 377.523738861084, 380.94299507141113, 384.3271586894989, 387.71132230758667, 390.84014320373535, 393.96896409988403, 397.03988575935364, 400.11080741882324, 403.16443729400635, 406.21806716918945, 409.30384969711304, 412.3896322250366, 415.53169798851013, 418.67376375198364, 421.8047273159027, 424.9356908798218, 427.9681694507599, 431.000648021698, 434.12103819847107, 437.24142837524414, 440.69896268844604, 444.15649700164795, 447.5598928928375, 450.9632887840271, 454.14801001548767, 457.33273124694824, 460.43892669677734, 463.54512214660645, 466.67339038848877, 469.8016586303711, 472.8911440372467, 475.9806294441223, 479.1156187057495, 482.2506079673767, 485.45034074783325, 488.6500735282898, 491.76272463798523, 494.87537574768066, 497.98171162605286, 501.08804750442505, 504.15985083580017, 507.2316541671753, 510.26511001586914, 513.298565864563, 516.396744966507, 519.4949240684509, 522.6659643650055, 525.8370046615601, 528.9425451755524, 532.0480856895447, 535.201464176178, 538.3548426628113, 541.4155955314636, 544.476348400116, 547.696713924408, 550.9170794487, 553.9804770946503, 557.0438747406006, 560.1339485645294, 563.2240223884583, 566.5771913528442, 569.9303603172302, 572.9930930137634, 576.0558257102966, 579.1596503257751, 582.2634749412537, 585.2977468967438, 588.3320188522339, 591.5504264831543, 594.7688341140747, 597.9363257884979, 601.1038174629211, 604.2537839412689, 607.4037504196167, 610.81738114357, 614.2310118675232, 617.4995563030243, 620.7681007385254, 623.8567504882812, 626.9454002380371, 630.0206046104431, 633.0958089828491, 635.9156022071838, 638.7353954315186, 642.018084526062, 645.3007736206055, 647.0158877372742, 648.7310018539429]
[31.06, 31.06, 70.4875, 70.4875, 71.675, 71.675, 77.13, 77.13, 80.2825, 80.2825, 83.515, 83.515, 83.9275, 83.9275, 84.1825, 84.1825, 86.595, 86.595, 88.1025, 88.1025, 89.51, 89.51, 90.6325, 90.6325, 91.3325, 91.3325, 92.4775, 92.4775, 92.6375, 92.6375, 92.9575, 92.9575, 93.7475, 93.7475, 94.05, 94.05, 94.14, 94.14, 94.255, 94.255, 94.53, 94.53, 94.7275, 94.7275, 94.9025, 94.9025, 94.995, 94.995, 95.105, 95.105, 95.22, 95.22, 95.2625, 95.2625, 95.32, 95.32, 95.4225, 95.4225, 95.48, 95.48, 95.57, 95.57, 95.6475, 95.6475, 95.7225, 95.7225, 95.8275, 95.8275, 95.91, 95.91, 95.875, 95.875, 95.9375, 95.9375, 96.0225, 96.0225, 96.0525, 96.0525, 96.12, 96.12, 96.1925, 96.1925, 96.3025, 96.3025, 96.32, 96.32, 96.35, 96.35, 96.385, 96.385, 96.395, 96.395, 96.3925, 96.3925, 96.405, 96.405, 96.42, 96.42, 96.455, 96.455, 96.565, 96.565, 96.5675, 96.5675, 96.64, 96.64, 96.625, 96.625, 96.65, 96.65, 96.6525, 96.6525, 96.705, 96.705, 96.685, 96.685, 96.68, 96.68, 96.715, 96.715, 96.7425, 96.7425, 96.7525, 96.7525, 96.7675, 96.7675, 96.765, 96.765, 96.8, 96.8, 96.8725, 96.8725, 96.87, 96.87, 96.835, 96.835, 96.8325, 96.8325, 96.84, 96.84, 96.8475, 96.8475, 96.86, 96.86, 96.875, 96.875, 96.8675, 96.8675, 96.8875, 96.8875, 96.8975, 96.8975, 96.92, 96.92, 96.9375, 96.9375, 96.965, 96.965, 96.965, 96.965, 96.9675, 96.9675, 96.9725, 96.9725, 96.9725, 96.9725, 96.97, 96.97, 97.0125, 97.0125, 97.005, 97.005, 97.0125, 97.0125, 97.0125, 97.0125, 96.9925, 96.9925, 96.9825, 96.9825, 96.9775, 96.9775, 96.965, 96.965, 97.0, 97.0, 97.0275, 97.0275, 97.0375, 97.0375, 97.0375, 97.0375, 97.0325, 97.0325, 97.0525, 97.0525, 97.055, 97.055, 97.0725, 97.0725, 97.0075, 97.0075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.293, Test accuracy: 28.23
Round   1, Train loss: 2.282, Test loss: 2.235, Test accuracy: 52.86
Round   2, Train loss: 2.066, Test loss: 1.867, Test accuracy: 67.82
Round   3, Train loss: 1.749, Test loss: 1.716, Test accuracy: 78.02
Round   4, Train loss: 1.669, Test loss: 1.671, Test accuracy: 80.98
Round   5, Train loss: 1.646, Test loss: 1.643, Test accuracy: 82.93
Round   6, Train loss: 1.634, Test loss: 1.635, Test accuracy: 83.32
Round   7, Train loss: 1.627, Test loss: 1.630, Test accuracy: 83.70
Round   8, Train loss: 1.618, Test loss: 1.627, Test accuracy: 84.03
Round   9, Train loss: 1.613, Test loss: 1.625, Test accuracy: 84.06
Round  10, Train loss: 1.612, Test loss: 1.623, Test accuracy: 84.28
Round  11, Train loss: 1.609, Test loss: 1.620, Test accuracy: 84.48
Round  12, Train loss: 1.607, Test loss: 1.618, Test accuracy: 84.69
Round  13, Train loss: 1.602, Test loss: 1.616, Test accuracy: 84.93
Round  14, Train loss: 1.600, Test loss: 1.615, Test accuracy: 85.06
Round  15, Train loss: 1.601, Test loss: 1.608, Test accuracy: 85.69
Round  16, Train loss: 1.590, Test loss: 1.605, Test accuracy: 85.91
Round  17, Train loss: 1.592, Test loss: 1.604, Test accuracy: 86.03
Round  18, Train loss: 1.593, Test loss: 1.603, Test accuracy: 86.19
Round  19, Train loss: 1.589, Test loss: 1.602, Test accuracy: 86.25
Round  20, Train loss: 1.591, Test loss: 1.601, Test accuracy: 86.45
Round  21, Train loss: 1.594, Test loss: 1.599, Test accuracy: 86.44
Round  22, Train loss: 1.585, Test loss: 1.599, Test accuracy: 86.49
Round  23, Train loss: 1.587, Test loss: 1.598, Test accuracy: 86.58
Round  24, Train loss: 1.572, Test loss: 1.595, Test accuracy: 86.86
Round  25, Train loss: 1.566, Test loss: 1.595, Test accuracy: 86.86
Round  26, Train loss: 1.580, Test loss: 1.594, Test accuracy: 86.94
Round  27, Train loss: 1.582, Test loss: 1.594, Test accuracy: 86.97
Round  28, Train loss: 1.570, Test loss: 1.593, Test accuracy: 86.98
Round  29, Train loss: 1.582, Test loss: 1.593, Test accuracy: 87.01
Round  30, Train loss: 1.571, Test loss: 1.592, Test accuracy: 87.09
Round  31, Train loss: 1.578, Test loss: 1.591, Test accuracy: 87.22
Round  32, Train loss: 1.580, Test loss: 1.591, Test accuracy: 87.19
Round  33, Train loss: 1.570, Test loss: 1.588, Test accuracy: 87.43
Round  34, Train loss: 1.568, Test loss: 1.585, Test accuracy: 87.76
Round  35, Train loss: 1.556, Test loss: 1.585, Test accuracy: 87.75
Round  36, Train loss: 1.546, Test loss: 1.581, Test accuracy: 88.09
Round  37, Train loss: 1.557, Test loss: 1.581, Test accuracy: 88.14
Round  38, Train loss: 1.576, Test loss: 1.581, Test accuracy: 88.19
Round  39, Train loss: 1.564, Test loss: 1.580, Test accuracy: 88.24
Round  40, Train loss: 1.576, Test loss: 1.580, Test accuracy: 88.22
Round  41, Train loss: 1.555, Test loss: 1.576, Test accuracy: 88.67
Round  42, Train loss: 1.550, Test loss: 1.575, Test accuracy: 88.77
Round  43, Train loss: 1.552, Test loss: 1.574, Test accuracy: 88.83
Round  44, Train loss: 1.550, Test loss: 1.574, Test accuracy: 88.80
Round  45, Train loss: 1.549, Test loss: 1.574, Test accuracy: 88.86
Round  46, Train loss: 1.536, Test loss: 1.573, Test accuracy: 88.95
Round  47, Train loss: 1.572, Test loss: 1.573, Test accuracy: 88.99
Round  48, Train loss: 1.550, Test loss: 1.570, Test accuracy: 89.34
Round  49, Train loss: 1.546, Test loss: 1.569, Test accuracy: 89.31
Round  50, Train loss: 1.546, Test loss: 1.569, Test accuracy: 89.34
Round  51, Train loss: 1.546, Test loss: 1.565, Test accuracy: 89.75
Round  52, Train loss: 1.541, Test loss: 1.563, Test accuracy: 90.05
Round  53, Train loss: 1.534, Test loss: 1.558, Test accuracy: 90.48
Round  54, Train loss: 1.533, Test loss: 1.556, Test accuracy: 90.80
Round  55, Train loss: 1.504, Test loss: 1.543, Test accuracy: 92.07
Round  56, Train loss: 1.531, Test loss: 1.542, Test accuracy: 92.17
Round  57, Train loss: 1.529, Test loss: 1.542, Test accuracy: 92.15
Round  58, Train loss: 1.519, Test loss: 1.541, Test accuracy: 92.25
Round  59, Train loss: 1.501, Test loss: 1.541, Test accuracy: 92.28
Round  60, Train loss: 1.516, Test loss: 1.540, Test accuracy: 92.39
Round  61, Train loss: 1.514, Test loss: 1.539, Test accuracy: 92.36
Round  62, Train loss: 1.508, Test loss: 1.537, Test accuracy: 92.66
Round  63, Train loss: 1.511, Test loss: 1.528, Test accuracy: 93.55
Round  64, Train loss: 1.506, Test loss: 1.528, Test accuracy: 93.58
Round  65, Train loss: 1.501, Test loss: 1.528, Test accuracy: 93.72
Round  66, Train loss: 1.487, Test loss: 1.525, Test accuracy: 93.96
Round  67, Train loss: 1.486, Test loss: 1.522, Test accuracy: 94.17
Round  68, Train loss: 1.489, Test loss: 1.516, Test accuracy: 94.77
Round  69, Train loss: 1.487, Test loss: 1.513, Test accuracy: 95.02
Round  70, Train loss: 1.484, Test loss: 1.513, Test accuracy: 95.07
Round  71, Train loss: 1.484, Test loss: 1.513, Test accuracy: 95.06
Round  72, Train loss: 1.480, Test loss: 1.513, Test accuracy: 95.08
Round  73, Train loss: 1.485, Test loss: 1.513, Test accuracy: 95.09
Round  74, Train loss: 1.482, Test loss: 1.513, Test accuracy: 95.05
Round  75, Train loss: 1.483, Test loss: 1.508, Test accuracy: 95.53
Round  76, Train loss: 1.484, Test loss: 1.506, Test accuracy: 95.75
Round  77, Train loss: 1.481, Test loss: 1.505, Test accuracy: 95.80
Round  78, Train loss: 1.481, Test loss: 1.505, Test accuracy: 95.84
Round  79, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.84
Round  80, Train loss: 1.482, Test loss: 1.505, Test accuracy: 95.83
Round  81, Train loss: 1.480, Test loss: 1.505, Test accuracy: 95.84
Round  82, Train loss: 1.480, Test loss: 1.504, Test accuracy: 95.95
Round  83, Train loss: 1.480, Test loss: 1.504, Test accuracy: 95.93
Round  84, Train loss: 1.478, Test loss: 1.504, Test accuracy: 96.00
Round  85, Train loss: 1.479, Test loss: 1.504, Test accuracy: 96.02
Round  86, Train loss: 1.479, Test loss: 1.503, Test accuracy: 96.05
Round  87, Train loss: 1.477, Test loss: 1.503, Test accuracy: 96.08
Round  88, Train loss: 1.479, Test loss: 1.503, Test accuracy: 96.06
Round  89, Train loss: 1.476, Test loss: 1.503, Test accuracy: 96.05
Round  90, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.07
Round  91, Train loss: 1.477, Test loss: 1.502, Test accuracy: 96.09
Round  92, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.12
Round  93, Train loss: 1.478, Test loss: 1.502, Test accuracy: 96.11
Round  94, Train loss: 1.477, Test loss: 1.502, Test accuracy: 96.17
Round  95, Train loss: 1.477, Test loss: 1.502, Test accuracy: 96.13
Round  96, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.13
Round  97, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.14
Round  98, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.19/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.22
Final Round, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.22
Average accuracy final 10 rounds: 96.13725 

4151.85293841362
[3.1042380332946777, 6.2084760665893555, 9.349327564239502, 12.490179061889648, 15.623443365097046, 18.756707668304443, 22.07085084915161, 25.38499402999878, 28.77542996406555, 32.165865898132324, 35.22525119781494, 38.28463649749756, 41.37298083305359, 44.46132516860962, 47.49956202507019, 50.53779888153076, 53.37718915939331, 56.21657943725586, 59.14655947685242, 62.076539516448975, 65.34645366668701, 68.61636781692505, 71.5545003414154, 74.49263286590576, 77.31330442428589, 80.13397598266602, 83.2376537322998, 86.3413314819336, 89.45941996574402, 92.57750844955444, 95.48842525482178, 98.39934206008911, 101.2754533290863, 104.1515645980835, 107.119473695755, 110.08738279342651, 112.95307731628418, 115.81877183914185, 118.74939703941345, 121.68002223968506, 124.72419714927673, 127.76837205886841, 130.62505531311035, 133.4817385673523, 136.31843423843384, 139.15512990951538, 141.96765542030334, 144.7801809310913, 147.62422561645508, 150.46827030181885, 153.51809215545654, 156.56791400909424, 159.44025778770447, 162.3126015663147, 165.14459657669067, 167.97659158706665, 170.9715461730957, 173.96650075912476, 176.810142993927, 179.65378522872925, 182.5426471233368, 185.43150901794434, 188.49578547477722, 191.5600619316101, 194.44353461265564, 197.32700729370117, 200.26075387001038, 203.19450044631958, 206.2203619480133, 209.24622344970703, 212.2079451084137, 215.16966676712036, 218.20435643196106, 221.23904609680176, 224.02401638031006, 226.80898666381836, 229.62987232208252, 232.45075798034668, 235.3981328010559, 238.34550762176514, 241.2088804244995, 244.0722532272339, 247.05392956733704, 250.03560590744019, 252.95128631591797, 255.86696672439575, 258.76720118522644, 261.66743564605713, 264.75261330604553, 267.83779096603394, 270.86401772499084, 273.89024448394775, 276.7353210449219, 279.580397605896, 282.77487349510193, 285.96934938430786, 289.2393310070038, 292.5093126296997, 295.6472237110138, 298.7851347923279, 302.1609539985657, 305.53677320480347, 308.7219145298004, 311.90705585479736, 315.19189047813416, 318.47672510147095, 322.18639183044434, 325.8960585594177, 329.29644751548767, 332.6968364715576, 335.79886174201965, 338.9008870124817, 342.04723358154297, 345.19358015060425, 348.89017510414124, 352.5867700576782, 355.9421067237854, 359.2974433898926, 362.4965789318085, 365.69571447372437, 368.83918595314026, 371.98265743255615, 375.31040716171265, 378.63815689086914, 381.8152160644531, 384.9922752380371, 388.278400182724, 391.5645251274109, 395.193039894104, 398.8215546607971, 401.87005376815796, 404.9185528755188, 408.11958837509155, 411.3206238746643, 414.5484776496887, 417.77633142471313, 421.02941703796387, 424.2825026512146, 427.5134584903717, 430.7444143295288, 434.0519645214081, 437.35951471328735, 440.47885298728943, 443.5981912612915, 447.0236167907715, 450.44904232025146, 453.9180746078491, 457.3871068954468, 460.7611355781555, 464.13516426086426, 467.4810588359833, 470.8269534111023, 474.4649169445038, 478.1028804779053, 481.5798308849335, 485.05678129196167, 488.39726972579956, 491.73775815963745, 495.190452337265, 498.6431465148926, 501.727942943573, 504.8127393722534, 508.10736107826233, 511.40198278427124, 514.7874002456665, 518.1728177070618, 521.5583703517914, 524.943922996521, 528.3904986381531, 531.8370742797852, 535.0580582618713, 538.2790422439575, 541.5934369564056, 544.9078316688538, 548.130619764328, 551.3534078598022, 554.6345436573029, 557.9156794548035, 561.1633565425873, 564.4110336303711, 567.6323547363281, 570.8536758422852, 574.363698720932, 577.8737215995789, 581.3926067352295, 584.9114918708801, 588.1121609210968, 591.3128299713135, 594.56613945961, 597.8194489479065, 601.080518245697, 604.3415875434875, 607.5096232891083, 610.677659034729, 613.9250936508179, 617.1725282669067, 620.6301651000977, 624.0878019332886, 627.2337212562561, 630.3796405792236, 631.9146206378937, 633.4496006965637]
[28.235, 28.235, 52.8575, 52.8575, 67.82, 67.82, 78.02, 78.02, 80.9775, 80.9775, 82.9275, 82.9275, 83.3175, 83.3175, 83.7, 83.7, 84.035, 84.035, 84.065, 84.065, 84.285, 84.285, 84.48, 84.48, 84.685, 84.685, 84.93, 84.93, 85.06, 85.06, 85.685, 85.685, 85.9075, 85.9075, 86.0275, 86.0275, 86.195, 86.195, 86.2525, 86.2525, 86.4475, 86.4475, 86.435, 86.435, 86.49, 86.49, 86.585, 86.585, 86.86, 86.86, 86.855, 86.855, 86.9425, 86.9425, 86.975, 86.975, 86.9825, 86.9825, 87.01, 87.01, 87.0875, 87.0875, 87.215, 87.215, 87.1875, 87.1875, 87.4275, 87.4275, 87.7625, 87.7625, 87.75, 87.75, 88.09, 88.09, 88.145, 88.145, 88.185, 88.185, 88.2425, 88.2425, 88.2225, 88.2225, 88.67, 88.67, 88.7725, 88.7725, 88.8325, 88.8325, 88.8, 88.8, 88.865, 88.865, 88.9475, 88.9475, 88.9925, 88.9925, 89.34, 89.34, 89.3125, 89.3125, 89.345, 89.345, 89.7475, 89.7475, 90.0475, 90.0475, 90.48, 90.48, 90.8025, 90.8025, 92.0675, 92.0675, 92.165, 92.165, 92.1525, 92.1525, 92.25, 92.25, 92.285, 92.285, 92.3875, 92.3875, 92.36, 92.36, 92.6625, 92.6625, 93.5525, 93.5525, 93.585, 93.585, 93.725, 93.725, 93.9575, 93.9575, 94.175, 94.175, 94.765, 94.765, 95.0225, 95.0225, 95.0725, 95.0725, 95.0625, 95.0625, 95.085, 95.085, 95.095, 95.095, 95.0525, 95.0525, 95.5275, 95.5275, 95.75, 95.75, 95.8, 95.8, 95.8375, 95.8375, 95.8425, 95.8425, 95.83, 95.83, 95.8425, 95.8425, 95.955, 95.955, 95.9325, 95.9325, 96.0, 96.0, 96.015, 96.015, 96.0475, 96.0475, 96.0775, 96.0775, 96.055, 96.055, 96.0525, 96.0525, 96.0725, 96.0725, 96.09, 96.09, 96.125, 96.125, 96.105, 96.105, 96.17, 96.17, 96.1275, 96.1275, 96.1325, 96.1325, 96.1375, 96.1375, 96.1925, 96.1925, 96.22, 96.22, 96.22, 96.22]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.298, Test accuracy: 25.76
Round   1, Train loss: 2.294, Test loss: 2.289, Test accuracy: 39.89
Round   2, Train loss: 2.271, Test loss: 2.240, Test accuracy: 36.57
Round   3, Train loss: 2.151, Test loss: 2.076, Test accuracy: 51.33
Round   4, Train loss: 1.945, Test loss: 1.884, Test accuracy: 65.84
Round   5, Train loss: 1.793, Test loss: 1.751, Test accuracy: 75.72
Round   6, Train loss: 1.708, Test loss: 1.698, Test accuracy: 79.43
Round   7, Train loss: 1.672, Test loss: 1.678, Test accuracy: 80.67
Round   8, Train loss: 1.645, Test loss: 1.662, Test accuracy: 81.77
Round   9, Train loss: 1.628, Test loss: 1.655, Test accuracy: 82.10
Round  10, Train loss: 1.631, Test loss: 1.645, Test accuracy: 82.77
Round  11, Train loss: 1.622, Test loss: 1.640, Test accuracy: 82.97
Round  12, Train loss: 1.633, Test loss: 1.635, Test accuracy: 83.39
Round  13, Train loss: 1.615, Test loss: 1.632, Test accuracy: 83.54
Round  14, Train loss: 1.609, Test loss: 1.630, Test accuracy: 83.81
Round  15, Train loss: 1.612, Test loss: 1.628, Test accuracy: 84.12
Round  16, Train loss: 1.601, Test loss: 1.627, Test accuracy: 84.05
Round  17, Train loss: 1.614, Test loss: 1.622, Test accuracy: 84.39
Round  18, Train loss: 1.611, Test loss: 1.621, Test accuracy: 84.48
Round  19, Train loss: 1.595, Test loss: 1.619, Test accuracy: 84.54
Round  20, Train loss: 1.604, Test loss: 1.618, Test accuracy: 84.61
Round  21, Train loss: 1.598, Test loss: 1.618, Test accuracy: 84.60
Round  22, Train loss: 1.601, Test loss: 1.618, Test accuracy: 84.65
Round  23, Train loss: 1.607, Test loss: 1.617, Test accuracy: 84.67
Round  24, Train loss: 1.592, Test loss: 1.616, Test accuracy: 84.78
Round  25, Train loss: 1.598, Test loss: 1.615, Test accuracy: 84.88
Round  26, Train loss: 1.595, Test loss: 1.615, Test accuracy: 84.87
Round  27, Train loss: 1.592, Test loss: 1.615, Test accuracy: 84.97
Round  28, Train loss: 1.591, Test loss: 1.615, Test accuracy: 84.98
Round  29, Train loss: 1.594, Test loss: 1.615, Test accuracy: 84.97
Round  30, Train loss: 1.599, Test loss: 1.614, Test accuracy: 85.08
Round  31, Train loss: 1.586, Test loss: 1.613, Test accuracy: 85.05
Round  32, Train loss: 1.594, Test loss: 1.614, Test accuracy: 84.94
Round  33, Train loss: 1.578, Test loss: 1.613, Test accuracy: 85.02
Round  34, Train loss: 1.585, Test loss: 1.613, Test accuracy: 85.08
Round  35, Train loss: 1.582, Test loss: 1.613, Test accuracy: 85.02
Round  36, Train loss: 1.588, Test loss: 1.612, Test accuracy: 85.12
Round  37, Train loss: 1.586, Test loss: 1.611, Test accuracy: 85.20
Round  38, Train loss: 1.584, Test loss: 1.611, Test accuracy: 85.15
Round  39, Train loss: 1.585, Test loss: 1.612, Test accuracy: 85.12
Round  40, Train loss: 1.582, Test loss: 1.612, Test accuracy: 85.10
Round  41, Train loss: 1.582, Test loss: 1.610, Test accuracy: 85.24
Round  42, Train loss: 1.576, Test loss: 1.611, Test accuracy: 85.10
Round  43, Train loss: 1.577, Test loss: 1.611, Test accuracy: 85.18
Round  44, Train loss: 1.586, Test loss: 1.611, Test accuracy: 85.22
Round  45, Train loss: 1.573, Test loss: 1.611, Test accuracy: 85.08
Round  46, Train loss: 1.588, Test loss: 1.610, Test accuracy: 85.27
Round  47, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.26
Round  48, Train loss: 1.584, Test loss: 1.610, Test accuracy: 85.32
Round  49, Train loss: 1.578, Test loss: 1.610, Test accuracy: 85.33
Round  50, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.36
Round  51, Train loss: 1.571, Test loss: 1.609, Test accuracy: 85.32
Round  52, Train loss: 1.570, Test loss: 1.609, Test accuracy: 85.27
Round  53, Train loss: 1.583, Test loss: 1.609, Test accuracy: 85.38
Round  54, Train loss: 1.578, Test loss: 1.609, Test accuracy: 85.34
Round  55, Train loss: 1.579, Test loss: 1.609, Test accuracy: 85.37
Round  56, Train loss: 1.570, Test loss: 1.608, Test accuracy: 85.43
Round  57, Train loss: 1.578, Test loss: 1.609, Test accuracy: 85.30
Round  58, Train loss: 1.576, Test loss: 1.609, Test accuracy: 85.29
Round  59, Train loss: 1.573, Test loss: 1.608, Test accuracy: 85.37
Round  60, Train loss: 1.580, Test loss: 1.609, Test accuracy: 85.36
Round  61, Train loss: 1.574, Test loss: 1.608, Test accuracy: 85.46
Round  62, Train loss: 1.575, Test loss: 1.608, Test accuracy: 85.34
Round  63, Train loss: 1.581, Test loss: 1.608, Test accuracy: 85.40
Round  64, Train loss: 1.581, Test loss: 1.608, Test accuracy: 85.42
Round  65, Train loss: 1.574, Test loss: 1.608, Test accuracy: 85.42
Round  66, Train loss: 1.572, Test loss: 1.608, Test accuracy: 85.46
Round  67, Train loss: 1.569, Test loss: 1.607, Test accuracy: 85.51
Round  68, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.47
Round  69, Train loss: 1.569, Test loss: 1.607, Test accuracy: 85.47
Round  70, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.53
Round  71, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.54
Round  72, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.51
Round  73, Train loss: 1.569, Test loss: 1.607, Test accuracy: 85.55
Round  74, Train loss: 1.581, Test loss: 1.607, Test accuracy: 85.55
Round  75, Train loss: 1.576, Test loss: 1.607, Test accuracy: 85.52
Round  76, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.65
Round  77, Train loss: 1.575, Test loss: 1.607, Test accuracy: 85.58
Round  78, Train loss: 1.578, Test loss: 1.606, Test accuracy: 85.65
Round  79, Train loss: 1.572, Test loss: 1.606, Test accuracy: 85.67
Round  80, Train loss: 1.567, Test loss: 1.606, Test accuracy: 85.67
Round  81, Train loss: 1.570, Test loss: 1.607, Test accuracy: 85.57
Round  82, Train loss: 1.571, Test loss: 1.606, Test accuracy: 85.62
Round  83, Train loss: 1.569, Test loss: 1.606, Test accuracy: 85.62
Round  84, Train loss: 1.575, Test loss: 1.606, Test accuracy: 85.62
Round  85, Train loss: 1.566, Test loss: 1.606, Test accuracy: 85.68
Round  86, Train loss: 1.571, Test loss: 1.606, Test accuracy: 85.69
Round  87, Train loss: 1.566, Test loss: 1.606, Test accuracy: 85.73
Round  88, Train loss: 1.567, Test loss: 1.605, Test accuracy: 85.73
Round  89, Train loss: 1.565, Test loss: 1.605, Test accuracy: 85.79
Round  90, Train loss: 1.578, Test loss: 1.605, Test accuracy: 85.82
Round  91, Train loss: 1.571, Test loss: 1.605, Test accuracy: 85.84
Round  92, Train loss: 1.569, Test loss: 1.605, Test accuracy: 85.80
Round  93, Train loss: 1.575, Test loss: 1.605, Test accuracy: 85.77
Round  94, Train loss: 1.565, Test loss: 1.605, Test accuracy: 85.83
Round  95, Train loss: 1.566, Test loss: 1.605, Test accuracy: 85.78
Round  96, Train loss: 1.574, Test loss: 1.605, Test accuracy: 85.74
Round  97, Train loss: 1.576, Test loss: 1.605, Test accuracy: 85.76
Round  98, Train loss: 1.568, Test loss: 1.605, Test accuracy: 85.79/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.564, Test loss: 1.605, Test accuracy: 85.76
Final Round, Train loss: 1.569, Test loss: 1.606, Test accuracy: 85.69
Average accuracy final 10 rounds: 85.78833333333334 

1362.093878030777
[1.2968358993530273, 2.5936717987060547, 3.7971203327178955, 5.000568866729736, 6.162575006484985, 7.324581146240234, 8.53959035873413, 9.754599571228027, 10.893835306167603, 12.033071041107178, 13.16342544555664, 14.293779850006104, 15.457037925720215, 16.620296001434326, 17.783292531967163, 18.9462890625, 20.10463571548462, 21.26298236846924, 22.435977935791016, 23.608973503112793, 24.78184199333191, 25.954710483551025, 27.11422348022461, 28.273736476898193, 29.41520929336548, 30.556682109832764, 31.71173334121704, 32.86678457260132, 33.94534778594971, 35.023910999298096, 36.140852212905884, 37.25779342651367, 38.44614815711975, 39.63450288772583, 40.629584312438965, 41.6246657371521, 42.803592920303345, 43.98252010345459, 45.113187074661255, 46.24385404586792, 47.29950523376465, 48.35515642166138, 49.43514895439148, 50.51514148712158, 51.63157796859741, 52.74801445007324, 53.93882703781128, 55.129639625549316, 56.2643985748291, 57.39915752410889, 58.535508155822754, 59.67185878753662, 60.87051439285278, 62.069169998168945, 63.18367624282837, 64.2981824874878, 65.32146048545837, 66.34473848342896, 67.39444422721863, 68.4441499710083, 69.596186876297, 70.7482237815857, 71.95440912246704, 73.16059446334839, 74.30522274971008, 75.44985103607178, 76.64668917655945, 77.84352731704712, 78.94210243225098, 80.04067754745483, 81.23005938529968, 82.41944122314453, 83.54257273674011, 84.6657042503357, 85.79845976829529, 86.93121528625488, 88.080815076828, 89.23041486740112, 90.35200572013855, 91.47359657287598, 92.64962100982666, 93.82564544677734, 95.01398086547852, 96.20231628417969, 97.3385910987854, 98.47486591339111, 99.64751744270325, 100.82016897201538, 101.99730110168457, 103.17443323135376, 104.2735230922699, 105.37261295318604, 106.52548694610596, 107.67836093902588, 108.60417699813843, 109.52999305725098, 110.54015731811523, 111.55032157897949, 112.56993818283081, 113.58955478668213, 114.52308869361877, 115.45662260055542, 116.44097590446472, 117.42532920837402, 118.43769764900208, 119.45006608963013, 120.42757034301758, 121.40507459640503, 122.43908309936523, 123.47309160232544, 124.4534101486206, 125.43372869491577, 126.41366052627563, 127.3935923576355, 128.49886345863342, 129.60413455963135, 130.61199498176575, 131.61985540390015, 132.63328981399536, 133.64672422409058, 134.66956496238708, 135.6924057006836, 136.67328310012817, 137.65416049957275, 138.64659070968628, 139.6390209197998, 140.67856097221375, 141.71810102462769, 142.765766620636, 143.8134322166443, 144.75786304473877, 145.70229387283325, 146.67919635772705, 147.65609884262085, 148.6484673023224, 149.64083576202393, 150.6653027534485, 151.68976974487305, 152.6882824897766, 153.68679523468018, 154.62469696998596, 155.56259870529175, 156.51182579994202, 157.46105289459229, 158.51858925819397, 159.57612562179565, 160.58666920661926, 161.59721279144287, 162.52538418769836, 163.45355558395386, 164.43078541755676, 165.40801525115967, 166.40611720085144, 167.4042191505432, 168.4088592529297, 169.41349935531616, 170.44815683364868, 171.4828143119812, 172.43670010566711, 173.39058589935303, 174.40191006660461, 175.4132342338562, 176.4481611251831, 177.48308801651, 178.4668300151825, 179.45057201385498, 180.50062656402588, 181.55068111419678, 182.53335976600647, 183.51603841781616, 184.50952672958374, 185.50301504135132, 186.52125525474548, 187.53949546813965, 188.56641602516174, 189.59333658218384, 190.56103348731995, 191.52873039245605, 192.5064902305603, 193.48425006866455, 194.46873927116394, 195.45322847366333, 196.4406509399414, 197.42807340621948, 198.44486451148987, 199.46165561676025, 200.4380693435669, 201.41448307037354, 202.37968254089355, 203.34488201141357, 204.3423933982849, 205.33990478515625, 206.3665611743927, 207.39321756362915, 208.3992269039154, 209.40523624420166, 210.45186972618103, 211.4985032081604, 212.48080968856812, 213.46311616897583, 215.1834421157837, 216.90376806259155]
[25.758333333333333, 25.758333333333333, 39.891666666666666, 39.891666666666666, 36.56666666666667, 36.56666666666667, 51.325, 51.325, 65.84166666666667, 65.84166666666667, 75.71666666666667, 75.71666666666667, 79.43333333333334, 79.43333333333334, 80.66666666666667, 80.66666666666667, 81.76666666666667, 81.76666666666667, 82.1, 82.1, 82.76666666666667, 82.76666666666667, 82.975, 82.975, 83.39166666666667, 83.39166666666667, 83.54166666666667, 83.54166666666667, 83.80833333333334, 83.80833333333334, 84.125, 84.125, 84.05, 84.05, 84.39166666666667, 84.39166666666667, 84.48333333333333, 84.48333333333333, 84.54166666666667, 84.54166666666667, 84.60833333333333, 84.60833333333333, 84.6, 84.6, 84.65, 84.65, 84.675, 84.675, 84.775, 84.775, 84.88333333333334, 84.88333333333334, 84.86666666666666, 84.86666666666666, 84.975, 84.975, 84.98333333333333, 84.98333333333333, 84.96666666666667, 84.96666666666667, 85.075, 85.075, 85.05, 85.05, 84.94166666666666, 84.94166666666666, 85.01666666666667, 85.01666666666667, 85.075, 85.075, 85.01666666666667, 85.01666666666667, 85.125, 85.125, 85.2, 85.2, 85.15, 85.15, 85.125, 85.125, 85.1, 85.1, 85.24166666666666, 85.24166666666666, 85.1, 85.1, 85.18333333333334, 85.18333333333334, 85.225, 85.225, 85.075, 85.075, 85.26666666666667, 85.26666666666667, 85.25833333333334, 85.25833333333334, 85.31666666666666, 85.31666666666666, 85.33333333333333, 85.33333333333333, 85.35833333333333, 85.35833333333333, 85.31666666666666, 85.31666666666666, 85.26666666666667, 85.26666666666667, 85.375, 85.375, 85.34166666666667, 85.34166666666667, 85.36666666666666, 85.36666666666666, 85.43333333333334, 85.43333333333334, 85.3, 85.3, 85.29166666666667, 85.29166666666667, 85.36666666666666, 85.36666666666666, 85.35833333333333, 85.35833333333333, 85.45833333333333, 85.45833333333333, 85.34166666666667, 85.34166666666667, 85.4, 85.4, 85.425, 85.425, 85.425, 85.425, 85.45833333333333, 85.45833333333333, 85.50833333333334, 85.50833333333334, 85.46666666666667, 85.46666666666667, 85.46666666666667, 85.46666666666667, 85.53333333333333, 85.53333333333333, 85.54166666666667, 85.54166666666667, 85.50833333333334, 85.50833333333334, 85.55, 85.55, 85.55, 85.55, 85.51666666666667, 85.51666666666667, 85.65, 85.65, 85.575, 85.575, 85.65, 85.65, 85.66666666666667, 85.66666666666667, 85.675, 85.675, 85.56666666666666, 85.56666666666666, 85.625, 85.625, 85.625, 85.625, 85.61666666666666, 85.61666666666666, 85.68333333333334, 85.68333333333334, 85.69166666666666, 85.69166666666666, 85.73333333333333, 85.73333333333333, 85.73333333333333, 85.73333333333333, 85.79166666666667, 85.79166666666667, 85.81666666666666, 85.81666666666666, 85.84166666666667, 85.84166666666667, 85.8, 85.8, 85.76666666666667, 85.76666666666667, 85.825, 85.825, 85.78333333333333, 85.78333333333333, 85.74166666666666, 85.74166666666666, 85.75833333333334, 85.75833333333334, 85.79166666666667, 85.79166666666667, 85.75833333333334, 85.75833333333334, 85.69166666666666, 85.69166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.289, Test loss: 2.237, Test accuracy: 30.20
Round   1, Train loss: 1.953, Test loss: 1.824, Test accuracy: 70.58
Round   2, Train loss: 1.642, Test loss: 1.704, Test accuracy: 82.47
Round   3, Train loss: 1.590, Test loss: 1.661, Test accuracy: 83.97
Round   4, Train loss: 1.552, Test loss: 1.646, Test accuracy: 84.89
Round   5, Train loss: 1.559, Test loss: 1.616, Test accuracy: 86.32
Round   6, Train loss: 1.546, Test loss: 1.594, Test accuracy: 87.88
Round   7, Train loss: 1.527, Test loss: 1.589, Test accuracy: 88.39
Round   8, Train loss: 1.532, Test loss: 1.570, Test accuracy: 89.66
Round   9, Train loss: 1.519, Test loss: 1.568, Test accuracy: 89.71
Round  10, Train loss: 1.510, Test loss: 1.566, Test accuracy: 89.93
Round  11, Train loss: 1.514, Test loss: 1.563, Test accuracy: 90.26
Round  12, Train loss: 1.500, Test loss: 1.562, Test accuracy: 90.36
Round  13, Train loss: 1.499, Test loss: 1.561, Test accuracy: 90.36
Round  14, Train loss: 1.501, Test loss: 1.561, Test accuracy: 90.43
Round  15, Train loss: 1.494, Test loss: 1.561, Test accuracy: 90.42
Round  16, Train loss: 1.495, Test loss: 1.560, Test accuracy: 90.47
Round  17, Train loss: 1.493, Test loss: 1.560, Test accuracy: 90.47
Round  18, Train loss: 1.495, Test loss: 1.558, Test accuracy: 90.55
Round  19, Train loss: 1.491, Test loss: 1.558, Test accuracy: 90.60
Round  20, Train loss: 1.497, Test loss: 1.558, Test accuracy: 90.58
Round  21, Train loss: 1.492, Test loss: 1.558, Test accuracy: 90.58
Round  22, Train loss: 1.499, Test loss: 1.557, Test accuracy: 90.68
Round  23, Train loss: 1.492, Test loss: 1.557, Test accuracy: 90.69
Round  24, Train loss: 1.490, Test loss: 1.556, Test accuracy: 90.73
Round  25, Train loss: 1.492, Test loss: 1.556, Test accuracy: 90.77
Round  26, Train loss: 1.488, Test loss: 1.556, Test accuracy: 90.77
Round  27, Train loss: 1.491, Test loss: 1.555, Test accuracy: 90.78
Round  28, Train loss: 1.491, Test loss: 1.555, Test accuracy: 90.83
Round  29, Train loss: 1.490, Test loss: 1.555, Test accuracy: 90.80
Round  30, Train loss: 1.488, Test loss: 1.555, Test accuracy: 90.86
Round  31, Train loss: 1.492, Test loss: 1.555, Test accuracy: 90.83
Round  32, Train loss: 1.489, Test loss: 1.555, Test accuracy: 90.81
Round  33, Train loss: 1.493, Test loss: 1.555, Test accuracy: 90.82
Round  34, Train loss: 1.488, Test loss: 1.555, Test accuracy: 90.83
Round  35, Train loss: 1.487, Test loss: 1.555, Test accuracy: 90.84
Round  36, Train loss: 1.486, Test loss: 1.555, Test accuracy: 90.86
Round  37, Train loss: 1.491, Test loss: 1.554, Test accuracy: 90.85
Round  38, Train loss: 1.492, Test loss: 1.554, Test accuracy: 90.85
Round  39, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.84
Round  40, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.86
Round  41, Train loss: 1.492, Test loss: 1.554, Test accuracy: 90.84
Round  42, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.86
Round  43, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.89
Round  44, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.89
Round  45, Train loss: 1.490, Test loss: 1.554, Test accuracy: 90.89
Round  46, Train loss: 1.492, Test loss: 1.554, Test accuracy: 90.90
Round  47, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.91
Round  48, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.92
Round  49, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.94
Round  50, Train loss: 1.488, Test loss: 1.554, Test accuracy: 90.93
Round  51, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.95
Round  52, Train loss: 1.488, Test loss: 1.554, Test accuracy: 90.92
Round  53, Train loss: 1.490, Test loss: 1.554, Test accuracy: 90.91
Round  54, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.94
Round  55, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.94
Round  56, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.94
Round  57, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.95
Round  58, Train loss: 1.488, Test loss: 1.554, Test accuracy: 90.92
Round  59, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.94
Round  60, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.95
Round  61, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.94
Round  62, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.92
Round  63, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.94
Round  64, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.94
Round  65, Train loss: 1.488, Test loss: 1.554, Test accuracy: 90.95
Round  66, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.97
Round  67, Train loss: 1.491, Test loss: 1.554, Test accuracy: 90.97
Round  68, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.95
Round  69, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.95
Round  70, Train loss: 1.489, Test loss: 1.554, Test accuracy: 90.95
Round  71, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.95
Round  72, Train loss: 1.485, Test loss: 1.554, Test accuracy: 90.96
Round  73, Train loss: 1.489, Test loss: 1.553, Test accuracy: 90.96
Round  74, Train loss: 1.487, Test loss: 1.554, Test accuracy: 90.95
Round  75, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.93
Round  76, Train loss: 1.486, Test loss: 1.554, Test accuracy: 90.97
Round  77, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.96
Round  78, Train loss: 1.487, Test loss: 1.553, Test accuracy: 90.98
Round  79, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.98
Round  80, Train loss: 1.485, Test loss: 1.553, Test accuracy: 90.99
Round  81, Train loss: 1.487, Test loss: 1.553, Test accuracy: 91.00
Round  82, Train loss: 1.488, Test loss: 1.553, Test accuracy: 91.00
Round  83, Train loss: 1.486, Test loss: 1.553, Test accuracy: 90.99
Round  84, Train loss: 1.490, Test loss: 1.553, Test accuracy: 91.00
Round  85, Train loss: 1.487, Test loss: 1.553, Test accuracy: 91.00
Round  86, Train loss: 1.485, Test loss: 1.553, Test accuracy: 90.98
Round  87, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.99
Round  88, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.99
Round  89, Train loss: 1.485, Test loss: 1.553, Test accuracy: 91.00
Round  90, Train loss: 1.482, Test loss: 1.553, Test accuracy: 91.01
Round  91, Train loss: 1.483, Test loss: 1.553, Test accuracy: 91.02
Round  92, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.99
Round  93, Train loss: 1.485, Test loss: 1.553, Test accuracy: 90.98
Round  94, Train loss: 1.486, Test loss: 1.553, Test accuracy: 90.98
Round  95, Train loss: 1.482, Test loss: 1.553, Test accuracy: 90.98
Round  96, Train loss: 1.484, Test loss: 1.553, Test accuracy: 91.00
Round  97, Train loss: 1.487, Test loss: 1.553, Test accuracy: 91.00
Round  98, Train loss: 1.485, Test loss: 1.553, Test accuracy: 90.98
Round  99, Train loss: 1.486, Test loss: 1.553, Test accuracy: 90.98/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.484, Test loss: 1.553, Test accuracy: 90.98
Average accuracy final 10 rounds: 90.99175 

4282.736147642136
[2.5665767192840576, 5.133153438568115, 7.7849390506744385, 10.436724662780762, 13.021049737930298, 15.605374813079834, 18.227288246154785, 20.849201679229736, 23.365391492843628, 25.88158130645752, 28.38517189025879, 30.88876247406006, 33.7054717540741, 36.522181034088135, 39.41102075576782, 42.29986047744751, 45.253742933273315, 48.20762538909912, 51.246575593948364, 54.28552579879761, 57.19935345649719, 60.11318111419678, 63.248175621032715, 66.38317012786865, 69.55757808685303, 72.7319860458374, 75.9301815032959, 79.1283769607544, 82.81270933151245, 86.49704170227051, 89.92322778701782, 93.34941387176514, 96.51136326789856, 99.67331266403198, 103.1108410358429, 106.54836940765381, 109.9453067779541, 113.3422441482544, 116.52050018310547, 119.69875621795654, 123.00830101966858, 126.31784582138062, 129.66406750679016, 133.0102891921997, 136.32869338989258, 139.64709758758545, 143.15394163131714, 146.66078567504883, 150.0623073577881, 153.46382904052734, 156.77379417419434, 160.08375930786133, 163.41743302345276, 166.7511067390442, 170.06626653671265, 173.3814263343811, 176.76961755752563, 180.15780878067017, 183.6155002117157, 187.07319164276123, 190.32452988624573, 193.57586812973022, 196.7484414577484, 199.9210147857666, 203.18318557739258, 206.44535636901855, 209.76532316207886, 213.08528995513916, 216.4792492389679, 219.87320852279663, 223.52814269065857, 227.1830768585205, 230.58315706253052, 233.98323726654053, 237.36666631698608, 240.75009536743164, 244.14242005348206, 247.53474473953247, 250.86126041412354, 254.1877760887146, 257.4853880405426, 260.7829999923706, 264.1864798069, 267.58995962142944, 270.84940338134766, 274.10884714126587, 277.40533542633057, 280.70182371139526, 284.08152532577515, 287.46122694015503, 290.75016355514526, 294.0391001701355, 297.2102301120758, 300.3813600540161, 304.0063388347626, 307.63131761550903, 311.4306900501251, 315.2300624847412, 318.4853916168213, 321.74072074890137, 325.38952803611755, 329.03833532333374, 332.67973017692566, 336.3211250305176, 339.6713788509369, 343.0216326713562, 346.5695741176605, 350.11751556396484, 353.74088501930237, 357.3642544746399, 361.24071168899536, 365.11716890335083, 368.7512195110321, 372.3852701187134, 376.22021555900574, 380.0551609992981, 383.6987202167511, 387.3422794342041, 390.7350058555603, 394.1277322769165, 397.5788609981537, 401.02998971939087, 404.5716087818146, 408.1132278442383, 411.4869601726532, 414.8606925010681, 418.2200870513916, 421.5794816017151, 425.099689245224, 428.6198968887329, 432.0894374847412, 435.5589780807495, 439.0501925945282, 442.5414071083069, 445.95774102211, 449.3740749359131, 452.7448139190674, 456.1155529022217, 459.5776951313019, 463.0398373603821, 466.4631607532501, 469.88648414611816, 473.3106346130371, 476.73478507995605, 480.0584292411804, 483.3820734024048, 486.8849995136261, 490.3879256248474, 493.8390805721283, 497.2902355194092, 500.6958384513855, 504.1014413833618, 507.47617864608765, 510.8509159088135, 514.2869343757629, 517.7229528427124, 521.1032269001007, 524.483500957489, 527.8771917819977, 531.2708826065063, 534.7062084674835, 538.1415343284607, 541.6223275661469, 545.103120803833, 548.6838405132294, 552.2645602226257, 555.752911567688, 559.2412629127502, 562.510808467865, 565.7803540229797, 569.3975734710693, 573.0147929191589, 576.5160932540894, 580.0173935890198, 583.3672318458557, 586.7170701026917, 590.0850851535797, 593.4531002044678, 597.0220687389374, 600.591037273407, 603.8629951477051, 607.1349530220032, 610.5285840034485, 613.9222149848938, 617.4533972740173, 620.9845795631409, 624.3476333618164, 627.710687160492, 630.9757013320923, 634.2407155036926, 637.8420970439911, 641.4434785842896, 645.0221807956696, 648.6008830070496, 651.7637372016907, 654.9265913963318, 658.5133860111237, 662.1001806259155, 665.6296539306641, 669.1591272354126, 670.9433901309967, 672.7276530265808]
[30.1975, 30.1975, 70.58, 70.58, 82.4725, 82.4725, 83.97, 83.97, 84.885, 84.885, 86.3175, 86.3175, 87.8775, 87.8775, 88.3875, 88.3875, 89.66, 89.66, 89.71, 89.71, 89.93, 89.93, 90.2625, 90.2625, 90.36, 90.36, 90.3575, 90.3575, 90.43, 90.43, 90.4175, 90.4175, 90.475, 90.475, 90.465, 90.465, 90.5475, 90.5475, 90.5975, 90.5975, 90.58, 90.58, 90.575, 90.575, 90.68, 90.68, 90.6875, 90.6875, 90.73, 90.73, 90.77, 90.77, 90.765, 90.765, 90.785, 90.785, 90.8275, 90.8275, 90.7975, 90.7975, 90.86, 90.86, 90.835, 90.835, 90.815, 90.815, 90.82, 90.82, 90.8275, 90.8275, 90.8375, 90.8375, 90.86, 90.86, 90.85, 90.85, 90.8475, 90.8475, 90.845, 90.845, 90.865, 90.865, 90.8425, 90.8425, 90.86, 90.86, 90.89, 90.89, 90.895, 90.895, 90.8925, 90.8925, 90.9, 90.9, 90.9125, 90.9125, 90.925, 90.925, 90.945, 90.945, 90.9325, 90.9325, 90.9475, 90.9475, 90.925, 90.925, 90.9125, 90.9125, 90.9375, 90.9375, 90.9425, 90.9425, 90.9425, 90.9425, 90.955, 90.955, 90.9175, 90.9175, 90.94, 90.94, 90.9475, 90.9475, 90.935, 90.935, 90.92, 90.92, 90.94, 90.94, 90.945, 90.945, 90.9475, 90.9475, 90.965, 90.965, 90.9675, 90.9675, 90.955, 90.955, 90.955, 90.955, 90.9475, 90.9475, 90.95, 90.95, 90.96, 90.96, 90.9625, 90.9625, 90.95, 90.95, 90.93, 90.93, 90.975, 90.975, 90.9625, 90.9625, 90.9775, 90.9775, 90.98, 90.98, 90.9875, 90.9875, 91.005, 91.005, 91.0, 91.0, 90.9925, 90.9925, 90.995, 90.995, 91.0025, 91.0025, 90.98, 90.98, 90.9875, 90.9875, 90.99, 90.99, 90.995, 90.995, 91.0075, 91.0075, 91.0175, 91.0175, 90.99, 90.99, 90.985, 90.985, 90.98, 90.98, 90.98, 90.98, 90.9975, 90.9975, 91.0, 91.0, 90.9775, 90.9775, 90.9825, 90.9825, 90.98, 90.98]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.518, Test loss: 1.923, Test accuracy: 65.89
Round   1, Train loss: 1.270, Test loss: 1.726, Test accuracy: 80.40
Round   2, Train loss: 1.220, Test loss: 1.696, Test accuracy: 81.54
Round   3, Train loss: 1.217, Test loss: 1.682, Test accuracy: 82.47
Round   4, Train loss: 1.202, Test loss: 1.676, Test accuracy: 82.77
Round   5, Train loss: 1.205, Test loss: 1.675, Test accuracy: 82.69
Round   6, Train loss: 1.194, Test loss: 1.673, Test accuracy: 82.82
Round   7, Train loss: 1.189, Test loss: 1.671, Test accuracy: 83.28
Round   8, Train loss: 1.197, Test loss: 1.669, Test accuracy: 83.56
Round   9, Train loss: 1.185, Test loss: 1.665, Test accuracy: 84.06
Round  10, Train loss: 1.192, Test loss: 1.664, Test accuracy: 84.19
Round  11, Train loss: 1.186, Test loss: 1.665, Test accuracy: 84.03
Round  12, Train loss: 1.180, Test loss: 1.664, Test accuracy: 84.02
Round  13, Train loss: 1.185, Test loss: 1.664, Test accuracy: 83.97
Round  14, Train loss: 1.178, Test loss: 1.665, Test accuracy: 83.93
Round  15, Train loss: 1.180, Test loss: 1.665, Test accuracy: 83.83
Round  16, Train loss: 1.181, Test loss: 1.665, Test accuracy: 83.84
Round  17, Train loss: 1.184, Test loss: 1.666, Test accuracy: 83.75
Round  18, Train loss: 1.183, Test loss: 1.668, Test accuracy: 83.61
Round  19, Train loss: 1.183, Test loss: 1.668, Test accuracy: 83.62
Round  20, Train loss: 1.179, Test loss: 1.668, Test accuracy: 83.58
Round  21, Train loss: 1.179, Test loss: 1.670, Test accuracy: 83.50
Round  22, Train loss: 1.177, Test loss: 1.670, Test accuracy: 83.44
Round  23, Train loss: 1.177, Test loss: 1.671, Test accuracy: 83.48
Round  24, Train loss: 1.179, Test loss: 1.671, Test accuracy: 83.31
Round  25, Train loss: 1.178, Test loss: 1.672, Test accuracy: 83.29
Round  26, Train loss: 1.178, Test loss: 1.674, Test accuracy: 83.19
Round  27, Train loss: 1.173, Test loss: 1.674, Test accuracy: 83.08
Round  28, Train loss: 1.176, Test loss: 1.675, Test accuracy: 83.00
Round  29, Train loss: 1.172, Test loss: 1.676, Test accuracy: 82.94
Round  30, Train loss: 1.174, Test loss: 1.677, Test accuracy: 82.89
Round  31, Train loss: 1.170, Test loss: 1.678, Test accuracy: 82.85
Round  32, Train loss: 1.177, Test loss: 1.679, Test accuracy: 82.74
Round  33, Train loss: 1.178, Test loss: 1.679, Test accuracy: 82.74
Round  34, Train loss: 1.176, Test loss: 1.680, Test accuracy: 82.71
Round  35, Train loss: 1.175, Test loss: 1.681, Test accuracy: 82.56
Round  36, Train loss: 1.174, Test loss: 1.681, Test accuracy: 82.56
Round  37, Train loss: 1.172, Test loss: 1.682, Test accuracy: 82.45
Round  38, Train loss: 1.174, Test loss: 1.684, Test accuracy: 82.35
Round  39, Train loss: 1.172, Test loss: 1.685, Test accuracy: 82.22
Round  40, Train loss: 1.176, Test loss: 1.686, Test accuracy: 82.20
Round  41, Train loss: 1.175, Test loss: 1.686, Test accuracy: 82.11
Round  42, Train loss: 1.172, Test loss: 1.687, Test accuracy: 82.05
Round  43, Train loss: 1.175, Test loss: 1.688, Test accuracy: 82.01
Round  44, Train loss: 1.172, Test loss: 1.689, Test accuracy: 81.89
Round  45, Train loss: 1.174, Test loss: 1.691, Test accuracy: 81.81
Round  46, Train loss: 1.172, Test loss: 1.692, Test accuracy: 81.73
Round  47, Train loss: 1.172, Test loss: 1.693, Test accuracy: 81.60
Round  48, Train loss: 1.174, Test loss: 1.694, Test accuracy: 81.60
Round  49, Train loss: 1.172, Test loss: 1.695, Test accuracy: 81.56
Round  50, Train loss: 1.173, Test loss: 1.695, Test accuracy: 81.49
Round  51, Train loss: 1.174, Test loss: 1.697, Test accuracy: 81.38
Round  52, Train loss: 1.171, Test loss: 1.697, Test accuracy: 81.39
Round  53, Train loss: 1.147, Test loss: 1.690, Test accuracy: 83.03
Round  54, Train loss: 1.126, Test loss: 1.683, Test accuracy: 83.89
Round  55, Train loss: 1.135, Test loss: 1.676, Test accuracy: 84.98
Round  56, Train loss: 1.108, Test loss: 1.674, Test accuracy: 85.06
Round  57, Train loss: 1.115, Test loss: 1.671, Test accuracy: 85.58
Round  58, Train loss: 1.118, Test loss: 1.665, Test accuracy: 86.31
Round  59, Train loss: 1.114, Test loss: 1.663, Test accuracy: 87.00
Round  60, Train loss: 1.110, Test loss: 1.663, Test accuracy: 87.36
Round  61, Train loss: 1.107, Test loss: 1.663, Test accuracy: 87.38
Round  62, Train loss: 1.106, Test loss: 1.664, Test accuracy: 87.35
Round  63, Train loss: 1.105, Test loss: 1.662, Test accuracy: 87.47
Round  64, Train loss: 1.107, Test loss: 1.663, Test accuracy: 87.36
Round  65, Train loss: 1.106, Test loss: 1.662, Test accuracy: 87.42
Round  66, Train loss: 1.105, Test loss: 1.663, Test accuracy: 87.33
Round  67, Train loss: 1.105, Test loss: 1.663, Test accuracy: 87.22
Round  68, Train loss: 1.105, Test loss: 1.664, Test accuracy: 87.17
Round  69, Train loss: 1.105, Test loss: 1.665, Test accuracy: 87.12
Round  70, Train loss: 1.105, Test loss: 1.666, Test accuracy: 86.97
Round  71, Train loss: 1.104, Test loss: 1.666, Test accuracy: 86.92
Round  72, Train loss: 1.104, Test loss: 1.666, Test accuracy: 86.85
Round  73, Train loss: 1.103, Test loss: 1.668, Test accuracy: 86.71
Round  74, Train loss: 1.102, Test loss: 1.668, Test accuracy: 86.61
Round  75, Train loss: 1.104, Test loss: 1.669, Test accuracy: 86.59
Round  76, Train loss: 1.104, Test loss: 1.669, Test accuracy: 86.53
Round  77, Train loss: 1.102, Test loss: 1.670, Test accuracy: 86.48
Round  78, Train loss: 1.103, Test loss: 1.670, Test accuracy: 86.51
Round  79, Train loss: 1.103, Test loss: 1.670, Test accuracy: 86.50
Round  80, Train loss: 1.103, Test loss: 1.670, Test accuracy: 86.51
Round  81, Train loss: 1.103, Test loss: 1.670, Test accuracy: 86.57
Round  82, Train loss: 1.102, Test loss: 1.671, Test accuracy: 86.50
Round  83, Train loss: 1.103, Test loss: 1.671, Test accuracy: 86.47
Round  84, Train loss: 1.103, Test loss: 1.672, Test accuracy: 86.34
Round  85, Train loss: 1.104, Test loss: 1.672, Test accuracy: 86.32
Round  86, Train loss: 1.103, Test loss: 1.673, Test accuracy: 86.23
Round  87, Train loss: 1.103, Test loss: 1.674, Test accuracy: 86.17
Round  88, Train loss: 1.102, Test loss: 1.674, Test accuracy: 86.18
Round  89, Train loss: 1.103, Test loss: 1.675, Test accuracy: 86.06
Round  90, Train loss: 1.102, Test loss: 1.676, Test accuracy: 85.94
Round  91, Train loss: 1.102, Test loss: 1.676, Test accuracy: 85.96
Round  92, Train loss: 1.103, Test loss: 1.676, Test accuracy: 85.85
Round  93, Train loss: 1.102, Test loss: 1.677, Test accuracy: 85.72
Round  94, Train loss: 1.103, Test loss: 1.678, Test accuracy: 85.67
Round  95, Train loss: 1.103, Test loss: 1.679, Test accuracy: 85.59
Round  96, Train loss: 1.102, Test loss: 1.678, Test accuracy: 85.64
Round  97, Train loss: 1.103, Test loss: 1.679, Test accuracy: 85.59
Round  98, Train loss: 1.103, Test loss: 1.680, Test accuracy: 85.43
Round  99, Train loss: 1.103, Test loss: 1.680, Test accuracy: 85.44
Final Round, Train loss: 1.102, Test loss: 1.683, Test accuracy: 85.32
Average accuracy final 10 rounds: 85.68275
5555.044305801392
[]
[65.89, 80.4025, 81.5375, 82.4725, 82.7725, 82.695, 82.8175, 83.285, 83.565, 84.065, 84.1875, 84.035, 84.015, 83.975, 83.9275, 83.825, 83.8375, 83.755, 83.615, 83.6175, 83.5775, 83.495, 83.445, 83.48, 83.3075, 83.2925, 83.1875, 83.08, 83.005, 82.9425, 82.885, 82.8525, 82.7375, 82.7375, 82.7125, 82.5575, 82.56, 82.45, 82.3525, 82.2175, 82.2, 82.11, 82.05, 82.0125, 81.8875, 81.8125, 81.73, 81.6025, 81.5975, 81.5575, 81.4875, 81.3825, 81.3875, 83.0275, 83.885, 84.9775, 85.06, 85.575, 86.3125, 87.005, 87.365, 87.3825, 87.35, 87.475, 87.365, 87.4175, 87.33, 87.2225, 87.1725, 87.1175, 86.97, 86.9225, 86.8475, 86.71, 86.61, 86.5875, 86.535, 86.485, 86.51, 86.5025, 86.5125, 86.5725, 86.505, 86.465, 86.34, 86.3175, 86.2275, 86.17, 86.18, 86.055, 85.945, 85.9575, 85.8525, 85.715, 85.665, 85.5925, 85.64, 85.5875, 85.4325, 85.44, 85.3225]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.293, Test loss: 2.292, Test accuracy: 23.37
Round   1, Train loss: 2.202, Test loss: 2.201, Test accuracy: 41.34
Round   2, Train loss: 1.973, Test loss: 2.103, Test accuracy: 50.65
Round   3, Train loss: 1.990, Test loss: 2.089, Test accuracy: 48.02
Round   4, Train loss: 1.822, Test loss: 2.082, Test accuracy: 44.56
Round   5, Train loss: 1.694, Test loss: 2.022, Test accuracy: 47.61
Round   6, Train loss: 1.623, Test loss: 1.989, Test accuracy: 48.75
Round   7, Train loss: 0.704, Test loss: 1.883, Test accuracy: 59.52
Round   8, Train loss: 1.362, Test loss: 1.836, Test accuracy: 64.96
Round   9, Train loss: 1.572, Test loss: 1.812, Test accuracy: 68.15
Round  10, Train loss: 1.771, Test loss: 1.792, Test accuracy: 70.09
Round  11, Train loss: 0.964, Test loss: 1.759, Test accuracy: 72.93
Round  12, Train loss: 1.623, Test loss: 1.757, Test accuracy: 73.19
Round  13, Train loss: 1.646, Test loss: 1.740, Test accuracy: 74.56
Round  14, Train loss: 1.411, Test loss: 1.733, Test accuracy: 74.59
Round  15, Train loss: 1.377, Test loss: 1.741, Test accuracy: 73.87
Round  16, Train loss: 1.499, Test loss: 1.721, Test accuracy: 75.79
Round  17, Train loss: 1.292, Test loss: 1.711, Test accuracy: 76.26
Round  18, Train loss: 1.583, Test loss: 1.711, Test accuracy: 76.59
Round  19, Train loss: 1.452, Test loss: 1.690, Test accuracy: 78.47
Round  20, Train loss: 1.364, Test loss: 1.687, Test accuracy: 78.76
Round  21, Train loss: 1.494, Test loss: 1.677, Test accuracy: 79.60
Round  22, Train loss: 1.353, Test loss: 1.676, Test accuracy: 79.74
Round  23, Train loss: 1.246, Test loss: 1.674, Test accuracy: 79.83
Round  24, Train loss: 1.279, Test loss: 1.668, Test accuracy: 80.38
Round  25, Train loss: 1.514, Test loss: 1.663, Test accuracy: 80.41
Round  26, Train loss: 1.173, Test loss: 1.662, Test accuracy: 80.49
Round  27, Train loss: 1.242, Test loss: 1.662, Test accuracy: 80.51
Round  28, Train loss: 1.276, Test loss: 1.655, Test accuracy: 81.21
Round  29, Train loss: 1.208, Test loss: 1.655, Test accuracy: 81.27
Round  30, Train loss: 1.073, Test loss: 1.648, Test accuracy: 81.89
Round  31, Train loss: 1.255, Test loss: 1.646, Test accuracy: 82.12
Round  32, Train loss: 1.147, Test loss: 1.642, Test accuracy: 82.51
Round  33, Train loss: 1.226, Test loss: 1.628, Test accuracy: 84.08
Round  34, Train loss: 1.180, Test loss: 1.619, Test accuracy: 85.01
Round  35, Train loss: 1.156, Test loss: 1.621, Test accuracy: 84.69
Round  36, Train loss: 1.115, Test loss: 1.625, Test accuracy: 84.24
Round  37, Train loss: 1.195, Test loss: 1.631, Test accuracy: 83.71
Round  38, Train loss: 1.181, Test loss: 1.639, Test accuracy: 82.63
Round  39, Train loss: 1.066, Test loss: 1.631, Test accuracy: 83.51
Round  40, Train loss: 1.034, Test loss: 1.635, Test accuracy: 83.07
Round  41, Train loss: 1.019, Test loss: 1.623, Test accuracy: 84.23
Round  42, Train loss: 0.934, Test loss: 1.619, Test accuracy: 84.60
Round  43, Train loss: 0.876, Test loss: 1.614, Test accuracy: 85.12
Round  44, Train loss: 1.030, Test loss: 1.613, Test accuracy: 85.18
Round  45, Train loss: 0.818, Test loss: 1.610, Test accuracy: 85.44
Round  46, Train loss: 0.743, Test loss: 1.609, Test accuracy: 85.54
Round  47, Train loss: 0.985, Test loss: 1.613, Test accuracy: 85.06
Round  48, Train loss: 1.029, Test loss: 1.609, Test accuracy: 85.55
Round  49, Train loss: 0.962, Test loss: 1.615, Test accuracy: 84.90
Round  50, Train loss: 0.988, Test loss: 1.625, Test accuracy: 83.94
Round  51, Train loss: 0.770, Test loss: 1.608, Test accuracy: 85.62
Round  52, Train loss: 0.885, Test loss: 1.596, Test accuracy: 86.76
Round  53, Train loss: 0.739, Test loss: 1.591, Test accuracy: 87.32
Round  54, Train loss: 0.963, Test loss: 1.594, Test accuracy: 87.00
Round  55, Train loss: 1.048, Test loss: 1.597, Test accuracy: 86.81
Round  56, Train loss: 0.803, Test loss: 1.595, Test accuracy: 87.07
Round  57, Train loss: 0.854, Test loss: 1.589, Test accuracy: 87.67
Round  58, Train loss: 0.765, Test loss: 1.585, Test accuracy: 88.08
Round  59, Train loss: 0.808, Test loss: 1.583, Test accuracy: 88.24
Round  60, Train loss: 1.000, Test loss: 1.583, Test accuracy: 88.32
Round  61, Train loss: 0.735, Test loss: 1.582, Test accuracy: 88.45
Round  62, Train loss: 0.786, Test loss: 1.584, Test accuracy: 88.09
Round  63, Train loss: 0.752, Test loss: 1.581, Test accuracy: 88.30
Round  64, Train loss: 0.814, Test loss: 1.579, Test accuracy: 88.57
Round  65, Train loss: 0.617, Test loss: 1.573, Test accuracy: 89.17
Round  66, Train loss: 0.641, Test loss: 1.573, Test accuracy: 89.20
Round  67, Train loss: 0.711, Test loss: 1.572, Test accuracy: 89.30
Round  68, Train loss: 0.900, Test loss: 1.574, Test accuracy: 89.19
Round  69, Train loss: 0.605, Test loss: 1.571, Test accuracy: 89.37
Round  70, Train loss: 0.654, Test loss: 1.569, Test accuracy: 89.58
Round  71, Train loss: 0.831, Test loss: 1.568, Test accuracy: 89.67
Round  72, Train loss: 0.784, Test loss: 1.569, Test accuracy: 89.58
Round  73, Train loss: 0.666, Test loss: 1.571, Test accuracy: 89.36
Round  74, Train loss: 0.735, Test loss: 1.568, Test accuracy: 89.74
Round  75, Train loss: 0.815, Test loss: 1.568, Test accuracy: 89.70
Round  76, Train loss: 0.739, Test loss: 1.568, Test accuracy: 89.65
Round  77, Train loss: 0.704, Test loss: 1.570, Test accuracy: 89.49
Round  78, Train loss: 0.720, Test loss: 1.570, Test accuracy: 89.37
Round  79, Train loss: 0.682, Test loss: 1.568, Test accuracy: 89.59
Round  80, Train loss: 0.652, Test loss: 1.567, Test accuracy: 89.74
Round  81, Train loss: 0.603, Test loss: 1.566, Test accuracy: 89.84
Round  82, Train loss: 0.818, Test loss: 1.567, Test accuracy: 89.75
Round  83, Train loss: 0.560, Test loss: 1.566, Test accuracy: 89.86
Round  84, Train loss: 0.580, Test loss: 1.566, Test accuracy: 89.86
Round  85, Train loss: 0.743, Test loss: 1.565, Test accuracy: 89.98
Round  86, Train loss: 0.680, Test loss: 1.566, Test accuracy: 89.86
Round  87, Train loss: 0.733, Test loss: 1.566, Test accuracy: 89.82
Round  88, Train loss: 0.738, Test loss: 1.566, Test accuracy: 89.91
Round  89, Train loss: 0.685, Test loss: 1.566, Test accuracy: 89.93
Round  90, Train loss: 0.623, Test loss: 1.566, Test accuracy: 89.90
Round  91, Train loss: 0.637, Test loss: 1.564, Test accuracy: 90.03
Round  92, Train loss: 0.732, Test loss: 1.565, Test accuracy: 89.93
Round  93, Train loss: 0.724, Test loss: 1.566, Test accuracy: 89.92
Round  94, Train loss: 0.691, Test loss: 1.563, Test accuracy: 90.22
Round  95, Train loss: 0.838, Test loss: 1.563, Test accuracy: 90.20
Round  96, Train loss: 0.588, Test loss: 1.564, Test accuracy: 90.09
Round  97, Train loss: 0.731, Test loss: 1.564, Test accuracy: 90.06
Round  98, Train loss: 0.718, Test loss: 1.565, Test accuracy: 89.96
Round  99, Train loss: 0.635, Test loss: 1.565, Test accuracy: 90.03
Final Round, Train loss: 1.507, Test loss: 1.550, Test accuracy: 91.66
Average accuracy final 10 rounds: 90.03305555555555
Average global accuracy final 10 rounds: 90.03305555555555
3829.7804656028748
[]
[23.372222222222224, 41.33611111111111, 50.65, 48.019444444444446, 44.56388888888889, 47.605555555555554, 48.74722222222222, 59.522222222222226, 64.95833333333333, 68.15277777777777, 70.08611111111111, 72.92777777777778, 73.18611111111112, 74.55555555555556, 74.58611111111111, 73.86944444444444, 75.78611111111111, 76.25555555555556, 76.59166666666667, 78.46666666666667, 78.76388888888889, 79.60277777777777, 79.74166666666666, 79.83055555555555, 80.375, 80.41111111111111, 80.49444444444444, 80.50833333333334, 81.21111111111111, 81.27222222222223, 81.88888888888889, 82.11944444444444, 82.5111111111111, 84.07777777777778, 85.0111111111111, 84.69166666666666, 84.24444444444444, 83.71111111111111, 82.63333333333334, 83.50555555555556, 83.06666666666666, 84.23333333333333, 84.6, 85.11944444444444, 85.18055555555556, 85.44444444444444, 85.53611111111111, 85.05833333333334, 85.55, 84.9, 83.94444444444444, 85.625, 86.76388888888889, 87.31944444444444, 87.0, 86.80833333333334, 87.06666666666666, 87.67222222222222, 88.07777777777778, 88.24444444444444, 88.31944444444444, 88.44722222222222, 88.09444444444445, 88.3, 88.56944444444444, 89.16944444444445, 89.19722222222222, 89.29722222222222, 89.18888888888888, 89.36666666666666, 89.575, 89.675, 89.58333333333333, 89.36111111111111, 89.74166666666666, 89.7, 89.65, 89.49444444444444, 89.36666666666666, 89.59444444444445, 89.74444444444444, 89.83611111111111, 89.75, 89.85555555555555, 89.8638888888889, 89.98333333333333, 89.86111111111111, 89.82222222222222, 89.90833333333333, 89.93333333333334, 89.9, 90.02777777777777, 89.93055555555556, 89.92222222222222, 90.21666666666667, 90.2, 90.09444444444445, 90.05833333333334, 89.95555555555555, 90.025, 91.65833333333333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.22
Round   0, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.22
Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.22
Round   1, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.22
Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.23
Round   2, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.28
Round   3, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.27
Round   3, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.27
Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.27
Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.26
Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.27
Round   5, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.30
Round   6, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.30
Round   6, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.32
Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.34
Round   7, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.31
Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.34
Round   8, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.33
Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.36
Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.39
Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.39
Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.41
Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.43
Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.50
Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.47
Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.52
Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.49
Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.59
Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.56
Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.68
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.61
Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.69
Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.66
Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.77
Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.70
Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.77
Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.75
Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.80
Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.90
Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.84
Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.94
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88
Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.02
Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.95
Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.08
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.01
Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.09
Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.05
Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.15
Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.10
Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.20
Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.17
Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.27
Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.19
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.31
Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.24
Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.34
Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.30
Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.46
Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.34
Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.49
Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.42
Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.52
Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.45
Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.52
Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.52
Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.60
Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.57
Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.63
Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.64
Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.71
Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.67
Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.75
Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.71
Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.79
Round  38, Train loss: 2.301, Test loss: 2.302, Test accuracy: 13.74
Round  38, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 13.81
Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.77
Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.82
Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.79
Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.86
Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.83
Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.92
Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.85
Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.91
Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.89
Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.98
Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.93
Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.99
Round  45, Train loss: 2.301, Test loss: 2.302, Test accuracy: 13.96
Round  45, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.03
Round  46, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.02
Round  46, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.06
Round  47, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.04
Round  47, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.09
Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.10
Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.16
Round  49, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.12
Round  49, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.16
Round  50, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.14
Round  50, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.16
Round  51, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.16
Round  51, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.25
Round  52, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.21
Round  52, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.28
Round  53, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.24
Round  53, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.35
Round  54, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.26
Round  54, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.36
Round  55, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.34
Round  55, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.47
Round  56, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.38
Round  56, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.47
Round  57, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.43
Round  57, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.50
Round  58, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.43
Round  58, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.53
Round  59, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.46
Round  59, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.54
Round  60, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.49
Round  60, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.58
Round  61, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.51
Round  61, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.57
Round  62, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.54
Round  62, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.60
Round  63, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.58
Round  63, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.71
Round  64, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.62
Round  64, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.67
Round  65, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.65
Round  65, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.68
Round  66, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.70
Round  66, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.74
Round  67, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.74
Round  67, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.84
Round  68, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.79
Round  68, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.93
Round  69, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.83
Round  69, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.96
Round  70, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.87
Round  70, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.96
Round  71, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.90
Round  71, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.04
Round  72, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.93
Round  72, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.05
Round  73, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.98
Round  73, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.11
Round  74, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.06
Round  74, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.20
Round  75, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.12
Round  75, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.23
Round  76, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.21
Round  76, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.29
Round  77, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.27
Round  77, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.37
Round  78, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.31
Round  78, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.51
Round  79, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.38
Round  79, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.54
Round  80, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.43
Round  80, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.57
Round  81, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.52
Round  81, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.69
Round  82, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.56
Round  82, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.70
Round  83, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.62
Round  83, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.74
Round  84, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.66
Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.75
Round  85, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.70
Round  85, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.80
Round  86, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.73
Round  86, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.82
Round  87, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.77
Round  87, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.90
Round  88, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.81
Round  88, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.91
Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.85
Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.96
Round  90, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.89
Round  90, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.97
Round  91, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.93
Round  91, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.98
Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.96
Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.01
Round  93, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.96
Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.01
Round  94, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.98
Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.09
Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.99
Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.11
Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.08
Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.16/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.14
Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.23
Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.17
Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.29
Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.20
Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.38
Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.45
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.38
Average accuracy final 10 rounds: 16.03025 

Average global accuracy final 10 rounds: 16.12175 

5094.493855953217
[4.508702039718628, 8.581531763076782, 13.067278861999512, 17.27228546142578, 21.58528447151184, 25.75183129310608, 29.976293802261353, 34.3009672164917, 38.68493700027466, 42.94258403778076, 47.2479989528656, 51.52560877799988, 55.85429239273071, 60.164915800094604, 64.58789658546448, 68.80404853820801, 72.85351490974426, 76.94397473335266, 81.38966369628906, 85.81671261787415, 90.34549260139465, 94.91104745864868, 99.00733804702759, 103.09248042106628, 107.18189787864685, 111.14309310913086, 115.24141788482666, 119.64856743812561, 123.79462933540344, 127.95480966567993, 132.18062233924866, 136.1995813846588, 140.3780345916748, 144.70395708084106, 148.81972765922546, 152.8419497013092, 156.9302704334259, 161.1497049331665, 165.4647204875946, 169.26802945137024, 173.00830125808716, 176.90457344055176, 180.82667136192322, 184.56640195846558, 188.29823899269104, 192.27606415748596, 196.21294236183167, 199.95066142082214, 204.03905487060547, 208.11991715431213, 212.0632824897766, 215.80443334579468, 219.89651083946228, 223.9928011894226, 227.9270372390747, 231.6399130821228, 235.52163743972778, 239.46585512161255, 243.2317762374878, 246.97271966934204, 250.8281409740448, 254.69478249549866, 258.2542419433594, 262.00812315940857, 265.7014639377594, 269.69799065589905, 273.82115292549133, 278.1881754398346, 282.56011724472046, 286.5347583293915, 290.68982338905334, 294.9829041957855, 299.3552203178406, 303.7272837162018, 308.0758521556854, 312.3375415802002, 316.65093636512756, 320.8980255126953, 325.2164657115936, 329.5521056652069, 334.0722541809082, 338.5145502090454, 342.8997564315796, 347.1471815109253, 351.3618128299713, 355.80442070961, 360.24254393577576, 364.70861625671387, 369.109002828598, 373.3504271507263, 377.7874619960785, 382.25958824157715, 386.6551821231842, 391.09562969207764, 395.42405891418457, 399.85498237609863, 404.33044505119324, 408.8184254169464, 413.2162654399872, 417.5727093219757, 419.82385754585266]
[12.225, 12.2175, 12.2325, 12.27, 12.2675, 12.2675, 12.3, 12.335, 12.34, 12.3625, 12.395, 12.4325, 12.4675, 12.4925, 12.565, 12.605, 12.66, 12.7, 12.7475, 12.8, 12.84, 12.8825, 12.95, 13.0125, 13.0475, 13.1, 13.1675, 13.1925, 13.2375, 13.2975, 13.34, 13.4175, 13.45, 13.5175, 13.57, 13.6425, 13.67, 13.71, 13.74, 13.77, 13.795, 13.8275, 13.8525, 13.8875, 13.93, 13.955, 14.0175, 14.0425, 14.095, 14.12, 14.14, 14.1625, 14.215, 14.2375, 14.26, 14.34, 14.3825, 14.4325, 14.4275, 14.46, 14.4925, 14.51, 14.54, 14.58, 14.62, 14.65, 14.6975, 14.7425, 14.795, 14.8275, 14.87, 14.9025, 14.93, 14.9825, 15.065, 15.12, 15.2125, 15.27, 15.3125, 15.3825, 15.4325, 15.52, 15.565, 15.625, 15.66, 15.6975, 15.7275, 15.77, 15.81, 15.8525, 15.8875, 15.93, 15.965, 15.9625, 15.9825, 15.99, 16.0775, 16.1375, 16.1675, 16.2025, 16.4475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.220, Test accuracy: 42.68
Round   1, Train loss: 2.199, Test loss: 1.751, Test accuracy: 73.08
Round   2, Train loss: 1.724, Test loss: 1.598, Test accuracy: 88.33
Round   3, Train loss: 1.620, Test loss: 1.568, Test accuracy: 90.23
Round   4, Train loss: 1.591, Test loss: 1.555, Test accuracy: 91.16
Round   5, Train loss: 1.564, Test loss: 1.546, Test accuracy: 92.02
Round   6, Train loss: 1.555, Test loss: 1.540, Test accuracy: 92.47
Round   7, Train loss: 1.541, Test loss: 1.537, Test accuracy: 92.91
Round   8, Train loss: 1.534, Test loss: 1.532, Test accuracy: 93.20
Round   9, Train loss: 1.533, Test loss: 1.528, Test accuracy: 93.52
Round  10, Train loss: 1.527, Test loss: 1.526, Test accuracy: 93.64
Round  11, Train loss: 1.519, Test loss: 1.523, Test accuracy: 93.96
Round  12, Train loss: 1.514, Test loss: 1.522, Test accuracy: 94.11
Round  13, Train loss: 1.513, Test loss: 1.520, Test accuracy: 94.34
Round  14, Train loss: 1.507, Test loss: 1.519, Test accuracy: 94.31
Round  15, Train loss: 1.507, Test loss: 1.518, Test accuracy: 94.64
Round  16, Train loss: 1.505, Test loss: 1.516, Test accuracy: 94.73
Round  17, Train loss: 1.510, Test loss: 1.513, Test accuracy: 95.02
Round  18, Train loss: 1.500, Test loss: 1.513, Test accuracy: 95.01
Round  19, Train loss: 1.500, Test loss: 1.512, Test accuracy: 95.11
Round  20, Train loss: 1.498, Test loss: 1.509, Test accuracy: 95.39
Round  21, Train loss: 1.496, Test loss: 1.508, Test accuracy: 95.33
Round  22, Train loss: 1.492, Test loss: 1.508, Test accuracy: 95.47
Round  23, Train loss: 1.492, Test loss: 1.507, Test accuracy: 95.53
Round  24, Train loss: 1.492, Test loss: 1.506, Test accuracy: 95.54
Round  25, Train loss: 1.488, Test loss: 1.506, Test accuracy: 95.58
Round  26, Train loss: 1.489, Test loss: 1.505, Test accuracy: 95.73
Round  27, Train loss: 1.488, Test loss: 1.504, Test accuracy: 95.89
Round  28, Train loss: 1.489, Test loss: 1.503, Test accuracy: 95.96
Round  29, Train loss: 1.487, Test loss: 1.503, Test accuracy: 96.12
Round  30, Train loss: 1.487, Test loss: 1.502, Test accuracy: 96.06
Round  31, Train loss: 1.485, Test loss: 1.502, Test accuracy: 96.05
Round  32, Train loss: 1.486, Test loss: 1.502, Test accuracy: 96.09
Round  33, Train loss: 1.481, Test loss: 1.502, Test accuracy: 96.15
Round  34, Train loss: 1.482, Test loss: 1.501, Test accuracy: 96.22
Round  35, Train loss: 1.480, Test loss: 1.501, Test accuracy: 96.19
Round  36, Train loss: 1.479, Test loss: 1.500, Test accuracy: 96.22
Round  37, Train loss: 1.479, Test loss: 1.500, Test accuracy: 96.29
Round  38, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.13
Round  39, Train loss: 1.477, Test loss: 1.500, Test accuracy: 96.23
Round  40, Train loss: 1.478, Test loss: 1.500, Test accuracy: 96.29
Round  41, Train loss: 1.481, Test loss: 1.499, Test accuracy: 96.29
Round  42, Train loss: 1.476, Test loss: 1.499, Test accuracy: 96.36
Round  43, Train loss: 1.477, Test loss: 1.499, Test accuracy: 96.25
Round  44, Train loss: 1.478, Test loss: 1.499, Test accuracy: 96.39
Round  45, Train loss: 1.477, Test loss: 1.498, Test accuracy: 96.40
Round  46, Train loss: 1.476, Test loss: 1.498, Test accuracy: 96.44
Round  47, Train loss: 1.478, Test loss: 1.498, Test accuracy: 96.54
Round  48, Train loss: 1.476, Test loss: 1.497, Test accuracy: 96.53
Round  49, Train loss: 1.477, Test loss: 1.497, Test accuracy: 96.62
Round  50, Train loss: 1.474, Test loss: 1.497, Test accuracy: 96.55
Round  51, Train loss: 1.475, Test loss: 1.497, Test accuracy: 96.58
Round  52, Train loss: 1.475, Test loss: 1.496, Test accuracy: 96.69
Round  53, Train loss: 1.473, Test loss: 1.496, Test accuracy: 96.74
Round  54, Train loss: 1.473, Test loss: 1.496, Test accuracy: 96.73
Round  55, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.70
Round  56, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.72
Round  57, Train loss: 1.473, Test loss: 1.496, Test accuracy: 96.69
Round  58, Train loss: 1.472, Test loss: 1.496, Test accuracy: 96.62
Round  59, Train loss: 1.471, Test loss: 1.496, Test accuracy: 96.65
Round  60, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.73
Round  61, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.74
Round  62, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.67
Round  63, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.78
Round  64, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.80
Round  65, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.71
Round  66, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.83
Round  67, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.84
Round  68, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.83
Round  69, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.90
Round  70, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.84
Round  71, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.86
Round  72, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.82
Round  73, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.76
Round  74, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.81
Round  75, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.78
Round  76, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.81
Round  77, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.81
Round  78, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.83
Round  79, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.77
Round  80, Train loss: 1.471, Test loss: 1.494, Test accuracy: 96.85
Round  81, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.89
Round  82, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.87
Round  83, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.87
Round  84, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.73
Round  85, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.87
Round  86, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.91
Round  87, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.89
Round  88, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.91
Round  89, Train loss: 1.471, Test loss: 1.494, Test accuracy: 96.93
Round  90, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.85
Round  91, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.88
Round  92, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.92
Round  93, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.95
Round  94, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.92
Round  95, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.98
Round  96, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.89
Round  97, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.85
Round  98, Train loss: 1.471, Test loss: 1.493, Test accuracy: 96.97
Round  99, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.95
Final Round, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.99
Average accuracy final 10 rounds: 96.91599999999998
7568.788421154022
[10.400645732879639, 21.033329725265503, 31.60121726989746, 42.01387143135071, 52.216978311538696, 62.703754901885986, 72.99411201477051, 83.4039409160614, 93.90336441993713, 104.32038068771362, 114.65614771842957, 125.17689943313599, 135.53233003616333, 145.90452766418457, 156.3507022857666, 166.75912761688232, 177.24327397346497, 188.05440163612366, 198.82538533210754, 209.84946513175964, 220.93943548202515, 231.96623516082764, 243.14920473098755, 254.17529845237732, 265.07750177383423, 276.03759360313416, 287.0703778266907, 298.03928685188293, 308.7881922721863, 319.5102586746216, 330.32980370521545, 341.2198407649994, 352.28675270080566, 363.1371765136719, 373.97079157829285, 384.8643329143524, 395.8770225048065, 406.83095622062683, 417.6471893787384, 428.7022051811218, 439.7367510795593, 450.4644477367401, 461.4314908981323, 472.4893436431885, 483.32436752319336, 494.2890899181366, 505.2567648887634, 516.1287045478821, 527.0915229320526, 538.0130705833435, 548.9820890426636, 559.8215565681458, 570.7973005771637, 581.8587362766266, 592.7048270702362, 603.7256481647491, 614.7435595989227, 625.8616857528687, 636.8058354854584, 647.5802092552185, 658.36354804039, 668.8659517765045, 679.3173944950104, 689.0, 698.8709044456482, 708.8293833732605, 718.8767304420471, 729.003714799881, 738.900191783905, 748.8432967662811, 758.8224036693573, 768.2897372245789, 778.024665594101, 787.7561030387878, 797.4743659496307, 807.0979735851288, 816.7138469219208, 826.3242802619934, 835.9268169403076, 845.5569915771484, 855.1402578353882, 864.8057811260223, 874.4786920547485, 884.0901472568512, 893.7070627212524, 903.5717403888702, 913.101461648941, 922.7585093975067, 932.5826890468597, 942.3229472637177, 952.0033786296844, 961.7735786437988, 971.4976208209991, 981.1675295829773, 990.9303476810455, 1000.4617223739624, 1010.0297510623932, 1019.5969262123108, 1029.0900444984436, 1038.6316142082214, 1041.0613713264465]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[42.68, 73.085, 88.3325, 90.2325, 91.1575, 92.0225, 92.47, 92.9075, 93.205, 93.5225, 93.645, 93.96, 94.1075, 94.3375, 94.31, 94.635, 94.7325, 95.0225, 95.0075, 95.105, 95.3925, 95.325, 95.4675, 95.535, 95.5425, 95.575, 95.7275, 95.8925, 95.9575, 96.1175, 96.0575, 96.045, 96.0875, 96.1525, 96.225, 96.1875, 96.2175, 96.2875, 96.1325, 96.2275, 96.2875, 96.29, 96.3575, 96.25, 96.395, 96.3975, 96.445, 96.54, 96.53, 96.6175, 96.55, 96.5825, 96.685, 96.7425, 96.735, 96.7, 96.725, 96.6875, 96.625, 96.6525, 96.73, 96.7425, 96.6725, 96.7775, 96.795, 96.7075, 96.8275, 96.84, 96.83, 96.8975, 96.845, 96.855, 96.8225, 96.7575, 96.8125, 96.7825, 96.81, 96.815, 96.8325, 96.7725, 96.85, 96.89, 96.8725, 96.8725, 96.735, 96.8675, 96.905, 96.8875, 96.91, 96.9325, 96.8525, 96.88, 96.92, 96.9475, 96.925, 96.98, 96.885, 96.8475, 96.9675, 96.955, 96.99]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.302, Test accuracy: 9.42
Round   1, Train loss: 2.301, Test loss: 2.300, Test accuracy: 9.47
Round   2, Train loss: 2.300, Test loss: 2.299, Test accuracy: 9.57
Round   3, Train loss: 2.296, Test loss: 2.296, Test accuracy: 9.90
Round   4, Train loss: 2.295, Test loss: 2.293, Test accuracy: 10.27
Round   5, Train loss: 2.290, Test loss: 2.288, Test accuracy: 10.42
Round   6, Train loss: 2.280, Test loss: 2.276, Test accuracy: 10.43
Round   7, Train loss: 2.258, Test loss: 2.257, Test accuracy: 13.57
Round   8, Train loss: 2.239, Test loss: 2.245, Test accuracy: 27.52
Round   9, Train loss: 2.234, Test loss: 2.235, Test accuracy: 38.55
Round  10, Train loss: 2.222, Test loss: 2.225, Test accuracy: 46.30
Round  11, Train loss: 2.214, Test loss: 2.214, Test accuracy: 51.52
Round  12, Train loss: 2.205, Test loss: 2.198, Test accuracy: 55.35
Round  13, Train loss: 2.180, Test loss: 2.175, Test accuracy: 58.42
Round  14, Train loss: 2.135, Test loss: 2.123, Test accuracy: 56.93
Round  15, Train loss: 2.066, Test loss: 2.051, Test accuracy: 57.65
Round  16, Train loss: 1.996, Test loss: 1.996, Test accuracy: 59.63
Round  17, Train loss: 1.961, Test loss: 1.946, Test accuracy: 62.00
Round  18, Train loss: 1.919, Test loss: 1.910, Test accuracy: 63.97
Round  19, Train loss: 1.881, Test loss: 1.880, Test accuracy: 66.40
Round  20, Train loss: 1.858, Test loss: 1.851, Test accuracy: 68.82
Round  21, Train loss: 1.826, Test loss: 1.828, Test accuracy: 70.38
Round  22, Train loss: 1.816, Test loss: 1.807, Test accuracy: 71.88
Round  23, Train loss: 1.812, Test loss: 1.788, Test accuracy: 72.88
Round  24, Train loss: 1.769, Test loss: 1.774, Test accuracy: 74.25
Round  25, Train loss: 1.776, Test loss: 1.761, Test accuracy: 75.62
Round  26, Train loss: 1.752, Test loss: 1.751, Test accuracy: 76.82
Round  27, Train loss: 1.752, Test loss: 1.738, Test accuracy: 78.18
Round  28, Train loss: 1.723, Test loss: 1.728, Test accuracy: 79.17
Round  29, Train loss: 1.721, Test loss: 1.721, Test accuracy: 80.27
Round  30, Train loss: 1.725, Test loss: 1.712, Test accuracy: 80.53
Round  31, Train loss: 1.696, Test loss: 1.706, Test accuracy: 80.77
Round  32, Train loss: 1.686, Test loss: 1.700, Test accuracy: 81.20
Round  33, Train loss: 1.689, Test loss: 1.694, Test accuracy: 81.57
Round  34, Train loss: 1.684, Test loss: 1.690, Test accuracy: 81.80
Round  35, Train loss: 1.690, Test loss: 1.685, Test accuracy: 82.07
Round  36, Train loss: 1.659, Test loss: 1.681, Test accuracy: 82.40
Round  37, Train loss: 1.660, Test loss: 1.677, Test accuracy: 82.78
Round  38, Train loss: 1.655, Test loss: 1.669, Test accuracy: 83.87
Round  39, Train loss: 1.654, Test loss: 1.662, Test accuracy: 84.85
Round  40, Train loss: 1.648, Test loss: 1.654, Test accuracy: 85.73
Round  41, Train loss: 1.632, Test loss: 1.645, Test accuracy: 86.60
Round  42, Train loss: 1.616, Test loss: 1.641, Test accuracy: 87.03
Round  43, Train loss: 1.611, Test loss: 1.631, Test accuracy: 88.57
Round  44, Train loss: 1.607, Test loss: 1.625, Test accuracy: 88.98
Round  45, Train loss: 1.606, Test loss: 1.617, Test accuracy: 89.82
Round  46, Train loss: 1.595, Test loss: 1.612, Test accuracy: 90.08
Round  47, Train loss: 1.592, Test loss: 1.608, Test accuracy: 90.23
Round  48, Train loss: 1.585, Test loss: 1.605, Test accuracy: 90.30
Round  49, Train loss: 1.585, Test loss: 1.600, Test accuracy: 90.63
Round  50, Train loss: 1.576, Test loss: 1.597, Test accuracy: 90.80
Round  51, Train loss: 1.573, Test loss: 1.595, Test accuracy: 90.83
Round  52, Train loss: 1.585, Test loss: 1.591, Test accuracy: 91.12
Round  53, Train loss: 1.577, Test loss: 1.589, Test accuracy: 91.27
Round  54, Train loss: 1.575, Test loss: 1.587, Test accuracy: 91.25
Round  55, Train loss: 1.559, Test loss: 1.585, Test accuracy: 91.33
Round  56, Train loss: 1.562, Test loss: 1.583, Test accuracy: 91.58
Round  57, Train loss: 1.566, Test loss: 1.582, Test accuracy: 91.82
Round  58, Train loss: 1.553, Test loss: 1.581, Test accuracy: 91.77
Round  59, Train loss: 1.555, Test loss: 1.579, Test accuracy: 91.85
Round  60, Train loss: 1.553, Test loss: 1.577, Test accuracy: 91.85
Round  61, Train loss: 1.549, Test loss: 1.575, Test accuracy: 91.92
Round  62, Train loss: 1.555, Test loss: 1.574, Test accuracy: 92.17
Round  63, Train loss: 1.545, Test loss: 1.572, Test accuracy: 92.20
Round  64, Train loss: 1.555, Test loss: 1.571, Test accuracy: 92.28
Round  65, Train loss: 1.539, Test loss: 1.570, Test accuracy: 92.37
Round  66, Train loss: 1.542, Test loss: 1.569, Test accuracy: 92.57
Round  67, Train loss: 1.541, Test loss: 1.568, Test accuracy: 92.48
Round  68, Train loss: 1.533, Test loss: 1.567, Test accuracy: 92.65
Round  69, Train loss: 1.541, Test loss: 1.567, Test accuracy: 92.65
Round  70, Train loss: 1.539, Test loss: 1.566, Test accuracy: 92.70
Round  71, Train loss: 1.533, Test loss: 1.564, Test accuracy: 92.82
Round  72, Train loss: 1.543, Test loss: 1.563, Test accuracy: 92.93
Round  73, Train loss: 1.537, Test loss: 1.563, Test accuracy: 92.88
Round  74, Train loss: 1.532, Test loss: 1.562, Test accuracy: 92.93
Round  75, Train loss: 1.524, Test loss: 1.562, Test accuracy: 92.98
Round  76, Train loss: 1.528, Test loss: 1.560, Test accuracy: 93.03
Round  77, Train loss: 1.528, Test loss: 1.560, Test accuracy: 92.97
Round  78, Train loss: 1.532, Test loss: 1.559, Test accuracy: 93.07
Round  79, Train loss: 1.518, Test loss: 1.559, Test accuracy: 93.00
Round  80, Train loss: 1.524, Test loss: 1.557, Test accuracy: 93.30
Round  81, Train loss: 1.518, Test loss: 1.556, Test accuracy: 93.20
Round  82, Train loss: 1.530, Test loss: 1.556, Test accuracy: 93.32
Round  83, Train loss: 1.517, Test loss: 1.555, Test accuracy: 93.33
Round  84, Train loss: 1.522, Test loss: 1.554, Test accuracy: 93.37
Round  85, Train loss: 1.515, Test loss: 1.555, Test accuracy: 93.28
Round  86, Train loss: 1.521, Test loss: 1.554, Test accuracy: 93.47
Round  87, Train loss: 1.516, Test loss: 1.553, Test accuracy: 93.48
Round  88, Train loss: 1.521, Test loss: 1.552, Test accuracy: 93.45
Round  89, Train loss: 1.523, Test loss: 1.551, Test accuracy: 93.48
Round  90, Train loss: 1.523, Test loss: 1.551, Test accuracy: 93.48
Round  91, Train loss: 1.513, Test loss: 1.551, Test accuracy: 93.45
Round  92, Train loss: 1.517, Test loss: 1.550, Test accuracy: 93.45
Round  93, Train loss: 1.512, Test loss: 1.549, Test accuracy: 93.60
Round  94, Train loss: 1.515, Test loss: 1.549, Test accuracy: 93.52/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.504, Test loss: 1.548, Test accuracy: 93.63
Round  96, Train loss: 1.514, Test loss: 1.548, Test accuracy: 93.77
Round  97, Train loss: 1.513, Test loss: 1.547, Test accuracy: 93.77
Round  98, Train loss: 1.513, Test loss: 1.547, Test accuracy: 93.83
Round  99, Train loss: 1.516, Test loss: 1.547, Test accuracy: 93.83
Final Round, Train loss: 1.499, Test loss: 1.545, Test accuracy: 93.78
Average accuracy final 10 rounds: 93.63333333333333
676.8124577999115
[0.9251487255096436, 1.741847276687622, 2.527006149291992, 3.3106729984283447, 4.083233594894409, 4.91316294670105, 5.703510761260986, 6.46515154838562, 7.252104759216309, 8.02939248085022, 8.79038119316101, 9.546809911727905, 10.270069599151611, 11.020633935928345, 11.776048183441162, 12.533546924591064, 13.294961929321289, 14.053802490234375, 14.839960098266602, 15.597003698348999, 16.364869594573975, 17.129474878311157, 17.896178483963013, 18.65776753425598, 19.433483839035034, 20.18592143058777, 20.91315770149231, 21.665785312652588, 22.426446199417114, 23.183331727981567, 23.948986768722534, 24.723422288894653, 25.48666763305664, 26.247647762298584, 26.999661684036255, 27.75613808631897, 28.490603923797607, 29.233840703964233, 29.97525668144226, 30.737850427627563, 31.51589035987854, 32.27764582633972, 33.03347373008728, 33.8036527633667, 34.55300283432007, 35.317158699035645, 36.089603662490845, 36.84874153137207, 37.60018849372864, 38.37174940109253, 39.110379695892334, 39.84114098548889, 40.56570053100586, 41.31802201271057, 42.07544255256653, 42.84369087219238, 43.61069869995117, 44.348705768585205, 45.09496545791626, 45.83591032028198, 46.58703017234802, 47.333370208740234, 48.07592296600342, 48.845561265945435, 49.619220495224, 50.345619916915894, 51.08383536338806, 51.822569608688354, 52.58634376525879, 53.34138321876526, 54.107178926467896, 54.873284339904785, 55.623088121414185, 56.367921113967896, 57.12542724609375, 57.863396644592285, 58.60878300666809, 59.36707019805908, 60.112465381622314, 60.8577663898468, 61.609044313430786, 62.36127591133118, 63.10583806037903, 63.81950497627258, 64.54864573478699, 65.29154086112976, 66.03714036941528, 66.7812249660492, 67.46146988868713, 68.16230797767639, 68.85974335670471, 69.56741166114807, 70.2491238117218, 70.95781540870667, 71.6785397529602, 72.39885973930359, 73.10851573944092, 73.7930600643158, 74.49125266075134, 75.19414854049683, 76.29013276100159]
[9.416666666666666, 9.466666666666667, 9.566666666666666, 9.9, 10.266666666666667, 10.416666666666666, 10.433333333333334, 13.566666666666666, 27.516666666666666, 38.55, 46.3, 51.516666666666666, 55.35, 58.416666666666664, 56.93333333333333, 57.65, 59.63333333333333, 62.0, 63.96666666666667, 66.4, 68.81666666666666, 70.38333333333334, 71.88333333333334, 72.88333333333334, 74.25, 75.61666666666666, 76.81666666666666, 78.18333333333334, 79.16666666666667, 80.26666666666667, 80.53333333333333, 80.76666666666667, 81.2, 81.56666666666666, 81.8, 82.06666666666666, 82.4, 82.78333333333333, 83.86666666666666, 84.85, 85.73333333333333, 86.6, 87.03333333333333, 88.56666666666666, 88.98333333333333, 89.81666666666666, 90.08333333333333, 90.23333333333333, 90.3, 90.63333333333334, 90.8, 90.83333333333333, 91.11666666666666, 91.26666666666667, 91.25, 91.33333333333333, 91.58333333333333, 91.81666666666666, 91.76666666666667, 91.85, 91.85, 91.91666666666667, 92.16666666666667, 92.2, 92.28333333333333, 92.36666666666666, 92.56666666666666, 92.48333333333333, 92.65, 92.65, 92.7, 92.81666666666666, 92.93333333333334, 92.88333333333334, 92.93333333333334, 92.98333333333333, 93.03333333333333, 92.96666666666667, 93.06666666666666, 93.0, 93.3, 93.2, 93.31666666666666, 93.33333333333333, 93.36666666666666, 93.28333333333333, 93.46666666666667, 93.48333333333333, 93.45, 93.48333333333333, 93.48333333333333, 93.45, 93.45, 93.6, 93.51666666666667, 93.63333333333334, 93.76666666666667, 93.76666666666667, 93.83333333333333, 93.83333333333333, 93.78333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.322, Test loss: 2.301, Test accuracy: 12.83
Round   1, Train loss: 2.318, Test loss: 2.299, Test accuracy: 18.13
Round   2, Train loss: 2.313, Test loss: 2.296, Test accuracy: 20.57
Round   3, Train loss: 2.307, Test loss: 2.291, Test accuracy: 18.18
Round   4, Train loss: 2.300, Test loss: 2.285, Test accuracy: 21.82
Round   5, Train loss: 2.291, Test loss: 2.275, Test accuracy: 25.78
Round   6, Train loss: 2.273, Test loss: 2.255, Test accuracy: 27.32
Round   7, Train loss: 2.256, Test loss: 2.229, Test accuracy: 39.85
Round   8, Train loss: 2.220, Test loss: 2.186, Test accuracy: 44.65
Round   9, Train loss: 2.160, Test loss: 2.125, Test accuracy: 46.52
Round  10, Train loss: 2.089, Test loss: 2.080, Test accuracy: 52.83
Round  11, Train loss: 2.078, Test loss: 2.039, Test accuracy: 56.82
Round  12, Train loss: 2.038, Test loss: 2.007, Test accuracy: 61.00
Round  13, Train loss: 2.019, Test loss: 1.973, Test accuracy: 63.15
Round  14, Train loss: 1.962, Test loss: 1.945, Test accuracy: 65.48
Round  15, Train loss: 1.946, Test loss: 1.919, Test accuracy: 66.55
Round  16, Train loss: 1.922, Test loss: 1.897, Test accuracy: 67.68
Round  17, Train loss: 1.927, Test loss: 1.882, Test accuracy: 68.13
Round  18, Train loss: 1.902, Test loss: 1.864, Test accuracy: 69.52
Round  19, Train loss: 1.884, Test loss: 1.853, Test accuracy: 70.15
Round  20, Train loss: 1.861, Test loss: 1.840, Test accuracy: 71.45
Round  21, Train loss: 1.860, Test loss: 1.835, Test accuracy: 72.10
Round  22, Train loss: 1.860, Test loss: 1.821, Test accuracy: 73.02
Round  23, Train loss: 1.835, Test loss: 1.811, Test accuracy: 73.52
Round  24, Train loss: 1.827, Test loss: 1.805, Test accuracy: 74.45
Round  25, Train loss: 1.813, Test loss: 1.798, Test accuracy: 74.70
Round  26, Train loss: 1.819, Test loss: 1.785, Test accuracy: 75.68
Round  27, Train loss: 1.812, Test loss: 1.776, Test accuracy: 76.67
Round  28, Train loss: 1.778, Test loss: 1.769, Test accuracy: 78.23
Round  29, Train loss: 1.772, Test loss: 1.760, Test accuracy: 79.38
Round  30, Train loss: 1.783, Test loss: 1.750, Test accuracy: 80.20
Round  31, Train loss: 1.760, Test loss: 1.746, Test accuracy: 80.83
Round  32, Train loss: 1.752, Test loss: 1.739, Test accuracy: 81.18
Round  33, Train loss: 1.763, Test loss: 1.732, Test accuracy: 81.72
Round  34, Train loss: 1.738, Test loss: 1.728, Test accuracy: 82.15
Round  35, Train loss: 1.738, Test loss: 1.722, Test accuracy: 82.50
Round  36, Train loss: 1.739, Test loss: 1.717, Test accuracy: 82.72
Round  37, Train loss: 1.729, Test loss: 1.715, Test accuracy: 83.02
Round  38, Train loss: 1.727, Test loss: 1.711, Test accuracy: 83.17
Round  39, Train loss: 1.709, Test loss: 1.706, Test accuracy: 83.37
Round  40, Train loss: 1.709, Test loss: 1.704, Test accuracy: 83.45
Round  41, Train loss: 1.713, Test loss: 1.701, Test accuracy: 83.68
Round  42, Train loss: 1.713, Test loss: 1.694, Test accuracy: 83.73
Round  43, Train loss: 1.718, Test loss: 1.692, Test accuracy: 83.78
Round  44, Train loss: 1.722, Test loss: 1.684, Test accuracy: 83.88
Round  45, Train loss: 1.694, Test loss: 1.685, Test accuracy: 84.15
Round  46, Train loss: 1.727, Test loss: 1.677, Test accuracy: 84.33
Round  47, Train loss: 1.679, Test loss: 1.681, Test accuracy: 84.37
Round  48, Train loss: 1.698, Test loss: 1.676, Test accuracy: 84.42
Round  49, Train loss: 1.701, Test loss: 1.671, Test accuracy: 84.47
Round  50, Train loss: 1.681, Test loss: 1.670, Test accuracy: 84.65
Round  51, Train loss: 1.682, Test loss: 1.669, Test accuracy: 84.57
Round  52, Train loss: 1.674, Test loss: 1.668, Test accuracy: 84.72
Round  53, Train loss: 1.670, Test loss: 1.667, Test accuracy: 84.80
Round  54, Train loss: 1.689, Test loss: 1.662, Test accuracy: 85.23
Round  55, Train loss: 1.676, Test loss: 1.659, Test accuracy: 85.27
Round  56, Train loss: 1.683, Test loss: 1.655, Test accuracy: 86.12
Round  57, Train loss: 1.662, Test loss: 1.653, Test accuracy: 86.78
Round  58, Train loss: 1.655, Test loss: 1.649, Test accuracy: 87.83
Round  59, Train loss: 1.654, Test loss: 1.646, Test accuracy: 88.97
Round  60, Train loss: 1.648, Test loss: 1.637, Test accuracy: 89.97
Round  61, Train loss: 1.636, Test loss: 1.631, Test accuracy: 90.50
Round  62, Train loss: 1.640, Test loss: 1.626, Test accuracy: 91.10
Round  63, Train loss: 1.627, Test loss: 1.620, Test accuracy: 91.43
Round  64, Train loss: 1.612, Test loss: 1.617, Test accuracy: 91.78
Round  65, Train loss: 1.620, Test loss: 1.613, Test accuracy: 91.83
Round  66, Train loss: 1.616, Test loss: 1.607, Test accuracy: 92.12
Round  67, Train loss: 1.612, Test loss: 1.604, Test accuracy: 92.40
Round  68, Train loss: 1.607, Test loss: 1.601, Test accuracy: 92.43
Round  69, Train loss: 1.599, Test loss: 1.599, Test accuracy: 92.57
Round  70, Train loss: 1.600, Test loss: 1.596, Test accuracy: 92.65
Round  71, Train loss: 1.597, Test loss: 1.595, Test accuracy: 92.62
Round  72, Train loss: 1.588, Test loss: 1.595, Test accuracy: 92.82
Round  73, Train loss: 1.592, Test loss: 1.592, Test accuracy: 92.98
Round  74, Train loss: 1.595, Test loss: 1.590, Test accuracy: 93.07
Round  75, Train loss: 1.586, Test loss: 1.588, Test accuracy: 93.02
Round  76, Train loss: 1.578, Test loss: 1.589, Test accuracy: 93.12
Round  77, Train loss: 1.591, Test loss: 1.586, Test accuracy: 93.10
Round  78, Train loss: 1.589, Test loss: 1.585, Test accuracy: 93.15
Round  79, Train loss: 1.582, Test loss: 1.584, Test accuracy: 93.25
Round  80, Train loss: 1.579, Test loss: 1.583, Test accuracy: 93.37
Round  81, Train loss: 1.578, Test loss: 1.581, Test accuracy: 93.40
Round  82, Train loss: 1.577, Test loss: 1.580, Test accuracy: 93.43
Round  83, Train loss: 1.568, Test loss: 1.580, Test accuracy: 93.43
Round  84, Train loss: 1.574, Test loss: 1.580, Test accuracy: 93.50
Round  85, Train loss: 1.565, Test loss: 1.580, Test accuracy: 93.48
Round  86, Train loss: 1.570, Test loss: 1.578, Test accuracy: 93.43
Round  87, Train loss: 1.569, Test loss: 1.576, Test accuracy: 93.70
Round  88, Train loss: 1.565, Test loss: 1.577, Test accuracy: 93.73
Round  89, Train loss: 1.560, Test loss: 1.577, Test accuracy: 93.68
Round  90, Train loss: 1.567, Test loss: 1.574, Test accuracy: 93.63
Round  91, Train loss: 1.564, Test loss: 1.574, Test accuracy: 93.67
Round  92, Train loss: 1.562, Test loss: 1.572, Test accuracy: 93.70
Round  93, Train loss: 1.565, Test loss: 1.573, Test accuracy: 93.85/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.561, Test loss: 1.571, Test accuracy: 93.78
Round  95, Train loss: 1.555, Test loss: 1.571, Test accuracy: 93.93
Round  96, Train loss: 1.555, Test loss: 1.570, Test accuracy: 93.95
Round  97, Train loss: 1.561, Test loss: 1.570, Test accuracy: 94.13
Round  98, Train loss: 1.553, Test loss: 1.569, Test accuracy: 94.08
Round  99, Train loss: 1.557, Test loss: 1.568, Test accuracy: 94.08
Final Round, Train loss: 1.518, Test loss: 1.562, Test accuracy: 94.13
Average accuracy final 10 rounds: 93.88166666666666
858.0433385372162
[0.8737804889678955, 1.747560977935791, 2.4758996963500977, 3.2042384147644043, 3.963881731033325, 4.723525047302246, 5.5050365924835205, 6.286548137664795, 7.059857606887817, 7.83316707611084, 8.589112281799316, 9.345057487487793, 10.103520631790161, 10.86198377609253, 11.591235399246216, 12.320487022399902, 13.087521076202393, 13.854555130004883, 14.642961263656616, 15.43136739730835, 16.226981163024902, 17.022594928741455, 17.76557755470276, 18.508560180664062, 19.241358041763306, 19.97415590286255, 20.72872304916382, 21.483290195465088, 22.26089096069336, 23.03849172592163, 23.81898522377014, 24.599478721618652, 25.378936052322388, 26.158393383026123, 26.926507472991943, 27.694621562957764, 28.46446180343628, 29.234302043914795, 30.012076139450073, 30.78985023498535, 31.571584224700928, 32.353318214416504, 33.14650201797485, 33.9396858215332, 34.69946503639221, 35.45924425125122, 36.25295376777649, 37.04666328430176, 37.843743324279785, 38.64082336425781, 39.445083141326904, 40.249342918395996, 41.030147075653076, 41.810951232910156, 42.573774576187134, 43.33659791946411, 44.09491419792175, 44.853230476379395, 45.63233542442322, 46.41144037246704, 47.20237684249878, 47.99331331253052, 48.7888925075531, 49.584471702575684, 50.37432289123535, 51.16417407989502, 51.94194579124451, 52.719717502593994, 53.46889686584473, 54.21807622909546, 54.93921685218811, 55.66035747528076, 56.38332557678223, 57.10629367828369, 57.83198666572571, 58.557679653167725, 59.27602767944336, 59.994375705718994, 60.7154335975647, 61.4364914894104, 62.16130352020264, 62.88611555099487, 63.58380055427551, 64.28148555755615, 64.98097681999207, 65.68046808242798, 66.39361381530762, 67.10675954818726, 67.84094333648682, 68.57512712478638, 69.29901075363159, 70.0228943824768, 70.75365424156189, 71.48441410064697, 72.22306394577026, 72.96171379089355, 73.71229338645935, 74.46287298202515, 75.20677423477173, 75.95067548751831, 76.67313742637634, 77.39559936523438, 78.11536478996277, 78.83513021469116, 79.53046131134033, 80.2257924079895, 80.96438026428223, 81.70296812057495, 82.4795892238617, 83.25621032714844, 84.01735758781433, 84.77850484848022, 85.50463008880615, 86.23075532913208, 86.94307923316956, 87.65540313720703, 88.37753510475159, 89.09966707229614, 89.82535409927368, 90.55104112625122, 91.28803396224976, 92.02502679824829, 92.7734215259552, 93.52181625366211, 94.22274947166443, 94.92368268966675, 95.62598299980164, 96.32828330993652, 97.0617516040802, 97.79521989822388, 98.50951600074768, 99.22381210327148, 99.93380331993103, 100.64379453659058, 101.33266282081604, 102.0215311050415, 102.72445273399353, 103.42737436294556, 104.1389045715332, 104.85043478012085, 105.56707763671875, 106.28372049331665, 107.01989555358887, 107.75607061386108, 108.46524357795715, 109.17441654205322, 109.8530786037445, 110.53174066543579, 111.22314405441284, 111.91454744338989, 112.6114604473114, 113.30837345123291, 114.02728199958801, 114.74619054794312, 115.49472737312317, 116.24326419830322, 116.98592448234558, 117.72858476638794, 118.43555641174316, 119.14252805709839, 119.83635568618774, 120.5301833152771, 121.2167649269104, 121.9033465385437, 122.61492681503296, 123.32650709152222, 124.03275060653687, 124.73899412155151, 125.44723796844482, 126.15548181533813, 126.87783455848694, 127.60018730163574, 128.30365753173828, 129.00712776184082, 129.71974062919617, 130.4323534965515, 131.1389467716217, 131.8455400466919, 132.57454705238342, 133.30355405807495, 134.0115773677826, 134.71960067749023, 135.4273874759674, 136.13517427444458, 136.8380024433136, 137.54083061218262, 138.26712083816528, 138.99341106414795, 139.70379042625427, 140.4141697883606, 141.1364095211029, 141.85864925384521, 142.54941606521606, 143.2401828765869, 143.95094513893127, 144.66170740127563, 145.3133225440979, 145.96493768692017, 146.6865119934082, 147.40808629989624, 148.50809240341187, 149.6080985069275]
[12.833333333333334, 12.833333333333334, 18.133333333333333, 18.133333333333333, 20.566666666666666, 20.566666666666666, 18.183333333333334, 18.183333333333334, 21.816666666666666, 21.816666666666666, 25.783333333333335, 25.783333333333335, 27.316666666666666, 27.316666666666666, 39.85, 39.85, 44.65, 44.65, 46.516666666666666, 46.516666666666666, 52.833333333333336, 52.833333333333336, 56.81666666666667, 56.81666666666667, 61.0, 61.0, 63.15, 63.15, 65.48333333333333, 65.48333333333333, 66.55, 66.55, 67.68333333333334, 67.68333333333334, 68.13333333333334, 68.13333333333334, 69.51666666666667, 69.51666666666667, 70.15, 70.15, 71.45, 71.45, 72.1, 72.1, 73.01666666666667, 73.01666666666667, 73.51666666666667, 73.51666666666667, 74.45, 74.45, 74.7, 74.7, 75.68333333333334, 75.68333333333334, 76.66666666666667, 76.66666666666667, 78.23333333333333, 78.23333333333333, 79.38333333333334, 79.38333333333334, 80.2, 80.2, 80.83333333333333, 80.83333333333333, 81.18333333333334, 81.18333333333334, 81.71666666666667, 81.71666666666667, 82.15, 82.15, 82.5, 82.5, 82.71666666666667, 82.71666666666667, 83.01666666666667, 83.01666666666667, 83.16666666666667, 83.16666666666667, 83.36666666666666, 83.36666666666666, 83.45, 83.45, 83.68333333333334, 83.68333333333334, 83.73333333333333, 83.73333333333333, 83.78333333333333, 83.78333333333333, 83.88333333333334, 83.88333333333334, 84.15, 84.15, 84.33333333333333, 84.33333333333333, 84.36666666666666, 84.36666666666666, 84.41666666666667, 84.41666666666667, 84.46666666666667, 84.46666666666667, 84.65, 84.65, 84.56666666666666, 84.56666666666666, 84.71666666666667, 84.71666666666667, 84.8, 84.8, 85.23333333333333, 85.23333333333333, 85.26666666666667, 85.26666666666667, 86.11666666666666, 86.11666666666666, 86.78333333333333, 86.78333333333333, 87.83333333333333, 87.83333333333333, 88.96666666666667, 88.96666666666667, 89.96666666666667, 89.96666666666667, 90.5, 90.5, 91.1, 91.1, 91.43333333333334, 91.43333333333334, 91.78333333333333, 91.78333333333333, 91.83333333333333, 91.83333333333333, 92.11666666666666, 92.11666666666666, 92.4, 92.4, 92.43333333333334, 92.43333333333334, 92.56666666666666, 92.56666666666666, 92.65, 92.65, 92.61666666666666, 92.61666666666666, 92.81666666666666, 92.81666666666666, 92.98333333333333, 92.98333333333333, 93.06666666666666, 93.06666666666666, 93.01666666666667, 93.01666666666667, 93.11666666666666, 93.11666666666666, 93.1, 93.1, 93.15, 93.15, 93.25, 93.25, 93.36666666666666, 93.36666666666666, 93.4, 93.4, 93.43333333333334, 93.43333333333334, 93.43333333333334, 93.43333333333334, 93.5, 93.5, 93.48333333333333, 93.48333333333333, 93.43333333333334, 93.43333333333334, 93.7, 93.7, 93.73333333333333, 93.73333333333333, 93.68333333333334, 93.68333333333334, 93.63333333333334, 93.63333333333334, 93.66666666666667, 93.66666666666667, 93.7, 93.7, 93.85, 93.85, 93.78333333333333, 93.78333333333333, 93.93333333333334, 93.93333333333334, 93.95, 93.95, 94.13333333333334, 94.13333333333334, 94.08333333333333, 94.08333333333333, 94.08333333333333, 94.08333333333333, 94.13333333333334, 94.13333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.186, Test loss: 2.171, Test accuracy: 28.50
Round   0, Global train loss: 2.186, Global test loss: 2.274, Global test accuracy: 20.52
Round   1, Train loss: 1.686, Test loss: 1.939, Test accuracy: 55.91
Round   1, Global train loss: 1.686, Global test loss: 2.167, Global test accuracy: 33.37
Round   2, Train loss: 1.567, Test loss: 1.857, Test accuracy: 61.40
Round   2, Global train loss: 1.567, Global test loss: 2.178, Global test accuracy: 30.66
Round   3, Train loss: 1.552, Test loss: 1.740, Test accuracy: 73.11
Round   3, Global train loss: 1.552, Global test loss: 2.114, Global test accuracy: 34.84
Round   4, Train loss: 1.525, Test loss: 1.693, Test accuracy: 76.99
Round   4, Global train loss: 1.525, Global test loss: 2.144, Global test accuracy: 28.48
Round   5, Train loss: 1.593, Test loss: 1.616, Test accuracy: 85.71
Round   5, Global train loss: 1.593, Global test loss: 2.088, Global test accuracy: 39.85
Round   6, Train loss: 1.511, Test loss: 1.585, Test accuracy: 88.67
Round   6, Global train loss: 1.511, Global test loss: 2.100, Global test accuracy: 38.59
Round   7, Train loss: 1.488, Test loss: 1.559, Test accuracy: 90.92
Round   7, Global train loss: 1.488, Global test loss: 2.021, Global test accuracy: 49.27
Round   8, Train loss: 1.481, Test loss: 1.547, Test accuracy: 92.28
Round   8, Global train loss: 1.481, Global test loss: 2.108, Global test accuracy: 35.34
Round   9, Train loss: 1.477, Test loss: 1.552, Test accuracy: 92.20
Round   9, Global train loss: 1.477, Global test loss: 2.090, Global test accuracy: 43.85
Round  10, Train loss: 1.474, Test loss: 1.560, Test accuracy: 90.30
Round  10, Global train loss: 1.474, Global test loss: 2.145, Global test accuracy: 34.37
Round  11, Train loss: 1.510, Test loss: 1.520, Test accuracy: 94.57
Round  11, Global train loss: 1.510, Global test loss: 2.038, Global test accuracy: 44.80
Round  12, Train loss: 1.478, Test loss: 1.520, Test accuracy: 94.62
Round  12, Global train loss: 1.478, Global test loss: 2.136, Global test accuracy: 31.29
Round  13, Train loss: 1.478, Test loss: 1.518, Test accuracy: 94.72
Round  13, Global train loss: 1.478, Global test loss: 2.156, Global test accuracy: 32.10
Round  14, Train loss: 1.522, Test loss: 1.510, Test accuracy: 95.88
Round  14, Global train loss: 1.522, Global test loss: 2.071, Global test accuracy: 38.77
Round  15, Train loss: 1.484, Test loss: 1.503, Test accuracy: 96.30
Round  15, Global train loss: 1.484, Global test loss: 2.203, Global test accuracy: 23.44
Round  16, Train loss: 1.470, Test loss: 1.503, Test accuracy: 96.28
Round  16, Global train loss: 1.470, Global test loss: 2.068, Global test accuracy: 41.11
Round  17, Train loss: 1.471, Test loss: 1.502, Test accuracy: 96.35
Round  17, Global train loss: 1.471, Global test loss: 2.140, Global test accuracy: 27.43
Round  18, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.37
Round  18, Global train loss: 1.476, Global test loss: 2.069, Global test accuracy: 41.37
Round  19, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.44
Round  19, Global train loss: 1.474, Global test loss: 2.132, Global test accuracy: 34.66
Round  20, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.47
Round  20, Global train loss: 1.468, Global test loss: 2.140, Global test accuracy: 30.47
Round  21, Train loss: 1.468, Test loss: 1.500, Test accuracy: 96.52
Round  21, Global train loss: 1.468, Global test loss: 2.111, Global test accuracy: 37.48
Round  22, Train loss: 1.467, Test loss: 1.500, Test accuracy: 96.53
Round  22, Global train loss: 1.467, Global test loss: 2.020, Global test accuracy: 48.24
Round  23, Train loss: 1.469, Test loss: 1.500, Test accuracy: 96.50
Round  23, Global train loss: 1.469, Global test loss: 2.028, Global test accuracy: 46.79
Round  24, Train loss: 1.468, Test loss: 1.500, Test accuracy: 96.50
Round  24, Global train loss: 1.468, Global test loss: 2.028, Global test accuracy: 46.95
Round  25, Train loss: 1.466, Test loss: 1.499, Test accuracy: 96.54
Round  25, Global train loss: 1.466, Global test loss: 2.079, Global test accuracy: 37.54
Round  26, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.56
Round  26, Global train loss: 1.472, Global test loss: 2.096, Global test accuracy: 33.13
Round  27, Train loss: 1.468, Test loss: 1.499, Test accuracy: 96.57
Round  27, Global train loss: 1.468, Global test loss: 2.053, Global test accuracy: 44.39
Round  28, Train loss: 1.468, Test loss: 1.499, Test accuracy: 96.58
Round  28, Global train loss: 1.468, Global test loss: 2.053, Global test accuracy: 39.73
Round  29, Train loss: 1.467, Test loss: 1.499, Test accuracy: 96.58
Round  29, Global train loss: 1.467, Global test loss: 2.077, Global test accuracy: 40.88
Round  30, Train loss: 1.468, Test loss: 1.499, Test accuracy: 96.58
Round  30, Global train loss: 1.468, Global test loss: 2.144, Global test accuracy: 30.67
Round  31, Train loss: 1.469, Test loss: 1.499, Test accuracy: 96.56
Round  31, Global train loss: 1.469, Global test loss: 2.156, Global test accuracy: 30.18
Round  32, Train loss: 1.470, Test loss: 1.499, Test accuracy: 96.58
Round  32, Global train loss: 1.470, Global test loss: 2.108, Global test accuracy: 33.59
Round  33, Train loss: 1.467, Test loss: 1.499, Test accuracy: 96.56
Round  33, Global train loss: 1.467, Global test loss: 2.102, Global test accuracy: 33.41
Round  34, Train loss: 1.469, Test loss: 1.499, Test accuracy: 96.53
Round  34, Global train loss: 1.469, Global test loss: 2.136, Global test accuracy: 29.73
Round  35, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.55
Round  35, Global train loss: 1.469, Global test loss: 2.122, Global test accuracy: 37.06
Round  36, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.55
Round  36, Global train loss: 1.468, Global test loss: 2.064, Global test accuracy: 45.28
Round  37, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.54
Round  37, Global train loss: 1.468, Global test loss: 2.073, Global test accuracy: 36.53
Round  38, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.55
Round  38, Global train loss: 1.467, Global test loss: 2.113, Global test accuracy: 36.00
Round  39, Train loss: 1.465, Test loss: 1.498, Test accuracy: 96.57
Round  39, Global train loss: 1.465, Global test loss: 2.142, Global test accuracy: 30.05
Round  40, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.58
Round  40, Global train loss: 1.467, Global test loss: 2.126, Global test accuracy: 28.94
Round  41, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.58
Round  41, Global train loss: 1.469, Global test loss: 2.096, Global test accuracy: 55.67
Round  42, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.57
Round  42, Global train loss: 1.468, Global test loss: 2.075, Global test accuracy: 38.70
Round  43, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.57
Round  43, Global train loss: 1.466, Global test loss: 2.125, Global test accuracy: 32.57
Round  44, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.57
Round  44, Global train loss: 1.468, Global test loss: 2.161, Global test accuracy: 27.22
Round  45, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.58
Round  45, Global train loss: 1.469, Global test loss: 2.128, Global test accuracy: 37.83
Round  46, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.58
Round  46, Global train loss: 1.468, Global test loss: 2.064, Global test accuracy: 39.18
Round  47, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.61
Round  47, Global train loss: 1.470, Global test loss: 2.034, Global test accuracy: 43.96
Round  48, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.61
Round  48, Global train loss: 1.471, Global test loss: 2.184, Global test accuracy: 26.53
Round  49, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.62
Round  49, Global train loss: 1.467, Global test loss: 2.150, Global test accuracy: 29.63
Round  50, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.60
Round  50, Global train loss: 1.469, Global test loss: 2.117, Global test accuracy: 34.22
Round  51, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.60
Round  51, Global train loss: 1.469, Global test loss: 2.219, Global test accuracy: 20.49
Round  52, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.61
Round  52, Global train loss: 1.467, Global test loss: 2.070, Global test accuracy: 38.01
Round  53, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.61
Round  53, Global train loss: 1.466, Global test loss: 2.100, Global test accuracy: 36.23
Round  54, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.58
Round  54, Global train loss: 1.467, Global test loss: 2.055, Global test accuracy: 45.15
Round  55, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.58
Round  55, Global train loss: 1.466, Global test loss: 2.077, Global test accuracy: 39.21
Round  56, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.58
Round  56, Global train loss: 1.466, Global test loss: 2.091, Global test accuracy: 37.77
Round  57, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.58
Round  57, Global train loss: 1.468, Global test loss: 2.067, Global test accuracy: 38.52
Round  58, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.58
Round  58, Global train loss: 1.467, Global test loss: 2.134, Global test accuracy: 30.12
Round  59, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.58
Round  59, Global train loss: 1.467, Global test loss: 2.119, Global test accuracy: 32.23
Round  60, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.59
Round  60, Global train loss: 1.469, Global test loss: 2.130, Global test accuracy: 30.82
Round  61, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.59
Round  61, Global train loss: 1.465, Global test loss: 2.109, Global test accuracy: 37.59
Round  62, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.58
Round  62, Global train loss: 1.466, Global test loss: 2.166, Global test accuracy: 27.90
Round  63, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.59
Round  63, Global train loss: 1.467, Global test loss: 2.197, Global test accuracy: 22.32
Round  64, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.59
Round  64, Global train loss: 1.469, Global test loss: 2.064, Global test accuracy: 39.97
Round  65, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.61
Round  65, Global train loss: 1.467, Global test loss: 2.136, Global test accuracy: 33.14
Round  66, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  66, Global train loss: 1.469, Global test loss: 2.092, Global test accuracy: 39.20
Round  67, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  67, Global train loss: 1.469, Global test loss: 2.079, Global test accuracy: 35.61
Round  68, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.61
Round  68, Global train loss: 1.465, Global test loss: 2.078, Global test accuracy: 42.75
Round  69, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.61
Round  69, Global train loss: 1.467, Global test loss: 2.056, Global test accuracy: 40.88
Round  70, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.61
Round  70, Global train loss: 1.466, Global test loss: 2.056, Global test accuracy: 38.12
Round  71, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.61
Round  71, Global train loss: 1.467, Global test loss: 2.017, Global test accuracy: 44.27
Round  72, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  72, Global train loss: 1.469, Global test loss: 2.099, Global test accuracy: 35.87
Round  73, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.61
Round  73, Global train loss: 1.466, Global test loss: 2.161, Global test accuracy: 27.11
Round  74, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.62
Round  74, Global train loss: 1.467, Global test loss: 2.009, Global test accuracy: 43.82
Round  75, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.62
Round  75, Global train loss: 1.466, Global test loss: 2.076, Global test accuracy: 53.15
Round  76, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.61
Round  76, Global train loss: 1.465, Global test loss: 2.188, Global test accuracy: 25.25
Round  77, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.61
Round  77, Global train loss: 1.470, Global test loss: 2.052, Global test accuracy: 39.21
Round  78, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.61
Round  78, Global train loss: 1.464, Global test loss: 2.085, Global test accuracy: 38.79
Round  79, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  79, Global train loss: 1.469, Global test loss: 2.116, Global test accuracy: 31.32
Round  80, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.61
Round  80, Global train loss: 1.468, Global test loss: 2.082, Global test accuracy: 43.28
Round  81, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.61
Round  81, Global train loss: 1.467, Global test loss: 2.048, Global test accuracy: 49.05
Round  82, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.62
Round  82, Global train loss: 1.468, Global test loss: 2.114, Global test accuracy: 30.50
Round  83, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.62
Round  83, Global train loss: 1.466, Global test loss: 2.143, Global test accuracy: 31.82
Round  84, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.62
Round  84, Global train loss: 1.468, Global test loss: 2.048, Global test accuracy: 43.21
Round  85, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.61
Round  85, Global train loss: 1.467, Global test loss: 2.002, Global test accuracy: 46.85
Round  86, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.62
Round  86, Global train loss: 1.467, Global test loss: 2.098, Global test accuracy: 38.83
Round  87, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.61
Round  87, Global train loss: 1.468, Global test loss: 2.168, Global test accuracy: 24.28
Round  88, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.62
Round  88, Global train loss: 1.465, Global test loss: 2.046, Global test accuracy: 41.49
Round  89, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.62
Round  89, Global train loss: 1.467, Global test loss: 2.076, Global test accuracy: 42.26
Round  90, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.62
Round  90, Global train loss: 1.468, Global test loss: 2.038, Global test accuracy: 44.98
Round  91, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.59
Round  91, Global train loss: 1.469, Global test loss: 2.168, Global test accuracy: 26.40
Round  92, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.59
Round  92, Global train loss: 1.468, Global test loss: 2.133, Global test accuracy: 29.19
Round  93, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.61
Round  93, Global train loss: 1.464, Global test loss: 2.206, Global test accuracy: 21.81
Round  94, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.61
Round  94, Global train loss: 1.465, Global test loss: 2.048, Global test accuracy: 41.97
Round  95, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.62
Round  95, Global train loss: 1.469, Global test loss: 2.093, Global test accuracy: 36.51/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.63
Round  96, Global train loss: 1.466, Global test loss: 2.179, Global test accuracy: 23.06
Round  97, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.63
Round  97, Global train loss: 1.467, Global test loss: 2.057, Global test accuracy: 56.13
Round  98, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.63
Round  98, Global train loss: 1.467, Global test loss: 1.982, Global test accuracy: 55.11
Round  99, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.63
Round  99, Global train loss: 1.468, Global test loss: 2.154, Global test accuracy: 29.73
Final Round, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.64
Final Round, Global train loss: 1.467, Global test loss: 2.154, Global test accuracy: 29.73
Average accuracy final 10 rounds: 96.61666666666665 

Average global accuracy final 10 rounds: 36.48916666666667 

1862.498660326004
[1.2731776237487793, 2.5463552474975586, 3.6265242099761963, 4.706693172454834, 5.7435760498046875, 6.780458927154541, 7.869782209396362, 8.959105491638184, 10.022645473480225, 11.086185455322266, 12.0753173828125, 13.064449310302734, 14.141101837158203, 15.217754364013672, 16.272948026657104, 17.328141689300537, 18.368265628814697, 19.408389568328857, 20.45296025276184, 21.497530937194824, 23.160943269729614, 24.824355602264404, 25.908575773239136, 26.992795944213867, 28.01714539527893, 29.041494846343994, 30.04773473739624, 31.053974628448486, 32.10659146308899, 33.15920829772949, 34.18903422355652, 35.218860149383545, 36.273696422576904, 37.328532695770264, 38.394959688186646, 39.46138668060303, 40.50674486160278, 41.55210304260254, 42.66410994529724, 43.77611684799194, 44.85891795158386, 45.94171905517578, 46.991960525512695, 48.04220199584961, 49.104514598846436, 50.16682720184326, 51.205541133880615, 52.24425506591797, 53.31991124153137, 54.395567417144775, 55.43202567100525, 56.46848392486572, 57.50941729545593, 58.55035066604614, 59.62363409996033, 60.69691753387451, 61.71460819244385, 62.732298851013184, 63.7828574180603, 64.83341598510742, 65.87073230743408, 66.90804862976074, 67.9386260509491, 68.96920347213745, 70.0154287815094, 71.06165409088135, 72.11850500106812, 73.17535591125488, 74.21435546875, 75.25335502624512, 76.31404399871826, 77.3747329711914, 78.42935848236084, 79.48398399353027, 80.55907773971558, 81.63417148590088, 82.69682145118713, 83.75947141647339, 84.79937028884888, 85.83926916122437, 86.90406608581543, 87.9688630104065, 88.98909378051758, 90.00932455062866, 91.03452372550964, 92.05972290039062, 93.12180829048157, 94.18389368057251, 95.23370385169983, 96.28351402282715, 97.34971141815186, 98.41590881347656, 99.49414396286011, 100.57237911224365, 101.62661838531494, 102.68085765838623, 103.71662998199463, 104.75240230560303, 105.81059718132019, 106.86879205703735, 107.9140031337738, 108.95921421051025, 110.01270294189453, 111.06619167327881, 112.13883137702942, 113.21147108078003, 114.28766584396362, 115.36386060714722, 116.45475268363953, 117.54564476013184, 118.60183382034302, 119.6580228805542, 120.72657513618469, 121.79512739181519, 122.91285014152527, 124.03057289123535, 125.10563445091248, 126.1806960105896, 127.25581693649292, 128.33093786239624, 129.41991639137268, 130.50889492034912, 131.5868682861328, 132.6648416519165, 133.72473645210266, 134.78463125228882, 135.87845587730408, 136.97228050231934, 138.0232493877411, 139.07421827316284, 140.16582250595093, 141.257426738739, 142.34604287147522, 143.43465900421143, 144.52586817741394, 145.61707735061646, 146.70865941047668, 147.8002414703369, 148.86920714378357, 149.93817281723022, 151.03160786628723, 152.12504291534424, 153.21489572525024, 154.30474853515625, 155.38749980926514, 156.47025108337402, 157.52809381484985, 158.58593654632568, 159.67493534088135, 160.763934135437, 161.85696291923523, 162.94999170303345, 164.0684232711792, 165.18685483932495, 166.29336786270142, 167.39988088607788, 168.47748398780823, 169.55508708953857, 170.643230676651, 171.73137426376343, 172.82767391204834, 173.92397356033325, 174.99775862693787, 176.07154369354248, 177.14504170417786, 178.21853971481323, 179.31453204154968, 180.41052436828613, 181.49681544303894, 182.58310651779175, 183.65961360931396, 184.73612070083618, 185.80525851249695, 186.87439632415771, 187.92562079429626, 188.97684526443481, 190.11715745925903, 191.25746965408325, 192.34103345870972, 193.42459726333618, 194.53015208244324, 195.6357069015503, 196.77565336227417, 197.91559982299805, 199.00360536575317, 200.0916109085083, 201.18523955345154, 202.27886819839478, 203.3795268535614, 204.48018550872803, 205.58980679512024, 206.69942808151245, 207.77513360977173, 208.850839138031, 209.94897556304932, 211.04711198806763, 212.15760207176208, 213.26809215545654, 214.401113986969, 215.53413581848145, 217.35505294799805, 219.17597007751465]
[28.5, 28.5, 55.90833333333333, 55.90833333333333, 61.4, 61.4, 73.10833333333333, 73.10833333333333, 76.99166666666666, 76.99166666666666, 85.70833333333333, 85.70833333333333, 88.675, 88.675, 90.91666666666667, 90.91666666666667, 92.275, 92.275, 92.2, 92.2, 90.3, 90.3, 94.56666666666666, 94.56666666666666, 94.625, 94.625, 94.71666666666667, 94.71666666666667, 95.88333333333334, 95.88333333333334, 96.3, 96.3, 96.275, 96.275, 96.35, 96.35, 96.36666666666666, 96.36666666666666, 96.44166666666666, 96.44166666666666, 96.46666666666667, 96.46666666666667, 96.51666666666667, 96.51666666666667, 96.525, 96.525, 96.5, 96.5, 96.5, 96.5, 96.54166666666667, 96.54166666666667, 96.55833333333334, 96.55833333333334, 96.56666666666666, 96.56666666666666, 96.575, 96.575, 96.575, 96.575, 96.58333333333333, 96.58333333333333, 96.55833333333334, 96.55833333333334, 96.58333333333333, 96.58333333333333, 96.55833333333334, 96.55833333333334, 96.53333333333333, 96.53333333333333, 96.55, 96.55, 96.55, 96.55, 96.54166666666667, 96.54166666666667, 96.55, 96.55, 96.56666666666666, 96.56666666666666, 96.575, 96.575, 96.575, 96.575, 96.56666666666666, 96.56666666666666, 96.56666666666666, 96.56666666666666, 96.56666666666666, 96.56666666666666, 96.575, 96.575, 96.58333333333333, 96.58333333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.6, 96.6, 96.6, 96.6, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.58333333333333, 96.58333333333333, 96.58333333333333, 96.58333333333333, 96.575, 96.575, 96.575, 96.575, 96.58333333333333, 96.58333333333333, 96.58333333333333, 96.58333333333333, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.58333333333333, 96.58333333333333, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.59166666666667, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.64166666666667, 96.64166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.299, Test accuracy: 22.01
Round   0, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 21.83
Round   1, Train loss: 2.296, Test loss: 2.294, Test accuracy: 42.60
Round   1, Global train loss: 2.296, Global test loss: 2.292, Global test accuracy: 49.89
Round   2, Train loss: 2.285, Test loss: 2.280, Test accuracy: 43.50
Round   2, Global train loss: 2.285, Global test loss: 2.273, Global test accuracy: 49.40
Round   3, Train loss: 2.222, Test loss: 2.194, Test accuracy: 39.80
Round   3, Global train loss: 2.222, Global test loss: 2.122, Global test accuracy: 40.95
Round   4, Train loss: 2.009, Test loss: 2.051, Test accuracy: 53.07
Round   4, Global train loss: 2.009, Global test loss: 1.882, Global test accuracy: 67.25
Round   5, Train loss: 1.821, Test loss: 1.959, Test accuracy: 61.17
Round   5, Global train loss: 1.821, Global test loss: 1.764, Global test accuracy: 73.92
Round   6, Train loss: 1.755, Test loss: 1.883, Test accuracy: 65.56
Round   6, Global train loss: 1.755, Global test loss: 1.726, Global test accuracy: 75.52
Round   7, Train loss: 1.708, Test loss: 1.842, Test accuracy: 67.71
Round   7, Global train loss: 1.708, Global test loss: 1.690, Global test accuracy: 79.54
Round   8, Train loss: 1.673, Test loss: 1.791, Test accuracy: 70.91
Round   8, Global train loss: 1.673, Global test loss: 1.666, Global test accuracy: 81.30
Round   9, Train loss: 1.646, Test loss: 1.747, Test accuracy: 74.20
Round   9, Global train loss: 1.646, Global test loss: 1.656, Global test accuracy: 81.71
Round  10, Train loss: 1.639, Test loss: 1.740, Test accuracy: 74.90
Round  10, Global train loss: 1.639, Global test loss: 1.649, Global test accuracy: 82.22
Round  11, Train loss: 1.630, Test loss: 1.709, Test accuracy: 77.43
Round  11, Global train loss: 1.630, Global test loss: 1.643, Global test accuracy: 82.50
Round  12, Train loss: 1.628, Test loss: 1.691, Test accuracy: 78.56
Round  12, Global train loss: 1.628, Global test loss: 1.639, Global test accuracy: 82.67
Round  13, Train loss: 1.629, Test loss: 1.686, Test accuracy: 78.83
Round  13, Global train loss: 1.629, Global test loss: 1.636, Global test accuracy: 82.92
Round  14, Train loss: 1.617, Test loss: 1.683, Test accuracy: 79.04
Round  14, Global train loss: 1.617, Global test loss: 1.634, Global test accuracy: 83.12
Round  15, Train loss: 1.612, Test loss: 1.679, Test accuracy: 79.31
Round  15, Global train loss: 1.612, Global test loss: 1.631, Global test accuracy: 83.39
Round  16, Train loss: 1.616, Test loss: 1.674, Test accuracy: 79.65
Round  16, Global train loss: 1.616, Global test loss: 1.629, Global test accuracy: 83.57
Round  17, Train loss: 1.605, Test loss: 1.673, Test accuracy: 79.86
Round  17, Global train loss: 1.605, Global test loss: 1.627, Global test accuracy: 83.74
Round  18, Train loss: 1.614, Test loss: 1.668, Test accuracy: 80.22
Round  18, Global train loss: 1.614, Global test loss: 1.625, Global test accuracy: 83.89
Round  19, Train loss: 1.602, Test loss: 1.668, Test accuracy: 80.32
Round  19, Global train loss: 1.602, Global test loss: 1.624, Global test accuracy: 84.01
Round  20, Train loss: 1.599, Test loss: 1.665, Test accuracy: 80.57
Round  20, Global train loss: 1.599, Global test loss: 1.623, Global test accuracy: 84.04
Round  21, Train loss: 1.602, Test loss: 1.664, Test accuracy: 80.76
Round  21, Global train loss: 1.602, Global test loss: 1.622, Global test accuracy: 84.11
Round  22, Train loss: 1.597, Test loss: 1.628, Test accuracy: 83.79
Round  22, Global train loss: 1.597, Global test loss: 1.620, Global test accuracy: 84.33
Round  23, Train loss: 1.600, Test loss: 1.627, Test accuracy: 83.82
Round  23, Global train loss: 1.600, Global test loss: 1.619, Global test accuracy: 84.53
Round  24, Train loss: 1.589, Test loss: 1.627, Test accuracy: 83.79
Round  24, Global train loss: 1.589, Global test loss: 1.618, Global test accuracy: 84.60
Round  25, Train loss: 1.589, Test loss: 1.626, Test accuracy: 83.82
Round  25, Global train loss: 1.589, Global test loss: 1.618, Global test accuracy: 84.42
Round  26, Train loss: 1.602, Test loss: 1.624, Test accuracy: 83.93
Round  26, Global train loss: 1.602, Global test loss: 1.619, Global test accuracy: 84.40
Round  27, Train loss: 1.593, Test loss: 1.622, Test accuracy: 84.09
Round  27, Global train loss: 1.593, Global test loss: 1.617, Global test accuracy: 84.69
Round  28, Train loss: 1.604, Test loss: 1.622, Test accuracy: 84.18
Round  28, Global train loss: 1.604, Global test loss: 1.617, Global test accuracy: 84.55
Round  29, Train loss: 1.591, Test loss: 1.621, Test accuracy: 84.23
Round  29, Global train loss: 1.591, Global test loss: 1.615, Global test accuracy: 84.89
Round  30, Train loss: 1.594, Test loss: 1.621, Test accuracy: 84.25
Round  30, Global train loss: 1.594, Global test loss: 1.616, Global test accuracy: 84.81
Round  31, Train loss: 1.589, Test loss: 1.620, Test accuracy: 84.24
Round  31, Global train loss: 1.589, Global test loss: 1.615, Global test accuracy: 84.76
Round  32, Train loss: 1.590, Test loss: 1.619, Test accuracy: 84.36
Round  32, Global train loss: 1.590, Global test loss: 1.614, Global test accuracy: 84.78
Round  33, Train loss: 1.587, Test loss: 1.619, Test accuracy: 84.47
Round  33, Global train loss: 1.587, Global test loss: 1.613, Global test accuracy: 84.84
Round  34, Train loss: 1.591, Test loss: 1.618, Test accuracy: 84.48
Round  34, Global train loss: 1.591, Global test loss: 1.614, Global test accuracy: 84.88
Round  35, Train loss: 1.584, Test loss: 1.618, Test accuracy: 84.56
Round  35, Global train loss: 1.584, Global test loss: 1.613, Global test accuracy: 84.97
Round  36, Train loss: 1.588, Test loss: 1.618, Test accuracy: 84.47
Round  36, Global train loss: 1.588, Global test loss: 1.613, Global test accuracy: 84.87
Round  37, Train loss: 1.586, Test loss: 1.617, Test accuracy: 84.56
Round  37, Global train loss: 1.586, Global test loss: 1.612, Global test accuracy: 84.94
Round  38, Train loss: 1.594, Test loss: 1.616, Test accuracy: 84.72
Round  38, Global train loss: 1.594, Global test loss: 1.612, Global test accuracy: 84.96
Round  39, Train loss: 1.580, Test loss: 1.615, Test accuracy: 84.81
Round  39, Global train loss: 1.580, Global test loss: 1.612, Global test accuracy: 85.03
Round  40, Train loss: 1.585, Test loss: 1.615, Test accuracy: 84.80
Round  40, Global train loss: 1.585, Global test loss: 1.612, Global test accuracy: 85.09
Round  41, Train loss: 1.587, Test loss: 1.615, Test accuracy: 84.82
Round  41, Global train loss: 1.587, Global test loss: 1.611, Global test accuracy: 85.17
Round  42, Train loss: 1.584, Test loss: 1.614, Test accuracy: 84.76
Round  42, Global train loss: 1.584, Global test loss: 1.611, Global test accuracy: 85.07
Round  43, Train loss: 1.583, Test loss: 1.614, Test accuracy: 84.80
Round  43, Global train loss: 1.583, Global test loss: 1.610, Global test accuracy: 85.19
Round  44, Train loss: 1.588, Test loss: 1.614, Test accuracy: 84.77
Round  44, Global train loss: 1.588, Global test loss: 1.611, Global test accuracy: 85.14
Round  45, Train loss: 1.578, Test loss: 1.614, Test accuracy: 84.73
Round  45, Global train loss: 1.578, Global test loss: 1.610, Global test accuracy: 85.21
Round  46, Train loss: 1.583, Test loss: 1.613, Test accuracy: 84.80
Round  46, Global train loss: 1.583, Global test loss: 1.609, Global test accuracy: 85.38
Round  47, Train loss: 1.583, Test loss: 1.613, Test accuracy: 84.86
Round  47, Global train loss: 1.583, Global test loss: 1.609, Global test accuracy: 85.27
Round  48, Train loss: 1.589, Test loss: 1.612, Test accuracy: 84.89
Round  48, Global train loss: 1.589, Global test loss: 1.609, Global test accuracy: 85.23
Round  49, Train loss: 1.583, Test loss: 1.612, Test accuracy: 84.94
Round  49, Global train loss: 1.583, Global test loss: 1.609, Global test accuracy: 85.22
Round  50, Train loss: 1.579, Test loss: 1.612, Test accuracy: 85.02
Round  50, Global train loss: 1.579, Global test loss: 1.608, Global test accuracy: 85.37
Round  51, Train loss: 1.580, Test loss: 1.612, Test accuracy: 84.99
Round  51, Global train loss: 1.580, Global test loss: 1.608, Global test accuracy: 85.29
Round  52, Train loss: 1.586, Test loss: 1.611, Test accuracy: 85.02
Round  52, Global train loss: 1.586, Global test loss: 1.608, Global test accuracy: 85.53
Round  53, Train loss: 1.581, Test loss: 1.611, Test accuracy: 85.04
Round  53, Global train loss: 1.581, Global test loss: 1.608, Global test accuracy: 85.51
Round  54, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.02
Round  54, Global train loss: 1.580, Global test loss: 1.608, Global test accuracy: 85.36
Round  55, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.03
Round  55, Global train loss: 1.580, Global test loss: 1.607, Global test accuracy: 85.50
Round  56, Train loss: 1.576, Test loss: 1.611, Test accuracy: 85.03
Round  56, Global train loss: 1.576, Global test loss: 1.607, Global test accuracy: 85.47
Round  57, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.02
Round  57, Global train loss: 1.579, Global test loss: 1.607, Global test accuracy: 85.42
Round  58, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.12
Round  58, Global train loss: 1.579, Global test loss: 1.607, Global test accuracy: 85.47
Round  59, Train loss: 1.586, Test loss: 1.610, Test accuracy: 85.12
Round  59, Global train loss: 1.586, Global test loss: 1.607, Global test accuracy: 85.51
Round  60, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.09
Round  60, Global train loss: 1.575, Global test loss: 1.606, Global test accuracy: 85.56
Round  61, Train loss: 1.576, Test loss: 1.610, Test accuracy: 85.17
Round  61, Global train loss: 1.576, Global test loss: 1.606, Global test accuracy: 85.58
Round  62, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.22
Round  62, Global train loss: 1.574, Global test loss: 1.606, Global test accuracy: 85.59
Round  63, Train loss: 1.576, Test loss: 1.609, Test accuracy: 85.27
Round  63, Global train loss: 1.576, Global test loss: 1.606, Global test accuracy: 85.42
Round  64, Train loss: 1.576, Test loss: 1.609, Test accuracy: 85.24
Round  64, Global train loss: 1.576, Global test loss: 1.606, Global test accuracy: 85.52
Round  65, Train loss: 1.583, Test loss: 1.609, Test accuracy: 85.29
Round  65, Global train loss: 1.583, Global test loss: 1.607, Global test accuracy: 85.47
Round  66, Train loss: 1.580, Test loss: 1.609, Test accuracy: 85.30
Round  66, Global train loss: 1.580, Global test loss: 1.606, Global test accuracy: 85.39
Round  67, Train loss: 1.584, Test loss: 1.609, Test accuracy: 85.30
Round  67, Global train loss: 1.584, Global test loss: 1.606, Global test accuracy: 85.56
Round  68, Train loss: 1.575, Test loss: 1.608, Test accuracy: 85.28
Round  68, Global train loss: 1.575, Global test loss: 1.605, Global test accuracy: 85.66
Round  69, Train loss: 1.574, Test loss: 1.608, Test accuracy: 85.32
Round  69, Global train loss: 1.574, Global test loss: 1.605, Global test accuracy: 85.68
Round  70, Train loss: 1.578, Test loss: 1.608, Test accuracy: 85.36
Round  70, Global train loss: 1.578, Global test loss: 1.605, Global test accuracy: 85.72
Round  71, Train loss: 1.571, Test loss: 1.608, Test accuracy: 85.43
Round  71, Global train loss: 1.571, Global test loss: 1.605, Global test accuracy: 85.74
Round  72, Train loss: 1.583, Test loss: 1.607, Test accuracy: 85.46
Round  72, Global train loss: 1.583, Global test loss: 1.605, Global test accuracy: 85.56
Round  73, Train loss: 1.572, Test loss: 1.608, Test accuracy: 85.48
Round  73, Global train loss: 1.572, Global test loss: 1.605, Global test accuracy: 85.62
Round  74, Train loss: 1.571, Test loss: 1.607, Test accuracy: 85.49
Round  74, Global train loss: 1.571, Global test loss: 1.605, Global test accuracy: 85.76
Round  75, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.48
Round  75, Global train loss: 1.572, Global test loss: 1.604, Global test accuracy: 85.78
Round  76, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.50
Round  76, Global train loss: 1.574, Global test loss: 1.604, Global test accuracy: 85.72
Round  77, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.53
Round  77, Global train loss: 1.573, Global test loss: 1.605, Global test accuracy: 85.63
Round  78, Train loss: 1.578, Test loss: 1.607, Test accuracy: 85.54
Round  78, Global train loss: 1.578, Global test loss: 1.605, Global test accuracy: 85.63
Round  79, Train loss: 1.580, Test loss: 1.607, Test accuracy: 85.55
Round  79, Global train loss: 1.580, Global test loss: 1.605, Global test accuracy: 85.68
Round  80, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.55
Round  80, Global train loss: 1.573, Global test loss: 1.604, Global test accuracy: 85.78
Round  81, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.50
Round  81, Global train loss: 1.574, Global test loss: 1.604, Global test accuracy: 85.76
Round  82, Train loss: 1.578, Test loss: 1.607, Test accuracy: 85.49
Round  82, Global train loss: 1.578, Global test loss: 1.604, Global test accuracy: 85.78
Round  83, Train loss: 1.579, Test loss: 1.607, Test accuracy: 85.46
Round  83, Global train loss: 1.579, Global test loss: 1.604, Global test accuracy: 85.78
Round  84, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.50
Round  84, Global train loss: 1.572, Global test loss: 1.604, Global test accuracy: 85.77
Round  85, Train loss: 1.574, Test loss: 1.606, Test accuracy: 85.58
Round  85, Global train loss: 1.574, Global test loss: 1.604, Global test accuracy: 85.64
Round  86, Train loss: 1.576, Test loss: 1.606, Test accuracy: 85.58
Round  86, Global train loss: 1.576, Global test loss: 1.603, Global test accuracy: 85.83
Round  87, Train loss: 1.579, Test loss: 1.606, Test accuracy: 85.59
Round  87, Global train loss: 1.579, Global test loss: 1.603, Global test accuracy: 85.84
Round  88, Train loss: 1.575, Test loss: 1.606, Test accuracy: 85.62
Round  88, Global train loss: 1.575, Global test loss: 1.603, Global test accuracy: 85.84
Round  89, Train loss: 1.580, Test loss: 1.606, Test accuracy: 85.62
Round  89, Global train loss: 1.580, Global test loss: 1.603, Global test accuracy: 85.74
Round  90, Train loss: 1.577, Test loss: 1.606, Test accuracy: 85.60
Round  90, Global train loss: 1.577, Global test loss: 1.603, Global test accuracy: 85.87
Round  91, Train loss: 1.568, Test loss: 1.606, Test accuracy: 85.59
Round  91, Global train loss: 1.568, Global test loss: 1.603, Global test accuracy: 85.96
Round  92, Train loss: 1.572, Test loss: 1.605, Test accuracy: 85.61
Round  92, Global train loss: 1.572, Global test loss: 1.603, Global test accuracy: 85.85
Round  93, Train loss: 1.577, Test loss: 1.605, Test accuracy: 85.64
Round  93, Global train loss: 1.577, Global test loss: 1.603, Global test accuracy: 85.84
Round  94, Train loss: 1.575, Test loss: 1.605, Test accuracy: 85.68
Round  94, Global train loss: 1.575, Global test loss: 1.602, Global test accuracy: 85.80
Round  95, Train loss: 1.576, Test loss: 1.605, Test accuracy: 85.69
Round  95, Global train loss: 1.576, Global test loss: 1.603, Global test accuracy: 85.94/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.569, Test loss: 1.605, Test accuracy: 85.66
Round  96, Global train loss: 1.569, Global test loss: 1.602, Global test accuracy: 85.97
Round  97, Train loss: 1.572, Test loss: 1.605, Test accuracy: 85.68
Round  97, Global train loss: 1.572, Global test loss: 1.602, Global test accuracy: 85.89
Round  98, Train loss: 1.571, Test loss: 1.604, Test accuracy: 85.73
Round  98, Global train loss: 1.571, Global test loss: 1.602, Global test accuracy: 86.00
Round  99, Train loss: 1.576, Test loss: 1.604, Test accuracy: 85.72
Round  99, Global train loss: 1.576, Global test loss: 1.602, Global test accuracy: 85.95
Final Round, Train loss: 1.572, Test loss: 1.604, Test accuracy: 85.77
Final Round, Global train loss: 1.572, Global test loss: 1.602, Global test accuracy: 85.95
Average accuracy final 10 rounds: 85.66083333333334 

Average global accuracy final 10 rounds: 85.9075 

1899.6647062301636
[1.3997843265533447, 2.7995686531066895, 4.078166484832764, 5.356764316558838, 6.647262334823608, 7.937760353088379, 9.223896026611328, 10.510031700134277, 11.790722370147705, 13.071413040161133, 14.366656303405762, 15.66189956665039, 16.923475980758667, 18.185052394866943, 19.44253420829773, 20.700016021728516, 21.968594551086426, 23.237173080444336, 24.522416591644287, 25.80766010284424, 27.07195258140564, 28.33624505996704, 29.590100288391113, 30.843955516815186, 32.08951926231384, 33.3350830078125, 34.6443395614624, 35.953596115112305, 37.22858023643494, 38.50356435775757, 39.72669553756714, 40.94982671737671, 42.108619928359985, 43.26741313934326, 44.41824913024902, 45.569085121154785, 46.78674817085266, 48.00441122055054, 49.20047664642334, 50.39654207229614, 51.58549356460571, 52.77444505691528, 53.97318387031555, 55.17192268371582, 56.374797344207764, 57.57767200469971, 58.67144298553467, 59.76521396636963, 60.83178424835205, 61.89835453033447, 62.98613166809082, 64.07390880584717, 65.14314723014832, 66.21238565444946, 67.27391862869263, 68.33545160293579, 69.39421844482422, 70.45298528671265, 71.5799617767334, 72.70693826675415, 73.87639331817627, 75.04584836959839, 76.1234712600708, 77.20109415054321, 78.30524492263794, 79.40939569473267, 80.5735695362091, 81.73774337768555, 82.84361982345581, 83.94949626922607, 85.03286552429199, 86.11623477935791, 87.232248544693, 88.34826231002808, 89.46368956565857, 90.57911682128906, 91.67516541481018, 92.7712140083313, 93.84969234466553, 94.92817068099976, 96.06908416748047, 97.20999765396118, 98.33426570892334, 99.4585337638855, 100.50737357139587, 101.55621337890625, 102.61652874946594, 103.67684412002563, 104.77345252037048, 105.87006092071533, 106.94466543197632, 108.0192699432373, 109.05517339706421, 110.09107685089111, 111.13225817680359, 112.17343950271606, 113.26523327827454, 114.35702705383301, 115.3896279335022, 116.42222881317139, 117.46993851661682, 118.51764822006226, 119.60506868362427, 120.69248914718628, 121.78161406517029, 122.8707389831543, 123.9604880809784, 125.05023717880249, 126.13986444473267, 127.22949171066284, 128.34596371650696, 129.46243572235107, 130.56559205055237, 131.66874837875366, 132.76367807388306, 133.85860776901245, 134.9755802154541, 136.09255266189575, 137.1980242729187, 138.30349588394165, 139.3939356803894, 140.48437547683716, 141.56481432914734, 142.64525318145752, 143.72861123085022, 144.81196928024292, 145.89414882659912, 146.97632837295532, 148.04801964759827, 149.1197109222412, 150.22703313827515, 151.33435535430908, 152.3886260986328, 153.44289684295654, 154.5544741153717, 155.66605138778687, 156.72964811325073, 157.7932448387146, 158.87886762619019, 159.96449041366577, 161.12713384628296, 162.28977727890015, 163.3697154521942, 164.44965362548828, 165.51646041870117, 166.58326721191406, 167.62405943870544, 168.66485166549683, 169.75821447372437, 170.8515772819519, 171.96046328544617, 173.06934928894043, 174.12685537338257, 175.1843614578247, 176.2303478717804, 177.27633428573608, 178.44302129745483, 179.60970830917358, 180.71451210975647, 181.81931591033936, 182.88326001167297, 183.9472041130066, 185.058087348938, 186.16897058486938, 187.26177406311035, 188.35457754135132, 189.48152589797974, 190.60847425460815, 191.62453508377075, 192.64059591293335, 193.66813373565674, 194.69567155838013, 195.7932686805725, 196.8908658027649, 197.9028000831604, 198.9147343635559, 199.97420048713684, 201.03366661071777, 202.10778331756592, 203.18190002441406, 204.27355861663818, 205.3652172088623, 206.43902492523193, 207.51283264160156, 208.56706762313843, 209.6213026046753, 210.7593433856964, 211.89738416671753, 212.96214747428894, 214.02691078186035, 215.05260705947876, 216.07830333709717, 217.11234784126282, 218.14639234542847, 219.17155647277832, 220.19672060012817, 221.22957229614258, 222.26242399215698, 223.32209038734436, 224.38175678253174, 226.24252557754517, 228.1032943725586]
[22.008333333333333, 22.008333333333333, 42.6, 42.6, 43.5, 43.5, 39.8, 39.8, 53.06666666666667, 53.06666666666667, 61.175, 61.175, 65.55833333333334, 65.55833333333334, 67.70833333333333, 67.70833333333333, 70.90833333333333, 70.90833333333333, 74.2, 74.2, 74.9, 74.9, 77.43333333333334, 77.43333333333334, 78.55833333333334, 78.55833333333334, 78.825, 78.825, 79.04166666666667, 79.04166666666667, 79.30833333333334, 79.30833333333334, 79.65, 79.65, 79.85833333333333, 79.85833333333333, 80.21666666666667, 80.21666666666667, 80.31666666666666, 80.31666666666666, 80.56666666666666, 80.56666666666666, 80.75833333333334, 80.75833333333334, 83.79166666666667, 83.79166666666667, 83.81666666666666, 83.81666666666666, 83.79166666666667, 83.79166666666667, 83.81666666666666, 83.81666666666666, 83.93333333333334, 83.93333333333334, 84.09166666666667, 84.09166666666667, 84.18333333333334, 84.18333333333334, 84.23333333333333, 84.23333333333333, 84.25, 84.25, 84.24166666666666, 84.24166666666666, 84.35833333333333, 84.35833333333333, 84.46666666666667, 84.46666666666667, 84.48333333333333, 84.48333333333333, 84.55833333333334, 84.55833333333334, 84.475, 84.475, 84.55833333333334, 84.55833333333334, 84.725, 84.725, 84.80833333333334, 84.80833333333334, 84.8, 84.8, 84.81666666666666, 84.81666666666666, 84.75833333333334, 84.75833333333334, 84.8, 84.8, 84.76666666666667, 84.76666666666667, 84.73333333333333, 84.73333333333333, 84.8, 84.8, 84.85833333333333, 84.85833333333333, 84.89166666666667, 84.89166666666667, 84.94166666666666, 84.94166666666666, 85.01666666666667, 85.01666666666667, 84.99166666666666, 84.99166666666666, 85.01666666666667, 85.01666666666667, 85.04166666666667, 85.04166666666667, 85.01666666666667, 85.01666666666667, 85.025, 85.025, 85.025, 85.025, 85.01666666666667, 85.01666666666667, 85.125, 85.125, 85.125, 85.125, 85.09166666666667, 85.09166666666667, 85.175, 85.175, 85.21666666666667, 85.21666666666667, 85.26666666666667, 85.26666666666667, 85.24166666666666, 85.24166666666666, 85.29166666666667, 85.29166666666667, 85.3, 85.3, 85.3, 85.3, 85.275, 85.275, 85.31666666666666, 85.31666666666666, 85.35833333333333, 85.35833333333333, 85.43333333333334, 85.43333333333334, 85.45833333333333, 85.45833333333333, 85.48333333333333, 85.48333333333333, 85.49166666666666, 85.49166666666666, 85.48333333333333, 85.48333333333333, 85.5, 85.5, 85.525, 85.525, 85.54166666666667, 85.54166666666667, 85.55, 85.55, 85.55, 85.55, 85.5, 85.5, 85.49166666666666, 85.49166666666666, 85.45833333333333, 85.45833333333333, 85.5, 85.5, 85.575, 85.575, 85.58333333333333, 85.58333333333333, 85.59166666666667, 85.59166666666667, 85.625, 85.625, 85.625, 85.625, 85.6, 85.6, 85.59166666666667, 85.59166666666667, 85.60833333333333, 85.60833333333333, 85.64166666666667, 85.64166666666667, 85.68333333333334, 85.68333333333334, 85.69166666666666, 85.69166666666666, 85.65833333333333, 85.65833333333333, 85.68333333333334, 85.68333333333334, 85.73333333333333, 85.73333333333333, 85.71666666666667, 85.71666666666667, 85.76666666666667, 85.76666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 21.42
Round   1, Train loss: 2.300, Test loss: 2.299, Test accuracy: 23.47
Round   2, Train loss: 2.298, Test loss: 2.296, Test accuracy: 23.76
Round   3, Train loss: 2.295, Test loss: 2.291, Test accuracy: 25.66
Round   4, Train loss: 2.289, Test loss: 2.284, Test accuracy: 30.09
Round   5, Train loss: 2.277, Test loss: 2.266, Test accuracy: 30.27
Round   6, Train loss: 2.246, Test loss: 2.217, Test accuracy: 38.92
Round   7, Train loss: 2.144, Test loss: 2.097, Test accuracy: 41.83
Round   8, Train loss: 2.057, Test loss: 2.034, Test accuracy: 47.32
Round   9, Train loss: 1.985, Test loss: 1.970, Test accuracy: 51.38
Round  10, Train loss: 1.901, Test loss: 1.904, Test accuracy: 58.65
Round  11, Train loss: 1.829, Test loss: 1.831, Test accuracy: 67.96
Round  12, Train loss: 1.750, Test loss: 1.769, Test accuracy: 73.88
Round  13, Train loss: 1.715, Test loss: 1.730, Test accuracy: 76.97
Round  14, Train loss: 1.698, Test loss: 1.704, Test accuracy: 78.96
Round  15, Train loss: 1.673, Test loss: 1.691, Test accuracy: 79.37
Round  16, Train loss: 1.670, Test loss: 1.675, Test accuracy: 80.58
Round  17, Train loss: 1.655, Test loss: 1.660, Test accuracy: 81.92
Round  18, Train loss: 1.634, Test loss: 1.647, Test accuracy: 83.24
Round  19, Train loss: 1.624, Test loss: 1.634, Test accuracy: 84.23
Round  20, Train loss: 1.610, Test loss: 1.625, Test accuracy: 85.04
Round  21, Train loss: 1.597, Test loss: 1.616, Test accuracy: 86.30
Round  22, Train loss: 1.590, Test loss: 1.610, Test accuracy: 86.52
Round  23, Train loss: 1.586, Test loss: 1.602, Test accuracy: 87.39
Round  24, Train loss: 1.577, Test loss: 1.598, Test accuracy: 87.57
Round  25, Train loss: 1.567, Test loss: 1.591, Test accuracy: 88.25
Round  26, Train loss: 1.569, Test loss: 1.588, Test accuracy: 88.38
Round  27, Train loss: 1.564, Test loss: 1.584, Test accuracy: 88.79
Round  28, Train loss: 1.560, Test loss: 1.581, Test accuracy: 89.09
Round  29, Train loss: 1.548, Test loss: 1.576, Test accuracy: 89.62
Round  30, Train loss: 1.554, Test loss: 1.575, Test accuracy: 89.50
Round  31, Train loss: 1.550, Test loss: 1.572, Test accuracy: 90.04
Round  32, Train loss: 1.549, Test loss: 1.570, Test accuracy: 90.13
Round  33, Train loss: 1.540, Test loss: 1.572, Test accuracy: 89.88
Round  34, Train loss: 1.543, Test loss: 1.569, Test accuracy: 90.09
Round  35, Train loss: 1.540, Test loss: 1.569, Test accuracy: 89.96
Round  36, Train loss: 1.533, Test loss: 1.568, Test accuracy: 90.08
Round  37, Train loss: 1.535, Test loss: 1.565, Test accuracy: 90.32
Round  38, Train loss: 1.532, Test loss: 1.564, Test accuracy: 90.35
Round  39, Train loss: 1.524, Test loss: 1.566, Test accuracy: 90.19
Round  40, Train loss: 1.527, Test loss: 1.564, Test accuracy: 90.19
Round  41, Train loss: 1.523, Test loss: 1.563, Test accuracy: 90.27
Round  42, Train loss: 1.521, Test loss: 1.563, Test accuracy: 90.32
Round  43, Train loss: 1.522, Test loss: 1.563, Test accuracy: 90.33
Round  44, Train loss: 1.521, Test loss: 1.562, Test accuracy: 90.37
Round  45, Train loss: 1.521, Test loss: 1.562, Test accuracy: 90.38
Round  46, Train loss: 1.527, Test loss: 1.561, Test accuracy: 90.56
Round  47, Train loss: 1.526, Test loss: 1.560, Test accuracy: 90.65
Round  48, Train loss: 1.520, Test loss: 1.559, Test accuracy: 90.76
Round  49, Train loss: 1.524, Test loss: 1.558, Test accuracy: 90.76
Round  50, Train loss: 1.515, Test loss: 1.558, Test accuracy: 90.87
Round  51, Train loss: 1.520, Test loss: 1.557, Test accuracy: 90.70
Round  52, Train loss: 1.514, Test loss: 1.556, Test accuracy: 90.95
Round  53, Train loss: 1.508, Test loss: 1.558, Test accuracy: 90.82
Round  54, Train loss: 1.516, Test loss: 1.555, Test accuracy: 91.07
Round  55, Train loss: 1.517, Test loss: 1.556, Test accuracy: 91.01
Round  56, Train loss: 1.507, Test loss: 1.555, Test accuracy: 91.00
Round  57, Train loss: 1.514, Test loss: 1.555, Test accuracy: 91.04
Round  58, Train loss: 1.513, Test loss: 1.554, Test accuracy: 91.17
Round  59, Train loss: 1.508, Test loss: 1.553, Test accuracy: 91.22
Round  60, Train loss: 1.511, Test loss: 1.552, Test accuracy: 91.28
Round  61, Train loss: 1.508, Test loss: 1.553, Test accuracy: 91.25
Round  62, Train loss: 1.510, Test loss: 1.553, Test accuracy: 91.26
Round  63, Train loss: 1.504, Test loss: 1.552, Test accuracy: 91.23
Round  64, Train loss: 1.507, Test loss: 1.551, Test accuracy: 91.28
Round  65, Train loss: 1.508, Test loss: 1.552, Test accuracy: 91.33
Round  66, Train loss: 1.500, Test loss: 1.552, Test accuracy: 91.06
Round  67, Train loss: 1.506, Test loss: 1.552, Test accuracy: 91.18
Round  68, Train loss: 1.511, Test loss: 1.552, Test accuracy: 91.19
Round  69, Train loss: 1.500, Test loss: 1.551, Test accuracy: 91.24
Round  70, Train loss: 1.500, Test loss: 1.551, Test accuracy: 91.35
Round  71, Train loss: 1.500, Test loss: 1.550, Test accuracy: 91.43
Round  72, Train loss: 1.502, Test loss: 1.549, Test accuracy: 91.42
Round  73, Train loss: 1.509, Test loss: 1.549, Test accuracy: 91.44
Round  74, Train loss: 1.500, Test loss: 1.549, Test accuracy: 91.47
Round  75, Train loss: 1.497, Test loss: 1.548, Test accuracy: 91.53
Round  76, Train loss: 1.497, Test loss: 1.548, Test accuracy: 91.57
Round  77, Train loss: 1.505, Test loss: 1.549, Test accuracy: 91.52
Round  78, Train loss: 1.501, Test loss: 1.549, Test accuracy: 91.56
Round  79, Train loss: 1.501, Test loss: 1.549, Test accuracy: 91.53
Round  80, Train loss: 1.500, Test loss: 1.549, Test accuracy: 91.43
Round  81, Train loss: 1.499, Test loss: 1.548, Test accuracy: 91.49
Round  82, Train loss: 1.502, Test loss: 1.548, Test accuracy: 91.46
Round  83, Train loss: 1.496, Test loss: 1.549, Test accuracy: 91.44
Round  84, Train loss: 1.500, Test loss: 1.549, Test accuracy: 91.48
Round  85, Train loss: 1.494, Test loss: 1.549, Test accuracy: 91.29
Round  86, Train loss: 1.499, Test loss: 1.548, Test accuracy: 91.47
Round  87, Train loss: 1.496, Test loss: 1.548, Test accuracy: 91.46
Round  88, Train loss: 1.498, Test loss: 1.548, Test accuracy: 91.55
Round  89, Train loss: 1.499, Test loss: 1.547, Test accuracy: 91.52
Round  90, Train loss: 1.500, Test loss: 1.546, Test accuracy: 91.57
Round  91, Train loss: 1.496, Test loss: 1.546, Test accuracy: 91.58
Round  92, Train loss: 1.498, Test loss: 1.546, Test accuracy: 91.68
Round  93, Train loss: 1.497, Test loss: 1.546, Test accuracy: 91.65
Round  94, Train loss: 1.495, Test loss: 1.546, Test accuracy: 91.71
Round  95, Train loss: 1.496, Test loss: 1.546, Test accuracy: 91.78
Round  96, Train loss: 1.495, Test loss: 1.546, Test accuracy: 91.73
Round  97, Train loss: 1.492, Test loss: 1.546, Test accuracy: 91.78
Round  98, Train loss: 1.497, Test loss: 1.546, Test accuracy: 91.72/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.502, Test loss: 1.546, Test accuracy: 91.87
Final Round, Train loss: 1.497, Test loss: 1.546, Test accuracy: 91.70
Average accuracy final 10 rounds: 91.7075 

1464.5953578948975
[1.144723892211914, 2.289447784423828, 3.3081023693084717, 4.326756954193115, 5.445935487747192, 6.5651140213012695, 7.720194339752197, 8.875274658203125, 10.05892562866211, 11.242576599121094, 12.420680284500122, 13.59878396987915, 14.712285041809082, 15.825786113739014, 16.952881813049316, 18.07997751235962, 19.28310775756836, 20.4862380027771, 21.632781267166138, 22.779324531555176, 23.887059688568115, 24.994794845581055, 26.124056100845337, 27.25331735610962, 28.421566486358643, 29.589815616607666, 30.74164652824402, 31.89347743988037, 33.016931772232056, 34.14038610458374, 35.254517793655396, 36.36864948272705, 37.41401505470276, 38.45938062667847, 39.57668662071228, 40.693992614746094, 41.79447269439697, 42.89495277404785, 43.932961225509644, 44.970969676971436, 46.0489981174469, 47.12702655792236, 48.276286125183105, 49.42554569244385, 50.561041831970215, 51.69653797149658, 52.85284423828125, 54.00915050506592, 55.16408967971802, 56.31902885437012, 57.490570306777954, 58.66211175918579, 59.82073211669922, 60.97935247421265, 62.15676188468933, 63.334171295166016, 64.49282932281494, 65.65148735046387, 66.79626607894897, 67.94104480743408, 69.10910940170288, 70.27717399597168, 71.4371166229248, 72.59705924987793, 73.75546264648438, 74.91386604309082, 76.06718301773071, 77.2204999923706, 78.3610463142395, 79.5015926361084, 80.59068155288696, 81.67977046966553, 82.8526656627655, 84.02556085586548, 85.22487902641296, 86.42419719696045, 87.56280970573425, 88.70142221450806, 89.81303715705872, 90.92465209960938, 92.07775044441223, 93.23084878921509, 94.38200759887695, 95.53316640853882, 96.70578813552856, 97.87840986251831, 98.9691915512085, 100.05997323989868, 101.19873452186584, 102.33749580383301, 103.48353815078735, 104.6295804977417, 105.80602383613586, 106.98246717453003, 108.0846152305603, 109.18676328659058, 110.29982995986938, 111.4128966331482, 112.60057663917542, 113.78825664520264, 114.95594787597656, 116.12363910675049, 117.25680804252625, 118.389976978302, 119.50959038734436, 120.62920379638672, 121.7886266708374, 122.94804954528809, 124.06759858131409, 125.18714761734009, 126.33539485931396, 127.48364210128784, 128.62186217308044, 129.76008224487305, 130.9347333908081, 132.10938453674316, 133.26536226272583, 134.4213399887085, 135.58927583694458, 136.75721168518066, 137.85521817207336, 138.95322465896606, 139.96452641487122, 140.97582817077637, 141.98647022247314, 142.99711227416992, 144.01028680801392, 145.0234613418579, 146.02653527259827, 147.02960920333862, 148.06215023994446, 149.0946912765503, 150.12450218200684, 151.15431308746338, 152.10769057273865, 153.06106805801392, 154.0494978427887, 155.03792762756348, 156.08959555625916, 157.14126348495483, 158.19318866729736, 159.2451138496399, 160.22088646888733, 161.19665908813477, 162.2035539150238, 163.21044874191284, 164.24955010414124, 165.28865146636963, 166.32256412506104, 167.35647678375244, 168.37090373039246, 169.38533067703247, 170.41133522987366, 171.43733978271484, 172.47037076950073, 173.50340175628662, 174.50695490837097, 175.51050806045532, 176.53478837013245, 177.55906867980957, 178.57216453552246, 179.58526039123535, 180.5986042022705, 181.61194801330566, 182.59685134887695, 183.58175468444824, 184.62473917007446, 185.66772365570068, 186.72880601882935, 187.789888381958, 188.7810537815094, 189.7722191810608, 190.7741162776947, 191.7760133743286, 192.82000517845154, 193.86399698257446, 194.88237690925598, 195.9007568359375, 196.904687166214, 197.90861749649048, 198.98842644691467, 200.06823539733887, 201.109699010849, 202.15116262435913, 203.15910935401917, 204.1670560836792, 205.20375037193298, 206.24044466018677, 207.28501415252686, 208.32958364486694, 209.3722152709961, 210.41484689712524, 211.43106770515442, 212.4472885131836, 213.46608233451843, 214.48487615585327, 215.45026874542236, 216.41566133499146, 217.43808674812317, 218.46051216125488, 220.1197099685669, 221.7789077758789]
[21.416666666666668, 21.416666666666668, 23.466666666666665, 23.466666666666665, 23.758333333333333, 23.758333333333333, 25.658333333333335, 25.658333333333335, 30.091666666666665, 30.091666666666665, 30.275, 30.275, 38.916666666666664, 38.916666666666664, 41.825, 41.825, 47.31666666666667, 47.31666666666667, 51.38333333333333, 51.38333333333333, 58.65, 58.65, 67.95833333333333, 67.95833333333333, 73.875, 73.875, 76.96666666666667, 76.96666666666667, 78.95833333333333, 78.95833333333333, 79.36666666666666, 79.36666666666666, 80.575, 80.575, 81.91666666666667, 81.91666666666667, 83.24166666666666, 83.24166666666666, 84.23333333333333, 84.23333333333333, 85.04166666666667, 85.04166666666667, 86.3, 86.3, 86.51666666666667, 86.51666666666667, 87.39166666666667, 87.39166666666667, 87.56666666666666, 87.56666666666666, 88.25, 88.25, 88.375, 88.375, 88.79166666666667, 88.79166666666667, 89.09166666666667, 89.09166666666667, 89.61666666666666, 89.61666666666666, 89.5, 89.5, 90.04166666666667, 90.04166666666667, 90.13333333333334, 90.13333333333334, 89.875, 89.875, 90.09166666666667, 90.09166666666667, 89.95833333333333, 89.95833333333333, 90.075, 90.075, 90.31666666666666, 90.31666666666666, 90.35, 90.35, 90.19166666666666, 90.19166666666666, 90.19166666666666, 90.19166666666666, 90.26666666666667, 90.26666666666667, 90.31666666666666, 90.31666666666666, 90.325, 90.325, 90.36666666666666, 90.36666666666666, 90.38333333333334, 90.38333333333334, 90.55833333333334, 90.55833333333334, 90.65, 90.65, 90.75833333333334, 90.75833333333334, 90.75833333333334, 90.75833333333334, 90.86666666666666, 90.86666666666666, 90.7, 90.7, 90.95, 90.95, 90.81666666666666, 90.81666666666666, 91.06666666666666, 91.06666666666666, 91.00833333333334, 91.00833333333334, 91.0, 91.0, 91.04166666666667, 91.04166666666667, 91.16666666666667, 91.16666666666667, 91.21666666666667, 91.21666666666667, 91.275, 91.275, 91.25, 91.25, 91.25833333333334, 91.25833333333334, 91.23333333333333, 91.23333333333333, 91.275, 91.275, 91.325, 91.325, 91.05833333333334, 91.05833333333334, 91.18333333333334, 91.18333333333334, 91.19166666666666, 91.19166666666666, 91.24166666666666, 91.24166666666666, 91.35, 91.35, 91.43333333333334, 91.43333333333334, 91.41666666666667, 91.41666666666667, 91.44166666666666, 91.44166666666666, 91.46666666666667, 91.46666666666667, 91.525, 91.525, 91.56666666666666, 91.56666666666666, 91.51666666666667, 91.51666666666667, 91.55833333333334, 91.55833333333334, 91.525, 91.525, 91.43333333333334, 91.43333333333334, 91.49166666666666, 91.49166666666666, 91.45833333333333, 91.45833333333333, 91.44166666666666, 91.44166666666666, 91.48333333333333, 91.48333333333333, 91.29166666666667, 91.29166666666667, 91.46666666666667, 91.46666666666667, 91.45833333333333, 91.45833333333333, 91.55, 91.55, 91.51666666666667, 91.51666666666667, 91.56666666666666, 91.56666666666666, 91.58333333333333, 91.58333333333333, 91.68333333333334, 91.68333333333334, 91.65, 91.65, 91.70833333333333, 91.70833333333333, 91.78333333333333, 91.78333333333333, 91.73333333333333, 91.73333333333333, 91.775, 91.775, 91.725, 91.725, 91.86666666666666, 91.86666666666666, 91.7, 91.7]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.298, Test accuracy: 22.16
Round   1, Train loss: 2.293, Test loss: 2.286, Test accuracy: 20.54
Round   2, Train loss: 2.255, Test loss: 2.218, Test accuracy: 25.68
Round   3, Train loss: 2.166, Test loss: 2.125, Test accuracy: 45.29
Round   4, Train loss: 2.017, Test loss: 1.943, Test accuracy: 59.74
Round   5, Train loss: 1.835, Test loss: 1.837, Test accuracy: 66.17
Round   6, Train loss: 1.757, Test loss: 1.765, Test accuracy: 72.83
Round   7, Train loss: 1.684, Test loss: 1.735, Test accuracy: 74.88
Round   8, Train loss: 1.665, Test loss: 1.712, Test accuracy: 76.79
Round   9, Train loss: 1.661, Test loss: 1.680, Test accuracy: 79.67
Round  10, Train loss: 1.644, Test loss: 1.666, Test accuracy: 81.05
Round  11, Train loss: 1.630, Test loss: 1.649, Test accuracy: 82.38
Round  12, Train loss: 1.621, Test loss: 1.645, Test accuracy: 82.54
Round  13, Train loss: 1.617, Test loss: 1.637, Test accuracy: 83.33
Round  14, Train loss: 1.615, Test loss: 1.624, Test accuracy: 84.55
Round  15, Train loss: 1.597, Test loss: 1.617, Test accuracy: 85.22
Round  16, Train loss: 1.582, Test loss: 1.610, Test accuracy: 86.05
Round  17, Train loss: 1.569, Test loss: 1.602, Test accuracy: 86.60
Round  18, Train loss: 1.572, Test loss: 1.599, Test accuracy: 86.89
Round  19, Train loss: 1.549, Test loss: 1.592, Test accuracy: 87.58
Round  20, Train loss: 1.541, Test loss: 1.587, Test accuracy: 88.19
Round  21, Train loss: 1.535, Test loss: 1.582, Test accuracy: 88.46
Round  22, Train loss: 1.537, Test loss: 1.575, Test accuracy: 89.36
Round  23, Train loss: 1.527, Test loss: 1.570, Test accuracy: 89.70
Round  24, Train loss: 1.532, Test loss: 1.565, Test accuracy: 90.20
Round  25, Train loss: 1.525, Test loss: 1.558, Test accuracy: 91.02
Round  26, Train loss: 1.519, Test loss: 1.556, Test accuracy: 91.08
Round  27, Train loss: 1.516, Test loss: 1.556, Test accuracy: 91.09
Round  28, Train loss: 1.515, Test loss: 1.553, Test accuracy: 91.28
Round  29, Train loss: 1.514, Test loss: 1.552, Test accuracy: 91.43
Round  30, Train loss: 1.512, Test loss: 1.551, Test accuracy: 91.53
Round  31, Train loss: 1.513, Test loss: 1.551, Test accuracy: 91.56
Round  32, Train loss: 1.506, Test loss: 1.550, Test accuracy: 91.64
Round  33, Train loss: 1.510, Test loss: 1.550, Test accuracy: 91.58
Round  34, Train loss: 1.509, Test loss: 1.549, Test accuracy: 91.59
Round  35, Train loss: 1.507, Test loss: 1.548, Test accuracy: 91.77
Round  36, Train loss: 1.501, Test loss: 1.548, Test accuracy: 91.71
Round  37, Train loss: 1.502, Test loss: 1.548, Test accuracy: 91.70
Round  38, Train loss: 1.498, Test loss: 1.546, Test accuracy: 91.93
Round  39, Train loss: 1.503, Test loss: 1.546, Test accuracy: 91.87
Round  40, Train loss: 1.504, Test loss: 1.546, Test accuracy: 91.78
Round  41, Train loss: 1.498, Test loss: 1.545, Test accuracy: 91.97
Round  42, Train loss: 1.501, Test loss: 1.544, Test accuracy: 91.90
Round  43, Train loss: 1.495, Test loss: 1.543, Test accuracy: 92.12
Round  44, Train loss: 1.502, Test loss: 1.543, Test accuracy: 92.08
Round  45, Train loss: 1.498, Test loss: 1.543, Test accuracy: 92.00
Round  46, Train loss: 1.495, Test loss: 1.543, Test accuracy: 92.01
Round  47, Train loss: 1.497, Test loss: 1.542, Test accuracy: 92.12
Round  48, Train loss: 1.498, Test loss: 1.542, Test accuracy: 92.17
Round  49, Train loss: 1.495, Test loss: 1.541, Test accuracy: 92.25
Round  50, Train loss: 1.495, Test loss: 1.541, Test accuracy: 92.27
Round  51, Train loss: 1.496, Test loss: 1.540, Test accuracy: 92.37
Round  52, Train loss: 1.497, Test loss: 1.539, Test accuracy: 92.32
Round  53, Train loss: 1.492, Test loss: 1.539, Test accuracy: 92.43
Round  54, Train loss: 1.497, Test loss: 1.539, Test accuracy: 92.47
Round  55, Train loss: 1.492, Test loss: 1.539, Test accuracy: 92.38
Round  56, Train loss: 1.492, Test loss: 1.539, Test accuracy: 92.40
Round  57, Train loss: 1.494, Test loss: 1.539, Test accuracy: 92.38
Round  58, Train loss: 1.489, Test loss: 1.538, Test accuracy: 92.57
Round  59, Train loss: 1.494, Test loss: 1.538, Test accuracy: 92.41
Round  60, Train loss: 1.490, Test loss: 1.538, Test accuracy: 92.46
Round  61, Train loss: 1.490, Test loss: 1.538, Test accuracy: 92.53
Round  62, Train loss: 1.490, Test loss: 1.538, Test accuracy: 92.41
Round  63, Train loss: 1.490, Test loss: 1.538, Test accuracy: 92.39
Round  64, Train loss: 1.489, Test loss: 1.538, Test accuracy: 92.53
Round  65, Train loss: 1.488, Test loss: 1.538, Test accuracy: 92.51
Round  66, Train loss: 1.489, Test loss: 1.538, Test accuracy: 92.52
Round  67, Train loss: 1.489, Test loss: 1.537, Test accuracy: 92.62
Round  68, Train loss: 1.492, Test loss: 1.536, Test accuracy: 92.70
Round  69, Train loss: 1.486, Test loss: 1.537, Test accuracy: 92.62
Round  70, Train loss: 1.490, Test loss: 1.536, Test accuracy: 92.72
Round  71, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.62
Round  72, Train loss: 1.485, Test loss: 1.537, Test accuracy: 92.63
Round  73, Train loss: 1.489, Test loss: 1.536, Test accuracy: 92.76
Round  74, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.66
Round  75, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.67
Round  76, Train loss: 1.490, Test loss: 1.536, Test accuracy: 92.78
Round  77, Train loss: 1.488, Test loss: 1.535, Test accuracy: 92.76
Round  78, Train loss: 1.491, Test loss: 1.535, Test accuracy: 92.81
Round  79, Train loss: 1.483, Test loss: 1.535, Test accuracy: 92.82
Round  80, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.87
Round  81, Train loss: 1.488, Test loss: 1.535, Test accuracy: 92.81
Round  82, Train loss: 1.486, Test loss: 1.535, Test accuracy: 92.78
Round  83, Train loss: 1.486, Test loss: 1.535, Test accuracy: 92.73
Round  84, Train loss: 1.486, Test loss: 1.535, Test accuracy: 92.78
Round  85, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.81
Round  86, Train loss: 1.490, Test loss: 1.535, Test accuracy: 92.79
Round  87, Train loss: 1.488, Test loss: 1.535, Test accuracy: 92.77
Round  88, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.78
Round  89, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.79
Round  90, Train loss: 1.487, Test loss: 1.534, Test accuracy: 92.84
Round  91, Train loss: 1.487, Test loss: 1.534, Test accuracy: 92.92
Round  92, Train loss: 1.489, Test loss: 1.534, Test accuracy: 92.83
Round  93, Train loss: 1.487, Test loss: 1.534, Test accuracy: 92.91
Round  94, Train loss: 1.488, Test loss: 1.534, Test accuracy: 92.90
Round  95, Train loss: 1.483, Test loss: 1.534, Test accuracy: 92.99
Round  96, Train loss: 1.488, Test loss: 1.534, Test accuracy: 92.92
Round  97, Train loss: 1.486, Test loss: 1.534, Test accuracy: 92.94
Round  98, Train loss: 1.483, Test loss: 1.534, Test accuracy: 93.01/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.484, Test loss: 1.534, Test accuracy: 92.96
Final Round, Train loss: 1.486, Test loss: 1.534, Test accuracy: 92.89
Average accuracy final 10 rounds: 92.92333333333333 

1491.2249071598053
[1.3443148136138916, 2.688629627227783, 3.8894925117492676, 5.090355396270752, 6.249100208282471, 7.4078450202941895, 8.605440378189087, 9.803035736083984, 10.993014335632324, 12.182992935180664, 13.414763927459717, 14.64653491973877, 15.851973056793213, 17.057411193847656, 18.24536633491516, 19.433321475982666, 20.60528826713562, 21.777255058288574, 22.99303650856018, 24.208817958831787, 25.42117166519165, 26.633525371551514, 27.80776596069336, 28.982006549835205, 30.14248561859131, 31.302964687347412, 32.49832892417908, 33.69369316101074, 34.908724308013916, 36.12375545501709, 37.31027388572693, 38.49679231643677, 39.68870234489441, 40.88061237335205, 42.067941188812256, 43.25527000427246, 44.47039437294006, 45.685518741607666, 46.88672232627869, 48.08792591094971, 49.2689471244812, 50.449968338012695, 51.63508152961731, 52.820194721221924, 54.03812885284424, 55.25606298446655, 56.473700284957886, 57.69133758544922, 58.83600115776062, 59.98066473007202, 61.13870310783386, 62.2967414855957, 63.48384237289429, 64.67094326019287, 65.84691214561462, 67.02288103103638, 68.22229671478271, 69.42171239852905, 70.60147714614868, 71.78124189376831, 72.9421124458313, 74.10298299789429, 75.2818021774292, 76.46062135696411, 77.69117879867554, 78.92173624038696, 80.07823491096497, 81.23473358154297, 82.42070436477661, 83.60667514801025, 84.83949971199036, 86.07232427597046, 87.25502276420593, 88.4377212524414, 89.60122847557068, 90.76473569869995, 91.93470549583435, 93.10467529296875, 94.27113318443298, 95.43759107589722, 96.63506507873535, 97.83253908157349, 99.05959177017212, 100.28664445877075, 101.45957469940186, 102.63250494003296, 103.77371788024902, 104.91493082046509, 106.11994695663452, 107.32496309280396, 108.55307745933533, 109.7811918258667, 110.84539294242859, 111.90959405899048, 112.97755026817322, 114.04550647735596, 115.11630201339722, 116.18709754943848, 117.27152585983276, 118.35595417022705, 119.41882562637329, 120.48169708251953, 121.56508994102478, 122.64848279953003, 123.6924901008606, 124.73649740219116, 125.83233571052551, 126.92817401885986, 128.0235025882721, 129.11883115768433, 130.16481137275696, 131.2107915878296, 132.26126146316528, 133.31173133850098, 134.3878800868988, 135.46402883529663, 136.58850359916687, 137.7129783630371, 138.7981207370758, 139.8832631111145, 140.91729307174683, 141.95132303237915, 143.01395320892334, 144.07658338546753, 145.11492109298706, 146.1532588005066, 147.2461395263672, 148.33902025222778, 149.41713070869446, 150.49524116516113, 151.56496739387512, 152.6346936225891, 153.6909317970276, 154.74716997146606, 155.89556241035461, 157.04395484924316, 158.16431283950806, 159.28467082977295, 160.34376430511475, 161.40285778045654, 162.47888016700745, 163.55490255355835, 164.59923362731934, 165.64356470108032, 166.70203375816345, 167.76050281524658, 168.8390245437622, 169.91754627227783, 170.99162125587463, 172.06569623947144, 173.10919451713562, 174.1526927947998, 175.22723722457886, 176.3017816543579, 177.44604301452637, 178.59030437469482, 179.69181871414185, 180.79333305358887, 181.8516447544098, 182.9099564552307, 184.057865858078, 185.2057752609253, 186.357563495636, 187.50935173034668, 188.59563899040222, 189.68192625045776, 190.7562437057495, 191.83056116104126, 192.95873999595642, 194.08691883087158, 195.18698143959045, 196.28704404830933, 197.38276886940002, 198.47849369049072, 199.54216122627258, 200.60582876205444, 201.6928195953369, 202.77981042861938, 203.8462302684784, 204.9126501083374, 205.99974942207336, 207.08684873580933, 208.2069833278656, 209.32711791992188, 210.34652161598206, 211.36592531204224, 212.3908851146698, 213.41584491729736, 214.51078009605408, 215.6057152748108, 216.69311928749084, 217.7805233001709, 218.86588788032532, 219.95125246047974, 221.02565002441406, 222.1000475883484, 223.16756296157837, 224.23507833480835, 225.33034920692444, 226.42562007904053, 228.14054322242737, 229.8554663658142]
[22.158333333333335, 22.158333333333335, 20.541666666666668, 20.541666666666668, 25.683333333333334, 25.683333333333334, 45.291666666666664, 45.291666666666664, 59.74166666666667, 59.74166666666667, 66.175, 66.175, 72.83333333333333, 72.83333333333333, 74.88333333333334, 74.88333333333334, 76.79166666666667, 76.79166666666667, 79.675, 79.675, 81.05, 81.05, 82.38333333333334, 82.38333333333334, 82.54166666666667, 82.54166666666667, 83.33333333333333, 83.33333333333333, 84.55, 84.55, 85.21666666666667, 85.21666666666667, 86.05, 86.05, 86.6, 86.6, 86.89166666666667, 86.89166666666667, 87.58333333333333, 87.58333333333333, 88.19166666666666, 88.19166666666666, 88.45833333333333, 88.45833333333333, 89.35833333333333, 89.35833333333333, 89.7, 89.7, 90.2, 90.2, 91.01666666666667, 91.01666666666667, 91.075, 91.075, 91.09166666666667, 91.09166666666667, 91.28333333333333, 91.28333333333333, 91.43333333333334, 91.43333333333334, 91.53333333333333, 91.53333333333333, 91.55833333333334, 91.55833333333334, 91.64166666666667, 91.64166666666667, 91.58333333333333, 91.58333333333333, 91.59166666666667, 91.59166666666667, 91.76666666666667, 91.76666666666667, 91.70833333333333, 91.70833333333333, 91.7, 91.7, 91.93333333333334, 91.93333333333334, 91.86666666666666, 91.86666666666666, 91.78333333333333, 91.78333333333333, 91.975, 91.975, 91.9, 91.9, 92.125, 92.125, 92.075, 92.075, 92.0, 92.0, 92.00833333333334, 92.00833333333334, 92.11666666666666, 92.11666666666666, 92.175, 92.175, 92.25, 92.25, 92.26666666666667, 92.26666666666667, 92.36666666666666, 92.36666666666666, 92.31666666666666, 92.31666666666666, 92.43333333333334, 92.43333333333334, 92.46666666666667, 92.46666666666667, 92.375, 92.375, 92.4, 92.4, 92.38333333333334, 92.38333333333334, 92.56666666666666, 92.56666666666666, 92.40833333333333, 92.40833333333333, 92.45833333333333, 92.45833333333333, 92.525, 92.525, 92.40833333333333, 92.40833333333333, 92.39166666666667, 92.39166666666667, 92.525, 92.525, 92.50833333333334, 92.50833333333334, 92.51666666666667, 92.51666666666667, 92.625, 92.625, 92.7, 92.7, 92.61666666666666, 92.61666666666666, 92.71666666666667, 92.71666666666667, 92.61666666666666, 92.61666666666666, 92.63333333333334, 92.63333333333334, 92.75833333333334, 92.75833333333334, 92.65833333333333, 92.65833333333333, 92.675, 92.675, 92.775, 92.775, 92.75833333333334, 92.75833333333334, 92.80833333333334, 92.80833333333334, 92.81666666666666, 92.81666666666666, 92.86666666666666, 92.86666666666666, 92.80833333333334, 92.80833333333334, 92.78333333333333, 92.78333333333333, 92.73333333333333, 92.73333333333333, 92.775, 92.775, 92.80833333333334, 92.80833333333334, 92.79166666666667, 92.79166666666667, 92.76666666666667, 92.76666666666667, 92.775, 92.775, 92.79166666666667, 92.79166666666667, 92.84166666666667, 92.84166666666667, 92.925, 92.925, 92.83333333333333, 92.83333333333333, 92.90833333333333, 92.90833333333333, 92.9, 92.9, 92.99166666666666, 92.99166666666666, 92.925, 92.925, 92.94166666666666, 92.94166666666666, 93.00833333333334, 93.00833333333334, 92.95833333333333, 92.95833333333333, 92.89166666666667, 92.89166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.298, Test accuracy: 16.53
Round   1, Train loss: 2.293, Test loss: 2.290, Test accuracy: 30.00
Round   2, Train loss: 2.276, Test loss: 2.265, Test accuracy: 30.90
Round   3, Train loss: 2.187, Test loss: 2.179, Test accuracy: 40.58
Round   4, Train loss: 2.046, Test loss: 2.077, Test accuracy: 50.13
Round   5, Train loss: 1.942, Test loss: 1.998, Test accuracy: 55.10
Round   6, Train loss: 1.892, Test loss: 1.930, Test accuracy: 60.12
Round   7, Train loss: 1.818, Test loss: 1.863, Test accuracy: 66.06
Round   8, Train loss: 1.740, Test loss: 1.808, Test accuracy: 71.77
Round   9, Train loss: 1.688, Test loss: 1.782, Test accuracy: 73.68
Round  10, Train loss: 1.689, Test loss: 1.736, Test accuracy: 76.57
Round  11, Train loss: 1.636, Test loss: 1.725, Test accuracy: 77.41
Round  12, Train loss: 1.612, Test loss: 1.717, Test accuracy: 77.61
Round  13, Train loss: 1.618, Test loss: 1.710, Test accuracy: 78.13
Round  14, Train loss: 1.618, Test loss: 1.705, Test accuracy: 78.33
Round  15, Train loss: 1.627, Test loss: 1.683, Test accuracy: 79.31
Round  16, Train loss: 1.597, Test loss: 1.680, Test accuracy: 79.47
Round  17, Train loss: 1.609, Test loss: 1.677, Test accuracy: 79.64
Round  18, Train loss: 1.581, Test loss: 1.676, Test accuracy: 79.77
Round  19, Train loss: 1.579, Test loss: 1.675, Test accuracy: 79.72
Round  20, Train loss: 1.579, Test loss: 1.674, Test accuracy: 79.77
Round  21, Train loss: 1.583, Test loss: 1.673, Test accuracy: 79.80
Round  22, Train loss: 1.587, Test loss: 1.672, Test accuracy: 79.88
Round  23, Train loss: 1.583, Test loss: 1.671, Test accuracy: 79.86
Round  24, Train loss: 1.587, Test loss: 1.670, Test accuracy: 80.01
Round  25, Train loss: 1.578, Test loss: 1.669, Test accuracy: 80.01
Round  26, Train loss: 1.579, Test loss: 1.669, Test accuracy: 79.94
Round  27, Train loss: 1.577, Test loss: 1.668, Test accuracy: 79.86
Round  28, Train loss: 1.583, Test loss: 1.668, Test accuracy: 79.94
Round  29, Train loss: 1.571, Test loss: 1.667, Test accuracy: 80.02
Round  30, Train loss: 1.583, Test loss: 1.667, Test accuracy: 79.99
Round  31, Train loss: 1.576, Test loss: 1.667, Test accuracy: 80.02
Round  32, Train loss: 1.578, Test loss: 1.666, Test accuracy: 80.02
Round  33, Train loss: 1.574, Test loss: 1.666, Test accuracy: 80.00
Round  34, Train loss: 1.567, Test loss: 1.666, Test accuracy: 80.11
Round  35, Train loss: 1.573, Test loss: 1.666, Test accuracy: 80.04
Round  36, Train loss: 1.573, Test loss: 1.666, Test accuracy: 80.03
Round  37, Train loss: 1.571, Test loss: 1.666, Test accuracy: 80.02
Round  38, Train loss: 1.566, Test loss: 1.666, Test accuracy: 79.98
Round  39, Train loss: 1.562, Test loss: 1.664, Test accuracy: 80.06
Round  40, Train loss: 1.519, Test loss: 1.628, Test accuracy: 84.47
Round  41, Train loss: 1.509, Test loss: 1.622, Test accuracy: 84.93
Round  42, Train loss: 1.497, Test loss: 1.618, Test accuracy: 85.33
Round  43, Train loss: 1.490, Test loss: 1.616, Test accuracy: 85.50
Round  44, Train loss: 1.495, Test loss: 1.615, Test accuracy: 85.53
Round  45, Train loss: 1.490, Test loss: 1.613, Test accuracy: 85.67
Round  46, Train loss: 1.485, Test loss: 1.611, Test accuracy: 85.92
Round  47, Train loss: 1.490, Test loss: 1.609, Test accuracy: 86.06
Round  48, Train loss: 1.483, Test loss: 1.608, Test accuracy: 86.12
Round  49, Train loss: 1.484, Test loss: 1.607, Test accuracy: 86.17
Round  50, Train loss: 1.483, Test loss: 1.606, Test accuracy: 86.28
Round  51, Train loss: 1.485, Test loss: 1.605, Test accuracy: 86.31
Round  52, Train loss: 1.488, Test loss: 1.604, Test accuracy: 86.28
Round  53, Train loss: 1.481, Test loss: 1.604, Test accuracy: 86.33
Round  54, Train loss: 1.487, Test loss: 1.604, Test accuracy: 86.32
Round  55, Train loss: 1.481, Test loss: 1.603, Test accuracy: 86.42
Round  56, Train loss: 1.484, Test loss: 1.603, Test accuracy: 86.48
Round  57, Train loss: 1.480, Test loss: 1.603, Test accuracy: 86.44
Round  58, Train loss: 1.481, Test loss: 1.602, Test accuracy: 86.43
Round  59, Train loss: 1.482, Test loss: 1.602, Test accuracy: 86.50
Round  60, Train loss: 1.481, Test loss: 1.602, Test accuracy: 86.48
Round  61, Train loss: 1.476, Test loss: 1.602, Test accuracy: 86.45
Round  62, Train loss: 1.482, Test loss: 1.602, Test accuracy: 86.46
Round  63, Train loss: 1.479, Test loss: 1.602, Test accuracy: 86.54
Round  64, Train loss: 1.480, Test loss: 1.601, Test accuracy: 86.54
Round  65, Train loss: 1.482, Test loss: 1.601, Test accuracy: 86.54
Round  66, Train loss: 1.480, Test loss: 1.601, Test accuracy: 86.58
Round  67, Train loss: 1.481, Test loss: 1.601, Test accuracy: 86.60
Round  68, Train loss: 1.477, Test loss: 1.601, Test accuracy: 86.60
Round  69, Train loss: 1.482, Test loss: 1.601, Test accuracy: 86.58
Round  70, Train loss: 1.477, Test loss: 1.600, Test accuracy: 86.66
Round  71, Train loss: 1.481, Test loss: 1.600, Test accuracy: 86.69
Round  72, Train loss: 1.479, Test loss: 1.600, Test accuracy: 86.71
Round  73, Train loss: 1.481, Test loss: 1.600, Test accuracy: 86.62
Round  74, Train loss: 1.480, Test loss: 1.600, Test accuracy: 86.69
Round  75, Train loss: 1.480, Test loss: 1.600, Test accuracy: 86.70
Round  76, Train loss: 1.482, Test loss: 1.600, Test accuracy: 86.70
Round  77, Train loss: 1.477, Test loss: 1.600, Test accuracy: 86.71
Round  78, Train loss: 1.476, Test loss: 1.599, Test accuracy: 86.67
Round  79, Train loss: 1.478, Test loss: 1.599, Test accuracy: 86.73
Round  80, Train loss: 1.480, Test loss: 1.599, Test accuracy: 86.75
Round  81, Train loss: 1.478, Test loss: 1.599, Test accuracy: 86.72
Round  82, Train loss: 1.481, Test loss: 1.599, Test accuracy: 86.68
Round  83, Train loss: 1.479, Test loss: 1.599, Test accuracy: 86.67
Round  84, Train loss: 1.482, Test loss: 1.599, Test accuracy: 86.70
Round  85, Train loss: 1.478, Test loss: 1.599, Test accuracy: 86.70
Round  86, Train loss: 1.475, Test loss: 1.599, Test accuracy: 86.72
Round  87, Train loss: 1.478, Test loss: 1.599, Test accuracy: 86.67
Round  88, Train loss: 1.479, Test loss: 1.599, Test accuracy: 86.67
Round  89, Train loss: 1.479, Test loss: 1.599, Test accuracy: 86.67
Round  90, Train loss: 1.479, Test loss: 1.599, Test accuracy: 86.67
Round  91, Train loss: 1.480, Test loss: 1.599, Test accuracy: 86.69
Round  92, Train loss: 1.482, Test loss: 1.599, Test accuracy: 86.72
Round  93, Train loss: 1.479, Test loss: 1.599, Test accuracy: 86.75
Round  94, Train loss: 1.480, Test loss: 1.599, Test accuracy: 86.78
Round  95, Train loss: 1.477, Test loss: 1.598, Test accuracy: 86.72
Round  96, Train loss: 1.478, Test loss: 1.599, Test accuracy: 86.72
Round  97, Train loss: 1.478, Test loss: 1.598, Test accuracy: 86.75
Round  98, Train loss: 1.476, Test loss: 1.598, Test accuracy: 86.76
Round  99, Train loss: 1.478, Test loss: 1.598, Test accuracy: 86.82/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.479, Test loss: 1.598, Test accuracy: 86.85
Average accuracy final 10 rounds: 86.73916666666668 

1511.661565065384
[1.2590851783752441, 2.5181703567504883, 3.749950647354126, 4.981730937957764, 6.23357081413269, 7.485410690307617, 8.72631049156189, 9.967210292816162, 11.18976092338562, 12.412311553955078, 13.699485778808594, 14.98666000366211, 16.246419191360474, 17.506178379058838, 18.750125885009766, 19.994073390960693, 21.22144365310669, 22.448813915252686, 23.736154794692993, 25.0234956741333, 26.322089195251465, 27.62068271636963, 28.845895767211914, 30.0711088180542, 31.307135105133057, 32.543161392211914, 33.79792404174805, 35.05268669128418, 36.29404282569885, 37.535398960113525, 38.753132343292236, 39.97086572647095, 41.188639640808105, 42.406413555145264, 43.62606930732727, 44.84572505950928, 46.10637855529785, 47.367032051086426, 48.61286997795105, 49.858707904815674, 51.13662314414978, 52.41453838348389, 53.67260456085205, 54.930670738220215, 56.203245878219604, 57.475821018218994, 58.54754304885864, 59.61926507949829, 60.62166738510132, 61.624069690704346, 62.68571352958679, 63.74735736846924, 64.8573853969574, 65.96741342544556, 67.05446982383728, 68.141526222229, 69.21654319763184, 70.29156017303467, 71.34274387359619, 72.39392757415771, 73.44090723991394, 74.48788690567017, 75.58726572990417, 76.68664455413818, 77.8269259929657, 78.96720743179321, 80.04328489303589, 81.11936235427856, 82.13454794883728, 83.149733543396, 84.1731026172638, 85.19647169113159, 86.29347133636475, 87.3904709815979, 88.48995065689087, 89.58943033218384, 90.6772871017456, 91.76514387130737, 92.82678842544556, 93.88843297958374, 94.96717953681946, 96.04592609405518, 97.11131834983826, 98.17671060562134, 99.27359199523926, 100.37047338485718, 101.43255162239075, 102.49462985992432, 103.51609230041504, 104.53755474090576, 105.60922479629517, 106.68089485168457, 107.73069262504578, 108.78049039840698, 109.83841896057129, 110.8963475227356, 111.94350862503052, 112.99066972732544, 114.06483936309814, 115.13900899887085, 116.2063536643982, 117.27369832992554, 118.32803058624268, 119.38236284255981, 120.45455813407898, 121.52675342559814, 122.55539298057556, 123.58403253555298, 124.64935731887817, 125.71468210220337, 126.79568934440613, 127.87669658660889, 128.93500661849976, 129.99331665039062, 131.0057442188263, 132.01817178726196, 133.0301878452301, 134.04220390319824, 135.15259432792664, 136.26298475265503, 137.3521101474762, 138.44123554229736, 139.5102195739746, 140.57920360565186, 141.6596794128418, 142.74015522003174, 143.8266725540161, 144.9131898880005, 145.98462843894958, 147.05606698989868, 148.11397695541382, 149.17188692092896, 150.21895813941956, 151.26602935791016, 152.29137992858887, 153.31673049926758, 154.37634563446045, 155.43596076965332, 156.53782773017883, 157.63969469070435, 158.66682314872742, 159.6939516067505, 160.71633458137512, 161.73871755599976, 162.86172318458557, 163.9847288131714, 165.2354953289032, 166.486261844635, 167.76356410980225, 169.04086637496948, 170.3064308166504, 171.5719952583313, 172.8314425945282, 174.0908899307251, 175.41488075256348, 176.73887157440186, 177.9900631904602, 179.24125480651855, 180.48264527320862, 181.72403573989868, 182.98170924186707, 184.23938274383545, 185.55578923225403, 186.8721957206726, 188.18350315093994, 189.49481058120728, 190.79868483543396, 192.10255908966064, 193.3720874786377, 194.64161586761475, 195.86710023880005, 197.09258460998535, 198.3176941871643, 199.54280376434326, 200.73438453674316, 201.92596530914307, 203.07708764076233, 204.2282099723816, 205.4437940120697, 206.6593780517578, 207.82888555526733, 208.99839305877686, 210.20419836044312, 211.41000366210938, 212.61991906166077, 213.82983446121216, 215.04535269737244, 216.26087093353271, 217.514328956604, 218.7677869796753, 220.0141339302063, 221.2604808807373, 222.4832112789154, 223.7059416770935, 224.92058205604553, 226.13522243499756, 227.37787222862244, 228.62052202224731, 229.864155292511, 231.10778856277466, 232.98306918144226, 234.85834980010986]
[16.533333333333335, 16.533333333333335, 30.0, 30.0, 30.9, 30.9, 40.575, 40.575, 50.13333333333333, 50.13333333333333, 55.1, 55.1, 60.125, 60.125, 66.05833333333334, 66.05833333333334, 71.76666666666667, 71.76666666666667, 73.68333333333334, 73.68333333333334, 76.56666666666666, 76.56666666666666, 77.40833333333333, 77.40833333333333, 77.60833333333333, 77.60833333333333, 78.13333333333334, 78.13333333333334, 78.325, 78.325, 79.30833333333334, 79.30833333333334, 79.46666666666667, 79.46666666666667, 79.64166666666667, 79.64166666666667, 79.76666666666667, 79.76666666666667, 79.725, 79.725, 79.76666666666667, 79.76666666666667, 79.8, 79.8, 79.88333333333334, 79.88333333333334, 79.85833333333333, 79.85833333333333, 80.00833333333334, 80.00833333333334, 80.00833333333334, 80.00833333333334, 79.94166666666666, 79.94166666666666, 79.85833333333333, 79.85833333333333, 79.94166666666666, 79.94166666666666, 80.01666666666667, 80.01666666666667, 79.99166666666666, 79.99166666666666, 80.01666666666667, 80.01666666666667, 80.01666666666667, 80.01666666666667, 80.0, 80.0, 80.10833333333333, 80.10833333333333, 80.04166666666667, 80.04166666666667, 80.03333333333333, 80.03333333333333, 80.01666666666667, 80.01666666666667, 79.98333333333333, 79.98333333333333, 80.05833333333334, 80.05833333333334, 84.46666666666667, 84.46666666666667, 84.93333333333334, 84.93333333333334, 85.325, 85.325, 85.5, 85.5, 85.525, 85.525, 85.675, 85.675, 85.91666666666667, 85.91666666666667, 86.05833333333334, 86.05833333333334, 86.11666666666666, 86.11666666666666, 86.175, 86.175, 86.275, 86.275, 86.30833333333334, 86.30833333333334, 86.275, 86.275, 86.33333333333333, 86.33333333333333, 86.31666666666666, 86.31666666666666, 86.41666666666667, 86.41666666666667, 86.48333333333333, 86.48333333333333, 86.44166666666666, 86.44166666666666, 86.43333333333334, 86.43333333333334, 86.5, 86.5, 86.48333333333333, 86.48333333333333, 86.45, 86.45, 86.45833333333333, 86.45833333333333, 86.54166666666667, 86.54166666666667, 86.54166666666667, 86.54166666666667, 86.54166666666667, 86.54166666666667, 86.58333333333333, 86.58333333333333, 86.6, 86.6, 86.6, 86.6, 86.58333333333333, 86.58333333333333, 86.65833333333333, 86.65833333333333, 86.69166666666666, 86.69166666666666, 86.70833333333333, 86.70833333333333, 86.625, 86.625, 86.69166666666666, 86.69166666666666, 86.7, 86.7, 86.7, 86.7, 86.70833333333333, 86.70833333333333, 86.675, 86.675, 86.73333333333333, 86.73333333333333, 86.75, 86.75, 86.71666666666667, 86.71666666666667, 86.68333333333334, 86.68333333333334, 86.66666666666667, 86.66666666666667, 86.7, 86.7, 86.7, 86.7, 86.725, 86.725, 86.675, 86.675, 86.675, 86.675, 86.675, 86.675, 86.675, 86.675, 86.69166666666666, 86.69166666666666, 86.725, 86.725, 86.75, 86.75, 86.78333333333333, 86.78333333333333, 86.725, 86.725, 86.71666666666667, 86.71666666666667, 86.75, 86.75, 86.75833333333334, 86.75833333333334, 86.81666666666666, 86.81666666666666, 86.85, 86.85]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.695, Test loss: 2.288, Test accuracy: 30.42
Round   1, Train loss: 1.560, Test loss: 2.218, Test accuracy: 54.69
Round   2, Train loss: 1.387, Test loss: 2.076, Test accuracy: 62.42
Round   3, Train loss: 1.347, Test loss: 2.001, Test accuracy: 64.28
Round   4, Train loss: 1.329, Test loss: 1.978, Test accuracy: 64.90
Round   5, Train loss: 1.337, Test loss: 1.968, Test accuracy: 65.66
Round   6, Train loss: 1.308, Test loss: 1.960, Test accuracy: 66.37
Round   7, Train loss: 1.279, Test loss: 1.944, Test accuracy: 68.65
Round   8, Train loss: 1.273, Test loss: 1.935, Test accuracy: 69.59
Round   9, Train loss: 1.269, Test loss: 1.924, Test accuracy: 70.62
Round  10, Train loss: 1.267, Test loss: 1.916, Test accuracy: 72.09
Round  11, Train loss: 1.265, Test loss: 1.912, Test accuracy: 72.62
Round  12, Train loss: 1.260, Test loss: 1.909, Test accuracy: 73.03
Round  13, Train loss: 1.250, Test loss: 1.906, Test accuracy: 72.92
Round  14, Train loss: 1.250, Test loss: 1.905, Test accuracy: 72.79
Round  15, Train loss: 1.246, Test loss: 1.903, Test accuracy: 72.54
Round  16, Train loss: 1.254, Test loss: 1.901, Test accuracy: 72.35
Round  17, Train loss: 1.247, Test loss: 1.899, Test accuracy: 72.77
Round  18, Train loss: 1.249, Test loss: 1.899, Test accuracy: 72.61
Round  19, Train loss: 1.251, Test loss: 1.900, Test accuracy: 72.50
Round  20, Train loss: 1.243, Test loss: 1.899, Test accuracy: 72.21
Round  21, Train loss: 1.247, Test loss: 1.898, Test accuracy: 72.09
Round  22, Train loss: 1.244, Test loss: 1.898, Test accuracy: 72.12
Round  23, Train loss: 1.246, Test loss: 1.898, Test accuracy: 72.06
Round  24, Train loss: 1.244, Test loss: 1.899, Test accuracy: 72.00
Round  25, Train loss: 1.239, Test loss: 1.901, Test accuracy: 72.14
Round  26, Train loss: 1.207, Test loss: 1.898, Test accuracy: 72.88
Round  27, Train loss: 1.206, Test loss: 1.894, Test accuracy: 73.24
Round  28, Train loss: 1.197, Test loss: 1.887, Test accuracy: 74.68
Round  29, Train loss: 1.194, Test loss: 1.887, Test accuracy: 75.69
Round  30, Train loss: 1.181, Test loss: 1.885, Test accuracy: 75.60
Round  31, Train loss: 1.190, Test loss: 1.884, Test accuracy: 76.12
Round  32, Train loss: 1.184, Test loss: 1.883, Test accuracy: 76.31
Round  33, Train loss: 1.186, Test loss: 1.882, Test accuracy: 76.68
Round  34, Train loss: 1.190, Test loss: 1.880, Test accuracy: 76.75
Round  35, Train loss: 1.182, Test loss: 1.879, Test accuracy: 76.62
Round  36, Train loss: 1.184, Test loss: 1.878, Test accuracy: 76.46
Round  37, Train loss: 1.183, Test loss: 1.878, Test accuracy: 76.22
Round  38, Train loss: 1.183, Test loss: 1.877, Test accuracy: 76.11
Round  39, Train loss: 1.180, Test loss: 1.879, Test accuracy: 75.97
Round  40, Train loss: 1.179, Test loss: 1.879, Test accuracy: 75.92
Round  41, Train loss: 1.179, Test loss: 1.879, Test accuracy: 75.52
Round  42, Train loss: 1.183, Test loss: 1.878, Test accuracy: 75.47
Round  43, Train loss: 1.176, Test loss: 1.879, Test accuracy: 75.30
Round  44, Train loss: 1.178, Test loss: 1.880, Test accuracy: 75.25
Round  45, Train loss: 1.176, Test loss: 1.879, Test accuracy: 75.08
Round  46, Train loss: 1.174, Test loss: 1.881, Test accuracy: 74.74
Round  47, Train loss: 1.175, Test loss: 1.881, Test accuracy: 74.54
Round  48, Train loss: 1.175, Test loss: 1.881, Test accuracy: 74.36
Round  49, Train loss: 1.180, Test loss: 1.880, Test accuracy: 74.16
Round  50, Train loss: 1.182, Test loss: 1.881, Test accuracy: 74.14
Round  51, Train loss: 1.182, Test loss: 1.880, Test accuracy: 74.04
Round  52, Train loss: 1.180, Test loss: 1.880, Test accuracy: 73.94
Round  53, Train loss: 1.182, Test loss: 1.881, Test accuracy: 73.78
Round  54, Train loss: 1.176, Test loss: 1.881, Test accuracy: 73.61
Round  55, Train loss: 1.178, Test loss: 1.882, Test accuracy: 73.44
Round  56, Train loss: 1.176, Test loss: 1.882, Test accuracy: 73.38
Round  57, Train loss: 1.172, Test loss: 1.882, Test accuracy: 73.04
Round  58, Train loss: 1.169, Test loss: 1.883, Test accuracy: 73.07
Round  59, Train loss: 1.179, Test loss: 1.883, Test accuracy: 73.08
Round  60, Train loss: 1.172, Test loss: 1.884, Test accuracy: 72.82
Round  61, Train loss: 1.175, Test loss: 1.884, Test accuracy: 72.62
Round  62, Train loss: 1.178, Test loss: 1.884, Test accuracy: 72.39
Round  63, Train loss: 1.177, Test loss: 1.884, Test accuracy: 72.33
Round  64, Train loss: 1.175, Test loss: 1.885, Test accuracy: 72.11
Round  65, Train loss: 1.179, Test loss: 1.886, Test accuracy: 72.09
Round  66, Train loss: 1.175, Test loss: 1.887, Test accuracy: 71.84
Round  67, Train loss: 1.173, Test loss: 1.887, Test accuracy: 71.64
Round  68, Train loss: 1.174, Test loss: 1.888, Test accuracy: 71.55
Round  69, Train loss: 1.175, Test loss: 1.889, Test accuracy: 71.39
Round  70, Train loss: 1.177, Test loss: 1.888, Test accuracy: 71.47
Round  71, Train loss: 1.173, Test loss: 1.889, Test accuracy: 71.37
Round  72, Train loss: 1.177, Test loss: 1.889, Test accuracy: 71.20
Round  73, Train loss: 1.176, Test loss: 1.890, Test accuracy: 71.05
Round  74, Train loss: 1.175, Test loss: 1.891, Test accuracy: 70.95
Round  75, Train loss: 1.171, Test loss: 1.891, Test accuracy: 70.79
Round  76, Train loss: 1.180, Test loss: 1.891, Test accuracy: 70.61
Round  77, Train loss: 1.176, Test loss: 1.892, Test accuracy: 70.61
Round  78, Train loss: 1.172, Test loss: 1.893, Test accuracy: 70.62
Round  79, Train loss: 1.175, Test loss: 1.893, Test accuracy: 70.49
Round  80, Train loss: 1.175, Test loss: 1.893, Test accuracy: 70.38
Round  81, Train loss: 1.178, Test loss: 1.894, Test accuracy: 70.28
Round  82, Train loss: 1.177, Test loss: 1.894, Test accuracy: 70.17
Round  83, Train loss: 1.176, Test loss: 1.895, Test accuracy: 70.13
Round  84, Train loss: 1.174, Test loss: 1.894, Test accuracy: 70.20
Round  85, Train loss: 1.174, Test loss: 1.895, Test accuracy: 70.08
Round  86, Train loss: 1.163, Test loss: 1.897, Test accuracy: 70.02
Round  87, Train loss: 1.171, Test loss: 1.896, Test accuracy: 69.99
Round  88, Train loss: 1.170, Test loss: 1.897, Test accuracy: 69.92
Round  89, Train loss: 1.176, Test loss: 1.896, Test accuracy: 69.84
Round  90, Train loss: 1.174, Test loss: 1.897, Test accuracy: 69.84
Round  91, Train loss: 1.176, Test loss: 1.897, Test accuracy: 69.80
Round  92, Train loss: 1.173, Test loss: 1.897, Test accuracy: 69.72
Round  93, Train loss: 1.173, Test loss: 1.897, Test accuracy: 69.77
Round  94, Train loss: 1.175, Test loss: 1.897, Test accuracy: 69.85
Round  95, Train loss: 1.175, Test loss: 1.898, Test accuracy: 69.72
Round  96, Train loss: 1.170, Test loss: 1.898, Test accuracy: 69.54
Round  97, Train loss: 1.167, Test loss: 1.899, Test accuracy: 69.42
Round  98, Train loss: 1.170, Test loss: 1.899, Test accuracy: 69.41
Round  99, Train loss: 1.174, Test loss: 1.899, Test accuracy: 69.32
Final Round, Train loss: 1.173, Test loss: 1.902, Test accuracy: 68.94
Average accuracy final 10 rounds: 69.63916666666667
1656.6209440231323
[]
[30.416666666666668, 54.69166666666667, 62.416666666666664, 64.275, 64.9, 65.65833333333333, 66.36666666666666, 68.65, 69.59166666666667, 70.61666666666666, 72.09166666666667, 72.625, 73.03333333333333, 72.925, 72.79166666666667, 72.54166666666667, 72.35, 72.76666666666667, 72.60833333333333, 72.5, 72.20833333333333, 72.09166666666667, 72.11666666666666, 72.05833333333334, 72.0, 72.14166666666667, 72.88333333333334, 73.24166666666666, 74.68333333333334, 75.69166666666666, 75.6, 76.11666666666666, 76.30833333333334, 76.68333333333334, 76.75, 76.61666666666666, 76.45833333333333, 76.21666666666667, 76.10833333333333, 75.975, 75.925, 75.51666666666667, 75.46666666666667, 75.3, 75.25, 75.08333333333333, 74.74166666666666, 74.54166666666667, 74.35833333333333, 74.15833333333333, 74.14166666666667, 74.04166666666667, 73.94166666666666, 73.775, 73.60833333333333, 73.44166666666666, 73.375, 73.04166666666667, 73.06666666666666, 73.075, 72.81666666666666, 72.625, 72.39166666666667, 72.325, 72.10833333333333, 72.09166666666667, 71.84166666666667, 71.64166666666667, 71.55, 71.39166666666667, 71.475, 71.36666666666666, 71.2, 71.05, 70.95, 70.79166666666667, 70.60833333333333, 70.60833333333333, 70.61666666666666, 70.49166666666666, 70.38333333333334, 70.28333333333333, 70.16666666666667, 70.13333333333334, 70.2, 70.08333333333333, 70.01666666666667, 69.99166666666666, 69.925, 69.84166666666667, 69.84166666666667, 69.8, 69.725, 69.76666666666667, 69.85, 69.71666666666667, 69.54166666666667, 69.425, 69.40833333333333, 69.31666666666666, 68.94166666666666]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.286, Test loss: 2.274, Test accuracy: 18.32
Round   1, Train loss: 2.106, Test loss: 2.149, Test accuracy: 38.18
Round   2, Train loss: 1.937, Test loss: 2.098, Test accuracy: 42.48
Round   3, Train loss: 1.339, Test loss: 1.930, Test accuracy: 61.62
Round   4, Train loss: 1.745, Test loss: 1.893, Test accuracy: 61.59
Round   5, Train loss: 1.507, Test loss: 1.826, Test accuracy: 66.18
Round   6, Train loss: 1.643, Test loss: 1.807, Test accuracy: 68.97
Round   7, Train loss: 1.584, Test loss: 1.777, Test accuracy: 71.16
Round   8, Train loss: 1.600, Test loss: 1.761, Test accuracy: 71.37
Round   9, Train loss: 1.709, Test loss: 1.766, Test accuracy: 70.98
Round  10, Train loss: 1.589, Test loss: 1.748, Test accuracy: 72.55
Round  11, Train loss: 1.483, Test loss: 1.746, Test accuracy: 72.48
Round  12, Train loss: 1.457, Test loss: 1.737, Test accuracy: 73.33
Round  13, Train loss: 1.528, Test loss: 1.726, Test accuracy: 74.49
Round  14, Train loss: 1.618, Test loss: 1.689, Test accuracy: 78.52
Round  15, Train loss: 1.412, Test loss: 1.670, Test accuracy: 80.33
Round  16, Train loss: 1.434, Test loss: 1.673, Test accuracy: 80.04
Round  17, Train loss: 1.265, Test loss: 1.668, Test accuracy: 80.44
Round  18, Train loss: 1.280, Test loss: 1.649, Test accuracy: 82.25
Round  19, Train loss: 1.306, Test loss: 1.640, Test accuracy: 82.97
Round  20, Train loss: 1.349, Test loss: 1.628, Test accuracy: 84.36
Round  21, Train loss: 1.451, Test loss: 1.624, Test accuracy: 84.68
Round  22, Train loss: 1.211, Test loss: 1.612, Test accuracy: 85.73
Round  23, Train loss: 1.225, Test loss: 1.602, Test accuracy: 86.64
Round  24, Train loss: 1.141, Test loss: 1.598, Test accuracy: 86.95
Round  25, Train loss: 1.235, Test loss: 1.614, Test accuracy: 85.30
Round  26, Train loss: 1.026, Test loss: 1.597, Test accuracy: 87.03
Round  27, Train loss: 1.174, Test loss: 1.607, Test accuracy: 86.02
Round  28, Train loss: 1.193, Test loss: 1.607, Test accuracy: 85.89
Round  29, Train loss: 0.951, Test loss: 1.607, Test accuracy: 85.88
Round  30, Train loss: 1.193, Test loss: 1.615, Test accuracy: 85.01
Round  31, Train loss: 1.193, Test loss: 1.608, Test accuracy: 85.69
Round  32, Train loss: 0.994, Test loss: 1.607, Test accuracy: 85.77
Round  33, Train loss: 0.996, Test loss: 1.599, Test accuracy: 86.53
Round  34, Train loss: 1.103, Test loss: 1.598, Test accuracy: 86.60
Round  35, Train loss: 0.979, Test loss: 1.598, Test accuracy: 86.59
Round  36, Train loss: 0.856, Test loss: 1.597, Test accuracy: 86.67
Round  37, Train loss: 0.962, Test loss: 1.600, Test accuracy: 86.40
Round  38, Train loss: 0.811, Test loss: 1.593, Test accuracy: 87.14
Round  39, Train loss: 0.855, Test loss: 1.592, Test accuracy: 87.39
Round  40, Train loss: 0.697, Test loss: 1.579, Test accuracy: 88.72
Round  41, Train loss: 0.732, Test loss: 1.570, Test accuracy: 89.50
Round  42, Train loss: 0.832, Test loss: 1.565, Test accuracy: 90.00
Round  43, Train loss: 0.872, Test loss: 1.560, Test accuracy: 90.55
Round  44, Train loss: 0.886, Test loss: 1.559, Test accuracy: 90.53
Round  45, Train loss: 0.923, Test loss: 1.559, Test accuracy: 90.45
Round  46, Train loss: 0.844, Test loss: 1.559, Test accuracy: 90.49
Round  47, Train loss: 0.707, Test loss: 1.560, Test accuracy: 90.44
Round  48, Train loss: 0.969, Test loss: 1.560, Test accuracy: 90.48
Round  49, Train loss: 0.708, Test loss: 1.559, Test accuracy: 90.62
Round  50, Train loss: 0.680, Test loss: 1.557, Test accuracy: 90.78
Round  51, Train loss: 0.808, Test loss: 1.559, Test accuracy: 90.54
Round  52, Train loss: 0.822, Test loss: 1.558, Test accuracy: 90.65
Round  53, Train loss: 0.650, Test loss: 1.558, Test accuracy: 90.64
Round  54, Train loss: 0.926, Test loss: 1.558, Test accuracy: 90.62
Round  55, Train loss: 0.698, Test loss: 1.557, Test accuracy: 90.73
Round  56, Train loss: 0.885, Test loss: 1.557, Test accuracy: 90.75
Round  57, Train loss: 0.937, Test loss: 1.554, Test accuracy: 91.00
Round  58, Train loss: 0.549, Test loss: 1.554, Test accuracy: 91.04
Round  59, Train loss: 0.721, Test loss: 1.555, Test accuracy: 90.93
Round  60, Train loss: 0.744, Test loss: 1.554, Test accuracy: 90.95
Round  61, Train loss: 0.843, Test loss: 1.555, Test accuracy: 90.84
Round  62, Train loss: 0.747, Test loss: 1.556, Test accuracy: 90.81
Round  63, Train loss: 0.755, Test loss: 1.557, Test accuracy: 90.64
Round  64, Train loss: 0.750, Test loss: 1.556, Test accuracy: 90.77
Round  65, Train loss: 0.728, Test loss: 1.556, Test accuracy: 90.79
Round  66, Train loss: 0.684, Test loss: 1.555, Test accuracy: 90.82
Round  67, Train loss: 0.661, Test loss: 1.555, Test accuracy: 90.83
Round  68, Train loss: 0.653, Test loss: 1.555, Test accuracy: 90.82
Round  69, Train loss: 0.629, Test loss: 1.557, Test accuracy: 90.62
Round  70, Train loss: 0.871, Test loss: 1.555, Test accuracy: 90.78
Round  71, Train loss: 0.584, Test loss: 1.554, Test accuracy: 90.97
Round  72, Train loss: 0.499, Test loss: 1.555, Test accuracy: 90.83
Round  73, Train loss: 0.668, Test loss: 1.555, Test accuracy: 90.83
Round  74, Train loss: 0.709, Test loss: 1.556, Test accuracy: 90.71
Round  75, Train loss: 0.763, Test loss: 1.555, Test accuracy: 90.80
Round  76, Train loss: 0.741, Test loss: 1.555, Test accuracy: 90.83
Round  77, Train loss: 0.662, Test loss: 1.552, Test accuracy: 91.13
Round  78, Train loss: 0.768, Test loss: 1.553, Test accuracy: 91.00
Round  79, Train loss: 0.802, Test loss: 1.552, Test accuracy: 91.10
Round  80, Train loss: 0.716, Test loss: 1.551, Test accuracy: 91.16
Round  81, Train loss: 0.708, Test loss: 1.552, Test accuracy: 91.15
Round  82, Train loss: 0.573, Test loss: 1.552, Test accuracy: 91.10
Round  83, Train loss: 0.790, Test loss: 1.552, Test accuracy: 91.10
Round  84, Train loss: 0.702, Test loss: 1.551, Test accuracy: 91.19
Round  85, Train loss: 0.804, Test loss: 1.550, Test accuracy: 91.30
Round  86, Train loss: 0.793, Test loss: 1.551, Test accuracy: 91.19
Round  87, Train loss: 0.857, Test loss: 1.552, Test accuracy: 91.20
Round  88, Train loss: 0.633, Test loss: 1.551, Test accuracy: 91.22
Round  89, Train loss: 0.786, Test loss: 1.549, Test accuracy: 91.39
Round  90, Train loss: 0.781, Test loss: 1.550, Test accuracy: 91.30
Round  91, Train loss: 0.668, Test loss: 1.550, Test accuracy: 91.27
Round  92, Train loss: 0.742, Test loss: 1.549, Test accuracy: 91.42
Round  93, Train loss: 0.692, Test loss: 1.549, Test accuracy: 91.36
Round  94, Train loss: 0.649, Test loss: 1.550, Test accuracy: 91.26
Round  95, Train loss: 0.812, Test loss: 1.548, Test accuracy: 91.48
Round  96, Train loss: 0.676, Test loss: 1.547, Test accuracy: 91.54
Round  97, Train loss: 0.633, Test loss: 1.547, Test accuracy: 91.65
Round  98, Train loss: 0.724, Test loss: 1.548, Test accuracy: 91.53
Round  99, Train loss: 0.749, Test loss: 1.547, Test accuracy: 91.53
Final Round, Train loss: 1.503, Test loss: 1.535, Test accuracy: 92.94
Average accuracy final 10 rounds: 91.43475
Average global accuracy final 10 rounds: 91.43475
4146.6141221523285
[]
[18.3225, 38.18, 42.48, 61.6175, 61.595, 66.1825, 68.9675, 71.155, 71.3675, 70.9775, 72.5525, 72.48, 73.3325, 74.4925, 78.52, 80.325, 80.0425, 80.44, 82.245, 82.9725, 84.3625, 84.6825, 85.7325, 86.635, 86.9525, 85.2975, 87.025, 86.0225, 85.895, 85.88, 85.01, 85.685, 85.7675, 86.535, 86.5975, 86.595, 86.675, 86.3975, 87.135, 87.395, 88.7225, 89.4975, 90.005, 90.5525, 90.535, 90.4525, 90.4925, 90.4425, 90.485, 90.6225, 90.785, 90.54, 90.6475, 90.645, 90.625, 90.735, 90.755, 91.0, 91.0375, 90.9325, 90.9475, 90.845, 90.8125, 90.635, 90.77, 90.79, 90.8225, 90.83, 90.8175, 90.62, 90.7825, 90.9725, 90.825, 90.8325, 90.7125, 90.795, 90.835, 91.1275, 90.9975, 91.1, 91.1625, 91.1525, 91.1, 91.0975, 91.195, 91.2975, 91.1875, 91.1975, 91.215, 91.395, 91.2975, 91.265, 91.42, 91.36, 91.2625, 91.4825, 91.5425, 91.6475, 91.535, 91.535, 92.9425]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.80
Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 7.78
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.80
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 7.77
Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.88
Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 7.92
Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.88
Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 7.88
Round   4, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.92
Round   4, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 7.98
Round   5, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.94
Round   5, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.03
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 7.96
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.07
Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.02
Round   7, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.14
Round   8, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.06
Round   8, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.06
Round   9, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.07
Round   9, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.06
Round  10, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.05
Round  10, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.14
Round  11, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.10
Round  11, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.26
Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.12
Round  12, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.20
Round  13, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.19
Round  13, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.25
Round  14, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.27
Round  14, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.28
Round  15, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.29
Round  15, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.30
Round  16, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.28
Round  16, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.34
Round  17, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.24
Round  17, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.36
Round  18, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.25
Round  18, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.37
Round  19, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.31
Round  19, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.43
Round  20, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.37
Round  20, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.44
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.46
Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.50
Round  22, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.41
Round  22, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.53
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.44
Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.54
Round  24, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.48
Round  24, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.53
Round  25, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.53
Round  25, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.55
Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.59
Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.62
Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.66
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.69
Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.66
Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.75
Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.72
Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.72
Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.74
Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.77
Round  31, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.78
Round  31, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.73
Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.72
Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.82
Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.72
Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.83
Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.75
Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.88
Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.74
Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.88
Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.79
Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.91
Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.83
Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.93
Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.89
Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.95
Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.88
Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.97
Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.88
Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.99
Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.96
Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.04
Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.02
Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08
Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.03
Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.11
Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.05
Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.10
Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.12
Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.14
Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.18
Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.23
Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.19
Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.28
Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.20
Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.32
Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.29
Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.38
Round  50, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.34
Round  50, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.44
Round  51, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.35
Round  51, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.47
Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.43
Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  53, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.47
Round  53, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.62
Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.54
Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.67
Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.60
Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.73
Round  56, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.63
Round  56, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.72
Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.68
Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.80
Round  58, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.77
Round  58, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.83
Round  59, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.82
Round  59, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.83
Round  60, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.83
Round  60, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.95
Round  61, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.88
Round  61, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.00
Round  62, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.91
Round  62, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.09
Round  63, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.95
Round  63, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12
Round  64, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.04
Round  64, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.13
Round  65, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.07
Round  65, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.19
Round  66, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.10
Round  66, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.24
Round  67, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.15
Round  67, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.26
Round  68, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.16
Round  68, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.26
Round  69, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.18
Round  69, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.31
Round  70, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.21
Round  70, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.34
Round  71, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.23
Round  71, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.32
Round  72, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.27
Round  72, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.35
Round  73, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.32
Round  73, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.38
Round  74, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.38
Round  74, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.44
Round  75, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.45
Round  75, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.47
Round  76, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.49
Round  76, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.59
Round  77, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.51
Round  77, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.62
Round  78, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.50
Round  78, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.63
Round  79, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.54
Round  79, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.74
Round  80, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.62
Round  80, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.73
Round  81, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.70
Round  81, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 10.78
Round  82, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.76
Round  82, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.76
Round  83, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.75
Round  83, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.84
Round  84, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.81
Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.97
Round  85, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.90
Round  85, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.06
Round  86, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.92
Round  86, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.21
Round  87, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.11
Round  87, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.19
Round  88, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.13
Round  88, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.27
Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.21
Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.28
Round  90, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.26
Round  90, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.42
Round  91, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.37
Round  91, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.55
Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.44
Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.57
Round  93, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.60
Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.72
Round  94, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.66
Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.77
Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.65
Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.73
Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.68
Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.88
Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.77/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.95
Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.85
Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.20
Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.99
Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.12
Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.24
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.12
Average accuracy final 10 rounds: 11.625833333333334 

Average global accuracy final 10 rounds: 11.790000000000001 

1538.9132664203644
[1.3532044887542725, 2.6342616081237793, 3.966350793838501, 5.289721488952637, 6.5362420082092285, 7.800758361816406, 9.039989948272705, 10.341791152954102, 11.58493971824646, 12.847169637680054, 14.217298984527588, 15.58506464958191, 16.838356494903564, 18.108829736709595, 19.44683837890625, 20.80109214782715, 22.085822343826294, 23.352311849594116, 24.66079306602478, 25.999967575073242, 27.32211661338806, 28.58534550666809, 29.861334323883057, 31.233317613601685, 32.54749298095703, 33.783979177474976, 34.96440505981445, 36.21650171279907, 37.4117648601532, 38.66688394546509, 39.89256024360657, 41.19761323928833, 42.544851303100586, 43.832486391067505, 45.09209060668945, 46.292338371276855, 47.670711040496826, 48.93832564353943, 50.19951510429382, 51.465052127838135, 52.79139709472656, 54.07193660736084, 55.34259343147278, 56.60689067840576, 57.90203666687012, 59.19431281089783, 60.480560302734375, 61.74802923202515, 63.03634452819824, 64.39605236053467, 65.72396087646484, 66.97266507148743, 68.21669316291809, 69.54377126693726, 70.85295915603638, 72.13944482803345, 73.40432810783386, 74.70284628868103, 76.0421929359436, 77.35075235366821, 78.59107971191406, 79.88983249664307, 81.2093095779419, 82.50297689437866, 83.75930643081665, 84.99848294258118, 86.27882528305054, 87.64666318893433, 88.94084048271179, 90.17843341827393, 91.44748163223267, 92.74330568313599, 94.07249307632446, 95.32809543609619, 96.59898138046265, 97.90640950202942, 99.19781446456909, 100.4785327911377, 101.78397870063782, 103.04479217529297, 104.32854747772217, 105.68084120750427, 106.95814204216003, 108.22892427444458, 109.51749396324158, 110.81140065193176, 112.0928840637207, 113.3735363483429, 114.67956471443176, 115.986243724823, 117.29441690444946, 118.59514331817627, 119.87928771972656, 121.13443398475647, 122.46401500701904, 123.80354952812195, 125.12195134162903, 126.38397312164307, 127.69129538536072, 129.02139019966125, 131.23050141334534]
[7.8, 7.8, 7.883333333333334, 7.875, 7.925, 7.941666666666666, 7.958333333333333, 8.016666666666667, 8.058333333333334, 8.066666666666666, 8.05, 8.1, 8.125, 8.191666666666666, 8.266666666666667, 8.291666666666666, 8.275, 8.241666666666667, 8.25, 8.308333333333334, 8.366666666666667, 8.458333333333334, 8.408333333333333, 8.441666666666666, 8.483333333333333, 8.525, 8.591666666666667, 8.658333333333333, 8.658333333333333, 8.725, 8.741666666666667, 8.775, 8.725, 8.716666666666667, 8.75, 8.741666666666667, 8.791666666666666, 8.833333333333334, 8.891666666666667, 8.875, 8.883333333333333, 8.958333333333334, 9.016666666666667, 9.033333333333333, 9.05, 9.125, 9.183333333333334, 9.191666666666666, 9.2, 9.291666666666666, 9.341666666666667, 9.35, 9.433333333333334, 9.475, 9.541666666666666, 9.6, 9.633333333333333, 9.683333333333334, 9.766666666666667, 9.816666666666666, 9.833333333333334, 9.875, 9.908333333333333, 9.95, 10.041666666666666, 10.075, 10.1, 10.15, 10.158333333333333, 10.175, 10.208333333333334, 10.233333333333333, 10.266666666666667, 10.325, 10.375, 10.45, 10.491666666666667, 10.508333333333333, 10.5, 10.541666666666666, 10.625, 10.7, 10.758333333333333, 10.75, 10.808333333333334, 10.9, 10.916666666666666, 11.108333333333333, 11.133333333333333, 11.208333333333334, 11.258333333333333, 11.366666666666667, 11.441666666666666, 11.6, 11.658333333333333, 11.65, 11.675, 11.766666666666667, 11.85, 11.991666666666667, 12.241666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.300, Test accuracy: 25.89
Round   1, Train loss: 2.300, Test loss: 2.296, Test accuracy: 35.52
Round   2, Train loss: 2.296, Test loss: 2.286, Test accuracy: 47.89
Round   3, Train loss: 2.282, Test loss: 2.207, Test accuracy: 35.88
Round   4, Train loss: 2.194, Test loss: 2.000, Test accuracy: 50.96
Round   5, Train loss: 2.011, Test loss: 1.851, Test accuracy: 63.63
Round   6, Train loss: 1.908, Test loss: 1.770, Test accuracy: 72.58
Round   7, Train loss: 1.788, Test loss: 1.744, Test accuracy: 73.24
Round   8, Train loss: 1.763, Test loss: 1.734, Test accuracy: 73.46
Round   9, Train loss: 1.748, Test loss: 1.727, Test accuracy: 74.12
Round  10, Train loss: 1.731, Test loss: 1.725, Test accuracy: 73.97
Round  11, Train loss: 1.723, Test loss: 1.720, Test accuracy: 74.47
Round  12, Train loss: 1.719, Test loss: 1.718, Test accuracy: 74.66
Round  13, Train loss: 1.718, Test loss: 1.717, Test accuracy: 74.53
Round  14, Train loss: 1.702, Test loss: 1.714, Test accuracy: 74.85
Round  15, Train loss: 1.706, Test loss: 1.713, Test accuracy: 75.02
Round  16, Train loss: 1.701, Test loss: 1.712, Test accuracy: 75.08
Round  17, Train loss: 1.689, Test loss: 1.712, Test accuracy: 75.12
Round  18, Train loss: 1.697, Test loss: 1.709, Test accuracy: 75.47
Round  19, Train loss: 1.693, Test loss: 1.708, Test accuracy: 75.52
Round  20, Train loss: 1.693, Test loss: 1.708, Test accuracy: 75.30
Round  21, Train loss: 1.688, Test loss: 1.694, Test accuracy: 77.06
Round  22, Train loss: 1.673, Test loss: 1.666, Test accuracy: 80.32
Round  23, Train loss: 1.655, Test loss: 1.655, Test accuracy: 81.43
Round  24, Train loss: 1.645, Test loss: 1.649, Test accuracy: 82.02
Round  25, Train loss: 1.628, Test loss: 1.646, Test accuracy: 82.14
Round  26, Train loss: 1.630, Test loss: 1.643, Test accuracy: 82.46
Round  27, Train loss: 1.619, Test loss: 1.642, Test accuracy: 82.42
Round  28, Train loss: 1.610, Test loss: 1.641, Test accuracy: 82.63
Round  29, Train loss: 1.614, Test loss: 1.638, Test accuracy: 82.67
Round  30, Train loss: 1.607, Test loss: 1.638, Test accuracy: 82.69
Round  31, Train loss: 1.612, Test loss: 1.637, Test accuracy: 82.97
Round  32, Train loss: 1.604, Test loss: 1.636, Test accuracy: 82.78
Round  33, Train loss: 1.612, Test loss: 1.635, Test accuracy: 82.88
Round  34, Train loss: 1.596, Test loss: 1.634, Test accuracy: 83.06
Round  35, Train loss: 1.597, Test loss: 1.631, Test accuracy: 83.24
Round  36, Train loss: 1.603, Test loss: 1.630, Test accuracy: 83.33
Round  37, Train loss: 1.590, Test loss: 1.630, Test accuracy: 83.54
Round  38, Train loss: 1.598, Test loss: 1.629, Test accuracy: 83.45
Round  39, Train loss: 1.596, Test loss: 1.629, Test accuracy: 83.30
Round  40, Train loss: 1.592, Test loss: 1.628, Test accuracy: 83.67
Round  41, Train loss: 1.592, Test loss: 1.628, Test accuracy: 83.63
Round  42, Train loss: 1.591, Test loss: 1.627, Test accuracy: 83.53
Round  43, Train loss: 1.597, Test loss: 1.626, Test accuracy: 83.62
Round  44, Train loss: 1.590, Test loss: 1.626, Test accuracy: 83.54
Round  45, Train loss: 1.591, Test loss: 1.625, Test accuracy: 83.79
Round  46, Train loss: 1.589, Test loss: 1.625, Test accuracy: 83.68
Round  47, Train loss: 1.593, Test loss: 1.624, Test accuracy: 83.88
Round  48, Train loss: 1.592, Test loss: 1.625, Test accuracy: 83.88
Round  49, Train loss: 1.584, Test loss: 1.623, Test accuracy: 84.00
Round  50, Train loss: 1.596, Test loss: 1.623, Test accuracy: 83.96
Round  51, Train loss: 1.594, Test loss: 1.623, Test accuracy: 83.88
Round  52, Train loss: 1.583, Test loss: 1.622, Test accuracy: 83.99
Round  53, Train loss: 1.585, Test loss: 1.622, Test accuracy: 83.89
Round  54, Train loss: 1.584, Test loss: 1.622, Test accuracy: 84.11
Round  55, Train loss: 1.585, Test loss: 1.620, Test accuracy: 84.23
Round  56, Train loss: 1.590, Test loss: 1.621, Test accuracy: 84.17
Round  57, Train loss: 1.581, Test loss: 1.620, Test accuracy: 84.15
Round  58, Train loss: 1.580, Test loss: 1.620, Test accuracy: 84.13
Round  59, Train loss: 1.581, Test loss: 1.620, Test accuracy: 84.23
Round  60, Train loss: 1.583, Test loss: 1.619, Test accuracy: 84.26
Round  61, Train loss: 1.581, Test loss: 1.619, Test accuracy: 84.38
Round  62, Train loss: 1.578, Test loss: 1.619, Test accuracy: 84.42
Round  63, Train loss: 1.583, Test loss: 1.618, Test accuracy: 84.38
Round  64, Train loss: 1.581, Test loss: 1.619, Test accuracy: 84.39
Round  65, Train loss: 1.581, Test loss: 1.619, Test accuracy: 84.52
Round  66, Train loss: 1.583, Test loss: 1.618, Test accuracy: 84.44
Round  67, Train loss: 1.579, Test loss: 1.618, Test accuracy: 84.52
Round  68, Train loss: 1.579, Test loss: 1.617, Test accuracy: 84.62
Round  69, Train loss: 1.584, Test loss: 1.617, Test accuracy: 84.58
Round  70, Train loss: 1.581, Test loss: 1.618, Test accuracy: 84.56
Round  71, Train loss: 1.584, Test loss: 1.617, Test accuracy: 84.50
Round  72, Train loss: 1.584, Test loss: 1.617, Test accuracy: 84.48
Round  73, Train loss: 1.581, Test loss: 1.617, Test accuracy: 84.48
Round  74, Train loss: 1.581, Test loss: 1.616, Test accuracy: 84.65
Round  75, Train loss: 1.576, Test loss: 1.616, Test accuracy: 84.73
Round  76, Train loss: 1.579, Test loss: 1.616, Test accuracy: 84.53
Round  77, Train loss: 1.578, Test loss: 1.615, Test accuracy: 84.62
Round  78, Train loss: 1.573, Test loss: 1.615, Test accuracy: 84.62
Round  79, Train loss: 1.579, Test loss: 1.616, Test accuracy: 84.62
Round  80, Train loss: 1.583, Test loss: 1.615, Test accuracy: 84.80
Round  81, Train loss: 1.572, Test loss: 1.615, Test accuracy: 84.82
Round  82, Train loss: 1.577, Test loss: 1.615, Test accuracy: 84.80
Round  83, Train loss: 1.581, Test loss: 1.615, Test accuracy: 84.85
Round  84, Train loss: 1.576, Test loss: 1.614, Test accuracy: 84.88
Round  85, Train loss: 1.574, Test loss: 1.614, Test accuracy: 84.87
Round  86, Train loss: 1.576, Test loss: 1.614, Test accuracy: 84.93
Round  87, Train loss: 1.568, Test loss: 1.613, Test accuracy: 84.95
Round  88, Train loss: 1.575, Test loss: 1.613, Test accuracy: 84.95
Round  89, Train loss: 1.581, Test loss: 1.613, Test accuracy: 84.97
Round  90, Train loss: 1.570, Test loss: 1.613, Test accuracy: 84.89
Round  91, Train loss: 1.570, Test loss: 1.613, Test accuracy: 84.87
Round  92, Train loss: 1.573, Test loss: 1.612, Test accuracy: 85.04
Round  93, Train loss: 1.578, Test loss: 1.612, Test accuracy: 85.00
Round  94, Train loss: 1.576, Test loss: 1.613, Test accuracy: 84.91
Round  95, Train loss: 1.580, Test loss: 1.613, Test accuracy: 84.97
Round  96, Train loss: 1.575, Test loss: 1.612, Test accuracy: 85.04
Round  97, Train loss: 1.579, Test loss: 1.612, Test accuracy: 85.10
Round  98, Train loss: 1.580, Test loss: 1.612, Test accuracy: 85.05
Round  99, Train loss: 1.578, Test loss: 1.612, Test accuracy: 85.01
Final Round, Train loss: 1.572, Test loss: 1.611, Test accuracy: 85.17
Average accuracy final 10 rounds: 84.98833333333333
2174.3934288024902
[3.0905416011810303, 6.038847208023071, 9.004701852798462, 12.033514499664307, 15.031401872634888, 18.107611656188965, 21.27397871017456, 24.385071992874146, 27.43814516067505, 30.545291423797607, 33.60923218727112, 36.74669885635376, 39.85393238067627, 42.9967155456543, 46.05895948410034, 49.26123023033142, 52.30790090560913, 55.48307657241821, 58.62778663635254, 61.66920709609985, 64.6961464881897, 67.85885238647461, 70.97501587867737, 74.01054263114929, 77.06857991218567, 80.13679194450378, 83.24382758140564, 86.28459692001343, 89.32371187210083, 92.37146401405334, 95.43280458450317, 98.39618372917175, 101.45525312423706, 104.49402976036072, 107.55524921417236, 110.60716915130615, 113.5685887336731, 116.59005522727966, 119.75631523132324, 122.78288912773132, 125.80948448181152, 128.83035564422607, 131.83376932144165, 134.85160040855408, 137.77121710777283, 140.79603910446167, 143.7754249572754, 146.75932550430298, 149.78141713142395, 152.8279106616974, 155.8202896118164, 158.85327243804932, 161.86774253845215, 164.88926529884338, 167.92511200904846, 170.92537999153137, 173.92109060287476, 176.9567379951477, 179.9721760749817, 183.0562641620636, 186.1002631187439, 189.1028344631195, 192.1449899673462, 195.1188428401947, 198.06340289115906, 201.0257682800293, 203.8925564289093, 206.78248643875122, 209.7088224887848, 212.59160661697388, 215.50279116630554, 218.48990988731384, 221.48558592796326, 224.48046231269836, 227.4965169429779, 230.46849083900452, 233.416184425354, 236.43118953704834, 239.36440229415894, 242.3262038230896, 245.30935549736023, 248.20755529403687, 251.13196325302124, 254.17998909950256, 257.05002880096436, 259.9824047088623, 262.9679961204529, 265.8868079185486, 268.85461926460266, 271.8653128147125, 274.83133578300476, 277.8183000087738, 280.8103492259979, 283.66939997673035, 286.38616490364075, 289.1345157623291, 291.8745141029358, 294.6381883621216, 297.38285875320435, 300.1408460140228, 302.4407842159271]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[25.891666666666666, 35.525, 47.891666666666666, 35.875, 50.958333333333336, 63.63333333333333, 72.575, 73.24166666666666, 73.45833333333333, 74.11666666666666, 73.975, 74.46666666666667, 74.65833333333333, 74.525, 74.85, 75.01666666666667, 75.075, 75.11666666666666, 75.46666666666667, 75.51666666666667, 75.3, 77.05833333333334, 80.31666666666666, 81.43333333333334, 82.01666666666667, 82.14166666666667, 82.45833333333333, 82.41666666666667, 82.63333333333334, 82.675, 82.69166666666666, 82.975, 82.775, 82.875, 83.05833333333334, 83.24166666666666, 83.325, 83.54166666666667, 83.45, 83.3, 83.675, 83.63333333333334, 83.525, 83.61666666666666, 83.54166666666667, 83.79166666666667, 83.68333333333334, 83.875, 83.88333333333334, 84.0, 83.95833333333333, 83.875, 83.99166666666666, 83.89166666666667, 84.10833333333333, 84.23333333333333, 84.175, 84.15, 84.13333333333334, 84.23333333333333, 84.25833333333334, 84.375, 84.41666666666667, 84.375, 84.39166666666667, 84.51666666666667, 84.44166666666666, 84.51666666666667, 84.625, 84.58333333333333, 84.55833333333334, 84.5, 84.48333333333333, 84.48333333333333, 84.65, 84.73333333333333, 84.53333333333333, 84.625, 84.625, 84.625, 84.8, 84.81666666666666, 84.8, 84.85, 84.875, 84.86666666666666, 84.93333333333334, 84.95, 84.95, 84.975, 84.89166666666667, 84.86666666666666, 85.04166666666667, 85.0, 84.90833333333333, 84.975, 85.04166666666667, 85.1, 85.05, 85.00833333333334, 85.175]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.320, Test loss: 2.300, Test accuracy: 19.48
Round   1, Train loss: 2.299, Test loss: 2.296, Test accuracy: 31.46
Round   2, Train loss: 2.294, Test loss: 2.290, Test accuracy: 38.45
Round   3, Train loss: 2.287, Test loss: 2.279, Test accuracy: 42.38
Round   4, Train loss: 2.269, Test loss: 2.249, Test accuracy: 39.57
Round   5, Train loss: 2.225, Test loss: 2.200, Test accuracy: 47.45
Round   6, Train loss: 2.175, Test loss: 2.123, Test accuracy: 52.12
Round   7, Train loss: 2.065, Test loss: 2.031, Test accuracy: 56.74
Round   8, Train loss: 1.985, Test loss: 1.949, Test accuracy: 67.58
Round   9, Train loss: 1.879, Test loss: 1.856, Test accuracy: 76.03
Round  10, Train loss: 1.815, Test loss: 1.779, Test accuracy: 80.83
Round  11, Train loss: 1.741, Test loss: 1.735, Test accuracy: 84.38
Round  12, Train loss: 1.720, Test loss: 1.698, Test accuracy: 86.02
Round  13, Train loss: 1.685, Test loss: 1.677, Test accuracy: 87.22
Round  14, Train loss: 1.661, Test loss: 1.657, Test accuracy: 88.04
Round  15, Train loss: 1.647, Test loss: 1.644, Test accuracy: 88.87
Round  16, Train loss: 1.652, Test loss: 1.629, Test accuracy: 89.58
Round  17, Train loss: 1.627, Test loss: 1.618, Test accuracy: 90.05
Round  18, Train loss: 1.632, Test loss: 1.605, Test accuracy: 90.50
Round  19, Train loss: 1.614, Test loss: 1.599, Test accuracy: 90.79
Round  20, Train loss: 1.607, Test loss: 1.594, Test accuracy: 91.12
Round  21, Train loss: 1.588, Test loss: 1.589, Test accuracy: 91.44
Round  22, Train loss: 1.589, Test loss: 1.584, Test accuracy: 91.73
Round  23, Train loss: 1.583, Test loss: 1.580, Test accuracy: 91.90
Round  24, Train loss: 1.577, Test loss: 1.577, Test accuracy: 92.14
Round  25, Train loss: 1.577, Test loss: 1.573, Test accuracy: 92.21
Round  26, Train loss: 1.573, Test loss: 1.568, Test accuracy: 92.49
Round  27, Train loss: 1.567, Test loss: 1.565, Test accuracy: 92.63
Round  28, Train loss: 1.570, Test loss: 1.562, Test accuracy: 92.69
Round  29, Train loss: 1.561, Test loss: 1.560, Test accuracy: 92.83
Round  30, Train loss: 1.559, Test loss: 1.558, Test accuracy: 93.03
Round  31, Train loss: 1.553, Test loss: 1.556, Test accuracy: 93.17
Round  32, Train loss: 1.551, Test loss: 1.553, Test accuracy: 93.42
Round  33, Train loss: 1.543, Test loss: 1.552, Test accuracy: 93.49
Round  34, Train loss: 1.546, Test loss: 1.549, Test accuracy: 93.77
Round  35, Train loss: 1.544, Test loss: 1.548, Test accuracy: 93.76
Round  36, Train loss: 1.538, Test loss: 1.548, Test accuracy: 93.73
Round  37, Train loss: 1.537, Test loss: 1.546, Test accuracy: 93.92
Round  38, Train loss: 1.539, Test loss: 1.544, Test accuracy: 94.08
Round  39, Train loss: 1.530, Test loss: 1.542, Test accuracy: 94.20
Round  40, Train loss: 1.528, Test loss: 1.542, Test accuracy: 94.22
Round  41, Train loss: 1.532, Test loss: 1.540, Test accuracy: 94.29
Round  42, Train loss: 1.526, Test loss: 1.538, Test accuracy: 94.47
Round  43, Train loss: 1.527, Test loss: 1.537, Test accuracy: 94.58
Round  44, Train loss: 1.520, Test loss: 1.536, Test accuracy: 94.56
Round  45, Train loss: 1.520, Test loss: 1.536, Test accuracy: 94.70
Round  46, Train loss: 1.520, Test loss: 1.536, Test accuracy: 94.58
Round  47, Train loss: 1.522, Test loss: 1.534, Test accuracy: 94.61
Round  48, Train loss: 1.515, Test loss: 1.534, Test accuracy: 94.69
Round  49, Train loss: 1.516, Test loss: 1.532, Test accuracy: 94.90
Round  50, Train loss: 1.517, Test loss: 1.532, Test accuracy: 94.88
Round  51, Train loss: 1.514, Test loss: 1.531, Test accuracy: 94.92
Round  52, Train loss: 1.512, Test loss: 1.530, Test accuracy: 95.01
Round  53, Train loss: 1.514, Test loss: 1.529, Test accuracy: 94.97
Round  54, Train loss: 1.508, Test loss: 1.529, Test accuracy: 94.97
Round  55, Train loss: 1.509, Test loss: 1.528, Test accuracy: 95.09
Round  56, Train loss: 1.511, Test loss: 1.527, Test accuracy: 95.08
Round  57, Train loss: 1.506, Test loss: 1.527, Test accuracy: 95.06
Round  58, Train loss: 1.506, Test loss: 1.526, Test accuracy: 95.17
Round  59, Train loss: 1.506, Test loss: 1.526, Test accuracy: 95.16
Round  60, Train loss: 1.504, Test loss: 1.525, Test accuracy: 95.22
Round  61, Train loss: 1.505, Test loss: 1.525, Test accuracy: 95.22
Round  62, Train loss: 1.502, Test loss: 1.525, Test accuracy: 95.26
Round  63, Train loss: 1.503, Test loss: 1.524, Test accuracy: 95.35
Round  64, Train loss: 1.497, Test loss: 1.524, Test accuracy: 95.33
Round  65, Train loss: 1.500, Test loss: 1.523, Test accuracy: 95.38
Round  66, Train loss: 1.495, Test loss: 1.523, Test accuracy: 95.37
Round  67, Train loss: 1.497, Test loss: 1.522, Test accuracy: 95.38
Round  68, Train loss: 1.501, Test loss: 1.522, Test accuracy: 95.53
Round  69, Train loss: 1.497, Test loss: 1.521, Test accuracy: 95.58
Round  70, Train loss: 1.501, Test loss: 1.521, Test accuracy: 95.68
Round  71, Train loss: 1.494, Test loss: 1.521, Test accuracy: 95.63
Round  72, Train loss: 1.497, Test loss: 1.521, Test accuracy: 95.62
Round  73, Train loss: 1.494, Test loss: 1.520, Test accuracy: 95.72
Round  74, Train loss: 1.492, Test loss: 1.520, Test accuracy: 95.63
Round  75, Train loss: 1.495, Test loss: 1.520, Test accuracy: 95.74
Round  76, Train loss: 1.492, Test loss: 1.519, Test accuracy: 95.73
Round  77, Train loss: 1.493, Test loss: 1.519, Test accuracy: 95.69
Round  78, Train loss: 1.488, Test loss: 1.518, Test accuracy: 95.78
Round  79, Train loss: 1.489, Test loss: 1.518, Test accuracy: 95.75
Round  80, Train loss: 1.491, Test loss: 1.518, Test accuracy: 95.78
Round  81, Train loss: 1.493, Test loss: 1.518, Test accuracy: 95.88
Round  82, Train loss: 1.490, Test loss: 1.517, Test accuracy: 95.82
Round  83, Train loss: 1.488, Test loss: 1.517, Test accuracy: 95.87
Round  84, Train loss: 1.488, Test loss: 1.517, Test accuracy: 95.83
Round  85, Train loss: 1.492, Test loss: 1.516, Test accuracy: 95.92
Round  86, Train loss: 1.488, Test loss: 1.516, Test accuracy: 95.83
Round  87, Train loss: 1.488, Test loss: 1.516, Test accuracy: 95.92
Round  88, Train loss: 1.485, Test loss: 1.516, Test accuracy: 95.94
Round  89, Train loss: 1.485, Test loss: 1.515, Test accuracy: 95.95
Round  90, Train loss: 1.485, Test loss: 1.515, Test accuracy: 96.03
Round  91, Train loss: 1.486, Test loss: 1.515, Test accuracy: 95.97
Round  92, Train loss: 1.485, Test loss: 1.515, Test accuracy: 96.01
Round  93, Train loss: 1.485, Test loss: 1.514, Test accuracy: 96.06
Round  94, Train loss: 1.486, Test loss: 1.514, Test accuracy: 96.09/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.486, Test loss: 1.513, Test accuracy: 96.05
Round  96, Train loss: 1.485, Test loss: 1.513, Test accuracy: 96.02
Round  97, Train loss: 1.485, Test loss: 1.513, Test accuracy: 96.15
Round  98, Train loss: 1.483, Test loss: 1.513, Test accuracy: 96.11
Round  99, Train loss: 1.483, Test loss: 1.513, Test accuracy: 96.07
Final Round, Train loss: 1.477, Test loss: 1.512, Test accuracy: 96.14
Average accuracy final 10 rounds: 96.05499999999999
1216.4130160808563
[1.5616660118103027, 2.9976208209991455, 4.422950267791748, 5.832774639129639, 7.26511812210083, 8.65614938735962, 10.034319639205933, 11.480600595474243, 12.93125057220459, 14.417518854141235, 15.95972228050232, 17.507853507995605, 18.95095705986023, 20.452418088912964, 21.941532135009766, 23.42617678642273, 24.944435596466064, 26.42912244796753, 27.994996547698975, 29.432262897491455, 30.933004140853882, 32.43524193763733, 33.97403907775879, 35.43221068382263, 36.96679759025574, 38.51637101173401, 39.938631772994995, 41.42620134353638, 42.90643000602722, 44.352497816085815, 45.823662757873535, 47.2185332775116, 48.59868144989014, 49.96057653427124, 51.37681698799133, 52.70527768135071, 54.10187363624573, 55.517937421798706, 56.83785271644592, 58.244627952575684, 59.59470796585083, 60.93804860115051, 62.28361105918884, 63.64550757408142, 65.0086030960083, 66.35198378562927, 67.75703001022339, 69.10706233978271, 70.4492871761322, 71.8622407913208, 73.2295229434967, 74.58330535888672, 75.96228408813477, 77.28579545021057, 78.63868260383606, 79.96991443634033, 81.34730815887451, 82.66382145881653, 84.03848218917847, 85.3993661403656, 86.74427366256714, 88.13070154190063, 89.44153451919556, 90.76828503608704, 92.15775632858276, 93.46962428092957, 94.80487203598022, 96.13516306877136, 97.44780683517456, 98.77128171920776, 100.09659004211426, 101.4139244556427, 102.7498242855072, 104.09499073028564, 105.37013220787048, 106.69917368888855, 107.99281787872314, 109.34023642539978, 110.69120168685913, 111.97214865684509, 113.30903339385986, 114.60619354248047, 115.93333792686462, 117.23308420181274, 118.5830454826355, 119.89860606193542, 121.2481677532196, 122.6004467010498, 123.92069292068481, 125.25162029266357, 126.60668706893921, 127.97803974151611, 129.3195526599884, 130.6839199066162, 132.01995992660522, 133.4218339920044, 134.79097318649292, 136.14210629463196, 137.48057079315186, 138.85200119018555, 140.60088396072388]
[19.475, 31.458333333333332, 38.45, 42.38333333333333, 39.56666666666667, 47.45, 52.125, 56.74166666666667, 67.58333333333333, 76.025, 80.825, 84.38333333333334, 86.01666666666667, 87.225, 88.04166666666667, 88.86666666666666, 89.575, 90.05, 90.5, 90.79166666666667, 91.11666666666666, 91.44166666666666, 91.73333333333333, 91.9, 92.14166666666667, 92.20833333333333, 92.49166666666666, 92.63333333333334, 92.69166666666666, 92.825, 93.025, 93.16666666666667, 93.425, 93.49166666666666, 93.76666666666667, 93.75833333333334, 93.73333333333333, 93.91666666666667, 94.08333333333333, 94.2, 94.225, 94.29166666666667, 94.475, 94.575, 94.55833333333334, 94.7, 94.58333333333333, 94.60833333333333, 94.69166666666666, 94.9, 94.875, 94.925, 95.00833333333334, 94.975, 94.96666666666667, 95.09166666666667, 95.075, 95.05833333333334, 95.16666666666667, 95.15833333333333, 95.21666666666667, 95.21666666666667, 95.25833333333334, 95.35, 95.325, 95.38333333333334, 95.36666666666666, 95.375, 95.53333333333333, 95.58333333333333, 95.68333333333334, 95.63333333333334, 95.61666666666666, 95.725, 95.63333333333334, 95.74166666666666, 95.73333333333333, 95.69166666666666, 95.78333333333333, 95.75, 95.775, 95.875, 95.81666666666666, 95.86666666666666, 95.83333333333333, 95.925, 95.83333333333333, 95.91666666666667, 95.94166666666666, 95.95, 96.03333333333333, 95.96666666666667, 96.00833333333334, 96.05833333333334, 96.09166666666667, 96.05, 96.01666666666667, 96.15, 96.10833333333333, 96.06666666666666, 96.14166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.300, Test accuracy: 13.29
Round   1, Train loss: 2.314, Test loss: 2.294, Test accuracy: 19.18
Round   2, Train loss: 2.301, Test loss: 2.282, Test accuracy: 19.35
Round   3, Train loss: 2.281, Test loss: 2.252, Test accuracy: 25.27
Round   4, Train loss: 2.230, Test loss: 2.176, Test accuracy: 41.88
Round   5, Train loss: 2.136, Test loss: 2.070, Test accuracy: 55.72
Round   6, Train loss: 2.031, Test loss: 1.991, Test accuracy: 56.77
Round   7, Train loss: 1.984, Test loss: 1.959, Test accuracy: 57.81
Round   8, Train loss: 1.960, Test loss: 1.931, Test accuracy: 59.58
Round   9, Train loss: 1.930, Test loss: 1.906, Test accuracy: 63.62
Round  10, Train loss: 1.931, Test loss: 1.876, Test accuracy: 65.13
Round  11, Train loss: 1.873, Test loss: 1.862, Test accuracy: 66.32
Round  12, Train loss: 1.869, Test loss: 1.845, Test accuracy: 68.51
Round  13, Train loss: 1.856, Test loss: 1.819, Test accuracy: 71.42
Round  14, Train loss: 1.815, Test loss: 1.800, Test accuracy: 73.61
Round  15, Train loss: 1.812, Test loss: 1.783, Test accuracy: 74.17
Round  16, Train loss: 1.793, Test loss: 1.772, Test accuracy: 75.27
Round  17, Train loss: 1.792, Test loss: 1.757, Test accuracy: 76.76
Round  18, Train loss: 1.776, Test loss: 1.736, Test accuracy: 79.17
Round  19, Train loss: 1.749, Test loss: 1.723, Test accuracy: 80.43
Round  20, Train loss: 1.726, Test loss: 1.715, Test accuracy: 81.30
Round  21, Train loss: 1.745, Test loss: 1.700, Test accuracy: 82.64
Round  22, Train loss: 1.710, Test loss: 1.692, Test accuracy: 85.30
Round  23, Train loss: 1.728, Test loss: 1.663, Test accuracy: 87.83
Round  24, Train loss: 1.688, Test loss: 1.643, Test accuracy: 89.29
Round  25, Train loss: 1.665, Test loss: 1.635, Test accuracy: 90.35
Round  26, Train loss: 1.651, Test loss: 1.628, Test accuracy: 90.91
Round  27, Train loss: 1.646, Test loss: 1.622, Test accuracy: 91.33
Round  28, Train loss: 1.639, Test loss: 1.614, Test accuracy: 91.80
Round  29, Train loss: 1.635, Test loss: 1.606, Test accuracy: 91.96
Round  30, Train loss: 1.636, Test loss: 1.600, Test accuracy: 92.28
Round  31, Train loss: 1.614, Test loss: 1.598, Test accuracy: 92.26
Round  32, Train loss: 1.617, Test loss: 1.594, Test accuracy: 92.61
Round  33, Train loss: 1.616, Test loss: 1.589, Test accuracy: 92.90
Round  34, Train loss: 1.607, Test loss: 1.588, Test accuracy: 93.07
Round  35, Train loss: 1.598, Test loss: 1.587, Test accuracy: 93.19
Round  36, Train loss: 1.594, Test loss: 1.584, Test accuracy: 93.27
Round  37, Train loss: 1.588, Test loss: 1.583, Test accuracy: 93.37
Round  38, Train loss: 1.603, Test loss: 1.577, Test accuracy: 93.47
Round  39, Train loss: 1.585, Test loss: 1.578, Test accuracy: 93.49
Round  40, Train loss: 1.603, Test loss: 1.569, Test accuracy: 93.67
Round  41, Train loss: 1.573, Test loss: 1.573, Test accuracy: 93.98
Round  42, Train loss: 1.575, Test loss: 1.572, Test accuracy: 94.05
Round  43, Train loss: 1.581, Test loss: 1.567, Test accuracy: 94.03
Round  44, Train loss: 1.579, Test loss: 1.565, Test accuracy: 94.22
Round  45, Train loss: 1.572, Test loss: 1.565, Test accuracy: 94.28
Round  46, Train loss: 1.568, Test loss: 1.564, Test accuracy: 94.25
Round  47, Train loss: 1.568, Test loss: 1.563, Test accuracy: 94.40
Round  48, Train loss: 1.570, Test loss: 1.561, Test accuracy: 94.47
Round  49, Train loss: 1.559, Test loss: 1.560, Test accuracy: 94.53
Round  50, Train loss: 1.562, Test loss: 1.558, Test accuracy: 94.49
Round  51, Train loss: 1.567, Test loss: 1.554, Test accuracy: 94.56
Round  52, Train loss: 1.564, Test loss: 1.553, Test accuracy: 94.64
Round  53, Train loss: 1.552, Test loss: 1.554, Test accuracy: 94.67
Round  54, Train loss: 1.555, Test loss: 1.552, Test accuracy: 94.88
Round  55, Train loss: 1.552, Test loss: 1.552, Test accuracy: 94.86
Round  56, Train loss: 1.548, Test loss: 1.552, Test accuracy: 95.04
Round  57, Train loss: 1.551, Test loss: 1.550, Test accuracy: 94.91
Round  58, Train loss: 1.548, Test loss: 1.549, Test accuracy: 94.99
Round  59, Train loss: 1.547, Test loss: 1.547, Test accuracy: 95.17
Round  60, Train loss: 1.547, Test loss: 1.547, Test accuracy: 95.12
Round  61, Train loss: 1.544, Test loss: 1.546, Test accuracy: 95.28
Round  62, Train loss: 1.541, Test loss: 1.545, Test accuracy: 95.31
Round  63, Train loss: 1.541, Test loss: 1.544, Test accuracy: 95.29
Round  64, Train loss: 1.541, Test loss: 1.543, Test accuracy: 95.42
Round  65, Train loss: 1.534, Test loss: 1.545, Test accuracy: 95.48
Round  66, Train loss: 1.540, Test loss: 1.542, Test accuracy: 95.42
Round  67, Train loss: 1.539, Test loss: 1.541, Test accuracy: 95.58
Round  68, Train loss: 1.532, Test loss: 1.543, Test accuracy: 95.55
Round  69, Train loss: 1.534, Test loss: 1.541, Test accuracy: 95.53
Round  70, Train loss: 1.536, Test loss: 1.539, Test accuracy: 95.62
Round  71, Train loss: 1.530, Test loss: 1.541, Test accuracy: 95.73
Round  72, Train loss: 1.531, Test loss: 1.539, Test accuracy: 95.65
Round  73, Train loss: 1.530, Test loss: 1.539, Test accuracy: 95.79
Round  74, Train loss: 1.533, Test loss: 1.538, Test accuracy: 95.76
Round  75, Train loss: 1.529, Test loss: 1.538, Test accuracy: 95.71
Round  76, Train loss: 1.530, Test loss: 1.537, Test accuracy: 95.80
Round  77, Train loss: 1.528, Test loss: 1.536, Test accuracy: 95.78
Round  78, Train loss: 1.528, Test loss: 1.535, Test accuracy: 95.81
Round  79, Train loss: 1.529, Test loss: 1.534, Test accuracy: 95.83
Round  80, Train loss: 1.520, Test loss: 1.536, Test accuracy: 95.73
Round  81, Train loss: 1.524, Test loss: 1.535, Test accuracy: 95.87
Round  82, Train loss: 1.524, Test loss: 1.534, Test accuracy: 95.84
Round  83, Train loss: 1.518, Test loss: 1.536, Test accuracy: 95.87
Round  84, Train loss: 1.525, Test loss: 1.533, Test accuracy: 95.92
Round  85, Train loss: 1.518, Test loss: 1.534, Test accuracy: 95.96
Round  86, Train loss: 1.524, Test loss: 1.532, Test accuracy: 95.98
Round  87, Train loss: 1.522, Test loss: 1.531, Test accuracy: 95.99
Round  88, Train loss: 1.514, Test loss: 1.534, Test accuracy: 96.02
Round  89, Train loss: 1.521, Test loss: 1.532, Test accuracy: 96.03
Round  90, Train loss: 1.518, Test loss: 1.531, Test accuracy: 96.07
Round  91, Train loss: 1.519, Test loss: 1.531, Test accuracy: 96.07
Round  92, Train loss: 1.518, Test loss: 1.530, Test accuracy: 96.03
Round  93, Train loss: 1.517, Test loss: 1.530, Test accuracy: 96.11/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.516, Test loss: 1.530, Test accuracy: 96.20
Round  95, Train loss: 1.517, Test loss: 1.529, Test accuracy: 96.14
Round  96, Train loss: 1.513, Test loss: 1.530, Test accuracy: 96.13
Round  97, Train loss: 1.512, Test loss: 1.530, Test accuracy: 96.16
Round  98, Train loss: 1.511, Test loss: 1.529, Test accuracy: 96.09
Round  99, Train loss: 1.513, Test loss: 1.529, Test accuracy: 96.04
Final Round, Train loss: 1.489, Test loss: 1.525, Test accuracy: 96.12
Average accuracy final 10 rounds: 96.10333333333332
1657.7910447120667
[1.6413383483886719, 3.2826766967773438, 4.853394269943237, 6.424111843109131, 7.976399183273315, 9.5286865234375, 11.041675806045532, 12.554665088653564, 14.059593439102173, 15.564521789550781, 17.05356001853943, 18.542598247528076, 20.099026679992676, 21.655455112457275, 23.140681266784668, 24.62590742111206, 26.125620365142822, 27.625333309173584, 29.17519974708557, 30.72506618499756, 32.22988295555115, 33.734699726104736, 35.273417234420776, 36.812134742736816, 38.37763953208923, 39.94314432144165, 41.44765567779541, 42.95216703414917, 44.4875066280365, 46.02284622192383, 47.577388286590576, 49.131930351257324, 50.689733266830444, 52.247536182403564, 53.80824899673462, 55.368961811065674, 56.92311882972717, 58.47727584838867, 59.97357487678528, 61.469873905181885, 62.96384334564209, 64.4578127861023, 65.96571564674377, 67.47361850738525, 69.00837779045105, 70.54313707351685, 72.08306932449341, 73.62300157546997, 75.11857318878174, 76.6141448020935, 78.0969169139862, 79.5796890258789, 81.12470126152039, 82.66971349716187, 84.22912096977234, 85.78852844238281, 87.34730076789856, 88.9060730934143, 90.44462418556213, 91.98317527770996, 93.53441596031189, 95.08565664291382, 96.61204433441162, 98.13843202590942, 99.660893201828, 101.18335437774658, 102.72568225860596, 104.26801013946533, 105.81917452812195, 107.37033891677856, 108.92912578582764, 110.48791265487671, 112.05880355834961, 113.62969446182251, 115.14258217811584, 116.65546989440918, 118.17429804801941, 119.69312620162964, 121.2671890258789, 122.84125185012817, 124.41765904426575, 125.99406623840332, 127.52352547645569, 129.05298471450806, 130.58209323883057, 132.11120176315308, 133.64564442634583, 135.18008708953857, 136.73544096946716, 138.29079484939575, 139.8843879699707, 141.47798109054565, 142.98579859733582, 144.49361610412598, 146.09329676628113, 147.69297742843628, 149.26559901237488, 150.83822059631348, 152.38598489761353, 153.93374919891357, 155.4303638935089, 156.92697858810425, 158.3485701084137, 159.77016162872314, 161.2493965625763, 162.72863149642944, 164.2048544883728, 165.68107748031616, 167.17322087287903, 168.6653642654419, 170.14166474342346, 171.61796522140503, 173.14477968215942, 174.67159414291382, 176.16501307487488, 177.65843200683594, 179.17878413200378, 180.69913625717163, 182.26809215545654, 183.83704805374146, 185.32697129249573, 186.81689453125, 188.3150451183319, 189.81319570541382, 191.32190775871277, 192.83061981201172, 194.33784127235413, 195.84506273269653, 197.3151834011078, 198.78530406951904, 200.24228048324585, 201.69925689697266, 203.14999556541443, 204.6007342338562, 206.1180145740509, 207.6352949142456, 209.1652476787567, 210.69520044326782, 212.2544765472412, 213.8137526512146, 215.30612587928772, 216.79849910736084, 218.3198528289795, 219.84120655059814, 221.34722328186035, 222.85324001312256, 224.37118577957153, 225.8891315460205, 227.39204573631287, 228.89495992660522, 230.3565456867218, 231.81813144683838, 233.31483459472656, 234.81153774261475, 236.3131446838379, 237.81475162506104, 239.25744915008545, 240.70014667510986, 242.15165543556213, 243.6031641960144, 245.0945975780487, 246.586030960083, 248.04013204574585, 249.4942331314087, 250.960631608963, 252.42703008651733, 253.92390370368958, 255.42077732086182, 256.89638662338257, 258.3719959259033, 259.9009029865265, 261.42981004714966, 262.91330575942993, 264.3968014717102, 265.8507242202759, 267.30464696884155, 268.73868465423584, 270.1727223396301, 271.6064395904541, 273.0401568412781, 274.5006160736084, 275.9610753059387, 277.37591457366943, 278.79075384140015, 280.27036905288696, 281.7499842643738, 283.23322105407715, 284.7164578437805, 286.1805303096771, 287.64460277557373, 289.0993502140045, 290.5540976524353, 291.983078956604, 293.4120602607727, 294.9102737903595, 296.4084873199463, 297.8441162109375, 299.2797451019287, 300.7088186740875, 302.13789224624634, 304.01370429992676, 305.8895163536072]
[13.291666666666666, 13.291666666666666, 19.175, 19.175, 19.35, 19.35, 25.266666666666666, 25.266666666666666, 41.88333333333333, 41.88333333333333, 55.71666666666667, 55.71666666666667, 56.775, 56.775, 57.80833333333333, 57.80833333333333, 59.583333333333336, 59.583333333333336, 63.61666666666667, 63.61666666666667, 65.13333333333334, 65.13333333333334, 66.31666666666666, 66.31666666666666, 68.50833333333334, 68.50833333333334, 71.425, 71.425, 73.60833333333333, 73.60833333333333, 74.16666666666667, 74.16666666666667, 75.26666666666667, 75.26666666666667, 76.75833333333334, 76.75833333333334, 79.175, 79.175, 80.43333333333334, 80.43333333333334, 81.3, 81.3, 82.64166666666667, 82.64166666666667, 85.3, 85.3, 87.825, 87.825, 89.29166666666667, 89.29166666666667, 90.35, 90.35, 90.90833333333333, 90.90833333333333, 91.325, 91.325, 91.8, 91.8, 91.95833333333333, 91.95833333333333, 92.28333333333333, 92.28333333333333, 92.25833333333334, 92.25833333333334, 92.60833333333333, 92.60833333333333, 92.9, 92.9, 93.06666666666666, 93.06666666666666, 93.19166666666666, 93.19166666666666, 93.26666666666667, 93.26666666666667, 93.36666666666666, 93.36666666666666, 93.46666666666667, 93.46666666666667, 93.49166666666666, 93.49166666666666, 93.66666666666667, 93.66666666666667, 93.98333333333333, 93.98333333333333, 94.05, 94.05, 94.025, 94.025, 94.21666666666667, 94.21666666666667, 94.275, 94.275, 94.25, 94.25, 94.4, 94.4, 94.46666666666667, 94.46666666666667, 94.53333333333333, 94.53333333333333, 94.49166666666666, 94.49166666666666, 94.55833333333334, 94.55833333333334, 94.64166666666667, 94.64166666666667, 94.675, 94.675, 94.88333333333334, 94.88333333333334, 94.85833333333333, 94.85833333333333, 95.04166666666667, 95.04166666666667, 94.90833333333333, 94.90833333333333, 94.99166666666666, 94.99166666666666, 95.16666666666667, 95.16666666666667, 95.11666666666666, 95.11666666666666, 95.28333333333333, 95.28333333333333, 95.30833333333334, 95.30833333333334, 95.29166666666667, 95.29166666666667, 95.425, 95.425, 95.48333333333333, 95.48333333333333, 95.425, 95.425, 95.58333333333333, 95.58333333333333, 95.55, 95.55, 95.525, 95.525, 95.61666666666666, 95.61666666666666, 95.73333333333333, 95.73333333333333, 95.65, 95.65, 95.79166666666667, 95.79166666666667, 95.75833333333334, 95.75833333333334, 95.70833333333333, 95.70833333333333, 95.8, 95.8, 95.775, 95.775, 95.80833333333334, 95.80833333333334, 95.83333333333333, 95.83333333333333, 95.73333333333333, 95.73333333333333, 95.86666666666666, 95.86666666666666, 95.84166666666667, 95.84166666666667, 95.86666666666666, 95.86666666666666, 95.91666666666667, 95.91666666666667, 95.95833333333333, 95.95833333333333, 95.98333333333333, 95.98333333333333, 95.99166666666666, 95.99166666666666, 96.01666666666667, 96.01666666666667, 96.025, 96.025, 96.06666666666666, 96.06666666666666, 96.06666666666666, 96.06666666666666, 96.025, 96.025, 96.10833333333333, 96.10833333333333, 96.2, 96.2, 96.14166666666667, 96.14166666666667, 96.13333333333334, 96.13333333333334, 96.15833333333333, 96.15833333333333, 96.09166666666667, 96.09166666666667, 96.04166666666667, 96.04166666666667, 96.11666666666666, 96.11666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.230, Test loss: 2.221, Test accuracy: 19.56
Round   0, Global train loss: 2.230, Global test loss: 2.300, Global test accuracy: 12.67
Round   1, Train loss: 1.924, Test loss: 2.076, Test accuracy: 39.98
Round   1, Global train loss: 1.924, Global test loss: 2.283, Global test accuracy: 15.00
Round   2, Train loss: 1.832, Test loss: 1.951, Test accuracy: 52.38
Round   2, Global train loss: 1.832, Global test loss: 2.266, Global test accuracy: 18.34
Round   3, Train loss: 1.713, Test loss: 1.878, Test accuracy: 58.78
Round   3, Global train loss: 1.713, Global test loss: 2.290, Global test accuracy: 11.77
Round   4, Train loss: 1.716, Test loss: 1.793, Test accuracy: 67.94
Round   4, Global train loss: 1.716, Global test loss: 2.267, Global test accuracy: 16.79
Round   5, Train loss: 1.763, Test loss: 1.724, Test accuracy: 76.24
Round   5, Global train loss: 1.763, Global test loss: 2.251, Global test accuracy: 18.97
Round   6, Train loss: 1.645, Test loss: 1.720, Test accuracy: 76.34
Round   6, Global train loss: 1.645, Global test loss: 2.296, Global test accuracy: 11.24
Round   7, Train loss: 1.652, Test loss: 1.659, Test accuracy: 81.14
Round   7, Global train loss: 1.652, Global test loss: 2.286, Global test accuracy: 16.45
Round   8, Train loss: 1.705, Test loss: 1.655, Test accuracy: 81.28
Round   8, Global train loss: 1.705, Global test loss: 2.261, Global test accuracy: 18.73
Round   9, Train loss: 1.637, Test loss: 1.653, Test accuracy: 81.32
Round   9, Global train loss: 1.637, Global test loss: 2.259, Global test accuracy: 20.31
Round  10, Train loss: 1.571, Test loss: 1.637, Test accuracy: 83.03
Round  10, Global train loss: 1.571, Global test loss: 2.265, Global test accuracy: 16.53
Round  11, Train loss: 1.583, Test loss: 1.635, Test accuracy: 83.08
Round  11, Global train loss: 1.583, Global test loss: 2.262, Global test accuracy: 17.81
Round  12, Train loss: 1.530, Test loss: 1.634, Test accuracy: 83.20
Round  12, Global train loss: 1.530, Global test loss: 2.272, Global test accuracy: 15.16
Round  13, Train loss: 1.581, Test loss: 1.632, Test accuracy: 83.27
Round  13, Global train loss: 1.581, Global test loss: 2.247, Global test accuracy: 19.38
Round  14, Train loss: 1.616, Test loss: 1.603, Test accuracy: 86.32
Round  14, Global train loss: 1.616, Global test loss: 2.260, Global test accuracy: 16.66
Round  15, Train loss: 1.693, Test loss: 1.603, Test accuracy: 86.33
Round  15, Global train loss: 1.693, Global test loss: 2.268, Global test accuracy: 19.02
Round  16, Train loss: 1.527, Test loss: 1.602, Test accuracy: 86.37
Round  16, Global train loss: 1.527, Global test loss: 2.238, Global test accuracy: 19.95
Round  17, Train loss: 1.580, Test loss: 1.601, Test accuracy: 86.40
Round  17, Global train loss: 1.580, Global test loss: 2.233, Global test accuracy: 24.70
Round  18, Train loss: 1.525, Test loss: 1.601, Test accuracy: 86.41
Round  18, Global train loss: 1.525, Global test loss: 2.260, Global test accuracy: 16.77
Round  19, Train loss: 1.523, Test loss: 1.601, Test accuracy: 86.40
Round  19, Global train loss: 1.523, Global test loss: 2.246, Global test accuracy: 17.91
Round  20, Train loss: 1.523, Test loss: 1.601, Test accuracy: 86.42
Round  20, Global train loss: 1.523, Global test loss: 2.239, Global test accuracy: 19.64
Round  21, Train loss: 1.474, Test loss: 1.600, Test accuracy: 86.44
Round  21, Global train loss: 1.474, Global test loss: 2.266, Global test accuracy: 19.57
Round  22, Train loss: 1.523, Test loss: 1.600, Test accuracy: 86.44
Round  22, Global train loss: 1.523, Global test loss: 2.248, Global test accuracy: 19.48
Round  23, Train loss: 1.470, Test loss: 1.600, Test accuracy: 86.42
Round  23, Global train loss: 1.470, Global test loss: 2.279, Global test accuracy: 15.15
Round  24, Train loss: 1.679, Test loss: 1.589, Test accuracy: 87.72
Round  24, Global train loss: 1.679, Global test loss: 2.266, Global test accuracy: 17.52
Round  25, Train loss: 1.524, Test loss: 1.589, Test accuracy: 87.70
Round  25, Global train loss: 1.524, Global test loss: 2.272, Global test accuracy: 14.93
Round  26, Train loss: 1.584, Test loss: 1.586, Test accuracy: 87.87
Round  26, Global train loss: 1.584, Global test loss: 2.261, Global test accuracy: 17.00
Round  27, Train loss: 1.468, Test loss: 1.585, Test accuracy: 87.92
Round  27, Global train loss: 1.468, Global test loss: 2.279, Global test accuracy: 13.76
Round  28, Train loss: 1.616, Test loss: 1.573, Test accuracy: 89.27
Round  28, Global train loss: 1.616, Global test loss: 2.278, Global test accuracy: 14.32
Round  29, Train loss: 1.468, Test loss: 1.573, Test accuracy: 89.26
Round  29, Global train loss: 1.468, Global test loss: 2.262, Global test accuracy: 18.48
Round  30, Train loss: 1.588, Test loss: 1.571, Test accuracy: 89.33
Round  30, Global train loss: 1.588, Global test loss: 2.274, Global test accuracy: 18.19
Round  31, Train loss: 1.547, Test loss: 1.559, Test accuracy: 90.65
Round  31, Global train loss: 1.547, Global test loss: 2.241, Global test accuracy: 22.21
Round  32, Train loss: 1.528, Test loss: 1.557, Test accuracy: 90.78
Round  32, Global train loss: 1.528, Global test loss: 2.268, Global test accuracy: 17.38
Round  33, Train loss: 1.528, Test loss: 1.557, Test accuracy: 90.79
Round  33, Global train loss: 1.528, Global test loss: 2.283, Global test accuracy: 14.40
Round  34, Train loss: 1.471, Test loss: 1.557, Test accuracy: 90.77
Round  34, Global train loss: 1.471, Global test loss: 2.278, Global test accuracy: 15.50
Round  35, Train loss: 1.525, Test loss: 1.557, Test accuracy: 90.79
Round  35, Global train loss: 1.525, Global test loss: 2.240, Global test accuracy: 20.54
Round  36, Train loss: 1.578, Test loss: 1.556, Test accuracy: 90.81
Round  36, Global train loss: 1.578, Global test loss: 2.309, Global test accuracy: 12.83
Round  37, Train loss: 1.468, Test loss: 1.556, Test accuracy: 90.83
Round  37, Global train loss: 1.468, Global test loss: 2.268, Global test accuracy: 16.82
Round  38, Train loss: 1.524, Test loss: 1.556, Test accuracy: 90.83
Round  38, Global train loss: 1.524, Global test loss: 2.293, Global test accuracy: 13.84
Round  39, Train loss: 1.468, Test loss: 1.556, Test accuracy: 90.83
Round  39, Global train loss: 1.468, Global test loss: 2.244, Global test accuracy: 22.67
Round  40, Train loss: 1.578, Test loss: 1.556, Test accuracy: 90.82
Round  40, Global train loss: 1.578, Global test loss: 2.288, Global test accuracy: 15.07
Round  41, Train loss: 1.466, Test loss: 1.555, Test accuracy: 90.90
Round  41, Global train loss: 1.466, Global test loss: 2.229, Global test accuracy: 22.87
Round  42, Train loss: 1.520, Test loss: 1.555, Test accuracy: 90.88
Round  42, Global train loss: 1.520, Global test loss: 2.235, Global test accuracy: 22.38
Round  43, Train loss: 1.577, Test loss: 1.555, Test accuracy: 90.89
Round  43, Global train loss: 1.577, Global test loss: 2.262, Global test accuracy: 16.03
Round  44, Train loss: 1.577, Test loss: 1.555, Test accuracy: 90.87
Round  44, Global train loss: 1.577, Global test loss: 2.262, Global test accuracy: 18.58
Round  45, Train loss: 1.469, Test loss: 1.555, Test accuracy: 90.88
Round  45, Global train loss: 1.469, Global test loss: 2.266, Global test accuracy: 16.70
Round  46, Train loss: 1.520, Test loss: 1.555, Test accuracy: 90.89
Round  46, Global train loss: 1.520, Global test loss: 2.258, Global test accuracy: 22.84
Round  47, Train loss: 1.470, Test loss: 1.555, Test accuracy: 90.93
Round  47, Global train loss: 1.470, Global test loss: 2.291, Global test accuracy: 14.17
Round  48, Train loss: 1.575, Test loss: 1.555, Test accuracy: 90.90
Round  48, Global train loss: 1.575, Global test loss: 2.247, Global test accuracy: 20.82
Round  49, Train loss: 1.521, Test loss: 1.555, Test accuracy: 90.90
Round  49, Global train loss: 1.521, Global test loss: 2.281, Global test accuracy: 13.79
Round  50, Train loss: 1.577, Test loss: 1.555, Test accuracy: 90.92
Round  50, Global train loss: 1.577, Global test loss: 2.274, Global test accuracy: 16.89
Round  51, Train loss: 1.522, Test loss: 1.555, Test accuracy: 90.92
Round  51, Global train loss: 1.522, Global test loss: 2.320, Global test accuracy: 10.01
Round  52, Train loss: 1.467, Test loss: 1.555, Test accuracy: 90.92
Round  52, Global train loss: 1.467, Global test loss: 2.307, Global test accuracy: 11.59
Round  53, Train loss: 1.576, Test loss: 1.555, Test accuracy: 90.91
Round  53, Global train loss: 1.576, Global test loss: 2.307, Global test accuracy: 12.36
Round  54, Train loss: 1.522, Test loss: 1.555, Test accuracy: 90.88
Round  54, Global train loss: 1.522, Global test loss: 2.291, Global test accuracy: 12.65
Round  55, Train loss: 1.520, Test loss: 1.555, Test accuracy: 90.87
Round  55, Global train loss: 1.520, Global test loss: 2.241, Global test accuracy: 21.59
Round  56, Train loss: 1.523, Test loss: 1.555, Test accuracy: 90.88
Round  56, Global train loss: 1.523, Global test loss: 2.292, Global test accuracy: 13.88
Round  57, Train loss: 1.466, Test loss: 1.554, Test accuracy: 90.86
Round  57, Global train loss: 1.466, Global test loss: 2.259, Global test accuracy: 17.31
Round  58, Train loss: 1.466, Test loss: 1.554, Test accuracy: 90.86
Round  58, Global train loss: 1.466, Global test loss: 2.262, Global test accuracy: 16.84
Round  59, Train loss: 1.465, Test loss: 1.555, Test accuracy: 90.84
Round  59, Global train loss: 1.465, Global test loss: 2.272, Global test accuracy: 15.04
Round  60, Train loss: 1.576, Test loss: 1.555, Test accuracy: 90.88
Round  60, Global train loss: 1.576, Global test loss: 2.295, Global test accuracy: 10.79
Round  61, Train loss: 1.522, Test loss: 1.555, Test accuracy: 90.88
Round  61, Global train loss: 1.522, Global test loss: 2.287, Global test accuracy: 13.62
Round  62, Train loss: 1.521, Test loss: 1.555, Test accuracy: 90.87
Round  62, Global train loss: 1.521, Global test loss: 2.249, Global test accuracy: 20.51
Round  63, Train loss: 1.575, Test loss: 1.554, Test accuracy: 90.85
Round  63, Global train loss: 1.575, Global test loss: 2.314, Global test accuracy: 13.31
Round  64, Train loss: 1.573, Test loss: 1.554, Test accuracy: 90.86
Round  64, Global train loss: 1.573, Global test loss: 2.291, Global test accuracy: 14.28
Round  65, Train loss: 1.468, Test loss: 1.554, Test accuracy: 90.88
Round  65, Global train loss: 1.468, Global test loss: 2.283, Global test accuracy: 17.01
Round  66, Train loss: 1.469, Test loss: 1.554, Test accuracy: 90.88
Round  66, Global train loss: 1.469, Global test loss: 2.324, Global test accuracy: 10.09
Round  67, Train loss: 1.464, Test loss: 1.554, Test accuracy: 90.88
Round  67, Global train loss: 1.464, Global test loss: 2.251, Global test accuracy: 20.71
Round  68, Train loss: 1.518, Test loss: 1.554, Test accuracy: 90.90
Round  68, Global train loss: 1.518, Global test loss: 2.256, Global test accuracy: 19.98
Round  69, Train loss: 1.576, Test loss: 1.554, Test accuracy: 90.90
Round  69, Global train loss: 1.576, Global test loss: 2.272, Global test accuracy: 15.41
Round  70, Train loss: 1.576, Test loss: 1.554, Test accuracy: 90.90
Round  70, Global train loss: 1.576, Global test loss: 2.241, Global test accuracy: 21.56
Round  71, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.90
Round  71, Global train loss: 1.520, Global test loss: 2.269, Global test accuracy: 17.33
Round  72, Train loss: 1.521, Test loss: 1.554, Test accuracy: 90.92
Round  72, Global train loss: 1.521, Global test loss: 2.245, Global test accuracy: 21.92
Round  73, Train loss: 1.469, Test loss: 1.554, Test accuracy: 90.92
Round  73, Global train loss: 1.469, Global test loss: 2.282, Global test accuracy: 15.83
Round  74, Train loss: 1.521, Test loss: 1.554, Test accuracy: 90.93
Round  74, Global train loss: 1.521, Global test loss: 2.323, Global test accuracy: 10.22
Round  75, Train loss: 1.467, Test loss: 1.554, Test accuracy: 90.92
Round  75, Global train loss: 1.467, Global test loss: 2.304, Global test accuracy: 14.98
Round  76, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.91
Round  76, Global train loss: 1.520, Global test loss: 2.244, Global test accuracy: 20.88
Round  77, Train loss: 1.576, Test loss: 1.554, Test accuracy: 90.91
Round  77, Global train loss: 1.576, Global test loss: 2.274, Global test accuracy: 16.41
Round  78, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.92
Round  78, Global train loss: 1.520, Global test loss: 2.228, Global test accuracy: 24.08
Round  79, Train loss: 1.519, Test loss: 1.554, Test accuracy: 90.92
Round  79, Global train loss: 1.519, Global test loss: 2.227, Global test accuracy: 22.99
Round  80, Train loss: 1.522, Test loss: 1.554, Test accuracy: 90.92
Round  80, Global train loss: 1.522, Global test loss: 2.286, Global test accuracy: 14.48
Round  81, Train loss: 1.465, Test loss: 1.554, Test accuracy: 90.92
Round  81, Global train loss: 1.465, Global test loss: 2.256, Global test accuracy: 20.10
Round  82, Train loss: 1.518, Test loss: 1.554, Test accuracy: 90.91
Round  82, Global train loss: 1.518, Global test loss: 2.280, Global test accuracy: 15.94
Round  83, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.92
Round  83, Global train loss: 1.520, Global test loss: 2.286, Global test accuracy: 15.25
Round  84, Train loss: 1.466, Test loss: 1.554, Test accuracy: 90.92
Round  84, Global train loss: 1.466, Global test loss: 2.261, Global test accuracy: 16.98
Round  85, Train loss: 1.578, Test loss: 1.554, Test accuracy: 90.92
Round  85, Global train loss: 1.578, Global test loss: 2.273, Global test accuracy: 16.29
Round  86, Train loss: 1.464, Test loss: 1.554, Test accuracy: 90.94
Round  86, Global train loss: 1.464, Global test loss: 2.249, Global test accuracy: 19.37
Round  87, Train loss: 1.573, Test loss: 1.554, Test accuracy: 90.94
Round  87, Global train loss: 1.573, Global test loss: 2.276, Global test accuracy: 15.23
Round  88, Train loss: 1.577, Test loss: 1.554, Test accuracy: 90.94
Round  88, Global train loss: 1.577, Global test loss: 2.284, Global test accuracy: 14.94
Round  89, Train loss: 1.521, Test loss: 1.554, Test accuracy: 90.95
Round  89, Global train loss: 1.521, Global test loss: 2.298, Global test accuracy: 10.67
Round  90, Train loss: 1.465, Test loss: 1.554, Test accuracy: 90.95
Round  90, Global train loss: 1.465, Global test loss: 2.267, Global test accuracy: 16.57
Round  91, Train loss: 1.519, Test loss: 1.554, Test accuracy: 90.95
Round  91, Global train loss: 1.519, Global test loss: 2.283, Global test accuracy: 14.12
Round  92, Train loss: 1.467, Test loss: 1.554, Test accuracy: 90.94
Round  92, Global train loss: 1.467, Global test loss: 2.253, Global test accuracy: 17.53
Round  93, Train loss: 1.522, Test loss: 1.554, Test accuracy: 90.94
Round  93, Global train loss: 1.522, Global test loss: 2.294, Global test accuracy: 14.11
Round  94, Train loss: 1.573, Test loss: 1.554, Test accuracy: 90.94
Round  94, Global train loss: 1.573, Global test loss: 2.278, Global test accuracy: 15.53
Round  95, Train loss: 1.465, Test loss: 1.554, Test accuracy: 90.93
Round  95, Global train loss: 1.465, Global test loss: 2.259, Global test accuracy: 18.36/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.94
Round  96, Global train loss: 1.520, Global test loss: 2.294, Global test accuracy: 13.21
Round  97, Train loss: 1.521, Test loss: 1.552, Test accuracy: 91.04
Round  97, Global train loss: 1.521, Global test loss: 2.289, Global test accuracy: 12.89
Round  98, Train loss: 1.520, Test loss: 1.552, Test accuracy: 91.04
Round  98, Global train loss: 1.520, Global test loss: 2.233, Global test accuracy: 21.27
Round  99, Train loss: 1.522, Test loss: 1.552, Test accuracy: 91.04
Round  99, Global train loss: 1.522, Global test loss: 2.261, Global test accuracy: 19.27
Final Round, Train loss: 1.502, Test loss: 1.540, Test accuracy: 92.36
Final Round, Global train loss: 1.502, Global test loss: 2.261, Global test accuracy: 19.27
Average accuracy final 10 rounds: 90.97250000000001 

Average global accuracy final 10 rounds: 16.285833333333333 

1801.404032945633
[1.202974557876587, 2.405949115753174, 3.5640451908111572, 4.722141265869141, 5.947144985198975, 7.172148704528809, 8.408240795135498, 9.644332885742188, 10.85632848739624, 12.068324089050293, 13.24002742767334, 14.411730766296387, 15.65399718284607, 16.896263599395752, 18.114308834075928, 19.332354068756104, 20.50759720802307, 21.68284034729004, 22.90056610107422, 24.1182918548584, 25.35043168067932, 26.582571506500244, 27.759640216827393, 28.93670892715454, 30.119654417037964, 31.302599906921387, 32.52321147918701, 33.74382305145264, 34.98256874084473, 36.221314430236816, 37.422566175460815, 38.623817920684814, 39.845356941223145, 41.066895961761475, 42.26981735229492, 43.47273874282837, 44.67629528045654, 45.87985181808472, 47.122785329818726, 48.365718841552734, 49.630249977111816, 50.8947811126709, 52.1160204410553, 53.3372597694397, 54.475598096847534, 55.61393642425537, 56.803555488586426, 57.99317455291748, 59.16562509536743, 60.33807563781738, 61.50686287879944, 62.675650119781494, 63.831395626068115, 64.98714113235474, 66.27583527565002, 67.56452941894531, 68.781809091568, 69.99908876419067, 71.17025232315063, 72.3414158821106, 73.53971982002258, 74.73802375793457, 75.92772436141968, 77.11742496490479, 78.2515480518341, 79.38567113876343, 80.55656933784485, 81.72746753692627, 82.9206211566925, 84.11377477645874, 85.288747549057, 86.46372032165527, 87.61806750297546, 88.77241468429565, 89.90862154960632, 91.04482841491699, 92.260178565979, 93.47552871704102, 94.63362693786621, 95.7917251586914, 96.95106744766235, 98.1104097366333, 99.32282853126526, 100.53524732589722, 101.71024370193481, 102.88524007797241, 103.88538360595703, 104.88552713394165, 106.05201506614685, 107.21850299835205, 108.37666535377502, 109.534827709198, 110.70044565200806, 111.86606359481812, 113.05040740966797, 114.23475122451782, 115.42209601402283, 116.60944080352783, 117.82506442070007, 119.04068803787231, 120.18178176879883, 121.32287549972534, 122.4629898071289, 123.60310411453247, 124.83839058876038, 126.07367706298828, 127.23813796043396, 128.40259885787964, 129.58421087265015, 130.76582288742065, 131.9867241382599, 133.20762538909912, 134.39084577560425, 135.57406616210938, 136.75962567329407, 137.94518518447876, 139.15305519104004, 140.36092519760132, 141.59630131721497, 142.8316774368286, 144.05815649032593, 145.28463554382324, 146.4719524383545, 147.65926933288574, 148.87217712402344, 150.08508491516113, 151.30055236816406, 152.516019821167, 153.69047379493713, 154.86492776870728, 156.03936314582825, 157.21379852294922, 158.4103581905365, 159.60691785812378, 160.82915592193604, 162.0513939857483, 163.217209815979, 164.38302564620972, 165.55000925064087, 166.71699285507202, 167.8670403957367, 169.01708793640137, 170.1622326374054, 171.30737733840942, 172.43615865707397, 173.56493997573853, 174.77116322517395, 175.97738647460938, 177.19971823692322, 178.42204999923706, 179.56673002243042, 180.71141004562378, 181.8573977947235, 183.00338554382324, 184.16593289375305, 185.32848024368286, 186.47112607955933, 187.6137719154358, 188.79717636108398, 189.98058080673218, 191.16113257408142, 192.34168434143066, 193.4669976234436, 194.59231090545654, 195.7334485054016, 196.87458610534668, 198.01002597808838, 199.14546585083008, 200.33109617233276, 201.51672649383545, 202.6541452407837, 203.79156398773193, 204.9309446811676, 206.07032537460327, 207.29072499275208, 208.51112461090088, 209.69191980361938, 210.8727149963379, 212.06181240081787, 213.25090980529785, 214.43434953689575, 215.61778926849365, 216.73513531684875, 217.85248136520386, 218.9941508769989, 220.13582038879395, 221.26612877845764, 222.39643716812134, 223.5663845539093, 224.73633193969727, 225.87589120864868, 227.0154504776001, 228.16839909553528, 229.32134771347046, 230.5105278491974, 231.69970798492432, 232.81796026229858, 233.93621253967285, 235.09271025657654, 236.24920797348022, 238.18091988563538, 240.11263179779053]
[19.558333333333334, 19.558333333333334, 39.975, 39.975, 52.375, 52.375, 58.78333333333333, 58.78333333333333, 67.94166666666666, 67.94166666666666, 76.24166666666666, 76.24166666666666, 76.34166666666667, 76.34166666666667, 81.14166666666667, 81.14166666666667, 81.275, 81.275, 81.31666666666666, 81.31666666666666, 83.03333333333333, 83.03333333333333, 83.075, 83.075, 83.2, 83.2, 83.26666666666667, 83.26666666666667, 86.31666666666666, 86.31666666666666, 86.33333333333333, 86.33333333333333, 86.36666666666666, 86.36666666666666, 86.4, 86.4, 86.40833333333333, 86.40833333333333, 86.4, 86.4, 86.425, 86.425, 86.44166666666666, 86.44166666666666, 86.44166666666666, 86.44166666666666, 86.425, 86.425, 87.725, 87.725, 87.7, 87.7, 87.86666666666666, 87.86666666666666, 87.925, 87.925, 89.26666666666667, 89.26666666666667, 89.25833333333334, 89.25833333333334, 89.33333333333333, 89.33333333333333, 90.65, 90.65, 90.775, 90.775, 90.79166666666667, 90.79166666666667, 90.76666666666667, 90.76666666666667, 90.79166666666667, 90.79166666666667, 90.80833333333334, 90.80833333333334, 90.825, 90.825, 90.83333333333333, 90.83333333333333, 90.83333333333333, 90.83333333333333, 90.81666666666666, 90.81666666666666, 90.9, 90.9, 90.875, 90.875, 90.89166666666667, 90.89166666666667, 90.86666666666666, 90.86666666666666, 90.88333333333334, 90.88333333333334, 90.89166666666667, 90.89166666666667, 90.93333333333334, 90.93333333333334, 90.9, 90.9, 90.9, 90.9, 90.925, 90.925, 90.91666666666667, 90.91666666666667, 90.91666666666667, 90.91666666666667, 90.90833333333333, 90.90833333333333, 90.875, 90.875, 90.86666666666666, 90.86666666666666, 90.875, 90.875, 90.85833333333333, 90.85833333333333, 90.85833333333333, 90.85833333333333, 90.84166666666667, 90.84166666666667, 90.88333333333334, 90.88333333333334, 90.875, 90.875, 90.86666666666666, 90.86666666666666, 90.85, 90.85, 90.85833333333333, 90.85833333333333, 90.88333333333334, 90.88333333333334, 90.875, 90.875, 90.88333333333334, 90.88333333333334, 90.9, 90.9, 90.9, 90.9, 90.9, 90.9, 90.9, 90.9, 90.925, 90.925, 90.925, 90.925, 90.93333333333334, 90.93333333333334, 90.91666666666667, 90.91666666666667, 90.90833333333333, 90.90833333333333, 90.90833333333333, 90.90833333333333, 90.925, 90.925, 90.925, 90.925, 90.91666666666667, 90.91666666666667, 90.91666666666667, 90.91666666666667, 90.90833333333333, 90.90833333333333, 90.91666666666667, 90.91666666666667, 90.925, 90.925, 90.925, 90.925, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.95, 90.95, 90.95, 90.95, 90.95, 90.95, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.94166666666666, 90.93333333333334, 90.93333333333334, 90.94166666666666, 90.94166666666666, 91.04166666666667, 91.04166666666667, 91.04166666666667, 91.04166666666667, 91.04166666666667, 91.04166666666667, 92.35833333333333, 92.35833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.300, Test accuracy: 16.57
Round   0, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.38
Round   1, Train loss: 2.294, Test loss: 2.293, Test accuracy: 18.71
Round   1, Global train loss: 2.294, Global test loss: 2.301, Global test accuracy: 11.80
Round   2, Train loss: 2.278, Test loss: 2.275, Test accuracy: 23.35
Round   2, Global train loss: 2.278, Global test loss: 2.300, Global test accuracy: 11.57
Round   3, Train loss: 2.218, Test loss: 2.231, Test accuracy: 27.83
Round   3, Global train loss: 2.218, Global test loss: 2.296, Global test accuracy: 11.72
Round   4, Train loss: 2.099, Test loss: 2.180, Test accuracy: 30.98
Round   4, Global train loss: 2.099, Global test loss: 2.302, Global test accuracy: 12.43
Round   5, Train loss: 2.034, Test loss: 2.135, Test accuracy: 35.83
Round   5, Global train loss: 2.034, Global test loss: 2.307, Global test accuracy: 12.59
Round   6, Train loss: 2.089, Test loss: 2.092, Test accuracy: 39.30
Round   6, Global train loss: 2.089, Global test loss: 2.305, Global test accuracy: 12.67
Round   7, Train loss: 1.979, Test loss: 2.065, Test accuracy: 41.39
Round   7, Global train loss: 1.979, Global test loss: 2.308, Global test accuracy: 12.87
Round   8, Train loss: 2.123, Test loss: 2.042, Test accuracy: 41.97
Round   8, Global train loss: 2.123, Global test loss: 2.302, Global test accuracy: 12.59
Round   9, Train loss: 2.049, Test loss: 2.037, Test accuracy: 42.72
Round   9, Global train loss: 2.049, Global test loss: 2.304, Global test accuracy: 13.10
Round  10, Train loss: 2.061, Test loss: 2.029, Test accuracy: 43.60
Round  10, Global train loss: 2.061, Global test loss: 2.304, Global test accuracy: 13.31
Round  11, Train loss: 1.992, Test loss: 2.017, Test accuracy: 44.91
Round  11, Global train loss: 1.992, Global test loss: 2.295, Global test accuracy: 14.28
Round  12, Train loss: 2.052, Test loss: 2.007, Test accuracy: 45.97
Round  12, Global train loss: 2.052, Global test loss: 2.295, Global test accuracy: 14.08
Round  13, Train loss: 2.032, Test loss: 1.995, Test accuracy: 46.81
Round  13, Global train loss: 2.032, Global test loss: 2.293, Global test accuracy: 13.96
Round  14, Train loss: 2.025, Test loss: 1.987, Test accuracy: 47.58
Round  14, Global train loss: 2.025, Global test loss: 2.293, Global test accuracy: 14.67
Round  15, Train loss: 1.982, Test loss: 1.982, Test accuracy: 48.11
Round  15, Global train loss: 1.982, Global test loss: 2.294, Global test accuracy: 13.96
Round  16, Train loss: 1.994, Test loss: 1.985, Test accuracy: 47.54
Round  16, Global train loss: 1.994, Global test loss: 2.297, Global test accuracy: 14.03
Round  17, Train loss: 1.987, Test loss: 1.980, Test accuracy: 48.14
Round  17, Global train loss: 1.987, Global test loss: 2.298, Global test accuracy: 14.13
Round  18, Train loss: 1.995, Test loss: 1.974, Test accuracy: 48.43
Round  18, Global train loss: 1.995, Global test loss: 2.294, Global test accuracy: 14.89
Round  19, Train loss: 1.963, Test loss: 1.971, Test accuracy: 48.81
Round  19, Global train loss: 1.963, Global test loss: 2.295, Global test accuracy: 14.41
Round  20, Train loss: 1.980, Test loss: 1.968, Test accuracy: 49.24
Round  20, Global train loss: 1.980, Global test loss: 2.297, Global test accuracy: 13.57
Round  21, Train loss: 1.993, Test loss: 1.953, Test accuracy: 50.69
Round  21, Global train loss: 1.993, Global test loss: 2.286, Global test accuracy: 15.27
Round  22, Train loss: 2.002, Test loss: 1.960, Test accuracy: 50.01
Round  22, Global train loss: 2.002, Global test loss: 2.295, Global test accuracy: 13.65
Round  23, Train loss: 1.948, Test loss: 1.955, Test accuracy: 50.51
Round  23, Global train loss: 1.948, Global test loss: 2.289, Global test accuracy: 15.02
Round  24, Train loss: 1.954, Test loss: 1.948, Test accuracy: 51.26
Round  24, Global train loss: 1.954, Global test loss: 2.288, Global test accuracy: 14.61
Round  25, Train loss: 1.929, Test loss: 1.940, Test accuracy: 52.07
Round  25, Global train loss: 1.929, Global test loss: 2.286, Global test accuracy: 15.04
Round  26, Train loss: 1.950, Test loss: 1.937, Test accuracy: 52.31
Round  26, Global train loss: 1.950, Global test loss: 2.293, Global test accuracy: 13.97
Round  27, Train loss: 1.974, Test loss: 1.934, Test accuracy: 52.68
Round  27, Global train loss: 1.974, Global test loss: 2.285, Global test accuracy: 15.52
Round  28, Train loss: 1.943, Test loss: 1.938, Test accuracy: 52.16
Round  28, Global train loss: 1.943, Global test loss: 2.290, Global test accuracy: 14.92
Round  29, Train loss: 1.986, Test loss: 1.943, Test accuracy: 51.67
Round  29, Global train loss: 1.986, Global test loss: 2.300, Global test accuracy: 14.19
Round  30, Train loss: 1.964, Test loss: 1.932, Test accuracy: 52.86
Round  30, Global train loss: 1.964, Global test loss: 2.287, Global test accuracy: 15.20
Round  31, Train loss: 1.916, Test loss: 1.922, Test accuracy: 53.87
Round  31, Global train loss: 1.916, Global test loss: 2.292, Global test accuracy: 13.80
Round  32, Train loss: 1.890, Test loss: 1.923, Test accuracy: 53.66
Round  32, Global train loss: 1.890, Global test loss: 2.290, Global test accuracy: 15.06
Round  33, Train loss: 1.849, Test loss: 1.909, Test accuracy: 55.06
Round  33, Global train loss: 1.849, Global test loss: 2.292, Global test accuracy: 14.42
Round  34, Train loss: 1.914, Test loss: 1.905, Test accuracy: 55.49
Round  34, Global train loss: 1.914, Global test loss: 2.287, Global test accuracy: 15.31
Round  35, Train loss: 1.919, Test loss: 1.898, Test accuracy: 56.23
Round  35, Global train loss: 1.919, Global test loss: 2.290, Global test accuracy: 14.79
Round  36, Train loss: 1.972, Test loss: 1.906, Test accuracy: 55.28
Round  36, Global train loss: 1.972, Global test loss: 2.281, Global test accuracy: 15.58
Round  37, Train loss: 1.896, Test loss: 1.901, Test accuracy: 55.83
Round  37, Global train loss: 1.896, Global test loss: 2.289, Global test accuracy: 15.19
Round  38, Train loss: 1.919, Test loss: 1.900, Test accuracy: 55.92
Round  38, Global train loss: 1.919, Global test loss: 2.286, Global test accuracy: 14.77
Round  39, Train loss: 2.022, Test loss: 1.896, Test accuracy: 56.38
Round  39, Global train loss: 2.022, Global test loss: 2.284, Global test accuracy: 15.16
Round  40, Train loss: 1.955, Test loss: 1.890, Test accuracy: 57.02
Round  40, Global train loss: 1.955, Global test loss: 2.289, Global test accuracy: 14.43
Round  41, Train loss: 1.884, Test loss: 1.905, Test accuracy: 55.41
Round  41, Global train loss: 1.884, Global test loss: 2.287, Global test accuracy: 14.94
Round  42, Train loss: 1.874, Test loss: 1.902, Test accuracy: 55.71
Round  42, Global train loss: 1.874, Global test loss: 2.285, Global test accuracy: 15.63
Round  43, Train loss: 1.927, Test loss: 1.902, Test accuracy: 55.67
Round  43, Global train loss: 1.927, Global test loss: 2.284, Global test accuracy: 15.37
Round  44, Train loss: 1.885, Test loss: 1.893, Test accuracy: 56.54
Round  44, Global train loss: 1.885, Global test loss: 2.292, Global test accuracy: 14.36
Round  45, Train loss: 1.932, Test loss: 1.892, Test accuracy: 56.61
Round  45, Global train loss: 1.932, Global test loss: 2.278, Global test accuracy: 16.53
Round  46, Train loss: 1.894, Test loss: 1.896, Test accuracy: 56.23
Round  46, Global train loss: 1.894, Global test loss: 2.284, Global test accuracy: 15.51
Round  47, Train loss: 1.872, Test loss: 1.905, Test accuracy: 55.26
Round  47, Global train loss: 1.872, Global test loss: 2.289, Global test accuracy: 14.19
Round  48, Train loss: 1.935, Test loss: 1.900, Test accuracy: 55.78
Round  48, Global train loss: 1.935, Global test loss: 2.282, Global test accuracy: 16.05
Round  49, Train loss: 1.864, Test loss: 1.902, Test accuracy: 55.67
Round  49, Global train loss: 1.864, Global test loss: 2.289, Global test accuracy: 14.38
Round  50, Train loss: 1.925, Test loss: 1.894, Test accuracy: 56.50
Round  50, Global train loss: 1.925, Global test loss: 2.285, Global test accuracy: 15.20
Round  51, Train loss: 1.852, Test loss: 1.888, Test accuracy: 57.06
Round  51, Global train loss: 1.852, Global test loss: 2.288, Global test accuracy: 15.35
Round  52, Train loss: 1.956, Test loss: 1.883, Test accuracy: 57.69
Round  52, Global train loss: 1.956, Global test loss: 2.279, Global test accuracy: 16.19
Round  53, Train loss: 1.908, Test loss: 1.867, Test accuracy: 59.40
Round  53, Global train loss: 1.908, Global test loss: 2.283, Global test accuracy: 15.36
Round  54, Train loss: 1.817, Test loss: 1.854, Test accuracy: 60.87
Round  54, Global train loss: 1.817, Global test loss: 2.287, Global test accuracy: 14.99
Round  55, Train loss: 1.817, Test loss: 1.842, Test accuracy: 62.21
Round  55, Global train loss: 1.817, Global test loss: 2.305, Global test accuracy: 13.02
Round  56, Train loss: 1.791, Test loss: 1.841, Test accuracy: 62.24
Round  56, Global train loss: 1.791, Global test loss: 2.294, Global test accuracy: 14.39
Round  57, Train loss: 1.932, Test loss: 1.849, Test accuracy: 61.24
Round  57, Global train loss: 1.932, Global test loss: 2.304, Global test accuracy: 12.77
Round  58, Train loss: 1.779, Test loss: 1.842, Test accuracy: 62.06
Round  58, Global train loss: 1.779, Global test loss: 2.292, Global test accuracy: 14.12
Round  59, Train loss: 1.858, Test loss: 1.855, Test accuracy: 60.61
Round  59, Global train loss: 1.858, Global test loss: 2.298, Global test accuracy: 13.73
Round  60, Train loss: 1.895, Test loss: 1.872, Test accuracy: 58.57
Round  60, Global train loss: 1.895, Global test loss: 2.298, Global test accuracy: 13.76
Round  61, Train loss: 1.865, Test loss: 1.869, Test accuracy: 58.99
Round  61, Global train loss: 1.865, Global test loss: 2.286, Global test accuracy: 15.28
Round  62, Train loss: 1.889, Test loss: 1.864, Test accuracy: 59.47
Round  62, Global train loss: 1.889, Global test loss: 2.302, Global test accuracy: 13.78
Round  63, Train loss: 1.940, Test loss: 1.859, Test accuracy: 60.09
Round  63, Global train loss: 1.940, Global test loss: 2.294, Global test accuracy: 13.84
Round  64, Train loss: 1.853, Test loss: 1.862, Test accuracy: 59.66
Round  64, Global train loss: 1.853, Global test loss: 2.297, Global test accuracy: 13.79
Round  65, Train loss: 1.830, Test loss: 1.869, Test accuracy: 58.91
Round  65, Global train loss: 1.830, Global test loss: 2.281, Global test accuracy: 15.88
Round  66, Train loss: 1.837, Test loss: 1.867, Test accuracy: 59.15
Round  66, Global train loss: 1.837, Global test loss: 2.285, Global test accuracy: 15.02
Round  67, Train loss: 1.796, Test loss: 1.857, Test accuracy: 60.07
Round  67, Global train loss: 1.796, Global test loss: 2.295, Global test accuracy: 14.60
Round  68, Train loss: 1.823, Test loss: 1.865, Test accuracy: 59.22
Round  68, Global train loss: 1.823, Global test loss: 2.295, Global test accuracy: 13.82
Round  69, Train loss: 1.903, Test loss: 1.869, Test accuracy: 58.79
Round  69, Global train loss: 1.903, Global test loss: 2.288, Global test accuracy: 15.33
Round  70, Train loss: 1.932, Test loss: 1.860, Test accuracy: 59.79
Round  70, Global train loss: 1.932, Global test loss: 2.282, Global test accuracy: 15.72
Round  71, Train loss: 1.838, Test loss: 1.846, Test accuracy: 61.29
Round  71, Global train loss: 1.838, Global test loss: 2.285, Global test accuracy: 14.89
Round  72, Train loss: 1.762, Test loss: 1.842, Test accuracy: 61.56
Round  72, Global train loss: 1.762, Global test loss: 2.280, Global test accuracy: 15.57
Round  73, Train loss: 1.874, Test loss: 1.850, Test accuracy: 60.94
Round  73, Global train loss: 1.874, Global test loss: 2.280, Global test accuracy: 16.11
Round  74, Train loss: 1.865, Test loss: 1.829, Test accuracy: 63.06
Round  74, Global train loss: 1.865, Global test loss: 2.288, Global test accuracy: 14.63
Round  75, Train loss: 1.825, Test loss: 1.824, Test accuracy: 63.68
Round  75, Global train loss: 1.825, Global test loss: 2.281, Global test accuracy: 15.92
Round  76, Train loss: 1.806, Test loss: 1.821, Test accuracy: 63.98
Round  76, Global train loss: 1.806, Global test loss: 2.283, Global test accuracy: 15.09
Round  77, Train loss: 1.810, Test loss: 1.820, Test accuracy: 64.09
Round  77, Global train loss: 1.810, Global test loss: 2.289, Global test accuracy: 14.56
Round  78, Train loss: 1.804, Test loss: 1.818, Test accuracy: 64.23
Round  78, Global train loss: 1.804, Global test loss: 2.282, Global test accuracy: 15.56
Round  79, Train loss: 1.820, Test loss: 1.820, Test accuracy: 64.12
Round  79, Global train loss: 1.820, Global test loss: 2.277, Global test accuracy: 16.20
Round  80, Train loss: 1.852, Test loss: 1.819, Test accuracy: 64.11
Round  80, Global train loss: 1.852, Global test loss: 2.279, Global test accuracy: 16.33
Round  81, Train loss: 1.776, Test loss: 1.809, Test accuracy: 65.14
Round  81, Global train loss: 1.776, Global test loss: 2.279, Global test accuracy: 16.12
Round  82, Train loss: 1.804, Test loss: 1.813, Test accuracy: 64.69
Round  82, Global train loss: 1.804, Global test loss: 2.294, Global test accuracy: 13.88
Round  83, Train loss: 1.829, Test loss: 1.821, Test accuracy: 63.90
Round  83, Global train loss: 1.829, Global test loss: 2.288, Global test accuracy: 14.99
Round  84, Train loss: 1.808, Test loss: 1.811, Test accuracy: 64.94
Round  84, Global train loss: 1.808, Global test loss: 2.294, Global test accuracy: 14.51
Round  85, Train loss: 1.806, Test loss: 1.807, Test accuracy: 65.42
Round  85, Global train loss: 1.806, Global test loss: 2.292, Global test accuracy: 14.34
Round  86, Train loss: 1.843, Test loss: 1.802, Test accuracy: 65.84
Round  86, Global train loss: 1.843, Global test loss: 2.300, Global test accuracy: 13.73
Round  87, Train loss: 1.805, Test loss: 1.814, Test accuracy: 64.61
Round  87, Global train loss: 1.805, Global test loss: 2.286, Global test accuracy: 14.84
Round  88, Train loss: 1.790, Test loss: 1.810, Test accuracy: 65.07
Round  88, Global train loss: 1.790, Global test loss: 2.287, Global test accuracy: 14.98
Round  89, Train loss: 1.832, Test loss: 1.804, Test accuracy: 65.54
Round  89, Global train loss: 1.832, Global test loss: 2.289, Global test accuracy: 15.11
Round  90, Train loss: 1.860, Test loss: 1.805, Test accuracy: 65.53
Round  90, Global train loss: 1.860, Global test loss: 2.283, Global test accuracy: 15.28
Round  91, Train loss: 1.783, Test loss: 1.798, Test accuracy: 66.22
Round  91, Global train loss: 1.783, Global test loss: 2.279, Global test accuracy: 16.17
Round  92, Train loss: 1.796, Test loss: 1.800, Test accuracy: 66.08
Round  92, Global train loss: 1.796, Global test loss: 2.283, Global test accuracy: 15.31
Round  93, Train loss: 1.753, Test loss: 1.797, Test accuracy: 66.40
Round  93, Global train loss: 1.753, Global test loss: 2.281, Global test accuracy: 15.19
Round  94, Train loss: 1.779, Test loss: 1.785, Test accuracy: 67.66
Round  94, Global train loss: 1.779, Global test loss: 2.283, Global test accuracy: 15.26
Round  95, Train loss: 1.765, Test loss: 1.779, Test accuracy: 68.23
Round  95, Global train loss: 1.765, Global test loss: 2.286, Global test accuracy: 14.88
Round  96, Train loss: 1.710, Test loss: 1.779, Test accuracy: 68.34
Round  96, Global train loss: 1.710, Global test loss: 2.279, Global test accuracy: 15.91
Round  97, Train loss: 1.772, Test loss: 1.776, Test accuracy: 68.64
Round  97, Global train loss: 1.772, Global test loss: 2.287, Global test accuracy: 14.86
Round  98, Train loss: 1.812, Test loss: 1.770, Test accuracy: 69.33
Round  98, Global train loss: 1.812, Global test loss: 2.282, Global test accuracy: 15.16
Round  99, Train loss: 1.736, Test loss: 1.770, Test accuracy: 69.27
Round  99, Global train loss: 1.736, Global test loss: 2.286, Global test accuracy: 15.06
Final Round, Train loss: 1.725, Test loss: 1.767, Test accuracy: 69.61
Final Round, Global train loss: 1.725, Global test loss: 2.286, Global test accuracy: 15.06
Average accuracy final 10 rounds: 67.57 

Average global accuracy final 10 rounds: 15.30777777777778 

2613.2193512916565
[1.7365078926086426, 3.473015785217285, 5.123746633529663, 6.774477481842041, 8.411771059036255, 10.049064636230469, 11.780347108840942, 13.511629581451416, 15.194716453552246, 16.877803325653076, 18.5593364238739, 20.240869522094727, 21.92493486404419, 23.609000205993652, 25.29995822906494, 26.99091625213623, 28.650312423706055, 30.30970859527588, 31.976306676864624, 33.64290475845337, 35.2991988658905, 36.95549297332764, 38.61063265800476, 40.265772342681885, 41.921950340270996, 43.57812833786011, 45.26247525215149, 46.94682216644287, 48.61033773422241, 50.27385330200195, 52.00873875617981, 53.743624210357666, 55.38912892341614, 57.03463363647461, 58.66936445236206, 60.30409526824951, 61.99014639854431, 63.67619752883911, 65.28466939926147, 66.89314126968384, 68.60535931587219, 70.31757736206055, 71.90882182121277, 73.50006628036499, 75.1413266658783, 76.7825870513916, 78.4463369846344, 80.1100869178772, 81.6937415599823, 83.2773962020874, 85.05024838447571, 86.82310056686401, 88.44569206237793, 90.06828355789185, 91.77232670783997, 93.47636985778809, 95.16027593612671, 96.84418201446533, 98.54970002174377, 100.25521802902222, 101.99908757209778, 103.74295711517334, 105.38899159431458, 107.03502607345581, 108.81931948661804, 110.60361289978027, 112.27465987205505, 113.94570684432983, 115.75737953186035, 117.56905221939087, 119.35337781906128, 121.13770341873169, 122.83042335510254, 124.52314329147339, 126.21079874038696, 127.89845418930054, 129.58161091804504, 131.26476764678955, 132.97317457199097, 134.68158149719238, 136.39855074882507, 138.11552000045776, 139.77727723121643, 141.4390344619751, 143.12541699409485, 144.8117995262146, 146.48306822776794, 148.1543369293213, 149.8046474456787, 151.45495796203613, 153.13137674331665, 154.80779552459717, 156.4464933872223, 158.0851912498474, 159.79347562789917, 161.50176000595093, 163.1137535572052, 164.72574710845947, 166.40324091911316, 168.08073472976685, 169.74921870231628, 171.41770267486572, 173.06168627738953, 174.70566987991333, 176.46702814102173, 178.22838640213013, 179.9269859790802, 181.62558555603027, 183.31475114822388, 185.00391674041748, 186.7025363445282, 188.40115594863892, 190.11351895332336, 191.8258819580078, 193.51667428016663, 195.20746660232544, 196.976459980011, 198.74545335769653, 200.4303696155548, 202.1152858734131, 203.8013792037964, 205.4874725341797, 207.1654508113861, 208.84342908859253, 210.52811431884766, 212.21279954910278, 213.91799235343933, 215.62318515777588, 217.2733941078186, 218.92360305786133, 220.63256168365479, 222.34152030944824, 223.92746877670288, 225.51341724395752, 227.2004678249359, 228.8875184059143, 230.52179431915283, 232.15607023239136, 233.76443004608154, 235.37278985977173, 237.03011012077332, 238.6874303817749, 240.2613205909729, 241.8352108001709, 243.5921630859375, 245.3491153717041, 246.99085116386414, 248.63258695602417, 250.31594014167786, 251.99929332733154, 253.70525598526, 255.41121864318848, 257.0897870063782, 258.76835536956787, 260.53243494033813, 262.2965145111084, 263.97334480285645, 265.6501750946045, 267.3214089870453, 268.9926428794861, 270.724161863327, 272.45568084716797, 274.1973400115967, 275.9389991760254, 277.692182302475, 279.44536542892456, 281.1542341709137, 282.86310291290283, 284.4872844219208, 286.1114659309387, 287.51434230804443, 288.91721868515015, 290.30932569503784, 291.70143270492554, 293.22133708000183, 294.7412414550781, 296.1336135864258, 297.52598571777344, 299.22707200050354, 300.92815828323364, 302.5228669643402, 304.1175756454468, 305.6741864681244, 307.230797290802, 308.87333703041077, 310.51587677001953, 312.10780024528503, 313.69972372055054, 315.3788650035858, 317.0580062866211, 318.6896941661835, 320.32138204574585, 322.0149266719818, 323.7084712982178, 325.35819125175476, 327.00791120529175, 328.6419909000397, 330.2760705947876, 331.9506151676178, 333.625159740448, 335.57940578460693, 337.53365182876587]
[16.572222222222223, 16.572222222222223, 18.705555555555556, 18.705555555555556, 23.35, 23.35, 27.833333333333332, 27.833333333333332, 30.977777777777778, 30.977777777777778, 35.827777777777776, 35.827777777777776, 39.3, 39.3, 41.394444444444446, 41.394444444444446, 41.97222222222222, 41.97222222222222, 42.72222222222222, 42.72222222222222, 43.6, 43.6, 44.90555555555556, 44.90555555555556, 45.97222222222222, 45.97222222222222, 46.80555555555556, 46.80555555555556, 47.583333333333336, 47.583333333333336, 48.105555555555554, 48.105555555555554, 47.544444444444444, 47.544444444444444, 48.144444444444446, 48.144444444444446, 48.43333333333333, 48.43333333333333, 48.80555555555556, 48.80555555555556, 49.23888888888889, 49.23888888888889, 50.68888888888889, 50.68888888888889, 50.00555555555555, 50.00555555555555, 50.50555555555555, 50.50555555555555, 51.25555555555555, 51.25555555555555, 52.07222222222222, 52.07222222222222, 52.31111111111111, 52.31111111111111, 52.68333333333333, 52.68333333333333, 52.16111111111111, 52.16111111111111, 51.666666666666664, 51.666666666666664, 52.855555555555554, 52.855555555555554, 53.87222222222222, 53.87222222222222, 53.66111111111111, 53.66111111111111, 55.06111111111111, 55.06111111111111, 55.49444444444445, 55.49444444444445, 56.233333333333334, 56.233333333333334, 55.28333333333333, 55.28333333333333, 55.833333333333336, 55.833333333333336, 55.922222222222224, 55.922222222222224, 56.37777777777778, 56.37777777777778, 57.022222222222226, 57.022222222222226, 55.40555555555556, 55.40555555555556, 55.705555555555556, 55.705555555555556, 55.672222222222224, 55.672222222222224, 56.544444444444444, 56.544444444444444, 56.611111111111114, 56.611111111111114, 56.233333333333334, 56.233333333333334, 55.25555555555555, 55.25555555555555, 55.77777777777778, 55.77777777777778, 55.672222222222224, 55.672222222222224, 56.5, 56.5, 57.05555555555556, 57.05555555555556, 57.69444444444444, 57.69444444444444, 59.4, 59.4, 60.87222222222222, 60.87222222222222, 62.205555555555556, 62.205555555555556, 62.23888888888889, 62.23888888888889, 61.24444444444445, 61.24444444444445, 62.06111111111111, 62.06111111111111, 60.605555555555554, 60.605555555555554, 58.56666666666667, 58.56666666666667, 58.98888888888889, 58.98888888888889, 59.46666666666667, 59.46666666666667, 60.08888888888889, 60.08888888888889, 59.66111111111111, 59.66111111111111, 58.90555555555556, 58.90555555555556, 59.15, 59.15, 60.07222222222222, 60.07222222222222, 59.21666666666667, 59.21666666666667, 58.794444444444444, 58.794444444444444, 59.78888888888889, 59.78888888888889, 61.28888888888889, 61.28888888888889, 61.56111111111111, 61.56111111111111, 60.93888888888889, 60.93888888888889, 63.05555555555556, 63.05555555555556, 63.67777777777778, 63.67777777777778, 63.983333333333334, 63.983333333333334, 64.08888888888889, 64.08888888888889, 64.23333333333333, 64.23333333333333, 64.12222222222222, 64.12222222222222, 64.11111111111111, 64.11111111111111, 65.14444444444445, 65.14444444444445, 64.68888888888888, 64.68888888888888, 63.9, 63.9, 64.93888888888888, 64.93888888888888, 65.42222222222222, 65.42222222222222, 65.84444444444445, 65.84444444444445, 64.61111111111111, 64.61111111111111, 65.06666666666666, 65.06666666666666, 65.54444444444445, 65.54444444444445, 65.53333333333333, 65.53333333333333, 66.21666666666667, 66.21666666666667, 66.07777777777778, 66.07777777777778, 66.4, 66.4, 67.65555555555555, 67.65555555555555, 68.23333333333333, 68.23333333333333, 68.34444444444445, 68.34444444444445, 68.63888888888889, 68.63888888888889, 69.33333333333333, 69.33333333333333, 69.26666666666667, 69.26666666666667, 69.61111111111111, 69.61111111111111]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.09
Round   1, Train loss: 2.300, Test loss: 2.302, Test accuracy: 11.12
Round   2, Train loss: 2.299, Test loss: 2.301, Test accuracy: 11.77
Round   3, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.28
Round   4, Train loss: 2.297, Test loss: 2.300, Test accuracy: 12.21
Round   5, Train loss: 2.295, Test loss: 2.298, Test accuracy: 11.20
Round   6, Train loss: 2.288, Test loss: 2.295, Test accuracy: 12.44
Round   7, Train loss: 2.269, Test loss: 2.284, Test accuracy: 16.66
Round   8, Train loss: 2.220, Test loss: 2.260, Test accuracy: 18.06
Round   9, Train loss: 2.156, Test loss: 2.220, Test accuracy: 22.69
Round  10, Train loss: 2.104, Test loss: 2.186, Test accuracy: 26.13
Round  11, Train loss: 2.087, Test loss: 2.134, Test accuracy: 32.55
Round  12, Train loss: 2.032, Test loss: 2.112, Test accuracy: 34.85
Round  13, Train loss: 1.998, Test loss: 2.084, Test accuracy: 37.69
Round  14, Train loss: 2.031, Test loss: 2.053, Test accuracy: 40.98
Round  15, Train loss: 1.992, Test loss: 2.041, Test accuracy: 41.91
Round  16, Train loss: 1.946, Test loss: 2.011, Test accuracy: 45.25
Round  17, Train loss: 1.990, Test loss: 1.996, Test accuracy: 46.82
Round  18, Train loss: 2.002, Test loss: 1.990, Test accuracy: 47.38
Round  19, Train loss: 1.923, Test loss: 1.987, Test accuracy: 47.57
Round  20, Train loss: 1.948, Test loss: 1.981, Test accuracy: 48.09
Round  21, Train loss: 1.996, Test loss: 1.979, Test accuracy: 48.29
Round  22, Train loss: 1.936, Test loss: 1.967, Test accuracy: 49.45
Round  23, Train loss: 1.944, Test loss: 1.965, Test accuracy: 49.58
Round  24, Train loss: 1.956, Test loss: 1.964, Test accuracy: 49.68
Round  25, Train loss: 1.851, Test loss: 1.946, Test accuracy: 51.56
Round  26, Train loss: 1.975, Test loss: 1.944, Test accuracy: 51.75
Round  27, Train loss: 1.940, Test loss: 1.925, Test accuracy: 53.94
Round  28, Train loss: 1.889, Test loss: 1.922, Test accuracy: 54.28
Round  29, Train loss: 1.891, Test loss: 1.915, Test accuracy: 54.93
Round  30, Train loss: 1.887, Test loss: 1.913, Test accuracy: 55.06
Round  31, Train loss: 1.884, Test loss: 1.911, Test accuracy: 55.11
Round  32, Train loss: 1.858, Test loss: 1.909, Test accuracy: 55.51
Round  33, Train loss: 1.843, Test loss: 1.905, Test accuracy: 55.89
Round  34, Train loss: 1.874, Test loss: 1.902, Test accuracy: 55.97
Round  35, Train loss: 1.871, Test loss: 1.901, Test accuracy: 56.14
Round  36, Train loss: 1.849, Test loss: 1.900, Test accuracy: 56.19
Round  37, Train loss: 1.842, Test loss: 1.899, Test accuracy: 56.27
Round  38, Train loss: 1.880, Test loss: 1.894, Test accuracy: 56.72
Round  39, Train loss: 1.840, Test loss: 1.889, Test accuracy: 57.33
Round  40, Train loss: 1.902, Test loss: 1.885, Test accuracy: 57.67
Round  41, Train loss: 1.829, Test loss: 1.883, Test accuracy: 57.79
Round  42, Train loss: 1.881, Test loss: 1.883, Test accuracy: 57.82
Round  43, Train loss: 1.852, Test loss: 1.879, Test accuracy: 58.07
Round  44, Train loss: 1.848, Test loss: 1.876, Test accuracy: 58.39
Round  45, Train loss: 1.856, Test loss: 1.875, Test accuracy: 58.54
Round  46, Train loss: 1.876, Test loss: 1.874, Test accuracy: 58.66
Round  47, Train loss: 1.848, Test loss: 1.873, Test accuracy: 58.59
Round  48, Train loss: 1.830, Test loss: 1.873, Test accuracy: 58.63
Round  49, Train loss: 1.835, Test loss: 1.873, Test accuracy: 58.67
Round  50, Train loss: 1.797, Test loss: 1.873, Test accuracy: 58.76
Round  51, Train loss: 1.870, Test loss: 1.872, Test accuracy: 58.82
Round  52, Train loss: 1.825, Test loss: 1.872, Test accuracy: 58.74
Round  53, Train loss: 1.836, Test loss: 1.872, Test accuracy: 58.78
Round  54, Train loss: 1.835, Test loss: 1.872, Test accuracy: 58.73
Round  55, Train loss: 1.849, Test loss: 1.870, Test accuracy: 58.93
Round  56, Train loss: 1.846, Test loss: 1.869, Test accuracy: 58.99
Round  57, Train loss: 1.844, Test loss: 1.864, Test accuracy: 59.48
Round  58, Train loss: 1.861, Test loss: 1.864, Test accuracy: 59.56
Round  59, Train loss: 1.877, Test loss: 1.862, Test accuracy: 59.71
Round  60, Train loss: 1.855, Test loss: 1.857, Test accuracy: 60.27
Round  61, Train loss: 1.860, Test loss: 1.857, Test accuracy: 60.34
Round  62, Train loss: 1.791, Test loss: 1.856, Test accuracy: 60.34
Round  63, Train loss: 1.889, Test loss: 1.855, Test accuracy: 60.49
Round  64, Train loss: 1.837, Test loss: 1.855, Test accuracy: 60.44
Round  65, Train loss: 1.815, Test loss: 1.854, Test accuracy: 60.49
Round  66, Train loss: 1.790, Test loss: 1.854, Test accuracy: 60.55
Round  67, Train loss: 1.808, Test loss: 1.853, Test accuracy: 60.59
Round  68, Train loss: 1.824, Test loss: 1.850, Test accuracy: 60.91
Round  69, Train loss: 1.796, Test loss: 1.849, Test accuracy: 60.98
Round  70, Train loss: 1.805, Test loss: 1.849, Test accuracy: 61.03
Round  71, Train loss: 1.808, Test loss: 1.850, Test accuracy: 60.85
Round  72, Train loss: 1.848, Test loss: 1.848, Test accuracy: 61.07
Round  73, Train loss: 1.825, Test loss: 1.848, Test accuracy: 61.03
Round  74, Train loss: 1.779, Test loss: 1.847, Test accuracy: 61.09
Round  75, Train loss: 1.847, Test loss: 1.847, Test accuracy: 61.24
Round  76, Train loss: 1.810, Test loss: 1.847, Test accuracy: 61.19
Round  77, Train loss: 1.855, Test loss: 1.847, Test accuracy: 61.11
Round  78, Train loss: 1.803, Test loss: 1.847, Test accuracy: 61.12
Round  79, Train loss: 1.819, Test loss: 1.847, Test accuracy: 61.14
Round  80, Train loss: 1.847, Test loss: 1.846, Test accuracy: 61.18
Round  81, Train loss: 1.792, Test loss: 1.846, Test accuracy: 61.23
Round  82, Train loss: 1.835, Test loss: 1.842, Test accuracy: 61.66
Round  83, Train loss: 1.845, Test loss: 1.837, Test accuracy: 62.21
Round  84, Train loss: 1.797, Test loss: 1.837, Test accuracy: 62.23
Round  85, Train loss: 1.804, Test loss: 1.836, Test accuracy: 62.19
Round  86, Train loss: 1.809, Test loss: 1.835, Test accuracy: 62.28
Round  87, Train loss: 1.796, Test loss: 1.835, Test accuracy: 62.35
Round  88, Train loss: 1.823, Test loss: 1.835, Test accuracy: 62.38
Round  89, Train loss: 1.785, Test loss: 1.835, Test accuracy: 62.37
Round  90, Train loss: 1.816, Test loss: 1.835, Test accuracy: 62.36
Round  91, Train loss: 1.811, Test loss: 1.835, Test accuracy: 62.37
Round  92, Train loss: 1.831, Test loss: 1.834, Test accuracy: 62.38
Round  93, Train loss: 1.777, Test loss: 1.834, Test accuracy: 62.39
Round  94, Train loss: 1.792, Test loss: 1.834, Test accuracy: 62.46
Round  95, Train loss: 1.813, Test loss: 1.834, Test accuracy: 62.48
Round  96, Train loss: 1.803, Test loss: 1.833, Test accuracy: 62.42
Round  97, Train loss: 1.774, Test loss: 1.833, Test accuracy: 62.47
Round  98, Train loss: 1.795, Test loss: 1.833, Test accuracy: 62.52/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.794, Test loss: 1.833, Test accuracy: 62.56
Final Round, Train loss: 1.803, Test loss: 1.830, Test accuracy: 62.84
Average accuracy final 10 rounds: 62.44111111111111 

1996.5081324577332
[1.532517910003662, 3.065035820007324, 4.710226774215698, 6.355417728424072, 7.955010890960693, 9.554604053497314, 11.112279415130615, 12.669954776763916, 14.259313106536865, 15.848671436309814, 17.49692177772522, 19.145172119140625, 20.71688461303711, 22.288597106933594, 23.913411378860474, 25.538225650787354, 27.172008275985718, 28.805790901184082, 30.29602885246277, 31.786266803741455, 33.42288112640381, 35.05949544906616, 36.69851303100586, 38.33753061294556, 39.90210270881653, 41.4666748046875, 43.033796072006226, 44.60091733932495, 46.25863242149353, 47.91634750366211, 49.45176076889038, 50.98717403411865, 52.63157677650452, 54.27597951889038, 55.82721304893494, 57.37844657897949, 58.90436148643494, 60.43027639389038, 62.05902981758118, 63.68778324127197, 65.24448251724243, 66.80118179321289, 68.37840032577515, 69.9556188583374, 71.55957841873169, 73.16353797912598, 74.7385003566742, 76.31346273422241, 77.8212399482727, 79.329017162323, 80.93741822242737, 82.54581928253174, 84.11327528953552, 85.6807312965393, 87.21297240257263, 88.74521350860596, 90.34744811058044, 91.94968271255493, 93.53686141967773, 95.12404012680054, 96.66513228416443, 98.20622444152832, 99.76985478401184, 101.33348512649536, 102.89582395553589, 104.45816278457642, 106.05814361572266, 107.6581244468689, 109.29174327850342, 110.92536211013794, 112.492680311203, 114.05999851226807, 115.66589641571045, 117.27179431915283, 118.89377760887146, 120.51576089859009, 122.06078147888184, 123.60580205917358, 125.17674446105957, 126.74768686294556, 128.35694479942322, 129.96620273590088, 131.50198888778687, 133.03777503967285, 134.60439085960388, 136.1710066795349, 137.7950839996338, 139.41916131973267, 140.9647581577301, 142.51035499572754, 144.0603699684143, 145.61038494110107, 147.2516074180603, 148.89282989501953, 150.4982945919037, 152.10375928878784, 153.65637969970703, 155.20900011062622, 156.7946846485138, 158.38036918640137, 159.98622369766235, 161.59207820892334, 163.19791960716248, 164.8037610054016, 166.40581965446472, 168.00787830352783, 169.56745100021362, 171.1270236968994, 172.7081983089447, 174.28937292099, 175.89252042770386, 177.49566793441772, 179.0784113407135, 180.66115474700928, 182.24736142158508, 183.8335680961609, 185.43349146842957, 187.03341484069824, 188.63524389266968, 190.2370729446411, 191.80151271820068, 193.36595249176025, 194.9812891483307, 196.59662580490112, 198.20663404464722, 199.8166422843933, 201.36657619476318, 202.91651010513306, 204.5191490650177, 206.12178802490234, 207.7474868297577, 209.37318563461304, 210.95205330848694, 212.53092098236084, 214.10832285881042, 215.68572473526, 217.35336709022522, 219.02100944519043, 220.59333038330078, 222.16565132141113, 223.71853637695312, 225.27142143249512, 226.9114637374878, 228.55150604248047, 230.16790866851807, 231.78431129455566, 233.35238337516785, 234.92045545578003, 236.55601286888123, 238.19157028198242, 239.7982633113861, 241.4049563407898, 243.02086091041565, 244.6367654800415, 246.23674654960632, 247.83672761917114, 249.4079954624176, 250.97926330566406, 252.5550434589386, 254.13082361221313, 255.74146056175232, 257.3520975112915, 258.824449300766, 260.2968010902405, 261.72572112083435, 263.1546411514282, 264.6198196411133, 266.08499813079834, 267.426230430603, 268.7674627304077, 270.1306266784668, 271.4937906265259, 272.9933273792267, 274.4928641319275, 275.9293076992035, 277.3657512664795, 278.7274694442749, 280.0891876220703, 281.57769322395325, 283.0661988258362, 284.46568274497986, 285.86516666412354, 287.1873617172241, 288.5095567703247, 290.0241687297821, 291.5387806892395, 292.9612810611725, 294.38378143310547, 295.6995711326599, 297.01536083221436, 298.49177861213684, 299.9681963920593, 301.3633954524994, 302.75859451293945, 304.0737419128418, 305.38888931274414, 306.8824954032898, 308.37610149383545, 309.8270752429962, 311.278048992157, 312.8382053375244, 314.39836168289185]
[10.094444444444445, 10.094444444444445, 11.116666666666667, 11.116666666666667, 11.766666666666667, 11.766666666666667, 12.283333333333333, 12.283333333333333, 12.21111111111111, 12.21111111111111, 11.2, 11.2, 12.444444444444445, 12.444444444444445, 16.66111111111111, 16.66111111111111, 18.06111111111111, 18.06111111111111, 22.68888888888889, 22.68888888888889, 26.133333333333333, 26.133333333333333, 32.55, 32.55, 34.85, 34.85, 37.68888888888889, 37.68888888888889, 40.983333333333334, 40.983333333333334, 41.91111111111111, 41.91111111111111, 45.25, 45.25, 46.82222222222222, 46.82222222222222, 47.38333333333333, 47.38333333333333, 47.56666666666667, 47.56666666666667, 48.08888888888889, 48.08888888888889, 48.28888888888889, 48.28888888888889, 49.45, 49.45, 49.577777777777776, 49.577777777777776, 49.67777777777778, 49.67777777777778, 51.55555555555556, 51.55555555555556, 51.75, 51.75, 53.93888888888889, 53.93888888888889, 54.27777777777778, 54.27777777777778, 54.93333333333333, 54.93333333333333, 55.06111111111111, 55.06111111111111, 55.111111111111114, 55.111111111111114, 55.50555555555555, 55.50555555555555, 55.894444444444446, 55.894444444444446, 55.97222222222222, 55.97222222222222, 56.138888888888886, 56.138888888888886, 56.19444444444444, 56.19444444444444, 56.272222222222226, 56.272222222222226, 56.72222222222222, 56.72222222222222, 57.327777777777776, 57.327777777777776, 57.666666666666664, 57.666666666666664, 57.78888888888889, 57.78888888888889, 57.81666666666667, 57.81666666666667, 58.07222222222222, 58.07222222222222, 58.394444444444446, 58.394444444444446, 58.53888888888889, 58.53888888888889, 58.65555555555556, 58.65555555555556, 58.58888888888889, 58.58888888888889, 58.63333333333333, 58.63333333333333, 58.672222222222224, 58.672222222222224, 58.76111111111111, 58.76111111111111, 58.81666666666667, 58.81666666666667, 58.74444444444445, 58.74444444444445, 58.78333333333333, 58.78333333333333, 58.733333333333334, 58.733333333333334, 58.92777777777778, 58.92777777777778, 58.98888888888889, 58.98888888888889, 59.483333333333334, 59.483333333333334, 59.56111111111111, 59.56111111111111, 59.71111111111111, 59.71111111111111, 60.272222222222226, 60.272222222222226, 60.33888888888889, 60.33888888888889, 60.33888888888889, 60.33888888888889, 60.49444444444445, 60.49444444444445, 60.43888888888889, 60.43888888888889, 60.49444444444445, 60.49444444444445, 60.55, 60.55, 60.59444444444444, 60.59444444444444, 60.90555555555556, 60.90555555555556, 60.983333333333334, 60.983333333333334, 61.02777777777778, 61.02777777777778, 60.85, 60.85, 61.06666666666667, 61.06666666666667, 61.03333333333333, 61.03333333333333, 61.08888888888889, 61.08888888888889, 61.23888888888889, 61.23888888888889, 61.18888888888889, 61.18888888888889, 61.111111111111114, 61.111111111111114, 61.11666666666667, 61.11666666666667, 61.138888888888886, 61.138888888888886, 61.17777777777778, 61.17777777777778, 61.227777777777774, 61.227777777777774, 61.65555555555556, 61.65555555555556, 62.205555555555556, 62.205555555555556, 62.233333333333334, 62.233333333333334, 62.19444444444444, 62.19444444444444, 62.28333333333333, 62.28333333333333, 62.35, 62.35, 62.37777777777778, 62.37777777777778, 62.36666666666667, 62.36666666666667, 62.355555555555554, 62.355555555555554, 62.37222222222222, 62.37222222222222, 62.38333333333333, 62.38333333333333, 62.394444444444446, 62.394444444444446, 62.46111111111111, 62.46111111111111, 62.477777777777774, 62.477777777777774, 62.422222222222224, 62.422222222222224, 62.46666666666667, 62.46666666666667, 62.516666666666666, 62.516666666666666, 62.56111111111111, 62.56111111111111, 62.83888888888889, 62.83888888888889]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.302, Test accuracy: 10.22
Round   1, Train loss: 2.289, Test loss: 2.300, Test accuracy: 11.50
Round   2, Train loss: 2.285, Test loss: 2.295, Test accuracy: 12.71
Round   3, Train loss: 2.227, Test loss: 2.280, Test accuracy: 17.46
Round   4, Train loss: 2.115, Test loss: 2.237, Test accuracy: 23.34
Round   5, Train loss: 1.989, Test loss: 2.178, Test accuracy: 27.94
Round   6, Train loss: 2.093, Test loss: 2.117, Test accuracy: 35.67
Round   7, Train loss: 1.935, Test loss: 2.074, Test accuracy: 39.65
Round   8, Train loss: 1.975, Test loss: 2.041, Test accuracy: 43.34
Round   9, Train loss: 1.973, Test loss: 2.022, Test accuracy: 45.01
Round  10, Train loss: 1.940, Test loss: 1.996, Test accuracy: 47.31
Round  11, Train loss: 1.895, Test loss: 1.970, Test accuracy: 50.14
Round  12, Train loss: 1.909, Test loss: 1.960, Test accuracy: 51.03
Round  13, Train loss: 1.875, Test loss: 1.950, Test accuracy: 51.89
Round  14, Train loss: 1.865, Test loss: 1.939, Test accuracy: 52.78
Round  15, Train loss: 1.852, Test loss: 1.921, Test accuracy: 54.57
Round  16, Train loss: 1.870, Test loss: 1.906, Test accuracy: 56.07
Round  17, Train loss: 1.908, Test loss: 1.897, Test accuracy: 56.85
Round  18, Train loss: 1.821, Test loss: 1.892, Test accuracy: 57.23
Round  19, Train loss: 1.838, Test loss: 1.887, Test accuracy: 57.57
Round  20, Train loss: 1.866, Test loss: 1.880, Test accuracy: 58.24
Round  21, Train loss: 1.755, Test loss: 1.878, Test accuracy: 58.37
Round  22, Train loss: 1.800, Test loss: 1.876, Test accuracy: 58.51
Round  23, Train loss: 1.821, Test loss: 1.872, Test accuracy: 58.94
Round  24, Train loss: 1.822, Test loss: 1.861, Test accuracy: 60.07
Round  25, Train loss: 1.837, Test loss: 1.860, Test accuracy: 60.20
Round  26, Train loss: 1.830, Test loss: 1.858, Test accuracy: 60.32
Round  27, Train loss: 1.901, Test loss: 1.854, Test accuracy: 60.96
Round  28, Train loss: 1.835, Test loss: 1.851, Test accuracy: 61.08
Round  29, Train loss: 1.858, Test loss: 1.849, Test accuracy: 61.22
Round  30, Train loss: 1.783, Test loss: 1.844, Test accuracy: 61.68
Round  31, Train loss: 1.738, Test loss: 1.841, Test accuracy: 62.04
Round  32, Train loss: 1.863, Test loss: 1.841, Test accuracy: 62.04
Round  33, Train loss: 1.805, Test loss: 1.841, Test accuracy: 61.92
Round  34, Train loss: 1.770, Test loss: 1.834, Test accuracy: 62.72
Round  35, Train loss: 1.819, Test loss: 1.833, Test accuracy: 62.72
Round  36, Train loss: 1.830, Test loss: 1.831, Test accuracy: 62.96
Round  37, Train loss: 1.820, Test loss: 1.826, Test accuracy: 63.51
Round  38, Train loss: 1.809, Test loss: 1.824, Test accuracy: 63.64
Round  39, Train loss: 1.805, Test loss: 1.824, Test accuracy: 63.75
Round  40, Train loss: 1.831, Test loss: 1.823, Test accuracy: 63.78
Round  41, Train loss: 1.769, Test loss: 1.822, Test accuracy: 63.83
Round  42, Train loss: 1.729, Test loss: 1.822, Test accuracy: 63.86
Round  43, Train loss: 1.768, Test loss: 1.821, Test accuracy: 63.83
Round  44, Train loss: 1.799, Test loss: 1.821, Test accuracy: 63.84
Round  45, Train loss: 1.725, Test loss: 1.819, Test accuracy: 64.07
Round  46, Train loss: 1.715, Test loss: 1.818, Test accuracy: 64.09
Round  47, Train loss: 1.730, Test loss: 1.818, Test accuracy: 64.19
Round  48, Train loss: 1.834, Test loss: 1.817, Test accuracy: 64.19
Round  49, Train loss: 1.858, Test loss: 1.817, Test accuracy: 64.23
Round  50, Train loss: 1.798, Test loss: 1.817, Test accuracy: 64.28
Round  51, Train loss: 1.794, Test loss: 1.814, Test accuracy: 64.48
Round  52, Train loss: 1.809, Test loss: 1.814, Test accuracy: 64.49
Round  53, Train loss: 1.801, Test loss: 1.814, Test accuracy: 64.53
Round  54, Train loss: 1.709, Test loss: 1.811, Test accuracy: 64.74
Round  55, Train loss: 1.787, Test loss: 1.810, Test accuracy: 64.91
Round  56, Train loss: 1.817, Test loss: 1.804, Test accuracy: 65.40
Round  57, Train loss: 1.787, Test loss: 1.805, Test accuracy: 65.36
Round  58, Train loss: 1.720, Test loss: 1.804, Test accuracy: 65.51
Round  59, Train loss: 1.759, Test loss: 1.801, Test accuracy: 65.84
Round  60, Train loss: 1.730, Test loss: 1.801, Test accuracy: 65.77
Round  61, Train loss: 1.748, Test loss: 1.800, Test accuracy: 65.92
Round  62, Train loss: 1.723, Test loss: 1.800, Test accuracy: 65.82
Round  63, Train loss: 1.761, Test loss: 1.800, Test accuracy: 65.84
Round  64, Train loss: 1.820, Test loss: 1.799, Test accuracy: 66.01
Round  65, Train loss: 1.747, Test loss: 1.796, Test accuracy: 66.43
Round  66, Train loss: 1.715, Test loss: 1.795, Test accuracy: 66.44
Round  67, Train loss: 1.856, Test loss: 1.795, Test accuracy: 66.36
Round  68, Train loss: 1.786, Test loss: 1.795, Test accuracy: 66.46
Round  69, Train loss: 1.709, Test loss: 1.791, Test accuracy: 66.91
Round  70, Train loss: 1.783, Test loss: 1.789, Test accuracy: 67.02
Round  71, Train loss: 1.810, Test loss: 1.789, Test accuracy: 67.06
Round  72, Train loss: 1.733, Test loss: 1.788, Test accuracy: 67.06
Round  73, Train loss: 1.822, Test loss: 1.788, Test accuracy: 67.06
Round  74, Train loss: 1.754, Test loss: 1.789, Test accuracy: 67.00
Round  75, Train loss: 1.796, Test loss: 1.788, Test accuracy: 67.06
Round  76, Train loss: 1.777, Test loss: 1.788, Test accuracy: 67.03
Round  77, Train loss: 1.694, Test loss: 1.787, Test accuracy: 67.13
Round  78, Train loss: 1.810, Test loss: 1.787, Test accuracy: 67.16
Round  79, Train loss: 1.752, Test loss: 1.784, Test accuracy: 67.47
Round  80, Train loss: 1.774, Test loss: 1.785, Test accuracy: 67.42
Round  81, Train loss: 1.755, Test loss: 1.782, Test accuracy: 67.68
Round  82, Train loss: 1.716, Test loss: 1.775, Test accuracy: 68.39
Round  83, Train loss: 1.787, Test loss: 1.773, Test accuracy: 68.59
Round  84, Train loss: 1.786, Test loss: 1.770, Test accuracy: 68.93
Round  85, Train loss: 1.804, Test loss: 1.769, Test accuracy: 69.07
Round  86, Train loss: 1.763, Test loss: 1.768, Test accuracy: 69.05
Round  87, Train loss: 1.772, Test loss: 1.766, Test accuracy: 69.38
Round  88, Train loss: 1.663, Test loss: 1.765, Test accuracy: 69.50
Round  89, Train loss: 1.722, Test loss: 1.764, Test accuracy: 69.58
Round  90, Train loss: 1.712, Test loss: 1.763, Test accuracy: 69.52
Round  91, Train loss: 1.813, Test loss: 1.763, Test accuracy: 69.58
Round  92, Train loss: 1.750, Test loss: 1.763, Test accuracy: 69.63
Round  93, Train loss: 1.679, Test loss: 1.763, Test accuracy: 69.54
Round  94, Train loss: 1.753, Test loss: 1.762, Test accuracy: 69.61
Round  95, Train loss: 1.662, Test loss: 1.762, Test accuracy: 69.61
Round  96, Train loss: 1.728, Test loss: 1.762, Test accuracy: 69.62
Round  97, Train loss: 1.696, Test loss: 1.759, Test accuracy: 70.03
Round  98, Train loss: 1.758, Test loss: 1.759, Test accuracy: 70.02/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.769, Test loss: 1.758, Test accuracy: 70.04
Final Round, Train loss: 1.736, Test loss: 1.755, Test accuracy: 70.40
Average accuracy final 10 rounds: 69.7188888888889 

2005.1992001533508
[1.7495274543762207, 3.4990549087524414, 5.1457836627960205, 6.7925124168396, 8.508836507797241, 10.225160598754883, 11.881489992141724, 13.537819385528564, 15.175763845443726, 16.813708305358887, 18.523821115493774, 20.233933925628662, 21.885016441345215, 23.536098957061768, 25.23034381866455, 26.924588680267334, 28.614607095718384, 30.304625511169434, 31.947171449661255, 33.589717388153076, 35.28760838508606, 36.98549938201904, 38.64709115028381, 40.308682918548584, 41.94399881362915, 43.57931470870972, 45.2782301902771, 46.97714567184448, 48.6943576335907, 50.411569595336914, 52.100030183792114, 53.788490772247314, 55.542877435684204, 57.297264099121094, 58.77155041694641, 60.24583673477173, 61.903977394104004, 63.56211805343628, 65.02871751785278, 66.49531698226929, 68.02135252952576, 69.54738807678223, 71.09105086326599, 72.63471364974976, 74.14933180809021, 75.66394996643066, 77.07873630523682, 78.49352264404297, 79.98600912094116, 81.47849559783936, 83.01462244987488, 84.5507493019104, 85.94069242477417, 87.33063554763794, 88.73993372917175, 90.14923191070557, 91.65404391288757, 93.15885591506958, 94.5818281173706, 96.00480031967163, 97.45072746276855, 98.89665460586548, 100.35667943954468, 101.81670427322388, 103.22691583633423, 104.63712739944458, 106.06062722206116, 107.48412704467773, 108.93420076370239, 110.38427448272705, 111.84076142311096, 113.29724836349487, 114.7320671081543, 116.16688585281372, 117.60261845588684, 119.03835105895996, 120.5434160232544, 122.04848098754883, 123.52537560462952, 125.0022702217102, 126.42350172996521, 127.84473323822021, 129.343763589859, 130.8427939414978, 132.34633564949036, 133.8498773574829, 135.26033425331116, 136.6707911491394, 138.18813061714172, 139.70547008514404, 141.20606017112732, 142.7066502571106, 144.14260578155518, 145.57856130599976, 147.00165629386902, 148.42475128173828, 149.92074155807495, 151.41673183441162, 152.8720006942749, 154.32726955413818, 155.96408605575562, 157.60090255737305, 159.29330468177795, 160.98570680618286, 162.70694637298584, 164.42818593978882, 166.07543420791626, 167.7226824760437, 169.40114426612854, 171.07960605621338, 172.73741388320923, 174.39522171020508, 176.06586146354675, 177.73650121688843, 179.4016604423523, 181.06681966781616, 182.73085045814514, 184.39488124847412, 186.0668613910675, 187.7388415336609, 189.450914144516, 191.1629867553711, 192.86918902397156, 194.57539129257202, 196.2175064086914, 197.8596215248108, 199.49626922607422, 201.13291692733765, 202.827538728714, 204.52216053009033, 206.19649362564087, 207.8708267211914, 209.50305104255676, 211.13527536392212, 212.8573660850525, 214.57945680618286, 216.30095791816711, 218.02245903015137, 219.68733716011047, 221.35221529006958, 222.96485233306885, 224.57748937606812, 226.31025075912476, 228.0430121421814, 229.75325083732605, 231.4634895324707, 233.1310691833496, 234.79864883422852, 236.42182111740112, 238.04499340057373, 239.72997045516968, 241.41494750976562, 243.07220482826233, 244.72946214675903, 246.32329058647156, 247.91711902618408, 249.5633099079132, 251.20950078964233, 252.8971962928772, 254.58489179611206, 256.2297012805939, 257.8745107650757, 259.4283649921417, 260.98221921920776, 262.62767481803894, 264.2731304168701, 265.9422254562378, 267.61132049560547, 269.2303743362427, 270.8494281768799, 272.4968967437744, 274.14436531066895, 275.62482047080994, 277.1052756309509, 278.53424763679504, 279.96321964263916, 281.4500825405121, 282.936945438385, 284.3986186981201, 285.8602919578552, 287.32367062568665, 288.78704929351807, 290.1774318218231, 291.5678143501282, 292.974347114563, 294.3808798789978, 295.8501477241516, 297.3194155693054, 298.8418393135071, 300.36426305770874, 301.7854402065277, 303.2066173553467, 304.67388010025024, 306.1411428451538, 307.65838980674744, 309.17563676834106, 310.5844638347626, 311.9932909011841, 313.478880405426, 314.96446990966797, 316.4809560775757, 317.9974422454834]
[10.222222222222221, 10.222222222222221, 11.5, 11.5, 12.705555555555556, 12.705555555555556, 17.455555555555556, 17.455555555555556, 23.344444444444445, 23.344444444444445, 27.944444444444443, 27.944444444444443, 35.672222222222224, 35.672222222222224, 39.65, 39.65, 43.34444444444444, 43.34444444444444, 45.01111111111111, 45.01111111111111, 47.30555555555556, 47.30555555555556, 50.144444444444446, 50.144444444444446, 51.02777777777778, 51.02777777777778, 51.894444444444446, 51.894444444444446, 52.78333333333333, 52.78333333333333, 54.57222222222222, 54.57222222222222, 56.06666666666667, 56.06666666666667, 56.85, 56.85, 57.233333333333334, 57.233333333333334, 57.56666666666667, 57.56666666666667, 58.24444444444445, 58.24444444444445, 58.37222222222222, 58.37222222222222, 58.51111111111111, 58.51111111111111, 58.94444444444444, 58.94444444444444, 60.07222222222222, 60.07222222222222, 60.2, 60.2, 60.32222222222222, 60.32222222222222, 60.96111111111111, 60.96111111111111, 61.077777777777776, 61.077777777777776, 61.22222222222222, 61.22222222222222, 61.67777777777778, 61.67777777777778, 62.044444444444444, 62.044444444444444, 62.03888888888889, 62.03888888888889, 61.916666666666664, 61.916666666666664, 62.72222222222222, 62.72222222222222, 62.72222222222222, 62.72222222222222, 62.96111111111111, 62.96111111111111, 63.51111111111111, 63.51111111111111, 63.644444444444446, 63.644444444444446, 63.75, 63.75, 63.77777777777778, 63.77777777777778, 63.827777777777776, 63.827777777777776, 63.855555555555554, 63.855555555555554, 63.833333333333336, 63.833333333333336, 63.84444444444444, 63.84444444444444, 64.06666666666666, 64.06666666666666, 64.08888888888889, 64.08888888888889, 64.19444444444444, 64.19444444444444, 64.19444444444444, 64.19444444444444, 64.23333333333333, 64.23333333333333, 64.28333333333333, 64.28333333333333, 64.47777777777777, 64.47777777777777, 64.49444444444444, 64.49444444444444, 64.53333333333333, 64.53333333333333, 64.74444444444444, 64.74444444444444, 64.90555555555555, 64.90555555555555, 65.4, 65.4, 65.36111111111111, 65.36111111111111, 65.50555555555556, 65.50555555555556, 65.84444444444445, 65.84444444444445, 65.77222222222223, 65.77222222222223, 65.91666666666667, 65.91666666666667, 65.81666666666666, 65.81666666666666, 65.83888888888889, 65.83888888888889, 66.0111111111111, 66.0111111111111, 66.43333333333334, 66.43333333333334, 66.44444444444444, 66.44444444444444, 66.36111111111111, 66.36111111111111, 66.45555555555555, 66.45555555555555, 66.90555555555555, 66.90555555555555, 67.02222222222223, 67.02222222222223, 67.05555555555556, 67.05555555555556, 67.05555555555556, 67.05555555555556, 67.05555555555556, 67.05555555555556, 67.0, 67.0, 67.06111111111112, 67.06111111111112, 67.02777777777777, 67.02777777777777, 67.13333333333334, 67.13333333333334, 67.15555555555555, 67.15555555555555, 67.46666666666667, 67.46666666666667, 67.42222222222222, 67.42222222222222, 67.68333333333334, 67.68333333333334, 68.38888888888889, 68.38888888888889, 68.59444444444445, 68.59444444444445, 68.92777777777778, 68.92777777777778, 69.06666666666666, 69.06666666666666, 69.05, 69.05, 69.38333333333334, 69.38333333333334, 69.5, 69.5, 69.58333333333333, 69.58333333333333, 69.52222222222223, 69.52222222222223, 69.58333333333333, 69.58333333333333, 69.62777777777778, 69.62777777777778, 69.54444444444445, 69.54444444444445, 69.60555555555555, 69.60555555555555, 69.60555555555555, 69.60555555555555, 69.61666666666666, 69.61666666666666, 70.02777777777777, 70.02777777777777, 70.01666666666667, 70.01666666666667, 70.03888888888889, 70.03888888888889, 70.4, 70.4]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.50
Round   1, Train loss: 2.300, Test loss: 2.301, Test accuracy: 18.48
Round   2, Train loss: 2.298, Test loss: 2.299, Test accuracy: 22.28
Round   3, Train loss: 2.293, Test loss: 2.296, Test accuracy: 25.73
Round   4, Train loss: 2.282, Test loss: 2.291, Test accuracy: 26.53
Round   5, Train loss: 2.261, Test loss: 2.271, Test accuracy: 26.02
Round   6, Train loss: 2.148, Test loss: 2.218, Test accuracy: 29.34
Round   7, Train loss: 2.122, Test loss: 2.170, Test accuracy: 34.12
Round   8, Train loss: 2.005, Test loss: 2.104, Test accuracy: 41.03
Round   9, Train loss: 2.007, Test loss: 2.035, Test accuracy: 46.46
Round  10, Train loss: 1.903, Test loss: 1.962, Test accuracy: 53.23
Round  11, Train loss: 1.798, Test loss: 1.904, Test accuracy: 58.81
Round  12, Train loss: 1.716, Test loss: 1.867, Test accuracy: 61.84
Round  13, Train loss: 1.718, Test loss: 1.829, Test accuracy: 65.42
Round  14, Train loss: 1.686, Test loss: 1.812, Test accuracy: 66.82
Round  15, Train loss: 1.629, Test loss: 1.797, Test accuracy: 68.07
Round  16, Train loss: 1.667, Test loss: 1.783, Test accuracy: 69.29
Round  17, Train loss: 1.656, Test loss: 1.773, Test accuracy: 70.16
Round  18, Train loss: 1.684, Test loss: 1.744, Test accuracy: 73.02
Round  19, Train loss: 1.605, Test loss: 1.739, Test accuracy: 73.28
Round  20, Train loss: 1.635, Test loss: 1.736, Test accuracy: 73.63
Round  21, Train loss: 1.615, Test loss: 1.729, Test accuracy: 74.20
Round  22, Train loss: 1.580, Test loss: 1.725, Test accuracy: 74.71
Round  23, Train loss: 1.589, Test loss: 1.721, Test accuracy: 75.03
Round  24, Train loss: 1.587, Test loss: 1.715, Test accuracy: 75.65
Round  25, Train loss: 1.543, Test loss: 1.701, Test accuracy: 77.40
Round  26, Train loss: 1.560, Test loss: 1.692, Test accuracy: 78.17
Round  27, Train loss: 1.580, Test loss: 1.682, Test accuracy: 79.27
Round  28, Train loss: 1.506, Test loss: 1.674, Test accuracy: 80.19
Round  29, Train loss: 1.551, Test loss: 1.666, Test accuracy: 80.61
Round  30, Train loss: 1.535, Test loss: 1.665, Test accuracy: 80.69
Round  31, Train loss: 1.512, Test loss: 1.657, Test accuracy: 81.57
Round  32, Train loss: 1.529, Test loss: 1.655, Test accuracy: 81.82
Round  33, Train loss: 1.503, Test loss: 1.653, Test accuracy: 81.94
Round  34, Train loss: 1.500, Test loss: 1.647, Test accuracy: 82.40
Round  35, Train loss: 1.489, Test loss: 1.645, Test accuracy: 82.58
Round  36, Train loss: 1.488, Test loss: 1.643, Test accuracy: 82.64
Round  37, Train loss: 1.484, Test loss: 1.642, Test accuracy: 82.69
Round  38, Train loss: 1.489, Test loss: 1.641, Test accuracy: 82.78
Round  39, Train loss: 1.501, Test loss: 1.640, Test accuracy: 82.86
Round  40, Train loss: 1.542, Test loss: 1.640, Test accuracy: 82.89
Round  41, Train loss: 1.540, Test loss: 1.639, Test accuracy: 82.94
Round  42, Train loss: 1.489, Test loss: 1.639, Test accuracy: 82.99
Round  43, Train loss: 1.520, Test loss: 1.638, Test accuracy: 82.99
Round  44, Train loss: 1.496, Test loss: 1.638, Test accuracy: 83.01
Round  45, Train loss: 1.485, Test loss: 1.637, Test accuracy: 83.03
Round  46, Train loss: 1.494, Test loss: 1.637, Test accuracy: 83.12
Round  47, Train loss: 1.493, Test loss: 1.636, Test accuracy: 83.15
Round  48, Train loss: 1.493, Test loss: 1.636, Test accuracy: 83.13
Round  49, Train loss: 1.500, Test loss: 1.636, Test accuracy: 83.13
Round  50, Train loss: 1.522, Test loss: 1.635, Test accuracy: 83.10
Round  51, Train loss: 1.550, Test loss: 1.631, Test accuracy: 83.49
Round  52, Train loss: 1.485, Test loss: 1.631, Test accuracy: 83.58
Round  53, Train loss: 1.493, Test loss: 1.631, Test accuracy: 83.54
Round  54, Train loss: 1.498, Test loss: 1.630, Test accuracy: 83.62
Round  55, Train loss: 1.495, Test loss: 1.630, Test accuracy: 83.60
Round  56, Train loss: 1.489, Test loss: 1.630, Test accuracy: 83.59
Round  57, Train loss: 1.485, Test loss: 1.630, Test accuracy: 83.59
Round  58, Train loss: 1.480, Test loss: 1.630, Test accuracy: 83.64
Round  59, Train loss: 1.483, Test loss: 1.630, Test accuracy: 83.62
Round  60, Train loss: 1.495, Test loss: 1.629, Test accuracy: 83.77
Round  61, Train loss: 1.496, Test loss: 1.629, Test accuracy: 83.69
Round  62, Train loss: 1.493, Test loss: 1.629, Test accuracy: 83.72
Round  63, Train loss: 1.494, Test loss: 1.629, Test accuracy: 83.76
Round  64, Train loss: 1.495, Test loss: 1.629, Test accuracy: 83.69
Round  65, Train loss: 1.480, Test loss: 1.629, Test accuracy: 83.74
Round  66, Train loss: 1.491, Test loss: 1.628, Test accuracy: 83.81
Round  67, Train loss: 1.481, Test loss: 1.628, Test accuracy: 83.78
Round  68, Train loss: 1.514, Test loss: 1.628, Test accuracy: 83.78
Round  69, Train loss: 1.480, Test loss: 1.628, Test accuracy: 83.88
Round  70, Train loss: 1.480, Test loss: 1.628, Test accuracy: 83.87
Round  71, Train loss: 1.510, Test loss: 1.628, Test accuracy: 83.84
Round  72, Train loss: 1.494, Test loss: 1.628, Test accuracy: 83.85
Round  73, Train loss: 1.493, Test loss: 1.628, Test accuracy: 83.86
Round  74, Train loss: 1.494, Test loss: 1.627, Test accuracy: 83.91
Round  75, Train loss: 1.482, Test loss: 1.627, Test accuracy: 83.88
Round  76, Train loss: 1.524, Test loss: 1.627, Test accuracy: 83.86
Round  77, Train loss: 1.493, Test loss: 1.627, Test accuracy: 83.87
Round  78, Train loss: 1.510, Test loss: 1.627, Test accuracy: 83.88
Round  79, Train loss: 1.511, Test loss: 1.627, Test accuracy: 83.87
Round  80, Train loss: 1.491, Test loss: 1.627, Test accuracy: 83.87
Round  81, Train loss: 1.489, Test loss: 1.627, Test accuracy: 83.88
Round  82, Train loss: 1.513, Test loss: 1.627, Test accuracy: 83.88
Round  83, Train loss: 1.493, Test loss: 1.627, Test accuracy: 83.86
Round  84, Train loss: 1.507, Test loss: 1.627, Test accuracy: 83.83
Round  85, Train loss: 1.497, Test loss: 1.627, Test accuracy: 83.80
Round  86, Train loss: 1.510, Test loss: 1.627, Test accuracy: 83.81
Round  87, Train loss: 1.483, Test loss: 1.627, Test accuracy: 83.80
Round  88, Train loss: 1.488, Test loss: 1.626, Test accuracy: 83.80
Round  89, Train loss: 1.480, Test loss: 1.626, Test accuracy: 83.84
Round  90, Train loss: 1.477, Test loss: 1.626, Test accuracy: 83.90
Round  91, Train loss: 1.494, Test loss: 1.626, Test accuracy: 83.85
Round  92, Train loss: 1.525, Test loss: 1.626, Test accuracy: 83.89
Round  93, Train loss: 1.518, Test loss: 1.626, Test accuracy: 83.81
Round  94, Train loss: 1.488, Test loss: 1.626, Test accuracy: 83.83
Round  95, Train loss: 1.488, Test loss: 1.626, Test accuracy: 83.84
Round  96, Train loss: 1.525, Test loss: 1.626, Test accuracy: 83.87
Round  97, Train loss: 1.480, Test loss: 1.626, Test accuracy: 83.87
Round  98, Train loss: 1.479, Test loss: 1.626, Test accuracy: 83.86
Round  99, Train loss: 1.509, Test loss: 1.626, Test accuracy: 83.88/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.496, Test loss: 1.625, Test accuracy: 83.87
Average accuracy final 10 rounds: 83.85916666666668 

1323.5252902507782
[1.2428152561187744, 2.485630512237549, 3.674614667892456, 4.863598823547363, 5.951647758483887, 7.03969669342041, 8.031782150268555, 9.0238676071167, 9.950754880905151, 10.877642154693604, 11.871899127960205, 12.866156101226807, 13.814558267593384, 14.762960433959961, 15.704482078552246, 16.64600372314453, 17.65276575088501, 18.65952777862549, 19.635381937026978, 20.611236095428467, 21.561065435409546, 22.510894775390625, 23.44971227645874, 24.388529777526855, 25.37814998626709, 26.367770195007324, 27.282697916030884, 28.197625637054443, 29.24933099746704, 30.30103635787964, 31.26936388015747, 32.2376914024353, 33.24158763885498, 34.24548387527466, 35.23578882217407, 36.226093769073486, 37.2728796005249, 38.31966543197632, 39.26609659194946, 40.21252775192261, 41.19581198692322, 42.17909622192383, 43.202001094818115, 44.2249059677124, 45.227733850479126, 46.23056173324585, 47.236327171325684, 48.24209260940552, 49.21373534202576, 50.185378074645996, 51.19362831115723, 52.20187854766846, 53.15974473953247, 54.117610931396484, 55.13282585144043, 56.148040771484375, 57.10110306739807, 58.05416536331177, 59.07205557823181, 60.089945793151855, 61.09996271133423, 62.1099796295166, 63.1006076335907, 64.0912356376648, 65.07982540130615, 66.06841516494751, 67.05407166481018, 68.03972816467285, 69.03331351280212, 70.0268988609314, 71.04307126998901, 72.05924367904663, 73.02909803390503, 73.99895238876343, 74.96218848228455, 75.92542457580566, 76.91726756095886, 77.90911054611206, 78.91689705848694, 79.92468357086182, 80.85902976989746, 81.7933759689331, 82.76002717018127, 83.72667837142944, 84.71797108650208, 85.7092638015747, 86.67402720451355, 87.63879060745239, 88.64473795890808, 89.65068531036377, 90.59057283401489, 91.53046035766602, 92.50526475906372, 93.48006916046143, 94.43105292320251, 95.3820366859436, 96.34675788879395, 97.31147909164429, 98.27467060089111, 99.23786211013794, 100.18422198295593, 101.13058185577393, 102.15475153923035, 103.17892122268677, 104.1482446193695, 105.11756801605225, 106.11125659942627, 107.1049451828003, 108.11101460456848, 109.11708402633667, 110.11308598518372, 111.10908794403076, 112.0298011302948, 112.95051431655884, 113.9792070388794, 115.00789976119995, 115.99346256256104, 116.97902536392212, 117.94618582725525, 118.91334629058838, 120.00433492660522, 121.09532356262207, 122.18656325340271, 123.27780294418335, 124.36713767051697, 125.45647239685059, 126.55319476127625, 127.6499171257019, 128.72283387184143, 129.79575061798096, 130.85472345352173, 131.9136962890625, 133.00045323371887, 134.08721017837524, 135.1576371192932, 136.22806406021118, 137.31336212158203, 138.39866018295288, 139.4808897972107, 140.5631194114685, 141.5876269340515, 142.61213445663452, 143.69178438186646, 144.7714343070984, 145.7197618484497, 146.66808938980103, 147.6565203666687, 148.64495134353638, 149.62870383262634, 150.6124563217163, 151.5716917514801, 152.5309271812439, 153.52763891220093, 154.52435064315796, 155.52093720436096, 156.51752376556396, 157.45296216011047, 158.38840055465698, 159.36808061599731, 160.34776067733765, 161.32506561279297, 162.3023705482483, 163.25132608413696, 164.20028162002563, 165.1948254108429, 166.18936920166016, 167.1984887123108, 168.20760822296143, 169.131591796875, 170.05557537078857, 171.05552697181702, 172.05547857284546, 173.0646688938141, 174.07385921478271, 175.01452374458313, 175.95518827438354, 176.93616223335266, 177.91713619232178, 178.88964939117432, 179.86216259002686, 180.8451657295227, 181.82816886901855, 182.82690024375916, 183.82563161849976, 184.79740285873413, 185.7691740989685, 186.75147414207458, 187.73377418518066, 188.69042420387268, 189.6470742225647, 190.62420868873596, 191.60134315490723, 192.54062914848328, 193.47991514205933, 194.48062109947205, 195.48132705688477, 196.51539015769958, 197.5494532585144, 198.5161440372467, 199.482834815979, 201.1753568649292, 202.8678789138794]
[14.5, 14.5, 18.483333333333334, 18.483333333333334, 22.283333333333335, 22.283333333333335, 25.733333333333334, 25.733333333333334, 26.533333333333335, 26.533333333333335, 26.025, 26.025, 29.341666666666665, 29.341666666666665, 34.125, 34.125, 41.03333333333333, 41.03333333333333, 46.458333333333336, 46.458333333333336, 53.233333333333334, 53.233333333333334, 58.80833333333333, 58.80833333333333, 61.84166666666667, 61.84166666666667, 65.425, 65.425, 66.81666666666666, 66.81666666666666, 68.06666666666666, 68.06666666666666, 69.29166666666667, 69.29166666666667, 70.15833333333333, 70.15833333333333, 73.01666666666667, 73.01666666666667, 73.275, 73.275, 73.63333333333334, 73.63333333333334, 74.2, 74.2, 74.70833333333333, 74.70833333333333, 75.03333333333333, 75.03333333333333, 75.65, 75.65, 77.4, 77.4, 78.175, 78.175, 79.26666666666667, 79.26666666666667, 80.19166666666666, 80.19166666666666, 80.60833333333333, 80.60833333333333, 80.69166666666666, 80.69166666666666, 81.56666666666666, 81.56666666666666, 81.81666666666666, 81.81666666666666, 81.94166666666666, 81.94166666666666, 82.4, 82.4, 82.575, 82.575, 82.64166666666667, 82.64166666666667, 82.69166666666666, 82.69166666666666, 82.775, 82.775, 82.85833333333333, 82.85833333333333, 82.89166666666667, 82.89166666666667, 82.94166666666666, 82.94166666666666, 82.99166666666666, 82.99166666666666, 82.99166666666666, 82.99166666666666, 83.00833333333334, 83.00833333333334, 83.025, 83.025, 83.11666666666666, 83.11666666666666, 83.15, 83.15, 83.13333333333334, 83.13333333333334, 83.13333333333334, 83.13333333333334, 83.1, 83.1, 83.49166666666666, 83.49166666666666, 83.575, 83.575, 83.54166666666667, 83.54166666666667, 83.61666666666666, 83.61666666666666, 83.6, 83.6, 83.59166666666667, 83.59166666666667, 83.59166666666667, 83.59166666666667, 83.64166666666667, 83.64166666666667, 83.625, 83.625, 83.76666666666667, 83.76666666666667, 83.69166666666666, 83.69166666666666, 83.725, 83.725, 83.75833333333334, 83.75833333333334, 83.69166666666666, 83.69166666666666, 83.74166666666666, 83.74166666666666, 83.80833333333334, 83.80833333333334, 83.78333333333333, 83.78333333333333, 83.78333333333333, 83.78333333333333, 83.875, 83.875, 83.86666666666666, 83.86666666666666, 83.84166666666667, 83.84166666666667, 83.85, 83.85, 83.85833333333333, 83.85833333333333, 83.90833333333333, 83.90833333333333, 83.875, 83.875, 83.85833333333333, 83.85833333333333, 83.86666666666666, 83.86666666666666, 83.875, 83.875, 83.86666666666666, 83.86666666666666, 83.86666666666666, 83.86666666666666, 83.875, 83.875, 83.875, 83.875, 83.85833333333333, 83.85833333333333, 83.83333333333333, 83.83333333333333, 83.8, 83.8, 83.80833333333334, 83.80833333333334, 83.8, 83.8, 83.8, 83.8, 83.84166666666667, 83.84166666666667, 83.9, 83.9, 83.85, 83.85, 83.89166666666667, 83.89166666666667, 83.80833333333334, 83.80833333333334, 83.825, 83.825, 83.84166666666667, 83.84166666666667, 83.86666666666666, 83.86666666666666, 83.86666666666666, 83.86666666666666, 83.85833333333333, 83.85833333333333, 83.88333333333334, 83.88333333333334, 83.86666666666666, 83.86666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.706, Test loss: 2.299, Test accuracy: 15.86
Round   1, Train loss: 1.651, Test loss: 2.288, Test accuracy: 25.18
Round   2, Train loss: 1.567, Test loss: 2.266, Test accuracy: 34.78
Round   3, Train loss: 1.484, Test loss: 2.236, Test accuracy: 39.83
Round   4, Train loss: 1.473, Test loss: 2.201, Test accuracy: 44.77
Round   5, Train loss: 1.437, Test loss: 2.162, Test accuracy: 49.69
Round   6, Train loss: 1.416, Test loss: 2.130, Test accuracy: 52.59
Round   7, Train loss: 1.398, Test loss: 2.100, Test accuracy: 52.94
Round   8, Train loss: 1.337, Test loss: 2.086, Test accuracy: 52.53
Round   9, Train loss: 1.363, Test loss: 2.073, Test accuracy: 52.93
Round  10, Train loss: 1.333, Test loss: 2.062, Test accuracy: 52.41
Round  11, Train loss: 1.371, Test loss: 2.054, Test accuracy: 51.68
Round  12, Train loss: 1.374, Test loss: 2.043, Test accuracy: 52.32
Round  13, Train loss: 1.367, Test loss: 2.035, Test accuracy: 52.18
Round  14, Train loss: 1.391, Test loss: 2.031, Test accuracy: 52.42
Round  15, Train loss: 1.408, Test loss: 2.023, Test accuracy: 52.52
Round  16, Train loss: 1.303, Test loss: 2.020, Test accuracy: 52.63
Round  17, Train loss: 1.327, Test loss: 2.014, Test accuracy: 53.31
Round  18, Train loss: 1.396, Test loss: 2.007, Test accuracy: 53.76
Round  19, Train loss: 1.335, Test loss: 2.006, Test accuracy: 53.49
Round  20, Train loss: 1.356, Test loss: 2.003, Test accuracy: 53.83
Round  21, Train loss: 1.330, Test loss: 2.001, Test accuracy: 53.78
Round  22, Train loss: 1.406, Test loss: 1.998, Test accuracy: 53.58
Round  23, Train loss: 1.298, Test loss: 1.996, Test accuracy: 53.61
Round  24, Train loss: 1.323, Test loss: 1.992, Test accuracy: 53.67
Round  25, Train loss: 1.323, Test loss: 1.991, Test accuracy: 53.73
Round  26, Train loss: 1.376, Test loss: 1.989, Test accuracy: 53.68
Round  27, Train loss: 1.276, Test loss: 1.990, Test accuracy: 53.40
Round  28, Train loss: 1.329, Test loss: 1.990, Test accuracy: 53.21
Round  29, Train loss: 1.337, Test loss: 1.991, Test accuracy: 53.23
Round  30, Train loss: 1.337, Test loss: 1.993, Test accuracy: 52.46
Round  31, Train loss: 1.274, Test loss: 1.994, Test accuracy: 52.47
Round  32, Train loss: 1.318, Test loss: 1.993, Test accuracy: 52.23
Round  33, Train loss: 1.327, Test loss: 1.990, Test accuracy: 52.32
Round  34, Train loss: 1.371, Test loss: 1.989, Test accuracy: 52.09
Round  35, Train loss: 1.330, Test loss: 1.987, Test accuracy: 52.18
Round  36, Train loss: 1.306, Test loss: 1.987, Test accuracy: 51.99
Round  37, Train loss: 1.352, Test loss: 1.987, Test accuracy: 51.97
Round  38, Train loss: 1.293, Test loss: 1.986, Test accuracy: 52.06
Round  39, Train loss: 1.306, Test loss: 1.986, Test accuracy: 52.06
Round  40, Train loss: 1.278, Test loss: 1.988, Test accuracy: 51.57
Round  41, Train loss: 1.349, Test loss: 1.988, Test accuracy: 51.68
Round  42, Train loss: 1.381, Test loss: 1.988, Test accuracy: 51.52
Round  43, Train loss: 1.296, Test loss: 1.989, Test accuracy: 51.23
Round  44, Train loss: 1.316, Test loss: 1.989, Test accuracy: 51.10
Round  45, Train loss: 1.321, Test loss: 1.990, Test accuracy: 51.11
Round  46, Train loss: 1.358, Test loss: 1.990, Test accuracy: 50.89
Round  47, Train loss: 1.305, Test loss: 1.988, Test accuracy: 51.08
Round  48, Train loss: 1.301, Test loss: 1.989, Test accuracy: 50.85
Round  49, Train loss: 1.305, Test loss: 1.990, Test accuracy: 50.67
Round  50, Train loss: 1.307, Test loss: 1.985, Test accuracy: 50.97
Round  51, Train loss: 1.310, Test loss: 1.981, Test accuracy: 51.62
Round  52, Train loss: 1.252, Test loss: 1.979, Test accuracy: 51.69
Round  53, Train loss: 1.324, Test loss: 1.986, Test accuracy: 50.54
Round  54, Train loss: 1.304, Test loss: 1.988, Test accuracy: 50.27
Round  55, Train loss: 1.289, Test loss: 1.986, Test accuracy: 50.43
Round  56, Train loss: 1.290, Test loss: 1.986, Test accuracy: 50.33
Round  57, Train loss: 1.304, Test loss: 1.983, Test accuracy: 50.86
Round  58, Train loss: 1.326, Test loss: 1.984, Test accuracy: 50.76
Round  59, Train loss: 1.307, Test loss: 1.982, Test accuracy: 50.79
Round  60, Train loss: 1.280, Test loss: 1.986, Test accuracy: 49.98
Round  61, Train loss: 1.316, Test loss: 1.986, Test accuracy: 49.79
Round  62, Train loss: 1.242, Test loss: 1.988, Test accuracy: 49.46
Round  63, Train loss: 1.282, Test loss: 1.987, Test accuracy: 49.77
Round  64, Train loss: 1.314, Test loss: 1.987, Test accuracy: 49.83
Round  65, Train loss: 1.214, Test loss: 1.983, Test accuracy: 50.42
Round  66, Train loss: 1.267, Test loss: 1.988, Test accuracy: 49.95
Round  67, Train loss: 1.338, Test loss: 1.987, Test accuracy: 50.00
Round  68, Train loss: 1.262, Test loss: 1.989, Test accuracy: 49.92
Round  69, Train loss: 1.352, Test loss: 1.992, Test accuracy: 49.55
Round  70, Train loss: 1.309, Test loss: 1.994, Test accuracy: 49.39
Round  71, Train loss: 1.259, Test loss: 1.992, Test accuracy: 49.53
Round  72, Train loss: 1.270, Test loss: 1.992, Test accuracy: 49.63
Round  73, Train loss: 1.275, Test loss: 1.986, Test accuracy: 50.47
Round  74, Train loss: 1.299, Test loss: 1.987, Test accuracy: 50.26
Round  75, Train loss: 1.305, Test loss: 1.991, Test accuracy: 49.62
Round  76, Train loss: 1.287, Test loss: 1.990, Test accuracy: 49.61
Round  77, Train loss: 1.265, Test loss: 1.995, Test accuracy: 48.86
Round  78, Train loss: 1.312, Test loss: 1.991, Test accuracy: 49.52
Round  79, Train loss: 1.257, Test loss: 1.989, Test accuracy: 49.54
Round  80, Train loss: 1.259, Test loss: 1.996, Test accuracy: 48.43
Round  81, Train loss: 1.251, Test loss: 1.991, Test accuracy: 49.15
Round  82, Train loss: 1.314, Test loss: 1.994, Test accuracy: 48.98
Round  83, Train loss: 1.305, Test loss: 1.994, Test accuracy: 48.71
Round  84, Train loss: 1.283, Test loss: 1.996, Test accuracy: 48.53
Round  85, Train loss: 1.298, Test loss: 2.000, Test accuracy: 48.03
Round  86, Train loss: 1.288, Test loss: 2.000, Test accuracy: 48.08
Round  87, Train loss: 1.304, Test loss: 1.998, Test accuracy: 48.23
Round  88, Train loss: 1.276, Test loss: 2.000, Test accuracy: 47.80
Round  89, Train loss: 1.259, Test loss: 2.002, Test accuracy: 47.59
Round  90, Train loss: 1.273, Test loss: 2.001, Test accuracy: 47.77
Round  91, Train loss: 1.292, Test loss: 2.005, Test accuracy: 47.35
Round  92, Train loss: 1.310, Test loss: 2.007, Test accuracy: 47.08
Round  93, Train loss: 1.277, Test loss: 2.004, Test accuracy: 47.34
Round  94, Train loss: 1.332, Test loss: 2.004, Test accuracy: 47.42
Round  95, Train loss: 1.297, Test loss: 2.003, Test accuracy: 47.53
Round  96, Train loss: 1.275, Test loss: 2.005, Test accuracy: 47.47
Round  97, Train loss: 1.275, Test loss: 2.006, Test accuracy: 47.37
Round  98, Train loss: 1.215, Test loss: 2.005, Test accuracy: 47.31
Round  99, Train loss: 1.241, Test loss: 2.005, Test accuracy: 47.39
Final Round, Train loss: 1.286, Test loss: 2.011, Test accuracy: 46.43
Average accuracy final 10 rounds: 47.401666666666664
1542.4355726242065
[]
[15.858333333333333, 25.183333333333334, 34.78333333333333, 39.833333333333336, 44.766666666666666, 49.69166666666667, 52.59166666666667, 52.94166666666667, 52.53333333333333, 52.93333333333333, 52.40833333333333, 51.68333333333333, 52.31666666666667, 52.18333333333333, 52.416666666666664, 52.525, 52.63333333333333, 53.30833333333333, 53.75833333333333, 53.49166666666667, 53.825, 53.78333333333333, 53.583333333333336, 53.608333333333334, 53.675, 53.725, 53.68333333333333, 53.4, 53.208333333333336, 53.225, 52.458333333333336, 52.46666666666667, 52.233333333333334, 52.31666666666667, 52.09166666666667, 52.18333333333333, 51.99166666666667, 51.96666666666667, 52.05833333333333, 52.05833333333333, 51.56666666666667, 51.68333333333333, 51.525, 51.233333333333334, 51.1, 51.108333333333334, 50.891666666666666, 51.075, 50.85, 50.666666666666664, 50.96666666666667, 51.61666666666667, 51.69166666666667, 50.541666666666664, 50.275, 50.43333333333333, 50.333333333333336, 50.858333333333334, 50.75833333333333, 50.791666666666664, 49.983333333333334, 49.791666666666664, 49.458333333333336, 49.766666666666666, 49.825, 50.425, 49.95, 50.0, 49.916666666666664, 49.55, 49.391666666666666, 49.53333333333333, 49.63333333333333, 50.46666666666667, 50.25833333333333, 49.625, 49.608333333333334, 48.858333333333334, 49.525, 49.541666666666664, 48.43333333333333, 49.15, 48.983333333333334, 48.708333333333336, 48.53333333333333, 48.03333333333333, 48.075, 48.233333333333334, 47.8, 47.59166666666667, 47.766666666666666, 47.35, 47.075, 47.34166666666667, 47.416666666666664, 47.53333333333333, 47.46666666666667, 47.36666666666667, 47.30833333333333, 47.391666666666666, 46.43333333333333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.287, Test loss: 2.281, Test accuracy: 16.81
Round   1, Train loss: 2.246, Test loss: 2.252, Test accuracy: 19.09
Round   2, Train loss: 2.197, Test loss: 2.185, Test accuracy: 27.29
Round   3, Train loss: 2.019, Test loss: 2.121, Test accuracy: 34.70
Round   4, Train loss: 1.985, Test loss: 2.099, Test accuracy: 37.36
Round   5, Train loss: 1.940, Test loss: 2.082, Test accuracy: 37.02
Round   6, Train loss: 1.696, Test loss: 2.095, Test accuracy: 36.31
Round   7, Train loss: 1.724, Test loss: 1.975, Test accuracy: 50.12
Round   8, Train loss: 1.314, Test loss: 1.970, Test accuracy: 50.58
Round   9, Train loss: 1.660, Test loss: 2.026, Test accuracy: 44.93
Round  10, Train loss: 1.067, Test loss: 1.948, Test accuracy: 53.20
Round  11, Train loss: 0.980, Test loss: 1.930, Test accuracy: 54.15
Round  12, Train loss: 1.365, Test loss: 1.926, Test accuracy: 54.48
Round  13, Train loss: 0.910, Test loss: 1.867, Test accuracy: 61.09
Round  14, Train loss: 0.308, Test loss: 1.827, Test accuracy: 64.56
Round  15, Train loss: 0.731, Test loss: 1.801, Test accuracy: 66.94
Round  16, Train loss: 0.534, Test loss: 1.790, Test accuracy: 67.81
Round  17, Train loss: 0.742, Test loss: 1.800, Test accuracy: 66.83
Round  18, Train loss: 0.589, Test loss: 1.779, Test accuracy: 69.06
Round  19, Train loss: 0.228, Test loss: 1.776, Test accuracy: 69.06
Round  20, Train loss: 0.272, Test loss: 1.769, Test accuracy: 69.48
Round  21, Train loss: -0.215, Test loss: 1.748, Test accuracy: 71.31
Round  22, Train loss: -0.850, Test loss: 1.728, Test accuracy: 73.33
Round  23, Train loss: -0.643, Test loss: 1.726, Test accuracy: 73.55
Round  24, Train loss: 0.251, Test loss: 1.723, Test accuracy: 73.86
Round  25, Train loss: -0.021, Test loss: 1.732, Test accuracy: 72.90
Round  26, Train loss: 0.169, Test loss: 1.724, Test accuracy: 73.71
Round  27, Train loss: -0.161, Test loss: 1.731, Test accuracy: 72.94
Round  28, Train loss: -0.904, Test loss: 1.736, Test accuracy: 72.50
Round  29, Train loss: -0.848, Test loss: 1.738, Test accuracy: 72.27
Round  30, Train loss: -0.936, Test loss: 1.733, Test accuracy: 72.77
Round  31, Train loss: -0.833, Test loss: 1.702, Test accuracy: 75.85
Round  32, Train loss: -0.658, Test loss: 1.721, Test accuracy: 73.88
Round  33, Train loss: -1.156, Test loss: 1.717, Test accuracy: 74.37
Round  34, Train loss: -0.759, Test loss: 1.700, Test accuracy: 76.03
Round  35, Train loss: -0.705, Test loss: 1.698, Test accuracy: 76.22
Round  36, Train loss: -1.235, Test loss: 1.689, Test accuracy: 77.09
Round  37, Train loss: -1.561, Test loss: 1.677, Test accuracy: 78.29
Round  38, Train loss: -0.490, Test loss: 1.712, Test accuracy: 74.84
Round  39, Train loss: -0.764, Test loss: 1.704, Test accuracy: 75.61
Round  40, Train loss: -1.165, Test loss: 1.697, Test accuracy: 76.30
Round  41, Train loss: -1.769, Test loss: 1.697, Test accuracy: 76.34
Round  42, Train loss: -1.127, Test loss: 1.695, Test accuracy: 76.53
Round  43, Train loss: -1.079, Test loss: 1.711, Test accuracy: 74.97
Round  44, Train loss: -1.623, Test loss: 1.706, Test accuracy: 75.44
Round  45, Train loss: -0.804, Test loss: 1.684, Test accuracy: 77.71
Round  46, Train loss: -1.649, Test loss: 1.655, Test accuracy: 80.64
Round  47, Train loss: -1.074, Test loss: 1.681, Test accuracy: 77.94
Round  48, Train loss: -1.148, Test loss: 1.676, Test accuracy: 78.45
Round  49, Train loss: -1.230, Test loss: 1.704, Test accuracy: 75.66
Round  50, Train loss: -0.973, Test loss: 1.683, Test accuracy: 77.77
Round  51, Train loss: -1.632, Test loss: 1.679, Test accuracy: 78.17
Round  52, Train loss: -1.086, Test loss: 1.650, Test accuracy: 81.04
Round  53, Train loss: -0.631, Test loss: 1.684, Test accuracy: 77.67
Round  54, Train loss: -0.847, Test loss: 1.685, Test accuracy: 77.54
Round  55, Train loss: -1.455, Test loss: 1.696, Test accuracy: 76.53
Round  56, Train loss: -1.970, Test loss: 1.688, Test accuracy: 77.28
Round  57, Train loss: -1.101, Test loss: 1.683, Test accuracy: 77.78
Round  58, Train loss: -1.407, Test loss: 1.675, Test accuracy: 78.58
Round  59, Train loss: -1.223, Test loss: 1.663, Test accuracy: 79.76
Round  60, Train loss: -1.198, Test loss: 1.663, Test accuracy: 79.74
Round  61, Train loss: -1.425, Test loss: 1.633, Test accuracy: 82.80
Round  62, Train loss: -1.685, Test loss: 1.655, Test accuracy: 80.62
Round  63, Train loss: -1.435, Test loss: 1.659, Test accuracy: 80.20
Round  64, Train loss: -1.288, Test loss: 1.663, Test accuracy: 79.78
Round  65, Train loss: -1.374, Test loss: 1.685, Test accuracy: 77.55
Round  66, Train loss: -1.338, Test loss: 1.676, Test accuracy: 78.42
Round  67, Train loss: -1.260, Test loss: 1.678, Test accuracy: 78.26
Round  68, Train loss: -1.465, Test loss: 1.680, Test accuracy: 78.03
Round  69, Train loss: -1.574, Test loss: 1.676, Test accuracy: 78.37
Round  70, Train loss: -1.034, Test loss: 1.652, Test accuracy: 80.86
Round  71, Train loss: -1.518, Test loss: 1.664, Test accuracy: 79.69
Round  72, Train loss: -1.460, Test loss: 1.650, Test accuracy: 81.13
Round  73, Train loss: -1.042, Test loss: 1.650, Test accuracy: 81.10
Round  74, Train loss: -1.485, Test loss: 1.648, Test accuracy: 81.32
Round  75, Train loss: -1.495, Test loss: 1.647, Test accuracy: 81.36
Round  76, Train loss: -1.797, Test loss: 1.617, Test accuracy: 84.34
Round  77, Train loss: -1.419, Test loss: 1.666, Test accuracy: 79.40
Round  78, Train loss: -1.628, Test loss: 1.637, Test accuracy: 82.34
Round  79, Train loss: -1.449, Test loss: 1.658, Test accuracy: 80.19
Round  80, Train loss: -1.575, Test loss: 1.654, Test accuracy: 80.63
Round  81, Train loss: -1.596, Test loss: 1.644, Test accuracy: 81.67
Round  82, Train loss: -1.309, Test loss: 1.663, Test accuracy: 79.78
Round  83, Train loss: -1.776, Test loss: 1.666, Test accuracy: 79.52
Round  84, Train loss: -1.923, Test loss: 1.649, Test accuracy: 81.20
Round  85, Train loss: -1.148, Test loss: 1.645, Test accuracy: 81.53
Round  86, Train loss: -1.253, Test loss: 1.615, Test accuracy: 84.56
Round  87, Train loss: -1.282, Test loss: 1.643, Test accuracy: 81.70
Round  88, Train loss: -1.363, Test loss: 1.634, Test accuracy: 82.61
Round  89, Train loss: -1.211, Test loss: 1.606, Test accuracy: 85.48
Round  90, Train loss: -1.340, Test loss: 1.629, Test accuracy: 83.14
Round  91, Train loss: -1.153, Test loss: 1.664, Test accuracy: 79.62
Round  92, Train loss: -1.607, Test loss: 1.635, Test accuracy: 82.51
Round  93, Train loss: -1.275, Test loss: 1.626, Test accuracy: 83.40
Round  94, Train loss: -1.270, Test loss: 1.611, Test accuracy: 84.95
Round  95, Train loss: -1.273, Test loss: 1.641, Test accuracy: 81.95
Round  96, Train loss: -1.521, Test loss: 1.642, Test accuracy: 81.91
Round  97, Train loss: -1.454, Test loss: 1.642, Test accuracy: 81.90
Round  98, Train loss: -1.513, Test loss: 1.619, Test accuracy: 84.15
Round  99, Train loss: -1.466, Test loss: 1.633, Test accuracy: 82.74
Final Round, Train loss: 1.767, Test loss: 1.730, Test accuracy: 73.33
Average accuracy final 10 rounds: 82.62700000000001
Average global accuracy final 10 rounds: 82.62700000000001
3894.9198553562164
[]
[16.8075, 19.0925, 27.2875, 34.7025, 37.3625, 37.025, 36.3125, 50.1175, 50.5825, 44.9325, 53.2, 54.1525, 54.475, 61.095, 64.565, 66.9375, 67.81, 66.835, 69.055, 69.065, 69.4775, 71.3125, 73.33, 73.5525, 73.8625, 72.9025, 73.71, 72.9425, 72.4975, 72.27, 72.7675, 75.85, 73.875, 74.37, 76.0275, 76.22, 77.0925, 78.29, 74.84, 75.61, 76.2975, 76.345, 76.5275, 74.975, 75.4425, 77.7125, 80.64, 77.935, 78.455, 75.655, 77.765, 78.165, 81.0375, 77.6725, 77.54, 76.53, 77.2775, 77.775, 78.575, 79.76, 79.7375, 82.795, 80.6225, 80.2025, 79.775, 77.5525, 78.4175, 78.26, 78.035, 78.3675, 80.86, 79.6925, 81.1275, 81.0975, 81.3225, 81.355, 84.34, 79.4025, 82.34, 80.19, 80.63, 81.675, 79.7825, 79.5225, 81.2025, 81.525, 84.565, 81.7, 82.615, 85.4825, 83.1375, 79.6225, 82.5125, 83.4025, 84.95, 81.95, 81.9075, 81.8975, 84.1525, 82.7375, 73.3275]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.71
Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.66
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.71
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.67
Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.72
Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.66
Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.73
Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.66
Round   4, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.70
Round   4, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.64
Round   5, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.70
Round   5, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.67
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.66
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.66
Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.67
Round   7, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.67
Round   8, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.69
Round   8, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.67
Round   9, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.70
Round   9, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.67
Round  10, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.72
Round  10, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.66
Round  11, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.71
Round  11, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.69
Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.68
Round  12, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.69
Round  13, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.72
Round  13, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.71
Round  14, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.76
Round  14, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.71
Round  15, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.77
Round  15, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.73
Round  16, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  16, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.74
Round  17, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.78
Round  17, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  18, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.78
Round  18, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.70
Round  19, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  19, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.73
Round  20, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.78
Round  20, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  21, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  21, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  22, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.81
Round  22, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  23, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.81
Round  23, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  24, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  24, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  25, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.81
Round  25, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  26, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.80
Round  26, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.74
Round  27, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  27, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  28, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  28, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  29, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  29, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  30, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  30, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  31, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  31, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  32, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  32, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  33, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.82
Round  33, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.76
Round  34, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  34, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  35, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.82
Round  35, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  36, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.82
Round  36, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  37, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.82
Round  37, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  38, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  38, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  39, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  39, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  40, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  40, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  41, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  41, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  42, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.85
Round  42, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.76
Round  43, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  43, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  44, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.85
Round  44, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.76
Round  45, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  45, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  46, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  46, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  47, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  47, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  48, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  48, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  49, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  49, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  50, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  50, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  51, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.80
Round  51, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.74
Round  52, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  52, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  53, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  53, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  54, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  54, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.73
Round  55, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  55, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.75
Round  56, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  56, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  57, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  57, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  58, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  58, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  59, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  59, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  60, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  60, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  61, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  61, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  62, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  62, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  63, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.82
Round  63, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.81
Round  64, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  64, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  65, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  65, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.80
Round  66, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  66, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.81
Round  67, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.87
Round  67, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.81
Round  68, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  68, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.81
Round  69, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  69, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.81
Round  70, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.86
Round  70, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.80
Round  71, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  71, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.79
Round  72, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  72, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  73, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  73, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  74, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  74, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  75, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  75, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.76
Round  76, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.79
Round  76, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  77, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.82
Round  77, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.74
Round  78, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  78, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  79, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.81
Round  79, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  80, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.81
Round  80, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.77
Round  81, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  81, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  82, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.81
Round  82, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  83, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.82
Round  83, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.79
Round  84, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.83
Round  84, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.77
Round  85, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.84
Round  85, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.78
Round  86, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.83
Round  86, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.79
Round  87, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.80
Round  87, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.77
Round  88, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.80
Round  88, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.78
Round  89, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  89, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.82
Round  90, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.88
Round  90, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  91, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.88
Round  91, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  92, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  92, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  93, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  93, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.82
Round  94, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  94, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  95, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.86
Round  95, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  96, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.85
Round  96, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  97, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.87
Round  97, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.84/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.88
Round  98, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.83
Round  99, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.87
Round  99, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.84
Final Round, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.94
Final Round, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.84
Average accuracy final 10 rounds: 9.863750000000001 

Average global accuracy final 10 rounds: 9.8305 

4465.310877323151
[3.7355759143829346, 7.1133873462677, 10.592016220092773, 14.038311243057251, 17.601195573806763, 21.21653151512146, 24.701862573623657, 28.08051586151123, 31.573898315429688, 35.07039546966553, 38.596328020095825, 42.06899642944336, 45.652220010757446, 49.15260100364685, 52.70014953613281, 56.25193667411804, 59.79173040390015, 63.20315146446228, 66.86389923095703, 70.57476425170898, 74.28189754486084, 77.87626218795776, 81.42658853530884, 84.87735271453857, 88.42195105552673, 92.1154215335846, 95.74990916252136, 99.42687392234802, 102.90167140960693, 106.44490504264832, 110.0309431552887, 113.5478286743164, 117.10482573509216, 120.65081596374512, 124.21660995483398, 127.7170205116272, 131.34080576896667, 135.42671179771423, 139.48385906219482, 143.4028513431549, 147.39509105682373, 151.37320923805237, 155.3565480709076, 159.1502480506897, 162.6561884880066, 166.5829746723175, 170.51562452316284, 174.52663946151733, 178.52798295021057, 182.47801232337952, 186.47666335105896, 190.433251619339, 193.98694276809692, 197.45097613334656, 200.9965717792511, 205.0062358379364, 209.00410652160645, 212.99027395248413, 216.7874834537506, 220.2811279296875, 223.72194170951843, 227.30285024642944, 230.8995759487152, 234.42065525054932, 237.98866415023804, 241.62026476860046, 245.27041387557983, 248.90881156921387, 252.37017560005188, 255.98933458328247, 259.63824439048767, 263.2212038040161, 266.74303936958313, 270.4774343967438, 274.0428168773651, 277.60140323638916, 281.5068621635437, 285.45269799232483, 289.41906213760376, 293.3561670780182, 297.31923842430115, 301.3811318874359, 305.4079189300537, 308.98169016838074, 312.5488302707672, 316.10341143608093, 319.66718912124634, 323.1507785320282, 326.646116733551, 330.3103404045105, 333.95027899742126, 337.51392793655396, 341.3083965778351, 345.1650640964508, 348.99357891082764, 352.5656521320343, 356.1251232624054, 359.80872988700867, 363.74071431159973, 367.6947491168976, 369.76402258872986]
[9.71, 9.71, 9.7225, 9.73, 9.695, 9.6975, 9.66, 9.67, 9.6925, 9.7025, 9.72, 9.7125, 9.675, 9.7175, 9.755, 9.77, 9.79, 9.7775, 9.78, 9.7975, 9.7825, 9.7875, 9.8125, 9.815, 9.8, 9.8125, 9.805, 9.8025, 9.7925, 9.7975, 9.785, 9.795, 9.7875, 9.8225, 9.805, 9.8175, 9.825, 9.8225, 9.84, 9.865, 9.85, 9.86, 9.8525, 9.8475, 9.8525, 9.84, 9.8475, 9.8575, 9.83, 9.85, 9.83, 9.805, 9.835, 9.87, 9.8625, 9.8625, 9.8625, 9.8325, 9.84, 9.8675, 9.8525, 9.83, 9.8375, 9.82, 9.8425, 9.86, 9.865, 9.865, 9.87, 9.855, 9.855, 9.8625, 9.8525, 9.8375, 9.835, 9.8, 9.79, 9.82, 9.8275, 9.81, 9.81, 9.8025, 9.81, 9.8175, 9.83, 9.8375, 9.8275, 9.805, 9.8025, 9.8675, 9.88, 9.88, 9.855, 9.85, 9.8525, 9.8625, 9.85, 9.865, 9.875, 9.8675, 9.9375]
