nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.275, Test loss: 2.156, Test accuracy: 36.53
Round   0, Global train loss: 2.275, Global test loss: 2.157, Global test accuracy: 37.22
Round   1, Train loss: 1.860, Test loss: 1.807, Test accuracy: 67.58
Round   1, Global train loss: 1.860, Global test loss: 1.745, Global test accuracy: 73.25
Round   2, Train loss: 1.659, Test loss: 1.737, Test accuracy: 74.43
Round   2, Global train loss: 1.659, Global test loss: 1.651, Global test accuracy: 82.58
Round   3, Train loss: 1.659, Test loss: 1.665, Test accuracy: 81.87
Round   3, Global train loss: 1.659, Global test loss: 1.605, Global test accuracy: 87.87
Round   4, Train loss: 1.589, Test loss: 1.653, Test accuracy: 82.77
Round   4, Global train loss: 1.589, Global test loss: 1.597, Global test accuracy: 88.05
Round   5, Train loss: 1.547, Test loss: 1.640, Test accuracy: 83.83
Round   5, Global train loss: 1.547, Global test loss: 1.564, Global test accuracy: 90.52
Round   6, Train loss: 1.562, Test loss: 1.635, Test accuracy: 84.22
Round   6, Global train loss: 1.562, Global test loss: 1.584, Global test accuracy: 89.03
Round   7, Train loss: 1.536, Test loss: 1.633, Test accuracy: 84.36
Round   7, Global train loss: 1.536, Global test loss: 1.565, Global test accuracy: 90.41
Round   8, Train loss: 1.536, Test loss: 1.630, Test accuracy: 84.53
Round   8, Global train loss: 1.536, Global test loss: 1.560, Global test accuracy: 90.62
Round   9, Train loss: 1.587, Test loss: 1.602, Test accuracy: 86.86
Round   9, Global train loss: 1.587, Global test loss: 1.587, Global test accuracy: 88.85
Round  10, Train loss: 1.517, Test loss: 1.600, Test accuracy: 86.97
Round  10, Global train loss: 1.517, Global test loss: 1.565, Global test accuracy: 90.26
Round  11, Train loss: 1.518, Test loss: 1.598, Test accuracy: 87.15
Round  11, Global train loss: 1.518, Global test loss: 1.563, Global test accuracy: 90.50
Round  12, Train loss: 1.555, Test loss: 1.590, Test accuracy: 87.86
Round  12, Global train loss: 1.555, Global test loss: 1.605, Global test accuracy: 86.44
Round  13, Train loss: 1.519, Test loss: 1.589, Test accuracy: 87.85
Round  13, Global train loss: 1.519, Global test loss: 1.563, Global test accuracy: 90.70
Round  14, Train loss: 1.511, Test loss: 1.587, Test accuracy: 87.98
Round  14, Global train loss: 1.511, Global test loss: 1.563, Global test accuracy: 90.29
Round  15, Train loss: 1.506, Test loss: 1.585, Test accuracy: 88.11
Round  15, Global train loss: 1.506, Global test loss: 1.558, Global test accuracy: 90.78
Round  16, Train loss: 1.525, Test loss: 1.584, Test accuracy: 88.13
Round  16, Global train loss: 1.525, Global test loss: 1.595, Global test accuracy: 87.04
Round  17, Train loss: 1.506, Test loss: 1.584, Test accuracy: 88.14
Round  17, Global train loss: 1.506, Global test loss: 1.561, Global test accuracy: 90.33
Round  18, Train loss: 1.492, Test loss: 1.583, Test accuracy: 88.21
Round  18, Global train loss: 1.492, Global test loss: 1.554, Global test accuracy: 91.20
Round  19, Train loss: 1.509, Test loss: 1.583, Test accuracy: 88.23
Round  19, Global train loss: 1.509, Global test loss: 1.561, Global test accuracy: 90.31
Round  20, Train loss: 1.504, Test loss: 1.583, Test accuracy: 88.25
Round  20, Global train loss: 1.504, Global test loss: 1.558, Global test accuracy: 90.85
Round  21, Train loss: 1.523, Test loss: 1.582, Test accuracy: 88.23
Round  21, Global train loss: 1.523, Global test loss: 1.594, Global test accuracy: 87.06
Round  22, Train loss: 1.490, Test loss: 1.582, Test accuracy: 88.22
Round  22, Global train loss: 1.490, Global test loss: 1.551, Global test accuracy: 91.38
Round  23, Train loss: 1.505, Test loss: 1.582, Test accuracy: 88.24
Round  23, Global train loss: 1.505, Global test loss: 1.554, Global test accuracy: 91.20
Round  24, Train loss: 1.519, Test loss: 1.582, Test accuracy: 88.22
Round  24, Global train loss: 1.519, Global test loss: 1.580, Global test accuracy: 88.52
Round  25, Train loss: 1.524, Test loss: 1.579, Test accuracy: 88.45
Round  25, Global train loss: 1.524, Global test loss: 1.570, Global test accuracy: 89.69
Round  26, Train loss: 1.488, Test loss: 1.579, Test accuracy: 88.44
Round  26, Global train loss: 1.488, Global test loss: 1.548, Global test accuracy: 91.65
Round  27, Train loss: 1.507, Test loss: 1.578, Test accuracy: 88.53
Round  27, Global train loss: 1.507, Global test loss: 1.559, Global test accuracy: 90.73
Round  28, Train loss: 1.509, Test loss: 1.578, Test accuracy: 88.55
Round  28, Global train loss: 1.509, Global test loss: 1.562, Global test accuracy: 90.49
Round  29, Train loss: 1.490, Test loss: 1.578, Test accuracy: 88.59
Round  29, Global train loss: 1.490, Global test loss: 1.551, Global test accuracy: 91.28
Round  30, Train loss: 1.487, Test loss: 1.577, Test accuracy: 88.58
Round  30, Global train loss: 1.487, Global test loss: 1.552, Global test accuracy: 91.30
Round  31, Train loss: 1.503, Test loss: 1.577, Test accuracy: 88.57
Round  31, Global train loss: 1.503, Global test loss: 1.561, Global test accuracy: 90.55
Round  32, Train loss: 1.490, Test loss: 1.577, Test accuracy: 88.62
Round  32, Global train loss: 1.490, Global test loss: 1.548, Global test accuracy: 91.69
Round  33, Train loss: 1.484, Test loss: 1.577, Test accuracy: 88.59
Round  33, Global train loss: 1.484, Global test loss: 1.550, Global test accuracy: 91.43
Round  34, Train loss: 1.486, Test loss: 1.577, Test accuracy: 88.62
Round  34, Global train loss: 1.486, Global test loss: 1.555, Global test accuracy: 90.85
Round  35, Train loss: 1.485, Test loss: 1.577, Test accuracy: 88.63
Round  35, Global train loss: 1.485, Global test loss: 1.551, Global test accuracy: 91.46
Round  36, Train loss: 1.483, Test loss: 1.577, Test accuracy: 88.64
Round  36, Global train loss: 1.483, Global test loss: 1.548, Global test accuracy: 91.72
Round  37, Train loss: 1.484, Test loss: 1.577, Test accuracy: 88.65
Round  37, Global train loss: 1.484, Global test loss: 1.552, Global test accuracy: 91.22
Round  38, Train loss: 1.488, Test loss: 1.577, Test accuracy: 88.67
Round  38, Global train loss: 1.488, Global test loss: 1.550, Global test accuracy: 91.46
Round  39, Train loss: 1.500, Test loss: 1.576, Test accuracy: 88.67
Round  39, Global train loss: 1.500, Global test loss: 1.559, Global test accuracy: 90.55
Round  40, Train loss: 1.500, Test loss: 1.576, Test accuracy: 88.69
Round  40, Global train loss: 1.500, Global test loss: 1.559, Global test accuracy: 90.57
Round  41, Train loss: 1.500, Test loss: 1.576, Test accuracy: 88.69
Round  41, Global train loss: 1.500, Global test loss: 1.553, Global test accuracy: 91.25
Round  42, Train loss: 1.500, Test loss: 1.576, Test accuracy: 88.72
Round  42, Global train loss: 1.500, Global test loss: 1.558, Global test accuracy: 90.77
Round  43, Train loss: 1.483, Test loss: 1.576, Test accuracy: 88.74
Round  43, Global train loss: 1.483, Global test loss: 1.550, Global test accuracy: 91.27
Round  44, Train loss: 1.483, Test loss: 1.576, Test accuracy: 88.74
Round  44, Global train loss: 1.483, Global test loss: 1.552, Global test accuracy: 91.11
Round  45, Train loss: 1.513, Test loss: 1.576, Test accuracy: 88.75
Round  45, Global train loss: 1.513, Global test loss: 1.589, Global test accuracy: 87.63
Round  46, Train loss: 1.485, Test loss: 1.575, Test accuracy: 88.77
Round  46, Global train loss: 1.485, Global test loss: 1.546, Global test accuracy: 91.86
Round  47, Train loss: 1.501, Test loss: 1.575, Test accuracy: 88.76
Round  47, Global train loss: 1.501, Global test loss: 1.557, Global test accuracy: 90.82
Round  48, Train loss: 1.498, Test loss: 1.575, Test accuracy: 88.79
Round  48, Global train loss: 1.498, Global test loss: 1.557, Global test accuracy: 90.93
Round  49, Train loss: 1.482, Test loss: 1.575, Test accuracy: 88.79
Round  49, Global train loss: 1.482, Global test loss: 1.551, Global test accuracy: 91.18
Round  50, Train loss: 1.484, Test loss: 1.575, Test accuracy: 88.78
Round  50, Global train loss: 1.484, Global test loss: 1.553, Global test accuracy: 91.03
Round  51, Train loss: 1.500, Test loss: 1.575, Test accuracy: 88.78
Round  51, Global train loss: 1.500, Global test loss: 1.554, Global test accuracy: 91.25
Round  52, Train loss: 1.484, Test loss: 1.575, Test accuracy: 88.78
Round  52, Global train loss: 1.484, Global test loss: 1.551, Global test accuracy: 91.39
Round  53, Train loss: 1.485, Test loss: 1.575, Test accuracy: 88.76
Round  53, Global train loss: 1.485, Global test loss: 1.549, Global test accuracy: 91.38
Round  54, Train loss: 1.500, Test loss: 1.575, Test accuracy: 88.77
Round  54, Global train loss: 1.500, Global test loss: 1.559, Global test accuracy: 90.75
Round  55, Train loss: 1.500, Test loss: 1.575, Test accuracy: 88.78
Round  55, Global train loss: 1.500, Global test loss: 1.551, Global test accuracy: 91.46
Round  56, Train loss: 1.483, Test loss: 1.575, Test accuracy: 88.82
Round  56, Global train loss: 1.483, Global test loss: 1.549, Global test accuracy: 91.42
Round  57, Train loss: 1.484, Test loss: 1.575, Test accuracy: 88.81
Round  57, Global train loss: 1.484, Global test loss: 1.546, Global test accuracy: 91.94
Round  58, Train loss: 1.501, Test loss: 1.575, Test accuracy: 88.86
Round  58, Global train loss: 1.501, Global test loss: 1.552, Global test accuracy: 91.27
Round  59, Train loss: 1.483, Test loss: 1.575, Test accuracy: 88.83
Round  59, Global train loss: 1.483, Global test loss: 1.551, Global test accuracy: 91.31
Round  60, Train loss: 1.513, Test loss: 1.575, Test accuracy: 88.84
Round  60, Global train loss: 1.513, Global test loss: 1.581, Global test accuracy: 88.29
Round  61, Train loss: 1.485, Test loss: 1.575, Test accuracy: 88.85
Round  61, Global train loss: 1.485, Global test loss: 1.547, Global test accuracy: 91.57
Round  62, Train loss: 1.500, Test loss: 1.574, Test accuracy: 88.87
Round  62, Global train loss: 1.500, Global test loss: 1.553, Global test accuracy: 91.31
Round  63, Train loss: 1.483, Test loss: 1.575, Test accuracy: 88.86
Round  63, Global train loss: 1.483, Global test loss: 1.549, Global test accuracy: 91.41
Round  64, Train loss: 1.486, Test loss: 1.575, Test accuracy: 88.83
Round  64, Global train loss: 1.486, Global test loss: 1.547, Global test accuracy: 91.56
Round  65, Train loss: 1.498, Test loss: 1.575, Test accuracy: 88.81
Round  65, Global train loss: 1.498, Global test loss: 1.552, Global test accuracy: 91.32
Round  66, Train loss: 1.482, Test loss: 1.575, Test accuracy: 88.80
Round  66, Global train loss: 1.482, Global test loss: 1.545, Global test accuracy: 91.88
Round  67, Train loss: 1.499, Test loss: 1.575, Test accuracy: 88.81
Round  67, Global train loss: 1.499, Global test loss: 1.550, Global test accuracy: 91.44
Round  68, Train loss: 1.483, Test loss: 1.575, Test accuracy: 88.81
Round  68, Global train loss: 1.483, Global test loss: 1.547, Global test accuracy: 91.72
Round  69, Train loss: 1.498, Test loss: 1.575, Test accuracy: 88.77
Round  69, Global train loss: 1.498, Global test loss: 1.556, Global test accuracy: 90.97
Round  70, Train loss: 1.480, Test loss: 1.575, Test accuracy: 88.77
Round  70, Global train loss: 1.480, Global test loss: 1.550, Global test accuracy: 91.22
Round  71, Train loss: 1.497, Test loss: 1.574, Test accuracy: 88.79
Round  71, Global train loss: 1.497, Global test loss: 1.561, Global test accuracy: 90.38
Round  72, Train loss: 1.500, Test loss: 1.574, Test accuracy: 88.79
Round  72, Global train loss: 1.500, Global test loss: 1.548, Global test accuracy: 91.77
Round  73, Train loss: 1.484, Test loss: 1.574, Test accuracy: 88.78
Round  73, Global train loss: 1.484, Global test loss: 1.548, Global test accuracy: 91.63
Round  74, Train loss: 1.482, Test loss: 1.574, Test accuracy: 88.81
Round  74, Global train loss: 1.482, Global test loss: 1.551, Global test accuracy: 91.17
Round  75, Train loss: 1.483, Test loss: 1.574, Test accuracy: 88.81
Round  75, Global train loss: 1.483, Global test loss: 1.550, Global test accuracy: 91.41
Round  76, Train loss: 1.497, Test loss: 1.574, Test accuracy: 88.82
Round  76, Global train loss: 1.497, Global test loss: 1.555, Global test accuracy: 91.03
Round  77, Train loss: 1.479, Test loss: 1.574, Test accuracy: 88.80
Round  77, Global train loss: 1.479, Global test loss: 1.550, Global test accuracy: 91.35
Round  78, Train loss: 1.483, Test loss: 1.574, Test accuracy: 88.81
Round  78, Global train loss: 1.483, Global test loss: 1.550, Global test accuracy: 91.26
Round  79, Train loss: 1.484, Test loss: 1.574, Test accuracy: 88.83
Round  79, Global train loss: 1.484, Global test loss: 1.545, Global test accuracy: 91.87
Round  80, Train loss: 1.483, Test loss: 1.574, Test accuracy: 88.83
Round  80, Global train loss: 1.483, Global test loss: 1.553, Global test accuracy: 91.12
Round  81, Train loss: 1.496, Test loss: 1.574, Test accuracy: 88.85
Round  81, Global train loss: 1.496, Global test loss: 1.551, Global test accuracy: 91.32
Round  82, Train loss: 1.482, Test loss: 1.574, Test accuracy: 88.85
Round  82, Global train loss: 1.482, Global test loss: 1.547, Global test accuracy: 91.62
Round  83, Train loss: 1.498, Test loss: 1.574, Test accuracy: 88.84
Round  83, Global train loss: 1.498, Global test loss: 1.560, Global test accuracy: 90.54
Round  84, Train loss: 1.499, Test loss: 1.574, Test accuracy: 88.84
Round  84, Global train loss: 1.499, Global test loss: 1.550, Global test accuracy: 91.44
Round  85, Train loss: 1.497, Test loss: 1.574, Test accuracy: 88.83
Round  85, Global train loss: 1.497, Global test loss: 1.557, Global test accuracy: 90.75
Round  86, Train loss: 1.510, Test loss: 1.571, Test accuracy: 89.12
Round  86, Global train loss: 1.510, Global test loss: 1.556, Global test accuracy: 91.00
Round  87, Train loss: 1.498, Test loss: 1.571, Test accuracy: 89.13
Round  87, Global train loss: 1.498, Global test loss: 1.559, Global test accuracy: 90.66
Round  88, Train loss: 1.483, Test loss: 1.571, Test accuracy: 89.11
Round  88, Global train loss: 1.483, Global test loss: 1.545, Global test accuracy: 91.95
Round  89, Train loss: 1.482, Test loss: 1.571, Test accuracy: 89.13
Round  89, Global train loss: 1.482, Global test loss: 1.544, Global test accuracy: 92.13
Round  90, Train loss: 1.482, Test loss: 1.571, Test accuracy: 89.15
Round  90, Global train loss: 1.482, Global test loss: 1.547, Global test accuracy: 91.68
Round  91, Train loss: 1.480, Test loss: 1.571, Test accuracy: 89.14
Round  91, Global train loss: 1.480, Global test loss: 1.550, Global test accuracy: 91.31
Round  92, Train loss: 1.482, Test loss: 1.571, Test accuracy: 89.15
Round  92, Global train loss: 1.482, Global test loss: 1.547, Global test accuracy: 91.66
Round  93, Train loss: 1.481, Test loss: 1.571, Test accuracy: 89.17
Round  93, Global train loss: 1.481, Global test loss: 1.550, Global test accuracy: 91.44
Round  94, Train loss: 1.480, Test loss: 1.571, Test accuracy: 89.17
Round  94, Global train loss: 1.480, Global test loss: 1.549, Global test accuracy: 91.34
Round  95, Train loss: 1.499, Test loss: 1.571, Test accuracy: 89.17
Round  95, Global train loss: 1.499, Global test loss: 1.552, Global test accuracy: 91.27/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.482, Test loss: 1.570, Test accuracy: 89.17
Round  96, Global train loss: 1.482, Global test loss: 1.548, Global test accuracy: 91.64
Round  97, Train loss: 1.497, Test loss: 1.570, Test accuracy: 89.20
Round  97, Global train loss: 1.497, Global test loss: 1.555, Global test accuracy: 90.81
Round  98, Train loss: 1.498, Test loss: 1.570, Test accuracy: 89.20
Round  98, Global train loss: 1.498, Global test loss: 1.551, Global test accuracy: 91.25
Round  99, Train loss: 1.481, Test loss: 1.570, Test accuracy: 89.19
Round  99, Global train loss: 1.481, Global test loss: 1.544, Global test accuracy: 92.11
Final Round, Train loss: 1.488, Test loss: 1.571, Test accuracy: 89.19
Final Round, Global train loss: 1.488, Global test loss: 1.544, Global test accuracy: 92.11
Average accuracy final 10 rounds: 89.17175 

Average global accuracy final 10 rounds: 91.45025 

4273.788010120392
[2.6883115768432617, 5.376623153686523, 8.02195930480957, 10.667295455932617, 13.315234899520874, 15.96317434310913, 18.74906826019287, 21.53496217727661, 24.24971890449524, 26.964475631713867, 29.65106201171875, 32.33764839172363, 35.0863995552063, 37.835150718688965, 40.618988037109375, 43.402825355529785, 46.111693382263184, 48.82056140899658, 51.54080319404602, 54.26104497909546, 57.0088586807251, 59.756672382354736, 62.51247262954712, 65.2682728767395, 67.96702003479004, 70.66576719284058, 73.37649965286255, 76.08723211288452, 78.83811736106873, 81.58900260925293, 84.3456768989563, 87.10235118865967, 89.84311699867249, 92.5838828086853, 95.33982253074646, 98.09576225280762, 100.87023043632507, 103.64469861984253, 106.43026351928711, 109.21582841873169, 111.96069836616516, 114.70556831359863, 117.46027398109436, 120.21497964859009, 122.96942615509033, 125.72387266159058, 128.49463486671448, 131.26539707183838, 134.01841020584106, 136.77142333984375, 139.50116515159607, 142.2309069633484, 145.00224542617798, 147.77358388900757, 150.5234339237213, 153.27328395843506, 156.02706933021545, 158.78085470199585, 161.52183985710144, 164.26282501220703, 166.9786500930786, 169.6944751739502, 172.42729949951172, 175.16012382507324, 177.8637945652008, 180.56746530532837, 183.25826001167297, 185.94905471801758, 188.6733157634735, 191.39757680892944, 194.1245744228363, 196.85157203674316, 199.5908386707306, 202.33010530471802, 205.0225887298584, 207.71507215499878, 210.42383575439453, 213.13259935379028, 215.86835885047913, 218.60411834716797, 221.36113572120667, 224.11815309524536, 226.83400917053223, 229.5498652458191, 232.28179097175598, 235.01371669769287, 237.76919889450073, 240.5246810913086, 243.297837972641, 246.0709948539734, 248.8327567577362, 251.59451866149902, 254.37960815429688, 257.1646976470947, 259.8973903656006, 262.63008308410645, 265.3905339241028, 268.1509847640991, 270.89687490463257, 273.642765045166, 276.4146797657013, 279.1865944862366, 281.95273876190186, 284.71888303756714, 287.4919888973236, 290.2650947570801, 293.0132009983063, 295.76130723953247, 298.5287756919861, 301.2962441444397, 304.07233905792236, 306.84843397140503, 309.61450123786926, 312.3805685043335, 315.1391146183014, 317.8976607322693, 320.6353585720062, 323.37305641174316, 326.13963985443115, 328.90622329711914, 331.66998839378357, 334.433753490448, 337.1738591194153, 339.91396474838257, 342.6816051006317, 345.44924545288086, 348.2174072265625, 350.98556900024414, 353.74745202064514, 356.50933504104614, 359.2702705860138, 362.03120613098145, 364.8580689430237, 367.6849317550659, 370.4791958332062, 373.27345991134644, 376.07572531700134, 378.87799072265625, 381.63360810279846, 384.3892254829407, 387.1348156929016, 389.88040590286255, 392.6526927947998, 395.42497968673706, 398.1888380050659, 400.9526963233948, 403.7152349948883, 406.47777366638184, 409.2239718437195, 411.97017002105713, 414.7344093322754, 417.49864864349365, 420.27901816368103, 423.0593876838684, 425.83547925949097, 428.6115708351135, 431.34262776374817, 434.0736846923828, 436.84501242637634, 439.6163401603699, 442.3912136554718, 445.16608715057373, 447.9435205459595, 450.7209539413452, 453.48028111457825, 456.2396082878113, 459.0038356781006, 461.7680630683899, 464.5840210914612, 467.39997911453247, 470.19003462791443, 472.9800901412964, 475.7573537826538, 478.53461742401123, 481.314439535141, 484.09426164627075, 486.87257981300354, 489.6508979797363, 492.4060139656067, 495.16112995147705, 497.90721583366394, 500.65330171585083, 503.407607793808, 506.16191387176514, 508.9201145172119, 511.6783151626587, 514.4346601963043, 517.19100522995, 519.9312264919281, 522.6714477539062, 525.4319775104523, 528.1925072669983, 530.9385702610016, 533.6846332550049, 536.4226455688477, 539.1606578826904, 541.8986213207245, 544.6365847587585, 547.3955128192902, 550.1544408798218, 551.5347516536713, 552.9150624275208]
[36.53, 36.53, 67.58, 67.58, 74.43, 74.43, 81.8725, 81.8725, 82.7725, 82.7725, 83.835, 83.835, 84.225, 84.225, 84.365, 84.365, 84.53, 84.53, 86.865, 86.865, 86.975, 86.975, 87.15, 87.15, 87.855, 87.855, 87.8525, 87.8525, 87.98, 87.98, 88.1075, 88.1075, 88.13, 88.13, 88.145, 88.145, 88.21, 88.21, 88.235, 88.235, 88.25, 88.25, 88.2325, 88.2325, 88.225, 88.225, 88.2425, 88.2425, 88.22, 88.22, 88.455, 88.455, 88.4425, 88.4425, 88.535, 88.535, 88.55, 88.55, 88.59, 88.59, 88.58, 88.58, 88.5675, 88.5675, 88.6225, 88.6225, 88.5875, 88.5875, 88.62, 88.62, 88.6325, 88.6325, 88.645, 88.645, 88.6525, 88.6525, 88.665, 88.665, 88.6675, 88.6675, 88.685, 88.685, 88.6925, 88.6925, 88.7225, 88.7225, 88.7375, 88.7375, 88.7425, 88.7425, 88.745, 88.745, 88.77, 88.77, 88.76, 88.76, 88.7875, 88.7875, 88.79, 88.79, 88.7775, 88.7775, 88.775, 88.775, 88.78, 88.78, 88.76, 88.76, 88.77, 88.77, 88.775, 88.775, 88.82, 88.82, 88.8075, 88.8075, 88.865, 88.865, 88.8275, 88.8275, 88.84, 88.84, 88.85, 88.85, 88.8725, 88.8725, 88.8575, 88.8575, 88.835, 88.835, 88.815, 88.815, 88.795, 88.795, 88.805, 88.805, 88.8125, 88.8125, 88.7675, 88.7675, 88.77, 88.77, 88.7875, 88.7875, 88.7875, 88.7875, 88.78, 88.78, 88.8075, 88.8075, 88.815, 88.815, 88.82, 88.82, 88.795, 88.795, 88.815, 88.815, 88.825, 88.825, 88.835, 88.835, 88.85, 88.85, 88.8475, 88.8475, 88.845, 88.845, 88.84, 88.84, 88.8275, 88.8275, 89.125, 89.125, 89.1325, 89.1325, 89.1125, 89.1125, 89.1275, 89.1275, 89.1525, 89.1525, 89.145, 89.145, 89.1525, 89.1525, 89.1675, 89.1675, 89.165, 89.165, 89.1675, 89.1675, 89.1725, 89.1725, 89.2025, 89.2025, 89.205, 89.205, 89.1875, 89.1875, 89.1925, 89.1925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.278, Test loss: 2.197, Test accuracy: 45.25
Round   0, Global train loss: 2.278, Global test loss: 2.198, Global test accuracy: 46.21
Round   1, Train loss: 1.852, Test loss: 1.757, Test accuracy: 76.53
Round   1, Global train loss: 1.852, Global test loss: 1.639, Global test accuracy: 86.16
Round   2, Train loss: 1.586, Test loss: 1.724, Test accuracy: 78.37
Round   2, Global train loss: 1.586, Global test loss: 1.580, Global test accuracy: 89.63
Round   3, Train loss: 1.550, Test loss: 1.653, Test accuracy: 83.35
Round   3, Global train loss: 1.550, Global test loss: 1.564, Global test accuracy: 90.63
Round   4, Train loss: 1.540, Test loss: 1.615, Test accuracy: 86.75
Round   4, Global train loss: 1.540, Global test loss: 1.555, Global test accuracy: 91.28
Round   5, Train loss: 1.525, Test loss: 1.604, Test accuracy: 87.62
Round   5, Global train loss: 1.525, Global test loss: 1.547, Global test accuracy: 91.98
Round   6, Train loss: 1.523, Test loss: 1.561, Test accuracy: 90.68
Round   6, Global train loss: 1.523, Global test loss: 1.542, Global test accuracy: 92.27
Round   7, Train loss: 1.518, Test loss: 1.560, Test accuracy: 90.81
Round   7, Global train loss: 1.518, Global test loss: 1.539, Global test accuracy: 92.66
Round   8, Train loss: 1.513, Test loss: 1.557, Test accuracy: 91.01
Round   8, Global train loss: 1.513, Global test loss: 1.535, Global test accuracy: 92.97
Round   9, Train loss: 1.504, Test loss: 1.549, Test accuracy: 91.73
Round   9, Global train loss: 1.504, Global test loss: 1.532, Global test accuracy: 93.13
Round  10, Train loss: 1.507, Test loss: 1.545, Test accuracy: 92.04
Round  10, Global train loss: 1.507, Global test loss: 1.529, Global test accuracy: 93.48
Round  11, Train loss: 1.503, Test loss: 1.541, Test accuracy: 92.50
Round  11, Global train loss: 1.503, Global test loss: 1.526, Global test accuracy: 93.68
Round  12, Train loss: 1.500, Test loss: 1.537, Test accuracy: 92.83
Round  12, Global train loss: 1.500, Global test loss: 1.523, Global test accuracy: 93.98
Round  13, Train loss: 1.495, Test loss: 1.535, Test accuracy: 93.02
Round  13, Global train loss: 1.495, Global test loss: 1.522, Global test accuracy: 94.12
Round  14, Train loss: 1.496, Test loss: 1.532, Test accuracy: 93.27
Round  14, Global train loss: 1.496, Global test loss: 1.520, Global test accuracy: 94.24
Round  15, Train loss: 1.498, Test loss: 1.530, Test accuracy: 93.44
Round  15, Global train loss: 1.498, Global test loss: 1.518, Global test accuracy: 94.50
Round  16, Train loss: 1.494, Test loss: 1.529, Test accuracy: 93.59
Round  16, Global train loss: 1.494, Global test loss: 1.516, Global test accuracy: 94.75
Round  17, Train loss: 1.490, Test loss: 1.527, Test accuracy: 93.72
Round  17, Global train loss: 1.490, Global test loss: 1.516, Global test accuracy: 94.72
Round  18, Train loss: 1.488, Test loss: 1.523, Test accuracy: 94.11
Round  18, Global train loss: 1.488, Global test loss: 1.515, Global test accuracy: 94.95
Round  19, Train loss: 1.491, Test loss: 1.521, Test accuracy: 94.27
Round  19, Global train loss: 1.491, Global test loss: 1.513, Global test accuracy: 95.05
Round  20, Train loss: 1.487, Test loss: 1.520, Test accuracy: 94.38
Round  20, Global train loss: 1.487, Global test loss: 1.513, Global test accuracy: 95.16
Round  21, Train loss: 1.486, Test loss: 1.518, Test accuracy: 94.50
Round  21, Global train loss: 1.486, Global test loss: 1.511, Global test accuracy: 95.30
Round  22, Train loss: 1.482, Test loss: 1.517, Test accuracy: 94.58
Round  22, Global train loss: 1.482, Global test loss: 1.511, Global test accuracy: 95.17
Round  23, Train loss: 1.483, Test loss: 1.517, Test accuracy: 94.64
Round  23, Global train loss: 1.483, Global test loss: 1.510, Global test accuracy: 95.42
Round  24, Train loss: 1.483, Test loss: 1.515, Test accuracy: 94.73
Round  24, Global train loss: 1.483, Global test loss: 1.509, Global test accuracy: 95.47
Round  25, Train loss: 1.482, Test loss: 1.515, Test accuracy: 94.78
Round  25, Global train loss: 1.482, Global test loss: 1.508, Global test accuracy: 95.58
Round  26, Train loss: 1.482, Test loss: 1.514, Test accuracy: 94.92
Round  26, Global train loss: 1.482, Global test loss: 1.507, Global test accuracy: 95.70
Round  27, Train loss: 1.483, Test loss: 1.512, Test accuracy: 95.06
Round  27, Global train loss: 1.483, Global test loss: 1.507, Global test accuracy: 95.70
Round  28, Train loss: 1.478, Test loss: 1.512, Test accuracy: 95.13
Round  28, Global train loss: 1.478, Global test loss: 1.507, Global test accuracy: 95.65
Round  29, Train loss: 1.482, Test loss: 1.511, Test accuracy: 95.15
Round  29, Global train loss: 1.482, Global test loss: 1.506, Global test accuracy: 95.68
Round  30, Train loss: 1.480, Test loss: 1.511, Test accuracy: 95.15
Round  30, Global train loss: 1.480, Global test loss: 1.506, Global test accuracy: 95.72
Round  31, Train loss: 1.480, Test loss: 1.511, Test accuracy: 95.20
Round  31, Global train loss: 1.480, Global test loss: 1.506, Global test accuracy: 95.66
Round  32, Train loss: 1.480, Test loss: 1.510, Test accuracy: 95.25
Round  32, Global train loss: 1.480, Global test loss: 1.505, Global test accuracy: 95.75
Round  33, Train loss: 1.481, Test loss: 1.510, Test accuracy: 95.34
Round  33, Global train loss: 1.481, Global test loss: 1.504, Global test accuracy: 95.78
Round  34, Train loss: 1.479, Test loss: 1.509, Test accuracy: 95.43
Round  34, Global train loss: 1.479, Global test loss: 1.503, Global test accuracy: 96.06
Round  35, Train loss: 1.478, Test loss: 1.508, Test accuracy: 95.52
Round  35, Global train loss: 1.478, Global test loss: 1.504, Global test accuracy: 95.94
Round  36, Train loss: 1.477, Test loss: 1.508, Test accuracy: 95.54
Round  36, Global train loss: 1.477, Global test loss: 1.504, Global test accuracy: 95.90
Round  37, Train loss: 1.476, Test loss: 1.507, Test accuracy: 95.57
Round  37, Global train loss: 1.476, Global test loss: 1.502, Global test accuracy: 96.12
Round  38, Train loss: 1.476, Test loss: 1.506, Test accuracy: 95.69
Round  38, Global train loss: 1.476, Global test loss: 1.502, Global test accuracy: 96.13
Round  39, Train loss: 1.476, Test loss: 1.506, Test accuracy: 95.72
Round  39, Global train loss: 1.476, Global test loss: 1.502, Global test accuracy: 96.17
Round  40, Train loss: 1.476, Test loss: 1.505, Test accuracy: 95.74
Round  40, Global train loss: 1.476, Global test loss: 1.501, Global test accuracy: 96.17
Round  41, Train loss: 1.476, Test loss: 1.505, Test accuracy: 95.83
Round  41, Global train loss: 1.476, Global test loss: 1.501, Global test accuracy: 96.14
Round  42, Train loss: 1.475, Test loss: 1.505, Test accuracy: 95.90
Round  42, Global train loss: 1.475, Global test loss: 1.501, Global test accuracy: 96.14
Round  43, Train loss: 1.473, Test loss: 1.504, Test accuracy: 95.96
Round  43, Global train loss: 1.473, Global test loss: 1.501, Global test accuracy: 96.30
Round  44, Train loss: 1.474, Test loss: 1.504, Test accuracy: 96.00
Round  44, Global train loss: 1.474, Global test loss: 1.501, Global test accuracy: 96.21
Round  45, Train loss: 1.474, Test loss: 1.503, Test accuracy: 96.02
Round  45, Global train loss: 1.474, Global test loss: 1.501, Global test accuracy: 96.21
Round  46, Train loss: 1.473, Test loss: 1.503, Test accuracy: 96.01
Round  46, Global train loss: 1.473, Global test loss: 1.500, Global test accuracy: 96.42
Round  47, Train loss: 1.474, Test loss: 1.503, Test accuracy: 96.06
Round  47, Global train loss: 1.474, Global test loss: 1.500, Global test accuracy: 96.45
Round  48, Train loss: 1.473, Test loss: 1.502, Test accuracy: 96.07
Round  48, Global train loss: 1.473, Global test loss: 1.500, Global test accuracy: 96.29
Round  49, Train loss: 1.472, Test loss: 1.502, Test accuracy: 96.08
Round  49, Global train loss: 1.472, Global test loss: 1.499, Global test accuracy: 96.41
Round  50, Train loss: 1.473, Test loss: 1.502, Test accuracy: 96.11
Round  50, Global train loss: 1.473, Global test loss: 1.499, Global test accuracy: 96.38
Round  51, Train loss: 1.473, Test loss: 1.502, Test accuracy: 96.14
Round  51, Global train loss: 1.473, Global test loss: 1.498, Global test accuracy: 96.39
Round  52, Train loss: 1.471, Test loss: 1.501, Test accuracy: 96.20
Round  52, Global train loss: 1.471, Global test loss: 1.498, Global test accuracy: 96.48
Round  53, Train loss: 1.473, Test loss: 1.501, Test accuracy: 96.24
Round  53, Global train loss: 1.473, Global test loss: 1.499, Global test accuracy: 96.42
Round  54, Train loss: 1.472, Test loss: 1.501, Test accuracy: 96.24
Round  54, Global train loss: 1.472, Global test loss: 1.499, Global test accuracy: 96.42
Round  55, Train loss: 1.472, Test loss: 1.500, Test accuracy: 96.28
Round  55, Global train loss: 1.472, Global test loss: 1.498, Global test accuracy: 96.46
Round  56, Train loss: 1.471, Test loss: 1.500, Test accuracy: 96.28
Round  56, Global train loss: 1.471, Global test loss: 1.498, Global test accuracy: 96.59
Round  57, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.32
Round  57, Global train loss: 1.474, Global test loss: 1.497, Global test accuracy: 96.53
Round  58, Train loss: 1.471, Test loss: 1.500, Test accuracy: 96.32
Round  58, Global train loss: 1.471, Global test loss: 1.497, Global test accuracy: 96.48
Round  59, Train loss: 1.472, Test loss: 1.500, Test accuracy: 96.32
Round  59, Global train loss: 1.472, Global test loss: 1.497, Global test accuracy: 96.66
Round  60, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.33
Round  60, Global train loss: 1.471, Global test loss: 1.498, Global test accuracy: 96.58
Round  61, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.36
Round  61, Global train loss: 1.471, Global test loss: 1.497, Global test accuracy: 96.57
Round  62, Train loss: 1.470, Test loss: 1.499, Test accuracy: 96.36
Round  62, Global train loss: 1.470, Global test loss: 1.497, Global test accuracy: 96.67
Round  63, Train loss: 1.470, Test loss: 1.499, Test accuracy: 96.44
Round  63, Global train loss: 1.470, Global test loss: 1.497, Global test accuracy: 96.65
Round  64, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.45
Round  64, Global train loss: 1.471, Global test loss: 1.497, Global test accuracy: 96.58
Round  65, Train loss: 1.470, Test loss: 1.499, Test accuracy: 96.44
Round  65, Global train loss: 1.470, Global test loss: 1.497, Global test accuracy: 96.56
Round  66, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.47
Round  66, Global train loss: 1.471, Global test loss: 1.497, Global test accuracy: 96.59
Round  67, Train loss: 1.472, Test loss: 1.498, Test accuracy: 96.47
Round  67, Global train loss: 1.472, Global test loss: 1.497, Global test accuracy: 96.62
Round  68, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.47
Round  68, Global train loss: 1.469, Global test loss: 1.496, Global test accuracy: 96.70
Round  69, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.45
Round  69, Global train loss: 1.469, Global test loss: 1.496, Global test accuracy: 96.55
Round  70, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.49
Round  70, Global train loss: 1.470, Global test loss: 1.496, Global test accuracy: 96.72
Round  71, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.51
Round  71, Global train loss: 1.470, Global test loss: 1.496, Global test accuracy: 96.74
Round  72, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.50
Round  72, Global train loss: 1.469, Global test loss: 1.496, Global test accuracy: 96.61
Round  73, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.54
Round  73, Global train loss: 1.471, Global test loss: 1.496, Global test accuracy: 96.72
Round  74, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.57
Round  74, Global train loss: 1.469, Global test loss: 1.496, Global test accuracy: 96.74
Round  75, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.55
Round  75, Global train loss: 1.470, Global test loss: 1.496, Global test accuracy: 96.73
Round  76, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.53
Round  76, Global train loss: 1.468, Global test loss: 1.495, Global test accuracy: 96.82
Round  77, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.56
Round  77, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.72
Round  78, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.58
Round  78, Global train loss: 1.469, Global test loss: 1.495, Global test accuracy: 96.75
Round  79, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.59
Round  79, Global train loss: 1.469, Global test loss: 1.496, Global test accuracy: 96.66
Round  80, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.59
Round  80, Global train loss: 1.469, Global test loss: 1.495, Global test accuracy: 96.75
Round  81, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.59
Round  81, Global train loss: 1.469, Global test loss: 1.495, Global test accuracy: 96.77
Round  82, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.60
Round  82, Global train loss: 1.469, Global test loss: 1.495, Global test accuracy: 96.75
Round  83, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.64
Round  83, Global train loss: 1.468, Global test loss: 1.495, Global test accuracy: 96.81
Round  84, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.66
Round  84, Global train loss: 1.468, Global test loss: 1.495, Global test accuracy: 96.82
Round  85, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.66
Round  85, Global train loss: 1.468, Global test loss: 1.495, Global test accuracy: 96.80
Round  86, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.68
Round  86, Global train loss: 1.469, Global test loss: 1.495, Global test accuracy: 96.84
Round  87, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.67
Round  87, Global train loss: 1.469, Global test loss: 1.495, Global test accuracy: 96.78
Round  88, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.66
Round  88, Global train loss: 1.468, Global test loss: 1.495, Global test accuracy: 96.76
Round  89, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.66
Round  89, Global train loss: 1.468, Global test loss: 1.495, Global test accuracy: 96.78
Round  90, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.69
Round  90, Global train loss: 1.468, Global test loss: 1.496, Global test accuracy: 96.65
Round  91, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.68
Round  91, Global train loss: 1.469, Global test loss: 1.495, Global test accuracy: 96.75
Round  92, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.69
Round  92, Global train loss: 1.468, Global test loss: 1.495, Global test accuracy: 96.72
Round  93, Train loss: 1.467, Test loss: 1.496, Test accuracy: 96.69
Round  93, Global train loss: 1.467, Global test loss: 1.495, Global test accuracy: 96.68
Round  94, Train loss: 1.467, Test loss: 1.496, Test accuracy: 96.69
Round  94, Global train loss: 1.467, Global test loss: 1.495, Global test accuracy: 96.78
Round  95, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.69
Round  95, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.81/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.69
Round  96, Global train loss: 1.469, Global test loss: 1.495, Global test accuracy: 96.81
Round  97, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.71
Round  97, Global train loss: 1.468, Global test loss: 1.495, Global test accuracy: 96.79
Round  98, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.72
Round  98, Global train loss: 1.468, Global test loss: 1.495, Global test accuracy: 96.72
Round  99, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.74
Round  99, Global train loss: 1.468, Global test loss: 1.495, Global test accuracy: 96.76
Final Round, Train loss: 1.466, Test loss: 1.495, Test accuracy: 96.75
Final Round, Global train loss: 1.466, Global test loss: 1.495, Global test accuracy: 96.76
Average accuracy final 10 rounds: 96.6995 

Average global accuracy final 10 rounds: 96.7465 

4150.086709260941
[2.7678112983703613, 5.535622596740723, 8.227858781814575, 10.920094966888428, 13.639807224273682, 16.359519481658936, 19.071074962615967, 21.782630443572998, 24.48507571220398, 27.18752098083496, 29.867775678634644, 32.548030376434326, 35.21912622451782, 37.89022207260132, 40.57168889045715, 43.25315570831299, 45.9442617893219, 48.63536787033081, 51.32902431488037, 54.02268075942993, 56.68335270881653, 59.344024658203125, 62.02749681472778, 64.71096897125244, 67.40147852897644, 70.09198808670044, 72.7595887184143, 75.42718935012817, 78.09713459014893, 80.76707983016968, 83.46662712097168, 86.16617441177368, 88.87950563430786, 91.59283685684204, 94.254714012146, 96.91659116744995, 99.57964396476746, 102.24269676208496, 104.92802214622498, 107.61334753036499, 110.28575921058655, 112.9581708908081, 115.61356830596924, 118.26896572113037, 120.96865224838257, 123.66833877563477, 126.36819767951965, 129.06805658340454, 131.7684862613678, 134.46891593933105, 137.15667700767517, 139.8444380760193, 142.55220317840576, 145.25996828079224, 147.9784812927246, 150.69699430465698, 153.37194561958313, 156.04689693450928, 158.71221470832825, 161.37753248214722, 164.0595259666443, 166.74151945114136, 169.4409682750702, 172.14041709899902, 174.8159248828888, 177.49143266677856, 180.18557739257812, 182.87972211837769, 185.57983136177063, 188.27994060516357, 190.96761775016785, 193.65529489517212, 196.36390948295593, 199.07252407073975, 201.75351285934448, 204.43450164794922, 207.06889533996582, 209.70328903198242, 212.3449010848999, 214.98651313781738, 217.67063665390015, 220.3547601699829, 223.04375576972961, 225.73275136947632, 228.37668824195862, 231.02062511444092, 233.65268421173096, 236.284743309021, 238.94955897331238, 241.61437463760376, 244.28161430358887, 246.94885396957397, 249.59162497520447, 252.23439598083496, 254.922913312912, 257.611430644989, 260.3023090362549, 262.99318742752075, 265.6591365337372, 268.3250856399536, 270.9733130931854, 273.62154054641724, 276.31566858291626, 279.0097966194153, 281.6958577632904, 284.3819189071655, 287.0284969806671, 289.6750750541687, 292.2880573272705, 294.9010396003723, 297.5828311443329, 300.26462268829346, 302.9462010860443, 305.62777948379517, 308.28426790237427, 310.94075632095337, 313.5892014503479, 316.23764657974243, 318.9187569618225, 321.5998673439026, 324.2871005535126, 326.97433376312256, 329.61536931991577, 332.256404876709, 334.89441895484924, 337.5324330329895, 340.2185711860657, 342.90470933914185, 345.5900628566742, 348.27541637420654, 350.92808198928833, 353.5807476043701, 356.2304790019989, 358.8802103996277, 361.5630249977112, 364.2458395957947, 366.92431688308716, 369.60279417037964, 372.2836639881134, 374.96453380584717, 377.6495394706726, 380.33454513549805, 383.0706081390381, 385.8066711425781, 388.4439115524292, 391.0811519622803, 393.72685956954956, 396.37256717681885, 399.0846152305603, 401.79666328430176, 404.48093247413635, 407.16520166397095, 409.8045439720154, 412.4438862800598, 415.11139011383057, 417.7788939476013, 420.4699890613556, 423.16108417510986, 425.8477954864502, 428.5345067977905, 431.1748960018158, 433.81528520584106, 436.43616580963135, 439.05704641342163, 441.7225477695465, 444.3880491256714, 447.0600130558014, 449.7319769859314, 452.3442192077637, 454.95646142959595, 457.52463817596436, 460.09281492233276, 462.7671844959259, 465.44155406951904, 468.10903763771057, 470.7765212059021, 473.40462136268616, 476.0327215194702, 478.60559368133545, 481.1784658432007, 483.8001992702484, 486.42193269729614, 489.0636577606201, 491.7053828239441, 494.31618428230286, 496.9269857406616, 499.54254841804504, 502.15811109542847, 504.7979109287262, 507.4377107620239, 510.1096091270447, 512.7815074920654, 515.3914370536804, 518.0013666152954, 520.5835399627686, 523.1657133102417, 525.8101670742035, 528.4546208381653, 531.0900113582611, 533.7254018783569, 535.0508470535278, 536.3762922286987]
[45.2475, 45.2475, 76.53, 76.53, 78.3675, 78.3675, 83.3525, 83.3525, 86.7475, 86.7475, 87.625, 87.625, 90.68, 90.68, 90.8075, 90.8075, 91.0075, 91.0075, 91.73, 91.73, 92.04, 92.04, 92.5, 92.5, 92.8275, 92.8275, 93.02, 93.02, 93.27, 93.27, 93.4425, 93.4425, 93.59, 93.59, 93.715, 93.715, 94.1075, 94.1075, 94.27, 94.27, 94.38, 94.38, 94.5025, 94.5025, 94.5775, 94.5775, 94.6375, 94.6375, 94.7325, 94.7325, 94.785, 94.785, 94.9175, 94.9175, 95.0625, 95.0625, 95.13, 95.13, 95.1525, 95.1525, 95.1475, 95.1475, 95.1975, 95.1975, 95.2525, 95.2525, 95.3425, 95.3425, 95.4325, 95.4325, 95.515, 95.515, 95.5425, 95.5425, 95.5725, 95.5725, 95.6875, 95.6875, 95.7225, 95.7225, 95.7425, 95.7425, 95.835, 95.835, 95.9, 95.9, 95.9625, 95.9625, 95.9975, 95.9975, 96.02, 96.02, 96.0075, 96.0075, 96.065, 96.065, 96.0675, 96.0675, 96.08, 96.08, 96.115, 96.115, 96.145, 96.145, 96.2025, 96.2025, 96.2375, 96.2375, 96.2375, 96.2375, 96.275, 96.275, 96.285, 96.285, 96.3225, 96.3225, 96.3175, 96.3175, 96.3225, 96.3225, 96.325, 96.325, 96.365, 96.365, 96.36, 96.36, 96.445, 96.445, 96.4475, 96.4475, 96.44, 96.44, 96.4675, 96.4675, 96.47, 96.47, 96.47, 96.47, 96.4475, 96.4475, 96.4875, 96.4875, 96.51, 96.51, 96.5025, 96.5025, 96.54, 96.54, 96.5725, 96.5725, 96.5475, 96.5475, 96.53, 96.53, 96.555, 96.555, 96.575, 96.575, 96.59, 96.59, 96.5875, 96.5875, 96.59, 96.59, 96.6025, 96.6025, 96.64, 96.64, 96.655, 96.655, 96.655, 96.655, 96.6825, 96.6825, 96.665, 96.665, 96.655, 96.655, 96.6625, 96.6625, 96.6875, 96.6875, 96.6825, 96.6825, 96.695, 96.695, 96.6875, 96.6875, 96.6925, 96.6925, 96.6925, 96.6925, 96.685, 96.685, 96.7075, 96.7075, 96.7225, 96.7225, 96.7425, 96.7425, 96.755, 96.755]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.294, Test accuracy: 28.71
Round   1, Train loss: 2.281, Test loss: 2.228, Test accuracy: 33.28
Round   2, Train loss: 2.095, Test loss: 1.928, Test accuracy: 63.74
Round   3, Train loss: 1.799, Test loss: 1.822, Test accuracy: 67.82
Round   4, Train loss: 1.762, Test loss: 1.770, Test accuracy: 71.15
Round   5, Train loss: 1.738, Test loss: 1.751, Test accuracy: 72.26
Round   6, Train loss: 1.723, Test loss: 1.742, Test accuracy: 72.92
Round   7, Train loss: 1.723, Test loss: 1.731, Test accuracy: 73.68
Round   8, Train loss: 1.716, Test loss: 1.728, Test accuracy: 73.81
Round   9, Train loss: 1.716, Test loss: 1.718, Test accuracy: 74.71
Round  10, Train loss: 1.698, Test loss: 1.714, Test accuracy: 75.06
Round  11, Train loss: 1.682, Test loss: 1.709, Test accuracy: 75.62
Round  12, Train loss: 1.681, Test loss: 1.705, Test accuracy: 76.08
Round  13, Train loss: 1.676, Test loss: 1.701, Test accuracy: 76.34
Round  14, Train loss: 1.675, Test loss: 1.696, Test accuracy: 76.84
Round  15, Train loss: 1.658, Test loss: 1.689, Test accuracy: 77.50
Round  16, Train loss: 1.678, Test loss: 1.688, Test accuracy: 77.65
Round  17, Train loss: 1.652, Test loss: 1.684, Test accuracy: 77.95
Round  18, Train loss: 1.648, Test loss: 1.688, Test accuracy: 77.47
Round  19, Train loss: 1.614, Test loss: 1.671, Test accuracy: 79.15
Round  20, Train loss: 1.610, Test loss: 1.648, Test accuracy: 81.64
Round  21, Train loss: 1.601, Test loss: 1.640, Test accuracy: 82.47
Round  22, Train loss: 1.606, Test loss: 1.631, Test accuracy: 83.33
Round  23, Train loss: 1.594, Test loss: 1.630, Test accuracy: 83.35
Round  24, Train loss: 1.599, Test loss: 1.625, Test accuracy: 83.91
Round  25, Train loss: 1.599, Test loss: 1.620, Test accuracy: 84.48
Round  26, Train loss: 1.597, Test loss: 1.618, Test accuracy: 84.65
Round  27, Train loss: 1.591, Test loss: 1.617, Test accuracy: 84.68
Round  28, Train loss: 1.593, Test loss: 1.617, Test accuracy: 84.77
Round  29, Train loss: 1.592, Test loss: 1.616, Test accuracy: 84.74
Round  30, Train loss: 1.592, Test loss: 1.615, Test accuracy: 84.85
Round  31, Train loss: 1.590, Test loss: 1.614, Test accuracy: 84.93
Round  32, Train loss: 1.591, Test loss: 1.614, Test accuracy: 84.99
Round  33, Train loss: 1.588, Test loss: 1.615, Test accuracy: 84.89
Round  34, Train loss: 1.588, Test loss: 1.614, Test accuracy: 84.92
Round  35, Train loss: 1.587, Test loss: 1.612, Test accuracy: 85.02
Round  36, Train loss: 1.587, Test loss: 1.611, Test accuracy: 85.14
Round  37, Train loss: 1.585, Test loss: 1.611, Test accuracy: 85.21
Round  38, Train loss: 1.586, Test loss: 1.611, Test accuracy: 85.22
Round  39, Train loss: 1.581, Test loss: 1.611, Test accuracy: 85.14
Round  40, Train loss: 1.584, Test loss: 1.610, Test accuracy: 85.20
Round  41, Train loss: 1.583, Test loss: 1.610, Test accuracy: 85.25
Round  42, Train loss: 1.579, Test loss: 1.609, Test accuracy: 85.30
Round  43, Train loss: 1.581, Test loss: 1.609, Test accuracy: 85.37
Round  44, Train loss: 1.584, Test loss: 1.609, Test accuracy: 85.33
Round  45, Train loss: 1.583, Test loss: 1.608, Test accuracy: 85.47
Round  46, Train loss: 1.580, Test loss: 1.607, Test accuracy: 85.53
Round  47, Train loss: 1.579, Test loss: 1.607, Test accuracy: 85.47
Round  48, Train loss: 1.578, Test loss: 1.607, Test accuracy: 85.52
Round  49, Train loss: 1.578, Test loss: 1.607, Test accuracy: 85.45
Round  50, Train loss: 1.579, Test loss: 1.607, Test accuracy: 85.52
Round  51, Train loss: 1.579, Test loss: 1.605, Test accuracy: 85.71
Round  52, Train loss: 1.579, Test loss: 1.606, Test accuracy: 85.68
Round  53, Train loss: 1.579, Test loss: 1.605, Test accuracy: 85.69
Round  54, Train loss: 1.578, Test loss: 1.604, Test accuracy: 85.83
Round  55, Train loss: 1.579, Test loss: 1.604, Test accuracy: 85.80
Round  56, Train loss: 1.578, Test loss: 1.604, Test accuracy: 85.76
Round  57, Train loss: 1.578, Test loss: 1.604, Test accuracy: 85.84
Round  58, Train loss: 1.578, Test loss: 1.603, Test accuracy: 85.87
Round  59, Train loss: 1.578, Test loss: 1.603, Test accuracy: 85.83
Round  60, Train loss: 1.576, Test loss: 1.603, Test accuracy: 85.91
Round  61, Train loss: 1.576, Test loss: 1.603, Test accuracy: 85.93
Round  62, Train loss: 1.575, Test loss: 1.603, Test accuracy: 85.93
Round  63, Train loss: 1.577, Test loss: 1.602, Test accuracy: 85.94
Round  64, Train loss: 1.574, Test loss: 1.602, Test accuracy: 85.95
Round  65, Train loss: 1.575, Test loss: 1.602, Test accuracy: 85.98
Round  66, Train loss: 1.574, Test loss: 1.602, Test accuracy: 85.95
Round  67, Train loss: 1.574, Test loss: 1.602, Test accuracy: 85.95
Round  68, Train loss: 1.575, Test loss: 1.602, Test accuracy: 85.92
Round  69, Train loss: 1.573, Test loss: 1.602, Test accuracy: 86.02
Round  70, Train loss: 1.573, Test loss: 1.601, Test accuracy: 86.03
Round  71, Train loss: 1.572, Test loss: 1.601, Test accuracy: 86.04
Round  72, Train loss: 1.573, Test loss: 1.601, Test accuracy: 86.02
Round  73, Train loss: 1.575, Test loss: 1.601, Test accuracy: 86.03
Round  74, Train loss: 1.572, Test loss: 1.600, Test accuracy: 86.09
Round  75, Train loss: 1.573, Test loss: 1.600, Test accuracy: 86.08
Round  76, Train loss: 1.572, Test loss: 1.600, Test accuracy: 86.08
Round  77, Train loss: 1.571, Test loss: 1.601, Test accuracy: 86.08
Round  78, Train loss: 1.573, Test loss: 1.600, Test accuracy: 86.14
Round  79, Train loss: 1.573, Test loss: 1.600, Test accuracy: 86.18
Round  80, Train loss: 1.571, Test loss: 1.600, Test accuracy: 86.15
Round  81, Train loss: 1.572, Test loss: 1.599, Test accuracy: 86.26
Round  82, Train loss: 1.572, Test loss: 1.599, Test accuracy: 86.21
Round  83, Train loss: 1.570, Test loss: 1.599, Test accuracy: 86.22
Round  84, Train loss: 1.570, Test loss: 1.598, Test accuracy: 86.29
Round  85, Train loss: 1.572, Test loss: 1.598, Test accuracy: 86.28
Round  86, Train loss: 1.569, Test loss: 1.598, Test accuracy: 86.33
Round  87, Train loss: 1.570, Test loss: 1.598, Test accuracy: 86.34
Round  88, Train loss: 1.571, Test loss: 1.598, Test accuracy: 86.36
Round  89, Train loss: 1.569, Test loss: 1.597, Test accuracy: 86.37
Round  90, Train loss: 1.570, Test loss: 1.597, Test accuracy: 86.41
Round  91, Train loss: 1.569, Test loss: 1.597, Test accuracy: 86.39
Round  92, Train loss: 1.568, Test loss: 1.597, Test accuracy: 86.42
Round  93, Train loss: 1.570, Test loss: 1.597, Test accuracy: 86.40
Round  94, Train loss: 1.568, Test loss: 1.597, Test accuracy: 86.36
Round  95, Train loss: 1.571, Test loss: 1.598, Test accuracy: 86.37
Round  96, Train loss: 1.569, Test loss: 1.597, Test accuracy: 86.40
Round  97, Train loss: 1.569, Test loss: 1.597, Test accuracy: 86.46
Round  98, Train loss: 1.569, Test loss: 1.596, Test accuracy: 86.44/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.570, Test loss: 1.596, Test accuracy: 86.40
Final Round, Train loss: 1.571, Test loss: 1.597, Test accuracy: 86.45
Average accuracy final 10 rounds: 86.40275 

3338.256989955902
[2.8020429611206055, 5.604085922241211, 8.32837986946106, 11.052673816680908, 13.776464462280273, 16.50025510787964, 19.123295783996582, 21.746336460113525, 24.378559112548828, 27.01078176498413, 29.613616943359375, 32.21645212173462, 34.84178376197815, 37.46711540222168, 40.089410066604614, 42.71170473098755, 45.33640456199646, 47.96110439300537, 50.58004450798035, 53.19898462295532, 55.81937766075134, 58.43977069854736, 61.05978298187256, 63.679795265197754, 66.28640365600586, 68.89301204681396, 71.5281593799591, 74.16330671310425, 76.79229974746704, 79.42129278182983, 82.03890323638916, 84.65651369094849, 87.31981182098389, 89.98310995101929, 92.63265419006348, 95.28219842910767, 97.9387936592102, 100.59538888931274, 103.24884486198425, 105.90230083465576, 108.56680655479431, 111.23131227493286, 113.89916348457336, 116.56701469421387, 119.19858717918396, 121.83015966415405, 124.48048853874207, 127.13081741333008, 129.7492332458496, 132.36764907836914, 134.9704065322876, 137.57316398620605, 140.21137595176697, 142.84958791732788, 145.46317100524902, 148.07675409317017, 150.70423865318298, 153.3317232131958, 156.0037407875061, 158.6757583618164, 161.3028221130371, 163.9298858642578, 166.59053230285645, 169.25117874145508, 171.88053154945374, 174.5098843574524, 177.11783695220947, 179.72578954696655, 182.33920049667358, 184.95261144638062, 187.56319999694824, 190.17378854751587, 192.85145688056946, 195.52912521362305, 198.141934633255, 200.75474405288696, 203.4010672569275, 206.04739046096802, 208.7118580341339, 211.3763256072998, 213.9829020500183, 216.58947849273682, 219.23018836975098, 221.87089824676514, 224.4762246608734, 227.0815510749817, 229.71573567390442, 232.34992027282715, 234.98834347724915, 237.62676668167114, 240.2039589881897, 242.78115129470825, 245.44197344779968, 248.1027956008911, 250.70122361183167, 253.29965162277222, 255.95538115501404, 258.61111068725586, 261.246080160141, 263.8810496330261, 266.49080514907837, 269.1005606651306, 271.7713589668274, 274.44215726852417, 277.06390929222107, 279.68566131591797, 282.3081088066101, 284.93055629730225, 287.74949645996094, 290.56843662261963, 293.18011379241943, 295.79179096221924, 298.4138026237488, 301.0358142852783, 303.62417817115784, 306.21254205703735, 308.8407094478607, 311.4688768386841, 314.0848948955536, 316.7009129524231, 319.31087589263916, 321.9208388328552, 324.58201479911804, 327.24319076538086, 329.85401725769043, 332.46484375, 335.1200430393219, 337.7752423286438, 340.41627836227417, 343.05731439590454, 345.71217226982117, 348.3670301437378, 350.9964644908905, 353.6258988380432, 356.23865604400635, 358.8514132499695, 361.4696502685547, 364.0878872871399, 366.73083233833313, 369.37377738952637, 372.0239760875702, 374.674174785614, 377.3155183792114, 379.95686197280884, 382.5816020965576, 385.2063422203064, 387.83903431892395, 390.4717264175415, 393.0778431892395, 395.6839599609375, 398.3131513595581, 400.9423427581787, 403.5800049304962, 406.2176671028137, 408.82757210731506, 411.4374771118164, 414.07729172706604, 416.7171063423157, 419.31665420532227, 421.91620206832886, 424.53948950767517, 427.1627769470215, 429.79170513153076, 432.42063331604004, 435.03246569633484, 437.64429807662964, 440.2683596611023, 442.89242124557495, 445.50175404548645, 448.11108684539795, 450.7466197013855, 453.38215255737305, 456.01081466674805, 458.63947677612305, 461.27522015571594, 463.91096353530884, 466.5448603630066, 469.17875719070435, 471.80964851379395, 474.44053983688354, 477.1097948551178, 479.77904987335205, 482.42636227607727, 485.0736746788025, 487.7193658351898, 490.36505699157715, 492.97451543807983, 495.5839738845825, 498.2322759628296, 500.88057804107666, 503.4741711616516, 506.06776428222656, 508.67131757736206, 511.27487087249756, 513.917994260788, 516.5611176490784, 519.2095336914062, 521.8579497337341, 524.5139422416687, 527.1699347496033, 528.4333333969116, 529.69673204422]
[28.7075, 28.7075, 33.285, 33.285, 63.74, 63.74, 67.8225, 67.8225, 71.15, 71.15, 72.26, 72.26, 72.9175, 72.9175, 73.68, 73.68, 73.8125, 73.8125, 74.7125, 74.7125, 75.0625, 75.0625, 75.625, 75.625, 76.075, 76.075, 76.3375, 76.3375, 76.84, 76.84, 77.505, 77.505, 77.6525, 77.6525, 77.955, 77.955, 77.475, 77.475, 79.1525, 79.1525, 81.635, 81.635, 82.475, 82.475, 83.33, 83.33, 83.3525, 83.3525, 83.9075, 83.9075, 84.4775, 84.4775, 84.6475, 84.6475, 84.68, 84.68, 84.7725, 84.7725, 84.74, 84.74, 84.85, 84.85, 84.9325, 84.9325, 84.99, 84.99, 84.885, 84.885, 84.925, 84.925, 85.02, 85.02, 85.145, 85.145, 85.2075, 85.2075, 85.225, 85.225, 85.1425, 85.1425, 85.205, 85.205, 85.255, 85.255, 85.3025, 85.3025, 85.37, 85.37, 85.3325, 85.3325, 85.475, 85.475, 85.535, 85.535, 85.475, 85.475, 85.5225, 85.5225, 85.4525, 85.4525, 85.52, 85.52, 85.7125, 85.7125, 85.68, 85.68, 85.6925, 85.6925, 85.8275, 85.8275, 85.7975, 85.7975, 85.7625, 85.7625, 85.8425, 85.8425, 85.87, 85.87, 85.8325, 85.8325, 85.91, 85.91, 85.9325, 85.9325, 85.9325, 85.9325, 85.935, 85.935, 85.955, 85.955, 85.98, 85.98, 85.95, 85.95, 85.955, 85.955, 85.9175, 85.9175, 86.02, 86.02, 86.035, 86.035, 86.04, 86.04, 86.0225, 86.0225, 86.03, 86.03, 86.0875, 86.0875, 86.085, 86.085, 86.075, 86.075, 86.0825, 86.0825, 86.1425, 86.1425, 86.18, 86.18, 86.1475, 86.1475, 86.26, 86.26, 86.2125, 86.2125, 86.22, 86.22, 86.29, 86.29, 86.2775, 86.2775, 86.33, 86.33, 86.3375, 86.3375, 86.36, 86.36, 86.3725, 86.3725, 86.4075, 86.4075, 86.385, 86.385, 86.415, 86.415, 86.4, 86.4, 86.355, 86.355, 86.37, 86.37, 86.4, 86.4, 86.4575, 86.4575, 86.44, 86.44, 86.3975, 86.3975, 86.4525, 86.4525]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.290, Test loss: 2.241, Test accuracy: 29.38
Round   1, Train loss: 1.965, Test loss: 1.821, Test accuracy: 68.35
Round   2, Train loss: 1.727, Test loss: 1.774, Test accuracy: 70.77
Round   3, Train loss: 1.712, Test loss: 1.740, Test accuracy: 73.24
Round   4, Train loss: 1.705, Test loss: 1.725, Test accuracy: 74.33
Round   5, Train loss: 1.702, Test loss: 1.709, Test accuracy: 75.47
Round   6, Train loss: 1.695, Test loss: 1.706, Test accuracy: 75.64
Round   7, Train loss: 1.691, Test loss: 1.705, Test accuracy: 75.77
Round   8, Train loss: 1.689, Test loss: 1.704, Test accuracy: 75.83
Round   9, Train loss: 1.687, Test loss: 1.702, Test accuracy: 76.06
Round  10, Train loss: 1.684, Test loss: 1.701, Test accuracy: 76.04
Round  11, Train loss: 1.683, Test loss: 1.700, Test accuracy: 76.12
Round  12, Train loss: 1.679, Test loss: 1.699, Test accuracy: 76.24
Round  13, Train loss: 1.677, Test loss: 1.697, Test accuracy: 76.44
Round  14, Train loss: 1.677, Test loss: 1.697, Test accuracy: 76.43
Round  15, Train loss: 1.678, Test loss: 1.695, Test accuracy: 76.59
Round  16, Train loss: 1.674, Test loss: 1.694, Test accuracy: 76.66
Round  17, Train loss: 1.667, Test loss: 1.692, Test accuracy: 77.00
Round  18, Train loss: 1.663, Test loss: 1.690, Test accuracy: 77.12
Round  19, Train loss: 1.648, Test loss: 1.677, Test accuracy: 78.56
Round  20, Train loss: 1.658, Test loss: 1.669, Test accuracy: 79.43
Round  21, Train loss: 1.633, Test loss: 1.658, Test accuracy: 80.48
Round  22, Train loss: 1.609, Test loss: 1.644, Test accuracy: 82.00
Round  23, Train loss: 1.595, Test loss: 1.636, Test accuracy: 82.72
Round  24, Train loss: 1.589, Test loss: 1.630, Test accuracy: 83.34
Round  25, Train loss: 1.591, Test loss: 1.618, Test accuracy: 84.56
Round  26, Train loss: 1.582, Test loss: 1.615, Test accuracy: 84.78
Round  27, Train loss: 1.582, Test loss: 1.609, Test accuracy: 85.33
Round  28, Train loss: 1.579, Test loss: 1.608, Test accuracy: 85.38
Round  29, Train loss: 1.577, Test loss: 1.606, Test accuracy: 85.59
Round  30, Train loss: 1.576, Test loss: 1.605, Test accuracy: 85.65
Round  31, Train loss: 1.576, Test loss: 1.604, Test accuracy: 85.78
Round  32, Train loss: 1.576, Test loss: 1.603, Test accuracy: 85.87
Round  33, Train loss: 1.572, Test loss: 1.603, Test accuracy: 85.89
Round  34, Train loss: 1.574, Test loss: 1.598, Test accuracy: 86.40
Round  35, Train loss: 1.573, Test loss: 1.598, Test accuracy: 86.41
Round  36, Train loss: 1.571, Test loss: 1.596, Test accuracy: 86.52
Round  37, Train loss: 1.570, Test loss: 1.596, Test accuracy: 86.61
Round  38, Train loss: 1.570, Test loss: 1.595, Test accuracy: 86.65
Round  39, Train loss: 1.570, Test loss: 1.595, Test accuracy: 86.67
Round  40, Train loss: 1.569, Test loss: 1.595, Test accuracy: 86.66
Round  41, Train loss: 1.570, Test loss: 1.595, Test accuracy: 86.63
Round  42, Train loss: 1.569, Test loss: 1.594, Test accuracy: 86.73
Round  43, Train loss: 1.569, Test loss: 1.594, Test accuracy: 86.76
Round  44, Train loss: 1.568, Test loss: 1.594, Test accuracy: 86.74
Round  45, Train loss: 1.570, Test loss: 1.593, Test accuracy: 86.86
Round  46, Train loss: 1.569, Test loss: 1.592, Test accuracy: 86.93
Round  47, Train loss: 1.569, Test loss: 1.592, Test accuracy: 86.91
Round  48, Train loss: 1.568, Test loss: 1.592, Test accuracy: 86.89
Round  49, Train loss: 1.567, Test loss: 1.593, Test accuracy: 86.82
Round  50, Train loss: 1.567, Test loss: 1.592, Test accuracy: 86.96
Round  51, Train loss: 1.569, Test loss: 1.591, Test accuracy: 87.02
Round  52, Train loss: 1.567, Test loss: 1.591, Test accuracy: 87.00
Round  53, Train loss: 1.567, Test loss: 1.591, Test accuracy: 87.02
Round  54, Train loss: 1.567, Test loss: 1.591, Test accuracy: 87.03
Round  55, Train loss: 1.568, Test loss: 1.591, Test accuracy: 87.02
Round  56, Train loss: 1.568, Test loss: 1.590, Test accuracy: 87.03
Round  57, Train loss: 1.566, Test loss: 1.590, Test accuracy: 87.09
Round  58, Train loss: 1.567, Test loss: 1.590, Test accuracy: 87.08
Round  59, Train loss: 1.565, Test loss: 1.590, Test accuracy: 87.07
Round  60, Train loss: 1.567, Test loss: 1.590, Test accuracy: 87.03
Round  61, Train loss: 1.565, Test loss: 1.590, Test accuracy: 87.08
Round  62, Train loss: 1.566, Test loss: 1.590, Test accuracy: 87.08
Round  63, Train loss: 1.565, Test loss: 1.590, Test accuracy: 87.06
Round  64, Train loss: 1.565, Test loss: 1.589, Test accuracy: 87.14
Round  65, Train loss: 1.567, Test loss: 1.589, Test accuracy: 87.14
Round  66, Train loss: 1.565, Test loss: 1.589, Test accuracy: 87.16
Round  67, Train loss: 1.565, Test loss: 1.588, Test accuracy: 87.22
Round  68, Train loss: 1.564, Test loss: 1.589, Test accuracy: 87.17
Round  69, Train loss: 1.565, Test loss: 1.589, Test accuracy: 87.19
Round  70, Train loss: 1.564, Test loss: 1.588, Test accuracy: 87.26
Round  71, Train loss: 1.565, Test loss: 1.588, Test accuracy: 87.31
Round  72, Train loss: 1.564, Test loss: 1.588, Test accuracy: 87.25
Round  73, Train loss: 1.565, Test loss: 1.588, Test accuracy: 87.28
Round  74, Train loss: 1.564, Test loss: 1.588, Test accuracy: 87.31
Round  75, Train loss: 1.564, Test loss: 1.587, Test accuracy: 87.31
Round  76, Train loss: 1.565, Test loss: 1.587, Test accuracy: 87.31
Round  77, Train loss: 1.563, Test loss: 1.587, Test accuracy: 87.36
Round  78, Train loss: 1.565, Test loss: 1.587, Test accuracy: 87.35
Round  79, Train loss: 1.564, Test loss: 1.587, Test accuracy: 87.31
Round  80, Train loss: 1.564, Test loss: 1.587, Test accuracy: 87.37
Round  81, Train loss: 1.566, Test loss: 1.587, Test accuracy: 87.34
Round  82, Train loss: 1.563, Test loss: 1.587, Test accuracy: 87.34
Round  83, Train loss: 1.564, Test loss: 1.587, Test accuracy: 87.36
Round  84, Train loss: 1.562, Test loss: 1.587, Test accuracy: 87.34
Round  85, Train loss: 1.562, Test loss: 1.587, Test accuracy: 87.34
Round  86, Train loss: 1.564, Test loss: 1.587, Test accuracy: 87.36
Round  87, Train loss: 1.563, Test loss: 1.587, Test accuracy: 87.38
Round  88, Train loss: 1.564, Test loss: 1.587, Test accuracy: 87.36
Round  89, Train loss: 1.563, Test loss: 1.586, Test accuracy: 87.41
Round  90, Train loss: 1.564, Test loss: 1.586, Test accuracy: 87.43
Round  91, Train loss: 1.563, Test loss: 1.586, Test accuracy: 87.46
Round  92, Train loss: 1.563, Test loss: 1.586, Test accuracy: 87.39
Round  93, Train loss: 1.563, Test loss: 1.586, Test accuracy: 87.41
Round  94, Train loss: 1.563, Test loss: 1.586, Test accuracy: 87.38
Round  95, Train loss: 1.563, Test loss: 1.586, Test accuracy: 87.42
Round  96, Train loss: 1.563, Test loss: 1.586, Test accuracy: 87.43
Round  97, Train loss: 1.563, Test loss: 1.586, Test accuracy: 87.42
Round  98, Train loss: 1.564, Test loss: 1.586, Test accuracy: 87.43/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.563, Test loss: 1.586, Test accuracy: 87.47
Final Round, Train loss: 1.560, Test loss: 1.586, Test accuracy: 87.39
Average accuracy final 10 rounds: 87.423 

3524.1901483535767
[3.10603666305542, 6.21207332611084, 9.077661275863647, 11.943249225616455, 14.7765793800354, 17.609909534454346, 20.501338958740234, 23.392768383026123, 26.2172110080719, 29.041653633117676, 31.844585418701172, 34.64751720428467, 37.47498154640198, 40.30244588851929, 43.07740330696106, 45.85236072540283, 48.69369173049927, 51.5350227355957, 54.343679666519165, 57.15233659744263, 59.94292950630188, 62.73352241516113, 65.56475400924683, 68.39598560333252, 71.17178344726562, 73.94758129119873, 76.76670336723328, 79.58582544326782, 82.3969783782959, 85.20813131332397, 88.01978850364685, 90.83144569396973, 93.73169112205505, 96.63193655014038, 99.41781806945801, 102.20369958877563, 105.03926801681519, 107.87483644485474, 110.7009060382843, 113.52697563171387, 116.35172462463379, 119.17647361755371, 122.17248249053955, 125.16849136352539, 128.10847902297974, 131.04846668243408, 133.89875054359436, 136.74903440475464, 139.6009430885315, 142.45285177230835, 145.3092324733734, 148.16561317443848, 151.06658458709717, 153.96755599975586, 156.7846531867981, 159.60175037384033, 162.45550274848938, 165.30925512313843, 168.1907148361206, 171.07217454910278, 173.84040594100952, 176.60863733291626, 179.41743683815002, 182.2262363433838, 184.98957204818726, 187.75290775299072, 190.5645875930786, 193.3762674331665, 196.19169187545776, 199.00711631774902, 201.86972665786743, 204.73233699798584, 207.54991555213928, 210.36749410629272, 213.170428276062, 215.9733624458313, 218.85455513000488, 221.73574781417847, 224.60397624969482, 227.47220468521118, 230.29372882843018, 233.11525297164917, 235.96063542366028, 238.8060178756714, 241.58785486221313, 244.36969184875488, 247.2352795600891, 250.10086727142334, 252.92873549461365, 255.75660371780396, 258.58478569984436, 261.41296768188477, 264.20693588256836, 267.00090408325195, 269.7582051753998, 272.5155062675476, 275.30887055397034, 278.10223484039307, 280.8909635543823, 283.6796922683716, 286.56664967536926, 289.45360708236694, 292.2848834991455, 295.1161599159241, 298.0895359516144, 301.0629119873047, 303.9806740283966, 306.8984360694885, 309.8090777397156, 312.7197194099426, 315.6363835334778, 318.55304765701294, 321.4315376281738, 324.3100275993347, 327.08264803886414, 329.85526847839355, 332.67202639579773, 335.4887843132019, 338.46299028396606, 341.4371962547302, 344.3640413284302, 347.2908864021301, 350.2001302242279, 353.1093740463257, 356.0049874782562, 358.90060091018677, 361.9180462360382, 364.93549156188965, 367.8902575969696, 370.84502363204956, 373.7597327232361, 376.6744418144226, 379.49798250198364, 382.3215231895447, 385.2788586616516, 388.23619413375854, 391.15211296081543, 394.0680317878723, 396.97584652900696, 399.8836612701416, 402.8601758480072, 405.8366904258728, 408.684130191803, 411.53156995773315, 414.6937999725342, 417.8560299873352, 420.9123954772949, 423.96876096725464, 426.7397150993347, 429.5106692314148, 432.3108015060425, 435.11093378067017, 437.92018866539, 440.72944355010986, 443.53560304641724, 446.3417625427246, 449.13538455963135, 451.9290065765381, 454.68862867355347, 457.44825077056885, 460.24662613868713, 463.0450015068054, 465.81079602241516, 468.5765905380249, 471.367972612381, 474.15935468673706, 476.9291250705719, 479.69889545440674, 482.4772334098816, 485.25557136535645, 488.0459084510803, 490.8362455368042, 493.6015317440033, 496.3668179512024, 499.40273571014404, 502.4386534690857, 505.44897198677063, 508.45929050445557, 511.46052646636963, 514.4617624282837, 517.5160858631134, 520.5704092979431, 523.6624953746796, 526.754581451416, 529.8461968898773, 532.9378123283386, 536.0800080299377, 539.2222037315369, 542.0055258274078, 544.7888479232788, 547.607171535492, 550.4254951477051, 553.2380909919739, 556.0506868362427, 558.8676514625549, 561.6846160888672, 564.4894852638245, 567.2943544387817, 570.0548210144043, 572.8152875900269, 574.1205065250397, 575.4257254600525]
[29.38, 29.38, 68.3525, 68.3525, 70.7675, 70.7675, 73.24, 73.24, 74.325, 74.325, 75.4675, 75.4675, 75.6375, 75.6375, 75.765, 75.765, 75.835, 75.835, 76.055, 76.055, 76.0425, 76.0425, 76.1175, 76.1175, 76.2425, 76.2425, 76.4425, 76.4425, 76.43, 76.43, 76.5925, 76.5925, 76.6625, 76.6625, 76.9975, 76.9975, 77.125, 77.125, 78.5625, 78.5625, 79.43, 79.43, 80.4825, 80.4825, 81.9975, 81.9975, 82.7225, 82.7225, 83.3425, 83.3425, 84.56, 84.56, 84.7825, 84.7825, 85.335, 85.335, 85.3775, 85.3775, 85.5925, 85.5925, 85.6475, 85.6475, 85.78, 85.78, 85.87, 85.87, 85.885, 85.885, 86.4, 86.4, 86.41, 86.41, 86.52, 86.52, 86.605, 86.605, 86.6525, 86.6525, 86.6675, 86.6675, 86.655, 86.655, 86.6275, 86.6275, 86.7275, 86.7275, 86.76, 86.76, 86.7425, 86.7425, 86.86, 86.86, 86.9275, 86.9275, 86.9075, 86.9075, 86.885, 86.885, 86.8225, 86.8225, 86.9575, 86.9575, 87.015, 87.015, 87.005, 87.005, 87.0225, 87.0225, 87.0325, 87.0325, 87.0175, 87.0175, 87.035, 87.035, 87.095, 87.095, 87.08, 87.08, 87.07, 87.07, 87.03, 87.03, 87.0775, 87.0775, 87.08, 87.08, 87.055, 87.055, 87.14, 87.14, 87.145, 87.145, 87.1625, 87.1625, 87.22, 87.22, 87.1675, 87.1675, 87.185, 87.185, 87.2625, 87.2625, 87.3075, 87.3075, 87.245, 87.245, 87.2825, 87.2825, 87.3075, 87.3075, 87.305, 87.305, 87.3075, 87.3075, 87.355, 87.355, 87.35, 87.35, 87.3125, 87.3125, 87.3675, 87.3675, 87.3375, 87.3375, 87.34, 87.34, 87.365, 87.365, 87.345, 87.345, 87.34, 87.34, 87.3625, 87.3625, 87.3825, 87.3825, 87.355, 87.355, 87.4125, 87.4125, 87.4325, 87.4325, 87.4575, 87.4575, 87.39, 87.39, 87.4125, 87.4125, 87.3825, 87.3825, 87.415, 87.415, 87.4275, 87.4275, 87.415, 87.415, 87.43, 87.43, 87.4675, 87.4675, 87.395, 87.395]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.288, Test loss: 2.232, Test accuracy: 38.66
Round   1, Train loss: 1.912, Test loss: 1.817, Test accuracy: 73.20
Round   2, Train loss: 1.648, Test loss: 1.678, Test accuracy: 83.82
Round   3, Train loss: 1.584, Test loss: 1.634, Test accuracy: 86.19
Round   4, Train loss: 1.540, Test loss: 1.626, Test accuracy: 86.40
Round   5, Train loss: 1.558, Test loss: 1.587, Test accuracy: 88.20
Round   6, Train loss: 1.534, Test loss: 1.577, Test accuracy: 89.00
Round   7, Train loss: 1.513, Test loss: 1.576, Test accuracy: 89.06
Round   8, Train loss: 1.510, Test loss: 1.574, Test accuracy: 89.19
Round   9, Train loss: 1.510, Test loss: 1.573, Test accuracy: 89.25
Round  10, Train loss: 1.504, Test loss: 1.571, Test accuracy: 89.41
Round  11, Train loss: 1.497, Test loss: 1.570, Test accuracy: 89.46
Round  12, Train loss: 1.496, Test loss: 1.569, Test accuracy: 89.52
Round  13, Train loss: 1.489, Test loss: 1.569, Test accuracy: 89.49
Round  14, Train loss: 1.495, Test loss: 1.569, Test accuracy: 89.50
Round  15, Train loss: 1.491, Test loss: 1.568, Test accuracy: 89.58
Round  16, Train loss: 1.491, Test loss: 1.568, Test accuracy: 89.62
Round  17, Train loss: 1.492, Test loss: 1.568, Test accuracy: 89.53
Round  18, Train loss: 1.492, Test loss: 1.568, Test accuracy: 89.56
Round  19, Train loss: 1.489, Test loss: 1.567, Test accuracy: 89.66
Round  20, Train loss: 1.486, Test loss: 1.567, Test accuracy: 89.67
Round  21, Train loss: 1.486, Test loss: 1.567, Test accuracy: 89.72
Round  22, Train loss: 1.489, Test loss: 1.566, Test accuracy: 89.75
Round  23, Train loss: 1.492, Test loss: 1.566, Test accuracy: 89.73
Round  24, Train loss: 1.487, Test loss: 1.566, Test accuracy: 89.68
Round  25, Train loss: 1.490, Test loss: 1.566, Test accuracy: 89.75
Round  26, Train loss: 1.492, Test loss: 1.565, Test accuracy: 89.82
Round  27, Train loss: 1.484, Test loss: 1.565, Test accuracy: 89.84
Round  28, Train loss: 1.489, Test loss: 1.565, Test accuracy: 89.80
Round  29, Train loss: 1.487, Test loss: 1.565, Test accuracy: 89.84
Round  30, Train loss: 1.484, Test loss: 1.565, Test accuracy: 89.86
Round  31, Train loss: 1.482, Test loss: 1.565, Test accuracy: 89.82
Round  32, Train loss: 1.486, Test loss: 1.565, Test accuracy: 89.84
Round  33, Train loss: 1.486, Test loss: 1.565, Test accuracy: 89.81
Round  34, Train loss: 1.483, Test loss: 1.565, Test accuracy: 89.83
Round  35, Train loss: 1.483, Test loss: 1.565, Test accuracy: 89.84
Round  36, Train loss: 1.487, Test loss: 1.565, Test accuracy: 89.81
Round  37, Train loss: 1.481, Test loss: 1.565, Test accuracy: 89.85
Round  38, Train loss: 1.484, Test loss: 1.565, Test accuracy: 89.83
Round  39, Train loss: 1.484, Test loss: 1.564, Test accuracy: 89.87
Round  40, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.86
Round  41, Train loss: 1.486, Test loss: 1.565, Test accuracy: 89.83
Round  42, Train loss: 1.487, Test loss: 1.564, Test accuracy: 89.86
Round  43, Train loss: 1.485, Test loss: 1.564, Test accuracy: 89.89
Round  44, Train loss: 1.488, Test loss: 1.564, Test accuracy: 89.88
Round  45, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.90
Round  46, Train loss: 1.484, Test loss: 1.564, Test accuracy: 89.86
Round  47, Train loss: 1.486, Test loss: 1.564, Test accuracy: 89.91
Round  48, Train loss: 1.480, Test loss: 1.564, Test accuracy: 89.91
Round  49, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.92
Round  50, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.91
Round  51, Train loss: 1.485, Test loss: 1.564, Test accuracy: 89.91
Round  52, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.90
Round  53, Train loss: 1.484, Test loss: 1.564, Test accuracy: 89.91
Round  54, Train loss: 1.480, Test loss: 1.564, Test accuracy: 89.88
Round  55, Train loss: 1.484, Test loss: 1.564, Test accuracy: 89.91
Round  56, Train loss: 1.487, Test loss: 1.564, Test accuracy: 89.89
Round  57, Train loss: 1.482, Test loss: 1.564, Test accuracy: 89.90
Round  58, Train loss: 1.486, Test loss: 1.564, Test accuracy: 89.91
Round  59, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.94
Round  60, Train loss: 1.479, Test loss: 1.564, Test accuracy: 89.95
Round  61, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.96
Round  62, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.91
Round  63, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.86
Round  64, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.88
Round  65, Train loss: 1.484, Test loss: 1.564, Test accuracy: 89.87
Round  66, Train loss: 1.484, Test loss: 1.564, Test accuracy: 89.88
Round  67, Train loss: 1.480, Test loss: 1.564, Test accuracy: 89.89
Round  68, Train loss: 1.484, Test loss: 1.564, Test accuracy: 89.91
Round  69, Train loss: 1.485, Test loss: 1.564, Test accuracy: 89.91
Round  70, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.92
Round  71, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.89
Round  72, Train loss: 1.480, Test loss: 1.564, Test accuracy: 89.89
Round  73, Train loss: 1.484, Test loss: 1.564, Test accuracy: 89.90
Round  74, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.91
Round  75, Train loss: 1.482, Test loss: 1.564, Test accuracy: 89.90
Round  76, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.88
Round  77, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.90
Round  78, Train loss: 1.484, Test loss: 1.564, Test accuracy: 89.90
Round  79, Train loss: 1.484, Test loss: 1.564, Test accuracy: 89.90
Round  80, Train loss: 1.482, Test loss: 1.564, Test accuracy: 89.90
Round  81, Train loss: 1.478, Test loss: 1.564, Test accuracy: 89.91
Round  82, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.91
Round  83, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.93
Round  84, Train loss: 1.480, Test loss: 1.564, Test accuracy: 89.95
Round  85, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.94
Round  86, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.94
Round  87, Train loss: 1.485, Test loss: 1.564, Test accuracy: 89.94
Round  88, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.92
Round  89, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.93
Round  90, Train loss: 1.480, Test loss: 1.564, Test accuracy: 89.92
Round  91, Train loss: 1.479, Test loss: 1.564, Test accuracy: 89.90
Round  92, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.90
Round  93, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.92
Round  94, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.91
Round  95, Train loss: 1.483, Test loss: 1.564, Test accuracy: 89.92
Round  96, Train loss: 1.479, Test loss: 1.564, Test accuracy: 89.93
Round  97, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.93
Round  98, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.94
Round  99, Train loss: 1.481, Test loss: 1.564, Test accuracy: 89.94/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.480, Test loss: 1.564, Test accuracy: 89.91
Average accuracy final 10 rounds: 89.9205 

3402.4963562488556
[3.03427791595459, 6.06855583190918, 8.769127368927002, 11.469698905944824, 14.208804368972778, 16.947909832000732, 19.6796875, 22.411465167999268, 25.135878086090088, 27.860291004180908, 30.61587953567505, 33.37146806716919, 36.08514475822449, 38.798821449279785, 41.56956386566162, 44.34030628204346, 47.06748723983765, 49.794668197631836, 52.56212759017944, 55.32958698272705, 58.09426832199097, 60.85894966125488, 63.5870099067688, 66.31507015228271, 69.09626412391663, 71.87745809555054, 74.61180758476257, 77.34615707397461, 80.12570452690125, 82.90525197982788, 85.65933036804199, 88.4134087562561, 91.16986012458801, 93.92631149291992, 96.68357396125793, 99.44083642959595, 102.14643812179565, 104.85203981399536, 107.6163694858551, 110.38069915771484, 113.07248377799988, 115.76426839828491, 118.51965641975403, 121.27504444122314, 124.02776026725769, 126.78047609329224, 129.4866144657135, 132.19275283813477, 134.94949412345886, 137.70623540878296, 140.3874180316925, 143.06860065460205, 145.83828735351562, 148.6079740524292, 151.34944820404053, 154.09092235565186, 156.83410930633545, 159.57729625701904, 162.35589289665222, 165.1344895362854, 167.85808944702148, 170.58168935775757, 173.34002256393433, 176.09835577011108, 178.83674383163452, 181.57513189315796, 184.34365892410278, 187.1121859550476, 189.91744780540466, 192.72270965576172, 195.45380640029907, 198.18490314483643, 200.95224046707153, 203.71957778930664, 206.45105028152466, 209.18252277374268, 211.97371435165405, 214.76490592956543, 217.5396327972412, 220.314359664917, 223.0879831314087, 225.8616065979004, 228.65777206420898, 231.45393753051758, 234.21899032592773, 236.9840431213379, 239.78038263320923, 242.57672214508057, 245.33656692504883, 248.0964117050171, 250.8958933353424, 253.69537496566772, 256.5252320766449, 259.35508918762207, 262.0746030807495, 264.79411697387695, 267.59275698661804, 270.39139699935913, 273.1302011013031, 275.86900520324707, 278.63974809646606, 281.41049098968506, 284.16770005226135, 286.92490911483765, 289.7011625766754, 292.4774160385132, 295.25266456604004, 298.0279130935669, 300.78104424476624, 303.5341753959656, 306.31832551956177, 309.10247564315796, 311.8895494937897, 314.6766233444214, 317.4929699897766, 320.30931663513184, 323.0832550525665, 325.8571934700012, 328.6050102710724, 331.35282707214355, 334.1168942451477, 336.88096141815186, 339.6326403617859, 342.3843193054199, 345.12640261650085, 347.8684859275818, 350.6269178390503, 353.3853497505188, 356.13867259025574, 358.8919954299927, 361.6586916446686, 364.4253878593445, 367.1414940357208, 369.85760021209717, 372.62506079673767, 375.3925213813782, 378.1078624725342, 380.8232035636902, 383.5306124687195, 386.2380213737488, 388.9966824054718, 391.7553434371948, 394.4684422016144, 397.18154096603394, 399.9149956703186, 402.64845037460327, 405.40449023246765, 408.16053009033203, 410.9174225330353, 413.6743149757385, 416.39027762413025, 419.106240272522, 421.85893726348877, 424.61163425445557, 427.3748047351837, 430.13797521591187, 432.8580017089844, 435.5780282020569, 438.3187074661255, 441.0593867301941, 443.8006293773651, 446.54187202453613, 449.277058839798, 452.0122456550598, 454.7584857940674, 457.50472593307495, 460.27223467826843, 463.0397434234619, 465.7980155944824, 468.55628776550293, 471.28291034698486, 474.0095329284668, 476.73295426368713, 479.45637559890747, 482.1777069568634, 484.89903831481934, 487.6543047428131, 490.4095711708069, 493.11827278137207, 495.82697439193726, 498.5802800655365, 501.33358573913574, 504.0749580860138, 506.81633043289185, 509.5303256511688, 512.2443208694458, 514.9938251972198, 517.7433295249939, 520.4939165115356, 523.2445034980774, 525.97709608078, 528.7096886634827, 531.455011844635, 534.2003350257874, 536.9438073635101, 539.6872797012329, 542.4289650917053, 545.1706504821777, 547.8987760543823, 550.6269016265869, 551.9320747852325, 553.2372479438782]
[38.6575, 38.6575, 73.1975, 73.1975, 83.8225, 83.8225, 86.185, 86.185, 86.4, 86.4, 88.1975, 88.1975, 89.0025, 89.0025, 89.055, 89.055, 89.19, 89.19, 89.2475, 89.2475, 89.405, 89.405, 89.4575, 89.4575, 89.515, 89.515, 89.4925, 89.4925, 89.4975, 89.4975, 89.58, 89.58, 89.62, 89.62, 89.5325, 89.5325, 89.555, 89.555, 89.655, 89.655, 89.665, 89.665, 89.7225, 89.7225, 89.7475, 89.7475, 89.7275, 89.7275, 89.6775, 89.6775, 89.7475, 89.7475, 89.8225, 89.8225, 89.8375, 89.8375, 89.7975, 89.7975, 89.845, 89.845, 89.8575, 89.8575, 89.8225, 89.8225, 89.845, 89.845, 89.815, 89.815, 89.83, 89.83, 89.8375, 89.8375, 89.8125, 89.8125, 89.85, 89.85, 89.835, 89.835, 89.87, 89.87, 89.855, 89.855, 89.835, 89.835, 89.865, 89.865, 89.8925, 89.8925, 89.875, 89.875, 89.9025, 89.9025, 89.865, 89.865, 89.905, 89.905, 89.9075, 89.9075, 89.9225, 89.9225, 89.905, 89.905, 89.91, 89.91, 89.8975, 89.8975, 89.9075, 89.9075, 89.8775, 89.8775, 89.91, 89.91, 89.8875, 89.8875, 89.8975, 89.8975, 89.9075, 89.9075, 89.94, 89.94, 89.9475, 89.9475, 89.96, 89.96, 89.905, 89.905, 89.8625, 89.8625, 89.875, 89.875, 89.8725, 89.8725, 89.88, 89.88, 89.8925, 89.8925, 89.905, 89.905, 89.9075, 89.9075, 89.915, 89.915, 89.885, 89.885, 89.8925, 89.8925, 89.9, 89.9, 89.9125, 89.9125, 89.8975, 89.8975, 89.88, 89.88, 89.9025, 89.9025, 89.9025, 89.9025, 89.9, 89.9, 89.9, 89.9, 89.9075, 89.9075, 89.9075, 89.9075, 89.93, 89.93, 89.955, 89.955, 89.9425, 89.9425, 89.945, 89.945, 89.945, 89.945, 89.915, 89.915, 89.9275, 89.9275, 89.92, 89.92, 89.9025, 89.9025, 89.8975, 89.8975, 89.915, 89.915, 89.9075, 89.9075, 89.915, 89.915, 89.93, 89.93, 89.93, 89.93, 89.945, 89.945, 89.9425, 89.9425, 89.9075, 89.9075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.539, Test loss: 1.986, Test accuracy: 61.64
Round   1, Train loss: 1.284, Test loss: 1.787, Test accuracy: 72.30
Round   2, Train loss: 1.239, Test loss: 1.728, Test accuracy: 78.77
Round   3, Train loss: 1.217, Test loss: 1.714, Test accuracy: 80.00
Round   4, Train loss: 1.210, Test loss: 1.704, Test accuracy: 81.27
Round   5, Train loss: 1.204, Test loss: 1.701, Test accuracy: 81.46
Round   6, Train loss: 1.198, Test loss: 1.698, Test accuracy: 81.60
Round   7, Train loss: 1.196, Test loss: 1.695, Test accuracy: 81.98
Round   8, Train loss: 1.195, Test loss: 1.696, Test accuracy: 82.03
Round   9, Train loss: 1.191, Test loss: 1.694, Test accuracy: 81.98
Round  10, Train loss: 1.193, Test loss: 1.696, Test accuracy: 81.88
Round  11, Train loss: 1.191, Test loss: 1.696, Test accuracy: 81.83
Round  12, Train loss: 1.188, Test loss: 1.695, Test accuracy: 82.19
Round  13, Train loss: 1.184, Test loss: 1.695, Test accuracy: 82.05
Round  14, Train loss: 1.185, Test loss: 1.697, Test accuracy: 81.98
Round  15, Train loss: 1.184, Test loss: 1.697, Test accuracy: 81.89
Round  16, Train loss: 1.182, Test loss: 1.697, Test accuracy: 81.72
Round  17, Train loss: 1.185, Test loss: 1.698, Test accuracy: 81.66
Round  18, Train loss: 1.184, Test loss: 1.700, Test accuracy: 81.50
Round  19, Train loss: 1.181, Test loss: 1.700, Test accuracy: 81.40
Round  20, Train loss: 1.181, Test loss: 1.702, Test accuracy: 81.34
Round  21, Train loss: 1.181, Test loss: 1.702, Test accuracy: 81.26
Round  22, Train loss: 1.181, Test loss: 1.704, Test accuracy: 81.17
Round  23, Train loss: 1.180, Test loss: 1.706, Test accuracy: 81.00
Round  24, Train loss: 1.181, Test loss: 1.706, Test accuracy: 81.00
Round  25, Train loss: 1.180, Test loss: 1.707, Test accuracy: 80.89
Round  26, Train loss: 1.176, Test loss: 1.707, Test accuracy: 81.37
Round  27, Train loss: 1.126, Test loss: 1.687, Test accuracy: 83.93
Round  28, Train loss: 1.122, Test loss: 1.677, Test accuracy: 85.11
Round  29, Train loss: 1.115, Test loss: 1.672, Test accuracy: 85.95
Round  30, Train loss: 1.114, Test loss: 1.669, Test accuracy: 86.66
Round  31, Train loss: 1.114, Test loss: 1.665, Test accuracy: 87.17
Round  32, Train loss: 1.111, Test loss: 1.665, Test accuracy: 87.52
Round  33, Train loss: 1.109, Test loss: 1.664, Test accuracy: 87.55
Round  34, Train loss: 1.109, Test loss: 1.664, Test accuracy: 87.53
Round  35, Train loss: 1.107, Test loss: 1.662, Test accuracy: 87.52
Round  36, Train loss: 1.107, Test loss: 1.663, Test accuracy: 87.36
Round  37, Train loss: 1.105, Test loss: 1.662, Test accuracy: 87.34
Round  38, Train loss: 1.109, Test loss: 1.661, Test accuracy: 87.41
Round  39, Train loss: 1.106, Test loss: 1.660, Test accuracy: 87.50
Round  40, Train loss: 1.108, Test loss: 1.661, Test accuracy: 87.55
Round  41, Train loss: 1.106, Test loss: 1.662, Test accuracy: 87.28
Round  42, Train loss: 1.106, Test loss: 1.663, Test accuracy: 87.17
Round  43, Train loss: 1.106, Test loss: 1.664, Test accuracy: 87.09
Round  44, Train loss: 1.104, Test loss: 1.665, Test accuracy: 86.93
Round  45, Train loss: 1.104, Test loss: 1.665, Test accuracy: 86.87
Round  46, Train loss: 1.104, Test loss: 1.665, Test accuracy: 86.89
Round  47, Train loss: 1.104, Test loss: 1.666, Test accuracy: 86.72
Round  48, Train loss: 1.105, Test loss: 1.668, Test accuracy: 86.47
Round  49, Train loss: 1.104, Test loss: 1.669, Test accuracy: 86.38
Round  50, Train loss: 1.103, Test loss: 1.669, Test accuracy: 86.23
Round  51, Train loss: 1.104, Test loss: 1.670, Test accuracy: 86.12
Round  52, Train loss: 1.103, Test loss: 1.671, Test accuracy: 85.93
Round  53, Train loss: 1.103, Test loss: 1.672, Test accuracy: 85.86
Round  54, Train loss: 1.103, Test loss: 1.672, Test accuracy: 85.79
Round  55, Train loss: 1.102, Test loss: 1.672, Test accuracy: 85.79
Round  56, Train loss: 1.104, Test loss: 1.674, Test accuracy: 85.75
Round  57, Train loss: 1.102, Test loss: 1.674, Test accuracy: 85.56
Round  58, Train loss: 1.102, Test loss: 1.675, Test accuracy: 85.42
Round  59, Train loss: 1.103, Test loss: 1.677, Test accuracy: 85.41
Round  60, Train loss: 1.103, Test loss: 1.677, Test accuracy: 85.33
Round  61, Train loss: 1.103, Test loss: 1.677, Test accuracy: 85.26
Round  62, Train loss: 1.103, Test loss: 1.678, Test accuracy: 85.16
Round  63, Train loss: 1.101, Test loss: 1.679, Test accuracy: 85.03
Round  64, Train loss: 1.103, Test loss: 1.680, Test accuracy: 84.89
Round  65, Train loss: 1.102, Test loss: 1.681, Test accuracy: 84.81
Round  66, Train loss: 1.103, Test loss: 1.681, Test accuracy: 84.78
Round  67, Train loss: 1.102, Test loss: 1.682, Test accuracy: 84.63
Round  68, Train loss: 1.102, Test loss: 1.682, Test accuracy: 84.61
Round  69, Train loss: 1.103, Test loss: 1.683, Test accuracy: 84.52
Round  70, Train loss: 1.102, Test loss: 1.684, Test accuracy: 84.41
Round  71, Train loss: 1.103, Test loss: 1.684, Test accuracy: 84.38
Round  72, Train loss: 1.102, Test loss: 1.685, Test accuracy: 84.25
Round  73, Train loss: 1.103, Test loss: 1.686, Test accuracy: 84.11
Round  74, Train loss: 1.102, Test loss: 1.686, Test accuracy: 84.15
Round  75, Train loss: 1.103, Test loss: 1.687, Test accuracy: 84.03
Round  76, Train loss: 1.102, Test loss: 1.688, Test accuracy: 84.00
Round  77, Train loss: 1.101, Test loss: 1.689, Test accuracy: 83.93
Round  78, Train loss: 1.101, Test loss: 1.688, Test accuracy: 83.97
Round  79, Train loss: 1.102, Test loss: 1.688, Test accuracy: 84.01
Round  80, Train loss: 1.102, Test loss: 1.690, Test accuracy: 83.84
Round  81, Train loss: 1.103, Test loss: 1.691, Test accuracy: 83.72
Round  82, Train loss: 1.102, Test loss: 1.691, Test accuracy: 83.71
Round  83, Train loss: 1.102, Test loss: 1.692, Test accuracy: 83.56
Round  84, Train loss: 1.103, Test loss: 1.693, Test accuracy: 83.54
Round  85, Train loss: 1.102, Test loss: 1.694, Test accuracy: 83.48
Round  86, Train loss: 1.102, Test loss: 1.695, Test accuracy: 83.32
Round  87, Train loss: 1.101, Test loss: 1.695, Test accuracy: 83.31
Round  88, Train loss: 1.102, Test loss: 1.695, Test accuracy: 83.31
Round  89, Train loss: 1.102, Test loss: 1.696, Test accuracy: 83.25
Round  90, Train loss: 1.101, Test loss: 1.696, Test accuracy: 83.25
Round  91, Train loss: 1.101, Test loss: 1.696, Test accuracy: 83.24
Round  92, Train loss: 1.102, Test loss: 1.696, Test accuracy: 83.16
Round  93, Train loss: 1.102, Test loss: 1.697, Test accuracy: 83.14
Round  94, Train loss: 1.102, Test loss: 1.698, Test accuracy: 83.07
Round  95, Train loss: 1.102, Test loss: 1.699, Test accuracy: 82.88
Round  96, Train loss: 1.101, Test loss: 1.700, Test accuracy: 82.83
Round  97, Train loss: 1.101, Test loss: 1.699, Test accuracy: 82.89
Round  98, Train loss: 1.100, Test loss: 1.700, Test accuracy: 82.75
Round  99, Train loss: 1.102, Test loss: 1.700, Test accuracy: 82.85
Final Round, Train loss: 1.101, Test loss: 1.703, Test accuracy: 82.67
Average accuracy final 10 rounds: 83.0055
4560.37552690506
[]
[61.6425, 72.3025, 78.77, 80.0, 81.2725, 81.4625, 81.5975, 81.9825, 82.03, 81.9775, 81.88, 81.835, 82.1875, 82.045, 81.985, 81.89, 81.715, 81.6575, 81.505, 81.3975, 81.3375, 81.26, 81.175, 81.005, 81.0, 80.8875, 81.3675, 83.9325, 85.1125, 85.95, 86.655, 87.165, 87.5225, 87.545, 87.53, 87.5225, 87.36, 87.3375, 87.4075, 87.5025, 87.545, 87.275, 87.1725, 87.0925, 86.9275, 86.87, 86.885, 86.725, 86.4675, 86.3825, 86.2275, 86.1225, 85.9275, 85.8625, 85.7875, 85.7875, 85.7525, 85.56, 85.425, 85.41, 85.335, 85.2625, 85.155, 85.03, 84.895, 84.815, 84.78, 84.6275, 84.6125, 84.5175, 84.405, 84.3825, 84.2475, 84.105, 84.1475, 84.0275, 83.995, 83.9275, 83.975, 84.0075, 83.84, 83.72, 83.71, 83.555, 83.5375, 83.48, 83.32, 83.3125, 83.3125, 83.2525, 83.245, 83.2375, 83.1575, 83.1425, 83.0675, 82.8775, 82.83, 82.8925, 82.7525, 82.8525, 82.665]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.263, Test loss: 2.244, Test accuracy: 27.41
Round   1, Train loss: 2.007, Test loss: 2.122, Test accuracy: 51.47
Round   2, Train loss: 1.812, Test loss: 2.041, Test accuracy: 59.76
Round   3, Train loss: 1.718, Test loss: 1.988, Test accuracy: 57.67
Round   4, Train loss: 2.243, Test loss: 2.061, Test accuracy: 46.09
Round   5, Train loss: 1.742, Test loss: 1.976, Test accuracy: 53.05
Round   6, Train loss: 1.671, Test loss: 1.923, Test accuracy: 56.18
Round   7, Train loss: 1.318, Test loss: 1.905, Test accuracy: 57.35
Round   8, Train loss: 1.311, Test loss: 1.842, Test accuracy: 63.50
Round   9, Train loss: 1.018, Test loss: 1.793, Test accuracy: 68.30
Round  10, Train loss: 1.277, Test loss: 1.761, Test accuracy: 71.83
Round  11, Train loss: 1.570, Test loss: 1.756, Test accuracy: 72.75
Round  12, Train loss: 1.417, Test loss: 1.750, Test accuracy: 73.29
Round  13, Train loss: 1.357, Test loss: 1.732, Test accuracy: 74.93
Round  14, Train loss: 1.320, Test loss: 1.695, Test accuracy: 78.26
Round  15, Train loss: 1.299, Test loss: 1.687, Test accuracy: 79.00
Round  16, Train loss: 1.110, Test loss: 1.666, Test accuracy: 80.47
Round  17, Train loss: 1.095, Test loss: 1.654, Test accuracy: 81.53
Round  18, Train loss: 0.994, Test loss: 1.651, Test accuracy: 81.84
Round  19, Train loss: 1.308, Test loss: 1.652, Test accuracy: 81.80
Round  20, Train loss: 1.164, Test loss: 1.652, Test accuracy: 81.56
Round  21, Train loss: 0.933, Test loss: 1.643, Test accuracy: 82.27
Round  22, Train loss: 0.882, Test loss: 1.641, Test accuracy: 82.36
Round  23, Train loss: 0.955, Test loss: 1.638, Test accuracy: 82.70
Round  24, Train loss: 0.943, Test loss: 1.639, Test accuracy: 82.57
Round  25, Train loss: 1.064, Test loss: 1.644, Test accuracy: 82.24
Round  26, Train loss: 1.117, Test loss: 1.638, Test accuracy: 82.90
Round  27, Train loss: 0.892, Test loss: 1.639, Test accuracy: 82.89
Round  28, Train loss: 0.847, Test loss: 1.637, Test accuracy: 82.94
Round  29, Train loss: 1.051, Test loss: 1.633, Test accuracy: 83.12
Round  30, Train loss: 1.032, Test loss: 1.631, Test accuracy: 83.15
Round  31, Train loss: 1.031, Test loss: 1.624, Test accuracy: 83.97
Round  32, Train loss: 0.747, Test loss: 1.614, Test accuracy: 84.93
Round  33, Train loss: 1.052, Test loss: 1.613, Test accuracy: 85.13
Round  34, Train loss: 0.845, Test loss: 1.620, Test accuracy: 84.36
Round  35, Train loss: 0.798, Test loss: 1.617, Test accuracy: 84.67
Round  36, Train loss: 0.849, Test loss: 1.606, Test accuracy: 85.82
Round  37, Train loss: 0.919, Test loss: 1.616, Test accuracy: 84.78
Round  38, Train loss: 0.806, Test loss: 1.618, Test accuracy: 84.59
Round  39, Train loss: 0.977, Test loss: 1.624, Test accuracy: 83.92
Round  40, Train loss: 0.706, Test loss: 1.602, Test accuracy: 86.17
Round  41, Train loss: 1.007, Test loss: 1.600, Test accuracy: 86.44
Round  42, Train loss: 0.964, Test loss: 1.607, Test accuracy: 85.63
Round  43, Train loss: 0.831, Test loss: 1.612, Test accuracy: 85.19
Round  44, Train loss: 0.915, Test loss: 1.602, Test accuracy: 86.19
Round  45, Train loss: 1.012, Test loss: 1.615, Test accuracy: 84.90
Round  46, Train loss: 0.838, Test loss: 1.616, Test accuracy: 84.74
Round  47, Train loss: 0.763, Test loss: 1.615, Test accuracy: 84.79
Round  48, Train loss: 0.904, Test loss: 1.608, Test accuracy: 85.40
Round  49, Train loss: 0.835, Test loss: 1.614, Test accuracy: 84.86
Round  50, Train loss: 0.931, Test loss: 1.603, Test accuracy: 86.00
Round  51, Train loss: 0.735, Test loss: 1.604, Test accuracy: 85.92
Round  52, Train loss: 0.840, Test loss: 1.619, Test accuracy: 84.36
Round  53, Train loss: 0.548, Test loss: 1.602, Test accuracy: 86.08
Round  54, Train loss: 0.577, Test loss: 1.587, Test accuracy: 87.64
Round  55, Train loss: 0.747, Test loss: 1.594, Test accuracy: 86.90
Round  56, Train loss: 0.847, Test loss: 1.601, Test accuracy: 86.17
Round  57, Train loss: 0.807, Test loss: 1.600, Test accuracy: 86.27
Round  58, Train loss: 0.946, Test loss: 1.609, Test accuracy: 85.39
Round  59, Train loss: 0.667, Test loss: 1.594, Test accuracy: 86.91
Round  60, Train loss: 0.878, Test loss: 1.601, Test accuracy: 86.23
Round  61, Train loss: 0.698, Test loss: 1.600, Test accuracy: 86.32
Round  62, Train loss: 0.870, Test loss: 1.616, Test accuracy: 84.55
Round  63, Train loss: 0.782, Test loss: 1.595, Test accuracy: 86.77
Round  64, Train loss: 0.616, Test loss: 1.600, Test accuracy: 86.24
Round  65, Train loss: 0.724, Test loss: 1.600, Test accuracy: 86.31
Round  66, Train loss: 0.824, Test loss: 1.599, Test accuracy: 86.40
Round  67, Train loss: 0.749, Test loss: 1.601, Test accuracy: 86.27
Round  68, Train loss: 0.585, Test loss: 1.600, Test accuracy: 86.28
Round  69, Train loss: 0.754, Test loss: 1.592, Test accuracy: 87.09
Round  70, Train loss: 0.699, Test loss: 1.598, Test accuracy: 86.43
Round  71, Train loss: 0.843, Test loss: 1.598, Test accuracy: 86.45
Round  72, Train loss: 0.729, Test loss: 1.609, Test accuracy: 85.34
Round  73, Train loss: 0.860, Test loss: 1.608, Test accuracy: 85.44
Round  74, Train loss: 0.818, Test loss: 1.601, Test accuracy: 86.17
Round  75, Train loss: 0.660, Test loss: 1.608, Test accuracy: 85.45
Round  76, Train loss: 0.772, Test loss: 1.608, Test accuracy: 85.44
Round  77, Train loss: 0.826, Test loss: 1.623, Test accuracy: 83.95
Round  78, Train loss: 0.777, Test loss: 1.611, Test accuracy: 85.23
Round  79, Train loss: 0.770, Test loss: 1.604, Test accuracy: 85.95
Round  80, Train loss: 0.709, Test loss: 1.601, Test accuracy: 86.30
Round  81, Train loss: 0.868, Test loss: 1.616, Test accuracy: 84.75
Round  82, Train loss: 0.707, Test loss: 1.611, Test accuracy: 85.21
Round  83, Train loss: 0.649, Test loss: 1.615, Test accuracy: 84.75
Round  84, Train loss: 0.692, Test loss: 1.618, Test accuracy: 84.47
Round  85, Train loss: 0.798, Test loss: 1.610, Test accuracy: 85.29
Round  86, Train loss: 0.637, Test loss: 1.603, Test accuracy: 85.99
Round  87, Train loss: 0.700, Test loss: 1.604, Test accuracy: 85.92
Round  88, Train loss: 0.815, Test loss: 1.598, Test accuracy: 86.55
Round  89, Train loss: 0.861, Test loss: 1.613, Test accuracy: 85.05
Round  90, Train loss: 0.760, Test loss: 1.609, Test accuracy: 85.36
Round  91, Train loss: 0.851, Test loss: 1.610, Test accuracy: 85.32
Round  92, Train loss: 0.677, Test loss: 1.614, Test accuracy: 84.85
Round  93, Train loss: 0.812, Test loss: 1.614, Test accuracy: 84.89
Round  94, Train loss: 0.836, Test loss: 1.590, Test accuracy: 87.36
Round  95, Train loss: 0.739, Test loss: 1.605, Test accuracy: 85.80
Round  96, Train loss: 0.673, Test loss: 1.607, Test accuracy: 85.61
Round  97, Train loss: 0.595, Test loss: 1.592, Test accuracy: 87.22
Round  98, Train loss: 0.831, Test loss: 1.601, Test accuracy: 86.30
Round  99, Train loss: 0.683, Test loss: 1.609, Test accuracy: 85.42
Final Round, Train loss: 1.590, Test loss: 1.620, Test accuracy: 84.37
Average accuracy final 10 rounds: 85.81225
Average global accuracy final 10 rounds: 85.81225
3273.5126299858093
[]
[27.4125, 51.47, 59.7625, 57.67, 46.09, 53.055, 56.18, 57.3525, 63.4975, 68.295, 71.835, 72.7475, 73.2925, 74.9275, 78.2625, 79.0, 80.465, 81.525, 81.84, 81.8025, 81.555, 82.265, 82.355, 82.7025, 82.5675, 82.2375, 82.9025, 82.885, 82.9375, 83.1225, 83.1525, 83.975, 84.9275, 85.13, 84.3575, 84.6675, 85.8225, 84.775, 84.595, 83.9225, 86.165, 86.44, 85.6325, 85.195, 86.1875, 84.8975, 84.7375, 84.7875, 85.3975, 84.855, 85.995, 85.915, 84.3575, 86.075, 87.6425, 86.9, 86.165, 86.27, 85.385, 86.9125, 86.2325, 86.3175, 84.55, 86.7725, 86.24, 86.3075, 86.4025, 86.265, 86.275, 87.095, 86.4275, 86.4525, 85.3375, 85.44, 86.1725, 85.45, 85.44, 83.955, 85.23, 85.95, 86.3, 84.7475, 85.21, 84.7525, 84.4675, 85.2875, 85.99, 85.925, 86.5475, 85.0475, 85.3625, 85.3175, 84.8525, 84.885, 87.3575, 85.8, 85.615, 87.215, 86.2975, 85.42, 84.3725]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.90
Round   0, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.90
Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.96
Round   1, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.99
Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round   2, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.04
Round   3, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.04
Round   3, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.08
Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.09
Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.16
Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.10
Round   5, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.15
Round   6, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.12
Round   6, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.19
Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.14
Round   7, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.20
Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.17
Round   8, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.23
Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.21
Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.32
Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.27
Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.34
Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.29
Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.41
Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.35
Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.43
Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.39
Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.44
Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.42
Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.50
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.46
Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.51
Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.48
Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.57
Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.50
Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.62
Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.56
Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.64
Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.61
Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.71
Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.68
Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.77
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.74
Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.82
Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.84
Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.89
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.84
Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.92
Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.87
Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.02
Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.93
Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.13
Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.01
Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.15
Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.06
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.22
Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.09
Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.30
Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.15
Round  29, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 14.37
Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.18
Round  30, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 14.41
Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.23
Round  31, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 14.36
Round  32, Train loss: 2.302, Test loss: 2.301, Test accuracy: 14.27
Round  32, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 14.36
Round  33, Train loss: 2.302, Test loss: 2.301, Test accuracy: 14.32
Round  33, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 14.46
Round  34, Train loss: 2.302, Test loss: 2.301, Test accuracy: 14.34
Round  34, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 14.55
Round  35, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.41
Round  35, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.57
Round  36, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.55
Round  36, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.64
Round  37, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.60
Round  37, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.73
Round  38, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.64
Round  38, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.80
Round  39, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.70
Round  39, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.84
Round  40, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.77
Round  40, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.98
Round  41, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.81
Round  41, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 14.96
Round  42, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.85
Round  42, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.06
Round  43, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.93
Round  43, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.16
Round  44, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.01
Round  44, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.16
Round  45, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.06
Round  45, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.21
Round  46, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.09
Round  46, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.20
Round  47, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.12
Round  47, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.38
Round  48, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.19
Round  48, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.41
Round  49, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.27
Round  49, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.48
Round  50, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.36
Round  50, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.55
Round  51, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.38
Round  51, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.59
Round  52, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.49
Round  52, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.70
Round  53, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.57
Round  53, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.76
Round  54, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.68
Round  54, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.81
Round  55, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.75
Round  55, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.87
Round  56, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.77
Round  56, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.92
Round  57, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.83
Round  57, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.96
Round  58, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.85
Round  58, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.03
Round  59, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.92
Round  59, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.04
Round  60, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.98
Round  60, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.01
Round  61, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.01
Round  61, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.09
Round  62, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.07
Round  62, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.09
Round  63, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.11
Round  63, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.12
Round  64, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.12
Round  64, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.20
Round  65, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.17
Round  65, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.21
Round  66, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.22
Round  66, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.40
Round  67, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.28
Round  67, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.44
Round  68, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.32
Round  68, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.59
Round  69, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.45
Round  69, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.61
Round  70, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.57
Round  70, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.65
Round  71, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.62
Round  71, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.68
Round  72, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.66
Round  72, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.77
Round  73, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.72
Round  73, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.84
Round  74, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.75
Round  74, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.86
Round  75, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.80
Round  75, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.89
Round  76, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.82
Round  76, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.89
Round  77, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.90
Round  77, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.98
Round  78, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.90
Round  78, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 16.94
Round  79, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.93
Round  79, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.02
Round  80, Train loss: 2.301, Test loss: 2.301, Test accuracy: 16.98
Round  80, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.10
Round  81, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.02
Round  81, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.15
Round  82, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.08
Round  82, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.19
Round  83, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.14
Round  83, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.22
Round  84, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.18
Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.22
Round  85, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.20
Round  85, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.31
Round  86, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.24
Round  86, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.36
Round  87, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.28
Round  87, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.40
Round  88, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.35
Round  88, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.41
Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.36
Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.49
Round  90, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.44
Round  90, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.54
Round  91, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.50
Round  91, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.61
Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.57
Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.68
Round  93, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.64
Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.77
Round  94, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.68
Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.82
Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.75
Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 17.88
Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.77
Round  96, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 17.93/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 17.86
Round  97, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 17.97
Round  98, Train loss: 2.301, Test loss: 2.300, Test accuracy: 17.96
Round  98, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 18.04
Round  99, Train loss: 2.301, Test loss: 2.300, Test accuracy: 17.99
Round  99, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 18.13
Final Round, Train loss: 2.301, Test loss: 2.300, Test accuracy: 18.22
Final Round, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 18.13
Average accuracy final 10 rounds: 17.717000000000002 

Average global accuracy final 10 rounds: 17.83725 

3771.3603489398956
[3.4515457153320312, 6.825150728225708, 10.113714933395386, 13.39911961555481, 16.62473440170288, 20.024124145507812, 23.31148362159729, 26.565893173217773, 29.917062282562256, 33.28054118156433, 36.574527978897095, 39.93025016784668, 43.365859270095825, 46.62806820869446, 49.97890019416809, 53.228941440582275, 56.39511299133301, 59.536537170410156, 62.71034216880798, 65.89092063903809, 69.05203866958618, 72.21549797058105, 75.35977554321289, 78.51115107536316, 81.65783715248108, 84.81349468231201, 87.97427153587341, 91.13495302200317, 94.2849280834198, 97.4338948726654, 100.59061551094055, 103.74105334281921, 106.89421939849854, 110.04447197914124, 113.1969404220581, 116.36572432518005, 119.50539207458496, 122.65121936798096, 125.79476308822632, 128.92880034446716, 132.05870866775513, 135.18814301490784, 138.3013093471527, 141.4139757156372, 144.52898383140564, 147.64288973808289, 150.751149892807, 153.87202548980713, 156.99162769317627, 160.12509489059448, 163.43551421165466, 166.72009205818176, 169.81084609031677, 172.9079611301422, 176.2549319267273, 179.63106417655945, 182.76946020126343, 185.91358423233032, 189.1435182094574, 192.255131483078, 195.46116304397583, 198.8063621520996, 201.9432020187378, 205.07625818252563, 208.21881771087646, 211.3551104068756, 214.4676375389099, 217.61159372329712, 220.76004338264465, 224.18889713287354, 227.6229510307312, 231.0916576385498, 234.5995795726776, 237.965749502182, 241.37408304214478, 244.8279731273651, 248.21920680999756, 251.73547458648682, 254.92735242843628, 258.4510705471039, 261.9702615737915, 265.5322062969208, 269.0363986492157, 272.53570890426636, 276.0920066833496, 279.6748294830322, 283.21433568000793, 286.71989464759827, 289.94080114364624, 293.17983508110046, 296.3580768108368, 299.53356647491455, 302.7114930152893, 305.8844759464264, 309.07064390182495, 312.27518010139465, 315.44457030296326, 318.6212332248688, 321.7930245399475, 324.9606149196625, 326.7336585521698]
[12.8975, 12.9575, 13.0025, 13.045, 13.0875, 13.1025, 13.1175, 13.145, 13.1725, 13.2125, 13.2725, 13.295, 13.345, 13.395, 13.42, 13.455, 13.485, 13.5025, 13.5625, 13.6075, 13.68, 13.7425, 13.8425, 13.8425, 13.87, 13.9275, 14.0075, 14.0575, 14.0875, 14.155, 14.18, 14.2325, 14.2725, 14.325, 14.3425, 14.4075, 14.555, 14.6, 14.645, 14.695, 14.765, 14.8125, 14.8525, 14.93, 15.0125, 15.0575, 15.0875, 15.115, 15.185, 15.27, 15.355, 15.375, 15.49, 15.575, 15.68, 15.7475, 15.7675, 15.8325, 15.845, 15.92, 15.9775, 16.0075, 16.075, 16.1075, 16.1225, 16.1725, 16.2175, 16.2825, 16.315, 16.455, 16.565, 16.625, 16.655, 16.7225, 16.7525, 16.7975, 16.82, 16.9, 16.9025, 16.9275, 16.9775, 17.025, 17.0825, 17.1425, 17.1775, 17.195, 17.2375, 17.2775, 17.3475, 17.3575, 17.4425, 17.5025, 17.57, 17.6425, 17.685, 17.75, 17.7725, 17.855, 17.9575, 17.9925, 18.22]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.182, Test accuracy: 35.80
Round   1, Train loss: 2.130, Test loss: 1.706, Test accuracy: 79.29
Round   2, Train loss: 1.717, Test loss: 1.621, Test accuracy: 85.48
Round   3, Train loss: 1.632, Test loss: 1.574, Test accuracy: 89.86
Round   4, Train loss: 1.585, Test loss: 1.560, Test accuracy: 90.91
Round   5, Train loss: 1.556, Test loss: 1.552, Test accuracy: 91.55
Round   6, Train loss: 1.549, Test loss: 1.546, Test accuracy: 92.02
Round   7, Train loss: 1.541, Test loss: 1.541, Test accuracy: 92.44
Round   8, Train loss: 1.536, Test loss: 1.536, Test accuracy: 92.86
Round   9, Train loss: 1.522, Test loss: 1.533, Test accuracy: 93.29
Round  10, Train loss: 1.523, Test loss: 1.530, Test accuracy: 93.41
Round  11, Train loss: 1.521, Test loss: 1.527, Test accuracy: 93.92
Round  12, Train loss: 1.513, Test loss: 1.526, Test accuracy: 93.89
Round  13, Train loss: 1.508, Test loss: 1.524, Test accuracy: 94.07
Round  14, Train loss: 1.506, Test loss: 1.521, Test accuracy: 94.31
Round  15, Train loss: 1.507, Test loss: 1.519, Test accuracy: 94.58
Round  16, Train loss: 1.500, Test loss: 1.519, Test accuracy: 94.42
Round  17, Train loss: 1.495, Test loss: 1.517, Test accuracy: 94.77
Round  18, Train loss: 1.506, Test loss: 1.515, Test accuracy: 94.91
Round  19, Train loss: 1.502, Test loss: 1.514, Test accuracy: 94.98
Round  20, Train loss: 1.496, Test loss: 1.513, Test accuracy: 95.09
Round  21, Train loss: 1.495, Test loss: 1.512, Test accuracy: 95.20
Round  22, Train loss: 1.493, Test loss: 1.511, Test accuracy: 95.27
Round  23, Train loss: 1.490, Test loss: 1.511, Test accuracy: 95.17
Round  24, Train loss: 1.489, Test loss: 1.510, Test accuracy: 95.26
Round  25, Train loss: 1.488, Test loss: 1.510, Test accuracy: 95.39
Round  26, Train loss: 1.489, Test loss: 1.509, Test accuracy: 95.44
Round  27, Train loss: 1.486, Test loss: 1.508, Test accuracy: 95.46
Round  28, Train loss: 1.488, Test loss: 1.507, Test accuracy: 95.53
Round  29, Train loss: 1.486, Test loss: 1.508, Test accuracy: 95.50
Round  30, Train loss: 1.485, Test loss: 1.507, Test accuracy: 95.63
Round  31, Train loss: 1.484, Test loss: 1.506, Test accuracy: 95.71
Round  32, Train loss: 1.483, Test loss: 1.505, Test accuracy: 95.87
Round  33, Train loss: 1.483, Test loss: 1.505, Test accuracy: 95.94
Round  34, Train loss: 1.480, Test loss: 1.505, Test accuracy: 95.81
Round  35, Train loss: 1.484, Test loss: 1.504, Test accuracy: 95.88
Round  36, Train loss: 1.481, Test loss: 1.504, Test accuracy: 95.88
Round  37, Train loss: 1.482, Test loss: 1.504, Test accuracy: 95.99
Round  38, Train loss: 1.479, Test loss: 1.503, Test accuracy: 95.96
Round  39, Train loss: 1.477, Test loss: 1.503, Test accuracy: 96.01
Round  40, Train loss: 1.481, Test loss: 1.503, Test accuracy: 96.10
Round  41, Train loss: 1.478, Test loss: 1.502, Test accuracy: 96.08
Round  42, Train loss: 1.477, Test loss: 1.503, Test accuracy: 96.02
Round  43, Train loss: 1.479, Test loss: 1.501, Test accuracy: 96.28
Round  44, Train loss: 1.476, Test loss: 1.501, Test accuracy: 96.20
Round  45, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.22
Round  46, Train loss: 1.477, Test loss: 1.501, Test accuracy: 96.34
Round  47, Train loss: 1.476, Test loss: 1.501, Test accuracy: 96.20
Round  48, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.32
Round  49, Train loss: 1.476, Test loss: 1.501, Test accuracy: 96.31
Round  50, Train loss: 1.476, Test loss: 1.500, Test accuracy: 96.31
Round  51, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.36
Round  52, Train loss: 1.475, Test loss: 1.500, Test accuracy: 96.30
Round  53, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.41
Round  54, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.36
Round  55, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.36
Round  56, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.35
Round  57, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.35
Round  58, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.41
Round  59, Train loss: 1.472, Test loss: 1.500, Test accuracy: 96.37
Round  60, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.44
Round  61, Train loss: 1.473, Test loss: 1.499, Test accuracy: 96.49
Round  62, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.43
Round  63, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.48
Round  64, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.47
Round  65, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.36
Round  66, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.54
Round  67, Train loss: 1.472, Test loss: 1.498, Test accuracy: 96.47
Round  68, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.53
Round  69, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.41
Round  70, Train loss: 1.473, Test loss: 1.498, Test accuracy: 96.54
Round  71, Train loss: 1.472, Test loss: 1.498, Test accuracy: 96.54
Round  72, Train loss: 1.472, Test loss: 1.498, Test accuracy: 96.61
Round  73, Train loss: 1.472, Test loss: 1.498, Test accuracy: 96.58
Round  74, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.62
Round  75, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.58
Round  76, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.56
Round  77, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.60
Round  78, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.67
Round  79, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.61
Round  80, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.61
Round  81, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.58
Round  82, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.56
Round  83, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.59
Round  84, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.64
Round  85, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.58
Round  86, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.64
Round  87, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.68
Round  88, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.68
Round  89, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.56
Round  90, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.58
Round  91, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.62
Round  92, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.65
Round  93, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.56
Round  94, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.64
Round  95, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.64
Round  96, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.63
Round  97, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.62
Round  98, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.65
Round  99, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.64
Final Round, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.64
Average accuracy final 10 rounds: 96.623
5668.5856149196625
[8.399549007415771, 16.807357788085938, 25.295602083206177, 33.18663167953491, 41.039045572280884, 48.81608986854553, 56.650585412979126, 64.43687295913696, 72.15538144111633, 79.87345337867737, 87.66778755187988, 95.50705122947693, 104.0056734085083, 112.03624176979065, 119.96933126449585, 127.9265604019165, 135.91450715065002, 143.88018798828125, 151.80027270317078, 159.66863822937012, 167.6541805267334, 175.60854578018188, 183.60939121246338, 191.6118984222412, 199.6218900680542, 207.49997901916504, 215.40132427215576, 223.3046498298645, 231.23838019371033, 239.22204041481018, 247.13421869277954, 255.05425238609314, 262.9461770057678, 270.88768458366394, 278.8278183937073, 286.77937507629395, 294.7113571166992, 302.62511110305786, 310.557724237442, 318.5182716846466, 326.4336838722229, 334.33380484580994, 342.2709729671478, 350.2477331161499, 358.2074239253998, 366.1407639980316, 374.09311151504517, 382.04240679740906, 390.12849831581116, 398.19090843200684, 406.15159273147583, 414.0910270214081, 422.1264338493347, 430.2158989906311, 438.22182846069336, 446.2460341453552, 454.206503868103, 462.2892782688141, 470.2959494590759, 478.273291349411, 486.32050943374634, 494.41252279281616, 502.54276752471924, 510.55381441116333, 518.6406302452087, 526.6059353351593, 534.6795670986176, 542.7053327560425, 550.8739907741547, 559.011421918869, 567.0259420871735, 575.0345265865326, 583.2455673217773, 591.0857560634613, 599.3994731903076, 607.7048995494843, 615.9296112060547, 623.991881608963, 632.0433347225189, 639.9295949935913, 648.2692923545837, 656.5089490413666, 664.5962224006653, 672.9890539646149, 681.0183095932007, 689.0824196338654, 697.3451273441315, 705.604266166687, 713.7546029090881, 721.9123544692993, 730.0848457813263, 738.2327733039856, 746.3574495315552, 754.4678823947906, 762.6695854663849, 771.0605635643005, 779.2620365619659, 787.6419885158539, 796.046487569809, 803.8833847045898, 806.1409175395966]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[35.805, 79.2875, 85.4775, 89.8625, 90.9125, 91.545, 92.0175, 92.4375, 92.855, 93.2875, 93.4075, 93.915, 93.89, 94.07, 94.3075, 94.58, 94.4175, 94.77, 94.9125, 94.985, 95.0875, 95.2025, 95.265, 95.17, 95.26, 95.385, 95.4375, 95.46, 95.5325, 95.505, 95.6275, 95.7075, 95.8675, 95.9375, 95.81, 95.8775, 95.88, 95.99, 95.9575, 96.01, 96.1, 96.0775, 96.0225, 96.2825, 96.205, 96.2175, 96.34, 96.2, 96.3225, 96.31, 96.3075, 96.36, 96.3, 96.405, 96.365, 96.36, 96.35, 96.3525, 96.405, 96.3725, 96.44, 96.4875, 96.43, 96.4825, 96.475, 96.365, 96.5425, 96.475, 96.53, 96.4075, 96.54, 96.5425, 96.6075, 96.58, 96.625, 96.58, 96.555, 96.6025, 96.665, 96.605, 96.6075, 96.5825, 96.5625, 96.5925, 96.6375, 96.5775, 96.635, 96.68, 96.6775, 96.5575, 96.5775, 96.62, 96.6475, 96.5625, 96.6375, 96.64, 96.63, 96.6225, 96.6525, 96.64, 96.635]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.313, Test loss: 2.282, Test accuracy: 28.39
Round   1, Train loss: 2.240, Test loss: 2.135, Test accuracy: 53.68
Round   2, Train loss: 1.992, Test loss: 1.905, Test accuracy: 63.19
Round   3, Train loss: 1.834, Test loss: 1.816, Test accuracy: 72.96
Round   4, Train loss: 1.735, Test loss: 1.736, Test accuracy: 82.02
Round   5, Train loss: 1.638, Test loss: 1.677, Test accuracy: 87.03
Round   6, Train loss: 1.614, Test loss: 1.636, Test accuracy: 89.38
Round   7, Train loss: 1.601, Test loss: 1.597, Test accuracy: 92.40
Round   8, Train loss: 1.573, Test loss: 1.580, Test accuracy: 92.89
Round   9, Train loss: 1.549, Test loss: 1.573, Test accuracy: 93.41
Round  10, Train loss: 1.562, Test loss: 1.557, Test accuracy: 93.81
Round  11, Train loss: 1.551, Test loss: 1.543, Test accuracy: 94.27
Round  12, Train loss: 1.533, Test loss: 1.540, Test accuracy: 94.63
Round  13, Train loss: 1.526, Test loss: 1.537, Test accuracy: 94.76
Round  14, Train loss: 1.525, Test loss: 1.532, Test accuracy: 94.92
Round  15, Train loss: 1.520, Test loss: 1.529, Test accuracy: 95.15
Round  16, Train loss: 1.513, Test loss: 1.527, Test accuracy: 95.27
Round  17, Train loss: 1.520, Test loss: 1.524, Test accuracy: 95.39
Round  18, Train loss: 1.512, Test loss: 1.521, Test accuracy: 95.61
Round  19, Train loss: 1.512, Test loss: 1.519, Test accuracy: 95.81
Round  20, Train loss: 1.506, Test loss: 1.518, Test accuracy: 95.82
Round  21, Train loss: 1.503, Test loss: 1.517, Test accuracy: 95.85
Round  22, Train loss: 1.505, Test loss: 1.515, Test accuracy: 96.06
Round  23, Train loss: 1.501, Test loss: 1.513, Test accuracy: 96.13
Round  24, Train loss: 1.500, Test loss: 1.512, Test accuracy: 96.26
Round  25, Train loss: 1.499, Test loss: 1.511, Test accuracy: 96.39
Round  26, Train loss: 1.498, Test loss: 1.511, Test accuracy: 96.38
Round  27, Train loss: 1.491, Test loss: 1.510, Test accuracy: 96.53
Round  28, Train loss: 1.494, Test loss: 1.509, Test accuracy: 96.60
Round  29, Train loss: 1.492, Test loss: 1.508, Test accuracy: 96.53
Round  30, Train loss: 1.491, Test loss: 1.507, Test accuracy: 96.60
Round  31, Train loss: 1.489, Test loss: 1.507, Test accuracy: 96.67
Round  32, Train loss: 1.490, Test loss: 1.506, Test accuracy: 96.77
Round  33, Train loss: 1.488, Test loss: 1.505, Test accuracy: 96.76
Round  34, Train loss: 1.487, Test loss: 1.504, Test accuracy: 96.85
Round  35, Train loss: 1.485, Test loss: 1.504, Test accuracy: 96.89
Round  36, Train loss: 1.483, Test loss: 1.504, Test accuracy: 96.92
Round  37, Train loss: 1.485, Test loss: 1.503, Test accuracy: 96.93
Round  38, Train loss: 1.484, Test loss: 1.502, Test accuracy: 97.00
Round  39, Train loss: 1.481, Test loss: 1.502, Test accuracy: 97.03
Round  40, Train loss: 1.482, Test loss: 1.501, Test accuracy: 97.06
Round  41, Train loss: 1.482, Test loss: 1.501, Test accuracy: 97.06
Round  42, Train loss: 1.481, Test loss: 1.500, Test accuracy: 97.06
Round  43, Train loss: 1.480, Test loss: 1.500, Test accuracy: 97.14
Round  44, Train loss: 1.481, Test loss: 1.500, Test accuracy: 97.13
Round  45, Train loss: 1.481, Test loss: 1.499, Test accuracy: 97.14
Round  46, Train loss: 1.479, Test loss: 1.499, Test accuracy: 97.18
Round  47, Train loss: 1.479, Test loss: 1.499, Test accuracy: 97.21
Round  48, Train loss: 1.480, Test loss: 1.499, Test accuracy: 97.20
Round  49, Train loss: 1.479, Test loss: 1.499, Test accuracy: 97.18
Round  50, Train loss: 1.478, Test loss: 1.498, Test accuracy: 97.25
Round  51, Train loss: 1.476, Test loss: 1.498, Test accuracy: 97.27
Round  52, Train loss: 1.476, Test loss: 1.498, Test accuracy: 97.31
Round  53, Train loss: 1.476, Test loss: 1.498, Test accuracy: 97.31
Round  54, Train loss: 1.476, Test loss: 1.497, Test accuracy: 97.29
Round  55, Train loss: 1.477, Test loss: 1.497, Test accuracy: 97.35
Round  56, Train loss: 1.476, Test loss: 1.497, Test accuracy: 97.36
Round  57, Train loss: 1.475, Test loss: 1.497, Test accuracy: 97.36
Round  58, Train loss: 1.475, Test loss: 1.497, Test accuracy: 97.38
Round  59, Train loss: 1.475, Test loss: 1.496, Test accuracy: 97.40
Round  60, Train loss: 1.473, Test loss: 1.496, Test accuracy: 97.40
Round  61, Train loss: 1.474, Test loss: 1.496, Test accuracy: 97.40
Round  62, Train loss: 1.475, Test loss: 1.496, Test accuracy: 97.40
Round  63, Train loss: 1.474, Test loss: 1.496, Test accuracy: 97.44
Round  64, Train loss: 1.475, Test loss: 1.496, Test accuracy: 97.39
Round  65, Train loss: 1.474, Test loss: 1.495, Test accuracy: 97.42
Round  66, Train loss: 1.474, Test loss: 1.495, Test accuracy: 97.46
Round  67, Train loss: 1.474, Test loss: 1.495, Test accuracy: 97.41
Round  68, Train loss: 1.472, Test loss: 1.495, Test accuracy: 97.46
Round  69, Train loss: 1.472, Test loss: 1.495, Test accuracy: 97.47
Round  70, Train loss: 1.471, Test loss: 1.495, Test accuracy: 97.47
Round  71, Train loss: 1.473, Test loss: 1.495, Test accuracy: 97.44
Round  72, Train loss: 1.471, Test loss: 1.495, Test accuracy: 97.50
Round  73, Train loss: 1.472, Test loss: 1.495, Test accuracy: 97.48
Round  74, Train loss: 1.471, Test loss: 1.495, Test accuracy: 97.48
Round  75, Train loss: 1.472, Test loss: 1.495, Test accuracy: 97.46
Round  76, Train loss: 1.472, Test loss: 1.494, Test accuracy: 97.49
Round  77, Train loss: 1.471, Test loss: 1.494, Test accuracy: 97.50
Round  78, Train loss: 1.471, Test loss: 1.494, Test accuracy: 97.47
Round  79, Train loss: 1.472, Test loss: 1.494, Test accuracy: 97.47
Round  80, Train loss: 1.471, Test loss: 1.494, Test accuracy: 97.48
Round  81, Train loss: 1.471, Test loss: 1.494, Test accuracy: 97.51
Round  82, Train loss: 1.470, Test loss: 1.494, Test accuracy: 97.51
Round  83, Train loss: 1.471, Test loss: 1.494, Test accuracy: 97.50
Round  84, Train loss: 1.470, Test loss: 1.494, Test accuracy: 97.47
Round  85, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.50
Round  86, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.53
Round  87, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.53
Round  88, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.55
Round  89, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.54
Round  90, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.52
Round  91, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.56
Round  92, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.53
Round  93, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.51
Round  94, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.55/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.57
Round  96, Train loss: 1.469, Test loss: 1.492, Test accuracy: 97.55
Round  97, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.56
Round  98, Train loss: 1.468, Test loss: 1.492, Test accuracy: 97.60
Round  99, Train loss: 1.469, Test loss: 1.492, Test accuracy: 97.62
Final Round, Train loss: 1.466, Test loss: 1.492, Test accuracy: 97.60
Average accuracy final 10 rounds: 97.55675000000001
2999.7776494026184
[3.648245096206665, 7.238008260726929, 10.76448392868042, 14.244682550430298, 17.813148736953735, 21.312088012695312, 24.78439688682556, 28.339464902877808, 31.803070783615112, 35.27359414100647, 38.74974775314331, 42.22683787345886, 45.785927534103394, 49.25462555885315, 52.773674726486206, 56.2654013633728, 59.741984128952026, 63.23193168640137, 66.73466777801514, 70.24655890464783, 73.76944422721863, 77.28324818611145, 80.8054461479187, 84.28556489944458, 87.78041005134583, 91.25817799568176, 94.7037661075592, 98.14841723442078, 101.60053730010986, 105.05072569847107, 108.5055468082428, 111.98837351799011, 115.48065042495728, 118.9676525592804, 122.4756817817688, 125.99473428726196, 129.48509907722473, 132.98193645477295, 136.536634683609, 140.02488565444946, 143.54683542251587, 147.02059864997864, 150.48296976089478, 153.968035697937, 157.51567125320435, 161.0409758090973, 164.51984024047852, 167.98888158798218, 171.49256229400635, 174.96458435058594, 178.43721652030945, 181.9034333229065, 185.5371391773224, 188.9850137233734, 192.55951070785522, 196.0343954563141, 199.46109080314636, 202.94818711280823, 206.49788641929626, 209.98361945152283, 213.47305965423584, 216.96544981002808, 220.42768812179565, 223.91930437088013, 227.39184427261353, 230.8714623451233, 234.35139226913452, 238.08474683761597, 242.74947118759155, 246.36752891540527, 249.826762676239, 253.43387031555176, 257.07982635498047, 260.6889543533325, 264.34354281425476, 267.95579504966736, 271.54982018470764, 275.1793420314789, 278.71925377845764, 282.29400849342346, 285.8106143474579, 289.54809069633484, 293.0875458717346, 296.7818250656128, 300.77852511405945, 304.46562027931213, 308.00027894973755, 311.56287240982056, 315.1866750717163, 318.67661929130554, 322.1487421989441, 325.6409921646118, 329.1087951660156, 332.62073588371277, 336.15796399116516, 339.71501636505127, 343.21429419517517, 346.67768383026123, 350.1455979347229, 353.64892745018005, 355.05654549598694]
[28.3925, 53.6825, 63.1875, 72.96, 82.015, 87.025, 89.3775, 92.4, 92.885, 93.4125, 93.805, 94.27, 94.6325, 94.76, 94.915, 95.15, 95.2675, 95.3875, 95.6075, 95.815, 95.82, 95.8525, 96.065, 96.1325, 96.26, 96.39, 96.3825, 96.5325, 96.5975, 96.5325, 96.6025, 96.665, 96.7725, 96.76, 96.8475, 96.8875, 96.915, 96.9275, 97.0025, 97.025, 97.055, 97.0575, 97.065, 97.14, 97.13, 97.145, 97.1775, 97.21, 97.2025, 97.18, 97.25, 97.2675, 97.3075, 97.315, 97.2925, 97.3475, 97.365, 97.355, 97.3775, 97.4, 97.3975, 97.4, 97.4025, 97.4425, 97.385, 97.42, 97.4575, 97.41, 97.46, 97.47, 97.47, 97.445, 97.5025, 97.48, 97.4775, 97.4625, 97.49, 97.4975, 97.475, 97.47, 97.48, 97.51, 97.51, 97.5025, 97.475, 97.5025, 97.5275, 97.5275, 97.5475, 97.5425, 97.515, 97.56, 97.5275, 97.51, 97.5475, 97.5675, 97.5525, 97.565, 97.5975, 97.625, 97.5975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.312, Test loss: 2.281, Test accuracy: 28.07
Round   1, Train loss: 2.239, Test loss: 2.106, Test accuracy: 45.46
Round   2, Train loss: 1.989, Test loss: 1.886, Test accuracy: 71.42
Round   3, Train loss: 1.809, Test loss: 1.783, Test accuracy: 80.09
Round   4, Train loss: 1.742, Test loss: 1.744, Test accuracy: 82.14
Round   5, Train loss: 1.713, Test loss: 1.683, Test accuracy: 88.32
Round   6, Train loss: 1.659, Test loss: 1.638, Test accuracy: 90.94
Round   7, Train loss: 1.633, Test loss: 1.613, Test accuracy: 92.25
Round   8, Train loss: 1.609, Test loss: 1.600, Test accuracy: 92.83
Round   9, Train loss: 1.602, Test loss: 1.586, Test accuracy: 93.72
Round  10, Train loss: 1.576, Test loss: 1.586, Test accuracy: 94.15
Round  11, Train loss: 1.577, Test loss: 1.575, Test accuracy: 94.44
Round  12, Train loss: 1.579, Test loss: 1.561, Test accuracy: 94.79
Round  13, Train loss: 1.563, Test loss: 1.559, Test accuracy: 95.05
Round  14, Train loss: 1.564, Test loss: 1.552, Test accuracy: 95.32
Round  15, Train loss: 1.544, Test loss: 1.555, Test accuracy: 95.56
Round  16, Train loss: 1.559, Test loss: 1.543, Test accuracy: 95.58
Round  17, Train loss: 1.546, Test loss: 1.542, Test accuracy: 95.82
Round  18, Train loss: 1.548, Test loss: 1.537, Test accuracy: 95.91
Round  19, Train loss: 1.538, Test loss: 1.536, Test accuracy: 96.00
Round  20, Train loss: 1.536, Test loss: 1.534, Test accuracy: 96.10
Round  21, Train loss: 1.532, Test loss: 1.533, Test accuracy: 96.16
Round  22, Train loss: 1.535, Test loss: 1.530, Test accuracy: 96.18
Round  23, Train loss: 1.530, Test loss: 1.528, Test accuracy: 96.34
Round  24, Train loss: 1.530, Test loss: 1.526, Test accuracy: 96.32
Round  25, Train loss: 1.526, Test loss: 1.526, Test accuracy: 96.50
Round  26, Train loss: 1.527, Test loss: 1.522, Test accuracy: 96.62
Round  27, Train loss: 1.522, Test loss: 1.522, Test accuracy: 96.70
Round  28, Train loss: 1.516, Test loss: 1.523, Test accuracy: 96.69
Round  29, Train loss: 1.524, Test loss: 1.519, Test accuracy: 96.75
Round  30, Train loss: 1.513, Test loss: 1.521, Test accuracy: 96.80
Round  31, Train loss: 1.517, Test loss: 1.518, Test accuracy: 96.80
Round  32, Train loss: 1.516, Test loss: 1.518, Test accuracy: 96.91
Round  33, Train loss: 1.515, Test loss: 1.517, Test accuracy: 96.92
Round  34, Train loss: 1.514, Test loss: 1.515, Test accuracy: 96.95
Round  35, Train loss: 1.511, Test loss: 1.515, Test accuracy: 97.02
Round  36, Train loss: 1.510, Test loss: 1.514, Test accuracy: 97.09
Round  37, Train loss: 1.509, Test loss: 1.513, Test accuracy: 97.06
Round  38, Train loss: 1.508, Test loss: 1.513, Test accuracy: 97.14
Round  39, Train loss: 1.510, Test loss: 1.511, Test accuracy: 97.19
Round  40, Train loss: 1.506, Test loss: 1.511, Test accuracy: 97.23
Round  41, Train loss: 1.505, Test loss: 1.512, Test accuracy: 97.20
Round  42, Train loss: 1.505, Test loss: 1.511, Test accuracy: 97.27
Round  43, Train loss: 1.504, Test loss: 1.510, Test accuracy: 97.25
Round  44, Train loss: 1.503, Test loss: 1.510, Test accuracy: 97.24
Round  45, Train loss: 1.502, Test loss: 1.510, Test accuracy: 97.22
Round  46, Train loss: 1.502, Test loss: 1.510, Test accuracy: 97.22
Round  47, Train loss: 1.503, Test loss: 1.509, Test accuracy: 97.28
Round  48, Train loss: 1.499, Test loss: 1.509, Test accuracy: 97.33
Round  49, Train loss: 1.501, Test loss: 1.508, Test accuracy: 97.32
Round  50, Train loss: 1.500, Test loss: 1.508, Test accuracy: 97.35
Round  51, Train loss: 1.499, Test loss: 1.508, Test accuracy: 97.34
Round  52, Train loss: 1.501, Test loss: 1.506, Test accuracy: 97.42
Round  53, Train loss: 1.499, Test loss: 1.506, Test accuracy: 97.40
Round  54, Train loss: 1.497, Test loss: 1.507, Test accuracy: 97.42
Round  55, Train loss: 1.496, Test loss: 1.506, Test accuracy: 97.40
Round  56, Train loss: 1.497, Test loss: 1.506, Test accuracy: 97.46
Round  57, Train loss: 1.496, Test loss: 1.506, Test accuracy: 97.48
Round  58, Train loss: 1.497, Test loss: 1.506, Test accuracy: 97.49
Round  59, Train loss: 1.496, Test loss: 1.506, Test accuracy: 97.50
Round  60, Train loss: 1.494, Test loss: 1.506, Test accuracy: 97.48
Round  61, Train loss: 1.493, Test loss: 1.506, Test accuracy: 97.47
Round  62, Train loss: 1.496, Test loss: 1.504, Test accuracy: 97.54
Round  63, Train loss: 1.495, Test loss: 1.504, Test accuracy: 97.59
Round  64, Train loss: 1.495, Test loss: 1.504, Test accuracy: 97.55
Round  65, Train loss: 1.493, Test loss: 1.505, Test accuracy: 97.56
Round  66, Train loss: 1.493, Test loss: 1.504, Test accuracy: 97.56
Round  67, Train loss: 1.491, Test loss: 1.504, Test accuracy: 97.56
Round  68, Train loss: 1.494, Test loss: 1.503, Test accuracy: 97.59
Round  69, Train loss: 1.492, Test loss: 1.503, Test accuracy: 97.62
Round  70, Train loss: 1.492, Test loss: 1.503, Test accuracy: 97.59
Round  71, Train loss: 1.493, Test loss: 1.502, Test accuracy: 97.55
Round  72, Train loss: 1.492, Test loss: 1.503, Test accuracy: 97.57
Round  73, Train loss: 1.491, Test loss: 1.503, Test accuracy: 97.55
Round  74, Train loss: 1.491, Test loss: 1.503, Test accuracy: 97.62
Round  75, Train loss: 1.490, Test loss: 1.503, Test accuracy: 97.61
Round  76, Train loss: 1.490, Test loss: 1.503, Test accuracy: 97.63
Round  77, Train loss: 1.491, Test loss: 1.502, Test accuracy: 97.58
Round  78, Train loss: 1.490, Test loss: 1.502, Test accuracy: 97.61
Round  79, Train loss: 1.491, Test loss: 1.501, Test accuracy: 97.68
Round  80, Train loss: 1.489, Test loss: 1.502, Test accuracy: 97.69
Round  81, Train loss: 1.490, Test loss: 1.502, Test accuracy: 97.72
Round  82, Train loss: 1.489, Test loss: 1.502, Test accuracy: 97.72
Round  83, Train loss: 1.489, Test loss: 1.501, Test accuracy: 97.72
Round  84, Train loss: 1.489, Test loss: 1.502, Test accuracy: 97.73
Round  85, Train loss: 1.489, Test loss: 1.501, Test accuracy: 97.69
Round  86, Train loss: 1.488, Test loss: 1.502, Test accuracy: 97.61
Round  87, Train loss: 1.489, Test loss: 1.501, Test accuracy: 97.65
Round  88, Train loss: 1.487, Test loss: 1.501, Test accuracy: 97.73
Round  89, Train loss: 1.488, Test loss: 1.501, Test accuracy: 97.71
Round  90, Train loss: 1.487, Test loss: 1.501, Test accuracy: 97.73
Round  91, Train loss: 1.487, Test loss: 1.501, Test accuracy: 97.72
Round  92, Train loss: 1.487, Test loss: 1.501, Test accuracy: 97.69
Round  93, Train loss: 1.487, Test loss: 1.501, Test accuracy: 97.69/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.487, Test loss: 1.501, Test accuracy: 97.70
Round  95, Train loss: 1.487, Test loss: 1.501, Test accuracy: 97.69
Round  96, Train loss: 1.488, Test loss: 1.500, Test accuracy: 97.69
Round  97, Train loss: 1.486, Test loss: 1.501, Test accuracy: 97.65
Round  98, Train loss: 1.486, Test loss: 1.501, Test accuracy: 97.66
Round  99, Train loss: 1.486, Test loss: 1.500, Test accuracy: 97.64
Final Round, Train loss: 1.471, Test loss: 1.500, Test accuracy: 97.64
Average accuracy final 10 rounds: 97.6865
3937.372784614563
[3.6859359741210938, 7.3718719482421875, 11.016920328140259, 14.66196870803833, 18.38513159751892, 22.10829448699951, 25.851232528686523, 29.594170570373535, 33.09829568862915, 36.602420806884766, 40.19224834442139, 43.78207588195801, 47.59683656692505, 51.41159725189209, 54.975828886032104, 58.54006052017212, 62.22963213920593, 65.91920375823975, 69.51902103424072, 73.1188383102417, 76.84039330482483, 80.56194829940796, 84.09513258934021, 87.62831687927246, 91.29331111907959, 94.95830535888672, 98.64573502540588, 102.33316469192505, 105.84292197227478, 109.35267925262451, 112.83645915985107, 116.32023906707764, 120.30192375183105, 124.28360843658447, 127.86471486091614, 131.4458212852478, 134.91223239898682, 138.37864351272583, 143.00363445281982, 147.62862539291382, 151.32799696922302, 155.02736854553223, 158.85481810569763, 162.68226766586304, 166.27707076072693, 169.87187385559082, 173.43901896476746, 177.0061640739441, 181.52321243286133, 186.04026079177856, 189.73452258110046, 193.42878437042236, 197.91935014724731, 202.40991592407227, 207.39652848243713, 212.383141040802, 217.1167552471161, 221.85036945343018, 226.46971368789673, 231.08905792236328, 235.74998116493225, 240.41090440750122, 245.05235743522644, 249.69381046295166, 254.23444938659668, 258.7750883102417, 262.2709333896637, 265.7667784690857, 269.337233543396, 272.9076886177063, 276.46233797073364, 280.016987323761, 283.65776681900024, 287.2985463142395, 290.94501519203186, 294.5914840698242, 298.21080708503723, 301.83013010025024, 305.42669582366943, 309.0232615470886, 312.54943108558655, 316.0756006240845, 319.7297022342682, 323.3838038444519, 326.9340567588806, 330.4843096733093, 334.19346165657043, 337.90261363983154, 341.5002074241638, 345.0978012084961, 348.76847219467163, 352.43914318084717, 355.9916536808014, 359.5441641807556, 363.19661688804626, 366.8490695953369, 370.4608645439148, 374.0726594924927, 377.73509097099304, 381.3975224494934, 385.032678604126, 388.66783475875854, 392.2733368873596, 395.8788390159607, 399.4928226470947, 403.10680627822876, 406.6920051574707, 410.27720403671265, 413.794376373291, 417.3115487098694, 420.94450283050537, 424.57745695114136, 428.53622126579285, 432.49498558044434, 436.0168228149414, 439.5386600494385, 443.01311445236206, 446.48756885528564, 449.97385239601135, 453.46013593673706, 456.89752316474915, 460.33491039276123, 463.8605978488922, 467.3862853050232, 470.8514699935913, 474.3166546821594, 477.7845895290375, 481.2525243759155, 484.70643305778503, 488.16034173965454, 491.613392829895, 495.0664439201355, 498.4708797931671, 501.87531566619873, 505.356915473938, 508.83851528167725, 512.2885270118713, 515.7385387420654, 519.2783539295197, 522.8181691169739, 526.3607034683228, 529.9032378196716, 533.3947110176086, 536.8861842155457, 540.3289980888367, 543.7718119621277, 547.4610691070557, 551.1503262519836, 554.9163835048676, 558.6824407577515, 562.3255455493927, 565.9686503410339, 569.5841357707977, 573.1996212005615, 576.961808681488, 580.7239961624146, 584.1931972503662, 587.6623983383179, 591.3398909568787, 595.0173835754395, 598.7767963409424, 602.5362091064453, 606.2737712860107, 610.0113334655762, 613.7268080711365, 617.4422826766968, 621.1276786327362, 624.8130745887756, 628.5052089691162, 632.1973433494568, 635.8853390216827, 639.5733346939087, 643.2544958591461, 646.9356570243835, 650.6323485374451, 654.3290400505066, 657.9546751976013, 661.580310344696, 665.1976351737976, 668.8149600028992, 672.4347059726715, 676.0544519424438, 679.6279449462891, 683.2014379501343, 686.7780849933624, 690.3547320365906, 694.0356879234314, 697.7166438102722, 701.3600986003876, 705.0035533905029, 708.5913352966309, 712.1791172027588, 715.6559550762177, 719.1327929496765, 722.7614629268646, 726.3901329040527, 729.8915579319, 733.3929829597473, 737.2038900852203, 741.0147972106934, 742.5394446849823, 744.0640921592712]
[28.0725, 28.0725, 45.4575, 45.4575, 71.425, 71.425, 80.095, 80.095, 82.145, 82.145, 88.32, 88.32, 90.94, 90.94, 92.2475, 92.2475, 92.8325, 92.8325, 93.7175, 93.7175, 94.1525, 94.1525, 94.4425, 94.4425, 94.7875, 94.7875, 95.045, 95.045, 95.3175, 95.3175, 95.555, 95.555, 95.585, 95.585, 95.8175, 95.8175, 95.9075, 95.9075, 95.995, 95.995, 96.1, 96.1, 96.1575, 96.1575, 96.1775, 96.1775, 96.3425, 96.3425, 96.3225, 96.3225, 96.495, 96.495, 96.6175, 96.6175, 96.7, 96.7, 96.69, 96.69, 96.7525, 96.7525, 96.8, 96.8, 96.795, 96.795, 96.9075, 96.9075, 96.9175, 96.9175, 96.955, 96.955, 97.0225, 97.0225, 97.0925, 97.0925, 97.0625, 97.0625, 97.1425, 97.1425, 97.19, 97.19, 97.2275, 97.2275, 97.2025, 97.2025, 97.27, 97.27, 97.245, 97.245, 97.2425, 97.2425, 97.2175, 97.2175, 97.225, 97.225, 97.285, 97.285, 97.335, 97.335, 97.32, 97.32, 97.35, 97.35, 97.34, 97.34, 97.415, 97.415, 97.3975, 97.3975, 97.4225, 97.4225, 97.4025, 97.4025, 97.46, 97.46, 97.4825, 97.4825, 97.4925, 97.4925, 97.5, 97.5, 97.485, 97.485, 97.4725, 97.4725, 97.54, 97.54, 97.5875, 97.5875, 97.55, 97.55, 97.5575, 97.5575, 97.56, 97.56, 97.565, 97.565, 97.59, 97.59, 97.6175, 97.6175, 97.5875, 97.5875, 97.545, 97.545, 97.5675, 97.5675, 97.5475, 97.5475, 97.6175, 97.6175, 97.6125, 97.6125, 97.6325, 97.6325, 97.58, 97.58, 97.6075, 97.6075, 97.6825, 97.6825, 97.685, 97.685, 97.715, 97.715, 97.7175, 97.7175, 97.7175, 97.7175, 97.735, 97.735, 97.69, 97.69, 97.6125, 97.6125, 97.6525, 97.6525, 97.7275, 97.7275, 97.7125, 97.7125, 97.73, 97.73, 97.725, 97.725, 97.69, 97.69, 97.69, 97.69, 97.7025, 97.7025, 97.6925, 97.6925, 97.69, 97.69, 97.65, 97.65, 97.6575, 97.6575, 97.6375, 97.6375, 97.645, 97.645]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.167, Test loss: 2.109, Test accuracy: 36.90
Round   0, Global train loss: 2.167, Global test loss: 2.206, Global test accuracy: 28.87
Round   1, Train loss: 1.641, Test loss: 1.982, Test accuracy: 47.52
Round   1, Global train loss: 1.641, Global test loss: 2.134, Global test accuracy: 31.66
Round   2, Train loss: 1.779, Test loss: 1.839, Test accuracy: 62.21
Round   2, Global train loss: 1.779, Global test loss: 2.119, Global test accuracy: 33.37
Round   3, Train loss: 1.597, Test loss: 1.824, Test accuracy: 63.01
Round   3, Global train loss: 1.597, Global test loss: 2.155, Global test accuracy: 27.32
Round   4, Train loss: 1.585, Test loss: 1.723, Test accuracy: 75.21
Round   4, Global train loss: 1.585, Global test loss: 2.018, Global test accuracy: 46.26
Round   5, Train loss: 1.655, Test loss: 1.689, Test accuracy: 78.17
Round   5, Global train loss: 1.655, Global test loss: 2.063, Global test accuracy: 37.97
Round   6, Train loss: 1.619, Test loss: 1.668, Test accuracy: 80.73
Round   6, Global train loss: 1.619, Global test loss: 2.145, Global test accuracy: 33.53
Round   7, Train loss: 1.588, Test loss: 1.670, Test accuracy: 78.99
Round   7, Global train loss: 1.588, Global test loss: 2.148, Global test accuracy: 28.62
Round   8, Train loss: 1.596, Test loss: 1.638, Test accuracy: 82.22
Round   8, Global train loss: 1.596, Global test loss: 2.104, Global test accuracy: 32.28
Round   9, Train loss: 1.615, Test loss: 1.579, Test accuracy: 88.86
Round   9, Global train loss: 1.615, Global test loss: 2.076, Global test accuracy: 41.12
Round  10, Train loss: 1.477, Test loss: 1.577, Test accuracy: 88.92
Round  10, Global train loss: 1.477, Global test loss: 2.172, Global test accuracy: 24.69
Round  11, Train loss: 1.584, Test loss: 1.576, Test accuracy: 88.89
Round  11, Global train loss: 1.584, Global test loss: 2.192, Global test accuracy: 25.00
Round  12, Train loss: 1.475, Test loss: 1.576, Test accuracy: 88.90
Round  12, Global train loss: 1.475, Global test loss: 2.055, Global test accuracy: 39.05
Round  13, Train loss: 1.582, Test loss: 1.575, Test accuracy: 88.89
Round  13, Global train loss: 1.582, Global test loss: 2.043, Global test accuracy: 39.93
Round  14, Train loss: 1.538, Test loss: 1.564, Test accuracy: 90.03
Round  14, Global train loss: 1.538, Global test loss: 2.133, Global test accuracy: 29.25
Round  15, Train loss: 1.583, Test loss: 1.563, Test accuracy: 90.05
Round  15, Global train loss: 1.583, Global test loss: 2.185, Global test accuracy: 24.34
Round  16, Train loss: 1.470, Test loss: 1.563, Test accuracy: 90.11
Round  16, Global train loss: 1.470, Global test loss: 2.109, Global test accuracy: 38.01
Round  17, Train loss: 1.525, Test loss: 1.563, Test accuracy: 90.14
Round  17, Global train loss: 1.525, Global test loss: 2.061, Global test accuracy: 39.01
Round  18, Train loss: 1.524, Test loss: 1.563, Test accuracy: 90.10
Round  18, Global train loss: 1.524, Global test loss: 2.021, Global test accuracy: 47.67
Round  19, Train loss: 1.545, Test loss: 1.547, Test accuracy: 91.75
Round  19, Global train loss: 1.545, Global test loss: 1.972, Global test accuracy: 62.21
Round  20, Train loss: 1.469, Test loss: 1.547, Test accuracy: 91.72
Round  20, Global train loss: 1.469, Global test loss: 2.086, Global test accuracy: 33.27
Round  21, Train loss: 1.521, Test loss: 1.547, Test accuracy: 91.72
Round  21, Global train loss: 1.521, Global test loss: 2.070, Global test accuracy: 37.35
Round  22, Train loss: 1.468, Test loss: 1.547, Test accuracy: 91.73
Round  22, Global train loss: 1.468, Global test loss: 2.053, Global test accuracy: 40.38
Round  23, Train loss: 1.526, Test loss: 1.546, Test accuracy: 91.74
Round  23, Global train loss: 1.526, Global test loss: 2.176, Global test accuracy: 27.51
Round  24, Train loss: 1.547, Test loss: 1.532, Test accuracy: 93.24
Round  24, Global train loss: 1.547, Global test loss: 2.082, Global test accuracy: 36.59
Round  25, Train loss: 1.467, Test loss: 1.532, Test accuracy: 93.22
Round  25, Global train loss: 1.467, Global test loss: 2.074, Global test accuracy: 36.41
Round  26, Train loss: 1.528, Test loss: 1.531, Test accuracy: 93.23
Round  26, Global train loss: 1.528, Global test loss: 2.056, Global test accuracy: 39.05
Round  27, Train loss: 1.469, Test loss: 1.531, Test accuracy: 93.25
Round  27, Global train loss: 1.469, Global test loss: 1.959, Global test accuracy: 55.09
Round  28, Train loss: 1.470, Test loss: 1.531, Test accuracy: 93.22
Round  28, Global train loss: 1.470, Global test loss: 2.056, Global test accuracy: 37.67
Round  29, Train loss: 1.525, Test loss: 1.531, Test accuracy: 93.20
Round  29, Global train loss: 1.525, Global test loss: 2.071, Global test accuracy: 36.23
Round  30, Train loss: 1.523, Test loss: 1.531, Test accuracy: 93.20
Round  30, Global train loss: 1.523, Global test loss: 2.039, Global test accuracy: 41.29
Round  31, Train loss: 1.525, Test loss: 1.531, Test accuracy: 93.21
Round  31, Global train loss: 1.525, Global test loss: 2.014, Global test accuracy: 44.33
Round  32, Train loss: 1.521, Test loss: 1.531, Test accuracy: 93.21
Round  32, Global train loss: 1.521, Global test loss: 2.075, Global test accuracy: 38.82
Round  33, Train loss: 1.521, Test loss: 1.531, Test accuracy: 93.21
Round  33, Global train loss: 1.521, Global test loss: 2.090, Global test accuracy: 38.85
Round  34, Train loss: 1.466, Test loss: 1.530, Test accuracy: 93.21
Round  34, Global train loss: 1.466, Global test loss: 2.010, Global test accuracy: 43.93
Round  35, Train loss: 1.468, Test loss: 1.530, Test accuracy: 93.18
Round  35, Global train loss: 1.468, Global test loss: 2.077, Global test accuracy: 33.73
Round  36, Train loss: 1.522, Test loss: 1.530, Test accuracy: 93.21
Round  36, Global train loss: 1.522, Global test loss: 2.034, Global test accuracy: 43.17
Round  37, Train loss: 1.523, Test loss: 1.530, Test accuracy: 93.22
Round  37, Global train loss: 1.523, Global test loss: 2.004, Global test accuracy: 49.42
Round  38, Train loss: 1.521, Test loss: 1.530, Test accuracy: 93.22
Round  38, Global train loss: 1.521, Global test loss: 2.070, Global test accuracy: 41.87
Round  39, Train loss: 1.521, Test loss: 1.530, Test accuracy: 93.21
Round  39, Global train loss: 1.521, Global test loss: 2.008, Global test accuracy: 49.05
Round  40, Train loss: 1.524, Test loss: 1.530, Test accuracy: 93.21
Round  40, Global train loss: 1.524, Global test loss: 2.133, Global test accuracy: 28.24
Round  41, Train loss: 1.520, Test loss: 1.530, Test accuracy: 93.21
Round  41, Global train loss: 1.520, Global test loss: 2.041, Global test accuracy: 52.35
Round  42, Train loss: 1.520, Test loss: 1.530, Test accuracy: 93.19
Round  42, Global train loss: 1.520, Global test loss: 2.082, Global test accuracy: 38.06
Round  43, Train loss: 1.468, Test loss: 1.530, Test accuracy: 93.18
Round  43, Global train loss: 1.468, Global test loss: 2.037, Global test accuracy: 51.33
Round  44, Train loss: 1.470, Test loss: 1.530, Test accuracy: 93.20
Round  44, Global train loss: 1.470, Global test loss: 1.984, Global test accuracy: 52.95
Round  45, Train loss: 1.523, Test loss: 1.530, Test accuracy: 93.18
Round  45, Global train loss: 1.523, Global test loss: 2.042, Global test accuracy: 44.72
Round  46, Train loss: 1.523, Test loss: 1.530, Test accuracy: 93.22
Round  46, Global train loss: 1.523, Global test loss: 2.136, Global test accuracy: 30.05
Round  47, Train loss: 1.470, Test loss: 1.529, Test accuracy: 93.22
Round  47, Global train loss: 1.470, Global test loss: 2.061, Global test accuracy: 37.76
Round  48, Train loss: 1.468, Test loss: 1.529, Test accuracy: 93.23
Round  48, Global train loss: 1.468, Global test loss: 2.084, Global test accuracy: 34.60
Round  49, Train loss: 1.519, Test loss: 1.529, Test accuracy: 93.24
Round  49, Global train loss: 1.519, Global test loss: 2.051, Global test accuracy: 40.49
Round  50, Train loss: 1.521, Test loss: 1.529, Test accuracy: 93.27
Round  50, Global train loss: 1.521, Global test loss: 2.058, Global test accuracy: 37.26
Round  51, Train loss: 1.520, Test loss: 1.529, Test accuracy: 93.28
Round  51, Global train loss: 1.520, Global test loss: 2.161, Global test accuracy: 28.31
Round  52, Train loss: 1.520, Test loss: 1.529, Test accuracy: 93.27
Round  52, Global train loss: 1.520, Global test loss: 2.062, Global test accuracy: 47.30
Round  53, Train loss: 1.521, Test loss: 1.529, Test accuracy: 93.27
Round  53, Global train loss: 1.521, Global test loss: 1.981, Global test accuracy: 50.95
Round  54, Train loss: 1.468, Test loss: 1.529, Test accuracy: 93.27
Round  54, Global train loss: 1.468, Global test loss: 2.063, Global test accuracy: 40.76
Round  55, Train loss: 1.519, Test loss: 1.529, Test accuracy: 93.30
Round  55, Global train loss: 1.519, Global test loss: 1.957, Global test accuracy: 60.12
Round  56, Train loss: 1.521, Test loss: 1.529, Test accuracy: 93.32
Round  56, Global train loss: 1.521, Global test loss: 2.028, Global test accuracy: 42.02
Round  57, Train loss: 1.521, Test loss: 1.529, Test accuracy: 93.32
Round  57, Global train loss: 1.521, Global test loss: 2.093, Global test accuracy: 32.41
Round  58, Train loss: 1.521, Test loss: 1.529, Test accuracy: 93.33
Round  58, Global train loss: 1.521, Global test loss: 2.014, Global test accuracy: 48.90
Round  59, Train loss: 1.469, Test loss: 1.529, Test accuracy: 93.33
Round  59, Global train loss: 1.469, Global test loss: 2.164, Global test accuracy: 24.11
Round  60, Train loss: 1.518, Test loss: 1.529, Test accuracy: 93.33
Round  60, Global train loss: 1.518, Global test loss: 2.141, Global test accuracy: 28.10
Round  61, Train loss: 1.466, Test loss: 1.529, Test accuracy: 93.33
Round  61, Global train loss: 1.466, Global test loss: 2.061, Global test accuracy: 42.22
Round  62, Train loss: 1.467, Test loss: 1.529, Test accuracy: 93.33
Round  62, Global train loss: 1.467, Global test loss: 2.037, Global test accuracy: 42.08
Round  63, Train loss: 1.520, Test loss: 1.529, Test accuracy: 93.33
Round  63, Global train loss: 1.520, Global test loss: 2.040, Global test accuracy: 43.71
Round  64, Train loss: 1.466, Test loss: 1.529, Test accuracy: 93.33
Round  64, Global train loss: 1.466, Global test loss: 2.087, Global test accuracy: 38.31
Round  65, Train loss: 1.468, Test loss: 1.529, Test accuracy: 93.33
Round  65, Global train loss: 1.468, Global test loss: 2.098, Global test accuracy: 30.50
Round  66, Train loss: 1.574, Test loss: 1.529, Test accuracy: 93.30
Round  66, Global train loss: 1.574, Global test loss: 2.063, Global test accuracy: 39.33
Round  67, Train loss: 1.466, Test loss: 1.529, Test accuracy: 93.30
Round  67, Global train loss: 1.466, Global test loss: 2.186, Global test accuracy: 24.73
Round  68, Train loss: 1.573, Test loss: 1.529, Test accuracy: 93.32
Round  68, Global train loss: 1.573, Global test loss: 2.020, Global test accuracy: 42.62
Round  69, Train loss: 1.522, Test loss: 1.529, Test accuracy: 93.33
Round  69, Global train loss: 1.522, Global test loss: 2.022, Global test accuracy: 45.38
Round  70, Train loss: 1.577, Test loss: 1.529, Test accuracy: 93.33
Round  70, Global train loss: 1.577, Global test loss: 2.217, Global test accuracy: 23.73
Round  71, Train loss: 1.467, Test loss: 1.529, Test accuracy: 93.33
Round  71, Global train loss: 1.467, Global test loss: 2.088, Global test accuracy: 35.60
Round  72, Train loss: 1.467, Test loss: 1.529, Test accuracy: 93.33
Round  72, Global train loss: 1.467, Global test loss: 1.989, Global test accuracy: 52.26
Round  73, Train loss: 1.466, Test loss: 1.529, Test accuracy: 93.29
Round  73, Global train loss: 1.466, Global test loss: 2.101, Global test accuracy: 36.70
Round  74, Train loss: 1.470, Test loss: 1.529, Test accuracy: 93.28
Round  74, Global train loss: 1.470, Global test loss: 2.020, Global test accuracy: 47.19
Round  75, Train loss: 1.522, Test loss: 1.529, Test accuracy: 93.28
Round  75, Global train loss: 1.522, Global test loss: 1.962, Global test accuracy: 55.93
Round  76, Train loss: 1.469, Test loss: 1.529, Test accuracy: 93.28
Round  76, Global train loss: 1.469, Global test loss: 2.059, Global test accuracy: 41.86
Round  77, Train loss: 1.522, Test loss: 1.529, Test accuracy: 93.30
Round  77, Global train loss: 1.522, Global test loss: 2.003, Global test accuracy: 53.30
Round  78, Train loss: 1.465, Test loss: 1.529, Test accuracy: 93.32
Round  78, Global train loss: 1.465, Global test loss: 2.156, Global test accuracy: 27.88
Round  79, Train loss: 1.467, Test loss: 1.529, Test accuracy: 93.30
Round  79, Global train loss: 1.467, Global test loss: 2.006, Global test accuracy: 52.61
Round  80, Train loss: 1.467, Test loss: 1.529, Test accuracy: 93.31
Round  80, Global train loss: 1.467, Global test loss: 2.098, Global test accuracy: 33.41
Round  81, Train loss: 1.467, Test loss: 1.529, Test accuracy: 93.33
Round  81, Global train loss: 1.467, Global test loss: 2.072, Global test accuracy: 41.57
Round  82, Train loss: 1.470, Test loss: 1.529, Test accuracy: 93.35
Round  82, Global train loss: 1.470, Global test loss: 2.011, Global test accuracy: 45.03
Round  83, Train loss: 1.521, Test loss: 1.529, Test accuracy: 93.35
Round  83, Global train loss: 1.521, Global test loss: 1.980, Global test accuracy: 53.31
Round  84, Train loss: 1.521, Test loss: 1.529, Test accuracy: 93.35
Round  84, Global train loss: 1.521, Global test loss: 2.044, Global test accuracy: 39.79
Round  85, Train loss: 1.468, Test loss: 1.529, Test accuracy: 93.34
Round  85, Global train loss: 1.468, Global test loss: 1.981, Global test accuracy: 45.71
Round  86, Train loss: 1.522, Test loss: 1.529, Test accuracy: 93.34
Round  86, Global train loss: 1.522, Global test loss: 2.139, Global test accuracy: 29.12
Round  87, Train loss: 1.520, Test loss: 1.529, Test accuracy: 93.34
Round  87, Global train loss: 1.520, Global test loss: 2.104, Global test accuracy: 29.30
Round  88, Train loss: 1.465, Test loss: 1.529, Test accuracy: 93.34
Round  88, Global train loss: 1.465, Global test loss: 1.994, Global test accuracy: 49.11
Round  89, Train loss: 1.519, Test loss: 1.529, Test accuracy: 93.32
Round  89, Global train loss: 1.519, Global test loss: 1.988, Global test accuracy: 54.26
Round  90, Train loss: 1.520, Test loss: 1.529, Test accuracy: 93.29
Round  90, Global train loss: 1.520, Global test loss: 2.060, Global test accuracy: 42.50
Round  91, Train loss: 1.467, Test loss: 1.529, Test accuracy: 93.29
Round  91, Global train loss: 1.467, Global test loss: 2.000, Global test accuracy: 48.13
Round  92, Train loss: 1.522, Test loss: 1.529, Test accuracy: 93.28
Round  92, Global train loss: 1.522, Global test loss: 2.035, Global test accuracy: 44.14
Round  93, Train loss: 1.522, Test loss: 1.529, Test accuracy: 93.29
Round  93, Global train loss: 1.522, Global test loss: 2.066, Global test accuracy: 42.18
Round  94, Train loss: 1.520, Test loss: 1.529, Test accuracy: 93.29
Round  94, Global train loss: 1.520, Global test loss: 1.995, Global test accuracy: 50.08
Round  95, Train loss: 1.520, Test loss: 1.529, Test accuracy: 93.31
Round  95, Global train loss: 1.520, Global test loss: 1.985, Global test accuracy: 50.51/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.467, Test loss: 1.529, Test accuracy: 93.32
Round  96, Global train loss: 1.467, Global test loss: 2.135, Global test accuracy: 28.67
Round  97, Train loss: 1.572, Test loss: 1.529, Test accuracy: 93.33
Round  97, Global train loss: 1.572, Global test loss: 1.999, Global test accuracy: 48.79
Round  98, Train loss: 1.522, Test loss: 1.529, Test accuracy: 93.33
Round  98, Global train loss: 1.522, Global test loss: 2.049, Global test accuracy: 43.51
Round  99, Train loss: 1.521, Test loss: 1.529, Test accuracy: 93.33
Round  99, Global train loss: 1.521, Global test loss: 2.029, Global test accuracy: 42.14
Final Round, Train loss: 1.498, Test loss: 1.528, Test accuracy: 93.39
Final Round, Global train loss: 1.498, Global test loss: 2.029, Global test accuracy: 42.14
Average accuracy final 10 rounds: 93.30749999999999 

Average global accuracy final 10 rounds: 44.065 

1305.9453341960907
[1.0043563842773438, 2.0087127685546875, 2.823716163635254, 3.6387195587158203, 4.466716766357422, 5.294713973999023, 6.108640193939209, 6.9225664138793945, 7.775012493133545, 8.627458572387695, 9.43721604347229, 10.246973514556885, 11.076561450958252, 11.90614938735962, 12.796678304672241, 13.687207221984863, 14.562742233276367, 15.438277244567871, 16.24759268760681, 17.056908130645752, 17.89219832420349, 18.72748851776123, 19.54076337814331, 20.35403823852539, 21.19898223876953, 22.043926239013672, 22.860633611679077, 23.677340984344482, 24.493461847305298, 25.309582710266113, 26.127883911132812, 26.94618511199951, 27.78149700164795, 28.616808891296387, 29.442386627197266, 30.267964363098145, 31.100780963897705, 31.933597564697266, 32.77364134788513, 33.613685131073, 34.43530225753784, 35.256919384002686, 36.13022994995117, 37.00354051589966, 37.827664852142334, 38.65178918838501, 39.48746085166931, 40.32313251495361, 41.187976360321045, 42.05282020568848, 42.972970485687256, 43.893120765686035, 44.723390102386475, 45.553659439086914, 46.46484398841858, 47.376028537750244, 48.21857929229736, 49.06113004684448, 49.951070070266724, 50.841010093688965, 51.73206567764282, 52.62312126159668, 53.46513223648071, 54.307143211364746, 55.13194918632507, 55.9567551612854, 56.78236103057861, 57.607966899871826, 58.45134878158569, 59.29473066329956, 60.191866874694824, 61.08900308609009, 61.926082372665405, 62.76316165924072, 63.585331201553345, 64.40750074386597, 65.25194692611694, 66.09639310836792, 66.91398978233337, 67.73158645629883, 68.55214238166809, 69.37269830703735, 70.20315289497375, 71.03360748291016, 71.88784265518188, 72.74207782745361, 73.58232975006104, 74.42258167266846, 75.22677779197693, 76.0309739112854, 76.85158920288086, 77.67220449447632, 78.52063512802124, 79.36906576156616, 80.20011734962463, 81.0311689376831, 81.87159705162048, 82.71202516555786, 83.54343676567078, 84.37484836578369, 85.24859499931335, 86.12234163284302, 86.97982430458069, 87.83730697631836, 88.67519521713257, 89.51308345794678, 90.34035468101501, 91.16762590408325, 91.9963550567627, 92.82508420944214, 93.68101787567139, 94.53695154190063, 95.3763427734375, 96.21573400497437, 97.12444472312927, 98.03315544128418, 98.8486385345459, 99.66412162780762, 100.53225994110107, 101.40039825439453, 102.22742033004761, 103.05444240570068, 103.9021635055542, 104.74988460540771, 105.6168532371521, 106.48382186889648, 107.35633730888367, 108.22885274887085, 109.05695700645447, 109.88506126403809, 110.7647774219513, 111.6444935798645, 112.45896053314209, 113.27342748641968, 114.09110140800476, 114.90877532958984, 115.74329614639282, 116.5778169631958, 117.4092333316803, 118.2406497001648, 119.04497146606445, 119.84929323196411, 120.67473912239075, 121.50018501281738, 122.33828735351562, 123.17638969421387, 123.99454498291016, 124.81270027160645, 125.63913106918335, 126.46556186676025, 127.28156018257141, 128.09755849838257, 128.9236524105072, 129.74974632263184, 130.59478759765625, 131.43982887268066, 132.29513597488403, 133.1504430770874, 133.98867964744568, 134.82691621780396, 135.66027641296387, 136.49363660812378, 137.34070420265198, 138.18777179718018, 139.03631114959717, 139.88485050201416, 140.72964668273926, 141.57444286346436, 142.39134287834167, 143.208242893219, 144.139142036438, 145.07004117965698, 145.98560237884521, 146.90116357803345, 147.82528352737427, 148.7494034767151, 149.58096647262573, 150.41252946853638, 151.2552297115326, 152.0979299545288, 152.96210169792175, 153.8262734413147, 154.6566936969757, 155.48711395263672, 156.3214545249939, 157.15579509735107, 157.9978470802307, 158.83989906311035, 159.67727994918823, 160.5146608352661, 161.34112000465393, 162.16757917404175, 162.98125553131104, 163.79493188858032, 164.6226212978363, 165.45031070709229, 166.28219509124756, 167.11407947540283, 167.94914531707764, 168.78421115875244, 170.2038516998291, 171.62349224090576]
[36.9, 36.9, 47.516666666666666, 47.516666666666666, 62.208333333333336, 62.208333333333336, 63.00833333333333, 63.00833333333333, 75.20833333333333, 75.20833333333333, 78.16666666666667, 78.16666666666667, 80.73333333333333, 80.73333333333333, 78.99166666666666, 78.99166666666666, 82.21666666666667, 82.21666666666667, 88.85833333333333, 88.85833333333333, 88.925, 88.925, 88.89166666666667, 88.89166666666667, 88.9, 88.9, 88.89166666666667, 88.89166666666667, 90.03333333333333, 90.03333333333333, 90.05, 90.05, 90.10833333333333, 90.10833333333333, 90.14166666666667, 90.14166666666667, 90.1, 90.1, 91.75, 91.75, 91.725, 91.725, 91.71666666666667, 91.71666666666667, 91.73333333333333, 91.73333333333333, 91.74166666666666, 91.74166666666666, 93.24166666666666, 93.24166666666666, 93.225, 93.225, 93.23333333333333, 93.23333333333333, 93.25, 93.25, 93.225, 93.225, 93.2, 93.2, 93.2, 93.2, 93.20833333333333, 93.20833333333333, 93.20833333333333, 93.20833333333333, 93.20833333333333, 93.20833333333333, 93.20833333333333, 93.20833333333333, 93.18333333333334, 93.18333333333334, 93.20833333333333, 93.20833333333333, 93.21666666666667, 93.21666666666667, 93.225, 93.225, 93.20833333333333, 93.20833333333333, 93.20833333333333, 93.20833333333333, 93.20833333333333, 93.20833333333333, 93.19166666666666, 93.19166666666666, 93.18333333333334, 93.18333333333334, 93.2, 93.2, 93.18333333333334, 93.18333333333334, 93.21666666666667, 93.21666666666667, 93.225, 93.225, 93.23333333333333, 93.23333333333333, 93.24166666666666, 93.24166666666666, 93.26666666666667, 93.26666666666667, 93.275, 93.275, 93.26666666666667, 93.26666666666667, 93.26666666666667, 93.26666666666667, 93.26666666666667, 93.26666666666667, 93.3, 93.3, 93.31666666666666, 93.31666666666666, 93.31666666666666, 93.31666666666666, 93.325, 93.325, 93.33333333333333, 93.33333333333333, 93.325, 93.325, 93.325, 93.325, 93.325, 93.325, 93.325, 93.325, 93.325, 93.325, 93.325, 93.325, 93.3, 93.3, 93.3, 93.3, 93.31666666666666, 93.31666666666666, 93.325, 93.325, 93.325, 93.325, 93.325, 93.325, 93.325, 93.325, 93.29166666666667, 93.29166666666667, 93.28333333333333, 93.28333333333333, 93.275, 93.275, 93.275, 93.275, 93.3, 93.3, 93.31666666666666, 93.31666666666666, 93.3, 93.3, 93.30833333333334, 93.30833333333334, 93.325, 93.325, 93.35, 93.35, 93.35, 93.35, 93.35, 93.35, 93.34166666666667, 93.34166666666667, 93.34166666666667, 93.34166666666667, 93.34166666666667, 93.34166666666667, 93.34166666666667, 93.34166666666667, 93.31666666666666, 93.31666666666666, 93.29166666666667, 93.29166666666667, 93.29166666666667, 93.29166666666667, 93.28333333333333, 93.28333333333333, 93.29166666666667, 93.29166666666667, 93.29166666666667, 93.29166666666667, 93.30833333333334, 93.30833333333334, 93.31666666666666, 93.31666666666666, 93.33333333333333, 93.33333333333333, 93.33333333333333, 93.33333333333333, 93.33333333333333, 93.33333333333333, 93.39166666666667, 93.39166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.205, Test loss: 2.168, Test accuracy: 32.83
Round   0, Global train loss: 2.205, Global test loss: 2.259, Global test accuracy: 23.67
Round   1, Train loss: 1.828, Test loss: 1.970, Test accuracy: 52.37
Round   1, Global train loss: 1.828, Global test loss: 2.127, Global test accuracy: 41.24
Round   2, Train loss: 1.797, Test loss: 1.879, Test accuracy: 60.36
Round   2, Global train loss: 1.797, Global test loss: 2.064, Global test accuracy: 46.64
Round   3, Train loss: 1.676, Test loss: 1.831, Test accuracy: 63.47
Round   3, Global train loss: 1.676, Global test loss: 1.975, Global test accuracy: 48.51
Round   4, Train loss: 1.689, Test loss: 1.772, Test accuracy: 71.01
Round   4, Global train loss: 1.689, Global test loss: 1.910, Global test accuracy: 62.65
Round   5, Train loss: 1.709, Test loss: 1.692, Test accuracy: 78.58
Round   5, Global train loss: 1.709, Global test loss: 1.840, Global test accuracy: 68.29
Round   6, Train loss: 1.557, Test loss: 1.648, Test accuracy: 82.29
Round   6, Global train loss: 1.557, Global test loss: 1.826, Global test accuracy: 69.34
Round   7, Train loss: 1.509, Test loss: 1.646, Test accuracy: 82.38
Round   7, Global train loss: 1.509, Global test loss: 1.814, Global test accuracy: 68.93
Round   8, Train loss: 1.516, Test loss: 1.617, Test accuracy: 85.40
Round   8, Global train loss: 1.516, Global test loss: 1.794, Global test accuracy: 67.38
Round   9, Train loss: 1.514, Test loss: 1.603, Test accuracy: 86.87
Round   9, Global train loss: 1.514, Global test loss: 1.752, Global test accuracy: 73.79
Round  10, Train loss: 1.503, Test loss: 1.563, Test accuracy: 90.62
Round  10, Global train loss: 1.503, Global test loss: 1.779, Global test accuracy: 70.57
Round  11, Train loss: 1.506, Test loss: 1.538, Test accuracy: 93.03
Round  11, Global train loss: 1.506, Global test loss: 1.776, Global test accuracy: 70.12
Round  12, Train loss: 1.492, Test loss: 1.537, Test accuracy: 93.09
Round  12, Global train loss: 1.492, Global test loss: 1.770, Global test accuracy: 70.65
Round  13, Train loss: 1.498, Test loss: 1.535, Test accuracy: 93.20
Round  13, Global train loss: 1.498, Global test loss: 1.735, Global test accuracy: 74.96
Round  14, Train loss: 1.490, Test loss: 1.534, Test accuracy: 93.17
Round  14, Global train loss: 1.490, Global test loss: 1.739, Global test accuracy: 73.84
Round  15, Train loss: 1.497, Test loss: 1.534, Test accuracy: 93.21
Round  15, Global train loss: 1.497, Global test loss: 1.725, Global test accuracy: 76.64
Round  16, Train loss: 1.496, Test loss: 1.532, Test accuracy: 93.44
Round  16, Global train loss: 1.496, Global test loss: 1.763, Global test accuracy: 71.49
Round  17, Train loss: 1.488, Test loss: 1.531, Test accuracy: 93.47
Round  17, Global train loss: 1.488, Global test loss: 1.741, Global test accuracy: 73.15
Round  18, Train loss: 1.490, Test loss: 1.532, Test accuracy: 93.38
Round  18, Global train loss: 1.490, Global test loss: 1.788, Global test accuracy: 67.09
Round  19, Train loss: 1.490, Test loss: 1.532, Test accuracy: 93.36
Round  19, Global train loss: 1.490, Global test loss: 1.706, Global test accuracy: 76.28
Round  20, Train loss: 1.547, Test loss: 1.518, Test accuracy: 94.68
Round  20, Global train loss: 1.547, Global test loss: 1.715, Global test accuracy: 75.43
Round  21, Train loss: 1.491, Test loss: 1.517, Test accuracy: 94.72
Round  21, Global train loss: 1.491, Global test loss: 1.715, Global test accuracy: 75.70
Round  22, Train loss: 1.482, Test loss: 1.517, Test accuracy: 94.69
Round  22, Global train loss: 1.482, Global test loss: 1.741, Global test accuracy: 71.79
Round  23, Train loss: 1.480, Test loss: 1.517, Test accuracy: 94.77
Round  23, Global train loss: 1.480, Global test loss: 1.718, Global test accuracy: 74.86
Round  24, Train loss: 1.545, Test loss: 1.516, Test accuracy: 94.82
Round  24, Global train loss: 1.545, Global test loss: 1.716, Global test accuracy: 74.80
Round  25, Train loss: 1.487, Test loss: 1.516, Test accuracy: 94.78
Round  25, Global train loss: 1.487, Global test loss: 1.714, Global test accuracy: 75.60
Round  26, Train loss: 1.537, Test loss: 1.516, Test accuracy: 94.80
Round  26, Global train loss: 1.537, Global test loss: 1.759, Global test accuracy: 70.03
Round  27, Train loss: 1.485, Test loss: 1.515, Test accuracy: 94.80
Round  27, Global train loss: 1.485, Global test loss: 1.673, Global test accuracy: 81.31
Round  28, Train loss: 1.482, Test loss: 1.516, Test accuracy: 94.81
Round  28, Global train loss: 1.482, Global test loss: 1.707, Global test accuracy: 76.67
Round  29, Train loss: 1.544, Test loss: 1.515, Test accuracy: 94.92
Round  29, Global train loss: 1.544, Global test loss: 1.709, Global test accuracy: 76.02
Round  30, Train loss: 1.481, Test loss: 1.516, Test accuracy: 94.89
Round  30, Global train loss: 1.481, Global test loss: 1.743, Global test accuracy: 72.47
Round  31, Train loss: 1.543, Test loss: 1.515, Test accuracy: 94.88
Round  31, Global train loss: 1.543, Global test loss: 1.672, Global test accuracy: 80.94
Round  32, Train loss: 1.482, Test loss: 1.514, Test accuracy: 94.97
Round  32, Global train loss: 1.482, Global test loss: 1.661, Global test accuracy: 81.14
Round  33, Train loss: 1.476, Test loss: 1.514, Test accuracy: 95.03
Round  33, Global train loss: 1.476, Global test loss: 1.685, Global test accuracy: 78.35
Round  34, Train loss: 1.536, Test loss: 1.514, Test accuracy: 95.02
Round  34, Global train loss: 1.536, Global test loss: 1.665, Global test accuracy: 79.36
Round  35, Train loss: 1.538, Test loss: 1.515, Test accuracy: 94.96
Round  35, Global train loss: 1.538, Global test loss: 1.655, Global test accuracy: 82.20
Round  36, Train loss: 1.479, Test loss: 1.515, Test accuracy: 94.93
Round  36, Global train loss: 1.479, Global test loss: 1.703, Global test accuracy: 76.59
Round  37, Train loss: 1.476, Test loss: 1.514, Test accuracy: 94.98
Round  37, Global train loss: 1.476, Global test loss: 1.747, Global test accuracy: 71.25
Round  38, Train loss: 1.476, Test loss: 1.514, Test accuracy: 95.03
Round  38, Global train loss: 1.476, Global test loss: 1.714, Global test accuracy: 73.53
Round  39, Train loss: 1.532, Test loss: 1.513, Test accuracy: 95.08
Round  39, Global train loss: 1.532, Global test loss: 1.697, Global test accuracy: 77.65
Round  40, Train loss: 1.476, Test loss: 1.513, Test accuracy: 95.08
Round  40, Global train loss: 1.476, Global test loss: 1.670, Global test accuracy: 79.44
Round  41, Train loss: 1.535, Test loss: 1.514, Test accuracy: 95.02
Round  41, Global train loss: 1.535, Global test loss: 1.646, Global test accuracy: 82.70
Round  42, Train loss: 1.530, Test loss: 1.513, Test accuracy: 95.03
Round  42, Global train loss: 1.530, Global test loss: 1.650, Global test accuracy: 81.97
Round  43, Train loss: 1.477, Test loss: 1.513, Test accuracy: 95.05
Round  43, Global train loss: 1.477, Global test loss: 1.670, Global test accuracy: 81.94
Round  44, Train loss: 1.474, Test loss: 1.512, Test accuracy: 95.17
Round  44, Global train loss: 1.474, Global test loss: 1.675, Global test accuracy: 79.39
Round  45, Train loss: 1.478, Test loss: 1.512, Test accuracy: 95.14
Round  45, Global train loss: 1.478, Global test loss: 1.676, Global test accuracy: 78.49
Round  46, Train loss: 1.481, Test loss: 1.512, Test accuracy: 95.17
Round  46, Global train loss: 1.481, Global test loss: 1.705, Global test accuracy: 77.36
Round  47, Train loss: 1.478, Test loss: 1.512, Test accuracy: 95.14
Round  47, Global train loss: 1.478, Global test loss: 1.686, Global test accuracy: 77.91
Round  48, Train loss: 1.469, Test loss: 1.512, Test accuracy: 95.12
Round  48, Global train loss: 1.469, Global test loss: 1.698, Global test accuracy: 75.31
Round  49, Train loss: 1.477, Test loss: 1.511, Test accuracy: 95.18
Round  49, Global train loss: 1.477, Global test loss: 1.709, Global test accuracy: 74.77
Round  50, Train loss: 1.478, Test loss: 1.512, Test accuracy: 95.15
Round  50, Global train loss: 1.478, Global test loss: 1.707, Global test accuracy: 75.27
Round  51, Train loss: 1.529, Test loss: 1.511, Test accuracy: 95.17
Round  51, Global train loss: 1.529, Global test loss: 1.676, Global test accuracy: 79.09
Round  52, Train loss: 1.478, Test loss: 1.512, Test accuracy: 95.10
Round  52, Global train loss: 1.478, Global test loss: 1.674, Global test accuracy: 79.74
Round  53, Train loss: 1.472, Test loss: 1.512, Test accuracy: 95.12
Round  53, Global train loss: 1.472, Global test loss: 1.629, Global test accuracy: 84.57
Round  54, Train loss: 1.528, Test loss: 1.511, Test accuracy: 95.20
Round  54, Global train loss: 1.528, Global test loss: 1.641, Global test accuracy: 82.72
Round  55, Train loss: 1.473, Test loss: 1.511, Test accuracy: 95.17
Round  55, Global train loss: 1.473, Global test loss: 1.633, Global test accuracy: 83.89
Round  56, Train loss: 1.474, Test loss: 1.511, Test accuracy: 95.14
Round  56, Global train loss: 1.474, Global test loss: 1.630, Global test accuracy: 85.27
Round  57, Train loss: 1.474, Test loss: 1.511, Test accuracy: 95.15
Round  57, Global train loss: 1.474, Global test loss: 1.640, Global test accuracy: 83.48
Round  58, Train loss: 1.527, Test loss: 1.511, Test accuracy: 95.15
Round  58, Global train loss: 1.527, Global test loss: 1.651, Global test accuracy: 82.32
Round  59, Train loss: 1.473, Test loss: 1.511, Test accuracy: 95.15
Round  59, Global train loss: 1.473, Global test loss: 1.659, Global test accuracy: 81.38
Round  60, Train loss: 1.470, Test loss: 1.510, Test accuracy: 95.22
Round  60, Global train loss: 1.470, Global test loss: 1.613, Global test accuracy: 87.39
Round  61, Train loss: 1.526, Test loss: 1.510, Test accuracy: 95.25
Round  61, Global train loss: 1.526, Global test loss: 1.655, Global test accuracy: 81.52
Round  62, Train loss: 1.472, Test loss: 1.511, Test accuracy: 95.24
Round  62, Global train loss: 1.472, Global test loss: 1.621, Global test accuracy: 85.06
Round  63, Train loss: 1.469, Test loss: 1.511, Test accuracy: 95.24
Round  63, Global train loss: 1.469, Global test loss: 1.645, Global test accuracy: 83.97
Round  64, Train loss: 1.472, Test loss: 1.510, Test accuracy: 95.25
Round  64, Global train loss: 1.472, Global test loss: 1.619, Global test accuracy: 86.62
Round  65, Train loss: 1.472, Test loss: 1.510, Test accuracy: 95.25
Round  65, Global train loss: 1.472, Global test loss: 1.684, Global test accuracy: 77.35
Round  66, Train loss: 1.524, Test loss: 1.509, Test accuracy: 95.35
Round  66, Global train loss: 1.524, Global test loss: 1.643, Global test accuracy: 82.10
Round  67, Train loss: 1.469, Test loss: 1.509, Test accuracy: 95.30
Round  67, Global train loss: 1.469, Global test loss: 1.643, Global test accuracy: 82.43
Round  68, Train loss: 1.469, Test loss: 1.509, Test accuracy: 95.32
Round  68, Global train loss: 1.469, Global test loss: 1.652, Global test accuracy: 81.24
Round  69, Train loss: 1.524, Test loss: 1.509, Test accuracy: 95.34
Round  69, Global train loss: 1.524, Global test loss: 1.663, Global test accuracy: 80.88
Round  70, Train loss: 1.471, Test loss: 1.509, Test accuracy: 95.35
Round  70, Global train loss: 1.471, Global test loss: 1.614, Global test accuracy: 86.28
Round  71, Train loss: 1.524, Test loss: 1.509, Test accuracy: 95.33
Round  71, Global train loss: 1.524, Global test loss: 1.621, Global test accuracy: 85.30
Round  72, Train loss: 1.469, Test loss: 1.509, Test accuracy: 95.33
Round  72, Global train loss: 1.469, Global test loss: 1.676, Global test accuracy: 78.76
Round  73, Train loss: 1.470, Test loss: 1.509, Test accuracy: 95.38
Round  73, Global train loss: 1.470, Global test loss: 1.627, Global test accuracy: 84.34
Round  74, Train loss: 1.468, Test loss: 1.509, Test accuracy: 95.42
Round  74, Global train loss: 1.468, Global test loss: 1.650, Global test accuracy: 81.26
Round  75, Train loss: 1.471, Test loss: 1.509, Test accuracy: 95.42
Round  75, Global train loss: 1.471, Global test loss: 1.657, Global test accuracy: 81.33
Round  76, Train loss: 1.471, Test loss: 1.508, Test accuracy: 95.42
Round  76, Global train loss: 1.471, Global test loss: 1.637, Global test accuracy: 83.47
Round  77, Train loss: 1.468, Test loss: 1.508, Test accuracy: 95.42
Round  77, Global train loss: 1.468, Global test loss: 1.646, Global test accuracy: 82.41
Round  78, Train loss: 1.469, Test loss: 1.508, Test accuracy: 95.41
Round  78, Global train loss: 1.469, Global test loss: 1.635, Global test accuracy: 82.83
Round  79, Train loss: 1.470, Test loss: 1.508, Test accuracy: 95.45
Round  79, Global train loss: 1.470, Global test loss: 1.653, Global test accuracy: 82.43
Round  80, Train loss: 1.523, Test loss: 1.508, Test accuracy: 95.45
Round  80, Global train loss: 1.523, Global test loss: 1.642, Global test accuracy: 83.18
Round  81, Train loss: 1.470, Test loss: 1.508, Test accuracy: 95.43
Round  81, Global train loss: 1.470, Global test loss: 1.651, Global test accuracy: 81.56
Round  82, Train loss: 1.522, Test loss: 1.508, Test accuracy: 95.47
Round  82, Global train loss: 1.522, Global test loss: 1.594, Global test accuracy: 88.65
Round  83, Train loss: 1.470, Test loss: 1.508, Test accuracy: 95.44
Round  83, Global train loss: 1.470, Global test loss: 1.627, Global test accuracy: 85.11
Round  84, Train loss: 1.468, Test loss: 1.508, Test accuracy: 95.43
Round  84, Global train loss: 1.468, Global test loss: 1.646, Global test accuracy: 82.44
Round  85, Train loss: 1.470, Test loss: 1.508, Test accuracy: 95.45
Round  85, Global train loss: 1.470, Global test loss: 1.601, Global test accuracy: 87.17
Round  86, Train loss: 1.470, Test loss: 1.508, Test accuracy: 95.47
Round  86, Global train loss: 1.470, Global test loss: 1.622, Global test accuracy: 84.99
Round  87, Train loss: 1.523, Test loss: 1.508, Test accuracy: 95.41
Round  87, Global train loss: 1.523, Global test loss: 1.650, Global test accuracy: 81.74
Round  88, Train loss: 1.522, Test loss: 1.508, Test accuracy: 95.43
Round  88, Global train loss: 1.522, Global test loss: 1.644, Global test accuracy: 82.40
Round  89, Train loss: 1.523, Test loss: 1.508, Test accuracy: 95.42
Round  89, Global train loss: 1.523, Global test loss: 1.643, Global test accuracy: 82.80
Round  90, Train loss: 1.467, Test loss: 1.508, Test accuracy: 95.35
Round  90, Global train loss: 1.467, Global test loss: 1.627, Global test accuracy: 83.88
Round  91, Train loss: 1.469, Test loss: 1.508, Test accuracy: 95.38
Round  91, Global train loss: 1.469, Global test loss: 1.638, Global test accuracy: 82.84
Round  92, Train loss: 1.466, Test loss: 1.508, Test accuracy: 95.43
Round  92, Global train loss: 1.466, Global test loss: 1.618, Global test accuracy: 85.54
Round  93, Train loss: 1.521, Test loss: 1.507, Test accuracy: 95.48
Round  93, Global train loss: 1.521, Global test loss: 1.608, Global test accuracy: 87.33
Round  94, Train loss: 1.521, Test loss: 1.507, Test accuracy: 95.51
Round  94, Global train loss: 1.521, Global test loss: 1.640, Global test accuracy: 83.07
Round  95, Train loss: 1.468, Test loss: 1.508, Test accuracy: 95.45
Round  95, Global train loss: 1.468, Global test loss: 1.600, Global test accuracy: 88.30/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.49
Round  96, Global train loss: 1.467, Global test loss: 1.651, Global test accuracy: 81.82
Round  97, Train loss: 1.468, Test loss: 1.507, Test accuracy: 95.51
Round  97, Global train loss: 1.468, Global test loss: 1.601, Global test accuracy: 88.07
Round  98, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.52
Round  98, Global train loss: 1.467, Global test loss: 1.589, Global test accuracy: 88.58
Round  99, Train loss: 1.468, Test loss: 1.507, Test accuracy: 95.53
Round  99, Global train loss: 1.468, Global test loss: 1.581, Global test accuracy: 89.67
Final Round, Train loss: 1.482, Test loss: 1.506, Test accuracy: 95.57
Final Round, Global train loss: 1.482, Global test loss: 1.581, Global test accuracy: 89.67
Average accuracy final 10 rounds: 95.46583333333332 

Average global accuracy final 10 rounds: 85.91 

1314.2757964134216
[0.9727358818054199, 1.9454717636108398, 2.7947094440460205, 3.643947124481201, 4.4700376987457275, 5.296128273010254, 6.109107255935669, 6.922086238861084, 7.756523132324219, 8.590960025787354, 9.424127101898193, 10.257294178009033, 11.10161280632019, 11.945931434631348, 12.760064125061035, 13.574196815490723, 14.414461612701416, 15.25472640991211, 16.11670970916748, 16.97869300842285, 17.819318771362305, 18.659944534301758, 19.49872589111328, 20.337507247924805, 21.17887020111084, 22.020233154296875, 22.880954265594482, 23.74167537689209, 24.561437606811523, 25.381199836730957, 26.21245813369751, 27.043716430664062, 27.881378889083862, 28.719041347503662, 29.547537088394165, 30.376032829284668, 31.214905977249146, 32.05377912521362, 32.8943727016449, 33.73496627807617, 34.5662682056427, 35.39757013320923, 36.22932767868042, 37.06108522415161, 37.89751148223877, 38.73393774032593, 39.56989550590515, 40.405853271484375, 41.241443395614624, 42.07703351974487, 42.93091869354248, 43.78480386734009, 44.725465536117554, 45.66612720489502, 46.50832486152649, 47.35052251815796, 48.189767837524414, 49.02901315689087, 49.8557984828949, 50.682583808898926, 51.512218713760376, 52.341853618621826, 53.21096396446228, 54.080074310302734, 54.92025923728943, 55.76044416427612, 56.578579902648926, 57.39671564102173, 58.24027180671692, 59.08382797241211, 59.9257447719574, 60.767661571502686, 61.614251136779785, 62.460840702056885, 63.28842830657959, 64.1160159111023, 64.95895195007324, 65.80188798904419, 66.6317412853241, 67.461594581604, 68.29826283454895, 69.1349310874939, 69.9668960571289, 70.79886102676392, 71.64703273773193, 72.49520444869995, 73.33089470863342, 74.1665849685669, 75.0037772655487, 75.84096956253052, 76.68747901916504, 77.53398847579956, 78.36482810974121, 79.19566774368286, 80.0347261428833, 80.87378454208374, 81.72609806060791, 82.57841157913208, 83.40935635566711, 84.24030113220215, 85.08361411094666, 85.92692708969116, 86.80741333961487, 87.68789958953857, 88.5173134803772, 89.34672737121582, 90.21211266517639, 91.07749795913696, 91.9322304725647, 92.78696298599243, 93.61430358886719, 94.44164419174194, 95.26374554634094, 96.08584690093994, 96.9137191772461, 97.74159145355225, 98.58568739891052, 99.4297833442688, 100.26704478263855, 101.1043062210083, 101.91845536231995, 102.73260450363159, 103.5858895778656, 104.43917465209961, 105.2888753414154, 106.1385760307312, 106.97814893722534, 107.81772184371948, 108.66169476509094, 109.5056676864624, 110.36447811126709, 111.22328853607178, 112.13158965110779, 113.0398907661438, 113.89371299743652, 114.74753522872925, 115.60752058029175, 116.46750593185425, 117.3377857208252, 118.20806550979614, 119.0519208908081, 119.89577627182007, 120.74394702911377, 121.59211778640747, 122.43084931373596, 123.26958084106445, 124.1225061416626, 124.97543144226074, 125.8286874294281, 126.68194341659546, 127.54114294052124, 128.40034246444702, 129.26515221595764, 130.12996196746826, 131.05523085594177, 131.98049974441528, 132.8511025905609, 133.72170543670654, 134.58137154579163, 135.4410376548767, 136.2798891067505, 137.11874055862427, 137.95553135871887, 138.79232215881348, 139.629789352417, 140.4672565460205, 141.3104751110077, 142.15369367599487, 142.97166061401367, 143.78962755203247, 144.6154818534851, 145.44133615493774, 146.27465772628784, 147.10797929763794, 147.93639302253723, 148.76480674743652, 149.59203124046326, 150.41925573349, 151.26127934455872, 152.10330295562744, 152.93735146522522, 153.771399974823, 154.59823083877563, 155.42506170272827, 156.25554251670837, 157.08602333068848, 157.93577075004578, 158.78551816940308, 159.60530257225037, 160.42508697509766, 161.2460355758667, 162.06698417663574, 162.89733910560608, 163.72769403457642, 164.5666539669037, 165.40561389923096, 166.24605679512024, 167.08649969100952, 167.9035964012146, 168.72069311141968, 170.12379217147827, 171.52689123153687]
[32.825, 32.825, 52.36666666666667, 52.36666666666667, 60.358333333333334, 60.358333333333334, 63.46666666666667, 63.46666666666667, 71.00833333333334, 71.00833333333334, 78.58333333333333, 78.58333333333333, 82.29166666666667, 82.29166666666667, 82.38333333333334, 82.38333333333334, 85.4, 85.4, 86.86666666666666, 86.86666666666666, 90.625, 90.625, 93.025, 93.025, 93.09166666666667, 93.09166666666667, 93.2, 93.2, 93.16666666666667, 93.16666666666667, 93.20833333333333, 93.20833333333333, 93.44166666666666, 93.44166666666666, 93.46666666666667, 93.46666666666667, 93.375, 93.375, 93.35833333333333, 93.35833333333333, 94.68333333333334, 94.68333333333334, 94.71666666666667, 94.71666666666667, 94.69166666666666, 94.69166666666666, 94.76666666666667, 94.76666666666667, 94.81666666666666, 94.81666666666666, 94.775, 94.775, 94.8, 94.8, 94.8, 94.8, 94.80833333333334, 94.80833333333334, 94.91666666666667, 94.91666666666667, 94.89166666666667, 94.89166666666667, 94.875, 94.875, 94.96666666666667, 94.96666666666667, 95.03333333333333, 95.03333333333333, 95.01666666666667, 95.01666666666667, 94.95833333333333, 94.95833333333333, 94.93333333333334, 94.93333333333334, 94.98333333333333, 94.98333333333333, 95.025, 95.025, 95.08333333333333, 95.08333333333333, 95.075, 95.075, 95.01666666666667, 95.01666666666667, 95.025, 95.025, 95.05, 95.05, 95.16666666666667, 95.16666666666667, 95.14166666666667, 95.14166666666667, 95.16666666666667, 95.16666666666667, 95.14166666666667, 95.14166666666667, 95.11666666666666, 95.11666666666666, 95.18333333333334, 95.18333333333334, 95.15, 95.15, 95.16666666666667, 95.16666666666667, 95.1, 95.1, 95.125, 95.125, 95.2, 95.2, 95.16666666666667, 95.16666666666667, 95.14166666666667, 95.14166666666667, 95.15, 95.15, 95.15, 95.15, 95.15, 95.15, 95.225, 95.225, 95.25, 95.25, 95.24166666666666, 95.24166666666666, 95.24166666666666, 95.24166666666666, 95.25, 95.25, 95.25, 95.25, 95.35, 95.35, 95.3, 95.3, 95.31666666666666, 95.31666666666666, 95.34166666666667, 95.34166666666667, 95.35, 95.35, 95.33333333333333, 95.33333333333333, 95.325, 95.325, 95.38333333333334, 95.38333333333334, 95.425, 95.425, 95.41666666666667, 95.41666666666667, 95.425, 95.425, 95.425, 95.425, 95.40833333333333, 95.40833333333333, 95.45, 95.45, 95.45, 95.45, 95.43333333333334, 95.43333333333334, 95.475, 95.475, 95.44166666666666, 95.44166666666666, 95.43333333333334, 95.43333333333334, 95.45, 95.45, 95.475, 95.475, 95.40833333333333, 95.40833333333333, 95.43333333333334, 95.43333333333334, 95.425, 95.425, 95.35, 95.35, 95.38333333333334, 95.38333333333334, 95.43333333333334, 95.43333333333334, 95.48333333333333, 95.48333333333333, 95.50833333333334, 95.50833333333334, 95.45, 95.45, 95.49166666666666, 95.49166666666666, 95.50833333333334, 95.50833333333334, 95.51666666666667, 95.51666666666667, 95.53333333333333, 95.53333333333333, 95.56666666666666, 95.56666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.286, Test loss: 2.291, Test accuracy: 14.93
Round   1, Train loss: 2.165, Test loss: 2.204, Test accuracy: 45.88
Round   2, Train loss: 1.894, Test loss: 2.052, Test accuracy: 48.38
Round   3, Train loss: 1.852, Test loss: 1.976, Test accuracy: 53.14
Round   4, Train loss: 1.712, Test loss: 1.872, Test accuracy: 61.02
Round   5, Train loss: 1.851, Test loss: 1.853, Test accuracy: 63.62
Round   6, Train loss: 1.759, Test loss: 1.843, Test accuracy: 63.97
Round   7, Train loss: 1.645, Test loss: 1.787, Test accuracy: 68.97
Round   8, Train loss: 1.751, Test loss: 1.751, Test accuracy: 73.64
Round   9, Train loss: 1.653, Test loss: 1.716, Test accuracy: 76.33
Round  10, Train loss: 1.737, Test loss: 1.703, Test accuracy: 76.97
Round  11, Train loss: 1.594, Test loss: 1.702, Test accuracy: 76.87
Round  12, Train loss: 1.624, Test loss: 1.697, Test accuracy: 77.22
Round  13, Train loss: 1.550, Test loss: 1.697, Test accuracy: 76.92
Round  14, Train loss: 1.648, Test loss: 1.696, Test accuracy: 76.99
Round  15, Train loss: 1.873, Test loss: 1.684, Test accuracy: 78.29
Round  16, Train loss: 1.549, Test loss: 1.680, Test accuracy: 78.46
Round  17, Train loss: 1.692, Test loss: 1.674, Test accuracy: 78.81
Round  18, Train loss: 1.704, Test loss: 1.671, Test accuracy: 78.90
Round  19, Train loss: 1.505, Test loss: 1.646, Test accuracy: 81.77
Round  20, Train loss: 1.654, Test loss: 1.645, Test accuracy: 81.84
Round  21, Train loss: 1.582, Test loss: 1.626, Test accuracy: 83.77
Round  22, Train loss: 1.541, Test loss: 1.603, Test accuracy: 86.28
Round  23, Train loss: 1.564, Test loss: 1.601, Test accuracy: 86.39
Round  24, Train loss: 1.504, Test loss: 1.586, Test accuracy: 88.12
Round  25, Train loss: 1.596, Test loss: 1.586, Test accuracy: 88.17
Round  26, Train loss: 1.503, Test loss: 1.571, Test accuracy: 89.78
Round  27, Train loss: 1.537, Test loss: 1.570, Test accuracy: 89.89
Round  28, Train loss: 1.493, Test loss: 1.569, Test accuracy: 89.92
Round  29, Train loss: 1.485, Test loss: 1.567, Test accuracy: 90.03
Round  30, Train loss: 1.540, Test loss: 1.566, Test accuracy: 90.12
Round  31, Train loss: 1.542, Test loss: 1.566, Test accuracy: 90.14
Round  32, Train loss: 1.546, Test loss: 1.564, Test accuracy: 90.18
Round  33, Train loss: 1.538, Test loss: 1.564, Test accuracy: 90.20
Round  34, Train loss: 1.486, Test loss: 1.563, Test accuracy: 90.22
Round  35, Train loss: 1.485, Test loss: 1.563, Test accuracy: 90.19
Round  36, Train loss: 1.590, Test loss: 1.563, Test accuracy: 90.22
Round  37, Train loss: 1.641, Test loss: 1.565, Test accuracy: 90.07
Round  38, Train loss: 1.583, Test loss: 1.565, Test accuracy: 89.94
Round  39, Train loss: 1.484, Test loss: 1.563, Test accuracy: 90.18
Round  40, Train loss: 1.481, Test loss: 1.562, Test accuracy: 90.22
Round  41, Train loss: 1.534, Test loss: 1.561, Test accuracy: 90.28
Round  42, Train loss: 1.474, Test loss: 1.562, Test accuracy: 90.26
Round  43, Train loss: 1.481, Test loss: 1.561, Test accuracy: 90.28
Round  44, Train loss: 1.537, Test loss: 1.560, Test accuracy: 90.45
Round  45, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.47
Round  46, Train loss: 1.479, Test loss: 1.559, Test accuracy: 90.59
Round  47, Train loss: 1.534, Test loss: 1.559, Test accuracy: 90.59
Round  48, Train loss: 1.530, Test loss: 1.559, Test accuracy: 90.62
Round  49, Train loss: 1.653, Test loss: 1.547, Test accuracy: 91.83
Round  50, Train loss: 1.579, Test loss: 1.547, Test accuracy: 91.88
Round  51, Train loss: 1.653, Test loss: 1.546, Test accuracy: 91.93
Round  52, Train loss: 1.534, Test loss: 1.546, Test accuracy: 91.84
Round  53, Train loss: 1.526, Test loss: 1.547, Test accuracy: 91.88
Round  54, Train loss: 1.514, Test loss: 1.542, Test accuracy: 92.40
Round  55, Train loss: 1.532, Test loss: 1.541, Test accuracy: 92.37
Round  56, Train loss: 1.530, Test loss: 1.542, Test accuracy: 92.39
Round  57, Train loss: 1.473, Test loss: 1.542, Test accuracy: 92.42
Round  58, Train loss: 1.528, Test loss: 1.541, Test accuracy: 92.41
Round  59, Train loss: 1.549, Test loss: 1.531, Test accuracy: 93.42
Round  60, Train loss: 1.531, Test loss: 1.530, Test accuracy: 93.52
Round  61, Train loss: 1.484, Test loss: 1.530, Test accuracy: 93.43
Round  62, Train loss: 1.487, Test loss: 1.530, Test accuracy: 93.54
Round  63, Train loss: 1.534, Test loss: 1.529, Test accuracy: 93.48
Round  64, Train loss: 1.489, Test loss: 1.529, Test accuracy: 93.56
Round  65, Train loss: 1.471, Test loss: 1.529, Test accuracy: 93.62
Round  66, Train loss: 1.527, Test loss: 1.528, Test accuracy: 93.68
Round  67, Train loss: 1.528, Test loss: 1.528, Test accuracy: 93.73
Round  68, Train loss: 1.475, Test loss: 1.528, Test accuracy: 93.70
Round  69, Train loss: 1.540, Test loss: 1.528, Test accuracy: 93.58
Round  70, Train loss: 1.474, Test loss: 1.528, Test accuracy: 93.61
Round  71, Train loss: 1.471, Test loss: 1.528, Test accuracy: 93.56
Round  72, Train loss: 1.479, Test loss: 1.528, Test accuracy: 93.62
Round  73, Train loss: 1.478, Test loss: 1.527, Test accuracy: 93.69
Round  74, Train loss: 1.468, Test loss: 1.527, Test accuracy: 93.71
Round  75, Train loss: 1.468, Test loss: 1.526, Test accuracy: 93.80
Round  76, Train loss: 1.525, Test loss: 1.527, Test accuracy: 93.80
Round  77, Train loss: 1.589, Test loss: 1.526, Test accuracy: 93.75
Round  78, Train loss: 1.473, Test loss: 1.526, Test accuracy: 93.73
Round  79, Train loss: 1.471, Test loss: 1.526, Test accuracy: 93.79
Round  80, Train loss: 1.475, Test loss: 1.526, Test accuracy: 93.80
Round  81, Train loss: 1.525, Test loss: 1.526, Test accuracy: 93.77
Round  82, Train loss: 1.468, Test loss: 1.526, Test accuracy: 93.83
Round  83, Train loss: 1.528, Test loss: 1.526, Test accuracy: 93.84
Round  84, Train loss: 1.581, Test loss: 1.526, Test accuracy: 93.83
Round  85, Train loss: 1.473, Test loss: 1.525, Test accuracy: 93.83
Round  86, Train loss: 1.523, Test loss: 1.525, Test accuracy: 93.81
Round  87, Train loss: 1.527, Test loss: 1.525, Test accuracy: 93.83
Round  88, Train loss: 1.528, Test loss: 1.525, Test accuracy: 93.83
Round  89, Train loss: 1.469, Test loss: 1.525, Test accuracy: 93.83
Round  90, Train loss: 1.584, Test loss: 1.527, Test accuracy: 93.62
Round  91, Train loss: 1.470, Test loss: 1.527, Test accuracy: 93.72
Round  92, Train loss: 1.470, Test loss: 1.526, Test accuracy: 93.79
Round  93, Train loss: 1.522, Test loss: 1.526, Test accuracy: 93.83
Round  94, Train loss: 1.472, Test loss: 1.525, Test accuracy: 93.84
Round  95, Train loss: 1.469, Test loss: 1.525, Test accuracy: 93.89
Round  96, Train loss: 1.494, Test loss: 1.512, Test accuracy: 95.31
Round  97, Train loss: 1.470, Test loss: 1.512, Test accuracy: 95.28
Round  98, Train loss: 1.479, Test loss: 1.512, Test accuracy: 95.28/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.483, Test loss: 1.512, Test accuracy: 95.24
Final Round, Train loss: 1.492, Test loss: 1.511, Test accuracy: 95.28
Average accuracy final 10 rounds: 94.38083333333336 

1024.9713323116302
[0.9073691368103027, 1.8147382736206055, 2.6214606761932373, 3.428183078765869, 4.217703104019165, 5.007223129272461, 5.805978059768677, 6.604732990264893, 7.429399490356445, 8.254065990447998, 9.059264421463013, 9.864462852478027, 10.651210308074951, 11.437957763671875, 12.23207402229309, 13.026190280914307, 13.819900035858154, 14.613609790802002, 15.392566680908203, 16.171523571014404, 16.96483874320984, 17.758153915405273, 18.546213388442993, 19.334272861480713, 20.117919921875, 20.901566982269287, 21.695146799087524, 22.48872661590576, 23.27144980430603, 24.0541729927063, 24.852834224700928, 25.651495456695557, 26.447222232818604, 27.24294900894165, 28.021018981933594, 28.799088954925537, 29.582859754562378, 30.36663055419922, 31.155100345611572, 31.943570137023926, 32.745818853378296, 33.548067569732666, 34.33298182487488, 35.11789608001709, 35.89616775512695, 36.674439430236816, 37.46281147003174, 38.25118350982666, 39.02775716781616, 39.804330825805664, 40.58888125419617, 41.37343168258667, 42.15767979621887, 42.941927909851074, 43.72892451286316, 44.515921115875244, 45.29506301879883, 46.07420492172241, 46.855366468429565, 47.63652801513672, 48.41628694534302, 49.196045875549316, 49.98227286338806, 50.76849985122681, 51.5561785697937, 52.343857288360596, 53.124279737472534, 53.90470218658447, 54.68075966835022, 55.45681715011597, 56.232078552246094, 57.00733995437622, 57.79474496841431, 58.58214998245239, 59.37665820121765, 60.17116641998291, 60.962409019470215, 61.75365161895752, 62.536598205566406, 63.31954479217529, 64.1018648147583, 64.88418483734131, 65.66876721382141, 66.45334959030151, 67.23426723480225, 68.01518487930298, 68.80636692047119, 69.5975489616394, 70.39674305915833, 71.19593715667725, 71.98001742362976, 72.76409769058228, 73.55102396011353, 74.33795022964478, 75.1264955997467, 75.91504096984863, 76.714688539505, 77.51433610916138, 78.31096935272217, 79.10760259628296, 79.89755964279175, 80.68751668930054, 81.47159171104431, 82.25566673278809, 83.06357669830322, 83.87148666381836, 84.65895414352417, 85.44642162322998, 86.24549531936646, 87.04456901550293, 87.83927965164185, 88.63399028778076, 89.43192410469055, 90.22985792160034, 91.03449201583862, 91.8391261100769, 92.65706610679626, 93.47500610351562, 94.30265998840332, 95.13031387329102, 95.9405026435852, 96.7506914138794, 97.55159020423889, 98.35248899459839, 99.16439247131348, 99.97629594802856, 100.79517245292664, 101.6140489578247, 102.45543575286865, 103.2968225479126, 104.0940613746643, 104.89130020141602, 105.6990053653717, 106.50671052932739, 107.32546091079712, 108.14421129226685, 108.9545886516571, 109.76496601104736, 110.57084155082703, 111.37671709060669, 112.17242503166199, 112.96813297271729, 113.76401090621948, 114.55988883972168, 115.3851068019867, 116.21032476425171, 117.02598094940186, 117.841637134552, 118.6386730670929, 119.43570899963379, 120.21099495887756, 120.98628091812134, 121.77881002426147, 122.57133913040161, 123.36001014709473, 124.14868116378784, 124.93881368637085, 125.72894620895386, 126.50640344619751, 127.28386068344116, 128.07487726211548, 128.8658938407898, 129.644606590271, 130.4233193397522, 131.19999623298645, 131.9766731262207, 132.7772879600525, 133.57790279388428, 134.36968851089478, 135.16147422790527, 135.94524097442627, 136.72900772094727, 137.51657819747925, 138.30414867401123, 139.0847568511963, 139.86536502838135, 140.6542263031006, 141.44308757781982, 142.2362208366394, 143.02935409545898, 143.81988143920898, 144.61040878295898, 145.40213012695312, 146.19385147094727, 146.9717047214508, 147.74955797195435, 148.54884958267212, 149.3481411933899, 150.13237071037292, 150.91660022735596, 151.69588208198547, 152.475163936615, 153.2631800174713, 154.05119609832764, 154.84337162971497, 155.6355471611023, 156.41540217399597, 157.19525718688965, 157.97454690933228, 158.7538366317749, 160.0147602558136, 161.2756838798523]
[14.925, 14.925, 45.88333333333333, 45.88333333333333, 48.375, 48.375, 53.141666666666666, 53.141666666666666, 61.016666666666666, 61.016666666666666, 63.61666666666667, 63.61666666666667, 63.96666666666667, 63.96666666666667, 68.975, 68.975, 73.64166666666667, 73.64166666666667, 76.325, 76.325, 76.975, 76.975, 76.86666666666666, 76.86666666666666, 77.21666666666667, 77.21666666666667, 76.925, 76.925, 76.99166666666666, 76.99166666666666, 78.29166666666667, 78.29166666666667, 78.45833333333333, 78.45833333333333, 78.80833333333334, 78.80833333333334, 78.9, 78.9, 81.76666666666667, 81.76666666666667, 81.84166666666667, 81.84166666666667, 83.76666666666667, 83.76666666666667, 86.28333333333333, 86.28333333333333, 86.39166666666667, 86.39166666666667, 88.11666666666666, 88.11666666666666, 88.16666666666667, 88.16666666666667, 89.775, 89.775, 89.89166666666667, 89.89166666666667, 89.925, 89.925, 90.03333333333333, 90.03333333333333, 90.125, 90.125, 90.14166666666667, 90.14166666666667, 90.18333333333334, 90.18333333333334, 90.2, 90.2, 90.225, 90.225, 90.19166666666666, 90.19166666666666, 90.225, 90.225, 90.06666666666666, 90.06666666666666, 89.94166666666666, 89.94166666666666, 90.18333333333334, 90.18333333333334, 90.225, 90.225, 90.28333333333333, 90.28333333333333, 90.25833333333334, 90.25833333333334, 90.275, 90.275, 90.45, 90.45, 90.475, 90.475, 90.59166666666667, 90.59166666666667, 90.59166666666667, 90.59166666666667, 90.625, 90.625, 91.83333333333333, 91.83333333333333, 91.88333333333334, 91.88333333333334, 91.93333333333334, 91.93333333333334, 91.84166666666667, 91.84166666666667, 91.875, 91.875, 92.4, 92.4, 92.36666666666666, 92.36666666666666, 92.39166666666667, 92.39166666666667, 92.41666666666667, 92.41666666666667, 92.40833333333333, 92.40833333333333, 93.425, 93.425, 93.51666666666667, 93.51666666666667, 93.43333333333334, 93.43333333333334, 93.54166666666667, 93.54166666666667, 93.48333333333333, 93.48333333333333, 93.55833333333334, 93.55833333333334, 93.625, 93.625, 93.68333333333334, 93.68333333333334, 93.73333333333333, 93.73333333333333, 93.7, 93.7, 93.575, 93.575, 93.60833333333333, 93.60833333333333, 93.55833333333334, 93.55833333333334, 93.61666666666666, 93.61666666666666, 93.69166666666666, 93.69166666666666, 93.70833333333333, 93.70833333333333, 93.8, 93.8, 93.8, 93.8, 93.75, 93.75, 93.73333333333333, 93.73333333333333, 93.79166666666667, 93.79166666666667, 93.8, 93.8, 93.76666666666667, 93.76666666666667, 93.83333333333333, 93.83333333333333, 93.84166666666667, 93.84166666666667, 93.83333333333333, 93.83333333333333, 93.825, 93.825, 93.80833333333334, 93.80833333333334, 93.83333333333333, 93.83333333333333, 93.83333333333333, 93.83333333333333, 93.825, 93.825, 93.61666666666666, 93.61666666666666, 93.725, 93.725, 93.79166666666667, 93.79166666666667, 93.83333333333333, 93.83333333333333, 93.84166666666667, 93.84166666666667, 93.89166666666667, 93.89166666666667, 95.30833333333334, 95.30833333333334, 95.28333333333333, 95.28333333333333, 95.275, 95.275, 95.24166666666666, 95.24166666666666, 95.275, 95.275]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.199, Test loss: 2.221, Test accuracy: 32.65
Round   1, Train loss: 1.759, Test loss: 2.040, Test accuracy: 41.90
Round   2, Train loss: 1.635, Test loss: 1.936, Test accuracy: 51.01
Round   3, Train loss: 1.593, Test loss: 1.849, Test accuracy: 65.03
Round   4, Train loss: 1.552, Test loss: 1.809, Test accuracy: 66.62
Round   5, Train loss: 1.573, Test loss: 1.692, Test accuracy: 77.67
Round   6, Train loss: 1.558, Test loss: 1.640, Test accuracy: 83.89
Round   7, Train loss: 1.562, Test loss: 1.616, Test accuracy: 86.56
Round   8, Train loss: 1.554, Test loss: 1.607, Test accuracy: 87.37
Round   9, Train loss: 1.542, Test loss: 1.590, Test accuracy: 89.09
Round  10, Train loss: 1.552, Test loss: 1.579, Test accuracy: 89.17
Round  11, Train loss: 1.485, Test loss: 1.573, Test accuracy: 89.61
Round  12, Train loss: 1.534, Test loss: 1.572, Test accuracy: 89.67
Round  13, Train loss: 1.541, Test loss: 1.567, Test accuracy: 90.03
Round  14, Train loss: 1.477, Test loss: 1.566, Test accuracy: 90.13
Round  15, Train loss: 1.535, Test loss: 1.565, Test accuracy: 90.21
Round  16, Train loss: 1.533, Test loss: 1.565, Test accuracy: 90.12
Round  17, Train loss: 1.638, Test loss: 1.564, Test accuracy: 90.16
Round  18, Train loss: 1.529, Test loss: 1.563, Test accuracy: 90.23
Round  19, Train loss: 1.587, Test loss: 1.562, Test accuracy: 90.30
Round  20, Train loss: 1.535, Test loss: 1.562, Test accuracy: 90.35
Round  21, Train loss: 1.478, Test loss: 1.561, Test accuracy: 90.41
Round  22, Train loss: 1.535, Test loss: 1.562, Test accuracy: 90.38
Round  23, Train loss: 1.584, Test loss: 1.560, Test accuracy: 90.38
Round  24, Train loss: 1.530, Test loss: 1.560, Test accuracy: 90.40
Round  25, Train loss: 1.582, Test loss: 1.559, Test accuracy: 90.42
Round  26, Train loss: 1.473, Test loss: 1.560, Test accuracy: 90.46
Round  27, Train loss: 1.528, Test loss: 1.559, Test accuracy: 90.47
Round  28, Train loss: 1.526, Test loss: 1.559, Test accuracy: 90.46
Round  29, Train loss: 1.632, Test loss: 1.554, Test accuracy: 91.00
Round  30, Train loss: 1.473, Test loss: 1.553, Test accuracy: 91.05
Round  31, Train loss: 1.555, Test loss: 1.528, Test accuracy: 93.77
Round  32, Train loss: 1.476, Test loss: 1.527, Test accuracy: 93.73
Round  33, Train loss: 1.476, Test loss: 1.525, Test accuracy: 94.03
Round  34, Train loss: 1.480, Test loss: 1.521, Test accuracy: 94.42
Round  35, Train loss: 1.478, Test loss: 1.520, Test accuracy: 94.42
Round  36, Train loss: 1.469, Test loss: 1.520, Test accuracy: 94.42
Round  37, Train loss: 1.475, Test loss: 1.519, Test accuracy: 94.48
Round  38, Train loss: 1.477, Test loss: 1.518, Test accuracy: 94.64
Round  39, Train loss: 1.525, Test loss: 1.518, Test accuracy: 94.58
Round  40, Train loss: 1.525, Test loss: 1.519, Test accuracy: 94.47
Round  41, Train loss: 1.474, Test loss: 1.519, Test accuracy: 94.47
Round  42, Train loss: 1.526, Test loss: 1.518, Test accuracy: 94.50
Round  43, Train loss: 1.527, Test loss: 1.517, Test accuracy: 94.62
Round  44, Train loss: 1.469, Test loss: 1.517, Test accuracy: 94.62
Round  45, Train loss: 1.525, Test loss: 1.517, Test accuracy: 94.63
Round  46, Train loss: 1.469, Test loss: 1.517, Test accuracy: 94.64
Round  47, Train loss: 1.526, Test loss: 1.518, Test accuracy: 94.54
Round  48, Train loss: 1.526, Test loss: 1.518, Test accuracy: 94.48
Round  49, Train loss: 1.473, Test loss: 1.515, Test accuracy: 94.86
Round  50, Train loss: 1.473, Test loss: 1.515, Test accuracy: 94.88
Round  51, Train loss: 1.472, Test loss: 1.514, Test accuracy: 94.94
Round  52, Train loss: 1.472, Test loss: 1.514, Test accuracy: 94.97
Round  53, Train loss: 1.471, Test loss: 1.514, Test accuracy: 94.92
Round  54, Train loss: 1.473, Test loss: 1.513, Test accuracy: 95.06
Round  55, Train loss: 1.473, Test loss: 1.513, Test accuracy: 95.08
Round  56, Train loss: 1.471, Test loss: 1.512, Test accuracy: 95.12
Round  57, Train loss: 1.526, Test loss: 1.512, Test accuracy: 95.15
Round  58, Train loss: 1.469, Test loss: 1.512, Test accuracy: 95.23
Round  59, Train loss: 1.471, Test loss: 1.511, Test accuracy: 95.29
Round  60, Train loss: 1.524, Test loss: 1.511, Test accuracy: 95.34
Round  61, Train loss: 1.470, Test loss: 1.511, Test accuracy: 95.32
Round  62, Train loss: 1.514, Test loss: 1.506, Test accuracy: 95.67
Round  63, Train loss: 1.470, Test loss: 1.506, Test accuracy: 95.67
Round  64, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.68
Round  65, Train loss: 1.476, Test loss: 1.500, Test accuracy: 96.42
Round  66, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.70
Round  67, Train loss: 1.468, Test loss: 1.497, Test accuracy: 96.73
Round  68, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.72
Round  69, Train loss: 1.472, Test loss: 1.497, Test accuracy: 96.67
Round  70, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.76
Round  71, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.75
Round  72, Train loss: 1.473, Test loss: 1.496, Test accuracy: 96.82
Round  73, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.79
Round  74, Train loss: 1.467, Test loss: 1.496, Test accuracy: 96.78
Round  75, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.87
Round  76, Train loss: 1.467, Test loss: 1.495, Test accuracy: 96.84
Round  77, Train loss: 1.472, Test loss: 1.494, Test accuracy: 96.99
Round  78, Train loss: 1.472, Test loss: 1.494, Test accuracy: 96.90
Round  79, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.92
Round  80, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.92
Round  81, Train loss: 1.468, Test loss: 1.494, Test accuracy: 97.01
Round  82, Train loss: 1.471, Test loss: 1.494, Test accuracy: 97.02
Round  83, Train loss: 1.467, Test loss: 1.494, Test accuracy: 96.97
Round  84, Train loss: 1.470, Test loss: 1.494, Test accuracy: 97.01
Round  85, Train loss: 1.466, Test loss: 1.494, Test accuracy: 96.96
Round  86, Train loss: 1.467, Test loss: 1.494, Test accuracy: 96.95
Round  87, Train loss: 1.470, Test loss: 1.493, Test accuracy: 96.97
Round  88, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.02
Round  89, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.97
Round  90, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.05
Round  91, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.07
Round  92, Train loss: 1.466, Test loss: 1.493, Test accuracy: 97.03
Round  93, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.03
Round  94, Train loss: 1.470, Test loss: 1.492, Test accuracy: 97.12
Round  95, Train loss: 1.467, Test loss: 1.492, Test accuracy: 97.13
Round  96, Train loss: 1.469, Test loss: 1.492, Test accuracy: 97.08
Round  97, Train loss: 1.467, Test loss: 1.492, Test accuracy: 97.05
Round  98, Train loss: 1.471, Test loss: 1.492, Test accuracy: 97.06/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.468, Test loss: 1.492, Test accuracy: 97.03
Final Round, Train loss: 1.468, Test loss: 1.493, Test accuracy: 97.04
Average accuracy final 10 rounds: 97.065 

1068.2422015666962
[0.9736320972442627, 1.9472641944885254, 2.7889955043792725, 3.6307268142700195, 4.474116563796997, 5.317506313323975, 6.160635948181152, 7.00376558303833, 7.840035676956177, 8.676305770874023, 9.524878740310669, 10.373451709747314, 11.221425533294678, 12.069399356842041, 12.905116319656372, 13.740833282470703, 14.580836296081543, 15.420839309692383, 16.262369394302368, 17.103899478912354, 17.946561336517334, 18.789223194122314, 19.63090181350708, 20.472580432891846, 21.30959916114807, 22.146617889404297, 22.966251850128174, 23.78588581085205, 24.622294902801514, 25.458703994750977, 26.305383920669556, 27.152063846588135, 28.00359320640564, 28.855122566223145, 29.71461534500122, 30.574108123779297, 31.406977891921997, 32.2398476600647, 33.081035137176514, 33.92222261428833, 34.75180792808533, 35.581393241882324, 36.42929649353027, 37.27719974517822, 38.10833954811096, 38.9394793510437, 39.792702198028564, 40.64592504501343, 41.480069398880005, 42.31421375274658, 43.15296483039856, 43.99171590805054, 44.82885384559631, 45.66599178314209, 46.51143455505371, 47.35687732696533, 48.21469235420227, 49.07250738143921, 49.90924692153931, 50.745986461639404, 51.599650144577026, 52.45331382751465, 53.2742874622345, 54.095261096954346, 54.950860023498535, 55.806458950042725, 56.654417991638184, 57.50237703323364, 58.352942943573, 59.20350885391235, 60.05356001853943, 60.903611183166504, 61.752864599227905, 62.60211801528931, 63.43812942504883, 64.27414083480835, 65.11099672317505, 65.94785261154175, 66.7869279384613, 67.62600326538086, 68.4534637928009, 69.28092432022095, 70.10709047317505, 70.93325662612915, 71.76154065132141, 72.58982467651367, 73.42792773246765, 74.26603078842163, 75.11298441886902, 75.9599380493164, 76.82991480827332, 77.69989156723022, 78.5347409248352, 79.36959028244019, 80.19708180427551, 81.02457332611084, 81.85570454597473, 82.68683576583862, 83.51625227928162, 84.34566879272461, 85.18369913101196, 86.02172946929932, 86.86340427398682, 87.70507907867432, 88.53533124923706, 89.3655834197998, 90.19248914718628, 91.01939487457275, 91.86727905273438, 92.715163230896, 93.55566310882568, 94.39616298675537, 95.25171327590942, 96.10726356506348, 96.9502022266388, 97.79314088821411, 98.62379097938538, 99.45444107055664, 100.29220151901245, 101.12996196746826, 101.97314763069153, 102.8163332939148, 103.65316534042358, 104.48999738693237, 105.33235740661621, 106.17471742630005, 107.00686693191528, 107.83901643753052, 108.7119140625, 109.58481168746948, 110.42219233512878, 111.25957298278809, 112.10611748695374, 112.95266199111938, 113.78371572494507, 114.61476945877075, 115.44454884529114, 116.27432823181152, 117.10760235786438, 117.94087648391724, 118.76987957954407, 119.5988826751709, 120.44119691848755, 121.2835111618042, 122.12148404121399, 122.95945692062378, 123.80230617523193, 124.64515542984009, 125.49455261230469, 126.34394979476929, 127.17561554908752, 128.00728130340576, 128.8195858001709, 129.63189029693604, 130.47651147842407, 131.3211326599121, 132.17086505889893, 133.02059745788574, 133.86247277259827, 134.7043480873108, 135.53386330604553, 136.36337852478027, 137.18939447402954, 138.0154104232788, 138.85686802864075, 139.69832563400269, 140.54980564117432, 141.40128564834595, 142.24176621437073, 143.0822467803955, 143.89297795295715, 144.7037091255188, 145.60168838500977, 146.49966764450073, 147.34072947502136, 148.181791305542, 149.04814863204956, 149.91450595855713, 150.7733244895935, 151.63214302062988, 152.48634147644043, 153.34053993225098, 154.1941704750061, 155.04780101776123, 155.8698697090149, 156.69193840026855, 157.52238702774048, 158.3528356552124, 159.1753695011139, 159.99790334701538, 160.83741879463196, 161.67693424224854, 162.58210062980652, 163.4872670173645, 164.34010243415833, 165.19293785095215, 166.03602194786072, 166.8791060447693, 167.72610592842102, 168.57310581207275, 169.85626029968262, 171.13941478729248]
[32.65, 32.65, 41.9, 41.9, 51.00833333333333, 51.00833333333333, 65.025, 65.025, 66.625, 66.625, 77.675, 77.675, 83.89166666666667, 83.89166666666667, 86.55833333333334, 86.55833333333334, 87.36666666666666, 87.36666666666666, 89.09166666666667, 89.09166666666667, 89.16666666666667, 89.16666666666667, 89.60833333333333, 89.60833333333333, 89.675, 89.675, 90.025, 90.025, 90.13333333333334, 90.13333333333334, 90.20833333333333, 90.20833333333333, 90.11666666666666, 90.11666666666666, 90.15833333333333, 90.15833333333333, 90.23333333333333, 90.23333333333333, 90.3, 90.3, 90.35, 90.35, 90.40833333333333, 90.40833333333333, 90.375, 90.375, 90.375, 90.375, 90.4, 90.4, 90.425, 90.425, 90.45833333333333, 90.45833333333333, 90.475, 90.475, 90.45833333333333, 90.45833333333333, 91.0, 91.0, 91.05, 91.05, 93.76666666666667, 93.76666666666667, 93.73333333333333, 93.73333333333333, 94.03333333333333, 94.03333333333333, 94.41666666666667, 94.41666666666667, 94.425, 94.425, 94.425, 94.425, 94.48333333333333, 94.48333333333333, 94.64166666666667, 94.64166666666667, 94.575, 94.575, 94.46666666666667, 94.46666666666667, 94.46666666666667, 94.46666666666667, 94.5, 94.5, 94.61666666666666, 94.61666666666666, 94.625, 94.625, 94.63333333333334, 94.63333333333334, 94.64166666666667, 94.64166666666667, 94.54166666666667, 94.54166666666667, 94.48333333333333, 94.48333333333333, 94.85833333333333, 94.85833333333333, 94.875, 94.875, 94.94166666666666, 94.94166666666666, 94.975, 94.975, 94.925, 94.925, 95.05833333333334, 95.05833333333334, 95.08333333333333, 95.08333333333333, 95.125, 95.125, 95.15, 95.15, 95.23333333333333, 95.23333333333333, 95.29166666666667, 95.29166666666667, 95.34166666666667, 95.34166666666667, 95.31666666666666, 95.31666666666666, 95.675, 95.675, 95.66666666666667, 95.66666666666667, 95.68333333333334, 95.68333333333334, 96.425, 96.425, 96.7, 96.7, 96.73333333333333, 96.73333333333333, 96.71666666666667, 96.71666666666667, 96.675, 96.675, 96.75833333333334, 96.75833333333334, 96.75, 96.75, 96.81666666666666, 96.81666666666666, 96.79166666666667, 96.79166666666667, 96.775, 96.775, 96.86666666666666, 96.86666666666666, 96.84166666666667, 96.84166666666667, 96.99166666666666, 96.99166666666666, 96.9, 96.9, 96.91666666666667, 96.91666666666667, 96.925, 96.925, 97.00833333333334, 97.00833333333334, 97.01666666666667, 97.01666666666667, 96.975, 96.975, 97.00833333333334, 97.00833333333334, 96.95833333333333, 96.95833333333333, 96.95, 96.95, 96.975, 96.975, 97.01666666666667, 97.01666666666667, 96.975, 96.975, 97.05, 97.05, 97.06666666666666, 97.06666666666666, 97.03333333333333, 97.03333333333333, 97.025, 97.025, 97.11666666666666, 97.11666666666666, 97.13333333333334, 97.13333333333334, 97.08333333333333, 97.08333333333333, 97.05, 97.05, 97.05833333333334, 97.05833333333334, 97.03333333333333, 97.03333333333333, 97.04166666666667, 97.04166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.211, Test loss: 2.236, Test accuracy: 17.42
Round   1, Train loss: 1.779, Test loss: 2.021, Test accuracy: 53.28
Round   2, Train loss: 1.662, Test loss: 1.969, Test accuracy: 52.88
Round   3, Train loss: 1.631, Test loss: 1.854, Test accuracy: 65.51
Round   4, Train loss: 1.649, Test loss: 1.786, Test accuracy: 71.12
Round   5, Train loss: 1.645, Test loss: 1.714, Test accuracy: 78.58
Round   6, Train loss: 1.486, Test loss: 1.697, Test accuracy: 79.08
Round   7, Train loss: 1.491, Test loss: 1.680, Test accuracy: 81.81
Round   8, Train loss: 1.516, Test loss: 1.649, Test accuracy: 83.96
Round   9, Train loss: 1.553, Test loss: 1.634, Test accuracy: 84.08
Round  10, Train loss: 1.488, Test loss: 1.613, Test accuracy: 85.23
Round  11, Train loss: 1.481, Test loss: 1.591, Test accuracy: 88.92
Round  12, Train loss: 1.521, Test loss: 1.575, Test accuracy: 91.06
Round  13, Train loss: 1.500, Test loss: 1.560, Test accuracy: 92.28
Round  14, Train loss: 1.483, Test loss: 1.541, Test accuracy: 94.42
Round  15, Train loss: 1.472, Test loss: 1.538, Test accuracy: 94.51
Round  16, Train loss: 1.470, Test loss: 1.539, Test accuracy: 94.12
Round  17, Train loss: 1.481, Test loss: 1.534, Test accuracy: 93.92
Round  18, Train loss: 1.475, Test loss: 1.539, Test accuracy: 93.76
Round  19, Train loss: 1.472, Test loss: 1.528, Test accuracy: 93.97
Round  20, Train loss: 1.474, Test loss: 1.519, Test accuracy: 95.28
Round  21, Train loss: 1.472, Test loss: 1.518, Test accuracy: 95.30
Round  22, Train loss: 1.472, Test loss: 1.514, Test accuracy: 95.53
Round  23, Train loss: 1.472, Test loss: 1.512, Test accuracy: 95.68
Round  24, Train loss: 1.470, Test loss: 1.514, Test accuracy: 95.42
Round  25, Train loss: 1.471, Test loss: 1.510, Test accuracy: 95.53
Round  26, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.92
Round  27, Train loss: 1.469, Test loss: 1.506, Test accuracy: 95.86
Round  28, Train loss: 1.471, Test loss: 1.507, Test accuracy: 95.82
Round  29, Train loss: 1.468, Test loss: 1.504, Test accuracy: 96.00
Round  30, Train loss: 1.472, Test loss: 1.504, Test accuracy: 95.99
Round  31, Train loss: 1.466, Test loss: 1.504, Test accuracy: 95.97
Round  32, Train loss: 1.468, Test loss: 1.504, Test accuracy: 95.97
Round  33, Train loss: 1.467, Test loss: 1.505, Test accuracy: 95.94
Round  34, Train loss: 1.467, Test loss: 1.505, Test accuracy: 95.88
Round  35, Train loss: 1.468, Test loss: 1.504, Test accuracy: 95.96
Round  36, Train loss: 1.468, Test loss: 1.503, Test accuracy: 96.12
Round  37, Train loss: 1.469, Test loss: 1.503, Test accuracy: 96.10
Round  38, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.15
Round  39, Train loss: 1.468, Test loss: 1.503, Test accuracy: 96.12
Round  40, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.13
Round  41, Train loss: 1.469, Test loss: 1.503, Test accuracy: 96.08
Round  42, Train loss: 1.470, Test loss: 1.503, Test accuracy: 96.11
Round  43, Train loss: 1.471, Test loss: 1.503, Test accuracy: 96.05
Round  44, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.18
Round  45, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.18
Round  46, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.17
Round  47, Train loss: 1.469, Test loss: 1.502, Test accuracy: 96.15
Round  48, Train loss: 1.469, Test loss: 1.502, Test accuracy: 96.17
Round  49, Train loss: 1.465, Test loss: 1.502, Test accuracy: 96.17
Round  50, Train loss: 1.469, Test loss: 1.502, Test accuracy: 96.15
Round  51, Train loss: 1.470, Test loss: 1.502, Test accuracy: 96.16
Round  52, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.16
Round  53, Train loss: 1.469, Test loss: 1.502, Test accuracy: 96.14
Round  54, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.12
Round  55, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.16
Round  56, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.14
Round  57, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.14
Round  58, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.17
Round  59, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.17
Round  60, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.19
Round  61, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.19
Round  62, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.19
Round  63, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.19
Round  64, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.17
Round  65, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.16
Round  66, Train loss: 1.469, Test loss: 1.501, Test accuracy: 96.14
Round  67, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.15
Round  68, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.16
Round  69, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.14
Round  70, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.13
Round  71, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.17
Round  72, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.16
Round  73, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.16
Round  74, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.12
Round  75, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.16
Round  76, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.17
Round  77, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.15
Round  78, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.17
Round  79, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.16
Round  80, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.17
Round  81, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.16
Round  82, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.17
Round  83, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.17
Round  84, Train loss: 1.469, Test loss: 1.501, Test accuracy: 96.17
Round  85, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.17
Round  86, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.19
Round  87, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.18
Round  88, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.19
Round  89, Train loss: 1.469, Test loss: 1.501, Test accuracy: 96.20
Round  90, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.19
Round  91, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.21
Round  92, Train loss: 1.469, Test loss: 1.501, Test accuracy: 96.16
Round  93, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.17
Round  94, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.20
Round  95, Train loss: 1.467, Test loss: 1.500, Test accuracy: 96.19
Round  96, Train loss: 1.468, Test loss: 1.500, Test accuracy: 96.19
Round  97, Train loss: 1.466, Test loss: 1.500, Test accuracy: 96.22
Round  98, Train loss: 1.464, Test loss: 1.500, Test accuracy: 96.21
Round  99, Train loss: 1.465, Test loss: 1.500, Test accuracy: 96.22/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.467, Test loss: 1.500, Test accuracy: 96.21
Average accuracy final 10 rounds: 96.19666666666667 

1064.8476893901825
[0.9492075443267822, 1.8984150886535645, 2.7373831272125244, 3.5763511657714844, 4.422006130218506, 5.267661094665527, 6.113123893737793, 6.958586692810059, 7.793283462524414, 8.62798023223877, 9.466941833496094, 10.305903434753418, 11.139693975448608, 11.973484516143799, 12.814889192581177, 13.656293869018555, 14.506998300552368, 15.357702732086182, 16.187772035598755, 17.017841339111328, 17.8591046333313, 18.70036792755127, 19.53245735168457, 20.36454677581787, 21.202770233154297, 22.040993690490723, 22.895216464996338, 23.749439239501953, 24.593408823013306, 25.437378406524658, 26.278520345687866, 27.119662284851074, 27.948487281799316, 28.77731227874756, 29.610352277755737, 30.443392276763916, 31.264426708221436, 32.085461139678955, 32.93239092826843, 33.77932071685791, 34.62322521209717, 35.467129707336426, 36.291831970214844, 37.11653423309326, 37.959346532821655, 38.80215883255005, 39.632631063461304, 40.46310329437256, 41.307955265045166, 42.15280723571777, 42.99457883834839, 43.836350440979004, 44.67827534675598, 45.52020025253296, 46.347774028778076, 47.17534780502319, 48.0219292640686, 48.868510723114014, 49.69611859321594, 50.52372646331787, 51.36758470535278, 52.211442947387695, 53.05223035812378, 53.89301776885986, 54.72058844566345, 55.54815912246704, 56.378780126571655, 57.20940113067627, 58.0426549911499, 58.875908851623535, 59.71794271469116, 60.55997657775879, 61.40461468696594, 62.249252796173096, 63.094324827194214, 63.93939685821533, 64.77225685119629, 65.60511684417725, 66.44113898277283, 67.27716112136841, 68.1071674823761, 68.93717384338379, 69.78267049789429, 70.62816715240479, 71.47011017799377, 72.31205320358276, 73.15763235092163, 74.0032114982605, 74.84069895744324, 75.67818641662598, 76.51956582069397, 77.36094522476196, 78.208242893219, 79.05554056167603, 79.91325521469116, 80.7709698677063, 81.62805461883545, 82.4851393699646, 83.33077025413513, 84.17640113830566, 85.00803852081299, 85.83967590332031, 86.67462849617004, 87.50958108901978, 88.36375856399536, 89.21793603897095, 90.07391023635864, 90.92988443374634, 91.78886294364929, 92.64784145355225, 93.49409651756287, 94.34035158157349, 95.1832492351532, 96.02614688873291, 96.86682748794556, 97.7075080871582, 98.56262183189392, 99.41773557662964, 100.27610516548157, 101.1344747543335, 101.97009634971619, 102.80571794509888, 103.64016246795654, 104.47460699081421, 105.30762672424316, 106.14064645767212, 106.98843264579773, 107.83621883392334, 108.69355869293213, 109.55089855194092, 110.3884289264679, 111.22595930099487, 112.0591561794281, 112.89235305786133, 113.72513198852539, 114.55791091918945, 115.39614915847778, 116.23438739776611, 117.07677412033081, 117.91916084289551, 118.76693558692932, 119.61471033096313, 120.45782542228699, 121.30094051361084, 122.13343524932861, 122.96592998504639, 123.7965304851532, 124.62713098526001, 125.46552777290344, 126.30392456054688, 127.15709614753723, 128.0102677345276, 128.85015320777893, 129.69003868103027, 130.5135853290558, 131.3371319770813, 132.16278290748596, 132.98843383789062, 133.81479120254517, 134.6411485671997, 135.48275208473206, 136.3243556022644, 137.17284655570984, 138.02133750915527, 138.86431193351746, 139.70728635787964, 140.53372597694397, 141.3601655960083, 142.19213199615479, 143.02409839630127, 143.8573408126831, 144.69058322906494, 145.5351996421814, 146.37981605529785, 147.2311656475067, 148.08251523971558, 148.92243146896362, 149.76234769821167, 150.59420323371887, 151.42605876922607, 152.26455116271973, 153.10304355621338, 153.94406962394714, 154.7850956916809, 155.611394405365, 156.43769311904907, 157.3089942932129, 158.1802954673767, 159.01748323440552, 159.85467100143433, 160.68081212043762, 161.50695323944092, 162.33760261535645, 163.16825199127197, 164.00507855415344, 164.8419051170349, 165.66931581497192, 166.49672651290894, 167.33472895622253, 168.17273139953613, 169.4897813796997, 170.80683135986328]
[17.416666666666668, 17.416666666666668, 53.28333333333333, 53.28333333333333, 52.875, 52.875, 65.50833333333334, 65.50833333333334, 71.125, 71.125, 78.575, 78.575, 79.08333333333333, 79.08333333333333, 81.80833333333334, 81.80833333333334, 83.95833333333333, 83.95833333333333, 84.075, 84.075, 85.23333333333333, 85.23333333333333, 88.91666666666667, 88.91666666666667, 91.05833333333334, 91.05833333333334, 92.28333333333333, 92.28333333333333, 94.41666666666667, 94.41666666666667, 94.50833333333334, 94.50833333333334, 94.11666666666666, 94.11666666666666, 93.91666666666667, 93.91666666666667, 93.75833333333334, 93.75833333333334, 93.975, 93.975, 95.28333333333333, 95.28333333333333, 95.3, 95.3, 95.525, 95.525, 95.68333333333334, 95.68333333333334, 95.425, 95.425, 95.525, 95.525, 95.925, 95.925, 95.85833333333333, 95.85833333333333, 95.81666666666666, 95.81666666666666, 96.0, 96.0, 95.99166666666666, 95.99166666666666, 95.96666666666667, 95.96666666666667, 95.975, 95.975, 95.94166666666666, 95.94166666666666, 95.88333333333334, 95.88333333333334, 95.95833333333333, 95.95833333333333, 96.125, 96.125, 96.1, 96.1, 96.15, 96.15, 96.11666666666666, 96.11666666666666, 96.13333333333334, 96.13333333333334, 96.08333333333333, 96.08333333333333, 96.10833333333333, 96.10833333333333, 96.05, 96.05, 96.18333333333334, 96.18333333333334, 96.18333333333334, 96.18333333333334, 96.16666666666667, 96.16666666666667, 96.15, 96.15, 96.175, 96.175, 96.175, 96.175, 96.15, 96.15, 96.15833333333333, 96.15833333333333, 96.15833333333333, 96.15833333333333, 96.14166666666667, 96.14166666666667, 96.11666666666666, 96.11666666666666, 96.15833333333333, 96.15833333333333, 96.14166666666667, 96.14166666666667, 96.14166666666667, 96.14166666666667, 96.175, 96.175, 96.175, 96.175, 96.19166666666666, 96.19166666666666, 96.19166666666666, 96.19166666666666, 96.19166666666666, 96.19166666666666, 96.19166666666666, 96.19166666666666, 96.16666666666667, 96.16666666666667, 96.15833333333333, 96.15833333333333, 96.14166666666667, 96.14166666666667, 96.15, 96.15, 96.15833333333333, 96.15833333333333, 96.14166666666667, 96.14166666666667, 96.13333333333334, 96.13333333333334, 96.175, 96.175, 96.15833333333333, 96.15833333333333, 96.15833333333333, 96.15833333333333, 96.125, 96.125, 96.15833333333333, 96.15833333333333, 96.16666666666667, 96.16666666666667, 96.15, 96.15, 96.16666666666667, 96.16666666666667, 96.15833333333333, 96.15833333333333, 96.175, 96.175, 96.15833333333333, 96.15833333333333, 96.175, 96.175, 96.16666666666667, 96.16666666666667, 96.175, 96.175, 96.175, 96.175, 96.19166666666666, 96.19166666666666, 96.18333333333334, 96.18333333333334, 96.19166666666666, 96.19166666666666, 96.2, 96.2, 96.19166666666666, 96.19166666666666, 96.20833333333333, 96.20833333333333, 96.15833333333333, 96.15833333333333, 96.175, 96.175, 96.2, 96.2, 96.19166666666666, 96.19166666666666, 96.19166666666666, 96.19166666666666, 96.21666666666667, 96.21666666666667, 96.20833333333333, 96.20833333333333, 96.225, 96.225, 96.20833333333333, 96.20833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.365, Test loss: 2.102, Test accuracy: 54.60
Round   1, Train loss: 1.314, Test loss: 1.978, Test accuracy: 58.58
Round   2, Train loss: 1.262, Test loss: 1.848, Test accuracy: 73.15
Round   3, Train loss: 1.146, Test loss: 1.796, Test accuracy: 73.58
Round   4, Train loss: 1.210, Test loss: 1.718, Test accuracy: 82.44
Round   5, Train loss: 1.155, Test loss: 1.679, Test accuracy: 83.85
Round   6, Train loss: 1.301, Test loss: 1.655, Test accuracy: 86.37
Round   7, Train loss: 1.231, Test loss: 1.654, Test accuracy: 85.53
Round   8, Train loss: 1.188, Test loss: 1.639, Test accuracy: 86.08
Round   9, Train loss: 1.192, Test loss: 1.617, Test accuracy: 88.23
Round  10, Train loss: 1.228, Test loss: 1.618, Test accuracy: 87.70
Round  11, Train loss: 1.269, Test loss: 1.608, Test accuracy: 88.42
Round  12, Train loss: 1.145, Test loss: 1.606, Test accuracy: 88.24
Round  13, Train loss: 1.184, Test loss: 1.603, Test accuracy: 88.39
Round  14, Train loss: 1.187, Test loss: 1.601, Test accuracy: 88.43
Round  15, Train loss: 1.107, Test loss: 1.601, Test accuracy: 88.33
Round  16, Train loss: 1.141, Test loss: 1.600, Test accuracy: 88.29
Round  17, Train loss: 1.145, Test loss: 1.599, Test accuracy: 88.22
Round  18, Train loss: 1.188, Test loss: 1.596, Test accuracy: 88.37
Round  19, Train loss: 1.182, Test loss: 1.596, Test accuracy: 88.31
Round  20, Train loss: 1.187, Test loss: 1.596, Test accuracy: 88.28
Round  21, Train loss: 1.144, Test loss: 1.594, Test accuracy: 88.28
Round  22, Train loss: 1.145, Test loss: 1.593, Test accuracy: 88.16
Round  23, Train loss: 1.267, Test loss: 1.593, Test accuracy: 88.17
Round  24, Train loss: 1.225, Test loss: 1.592, Test accuracy: 88.32
Round  25, Train loss: 1.185, Test loss: 1.593, Test accuracy: 88.16
Round  26, Train loss: 1.142, Test loss: 1.592, Test accuracy: 88.16
Round  27, Train loss: 1.183, Test loss: 1.592, Test accuracy: 88.13
Round  28, Train loss: 1.225, Test loss: 1.591, Test accuracy: 88.17
Round  29, Train loss: 1.182, Test loss: 1.592, Test accuracy: 88.14
Round  30, Train loss: 1.183, Test loss: 1.592, Test accuracy: 88.04
Round  31, Train loss: 1.141, Test loss: 1.593, Test accuracy: 87.84
Round  32, Train loss: 1.184, Test loss: 1.592, Test accuracy: 88.00
Round  33, Train loss: 1.224, Test loss: 1.594, Test accuracy: 87.89
Round  34, Train loss: 1.101, Test loss: 1.594, Test accuracy: 87.86
Round  35, Train loss: 1.224, Test loss: 1.595, Test accuracy: 87.71
Round  36, Train loss: 1.181, Test loss: 1.594, Test accuracy: 87.81
Round  37, Train loss: 1.141, Test loss: 1.594, Test accuracy: 87.72
Round  38, Train loss: 1.181, Test loss: 1.593, Test accuracy: 87.78
Round  39, Train loss: 1.141, Test loss: 1.594, Test accuracy: 87.73
Round  40, Train loss: 1.182, Test loss: 1.595, Test accuracy: 87.54
Round  41, Train loss: 1.265, Test loss: 1.595, Test accuracy: 87.58
Round  42, Train loss: 1.141, Test loss: 1.596, Test accuracy: 87.48
Round  43, Train loss: 1.181, Test loss: 1.595, Test accuracy: 87.58
Round  44, Train loss: 1.143, Test loss: 1.597, Test accuracy: 87.39
Round  45, Train loss: 1.182, Test loss: 1.597, Test accuracy: 87.31
Round  46, Train loss: 1.142, Test loss: 1.600, Test accuracy: 87.06
Round  47, Train loss: 1.142, Test loss: 1.597, Test accuracy: 87.29
Round  48, Train loss: 1.140, Test loss: 1.597, Test accuracy: 87.31
Round  49, Train loss: 1.140, Test loss: 1.599, Test accuracy: 87.22
Round  50, Train loss: 1.181, Test loss: 1.600, Test accuracy: 87.05
Round  51, Train loss: 1.182, Test loss: 1.600, Test accuracy: 87.08
Round  52, Train loss: 1.181, Test loss: 1.600, Test accuracy: 87.06
Round  53, Train loss: 1.182, Test loss: 1.599, Test accuracy: 87.18
Round  54, Train loss: 1.141, Test loss: 1.599, Test accuracy: 87.12
Round  55, Train loss: 1.182, Test loss: 1.600, Test accuracy: 87.04
Round  56, Train loss: 1.099, Test loss: 1.602, Test accuracy: 86.86
Round  57, Train loss: 1.142, Test loss: 1.602, Test accuracy: 86.68
Round  58, Train loss: 1.181, Test loss: 1.602, Test accuracy: 86.77
Round  59, Train loss: 1.140, Test loss: 1.601, Test accuracy: 86.75
Round  60, Train loss: 1.099, Test loss: 1.599, Test accuracy: 87.00
Round  61, Train loss: 1.141, Test loss: 1.600, Test accuracy: 87.03
Round  62, Train loss: 1.140, Test loss: 1.601, Test accuracy: 86.85
Round  63, Train loss: 1.099, Test loss: 1.601, Test accuracy: 86.90
Round  64, Train loss: 1.099, Test loss: 1.602, Test accuracy: 86.77
Round  65, Train loss: 1.181, Test loss: 1.600, Test accuracy: 86.89
Round  66, Train loss: 1.223, Test loss: 1.602, Test accuracy: 86.76
Round  67, Train loss: 1.140, Test loss: 1.602, Test accuracy: 86.83
Round  68, Train loss: 1.182, Test loss: 1.603, Test accuracy: 86.69
Round  69, Train loss: 1.140, Test loss: 1.605, Test accuracy: 86.45
Round  70, Train loss: 1.182, Test loss: 1.605, Test accuracy: 86.45
Round  71, Train loss: 1.223, Test loss: 1.606, Test accuracy: 86.42
Round  72, Train loss: 1.182, Test loss: 1.607, Test accuracy: 86.14
Round  73, Train loss: 1.140, Test loss: 1.609, Test accuracy: 86.11
Round  74, Train loss: 1.141, Test loss: 1.607, Test accuracy: 86.26
Round  75, Train loss: 1.180, Test loss: 1.604, Test accuracy: 86.43
Round  76, Train loss: 1.141, Test loss: 1.604, Test accuracy: 86.42
Round  77, Train loss: 1.099, Test loss: 1.605, Test accuracy: 86.39
Round  78, Train loss: 1.140, Test loss: 1.606, Test accuracy: 86.22
Round  79, Train loss: 1.182, Test loss: 1.606, Test accuracy: 86.38
Round  80, Train loss: 1.180, Test loss: 1.605, Test accuracy: 86.34
Round  81, Train loss: 1.141, Test loss: 1.608, Test accuracy: 86.07
Round  82, Train loss: 1.181, Test loss: 1.606, Test accuracy: 86.25
Round  83, Train loss: 1.223, Test loss: 1.607, Test accuracy: 86.15
Round  84, Train loss: 1.181, Test loss: 1.608, Test accuracy: 86.11
Round  85, Train loss: 1.181, Test loss: 1.607, Test accuracy: 86.19
Round  86, Train loss: 1.140, Test loss: 1.607, Test accuracy: 86.10
Round  87, Train loss: 1.181, Test loss: 1.605, Test accuracy: 86.22
Round  88, Train loss: 1.098, Test loss: 1.608, Test accuracy: 86.08
Round  89, Train loss: 1.222, Test loss: 1.608, Test accuracy: 85.95
Round  90, Train loss: 1.099, Test loss: 1.610, Test accuracy: 85.79
Round  91, Train loss: 1.099, Test loss: 1.610, Test accuracy: 85.80
Round  92, Train loss: 1.140, Test loss: 1.609, Test accuracy: 85.94
Round  93, Train loss: 1.181, Test loss: 1.609, Test accuracy: 85.87
Round  94, Train loss: 1.182, Test loss: 1.611, Test accuracy: 85.70
Round  95, Train loss: 1.141, Test loss: 1.611, Test accuracy: 85.61
Round  96, Train loss: 1.099, Test loss: 1.609, Test accuracy: 85.92
Round  97, Train loss: 1.223, Test loss: 1.608, Test accuracy: 85.97
Round  98, Train loss: 1.181, Test loss: 1.607, Test accuracy: 86.03
Round  99, Train loss: 1.140, Test loss: 1.608, Test accuracy: 85.88
Final Round, Train loss: 1.161, Test loss: 1.608, Test accuracy: 85.92
Average accuracy final 10 rounds: 85.85166666666669
1373.6746697425842
[]
[54.6, 58.575, 73.15, 73.575, 82.44166666666666, 83.85, 86.36666666666666, 85.525, 86.08333333333333, 88.23333333333333, 87.7, 88.425, 88.24166666666666, 88.39166666666667, 88.43333333333334, 88.325, 88.29166666666667, 88.225, 88.36666666666666, 88.30833333333334, 88.275, 88.28333333333333, 88.15833333333333, 88.175, 88.31666666666666, 88.15833333333333, 88.15833333333333, 88.13333333333334, 88.175, 88.14166666666667, 88.04166666666667, 87.84166666666667, 88.0, 87.89166666666667, 87.85833333333333, 87.70833333333333, 87.80833333333334, 87.725, 87.775, 87.73333333333333, 87.54166666666667, 87.575, 87.48333333333333, 87.575, 87.39166666666667, 87.30833333333334, 87.05833333333334, 87.29166666666667, 87.30833333333334, 87.21666666666667, 87.05, 87.08333333333333, 87.05833333333334, 87.18333333333334, 87.125, 87.04166666666667, 86.85833333333333, 86.68333333333334, 86.76666666666667, 86.75, 87.0, 87.025, 86.85, 86.9, 86.76666666666667, 86.89166666666667, 86.75833333333334, 86.825, 86.69166666666666, 86.45, 86.45, 86.41666666666667, 86.14166666666667, 86.10833333333333, 86.25833333333334, 86.43333333333334, 86.41666666666667, 86.39166666666667, 86.225, 86.375, 86.34166666666667, 86.06666666666666, 86.25, 86.15, 86.10833333333333, 86.19166666666666, 86.1, 86.21666666666667, 86.08333333333333, 85.95, 85.79166666666667, 85.8, 85.94166666666666, 85.86666666666666, 85.7, 85.60833333333333, 85.91666666666667, 85.975, 86.03333333333333, 85.88333333333334, 85.91666666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.143, Test loss: 2.139, Test accuracy: 32.84
Round   1, Train loss: 1.923, Test loss: 2.008, Test accuracy: 46.53
Round   2, Train loss: 1.843, Test loss: 1.967, Test accuracy: 48.81
Round   3, Train loss: 1.899, Test loss: 2.010, Test accuracy: 43.83
Round   4, Train loss: 1.407, Test loss: 2.033, Test accuracy: 38.55
Round   5, Train loss: 1.446, Test loss: 1.881, Test accuracy: 56.88
Round   6, Train loss: 0.975, Test loss: 1.864, Test accuracy: 62.47
Round   7, Train loss: 1.181, Test loss: 1.865, Test accuracy: 62.13
Round   8, Train loss: 0.581, Test loss: 1.809, Test accuracy: 67.16
Round   9, Train loss: 1.201, Test loss: 1.830, Test accuracy: 66.78
Round  10, Train loss: 1.077, Test loss: 1.843, Test accuracy: 66.50
Round  11, Train loss: 0.759, Test loss: 1.883, Test accuracy: 61.09
Round  12, Train loss: 0.766, Test loss: 1.853, Test accuracy: 64.02
Round  13, Train loss: 0.384, Test loss: 1.759, Test accuracy: 74.79
Round  14, Train loss: 0.214, Test loss: 1.750, Test accuracy: 75.33
Round  15, Train loss: 0.213, Test loss: 1.763, Test accuracy: 73.50
Round  16, Train loss: -0.409, Test loss: 1.740, Test accuracy: 76.08
Round  17, Train loss: 0.178, Test loss: 1.693, Test accuracy: 80.50
Round  18, Train loss: 0.004, Test loss: 1.645, Test accuracy: 82.50
Round  19, Train loss: -0.808, Test loss: 1.674, Test accuracy: 79.34
Round  20, Train loss: 0.112, Test loss: 1.630, Test accuracy: 84.06
Round  21, Train loss: -0.350, Test loss: 1.646, Test accuracy: 82.47
Round  22, Train loss: -0.427, Test loss: 1.641, Test accuracy: 81.80
Round  23, Train loss: 0.214, Test loss: 1.644, Test accuracy: 81.97
Round  24, Train loss: -0.509, Test loss: 1.583, Test accuracy: 87.97
Round  25, Train loss: -0.618, Test loss: 1.599, Test accuracy: 86.27
Round  26, Train loss: -0.838, Test loss: 1.580, Test accuracy: 88.83
Round  27, Train loss: -1.134, Test loss: 1.578, Test accuracy: 89.02
Round  28, Train loss: -0.941, Test loss: 1.588, Test accuracy: 88.03
Round  29, Train loss: -0.764, Test loss: 1.586, Test accuracy: 87.94
Round  30, Train loss: -0.910, Test loss: 1.574, Test accuracy: 89.15
Round  31, Train loss: -1.050, Test loss: 1.560, Test accuracy: 90.65
Round  32, Train loss: -1.850, Test loss: 1.563, Test accuracy: 90.36
Round  33, Train loss: -1.669, Test loss: 1.553, Test accuracy: 90.83
Round  34, Train loss: -0.616, Test loss: 1.537, Test accuracy: 92.41
Round  35, Train loss: -1.337, Test loss: 1.555, Test accuracy: 90.71
Round  36, Train loss: -1.060, Test loss: 1.540, Test accuracy: 92.23
Round  37, Train loss: -1.424, Test loss: 1.541, Test accuracy: 92.12
Round  38, Train loss: -0.884, Test loss: 1.566, Test accuracy: 89.47
Round  39, Train loss: -0.933, Test loss: 1.565, Test accuracy: 89.65
Round  40, Train loss: -1.217, Test loss: 1.562, Test accuracy: 89.87
Round  41, Train loss: -1.671, Test loss: 1.562, Test accuracy: 89.95
Round  42, Train loss: -0.980, Test loss: 1.551, Test accuracy: 91.08
Round  43, Train loss: -1.022, Test loss: 1.561, Test accuracy: 89.99
Round  44, Train loss: -1.060, Test loss: 1.562, Test accuracy: 89.86
Round  45, Train loss: -0.998, Test loss: 1.563, Test accuracy: 89.80
Round  46, Train loss: -0.744, Test loss: 1.560, Test accuracy: 90.26
Round  47, Train loss: -1.585, Test loss: 1.561, Test accuracy: 90.16
Round  48, Train loss: -1.277, Test loss: 1.551, Test accuracy: 90.97
Round  49, Train loss: -1.175, Test loss: 1.549, Test accuracy: 91.19
Round  50, Train loss: -1.782, Test loss: 1.549, Test accuracy: 91.20
Round  51, Train loss: -0.614, Test loss: 1.520, Test accuracy: 94.13
Round  52, Train loss: -0.586, Test loss: 1.506, Test accuracy: 95.48
Round  53, Train loss: -0.372, Test loss: 1.536, Test accuracy: 92.60
Round  54, Train loss: -0.880, Test loss: 1.535, Test accuracy: 92.65
Round  55, Train loss: -1.679, Test loss: 1.522, Test accuracy: 93.97
Round  56, Train loss: -1.136, Test loss: 1.521, Test accuracy: 94.06
Round  57, Train loss: -0.915, Test loss: 1.535, Test accuracy: 92.58
Round  58, Train loss: -0.690, Test loss: 1.521, Test accuracy: 94.03
Round  59, Train loss: -0.460, Test loss: 1.522, Test accuracy: 93.91
Round  60, Train loss: -1.967, Test loss: 1.522, Test accuracy: 93.94
Round  61, Train loss: -1.297, Test loss: 1.522, Test accuracy: 94.03
Round  62, Train loss: -0.381, Test loss: 1.536, Test accuracy: 92.56
Round  63, Train loss: -0.712, Test loss: 1.563, Test accuracy: 89.81
Round  64, Train loss: -0.473, Test loss: 1.548, Test accuracy: 91.32
Round  65, Train loss: -0.855, Test loss: 1.534, Test accuracy: 92.76
Round  66, Train loss: -0.236, Test loss: 1.550, Test accuracy: 91.17
Round  67, Train loss: -1.210, Test loss: 1.551, Test accuracy: 91.07
Round  68, Train loss: -0.674, Test loss: 1.566, Test accuracy: 89.60
Round  69, Train loss: -1.242, Test loss: 1.552, Test accuracy: 91.00
Round  70, Train loss: -1.363, Test loss: 1.551, Test accuracy: 91.10
Round  71, Train loss: -0.015, Test loss: 1.564, Test accuracy: 89.74
Round  72, Train loss: -0.649, Test loss: 1.549, Test accuracy: 91.27
Round  73, Train loss: -0.522, Test loss: 1.550, Test accuracy: 91.10
Round  74, Train loss: -0.584, Test loss: 1.551, Test accuracy: 90.99
Round  75, Train loss: -0.655, Test loss: 1.536, Test accuracy: 92.59
Round  76, Train loss: -0.485, Test loss: 1.507, Test accuracy: 95.49
Round  77, Train loss: -0.348, Test loss: 1.535, Test accuracy: 92.72
Round  78, Train loss: -0.388, Test loss: 1.505, Test accuracy: 95.68
Round  79, Train loss: -0.505, Test loss: 1.521, Test accuracy: 94.07
Round  80, Train loss: -0.254, Test loss: 1.535, Test accuracy: 92.64
Round  81, Train loss: -0.271, Test loss: 1.520, Test accuracy: 94.24
Round  82, Train loss: -0.798, Test loss: 1.505, Test accuracy: 95.72
Round  83, Train loss: -0.246, Test loss: 1.520, Test accuracy: 94.14
Round  84, Train loss: -1.101, Test loss: 1.519, Test accuracy: 94.27
Round  85, Train loss: -0.840, Test loss: 1.517, Test accuracy: 94.37
Round  86, Train loss: -0.328, Test loss: 1.546, Test accuracy: 91.54
Round  87, Train loss: -0.853, Test loss: 1.532, Test accuracy: 92.97
Round  88, Train loss: -0.190, Test loss: 1.519, Test accuracy: 94.32
Round  89, Train loss: -1.080, Test loss: 1.518, Test accuracy: 94.34
Round  90, Train loss: -0.515, Test loss: 1.504, Test accuracy: 95.83
Round  91, Train loss: -0.745, Test loss: 1.518, Test accuracy: 94.40
Round  92, Train loss: -0.329, Test loss: 1.533, Test accuracy: 92.83
Round  93, Train loss: -0.907, Test loss: 1.522, Test accuracy: 94.03
Round  94, Train loss: -0.485, Test loss: 1.522, Test accuracy: 93.93
Round  95, Train loss: -0.688, Test loss: 1.536, Test accuracy: 92.53
Round  96, Train loss: -0.249, Test loss: 1.536, Test accuracy: 92.54
Round  97, Train loss: -0.180, Test loss: 1.519, Test accuracy: 94.17
Round  98, Train loss: -0.563, Test loss: 1.519, Test accuracy: 94.28
Round  99, Train loss: -0.434, Test loss: 1.534, Test accuracy: 92.74
Final Round, Train loss: 1.610, Test loss: 1.561, Test accuracy: 90.51
Average accuracy final 10 rounds: 93.72833333333332
Average global accuracy final 10 rounds: 93.72833333333332
978.5187878608704
[]
[32.84166666666667, 46.53333333333333, 48.80833333333333, 43.825, 38.55, 56.875, 62.46666666666667, 62.13333333333333, 67.15833333333333, 66.775, 66.5, 61.09166666666667, 64.01666666666667, 74.79166666666667, 75.325, 73.5, 76.08333333333333, 80.5, 82.5, 79.34166666666667, 84.05833333333334, 82.46666666666667, 81.8, 81.96666666666667, 87.96666666666667, 86.26666666666667, 88.83333333333333, 89.01666666666667, 88.025, 87.94166666666666, 89.15, 90.65, 90.35833333333333, 90.83333333333333, 92.40833333333333, 90.70833333333333, 92.23333333333333, 92.11666666666666, 89.46666666666667, 89.65, 89.86666666666666, 89.95, 91.075, 89.99166666666666, 89.85833333333333, 89.8, 90.25833333333334, 90.15833333333333, 90.96666666666667, 91.19166666666666, 91.2, 94.13333333333334, 95.48333333333333, 92.6, 92.65, 93.96666666666667, 94.05833333333334, 92.58333333333333, 94.03333333333333, 93.90833333333333, 93.94166666666666, 94.03333333333333, 92.55833333333334, 89.80833333333334, 91.31666666666666, 92.75833333333334, 91.16666666666667, 91.06666666666666, 89.6, 91.0, 91.1, 89.74166666666666, 91.26666666666667, 91.1, 90.99166666666666, 92.59166666666667, 95.49166666666666, 92.725, 95.68333333333334, 94.06666666666666, 92.64166666666667, 94.24166666666666, 95.71666666666667, 94.14166666666667, 94.26666666666667, 94.36666666666666, 91.54166666666667, 92.96666666666667, 94.31666666666666, 94.34166666666667, 95.825, 94.4, 92.825, 94.03333333333333, 93.93333333333334, 92.53333333333333, 92.54166666666667, 94.16666666666667, 94.28333333333333, 92.74166666666666, 90.50833333333334]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.62
Round   0, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 12.44
Round   1, Train loss: 2.300, Test loss: 2.301, Test accuracy: 13.03
Round   1, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.55
Round   2, Train loss: 2.300, Test loss: 2.301, Test accuracy: 13.07
Round   2, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.54
Round   3, Train loss: 2.300, Test loss: 2.301, Test accuracy: 13.32
Round   3, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.83
Round   4, Train loss: 2.300, Test loss: 2.301, Test accuracy: 13.53
Round   4, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.93
Round   5, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.69
Round   5, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.07
Round   6, Train loss: 2.300, Test loss: 2.301, Test accuracy: 13.68
Round   6, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 13.11
Round   7, Train loss: 2.299, Test loss: 2.301, Test accuracy: 13.78
Round   7, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 13.08
Round   8, Train loss: 2.300, Test loss: 2.301, Test accuracy: 13.93
Round   8, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 13.24
Round   9, Train loss: 2.302, Test loss: 2.301, Test accuracy: 14.06
Round   9, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 13.40
Round  10, Train loss: 2.299, Test loss: 2.301, Test accuracy: 14.03
Round  10, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 13.33
Round  11, Train loss: 2.299, Test loss: 2.301, Test accuracy: 14.12
Round  11, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 13.41
Round  12, Train loss: 2.302, Test loss: 2.301, Test accuracy: 14.18
Round  12, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 13.57
Round  13, Train loss: 2.299, Test loss: 2.301, Test accuracy: 14.09
Round  13, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 13.78
Round  14, Train loss: 2.299, Test loss: 2.301, Test accuracy: 14.29
Round  14, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 13.80
Round  15, Train loss: 2.300, Test loss: 2.301, Test accuracy: 14.36
Round  15, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 13.93
Round  16, Train loss: 2.301, Test loss: 2.301, Test accuracy: 14.44
Round  16, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.95
Round  17, Train loss: 2.300, Test loss: 2.301, Test accuracy: 14.66
Round  17, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 14.12
Round  18, Train loss: 2.300, Test loss: 2.300, Test accuracy: 14.60
Round  18, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 14.16
Round  19, Train loss: 2.298, Test loss: 2.300, Test accuracy: 14.70
Round  19, Global train loss: 2.298, Global test loss: 2.301, Global test accuracy: 14.22
Round  20, Train loss: 2.298, Test loss: 2.300, Test accuracy: 15.06
Round  20, Global train loss: 2.298, Global test loss: 2.300, Global test accuracy: 14.27
Round  21, Train loss: 2.299, Test loss: 2.300, Test accuracy: 15.26
Round  21, Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 14.34
Round  22, Train loss: 2.300, Test loss: 2.300, Test accuracy: 15.25
Round  22, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 14.33
Round  23, Train loss: 2.301, Test loss: 2.300, Test accuracy: 15.40
Round  23, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 14.37
Round  24, Train loss: 2.299, Test loss: 2.300, Test accuracy: 15.33
Round  24, Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 14.44
Round  25, Train loss: 2.299, Test loss: 2.300, Test accuracy: 15.42
Round  25, Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 14.50
Round  26, Train loss: 2.299, Test loss: 2.300, Test accuracy: 15.37
Round  26, Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 14.62
Round  27, Train loss: 2.301, Test loss: 2.300, Test accuracy: 15.40
Round  27, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 14.50
Round  28, Train loss: 2.300, Test loss: 2.300, Test accuracy: 15.45
Round  28, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 14.51
Round  29, Train loss: 2.300, Test loss: 2.300, Test accuracy: 15.58
Round  29, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 14.47
Round  30, Train loss: 2.299, Test loss: 2.300, Test accuracy: 15.69
Round  30, Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 14.46
Round  31, Train loss: 2.300, Test loss: 2.300, Test accuracy: 15.35
Round  31, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 14.57
Round  32, Train loss: 2.300, Test loss: 2.300, Test accuracy: 15.33
Round  32, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 14.57
Round  33, Train loss: 2.298, Test loss: 2.300, Test accuracy: 15.28
Round  33, Global train loss: 2.298, Global test loss: 2.300, Global test accuracy: 14.59
Round  34, Train loss: 2.300, Test loss: 2.300, Test accuracy: 15.25
Round  34, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 14.66
Round  35, Train loss: 2.300, Test loss: 2.300, Test accuracy: 15.32
Round  35, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 14.69
Round  36, Train loss: 2.298, Test loss: 2.300, Test accuracy: 15.38
Round  36, Global train loss: 2.298, Global test loss: 2.300, Global test accuracy: 14.76
Round  37, Train loss: 2.299, Test loss: 2.300, Test accuracy: 15.39
Round  37, Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 14.78
Round  38, Train loss: 2.300, Test loss: 2.300, Test accuracy: 15.57
Round  38, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 14.80
Round  39, Train loss: 2.300, Test loss: 2.300, Test accuracy: 15.71
Round  39, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 14.87
Round  40, Train loss: 2.298, Test loss: 2.300, Test accuracy: 15.74
Round  40, Global train loss: 2.298, Global test loss: 2.300, Global test accuracy: 15.01
Round  41, Train loss: 2.299, Test loss: 2.300, Test accuracy: 15.68
Round  41, Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 15.07
Round  42, Train loss: 2.299, Test loss: 2.300, Test accuracy: 15.72
Round  42, Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 15.12
Round  43, Train loss: 2.298, Test loss: 2.300, Test accuracy: 15.72
Round  43, Global train loss: 2.298, Global test loss: 2.300, Global test accuracy: 15.24
Round  44, Train loss: 2.298, Test loss: 2.300, Test accuracy: 15.88
Round  44, Global train loss: 2.298, Global test loss: 2.300, Global test accuracy: 15.32
Round  45, Train loss: 2.300, Test loss: 2.300, Test accuracy: 15.88
Round  45, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 15.36
Round  46, Train loss: 2.301, Test loss: 2.299, Test accuracy: 15.90
Round  46, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 15.38
Round  47, Train loss: 2.299, Test loss: 2.299, Test accuracy: 16.01
Round  47, Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 15.47
Round  48, Train loss: 2.299, Test loss: 2.299, Test accuracy: 16.02
Round  48, Global train loss: 2.299, Global test loss: 2.300, Global test accuracy: 15.56
Round  49, Train loss: 2.299, Test loss: 2.299, Test accuracy: 16.10
Round  49, Global train loss: 2.299, Global test loss: 2.299, Global test accuracy: 15.39
Round  50, Train loss: 2.300, Test loss: 2.299, Test accuracy: 16.12
Round  50, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 15.11
Round  51, Train loss: 2.300, Test loss: 2.299, Test accuracy: 16.12
Round  51, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 15.26
Round  52, Train loss: 2.297, Test loss: 2.299, Test accuracy: 16.10
Round  52, Global train loss: 2.297, Global test loss: 2.299, Global test accuracy: 15.23
Round  53, Train loss: 2.300, Test loss: 2.299, Test accuracy: 16.08
Round  53, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 15.27
Round  54, Train loss: 2.299, Test loss: 2.299, Test accuracy: 16.11
Round  54, Global train loss: 2.299, Global test loss: 2.299, Global test accuracy: 15.08
Round  55, Train loss: 2.297, Test loss: 2.299, Test accuracy: 15.85
Round  55, Global train loss: 2.297, Global test loss: 2.299, Global test accuracy: 15.10
Round  56, Train loss: 2.297, Test loss: 2.299, Test accuracy: 15.94
Round  56, Global train loss: 2.297, Global test loss: 2.299, Global test accuracy: 14.90
Round  57, Train loss: 2.298, Test loss: 2.299, Test accuracy: 15.73
Round  57, Global train loss: 2.298, Global test loss: 2.299, Global test accuracy: 14.94
Round  58, Train loss: 2.298, Test loss: 2.299, Test accuracy: 15.67
Round  58, Global train loss: 2.298, Global test loss: 2.299, Global test accuracy: 15.21
Round  59, Train loss: 2.297, Test loss: 2.299, Test accuracy: 15.77
Round  59, Global train loss: 2.297, Global test loss: 2.299, Global test accuracy: 15.18
Round  60, Train loss: 2.295, Test loss: 2.299, Test accuracy: 15.88
Round  60, Global train loss: 2.295, Global test loss: 2.299, Global test accuracy: 15.37
Round  61, Train loss: 2.300, Test loss: 2.299, Test accuracy: 15.94
Round  61, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 15.45
Round  62, Train loss: 2.301, Test loss: 2.299, Test accuracy: 15.95
Round  62, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 15.49
Round  63, Train loss: 2.299, Test loss: 2.299, Test accuracy: 15.95
Round  63, Global train loss: 2.299, Global test loss: 2.299, Global test accuracy: 15.53
Round  64, Train loss: 2.298, Test loss: 2.299, Test accuracy: 15.91
Round  64, Global train loss: 2.298, Global test loss: 2.299, Global test accuracy: 15.43
Round  65, Train loss: 2.296, Test loss: 2.299, Test accuracy: 15.93
Round  65, Global train loss: 2.296, Global test loss: 2.299, Global test accuracy: 15.31
Round  66, Train loss: 2.297, Test loss: 2.299, Test accuracy: 15.99
Round  66, Global train loss: 2.297, Global test loss: 2.299, Global test accuracy: 15.27
Round  67, Train loss: 2.297, Test loss: 2.299, Test accuracy: 15.93
Round  67, Global train loss: 2.297, Global test loss: 2.299, Global test accuracy: 15.18
Round  68, Train loss: 2.297, Test loss: 2.299, Test accuracy: 15.82
Round  68, Global train loss: 2.297, Global test loss: 2.299, Global test accuracy: 15.43
Round  69, Train loss: 2.296, Test loss: 2.299, Test accuracy: 15.81
Round  69, Global train loss: 2.296, Global test loss: 2.299, Global test accuracy: 15.53
Round  70, Train loss: 2.299, Test loss: 2.299, Test accuracy: 15.82
Round  70, Global train loss: 2.299, Global test loss: 2.299, Global test accuracy: 15.59
Round  71, Train loss: 2.295, Test loss: 2.298, Test accuracy: 15.82
Round  71, Global train loss: 2.295, Global test loss: 2.299, Global test accuracy: 15.33
Round  72, Train loss: 2.297, Test loss: 2.298, Test accuracy: 15.89
Round  72, Global train loss: 2.297, Global test loss: 2.299, Global test accuracy: 15.39
Round  73, Train loss: 2.298, Test loss: 2.298, Test accuracy: 15.95
Round  73, Global train loss: 2.298, Global test loss: 2.299, Global test accuracy: 15.62
Round  74, Train loss: 2.297, Test loss: 2.298, Test accuracy: 16.00
Round  74, Global train loss: 2.297, Global test loss: 2.299, Global test accuracy: 15.67
Round  75, Train loss: 2.299, Test loss: 2.298, Test accuracy: 16.10
Round  75, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 15.68
Round  76, Train loss: 2.296, Test loss: 2.298, Test accuracy: 16.16
Round  76, Global train loss: 2.296, Global test loss: 2.298, Global test accuracy: 15.76
Round  77, Train loss: 2.297, Test loss: 2.298, Test accuracy: 16.22
Round  77, Global train loss: 2.297, Global test loss: 2.298, Global test accuracy: 15.93
Round  78, Train loss: 2.299, Test loss: 2.298, Test accuracy: 16.32
Round  78, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 16.08
Round  79, Train loss: 2.297, Test loss: 2.298, Test accuracy: 16.48
Round  79, Global train loss: 2.297, Global test loss: 2.298, Global test accuracy: 16.17
Round  80, Train loss: 2.299, Test loss: 2.298, Test accuracy: 16.66
Round  80, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 16.25
Round  81, Train loss: 2.296, Test loss: 2.298, Test accuracy: 16.72
Round  81, Global train loss: 2.296, Global test loss: 2.298, Global test accuracy: 16.27
Round  82, Train loss: 2.298, Test loss: 2.298, Test accuracy: 16.68
Round  82, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 16.20
Round  83, Train loss: 2.300, Test loss: 2.298, Test accuracy: 16.70
Round  83, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 16.22
Round  84, Train loss: 2.298, Test loss: 2.298, Test accuracy: 16.77
Round  84, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 16.23
Round  85, Train loss: 2.297, Test loss: 2.298, Test accuracy: 16.95
Round  85, Global train loss: 2.297, Global test loss: 2.298, Global test accuracy: 16.29
Round  86, Train loss: 2.299, Test loss: 2.298, Test accuracy: 16.89
Round  86, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 16.31
Round  87, Train loss: 2.298, Test loss: 2.298, Test accuracy: 16.92
Round  87, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 16.40
Round  88, Train loss: 2.296, Test loss: 2.298, Test accuracy: 16.97
Round  88, Global train loss: 2.296, Global test loss: 2.298, Global test accuracy: 16.38
Round  89, Train loss: 2.297, Test loss: 2.298, Test accuracy: 17.00
Round  89, Global train loss: 2.297, Global test loss: 2.298, Global test accuracy: 16.32
Round  90, Train loss: 2.300, Test loss: 2.298, Test accuracy: 17.05
Round  90, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 16.41
Round  91, Train loss: 2.297, Test loss: 2.298, Test accuracy: 17.10
Round  91, Global train loss: 2.297, Global test loss: 2.298, Global test accuracy: 16.43
Round  92, Train loss: 2.297, Test loss: 2.298, Test accuracy: 17.18
Round  92, Global train loss: 2.297, Global test loss: 2.298, Global test accuracy: 16.49
Round  93, Train loss: 2.293, Test loss: 2.298, Test accuracy: 17.31
Round  93, Global train loss: 2.293, Global test loss: 2.298, Global test accuracy: 16.67
Round  94, Train loss: 2.296, Test loss: 2.298, Test accuracy: 17.36
Round  94, Global train loss: 2.296, Global test loss: 2.298, Global test accuracy: 16.84
Round  95, Train loss: 2.295, Test loss: 2.298, Test accuracy: 17.44
Round  95, Global train loss: 2.295, Global test loss: 2.298, Global test accuracy: 16.93
Round  96, Train loss: 2.297, Test loss: 2.298, Test accuracy: 17.54
Round  96, Global train loss: 2.297, Global test loss: 2.298, Global test accuracy: 17.02/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 2.296, Test loss: 2.297, Test accuracy: 17.71
Round  97, Global train loss: 2.296, Global test loss: 2.298, Global test accuracy: 17.14
Round  98, Train loss: 2.300, Test loss: 2.297, Test accuracy: 17.83
Round  98, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 17.41
Round  99, Train loss: 2.298, Test loss: 2.297, Test accuracy: 17.97
Round  99, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 17.57
Final Round, Train loss: 2.297, Test loss: 2.297, Test accuracy: 19.27
Final Round, Global train loss: 2.297, Global test loss: 2.298, Global test accuracy: 17.57
Average accuracy final 10 rounds: 17.448333333333334 

Average global accuracy final 10 rounds: 16.892500000000002 

1127.7142882347107
[1.0393145084381104, 1.9636595249176025, 2.9093940258026123, 3.8502633571624756, 4.791484117507935, 5.719420433044434, 6.645466566085815, 7.586394309997559, 8.525017976760864, 9.462568044662476, 10.409172058105469, 11.352393865585327, 12.281064748764038, 13.226642847061157, 14.165602207183838, 15.107703685760498, 16.050720691680908, 16.979432582855225, 17.92113471031189, 18.8607759475708, 19.794953107833862, 20.719200372695923, 21.631896018981934, 22.561113595962524, 23.513811588287354, 24.465545892715454, 25.401318550109863, 26.325696229934692, 27.266849517822266, 28.2064266204834, 29.143741607666016, 30.070781469345093, 30.98567533493042, 31.920823574066162, 32.856369495391846, 33.79216527938843, 34.729294300079346, 35.66143298149109, 36.57727026939392, 37.506595849990845, 38.42935061454773, 39.368133783340454, 40.29538059234619, 41.20874285697937, 42.13391923904419, 43.066848039627075, 43.99827551841736, 44.9347620010376, 45.85265898704529, 46.7838077545166, 47.72591590881348, 48.65110230445862, 49.586912631988525, 50.51245069503784, 51.43438482284546, 52.36634182929993, 53.291043758392334, 54.22684073448181, 55.15093946456909, 56.066572189331055, 57.008586168289185, 57.9459023475647, 58.87492871284485, 59.811086654663086, 60.72944235801697, 61.65333414077759, 62.58893537521362, 63.514070987701416, 64.44580149650574, 65.37068724632263, 66.29350733757019, 67.22205948829651, 68.1498019695282, 69.0868468284607, 70.02254319190979, 70.94482254981995, 71.87253880500793, 72.80481791496277, 73.75813746452332, 74.70626330375671, 75.61928606033325, 76.54322361946106, 77.48007321357727, 78.40712475776672, 79.34107279777527, 80.2595648765564, 81.18356251716614, 82.11374068260193, 83.03767895698547, 83.97043061256409, 84.89145970344543, 85.81895017623901, 86.7482533454895, 87.68651580810547, 88.62049055099487, 89.59424138069153, 90.54281568527222, 91.48127675056458, 92.4219856262207, 93.34995865821838, 94.90152955055237]
[12.625, 13.025, 13.066666666666666, 13.316666666666666, 13.525, 13.691666666666666, 13.683333333333334, 13.783333333333333, 13.933333333333334, 14.058333333333334, 14.025, 14.116666666666667, 14.183333333333334, 14.091666666666667, 14.291666666666666, 14.358333333333333, 14.441666666666666, 14.658333333333333, 14.6, 14.7, 15.058333333333334, 15.258333333333333, 15.25, 15.4, 15.333333333333334, 15.416666666666666, 15.366666666666667, 15.4, 15.45, 15.583333333333334, 15.691666666666666, 15.35, 15.333333333333334, 15.283333333333333, 15.25, 15.316666666666666, 15.383333333333333, 15.391666666666667, 15.566666666666666, 15.708333333333334, 15.741666666666667, 15.683333333333334, 15.716666666666667, 15.716666666666667, 15.875, 15.883333333333333, 15.9, 16.008333333333333, 16.016666666666666, 16.1, 16.116666666666667, 16.116666666666667, 16.1, 16.083333333333332, 16.108333333333334, 15.85, 15.941666666666666, 15.733333333333333, 15.666666666666666, 15.766666666666667, 15.883333333333333, 15.941666666666666, 15.95, 15.95, 15.908333333333333, 15.933333333333334, 15.991666666666667, 15.925, 15.825, 15.808333333333334, 15.816666666666666, 15.825, 15.891666666666667, 15.95, 16.0, 16.1, 16.158333333333335, 16.216666666666665, 16.316666666666666, 16.475, 16.658333333333335, 16.716666666666665, 16.675, 16.7, 16.775, 16.95, 16.891666666666666, 16.916666666666668, 16.966666666666665, 17.0, 17.05, 17.1, 17.175, 17.308333333333334, 17.358333333333334, 17.441666666666666, 17.541666666666668, 17.708333333333332, 17.833333333333332, 17.966666666666665, 19.266666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.290, Test loss: 2.259, Test accuracy: 22.68
Round   1, Train loss: 2.028, Test loss: 2.196, Test accuracy: 34.24
Round   2, Train loss: 1.730, Test loss: 2.046, Test accuracy: 57.98
Round   3, Train loss: 1.570, Test loss: 1.934, Test accuracy: 56.90
Round   4, Train loss: 1.603, Test loss: 1.928, Test accuracy: 54.02
Round   5, Train loss: 1.550, Test loss: 1.832, Test accuracy: 67.21
Round   6, Train loss: 1.554, Test loss: 1.910, Test accuracy: 54.46
Round   7, Train loss: 1.546, Test loss: 1.835, Test accuracy: 64.51
Round   8, Train loss: 1.521, Test loss: 1.793, Test accuracy: 69.01
Round   9, Train loss: 1.521, Test loss: 1.775, Test accuracy: 70.88
Round  10, Train loss: 1.490, Test loss: 1.796, Test accuracy: 65.99
Round  11, Train loss: 1.503, Test loss: 1.741, Test accuracy: 74.28
Round  12, Train loss: 1.501, Test loss: 1.709, Test accuracy: 76.94
Round  13, Train loss: 1.502, Test loss: 1.711, Test accuracy: 77.37
Round  14, Train loss: 1.504, Test loss: 1.691, Test accuracy: 81.34
Round  15, Train loss: 1.502, Test loss: 1.716, Test accuracy: 76.30
Round  16, Train loss: 1.499, Test loss: 1.726, Test accuracy: 76.11
Round  17, Train loss: 1.495, Test loss: 1.701, Test accuracy: 78.96
Round  18, Train loss: 1.497, Test loss: 1.661, Test accuracy: 83.14
Round  19, Train loss: 1.493, Test loss: 1.676, Test accuracy: 80.85
Round  20, Train loss: 1.492, Test loss: 1.695, Test accuracy: 77.15
Round  21, Train loss: 1.483, Test loss: 1.786, Test accuracy: 67.25
Round  22, Train loss: 1.490, Test loss: 1.673, Test accuracy: 81.62
Round  23, Train loss: 1.491, Test loss: 1.701, Test accuracy: 78.62
Round  24, Train loss: 1.487, Test loss: 1.675, Test accuracy: 80.37
Round  25, Train loss: 1.481, Test loss: 1.694, Test accuracy: 75.89
Round  26, Train loss: 1.498, Test loss: 1.680, Test accuracy: 79.93
Round  27, Train loss: 1.488, Test loss: 1.713, Test accuracy: 75.58
Round  28, Train loss: 1.490, Test loss: 1.664, Test accuracy: 79.95
Round  29, Train loss: 1.492, Test loss: 1.652, Test accuracy: 82.58
Round  30, Train loss: 1.489, Test loss: 1.665, Test accuracy: 81.20
Round  31, Train loss: 1.479, Test loss: 1.667, Test accuracy: 79.54
Round  32, Train loss: 1.482, Test loss: 1.650, Test accuracy: 82.80
Round  33, Train loss: 1.482, Test loss: 1.645, Test accuracy: 82.68
Round  34, Train loss: 1.480, Test loss: 1.694, Test accuracy: 75.90
Round  35, Train loss: 1.478, Test loss: 1.625, Test accuracy: 85.35
Round  36, Train loss: 1.492, Test loss: 1.635, Test accuracy: 85.38
Round  37, Train loss: 1.479, Test loss: 1.682, Test accuracy: 77.77
Round  38, Train loss: 1.473, Test loss: 1.680, Test accuracy: 78.11
Round  39, Train loss: 1.479, Test loss: 1.612, Test accuracy: 86.92
Round  40, Train loss: 1.477, Test loss: 1.607, Test accuracy: 87.52
Round  41, Train loss: 1.473, Test loss: 1.646, Test accuracy: 83.22
Round  42, Train loss: 1.478, Test loss: 1.668, Test accuracy: 79.73
Round  43, Train loss: 1.482, Test loss: 1.617, Test accuracy: 85.78
Round  44, Train loss: 1.482, Test loss: 1.634, Test accuracy: 83.77
Round  45, Train loss: 1.480, Test loss: 1.681, Test accuracy: 78.47
Round  46, Train loss: 1.479, Test loss: 1.600, Test accuracy: 88.38
Round  47, Train loss: 1.478, Test loss: 1.611, Test accuracy: 86.34
Round  48, Train loss: 1.483, Test loss: 1.623, Test accuracy: 84.88
Round  49, Train loss: 1.474, Test loss: 1.612, Test accuracy: 86.53
Round  50, Train loss: 1.475, Test loss: 1.597, Test accuracy: 88.36
Round  51, Train loss: 1.473, Test loss: 1.630, Test accuracy: 84.59
Round  52, Train loss: 1.477, Test loss: 1.637, Test accuracy: 83.68
Round  53, Train loss: 1.476, Test loss: 1.632, Test accuracy: 83.83
Round  54, Train loss: 1.478, Test loss: 1.635, Test accuracy: 82.98
Round  55, Train loss: 1.482, Test loss: 1.642, Test accuracy: 82.53
Round  56, Train loss: 1.480, Test loss: 1.596, Test accuracy: 87.81
Round  57, Train loss: 1.475, Test loss: 1.594, Test accuracy: 88.24
Round  58, Train loss: 1.477, Test loss: 1.603, Test accuracy: 86.79
Round  59, Train loss: 1.471, Test loss: 1.606, Test accuracy: 87.21
Round  60, Train loss: 1.476, Test loss: 1.601, Test accuracy: 87.40
Round  61, Train loss: 1.472, Test loss: 1.610, Test accuracy: 87.01
Round  62, Train loss: 1.473, Test loss: 1.621, Test accuracy: 84.68
Round  63, Train loss: 1.472, Test loss: 1.625, Test accuracy: 84.96
Round  64, Train loss: 1.470, Test loss: 1.630, Test accuracy: 84.28
Round  65, Train loss: 1.473, Test loss: 1.580, Test accuracy: 89.57
Round  66, Train loss: 1.472, Test loss: 1.593, Test accuracy: 88.39
Round  67, Train loss: 1.471, Test loss: 1.588, Test accuracy: 88.73
Round  68, Train loss: 1.468, Test loss: 1.608, Test accuracy: 86.28
Round  69, Train loss: 1.475, Test loss: 1.645, Test accuracy: 81.95
Round  70, Train loss: 1.476, Test loss: 1.599, Test accuracy: 87.86
Round  71, Train loss: 1.471, Test loss: 1.590, Test accuracy: 88.38
Round  72, Train loss: 1.469, Test loss: 1.611, Test accuracy: 86.66
Round  73, Train loss: 1.470, Test loss: 1.624, Test accuracy: 84.27
Round  74, Train loss: 1.471, Test loss: 1.586, Test accuracy: 89.24
Round  75, Train loss: 1.473, Test loss: 1.612, Test accuracy: 85.67
Round  76, Train loss: 1.471, Test loss: 1.599, Test accuracy: 86.88
Round  77, Train loss: 1.474, Test loss: 1.597, Test accuracy: 87.85
Round  78, Train loss: 1.469, Test loss: 1.641, Test accuracy: 82.56
Round  79, Train loss: 1.472, Test loss: 1.593, Test accuracy: 87.95
Round  80, Train loss: 1.470, Test loss: 1.603, Test accuracy: 87.59
Round  81, Train loss: 1.471, Test loss: 1.583, Test accuracy: 88.91
Round  82, Train loss: 1.471, Test loss: 1.627, Test accuracy: 83.61
Round  83, Train loss: 1.468, Test loss: 1.610, Test accuracy: 85.38
Round  84, Train loss: 1.469, Test loss: 1.597, Test accuracy: 87.32
Round  85, Train loss: 1.468, Test loss: 1.579, Test accuracy: 89.05
Round  86, Train loss: 1.470, Test loss: 1.622, Test accuracy: 84.39
Round  87, Train loss: 1.468, Test loss: 1.599, Test accuracy: 87.48
Round  88, Train loss: 1.470, Test loss: 1.581, Test accuracy: 89.13
Round  89, Train loss: 1.469, Test loss: 1.577, Test accuracy: 89.30
Round  90, Train loss: 1.470, Test loss: 1.578, Test accuracy: 89.86
Round  91, Train loss: 1.471, Test loss: 1.593, Test accuracy: 88.03
Round  92, Train loss: 1.469, Test loss: 1.582, Test accuracy: 89.43
Round  93, Train loss: 1.470, Test loss: 1.584, Test accuracy: 89.15
Round  94, Train loss: 1.471, Test loss: 1.628, Test accuracy: 83.93
Round  95, Train loss: 1.469, Test loss: 1.598, Test accuracy: 87.32
Round  96, Train loss: 1.468, Test loss: 1.578, Test accuracy: 89.49
Round  97, Train loss: 1.468, Test loss: 1.584, Test accuracy: 89.02
Round  98, Train loss: 1.469, Test loss: 1.609, Test accuracy: 86.58
Round  99, Train loss: 1.468, Test loss: 1.592, Test accuracy: 88.26
Final Round, Train loss: 1.469, Test loss: 1.562, Test accuracy: 91.30
Average accuracy final 10 rounds: 88.10666666666668
1704.812560081482
[2.5730667114257812, 4.93305516242981, 7.310796737670898, 9.756985902786255, 12.124573707580566, 14.542593002319336, 16.91655158996582, 19.2270450592041, 21.591009378433228, 23.927876472473145, 26.30225110054016, 28.660863399505615, 30.99216318130493, 33.36540246009827, 35.690083742141724, 38.067243337631226, 40.40700554847717, 42.74932789802551, 45.12687540054321, 47.453075647354126, 49.81012773513794, 52.144253730773926, 54.4794762134552, 56.86308789253235, 59.22346377372742, 61.62445259094238, 63.9927339553833, 66.37735867500305, 68.74867248535156, 71.13811802864075, 73.51261234283447, 75.8883843421936, 78.28358340263367, 80.64798855781555, 83.04851937294006, 85.43825197219849, 87.80319094657898, 90.1726450920105, 92.5249559879303, 94.91361570358276, 97.2598295211792, 99.63349223136902, 102.00741767883301, 104.35341358184814, 106.71585321426392, 109.08682441711426, 111.47348308563232, 113.84774875640869, 116.21936702728271, 118.60411262512207, 120.94614386558533, 123.31832194328308, 125.69570684432983, 128.06838726997375, 130.4446005821228, 132.81373858451843, 135.20162320137024, 137.55287647247314, 139.9392647743225, 142.3214671611786, 144.68667650222778, 147.0858383178711, 149.45663809776306, 151.83349227905273, 154.16905426979065, 156.5049924850464, 158.86445450782776, 161.20333409309387, 163.5588595867157, 165.871728181839, 168.21256947517395, 170.58102464675903, 172.94207572937012, 175.2831699848175, 177.63028693199158, 179.9879846572876, 182.3319194316864, 184.70515060424805, 187.051025390625, 189.37312126159668, 191.73621797561646, 194.06061005592346, 196.4245548248291, 198.75344109535217, 201.11730980873108, 203.4925661087036, 205.8160274028778, 208.16091513633728, 210.48740315437317, 212.82136130332947, 215.17536854743958, 217.5250744819641, 219.88240671157837, 222.2298309803009, 224.61397862434387, 226.98432278633118, 229.36966466903687, 231.73093032836914, 234.1089596748352, 236.49406790733337, 238.49656057357788]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[22.683333333333334, 34.24166666666667, 57.983333333333334, 56.9, 54.025, 67.20833333333333, 54.458333333333336, 64.50833333333334, 69.00833333333334, 70.88333333333334, 65.99166666666666, 74.28333333333333, 76.94166666666666, 77.36666666666666, 81.34166666666667, 76.3, 76.10833333333333, 78.95833333333333, 83.14166666666667, 80.85, 77.15, 67.25, 81.625, 78.61666666666666, 80.36666666666666, 75.89166666666667, 79.93333333333334, 75.575, 79.95, 82.58333333333333, 81.2, 79.54166666666667, 82.8, 82.68333333333334, 75.9, 85.35, 85.38333333333334, 77.76666666666667, 78.10833333333333, 86.925, 87.51666666666667, 83.225, 79.73333333333333, 85.78333333333333, 83.76666666666667, 78.46666666666667, 88.375, 86.34166666666667, 84.875, 86.53333333333333, 88.35833333333333, 84.59166666666667, 83.68333333333334, 83.83333333333333, 82.98333333333333, 82.525, 87.80833333333334, 88.24166666666666, 86.79166666666667, 87.20833333333333, 87.4, 87.00833333333334, 84.68333333333334, 84.95833333333333, 84.275, 89.56666666666666, 88.39166666666667, 88.73333333333333, 86.275, 81.95, 87.85833333333333, 88.38333333333334, 86.65833333333333, 84.26666666666667, 89.24166666666666, 85.66666666666667, 86.88333333333334, 87.85, 82.55833333333334, 87.95, 87.59166666666667, 88.90833333333333, 83.60833333333333, 85.38333333333334, 87.31666666666666, 89.05, 84.39166666666667, 87.48333333333333, 89.13333333333334, 89.3, 89.85833333333333, 88.03333333333333, 89.43333333333334, 89.15, 83.93333333333334, 87.31666666666666, 89.49166666666666, 89.01666666666667, 86.575, 88.25833333333334, 91.3]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.279, Test loss: 2.273, Test accuracy: 16.55
Round   1, Train loss: 2.067, Test loss: 2.161, Test accuracy: 41.06
Round   2, Train loss: 1.889, Test loss: 2.023, Test accuracy: 48.31
Round   3, Train loss: 1.901, Test loss: 1.988, Test accuracy: 50.88
Round   4, Train loss: 1.800, Test loss: 1.908, Test accuracy: 58.70
Round   5, Train loss: 1.675, Test loss: 1.827, Test accuracy: 65.01
Round   6, Train loss: 1.697, Test loss: 1.780, Test accuracy: 71.29
Round   7, Train loss: 1.684, Test loss: 1.696, Test accuracy: 82.17
Round   8, Train loss: 1.591, Test loss: 1.635, Test accuracy: 88.35
Round   9, Train loss: 1.571, Test loss: 1.594, Test accuracy: 90.38
Round  10, Train loss: 1.551, Test loss: 1.577, Test accuracy: 91.96
Round  11, Train loss: 1.545, Test loss: 1.563, Test accuracy: 93.77
Round  12, Train loss: 1.537, Test loss: 1.550, Test accuracy: 95.05
Round  13, Train loss: 1.508, Test loss: 1.551, Test accuracy: 94.92
Round  14, Train loss: 1.536, Test loss: 1.538, Test accuracy: 95.31
Round  15, Train loss: 1.512, Test loss: 1.539, Test accuracy: 95.13
Round  16, Train loss: 1.511, Test loss: 1.536, Test accuracy: 95.39
Round  17, Train loss: 1.515, Test loss: 1.535, Test accuracy: 95.46
Round  18, Train loss: 1.531, Test loss: 1.525, Test accuracy: 96.16
Round  19, Train loss: 1.519, Test loss: 1.521, Test accuracy: 96.34
Round  20, Train loss: 1.505, Test loss: 1.518, Test accuracy: 96.50
Round  21, Train loss: 1.504, Test loss: 1.518, Test accuracy: 96.33
Round  22, Train loss: 1.514, Test loss: 1.515, Test accuracy: 96.62
Round  23, Train loss: 1.512, Test loss: 1.515, Test accuracy: 96.47
Round  24, Train loss: 1.493, Test loss: 1.514, Test accuracy: 96.43
Round  25, Train loss: 1.501, Test loss: 1.512, Test accuracy: 96.62
Round  26, Train loss: 1.492, Test loss: 1.510, Test accuracy: 96.83
Round  27, Train loss: 1.501, Test loss: 1.510, Test accuracy: 96.83
Round  28, Train loss: 1.493, Test loss: 1.508, Test accuracy: 96.91
Round  29, Train loss: 1.501, Test loss: 1.506, Test accuracy: 97.03
Round  30, Train loss: 1.496, Test loss: 1.507, Test accuracy: 97.03
Round  31, Train loss: 1.494, Test loss: 1.505, Test accuracy: 97.16
Round  32, Train loss: 1.492, Test loss: 1.506, Test accuracy: 97.03
Round  33, Train loss: 1.492, Test loss: 1.503, Test accuracy: 97.23
Round  34, Train loss: 1.488, Test loss: 1.504, Test accuracy: 97.16
Round  35, Train loss: 1.491, Test loss: 1.502, Test accuracy: 97.28
Round  36, Train loss: 1.492, Test loss: 1.502, Test accuracy: 97.17
Round  37, Train loss: 1.485, Test loss: 1.502, Test accuracy: 97.17
Round  38, Train loss: 1.482, Test loss: 1.503, Test accuracy: 97.10
Round  39, Train loss: 1.481, Test loss: 1.501, Test accuracy: 97.22
Round  40, Train loss: 1.481, Test loss: 1.502, Test accuracy: 97.27
Round  41, Train loss: 1.485, Test loss: 1.500, Test accuracy: 97.40
Round  42, Train loss: 1.487, Test loss: 1.500, Test accuracy: 97.39
Round  43, Train loss: 1.482, Test loss: 1.499, Test accuracy: 97.40
Round  44, Train loss: 1.484, Test loss: 1.499, Test accuracy: 97.33
Round  45, Train loss: 1.482, Test loss: 1.499, Test accuracy: 97.38
Round  46, Train loss: 1.480, Test loss: 1.499, Test accuracy: 97.45
Round  47, Train loss: 1.479, Test loss: 1.499, Test accuracy: 97.38
Round  48, Train loss: 1.483, Test loss: 1.500, Test accuracy: 97.21
Round  49, Train loss: 1.477, Test loss: 1.498, Test accuracy: 97.25
Round  50, Train loss: 1.479, Test loss: 1.496, Test accuracy: 97.46
Round  51, Train loss: 1.478, Test loss: 1.498, Test accuracy: 97.35
Round  52, Train loss: 1.476, Test loss: 1.497, Test accuracy: 97.43
Round  53, Train loss: 1.480, Test loss: 1.496, Test accuracy: 97.53
Round  54, Train loss: 1.477, Test loss: 1.496, Test accuracy: 97.56
Round  55, Train loss: 1.477, Test loss: 1.496, Test accuracy: 97.54
Round  56, Train loss: 1.477, Test loss: 1.495, Test accuracy: 97.60
Round  57, Train loss: 1.474, Test loss: 1.496, Test accuracy: 97.43
Round  58, Train loss: 1.476, Test loss: 1.497, Test accuracy: 97.46
Round  59, Train loss: 1.476, Test loss: 1.496, Test accuracy: 97.48
Round  60, Train loss: 1.479, Test loss: 1.496, Test accuracy: 97.45
Round  61, Train loss: 1.476, Test loss: 1.495, Test accuracy: 97.64
Round  62, Train loss: 1.476, Test loss: 1.496, Test accuracy: 97.57
Round  63, Train loss: 1.476, Test loss: 1.497, Test accuracy: 97.46
Round  64, Train loss: 1.473, Test loss: 1.496, Test accuracy: 97.53
Round  65, Train loss: 1.474, Test loss: 1.497, Test accuracy: 97.42
Round  66, Train loss: 1.474, Test loss: 1.494, Test accuracy: 97.55
Round  67, Train loss: 1.475, Test loss: 1.495, Test accuracy: 97.62
Round  68, Train loss: 1.473, Test loss: 1.495, Test accuracy: 97.60
Round  69, Train loss: 1.474, Test loss: 1.493, Test accuracy: 97.71
Round  70, Train loss: 1.475, Test loss: 1.492, Test accuracy: 97.83
Round  71, Train loss: 1.472, Test loss: 1.492, Test accuracy: 97.70
Round  72, Train loss: 1.474, Test loss: 1.492, Test accuracy: 97.73
Round  73, Train loss: 1.473, Test loss: 1.492, Test accuracy: 97.74
Round  74, Train loss: 1.473, Test loss: 1.492, Test accuracy: 97.82
Round  75, Train loss: 1.472, Test loss: 1.492, Test accuracy: 97.80
Round  76, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.74
Round  77, Train loss: 1.471, Test loss: 1.492, Test accuracy: 97.72
Round  78, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.83
Round  79, Train loss: 1.471, Test loss: 1.491, Test accuracy: 97.78
Round  80, Train loss: 1.472, Test loss: 1.492, Test accuracy: 97.71
Round  81, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.84
Round  82, Train loss: 1.473, Test loss: 1.490, Test accuracy: 97.92
Round  83, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.97
Round  84, Train loss: 1.471, Test loss: 1.491, Test accuracy: 97.87
Round  85, Train loss: 1.471, Test loss: 1.492, Test accuracy: 97.77
Round  86, Train loss: 1.471, Test loss: 1.491, Test accuracy: 97.83
Round  87, Train loss: 1.469, Test loss: 1.491, Test accuracy: 97.80
Round  88, Train loss: 1.468, Test loss: 1.491, Test accuracy: 97.83
Round  89, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.88
Round  90, Train loss: 1.470, Test loss: 1.491, Test accuracy: 97.83
Round  91, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.84
Round  92, Train loss: 1.469, Test loss: 1.489, Test accuracy: 97.89
Round  93, Train loss: 1.469, Test loss: 1.490, Test accuracy: 97.88
Round  94, Train loss: 1.469, Test loss: 1.490, Test accuracy: 97.88/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.471, Test loss: 1.489, Test accuracy: 97.92
Round  96, Train loss: 1.469, Test loss: 1.490, Test accuracy: 97.92
Round  97, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.83
Round  98, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.85
Round  99, Train loss: 1.467, Test loss: 1.489, Test accuracy: 97.88
Final Round, Train loss: 1.468, Test loss: 1.489, Test accuracy: 97.88
Average accuracy final 10 rounds: 97.8725
878.2038576602936
[1.197892665863037, 2.1890170574188232, 3.1788811683654785, 4.1831958293914795, 5.181955575942993, 6.164528131484985, 7.161321640014648, 8.16394853591919, 9.152389764785767, 10.152893304824829, 11.15787386894226, 12.153582572937012, 13.140133619308472, 14.1366446018219, 15.15027379989624, 16.13580584526062, 17.131397247314453, 18.128859758377075, 19.11941170692444, 20.12480139732361, 21.121338605880737, 22.118783950805664, 23.108309507369995, 24.107478618621826, 25.115593194961548, 26.103230953216553, 27.093584299087524, 28.092252254486084, 29.093453645706177, 30.093437671661377, 31.090888500213623, 32.085203647613525, 33.074172496795654, 34.06424403190613, 35.062857151031494, 36.058247566223145, 37.06001615524292, 38.05505657196045, 39.047282695770264, 40.04506468772888, 41.038410902023315, 42.03538203239441, 43.02320885658264, 44.00729179382324, 44.99800109863281, 45.99098348617554, 46.97611045837402, 47.971134424209595, 48.96594262123108, 49.94946312904358, 50.94159269332886, 51.93969798088074, 52.931310176849365, 53.91587257385254, 54.91033148765564, 55.89946150779724, 56.8919939994812, 57.87936568260193, 58.87140083312988, 59.86248993873596, 60.856719970703125, 61.8550488948822, 62.842546701431274, 63.82862329483032, 64.82404136657715, 65.81591963768005, 66.80641102790833, 67.80336594581604, 68.79640698432922, 69.79889273643494, 70.78577899932861, 71.77734041213989, 72.77340459823608, 73.76119375228882, 74.75231862068176, 75.77233982086182, 76.76363277435303, 77.76521921157837, 78.761403799057, 79.75417375564575, 80.75051975250244, 81.74344992637634, 82.74033904075623, 83.73418593406677, 84.73625493049622, 85.73294830322266, 86.72884750366211, 87.73172998428345, 88.73315858840942, 89.73672676086426, 90.73344564437866, 91.73449110984802, 92.74065494537354, 93.7437527179718, 94.75034356117249, 95.75022482872009, 96.73954892158508, 97.74428534507751, 98.7567093372345, 99.75339198112488, 101.10042381286621]
[16.55, 41.05833333333333, 48.30833333333333, 50.88333333333333, 58.7, 65.00833333333334, 71.29166666666667, 82.16666666666667, 88.35, 90.38333333333334, 91.95833333333333, 93.76666666666667, 95.05, 94.925, 95.30833333333334, 95.13333333333334, 95.39166666666667, 95.45833333333333, 96.15833333333333, 96.34166666666667, 96.5, 96.325, 96.61666666666666, 96.46666666666667, 96.43333333333334, 96.625, 96.83333333333333, 96.825, 96.90833333333333, 97.025, 97.025, 97.15833333333333, 97.03333333333333, 97.23333333333333, 97.15833333333333, 97.28333333333333, 97.175, 97.175, 97.1, 97.225, 97.26666666666667, 97.4, 97.39166666666667, 97.4, 97.33333333333333, 97.38333333333334, 97.45, 97.375, 97.20833333333333, 97.25, 97.45833333333333, 97.35, 97.43333333333334, 97.53333333333333, 97.55833333333334, 97.54166666666667, 97.6, 97.43333333333334, 97.45833333333333, 97.48333333333333, 97.45, 97.64166666666667, 97.56666666666666, 97.45833333333333, 97.53333333333333, 97.41666666666667, 97.55, 97.625, 97.6, 97.70833333333333, 97.83333333333333, 97.7, 97.73333333333333, 97.74166666666666, 97.81666666666666, 97.8, 97.74166666666666, 97.725, 97.83333333333333, 97.775, 97.70833333333333, 97.84166666666667, 97.91666666666667, 97.975, 97.86666666666666, 97.76666666666667, 97.825, 97.8, 97.83333333333333, 97.875, 97.83333333333333, 97.84166666666667, 97.89166666666667, 97.875, 97.875, 97.925, 97.91666666666667, 97.83333333333333, 97.85, 97.88333333333334, 97.88333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.291, Test loss: 2.284, Test accuracy: 18.57
Round   1, Train loss: 2.119, Test loss: 2.190, Test accuracy: 41.45
Round   2, Train loss: 1.946, Test loss: 2.089, Test accuracy: 46.36
Round   3, Train loss: 1.847, Test loss: 1.991, Test accuracy: 57.17
Round   4, Train loss: 1.792, Test loss: 1.896, Test accuracy: 69.57
Round   5, Train loss: 1.672, Test loss: 1.782, Test accuracy: 76.51
Round   6, Train loss: 1.639, Test loss: 1.729, Test accuracy: 81.26
Round   7, Train loss: 1.644, Test loss: 1.681, Test accuracy: 84.93
Round   8, Train loss: 1.606, Test loss: 1.661, Test accuracy: 88.10
Round   9, Train loss: 1.601, Test loss: 1.630, Test accuracy: 89.78
Round  10, Train loss: 1.573, Test loss: 1.597, Test accuracy: 92.68
Round  11, Train loss: 1.559, Test loss: 1.596, Test accuracy: 92.55
Round  12, Train loss: 1.562, Test loss: 1.589, Test accuracy: 92.57
Round  13, Train loss: 1.542, Test loss: 1.584, Test accuracy: 92.47
Round  14, Train loss: 1.540, Test loss: 1.588, Test accuracy: 92.49
Round  15, Train loss: 1.551, Test loss: 1.561, Test accuracy: 94.97
Round  16, Train loss: 1.550, Test loss: 1.554, Test accuracy: 95.25
Round  17, Train loss: 1.531, Test loss: 1.550, Test accuracy: 95.47
Round  18, Train loss: 1.532, Test loss: 1.550, Test accuracy: 95.64
Round  19, Train loss: 1.554, Test loss: 1.538, Test accuracy: 96.73
Round  20, Train loss: 1.516, Test loss: 1.540, Test accuracy: 96.67
Round  21, Train loss: 1.527, Test loss: 1.536, Test accuracy: 96.67
Round  22, Train loss: 1.517, Test loss: 1.535, Test accuracy: 96.62
Round  23, Train loss: 1.523, Test loss: 1.534, Test accuracy: 96.71
Round  24, Train loss: 1.515, Test loss: 1.533, Test accuracy: 96.81
Round  25, Train loss: 1.531, Test loss: 1.528, Test accuracy: 97.03
Round  26, Train loss: 1.530, Test loss: 1.523, Test accuracy: 97.12
Round  27, Train loss: 1.521, Test loss: 1.524, Test accuracy: 97.14
Round  28, Train loss: 1.519, Test loss: 1.521, Test accuracy: 97.17
Round  29, Train loss: 1.516, Test loss: 1.520, Test accuracy: 97.22
Round  30, Train loss: 1.510, Test loss: 1.519, Test accuracy: 97.29
Round  31, Train loss: 1.514, Test loss: 1.516, Test accuracy: 97.35
Round  32, Train loss: 1.505, Test loss: 1.517, Test accuracy: 97.49
Round  33, Train loss: 1.510, Test loss: 1.515, Test accuracy: 97.52
Round  34, Train loss: 1.506, Test loss: 1.514, Test accuracy: 97.60
Round  35, Train loss: 1.502, Test loss: 1.514, Test accuracy: 97.62
Round  36, Train loss: 1.505, Test loss: 1.512, Test accuracy: 97.54
Round  37, Train loss: 1.504, Test loss: 1.512, Test accuracy: 97.59
Round  38, Train loss: 1.504, Test loss: 1.511, Test accuracy: 97.72
Round  39, Train loss: 1.498, Test loss: 1.511, Test accuracy: 97.71
Round  40, Train loss: 1.504, Test loss: 1.509, Test accuracy: 97.77
Round  41, Train loss: 1.501, Test loss: 1.509, Test accuracy: 97.72
Round  42, Train loss: 1.496, Test loss: 1.508, Test accuracy: 97.88
Round  43, Train loss: 1.498, Test loss: 1.509, Test accuracy: 97.80
Round  44, Train loss: 1.498, Test loss: 1.507, Test accuracy: 97.87
Round  45, Train loss: 1.497, Test loss: 1.507, Test accuracy: 97.87
Round  46, Train loss: 1.495, Test loss: 1.508, Test accuracy: 97.78
Round  47, Train loss: 1.499, Test loss: 1.507, Test accuracy: 97.72
Round  48, Train loss: 1.493, Test loss: 1.507, Test accuracy: 97.72
Round  49, Train loss: 1.493, Test loss: 1.508, Test accuracy: 97.65
Round  50, Train loss: 1.496, Test loss: 1.505, Test accuracy: 97.88
Round  51, Train loss: 1.494, Test loss: 1.505, Test accuracy: 97.92
Round  52, Train loss: 1.493, Test loss: 1.505, Test accuracy: 97.88
Round  53, Train loss: 1.493, Test loss: 1.504, Test accuracy: 97.86
Round  54, Train loss: 1.494, Test loss: 1.504, Test accuracy: 97.87
Round  55, Train loss: 1.494, Test loss: 1.503, Test accuracy: 97.94
Round  56, Train loss: 1.488, Test loss: 1.504, Test accuracy: 97.92
Round  57, Train loss: 1.490, Test loss: 1.503, Test accuracy: 98.01
Round  58, Train loss: 1.493, Test loss: 1.502, Test accuracy: 98.08
Round  59, Train loss: 1.490, Test loss: 1.502, Test accuracy: 98.02
Round  60, Train loss: 1.490, Test loss: 1.501, Test accuracy: 98.00
Round  61, Train loss: 1.488, Test loss: 1.502, Test accuracy: 97.98
Round  62, Train loss: 1.488, Test loss: 1.504, Test accuracy: 97.99
Round  63, Train loss: 1.487, Test loss: 1.502, Test accuracy: 97.89
Round  64, Train loss: 1.491, Test loss: 1.500, Test accuracy: 98.12
Round  65, Train loss: 1.486, Test loss: 1.500, Test accuracy: 98.08
Round  66, Train loss: 1.487, Test loss: 1.500, Test accuracy: 98.05
Round  67, Train loss: 1.486, Test loss: 1.499, Test accuracy: 98.18
Round  68, Train loss: 1.486, Test loss: 1.499, Test accuracy: 98.05
Round  69, Train loss: 1.489, Test loss: 1.499, Test accuracy: 98.16
Round  70, Train loss: 1.486, Test loss: 1.501, Test accuracy: 98.04
Round  71, Train loss: 1.489, Test loss: 1.498, Test accuracy: 98.17
Round  72, Train loss: 1.488, Test loss: 1.498, Test accuracy: 98.23
Round  73, Train loss: 1.483, Test loss: 1.498, Test accuracy: 98.13
Round  74, Train loss: 1.487, Test loss: 1.498, Test accuracy: 98.14
Round  75, Train loss: 1.485, Test loss: 1.498, Test accuracy: 98.22
Round  76, Train loss: 1.486, Test loss: 1.498, Test accuracy: 98.11
Round  77, Train loss: 1.485, Test loss: 1.497, Test accuracy: 98.17
Round  78, Train loss: 1.485, Test loss: 1.498, Test accuracy: 98.06
Round  79, Train loss: 1.484, Test loss: 1.499, Test accuracy: 97.99
Round  80, Train loss: 1.483, Test loss: 1.499, Test accuracy: 98.05
Round  81, Train loss: 1.483, Test loss: 1.499, Test accuracy: 97.98
Round  82, Train loss: 1.485, Test loss: 1.499, Test accuracy: 98.01
Round  83, Train loss: 1.484, Test loss: 1.498, Test accuracy: 98.03
Round  84, Train loss: 1.483, Test loss: 1.498, Test accuracy: 98.03
Round  85, Train loss: 1.483, Test loss: 1.498, Test accuracy: 98.05
Round  86, Train loss: 1.483, Test loss: 1.499, Test accuracy: 98.02
Round  87, Train loss: 1.481, Test loss: 1.500, Test accuracy: 97.94
Round  88, Train loss: 1.482, Test loss: 1.500, Test accuracy: 97.92
Round  89, Train loss: 1.483, Test loss: 1.498, Test accuracy: 98.04
Round  90, Train loss: 1.483, Test loss: 1.498, Test accuracy: 98.03
Round  91, Train loss: 1.480, Test loss: 1.498, Test accuracy: 98.07
Round  92, Train loss: 1.481, Test loss: 1.498, Test accuracy: 98.01
Round  93, Train loss: 1.481, Test loss: 1.497, Test accuracy: 98.08/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.482, Test loss: 1.498, Test accuracy: 98.13
Round  95, Train loss: 1.481, Test loss: 1.499, Test accuracy: 98.00
Round  96, Train loss: 1.480, Test loss: 1.498, Test accuracy: 98.03
Round  97, Train loss: 1.480, Test loss: 1.498, Test accuracy: 98.04
Round  98, Train loss: 1.484, Test loss: 1.496, Test accuracy: 98.08
Round  99, Train loss: 1.480, Test loss: 1.496, Test accuracy: 98.15
Final Round, Train loss: 1.471, Test loss: 1.494, Test accuracy: 98.16
Average accuracy final 10 rounds: 98.0625
1163.535864830017
[1.189467191696167, 2.378934383392334, 3.425302267074585, 4.471670150756836, 5.506699085235596, 6.5417280197143555, 7.573110818862915, 8.604493618011475, 9.656821012496948, 10.709148406982422, 11.740997791290283, 12.772847175598145, 13.80819034576416, 14.843533515930176, 15.861616373062134, 16.879699230194092, 17.911943435668945, 18.9441876411438, 19.996038913726807, 21.047890186309814, 22.078969478607178, 23.11004877090454, 24.133766651153564, 25.157484531402588, 26.185224056243896, 27.212963581085205, 28.223285913467407, 29.23360824584961, 30.2322735786438, 31.23093891143799, 32.24289274215698, 33.25484657287598, 34.28857707977295, 35.32230758666992, 36.32631754875183, 37.33032751083374, 38.35050821304321, 39.370688915252686, 40.38607621192932, 41.40146350860596, 42.42017412185669, 43.43888473510742, 44.450945138931274, 45.46300554275513, 46.470388412475586, 47.477771282196045, 48.50361204147339, 49.52945280075073, 50.533665895462036, 51.53787899017334, 52.55021357536316, 53.56254816055298, 54.595909118652344, 55.62927007675171, 56.63962721824646, 57.64998435974121, 58.66743564605713, 59.68488693237305, 60.693910121917725, 61.7029333114624, 62.718459367752075, 63.73398542404175, 64.74266123771667, 65.7513370513916, 66.74603700637817, 67.74073696136475, 68.74894213676453, 69.7571473121643, 70.76941108703613, 71.78167486190796, 72.78367829322815, 73.78568172454834, 74.79503917694092, 75.8043966293335, 76.80525875091553, 77.80612087249756, 78.82864594459534, 79.85117101669312, 80.86667466163635, 81.88217830657959, 82.8921914100647, 83.9022045135498, 84.90970039367676, 85.91719627380371, 86.93420839309692, 87.95122051239014, 88.9607937335968, 89.97036695480347, 90.9907124042511, 92.01105785369873, 93.01742625236511, 94.0237946510315, 95.04488515853882, 96.06597566604614, 97.07525706291199, 98.08453845977783, 99.09703493118286, 100.10953140258789, 101.12303638458252, 102.13654136657715, 103.14023923873901, 104.14393711090088, 105.15543246269226, 106.16692781448364, 107.16482400894165, 108.16272020339966, 109.16970705986023, 110.1766939163208, 111.1815836429596, 112.18647336959839, 113.20587015151978, 114.22526693344116, 115.235360622406, 116.24545431137085, 117.25091814994812, 118.25638198852539, 119.26778101921082, 120.27918004989624, 121.31828212738037, 122.3573842048645, 123.3929054737091, 124.42842674255371, 125.47058057785034, 126.51273441314697, 127.533198595047, 128.55366277694702, 129.5766417980194, 130.5996208190918, 131.61324763298035, 132.6268744468689, 133.62802362442017, 134.62917280197144, 135.66783547401428, 136.70649814605713, 137.71848249435425, 138.73046684265137, 139.74751806259155, 140.76456928253174, 141.8001070022583, 142.83564472198486, 143.84308695793152, 144.85052919387817, 145.8659703731537, 146.8814115524292, 147.89776253700256, 148.91411352157593, 149.91150975227356, 150.9089059829712, 151.91654658317566, 152.92418718338013, 153.94044709205627, 154.95670700073242, 155.96694707870483, 156.97718715667725, 157.99795866012573, 159.01873016357422, 160.01213884353638, 161.00554752349854, 162.01573872566223, 163.02592992782593, 164.03672742843628, 165.04752492904663, 166.0476996898651, 167.0478744506836, 168.0624167919159, 169.0769591331482, 170.08247923851013, 171.08799934387207, 172.10784220695496, 173.12768507003784, 174.13169193267822, 175.1356987953186, 176.13585233688354, 177.1360058784485, 178.14623069763184, 179.15645551681519, 180.16236281394958, 181.16827011108398, 182.16396832466125, 183.15966653823853, 184.1632432937622, 185.1668200492859, 186.16827845573425, 187.16973686218262, 188.18116450309753, 189.19259214401245, 190.19680070877075, 191.20100927352905, 192.20511746406555, 193.20922565460205, 194.22396183013916, 195.23869800567627, 196.25244617462158, 197.2661943435669, 198.28027391433716, 199.29435348510742, 200.31310415267944, 201.33185482025146, 202.33440589904785, 203.33695697784424, 204.70301389694214, 206.06907081604004]
[18.566666666666666, 18.566666666666666, 41.45, 41.45, 46.358333333333334, 46.358333333333334, 57.166666666666664, 57.166666666666664, 69.56666666666666, 69.56666666666666, 76.50833333333334, 76.50833333333334, 81.25833333333334, 81.25833333333334, 84.93333333333334, 84.93333333333334, 88.1, 88.1, 89.78333333333333, 89.78333333333333, 92.68333333333334, 92.68333333333334, 92.55, 92.55, 92.56666666666666, 92.56666666666666, 92.475, 92.475, 92.49166666666666, 92.49166666666666, 94.975, 94.975, 95.25, 95.25, 95.475, 95.475, 95.64166666666667, 95.64166666666667, 96.73333333333333, 96.73333333333333, 96.675, 96.675, 96.66666666666667, 96.66666666666667, 96.61666666666666, 96.61666666666666, 96.70833333333333, 96.70833333333333, 96.80833333333334, 96.80833333333334, 97.025, 97.025, 97.125, 97.125, 97.14166666666667, 97.14166666666667, 97.16666666666667, 97.16666666666667, 97.225, 97.225, 97.29166666666667, 97.29166666666667, 97.35, 97.35, 97.49166666666666, 97.49166666666666, 97.51666666666667, 97.51666666666667, 97.6, 97.6, 97.625, 97.625, 97.54166666666667, 97.54166666666667, 97.59166666666667, 97.59166666666667, 97.71666666666667, 97.71666666666667, 97.70833333333333, 97.70833333333333, 97.76666666666667, 97.76666666666667, 97.71666666666667, 97.71666666666667, 97.875, 97.875, 97.8, 97.8, 97.86666666666666, 97.86666666666666, 97.86666666666666, 97.86666666666666, 97.78333333333333, 97.78333333333333, 97.71666666666667, 97.71666666666667, 97.725, 97.725, 97.65, 97.65, 97.88333333333334, 97.88333333333334, 97.91666666666667, 97.91666666666667, 97.875, 97.875, 97.85833333333333, 97.85833333333333, 97.86666666666666, 97.86666666666666, 97.94166666666666, 97.94166666666666, 97.925, 97.925, 98.00833333333334, 98.00833333333334, 98.08333333333333, 98.08333333333333, 98.01666666666667, 98.01666666666667, 98.0, 98.0, 97.98333333333333, 97.98333333333333, 97.99166666666666, 97.99166666666666, 97.89166666666667, 97.89166666666667, 98.11666666666666, 98.11666666666666, 98.075, 98.075, 98.05, 98.05, 98.18333333333334, 98.18333333333334, 98.05, 98.05, 98.15833333333333, 98.15833333333333, 98.04166666666667, 98.04166666666667, 98.16666666666667, 98.16666666666667, 98.23333333333333, 98.23333333333333, 98.13333333333334, 98.13333333333334, 98.14166666666667, 98.14166666666667, 98.21666666666667, 98.21666666666667, 98.10833333333333, 98.10833333333333, 98.16666666666667, 98.16666666666667, 98.05833333333334, 98.05833333333334, 97.99166666666666, 97.99166666666666, 98.05, 98.05, 97.98333333333333, 97.98333333333333, 98.00833333333334, 98.00833333333334, 98.03333333333333, 98.03333333333333, 98.03333333333333, 98.03333333333333, 98.05, 98.05, 98.01666666666667, 98.01666666666667, 97.94166666666666, 97.94166666666666, 97.91666666666667, 97.91666666666667, 98.04166666666667, 98.04166666666667, 98.03333333333333, 98.03333333333333, 98.06666666666666, 98.06666666666666, 98.00833333333334, 98.00833333333334, 98.075, 98.075, 98.13333333333334, 98.13333333333334, 98.0, 98.0, 98.03333333333333, 98.03333333333333, 98.04166666666667, 98.04166666666667, 98.08333333333333, 98.08333333333333, 98.15, 98.15, 98.15833333333333, 98.15833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.169, Test loss: 2.172, Test accuracy: 26.92
Round   0, Global train loss: 2.169, Global test loss: 2.296, Global test accuracy: 11.67
Round   1, Train loss: 1.802, Test loss: 2.042, Test accuracy: 41.17
Round   1, Global train loss: 1.802, Global test loss: 2.290, Global test accuracy: 12.01
Round   2, Train loss: 1.768, Test loss: 1.922, Test accuracy: 53.79
Round   2, Global train loss: 1.768, Global test loss: 2.299, Global test accuracy: 10.11
Round   3, Train loss: 1.578, Test loss: 1.819, Test accuracy: 65.14
Round   3, Global train loss: 1.578, Global test loss: 2.263, Global test accuracy: 18.99
Round   4, Train loss: 1.656, Test loss: 1.748, Test accuracy: 71.42
Round   4, Global train loss: 1.656, Global test loss: 2.280, Global test accuracy: 12.59
Round   5, Train loss: 1.587, Test loss: 1.678, Test accuracy: 78.42
Round   5, Global train loss: 1.587, Global test loss: 2.269, Global test accuracy: 15.85
Round   6, Train loss: 1.591, Test loss: 1.672, Test accuracy: 78.72
Round   6, Global train loss: 1.591, Global test loss: 2.282, Global test accuracy: 16.62
Round   7, Train loss: 1.582, Test loss: 1.673, Test accuracy: 78.47
Round   7, Global train loss: 1.582, Global test loss: 2.288, Global test accuracy: 11.58
Round   8, Train loss: 1.585, Test loss: 1.667, Test accuracy: 79.40
Round   8, Global train loss: 1.585, Global test loss: 2.278, Global test accuracy: 14.45
Round   9, Train loss: 1.639, Test loss: 1.659, Test accuracy: 80.33
Round   9, Global train loss: 1.639, Global test loss: 2.314, Global test accuracy: 8.88
Round  10, Train loss: 1.585, Test loss: 1.655, Test accuracy: 81.28
Round  10, Global train loss: 1.585, Global test loss: 2.273, Global test accuracy: 15.05
Round  11, Train loss: 1.475, Test loss: 1.657, Test accuracy: 80.33
Round  11, Global train loss: 1.475, Global test loss: 2.272, Global test accuracy: 17.57
Round  12, Train loss: 1.636, Test loss: 1.643, Test accuracy: 82.01
Round  12, Global train loss: 1.636, Global test loss: 2.270, Global test accuracy: 17.46
Round  13, Train loss: 1.526, Test loss: 1.643, Test accuracy: 82.00
Round  13, Global train loss: 1.526, Global test loss: 2.285, Global test accuracy: 14.62
Round  14, Train loss: 1.684, Test loss: 1.633, Test accuracy: 83.08
Round  14, Global train loss: 1.684, Global test loss: 2.271, Global test accuracy: 18.23
Round  15, Train loss: 1.575, Test loss: 1.633, Test accuracy: 83.10
Round  15, Global train loss: 1.575, Global test loss: 2.263, Global test accuracy: 19.18
Round  16, Train loss: 1.630, Test loss: 1.633, Test accuracy: 83.07
Round  16, Global train loss: 1.630, Global test loss: 2.286, Global test accuracy: 14.58
Round  17, Train loss: 1.599, Test loss: 1.619, Test accuracy: 84.57
Round  17, Global train loss: 1.599, Global test loss: 2.273, Global test accuracy: 15.96
Round  18, Train loss: 1.575, Test loss: 1.619, Test accuracy: 84.57
Round  18, Global train loss: 1.575, Global test loss: 2.273, Global test accuracy: 17.86
Round  19, Train loss: 1.578, Test loss: 1.618, Test accuracy: 84.59
Round  19, Global train loss: 1.578, Global test loss: 2.277, Global test accuracy: 16.73
Round  20, Train loss: 1.494, Test loss: 1.598, Test accuracy: 86.58
Round  20, Global train loss: 1.494, Global test loss: 2.270, Global test accuracy: 16.73
Round  21, Train loss: 1.681, Test loss: 1.587, Test accuracy: 88.21
Round  21, Global train loss: 1.681, Global test loss: 2.266, Global test accuracy: 17.88
Round  22, Train loss: 1.633, Test loss: 1.587, Test accuracy: 88.20
Round  22, Global train loss: 1.633, Global test loss: 2.281, Global test accuracy: 15.96
Round  23, Train loss: 1.528, Test loss: 1.582, Test accuracy: 88.26
Round  23, Global train loss: 1.528, Global test loss: 2.272, Global test accuracy: 15.34
Round  24, Train loss: 1.468, Test loss: 1.582, Test accuracy: 88.22
Round  24, Global train loss: 1.468, Global test loss: 2.269, Global test accuracy: 15.58
Round  25, Train loss: 1.580, Test loss: 1.581, Test accuracy: 88.30
Round  25, Global train loss: 1.580, Global test loss: 2.297, Global test accuracy: 13.68
Round  26, Train loss: 1.634, Test loss: 1.581, Test accuracy: 88.25
Round  26, Global train loss: 1.634, Global test loss: 2.263, Global test accuracy: 18.48
Round  27, Train loss: 1.512, Test loss: 1.567, Test accuracy: 89.75
Round  27, Global train loss: 1.512, Global test loss: 2.263, Global test accuracy: 17.86
Round  28, Train loss: 1.629, Test loss: 1.567, Test accuracy: 89.75
Round  28, Global train loss: 1.629, Global test loss: 2.258, Global test accuracy: 18.99
Round  29, Train loss: 1.466, Test loss: 1.567, Test accuracy: 89.75
Round  29, Global train loss: 1.466, Global test loss: 2.254, Global test accuracy: 20.12
Round  30, Train loss: 1.525, Test loss: 1.565, Test accuracy: 89.83
Round  30, Global train loss: 1.525, Global test loss: 2.262, Global test accuracy: 18.28
Round  31, Train loss: 1.634, Test loss: 1.565, Test accuracy: 89.82
Round  31, Global train loss: 1.634, Global test loss: 2.286, Global test accuracy: 13.85
Round  32, Train loss: 1.466, Test loss: 1.565, Test accuracy: 89.81
Round  32, Global train loss: 1.466, Global test loss: 2.281, Global test accuracy: 14.75
Round  33, Train loss: 1.519, Test loss: 1.565, Test accuracy: 89.82
Round  33, Global train loss: 1.519, Global test loss: 2.299, Global test accuracy: 14.07
Round  34, Train loss: 1.521, Test loss: 1.565, Test accuracy: 89.81
Round  34, Global train loss: 1.521, Global test loss: 2.283, Global test accuracy: 15.23
Round  35, Train loss: 1.576, Test loss: 1.565, Test accuracy: 89.80
Round  35, Global train loss: 1.576, Global test loss: 2.324, Global test accuracy: 11.28
Round  36, Train loss: 1.468, Test loss: 1.565, Test accuracy: 89.81
Round  36, Global train loss: 1.468, Global test loss: 2.286, Global test accuracy: 11.66
Round  37, Train loss: 1.466, Test loss: 1.565, Test accuracy: 89.81
Round  37, Global train loss: 1.466, Global test loss: 2.290, Global test accuracy: 15.51
Round  38, Train loss: 1.463, Test loss: 1.565, Test accuracy: 89.79
Round  38, Global train loss: 1.463, Global test loss: 2.275, Global test accuracy: 15.58
Round  39, Train loss: 1.469, Test loss: 1.565, Test accuracy: 89.77
Round  39, Global train loss: 1.469, Global test loss: 2.338, Global test accuracy: 8.09
Round  40, Train loss: 1.572, Test loss: 1.564, Test accuracy: 89.77
Round  40, Global train loss: 1.572, Global test loss: 2.273, Global test accuracy: 16.94
Round  41, Train loss: 1.521, Test loss: 1.564, Test accuracy: 89.81
Round  41, Global train loss: 1.521, Global test loss: 2.295, Global test accuracy: 12.83
Round  42, Train loss: 1.463, Test loss: 1.564, Test accuracy: 89.79
Round  42, Global train loss: 1.463, Global test loss: 2.290, Global test accuracy: 13.88
Round  43, Train loss: 1.576, Test loss: 1.564, Test accuracy: 89.83
Round  43, Global train loss: 1.576, Global test loss: 2.264, Global test accuracy: 18.30
Round  44, Train loss: 1.630, Test loss: 1.564, Test accuracy: 89.82
Round  44, Global train loss: 1.630, Global test loss: 2.306, Global test accuracy: 11.74
Round  45, Train loss: 1.523, Test loss: 1.564, Test accuracy: 89.81
Round  45, Global train loss: 1.523, Global test loss: 2.292, Global test accuracy: 14.67
Round  46, Train loss: 1.465, Test loss: 1.564, Test accuracy: 89.83
Round  46, Global train loss: 1.465, Global test loss: 2.279, Global test accuracy: 16.47
Round  47, Train loss: 1.466, Test loss: 1.564, Test accuracy: 89.83
Round  47, Global train loss: 1.466, Global test loss: 2.282, Global test accuracy: 14.53
Round  48, Train loss: 1.629, Test loss: 1.564, Test accuracy: 89.83
Round  48, Global train loss: 1.629, Global test loss: 2.300, Global test accuracy: 11.01
Round  49, Train loss: 1.516, Test loss: 1.564, Test accuracy: 89.84
Round  49, Global train loss: 1.516, Global test loss: 2.275, Global test accuracy: 16.74
Round  50, Train loss: 1.517, Test loss: 1.564, Test accuracy: 89.85
Round  50, Global train loss: 1.517, Global test loss: 2.277, Global test accuracy: 16.86
Round  51, Train loss: 1.522, Test loss: 1.564, Test accuracy: 89.83
Round  51, Global train loss: 1.522, Global test loss: 2.272, Global test accuracy: 16.84
Round  52, Train loss: 1.468, Test loss: 1.564, Test accuracy: 89.83
Round  52, Global train loss: 1.468, Global test loss: 2.277, Global test accuracy: 14.84
Round  53, Train loss: 1.631, Test loss: 1.564, Test accuracy: 89.84
Round  53, Global train loss: 1.631, Global test loss: 2.267, Global test accuracy: 17.73
Round  54, Train loss: 1.468, Test loss: 1.564, Test accuracy: 89.86
Round  54, Global train loss: 1.468, Global test loss: 2.279, Global test accuracy: 13.23
Round  55, Train loss: 1.518, Test loss: 1.564, Test accuracy: 89.86
Round  55, Global train loss: 1.518, Global test loss: 2.291, Global test accuracy: 14.14
Round  56, Train loss: 1.464, Test loss: 1.564, Test accuracy: 89.88
Round  56, Global train loss: 1.464, Global test loss: 2.321, Global test accuracy: 11.91
Round  57, Train loss: 1.518, Test loss: 1.564, Test accuracy: 89.88
Round  57, Global train loss: 1.518, Global test loss: 2.322, Global test accuracy: 11.67
Round  58, Train loss: 1.467, Test loss: 1.564, Test accuracy: 89.87
Round  58, Global train loss: 1.467, Global test loss: 2.277, Global test accuracy: 16.83
Round  59, Train loss: 1.519, Test loss: 1.564, Test accuracy: 89.87
Round  59, Global train loss: 1.519, Global test loss: 2.277, Global test accuracy: 16.83
Round  60, Train loss: 1.519, Test loss: 1.564, Test accuracy: 89.84
Round  60, Global train loss: 1.519, Global test loss: 2.275, Global test accuracy: 16.10
Round  61, Train loss: 1.467, Test loss: 1.564, Test accuracy: 89.85
Round  61, Global train loss: 1.467, Global test loss: 2.279, Global test accuracy: 14.39
Round  62, Train loss: 1.466, Test loss: 1.563, Test accuracy: 89.86
Round  62, Global train loss: 1.466, Global test loss: 2.278, Global test accuracy: 15.35
Round  63, Train loss: 1.573, Test loss: 1.563, Test accuracy: 89.85
Round  63, Global train loss: 1.573, Global test loss: 2.275, Global test accuracy: 15.63
Round  64, Train loss: 1.681, Test loss: 1.563, Test accuracy: 89.85
Round  64, Global train loss: 1.681, Global test loss: 2.337, Global test accuracy: 11.58
Round  65, Train loss: 1.628, Test loss: 1.563, Test accuracy: 89.84
Round  65, Global train loss: 1.628, Global test loss: 2.286, Global test accuracy: 13.75
Round  66, Train loss: 1.518, Test loss: 1.563, Test accuracy: 89.83
Round  66, Global train loss: 1.518, Global test loss: 2.270, Global test accuracy: 17.44
Round  67, Train loss: 1.574, Test loss: 1.563, Test accuracy: 89.82
Round  67, Global train loss: 1.574, Global test loss: 2.269, Global test accuracy: 15.26
Round  68, Train loss: 1.464, Test loss: 1.563, Test accuracy: 89.82
Round  68, Global train loss: 1.464, Global test loss: 2.283, Global test accuracy: 15.47
Round  69, Train loss: 1.466, Test loss: 1.563, Test accuracy: 89.81
Round  69, Global train loss: 1.466, Global test loss: 2.315, Global test accuracy: 10.36
Round  70, Train loss: 1.628, Test loss: 1.563, Test accuracy: 89.82
Round  70, Global train loss: 1.628, Global test loss: 2.292, Global test accuracy: 14.17
Round  71, Train loss: 1.519, Test loss: 1.563, Test accuracy: 89.82
Round  71, Global train loss: 1.519, Global test loss: 2.321, Global test accuracy: 8.23
Round  72, Train loss: 1.465, Test loss: 1.563, Test accuracy: 89.83
Round  72, Global train loss: 1.465, Global test loss: 2.299, Global test accuracy: 12.53
Round  73, Train loss: 1.520, Test loss: 1.563, Test accuracy: 89.83
Round  73, Global train loss: 1.520, Global test loss: 2.301, Global test accuracy: 13.95
Round  74, Train loss: 1.574, Test loss: 1.563, Test accuracy: 89.83
Round  74, Global train loss: 1.574, Global test loss: 2.260, Global test accuracy: 16.84
Round  75, Train loss: 1.574, Test loss: 1.563, Test accuracy: 89.83
Round  75, Global train loss: 1.574, Global test loss: 2.283, Global test accuracy: 16.43
Round  76, Train loss: 1.517, Test loss: 1.563, Test accuracy: 89.83
Round  76, Global train loss: 1.517, Global test loss: 2.286, Global test accuracy: 15.38
Round  77, Train loss: 1.630, Test loss: 1.563, Test accuracy: 89.83
Round  77, Global train loss: 1.630, Global test loss: 2.279, Global test accuracy: 15.53
Round  78, Train loss: 1.576, Test loss: 1.563, Test accuracy: 89.83
Round  78, Global train loss: 1.576, Global test loss: 2.270, Global test accuracy: 17.14
Round  79, Train loss: 1.571, Test loss: 1.563, Test accuracy: 89.83
Round  79, Global train loss: 1.571, Global test loss: 2.260, Global test accuracy: 18.19
Round  80, Train loss: 1.521, Test loss: 1.563, Test accuracy: 89.84
Round  80, Global train loss: 1.521, Global test loss: 2.258, Global test accuracy: 17.90
Round  81, Train loss: 1.465, Test loss: 1.563, Test accuracy: 89.84
Round  81, Global train loss: 1.465, Global test loss: 2.279, Global test accuracy: 17.54
Round  82, Train loss: 1.575, Test loss: 1.563, Test accuracy: 89.84
Round  82, Global train loss: 1.575, Global test loss: 2.269, Global test accuracy: 17.35
Round  83, Train loss: 1.518, Test loss: 1.563, Test accuracy: 89.83
Round  83, Global train loss: 1.518, Global test loss: 2.312, Global test accuracy: 12.09
Round  84, Train loss: 1.464, Test loss: 1.563, Test accuracy: 89.83
Round  84, Global train loss: 1.464, Global test loss: 2.299, Global test accuracy: 13.30
Round  85, Train loss: 1.576, Test loss: 1.563, Test accuracy: 89.83
Round  85, Global train loss: 1.576, Global test loss: 2.274, Global test accuracy: 16.26
Round  86, Train loss: 1.463, Test loss: 1.563, Test accuracy: 89.83
Round  86, Global train loss: 1.463, Global test loss: 2.285, Global test accuracy: 16.28
Round  87, Train loss: 1.465, Test loss: 1.563, Test accuracy: 89.83
Round  87, Global train loss: 1.465, Global test loss: 2.253, Global test accuracy: 19.99
Round  88, Train loss: 1.570, Test loss: 1.563, Test accuracy: 89.84
Round  88, Global train loss: 1.570, Global test loss: 2.294, Global test accuracy: 14.54
Round  89, Train loss: 1.520, Test loss: 1.563, Test accuracy: 89.84
Round  89, Global train loss: 1.520, Global test loss: 2.282, Global test accuracy: 15.33
Round  90, Train loss: 1.575, Test loss: 1.563, Test accuracy: 89.85
Round  90, Global train loss: 1.575, Global test loss: 2.306, Global test accuracy: 13.29
Round  91, Train loss: 1.518, Test loss: 1.563, Test accuracy: 89.84
Round  91, Global train loss: 1.518, Global test loss: 2.282, Global test accuracy: 13.39
Round  92, Train loss: 1.464, Test loss: 1.563, Test accuracy: 89.84
Round  92, Global train loss: 1.464, Global test loss: 2.302, Global test accuracy: 12.22
Round  93, Train loss: 1.519, Test loss: 1.563, Test accuracy: 89.84
Round  93, Global train loss: 1.519, Global test loss: 2.254, Global test accuracy: 19.93
Round  94, Train loss: 1.518, Test loss: 1.563, Test accuracy: 89.85
Round  94, Global train loss: 1.518, Global test loss: 2.307, Global test accuracy: 12.96
Round  95, Train loss: 1.517, Test loss: 1.563, Test accuracy: 89.85
Round  95, Global train loss: 1.517, Global test loss: 2.263, Global test accuracy: 17.93/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.574, Test loss: 1.563, Test accuracy: 89.86
Round  96, Global train loss: 1.574, Global test loss: 2.307, Global test accuracy: 11.71
Round  97, Train loss: 1.519, Test loss: 1.563, Test accuracy: 89.78
Round  97, Global train loss: 1.519, Global test loss: 2.282, Global test accuracy: 14.72
Round  98, Train loss: 1.575, Test loss: 1.563, Test accuracy: 89.87
Round  98, Global train loss: 1.575, Global test loss: 2.276, Global test accuracy: 15.63
Round  99, Train loss: 1.630, Test loss: 1.563, Test accuracy: 89.87
Round  99, Global train loss: 1.630, Global test loss: 2.294, Global test accuracy: 13.12
Final Round, Train loss: 1.530, Test loss: 1.563, Test accuracy: 89.90
Final Round, Global train loss: 1.530, Global test loss: 2.294, Global test accuracy: 13.12
Average accuracy final 10 rounds: 89.845 

Average global accuracy final 10 rounds: 14.490000000000002 

1351.349983215332
[0.9529054164886475, 1.905810832977295, 2.755725860595703, 3.6056408882141113, 4.45236349105835, 5.299086093902588, 6.133350849151611, 6.967615604400635, 7.814756631851196, 8.661897659301758, 9.495911836624146, 10.329926013946533, 11.172992467880249, 12.016058921813965, 12.84639048576355, 13.676722049713135, 14.518784046173096, 15.360846042633057, 16.193973064422607, 17.027100086212158, 17.868206024169922, 18.709311962127686, 19.564011812210083, 20.41871166229248, 21.241854906082153, 22.064998149871826, 22.935176372528076, 23.805354595184326, 24.632402658462524, 25.459450721740723, 26.304476976394653, 27.149503231048584, 27.95705533027649, 28.764607429504395, 29.58623957633972, 30.40787172317505, 31.236953020095825, 32.0660343170166, 32.89062690734863, 33.715219497680664, 34.59278130531311, 35.47034311294556, 36.334049701690674, 37.19775629043579, 38.045950174331665, 38.89414405822754, 39.73328518867493, 40.572426319122314, 41.429333209991455, 42.286240100860596, 43.14483451843262, 44.00342893600464, 44.86144709587097, 45.719465255737305, 46.5705406665802, 47.421616077423096, 48.275097131729126, 49.128578186035156, 49.98655319213867, 50.84452819824219, 51.69640278816223, 52.548277378082275, 53.402904748916626, 54.25753211975098, 55.0967001914978, 55.93586826324463, 56.79750847816467, 57.65914869308472, 58.507516622543335, 59.35588455200195, 60.20756649971008, 61.05924844741821, 61.91298532485962, 62.766722202301025, 63.628820180892944, 64.49091815948486, 65.33159518241882, 66.17227220535278, 67.02557492256165, 67.87887763977051, 68.76082468032837, 69.64277172088623, 70.50797963142395, 71.37318754196167, 72.2569363117218, 73.14068508148193, 74.006427526474, 74.87216997146606, 75.74745869636536, 76.62274742126465, 77.48174333572388, 78.3407392501831, 79.2172212600708, 80.0937032699585, 80.9555299282074, 81.8173565864563, 82.70328521728516, 83.58921384811401, 84.44834399223328, 85.30747413635254, 86.18712496757507, 87.06677579879761, 87.92805314064026, 88.78933048248291, 89.66151261329651, 90.53369474411011, 91.39897298812866, 92.26425123214722, 93.13224911689758, 94.00024700164795, 94.87514972686768, 95.7500524520874, 96.61786913871765, 97.4856858253479, 98.36238408088684, 99.23908233642578, 100.10094237327576, 100.96280241012573, 101.83506274223328, 102.70732307434082, 103.57129526138306, 104.4352674484253, 105.29774975776672, 106.16023206710815, 107.02123022079468, 107.8822283744812, 108.74379134178162, 109.60535430908203, 110.46924042701721, 111.33312654495239, 112.20176386833191, 113.07040119171143, 113.927570104599, 114.78473901748657, 115.65338921546936, 116.52203941345215, 117.39473009109497, 118.2674207687378, 119.11937761306763, 119.97133445739746, 120.83536744117737, 121.69940042495728, 122.55679607391357, 123.41419172286987, 124.29044079780579, 125.1666898727417, 126.02549982070923, 126.88430976867676, 127.75754070281982, 128.6307716369629, 129.4913511276245, 130.35193061828613, 131.2250006198883, 132.09807062149048, 132.9625051021576, 133.8269395828247, 134.69833779335022, 135.56973600387573, 136.44232082366943, 137.31490564346313, 138.20663404464722, 139.0983624458313, 139.96752166748047, 140.83668088912964, 141.69568991661072, 142.5546989440918, 143.42758560180664, 144.30047225952148, 145.15111804008484, 146.0017638206482, 146.95900297164917, 147.91624212265015, 148.76984214782715, 149.62344217300415, 150.49738907814026, 151.37133598327637, 152.22991514205933, 153.08849430084229, 153.94871997833252, 154.80894565582275, 155.67524123191833, 156.54153680801392, 157.4101116657257, 158.2786865234375, 159.13456892967224, 159.99045133590698, 160.8505780696869, 161.7107048034668, 162.57200026512146, 163.43329572677612, 164.28566980361938, 165.13804388046265, 165.99773526191711, 166.85742664337158, 167.7163279056549, 168.57522916793823, 169.4400191307068, 170.30480909347534, 171.1671268939972, 172.02944469451904, 173.48267221450806, 174.93589973449707]
[26.916666666666668, 26.916666666666668, 41.175, 41.175, 53.791666666666664, 53.791666666666664, 65.14166666666667, 65.14166666666667, 71.425, 71.425, 78.425, 78.425, 78.71666666666667, 78.71666666666667, 78.46666666666667, 78.46666666666667, 79.4, 79.4, 80.325, 80.325, 81.275, 81.275, 80.33333333333333, 80.33333333333333, 82.00833333333334, 82.00833333333334, 82.0, 82.0, 83.08333333333333, 83.08333333333333, 83.1, 83.1, 83.06666666666666, 83.06666666666666, 84.56666666666666, 84.56666666666666, 84.56666666666666, 84.56666666666666, 84.59166666666667, 84.59166666666667, 86.58333333333333, 86.58333333333333, 88.20833333333333, 88.20833333333333, 88.2, 88.2, 88.25833333333334, 88.25833333333334, 88.225, 88.225, 88.3, 88.3, 88.25, 88.25, 89.75, 89.75, 89.75, 89.75, 89.75, 89.75, 89.825, 89.825, 89.81666666666666, 89.81666666666666, 89.80833333333334, 89.80833333333334, 89.81666666666666, 89.81666666666666, 89.80833333333334, 89.80833333333334, 89.8, 89.8, 89.80833333333334, 89.80833333333334, 89.80833333333334, 89.80833333333334, 89.79166666666667, 89.79166666666667, 89.76666666666667, 89.76666666666667, 89.76666666666667, 89.76666666666667, 89.80833333333334, 89.80833333333334, 89.79166666666667, 89.79166666666667, 89.825, 89.825, 89.81666666666666, 89.81666666666666, 89.80833333333334, 89.80833333333334, 89.825, 89.825, 89.825, 89.825, 89.83333333333333, 89.83333333333333, 89.84166666666667, 89.84166666666667, 89.85, 89.85, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.84166666666667, 89.84166666666667, 89.85833333333333, 89.85833333333333, 89.85833333333333, 89.85833333333333, 89.875, 89.875, 89.875, 89.875, 89.86666666666666, 89.86666666666666, 89.86666666666666, 89.86666666666666, 89.84166666666667, 89.84166666666667, 89.85, 89.85, 89.85833333333333, 89.85833333333333, 89.85, 89.85, 89.85, 89.85, 89.84166666666667, 89.84166666666667, 89.83333333333333, 89.83333333333333, 89.81666666666666, 89.81666666666666, 89.81666666666666, 89.81666666666666, 89.80833333333334, 89.80833333333334, 89.81666666666666, 89.81666666666666, 89.81666666666666, 89.81666666666666, 89.825, 89.825, 89.825, 89.825, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.83333333333333, 89.84166666666667, 89.84166666666667, 89.84166666666667, 89.84166666666667, 89.84166666666667, 89.84166666666667, 89.825, 89.825, 89.825, 89.825, 89.825, 89.825, 89.825, 89.825, 89.825, 89.825, 89.84166666666667, 89.84166666666667, 89.84166666666667, 89.84166666666667, 89.85, 89.85, 89.84166666666667, 89.84166666666667, 89.84166666666667, 89.84166666666667, 89.84166666666667, 89.84166666666667, 89.85, 89.85, 89.85, 89.85, 89.85833333333333, 89.85833333333333, 89.78333333333333, 89.78333333333333, 89.86666666666666, 89.86666666666666, 89.86666666666666, 89.86666666666666, 89.9, 89.9]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.181, Test loss: 2.149, Test accuracy: 33.80
Round   0, Global train loss: 2.181, Global test loss: 2.264, Global test accuracy: 22.16
Round   1, Train loss: 1.975, Test loss: 2.014, Test accuracy: 47.02
Round   1, Global train loss: 1.975, Global test loss: 2.252, Global test accuracy: 20.00
Round   2, Train loss: 1.889, Test loss: 1.934, Test accuracy: 56.11
Round   2, Global train loss: 1.889, Global test loss: 2.254, Global test accuracy: 20.00
Round   3, Train loss: 1.852, Test loss: 1.886, Test accuracy: 58.73
Round   3, Global train loss: 1.852, Global test loss: 2.254, Global test accuracy: 18.24
Round   4, Train loss: 1.800, Test loss: 1.895, Test accuracy: 57.67
Round   4, Global train loss: 1.800, Global test loss: 2.229, Global test accuracy: 24.14
Round   5, Train loss: 1.865, Test loss: 1.849, Test accuracy: 61.96
Round   5, Global train loss: 1.865, Global test loss: 2.238, Global test accuracy: 22.32
Round   6, Train loss: 1.785, Test loss: 1.797, Test accuracy: 67.62
Round   6, Global train loss: 1.785, Global test loss: 2.237, Global test accuracy: 22.52
Round   7, Train loss: 1.835, Test loss: 1.788, Test accuracy: 68.78
Round   7, Global train loss: 1.835, Global test loss: 2.279, Global test accuracy: 15.42
Round   8, Train loss: 1.762, Test loss: 1.786, Test accuracy: 68.80
Round   8, Global train loss: 1.762, Global test loss: 2.225, Global test accuracy: 23.50
Round   9, Train loss: 1.815, Test loss: 1.788, Test accuracy: 68.38
Round   9, Global train loss: 1.815, Global test loss: 2.229, Global test accuracy: 23.15
Round  10, Train loss: 1.697, Test loss: 1.763, Test accuracy: 70.32
Round  10, Global train loss: 1.697, Global test loss: 2.229, Global test accuracy: 21.39
Round  11, Train loss: 1.775, Test loss: 1.759, Test accuracy: 70.82
Round  11, Global train loss: 1.775, Global test loss: 2.254, Global test accuracy: 18.72
Round  12, Train loss: 1.716, Test loss: 1.739, Test accuracy: 72.67
Round  12, Global train loss: 1.716, Global test loss: 2.233, Global test accuracy: 20.90
Round  13, Train loss: 1.619, Test loss: 1.738, Test accuracy: 72.69
Round  13, Global train loss: 1.619, Global test loss: 2.227, Global test accuracy: 21.38
Round  14, Train loss: 1.822, Test loss: 1.737, Test accuracy: 72.74
Round  14, Global train loss: 1.822, Global test loss: 2.211, Global test accuracy: 23.93
Round  15, Train loss: 1.858, Test loss: 1.724, Test accuracy: 74.04
Round  15, Global train loss: 1.858, Global test loss: 2.227, Global test accuracy: 23.25
Round  16, Train loss: 1.907, Test loss: 1.735, Test accuracy: 72.77
Round  16, Global train loss: 1.907, Global test loss: 2.262, Global test accuracy: 17.31
Round  17, Train loss: 1.613, Test loss: 1.735, Test accuracy: 72.82
Round  17, Global train loss: 1.613, Global test loss: 2.239, Global test accuracy: 20.32
Round  18, Train loss: 1.761, Test loss: 1.717, Test accuracy: 74.61
Round  18, Global train loss: 1.761, Global test loss: 2.239, Global test accuracy: 22.42
Round  19, Train loss: 1.796, Test loss: 1.705, Test accuracy: 75.82
Round  19, Global train loss: 1.796, Global test loss: 2.238, Global test accuracy: 20.08
Round  20, Train loss: 1.863, Test loss: 1.705, Test accuracy: 75.79
Round  20, Global train loss: 1.863, Global test loss: 2.238, Global test accuracy: 20.30
Round  21, Train loss: 1.743, Test loss: 1.689, Test accuracy: 77.48
Round  21, Global train loss: 1.743, Global test loss: 2.249, Global test accuracy: 20.29
Round  22, Train loss: 1.720, Test loss: 1.704, Test accuracy: 75.92
Round  22, Global train loss: 1.720, Global test loss: 2.226, Global test accuracy: 22.42
Round  23, Train loss: 1.762, Test loss: 1.704, Test accuracy: 75.89
Round  23, Global train loss: 1.762, Global test loss: 2.230, Global test accuracy: 20.64
Round  24, Train loss: 1.657, Test loss: 1.704, Test accuracy: 75.91
Round  24, Global train loss: 1.657, Global test loss: 2.223, Global test accuracy: 23.41
Round  25, Train loss: 1.761, Test loss: 1.687, Test accuracy: 77.53
Round  25, Global train loss: 1.761, Global test loss: 2.228, Global test accuracy: 21.55
Round  26, Train loss: 1.734, Test loss: 1.687, Test accuracy: 77.56
Round  26, Global train loss: 1.734, Global test loss: 2.231, Global test accuracy: 21.63
Round  27, Train loss: 1.835, Test loss: 1.702, Test accuracy: 76.06
Round  27, Global train loss: 1.835, Global test loss: 2.221, Global test accuracy: 21.62
Round  28, Train loss: 1.758, Test loss: 1.699, Test accuracy: 76.33
Round  28, Global train loss: 1.758, Global test loss: 2.218, Global test accuracy: 22.77
Round  29, Train loss: 1.730, Test loss: 1.684, Test accuracy: 77.90
Round  29, Global train loss: 1.730, Global test loss: 2.236, Global test accuracy: 20.18
Round  30, Train loss: 1.664, Test loss: 1.684, Test accuracy: 77.94
Round  30, Global train loss: 1.664, Global test loss: 2.237, Global test accuracy: 20.98
Round  31, Train loss: 1.709, Test loss: 1.683, Test accuracy: 78.00
Round  31, Global train loss: 1.709, Global test loss: 2.215, Global test accuracy: 22.90
Round  32, Train loss: 1.748, Test loss: 1.683, Test accuracy: 78.00
Round  32, Global train loss: 1.748, Global test loss: 2.213, Global test accuracy: 22.69
Round  33, Train loss: 1.789, Test loss: 1.671, Test accuracy: 79.18
Round  33, Global train loss: 1.789, Global test loss: 2.215, Global test accuracy: 23.27
Round  34, Train loss: 1.670, Test loss: 1.673, Test accuracy: 79.04
Round  34, Global train loss: 1.670, Global test loss: 2.221, Global test accuracy: 22.67
Round  35, Train loss: 1.715, Test loss: 1.675, Test accuracy: 78.96
Round  35, Global train loss: 1.715, Global test loss: 2.237, Global test accuracy: 20.82
Round  36, Train loss: 1.672, Test loss: 1.671, Test accuracy: 79.19
Round  36, Global train loss: 1.672, Global test loss: 2.220, Global test accuracy: 23.32
Round  37, Train loss: 1.621, Test loss: 1.670, Test accuracy: 79.17
Round  37, Global train loss: 1.621, Global test loss: 2.232, Global test accuracy: 21.60
Round  38, Train loss: 1.705, Test loss: 1.658, Test accuracy: 80.37
Round  38, Global train loss: 1.705, Global test loss: 2.198, Global test accuracy: 25.04
Round  39, Train loss: 1.493, Test loss: 1.657, Test accuracy: 80.48
Round  39, Global train loss: 1.493, Global test loss: 2.195, Global test accuracy: 25.34
Round  40, Train loss: 1.660, Test loss: 1.659, Test accuracy: 80.24
Round  40, Global train loss: 1.660, Global test loss: 2.219, Global test accuracy: 22.91
Round  41, Train loss: 1.512, Test loss: 1.645, Test accuracy: 81.72
Round  41, Global train loss: 1.512, Global test loss: 2.207, Global test accuracy: 24.52
Round  42, Train loss: 1.736, Test loss: 1.661, Test accuracy: 80.08
Round  42, Global train loss: 1.736, Global test loss: 2.246, Global test accuracy: 19.88
Round  43, Train loss: 1.616, Test loss: 1.674, Test accuracy: 78.78
Round  43, Global train loss: 1.616, Global test loss: 2.231, Global test accuracy: 21.62
Round  44, Train loss: 1.650, Test loss: 1.646, Test accuracy: 81.61
Round  44, Global train loss: 1.650, Global test loss: 2.209, Global test accuracy: 23.99
Round  45, Train loss: 1.620, Test loss: 1.646, Test accuracy: 81.63
Round  45, Global train loss: 1.620, Global test loss: 2.226, Global test accuracy: 22.50
Round  46, Train loss: 1.715, Test loss: 1.658, Test accuracy: 80.18
Round  46, Global train loss: 1.715, Global test loss: 2.229, Global test accuracy: 21.46
Round  47, Train loss: 1.667, Test loss: 1.658, Test accuracy: 80.20
Round  47, Global train loss: 1.667, Global test loss: 2.215, Global test accuracy: 23.57
Round  48, Train loss: 1.668, Test loss: 1.612, Test accuracy: 84.94
Round  48, Global train loss: 1.668, Global test loss: 2.215, Global test accuracy: 23.11
Round  49, Train loss: 1.660, Test loss: 1.612, Test accuracy: 84.97
Round  49, Global train loss: 1.660, Global test loss: 2.239, Global test accuracy: 20.09
Round  50, Train loss: 1.552, Test loss: 1.612, Test accuracy: 85.01
Round  50, Global train loss: 1.552, Global test loss: 2.230, Global test accuracy: 21.77
Round  51, Train loss: 1.547, Test loss: 1.626, Test accuracy: 83.52
Round  51, Global train loss: 1.547, Global test loss: 2.220, Global test accuracy: 22.12
Round  52, Train loss: 1.730, Test loss: 1.628, Test accuracy: 83.38
Round  52, Global train loss: 1.730, Global test loss: 2.208, Global test accuracy: 24.24
Round  53, Train loss: 1.660, Test loss: 1.629, Test accuracy: 83.25
Round  53, Global train loss: 1.660, Global test loss: 2.210, Global test accuracy: 23.26
Round  54, Train loss: 1.546, Test loss: 1.630, Test accuracy: 83.18
Round  54, Global train loss: 1.546, Global test loss: 2.204, Global test accuracy: 24.91
Round  55, Train loss: 1.540, Test loss: 1.630, Test accuracy: 83.10
Round  55, Global train loss: 1.540, Global test loss: 2.217, Global test accuracy: 23.16
Round  56, Train loss: 1.518, Test loss: 1.615, Test accuracy: 84.66
Round  56, Global train loss: 1.518, Global test loss: 2.204, Global test accuracy: 24.57
Round  57, Train loss: 1.609, Test loss: 1.631, Test accuracy: 83.08
Round  57, Global train loss: 1.609, Global test loss: 2.240, Global test accuracy: 21.14
Round  58, Train loss: 1.586, Test loss: 1.617, Test accuracy: 84.45
Round  58, Global train loss: 1.586, Global test loss: 2.229, Global test accuracy: 20.98
Round  59, Train loss: 1.658, Test loss: 1.618, Test accuracy: 84.44
Round  59, Global train loss: 1.658, Global test loss: 2.225, Global test accuracy: 21.67
Round  60, Train loss: 1.533, Test loss: 1.615, Test accuracy: 84.66
Round  60, Global train loss: 1.533, Global test loss: 2.218, Global test accuracy: 22.39
Round  61, Train loss: 1.647, Test loss: 1.631, Test accuracy: 83.10
Round  61, Global train loss: 1.647, Global test loss: 2.214, Global test accuracy: 23.16
Round  62, Train loss: 1.652, Test loss: 1.646, Test accuracy: 81.62
Round  62, Global train loss: 1.652, Global test loss: 2.218, Global test accuracy: 23.11
Round  63, Train loss: 1.523, Test loss: 1.626, Test accuracy: 83.67
Round  63, Global train loss: 1.523, Global test loss: 2.209, Global test accuracy: 23.78
Round  64, Train loss: 1.633, Test loss: 1.610, Test accuracy: 85.27
Round  64, Global train loss: 1.633, Global test loss: 2.205, Global test accuracy: 24.48
Round  65, Train loss: 1.551, Test loss: 1.612, Test accuracy: 85.10
Round  65, Global train loss: 1.551, Global test loss: 2.216, Global test accuracy: 22.75
Round  66, Train loss: 1.754, Test loss: 1.608, Test accuracy: 85.43
Round  66, Global train loss: 1.754, Global test loss: 2.206, Global test accuracy: 24.16
Round  67, Train loss: 1.702, Test loss: 1.623, Test accuracy: 83.88
Round  67, Global train loss: 1.702, Global test loss: 2.205, Global test accuracy: 24.25
Round  68, Train loss: 1.698, Test loss: 1.624, Test accuracy: 83.85
Round  68, Global train loss: 1.698, Global test loss: 2.192, Global test accuracy: 25.71
Round  69, Train loss: 1.650, Test loss: 1.626, Test accuracy: 83.60
Round  69, Global train loss: 1.650, Global test loss: 2.201, Global test accuracy: 24.62
Round  70, Train loss: 1.655, Test loss: 1.626, Test accuracy: 83.56
Round  70, Global train loss: 1.655, Global test loss: 2.205, Global test accuracy: 23.95
Round  71, Train loss: 1.649, Test loss: 1.624, Test accuracy: 83.72
Round  71, Global train loss: 1.649, Global test loss: 2.204, Global test accuracy: 24.02
Round  72, Train loss: 1.597, Test loss: 1.624, Test accuracy: 83.73
Round  72, Global train loss: 1.597, Global test loss: 2.220, Global test accuracy: 22.04
Round  73, Train loss: 1.673, Test loss: 1.641, Test accuracy: 82.03
Round  73, Global train loss: 1.673, Global test loss: 2.237, Global test accuracy: 20.97
Round  74, Train loss: 1.653, Test loss: 1.641, Test accuracy: 81.99
Round  74, Global train loss: 1.653, Global test loss: 2.225, Global test accuracy: 21.94
Round  75, Train loss: 1.697, Test loss: 1.641, Test accuracy: 82.03
Round  75, Global train loss: 1.697, Global test loss: 2.211, Global test accuracy: 23.68
Round  76, Train loss: 1.644, Test loss: 1.625, Test accuracy: 83.66
Round  76, Global train loss: 1.644, Global test loss: 2.199, Global test accuracy: 24.88
Round  77, Train loss: 1.539, Test loss: 1.609, Test accuracy: 85.33
Round  77, Global train loss: 1.539, Global test loss: 2.209, Global test accuracy: 23.68
Round  78, Train loss: 1.487, Test loss: 1.609, Test accuracy: 85.30
Round  78, Global train loss: 1.487, Global test loss: 2.197, Global test accuracy: 24.93
Round  79, Train loss: 1.536, Test loss: 1.594, Test accuracy: 86.90
Round  79, Global train loss: 1.536, Global test loss: 2.192, Global test accuracy: 25.78
Round  80, Train loss: 1.589, Test loss: 1.595, Test accuracy: 86.83
Round  80, Global train loss: 1.589, Global test loss: 2.210, Global test accuracy: 23.82
Round  81, Train loss: 1.595, Test loss: 1.594, Test accuracy: 86.85
Round  81, Global train loss: 1.595, Global test loss: 2.194, Global test accuracy: 25.37
Round  82, Train loss: 1.540, Test loss: 1.594, Test accuracy: 86.79
Round  82, Global train loss: 1.540, Global test loss: 2.189, Global test accuracy: 25.70
Round  83, Train loss: 1.596, Test loss: 1.594, Test accuracy: 86.79
Round  83, Global train loss: 1.596, Global test loss: 2.226, Global test accuracy: 21.55
Round  84, Train loss: 1.586, Test loss: 1.609, Test accuracy: 85.22
Round  84, Global train loss: 1.586, Global test loss: 2.224, Global test accuracy: 22.03
Round  85, Train loss: 1.567, Test loss: 1.609, Test accuracy: 85.28
Round  85, Global train loss: 1.567, Global test loss: 2.206, Global test accuracy: 23.44
Round  86, Train loss: 1.544, Test loss: 1.610, Test accuracy: 85.17
Round  86, Global train loss: 1.544, Global test loss: 2.209, Global test accuracy: 23.12
Round  87, Train loss: 1.572, Test loss: 1.595, Test accuracy: 86.78
Round  87, Global train loss: 1.572, Global test loss: 2.209, Global test accuracy: 23.21
Round  88, Train loss: 1.641, Test loss: 1.596, Test accuracy: 86.75
Round  88, Global train loss: 1.641, Global test loss: 2.226, Global test accuracy: 21.38
Round  89, Train loss: 1.530, Test loss: 1.595, Test accuracy: 86.81
Round  89, Global train loss: 1.530, Global test loss: 2.208, Global test accuracy: 23.95
Round  90, Train loss: 1.597, Test loss: 1.595, Test accuracy: 86.76
Round  90, Global train loss: 1.597, Global test loss: 2.220, Global test accuracy: 22.32
Round  91, Train loss: 1.664, Test loss: 1.595, Test accuracy: 86.73
Round  91, Global train loss: 1.664, Global test loss: 2.216, Global test accuracy: 23.25
Round  92, Train loss: 1.646, Test loss: 1.609, Test accuracy: 85.21
Round  92, Global train loss: 1.646, Global test loss: 2.208, Global test accuracy: 23.21
Round  93, Train loss: 1.552, Test loss: 1.609, Test accuracy: 85.22
Round  93, Global train loss: 1.552, Global test loss: 2.216, Global test accuracy: 22.62
Round  94, Train loss: 1.657, Test loss: 1.609, Test accuracy: 85.22
Round  94, Global train loss: 1.657, Global test loss: 2.211, Global test accuracy: 23.24
Round  95, Train loss: 1.659, Test loss: 1.609, Test accuracy: 85.30
Round  95, Global train loss: 1.659, Global test loss: 2.222, Global test accuracy: 22.03/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.572, Test loss: 1.609, Test accuracy: 85.23
Round  96, Global train loss: 1.572, Global test loss: 2.201, Global test accuracy: 24.69
Round  97, Train loss: 1.606, Test loss: 1.609, Test accuracy: 85.30
Round  97, Global train loss: 1.606, Global test loss: 2.203, Global test accuracy: 23.83
Round  98, Train loss: 1.541, Test loss: 1.608, Test accuracy: 85.33
Round  98, Global train loss: 1.541, Global test loss: 2.190, Global test accuracy: 25.56
Round  99, Train loss: 1.701, Test loss: 1.608, Test accuracy: 85.37
Round  99, Global train loss: 1.701, Global test loss: 2.210, Global test accuracy: 23.69
Final Round, Train loss: 1.585, Test loss: 1.594, Test accuracy: 86.71
Final Round, Global train loss: 1.585, Global test loss: 2.210, Global test accuracy: 23.69
Average accuracy final 10 rounds: 85.5675 

Average global accuracy final 10 rounds: 23.444166666666668 

1347.3467619419098
[1.0067744255065918, 2.0135488510131836, 2.876931667327881, 3.740314483642578, 4.59775185585022, 5.455189228057861, 6.32030987739563, 7.185430526733398, 8.045981168746948, 8.906531810760498, 9.762718915939331, 10.618906021118164, 11.48015832901001, 12.341410636901855, 13.218822956085205, 14.096235275268555, 14.94977855682373, 15.803321838378906, 16.665692567825317, 17.52806329727173, 18.39758062362671, 19.26709794998169, 20.127997636795044, 20.9888973236084, 21.852874755859375, 22.71685218811035, 23.582489013671875, 24.4481258392334, 25.320648193359375, 26.19317054748535, 27.048332929611206, 27.90349531173706, 28.77470088005066, 29.645906448364258, 30.533780574798584, 31.42165470123291, 32.27123665809631, 33.12081861495972, 33.98382210731506, 34.84682559967041, 35.72078490257263, 36.59474420547485, 37.452799797058105, 38.31085538864136, 39.15795397758484, 40.00505256652832, 40.86625814437866, 41.727463722229004, 42.583150148391724, 43.43883657455444, 44.28800010681152, 45.1371636390686, 45.99735617637634, 46.85754871368408, 47.71537375450134, 48.5731987953186, 49.432655572891235, 50.29211235046387, 51.15221834182739, 52.01232433319092, 52.87794899940491, 53.7435736656189, 54.603867292404175, 55.46416091918945, 56.315781593322754, 57.167402267456055, 58.02991771697998, 58.892433166503906, 59.7369441986084, 60.58145523071289, 61.442033767700195, 62.3026123046875, 63.17465353012085, 64.0466947555542, 64.89961647987366, 65.75253820419312, 66.60329365730286, 67.4540491104126, 68.3209912776947, 69.1879334449768, 70.0439236164093, 70.8999137878418, 71.74811816215515, 72.5963225364685, 73.45604515075684, 74.31576776504517, 75.17182302474976, 76.02787828445435, 76.88067317008972, 77.7334680557251, 78.58544754981995, 79.4374270439148, 80.29631614685059, 81.15520524978638, 82.00491046905518, 82.85461568832397, 83.70772814750671, 84.56084060668945, 85.41814684867859, 86.27545309066772, 87.12369441986084, 87.97193574905396, 88.83094215393066, 89.68994855880737, 90.54627680778503, 91.4026050567627, 92.2614324092865, 93.1202597618103, 93.97058606147766, 94.82091236114502, 95.68106508255005, 96.54121780395508, 97.39289355278015, 98.24456930160522, 99.10740685462952, 99.97024440765381, 100.82730960845947, 101.68437480926514, 102.5424644947052, 103.40055418014526, 104.26503276824951, 105.12951135635376, 106.00827074050903, 106.8870301246643, 107.7613730430603, 108.6357159614563, 109.50118684768677, 110.36665773391724, 111.24353384971619, 112.12040996551514, 112.98277425765991, 113.84513854980469, 114.70731854438782, 115.56949853897095, 116.43003296852112, 117.29056739807129, 118.1525285243988, 119.01448965072632, 119.87507605552673, 120.73566246032715, 121.59721827507019, 122.45877408981323, 123.3198390007019, 124.18090391159058, 125.03885245323181, 125.89680099487305, 126.75807356834412, 127.61934614181519, 128.4930989742279, 129.36685180664062, 130.24310326576233, 131.11935472488403, 131.98959374427795, 132.85983276367188, 133.80396604537964, 134.7480993270874, 135.64599466323853, 136.54388999938965, 137.39503645896912, 138.24618291854858, 139.10275053977966, 139.95931816101074, 140.82711100578308, 141.69490385055542, 142.54790019989014, 143.40089654922485, 144.26115775108337, 145.1214189529419, 145.98245191574097, 146.84348487854004, 147.70510339736938, 148.56672191619873, 149.4354338645935, 150.30414581298828, 151.161315202713, 152.01848459243774, 152.86611032485962, 153.7137360572815, 154.5612277984619, 155.40871953964233, 156.27696871757507, 157.1452178955078, 158.00384831428528, 158.86247873306274, 159.71376943588257, 160.5650601387024, 161.42276167869568, 162.28046321868896, 163.13469696044922, 163.98893070220947, 164.8508837223053, 165.71283674240112, 166.57044672966003, 167.42805671691895, 168.28737592697144, 169.14669513702393, 169.99369645118713, 170.84069776535034, 171.69281601905823, 172.5449342727661, 173.9836015701294, 175.42226886749268]
[33.8, 33.8, 47.016666666666666, 47.016666666666666, 56.108333333333334, 56.108333333333334, 58.733333333333334, 58.733333333333334, 57.666666666666664, 57.666666666666664, 61.958333333333336, 61.958333333333336, 67.625, 67.625, 68.78333333333333, 68.78333333333333, 68.8, 68.8, 68.375, 68.375, 70.31666666666666, 70.31666666666666, 70.81666666666666, 70.81666666666666, 72.675, 72.675, 72.69166666666666, 72.69166666666666, 72.74166666666666, 72.74166666666666, 74.04166666666667, 74.04166666666667, 72.76666666666667, 72.76666666666667, 72.81666666666666, 72.81666666666666, 74.60833333333333, 74.60833333333333, 75.81666666666666, 75.81666666666666, 75.79166666666667, 75.79166666666667, 77.48333333333333, 77.48333333333333, 75.91666666666667, 75.91666666666667, 75.89166666666667, 75.89166666666667, 75.90833333333333, 75.90833333333333, 77.525, 77.525, 77.55833333333334, 77.55833333333334, 76.05833333333334, 76.05833333333334, 76.33333333333333, 76.33333333333333, 77.9, 77.9, 77.94166666666666, 77.94166666666666, 78.0, 78.0, 78.0, 78.0, 79.18333333333334, 79.18333333333334, 79.04166666666667, 79.04166666666667, 78.95833333333333, 78.95833333333333, 79.19166666666666, 79.19166666666666, 79.175, 79.175, 80.36666666666666, 80.36666666666666, 80.48333333333333, 80.48333333333333, 80.24166666666666, 80.24166666666666, 81.71666666666667, 81.71666666666667, 80.075, 80.075, 78.775, 78.775, 81.60833333333333, 81.60833333333333, 81.63333333333334, 81.63333333333334, 80.18333333333334, 80.18333333333334, 80.2, 80.2, 84.94166666666666, 84.94166666666666, 84.975, 84.975, 85.00833333333334, 85.00833333333334, 83.51666666666667, 83.51666666666667, 83.38333333333334, 83.38333333333334, 83.25, 83.25, 83.18333333333334, 83.18333333333334, 83.1, 83.1, 84.65833333333333, 84.65833333333333, 83.075, 83.075, 84.45, 84.45, 84.44166666666666, 84.44166666666666, 84.65833333333333, 84.65833333333333, 83.1, 83.1, 81.625, 81.625, 83.66666666666667, 83.66666666666667, 85.26666666666667, 85.26666666666667, 85.1, 85.1, 85.43333333333334, 85.43333333333334, 83.88333333333334, 83.88333333333334, 83.85, 83.85, 83.6, 83.6, 83.55833333333334, 83.55833333333334, 83.71666666666667, 83.71666666666667, 83.73333333333333, 83.73333333333333, 82.03333333333333, 82.03333333333333, 81.99166666666666, 81.99166666666666, 82.025, 82.025, 83.65833333333333, 83.65833333333333, 85.33333333333333, 85.33333333333333, 85.3, 85.3, 86.9, 86.9, 86.83333333333333, 86.83333333333333, 86.85, 86.85, 86.79166666666667, 86.79166666666667, 86.79166666666667, 86.79166666666667, 85.225, 85.225, 85.275, 85.275, 85.16666666666667, 85.16666666666667, 86.775, 86.775, 86.75, 86.75, 86.80833333333334, 86.80833333333334, 86.75833333333334, 86.75833333333334, 86.73333333333333, 86.73333333333333, 85.20833333333333, 85.20833333333333, 85.21666666666667, 85.21666666666667, 85.225, 85.225, 85.3, 85.3, 85.23333333333333, 85.23333333333333, 85.3, 85.3, 85.33333333333333, 85.33333333333333, 85.36666666666666, 85.36666666666666, 86.70833333333333, 86.70833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.283, Test loss: 2.296, Test accuracy: 16.81
Round   1, Train loss: 2.183, Test loss: 2.235, Test accuracy: 25.57
Round   2, Train loss: 2.044, Test loss: 2.191, Test accuracy: 25.14
Round   3, Train loss: 2.011, Test loss: 2.113, Test accuracy: 35.18
Round   4, Train loss: 1.842, Test loss: 2.058, Test accuracy: 38.86
Round   5, Train loss: 1.923, Test loss: 2.005, Test accuracy: 46.88
Round   6, Train loss: 1.894, Test loss: 1.990, Test accuracy: 47.08
Round   7, Train loss: 1.828, Test loss: 1.937, Test accuracy: 53.98
Round   8, Train loss: 1.919, Test loss: 1.904, Test accuracy: 56.42
Round   9, Train loss: 1.846, Test loss: 1.888, Test accuracy: 57.99
Round  10, Train loss: 1.791, Test loss: 1.869, Test accuracy: 60.08
Round  11, Train loss: 1.906, Test loss: 1.839, Test accuracy: 63.26
Round  12, Train loss: 1.823, Test loss: 1.835, Test accuracy: 63.42
Round  13, Train loss: 1.715, Test loss: 1.825, Test accuracy: 64.22
Round  14, Train loss: 1.836, Test loss: 1.823, Test accuracy: 64.32
Round  15, Train loss: 1.765, Test loss: 1.821, Test accuracy: 64.42
Round  16, Train loss: 1.901, Test loss: 1.821, Test accuracy: 64.20
Round  17, Train loss: 1.725, Test loss: 1.814, Test accuracy: 64.88
Round  18, Train loss: 1.727, Test loss: 1.810, Test accuracy: 65.27
Round  19, Train loss: 1.819, Test loss: 1.809, Test accuracy: 65.38
Round  20, Train loss: 1.780, Test loss: 1.797, Test accuracy: 66.77
Round  21, Train loss: 1.764, Test loss: 1.797, Test accuracy: 66.78
Round  22, Train loss: 1.808, Test loss: 1.797, Test accuracy: 66.77
Round  23, Train loss: 1.829, Test loss: 1.783, Test accuracy: 68.22
Round  24, Train loss: 1.802, Test loss: 1.782, Test accuracy: 68.19
Round  25, Train loss: 1.740, Test loss: 1.782, Test accuracy: 68.19
Round  26, Train loss: 1.710, Test loss: 1.767, Test accuracy: 69.80
Round  27, Train loss: 1.665, Test loss: 1.755, Test accuracy: 71.04
Round  28, Train loss: 1.666, Test loss: 1.737, Test accuracy: 73.03
Round  29, Train loss: 1.649, Test loss: 1.736, Test accuracy: 73.03
Round  30, Train loss: 1.616, Test loss: 1.732, Test accuracy: 73.32
Round  31, Train loss: 1.700, Test loss: 1.732, Test accuracy: 73.25
Round  32, Train loss: 1.753, Test loss: 1.732, Test accuracy: 73.33
Round  33, Train loss: 1.807, Test loss: 1.730, Test accuracy: 73.42
Round  34, Train loss: 1.701, Test loss: 1.730, Test accuracy: 73.49
Round  35, Train loss: 1.598, Test loss: 1.729, Test accuracy: 73.47
Round  36, Train loss: 1.705, Test loss: 1.728, Test accuracy: 73.65
Round  37, Train loss: 1.697, Test loss: 1.727, Test accuracy: 73.62
Round  38, Train loss: 1.651, Test loss: 1.725, Test accuracy: 73.79
Round  39, Train loss: 1.661, Test loss: 1.724, Test accuracy: 73.83
Round  40, Train loss: 1.650, Test loss: 1.723, Test accuracy: 73.88
Round  41, Train loss: 1.749, Test loss: 1.723, Test accuracy: 73.88
Round  42, Train loss: 1.682, Test loss: 1.724, Test accuracy: 73.72
Round  43, Train loss: 1.698, Test loss: 1.723, Test accuracy: 73.86
Round  44, Train loss: 1.641, Test loss: 1.721, Test accuracy: 73.97
Round  45, Train loss: 1.589, Test loss: 1.720, Test accuracy: 74.10
Round  46, Train loss: 1.721, Test loss: 1.717, Test accuracy: 74.32
Round  47, Train loss: 1.655, Test loss: 1.704, Test accuracy: 75.88
Round  48, Train loss: 1.699, Test loss: 1.692, Test accuracy: 77.13
Round  49, Train loss: 1.651, Test loss: 1.691, Test accuracy: 77.10
Round  50, Train loss: 1.658, Test loss: 1.678, Test accuracy: 78.58
Round  51, Train loss: 1.644, Test loss: 1.677, Test accuracy: 78.59
Round  52, Train loss: 1.643, Test loss: 1.677, Test accuracy: 78.68
Round  53, Train loss: 1.644, Test loss: 1.676, Test accuracy: 78.77
Round  54, Train loss: 1.694, Test loss: 1.676, Test accuracy: 78.72
Round  55, Train loss: 1.590, Test loss: 1.675, Test accuracy: 78.87
Round  56, Train loss: 1.751, Test loss: 1.674, Test accuracy: 78.89
Round  57, Train loss: 1.644, Test loss: 1.673, Test accuracy: 78.95
Round  58, Train loss: 1.589, Test loss: 1.673, Test accuracy: 78.94
Round  59, Train loss: 1.648, Test loss: 1.673, Test accuracy: 78.97
Round  60, Train loss: 1.690, Test loss: 1.673, Test accuracy: 78.98
Round  61, Train loss: 1.695, Test loss: 1.672, Test accuracy: 79.02
Round  62, Train loss: 1.585, Test loss: 1.672, Test accuracy: 79.02
Round  63, Train loss: 1.583, Test loss: 1.672, Test accuracy: 79.00
Round  64, Train loss: 1.636, Test loss: 1.672, Test accuracy: 79.00
Round  65, Train loss: 1.635, Test loss: 1.672, Test accuracy: 78.97
Round  66, Train loss: 1.578, Test loss: 1.672, Test accuracy: 79.02
Round  67, Train loss: 1.640, Test loss: 1.672, Test accuracy: 79.05
Round  68, Train loss: 1.634, Test loss: 1.671, Test accuracy: 79.06
Round  69, Train loss: 1.745, Test loss: 1.671, Test accuracy: 79.12
Round  70, Train loss: 1.695, Test loss: 1.671, Test accuracy: 79.15
Round  71, Train loss: 1.581, Test loss: 1.670, Test accuracy: 79.17
Round  72, Train loss: 1.640, Test loss: 1.671, Test accuracy: 79.06
Round  73, Train loss: 1.530, Test loss: 1.671, Test accuracy: 79.11
Round  74, Train loss: 1.745, Test loss: 1.670, Test accuracy: 79.16
Round  75, Train loss: 1.695, Test loss: 1.670, Test accuracy: 79.16
Round  76, Train loss: 1.635, Test loss: 1.670, Test accuracy: 79.18
Round  77, Train loss: 1.638, Test loss: 1.670, Test accuracy: 79.17
Round  78, Train loss: 1.526, Test loss: 1.669, Test accuracy: 79.21
Round  79, Train loss: 1.578, Test loss: 1.669, Test accuracy: 79.19
Round  80, Train loss: 1.637, Test loss: 1.669, Test accuracy: 79.17
Round  81, Train loss: 1.688, Test loss: 1.669, Test accuracy: 79.22
Round  82, Train loss: 1.635, Test loss: 1.669, Test accuracy: 79.21
Round  83, Train loss: 1.784, Test loss: 1.669, Test accuracy: 79.19
Round  84, Train loss: 1.692, Test loss: 1.656, Test accuracy: 80.67
Round  85, Train loss: 1.633, Test loss: 1.655, Test accuracy: 80.69
Round  86, Train loss: 1.584, Test loss: 1.655, Test accuracy: 80.73
Round  87, Train loss: 1.581, Test loss: 1.654, Test accuracy: 80.72
Round  88, Train loss: 1.667, Test loss: 1.646, Test accuracy: 81.58
Round  89, Train loss: 1.583, Test loss: 1.646, Test accuracy: 81.58
Round  90, Train loss: 1.636, Test loss: 1.646, Test accuracy: 81.62
Round  91, Train loss: 1.635, Test loss: 1.646, Test accuracy: 81.59
Round  92, Train loss: 1.530, Test loss: 1.646, Test accuracy: 81.59
Round  93, Train loss: 1.686, Test loss: 1.646, Test accuracy: 81.54
Round  94, Train loss: 1.685, Test loss: 1.646, Test accuracy: 81.58
Round  95, Train loss: 1.529, Test loss: 1.646, Test accuracy: 81.55
Round  96, Train loss: 1.630, Test loss: 1.646, Test accuracy: 81.56
Round  97, Train loss: 1.579, Test loss: 1.646, Test accuracy: 81.56
Round  98, Train loss: 1.581, Test loss: 1.646, Test accuracy: 81.56/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.630, Test loss: 1.646, Test accuracy: 81.55
Final Round, Train loss: 1.626, Test loss: 1.644, Test accuracy: 81.75
Average accuracy final 10 rounds: 81.57 

1031.9134304523468
[0.9879474639892578, 1.9758949279785156, 2.793274402618408, 3.610653877258301, 4.421761512756348, 5.2328691482543945, 6.044455051422119, 6.856040954589844, 7.671010494232178, 8.485980033874512, 9.306178569793701, 10.12637710571289, 10.930716037750244, 11.735054969787598, 12.53798770904541, 13.340920448303223, 14.164326667785645, 14.987732887268066, 15.8012113571167, 16.614689826965332, 17.424259662628174, 18.233829498291016, 19.029537200927734, 19.825244903564453, 20.635567665100098, 21.445890426635742, 22.258139848709106, 23.07038927078247, 23.861278295516968, 24.652167320251465, 25.445812225341797, 26.23945713043213, 27.046698093414307, 27.853939056396484, 28.66192078590393, 29.469902515411377, 30.258558988571167, 31.047215461730957, 31.83812952041626, 32.62904357910156, 33.436299562454224, 34.243555545806885, 35.05377507209778, 35.86399459838867, 36.660102128982544, 37.456209659576416, 38.2445170879364, 39.03282451629639, 39.8397376537323, 40.64665079116821, 41.434930086135864, 42.223209381103516, 43.011175870895386, 43.799142360687256, 44.57329297065735, 45.34744358062744, 46.133389472961426, 46.91933536529541, 47.700427293777466, 48.48151922225952, 49.26638579368591, 50.051252365112305, 50.83233428001404, 51.61341619491577, 52.39648914337158, 53.17956209182739, 53.970579862594604, 54.761597633361816, 55.53851389884949, 56.31543016433716, 57.10236072540283, 57.889291286468506, 58.664148807525635, 59.439006328582764, 60.22968578338623, 61.0203652381897, 61.80987048149109, 62.59937572479248, 63.38320755958557, 64.16703939437866, 64.9482319355011, 65.72942447662354, 66.5163881778717, 67.30335187911987, 68.08259749412537, 68.86184310913086, 69.64476895332336, 70.42769479751587, 71.20324039459229, 71.9787859916687, 72.77259540557861, 73.56640481948853, 74.38531041145325, 75.20421600341797, 76.00873398780823, 76.81325197219849, 77.62933349609375, 78.44541501998901, 79.2584810256958, 80.07154703140259, 80.8884642124176, 81.70538139343262, 82.52721047401428, 83.34903955459595, 84.1568672657013, 84.96469497680664, 85.77166843414307, 86.57864189147949, 87.3852789402008, 88.19191598892212, 88.99729943275452, 89.80268287658691, 90.6170904636383, 91.4314980506897, 92.24280118942261, 93.05410432815552, 93.86958336830139, 94.68506240844727, 95.49029755592346, 96.29553270339966, 97.09466505050659, 97.89379739761353, 98.67356777191162, 99.45333814620972, 100.28655314445496, 101.1197681427002, 101.93482184410095, 102.74987554550171, 103.53574252128601, 104.32160949707031, 105.10415530204773, 105.88670110702515, 106.67311382293701, 107.45952653884888, 108.25247716903687, 109.04542779922485, 109.82561445236206, 110.60580110549927, 111.39102530479431, 112.17624950408936, 112.96391940116882, 113.75158929824829, 114.54573941230774, 115.33988952636719, 116.12486171722412, 116.90983390808105, 117.6864070892334, 118.46298027038574, 119.2536187171936, 120.04425716400146, 120.83374547958374, 121.62323379516602, 122.40962171554565, 123.1960096359253, 123.97825717926025, 124.76050472259521, 125.55151677131653, 126.34252882003784, 127.12702417373657, 127.9115195274353, 128.7000241279602, 129.4885287284851, 130.2677092552185, 131.0468897819519, 131.83225321769714, 132.61761665344238, 133.40544843673706, 134.19328022003174, 134.9810471534729, 135.76881408691406, 136.54988169670105, 137.33094930648804, 138.11436438560486, 138.89777946472168, 139.6792447566986, 140.46071004867554, 141.25161743164062, 142.0425248146057, 142.82968521118164, 143.61684560775757, 144.3990457057953, 145.181245803833, 145.96166563034058, 146.74208545684814, 147.53540778160095, 148.32873010635376, 149.1146411895752, 149.90055227279663, 150.69149446487427, 151.4824366569519, 152.27028441429138, 153.05813217163086, 153.8640697002411, 154.67000722885132, 155.45876097679138, 156.24751472473145, 157.0828664302826, 157.91821813583374, 158.7036476135254, 159.48907709121704, 160.74964880943298, 162.01022052764893]
[16.808333333333334, 16.808333333333334, 25.566666666666666, 25.566666666666666, 25.141666666666666, 25.141666666666666, 35.18333333333333, 35.18333333333333, 38.858333333333334, 38.858333333333334, 46.875, 46.875, 47.083333333333336, 47.083333333333336, 53.983333333333334, 53.983333333333334, 56.425, 56.425, 57.99166666666667, 57.99166666666667, 60.075, 60.075, 63.25833333333333, 63.25833333333333, 63.416666666666664, 63.416666666666664, 64.225, 64.225, 64.31666666666666, 64.31666666666666, 64.425, 64.425, 64.2, 64.2, 64.875, 64.875, 65.26666666666667, 65.26666666666667, 65.38333333333334, 65.38333333333334, 66.76666666666667, 66.76666666666667, 66.775, 66.775, 66.76666666666667, 66.76666666666667, 68.225, 68.225, 68.19166666666666, 68.19166666666666, 68.19166666666666, 68.19166666666666, 69.8, 69.8, 71.04166666666667, 71.04166666666667, 73.025, 73.025, 73.025, 73.025, 73.31666666666666, 73.31666666666666, 73.25, 73.25, 73.33333333333333, 73.33333333333333, 73.425, 73.425, 73.49166666666666, 73.49166666666666, 73.46666666666667, 73.46666666666667, 73.65, 73.65, 73.61666666666666, 73.61666666666666, 73.79166666666667, 73.79166666666667, 73.83333333333333, 73.83333333333333, 73.88333333333334, 73.88333333333334, 73.88333333333334, 73.88333333333334, 73.71666666666667, 73.71666666666667, 73.85833333333333, 73.85833333333333, 73.96666666666667, 73.96666666666667, 74.1, 74.1, 74.31666666666666, 74.31666666666666, 75.88333333333334, 75.88333333333334, 77.13333333333334, 77.13333333333334, 77.1, 77.1, 78.575, 78.575, 78.59166666666667, 78.59166666666667, 78.68333333333334, 78.68333333333334, 78.76666666666667, 78.76666666666667, 78.725, 78.725, 78.86666666666666, 78.86666666666666, 78.89166666666667, 78.89166666666667, 78.95, 78.95, 78.94166666666666, 78.94166666666666, 78.975, 78.975, 78.98333333333333, 78.98333333333333, 79.01666666666667, 79.01666666666667, 79.01666666666667, 79.01666666666667, 79.0, 79.0, 79.0, 79.0, 78.975, 78.975, 79.01666666666667, 79.01666666666667, 79.05, 79.05, 79.05833333333334, 79.05833333333334, 79.11666666666666, 79.11666666666666, 79.15, 79.15, 79.16666666666667, 79.16666666666667, 79.05833333333334, 79.05833333333334, 79.10833333333333, 79.10833333333333, 79.15833333333333, 79.15833333333333, 79.15833333333333, 79.15833333333333, 79.18333333333334, 79.18333333333334, 79.175, 79.175, 79.20833333333333, 79.20833333333333, 79.19166666666666, 79.19166666666666, 79.175, 79.175, 79.225, 79.225, 79.20833333333333, 79.20833333333333, 79.19166666666666, 79.19166666666666, 80.66666666666667, 80.66666666666667, 80.69166666666666, 80.69166666666666, 80.73333333333333, 80.73333333333333, 80.725, 80.725, 81.575, 81.575, 81.575, 81.575, 81.625, 81.625, 81.59166666666667, 81.59166666666667, 81.59166666666667, 81.59166666666667, 81.54166666666667, 81.54166666666667, 81.575, 81.575, 81.55, 81.55, 81.55833333333334, 81.55833333333334, 81.55833333333334, 81.55833333333334, 81.55833333333334, 81.55833333333334, 81.55, 81.55, 81.75, 81.75]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.162, Test loss: 2.255, Test accuracy: 16.88
Round   1, Train loss: 1.794, Test loss: 2.132, Test accuracy: 36.05
Round   2, Train loss: 1.699, Test loss: 2.059, Test accuracy: 45.20
Round   3, Train loss: 1.884, Test loss: 2.021, Test accuracy: 46.14
Round   4, Train loss: 1.794, Test loss: 1.938, Test accuracy: 55.87
Round   5, Train loss: 1.654, Test loss: 1.915, Test accuracy: 56.13
Round   6, Train loss: 1.724, Test loss: 1.871, Test accuracy: 61.33
Round   7, Train loss: 1.741, Test loss: 1.846, Test accuracy: 63.94
Round   8, Train loss: 1.666, Test loss: 1.832, Test accuracy: 63.95
Round   9, Train loss: 1.705, Test loss: 1.833, Test accuracy: 64.00
Round  10, Train loss: 1.708, Test loss: 1.827, Test accuracy: 64.34
Round  11, Train loss: 1.657, Test loss: 1.802, Test accuracy: 66.92
Round  12, Train loss: 1.807, Test loss: 1.782, Test accuracy: 68.85
Round  13, Train loss: 1.753, Test loss: 1.771, Test accuracy: 69.72
Round  14, Train loss: 1.752, Test loss: 1.773, Test accuracy: 69.30
Round  15, Train loss: 1.734, Test loss: 1.757, Test accuracy: 71.01
Round  16, Train loss: 1.642, Test loss: 1.750, Test accuracy: 71.77
Round  17, Train loss: 1.799, Test loss: 1.749, Test accuracy: 71.75
Round  18, Train loss: 1.691, Test loss: 1.747, Test accuracy: 71.90
Round  19, Train loss: 1.744, Test loss: 1.747, Test accuracy: 71.66
Round  20, Train loss: 1.649, Test loss: 1.738, Test accuracy: 72.74
Round  21, Train loss: 1.748, Test loss: 1.737, Test accuracy: 72.57
Round  22, Train loss: 1.645, Test loss: 1.733, Test accuracy: 73.22
Round  23, Train loss: 1.695, Test loss: 1.731, Test accuracy: 73.33
Round  24, Train loss: 1.648, Test loss: 1.725, Test accuracy: 74.02
Round  25, Train loss: 1.691, Test loss: 1.725, Test accuracy: 73.93
Round  26, Train loss: 1.800, Test loss: 1.723, Test accuracy: 73.99
Round  27, Train loss: 1.628, Test loss: 1.710, Test accuracy: 75.48
Round  28, Train loss: 1.530, Test loss: 1.710, Test accuracy: 75.42
Round  29, Train loss: 1.748, Test loss: 1.709, Test accuracy: 75.44
Round  30, Train loss: 1.589, Test loss: 1.706, Test accuracy: 75.72
Round  31, Train loss: 1.637, Test loss: 1.706, Test accuracy: 75.76
Round  32, Train loss: 1.630, Test loss: 1.706, Test accuracy: 75.81
Round  33, Train loss: 1.635, Test loss: 1.706, Test accuracy: 75.72
Round  34, Train loss: 1.742, Test loss: 1.703, Test accuracy: 75.92
Round  35, Train loss: 1.691, Test loss: 1.700, Test accuracy: 76.24
Round  36, Train loss: 1.689, Test loss: 1.699, Test accuracy: 76.33
Round  37, Train loss: 1.686, Test loss: 1.700, Test accuracy: 76.12
Round  38, Train loss: 1.583, Test loss: 1.697, Test accuracy: 76.49
Round  39, Train loss: 1.583, Test loss: 1.697, Test accuracy: 76.53
Round  40, Train loss: 1.687, Test loss: 1.696, Test accuracy: 76.55
Round  41, Train loss: 1.742, Test loss: 1.696, Test accuracy: 76.55
Round  42, Train loss: 1.742, Test loss: 1.696, Test accuracy: 76.46
Round  43, Train loss: 1.580, Test loss: 1.697, Test accuracy: 76.38
Round  44, Train loss: 1.763, Test loss: 1.695, Test accuracy: 76.47
Round  45, Train loss: 1.579, Test loss: 1.692, Test accuracy: 76.96
Round  46, Train loss: 1.633, Test loss: 1.692, Test accuracy: 76.93
Round  47, Train loss: 1.633, Test loss: 1.691, Test accuracy: 77.10
Round  48, Train loss: 1.709, Test loss: 1.679, Test accuracy: 78.36
Round  49, Train loss: 1.685, Test loss: 1.678, Test accuracy: 78.39
Round  50, Train loss: 1.633, Test loss: 1.676, Test accuracy: 78.55
Round  51, Train loss: 1.742, Test loss: 1.677, Test accuracy: 78.52
Round  52, Train loss: 1.482, Test loss: 1.672, Test accuracy: 79.03
Round  53, Train loss: 1.632, Test loss: 1.672, Test accuracy: 78.95
Round  54, Train loss: 1.579, Test loss: 1.672, Test accuracy: 78.97
Round  55, Train loss: 1.630, Test loss: 1.674, Test accuracy: 78.78
Round  56, Train loss: 1.795, Test loss: 1.672, Test accuracy: 79.03
Round  57, Train loss: 1.684, Test loss: 1.672, Test accuracy: 79.06
Round  58, Train loss: 1.694, Test loss: 1.671, Test accuracy: 79.04
Round  59, Train loss: 1.521, Test loss: 1.671, Test accuracy: 79.03
Round  60, Train loss: 1.581, Test loss: 1.669, Test accuracy: 79.13
Round  61, Train loss: 1.685, Test loss: 1.669, Test accuracy: 79.06
Round  62, Train loss: 1.792, Test loss: 1.669, Test accuracy: 79.13
Round  63, Train loss: 1.629, Test loss: 1.670, Test accuracy: 79.09
Round  64, Train loss: 1.579, Test loss: 1.669, Test accuracy: 79.15
Round  65, Train loss: 1.527, Test loss: 1.668, Test accuracy: 79.20
Round  66, Train loss: 1.629, Test loss: 1.667, Test accuracy: 79.44
Round  67, Train loss: 1.666, Test loss: 1.657, Test accuracy: 80.49
Round  68, Train loss: 1.686, Test loss: 1.657, Test accuracy: 80.47
Round  69, Train loss: 1.637, Test loss: 1.654, Test accuracy: 80.82
Round  70, Train loss: 1.680, Test loss: 1.655, Test accuracy: 80.66
Round  71, Train loss: 1.582, Test loss: 1.653, Test accuracy: 80.89
Round  72, Train loss: 1.577, Test loss: 1.654, Test accuracy: 80.83
Round  73, Train loss: 1.692, Test loss: 1.653, Test accuracy: 80.77
Round  74, Train loss: 1.580, Test loss: 1.651, Test accuracy: 81.03
Round  75, Train loss: 1.632, Test loss: 1.650, Test accuracy: 81.06
Round  76, Train loss: 1.632, Test loss: 1.649, Test accuracy: 81.25
Round  77, Train loss: 1.581, Test loss: 1.649, Test accuracy: 81.20
Round  78, Train loss: 1.522, Test loss: 1.649, Test accuracy: 81.27
Round  79, Train loss: 1.684, Test loss: 1.648, Test accuracy: 81.37
Round  80, Train loss: 1.578, Test loss: 1.648, Test accuracy: 81.40
Round  81, Train loss: 1.684, Test loss: 1.648, Test accuracy: 81.37
Round  82, Train loss: 1.525, Test loss: 1.648, Test accuracy: 81.35
Round  83, Train loss: 1.525, Test loss: 1.646, Test accuracy: 81.41
Round  84, Train loss: 1.684, Test loss: 1.646, Test accuracy: 81.41
Round  85, Train loss: 1.527, Test loss: 1.647, Test accuracy: 81.34
Round  86, Train loss: 1.574, Test loss: 1.645, Test accuracy: 81.52
Round  87, Train loss: 1.737, Test loss: 1.646, Test accuracy: 81.52
Round  88, Train loss: 1.574, Test loss: 1.646, Test accuracy: 81.43
Round  89, Train loss: 1.630, Test loss: 1.646, Test accuracy: 81.57
Round  90, Train loss: 1.632, Test loss: 1.644, Test accuracy: 81.64
Round  91, Train loss: 1.577, Test loss: 1.645, Test accuracy: 81.65
Round  92, Train loss: 1.523, Test loss: 1.645, Test accuracy: 81.53
Round  93, Train loss: 1.573, Test loss: 1.646, Test accuracy: 81.49
Round  94, Train loss: 1.681, Test loss: 1.644, Test accuracy: 81.67
Round  95, Train loss: 1.523, Test loss: 1.644, Test accuracy: 81.66
Round  96, Train loss: 1.576, Test loss: 1.644, Test accuracy: 81.68
Round  97, Train loss: 1.682, Test loss: 1.644, Test accuracy: 81.67
Round  98, Train loss: 1.629, Test loss: 1.644, Test accuracy: 81.60/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.519, Test loss: 1.644, Test accuracy: 81.68
Final Round, Train loss: 1.616, Test loss: 1.639, Test accuracy: 82.06
Average accuracy final 10 rounds: 81.62916666666666 

1058.3954181671143
[0.9537732601165771, 1.9075465202331543, 2.760207414627075, 3.612868309020996, 4.4449005126953125, 5.276932716369629, 6.105952262878418, 6.934971809387207, 7.768105745315552, 8.601239681243896, 9.437922716140747, 10.274605751037598, 11.11743450164795, 11.9602632522583, 12.792145252227783, 13.624027252197266, 14.451203107833862, 15.278378963470459, 16.152089834213257, 17.025800704956055, 17.854427576065063, 18.683054447174072, 19.508328676223755, 20.333602905273438, 21.158721446990967, 21.983839988708496, 22.821208000183105, 23.658576011657715, 24.49080729484558, 25.323038578033447, 26.18233013153076, 27.041621685028076, 27.901058673858643, 28.76049566268921, 29.62956976890564, 30.49864387512207, 31.383864402770996, 32.26908493041992, 33.12584114074707, 33.98259735107422, 34.865670680999756, 35.74874401092529, 36.63446021080017, 37.52017641067505, 38.3734073638916, 39.226638317108154, 40.11458611488342, 41.00253391265869, 41.90682244300842, 42.811110973358154, 43.679253816604614, 44.547396659851074, 45.42742657661438, 46.307456493377686, 47.23083567619324, 48.15421485900879, 49.01795768737793, 49.88170051574707, 50.76039409637451, 51.63908767700195, 52.54633116722107, 53.453574657440186, 54.3381621837616, 55.22274971008301, 56.09475874900818, 56.96676778793335, 57.87309241294861, 58.77941703796387, 59.661274671554565, 60.543132305145264, 61.432321310043335, 62.321510314941406, 63.216357469558716, 64.11120462417603, 64.94961047172546, 65.7880163192749, 66.61682915687561, 67.44564199447632, 68.26935124397278, 69.09306049346924, 69.91866445541382, 70.7442684173584, 71.58121752738953, 72.41816663742065, 73.24893116950989, 74.07969570159912, 74.91153335571289, 75.74337100982666, 76.58645939826965, 77.42954778671265, 78.27564191818237, 79.1217360496521, 79.96417951583862, 80.80662298202515, 81.63996934890747, 82.4733157157898, 83.29992270469666, 84.12652969360352, 84.96640467643738, 85.80627965927124, 86.64565587043762, 87.485032081604, 88.31175065040588, 89.13846921920776, 89.97949171066284, 90.82051420211792, 91.6594467163086, 92.49837923049927, 93.33715891838074, 94.1759386062622, 95.00671076774597, 95.83748292922974, 96.66206240653992, 97.4866418838501, 98.32612800598145, 99.1656141281128, 100.00965285301208, 100.85369157791138, 101.69566226005554, 102.5376329421997, 103.36748623847961, 104.19733953475952, 105.04238414764404, 105.88742876052856, 106.74455308914185, 107.60167741775513, 108.43835377693176, 109.2750301361084, 110.1031723022461, 110.93131446838379, 111.78169322013855, 112.63207197189331, 113.47364735603333, 114.31522274017334, 115.15299797058105, 115.99077320098877, 116.84621071815491, 117.70164823532104, 118.54086232185364, 119.38007640838623, 120.22341513633728, 121.06675386428833, 121.89985537528992, 122.7329568862915, 123.55900955200195, 124.3850622177124, 125.2256646156311, 126.0662670135498, 126.90473771095276, 127.74320840835571, 128.58308696746826, 129.4229655265808, 130.25011229515076, 131.0772590637207, 131.91832637786865, 132.7593936920166, 133.5977382659912, 134.43608283996582, 135.26693081855774, 136.09777879714966, 136.91475129127502, 137.7317237854004, 138.56938457489014, 139.40704536437988, 140.2478175163269, 141.08858966827393, 141.9227921962738, 142.75699472427368, 143.57300543785095, 144.38901615142822, 145.22163605690002, 146.05425596237183, 146.89019131660461, 147.7261266708374, 148.55363059043884, 149.38113451004028, 150.2007372379303, 151.0203399658203, 151.8520827293396, 152.6838254928589, 153.51549077033997, 154.34715604782104, 155.17020463943481, 155.99325323104858, 156.81495237350464, 157.6366515159607, 158.47461557388306, 159.31257963180542, 160.15227603912354, 160.99197244644165, 161.82267689704895, 162.65338134765625, 163.47481179237366, 164.29624223709106, 165.1352744102478, 165.97430658340454, 166.81764817237854, 167.66098976135254, 168.49649262428284, 169.33199548721313, 170.60440230369568, 171.87680912017822]
[16.875, 16.875, 36.05, 36.05, 45.2, 45.2, 46.141666666666666, 46.141666666666666, 55.86666666666667, 55.86666666666667, 56.13333333333333, 56.13333333333333, 61.333333333333336, 61.333333333333336, 63.94166666666667, 63.94166666666667, 63.95, 63.95, 64.0, 64.0, 64.34166666666667, 64.34166666666667, 66.91666666666667, 66.91666666666667, 68.85, 68.85, 69.71666666666667, 69.71666666666667, 69.3, 69.3, 71.00833333333334, 71.00833333333334, 71.76666666666667, 71.76666666666667, 71.75, 71.75, 71.9, 71.9, 71.65833333333333, 71.65833333333333, 72.74166666666666, 72.74166666666666, 72.56666666666666, 72.56666666666666, 73.21666666666667, 73.21666666666667, 73.33333333333333, 73.33333333333333, 74.01666666666667, 74.01666666666667, 73.93333333333334, 73.93333333333334, 73.99166666666666, 73.99166666666666, 75.48333333333333, 75.48333333333333, 75.425, 75.425, 75.44166666666666, 75.44166666666666, 75.71666666666667, 75.71666666666667, 75.75833333333334, 75.75833333333334, 75.80833333333334, 75.80833333333334, 75.725, 75.725, 75.925, 75.925, 76.24166666666666, 76.24166666666666, 76.325, 76.325, 76.11666666666666, 76.11666666666666, 76.49166666666666, 76.49166666666666, 76.525, 76.525, 76.55, 76.55, 76.55, 76.55, 76.45833333333333, 76.45833333333333, 76.375, 76.375, 76.475, 76.475, 76.95833333333333, 76.95833333333333, 76.93333333333334, 76.93333333333334, 77.1, 77.1, 78.35833333333333, 78.35833333333333, 78.39166666666667, 78.39166666666667, 78.55, 78.55, 78.51666666666667, 78.51666666666667, 79.03333333333333, 79.03333333333333, 78.95, 78.95, 78.96666666666667, 78.96666666666667, 78.78333333333333, 78.78333333333333, 79.025, 79.025, 79.05833333333334, 79.05833333333334, 79.04166666666667, 79.04166666666667, 79.025, 79.025, 79.13333333333334, 79.13333333333334, 79.05833333333334, 79.05833333333334, 79.13333333333334, 79.13333333333334, 79.09166666666667, 79.09166666666667, 79.15, 79.15, 79.2, 79.2, 79.44166666666666, 79.44166666666666, 80.49166666666666, 80.49166666666666, 80.46666666666667, 80.46666666666667, 80.81666666666666, 80.81666666666666, 80.65833333333333, 80.65833333333333, 80.89166666666667, 80.89166666666667, 80.83333333333333, 80.83333333333333, 80.76666666666667, 80.76666666666667, 81.025, 81.025, 81.05833333333334, 81.05833333333334, 81.25, 81.25, 81.2, 81.2, 81.26666666666667, 81.26666666666667, 81.36666666666666, 81.36666666666666, 81.4, 81.4, 81.36666666666666, 81.36666666666666, 81.35, 81.35, 81.40833333333333, 81.40833333333333, 81.40833333333333, 81.40833333333333, 81.34166666666667, 81.34166666666667, 81.51666666666667, 81.51666666666667, 81.51666666666667, 81.51666666666667, 81.43333333333334, 81.43333333333334, 81.56666666666666, 81.56666666666666, 81.64166666666667, 81.64166666666667, 81.65, 81.65, 81.53333333333333, 81.53333333333333, 81.49166666666666, 81.49166666666666, 81.675, 81.675, 81.65833333333333, 81.65833333333333, 81.68333333333334, 81.68333333333334, 81.675, 81.675, 81.6, 81.6, 81.68333333333334, 81.68333333333334, 82.05833333333334, 82.05833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.143, Test loss: 2.234, Test accuracy: 21.89
Round   1, Train loss: 1.925, Test loss: 2.132, Test accuracy: 39.94
Round   2, Train loss: 1.679, Test loss: 2.032, Test accuracy: 49.14
Round   3, Train loss: 1.633, Test loss: 1.937, Test accuracy: 58.12
Round   4, Train loss: 1.677, Test loss: 1.863, Test accuracy: 65.39
Round   5, Train loss: 1.670, Test loss: 1.876, Test accuracy: 60.74
Round   6, Train loss: 1.852, Test loss: 1.803, Test accuracy: 68.80
Round   7, Train loss: 1.590, Test loss: 1.762, Test accuracy: 72.30
Round   8, Train loss: 1.581, Test loss: 1.730, Test accuracy: 75.56
Round   9, Train loss: 1.553, Test loss: 1.702, Test accuracy: 77.78
Round  10, Train loss: 1.702, Test loss: 1.663, Test accuracy: 84.78
Round  11, Train loss: 1.539, Test loss: 1.636, Test accuracy: 86.39
Round  12, Train loss: 1.532, Test loss: 1.621, Test accuracy: 87.67
Round  13, Train loss: 1.534, Test loss: 1.609, Test accuracy: 87.18
Round  14, Train loss: 1.529, Test loss: 1.595, Test accuracy: 88.58
Round  15, Train loss: 1.529, Test loss: 1.591, Test accuracy: 88.62
Round  16, Train loss: 1.582, Test loss: 1.588, Test accuracy: 88.97
Round  17, Train loss: 1.525, Test loss: 1.587, Test accuracy: 88.84
Round  18, Train loss: 1.524, Test loss: 1.583, Test accuracy: 89.28
Round  19, Train loss: 1.520, Test loss: 1.584, Test accuracy: 88.93
Round  20, Train loss: 1.610, Test loss: 1.570, Test accuracy: 90.35
Round  21, Train loss: 1.468, Test loss: 1.571, Test accuracy: 90.31
Round  22, Train loss: 1.472, Test loss: 1.572, Test accuracy: 90.00
Round  23, Train loss: 1.519, Test loss: 1.570, Test accuracy: 90.28
Round  24, Train loss: 1.573, Test loss: 1.569, Test accuracy: 90.25
Round  25, Train loss: 1.471, Test loss: 1.561, Test accuracy: 91.00
Round  26, Train loss: 1.572, Test loss: 1.560, Test accuracy: 90.94
Round  27, Train loss: 1.477, Test loss: 1.554, Test accuracy: 91.10
Round  28, Train loss: 1.472, Test loss: 1.553, Test accuracy: 91.22
Round  29, Train loss: 1.530, Test loss: 1.543, Test accuracy: 92.28
Round  30, Train loss: 1.480, Test loss: 1.529, Test accuracy: 93.79
Round  31, Train loss: 1.469, Test loss: 1.526, Test accuracy: 94.03
Round  32, Train loss: 1.521, Test loss: 1.525, Test accuracy: 93.99
Round  33, Train loss: 1.470, Test loss: 1.524, Test accuracy: 94.03
Round  34, Train loss: 1.465, Test loss: 1.524, Test accuracy: 94.08
Round  35, Train loss: 1.469, Test loss: 1.523, Test accuracy: 94.16
Round  36, Train loss: 1.467, Test loss: 1.522, Test accuracy: 94.19
Round  37, Train loss: 1.521, Test loss: 1.522, Test accuracy: 94.20
Round  38, Train loss: 1.522, Test loss: 1.521, Test accuracy: 94.21
Round  39, Train loss: 1.463, Test loss: 1.521, Test accuracy: 94.17
Round  40, Train loss: 1.467, Test loss: 1.521, Test accuracy: 94.22
Round  41, Train loss: 1.466, Test loss: 1.521, Test accuracy: 94.23
Round  42, Train loss: 1.466, Test loss: 1.521, Test accuracy: 94.28
Round  43, Train loss: 1.465, Test loss: 1.521, Test accuracy: 94.26
Round  44, Train loss: 1.467, Test loss: 1.521, Test accuracy: 94.30
Round  45, Train loss: 1.468, Test loss: 1.521, Test accuracy: 94.28
Round  46, Train loss: 1.519, Test loss: 1.520, Test accuracy: 94.23
Round  47, Train loss: 1.464, Test loss: 1.520, Test accuracy: 94.27
Round  48, Train loss: 1.467, Test loss: 1.520, Test accuracy: 94.28
Round  49, Train loss: 1.467, Test loss: 1.520, Test accuracy: 94.30
Round  50, Train loss: 1.521, Test loss: 1.520, Test accuracy: 94.33
Round  51, Train loss: 1.467, Test loss: 1.520, Test accuracy: 94.31
Round  52, Train loss: 1.519, Test loss: 1.519, Test accuracy: 94.34
Round  53, Train loss: 1.466, Test loss: 1.519, Test accuracy: 94.31
Round  54, Train loss: 1.466, Test loss: 1.519, Test accuracy: 94.35
Round  55, Train loss: 1.520, Test loss: 1.519, Test accuracy: 94.32
Round  56, Train loss: 1.466, Test loss: 1.519, Test accuracy: 94.30
Round  57, Train loss: 1.466, Test loss: 1.519, Test accuracy: 94.31
Round  58, Train loss: 1.468, Test loss: 1.519, Test accuracy: 94.33
Round  59, Train loss: 1.467, Test loss: 1.519, Test accuracy: 94.35
Round  60, Train loss: 1.519, Test loss: 1.519, Test accuracy: 94.36
Round  61, Train loss: 1.520, Test loss: 1.519, Test accuracy: 94.35
Round  62, Train loss: 1.465, Test loss: 1.519, Test accuracy: 94.33
Round  63, Train loss: 1.467, Test loss: 1.519, Test accuracy: 94.37
Round  64, Train loss: 1.466, Test loss: 1.519, Test accuracy: 94.39
Round  65, Train loss: 1.465, Test loss: 1.519, Test accuracy: 94.40
Round  66, Train loss: 1.464, Test loss: 1.519, Test accuracy: 94.40
Round  67, Train loss: 1.521, Test loss: 1.519, Test accuracy: 94.39
Round  68, Train loss: 1.466, Test loss: 1.519, Test accuracy: 94.38
Round  69, Train loss: 1.519, Test loss: 1.518, Test accuracy: 94.42
Round  70, Train loss: 1.466, Test loss: 1.518, Test accuracy: 94.42
Round  71, Train loss: 1.520, Test loss: 1.518, Test accuracy: 94.43
Round  72, Train loss: 1.517, Test loss: 1.518, Test accuracy: 94.42
Round  73, Train loss: 1.466, Test loss: 1.518, Test accuracy: 94.43
Round  74, Train loss: 1.520, Test loss: 1.518, Test accuracy: 94.44
Round  75, Train loss: 1.464, Test loss: 1.518, Test accuracy: 94.45
Round  76, Train loss: 1.467, Test loss: 1.518, Test accuracy: 94.44
Round  77, Train loss: 1.464, Test loss: 1.518, Test accuracy: 94.45
Round  78, Train loss: 1.520, Test loss: 1.518, Test accuracy: 94.47
Round  79, Train loss: 1.464, Test loss: 1.518, Test accuracy: 94.43
Round  80, Train loss: 1.467, Test loss: 1.518, Test accuracy: 94.42
Round  81, Train loss: 1.519, Test loss: 1.518, Test accuracy: 94.45
Round  82, Train loss: 1.518, Test loss: 1.518, Test accuracy: 94.42
Round  83, Train loss: 1.464, Test loss: 1.518, Test accuracy: 94.42
Round  84, Train loss: 1.466, Test loss: 1.518, Test accuracy: 94.43
Round  85, Train loss: 1.465, Test loss: 1.518, Test accuracy: 94.42
Round  86, Train loss: 1.465, Test loss: 1.518, Test accuracy: 94.43
Round  87, Train loss: 1.465, Test loss: 1.518, Test accuracy: 94.41
Round  88, Train loss: 1.465, Test loss: 1.518, Test accuracy: 94.42
Round  89, Train loss: 1.464, Test loss: 1.518, Test accuracy: 94.43
Round  90, Train loss: 1.464, Test loss: 1.518, Test accuracy: 94.44
Round  91, Train loss: 1.520, Test loss: 1.518, Test accuracy: 94.44
Round  92, Train loss: 1.520, Test loss: 1.518, Test accuracy: 94.44
Round  93, Train loss: 1.465, Test loss: 1.518, Test accuracy: 94.45
Round  94, Train loss: 1.465, Test loss: 1.518, Test accuracy: 94.48
Round  95, Train loss: 1.519, Test loss: 1.518, Test accuracy: 94.47
Round  96, Train loss: 1.465, Test loss: 1.518, Test accuracy: 94.47
Round  97, Train loss: 1.464, Test loss: 1.518, Test accuracy: 94.47
Round  98, Train loss: 1.464, Test loss: 1.518, Test accuracy: 94.49
Round  99, Train loss: 1.465, Test loss: 1.518, Test accuracy: 94.47/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.481, Test loss: 1.518, Test accuracy: 94.50
Average accuracy final 10 rounds: 94.465 

1058.2862524986267
[0.9843864440917969, 1.9687728881835938, 2.801826238632202, 3.6348795890808105, 4.468677759170532, 5.302475929260254, 6.142519474029541, 6.982563018798828, 7.826996564865112, 8.671430110931396, 9.503441572189331, 10.335453033447266, 11.174447774887085, 12.013442516326904, 12.862343311309814, 13.711244106292725, 14.564191341400146, 15.417138576507568, 16.254343271255493, 17.091547966003418, 17.93744206428528, 18.78333616256714, 19.63769268989563, 20.49204921722412, 21.34098172187805, 22.189914226531982, 23.024455070495605, 23.85899591445923, 24.716253757476807, 25.573511600494385, 26.435028791427612, 27.29654598236084, 28.16755223274231, 29.03855848312378, 29.884202003479004, 30.72984552383423, 31.57417321205139, 32.418500900268555, 33.27225923538208, 34.126017570495605, 34.98428511619568, 35.84255266189575, 36.66829991340637, 37.49404716491699, 38.33920645713806, 39.18436574935913, 40.04251146316528, 40.900657176971436, 41.75178408622742, 42.6029109954834, 43.43055605888367, 44.258201122283936, 45.09125304222107, 45.9243049621582, 46.76094317436218, 47.59758138656616, 48.439576864242554, 49.281572341918945, 50.10748243331909, 50.93339252471924, 51.76160717010498, 52.58982181549072, 53.42435002326965, 54.258878231048584, 55.097259521484375, 55.935640811920166, 56.77425146102905, 57.61286211013794, 58.46675372123718, 59.320645332336426, 60.167253494262695, 61.013861656188965, 61.86132049560547, 62.70877933502197, 63.54708409309387, 64.38538885116577, 65.2358615398407, 66.08633422851562, 66.93216133117676, 67.77798843383789, 68.61637091636658, 69.45475339889526, 70.28354096412659, 71.11232852935791, 71.93770933151245, 72.76309013366699, 73.59798645973206, 74.43288278579712, 75.27181911468506, 76.110755443573, 76.95171546936035, 77.7926754951477, 78.63047766685486, 79.46827983856201, 80.31149435043335, 81.15470886230469, 81.99764060974121, 82.84057235717773, 83.67877650260925, 84.51698064804077, 85.35583901405334, 86.19469738006592, 87.06225252151489, 87.92980766296387, 88.79938340187073, 89.66895914077759, 90.50370240211487, 91.33844566345215, 92.17320656776428, 93.00796747207642, 93.8498170375824, 94.69166660308838, 95.55257987976074, 96.4134931564331, 97.25200843811035, 98.0905237197876, 98.96213054656982, 99.83373737335205, 100.71353697776794, 101.59333658218384, 102.44052648544312, 103.28771638870239, 104.1171932220459, 104.9466700553894, 105.77862286567688, 106.61057567596436, 107.44735383987427, 108.28413200378418, 109.11228919029236, 109.94044637680054, 110.768394947052, 111.59634351730347, 112.43422746658325, 113.27211141586304, 114.11864376068115, 114.96517610549927, 115.80517411231995, 116.64517211914062, 117.47061562538147, 118.29605913162231, 119.13329744338989, 119.97053575515747, 120.81783199310303, 121.66512823104858, 122.51364207267761, 123.36215591430664, 124.19755554199219, 125.03295516967773, 125.8861334323883, 126.73931169509888, 127.58170127868652, 128.42409086227417, 129.24317836761475, 130.06226587295532, 130.89330291748047, 131.72433996200562, 132.5604772567749, 133.3966145515442, 134.23408389091492, 135.07155323028564, 135.90022206306458, 136.7288908958435, 137.56792783737183, 138.40696477890015, 139.24542689323425, 140.08388900756836, 140.92901492118835, 141.77414083480835, 142.60031151771545, 143.42648220062256, 144.26660537719727, 145.10672855377197, 145.9467897415161, 146.78685092926025, 147.61993789672852, 148.45302486419678, 149.28313851356506, 150.11325216293335, 150.95788192749023, 151.80251169204712, 152.6405098438263, 153.47850799560547, 154.3178675174713, 155.15722703933716, 155.98852467536926, 156.81982231140137, 157.65510725975037, 158.49039220809937, 159.33612990379333, 160.1818675994873, 161.01287293434143, 161.84387826919556, 162.66277074813843, 163.4816632270813, 164.3014268875122, 165.12119054794312, 165.96645092964172, 166.81171131134033, 167.63409852981567, 168.45648574829102, 169.78623008728027, 171.11597442626953]
[21.891666666666666, 21.891666666666666, 39.94166666666667, 39.94166666666667, 49.141666666666666, 49.141666666666666, 58.11666666666667, 58.11666666666667, 65.39166666666667, 65.39166666666667, 60.74166666666667, 60.74166666666667, 68.8, 68.8, 72.3, 72.3, 75.55833333333334, 75.55833333333334, 77.775, 77.775, 84.775, 84.775, 86.39166666666667, 86.39166666666667, 87.66666666666667, 87.66666666666667, 87.18333333333334, 87.18333333333334, 88.58333333333333, 88.58333333333333, 88.61666666666666, 88.61666666666666, 88.975, 88.975, 88.84166666666667, 88.84166666666667, 89.275, 89.275, 88.93333333333334, 88.93333333333334, 90.35, 90.35, 90.30833333333334, 90.30833333333334, 90.0, 90.0, 90.28333333333333, 90.28333333333333, 90.25, 90.25, 91.0, 91.0, 90.94166666666666, 90.94166666666666, 91.1, 91.1, 91.21666666666667, 91.21666666666667, 92.275, 92.275, 93.79166666666667, 93.79166666666667, 94.03333333333333, 94.03333333333333, 93.99166666666666, 93.99166666666666, 94.03333333333333, 94.03333333333333, 94.08333333333333, 94.08333333333333, 94.15833333333333, 94.15833333333333, 94.19166666666666, 94.19166666666666, 94.2, 94.2, 94.20833333333333, 94.20833333333333, 94.16666666666667, 94.16666666666667, 94.21666666666667, 94.21666666666667, 94.23333333333333, 94.23333333333333, 94.275, 94.275, 94.25833333333334, 94.25833333333334, 94.3, 94.3, 94.275, 94.275, 94.23333333333333, 94.23333333333333, 94.26666666666667, 94.26666666666667, 94.28333333333333, 94.28333333333333, 94.3, 94.3, 94.33333333333333, 94.33333333333333, 94.30833333333334, 94.30833333333334, 94.34166666666667, 94.34166666666667, 94.30833333333334, 94.30833333333334, 94.35, 94.35, 94.31666666666666, 94.31666666666666, 94.3, 94.3, 94.30833333333334, 94.30833333333334, 94.325, 94.325, 94.35, 94.35, 94.35833333333333, 94.35833333333333, 94.35, 94.35, 94.33333333333333, 94.33333333333333, 94.36666666666666, 94.36666666666666, 94.39166666666667, 94.39166666666667, 94.4, 94.4, 94.4, 94.4, 94.39166666666667, 94.39166666666667, 94.38333333333334, 94.38333333333334, 94.425, 94.425, 94.425, 94.425, 94.43333333333334, 94.43333333333334, 94.425, 94.425, 94.43333333333334, 94.43333333333334, 94.44166666666666, 94.44166666666666, 94.45, 94.45, 94.44166666666666, 94.44166666666666, 94.45, 94.45, 94.46666666666667, 94.46666666666667, 94.43333333333334, 94.43333333333334, 94.425, 94.425, 94.45, 94.45, 94.41666666666667, 94.41666666666667, 94.425, 94.425, 94.43333333333334, 94.43333333333334, 94.425, 94.425, 94.43333333333334, 94.43333333333334, 94.40833333333333, 94.40833333333333, 94.425, 94.425, 94.43333333333334, 94.43333333333334, 94.44166666666666, 94.44166666666666, 94.44166666666666, 94.44166666666666, 94.44166666666666, 94.44166666666666, 94.45, 94.45, 94.48333333333333, 94.48333333333333, 94.475, 94.475, 94.475, 94.475, 94.475, 94.475, 94.49166666666666, 94.49166666666666, 94.475, 94.475, 94.5, 94.5]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.315, Test loss: 2.144, Test accuracy: 37.39
Round   1, Train loss: 1.300, Test loss: 2.048, Test accuracy: 45.88
Round   2, Train loss: 1.297, Test loss: 1.967, Test accuracy: 55.02
Round   3, Train loss: 1.418, Test loss: 1.916, Test accuracy: 60.85
Round   4, Train loss: 1.283, Test loss: 1.878, Test accuracy: 63.09
Round   5, Train loss: 1.353, Test loss: 1.859, Test accuracy: 64.42
Round   6, Train loss: 1.268, Test loss: 1.852, Test accuracy: 63.89
Round   7, Train loss: 1.389, Test loss: 1.838, Test accuracy: 65.19
Round   8, Train loss: 1.353, Test loss: 1.829, Test accuracy: 65.41
Round   9, Train loss: 1.311, Test loss: 1.816, Test accuracy: 66.42
Round  10, Train loss: 1.340, Test loss: 1.806, Test accuracy: 67.77
Round  11, Train loss: 1.350, Test loss: 1.804, Test accuracy: 67.82
Round  12, Train loss: 1.257, Test loss: 1.790, Test accuracy: 68.91
Round  13, Train loss: 1.266, Test loss: 1.785, Test accuracy: 69.32
Round  14, Train loss: 1.318, Test loss: 1.777, Test accuracy: 69.93
Round  15, Train loss: 1.261, Test loss: 1.763, Test accuracy: 71.59
Round  16, Train loss: 1.348, Test loss: 1.759, Test accuracy: 71.83
Round  17, Train loss: 1.323, Test loss: 1.735, Test accuracy: 74.43
Round  18, Train loss: 1.392, Test loss: 1.733, Test accuracy: 74.66
Round  19, Train loss: 1.234, Test loss: 1.726, Test accuracy: 74.88
Round  20, Train loss: 1.271, Test loss: 1.719, Test accuracy: 75.25
Round  21, Train loss: 1.307, Test loss: 1.719, Test accuracy: 75.21
Round  22, Train loss: 1.329, Test loss: 1.713, Test accuracy: 76.04
Round  23, Train loss: 1.228, Test loss: 1.713, Test accuracy: 75.98
Round  24, Train loss: 1.313, Test loss: 1.708, Test accuracy: 75.85
Round  25, Train loss: 1.266, Test loss: 1.707, Test accuracy: 75.98
Round  26, Train loss: 1.229, Test loss: 1.698, Test accuracy: 76.92
Round  27, Train loss: 1.225, Test loss: 1.698, Test accuracy: 76.92
Round  28, Train loss: 1.224, Test loss: 1.697, Test accuracy: 76.92
Round  29, Train loss: 1.231, Test loss: 1.695, Test accuracy: 76.90
Round  30, Train loss: 1.309, Test loss: 1.695, Test accuracy: 76.81
Round  31, Train loss: 1.144, Test loss: 1.696, Test accuracy: 76.72
Round  32, Train loss: 1.351, Test loss: 1.695, Test accuracy: 76.78
Round  33, Train loss: 1.266, Test loss: 1.694, Test accuracy: 76.79
Round  34, Train loss: 1.266, Test loss: 1.694, Test accuracy: 76.84
Round  35, Train loss: 1.184, Test loss: 1.695, Test accuracy: 76.74
Round  36, Train loss: 1.249, Test loss: 1.682, Test accuracy: 78.29
Round  37, Train loss: 1.308, Test loss: 1.683, Test accuracy: 78.25
Round  38, Train loss: 1.268, Test loss: 1.688, Test accuracy: 77.84
Round  39, Train loss: 1.184, Test loss: 1.686, Test accuracy: 78.09
Round  40, Train loss: 1.226, Test loss: 1.690, Test accuracy: 77.41
Round  41, Train loss: 1.144, Test loss: 1.688, Test accuracy: 77.83
Round  42, Train loss: 1.225, Test loss: 1.690, Test accuracy: 77.32
Round  43, Train loss: 1.182, Test loss: 1.687, Test accuracy: 77.83
Round  44, Train loss: 1.265, Test loss: 1.686, Test accuracy: 77.96
Round  45, Train loss: 1.223, Test loss: 1.688, Test accuracy: 77.72
Round  46, Train loss: 1.225, Test loss: 1.690, Test accuracy: 77.50
Round  47, Train loss: 1.226, Test loss: 1.690, Test accuracy: 77.47
Round  48, Train loss: 1.182, Test loss: 1.691, Test accuracy: 77.35
Round  49, Train loss: 1.224, Test loss: 1.695, Test accuracy: 77.01
Round  50, Train loss: 1.225, Test loss: 1.692, Test accuracy: 77.39
Round  51, Train loss: 1.225, Test loss: 1.697, Test accuracy: 76.47
Round  52, Train loss: 1.225, Test loss: 1.690, Test accuracy: 77.53
Round  53, Train loss: 1.307, Test loss: 1.690, Test accuracy: 77.50
Round  54, Train loss: 1.307, Test loss: 1.689, Test accuracy: 77.58
Round  55, Train loss: 1.225, Test loss: 1.690, Test accuracy: 77.42
Round  56, Train loss: 1.267, Test loss: 1.689, Test accuracy: 77.50
Round  57, Train loss: 1.267, Test loss: 1.689, Test accuracy: 77.53
Round  58, Train loss: 1.141, Test loss: 1.689, Test accuracy: 77.57
Round  59, Train loss: 1.265, Test loss: 1.688, Test accuracy: 77.58
Round  60, Train loss: 1.142, Test loss: 1.688, Test accuracy: 77.62
Round  61, Train loss: 1.182, Test loss: 1.689, Test accuracy: 77.55
Round  62, Train loss: 1.224, Test loss: 1.689, Test accuracy: 77.61
Round  63, Train loss: 1.224, Test loss: 1.695, Test accuracy: 76.78
Round  64, Train loss: 1.224, Test loss: 1.695, Test accuracy: 76.77
Round  65, Train loss: 1.306, Test loss: 1.693, Test accuracy: 77.14
Round  66, Train loss: 1.225, Test loss: 1.692, Test accuracy: 77.17
Round  67, Train loss: 1.182, Test loss: 1.696, Test accuracy: 76.79
Round  68, Train loss: 1.265, Test loss: 1.698, Test accuracy: 76.38
Round  69, Train loss: 1.184, Test loss: 1.696, Test accuracy: 76.78
Round  70, Train loss: 1.223, Test loss: 1.694, Test accuracy: 76.93
Round  71, Train loss: 1.142, Test loss: 1.699, Test accuracy: 76.10
Round  72, Train loss: 1.223, Test loss: 1.699, Test accuracy: 76.06
Round  73, Train loss: 1.266, Test loss: 1.700, Test accuracy: 76.08
Round  74, Train loss: 1.223, Test loss: 1.696, Test accuracy: 76.67
Round  75, Train loss: 1.265, Test loss: 1.696, Test accuracy: 76.72
Round  76, Train loss: 1.265, Test loss: 1.698, Test accuracy: 76.44
Round  77, Train loss: 1.265, Test loss: 1.699, Test accuracy: 76.28
Round  78, Train loss: 1.266, Test loss: 1.701, Test accuracy: 75.97
Round  79, Train loss: 1.224, Test loss: 1.704, Test accuracy: 75.54
Round  80, Train loss: 1.224, Test loss: 1.697, Test accuracy: 76.56
Round  81, Train loss: 1.264, Test loss: 1.696, Test accuracy: 76.66
Round  82, Train loss: 1.181, Test loss: 1.697, Test accuracy: 76.63
Round  83, Train loss: 1.222, Test loss: 1.695, Test accuracy: 76.84
Round  84, Train loss: 1.223, Test loss: 1.700, Test accuracy: 76.22
Round  85, Train loss: 1.181, Test loss: 1.702, Test accuracy: 75.90
Round  86, Train loss: 1.142, Test loss: 1.704, Test accuracy: 75.65
Round  87, Train loss: 1.224, Test loss: 1.706, Test accuracy: 75.42
Round  88, Train loss: 1.182, Test loss: 1.699, Test accuracy: 76.27
Round  89, Train loss: 1.306, Test loss: 1.697, Test accuracy: 76.44
Round  90, Train loss: 1.307, Test loss: 1.699, Test accuracy: 76.35
Round  91, Train loss: 1.265, Test loss: 1.699, Test accuracy: 76.25
Round  92, Train loss: 1.223, Test loss: 1.703, Test accuracy: 75.98
Round  93, Train loss: 1.182, Test loss: 1.700, Test accuracy: 76.16
Round  94, Train loss: 1.264, Test loss: 1.701, Test accuracy: 76.17
Round  95, Train loss: 1.265, Test loss: 1.702, Test accuracy: 75.97
Round  96, Train loss: 1.183, Test loss: 1.701, Test accuracy: 76.03
Round  97, Train loss: 1.307, Test loss: 1.702, Test accuracy: 75.90
Round  98, Train loss: 1.265, Test loss: 1.706, Test accuracy: 75.53
Round  99, Train loss: 1.182, Test loss: 1.702, Test accuracy: 76.04
Final Round, Train loss: 1.236, Test loss: 1.704, Test accuracy: 75.64
Average accuracy final 10 rounds: 76.03750000000001
1348.7582745552063
[]
[37.391666666666666, 45.88333333333333, 55.025, 60.85, 63.09166666666667, 64.425, 63.891666666666666, 65.19166666666666, 65.40833333333333, 66.425, 67.76666666666667, 67.81666666666666, 68.90833333333333, 69.31666666666666, 69.93333333333334, 71.59166666666667, 71.825, 74.43333333333334, 74.65833333333333, 74.88333333333334, 75.25, 75.20833333333333, 76.04166666666667, 75.98333333333333, 75.85, 75.98333333333333, 76.925, 76.91666666666667, 76.91666666666667, 76.9, 76.80833333333334, 76.725, 76.78333333333333, 76.79166666666667, 76.84166666666667, 76.74166666666666, 78.29166666666667, 78.25, 77.84166666666667, 78.09166666666667, 77.40833333333333, 77.83333333333333, 77.31666666666666, 77.825, 77.95833333333333, 77.725, 77.5, 77.475, 77.35, 77.00833333333334, 77.39166666666667, 76.46666666666667, 77.525, 77.5, 77.58333333333333, 77.41666666666667, 77.5, 77.525, 77.56666666666666, 77.58333333333333, 77.625, 77.55, 77.60833333333333, 76.78333333333333, 76.76666666666667, 77.14166666666667, 77.175, 76.79166666666667, 76.375, 76.78333333333333, 76.93333333333334, 76.1, 76.05833333333334, 76.08333333333333, 76.675, 76.725, 76.44166666666666, 76.28333333333333, 75.975, 75.54166666666667, 76.55833333333334, 76.65833333333333, 76.63333333333334, 76.84166666666667, 76.225, 75.9, 75.65, 75.41666666666667, 76.26666666666667, 76.44166666666666, 76.35, 76.25, 75.98333333333333, 76.15833333333333, 76.16666666666667, 75.975, 76.025, 75.9, 75.525, 76.04166666666667, 75.64166666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.164, Test loss: 2.171, Test accuracy: 23.96
Round   1, Train loss: 1.915, Test loss: 2.015, Test accuracy: 42.83
Round   2, Train loss: 1.904, Test loss: 2.084, Test accuracy: 33.58
Round   3, Train loss: 1.579, Test loss: 1.952, Test accuracy: 49.21
Round   4, Train loss: 1.700, Test loss: 1.979, Test accuracy: 46.35
Round   5, Train loss: 1.106, Test loss: 1.897, Test accuracy: 57.17
Round   6, Train loss: 1.337, Test loss: 1.954, Test accuracy: 53.37
Round   7, Train loss: 1.244, Test loss: 1.963, Test accuracy: 55.72
Round   8, Train loss: 0.588, Test loss: 1.917, Test accuracy: 58.49
Round   9, Train loss: 1.018, Test loss: 1.961, Test accuracy: 53.88
Round  10, Train loss: 0.796, Test loss: 1.943, Test accuracy: 56.27
Round  11, Train loss: 0.132, Test loss: 1.749, Test accuracy: 73.76
Round  12, Train loss: 0.748, Test loss: 1.755, Test accuracy: 74.38
Round  13, Train loss: 1.130, Test loss: 1.810, Test accuracy: 68.19
Round  14, Train loss: 1.155, Test loss: 1.811, Test accuracy: 67.26
Round  15, Train loss: 0.032, Test loss: 1.751, Test accuracy: 72.85
Round  16, Train loss: 0.077, Test loss: 1.770, Test accuracy: 70.22
Round  17, Train loss: -0.565, Test loss: 1.718, Test accuracy: 74.54
Round  18, Train loss: -0.660, Test loss: 1.658, Test accuracy: 80.90
Round  19, Train loss: -1.083, Test loss: 1.681, Test accuracy: 77.88
Round  20, Train loss: -0.038, Test loss: 1.648, Test accuracy: 82.05
Round  21, Train loss: -0.254, Test loss: 1.650, Test accuracy: 82.08
Round  22, Train loss: -0.506, Test loss: 1.629, Test accuracy: 83.59
Round  23, Train loss: -0.715, Test loss: 1.608, Test accuracy: 85.34
Round  24, Train loss: -0.723, Test loss: 1.607, Test accuracy: 85.41
Round  25, Train loss: -1.627, Test loss: 1.559, Test accuracy: 90.26
Round  26, Train loss: -1.513, Test loss: 1.561, Test accuracy: 90.11
Round  27, Train loss: -1.812, Test loss: 1.588, Test accuracy: 87.28
Round  28, Train loss: -1.271, Test loss: 1.574, Test accuracy: 88.77
Round  29, Train loss: -2.178, Test loss: 1.587, Test accuracy: 87.39
Round  30, Train loss: -0.446, Test loss: 1.605, Test accuracy: 85.65
Round  31, Train loss: -1.192, Test loss: 1.591, Test accuracy: 87.04
Round  32, Train loss: -1.662, Test loss: 1.560, Test accuracy: 90.18
Round  33, Train loss: -0.974, Test loss: 1.543, Test accuracy: 91.78
Round  34, Train loss: -1.549, Test loss: 1.573, Test accuracy: 88.74
Round  35, Train loss: -1.295, Test loss: 1.544, Test accuracy: 91.68
Round  36, Train loss: -1.535, Test loss: 1.544, Test accuracy: 91.67
Round  37, Train loss: -1.911, Test loss: 1.570, Test accuracy: 89.16
Round  38, Train loss: -1.764, Test loss: 1.585, Test accuracy: 87.64
Round  39, Train loss: -2.454, Test loss: 1.569, Test accuracy: 89.17
Round  40, Train loss: -1.277, Test loss: 1.581, Test accuracy: 87.93
Round  41, Train loss: -1.704, Test loss: 1.567, Test accuracy: 89.38
Round  42, Train loss: -2.264, Test loss: 1.568, Test accuracy: 89.28
Round  43, Train loss: -1.997, Test loss: 1.553, Test accuracy: 90.77
Round  44, Train loss: -2.617, Test loss: 1.553, Test accuracy: 90.75
Round  45, Train loss: -1.518, Test loss: 1.556, Test accuracy: 90.53
Round  46, Train loss: -2.308, Test loss: 1.527, Test accuracy: 93.42
Round  47, Train loss: -1.482, Test loss: 1.539, Test accuracy: 92.21
Round  48, Train loss: -2.298, Test loss: 1.556, Test accuracy: 90.43
Round  49, Train loss: -1.191, Test loss: 1.543, Test accuracy: 91.88
Round  50, Train loss: -1.560, Test loss: 1.554, Test accuracy: 90.72
Round  51, Train loss: -1.741, Test loss: 1.554, Test accuracy: 90.75
Round  52, Train loss: -1.902, Test loss: 1.540, Test accuracy: 92.12
Round  53, Train loss: -2.035, Test loss: 1.540, Test accuracy: 92.13
Round  54, Train loss: -2.259, Test loss: 1.538, Test accuracy: 92.24
Round  55, Train loss: -2.628, Test loss: 1.542, Test accuracy: 91.90
Round  56, Train loss: -2.170, Test loss: 1.556, Test accuracy: 90.48
Round  57, Train loss: -2.107, Test loss: 1.556, Test accuracy: 90.54
Round  58, Train loss: -1.774, Test loss: 1.554, Test accuracy: 90.66
Round  59, Train loss: -1.533, Test loss: 1.524, Test accuracy: 93.70
Round  60, Train loss: -1.405, Test loss: 1.553, Test accuracy: 90.79
Round  61, Train loss: -1.459, Test loss: 1.566, Test accuracy: 89.55
Round  62, Train loss: -1.495, Test loss: 1.567, Test accuracy: 89.42
Round  63, Train loss: -2.124, Test loss: 1.550, Test accuracy: 91.10
Round  64, Train loss: -1.574, Test loss: 1.522, Test accuracy: 93.90
Round  65, Train loss: -1.244, Test loss: 1.522, Test accuracy: 93.93
Round  66, Train loss: -2.003, Test loss: 1.521, Test accuracy: 94.01
Round  67, Train loss: -1.624, Test loss: 1.520, Test accuracy: 94.06
Round  68, Train loss: -1.324, Test loss: 1.535, Test accuracy: 92.64
Round  69, Train loss: -1.566, Test loss: 1.548, Test accuracy: 91.35
Round  70, Train loss: -1.715, Test loss: 1.551, Test accuracy: 91.00
Round  71, Train loss: -1.357, Test loss: 1.552, Test accuracy: 90.88
Round  72, Train loss: -1.658, Test loss: 1.539, Test accuracy: 92.23
Round  73, Train loss: -2.143, Test loss: 1.524, Test accuracy: 93.69
Round  74, Train loss: -2.397, Test loss: 1.522, Test accuracy: 93.81
Round  75, Train loss: -1.718, Test loss: 1.508, Test accuracy: 95.26
Round  76, Train loss: -1.803, Test loss: 1.525, Test accuracy: 93.56
Round  77, Train loss: -1.650, Test loss: 1.523, Test accuracy: 93.73
Round  78, Train loss: -1.759, Test loss: 1.538, Test accuracy: 92.24
Round  79, Train loss: -1.930, Test loss: 1.539, Test accuracy: 92.15
Round  80, Train loss: -1.444, Test loss: 1.539, Test accuracy: 92.26
Round  81, Train loss: -2.016, Test loss: 1.525, Test accuracy: 93.64
Round  82, Train loss: -1.493, Test loss: 1.524, Test accuracy: 93.66
Round  83, Train loss: -2.052, Test loss: 1.510, Test accuracy: 95.12
Round  84, Train loss: -1.437, Test loss: 1.523, Test accuracy: 93.82
Round  85, Train loss: -1.361, Test loss: 1.551, Test accuracy: 91.03
Round  86, Train loss: -1.762, Test loss: 1.537, Test accuracy: 92.49
Round  87, Train loss: -1.858, Test loss: 1.564, Test accuracy: 89.69
Round  88, Train loss: -1.206, Test loss: 1.539, Test accuracy: 92.24
Round  89, Train loss: -1.473, Test loss: 1.551, Test accuracy: 91.06
Round  90, Train loss: -1.622, Test loss: 1.549, Test accuracy: 91.22
Round  91, Train loss: -1.853, Test loss: 1.523, Test accuracy: 93.83
Round  92, Train loss: -1.425, Test loss: 1.523, Test accuracy: 93.75
Round  93, Train loss: -1.578, Test loss: 1.537, Test accuracy: 92.36
Round  94, Train loss: -1.492, Test loss: 1.551, Test accuracy: 91.00
Round  95, Train loss: -1.241, Test loss: 1.525, Test accuracy: 93.62
Round  96, Train loss: -1.816, Test loss: 1.537, Test accuracy: 92.44
Round  97, Train loss: -1.538, Test loss: 1.524, Test accuracy: 93.77
Round  98, Train loss: -1.652, Test loss: 1.536, Test accuracy: 92.49
Round  99, Train loss: -1.553, Test loss: 1.522, Test accuracy: 93.92
Final Round, Train loss: 1.636, Test loss: 1.616, Test accuracy: 85.02
Average accuracy final 10 rounds: 92.84
Average global accuracy final 10 rounds: 92.84
967.7066383361816
[]
[23.958333333333332, 42.825, 33.583333333333336, 49.208333333333336, 46.35, 57.166666666666664, 53.36666666666667, 55.71666666666667, 58.49166666666667, 53.88333333333333, 56.266666666666666, 73.75833333333334, 74.375, 68.19166666666666, 67.25833333333334, 72.85, 70.225, 74.54166666666667, 80.9, 77.88333333333334, 82.05, 82.075, 83.59166666666667, 85.34166666666667, 85.40833333333333, 90.25833333333334, 90.10833333333333, 87.275, 88.76666666666667, 87.39166666666667, 85.65, 87.04166666666667, 90.18333333333334, 91.775, 88.74166666666666, 91.68333333333334, 91.66666666666667, 89.15833333333333, 87.64166666666667, 89.16666666666667, 87.93333333333334, 89.38333333333334, 89.28333333333333, 90.76666666666667, 90.75, 90.525, 93.425, 92.20833333333333, 90.43333333333334, 91.88333333333334, 90.71666666666667, 90.75, 92.11666666666666, 92.13333333333334, 92.24166666666666, 91.9, 90.48333333333333, 90.54166666666667, 90.65833333333333, 93.7, 90.79166666666667, 89.55, 89.41666666666667, 91.1, 93.9, 93.93333333333334, 94.00833333333334, 94.05833333333334, 92.64166666666667, 91.35, 91.0, 90.875, 92.23333333333333, 93.69166666666666, 93.80833333333334, 95.25833333333334, 93.55833333333334, 93.73333333333333, 92.24166666666666, 92.15, 92.25833333333334, 93.64166666666667, 93.65833333333333, 95.125, 93.81666666666666, 91.03333333333333, 92.49166666666666, 89.69166666666666, 92.24166666666666, 91.05833333333334, 91.21666666666667, 93.83333333333333, 93.75, 92.35833333333333, 91.0, 93.61666666666666, 92.44166666666666, 93.76666666666667, 92.49166666666666, 93.925, 85.01666666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round   0, Global train loss: 2.303, Global test loss: 2.304, Global test accuracy: 6.72
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round   1, Global train loss: 2.303, Global test loss: 2.304, Global test accuracy: 6.72
Round   2, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.72
Round   2, Global train loss: 2.301, Global test loss: 2.304, Global test accuracy: 6.71
Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round   3, Global train loss: 2.303, Global test loss: 2.304, Global test accuracy: 6.72
Round   4, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.72
Round   4, Global train loss: 2.304, Global test loss: 2.304, Global test accuracy: 6.72
Round   5, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round   5, Global train loss: 2.302, Global test loss: 2.304, Global test accuracy: 6.72
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.72
Round   7, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round   7, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71
Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71
Round   9, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round   9, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.72
Round  10, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  10, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.72
Round  11, Train loss: 2.305, Test loss: 2.303, Test accuracy: 6.72
Round  11, Global train loss: 2.305, Global test loss: 2.303, Global test accuracy: 6.72
Round  12, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.72
Round  12, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.72
Round  13, Train loss: 2.305, Test loss: 2.303, Test accuracy: 6.72
Round  13, Global train loss: 2.305, Global test loss: 2.303, Global test accuracy: 6.72
Round  14, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.72
Round  14, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.72
Round  15, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.72
Round  15, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.72
Round  16, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  16, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.72
Round  17, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.72
Round  17, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.72
Round  18, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  18, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.71
Round  19, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  19, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.72
Round  20, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  20, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.72
Round  21, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  21, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.72
Round  22, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  22, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.71
Round  23, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  23, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.72
Round  24, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.72
Round  24, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.72
Round  25, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  25, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.71
Round  26, Train loss: 2.305, Test loss: 2.303, Test accuracy: 6.72
Round  26, Global train loss: 2.305, Global test loss: 2.303, Global test accuracy: 6.72
Round  27, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  27, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71
Round  28, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  28, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  29, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.71
Round  29, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.69
Round  30, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.72
Round  30, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.69
Round  31, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  31, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.69
Round  32, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.72
Round  32, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.69
Round  33, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  33, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.69
Round  34, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.72
Round  34, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.70
Round  35, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  35, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.70
Round  36, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.72
Round  36, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.70
Round  37, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  37, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  38, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.72
Round  38, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.70
Round  39, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.72
Round  39, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.70
Round  40, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.71
Round  40, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  41, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.70
Round  41, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.70
Round  42, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.70
Round  42, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.70
Round  43, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.70
Round  43, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  44, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.72
Round  44, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.71
Round  45, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.73
Round  45, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.71
Round  46, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.73
Round  46, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.70
Round  47, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.73
Round  47, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  48, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.73
Round  48, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.72
Round  49, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.73
Round  49, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.72
Round  50, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.73
Round  50, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.72
Round  51, Train loss: 2.300, Test loss: 2.303, Test accuracy: 6.73
Round  51, Global train loss: 2.300, Global test loss: 2.303, Global test accuracy: 6.71
Round  52, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.73
Round  52, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71
Round  53, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.73
Round  53, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71
Round  54, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.73
Round  54, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.71
Round  55, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.73
Round  55, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.71
Round  56, Train loss: 2.305, Test loss: 2.303, Test accuracy: 6.73
Round  56, Global train loss: 2.305, Global test loss: 2.303, Global test accuracy: 6.71
Round  57, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.73
Round  57, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.72
Round  58, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.74
Round  58, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.72
Round  59, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.74
Round  59, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.72
Round  60, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.74
Round  60, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.72
Round  61, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.73
Round  61, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.72
Round  62, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.74
Round  62, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.72
Round  63, Train loss: 2.300, Test loss: 2.303, Test accuracy: 6.73
Round  63, Global train loss: 2.300, Global test loss: 2.303, Global test accuracy: 6.72
Round  64, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.73
Round  64, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.72
Round  65, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.73
Round  65, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.72
Round  66, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.73
Round  66, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.72
Round  67, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.73
Round  67, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.71
Round  68, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  68, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71
Round  69, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.72
Round  69, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.71
Round  70, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  70, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.71
Round  71, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  71, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71
Round  72, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.72
Round  72, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.70
Round  73, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.72
Round  73, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  74, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.71
Round  74, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  75, Train loss: 2.300, Test loss: 2.303, Test accuracy: 6.71
Round  75, Global train loss: 2.300, Global test loss: 2.303, Global test accuracy: 6.70
Round  76, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.71
Round  76, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  77, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.71
Round  77, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  78, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.70
Round  78, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.70
Round  79, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.70
Round  79, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.70
Round  80, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.70
Round  80, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.70
Round  81, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.70
Round  81, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.70
Round  82, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.70
Round  82, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.70
Round  83, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.71
Round  83, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  84, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  84, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.70
Round  85, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  85, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.71
Round  86, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.72
Round  86, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.71
Round  87, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.72
Round  87, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.71
Round  88, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.72
Round  88, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71
Round  89, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.72
Round  89, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.70
Round  90, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.72
Round  90, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.70
Round  91, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.72
Round  91, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  92, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.72
Round  92, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.70
Round  93, Train loss: 2.300, Test loss: 2.302, Test accuracy: 6.72
Round  93, Global train loss: 2.300, Global test loss: 2.303, Global test accuracy: 6.71
Round  94, Train loss: 2.299, Test loss: 2.302, Test accuracy: 6.72
Round  94, Global train loss: 2.299, Global test loss: 2.303, Global test accuracy: 6.71
Round  95, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.72
Round  95, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.71
Round  96, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.72
Round  96, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.71
Round  97, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.72
Round  97, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.72
Round  98, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71
Round  99, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.72
Round  99, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.71
Final Round, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.73
Final Round, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.71
Average accuracy final 10 rounds: 6.7175 

Average global accuracy final 10 rounds: 6.7058333333333335 

1127.8662459850311
[1.0564515590667725, 2.0231871604919434, 2.994029998779297, 3.9452877044677734, 4.898625135421753, 5.818025350570679, 6.781219244003296, 7.746460199356079, 8.696418046951294, 9.643178462982178, 10.592975378036499, 11.51261043548584, 12.478660106658936, 13.447825908660889, 14.400969505310059, 15.349140167236328, 16.304279327392578, 17.22454071044922, 18.17877769470215, 19.135789394378662, 20.097907066345215, 21.059295892715454, 22.00910782814026, 22.94077181816101, 23.901226043701172, 24.850980520248413, 25.802866458892822, 26.761423587799072, 27.708218812942505, 28.656405925750732, 29.612285614013672, 30.57711172103882, 31.527261972427368, 32.48350214958191, 33.437244176864624, 34.37970805168152, 35.33160424232483, 36.27996802330017, 37.24614238739014, 38.21070051193237, 39.16225528717041, 40.11440372467041, 41.064473152160645, 42.02185773849487, 42.96598219871521, 43.9219765663147, 44.88349103927612, 45.835774183273315, 46.755483627319336, 47.706286668777466, 48.66255807876587, 49.61806344985962, 50.572731494903564, 51.52612924575806, 52.46117401123047, 53.42357611656189, 54.371649503707886, 55.32890200614929, 56.28137969970703, 57.23826575279236, 58.15923023223877, 59.11901640892029, 60.06243109703064, 61.0141019821167, 61.9654061794281, 62.912436962127686, 63.82961106300354, 64.78291845321655, 65.73122429847717, 66.67397236824036, 67.62470388412476, 68.56063199043274, 69.49036383628845, 70.45725750923157, 71.40832543373108, 72.37216830253601, 73.32601261138916, 74.27406048774719, 75.18301892280579, 76.14676547050476, 77.10350298881531, 78.04754281044006, 78.99285650253296, 79.95399451255798, 80.87911796569824, 81.83806109428406, 82.80668807029724, 83.77032423019409, 84.71786952018738, 85.67355608940125, 86.603360414505, 87.57562041282654, 88.53026747703552, 89.51170039176941, 90.47275638580322, 91.43151807785034, 92.37033915519714, 93.33727598190308, 94.30108737945557, 95.25064539909363, 96.850417137146]
[6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.725, 6.725, 6.725, 6.725, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.708333333333333, 6.716666666666667, 6.716666666666667, 6.725, 6.725, 6.725, 6.716666666666667, 6.725, 6.725, 6.716666666666667, 6.716666666666667, 6.708333333333333, 6.7, 6.7, 6.7, 6.716666666666667, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.741666666666666, 6.741666666666666, 6.741666666666666, 6.733333333333333, 6.741666666666666, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.733333333333333, 6.725, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.708333333333333, 6.708333333333333, 6.708333333333333, 6.708333333333333, 6.7, 6.7, 6.7, 6.7, 6.7, 6.708333333333333, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.725, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.716666666666667, 6.733333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.289, Test loss: 2.300, Test accuracy: 15.47
Round   1, Train loss: 2.034, Test loss: 2.287, Test accuracy: 10.70
Round   2, Train loss: 1.852, Test loss: 2.270, Test accuracy: 16.52
Round   3, Train loss: 1.746, Test loss: 2.257, Test accuracy: 17.04
Round   4, Train loss: 1.880, Test loss: 2.260, Test accuracy: 18.82
Round   5, Train loss: 1.745, Test loss: 2.242, Test accuracy: 20.32
Round   6, Train loss: 1.700, Test loss: 2.251, Test accuracy: 19.24
Round   7, Train loss: 1.790, Test loss: 2.223, Test accuracy: 21.61
Round   8, Train loss: 1.683, Test loss: 2.232, Test accuracy: 20.09
Round   9, Train loss: 1.685, Test loss: 2.216, Test accuracy: 23.41
Round  10, Train loss: 1.644, Test loss: 2.230, Test accuracy: 20.68
Round  11, Train loss: 1.686, Test loss: 2.188, Test accuracy: 25.97
Round  12, Train loss: 1.616, Test loss: 2.195, Test accuracy: 25.02
Round  13, Train loss: 1.712, Test loss: 2.201, Test accuracy: 24.71
Round  14, Train loss: 1.657, Test loss: 2.193, Test accuracy: 25.41
Round  15, Train loss: 1.581, Test loss: 2.209, Test accuracy: 23.38
Round  16, Train loss: 1.578, Test loss: 2.153, Test accuracy: 30.27
Round  17, Train loss: 1.664, Test loss: 2.156, Test accuracy: 29.58
Round  18, Train loss: 1.703, Test loss: 2.170, Test accuracy: 27.83
Round  19, Train loss: 1.609, Test loss: 2.204, Test accuracy: 24.82
Round  20, Train loss: 1.609, Test loss: 2.191, Test accuracy: 25.38
Round  21, Train loss: 1.605, Test loss: 2.215, Test accuracy: 22.26
Round  22, Train loss: 1.706, Test loss: 2.221, Test accuracy: 22.56
Round  23, Train loss: 1.615, Test loss: 2.221, Test accuracy: 22.82
Round  24, Train loss: 1.608, Test loss: 2.160, Test accuracy: 28.57
Round  25, Train loss: 1.595, Test loss: 2.191, Test accuracy: 25.49
Round  26, Train loss: 1.604, Test loss: 2.164, Test accuracy: 28.93
Round  27, Train loss: 1.647, Test loss: 2.141, Test accuracy: 30.99
Round  28, Train loss: 1.641, Test loss: 2.161, Test accuracy: 29.54
Round  29, Train loss: 1.546, Test loss: 2.175, Test accuracy: 27.14
Round  30, Train loss: 1.649, Test loss: 2.138, Test accuracy: 31.24
Round  31, Train loss: 1.500, Test loss: 2.138, Test accuracy: 31.64
Round  32, Train loss: 1.646, Test loss: 2.195, Test accuracy: 24.14
Round  33, Train loss: 1.605, Test loss: 2.150, Test accuracy: 30.11
Round  34, Train loss: 1.541, Test loss: 2.154, Test accuracy: 29.91
Round  35, Train loss: 1.701, Test loss: 2.168, Test accuracy: 28.55
Round  36, Train loss: 1.648, Test loss: 2.220, Test accuracy: 23.07
Round  37, Train loss: 1.555, Test loss: 2.146, Test accuracy: 30.30
Round  38, Train loss: 1.600, Test loss: 2.157, Test accuracy: 29.76
Round  39, Train loss: 1.596, Test loss: 2.166, Test accuracy: 29.08
Round  40, Train loss: 1.697, Test loss: 2.193, Test accuracy: 25.57
Round  41, Train loss: 1.692, Test loss: 2.148, Test accuracy: 30.38
Round  42, Train loss: 1.544, Test loss: 2.148, Test accuracy: 30.60
Round  43, Train loss: 1.641, Test loss: 2.197, Test accuracy: 25.18
Round  44, Train loss: 1.591, Test loss: 2.156, Test accuracy: 29.52
Round  45, Train loss: 1.689, Test loss: 2.150, Test accuracy: 30.86
Round  46, Train loss: 1.598, Test loss: 2.123, Test accuracy: 33.61
Round  47, Train loss: 1.587, Test loss: 2.175, Test accuracy: 26.86
Round  48, Train loss: 1.642, Test loss: 2.138, Test accuracy: 32.06
Round  49, Train loss: 1.592, Test loss: 2.156, Test accuracy: 29.39
Round  50, Train loss: 1.689, Test loss: 2.197, Test accuracy: 25.68
Round  51, Train loss: 1.593, Test loss: 2.123, Test accuracy: 33.01
Round  52, Train loss: 1.691, Test loss: 2.154, Test accuracy: 29.62
Round  53, Train loss: 1.536, Test loss: 2.174, Test accuracy: 27.36
Round  54, Train loss: 1.536, Test loss: 2.156, Test accuracy: 29.47
Round  55, Train loss: 1.588, Test loss: 2.161, Test accuracy: 28.91
Round  56, Train loss: 1.589, Test loss: 2.154, Test accuracy: 29.88
Round  57, Train loss: 1.543, Test loss: 2.153, Test accuracy: 29.40
Round  58, Train loss: 1.593, Test loss: 2.165, Test accuracy: 28.82
Round  59, Train loss: 1.591, Test loss: 2.161, Test accuracy: 29.37
Round  60, Train loss: 1.585, Test loss: 2.170, Test accuracy: 28.16
Round  61, Train loss: 1.692, Test loss: 2.188, Test accuracy: 25.77
Round  62, Train loss: 1.482, Test loss: 2.152, Test accuracy: 30.21
Round  63, Train loss: 1.593, Test loss: 2.139, Test accuracy: 31.59
Round  64, Train loss: 1.532, Test loss: 2.145, Test accuracy: 30.81
Round  65, Train loss: 1.636, Test loss: 2.178, Test accuracy: 26.77
Round  66, Train loss: 1.582, Test loss: 2.168, Test accuracy: 28.06
Round  67, Train loss: 1.637, Test loss: 2.142, Test accuracy: 30.95
Round  68, Train loss: 1.532, Test loss: 2.126, Test accuracy: 32.57
Round  69, Train loss: 1.580, Test loss: 2.180, Test accuracy: 26.60
Round  70, Train loss: 1.541, Test loss: 2.131, Test accuracy: 32.31
Round  71, Train loss: 1.585, Test loss: 2.127, Test accuracy: 32.86
Round  72, Train loss: 1.530, Test loss: 2.124, Test accuracy: 32.73
Round  73, Train loss: 1.636, Test loss: 2.108, Test accuracy: 34.86
Round  74, Train loss: 1.634, Test loss: 2.143, Test accuracy: 31.03
Round  75, Train loss: 1.687, Test loss: 2.119, Test accuracy: 33.52
Round  76, Train loss: 1.527, Test loss: 2.114, Test accuracy: 34.17
Round  77, Train loss: 1.582, Test loss: 2.146, Test accuracy: 30.75
Round  78, Train loss: 1.529, Test loss: 2.136, Test accuracy: 31.73
Round  79, Train loss: 1.533, Test loss: 2.101, Test accuracy: 35.63
Round  80, Train loss: 1.634, Test loss: 2.139, Test accuracy: 31.68
Round  81, Train loss: 1.633, Test loss: 2.164, Test accuracy: 28.44
Round  82, Train loss: 1.527, Test loss: 2.126, Test accuracy: 32.79
Round  83, Train loss: 1.634, Test loss: 2.115, Test accuracy: 33.58
Round  84, Train loss: 1.475, Test loss: 2.132, Test accuracy: 32.02
Round  85, Train loss: 1.600, Test loss: 2.161, Test accuracy: 28.93
Round  86, Train loss: 1.583, Test loss: 2.136, Test accuracy: 31.75
Round  87, Train loss: 1.479, Test loss: 2.156, Test accuracy: 29.53
Round  88, Train loss: 1.524, Test loss: 2.164, Test accuracy: 28.45
Round  89, Train loss: 1.544, Test loss: 2.129, Test accuracy: 32.64
Round  90, Train loss: 1.479, Test loss: 2.128, Test accuracy: 31.88
Round  91, Train loss: 1.591, Test loss: 2.170, Test accuracy: 28.13
Round  92, Train loss: 1.537, Test loss: 2.177, Test accuracy: 26.68
Round  93, Train loss: 1.483, Test loss: 2.134, Test accuracy: 31.72
Round  94, Train loss: 1.480, Test loss: 2.167, Test accuracy: 28.05
Round  95, Train loss: 1.487, Test loss: 2.159, Test accuracy: 29.11
Round  96, Train loss: 1.536, Test loss: 2.151, Test accuracy: 29.67
Round  97, Train loss: 1.528, Test loss: 2.141, Test accuracy: 31.52
Round  98, Train loss: 1.475, Test loss: 2.132, Test accuracy: 32.12
Round  99, Train loss: 1.476, Test loss: 2.105, Test accuracy: 35.12
Final Round, Train loss: 1.499, Test loss: 2.108, Test accuracy: 34.79
Average accuracy final 10 rounds: 30.4
1709.6322515010834
[2.4644052982330322, 4.837758302688599, 7.27290678024292, 9.642468214035034, 12.061330795288086, 14.452299356460571, 16.84406328201294, 19.24450159072876, 21.62764000892639, 24.003939867019653, 26.358121633529663, 28.721426725387573, 31.101869106292725, 33.46704363822937, 35.90002655982971, 38.2514328956604, 40.60391855239868, 42.94890856742859, 45.3380241394043, 47.667073488235474, 50.03374123573303, 52.37228608131409, 54.73713827133179, 57.09613800048828, 59.44731402397156, 61.82327318191528, 64.18835258483887, 66.53171062469482, 68.90089631080627, 71.26450371742249, 73.63539862632751, 75.99747133255005, 78.3576455116272, 80.71696543693542, 83.09860873222351, 85.444584608078, 87.84103393554688, 90.21697735786438, 92.57966780662537, 94.98675632476807, 97.37817621231079, 99.7785074710846, 102.15774512290955, 104.55526351928711, 106.92829370498657, 109.3405032157898, 111.75903224945068, 114.12569117546082, 116.5146861076355, 118.8858289718628, 121.27516222000122, 123.61142706871033, 126.02342009544373, 128.3962824344635, 130.80062532424927, 133.15119791030884, 135.49619245529175, 137.86826753616333, 140.2202968597412, 142.58990240097046, 144.9708662033081, 147.33967471122742, 149.7451982498169, 152.1588327884674, 154.54880237579346, 156.92855954170227, 159.30483841896057, 161.6963014602661, 164.10064959526062, 166.47874903678894, 168.86220121383667, 171.1936812400818, 173.560932636261, 175.92524552345276, 178.26314616203308, 180.57724690437317, 182.92666244506836, 185.29176020622253, 187.65819668769836, 189.97751450538635, 192.35439133644104, 194.72246384620667, 197.10446882247925, 199.42630076408386, 201.83683562278748, 204.22776818275452, 206.61570858955383, 208.94913697242737, 211.34245419502258, 213.72996711730957, 216.10344982147217, 218.44674015045166, 220.8154752254486, 223.23911786079407, 225.6205506324768, 227.96136236190796, 230.36261796951294, 232.7342939376831, 235.11221289634705, 237.44100689888, 239.45047545433044]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[15.475, 10.7, 16.525, 17.041666666666668, 18.816666666666666, 20.325, 19.241666666666667, 21.608333333333334, 20.091666666666665, 23.408333333333335, 20.675, 25.966666666666665, 25.016666666666666, 24.708333333333332, 25.408333333333335, 23.375, 30.275, 29.583333333333332, 27.833333333333332, 24.825, 25.383333333333333, 22.258333333333333, 22.558333333333334, 22.816666666666666, 28.575, 25.491666666666667, 28.933333333333334, 30.991666666666667, 29.541666666666668, 27.141666666666666, 31.241666666666667, 31.641666666666666, 24.141666666666666, 30.108333333333334, 29.908333333333335, 28.55, 23.075, 30.3, 29.758333333333333, 29.083333333333332, 25.575, 30.383333333333333, 30.6, 25.175, 29.525, 30.858333333333334, 33.608333333333334, 26.858333333333334, 32.05833333333333, 29.391666666666666, 25.683333333333334, 33.00833333333333, 29.616666666666667, 27.358333333333334, 29.466666666666665, 28.908333333333335, 29.883333333333333, 29.4, 28.825, 29.366666666666667, 28.158333333333335, 25.775, 30.208333333333332, 31.591666666666665, 30.808333333333334, 26.775, 28.058333333333334, 30.95, 32.56666666666667, 26.6, 32.30833333333333, 32.858333333333334, 32.733333333333334, 34.858333333333334, 31.033333333333335, 33.516666666666666, 34.175, 30.75, 31.733333333333334, 35.63333333333333, 31.683333333333334, 28.441666666666666, 32.791666666666664, 33.583333333333336, 32.025, 28.933333333333334, 31.75, 29.533333333333335, 28.45, 32.641666666666666, 31.883333333333333, 28.133333333333333, 26.683333333333334, 31.716666666666665, 28.05, 29.108333333333334, 29.666666666666668, 31.516666666666666, 32.125, 35.11666666666667, 34.791666666666664]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.298, Test loss: 2.295, Test accuracy: 15.33
Round   1, Train loss: 2.207, Test loss: 2.261, Test accuracy: 21.48
Round   2, Train loss: 2.023, Test loss: 2.209, Test accuracy: 25.68
Round   3, Train loss: 1.971, Test loss: 2.174, Test accuracy: 32.07
Round   4, Train loss: 1.919, Test loss: 2.095, Test accuracy: 39.27
Round   5, Train loss: 1.950, Test loss: 2.077, Test accuracy: 38.83
Round   6, Train loss: 1.850, Test loss: 2.034, Test accuracy: 44.30
Round   7, Train loss: 1.937, Test loss: 2.029, Test accuracy: 44.54
Round   8, Train loss: 1.884, Test loss: 1.965, Test accuracy: 51.98
Round   9, Train loss: 1.764, Test loss: 1.927, Test accuracy: 57.11
Round  10, Train loss: 1.743, Test loss: 1.889, Test accuracy: 61.98
Round  11, Train loss: 1.687, Test loss: 1.857, Test accuracy: 64.90
Round  12, Train loss: 1.692, Test loss: 1.821, Test accuracy: 68.97
Round  13, Train loss: 1.777, Test loss: 1.770, Test accuracy: 74.44
Round  14, Train loss: 1.692, Test loss: 1.756, Test accuracy: 75.82
Round  15, Train loss: 1.582, Test loss: 1.737, Test accuracy: 76.84
Round  16, Train loss: 1.647, Test loss: 1.725, Test accuracy: 78.96
Round  17, Train loss: 1.831, Test loss: 1.716, Test accuracy: 81.49
Round  18, Train loss: 1.676, Test loss: 1.703, Test accuracy: 81.88
Round  19, Train loss: 1.717, Test loss: 1.700, Test accuracy: 82.14
Round  20, Train loss: 1.581, Test loss: 1.685, Test accuracy: 82.66
Round  21, Train loss: 1.527, Test loss: 1.668, Test accuracy: 83.08
Round  22, Train loss: 1.600, Test loss: 1.665, Test accuracy: 83.10
Round  23, Train loss: 1.679, Test loss: 1.659, Test accuracy: 84.92
Round  24, Train loss: 1.564, Test loss: 1.648, Test accuracy: 85.66
Round  25, Train loss: 1.511, Test loss: 1.643, Test accuracy: 85.46
Round  26, Train loss: 1.633, Test loss: 1.643, Test accuracy: 85.31
Round  27, Train loss: 1.627, Test loss: 1.644, Test accuracy: 85.25
Round  28, Train loss: 1.599, Test loss: 1.635, Test accuracy: 86.18
Round  29, Train loss: 1.569, Test loss: 1.632, Test accuracy: 86.36
Round  30, Train loss: 1.654, Test loss: 1.637, Test accuracy: 86.39
Round  31, Train loss: 1.675, Test loss: 1.641, Test accuracy: 86.00
Round  32, Train loss: 1.624, Test loss: 1.637, Test accuracy: 86.01
Round  33, Train loss: 1.562, Test loss: 1.630, Test accuracy: 86.52
Round  34, Train loss: 1.618, Test loss: 1.639, Test accuracy: 85.58
Round  35, Train loss: 1.622, Test loss: 1.621, Test accuracy: 87.87
Round  36, Train loss: 1.568, Test loss: 1.620, Test accuracy: 87.75
Round  37, Train loss: 1.572, Test loss: 1.605, Test accuracy: 88.58
Round  38, Train loss: 1.656, Test loss: 1.614, Test accuracy: 88.10
Round  39, Train loss: 1.497, Test loss: 1.603, Test accuracy: 88.70
Round  40, Train loss: 1.553, Test loss: 1.605, Test accuracy: 88.60
Round  41, Train loss: 1.493, Test loss: 1.607, Test accuracy: 88.19
Round  42, Train loss: 1.555, Test loss: 1.603, Test accuracy: 88.62
Round  43, Train loss: 1.549, Test loss: 1.598, Test accuracy: 88.78
Round  44, Train loss: 1.488, Test loss: 1.605, Test accuracy: 88.08
Round  45, Train loss: 1.615, Test loss: 1.610, Test accuracy: 87.80
Round  46, Train loss: 1.605, Test loss: 1.600, Test accuracy: 88.69
Round  47, Train loss: 1.600, Test loss: 1.597, Test accuracy: 88.91
Round  48, Train loss: 1.543, Test loss: 1.598, Test accuracy: 88.68
Round  49, Train loss: 1.655, Test loss: 1.602, Test accuracy: 88.88
Round  50, Train loss: 1.645, Test loss: 1.600, Test accuracy: 88.84
Round  51, Train loss: 1.543, Test loss: 1.596, Test accuracy: 89.09
Round  52, Train loss: 1.645, Test loss: 1.604, Test accuracy: 88.93
Round  53, Train loss: 1.542, Test loss: 1.597, Test accuracy: 89.25
Round  54, Train loss: 1.596, Test loss: 1.598, Test accuracy: 89.08
Round  55, Train loss: 1.543, Test loss: 1.594, Test accuracy: 89.08
Round  56, Train loss: 1.484, Test loss: 1.594, Test accuracy: 89.10
Round  57, Train loss: 1.589, Test loss: 1.601, Test accuracy: 88.82
Round  58, Train loss: 1.596, Test loss: 1.601, Test accuracy: 88.94
Round  59, Train loss: 1.601, Test loss: 1.596, Test accuracy: 89.14
Round  60, Train loss: 1.597, Test loss: 1.598, Test accuracy: 89.04
Round  61, Train loss: 1.494, Test loss: 1.592, Test accuracy: 89.00
Round  62, Train loss: 1.537, Test loss: 1.590, Test accuracy: 89.31
Round  63, Train loss: 1.489, Test loss: 1.591, Test accuracy: 88.79
Round  64, Train loss: 1.592, Test loss: 1.590, Test accuracy: 89.08
Round  65, Train loss: 1.534, Test loss: 1.590, Test accuracy: 89.08
Round  66, Train loss: 1.533, Test loss: 1.589, Test accuracy: 88.96
Round  67, Train loss: 1.483, Test loss: 1.587, Test accuracy: 89.12
Round  68, Train loss: 1.585, Test loss: 1.593, Test accuracy: 88.87
Round  69, Train loss: 1.535, Test loss: 1.597, Test accuracy: 88.39
Round  70, Train loss: 1.532, Test loss: 1.588, Test accuracy: 89.19
Round  71, Train loss: 1.576, Test loss: 1.601, Test accuracy: 87.67
Round  72, Train loss: 1.562, Test loss: 1.576, Test accuracy: 90.62
Round  73, Train loss: 1.557, Test loss: 1.567, Test accuracy: 92.06
Round  74, Train loss: 1.538, Test loss: 1.567, Test accuracy: 92.24
Round  75, Train loss: 1.546, Test loss: 1.585, Test accuracy: 90.44
Round  76, Train loss: 1.538, Test loss: 1.580, Test accuracy: 91.17
Round  77, Train loss: 1.538, Test loss: 1.568, Test accuracy: 91.95
Round  78, Train loss: 1.594, Test loss: 1.580, Test accuracy: 90.84
Round  79, Train loss: 1.585, Test loss: 1.576, Test accuracy: 91.42
Round  80, Train loss: 1.548, Test loss: 1.559, Test accuracy: 92.50
Round  81, Train loss: 1.481, Test loss: 1.557, Test accuracy: 92.42
Round  82, Train loss: 1.537, Test loss: 1.558, Test accuracy: 92.08
Round  83, Train loss: 1.482, Test loss: 1.558, Test accuracy: 92.10
Round  84, Train loss: 1.533, Test loss: 1.564, Test accuracy: 91.97
Round  85, Train loss: 1.531, Test loss: 1.565, Test accuracy: 91.98
Round  86, Train loss: 1.536, Test loss: 1.560, Test accuracy: 92.25
Round  87, Train loss: 1.537, Test loss: 1.571, Test accuracy: 91.00
Round  88, Train loss: 1.529, Test loss: 1.562, Test accuracy: 91.88
Round  89, Train loss: 1.487, Test loss: 1.556, Test accuracy: 92.33
Round  90, Train loss: 1.533, Test loss: 1.556, Test accuracy: 92.49
Round  91, Train loss: 1.534, Test loss: 1.555, Test accuracy: 92.66
Round  92, Train loss: 1.533, Test loss: 1.563, Test accuracy: 92.08
Round  93, Train loss: 1.483, Test loss: 1.559, Test accuracy: 92.21
Round  94, Train loss: 1.475, Test loss: 1.552, Test accuracy: 92.71/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.475, Test loss: 1.553, Test accuracy: 92.38
Round  96, Train loss: 1.477, Test loss: 1.551, Test accuracy: 92.67
Round  97, Train loss: 1.475, Test loss: 1.554, Test accuracy: 92.62
Round  98, Train loss: 1.533, Test loss: 1.551, Test accuracy: 92.86
Round  99, Train loss: 1.533, Test loss: 1.551, Test accuracy: 92.88
Final Round, Train loss: 1.521, Test loss: 1.545, Test accuracy: 92.98
Average accuracy final 10 rounds: 92.55499999999999
915.3742418289185
[1.2376489639282227, 2.2546379566192627, 3.310012102127075, 4.372267246246338, 5.439897537231445, 6.505514860153198, 7.573866605758667, 8.63935923576355, 9.677376747131348, 10.745864152908325, 11.801013469696045, 12.861622095108032, 13.91937804222107, 14.996417760848999, 16.08216881752014, 17.14090085029602, 18.161768436431885, 19.246213912963867, 20.31756091117859, 21.380151748657227, 22.440398931503296, 23.495375633239746, 24.56028723716736, 25.575207948684692, 26.62433624267578, 27.689881324768066, 28.758511066436768, 29.8255136013031, 30.88814353942871, 31.95891237258911, 33.03618812561035, 34.063074350357056, 35.12264108657837, 36.166518688201904, 37.256369829177856, 38.35726881027222, 39.423821210861206, 40.49290204048157, 41.505866289138794, 42.588374376297, 43.65335726737976, 44.722232818603516, 45.80242371559143, 46.84678602218628, 47.91029930114746, 48.919352531433105, 49.9756498336792, 51.04056119918823, 52.1267204284668, 53.18145298957825, 54.25065517425537, 55.326257944107056, 56.37664580345154, 57.38845896720886, 58.43536162376404, 59.499558210372925, 60.56519937515259, 61.675620317459106, 62.76400971412659, 63.82770586013794, 64.83596754074097, 65.88971972465515, 66.95987367630005, 68.00828218460083, 69.0702977180481, 70.13784956932068, 71.18840265274048, 72.23613429069519, 73.2443995475769, 74.31299948692322, 75.37696576118469, 76.43211889266968, 77.5027711391449, 78.57516407966614, 79.62218236923218, 80.63259029388428, 81.68891859054565, 82.74296569824219, 83.79996538162231, 84.83655667304993, 85.909823179245, 86.96016311645508, 88.02445793151855, 89.03344058990479, 90.07942247390747, 91.14547443389893, 92.20844602584839, 93.28095436096191, 94.32870149612427, 95.39957880973816, 96.4056887626648, 97.4771499633789, 98.51926898956299, 99.588045835495, 100.65378952026367, 101.71046996116638, 102.75760984420776, 103.79277467727661, 104.81444048881531, 105.88486838340759, 107.32400894165039]
[15.333333333333334, 21.475, 25.683333333333334, 32.06666666666667, 39.266666666666666, 38.833333333333336, 44.3, 44.541666666666664, 51.975, 57.108333333333334, 61.983333333333334, 64.9, 68.96666666666667, 74.44166666666666, 75.81666666666666, 76.84166666666667, 78.95833333333333, 81.49166666666666, 81.875, 82.14166666666667, 82.65833333333333, 83.075, 83.1, 84.925, 85.65833333333333, 85.45833333333333, 85.30833333333334, 85.25, 86.18333333333334, 86.35833333333333, 86.39166666666667, 86.0, 86.00833333333334, 86.51666666666667, 85.575, 87.86666666666666, 87.75, 88.575, 88.1, 88.7, 88.6, 88.19166666666666, 88.625, 88.775, 88.075, 87.8, 88.69166666666666, 88.90833333333333, 88.68333333333334, 88.88333333333334, 88.84166666666667, 89.09166666666667, 88.93333333333334, 89.25, 89.08333333333333, 89.08333333333333, 89.1, 88.81666666666666, 88.94166666666666, 89.14166666666667, 89.04166666666667, 89.0, 89.30833333333334, 88.79166666666667, 89.08333333333333, 89.075, 88.95833333333333, 89.11666666666666, 88.86666666666666, 88.39166666666667, 89.19166666666666, 87.675, 90.61666666666666, 92.05833333333334, 92.24166666666666, 90.44166666666666, 91.16666666666667, 91.95, 90.84166666666667, 91.41666666666667, 92.5, 92.41666666666667, 92.075, 92.1, 91.96666666666667, 91.98333333333333, 92.25, 91.0, 91.875, 92.33333333333333, 92.49166666666666, 92.65833333333333, 92.075, 92.20833333333333, 92.70833333333333, 92.38333333333334, 92.66666666666667, 92.625, 92.85833333333333, 92.875, 92.98333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.301, Test loss: 2.295, Test accuracy: 15.62
Round   1, Train loss: 2.198, Test loss: 2.258, Test accuracy: 24.44
Round   2, Train loss: 2.073, Test loss: 2.202, Test accuracy: 28.26
Round   3, Train loss: 1.994, Test loss: 2.151, Test accuracy: 33.06
Round   4, Train loss: 1.933, Test loss: 2.066, Test accuracy: 47.45
Round   5, Train loss: 1.894, Test loss: 2.019, Test accuracy: 52.77
Round   6, Train loss: 1.812, Test loss: 1.954, Test accuracy: 58.23
Round   7, Train loss: 1.884, Test loss: 1.911, Test accuracy: 63.72
Round   8, Train loss: 1.782, Test loss: 1.845, Test accuracy: 70.72
Round   9, Train loss: 1.733, Test loss: 1.804, Test accuracy: 73.31
Round  10, Train loss: 1.636, Test loss: 1.782, Test accuracy: 74.62
Round  11, Train loss: 1.740, Test loss: 1.755, Test accuracy: 76.65
Round  12, Train loss: 1.598, Test loss: 1.717, Test accuracy: 79.81
Round  13, Train loss: 1.620, Test loss: 1.708, Test accuracy: 81.17
Round  14, Train loss: 1.651, Test loss: 1.697, Test accuracy: 82.38
Round  15, Train loss: 1.628, Test loss: 1.680, Test accuracy: 83.68
Round  16, Train loss: 1.589, Test loss: 1.639, Test accuracy: 86.87
Round  17, Train loss: 1.587, Test loss: 1.628, Test accuracy: 87.31
Round  18, Train loss: 1.582, Test loss: 1.607, Test accuracy: 89.89
Round  19, Train loss: 1.531, Test loss: 1.609, Test accuracy: 89.88
Round  20, Train loss: 1.553, Test loss: 1.598, Test accuracy: 91.06
Round  21, Train loss: 1.571, Test loss: 1.579, Test accuracy: 93.12
Round  22, Train loss: 1.528, Test loss: 1.576, Test accuracy: 93.46
Round  23, Train loss: 1.588, Test loss: 1.571, Test accuracy: 93.88
Round  24, Train loss: 1.578, Test loss: 1.570, Test accuracy: 93.88
Round  25, Train loss: 1.552, Test loss: 1.562, Test accuracy: 94.47
Round  26, Train loss: 1.526, Test loss: 1.558, Test accuracy: 94.61
Round  27, Train loss: 1.579, Test loss: 1.555, Test accuracy: 94.59
Round  28, Train loss: 1.575, Test loss: 1.557, Test accuracy: 94.66
Round  29, Train loss: 1.510, Test loss: 1.554, Test accuracy: 94.63
Round  30, Train loss: 1.576, Test loss: 1.552, Test accuracy: 95.02
Round  31, Train loss: 1.526, Test loss: 1.546, Test accuracy: 95.07
Round  32, Train loss: 1.511, Test loss: 1.547, Test accuracy: 95.05
Round  33, Train loss: 1.520, Test loss: 1.543, Test accuracy: 95.22
Round  34, Train loss: 1.514, Test loss: 1.542, Test accuracy: 95.16
Round  35, Train loss: 1.560, Test loss: 1.547, Test accuracy: 94.86
Round  36, Train loss: 1.561, Test loss: 1.547, Test accuracy: 95.06
Round  37, Train loss: 1.507, Test loss: 1.541, Test accuracy: 95.28
Round  38, Train loss: 1.562, Test loss: 1.542, Test accuracy: 95.40
Round  39, Train loss: 1.562, Test loss: 1.541, Test accuracy: 95.50
Round  40, Train loss: 1.512, Test loss: 1.534, Test accuracy: 95.68
Round  41, Train loss: 1.504, Test loss: 1.537, Test accuracy: 95.43
Round  42, Train loss: 1.506, Test loss: 1.535, Test accuracy: 95.46
Round  43, Train loss: 1.545, Test loss: 1.534, Test accuracy: 95.68
Round  44, Train loss: 1.494, Test loss: 1.532, Test accuracy: 95.89
Round  45, Train loss: 1.548, Test loss: 1.538, Test accuracy: 95.28
Round  46, Train loss: 1.501, Test loss: 1.535, Test accuracy: 95.63
Round  47, Train loss: 1.547, Test loss: 1.535, Test accuracy: 95.40
Round  48, Train loss: 1.505, Test loss: 1.529, Test accuracy: 95.83
Round  49, Train loss: 1.496, Test loss: 1.533, Test accuracy: 95.34
Round  50, Train loss: 1.503, Test loss: 1.527, Test accuracy: 95.81
Round  51, Train loss: 1.539, Test loss: 1.531, Test accuracy: 95.54
Round  52, Train loss: 1.526, Test loss: 1.527, Test accuracy: 95.78
Round  53, Train loss: 1.494, Test loss: 1.525, Test accuracy: 95.93
Round  54, Train loss: 1.491, Test loss: 1.525, Test accuracy: 95.89
Round  55, Train loss: 1.521, Test loss: 1.521, Test accuracy: 96.97
Round  56, Train loss: 1.494, Test loss: 1.520, Test accuracy: 97.26
Round  57, Train loss: 1.516, Test loss: 1.515, Test accuracy: 97.31
Round  58, Train loss: 1.510, Test loss: 1.516, Test accuracy: 97.12
Round  59, Train loss: 1.492, Test loss: 1.515, Test accuracy: 97.33
Round  60, Train loss: 1.491, Test loss: 1.514, Test accuracy: 97.50
Round  61, Train loss: 1.489, Test loss: 1.515, Test accuracy: 97.42
Round  62, Train loss: 1.493, Test loss: 1.515, Test accuracy: 97.28
Round  63, Train loss: 1.494, Test loss: 1.514, Test accuracy: 97.53
Round  64, Train loss: 1.491, Test loss: 1.514, Test accuracy: 97.47
Round  65, Train loss: 1.508, Test loss: 1.512, Test accuracy: 97.42
Round  66, Train loss: 1.490, Test loss: 1.513, Test accuracy: 97.34
Round  67, Train loss: 1.489, Test loss: 1.516, Test accuracy: 96.96
Round  68, Train loss: 1.486, Test loss: 1.514, Test accuracy: 97.20
Round  69, Train loss: 1.488, Test loss: 1.513, Test accuracy: 97.54
Round  70, Train loss: 1.486, Test loss: 1.517, Test accuracy: 96.88
Round  71, Train loss: 1.500, Test loss: 1.513, Test accuracy: 97.33
Round  72, Train loss: 1.486, Test loss: 1.516, Test accuracy: 97.17
Round  73, Train loss: 1.499, Test loss: 1.509, Test accuracy: 97.58
Round  74, Train loss: 1.488, Test loss: 1.510, Test accuracy: 97.38
Round  75, Train loss: 1.488, Test loss: 1.509, Test accuracy: 97.68
Round  76, Train loss: 1.486, Test loss: 1.510, Test accuracy: 97.56
Round  77, Train loss: 1.495, Test loss: 1.511, Test accuracy: 97.22
Round  78, Train loss: 1.492, Test loss: 1.510, Test accuracy: 97.39
Round  79, Train loss: 1.490, Test loss: 1.509, Test accuracy: 97.47
Round  80, Train loss: 1.491, Test loss: 1.506, Test accuracy: 97.68
Round  81, Train loss: 1.486, Test loss: 1.505, Test accuracy: 97.83
Round  82, Train loss: 1.483, Test loss: 1.506, Test accuracy: 97.80
Round  83, Train loss: 1.490, Test loss: 1.508, Test accuracy: 97.68
Round  84, Train loss: 1.486, Test loss: 1.510, Test accuracy: 97.43
Round  85, Train loss: 1.483, Test loss: 1.506, Test accuracy: 97.79
Round  86, Train loss: 1.486, Test loss: 1.507, Test accuracy: 97.74
Round  87, Train loss: 1.483, Test loss: 1.505, Test accuracy: 97.92
Round  88, Train loss: 1.483, Test loss: 1.508, Test accuracy: 97.69
Round  89, Train loss: 1.485, Test loss: 1.506, Test accuracy: 97.67
Round  90, Train loss: 1.481, Test loss: 1.505, Test accuracy: 97.92
Round  91, Train loss: 1.487, Test loss: 1.505, Test accuracy: 97.81
Round  92, Train loss: 1.482, Test loss: 1.510, Test accuracy: 97.27
Round  93, Train loss: 1.488, Test loss: 1.504, Test accuracy: 97.96/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.484, Test loss: 1.507, Test accuracy: 97.71
Round  95, Train loss: 1.486, Test loss: 1.506, Test accuracy: 97.60
Round  96, Train loss: 1.483, Test loss: 1.504, Test accuracy: 97.88
Round  97, Train loss: 1.480, Test loss: 1.506, Test accuracy: 97.65
Round  98, Train loss: 1.484, Test loss: 1.503, Test accuracy: 97.92
Round  99, Train loss: 1.482, Test loss: 1.503, Test accuracy: 97.73
Final Round, Train loss: 1.474, Test loss: 1.500, Test accuracy: 97.94
Average accuracy final 10 rounds: 97.745
1192.4110491275787
[1.3191959857940674, 2.6383919715881348, 3.673286199569702, 4.7081804275512695, 5.7751243114471436, 6.842068195343018, 7.912728548049927, 8.983388900756836, 10.05019211769104, 11.116995334625244, 12.189236402511597, 13.26147747039795, 14.32475996017456, 15.388042449951172, 16.42919683456421, 17.470351219177246, 18.517877340316772, 19.5654034614563, 20.630779027938843, 21.696154594421387, 22.765167236328125, 23.834179878234863, 24.87260675430298, 25.911033630371094, 26.923893451690674, 27.936753273010254, 29.00027322769165, 30.063793182373047, 31.125164270401, 32.186535358428955, 33.24778079986572, 34.30902624130249, 35.37454676628113, 36.440067291259766, 37.49403476715088, 38.54800224304199, 39.556784868240356, 40.56556749343872, 41.63441801071167, 42.70326852798462, 43.812849283218384, 44.92243003845215, 45.990989685058594, 47.05954933166504, 48.13121151924133, 49.20287370681763, 50.27153992652893, 51.340206146240234, 52.38054895401001, 53.420891761779785, 54.499329566955566, 55.57776737213135, 56.65665650367737, 57.73554563522339, 58.792428493499756, 59.84931135177612, 60.93146252632141, 62.0136137008667, 63.02742838859558, 64.04124307632446, 65.1160192489624, 66.19079542160034, 67.25561380386353, 68.32043218612671, 69.40026235580444, 70.48009252548218, 71.54391241073608, 72.60773229598999, 73.66920638084412, 74.73068046569824, 75.73441219329834, 76.73814392089844, 77.79318952560425, 78.84823513031006, 79.911367893219, 80.97450065612793, 82.04800033569336, 83.12150001525879, 84.18672895431519, 85.25195789337158, 86.37406969070435, 87.49618148803711, 88.56006193161011, 89.6239423751831, 90.68642020225525, 91.74889802932739, 92.81314587593079, 93.87739372253418, 94.93629479408264, 95.9951958656311, 97.06808710098267, 98.14097833633423, 99.14955878257751, 100.1581392288208, 101.21784472465515, 102.2775502204895, 103.34539985656738, 104.41324949264526, 105.47554087638855, 106.53783226013184, 107.57913398742676, 108.62043571472168, 109.69519233703613, 110.76994895935059, 111.78645586967468, 112.80296277999878, 113.87126421928406, 114.93956565856934, 115.98608422279358, 117.03260278701782, 118.11409711837769, 119.19559144973755, 120.26742148399353, 121.33925151824951, 122.38842916488647, 123.43760681152344, 124.45188689231873, 125.46616697311401, 126.52857851982117, 127.59099006652832, 128.68126034736633, 129.77153062820435, 130.83653211593628, 131.9015336036682, 132.97661972045898, 134.05170583724976, 135.1270730495453, 136.20244026184082, 137.270765542984, 138.3390908241272, 139.43383264541626, 140.52857446670532, 141.6378881931305, 142.74720191955566, 143.80459332466125, 144.86198472976685, 145.9259557723999, 146.98992681503296, 147.99926829338074, 149.00860977172852, 150.06476092338562, 151.12091207504272, 152.17299818992615, 153.22508430480957, 154.2735424041748, 155.32200050354004, 156.3828160762787, 157.44363164901733, 158.49791264533997, 159.5521936416626, 160.56639552116394, 161.58059740066528, 162.63750672340393, 163.69441604614258, 164.76860117912292, 165.84278631210327, 166.95453310012817, 168.06627988815308, 169.14088821411133, 170.21549654006958, 171.28372955322266, 172.35196256637573, 173.4185254573822, 174.48508834838867, 175.55882120132446, 176.63255405426025, 177.7059531211853, 178.77935218811035, 179.86385869979858, 180.94836521148682, 182.0254111289978, 183.1024570465088, 184.11528635025024, 185.1281156539917, 186.20277857780457, 187.27744150161743, 188.35359692573547, 189.42975234985352, 190.51099181175232, 191.59223127365112, 192.66936135292053, 193.74649143218994, 194.78698635101318, 195.82748126983643, 196.84064960479736, 197.8538179397583, 198.92857003211975, 200.0033221244812, 201.07875514030457, 202.15418815612793, 203.2229495048523, 204.29171085357666, 205.34152483940125, 206.39133882522583, 207.46966242790222, 208.5479860305786, 209.58421325683594, 210.62044048309326, 211.69769740104675, 212.77495431900024, 214.2102348804474, 215.64551544189453]
[15.616666666666667, 15.616666666666667, 24.441666666666666, 24.441666666666666, 28.258333333333333, 28.258333333333333, 33.05833333333333, 33.05833333333333, 47.45, 47.45, 52.766666666666666, 52.766666666666666, 58.233333333333334, 58.233333333333334, 63.71666666666667, 63.71666666666667, 70.71666666666667, 70.71666666666667, 73.30833333333334, 73.30833333333334, 74.61666666666666, 74.61666666666666, 76.65, 76.65, 79.80833333333334, 79.80833333333334, 81.16666666666667, 81.16666666666667, 82.375, 82.375, 83.68333333333334, 83.68333333333334, 86.86666666666666, 86.86666666666666, 87.30833333333334, 87.30833333333334, 89.89166666666667, 89.89166666666667, 89.875, 89.875, 91.05833333333334, 91.05833333333334, 93.125, 93.125, 93.45833333333333, 93.45833333333333, 93.88333333333334, 93.88333333333334, 93.875, 93.875, 94.475, 94.475, 94.60833333333333, 94.60833333333333, 94.59166666666667, 94.59166666666667, 94.65833333333333, 94.65833333333333, 94.63333333333334, 94.63333333333334, 95.01666666666667, 95.01666666666667, 95.06666666666666, 95.06666666666666, 95.05, 95.05, 95.21666666666667, 95.21666666666667, 95.15833333333333, 95.15833333333333, 94.85833333333333, 94.85833333333333, 95.05833333333334, 95.05833333333334, 95.28333333333333, 95.28333333333333, 95.4, 95.4, 95.5, 95.5, 95.68333333333334, 95.68333333333334, 95.43333333333334, 95.43333333333334, 95.45833333333333, 95.45833333333333, 95.68333333333334, 95.68333333333334, 95.89166666666667, 95.89166666666667, 95.28333333333333, 95.28333333333333, 95.63333333333334, 95.63333333333334, 95.4, 95.4, 95.83333333333333, 95.83333333333333, 95.34166666666667, 95.34166666666667, 95.80833333333334, 95.80833333333334, 95.54166666666667, 95.54166666666667, 95.775, 95.775, 95.93333333333334, 95.93333333333334, 95.89166666666667, 95.89166666666667, 96.96666666666667, 96.96666666666667, 97.25833333333334, 97.25833333333334, 97.30833333333334, 97.30833333333334, 97.125, 97.125, 97.33333333333333, 97.33333333333333, 97.5, 97.5, 97.425, 97.425, 97.275, 97.275, 97.525, 97.525, 97.46666666666667, 97.46666666666667, 97.41666666666667, 97.41666666666667, 97.34166666666667, 97.34166666666667, 96.95833333333333, 96.95833333333333, 97.2, 97.2, 97.54166666666667, 97.54166666666667, 96.875, 96.875, 97.33333333333333, 97.33333333333333, 97.175, 97.175, 97.58333333333333, 97.58333333333333, 97.375, 97.375, 97.68333333333334, 97.68333333333334, 97.55833333333334, 97.55833333333334, 97.225, 97.225, 97.39166666666667, 97.39166666666667, 97.46666666666667, 97.46666666666667, 97.68333333333334, 97.68333333333334, 97.825, 97.825, 97.8, 97.8, 97.68333333333334, 97.68333333333334, 97.43333333333334, 97.43333333333334, 97.79166666666667, 97.79166666666667, 97.74166666666666, 97.74166666666666, 97.91666666666667, 97.91666666666667, 97.69166666666666, 97.69166666666666, 97.66666666666667, 97.66666666666667, 97.925, 97.925, 97.80833333333334, 97.80833333333334, 97.26666666666667, 97.26666666666667, 97.95833333333333, 97.95833333333333, 97.70833333333333, 97.70833333333333, 97.6, 97.6, 97.88333333333334, 97.88333333333334, 97.65, 97.65, 97.91666666666667, 97.91666666666667, 97.73333333333333, 97.73333333333333, 97.94166666666666, 97.94166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.202, Test loss: 2.206, Test accuracy: 19.74
Round   0, Global train loss: 2.202, Global test loss: 2.299, Global test accuracy: 10.00
Round   1, Train loss: 1.965, Test loss: 2.038, Test accuracy: 43.74
Round   1, Global train loss: 1.965, Global test loss: 2.286, Global test accuracy: 16.00
Round   2, Train loss: 1.690, Test loss: 1.918, Test accuracy: 56.42
Round   2, Global train loss: 1.690, Global test loss: 2.262, Global test accuracy: 18.04
Round   3, Train loss: 1.558, Test loss: 1.860, Test accuracy: 59.72
Round   3, Global train loss: 1.558, Global test loss: 2.269, Global test accuracy: 16.44
Round   4, Train loss: 1.708, Test loss: 1.783, Test accuracy: 68.67
Round   4, Global train loss: 1.708, Global test loss: 2.276, Global test accuracy: 16.41
Round   5, Train loss: 1.706, Test loss: 1.722, Test accuracy: 74.45
Round   5, Global train loss: 1.706, Global test loss: 2.297, Global test accuracy: 12.68
Round   6, Train loss: 1.588, Test loss: 1.670, Test accuracy: 79.84
Round   6, Global train loss: 1.588, Global test loss: 2.283, Global test accuracy: 15.21
Round   7, Train loss: 1.546, Test loss: 1.672, Test accuracy: 79.59
Round   7, Global train loss: 1.546, Global test loss: 2.260, Global test accuracy: 19.22
Round   8, Train loss: 1.538, Test loss: 1.667, Test accuracy: 79.70
Round   8, Global train loss: 1.538, Global test loss: 2.276, Global test accuracy: 15.25
Round   9, Train loss: 1.563, Test loss: 1.624, Test accuracy: 84.58
Round   9, Global train loss: 1.563, Global test loss: 2.239, Global test accuracy: 21.75
Round  10, Train loss: 1.477, Test loss: 1.624, Test accuracy: 84.46
Round  10, Global train loss: 1.477, Global test loss: 2.258, Global test accuracy: 18.41
Round  11, Train loss: 1.626, Test loss: 1.589, Test accuracy: 87.77
Round  11, Global train loss: 1.626, Global test loss: 2.286, Global test accuracy: 14.21
Round  12, Train loss: 1.477, Test loss: 1.588, Test accuracy: 87.76
Round  12, Global train loss: 1.477, Global test loss: 2.287, Global test accuracy: 11.46
Round  13, Train loss: 1.643, Test loss: 1.588, Test accuracy: 87.71
Round  13, Global train loss: 1.643, Global test loss: 2.273, Global test accuracy: 17.79
Round  14, Train loss: 1.578, Test loss: 1.586, Test accuracy: 87.83
Round  14, Global train loss: 1.578, Global test loss: 2.274, Global test accuracy: 15.97
Round  15, Train loss: 1.532, Test loss: 1.585, Test accuracy: 87.91
Round  15, Global train loss: 1.532, Global test loss: 2.286, Global test accuracy: 14.16
Round  16, Train loss: 1.583, Test loss: 1.584, Test accuracy: 87.97
Round  16, Global train loss: 1.583, Global test loss: 2.277, Global test accuracy: 16.88
Round  17, Train loss: 1.526, Test loss: 1.584, Test accuracy: 87.89
Round  17, Global train loss: 1.526, Global test loss: 2.289, Global test accuracy: 13.78
Round  18, Train loss: 1.618, Test loss: 1.554, Test accuracy: 91.15
Round  18, Global train loss: 1.618, Global test loss: 2.262, Global test accuracy: 18.87
Round  19, Train loss: 1.498, Test loss: 1.539, Test accuracy: 92.73
Round  19, Global train loss: 1.498, Global test loss: 2.279, Global test accuracy: 15.17
Round  20, Train loss: 1.473, Test loss: 1.537, Test accuracy: 92.80
Round  20, Global train loss: 1.473, Global test loss: 2.240, Global test accuracy: 21.70
Round  21, Train loss: 1.474, Test loss: 1.537, Test accuracy: 92.82
Round  21, Global train loss: 1.474, Global test loss: 2.272, Global test accuracy: 15.06
Round  22, Train loss: 1.471, Test loss: 1.537, Test accuracy: 92.79
Round  22, Global train loss: 1.471, Global test loss: 2.258, Global test accuracy: 16.68
Round  23, Train loss: 1.474, Test loss: 1.536, Test accuracy: 92.76
Round  23, Global train loss: 1.474, Global test loss: 2.313, Global test accuracy: 11.22
Round  24, Train loss: 1.526, Test loss: 1.536, Test accuracy: 92.76
Round  24, Global train loss: 1.526, Global test loss: 2.302, Global test accuracy: 12.64
Round  25, Train loss: 1.469, Test loss: 1.536, Test accuracy: 92.78
Round  25, Global train loss: 1.469, Global test loss: 2.258, Global test accuracy: 18.18
Round  26, Train loss: 1.526, Test loss: 1.536, Test accuracy: 92.75
Round  26, Global train loss: 1.526, Global test loss: 2.271, Global test accuracy: 15.47
Round  27, Train loss: 1.521, Test loss: 1.536, Test accuracy: 92.75
Round  27, Global train loss: 1.521, Global test loss: 2.295, Global test accuracy: 12.14
Round  28, Train loss: 1.468, Test loss: 1.536, Test accuracy: 92.75
Round  28, Global train loss: 1.468, Global test loss: 2.312, Global test accuracy: 11.28
Round  29, Train loss: 1.469, Test loss: 1.535, Test accuracy: 92.77
Round  29, Global train loss: 1.469, Global test loss: 2.312, Global test accuracy: 11.93
Round  30, Train loss: 1.522, Test loss: 1.527, Test accuracy: 93.91
Round  30, Global train loss: 1.522, Global test loss: 2.266, Global test accuracy: 17.19
Round  31, Train loss: 1.468, Test loss: 1.527, Test accuracy: 93.94
Round  31, Global train loss: 1.468, Global test loss: 2.290, Global test accuracy: 15.39
Round  32, Train loss: 1.470, Test loss: 1.527, Test accuracy: 93.95
Round  32, Global train loss: 1.470, Global test loss: 2.279, Global test accuracy: 16.27
Round  33, Train loss: 1.467, Test loss: 1.527, Test accuracy: 93.94
Round  33, Global train loss: 1.467, Global test loss: 2.244, Global test accuracy: 21.33
Round  34, Train loss: 1.471, Test loss: 1.527, Test accuracy: 93.92
Round  34, Global train loss: 1.471, Global test loss: 2.280, Global test accuracy: 13.93
Round  35, Train loss: 1.531, Test loss: 1.520, Test accuracy: 94.36
Round  35, Global train loss: 1.531, Global test loss: 2.260, Global test accuracy: 18.24
Round  36, Train loss: 1.469, Test loss: 1.520, Test accuracy: 94.39
Round  36, Global train loss: 1.469, Global test loss: 2.270, Global test accuracy: 16.69
Round  37, Train loss: 1.470, Test loss: 1.520, Test accuracy: 94.38
Round  37, Global train loss: 1.470, Global test loss: 2.280, Global test accuracy: 13.81
Round  38, Train loss: 1.520, Test loss: 1.519, Test accuracy: 94.40
Round  38, Global train loss: 1.520, Global test loss: 2.267, Global test accuracy: 16.57
Round  39, Train loss: 1.469, Test loss: 1.519, Test accuracy: 94.42
Round  39, Global train loss: 1.469, Global test loss: 2.267, Global test accuracy: 17.93
Round  40, Train loss: 1.470, Test loss: 1.519, Test accuracy: 94.38
Round  40, Global train loss: 1.470, Global test loss: 2.258, Global test accuracy: 19.54
Round  41, Train loss: 1.489, Test loss: 1.505, Test accuracy: 95.97
Round  41, Global train loss: 1.489, Global test loss: 2.253, Global test accuracy: 17.82
Round  42, Train loss: 1.468, Test loss: 1.504, Test accuracy: 95.98
Round  42, Global train loss: 1.468, Global test loss: 2.325, Global test accuracy: 10.53
Round  43, Train loss: 1.467, Test loss: 1.504, Test accuracy: 96.03
Round  43, Global train loss: 1.467, Global test loss: 2.250, Global test accuracy: 20.55
Round  44, Train loss: 1.469, Test loss: 1.504, Test accuracy: 96.02
Round  44, Global train loss: 1.469, Global test loss: 2.281, Global test accuracy: 16.52
Round  45, Train loss: 1.468, Test loss: 1.504, Test accuracy: 96.01
Round  45, Global train loss: 1.468, Global test loss: 2.266, Global test accuracy: 16.29
Round  46, Train loss: 1.468, Test loss: 1.504, Test accuracy: 96.00
Round  46, Global train loss: 1.468, Global test loss: 2.295, Global test accuracy: 11.81
Round  47, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.03
Round  47, Global train loss: 1.467, Global test loss: 2.305, Global test accuracy: 12.00
Round  48, Train loss: 1.465, Test loss: 1.503, Test accuracy: 96.05
Round  48, Global train loss: 1.465, Global test loss: 2.263, Global test accuracy: 19.72
Round  49, Train loss: 1.465, Test loss: 1.503, Test accuracy: 96.05
Round  49, Global train loss: 1.465, Global test loss: 2.307, Global test accuracy: 15.00
Round  50, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.06
Round  50, Global train loss: 1.467, Global test loss: 2.262, Global test accuracy: 20.20
Round  51, Train loss: 1.469, Test loss: 1.503, Test accuracy: 96.07
Round  51, Global train loss: 1.469, Global test loss: 2.294, Global test accuracy: 12.00
Round  52, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.06
Round  52, Global train loss: 1.466, Global test loss: 2.263, Global test accuracy: 18.20
Round  53, Train loss: 1.470, Test loss: 1.503, Test accuracy: 96.05
Round  53, Global train loss: 1.470, Global test loss: 2.292, Global test accuracy: 12.96
Round  54, Train loss: 1.468, Test loss: 1.503, Test accuracy: 96.03
Round  54, Global train loss: 1.468, Global test loss: 2.255, Global test accuracy: 17.37
Round  55, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.07
Round  55, Global train loss: 1.467, Global test loss: 2.283, Global test accuracy: 14.88
Round  56, Train loss: 1.464, Test loss: 1.503, Test accuracy: 96.07
Round  56, Global train loss: 1.464, Global test loss: 2.296, Global test accuracy: 10.41
Round  57, Train loss: 1.465, Test loss: 1.503, Test accuracy: 96.06
Round  57, Global train loss: 1.465, Global test loss: 2.269, Global test accuracy: 16.22
Round  58, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.05
Round  58, Global train loss: 1.466, Global test loss: 2.241, Global test accuracy: 20.02
Round  59, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.03
Round  59, Global train loss: 1.466, Global test loss: 2.265, Global test accuracy: 17.31
Round  60, Train loss: 1.468, Test loss: 1.503, Test accuracy: 96.04
Round  60, Global train loss: 1.468, Global test loss: 2.295, Global test accuracy: 11.86
Round  61, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.03
Round  61, Global train loss: 1.467, Global test loss: 2.251, Global test accuracy: 18.15
Round  62, Train loss: 1.469, Test loss: 1.503, Test accuracy: 96.02
Round  62, Global train loss: 1.469, Global test loss: 2.284, Global test accuracy: 14.18
Round  63, Train loss: 1.464, Test loss: 1.503, Test accuracy: 96.02
Round  63, Global train loss: 1.464, Global test loss: 2.270, Global test accuracy: 16.80
Round  64, Train loss: 1.464, Test loss: 1.503, Test accuracy: 96.01
Round  64, Global train loss: 1.464, Global test loss: 2.299, Global test accuracy: 12.88
Round  65, Train loss: 1.468, Test loss: 1.503, Test accuracy: 96.03
Round  65, Global train loss: 1.468, Global test loss: 2.270, Global test accuracy: 17.96
Round  66, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.02
Round  66, Global train loss: 1.466, Global test loss: 2.295, Global test accuracy: 13.74
Round  67, Train loss: 1.469, Test loss: 1.503, Test accuracy: 95.98
Round  67, Global train loss: 1.469, Global test loss: 2.284, Global test accuracy: 14.12
Round  68, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.00
Round  68, Global train loss: 1.467, Global test loss: 2.267, Global test accuracy: 15.99
Round  69, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.00
Round  69, Global train loss: 1.467, Global test loss: 2.263, Global test accuracy: 17.07
Round  70, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.00
Round  70, Global train loss: 1.466, Global test loss: 2.261, Global test accuracy: 18.22
Round  71, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.00
Round  71, Global train loss: 1.466, Global test loss: 2.282, Global test accuracy: 15.68
Round  72, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.02
Round  72, Global train loss: 1.467, Global test loss: 2.260, Global test accuracy: 17.97
Round  73, Train loss: 1.468, Test loss: 1.503, Test accuracy: 96.01
Round  73, Global train loss: 1.468, Global test loss: 2.241, Global test accuracy: 21.98
Round  74, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.02
Round  74, Global train loss: 1.466, Global test loss: 2.302, Global test accuracy: 10.50
Round  75, Train loss: 1.468, Test loss: 1.503, Test accuracy: 95.96
Round  75, Global train loss: 1.468, Global test loss: 2.296, Global test accuracy: 13.71
Round  76, Train loss: 1.464, Test loss: 1.503, Test accuracy: 95.96
Round  76, Global train loss: 1.464, Global test loss: 2.235, Global test accuracy: 23.22
Round  77, Train loss: 1.468, Test loss: 1.503, Test accuracy: 95.96
Round  77, Global train loss: 1.468, Global test loss: 2.267, Global test accuracy: 14.57
Round  78, Train loss: 1.468, Test loss: 1.503, Test accuracy: 96.00
Round  78, Global train loss: 1.468, Global test loss: 2.264, Global test accuracy: 18.07
Round  79, Train loss: 1.470, Test loss: 1.503, Test accuracy: 96.01
Round  79, Global train loss: 1.470, Global test loss: 2.274, Global test accuracy: 15.74
Round  80, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.01
Round  80, Global train loss: 1.466, Global test loss: 2.261, Global test accuracy: 16.73
Round  81, Train loss: 1.465, Test loss: 1.503, Test accuracy: 96.02
Round  81, Global train loss: 1.465, Global test loss: 2.262, Global test accuracy: 16.99
Round  82, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.01
Round  82, Global train loss: 1.467, Global test loss: 2.279, Global test accuracy: 16.68
Round  83, Train loss: 1.469, Test loss: 1.503, Test accuracy: 96.02
Round  83, Global train loss: 1.469, Global test loss: 2.266, Global test accuracy: 17.73
Round  84, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.02
Round  84, Global train loss: 1.467, Global test loss: 2.308, Global test accuracy: 12.61
Round  85, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.03
Round  85, Global train loss: 1.466, Global test loss: 2.275, Global test accuracy: 16.98
Round  86, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.04
Round  86, Global train loss: 1.467, Global test loss: 2.298, Global test accuracy: 13.59
Round  87, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.05
Round  87, Global train loss: 1.467, Global test loss: 2.305, Global test accuracy: 11.18
Round  88, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.03
Round  88, Global train loss: 1.467, Global test loss: 2.275, Global test accuracy: 15.11
Round  89, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.03
Round  89, Global train loss: 1.467, Global test loss: 2.272, Global test accuracy: 16.65
Round  90, Train loss: 1.464, Test loss: 1.503, Test accuracy: 96.03
Round  90, Global train loss: 1.464, Global test loss: 2.261, Global test accuracy: 19.07
Round  91, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.03
Round  91, Global train loss: 1.467, Global test loss: 2.246, Global test accuracy: 21.85
Round  92, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.03
Round  92, Global train loss: 1.466, Global test loss: 2.278, Global test accuracy: 15.60
Round  93, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.03
Round  93, Global train loss: 1.467, Global test loss: 2.312, Global test accuracy: 12.09
Round  94, Train loss: 1.468, Test loss: 1.503, Test accuracy: 96.03
Round  94, Global train loss: 1.468, Global test loss: 2.260, Global test accuracy: 18.75
Round  95, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.03
Round  95, Global train loss: 1.466, Global test loss: 2.257, Global test accuracy: 18.32/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.03
Round  96, Global train loss: 1.467, Global test loss: 2.286, Global test accuracy: 15.12
Round  97, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.03
Round  97, Global train loss: 1.466, Global test loss: 2.261, Global test accuracy: 17.52
Round  98, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.03
Round  98, Global train loss: 1.466, Global test loss: 2.252, Global test accuracy: 19.78
Round  99, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.03
Round  99, Global train loss: 1.467, Global test loss: 2.286, Global test accuracy: 16.21
Final Round, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.05
Final Round, Global train loss: 1.466, Global test loss: 2.286, Global test accuracy: 16.21
Average accuracy final 10 rounds: 96.02916666666668 

Average global accuracy final 10 rounds: 17.430833333333332 

1342.9131922721863
[1.0298395156860352, 2.0596790313720703, 2.903632879257202, 3.747586727142334, 4.633851051330566, 5.520115375518799, 6.4114556312561035, 7.302795886993408, 8.179399728775024, 9.05600357055664, 9.936699151992798, 10.817394733428955, 11.660187482833862, 12.50298023223877, 13.337044715881348, 14.171109199523926, 15.009207248687744, 15.847305297851562, 16.6941397190094, 17.540974140167236, 18.375850200653076, 19.210726261138916, 20.06276845932007, 20.91481065750122, 21.75031328201294, 22.585815906524658, 23.433223485946655, 24.280631065368652, 25.13127088546753, 25.981910705566406, 26.830090045928955, 27.678269386291504, 28.51566982269287, 29.35307025909424, 30.19823122024536, 31.043392181396484, 31.893933534622192, 32.7444748878479, 33.59050965309143, 34.43654441833496, 35.276843786239624, 36.11714315414429, 36.95817732810974, 37.799211502075195, 38.6580114364624, 39.51681137084961, 40.379019021987915, 41.24122667312622, 42.091368436813354, 42.94151020050049, 43.77077889442444, 44.60004758834839, 45.4565532207489, 46.313058853149414, 47.17658472061157, 48.04011058807373, 48.891345262527466, 49.7425799369812, 50.59024691581726, 51.43791389465332, 52.299875259399414, 53.16183662414551, 54.01962113380432, 54.877405643463135, 55.7229642868042, 56.568522930145264, 57.42556595802307, 58.28260898590088, 59.126636266708374, 59.97066354751587, 60.822998046875, 61.67533254623413, 62.532564640045166, 63.3897967338562, 64.24120950698853, 65.09262228012085, 65.94164037704468, 66.7906584739685, 67.64752125740051, 68.50438404083252, 69.3606059551239, 70.21682786941528, 71.07320952415466, 71.92959117889404, 72.77858829498291, 73.62758541107178, 74.4720368385315, 75.31648826599121, 76.16434574127197, 77.01220321655273, 77.85821986198425, 78.70423650741577, 79.54597902297974, 80.3877215385437, 81.22073769569397, 82.05375385284424, 82.91424298286438, 83.77473211288452, 84.6298565864563, 85.48498106002808, 86.33991384506226, 87.19484663009644, 88.04824900627136, 88.90165138244629, 89.74745726585388, 90.59326314926147, 91.45064854621887, 92.30803394317627, 93.15257549285889, 93.9971170425415, 94.85385251045227, 95.71058797836304, 96.54907584190369, 97.38756370544434, 98.24308967590332, 99.0986156463623, 99.95322895050049, 100.80784225463867, 101.65791511535645, 102.50798797607422, 103.36194157600403, 104.21589517593384, 105.08064293861389, 105.94539070129395, 106.80055522918701, 107.65571975708008, 108.50381231307983, 109.35190486907959, 110.20320153236389, 111.0544981956482, 111.89637541770935, 112.73825263977051, 113.59742903709412, 114.45660543441772, 115.30946946144104, 116.16233348846436, 117.01196503639221, 117.86159658432007, 118.71566462516785, 119.56973266601562, 120.42721104621887, 121.28468942642212, 122.13345146179199, 122.98221349716187, 123.83252382278442, 124.68283414840698, 125.5409631729126, 126.39909219741821, 127.24212312698364, 128.08515405654907, 128.92768454551697, 129.77021503448486, 130.62751603126526, 131.48481702804565, 132.35087943077087, 133.2169418334961, 134.07132625579834, 134.9257106781006, 135.77102613449097, 136.61634159088135, 137.46739292144775, 138.31844425201416, 139.1616621017456, 140.00487995147705, 140.86480379104614, 141.72472763061523, 142.56069993972778, 143.39667224884033, 144.23731088638306, 145.07794952392578, 145.92130136489868, 146.76465320587158, 147.61404824256897, 148.46344327926636, 149.311998128891, 150.16055297851562, 151.00156426429749, 151.84257555007935, 152.68619632720947, 153.5298171043396, 154.38664078712463, 155.24346446990967, 156.09424138069153, 156.9450182914734, 157.77739357948303, 158.60976886749268, 159.4507405757904, 160.29171228408813, 161.1264946460724, 161.96127700805664, 162.8236105442047, 163.68594408035278, 164.53743267059326, 165.38892126083374, 166.2294044494629, 167.06988763809204, 167.9116816520691, 168.75347566604614, 169.60753273963928, 170.46158981323242, 171.87161445617676, 173.2816390991211]
[19.741666666666667, 19.741666666666667, 43.74166666666667, 43.74166666666667, 56.425, 56.425, 59.71666666666667, 59.71666666666667, 68.675, 68.675, 74.45, 74.45, 79.84166666666667, 79.84166666666667, 79.59166666666667, 79.59166666666667, 79.7, 79.7, 84.575, 84.575, 84.45833333333333, 84.45833333333333, 87.76666666666667, 87.76666666666667, 87.75833333333334, 87.75833333333334, 87.70833333333333, 87.70833333333333, 87.825, 87.825, 87.90833333333333, 87.90833333333333, 87.975, 87.975, 87.89166666666667, 87.89166666666667, 91.15, 91.15, 92.73333333333333, 92.73333333333333, 92.8, 92.8, 92.81666666666666, 92.81666666666666, 92.79166666666667, 92.79166666666667, 92.75833333333334, 92.75833333333334, 92.75833333333334, 92.75833333333334, 92.78333333333333, 92.78333333333333, 92.75, 92.75, 92.75, 92.75, 92.75, 92.75, 92.76666666666667, 92.76666666666667, 93.90833333333333, 93.90833333333333, 93.94166666666666, 93.94166666666666, 93.95, 93.95, 93.94166666666666, 93.94166666666666, 93.925, 93.925, 94.35833333333333, 94.35833333333333, 94.39166666666667, 94.39166666666667, 94.38333333333334, 94.38333333333334, 94.4, 94.4, 94.41666666666667, 94.41666666666667, 94.375, 94.375, 95.975, 95.975, 95.98333333333333, 95.98333333333333, 96.025, 96.025, 96.01666666666667, 96.01666666666667, 96.00833333333334, 96.00833333333334, 96.0, 96.0, 96.03333333333333, 96.03333333333333, 96.05, 96.05, 96.05, 96.05, 96.05833333333334, 96.05833333333334, 96.06666666666666, 96.06666666666666, 96.05833333333334, 96.05833333333334, 96.05, 96.05, 96.03333333333333, 96.03333333333333, 96.06666666666666, 96.06666666666666, 96.06666666666666, 96.06666666666666, 96.05833333333334, 96.05833333333334, 96.05, 96.05, 96.03333333333333, 96.03333333333333, 96.04166666666667, 96.04166666666667, 96.025, 96.025, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.00833333333334, 96.00833333333334, 96.025, 96.025, 96.01666666666667, 96.01666666666667, 95.98333333333333, 95.98333333333333, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.01666666666667, 96.01666666666667, 96.00833333333334, 96.00833333333334, 96.01666666666667, 96.01666666666667, 95.95833333333333, 95.95833333333333, 95.95833333333333, 95.95833333333333, 95.95833333333333, 95.95833333333333, 96.0, 96.0, 96.00833333333334, 96.00833333333334, 96.00833333333334, 96.00833333333334, 96.01666666666667, 96.01666666666667, 96.00833333333334, 96.00833333333334, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.03333333333333, 96.03333333333333, 96.04166666666667, 96.04166666666667, 96.05, 96.05, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.03333333333333, 96.025, 96.025, 96.025, 96.025, 96.025, 96.025, 96.025, 96.025, 96.025, 96.025, 96.03333333333333, 96.03333333333333, 96.05, 96.05]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.195, Test loss: 2.207, Test accuracy: 18.77
Round   0, Global train loss: 2.195, Global test loss: 2.299, Global test accuracy: 10.00
Round   1, Train loss: 1.906, Test loss: 2.019, Test accuracy: 43.83
Round   1, Global train loss: 1.906, Global test loss: 2.284, Global test accuracy: 16.10
Round   2, Train loss: 1.780, Test loss: 1.930, Test accuracy: 51.90
Round   2, Global train loss: 1.780, Global test loss: 2.271, Global test accuracy: 16.97
Round   3, Train loss: 1.946, Test loss: 1.827, Test accuracy: 65.04
Round   3, Global train loss: 1.946, Global test loss: 2.277, Global test accuracy: 14.67
Round   4, Train loss: 1.883, Test loss: 1.753, Test accuracy: 72.38
Round   4, Global train loss: 1.883, Global test loss: 2.275, Global test accuracy: 17.07
Round   5, Train loss: 1.883, Test loss: 1.787, Test accuracy: 68.46
Round   5, Global train loss: 1.883, Global test loss: 2.272, Global test accuracy: 17.13
Round   6, Train loss: 1.802, Test loss: 1.758, Test accuracy: 71.71
Round   6, Global train loss: 1.802, Global test loss: 2.276, Global test accuracy: 15.45
Round   7, Train loss: 1.787, Test loss: 1.744, Test accuracy: 73.07
Round   7, Global train loss: 1.787, Global test loss: 2.248, Global test accuracy: 19.17
Round   8, Train loss: 1.722, Test loss: 1.712, Test accuracy: 75.80
Round   8, Global train loss: 1.722, Global test loss: 2.236, Global test accuracy: 22.32
Round   9, Train loss: 1.808, Test loss: 1.707, Test accuracy: 75.81
Round   9, Global train loss: 1.808, Global test loss: 2.255, Global test accuracy: 18.00
Round  10, Train loss: 1.692, Test loss: 1.691, Test accuracy: 77.38
Round  10, Global train loss: 1.692, Global test loss: 2.246, Global test accuracy: 19.88
Round  11, Train loss: 1.685, Test loss: 1.676, Test accuracy: 79.00
Round  11, Global train loss: 1.685, Global test loss: 2.264, Global test accuracy: 18.07
Round  12, Train loss: 1.747, Test loss: 1.674, Test accuracy: 79.20
Round  12, Global train loss: 1.747, Global test loss: 2.235, Global test accuracy: 20.81
Round  13, Train loss: 1.785, Test loss: 1.674, Test accuracy: 79.13
Round  13, Global train loss: 1.785, Global test loss: 2.232, Global test accuracy: 20.38
Round  14, Train loss: 1.583, Test loss: 1.659, Test accuracy: 80.68
Round  14, Global train loss: 1.583, Global test loss: 2.251, Global test accuracy: 18.91
Round  15, Train loss: 1.729, Test loss: 1.659, Test accuracy: 80.65
Round  15, Global train loss: 1.729, Global test loss: 2.246, Global test accuracy: 19.18
Round  16, Train loss: 1.734, Test loss: 1.659, Test accuracy: 80.67
Round  16, Global train loss: 1.734, Global test loss: 2.230, Global test accuracy: 21.29
Round  17, Train loss: 1.665, Test loss: 1.658, Test accuracy: 80.78
Round  17, Global train loss: 1.665, Global test loss: 2.251, Global test accuracy: 18.57
Round  18, Train loss: 1.618, Test loss: 1.657, Test accuracy: 80.78
Round  18, Global train loss: 1.618, Global test loss: 2.230, Global test accuracy: 21.62
Round  19, Train loss: 1.672, Test loss: 1.657, Test accuracy: 80.79
Round  19, Global train loss: 1.672, Global test loss: 2.243, Global test accuracy: 20.18
Round  20, Train loss: 1.713, Test loss: 1.641, Test accuracy: 82.47
Round  20, Global train loss: 1.713, Global test loss: 2.230, Global test accuracy: 21.18
Round  21, Train loss: 1.707, Test loss: 1.641, Test accuracy: 82.47
Round  21, Global train loss: 1.707, Global test loss: 2.259, Global test accuracy: 17.78
Round  22, Train loss: 1.603, Test loss: 1.640, Test accuracy: 82.53
Round  22, Global train loss: 1.603, Global test loss: 2.252, Global test accuracy: 19.33
Round  23, Train loss: 1.769, Test loss: 1.657, Test accuracy: 80.64
Round  23, Global train loss: 1.769, Global test loss: 2.232, Global test accuracy: 21.06
Round  24, Train loss: 1.671, Test loss: 1.658, Test accuracy: 80.58
Round  24, Global train loss: 1.671, Global test loss: 2.249, Global test accuracy: 19.40
Round  25, Train loss: 1.676, Test loss: 1.656, Test accuracy: 80.61
Round  25, Global train loss: 1.676, Global test loss: 2.234, Global test accuracy: 20.93
Round  26, Train loss: 1.758, Test loss: 1.656, Test accuracy: 80.67
Round  26, Global train loss: 1.758, Global test loss: 2.237, Global test accuracy: 20.41
Round  27, Train loss: 1.716, Test loss: 1.657, Test accuracy: 80.53
Round  27, Global train loss: 1.716, Global test loss: 2.221, Global test accuracy: 22.61
Round  28, Train loss: 1.679, Test loss: 1.657, Test accuracy: 80.53
Round  28, Global train loss: 1.679, Global test loss: 2.229, Global test accuracy: 21.50
Round  29, Train loss: 1.682, Test loss: 1.661, Test accuracy: 80.17
Round  29, Global train loss: 1.682, Global test loss: 2.239, Global test accuracy: 20.10
Round  30, Train loss: 1.606, Test loss: 1.631, Test accuracy: 83.31
Round  30, Global train loss: 1.606, Global test loss: 2.238, Global test accuracy: 20.51
Round  31, Train loss: 1.742, Test loss: 1.624, Test accuracy: 83.97
Round  31, Global train loss: 1.742, Global test loss: 2.230, Global test accuracy: 21.17
Round  32, Train loss: 1.649, Test loss: 1.623, Test accuracy: 84.03
Round  32, Global train loss: 1.649, Global test loss: 2.238, Global test accuracy: 20.57
Round  33, Train loss: 1.514, Test loss: 1.623, Test accuracy: 84.03
Round  33, Global train loss: 1.514, Global test loss: 2.251, Global test accuracy: 18.40
Round  34, Train loss: 1.702, Test loss: 1.623, Test accuracy: 83.94
Round  34, Global train loss: 1.702, Global test loss: 2.252, Global test accuracy: 19.43
Round  35, Train loss: 1.613, Test loss: 1.621, Test accuracy: 84.26
Round  35, Global train loss: 1.613, Global test loss: 2.241, Global test accuracy: 20.51
Round  36, Train loss: 1.551, Test loss: 1.621, Test accuracy: 84.16
Round  36, Global train loss: 1.551, Global test loss: 2.256, Global test accuracy: 18.31
Round  37, Train loss: 1.599, Test loss: 1.620, Test accuracy: 84.19
Round  37, Global train loss: 1.599, Global test loss: 2.248, Global test accuracy: 19.87
Round  38, Train loss: 1.735, Test loss: 1.621, Test accuracy: 84.11
Round  38, Global train loss: 1.735, Global test loss: 2.228, Global test accuracy: 22.19
Round  39, Train loss: 1.710, Test loss: 1.621, Test accuracy: 84.06
Round  39, Global train loss: 1.710, Global test loss: 2.237, Global test accuracy: 20.75
Round  40, Train loss: 1.667, Test loss: 1.620, Test accuracy: 84.17
Round  40, Global train loss: 1.667, Global test loss: 2.232, Global test accuracy: 21.20
Round  41, Train loss: 1.550, Test loss: 1.636, Test accuracy: 82.53
Round  41, Global train loss: 1.550, Global test loss: 2.238, Global test accuracy: 20.58
Round  42, Train loss: 1.637, Test loss: 1.635, Test accuracy: 82.57
Round  42, Global train loss: 1.637, Global test loss: 2.269, Global test accuracy: 16.58
Round  43, Train loss: 1.651, Test loss: 1.636, Test accuracy: 82.50
Round  43, Global train loss: 1.651, Global test loss: 2.235, Global test accuracy: 20.62
Round  44, Train loss: 1.665, Test loss: 1.636, Test accuracy: 82.48
Round  44, Global train loss: 1.665, Global test loss: 2.224, Global test accuracy: 21.78
Round  45, Train loss: 1.646, Test loss: 1.636, Test accuracy: 82.47
Round  45, Global train loss: 1.646, Global test loss: 2.268, Global test accuracy: 17.17
Round  46, Train loss: 1.797, Test loss: 1.635, Test accuracy: 82.54
Round  46, Global train loss: 1.797, Global test loss: 2.236, Global test accuracy: 20.48
Round  47, Train loss: 1.703, Test loss: 1.635, Test accuracy: 82.67
Round  47, Global train loss: 1.703, Global test loss: 2.235, Global test accuracy: 20.81
Round  48, Train loss: 1.663, Test loss: 1.635, Test accuracy: 82.65
Round  48, Global train loss: 1.663, Global test loss: 2.252, Global test accuracy: 19.08
Round  49, Train loss: 1.654, Test loss: 1.634, Test accuracy: 82.71
Round  49, Global train loss: 1.654, Global test loss: 2.248, Global test accuracy: 19.07
Round  50, Train loss: 1.860, Test loss: 1.649, Test accuracy: 81.12
Round  50, Global train loss: 1.860, Global test loss: 2.242, Global test accuracy: 20.57
Round  51, Train loss: 1.594, Test loss: 1.650, Test accuracy: 81.03
Round  51, Global train loss: 1.594, Global test loss: 2.217, Global test accuracy: 23.25
Round  52, Train loss: 1.591, Test loss: 1.649, Test accuracy: 81.12
Round  52, Global train loss: 1.591, Global test loss: 2.239, Global test accuracy: 20.57
Round  53, Train loss: 1.708, Test loss: 1.649, Test accuracy: 81.21
Round  53, Global train loss: 1.708, Global test loss: 2.225, Global test accuracy: 22.04
Round  54, Train loss: 1.748, Test loss: 1.649, Test accuracy: 81.23
Round  54, Global train loss: 1.748, Global test loss: 2.216, Global test accuracy: 23.48
Round  55, Train loss: 1.591, Test loss: 1.648, Test accuracy: 81.19
Round  55, Global train loss: 1.591, Global test loss: 2.241, Global test accuracy: 19.88
Round  56, Train loss: 1.652, Test loss: 1.648, Test accuracy: 81.22
Round  56, Global train loss: 1.652, Global test loss: 2.225, Global test accuracy: 21.70
Round  57, Train loss: 1.596, Test loss: 1.649, Test accuracy: 81.19
Round  57, Global train loss: 1.596, Global test loss: 2.220, Global test accuracy: 22.27
Round  58, Train loss: 1.642, Test loss: 1.649, Test accuracy: 81.13
Round  58, Global train loss: 1.642, Global test loss: 2.229, Global test accuracy: 21.69
Round  59, Train loss: 1.569, Test loss: 1.664, Test accuracy: 79.53
Round  59, Global train loss: 1.569, Global test loss: 2.225, Global test accuracy: 21.71
Round  60, Train loss: 1.650, Test loss: 1.664, Test accuracy: 79.55
Round  60, Global train loss: 1.650, Global test loss: 2.233, Global test accuracy: 21.25
Round  61, Train loss: 1.647, Test loss: 1.664, Test accuracy: 79.58
Round  61, Global train loss: 1.647, Global test loss: 2.230, Global test accuracy: 21.68
Round  62, Train loss: 1.792, Test loss: 1.634, Test accuracy: 82.75
Round  62, Global train loss: 1.792, Global test loss: 2.232, Global test accuracy: 21.31
Round  63, Train loss: 1.615, Test loss: 1.633, Test accuracy: 82.81
Round  63, Global train loss: 1.615, Global test loss: 2.217, Global test accuracy: 22.63
Round  64, Train loss: 1.750, Test loss: 1.633, Test accuracy: 82.90
Round  64, Global train loss: 1.750, Global test loss: 2.218, Global test accuracy: 23.07
Round  65, Train loss: 1.642, Test loss: 1.633, Test accuracy: 82.85
Round  65, Global train loss: 1.642, Global test loss: 2.224, Global test accuracy: 22.93
Round  66, Train loss: 1.590, Test loss: 1.648, Test accuracy: 81.26
Round  66, Global train loss: 1.590, Global test loss: 2.252, Global test accuracy: 19.37
Round  67, Train loss: 1.604, Test loss: 1.648, Test accuracy: 81.25
Round  67, Global train loss: 1.604, Global test loss: 2.242, Global test accuracy: 19.91
Round  68, Train loss: 1.694, Test loss: 1.647, Test accuracy: 81.28
Round  68, Global train loss: 1.694, Global test loss: 2.232, Global test accuracy: 21.18
Round  69, Train loss: 1.749, Test loss: 1.648, Test accuracy: 81.26
Round  69, Global train loss: 1.749, Global test loss: 2.222, Global test accuracy: 22.13
Round  70, Train loss: 1.766, Test loss: 1.647, Test accuracy: 81.29
Round  70, Global train loss: 1.766, Global test loss: 2.237, Global test accuracy: 20.66
Round  71, Train loss: 1.535, Test loss: 1.648, Test accuracy: 81.23
Round  71, Global train loss: 1.535, Global test loss: 2.248, Global test accuracy: 19.11
Round  72, Train loss: 1.705, Test loss: 1.648, Test accuracy: 81.21
Round  72, Global train loss: 1.705, Global test loss: 2.238, Global test accuracy: 20.77
Round  73, Train loss: 1.653, Test loss: 1.648, Test accuracy: 81.18
Round  73, Global train loss: 1.653, Global test loss: 2.242, Global test accuracy: 20.74
Round  74, Train loss: 1.693, Test loss: 1.648, Test accuracy: 81.17
Round  74, Global train loss: 1.693, Global test loss: 2.223, Global test accuracy: 22.58
Round  75, Train loss: 1.715, Test loss: 1.648, Test accuracy: 81.23
Round  75, Global train loss: 1.715, Global test loss: 2.239, Global test accuracy: 20.64
Round  76, Train loss: 1.591, Test loss: 1.633, Test accuracy: 82.79
Round  76, Global train loss: 1.591, Global test loss: 2.250, Global test accuracy: 19.55
Round  77, Train loss: 1.677, Test loss: 1.635, Test accuracy: 82.64
Round  77, Global train loss: 1.677, Global test loss: 2.240, Global test accuracy: 21.19
Round  78, Train loss: 1.724, Test loss: 1.648, Test accuracy: 81.22
Round  78, Global train loss: 1.724, Global test loss: 2.229, Global test accuracy: 21.68
Round  79, Train loss: 1.752, Test loss: 1.634, Test accuracy: 82.72
Round  79, Global train loss: 1.752, Global test loss: 2.224, Global test accuracy: 21.80
Round  80, Train loss: 1.642, Test loss: 1.633, Test accuracy: 82.77
Round  80, Global train loss: 1.642, Global test loss: 2.221, Global test accuracy: 21.89
Round  81, Train loss: 1.695, Test loss: 1.633, Test accuracy: 82.83
Round  81, Global train loss: 1.695, Global test loss: 2.216, Global test accuracy: 22.88
Round  82, Train loss: 1.648, Test loss: 1.632, Test accuracy: 82.92
Round  82, Global train loss: 1.648, Global test loss: 2.235, Global test accuracy: 21.02
Round  83, Train loss: 1.710, Test loss: 1.635, Test accuracy: 82.64
Round  83, Global train loss: 1.710, Global test loss: 2.243, Global test accuracy: 19.71
Round  84, Train loss: 1.583, Test loss: 1.636, Test accuracy: 82.61
Round  84, Global train loss: 1.583, Global test loss: 2.252, Global test accuracy: 18.38
Round  85, Train loss: 1.588, Test loss: 1.637, Test accuracy: 82.42
Round  85, Global train loss: 1.588, Global test loss: 2.232, Global test accuracy: 21.34
Round  86, Train loss: 1.592, Test loss: 1.636, Test accuracy: 82.46
Round  86, Global train loss: 1.592, Global test loss: 2.233, Global test accuracy: 20.69
Round  87, Train loss: 1.639, Test loss: 1.636, Test accuracy: 82.45
Round  87, Global train loss: 1.639, Global test loss: 2.229, Global test accuracy: 21.49
Round  88, Train loss: 1.698, Test loss: 1.636, Test accuracy: 82.41
Round  88, Global train loss: 1.698, Global test loss: 2.241, Global test accuracy: 20.42
Round  89, Train loss: 1.590, Test loss: 1.636, Test accuracy: 82.55
Round  89, Global train loss: 1.590, Global test loss: 2.265, Global test accuracy: 17.38
Round  90, Train loss: 1.697, Test loss: 1.632, Test accuracy: 82.81
Round  90, Global train loss: 1.697, Global test loss: 2.241, Global test accuracy: 20.43
Round  91, Train loss: 1.582, Test loss: 1.632, Test accuracy: 82.85
Round  91, Global train loss: 1.582, Global test loss: 2.239, Global test accuracy: 20.52
Round  92, Train loss: 1.597, Test loss: 1.632, Test accuracy: 82.78
Round  92, Global train loss: 1.597, Global test loss: 2.227, Global test accuracy: 22.13
Round  93, Train loss: 1.700, Test loss: 1.632, Test accuracy: 82.83
Round  93, Global train loss: 1.700, Global test loss: 2.234, Global test accuracy: 20.58
Round  94, Train loss: 1.585, Test loss: 1.648, Test accuracy: 81.23
Round  94, Global train loss: 1.585, Global test loss: 2.234, Global test accuracy: 20.89
Round  95, Train loss: 1.636, Test loss: 1.648, Test accuracy: 81.20
Round  95, Global train loss: 1.636, Global test loss: 2.255, Global test accuracy: 18.23/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.587, Test loss: 1.648, Test accuracy: 81.24
Round  96, Global train loss: 1.587, Global test loss: 2.238, Global test accuracy: 20.57
Round  97, Train loss: 1.586, Test loss: 1.648, Test accuracy: 81.25
Round  97, Global train loss: 1.586, Global test loss: 2.231, Global test accuracy: 21.35
Round  98, Train loss: 1.630, Test loss: 1.650, Test accuracy: 81.08
Round  98, Global train loss: 1.630, Global test loss: 2.238, Global test accuracy: 20.32
Round  99, Train loss: 1.633, Test loss: 1.650, Test accuracy: 81.04
Round  99, Global train loss: 1.633, Global test loss: 2.243, Global test accuracy: 20.22
Final Round, Train loss: 1.636, Test loss: 1.622, Test accuracy: 83.79
Final Round, Global train loss: 1.636, Global test loss: 2.243, Global test accuracy: 20.22
Average accuracy final 10 rounds: 81.83166666666668 

Average global accuracy final 10 rounds: 20.525 

1333.1051378250122
[0.9782137870788574, 1.9564275741577148, 2.892780303955078, 3.8291330337524414, 4.761228322982788, 5.693323612213135, 6.565271615982056, 7.437219619750977, 8.382885217666626, 9.328550815582275, 10.233699798583984, 11.138848781585693, 11.98061990737915, 12.822391033172607, 13.67195177078247, 14.521512508392334, 15.364741086959839, 16.207969665527344, 17.05833339691162, 17.9086971282959, 18.755148887634277, 19.601600646972656, 20.43270254135132, 21.26380443572998, 22.09727931022644, 22.9307541847229, 23.775923490524292, 24.621092796325684, 25.46448016166687, 26.307867527008057, 27.14721989631653, 27.986572265625, 28.81836986541748, 29.65016746520996, 30.49763011932373, 31.3450927734375, 32.19305634498596, 33.041019916534424, 33.874359130859375, 34.707698345184326, 35.54659914970398, 36.38549995422363, 37.23255801200867, 38.0796160697937, 38.921531200408936, 39.76344633102417, 40.6187858581543, 41.474125385284424, 42.31799101829529, 43.16185665130615, 44.001713275909424, 44.841569900512695, 45.69527864456177, 46.54898738861084, 47.40589714050293, 48.26280689239502, 49.10686445236206, 49.9509220123291, 50.796560525894165, 51.64219903945923, 52.50620722770691, 53.37021541595459, 54.21839714050293, 55.06657886505127, 55.90857982635498, 56.75058078765869, 57.59128403663635, 58.431987285614014, 59.27814483642578, 60.12430238723755, 61.03536558151245, 61.94642877578735, 62.79932165145874, 63.65221452713013, 64.48841190338135, 65.32460927963257, 66.15728902816772, 66.98996877670288, 67.84654068946838, 68.70311260223389, 69.55934858322144, 70.41558456420898, 71.26522541046143, 72.11486625671387, 72.9658203125, 73.81677436828613, 74.6619086265564, 75.50704288482666, 76.36617469787598, 77.2253065109253, 78.06034874916077, 78.89539098739624, 79.75573539733887, 80.6160798072815, 81.4588975906372, 82.30171537399292, 83.15327382087708, 84.00483226776123, 84.84687495231628, 85.68891763687134, 86.54097008705139, 87.39302253723145, 88.24183535575867, 89.09064817428589, 89.93650245666504, 90.78235673904419, 91.68614411354065, 92.58993148803711, 93.46726441383362, 94.34459733963013, 95.18717193603516, 96.02974653244019, 96.85325217247009, 97.6767578125, 98.51833510398865, 99.3599123954773, 100.19513058662415, 101.030348777771, 101.86622953414917, 102.70211029052734, 103.54521560668945, 104.38832092285156, 105.23555517196655, 106.08278942108154, 106.92801022529602, 107.7732310295105, 108.61347579956055, 109.4537205696106, 110.30281519889832, 111.15190982818604, 111.9932508468628, 112.83459186553955, 113.68288540840149, 114.53117895126343, 115.37316632270813, 116.21515369415283, 117.06114673614502, 117.9071397781372, 118.75471591949463, 119.60229206085205, 120.44450449943542, 121.2867169380188, 122.1288673877716, 122.97101783752441, 123.81731367111206, 124.6636095046997, 125.51822781562805, 126.3728461265564, 127.2281506061554, 128.0834550857544, 128.9307587146759, 129.7780623435974, 130.62799072265625, 131.4779191017151, 132.41500282287598, 133.35208654403687, 134.24909925460815, 135.14611196517944, 135.98741459846497, 136.8287172317505, 137.66459727287292, 138.50047731399536, 139.3458924293518, 140.19130754470825, 141.03387713432312, 141.876446723938, 142.7087061405182, 143.5409655570984, 144.390282869339, 145.2396001815796, 146.0764560699463, 146.913311958313, 147.76368141174316, 148.61405086517334, 149.46115636825562, 150.3082618713379, 151.14118123054504, 151.9741005897522, 152.84225726127625, 153.7104139328003, 154.56399178504944, 155.41756963729858, 156.27974581718445, 157.1419219970703, 157.95582962036133, 158.76973724365234, 159.60731196403503, 160.44488668441772, 161.28843355178833, 162.13198041915894, 162.97738647460938, 163.82279253005981, 164.6617293357849, 165.50066614151, 166.33031105995178, 167.15995597839355, 167.99638390541077, 168.83281183242798, 169.67768144607544, 170.5225510597229, 171.93119716644287, 173.33984327316284]
[18.775, 18.775, 43.833333333333336, 43.833333333333336, 51.9, 51.9, 65.04166666666667, 65.04166666666667, 72.38333333333334, 72.38333333333334, 68.45833333333333, 68.45833333333333, 71.70833333333333, 71.70833333333333, 73.06666666666666, 73.06666666666666, 75.8, 75.8, 75.80833333333334, 75.80833333333334, 77.38333333333334, 77.38333333333334, 79.0, 79.0, 79.2, 79.2, 79.13333333333334, 79.13333333333334, 80.68333333333334, 80.68333333333334, 80.65, 80.65, 80.66666666666667, 80.66666666666667, 80.78333333333333, 80.78333333333333, 80.78333333333333, 80.78333333333333, 80.79166666666667, 80.79166666666667, 82.475, 82.475, 82.475, 82.475, 82.525, 82.525, 80.64166666666667, 80.64166666666667, 80.58333333333333, 80.58333333333333, 80.60833333333333, 80.60833333333333, 80.66666666666667, 80.66666666666667, 80.525, 80.525, 80.53333333333333, 80.53333333333333, 80.175, 80.175, 83.30833333333334, 83.30833333333334, 83.975, 83.975, 84.025, 84.025, 84.025, 84.025, 83.94166666666666, 83.94166666666666, 84.25833333333334, 84.25833333333334, 84.15833333333333, 84.15833333333333, 84.19166666666666, 84.19166666666666, 84.10833333333333, 84.10833333333333, 84.05833333333334, 84.05833333333334, 84.175, 84.175, 82.525, 82.525, 82.56666666666666, 82.56666666666666, 82.5, 82.5, 82.48333333333333, 82.48333333333333, 82.46666666666667, 82.46666666666667, 82.54166666666667, 82.54166666666667, 82.66666666666667, 82.66666666666667, 82.65, 82.65, 82.70833333333333, 82.70833333333333, 81.125, 81.125, 81.025, 81.025, 81.11666666666666, 81.11666666666666, 81.20833333333333, 81.20833333333333, 81.23333333333333, 81.23333333333333, 81.19166666666666, 81.19166666666666, 81.21666666666667, 81.21666666666667, 81.19166666666666, 81.19166666666666, 81.13333333333334, 81.13333333333334, 79.53333333333333, 79.53333333333333, 79.55, 79.55, 79.58333333333333, 79.58333333333333, 82.75, 82.75, 82.80833333333334, 82.80833333333334, 82.9, 82.9, 82.85, 82.85, 81.25833333333334, 81.25833333333334, 81.25, 81.25, 81.275, 81.275, 81.25833333333334, 81.25833333333334, 81.29166666666667, 81.29166666666667, 81.23333333333333, 81.23333333333333, 81.20833333333333, 81.20833333333333, 81.18333333333334, 81.18333333333334, 81.16666666666667, 81.16666666666667, 81.23333333333333, 81.23333333333333, 82.79166666666667, 82.79166666666667, 82.64166666666667, 82.64166666666667, 81.21666666666667, 81.21666666666667, 82.71666666666667, 82.71666666666667, 82.76666666666667, 82.76666666666667, 82.83333333333333, 82.83333333333333, 82.91666666666667, 82.91666666666667, 82.64166666666667, 82.64166666666667, 82.60833333333333, 82.60833333333333, 82.41666666666667, 82.41666666666667, 82.45833333333333, 82.45833333333333, 82.45, 82.45, 82.40833333333333, 82.40833333333333, 82.55, 82.55, 82.80833333333334, 82.80833333333334, 82.85, 82.85, 82.78333333333333, 82.78333333333333, 82.825, 82.825, 81.23333333333333, 81.23333333333333, 81.2, 81.2, 81.24166666666666, 81.24166666666666, 81.25, 81.25, 81.08333333333333, 81.08333333333333, 81.04166666666667, 81.04166666666667, 83.79166666666667, 83.79166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.286, Test loss: 2.300, Test accuracy: 12.07
Round   1, Train loss: 2.233, Test loss: 2.268, Test accuracy: 22.72
Round   2, Train loss: 2.008, Test loss: 2.197, Test accuracy: 27.77
Round   3, Train loss: 1.924, Test loss: 2.133, Test accuracy: 32.16
Round   4, Train loss: 1.873, Test loss: 2.064, Test accuracy: 38.94
Round   5, Train loss: 1.722, Test loss: 1.970, Test accuracy: 49.42
Round   6, Train loss: 1.834, Test loss: 1.918, Test accuracy: 55.39
Round   7, Train loss: 1.856, Test loss: 1.896, Test accuracy: 57.73
Round   8, Train loss: 1.791, Test loss: 1.867, Test accuracy: 60.34
Round   9, Train loss: 1.807, Test loss: 1.855, Test accuracy: 61.47
Round  10, Train loss: 1.895, Test loss: 1.830, Test accuracy: 63.83
Round  11, Train loss: 1.820, Test loss: 1.828, Test accuracy: 63.93
Round  12, Train loss: 1.792, Test loss: 1.812, Test accuracy: 65.67
Round  13, Train loss: 1.632, Test loss: 1.809, Test accuracy: 65.75
Round  14, Train loss: 1.844, Test loss: 1.792, Test accuracy: 67.63
Round  15, Train loss: 1.793, Test loss: 1.794, Test accuracy: 67.14
Round  16, Train loss: 1.872, Test loss: 1.791, Test accuracy: 67.38
Round  17, Train loss: 1.662, Test loss: 1.776, Test accuracy: 69.08
Round  18, Train loss: 1.659, Test loss: 1.773, Test accuracy: 69.23
Round  19, Train loss: 1.732, Test loss: 1.768, Test accuracy: 69.98
Round  20, Train loss: 1.767, Test loss: 1.767, Test accuracy: 70.17
Round  21, Train loss: 1.716, Test loss: 1.766, Test accuracy: 70.28
Round  22, Train loss: 1.789, Test loss: 1.759, Test accuracy: 71.21
Round  23, Train loss: 1.770, Test loss: 1.757, Test accuracy: 71.25
Round  24, Train loss: 1.709, Test loss: 1.755, Test accuracy: 71.33
Round  25, Train loss: 1.600, Test loss: 1.756, Test accuracy: 71.23
Round  26, Train loss: 1.596, Test loss: 1.757, Test accuracy: 71.01
Round  27, Train loss: 1.803, Test loss: 1.756, Test accuracy: 71.08
Round  28, Train loss: 1.650, Test loss: 1.754, Test accuracy: 71.37
Round  29, Train loss: 1.865, Test loss: 1.754, Test accuracy: 71.13
Round  30, Train loss: 1.657, Test loss: 1.752, Test accuracy: 71.32
Round  31, Train loss: 1.703, Test loss: 1.751, Test accuracy: 71.37
Round  32, Train loss: 1.645, Test loss: 1.750, Test accuracy: 71.46
Round  33, Train loss: 1.646, Test loss: 1.747, Test accuracy: 71.77
Round  34, Train loss: 1.823, Test loss: 1.744, Test accuracy: 72.09
Round  35, Train loss: 1.755, Test loss: 1.744, Test accuracy: 72.13
Round  36, Train loss: 1.705, Test loss: 1.743, Test accuracy: 72.16
Round  37, Train loss: 1.707, Test loss: 1.741, Test accuracy: 72.34
Round  38, Train loss: 1.652, Test loss: 1.740, Test accuracy: 72.41
Round  39, Train loss: 1.749, Test loss: 1.740, Test accuracy: 72.38
Round  40, Train loss: 1.643, Test loss: 1.739, Test accuracy: 72.45
Round  41, Train loss: 1.752, Test loss: 1.740, Test accuracy: 72.48
Round  42, Train loss: 1.801, Test loss: 1.740, Test accuracy: 72.45
Round  43, Train loss: 1.800, Test loss: 1.741, Test accuracy: 72.20
Round  44, Train loss: 1.642, Test loss: 1.739, Test accuracy: 72.48
Round  45, Train loss: 1.802, Test loss: 1.738, Test accuracy: 72.65
Round  46, Train loss: 1.747, Test loss: 1.738, Test accuracy: 72.56
Round  47, Train loss: 1.912, Test loss: 1.737, Test accuracy: 72.60
Round  48, Train loss: 1.695, Test loss: 1.737, Test accuracy: 72.54
Round  49, Train loss: 1.799, Test loss: 1.737, Test accuracy: 72.63
Round  50, Train loss: 1.691, Test loss: 1.737, Test accuracy: 72.57
Round  51, Train loss: 1.748, Test loss: 1.737, Test accuracy: 72.62
Round  52, Train loss: 1.744, Test loss: 1.737, Test accuracy: 72.58
Round  53, Train loss: 1.694, Test loss: 1.737, Test accuracy: 72.63
Round  54, Train loss: 1.689, Test loss: 1.739, Test accuracy: 72.25
Round  55, Train loss: 1.640, Test loss: 1.737, Test accuracy: 72.52
Round  56, Train loss: 1.636, Test loss: 1.735, Test accuracy: 72.63
Round  57, Train loss: 1.687, Test loss: 1.735, Test accuracy: 72.67
Round  58, Train loss: 1.636, Test loss: 1.735, Test accuracy: 72.63
Round  59, Train loss: 1.799, Test loss: 1.735, Test accuracy: 72.65
Round  60, Train loss: 1.581, Test loss: 1.735, Test accuracy: 72.76
Round  61, Train loss: 1.661, Test loss: 1.706, Test accuracy: 75.83
Round  62, Train loss: 1.688, Test loss: 1.706, Test accuracy: 75.78
Round  63, Train loss: 1.636, Test loss: 1.706, Test accuracy: 75.77
Round  64, Train loss: 1.696, Test loss: 1.703, Test accuracy: 75.99
Round  65, Train loss: 1.853, Test loss: 1.703, Test accuracy: 76.12
Round  66, Train loss: 1.661, Test loss: 1.695, Test accuracy: 76.95
Round  67, Train loss: 1.689, Test loss: 1.695, Test accuracy: 76.85
Round  68, Train loss: 1.703, Test loss: 1.691, Test accuracy: 77.36
Round  69, Train loss: 1.701, Test loss: 1.690, Test accuracy: 77.27
Round  70, Train loss: 1.634, Test loss: 1.691, Test accuracy: 77.34
Round  71, Train loss: 1.636, Test loss: 1.691, Test accuracy: 77.30
Round  72, Train loss: 1.688, Test loss: 1.690, Test accuracy: 77.23
Round  73, Train loss: 1.749, Test loss: 1.688, Test accuracy: 77.51
Round  74, Train loss: 1.762, Test loss: 1.675, Test accuracy: 79.06
Round  75, Train loss: 1.693, Test loss: 1.675, Test accuracy: 79.01
Round  76, Train loss: 1.637, Test loss: 1.674, Test accuracy: 78.92
Round  77, Train loss: 1.638, Test loss: 1.674, Test accuracy: 78.96
Round  78, Train loss: 1.688, Test loss: 1.673, Test accuracy: 79.11
Round  79, Train loss: 1.583, Test loss: 1.673, Test accuracy: 79.06
Round  80, Train loss: 1.803, Test loss: 1.673, Test accuracy: 79.11
Round  81, Train loss: 1.746, Test loss: 1.672, Test accuracy: 79.17
Round  82, Train loss: 1.741, Test loss: 1.672, Test accuracy: 79.11
Round  83, Train loss: 1.633, Test loss: 1.672, Test accuracy: 79.13
Round  84, Train loss: 1.589, Test loss: 1.672, Test accuracy: 79.02
Round  85, Train loss: 1.576, Test loss: 1.673, Test accuracy: 79.01
Round  86, Train loss: 1.661, Test loss: 1.678, Test accuracy: 78.59
Round  87, Train loss: 1.700, Test loss: 1.665, Test accuracy: 79.90
Round  88, Train loss: 1.635, Test loss: 1.663, Test accuracy: 80.21
Round  89, Train loss: 1.635, Test loss: 1.661, Test accuracy: 80.23
Round  90, Train loss: 1.585, Test loss: 1.657, Test accuracy: 80.65
Round  91, Train loss: 1.534, Test loss: 1.656, Test accuracy: 80.69
Round  92, Train loss: 1.586, Test loss: 1.657, Test accuracy: 80.61
Round  93, Train loss: 1.581, Test loss: 1.656, Test accuracy: 80.72
Round  94, Train loss: 1.527, Test loss: 1.655, Test accuracy: 80.83
Round  95, Train loss: 1.693, Test loss: 1.655, Test accuracy: 80.83
Round  96, Train loss: 1.633, Test loss: 1.655, Test accuracy: 80.81
Round  97, Train loss: 1.576, Test loss: 1.655, Test accuracy: 80.77
Round  98, Train loss: 1.580, Test loss: 1.654, Test accuracy: 80.78/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.611, Test loss: 1.656, Test accuracy: 80.66
Final Round, Train loss: 1.620, Test loss: 1.638, Test accuracy: 82.49
Average accuracy final 10 rounds: 80.73333333333332 

1042.8849472999573
[0.9329416751861572, 1.8658833503723145, 2.6728153228759766, 3.4797472953796387, 4.292178392410278, 5.104609489440918, 5.907110929489136, 6.7096123695373535, 7.500083923339844, 8.290555477142334, 9.08958888053894, 9.888622283935547, 10.693636417388916, 11.498650550842285, 12.319643020629883, 13.14063549041748, 13.944836139678955, 14.74903678894043, 15.547089576721191, 16.345142364501953, 17.139305114746094, 17.933467864990234, 18.736590147018433, 19.53971242904663, 20.355727910995483, 21.171743392944336, 21.9798481464386, 22.78795289993286, 23.582670211791992, 24.377387523651123, 25.16794228553772, 25.958497047424316, 26.761956691741943, 27.56541633605957, 28.373909950256348, 29.182403564453125, 29.99396800994873, 30.805532455444336, 31.6163432598114, 32.42715406417847, 33.22667574882507, 34.02619743347168, 34.81832313537598, 35.61044883728027, 36.423309326171875, 37.23616981506348, 38.04022455215454, 38.844279289245605, 39.64578032493591, 40.44728136062622, 41.24844789505005, 42.04961442947388, 42.86455678939819, 43.67949914932251, 44.47396993637085, 45.26844072341919, 46.07386565208435, 46.87929058074951, 47.690003395080566, 48.50071620941162, 49.30318093299866, 50.10564565658569, 50.90939784049988, 51.71315002441406, 52.51144242286682, 53.30973482131958, 54.12425112724304, 54.938767433166504, 55.74931311607361, 56.55985879898071, 57.36257481575012, 58.16529083251953, 58.98132848739624, 59.79736614227295, 60.600040912628174, 61.4027156829834, 62.206815242767334, 63.01091480255127, 63.818608045578, 64.62630128860474, 65.47103667259216, 66.31577205657959, 67.12082314491272, 67.92587423324585, 68.72472429275513, 69.5235743522644, 70.3190438747406, 71.1145133972168, 71.9095299243927, 72.7045464515686, 73.51491212844849, 74.32527780532837, 75.15878582000732, 75.99229383468628, 76.78603005409241, 77.57976627349854, 78.3743326663971, 79.16889905929565, 79.96995425224304, 80.77100944519043, 81.56969237327576, 82.36837530136108, 83.17856073379517, 83.98874616622925, 84.79591584205627, 85.6030855178833, 86.44348454475403, 87.28388357162476, 88.0922577381134, 88.90063190460205, 89.7122814655304, 90.52393102645874, 91.34165978431702, 92.1593885421753, 92.97506499290466, 93.79074144363403, 94.59466671943665, 95.39859199523926, 96.22828316688538, 97.0579743385315, 97.87410402297974, 98.69023370742798, 99.51272392272949, 100.335214138031, 101.14558124542236, 101.95594835281372, 102.76798629760742, 103.58002424240112, 104.3816089630127, 105.18319368362427, 105.98594307899475, 106.78869247436523, 107.60793685913086, 108.42718124389648, 109.22510695457458, 110.02303266525269, 110.84333777427673, 111.66364288330078, 112.45766830444336, 113.25169372558594, 114.05110597610474, 114.85051822662354, 115.65442657470703, 116.45833492279053, 117.2621705532074, 118.06600618362427, 118.86746335029602, 119.66892051696777, 120.52753973007202, 121.38615894317627, 122.18226099014282, 122.97836303710938, 123.77753400802612, 124.57670497894287, 125.3826322555542, 126.18855953216553, 127.00041604042053, 127.81227254867554, 128.61311316490173, 129.41395378112793, 130.20791220664978, 131.00187063217163, 131.7991714477539, 132.59647226333618, 133.3929226398468, 134.18937301635742, 134.9890468120575, 135.78872060775757, 136.57826948165894, 137.3678183555603, 138.1610927581787, 138.95436716079712, 139.75185775756836, 140.5493483543396, 141.3509931564331, 142.1526379585266, 142.95169377326965, 143.7507495880127, 144.55253720283508, 145.35432481765747, 146.14186596870422, 146.92940711975098, 147.72569942474365, 148.52199172973633, 149.3168008327484, 150.1116099357605, 150.90527248382568, 151.69893503189087, 152.50234746932983, 153.3057599067688, 154.10979199409485, 154.9138240814209, 155.7369818687439, 156.5601396560669, 157.35723638534546, 158.15433311462402, 158.94707036018372, 159.7398076057434, 160.54000854492188, 161.34020948410034, 162.63610744476318, 163.93200540542603]
[12.066666666666666, 12.066666666666666, 22.716666666666665, 22.716666666666665, 27.766666666666666, 27.766666666666666, 32.15833333333333, 32.15833333333333, 38.94166666666667, 38.94166666666667, 49.416666666666664, 49.416666666666664, 55.391666666666666, 55.391666666666666, 57.725, 57.725, 60.34166666666667, 60.34166666666667, 61.46666666666667, 61.46666666666667, 63.833333333333336, 63.833333333333336, 63.93333333333333, 63.93333333333333, 65.675, 65.675, 65.75, 65.75, 67.63333333333334, 67.63333333333334, 67.14166666666667, 67.14166666666667, 67.375, 67.375, 69.08333333333333, 69.08333333333333, 69.23333333333333, 69.23333333333333, 69.98333333333333, 69.98333333333333, 70.16666666666667, 70.16666666666667, 70.28333333333333, 70.28333333333333, 71.20833333333333, 71.20833333333333, 71.25, 71.25, 71.325, 71.325, 71.23333333333333, 71.23333333333333, 71.00833333333334, 71.00833333333334, 71.075, 71.075, 71.36666666666666, 71.36666666666666, 71.13333333333334, 71.13333333333334, 71.31666666666666, 71.31666666666666, 71.36666666666666, 71.36666666666666, 71.45833333333333, 71.45833333333333, 71.76666666666667, 71.76666666666667, 72.09166666666667, 72.09166666666667, 72.13333333333334, 72.13333333333334, 72.15833333333333, 72.15833333333333, 72.34166666666667, 72.34166666666667, 72.40833333333333, 72.40833333333333, 72.375, 72.375, 72.45, 72.45, 72.48333333333333, 72.48333333333333, 72.45, 72.45, 72.2, 72.2, 72.48333333333333, 72.48333333333333, 72.65, 72.65, 72.55833333333334, 72.55833333333334, 72.6, 72.6, 72.54166666666667, 72.54166666666667, 72.63333333333334, 72.63333333333334, 72.56666666666666, 72.56666666666666, 72.625, 72.625, 72.575, 72.575, 72.63333333333334, 72.63333333333334, 72.25, 72.25, 72.51666666666667, 72.51666666666667, 72.63333333333334, 72.63333333333334, 72.675, 72.675, 72.63333333333334, 72.63333333333334, 72.65, 72.65, 72.75833333333334, 72.75833333333334, 75.825, 75.825, 75.78333333333333, 75.78333333333333, 75.76666666666667, 75.76666666666667, 75.99166666666666, 75.99166666666666, 76.11666666666666, 76.11666666666666, 76.95, 76.95, 76.85, 76.85, 77.35833333333333, 77.35833333333333, 77.26666666666667, 77.26666666666667, 77.34166666666667, 77.34166666666667, 77.3, 77.3, 77.23333333333333, 77.23333333333333, 77.50833333333334, 77.50833333333334, 79.05833333333334, 79.05833333333334, 79.00833333333334, 79.00833333333334, 78.925, 78.925, 78.95833333333333, 78.95833333333333, 79.10833333333333, 79.10833333333333, 79.05833333333334, 79.05833333333334, 79.10833333333333, 79.10833333333333, 79.16666666666667, 79.16666666666667, 79.10833333333333, 79.10833333333333, 79.13333333333334, 79.13333333333334, 79.01666666666667, 79.01666666666667, 79.00833333333334, 79.00833333333334, 78.59166666666667, 78.59166666666667, 79.9, 79.9, 80.20833333333333, 80.20833333333333, 80.23333333333333, 80.23333333333333, 80.65, 80.65, 80.69166666666666, 80.69166666666666, 80.60833333333333, 80.60833333333333, 80.71666666666667, 80.71666666666667, 80.825, 80.825, 80.825, 80.825, 80.80833333333334, 80.80833333333334, 80.76666666666667, 80.76666666666667, 80.78333333333333, 80.78333333333333, 80.65833333333333, 80.65833333333333, 82.49166666666666, 82.49166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.188, Test loss: 2.262, Test accuracy: 22.95
Round   1, Train loss: 1.835, Test loss: 2.163, Test accuracy: 31.10
Round   2, Train loss: 1.779, Test loss: 2.107, Test accuracy: 37.14
Round   3, Train loss: 1.801, Test loss: 2.068, Test accuracy: 40.48
Round   4, Train loss: 1.815, Test loss: 2.035, Test accuracy: 44.13
Round   5, Train loss: 1.895, Test loss: 1.982, Test accuracy: 50.12
Round   6, Train loss: 1.784, Test loss: 1.932, Test accuracy: 54.61
Round   7, Train loss: 1.718, Test loss: 1.897, Test accuracy: 58.69
Round   8, Train loss: 1.519, Test loss: 1.851, Test accuracy: 63.44
Round   9, Train loss: 1.862, Test loss: 1.828, Test accuracy: 65.67
Round  10, Train loss: 1.778, Test loss: 1.804, Test accuracy: 67.53
Round  11, Train loss: 1.703, Test loss: 1.787, Test accuracy: 68.96
Round  12, Train loss: 1.755, Test loss: 1.781, Test accuracy: 69.46
Round  13, Train loss: 1.755, Test loss: 1.784, Test accuracy: 69.24
Round  14, Train loss: 1.709, Test loss: 1.766, Test accuracy: 70.51
Round  15, Train loss: 1.655, Test loss: 1.756, Test accuracy: 71.33
Round  16, Train loss: 1.702, Test loss: 1.741, Test accuracy: 72.73
Round  17, Train loss: 1.636, Test loss: 1.727, Test accuracy: 74.28
Round  18, Train loss: 1.645, Test loss: 1.719, Test accuracy: 75.05
Round  19, Train loss: 1.606, Test loss: 1.708, Test accuracy: 76.08
Round  20, Train loss: 1.539, Test loss: 1.707, Test accuracy: 76.12
Round  21, Train loss: 1.545, Test loss: 1.690, Test accuracy: 77.65
Round  22, Train loss: 1.579, Test loss: 1.691, Test accuracy: 77.53
Round  23, Train loss: 1.536, Test loss: 1.684, Test accuracy: 77.98
Round  24, Train loss: 1.592, Test loss: 1.674, Test accuracy: 79.27
Round  25, Train loss: 1.693, Test loss: 1.674, Test accuracy: 79.31
Round  26, Train loss: 1.584, Test loss: 1.669, Test accuracy: 79.66
Round  27, Train loss: 1.530, Test loss: 1.669, Test accuracy: 79.61
Round  28, Train loss: 1.638, Test loss: 1.667, Test accuracy: 79.91
Round  29, Train loss: 1.584, Test loss: 1.667, Test accuracy: 79.69
Round  30, Train loss: 1.530, Test loss: 1.660, Test accuracy: 80.45
Round  31, Train loss: 1.686, Test loss: 1.657, Test accuracy: 80.71
Round  32, Train loss: 1.633, Test loss: 1.656, Test accuracy: 80.76
Round  33, Train loss: 1.534, Test loss: 1.653, Test accuracy: 81.23
Round  34, Train loss: 1.682, Test loss: 1.654, Test accuracy: 81.06
Round  35, Train loss: 1.585, Test loss: 1.655, Test accuracy: 80.93
Round  36, Train loss: 1.582, Test loss: 1.652, Test accuracy: 81.12
Round  37, Train loss: 1.531, Test loss: 1.652, Test accuracy: 81.15
Round  38, Train loss: 1.581, Test loss: 1.653, Test accuracy: 81.05
Round  39, Train loss: 1.739, Test loss: 1.650, Test accuracy: 81.33
Round  40, Train loss: 1.582, Test loss: 1.649, Test accuracy: 81.37
Round  41, Train loss: 1.630, Test loss: 1.649, Test accuracy: 81.35
Round  42, Train loss: 1.577, Test loss: 1.649, Test accuracy: 81.38
Round  43, Train loss: 1.502, Test loss: 1.637, Test accuracy: 82.67
Round  44, Train loss: 1.532, Test loss: 1.634, Test accuracy: 83.04
Round  45, Train loss: 1.583, Test loss: 1.632, Test accuracy: 83.09
Round  46, Train loss: 1.632, Test loss: 1.631, Test accuracy: 83.22
Round  47, Train loss: 1.581, Test loss: 1.630, Test accuracy: 83.30
Round  48, Train loss: 1.523, Test loss: 1.630, Test accuracy: 83.31
Round  49, Train loss: 1.685, Test loss: 1.631, Test accuracy: 83.12
Round  50, Train loss: 1.526, Test loss: 1.628, Test accuracy: 83.45
Round  51, Train loss: 1.634, Test loss: 1.622, Test accuracy: 84.25
Round  52, Train loss: 1.526, Test loss: 1.622, Test accuracy: 84.13
Round  53, Train loss: 1.577, Test loss: 1.622, Test accuracy: 84.16
Round  54, Train loss: 1.579, Test loss: 1.622, Test accuracy: 83.98
Round  55, Train loss: 1.523, Test loss: 1.621, Test accuracy: 84.20
Round  56, Train loss: 1.685, Test loss: 1.622, Test accuracy: 84.07
Round  57, Train loss: 1.630, Test loss: 1.620, Test accuracy: 84.24
Round  58, Train loss: 1.575, Test loss: 1.621, Test accuracy: 84.28
Round  59, Train loss: 1.682, Test loss: 1.622, Test accuracy: 84.17
Round  60, Train loss: 1.467, Test loss: 1.621, Test accuracy: 84.16
Round  61, Train loss: 1.637, Test loss: 1.612, Test accuracy: 84.98
Round  62, Train loss: 1.632, Test loss: 1.611, Test accuracy: 85.08
Round  63, Train loss: 1.576, Test loss: 1.611, Test accuracy: 85.00
Round  64, Train loss: 1.590, Test loss: 1.602, Test accuracy: 86.20
Round  65, Train loss: 1.583, Test loss: 1.602, Test accuracy: 86.12
Round  66, Train loss: 1.634, Test loss: 1.601, Test accuracy: 86.24
Round  67, Train loss: 1.470, Test loss: 1.601, Test accuracy: 86.26
Round  68, Train loss: 1.477, Test loss: 1.599, Test accuracy: 86.29
Round  69, Train loss: 1.682, Test loss: 1.601, Test accuracy: 86.12
Round  70, Train loss: 1.576, Test loss: 1.600, Test accuracy: 86.22
Round  71, Train loss: 1.521, Test loss: 1.600, Test accuracy: 86.22
Round  72, Train loss: 1.635, Test loss: 1.599, Test accuracy: 86.33
Round  73, Train loss: 1.529, Test loss: 1.599, Test accuracy: 86.46
Round  74, Train loss: 1.467, Test loss: 1.598, Test accuracy: 86.53
Round  75, Train loss: 1.527, Test loss: 1.598, Test accuracy: 86.46
Round  76, Train loss: 1.525, Test loss: 1.597, Test accuracy: 86.49
Round  77, Train loss: 1.634, Test loss: 1.597, Test accuracy: 86.53
Round  78, Train loss: 1.685, Test loss: 1.597, Test accuracy: 86.47
Round  79, Train loss: 1.579, Test loss: 1.597, Test accuracy: 86.51
Round  80, Train loss: 1.526, Test loss: 1.596, Test accuracy: 86.51
Round  81, Train loss: 1.577, Test loss: 1.596, Test accuracy: 86.53
Round  82, Train loss: 1.523, Test loss: 1.596, Test accuracy: 86.59
Round  83, Train loss: 1.470, Test loss: 1.596, Test accuracy: 86.57
Round  84, Train loss: 1.466, Test loss: 1.596, Test accuracy: 86.64
Round  85, Train loss: 1.522, Test loss: 1.596, Test accuracy: 86.62
Round  86, Train loss: 1.576, Test loss: 1.595, Test accuracy: 86.69
Round  87, Train loss: 1.627, Test loss: 1.595, Test accuracy: 86.60
Round  88, Train loss: 1.628, Test loss: 1.596, Test accuracy: 86.53
Round  89, Train loss: 1.629, Test loss: 1.595, Test accuracy: 86.62
Round  90, Train loss: 1.575, Test loss: 1.595, Test accuracy: 86.76
Round  91, Train loss: 1.525, Test loss: 1.594, Test accuracy: 86.83
Round  92, Train loss: 1.573, Test loss: 1.594, Test accuracy: 86.78
Round  93, Train loss: 1.519, Test loss: 1.594, Test accuracy: 86.72
Round  94, Train loss: 1.575, Test loss: 1.593, Test accuracy: 86.84
Round  95, Train loss: 1.572, Test loss: 1.593, Test accuracy: 86.79
Round  96, Train loss: 1.626, Test loss: 1.593, Test accuracy: 86.85
Round  97, Train loss: 1.575, Test loss: 1.593, Test accuracy: 86.84
Round  98, Train loss: 1.574, Test loss: 1.594, Test accuracy: 86.81/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.467, Test loss: 1.593, Test accuracy: 86.78
Final Round, Train loss: 1.566, Test loss: 1.589, Test accuracy: 87.29
Average accuracy final 10 rounds: 86.79833333333333 

1045.9263951778412
[0.9517464637756348, 1.9034929275512695, 2.8331997394561768, 3.762906551361084, 4.67919921875, 5.595491886138916, 6.499898195266724, 7.404304504394531, 8.257809400558472, 9.111314296722412, 9.942399501800537, 10.773484706878662, 11.592522621154785, 12.411560535430908, 13.242997646331787, 14.074434757232666, 14.904478788375854, 15.734522819519043, 16.590905904769897, 17.447288990020752, 18.26326084136963, 19.079232692718506, 19.905757904052734, 20.732283115386963, 21.58374285697937, 22.435202598571777, 23.273488759994507, 24.111774921417236, 24.94123363494873, 25.770692348480225, 26.597348928451538, 27.42400550842285, 28.249077320098877, 29.074149131774902, 29.891966342926025, 30.70978355407715, 31.538771152496338, 32.36775875091553, 33.20547819137573, 34.04319763183594, 34.87659740447998, 35.70999717712402, 36.51893901824951, 37.327880859375, 38.160701513290405, 38.99352216720581, 39.82003736495972, 40.64655256271362, 41.49580669403076, 42.3450608253479, 43.175297021865845, 44.00553321838379, 44.83624076843262, 45.666948318481445, 46.48128318786621, 47.29561805725098, 48.12407207489014, 48.9525260925293, 49.788684129714966, 50.624842166900635, 51.47840070724487, 52.33195924758911, 53.16532349586487, 53.998687744140625, 54.845425605773926, 55.69216346740723, 56.49999213218689, 57.30782079696655, 58.13075399398804, 58.95368719100952, 59.786043882369995, 60.61840057373047, 61.45135831832886, 62.284316062927246, 63.11445331573486, 63.94459056854248, 64.77398586273193, 65.60338115692139, 66.42808151245117, 67.25278186798096, 68.09734916687012, 68.94191646575928, 69.77806210517883, 70.61420774459839, 71.44878959655762, 72.28337144851685, 73.10382843017578, 73.92428541183472, 74.742103099823, 75.55992078781128, 76.38027000427246, 77.20061922073364, 78.04003977775574, 78.87946033477783, 79.71248197555542, 80.54550361633301, 81.37156963348389, 82.19763565063477, 83.01158118247986, 83.82552671432495, 84.63311910629272, 85.4407114982605, 86.27275562286377, 87.10479974746704, 87.9401843547821, 88.77556896209717, 89.60835576057434, 90.44114255905151, 91.25928401947021, 92.07742547988892, 92.90459179878235, 93.73175811767578, 94.55287146568298, 95.37398481369019, 96.20711040496826, 97.04023599624634, 97.86618733406067, 98.692138671875, 99.51015448570251, 100.32817029953003, 101.16073775291443, 101.99330520629883, 102.82675814628601, 103.6602110862732, 104.48987412452698, 105.31953716278076, 106.1600272655487, 107.00051736831665, 107.84176850318909, 108.68301963806152, 109.53940796852112, 110.39579629898071, 111.2339837551117, 112.07217121124268, 112.89895534515381, 113.72573947906494, 114.56559443473816, 115.40544939041138, 116.24525141716003, 117.08505344390869, 117.92163729667664, 118.75822114944458, 119.59836030006409, 120.4384994506836, 121.26671242713928, 122.09492540359497, 122.91670417785645, 123.73848295211792, 124.57408213615417, 125.40968132019043, 126.25010299682617, 127.09052467346191, 127.90528106689453, 128.72003746032715, 129.53486728668213, 130.3496971130371, 131.17184853553772, 131.99399995803833, 132.81616854667664, 133.63833713531494, 134.46542263031006, 135.29250812530518, 136.11566591262817, 136.93882369995117, 137.76057696342468, 138.5823302268982, 139.39724779129028, 140.21216535568237, 141.03864765167236, 141.86512994766235, 142.688560962677, 143.51199197769165, 144.33606624603271, 145.16014051437378, 145.97982788085938, 146.79951524734497, 147.61343908309937, 148.42736291885376, 149.24529767036438, 150.063232421875, 150.8750123977661, 151.68679237365723, 152.51161766052246, 153.3364429473877, 154.17460083961487, 155.01275873184204, 155.82767581939697, 156.6425929069519, 157.4720766544342, 158.3015604019165, 159.12835502624512, 159.95514965057373, 160.76624703407288, 161.57734441757202, 162.41011881828308, 163.24289321899414, 164.06423783302307, 164.885582447052, 165.73192715644836, 166.57827186584473, 167.84401178359985, 169.10975170135498]
[22.95, 22.95, 31.1, 31.1, 37.141666666666666, 37.141666666666666, 40.475, 40.475, 44.13333333333333, 44.13333333333333, 50.11666666666667, 50.11666666666667, 54.608333333333334, 54.608333333333334, 58.69166666666667, 58.69166666666667, 63.44166666666667, 63.44166666666667, 65.66666666666667, 65.66666666666667, 67.525, 67.525, 68.95833333333333, 68.95833333333333, 69.45833333333333, 69.45833333333333, 69.24166666666666, 69.24166666666666, 70.50833333333334, 70.50833333333334, 71.33333333333333, 71.33333333333333, 72.73333333333333, 72.73333333333333, 74.28333333333333, 74.28333333333333, 75.05, 75.05, 76.075, 76.075, 76.125, 76.125, 77.65, 77.65, 77.525, 77.525, 77.98333333333333, 77.98333333333333, 79.26666666666667, 79.26666666666667, 79.30833333333334, 79.30833333333334, 79.65833333333333, 79.65833333333333, 79.60833333333333, 79.60833333333333, 79.90833333333333, 79.90833333333333, 79.69166666666666, 79.69166666666666, 80.45, 80.45, 80.70833333333333, 80.70833333333333, 80.75833333333334, 80.75833333333334, 81.23333333333333, 81.23333333333333, 81.05833333333334, 81.05833333333334, 80.93333333333334, 80.93333333333334, 81.11666666666666, 81.11666666666666, 81.15, 81.15, 81.05, 81.05, 81.325, 81.325, 81.36666666666666, 81.36666666666666, 81.35, 81.35, 81.375, 81.375, 82.66666666666667, 82.66666666666667, 83.04166666666667, 83.04166666666667, 83.09166666666667, 83.09166666666667, 83.225, 83.225, 83.3, 83.3, 83.30833333333334, 83.30833333333334, 83.125, 83.125, 83.45, 83.45, 84.25, 84.25, 84.13333333333334, 84.13333333333334, 84.15833333333333, 84.15833333333333, 83.98333333333333, 83.98333333333333, 84.2, 84.2, 84.06666666666666, 84.06666666666666, 84.24166666666666, 84.24166666666666, 84.275, 84.275, 84.16666666666667, 84.16666666666667, 84.15833333333333, 84.15833333333333, 84.98333333333333, 84.98333333333333, 85.08333333333333, 85.08333333333333, 85.0, 85.0, 86.2, 86.2, 86.11666666666666, 86.11666666666666, 86.24166666666666, 86.24166666666666, 86.25833333333334, 86.25833333333334, 86.29166666666667, 86.29166666666667, 86.11666666666666, 86.11666666666666, 86.225, 86.225, 86.21666666666667, 86.21666666666667, 86.33333333333333, 86.33333333333333, 86.45833333333333, 86.45833333333333, 86.53333333333333, 86.53333333333333, 86.45833333333333, 86.45833333333333, 86.49166666666666, 86.49166666666666, 86.525, 86.525, 86.46666666666667, 86.46666666666667, 86.50833333333334, 86.50833333333334, 86.50833333333334, 86.50833333333334, 86.53333333333333, 86.53333333333333, 86.59166666666667, 86.59166666666667, 86.56666666666666, 86.56666666666666, 86.64166666666667, 86.64166666666667, 86.61666666666666, 86.61666666666666, 86.69166666666666, 86.69166666666666, 86.6, 86.6, 86.525, 86.525, 86.61666666666666, 86.61666666666666, 86.75833333333334, 86.75833333333334, 86.825, 86.825, 86.775, 86.775, 86.71666666666667, 86.71666666666667, 86.84166666666667, 86.84166666666667, 86.79166666666667, 86.79166666666667, 86.85, 86.85, 86.84166666666667, 86.84166666666667, 86.80833333333334, 86.80833333333334, 86.775, 86.775, 87.29166666666667, 87.29166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.179, Test loss: 2.229, Test accuracy: 35.17
Round   1, Train loss: 1.813, Test loss: 2.124, Test accuracy: 38.83
Round   2, Train loss: 1.900, Test loss: 2.089, Test accuracy: 40.13
Round   3, Train loss: 1.674, Test loss: 1.980, Test accuracy: 54.02
Round   4, Train loss: 1.658, Test loss: 1.877, Test accuracy: 64.78
Round   5, Train loss: 1.557, Test loss: 1.837, Test accuracy: 68.76
Round   6, Train loss: 1.669, Test loss: 1.772, Test accuracy: 75.69
Round   7, Train loss: 1.561, Test loss: 1.750, Test accuracy: 75.49
Round   8, Train loss: 1.591, Test loss: 1.714, Test accuracy: 79.23
Round   9, Train loss: 1.566, Test loss: 1.656, Test accuracy: 83.88
Round  10, Train loss: 1.535, Test loss: 1.652, Test accuracy: 83.59
Round  11, Train loss: 1.591, Test loss: 1.628, Test accuracy: 86.16
Round  12, Train loss: 1.582, Test loss: 1.621, Test accuracy: 86.47
Round  13, Train loss: 1.490, Test loss: 1.609, Test accuracy: 86.15
Round  14, Train loss: 1.549, Test loss: 1.585, Test accuracy: 88.92
Round  15, Train loss: 1.475, Test loss: 1.576, Test accuracy: 89.87
Round  16, Train loss: 1.475, Test loss: 1.575, Test accuracy: 89.62
Round  17, Train loss: 1.534, Test loss: 1.563, Test accuracy: 90.78
Round  18, Train loss: 1.468, Test loss: 1.562, Test accuracy: 90.65
Round  19, Train loss: 1.480, Test loss: 1.548, Test accuracy: 92.06
Round  20, Train loss: 1.473, Test loss: 1.545, Test accuracy: 92.26
Round  21, Train loss: 1.469, Test loss: 1.540, Test accuracy: 92.53
Round  22, Train loss: 1.471, Test loss: 1.539, Test accuracy: 92.67
Round  23, Train loss: 1.468, Test loss: 1.539, Test accuracy: 92.62
Round  24, Train loss: 1.521, Test loss: 1.538, Test accuracy: 92.68
Round  25, Train loss: 1.522, Test loss: 1.538, Test accuracy: 92.68
Round  26, Train loss: 1.468, Test loss: 1.538, Test accuracy: 92.68
Round  27, Train loss: 1.473, Test loss: 1.535, Test accuracy: 92.97
Round  28, Train loss: 1.520, Test loss: 1.534, Test accuracy: 92.93
Round  29, Train loss: 1.576, Test loss: 1.534, Test accuracy: 92.97
Round  30, Train loss: 1.518, Test loss: 1.534, Test accuracy: 92.97
Round  31, Train loss: 1.523, Test loss: 1.534, Test accuracy: 92.97
Round  32, Train loss: 1.465, Test loss: 1.534, Test accuracy: 92.97
Round  33, Train loss: 1.469, Test loss: 1.534, Test accuracy: 92.99
Round  34, Train loss: 1.467, Test loss: 1.533, Test accuracy: 93.02
Round  35, Train loss: 1.467, Test loss: 1.533, Test accuracy: 93.06
Round  36, Train loss: 1.465, Test loss: 1.533, Test accuracy: 93.05
Round  37, Train loss: 1.525, Test loss: 1.533, Test accuracy: 93.02
Round  38, Train loss: 1.519, Test loss: 1.533, Test accuracy: 92.98
Round  39, Train loss: 1.520, Test loss: 1.533, Test accuracy: 93.02
Round  40, Train loss: 1.521, Test loss: 1.533, Test accuracy: 93.02
Round  41, Train loss: 1.518, Test loss: 1.533, Test accuracy: 93.02
Round  42, Train loss: 1.467, Test loss: 1.532, Test accuracy: 93.04
Round  43, Train loss: 1.522, Test loss: 1.532, Test accuracy: 93.07
Round  44, Train loss: 1.467, Test loss: 1.532, Test accuracy: 93.03
Round  45, Train loss: 1.574, Test loss: 1.532, Test accuracy: 93.07
Round  46, Train loss: 1.573, Test loss: 1.532, Test accuracy: 93.06
Round  47, Train loss: 1.468, Test loss: 1.532, Test accuracy: 93.08
Round  48, Train loss: 1.574, Test loss: 1.532, Test accuracy: 93.06
Round  49, Train loss: 1.520, Test loss: 1.532, Test accuracy: 93.07
Round  50, Train loss: 1.522, Test loss: 1.532, Test accuracy: 93.07
Round  51, Train loss: 1.521, Test loss: 1.532, Test accuracy: 93.11
Round  52, Train loss: 1.520, Test loss: 1.532, Test accuracy: 93.12
Round  53, Train loss: 1.522, Test loss: 1.532, Test accuracy: 93.09
Round  54, Train loss: 1.518, Test loss: 1.531, Test accuracy: 93.08
Round  55, Train loss: 1.469, Test loss: 1.531, Test accuracy: 93.08
Round  56, Train loss: 1.519, Test loss: 1.532, Test accuracy: 93.08
Round  57, Train loss: 1.465, Test loss: 1.531, Test accuracy: 93.06
Round  58, Train loss: 1.521, Test loss: 1.531, Test accuracy: 93.07
Round  59, Train loss: 1.467, Test loss: 1.531, Test accuracy: 93.09
Round  60, Train loss: 1.467, Test loss: 1.531, Test accuracy: 93.06
Round  61, Train loss: 1.467, Test loss: 1.531, Test accuracy: 93.06
Round  62, Train loss: 1.571, Test loss: 1.531, Test accuracy: 93.07
Round  63, Train loss: 1.468, Test loss: 1.531, Test accuracy: 93.11
Round  64, Train loss: 1.468, Test loss: 1.531, Test accuracy: 93.13
Round  65, Train loss: 1.469, Test loss: 1.531, Test accuracy: 93.10
Round  66, Train loss: 1.465, Test loss: 1.531, Test accuracy: 93.10
Round  67, Train loss: 1.518, Test loss: 1.531, Test accuracy: 93.12
Round  68, Train loss: 1.519, Test loss: 1.531, Test accuracy: 93.09
Round  69, Train loss: 1.465, Test loss: 1.531, Test accuracy: 93.10
Round  70, Train loss: 1.467, Test loss: 1.531, Test accuracy: 93.09
Round  71, Train loss: 1.465, Test loss: 1.531, Test accuracy: 93.10
Round  72, Train loss: 1.520, Test loss: 1.531, Test accuracy: 93.08
Round  73, Train loss: 1.466, Test loss: 1.531, Test accuracy: 93.08
Round  74, Train loss: 1.464, Test loss: 1.531, Test accuracy: 93.08
Round  75, Train loss: 1.519, Test loss: 1.531, Test accuracy: 93.09
Round  76, Train loss: 1.522, Test loss: 1.531, Test accuracy: 93.08
Round  77, Train loss: 1.469, Test loss: 1.531, Test accuracy: 93.09
Round  78, Train loss: 1.465, Test loss: 1.531, Test accuracy: 93.12
Round  79, Train loss: 1.466, Test loss: 1.531, Test accuracy: 93.12
Round  80, Train loss: 1.468, Test loss: 1.531, Test accuracy: 93.09
Round  81, Train loss: 1.464, Test loss: 1.531, Test accuracy: 93.09
Round  82, Train loss: 1.573, Test loss: 1.531, Test accuracy: 93.08
Round  83, Train loss: 1.520, Test loss: 1.531, Test accuracy: 93.10
Round  84, Train loss: 1.519, Test loss: 1.531, Test accuracy: 93.09
Round  85, Train loss: 1.523, Test loss: 1.531, Test accuracy: 93.08
Round  86, Train loss: 1.467, Test loss: 1.531, Test accuracy: 93.08
Round  87, Train loss: 1.465, Test loss: 1.531, Test accuracy: 93.07
Round  88, Train loss: 1.517, Test loss: 1.531, Test accuracy: 93.06
Round  89, Train loss: 1.468, Test loss: 1.531, Test accuracy: 93.07
Round  90, Train loss: 1.518, Test loss: 1.531, Test accuracy: 93.07
Round  91, Train loss: 1.469, Test loss: 1.531, Test accuracy: 93.07
Round  92, Train loss: 1.469, Test loss: 1.531, Test accuracy: 93.07
Round  93, Train loss: 1.516, Test loss: 1.531, Test accuracy: 93.06
Round  94, Train loss: 1.469, Test loss: 1.531, Test accuracy: 93.06
Round  95, Train loss: 1.467, Test loss: 1.531, Test accuracy: 93.08
Round  96, Train loss: 1.574, Test loss: 1.531, Test accuracy: 93.08
Round  97, Train loss: 1.465, Test loss: 1.531, Test accuracy: 93.07
Round  98, Train loss: 1.464, Test loss: 1.531, Test accuracy: 93.08
Round  99, Train loss: 1.468, Test loss: 1.531, Test accuracy: 93.08/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.499, Test loss: 1.531, Test accuracy: 93.04
Average accuracy final 10 rounds: 93.07083333333335 

1053.103678703308
[1.0728728771209717, 2.1457457542419434, 3.09189772605896, 4.038049697875977, 4.947268724441528, 5.85648775100708, 6.838525056838989, 7.820562362670898, 8.719449520111084, 9.61833667755127, 10.534583568572998, 11.450830459594727, 12.357478857040405, 13.264127254486084, 14.177605152130127, 15.09108304977417, 15.949421882629395, 16.80776071548462, 17.6589994430542, 18.51023817062378, 19.35053324699402, 20.190828323364258, 21.026882886886597, 21.862937450408936, 22.731086254119873, 23.59923505783081, 24.426024198532104, 25.2528133392334, 26.084619522094727, 26.916425704956055, 27.730332851409912, 28.54423999786377, 29.37262797355652, 30.201015949249268, 31.029356002807617, 31.857696056365967, 32.68872141838074, 33.51974678039551, 34.35170316696167, 35.18365955352783, 36.019261837005615, 36.8548641204834, 37.67484164237976, 38.49481916427612, 39.32619571685791, 40.1575722694397, 40.98643732070923, 41.81530237197876, 42.67216730117798, 43.5290322303772, 44.37882351875305, 45.228614807128906, 46.05686855316162, 46.885122299194336, 47.750903367996216, 48.616684436798096, 49.44734787940979, 50.278011322021484, 51.101381063461304, 51.92475080490112, 52.764103412628174, 53.603456020355225, 54.440455198287964, 55.2774543762207, 56.13415741920471, 56.99086046218872, 57.810113191604614, 58.62936592102051, 59.455426931381226, 60.28148794174194, 61.11525201797485, 61.949016094207764, 62.78547501564026, 63.621933937072754, 64.4587709903717, 65.29560804367065, 66.10289907455444, 66.91019010543823, 67.73861384391785, 68.56703758239746, 69.39292049407959, 70.21880340576172, 71.05728840827942, 71.89577341079712, 72.73849534988403, 73.58121728897095, 74.41198492050171, 75.24275255203247, 76.05851984024048, 76.87428712844849, 77.70357823371887, 78.53286933898926, 79.36556315422058, 80.1982569694519, 81.02778053283691, 81.85730409622192, 82.69182181358337, 83.52633953094482, 84.35723876953125, 85.18813800811768, 86.00891637802124, 86.8296947479248, 87.68612217903137, 88.54254961013794, 89.38243508338928, 90.22232055664062, 91.06359481811523, 91.90486907958984, 92.74949264526367, 93.5941162109375, 94.4337842464447, 95.2734522819519, 96.1217999458313, 96.9701476097107, 97.80893206596375, 98.6477165222168, 99.49025416374207, 100.33279180526733, 101.1854395866394, 102.03808736801147, 102.88902592658997, 103.73996448516846, 104.55625629425049, 105.37254810333252, 106.20673632621765, 107.04092454910278, 107.89341044425964, 108.7458963394165, 109.62168836593628, 110.49748039245605, 111.32881903648376, 112.16015768051147, 112.99162864685059, 113.8230996131897, 114.64414811134338, 115.46519660949707, 116.30102920532227, 117.13686180114746, 117.97132635116577, 118.80579090118408, 119.65517902374268, 120.50456714630127, 121.33704400062561, 122.16952085494995, 123.00629305839539, 123.84306526184082, 124.67673230171204, 125.51039934158325, 126.34774661064148, 127.1850938796997, 128.038183927536, 128.89127397537231, 129.73516035079956, 130.5790467262268, 131.4337122440338, 132.28837776184082, 133.1136622428894, 133.938946723938, 134.7660903930664, 135.59323406219482, 136.4332709312439, 137.27330780029297, 138.11309909820557, 138.95289039611816, 139.772958278656, 140.59302616119385, 141.41221046447754, 142.23139476776123, 143.0593843460083, 143.88737392425537, 144.72404956817627, 145.56072521209717, 146.38370275497437, 147.20668029785156, 148.04885339736938, 148.8910264968872, 149.73765754699707, 150.58428859710693, 151.38820385932922, 152.1921191215515, 153.007577419281, 153.8230357170105, 154.65601682662964, 155.48899793624878, 156.3162064552307, 157.14341497421265, 157.9675977230072, 158.79178047180176, 159.61292028427124, 160.43406009674072, 161.25972890853882, 162.0853977203369, 162.91662335395813, 163.74784898757935, 164.56706047058105, 165.38627195358276, 166.2160894870758, 167.04590702056885, 167.8795838356018, 168.71326065063477, 170.02645325660706, 171.33964586257935]
[35.175, 35.175, 38.825, 38.825, 40.13333333333333, 40.13333333333333, 54.016666666666666, 54.016666666666666, 64.78333333333333, 64.78333333333333, 68.75833333333334, 68.75833333333334, 75.69166666666666, 75.69166666666666, 75.49166666666666, 75.49166666666666, 79.23333333333333, 79.23333333333333, 83.88333333333334, 83.88333333333334, 83.59166666666667, 83.59166666666667, 86.15833333333333, 86.15833333333333, 86.475, 86.475, 86.15, 86.15, 88.91666666666667, 88.91666666666667, 89.86666666666666, 89.86666666666666, 89.625, 89.625, 90.775, 90.775, 90.65, 90.65, 92.05833333333334, 92.05833333333334, 92.25833333333334, 92.25833333333334, 92.53333333333333, 92.53333333333333, 92.66666666666667, 92.66666666666667, 92.61666666666666, 92.61666666666666, 92.68333333333334, 92.68333333333334, 92.68333333333334, 92.68333333333334, 92.68333333333334, 92.68333333333334, 92.96666666666667, 92.96666666666667, 92.93333333333334, 92.93333333333334, 92.96666666666667, 92.96666666666667, 92.96666666666667, 92.96666666666667, 92.96666666666667, 92.96666666666667, 92.96666666666667, 92.96666666666667, 92.99166666666666, 92.99166666666666, 93.01666666666667, 93.01666666666667, 93.05833333333334, 93.05833333333334, 93.05, 93.05, 93.01666666666667, 93.01666666666667, 92.98333333333333, 92.98333333333333, 93.01666666666667, 93.01666666666667, 93.01666666666667, 93.01666666666667, 93.01666666666667, 93.01666666666667, 93.04166666666667, 93.04166666666667, 93.06666666666666, 93.06666666666666, 93.03333333333333, 93.03333333333333, 93.06666666666666, 93.06666666666666, 93.05833333333334, 93.05833333333334, 93.08333333333333, 93.08333333333333, 93.05833333333334, 93.05833333333334, 93.06666666666666, 93.06666666666666, 93.06666666666666, 93.06666666666666, 93.10833333333333, 93.10833333333333, 93.11666666666666, 93.11666666666666, 93.09166666666667, 93.09166666666667, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.075, 93.075, 93.05833333333334, 93.05833333333334, 93.06666666666666, 93.06666666666666, 93.09166666666667, 93.09166666666667, 93.05833333333334, 93.05833333333334, 93.05833333333334, 93.05833333333334, 93.06666666666666, 93.06666666666666, 93.10833333333333, 93.10833333333333, 93.13333333333334, 93.13333333333334, 93.1, 93.1, 93.1, 93.1, 93.125, 93.125, 93.09166666666667, 93.09166666666667, 93.1, 93.1, 93.09166666666667, 93.09166666666667, 93.1, 93.1, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.09166666666667, 93.09166666666667, 93.075, 93.075, 93.09166666666667, 93.09166666666667, 93.11666666666666, 93.11666666666666, 93.125, 93.125, 93.09166666666667, 93.09166666666667, 93.09166666666667, 93.09166666666667, 93.08333333333333, 93.08333333333333, 93.1, 93.1, 93.09166666666667, 93.09166666666667, 93.075, 93.075, 93.08333333333333, 93.08333333333333, 93.06666666666666, 93.06666666666666, 93.05833333333334, 93.05833333333334, 93.06666666666666, 93.06666666666666, 93.06666666666666, 93.06666666666666, 93.06666666666666, 93.06666666666666, 93.06666666666666, 93.06666666666666, 93.05833333333334, 93.05833333333334, 93.05833333333334, 93.05833333333334, 93.08333333333333, 93.08333333333333, 93.075, 93.075, 93.06666666666666, 93.06666666666666, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.08333333333333, 93.04166666666667, 93.04166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.419, Test loss: 2.188, Test accuracy: 31.38
Round   1, Train loss: 1.478, Test loss: 2.113, Test accuracy: 45.46
Round   2, Train loss: 1.275, Test loss: 2.008, Test accuracy: 55.98
Round   3, Train loss: 1.265, Test loss: 1.927, Test accuracy: 62.73
Round   4, Train loss: 1.335, Test loss: 1.919, Test accuracy: 60.50
Round   5, Train loss: 1.401, Test loss: 1.860, Test accuracy: 65.40
Round   6, Train loss: 1.192, Test loss: 1.839, Test accuracy: 65.56
Round   7, Train loss: 1.332, Test loss: 1.825, Test accuracy: 67.44
Round   8, Train loss: 1.351, Test loss: 1.826, Test accuracy: 65.97
Round   9, Train loss: 1.309, Test loss: 1.821, Test accuracy: 66.55
Round  10, Train loss: 1.396, Test loss: 1.819, Test accuracy: 66.65
Round  11, Train loss: 1.311, Test loss: 1.804, Test accuracy: 68.77
Round  12, Train loss: 1.350, Test loss: 1.806, Test accuracy: 68.35
Round  13, Train loss: 1.350, Test loss: 1.803, Test accuracy: 67.92
Round  14, Train loss: 1.301, Test loss: 1.796, Test accuracy: 69.09
Round  15, Train loss: 1.233, Test loss: 1.785, Test accuracy: 69.73
Round  16, Train loss: 1.230, Test loss: 1.777, Test accuracy: 69.38
Round  17, Train loss: 1.308, Test loss: 1.777, Test accuracy: 69.28
Round  18, Train loss: 1.268, Test loss: 1.775, Test accuracy: 69.38
Round  19, Train loss: 1.391, Test loss: 1.772, Test accuracy: 69.42
Round  20, Train loss: 1.387, Test loss: 1.772, Test accuracy: 69.52
Round  21, Train loss: 1.392, Test loss: 1.772, Test accuracy: 69.55
Round  22, Train loss: 1.310, Test loss: 1.771, Test accuracy: 69.39
Round  23, Train loss: 1.268, Test loss: 1.770, Test accuracy: 69.43
Round  24, Train loss: 1.227, Test loss: 1.769, Test accuracy: 69.46
Round  25, Train loss: 1.227, Test loss: 1.770, Test accuracy: 69.31
Round  26, Train loss: 1.388, Test loss: 1.770, Test accuracy: 69.38
Round  27, Train loss: 1.263, Test loss: 1.771, Test accuracy: 69.20
Round  28, Train loss: 1.309, Test loss: 1.770, Test accuracy: 69.32
Round  29, Train loss: 1.348, Test loss: 1.770, Test accuracy: 69.33
Round  30, Train loss: 1.350, Test loss: 1.768, Test accuracy: 69.37
Round  31, Train loss: 1.223, Test loss: 1.768, Test accuracy: 69.34
Round  32, Train loss: 1.351, Test loss: 1.769, Test accuracy: 69.22
Round  33, Train loss: 1.306, Test loss: 1.769, Test accuracy: 69.28
Round  34, Train loss: 1.185, Test loss: 1.770, Test accuracy: 69.19
Round  35, Train loss: 1.306, Test loss: 1.771, Test accuracy: 69.03
Round  36, Train loss: 1.387, Test loss: 1.770, Test accuracy: 69.07
Round  37, Train loss: 1.389, Test loss: 1.770, Test accuracy: 69.06
Round  38, Train loss: 1.266, Test loss: 1.771, Test accuracy: 69.03
Round  39, Train loss: 1.307, Test loss: 1.771, Test accuracy: 69.01
Round  40, Train loss: 1.387, Test loss: 1.771, Test accuracy: 68.97
Round  41, Train loss: 1.306, Test loss: 1.770, Test accuracy: 69.06
Round  42, Train loss: 1.223, Test loss: 1.771, Test accuracy: 68.93
Round  43, Train loss: 1.304, Test loss: 1.771, Test accuracy: 68.89
Round  44, Train loss: 1.307, Test loss: 1.771, Test accuracy: 68.94
Round  45, Train loss: 1.267, Test loss: 1.774, Test accuracy: 68.69
Round  46, Train loss: 1.349, Test loss: 1.774, Test accuracy: 68.64
Round  47, Train loss: 1.345, Test loss: 1.774, Test accuracy: 68.62
Round  48, Train loss: 1.429, Test loss: 1.775, Test accuracy: 68.58
Round  49, Train loss: 1.182, Test loss: 1.774, Test accuracy: 68.63
Round  50, Train loss: 1.307, Test loss: 1.775, Test accuracy: 68.63
Round  51, Train loss: 1.429, Test loss: 1.775, Test accuracy: 68.49
Round  52, Train loss: 1.305, Test loss: 1.774, Test accuracy: 68.64
Round  53, Train loss: 1.307, Test loss: 1.774, Test accuracy: 68.63
Round  54, Train loss: 1.306, Test loss: 1.775, Test accuracy: 68.58
Round  55, Train loss: 1.223, Test loss: 1.777, Test accuracy: 68.38
Round  56, Train loss: 1.265, Test loss: 1.778, Test accuracy: 68.17
Round  57, Train loss: 1.345, Test loss: 1.777, Test accuracy: 68.39
Round  58, Train loss: 1.307, Test loss: 1.777, Test accuracy: 68.39
Round  59, Train loss: 1.429, Test loss: 1.777, Test accuracy: 68.34
Round  60, Train loss: 1.306, Test loss: 1.777, Test accuracy: 68.41
Round  61, Train loss: 1.386, Test loss: 1.777, Test accuracy: 68.43
Round  62, Train loss: 1.347, Test loss: 1.777, Test accuracy: 68.36
Round  63, Train loss: 1.266, Test loss: 1.779, Test accuracy: 68.23
Round  64, Train loss: 1.346, Test loss: 1.780, Test accuracy: 68.10
Round  65, Train loss: 1.306, Test loss: 1.779, Test accuracy: 68.06
Round  66, Train loss: 1.267, Test loss: 1.780, Test accuracy: 68.09
Round  67, Train loss: 1.305, Test loss: 1.780, Test accuracy: 68.03
Round  68, Train loss: 1.346, Test loss: 1.781, Test accuracy: 67.89
Round  69, Train loss: 1.263, Test loss: 1.783, Test accuracy: 67.83
Round  70, Train loss: 1.223, Test loss: 1.782, Test accuracy: 67.80
Round  71, Train loss: 1.305, Test loss: 1.781, Test accuracy: 67.92
Round  72, Train loss: 1.306, Test loss: 1.783, Test accuracy: 67.70
Round  73, Train loss: 1.283, Test loss: 1.774, Test accuracy: 68.78
Round  74, Train loss: 1.389, Test loss: 1.773, Test accuracy: 68.70
Round  75, Train loss: 1.264, Test loss: 1.769, Test accuracy: 69.18
Round  76, Train loss: 1.304, Test loss: 1.769, Test accuracy: 69.18
Round  77, Train loss: 1.267, Test loss: 1.773, Test accuracy: 68.80
Round  78, Train loss: 1.306, Test loss: 1.774, Test accuracy: 68.56
Round  79, Train loss: 1.239, Test loss: 1.773, Test accuracy: 68.78
Round  80, Train loss: 1.274, Test loss: 1.763, Test accuracy: 70.47
Round  81, Train loss: 1.283, Test loss: 1.760, Test accuracy: 70.62
Round  82, Train loss: 1.268, Test loss: 1.755, Test accuracy: 70.86
Round  83, Train loss: 1.307, Test loss: 1.755, Test accuracy: 70.83
Round  84, Train loss: 1.227, Test loss: 1.764, Test accuracy: 70.00
Round  85, Train loss: 1.271, Test loss: 1.760, Test accuracy: 70.60
Round  86, Train loss: 1.227, Test loss: 1.758, Test accuracy: 70.78
Round  87, Train loss: 1.306, Test loss: 1.757, Test accuracy: 70.88
Round  88, Train loss: 1.308, Test loss: 1.751, Test accuracy: 71.52
Round  89, Train loss: 1.223, Test loss: 1.750, Test accuracy: 71.53
Round  90, Train loss: 1.267, Test loss: 1.751, Test accuracy: 71.63
Round  91, Train loss: 1.268, Test loss: 1.745, Test accuracy: 71.84
Round  92, Train loss: 1.308, Test loss: 1.749, Test accuracy: 71.29
Round  93, Train loss: 1.348, Test loss: 1.747, Test accuracy: 71.54
Round  94, Train loss: 1.163, Test loss: 1.751, Test accuracy: 71.21
Round  95, Train loss: 1.265, Test loss: 1.752, Test accuracy: 71.08
Round  96, Train loss: 1.268, Test loss: 1.763, Test accuracy: 69.64
Round  97, Train loss: 1.265, Test loss: 1.765, Test accuracy: 69.42
Round  98, Train loss: 1.267, Test loss: 1.766, Test accuracy: 69.33
Round  99, Train loss: 1.308, Test loss: 1.767, Test accuracy: 69.16
Final Round, Train loss: 1.249, Test loss: 1.771, Test accuracy: 68.83
Average accuracy final 10 rounds: 70.615
1368.5680017471313
[]
[31.375, 45.458333333333336, 55.983333333333334, 62.733333333333334, 60.5, 65.4, 65.55833333333334, 67.44166666666666, 65.975, 66.55, 66.65, 68.76666666666667, 68.35, 67.925, 69.09166666666667, 69.73333333333333, 69.38333333333334, 69.275, 69.375, 69.41666666666667, 69.51666666666667, 69.55, 69.39166666666667, 69.43333333333334, 69.45833333333333, 69.30833333333334, 69.375, 69.2, 69.31666666666666, 69.33333333333333, 69.36666666666666, 69.34166666666667, 69.21666666666667, 69.275, 69.19166666666666, 69.03333333333333, 69.06666666666666, 69.05833333333334, 69.025, 69.00833333333334, 68.975, 69.05833333333334, 68.93333333333334, 68.89166666666667, 68.94166666666666, 68.69166666666666, 68.64166666666667, 68.625, 68.58333333333333, 68.63333333333334, 68.63333333333334, 68.49166666666666, 68.64166666666667, 68.63333333333334, 68.58333333333333, 68.38333333333334, 68.175, 68.39166666666667, 68.39166666666667, 68.34166666666667, 68.40833333333333, 68.43333333333334, 68.35833333333333, 68.23333333333333, 68.1, 68.05833333333334, 68.09166666666667, 68.025, 67.89166666666667, 67.825, 67.8, 67.925, 67.7, 68.78333333333333, 68.7, 69.18333333333334, 69.18333333333334, 68.8, 68.55833333333334, 68.78333333333333, 70.46666666666667, 70.61666666666666, 70.85833333333333, 70.83333333333333, 70.0, 70.6, 70.775, 70.88333333333334, 71.51666666666667, 71.525, 71.63333333333334, 71.84166666666667, 71.29166666666667, 71.54166666666667, 71.20833333333333, 71.075, 69.64166666666667, 69.425, 69.33333333333333, 69.15833333333333, 68.83333333333333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.151, Test loss: 2.146, Test accuracy: 30.70
Round   1, Train loss: 1.996, Test loss: 2.118, Test accuracy: 32.58
Round   2, Train loss: 1.792, Test loss: 2.079, Test accuracy: 38.24
Round   3, Train loss: 1.431, Test loss: 2.059, Test accuracy: 39.08
Round   4, Train loss: 1.425, Test loss: 1.913, Test accuracy: 56.98
Round   5, Train loss: 1.164, Test loss: 1.844, Test accuracy: 68.99
Round   6, Train loss: 1.387, Test loss: 1.802, Test accuracy: 72.50
Round   7, Train loss: 1.412, Test loss: 1.872, Test accuracy: 64.65
Round   8, Train loss: 1.427, Test loss: 1.931, Test accuracy: 58.83
Round   9, Train loss: 0.637, Test loss: 1.807, Test accuracy: 71.10
Round  10, Train loss: 0.919, Test loss: 1.888, Test accuracy: 66.60
Round  11, Train loss: 0.071, Test loss: 1.823, Test accuracy: 66.68
Round  12, Train loss: 0.380, Test loss: 1.766, Test accuracy: 73.74
Round  13, Train loss: 0.180, Test loss: 1.718, Test accuracy: 76.76
Round  14, Train loss: 0.659, Test loss: 1.663, Test accuracy: 82.85
Round  15, Train loss: 0.723, Test loss: 1.701, Test accuracy: 78.75
Round  16, Train loss: 0.137, Test loss: 1.682, Test accuracy: 79.88
Round  17, Train loss: 0.053, Test loss: 1.683, Test accuracy: 79.05
Round  18, Train loss: -0.222, Test loss: 1.656, Test accuracy: 80.69
Round  19, Train loss: -0.268, Test loss: 1.640, Test accuracy: 81.92
Round  20, Train loss: -0.598, Test loss: 1.579, Test accuracy: 88.72
Round  21, Train loss: -0.451, Test loss: 1.595, Test accuracy: 87.07
Round  22, Train loss: -1.418, Test loss: 1.578, Test accuracy: 88.56
Round  23, Train loss: -1.002, Test loss: 1.581, Test accuracy: 88.69
Round  24, Train loss: -1.380, Test loss: 1.578, Test accuracy: 88.91
Round  25, Train loss: -0.822, Test loss: 1.619, Test accuracy: 84.60
Round  26, Train loss: -1.487, Test loss: 1.617, Test accuracy: 84.77
Round  27, Train loss: -0.987, Test loss: 1.617, Test accuracy: 84.67
Round  28, Train loss: -1.222, Test loss: 1.571, Test accuracy: 89.32
Round  29, Train loss: -1.748, Test loss: 1.573, Test accuracy: 89.08
Round  30, Train loss: -1.850, Test loss: 1.573, Test accuracy: 89.00
Round  31, Train loss: -1.816, Test loss: 1.573, Test accuracy: 88.97
Round  32, Train loss: -2.255, Test loss: 1.553, Test accuracy: 90.98
Round  33, Train loss: -1.784, Test loss: 1.600, Test accuracy: 86.24
Round  34, Train loss: -1.755, Test loss: 1.581, Test accuracy: 87.97
Round  35, Train loss: -2.642, Test loss: 1.550, Test accuracy: 90.99
Round  36, Train loss: -2.506, Test loss: 1.540, Test accuracy: 92.09
Round  37, Train loss: -2.426, Test loss: 1.553, Test accuracy: 90.78
Round  38, Train loss: -2.046, Test loss: 1.567, Test accuracy: 89.42
Round  39, Train loss: -2.140, Test loss: 1.567, Test accuracy: 89.33
Round  40, Train loss: -2.266, Test loss: 1.582, Test accuracy: 87.82
Round  41, Train loss: -2.012, Test loss: 1.581, Test accuracy: 87.96
Round  42, Train loss: -2.380, Test loss: 1.579, Test accuracy: 88.25
Round  43, Train loss: -2.052, Test loss: 1.593, Test accuracy: 86.78
Round  44, Train loss: -1.735, Test loss: 1.609, Test accuracy: 85.15
Round  45, Train loss: -1.729, Test loss: 1.548, Test accuracy: 91.27
Round  46, Train loss: -2.068, Test loss: 1.580, Test accuracy: 88.11
Round  47, Train loss: -1.888, Test loss: 1.580, Test accuracy: 88.12
Round  48, Train loss: -2.188, Test loss: 1.552, Test accuracy: 90.85
Round  49, Train loss: -1.860, Test loss: 1.569, Test accuracy: 89.18
Round  50, Train loss: -2.843, Test loss: 1.582, Test accuracy: 87.89
Round  51, Train loss: -2.206, Test loss: 1.554, Test accuracy: 90.72
Round  52, Train loss: -2.145, Test loss: 1.567, Test accuracy: 89.36
Round  53, Train loss: -2.036, Test loss: 1.596, Test accuracy: 86.48
Round  54, Train loss: -1.897, Test loss: 1.596, Test accuracy: 86.45
Round  55, Train loss: -2.428, Test loss: 1.581, Test accuracy: 88.02
Round  56, Train loss: -2.718, Test loss: 1.584, Test accuracy: 87.72
Round  57, Train loss: -1.478, Test loss: 1.550, Test accuracy: 91.15
Round  58, Train loss: -1.836, Test loss: 1.550, Test accuracy: 91.05
Round  59, Train loss: -2.260, Test loss: 1.536, Test accuracy: 92.48
Round  60, Train loss: -1.565, Test loss: 1.565, Test accuracy: 89.58
Round  61, Train loss: -1.839, Test loss: 1.582, Test accuracy: 87.92
Round  62, Train loss: -1.656, Test loss: 1.552, Test accuracy: 90.93
Round  63, Train loss: -1.714, Test loss: 1.538, Test accuracy: 92.32
Round  64, Train loss: -2.063, Test loss: 1.525, Test accuracy: 93.62
Round  65, Train loss: -2.189, Test loss: 1.555, Test accuracy: 90.55
Round  66, Train loss: -1.545, Test loss: 1.580, Test accuracy: 88.06
Round  67, Train loss: -2.197, Test loss: 1.583, Test accuracy: 87.78
Round  68, Train loss: -1.969, Test loss: 1.570, Test accuracy: 89.08
Round  69, Train loss: -2.238, Test loss: 1.539, Test accuracy: 92.17
Round  70, Train loss: -2.109, Test loss: 1.553, Test accuracy: 90.83
Round  71, Train loss: -2.242, Test loss: 1.569, Test accuracy: 89.22
Round  72, Train loss: -2.230, Test loss: 1.552, Test accuracy: 90.80
Round  73, Train loss: -1.753, Test loss: 1.565, Test accuracy: 89.54
Round  74, Train loss: -3.303, Test loss: 1.582, Test accuracy: 87.87
Round  75, Train loss: -1.715, Test loss: 1.579, Test accuracy: 88.13
Round  76, Train loss: -1.584, Test loss: 1.581, Test accuracy: 88.03
Round  77, Train loss: -2.479, Test loss: 1.567, Test accuracy: 89.39
Round  78, Train loss: -2.768, Test loss: 1.565, Test accuracy: 89.61
Round  79, Train loss: -2.441, Test loss: 1.537, Test accuracy: 92.42
Round  80, Train loss: -2.139, Test loss: 1.553, Test accuracy: 90.81
Round  81, Train loss: -1.859, Test loss: 1.568, Test accuracy: 89.40
Round  82, Train loss: -2.618, Test loss: 1.584, Test accuracy: 87.76
Round  83, Train loss: -1.729, Test loss: 1.582, Test accuracy: 87.85
Round  84, Train loss: -1.519, Test loss: 1.582, Test accuracy: 87.85
Round  85, Train loss: -2.028, Test loss: 1.540, Test accuracy: 92.04
Round  86, Train loss: -2.170, Test loss: 1.553, Test accuracy: 90.76
Round  87, Train loss: -2.529, Test loss: 1.524, Test accuracy: 93.74
Round  88, Train loss: -1.574, Test loss: 1.536, Test accuracy: 92.47
Round  89, Train loss: -1.251, Test loss: 1.547, Test accuracy: 91.32
Round  90, Train loss: -1.889, Test loss: 1.548, Test accuracy: 91.30
Round  91, Train loss: -1.983, Test loss: 1.536, Test accuracy: 92.49
Round  92, Train loss: -1.378, Test loss: 1.541, Test accuracy: 92.00
Round  93, Train loss: -1.413, Test loss: 1.552, Test accuracy: 90.88
Round  94, Train loss: -1.566, Test loss: 1.537, Test accuracy: 92.35
Round  95, Train loss: -1.238, Test loss: 1.525, Test accuracy: 93.62
Round  96, Train loss: -1.645, Test loss: 1.540, Test accuracy: 92.06
Round  97, Train loss: -2.042, Test loss: 1.539, Test accuracy: 92.18
Round  98, Train loss: -1.594, Test loss: 1.538, Test accuracy: 92.23
Round  99, Train loss: -2.355, Test loss: 1.521, Test accuracy: 93.94
Final Round, Train loss: 1.682, Test loss: 1.612, Test accuracy: 85.28
Average accuracy final 10 rounds: 92.30583333333333
Average global accuracy final 10 rounds: 92.30583333333333
1022.6634237766266
[]
[30.7, 32.575, 38.24166666666667, 39.075, 56.975, 68.99166666666666, 72.5, 64.65, 58.833333333333336, 71.1, 66.6, 66.68333333333334, 73.74166666666666, 76.75833333333334, 82.85, 78.75, 79.875, 79.05, 80.69166666666666, 81.925, 88.71666666666667, 87.06666666666666, 88.55833333333334, 88.69166666666666, 88.90833333333333, 84.6, 84.76666666666667, 84.675, 89.31666666666666, 89.08333333333333, 89.0, 88.96666666666667, 90.98333333333333, 86.24166666666666, 87.975, 90.99166666666666, 92.09166666666667, 90.78333333333333, 89.41666666666667, 89.325, 87.81666666666666, 87.95833333333333, 88.25, 86.775, 85.15, 91.26666666666667, 88.10833333333333, 88.125, 90.85, 89.18333333333334, 87.89166666666667, 90.71666666666667, 89.35833333333333, 86.48333333333333, 86.45, 88.01666666666667, 87.725, 91.15, 91.05, 92.48333333333333, 89.58333333333333, 87.91666666666667, 90.93333333333334, 92.31666666666666, 93.625, 90.55, 88.05833333333334, 87.78333333333333, 89.075, 92.16666666666667, 90.83333333333333, 89.225, 90.8, 89.54166666666667, 87.86666666666666, 88.13333333333334, 88.03333333333333, 89.39166666666667, 89.60833333333333, 92.41666666666667, 90.80833333333334, 89.4, 87.75833333333334, 87.85, 87.85, 92.04166666666667, 90.75833333333334, 93.74166666666666, 92.46666666666667, 91.31666666666666, 91.3, 92.49166666666666, 92.0, 90.875, 92.35, 93.625, 92.05833333333334, 92.18333333333334, 92.23333333333333, 93.94166666666666, 85.275]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.67
Round   0, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.67
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round   2, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.67
Round   2, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.67
Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round   4, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round   4, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round   5, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.67
Round   5, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round   7, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round   8, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round   8, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round   9, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.67
Round   9, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  10, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round  10, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  11, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.67
Round  11, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  12, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.67
Round  12, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  13, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.67
Round  13, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.67
Round  14, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.67
Round  14, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  15, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round  15, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  16, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round  16, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  17, Train loss: 2.305, Test loss: 2.303, Test accuracy: 6.67
Round  17, Global train loss: 2.305, Global test loss: 2.303, Global test accuracy: 6.67
Round  18, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.67
Round  18, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.67
Round  19, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round  19, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  20, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.67
Round  20, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  21, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.67
Round  21, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  22, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.67
Round  22, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  23, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.67
Round  23, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  24, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.67
Round  24, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  25, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.67
Round  25, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.67
Round  26, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.68
Round  26, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  27, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.68
Round  27, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  28, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.68
Round  28, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.67
Round  29, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.68
Round  29, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  30, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.68
Round  30, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  31, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.68
Round  31, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  32, Train loss: 2.304, Test loss: 2.303, Test accuracy: 6.68
Round  32, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.67
Round  33, Train loss: 2.300, Test loss: 2.303, Test accuracy: 6.68
Round  33, Global train loss: 2.300, Global test loss: 2.303, Global test accuracy: 6.67
Round  34, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.68
Round  34, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  35, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.68
Round  35, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  36, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.68
Round  36, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  37, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.67
Round  37, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  38, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.67
Round  38, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  39, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.67
Round  39, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  40, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.67
Round  40, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  41, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.67
Round  41, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  42, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.68
Round  42, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  43, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.68
Round  43, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  44, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.68
Round  44, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  45, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.69
Round  45, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  46, Train loss: 2.300, Test loss: 2.303, Test accuracy: 6.69
Round  46, Global train loss: 2.300, Global test loss: 2.303, Global test accuracy: 6.67
Round  47, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.69
Round  47, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  48, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.69
Round  48, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  49, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.69
Round  49, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  50, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.69
Round  50, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  51, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.69
Round  51, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  52, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.68
Round  52, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  53, Train loss: 2.301, Test loss: 2.303, Test accuracy: 6.68
Round  53, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.67
Round  54, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.69
Round  54, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  55, Train loss: 2.302, Test loss: 2.303, Test accuracy: 6.69
Round  55, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.67
Round  56, Train loss: 2.303, Test loss: 2.303, Test accuracy: 6.69
Round  56, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  57, Train loss: 2.304, Test loss: 2.302, Test accuracy: 6.69
Round  57, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.67
Round  58, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.69
Round  58, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  59, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.69
Round  59, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  60, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.69
Round  60, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.67
Round  61, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.70
Round  61, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.68
Round  62, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.70
Round  62, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.68
Round  63, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.70
Round  63, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.69
Round  64, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.71
Round  64, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.69
Round  65, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.71
Round  65, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.69
Round  66, Train loss: 2.304, Test loss: 2.302, Test accuracy: 6.71
Round  66, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 6.69
Round  67, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.71
Round  67, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.70
Round  68, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.71
Round  68, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 6.69
Round  69, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.70
Round  69, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 6.70
Round  70, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.70
Round  70, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 6.69
Round  71, Train loss: 2.304, Test loss: 2.302, Test accuracy: 6.71
Round  71, Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 6.69
Round  72, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.70
Round  72, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 6.70
Round  73, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.71
Round  73, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 6.70
Round  74, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.71
Round  74, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 6.70
Round  75, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.72
Round  75, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 6.69
Round  76, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.72
Round  76, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 6.70
Round  77, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.72
Round  77, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 6.70
Round  78, Train loss: 2.304, Test loss: 2.302, Test accuracy: 6.72
Round  78, Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 6.70
Round  79, Train loss: 2.300, Test loss: 2.302, Test accuracy: 6.72
Round  79, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 6.71
Round  80, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.73
Round  80, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 6.70
Round  81, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.73
Round  81, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 6.72
Round  82, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.72
Round  82, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 6.72
Round  83, Train loss: 2.300, Test loss: 2.302, Test accuracy: 6.71
Round  83, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 6.72
Round  84, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.71
Round  84, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 6.72
Round  85, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.71
Round  85, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 6.72
Round  86, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.76
Round  86, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 6.72
Round  87, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.75
Round  87, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 6.72
Round  88, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.76
Round  88, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 6.72
Round  89, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.78
Round  89, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 6.72
Round  90, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.80
Round  90, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 6.72
Round  91, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.80
Round  91, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 6.72
Round  92, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.80
Round  92, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 6.72
Round  93, Train loss: 2.304, Test loss: 2.302, Test accuracy: 6.80
Round  93, Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 6.72
Round  94, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.81
Round  94, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 6.76
Round  95, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.81
Round  95, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 6.76
Round  96, Train loss: 2.303, Test loss: 2.302, Test accuracy: 6.83
Round  96, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 6.78
Round  97, Train loss: 2.300, Test loss: 2.302, Test accuracy: 6.83
Round  97, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 6.77/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.83
Round  98, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 6.78
Round  99, Train loss: 2.301, Test loss: 2.302, Test accuracy: 6.84
Round  99, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 6.78
Final Round, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.99
Final Round, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 6.78
Average accuracy final 10 rounds: 6.815 

Average global accuracy final 10 rounds: 6.750833333333334 

1114.741528749466
[1.0163977146148682, 1.957512617111206, 2.8768303394317627, 3.784708023071289, 4.698484659194946, 5.625795125961304, 6.547878265380859, 7.465541362762451, 8.384845495223999, 9.307318210601807, 10.234767198562622, 11.168017387390137, 12.135477066040039, 13.046480655670166, 13.961833953857422, 14.886276721954346, 15.812825202941895, 16.734888315200806, 17.65599822998047, 18.595672130584717, 19.519641876220703, 20.44404172897339, 21.369937419891357, 22.283792734146118, 23.191563606262207, 24.11811137199402, 25.05017638206482, 25.96908950805664, 26.895056009292603, 27.814976692199707, 28.73062300682068, 29.661925554275513, 30.589823246002197, 31.52554941177368, 32.44092679023743, 33.393516540527344, 34.32795858383179, 35.258764028549194, 36.19900155067444, 37.13668203353882, 38.06844210624695, 38.99493360519409, 39.962247371673584, 40.9113986492157, 41.832958698272705, 42.75528955459595, 43.68431377410889, 44.61744689941406, 45.54998779296875, 46.49530076980591, 47.42586803436279, 48.34587621688843, 49.282639503479004, 50.216456174850464, 51.15504717826843, 52.09150981903076, 53.022982120513916, 53.95193123817444, 54.89359760284424, 55.827197313308716, 56.77180743217468, 57.700923681259155, 58.63085675239563, 59.56081175804138, 60.47857332229614, 61.398828983306885, 62.33237028121948, 63.252363443374634, 64.1907148361206, 65.13019919395447, 66.06493401527405, 66.99489116668701, 67.92506885528564, 68.8583996295929, 69.78284430503845, 70.72686266899109, 71.6490216255188, 72.57275891304016, 73.49354338645935, 74.46818780899048, 75.393310546875, 76.30608415603638, 77.22821617126465, 78.16588473320007, 79.09195065498352, 80.02173638343811, 80.9444751739502, 81.86392450332642, 82.79561471939087, 83.72175025939941, 84.6500449180603, 85.56042265892029, 86.48596858978271, 87.40839529037476, 88.32166194915771, 89.24525833129883, 90.1653184890747, 91.07179093360901, 91.9828028678894, 92.90642094612122, 94.44335794448853]
[6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.675, 6.683333333333334, 6.683333333333334, 6.683333333333334, 6.683333333333334, 6.683333333333334, 6.683333333333334, 6.683333333333334, 6.683333333333334, 6.683333333333334, 6.683333333333334, 6.683333333333334, 6.675, 6.675, 6.675, 6.675, 6.675, 6.683333333333334, 6.683333333333334, 6.683333333333334, 6.691666666666666, 6.691666666666666, 6.691666666666666, 6.691666666666666, 6.691666666666666, 6.691666666666666, 6.691666666666666, 6.683333333333334, 6.683333333333334, 6.691666666666666, 6.691666666666666, 6.691666666666666, 6.691666666666666, 6.691666666666666, 6.691666666666666, 6.691666666666666, 6.7, 6.7, 6.7, 6.708333333333333, 6.708333333333333, 6.708333333333333, 6.708333333333333, 6.708333333333333, 6.7, 6.7, 6.708333333333333, 6.7, 6.708333333333333, 6.708333333333333, 6.716666666666667, 6.725, 6.725, 6.725, 6.725, 6.733333333333333, 6.733333333333333, 6.725, 6.708333333333333, 6.708333333333333, 6.708333333333333, 6.758333333333334, 6.75, 6.758333333333334, 6.783333333333333, 6.8, 6.8, 6.8, 6.8, 6.808333333333334, 6.808333333333334, 6.825, 6.833333333333333, 6.833333333333333, 6.841666666666667, 6.991666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.273, Test loss: 2.306, Test accuracy: 10.00
Round   1, Train loss: 2.026, Test loss: 2.303, Test accuracy: 11.03
Round   2, Train loss: 1.830, Test loss: 2.293, Test accuracy: 13.01
Round   3, Train loss: 1.785, Test loss: 2.286, Test accuracy: 13.02
Round   4, Train loss: 1.653, Test loss: 2.288, Test accuracy: 14.72
Round   5, Train loss: 1.759, Test loss: 2.263, Test accuracy: 17.77
Round   6, Train loss: 1.635, Test loss: 2.293, Test accuracy: 13.72
Round   7, Train loss: 1.737, Test loss: 2.287, Test accuracy: 14.12
Round   8, Train loss: 1.665, Test loss: 2.275, Test accuracy: 16.12
Round   9, Train loss: 1.648, Test loss: 2.271, Test accuracy: 17.43
Round  10, Train loss: 1.637, Test loss: 2.281, Test accuracy: 15.28
Round  11, Train loss: 1.772, Test loss: 2.263, Test accuracy: 17.89
Round  12, Train loss: 1.745, Test loss: 2.281, Test accuracy: 14.39
Round  13, Train loss: 1.644, Test loss: 2.265, Test accuracy: 17.21
Round  14, Train loss: 1.663, Test loss: 2.271, Test accuracy: 16.80
Round  15, Train loss: 1.667, Test loss: 2.273, Test accuracy: 15.93
Round  16, Train loss: 1.611, Test loss: 2.265, Test accuracy: 17.35
Round  17, Train loss: 1.769, Test loss: 2.266, Test accuracy: 17.10
Round  18, Train loss: 1.663, Test loss: 2.265, Test accuracy: 16.49
Round  19, Train loss: 1.669, Test loss: 2.254, Test accuracy: 18.46
Round  20, Train loss: 1.568, Test loss: 2.258, Test accuracy: 18.15
Round  21, Train loss: 1.580, Test loss: 2.269, Test accuracy: 17.66
Round  22, Train loss: 1.612, Test loss: 2.276, Test accuracy: 16.15
Round  23, Train loss: 1.625, Test loss: 2.255, Test accuracy: 17.90
Round  24, Train loss: 1.553, Test loss: 2.270, Test accuracy: 15.37
Round  25, Train loss: 1.621, Test loss: 2.264, Test accuracy: 17.36
Round  26, Train loss: 1.631, Test loss: 2.263, Test accuracy: 17.48
Round  27, Train loss: 1.538, Test loss: 2.257, Test accuracy: 17.95
Round  28, Train loss: 1.671, Test loss: 2.254, Test accuracy: 18.47
Round  29, Train loss: 1.562, Test loss: 2.241, Test accuracy: 20.78
Round  30, Train loss: 1.569, Test loss: 2.270, Test accuracy: 16.47
Round  31, Train loss: 1.571, Test loss: 2.247, Test accuracy: 19.62
Round  32, Train loss: 1.656, Test loss: 2.266, Test accuracy: 17.16
Round  33, Train loss: 1.514, Test loss: 2.249, Test accuracy: 18.41
Round  34, Train loss: 1.643, Test loss: 2.260, Test accuracy: 18.47
Round  35, Train loss: 1.597, Test loss: 2.229, Test accuracy: 21.77
Round  36, Train loss: 1.563, Test loss: 2.261, Test accuracy: 17.72
Round  37, Train loss: 1.555, Test loss: 2.247, Test accuracy: 19.24
Round  38, Train loss: 1.508, Test loss: 2.261, Test accuracy: 17.62
Round  39, Train loss: 1.546, Test loss: 2.272, Test accuracy: 15.68
Round  40, Train loss: 1.611, Test loss: 2.253, Test accuracy: 18.38
Round  41, Train loss: 1.599, Test loss: 2.278, Test accuracy: 15.28
Round  42, Train loss: 1.542, Test loss: 2.251, Test accuracy: 18.89
Round  43, Train loss: 1.655, Test loss: 2.241, Test accuracy: 20.52
Round  44, Train loss: 1.667, Test loss: 2.224, Test accuracy: 21.62
Round  45, Train loss: 1.555, Test loss: 2.243, Test accuracy: 20.30
Round  46, Train loss: 1.598, Test loss: 2.215, Test accuracy: 24.09
Round  47, Train loss: 1.591, Test loss: 2.231, Test accuracy: 21.62
Round  48, Train loss: 1.652, Test loss: 2.226, Test accuracy: 22.24
Round  49, Train loss: 1.553, Test loss: 2.254, Test accuracy: 19.17
Round  50, Train loss: 1.592, Test loss: 2.234, Test accuracy: 21.30
Round  51, Train loss: 1.551, Test loss: 2.223, Test accuracy: 22.91
Round  52, Train loss: 1.499, Test loss: 2.222, Test accuracy: 22.45
Round  53, Train loss: 1.598, Test loss: 2.228, Test accuracy: 22.17
Round  54, Train loss: 1.643, Test loss: 2.216, Test accuracy: 22.83
Round  55, Train loss: 1.532, Test loss: 2.231, Test accuracy: 21.27
Round  56, Train loss: 1.490, Test loss: 2.266, Test accuracy: 17.87
Round  57, Train loss: 1.636, Test loss: 2.217, Test accuracy: 23.02
Round  58, Train loss: 1.591, Test loss: 2.265, Test accuracy: 17.67
Round  59, Train loss: 1.592, Test loss: 2.214, Test accuracy: 24.18
Round  60, Train loss: 1.582, Test loss: 2.239, Test accuracy: 20.27
Round  61, Train loss: 1.609, Test loss: 2.233, Test accuracy: 20.96
Round  62, Train loss: 1.553, Test loss: 2.244, Test accuracy: 19.59
Round  63, Train loss: 1.537, Test loss: 2.221, Test accuracy: 22.96
Round  64, Train loss: 1.647, Test loss: 2.230, Test accuracy: 21.62
Round  65, Train loss: 1.601, Test loss: 2.230, Test accuracy: 21.52
Round  66, Train loss: 1.586, Test loss: 2.218, Test accuracy: 22.77
Round  67, Train loss: 1.541, Test loss: 2.231, Test accuracy: 20.73
Round  68, Train loss: 1.586, Test loss: 2.240, Test accuracy: 19.88
Round  69, Train loss: 1.546, Test loss: 2.226, Test accuracy: 22.36
Round  70, Train loss: 1.655, Test loss: 2.198, Test accuracy: 25.05
Round  71, Train loss: 1.648, Test loss: 2.221, Test accuracy: 22.96
Round  72, Train loss: 1.613, Test loss: 2.245, Test accuracy: 19.52
Round  73, Train loss: 1.545, Test loss: 2.224, Test accuracy: 22.38
Round  74, Train loss: 1.541, Test loss: 2.241, Test accuracy: 20.59
Round  75, Train loss: 1.482, Test loss: 2.213, Test accuracy: 22.79
Round  76, Train loss: 1.544, Test loss: 2.234, Test accuracy: 21.08
Round  77, Train loss: 1.640, Test loss: 2.226, Test accuracy: 21.71
Round  78, Train loss: 1.581, Test loss: 2.217, Test accuracy: 22.76
Round  79, Train loss: 1.591, Test loss: 2.273, Test accuracy: 17.00
Round  80, Train loss: 1.536, Test loss: 2.222, Test accuracy: 22.30
Round  81, Train loss: 1.588, Test loss: 2.212, Test accuracy: 23.71
Round  82, Train loss: 1.540, Test loss: 2.219, Test accuracy: 22.73
Round  83, Train loss: 1.490, Test loss: 2.225, Test accuracy: 22.06
Round  84, Train loss: 1.547, Test loss: 2.223, Test accuracy: 22.16
Round  85, Train loss: 1.642, Test loss: 2.224, Test accuracy: 22.43
Round  86, Train loss: 1.590, Test loss: 2.211, Test accuracy: 23.83
Round  87, Train loss: 1.538, Test loss: 2.216, Test accuracy: 23.54
Round  88, Train loss: 1.488, Test loss: 2.222, Test accuracy: 22.29
Round  89, Train loss: 1.598, Test loss: 2.215, Test accuracy: 23.08
Round  90, Train loss: 1.527, Test loss: 2.239, Test accuracy: 20.55
Round  91, Train loss: 1.524, Test loss: 2.248, Test accuracy: 19.27
Round  92, Train loss: 1.635, Test loss: 2.241, Test accuracy: 19.77
Round  93, Train loss: 1.536, Test loss: 2.227, Test accuracy: 21.99
Round  94, Train loss: 1.598, Test loss: 2.229, Test accuracy: 21.27
Round  95, Train loss: 1.529, Test loss: 2.227, Test accuracy: 21.62
Round  96, Train loss: 1.582, Test loss: 2.228, Test accuracy: 21.77
Round  97, Train loss: 1.550, Test loss: 2.213, Test accuracy: 23.43
Round  98, Train loss: 1.479, Test loss: 2.221, Test accuracy: 22.12
Round  99, Train loss: 1.547, Test loss: 2.221, Test accuracy: 22.84
Final Round, Train loss: 1.543, Test loss: 2.215, Test accuracy: 23.07
Average accuracy final 10 rounds: 21.464166666666667
1693.433209180832
[2.4368040561676025, 4.783834934234619, 7.099721908569336, 9.423324346542358, 11.726188898086548, 14.029443502426147, 16.374239683151245, 18.71549677848816, 21.04419445991516, 23.33448839187622, 25.662694931030273, 27.9523606300354, 30.28145146369934, 32.59209227561951, 34.894946575164795, 37.20089411735535, 39.50474405288696, 41.82094430923462, 44.126627683639526, 46.43684458732605, 48.719051122665405, 51.03310418128967, 53.38646411895752, 55.71632695198059, 58.010082483291626, 60.31128454208374, 62.60988759994507, 64.90073370933533, 67.19958972930908, 69.48927664756775, 71.8118543624878, 74.08468317985535, 76.39966678619385, 78.72645711898804, 81.06926560401917, 83.40876030921936, 85.75352573394775, 88.13020491600037, 90.4564881324768, 92.77107286453247, 95.058114528656, 97.3616075515747, 99.65099334716797, 101.96079397201538, 104.26733994483948, 106.59477019309998, 108.92211437225342, 111.24517726898193, 113.58070993423462, 115.87750267982483, 118.21736669540405, 120.51833891868591, 122.86244821548462, 125.18136858940125, 127.48977160453796, 129.79402351379395, 132.12103295326233, 134.48117303848267, 136.8084306716919, 139.13538455963135, 141.44623970985413, 143.83313298225403, 146.1181697845459, 148.46931171417236, 150.79789805412292, 153.1249716281891, 155.44955229759216, 157.93394017219543, 160.4337227344513, 162.88245558738708, 165.43574047088623, 167.92118668556213, 170.38338541984558, 173.0599455833435, 175.61430144309998, 178.2179296016693, 180.7864649295807, 183.33564066886902, 185.98543310165405, 188.5641586780548, 190.86721205711365, 193.19090223312378, 195.50690007209778, 197.94811463356018, 200.38133764266968, 202.8360834121704, 205.15105605125427, 207.4982123374939, 209.8396120071411, 212.16616868972778, 214.52789330482483, 216.86002683639526, 219.20723724365234, 221.52667880058289, 223.8826882839203, 226.17078828811646, 228.5263180732727, 230.85295939445496, 233.16672277450562, 235.47088360786438, 237.42134761810303]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[10.0, 11.033333333333333, 13.008333333333333, 13.016666666666667, 14.716666666666667, 17.775, 13.725, 14.125, 16.125, 17.425, 15.283333333333333, 17.891666666666666, 14.391666666666667, 17.208333333333332, 16.8, 15.933333333333334, 17.35, 17.1, 16.491666666666667, 18.458333333333332, 18.15, 17.658333333333335, 16.15, 17.9, 15.366666666666667, 17.358333333333334, 17.475, 17.95, 18.466666666666665, 20.783333333333335, 16.466666666666665, 19.625, 17.158333333333335, 18.408333333333335, 18.466666666666665, 21.775, 17.716666666666665, 19.241666666666667, 17.625, 15.683333333333334, 18.383333333333333, 15.283333333333333, 18.891666666666666, 20.525, 21.625, 20.3, 24.091666666666665, 21.616666666666667, 22.241666666666667, 19.166666666666668, 21.3, 22.908333333333335, 22.45, 22.166666666666668, 22.833333333333332, 21.266666666666666, 17.866666666666667, 23.016666666666666, 17.666666666666668, 24.175, 20.275, 20.958333333333332, 19.591666666666665, 22.958333333333332, 21.616666666666667, 21.516666666666666, 22.766666666666666, 20.733333333333334, 19.875, 22.358333333333334, 25.05, 22.958333333333332, 19.516666666666666, 22.375, 20.591666666666665, 22.791666666666668, 21.083333333333332, 21.708333333333332, 22.758333333333333, 17.0, 22.3, 23.708333333333332, 22.725, 22.058333333333334, 22.158333333333335, 22.425, 23.833333333333332, 23.541666666666668, 22.291666666666668, 23.083333333333332, 20.55, 19.266666666666666, 19.775, 21.991666666666667, 21.275, 21.625, 21.766666666666666, 23.433333333333334, 22.116666666666667, 22.841666666666665, 23.066666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.297, Test loss: 2.299, Test accuracy: 9.78
Round   1, Train loss: 2.186, Test loss: 2.264, Test accuracy: 26.55
Round   2, Train loss: 2.036, Test loss: 2.192, Test accuracy: 31.39
Round   3, Train loss: 1.965, Test loss: 2.105, Test accuracy: 42.19
Round   4, Train loss: 1.799, Test loss: 2.008, Test accuracy: 50.05
Round   5, Train loss: 1.781, Test loss: 1.956, Test accuracy: 54.54
Round   6, Train loss: 1.804, Test loss: 1.893, Test accuracy: 61.16
Round   7, Train loss: 1.847, Test loss: 1.868, Test accuracy: 63.88
Round   8, Train loss: 1.792, Test loss: 1.835, Test accuracy: 66.82
Round   9, Train loss: 1.709, Test loss: 1.790, Test accuracy: 70.97
Round  10, Train loss: 1.707, Test loss: 1.785, Test accuracy: 71.83
Round  11, Train loss: 1.704, Test loss: 1.743, Test accuracy: 76.95
Round  12, Train loss: 1.637, Test loss: 1.695, Test accuracy: 82.53
Round  13, Train loss: 1.580, Test loss: 1.678, Test accuracy: 83.79
Round  14, Train loss: 1.573, Test loss: 1.655, Test accuracy: 86.63
Round  15, Train loss: 1.643, Test loss: 1.635, Test accuracy: 88.58
Round  16, Train loss: 1.651, Test loss: 1.625, Test accuracy: 89.35
Round  17, Train loss: 1.544, Test loss: 1.611, Test accuracy: 89.86
Round  18, Train loss: 1.540, Test loss: 1.610, Test accuracy: 89.69
Round  19, Train loss: 1.589, Test loss: 1.604, Test accuracy: 90.36
Round  20, Train loss: 1.583, Test loss: 1.608, Test accuracy: 89.62
Round  21, Train loss: 1.576, Test loss: 1.606, Test accuracy: 89.53
Round  22, Train loss: 1.674, Test loss: 1.609, Test accuracy: 89.95
Round  23, Train loss: 1.569, Test loss: 1.600, Test accuracy: 89.91
Round  24, Train loss: 1.574, Test loss: 1.599, Test accuracy: 90.60
Round  25, Train loss: 1.568, Test loss: 1.614, Test accuracy: 88.25
Round  26, Train loss: 1.570, Test loss: 1.603, Test accuracy: 89.54
Round  27, Train loss: 1.519, Test loss: 1.589, Test accuracy: 90.69
Round  28, Train loss: 1.552, Test loss: 1.589, Test accuracy: 90.59
Round  29, Train loss: 1.586, Test loss: 1.586, Test accuracy: 90.77
Round  30, Train loss: 1.570, Test loss: 1.582, Test accuracy: 91.44
Round  31, Train loss: 1.612, Test loss: 1.595, Test accuracy: 90.23
Round  32, Train loss: 1.513, Test loss: 1.586, Test accuracy: 90.99
Round  33, Train loss: 1.509, Test loss: 1.572, Test accuracy: 91.68
Round  34, Train loss: 1.556, Test loss: 1.580, Test accuracy: 90.78
Round  35, Train loss: 1.590, Test loss: 1.589, Test accuracy: 89.74
Round  36, Train loss: 1.556, Test loss: 1.580, Test accuracy: 90.83
Round  37, Train loss: 1.507, Test loss: 1.588, Test accuracy: 90.27
Round  38, Train loss: 1.586, Test loss: 1.578, Test accuracy: 91.11
Round  39, Train loss: 1.551, Test loss: 1.582, Test accuracy: 90.91
Round  40, Train loss: 1.507, Test loss: 1.580, Test accuracy: 91.16
Round  41, Train loss: 1.505, Test loss: 1.571, Test accuracy: 91.88
Round  42, Train loss: 1.534, Test loss: 1.578, Test accuracy: 91.14
Round  43, Train loss: 1.620, Test loss: 1.567, Test accuracy: 92.65
Round  44, Train loss: 1.547, Test loss: 1.560, Test accuracy: 92.97
Round  45, Train loss: 1.495, Test loss: 1.564, Test accuracy: 92.59
Round  46, Train loss: 1.497, Test loss: 1.568, Test accuracy: 92.03
Round  47, Train loss: 1.505, Test loss: 1.557, Test accuracy: 93.05
Round  48, Train loss: 1.502, Test loss: 1.569, Test accuracy: 91.91
Round  49, Train loss: 1.547, Test loss: 1.554, Test accuracy: 93.48
Round  50, Train loss: 1.495, Test loss: 1.559, Test accuracy: 93.03
Round  51, Train loss: 1.493, Test loss: 1.555, Test accuracy: 93.28
Round  52, Train loss: 1.485, Test loss: 1.560, Test accuracy: 92.22
Round  53, Train loss: 1.484, Test loss: 1.565, Test accuracy: 91.83
Round  54, Train loss: 1.545, Test loss: 1.551, Test accuracy: 93.30
Round  55, Train loss: 1.558, Test loss: 1.566, Test accuracy: 92.83
Round  56, Train loss: 1.537, Test loss: 1.559, Test accuracy: 93.05
Round  57, Train loss: 1.541, Test loss: 1.559, Test accuracy: 93.28
Round  58, Train loss: 1.590, Test loss: 1.558, Test accuracy: 93.45
Round  59, Train loss: 1.536, Test loss: 1.556, Test accuracy: 93.45
Round  60, Train loss: 1.502, Test loss: 1.554, Test accuracy: 93.33
Round  61, Train loss: 1.545, Test loss: 1.553, Test accuracy: 93.55
Round  62, Train loss: 1.493, Test loss: 1.559, Test accuracy: 92.97
Round  63, Train loss: 1.537, Test loss: 1.566, Test accuracy: 92.59
Round  64, Train loss: 1.493, Test loss: 1.561, Test accuracy: 92.56
Round  65, Train loss: 1.487, Test loss: 1.546, Test accuracy: 93.42
Round  66, Train loss: 1.544, Test loss: 1.548, Test accuracy: 93.64
Round  67, Train loss: 1.486, Test loss: 1.548, Test accuracy: 93.74
Round  68, Train loss: 1.495, Test loss: 1.548, Test accuracy: 93.51
Round  69, Train loss: 1.539, Test loss: 1.545, Test accuracy: 93.78
Round  70, Train loss: 1.480, Test loss: 1.552, Test accuracy: 93.32
Round  71, Train loss: 1.484, Test loss: 1.549, Test accuracy: 93.58
Round  72, Train loss: 1.485, Test loss: 1.548, Test accuracy: 93.49
Round  73, Train loss: 1.542, Test loss: 1.554, Test accuracy: 93.67
Round  74, Train loss: 1.481, Test loss: 1.549, Test accuracy: 93.58
Round  75, Train loss: 1.488, Test loss: 1.556, Test accuracy: 92.98
Round  76, Train loss: 1.595, Test loss: 1.550, Test accuracy: 93.64
Round  77, Train loss: 1.490, Test loss: 1.542, Test accuracy: 94.12
Round  78, Train loss: 1.480, Test loss: 1.543, Test accuracy: 93.83
Round  79, Train loss: 1.537, Test loss: 1.544, Test accuracy: 93.87
Round  80, Train loss: 1.487, Test loss: 1.550, Test accuracy: 93.57
Round  81, Train loss: 1.482, Test loss: 1.544, Test accuracy: 93.98
Round  82, Train loss: 1.481, Test loss: 1.545, Test accuracy: 93.73
Round  83, Train loss: 1.534, Test loss: 1.550, Test accuracy: 93.28
Round  84, Train loss: 1.534, Test loss: 1.543, Test accuracy: 94.10
Round  85, Train loss: 1.480, Test loss: 1.541, Test accuracy: 94.05
Round  86, Train loss: 1.475, Test loss: 1.542, Test accuracy: 93.94
Round  87, Train loss: 1.475, Test loss: 1.540, Test accuracy: 94.05
Round  88, Train loss: 1.585, Test loss: 1.546, Test accuracy: 93.82
Round  89, Train loss: 1.485, Test loss: 1.546, Test accuracy: 93.64
Round  90, Train loss: 1.478, Test loss: 1.547, Test accuracy: 93.60
Round  91, Train loss: 1.533, Test loss: 1.549, Test accuracy: 93.48
Round  92, Train loss: 1.531, Test loss: 1.547, Test accuracy: 93.62
Round  93, Train loss: 1.507, Test loss: 1.534, Test accuracy: 95.21
Round  94, Train loss: 1.480, Test loss: 1.536, Test accuracy: 94.79/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.490, Test loss: 1.527, Test accuracy: 95.58
Round  96, Train loss: 1.484, Test loss: 1.531, Test accuracy: 95.23
Round  97, Train loss: 1.478, Test loss: 1.531, Test accuracy: 95.34
Round  98, Train loss: 1.532, Test loss: 1.534, Test accuracy: 95.33
Round  99, Train loss: 1.536, Test loss: 1.538, Test accuracy: 94.65
Final Round, Train loss: 1.493, Test loss: 1.521, Test accuracy: 95.80
Average accuracy final 10 rounds: 94.68416666666667
911.246524810791
[1.2279891967773438, 2.2440927028656006, 3.249997138977051, 4.2474448680877686, 5.259070873260498, 6.271184206008911, 7.28693151473999, 8.291953086853027, 9.303405284881592, 10.305562973022461, 11.311554670333862, 12.312036037445068, 13.349001407623291, 14.380589723587036, 15.393259763717651, 16.402429580688477, 17.406168460845947, 18.410540103912354, 19.453500509262085, 20.46938920021057, 21.527843236923218, 22.55381965637207, 23.60281538963318, 24.758538961410522, 25.83709478378296, 26.989392042160034, 28.07172417640686, 29.163543462753296, 30.203131198883057, 31.243830919265747, 32.324302434921265, 33.412986278533936, 34.431450843811035, 35.49444341659546, 36.54264307022095, 37.575727462768555, 38.6055474281311, 39.62846279144287, 40.681729555130005, 41.74310564994812, 42.876845836639404, 43.906935691833496, 44.974576234817505, 46.02857971191406, 47.104066133499146, 48.21678876876831, 49.24711465835571, 50.3568799495697, 51.39863562583923, 52.411210775375366, 53.44298005104065, 54.48196530342102, 55.519410371780396, 56.54165840148926, 57.56511878967285, 58.60475826263428, 59.61539697647095, 60.63136577606201, 61.633159160614014, 62.69179606437683, 63.71364188194275, 64.72526168823242, 65.74281859397888, 66.76304030418396, 67.79435110092163, 68.87016749382019, 69.91017079353333, 71.00876307487488, 72.05218434333801, 73.09217572212219, 74.10896730422974, 75.13507580757141, 76.15463256835938, 77.15638756752014, 78.1811797618866, 79.21919059753418, 80.2640655040741, 81.32986903190613, 82.39750051498413, 83.50111436843872, 84.66785621643066, 85.72031497955322, 86.75969529151917, 87.83539390563965, 88.87769222259521, 89.98054957389832, 91.0724732875824, 92.13967561721802, 93.24954056739807, 94.33132600784302, 95.37243151664734, 96.43410634994507, 97.49122381210327, 98.55858659744263, 99.64357376098633, 100.7048077583313, 101.76739645004272, 102.77495408058167, 103.8095805644989, 104.86024904251099, 106.31995463371277]
[9.783333333333333, 26.55, 31.391666666666666, 42.19166666666667, 50.05, 54.541666666666664, 61.15833333333333, 63.88333333333333, 66.81666666666666, 70.975, 71.83333333333333, 76.95, 82.53333333333333, 83.79166666666667, 86.63333333333334, 88.575, 89.35, 89.85833333333333, 89.69166666666666, 90.35833333333333, 89.625, 89.525, 89.95, 89.90833333333333, 90.6, 88.25, 89.54166666666667, 90.69166666666666, 90.59166666666667, 90.76666666666667, 91.44166666666666, 90.23333333333333, 90.99166666666666, 91.68333333333334, 90.775, 89.74166666666666, 90.83333333333333, 90.26666666666667, 91.10833333333333, 90.90833333333333, 91.15833333333333, 91.88333333333334, 91.14166666666667, 92.65, 92.96666666666667, 92.59166666666667, 92.03333333333333, 93.05, 91.90833333333333, 93.48333333333333, 93.03333333333333, 93.28333333333333, 92.225, 91.83333333333333, 93.3, 92.825, 93.05, 93.275, 93.45, 93.45, 93.33333333333333, 93.55, 92.975, 92.59166666666667, 92.55833333333334, 93.425, 93.64166666666667, 93.74166666666666, 93.50833333333334, 93.78333333333333, 93.31666666666666, 93.58333333333333, 93.49166666666666, 93.675, 93.575, 92.98333333333333, 93.64166666666667, 94.11666666666666, 93.83333333333333, 93.86666666666666, 93.56666666666666, 93.98333333333333, 93.73333333333333, 93.275, 94.1, 94.05, 93.94166666666666, 94.05, 93.81666666666666, 93.64166666666667, 93.6, 93.48333333333333, 93.625, 95.20833333333333, 94.79166666666667, 95.58333333333333, 95.23333333333333, 95.34166666666667, 95.325, 94.65, 95.8]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.302, Test loss: 2.298, Test accuracy: 17.82
Round   1, Train loss: 2.213, Test loss: 2.265, Test accuracy: 22.05
Round   2, Train loss: 2.080, Test loss: 2.222, Test accuracy: 29.78
Round   3, Train loss: 2.004, Test loss: 2.171, Test accuracy: 32.21
Round   4, Train loss: 1.915, Test loss: 2.074, Test accuracy: 41.50
Round   5, Train loss: 1.831, Test loss: 2.015, Test accuracy: 47.64
Round   6, Train loss: 1.747, Test loss: 1.943, Test accuracy: 57.06
Round   7, Train loss: 1.812, Test loss: 1.879, Test accuracy: 65.26
Round   8, Train loss: 1.771, Test loss: 1.849, Test accuracy: 68.33
Round   9, Train loss: 1.684, Test loss: 1.836, Test accuracy: 68.12
Round  10, Train loss: 1.647, Test loss: 1.796, Test accuracy: 71.84
Round  11, Train loss: 1.697, Test loss: 1.745, Test accuracy: 77.97
Round  12, Train loss: 1.688, Test loss: 1.708, Test accuracy: 81.62
Round  13, Train loss: 1.564, Test loss: 1.691, Test accuracy: 82.70
Round  14, Train loss: 1.621, Test loss: 1.683, Test accuracy: 84.62
Round  15, Train loss: 1.576, Test loss: 1.670, Test accuracy: 85.62
Round  16, Train loss: 1.644, Test loss: 1.661, Test accuracy: 87.01
Round  17, Train loss: 1.602, Test loss: 1.659, Test accuracy: 87.34
Round  18, Train loss: 1.616, Test loss: 1.643, Test accuracy: 87.73
Round  19, Train loss: 1.551, Test loss: 1.631, Test accuracy: 88.70
Round  20, Train loss: 1.538, Test loss: 1.626, Test accuracy: 89.64
Round  21, Train loss: 1.584, Test loss: 1.621, Test accuracy: 89.98
Round  22, Train loss: 1.594, Test loss: 1.599, Test accuracy: 92.22
Round  23, Train loss: 1.572, Test loss: 1.597, Test accuracy: 91.58
Round  24, Train loss: 1.538, Test loss: 1.589, Test accuracy: 92.10
Round  25, Train loss: 1.533, Test loss: 1.586, Test accuracy: 92.33
Round  26, Train loss: 1.557, Test loss: 1.577, Test accuracy: 92.78
Round  27, Train loss: 1.536, Test loss: 1.573, Test accuracy: 94.06
Round  28, Train loss: 1.541, Test loss: 1.559, Test accuracy: 95.82
Round  29, Train loss: 1.539, Test loss: 1.555, Test accuracy: 95.83
Round  30, Train loss: 1.530, Test loss: 1.553, Test accuracy: 95.90
Round  31, Train loss: 1.526, Test loss: 1.547, Test accuracy: 96.36
Round  32, Train loss: 1.539, Test loss: 1.542, Test accuracy: 96.54
Round  33, Train loss: 1.510, Test loss: 1.540, Test accuracy: 96.64
Round  34, Train loss: 1.519, Test loss: 1.545, Test accuracy: 96.34
Round  35, Train loss: 1.509, Test loss: 1.546, Test accuracy: 96.41
Round  36, Train loss: 1.515, Test loss: 1.540, Test accuracy: 96.67
Round  37, Train loss: 1.510, Test loss: 1.541, Test accuracy: 96.59
Round  38, Train loss: 1.525, Test loss: 1.539, Test accuracy: 96.88
Round  39, Train loss: 1.522, Test loss: 1.537, Test accuracy: 96.73
Round  40, Train loss: 1.501, Test loss: 1.538, Test accuracy: 96.75
Round  41, Train loss: 1.524, Test loss: 1.532, Test accuracy: 96.82
Round  42, Train loss: 1.499, Test loss: 1.532, Test accuracy: 96.85
Round  43, Train loss: 1.502, Test loss: 1.532, Test accuracy: 97.14
Round  44, Train loss: 1.501, Test loss: 1.532, Test accuracy: 96.78
Round  45, Train loss: 1.514, Test loss: 1.526, Test accuracy: 97.25
Round  46, Train loss: 1.511, Test loss: 1.523, Test accuracy: 97.23
Round  47, Train loss: 1.516, Test loss: 1.518, Test accuracy: 97.37
Round  48, Train loss: 1.496, Test loss: 1.519, Test accuracy: 97.40
Round  49, Train loss: 1.508, Test loss: 1.522, Test accuracy: 97.25
Round  50, Train loss: 1.505, Test loss: 1.518, Test accuracy: 97.27
Round  51, Train loss: 1.498, Test loss: 1.520, Test accuracy: 97.23
Round  52, Train loss: 1.492, Test loss: 1.521, Test accuracy: 97.28
Round  53, Train loss: 1.496, Test loss: 1.522, Test accuracy: 97.22
Round  54, Train loss: 1.496, Test loss: 1.521, Test accuracy: 97.34
Round  55, Train loss: 1.497, Test loss: 1.520, Test accuracy: 97.19
Round  56, Train loss: 1.495, Test loss: 1.520, Test accuracy: 97.24
Round  57, Train loss: 1.495, Test loss: 1.518, Test accuracy: 97.38
Round  58, Train loss: 1.497, Test loss: 1.515, Test accuracy: 97.69
Round  59, Train loss: 1.497, Test loss: 1.516, Test accuracy: 97.41
Round  60, Train loss: 1.496, Test loss: 1.513, Test accuracy: 97.68
Round  61, Train loss: 1.493, Test loss: 1.512, Test accuracy: 97.73
Round  62, Train loss: 1.487, Test loss: 1.514, Test accuracy: 97.63
Round  63, Train loss: 1.499, Test loss: 1.514, Test accuracy: 97.52
Round  64, Train loss: 1.491, Test loss: 1.517, Test accuracy: 97.35
Round  65, Train loss: 1.494, Test loss: 1.512, Test accuracy: 97.59
Round  66, Train loss: 1.491, Test loss: 1.512, Test accuracy: 97.81
Round  67, Train loss: 1.494, Test loss: 1.512, Test accuracy: 97.78
Round  68, Train loss: 1.492, Test loss: 1.512, Test accuracy: 97.68
Round  69, Train loss: 1.487, Test loss: 1.509, Test accuracy: 97.87
Round  70, Train loss: 1.487, Test loss: 1.509, Test accuracy: 97.87
Round  71, Train loss: 1.489, Test loss: 1.512, Test accuracy: 97.75
Round  72, Train loss: 1.493, Test loss: 1.509, Test accuracy: 97.63
Round  73, Train loss: 1.490, Test loss: 1.509, Test accuracy: 97.86
Round  74, Train loss: 1.486, Test loss: 1.508, Test accuracy: 97.88
Round  75, Train loss: 1.484, Test loss: 1.511, Test accuracy: 97.70
Round  76, Train loss: 1.487, Test loss: 1.508, Test accuracy: 97.85
Round  77, Train loss: 1.488, Test loss: 1.510, Test accuracy: 97.74
Round  78, Train loss: 1.487, Test loss: 1.507, Test accuracy: 97.91
Round  79, Train loss: 1.488, Test loss: 1.509, Test accuracy: 97.92
Round  80, Train loss: 1.487, Test loss: 1.509, Test accuracy: 97.83
Round  81, Train loss: 1.485, Test loss: 1.512, Test accuracy: 97.60
Round  82, Train loss: 1.494, Test loss: 1.505, Test accuracy: 97.89
Round  83, Train loss: 1.493, Test loss: 1.505, Test accuracy: 97.85
Round  84, Train loss: 1.487, Test loss: 1.506, Test accuracy: 97.92
Round  85, Train loss: 1.486, Test loss: 1.506, Test accuracy: 97.95
Round  86, Train loss: 1.487, Test loss: 1.504, Test accuracy: 97.88
Round  87, Train loss: 1.483, Test loss: 1.506, Test accuracy: 97.72
Round  88, Train loss: 1.487, Test loss: 1.507, Test accuracy: 97.74
Round  89, Train loss: 1.489, Test loss: 1.504, Test accuracy: 97.86
Round  90, Train loss: 1.483, Test loss: 1.505, Test accuracy: 97.88
Round  91, Train loss: 1.482, Test loss: 1.506, Test accuracy: 97.83
Round  92, Train loss: 1.481, Test loss: 1.502, Test accuracy: 98.06
Round  93, Train loss: 1.487, Test loss: 1.503, Test accuracy: 98.02/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.485, Test loss: 1.505, Test accuracy: 97.91
Round  95, Train loss: 1.484, Test loss: 1.503, Test accuracy: 97.84
Round  96, Train loss: 1.483, Test loss: 1.502, Test accuracy: 98.06
Round  97, Train loss: 1.482, Test loss: 1.505, Test accuracy: 97.84
Round  98, Train loss: 1.485, Test loss: 1.503, Test accuracy: 98.07
Round  99, Train loss: 1.481, Test loss: 1.504, Test accuracy: 97.92
Final Round, Train loss: 1.475, Test loss: 1.499, Test accuracy: 98.06
Average accuracy final 10 rounds: 97.94333333333334
1217.195851802826
[1.2708909511566162, 2.5417819023132324, 3.6634750366210938, 4.785168170928955, 5.908397197723389, 7.031626224517822, 8.115309000015259, 9.198991775512695, 10.25652027130127, 11.314048767089844, 12.632001638412476, 13.949954509735107, 15.019835710525513, 16.089716911315918, 17.21851348876953, 18.347310066223145, 19.40491795539856, 20.462525844573975, 21.513075590133667, 22.56362533569336, 23.619069814682007, 24.674514293670654, 25.7806236743927, 26.886733055114746, 27.94188666343689, 28.997040271759033, 30.057502031326294, 31.117963790893555, 32.145681381225586, 33.17339897155762, 34.25489377975464, 35.33638858795166, 36.39142680168152, 37.44646501541138, 38.50201869010925, 39.55757236480713, 40.62049198150635, 41.683411598205566, 42.721444606781006, 43.759477615356445, 44.80372428894043, 45.847970962524414, 46.97212815284729, 48.096285343170166, 49.20256948471069, 50.30885362625122, 51.34973478317261, 52.390615940093994, 53.42764329910278, 54.46467065811157, 55.56300210952759, 56.6613335609436, 57.725879430770874, 58.790425300598145, 59.87342047691345, 60.95641565322876, 62.01088786125183, 63.0653600692749, 64.11135601997375, 65.15735197067261, 66.20453095436096, 67.25170993804932, 68.27097821235657, 69.29024648666382, 70.3272864818573, 71.36432647705078, 72.39776229858398, 73.43119812011719, 74.45242500305176, 75.47365188598633, 76.49234294891357, 77.51103401184082, 78.55309081077576, 79.5951476097107, 80.61285758018494, 81.63056755065918, 82.67782521247864, 83.7250828742981, 84.75172019004822, 85.77835750579834, 86.80630612373352, 87.8342547416687, 88.87189149856567, 89.90952825546265, 91.03968739509583, 92.169846534729, 93.19941353797913, 94.22898054122925, 95.2512776851654, 96.27357482910156, 97.30771374702454, 98.34185266494751, 99.4048798084259, 100.4679069519043, 101.53148746490479, 102.59506797790527, 103.71221208572388, 104.82935619354248, 105.96222019195557, 107.09508419036865, 108.19514489173889, 109.29520559310913, 110.38032484054565, 111.46544408798218, 112.53236603736877, 113.59928798675537, 114.7018301486969, 115.80437231063843, 116.86956596374512, 117.9347596168518, 119.13611078262329, 120.33746194839478, 121.40761542320251, 122.47776889801025, 123.62157464027405, 124.76538038253784, 125.88512897491455, 127.00487756729126, 128.07852411270142, 129.15217065811157, 130.24342370033264, 131.3346767425537, 132.48055744171143, 133.62643814086914, 134.67489981651306, 135.72336149215698, 136.83169651031494, 137.9400315284729, 139.02373337745667, 140.10743522644043, 141.23435711860657, 142.3612790107727, 143.47252368927002, 144.58376836776733, 145.65441584587097, 146.7250633239746, 147.84708046913147, 148.96909761428833, 150.0357358455658, 151.10237407684326, 152.17597246170044, 153.24957084655762, 154.40269351005554, 155.55581617355347, 156.60688281059265, 157.65794944763184, 158.79671049118042, 159.935471534729, 161.05235838890076, 162.1692452430725, 163.25323247909546, 164.3372197151184, 165.38817143440247, 166.43912315368652, 167.49852180480957, 168.55792045593262, 169.60256028175354, 170.64720010757446, 171.75175714492798, 172.8563141822815, 173.93511152267456, 175.01390886306763, 176.13344764709473, 177.25298643112183, 178.35617470741272, 179.4593629837036, 180.60605096817017, 181.75273895263672, 182.8583390712738, 183.9639391899109, 185.09418892860413, 186.22443866729736, 187.30644297599792, 188.3884472846985, 189.56773161888123, 190.74701595306396, 191.85425281524658, 192.9614896774292, 194.09685587882996, 195.2322220802307, 196.36326599121094, 197.49430990219116, 198.61088013648987, 199.72745037078857, 200.83791065216064, 201.94837093353271, 203.10301971435547, 204.25766849517822, 205.42605423927307, 206.59443998336792, 207.68992829322815, 208.78541660308838, 209.92226600646973, 211.05911540985107, 212.14468359947205, 213.23025178909302, 214.33741688728333, 215.44458198547363, 216.61023831367493, 217.77589464187622, 219.25094962120056, 220.7260046005249]
[17.816666666666666, 17.816666666666666, 22.05, 22.05, 29.783333333333335, 29.783333333333335, 32.208333333333336, 32.208333333333336, 41.5, 41.5, 47.641666666666666, 47.641666666666666, 57.05833333333333, 57.05833333333333, 65.25833333333334, 65.25833333333334, 68.33333333333333, 68.33333333333333, 68.11666666666666, 68.11666666666666, 71.84166666666667, 71.84166666666667, 77.975, 77.975, 81.625, 81.625, 82.7, 82.7, 84.625, 84.625, 85.61666666666666, 85.61666666666666, 87.00833333333334, 87.00833333333334, 87.34166666666667, 87.34166666666667, 87.73333333333333, 87.73333333333333, 88.7, 88.7, 89.64166666666667, 89.64166666666667, 89.98333333333333, 89.98333333333333, 92.225, 92.225, 91.575, 91.575, 92.1, 92.1, 92.33333333333333, 92.33333333333333, 92.78333333333333, 92.78333333333333, 94.05833333333334, 94.05833333333334, 95.81666666666666, 95.81666666666666, 95.825, 95.825, 95.9, 95.9, 96.35833333333333, 96.35833333333333, 96.54166666666667, 96.54166666666667, 96.64166666666667, 96.64166666666667, 96.34166666666667, 96.34166666666667, 96.40833333333333, 96.40833333333333, 96.66666666666667, 96.66666666666667, 96.59166666666667, 96.59166666666667, 96.88333333333334, 96.88333333333334, 96.73333333333333, 96.73333333333333, 96.75, 96.75, 96.81666666666666, 96.81666666666666, 96.85, 96.85, 97.14166666666667, 97.14166666666667, 96.78333333333333, 96.78333333333333, 97.25, 97.25, 97.23333333333333, 97.23333333333333, 97.36666666666666, 97.36666666666666, 97.4, 97.4, 97.25, 97.25, 97.26666666666667, 97.26666666666667, 97.23333333333333, 97.23333333333333, 97.275, 97.275, 97.21666666666667, 97.21666666666667, 97.34166666666667, 97.34166666666667, 97.19166666666666, 97.19166666666666, 97.24166666666666, 97.24166666666666, 97.375, 97.375, 97.69166666666666, 97.69166666666666, 97.40833333333333, 97.40833333333333, 97.68333333333334, 97.68333333333334, 97.73333333333333, 97.73333333333333, 97.63333333333334, 97.63333333333334, 97.51666666666667, 97.51666666666667, 97.35, 97.35, 97.59166666666667, 97.59166666666667, 97.80833333333334, 97.80833333333334, 97.775, 97.775, 97.68333333333334, 97.68333333333334, 97.86666666666666, 97.86666666666666, 97.86666666666666, 97.86666666666666, 97.75, 97.75, 97.63333333333334, 97.63333333333334, 97.85833333333333, 97.85833333333333, 97.875, 97.875, 97.7, 97.7, 97.85, 97.85, 97.74166666666666, 97.74166666666666, 97.90833333333333, 97.90833333333333, 97.925, 97.925, 97.825, 97.825, 97.6, 97.6, 97.89166666666667, 97.89166666666667, 97.85, 97.85, 97.91666666666667, 97.91666666666667, 97.95, 97.95, 97.88333333333334, 97.88333333333334, 97.71666666666667, 97.71666666666667, 97.74166666666666, 97.74166666666666, 97.85833333333333, 97.85833333333333, 97.88333333333334, 97.88333333333334, 97.83333333333333, 97.83333333333333, 98.05833333333334, 98.05833333333334, 98.01666666666667, 98.01666666666667, 97.90833333333333, 97.90833333333333, 97.84166666666667, 97.84166666666667, 98.05833333333334, 98.05833333333334, 97.84166666666667, 97.84166666666667, 98.06666666666666, 98.06666666666666, 97.925, 97.925, 98.05833333333334, 98.05833333333334]
