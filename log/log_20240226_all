nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.227, Test loss: 2.082, Test accuracy: 25.67 

Round   0, Global train loss: 2.227, Global test loss: 2.083, Global test accuracy: 26.66 

Round   1, Train loss: 2.010, Test loss: 1.965, Test accuracy: 29.06 

Round   1, Global train loss: 2.010, Global test loss: 1.923, Global test accuracy: 31.22 

Round   2, Train loss: 1.890, Test loss: 1.878, Test accuracy: 31.46 

Round   2, Global train loss: 1.890, Global test loss: 1.787, Global test accuracy: 35.40 

Round   3, Train loss: 1.850, Test loss: 1.838, Test accuracy: 32.56 

Round   3, Global train loss: 1.850, Global test loss: 1.733, Global test accuracy: 37.27 

Round   4, Train loss: 1.748, Test loss: 1.815, Test accuracy: 33.73 

Round   4, Global train loss: 1.748, Global test loss: 1.697, Global test accuracy: 38.77 

Round   5, Train loss: 1.779, Test loss: 1.798, Test accuracy: 34.55 

Round   5, Global train loss: 1.779, Global test loss: 1.724, Global test accuracy: 38.71 

Round   6, Train loss: 1.702, Test loss: 1.778, Test accuracy: 35.07 

Round   6, Global train loss: 1.702, Global test loss: 1.681, Global test accuracy: 38.89 

Round   7, Train loss: 1.602, Test loss: 1.771, Test accuracy: 35.41 

Round   7, Global train loss: 1.602, Global test loss: 1.600, Global test accuracy: 41.74 

Round   8, Train loss: 1.543, Test loss: 1.760, Test accuracy: 35.98 

Round   8, Global train loss: 1.543, Global test loss: 1.573, Global test accuracy: 42.58 

Round   9, Train loss: 1.531, Test loss: 1.743, Test accuracy: 36.46 

Round   9, Global train loss: 1.531, Global test loss: 1.571, Global test accuracy: 42.81 

Round  10, Train loss: 1.470, Test loss: 1.764, Test accuracy: 36.67 

Round  10, Global train loss: 1.470, Global test loss: 1.560, Global test accuracy: 41.79 

Round  11, Train loss: 1.506, Test loss: 1.747, Test accuracy: 37.48 

Round  11, Global train loss: 1.506, Global test loss: 1.586, Global test accuracy: 41.95 

Round  12, Train loss: 1.530, Test loss: 1.738, Test accuracy: 38.26 

Round  12, Global train loss: 1.530, Global test loss: 1.615, Global test accuracy: 42.35 

Round  13, Train loss: 1.517, Test loss: 1.726, Test accuracy: 39.02 

Round  13, Global train loss: 1.517, Global test loss: 1.546, Global test accuracy: 43.80 

Round  14, Train loss: 1.470, Test loss: 1.700, Test accuracy: 39.90 

Round  14, Global train loss: 1.470, Global test loss: 1.536, Global test accuracy: 43.80 

Round  15, Train loss: 1.365, Test loss: 1.719, Test accuracy: 39.65 

Round  15, Global train loss: 1.365, Global test loss: 1.527, Global test accuracy: 44.70 

Round  16, Train loss: 1.311, Test loss: 1.761, Test accuracy: 38.96 

Round  16, Global train loss: 1.311, Global test loss: 1.522, Global test accuracy: 43.90 

Round  17, Train loss: 1.273, Test loss: 1.780, Test accuracy: 38.94 

Round  17, Global train loss: 1.273, Global test loss: 1.487, Global test accuracy: 45.93 

Round  18, Train loss: 1.172, Test loss: 1.785, Test accuracy: 39.31 

Round  18, Global train loss: 1.172, Global test loss: 1.470, Global test accuracy: 46.25 

Round  19, Train loss: 1.220, Test loss: 1.763, Test accuracy: 40.36 

Round  19, Global train loss: 1.220, Global test loss: 1.470, Global test accuracy: 45.79 

Round  20, Train loss: 1.127, Test loss: 1.780, Test accuracy: 40.17 

Round  20, Global train loss: 1.127, Global test loss: 1.469, Global test accuracy: 46.36 

Round  21, Train loss: 1.005, Test loss: 1.816, Test accuracy: 40.29 

Round  21, Global train loss: 1.005, Global test loss: 1.479, Global test accuracy: 46.02 

Round  22, Train loss: 1.281, Test loss: 1.817, Test accuracy: 40.87 

Round  22, Global train loss: 1.281, Global test loss: 1.527, Global test accuracy: 45.15 

Round  23, Train loss: 1.038, Test loss: 1.824, Test accuracy: 40.91 

Round  23, Global train loss: 1.038, Global test loss: 1.457, Global test accuracy: 47.82 

Round  24, Train loss: 0.956, Test loss: 1.847, Test accuracy: 41.02 

Round  24, Global train loss: 0.956, Global test loss: 1.451, Global test accuracy: 48.19 

Round  25, Train loss: 1.015, Test loss: 1.892, Test accuracy: 40.86 

Round  25, Global train loss: 1.015, Global test loss: 1.502, Global test accuracy: 45.63 

Round  26, Train loss: 1.002, Test loss: 1.910, Test accuracy: 40.81 

Round  26, Global train loss: 1.002, Global test loss: 1.474, Global test accuracy: 46.68 

Round  27, Train loss: 1.091, Test loss: 1.931, Test accuracy: 40.96 

Round  27, Global train loss: 1.091, Global test loss: 1.502, Global test accuracy: 45.18 

Round  28, Train loss: 0.978, Test loss: 1.966, Test accuracy: 40.86 

Round  28, Global train loss: 0.978, Global test loss: 1.479, Global test accuracy: 46.12 

Round  29, Train loss: 0.979, Test loss: 1.991, Test accuracy: 40.87 

Round  29, Global train loss: 0.979, Global test loss: 1.443, Global test accuracy: 47.96 

Round  30, Train loss: 0.841, Test loss: 2.028, Test accuracy: 40.79 

Round  30, Global train loss: 0.841, Global test loss: 1.468, Global test accuracy: 47.35 

Round  31, Train loss: 1.005, Test loss: 2.067, Test accuracy: 40.47 

Round  31, Global train loss: 1.005, Global test loss: 1.476, Global test accuracy: 46.05 

Round  32, Train loss: 0.758, Test loss: 2.079, Test accuracy: 40.69 

Round  32, Global train loss: 0.758, Global test loss: 1.471, Global test accuracy: 48.34 

Round  33, Train loss: 0.829, Test loss: 2.128, Test accuracy: 40.55 

Round  33, Global train loss: 0.829, Global test loss: 1.461, Global test accuracy: 47.41 

Round  34, Train loss: 0.832, Test loss: 2.152, Test accuracy: 40.72 

Round  34, Global train loss: 0.832, Global test loss: 1.475, Global test accuracy: 46.63 

Round  35, Train loss: 0.795, Test loss: 2.215, Test accuracy: 40.62 

Round  35, Global train loss: 0.795, Global test loss: 1.486, Global test accuracy: 47.74 

Round  36, Train loss: 0.802, Test loss: 2.234, Test accuracy: 41.09 

Round  36, Global train loss: 0.802, Global test loss: 1.464, Global test accuracy: 49.02 

Round  37, Train loss: 0.807, Test loss: 2.271, Test accuracy: 41.21 

Round  37, Global train loss: 0.807, Global test loss: 1.462, Global test accuracy: 48.94 

Round  38, Train loss: 0.684, Test loss: 2.277, Test accuracy: 41.47 

Round  38, Global train loss: 0.684, Global test loss: 1.503, Global test accuracy: 46.90 

Round  39, Train loss: 0.742, Test loss: 2.325, Test accuracy: 41.27 

Round  39, Global train loss: 0.742, Global test loss: 1.475, Global test accuracy: 48.13 

Round  40, Train loss: 0.646, Test loss: 2.333, Test accuracy: 41.50 

Round  40, Global train loss: 0.646, Global test loss: 1.492, Global test accuracy: 49.16 

Round  41, Train loss: 0.601, Test loss: 2.407, Test accuracy: 41.26 

Round  41, Global train loss: 0.601, Global test loss: 1.495, Global test accuracy: 46.38 

Round  42, Train loss: 0.619, Test loss: 2.406, Test accuracy: 41.67 

Round  42, Global train loss: 0.619, Global test loss: 1.469, Global test accuracy: 49.20 

Round  43, Train loss: 0.666, Test loss: 2.432, Test accuracy: 41.84 

Round  43, Global train loss: 0.666, Global test loss: 1.455, Global test accuracy: 47.89 

Round  44, Train loss: 0.677, Test loss: 2.466, Test accuracy: 41.66 

Round  44, Global train loss: 0.677, Global test loss: 1.469, Global test accuracy: 46.63 

Round  45, Train loss: 0.672, Test loss: 2.476, Test accuracy: 41.89 

Round  45, Global train loss: 0.672, Global test loss: 1.501, Global test accuracy: 47.07 

Round  46, Train loss: 0.435, Test loss: 2.522, Test accuracy: 41.87 

Round  46, Global train loss: 0.435, Global test loss: 1.520, Global test accuracy: 47.39 

Round  47, Train loss: 0.514, Test loss: 2.607, Test accuracy: 41.45 

Round  47, Global train loss: 0.514, Global test loss: 1.559, Global test accuracy: 48.11 

Round  48, Train loss: 0.426, Test loss: 2.635, Test accuracy: 41.43 

Round  48, Global train loss: 0.426, Global test loss: 1.503, Global test accuracy: 47.74 

Round  49, Train loss: 0.483, Test loss: 2.688, Test accuracy: 41.39 

Round  49, Global train loss: 0.483, Global test loss: 1.512, Global test accuracy: 47.23 

Round  50, Train loss: 0.447, Test loss: 2.692, Test accuracy: 41.41 

Round  50, Global train loss: 0.447, Global test loss: 1.519, Global test accuracy: 48.84 

Round  51, Train loss: 0.507, Test loss: 2.695, Test accuracy: 41.58 

Round  51, Global train loss: 0.507, Global test loss: 1.492, Global test accuracy: 46.96 

Round  52, Train loss: 0.420, Test loss: 2.732, Test accuracy: 41.61 

Round  52, Global train loss: 0.420, Global test loss: 1.554, Global test accuracy: 46.77 

Round  53, Train loss: 0.428, Test loss: 2.774, Test accuracy: 41.37 

Round  53, Global train loss: 0.428, Global test loss: 1.497, Global test accuracy: 47.44 

Round  54, Train loss: 0.396, Test loss: 2.838, Test accuracy: 41.30 

Round  54, Global train loss: 0.396, Global test loss: 1.570, Global test accuracy: 48.10 

Round  55, Train loss: 0.522, Test loss: 2.863, Test accuracy: 41.38 

Round  55, Global train loss: 0.522, Global test loss: 1.480, Global test accuracy: 48.26 

Round  56, Train loss: 0.372, Test loss: 2.919, Test accuracy: 41.45 

Round  56, Global train loss: 0.372, Global test loss: 1.575, Global test accuracy: 48.04 

Round  57, Train loss: 0.413, Test loss: 2.929, Test accuracy: 41.48 

Round  57, Global train loss: 0.413, Global test loss: 1.590, Global test accuracy: 48.26 

Round  58, Train loss: 0.405, Test loss: 2.951, Test accuracy: 41.11 

Round  58, Global train loss: 0.405, Global test loss: 1.561, Global test accuracy: 47.60 

Round  59, Train loss: 0.344, Test loss: 2.987, Test accuracy: 41.30 

Round  59, Global train loss: 0.344, Global test loss: 1.561, Global test accuracy: 48.65 

Round  60, Train loss: 0.363, Test loss: 3.057, Test accuracy: 41.31 

Round  60, Global train loss: 0.363, Global test loss: 1.570, Global test accuracy: 47.75 

Round  61, Train loss: 0.336, Test loss: 3.079, Test accuracy: 41.13 

Round  61, Global train loss: 0.336, Global test loss: 1.591, Global test accuracy: 47.31 

Round  62, Train loss: 0.361, Test loss: 3.091, Test accuracy: 41.43 

Round  62, Global train loss: 0.361, Global test loss: 1.537, Global test accuracy: 48.99 

Round  63, Train loss: 0.353, Test loss: 3.131, Test accuracy: 41.40 

Round  63, Global train loss: 0.353, Global test loss: 1.537, Global test accuracy: 47.99 

Round  64, Train loss: 0.299, Test loss: 3.212, Test accuracy: 41.12 

Round  64, Global train loss: 0.299, Global test loss: 1.581, Global test accuracy: 48.04 

Round  65, Train loss: 0.285, Test loss: 3.236, Test accuracy: 41.42 

Round  65, Global train loss: 0.285, Global test loss: 1.588, Global test accuracy: 47.69 

Round  66, Train loss: 0.272, Test loss: 3.245, Test accuracy: 41.57 

Round  66, Global train loss: 0.272, Global test loss: 1.556, Global test accuracy: 46.81 

Round  67, Train loss: 0.335, Test loss: 3.331, Test accuracy: 41.45 

Round  67, Global train loss: 0.335, Global test loss: 1.531, Global test accuracy: 46.65 

Round  68, Train loss: 0.287, Test loss: 3.362, Test accuracy: 41.61 

Round  68, Global train loss: 0.287, Global test loss: 1.627, Global test accuracy: 48.89 

Round  69, Train loss: 0.300, Test loss: 3.417, Test accuracy: 41.67 

Round  69, Global train loss: 0.300, Global test loss: 1.617, Global test accuracy: 46.33 

Round  70, Train loss: 0.272, Test loss: 3.400, Test accuracy: 41.26 

Round  70, Global train loss: 0.272, Global test loss: 1.561, Global test accuracy: 48.78 

Round  71, Train loss: 0.283, Test loss: 3.324, Test accuracy: 41.63 

Round  71, Global train loss: 0.283, Global test loss: 1.602, Global test accuracy: 46.30 

Round  72, Train loss: 0.233, Test loss: 3.396, Test accuracy: 41.80 

Round  72, Global train loss: 0.233, Global test loss: 1.565, Global test accuracy: 46.20 

Round  73, Train loss: 0.286, Test loss: 3.475, Test accuracy: 41.65 

Round  73, Global train loss: 0.286, Global test loss: 1.541, Global test accuracy: 47.37 

Round  74, Train loss: 0.249, Test loss: 3.501, Test accuracy: 41.64 

Round  74, Global train loss: 0.249, Global test loss: 1.562, Global test accuracy: 48.09 

Round  75, Train loss: 0.286, Test loss: 3.481, Test accuracy: 41.92 

Round  75, Global train loss: 0.286, Global test loss: 1.579, Global test accuracy: 47.41 

Round  76, Train loss: 0.258, Test loss: 3.535, Test accuracy: 41.83 

Round  76, Global train loss: 0.258, Global test loss: 1.537, Global test accuracy: 46.61 

Round  77, Train loss: 0.242, Test loss: 3.634, Test accuracy: 41.64 

Round  77, Global train loss: 0.242, Global test loss: 1.575, Global test accuracy: 48.66 

Round  78, Train loss: 0.234, Test loss: 3.617, Test accuracy: 41.68 

Round  78, Global train loss: 0.234, Global test loss: 1.533, Global test accuracy: 47.05 

Round  79, Train loss: 0.250, Test loss: 3.625, Test accuracy: 41.69 

Round  79, Global train loss: 0.250, Global test loss: 1.555, Global test accuracy: 49.80 

Round  80, Train loss: 0.177, Test loss: 3.699, Test accuracy: 41.71 

Round  80, Global train loss: 0.177, Global test loss: 1.625, Global test accuracy: 45.63 

Round  81, Train loss: 0.221, Test loss: 3.661, Test accuracy: 41.81 

Round  81, Global train loss: 0.221, Global test loss: 1.539, Global test accuracy: 47.81 

Round  82, Train loss: 0.173, Test loss: 3.772, Test accuracy: 41.48 

Round  82, Global train loss: 0.173, Global test loss: 1.641, Global test accuracy: 47.77 

Round  83, Train loss: 0.194, Test loss: 3.783, Test accuracy: 41.51 

Round  83, Global train loss: 0.194, Global test loss: 1.574, Global test accuracy: 46.82 

Round  84, Train loss: 0.201, Test loss: 3.836, Test accuracy: 41.47 

Round  84, Global train loss: 0.201, Global test loss: 1.653, Global test accuracy: 46.96 

Round  85, Train loss: 0.222, Test loss: 3.820, Test accuracy: 41.30 

Round  85, Global train loss: 0.222, Global test loss: 1.573, Global test accuracy: 47.81 

Round  86, Train loss: 0.230, Test loss: 3.843, Test accuracy: 41.42 

Round  86, Global train loss: 0.230, Global test loss: 1.587, Global test accuracy: 48.02 

Round  87, Train loss: 0.152, Test loss: 3.874, Test accuracy: 41.62 

Round  87, Global train loss: 0.152, Global test loss: 1.551, Global test accuracy: 45.95 

Round  88, Train loss: 0.182, Test loss: 3.812, Test accuracy: 41.63 

Round  88, Global train loss: 0.182, Global test loss: 1.622, Global test accuracy: 48.55 

Round  89, Train loss: 0.167, Test loss: 3.815, Test accuracy: 42.10 

Round  89, Global train loss: 0.167, Global test loss: 1.688, Global test accuracy: 48.44 

Round  90, Train loss: 0.148, Test loss: 3.841, Test accuracy: 42.05 

Round  90, Global train loss: 0.148, Global test loss: 1.760, Global test accuracy: 45.91 

Round  91, Train loss: 0.223, Test loss: 3.878, Test accuracy: 42.15 

Round  91, Global train loss: 0.223, Global test loss: 1.722, Global test accuracy: 47.41 

Round  92, Train loss: 0.220, Test loss: 3.872, Test accuracy: 41.96 

Round  92, Global train loss: 0.220, Global test loss: 1.593, Global test accuracy: 48.70 

Round  93, Train loss: 0.156, Test loss: 3.822, Test accuracy: 42.30 

Round  93, Global train loss: 0.156, Global test loss: 1.621, Global test accuracy: 47.30 

Round  94, Train loss: 0.153, Test loss: 3.937, Test accuracy: 42.11 

Round  94, Global train loss: 0.153, Global test loss: 1.637, Global test accuracy: 48.25 

Round  95, Train loss: 0.192, Test loss: 3.979, Test accuracy: 41.75 

Round  95, Global train loss: 0.192, Global test loss: 1.690, Global test accuracy: 49.84 

Round  96, Train loss: 0.173, Test loss: 4.046, Test accuracy: 41.63 

Round  96, Global train loss: 0.173, Global test loss: 1.566, Global test accuracy: 46.50 

Round  97, Train loss: 0.210, Test loss: 3.975, Test accuracy: 41.85 

Round  97, Global train loss: 0.210, Global test loss: 1.647, Global test accuracy: 47.86 

Round  98, Train loss: 0.129, Test loss: 3.970, Test accuracy: 42.06 

Round  98, Global train loss: 0.129, Global test loss: 1.603, Global test accuracy: 47.30 

Round  99, Train loss: 0.142, Test loss: 4.026, Test accuracy: 41.97 

Round  99, Global train loss: 0.142, Global test loss: 1.642, Global test accuracy: 46.92 

Final Round, Train loss: 0.154, Test loss: 4.276, Test accuracy: 41.48 

Final Round, Global train loss: 0.154, Global test loss: 1.642, Global test accuracy: 46.92 

Average accuracy final 10 rounds: 41.98333333333333 

Average global accuracy final 10 rounds: 47.59833333333333 

3442.9786751270294
[1.4078669548034668, 2.5854790210723877, 3.7604782581329346, 4.9379494190216064, 6.107939958572388, 7.272692918777466, 8.441473722457886, 9.610953569412231, 10.776527166366577, 11.945287942886353, 13.09828233718872, 14.240771293640137, 15.379443883895874, 16.51565194129944, 17.647135972976685, 18.78426456451416, 19.917388200759888, 21.04986071586609, 22.1911039352417, 23.330859422683716, 24.475985050201416, 25.62272810935974, 26.770379781723022, 27.91938614845276, 28.92496967315674, 29.91848063468933, 30.91738724708557, 31.911640644073486, 32.9051673412323, 33.904743671417236, 34.89936327934265, 35.89938306808472, 36.892348289489746, 37.886805295944214, 38.89080810546875, 39.88302779197693, 40.88598895072937, 41.891775608062744, 42.88705539703369, 43.89154505729675, 44.880207777023315, 45.86846470832825, 46.86656141281128, 47.86129117012024, 48.86022734642029, 49.8576717376709, 50.853628158569336, 51.85283637046814, 52.848334550857544, 53.84715819358826, 54.843159437179565, 55.83540678024292, 56.83438205718994, 57.82746148109436, 58.82485389709473, 59.82559657096863, 60.835541009902954, 61.834484815597534, 62.82829928398132, 63.82154130935669, 64.82110047340393, 65.81278324127197, 66.81351971626282, 67.80993962287903, 68.80279040336609, 69.80295658111572, 70.79630255699158, 71.79446291923523, 72.79431056976318, 73.78757691383362, 74.7860472202301, 75.77963304519653, 76.77489566802979, 77.77491116523743, 78.77174401283264, 79.77115821838379, 80.76643657684326, 81.75693535804749, 82.75815868377686, 83.7512423992157, 84.75070524215698, 85.74408078193665, 86.73797965049744, 87.73900723457336, 88.73198390007019, 89.73365044593811, 90.72552514076233, 91.71859574317932, 92.71919536590576, 93.71364903450012, 94.71454524993896, 95.71265602111816, 96.71064782142639, 97.70477986335754, 98.70005249977112, 99.70440006256104, 100.70353436470032, 101.69878911972046, 102.70150899887085, 103.69736647605896, 105.6984293460846]
[25.67, 29.063333333333333, 31.461666666666666, 32.565, 33.735, 34.54666666666667, 35.07333333333333, 35.406666666666666, 35.98166666666667, 36.46, 36.666666666666664, 37.483333333333334, 38.25833333333333, 39.01833333333333, 39.89666666666667, 39.645, 38.958333333333336, 38.94, 39.306666666666665, 40.36, 40.166666666666664, 40.29, 40.87, 40.90833333333333, 41.02333333333333, 40.861666666666665, 40.815, 40.95666666666666, 40.861666666666665, 40.86666666666667, 40.79, 40.47, 40.69166666666667, 40.545, 40.723333333333336, 40.625, 41.09, 41.211666666666666, 41.465, 41.275, 41.498333333333335, 41.25666666666667, 41.675, 41.83833333333333, 41.665, 41.891666666666666, 41.87166666666667, 41.445, 41.431666666666665, 41.388333333333335, 41.41166666666667, 41.583333333333336, 41.61, 41.373333333333335, 41.295, 41.38166666666667, 41.455, 41.47833333333333, 41.108333333333334, 41.303333333333335, 41.306666666666665, 41.126666666666665, 41.428333333333335, 41.39833333333333, 41.12166666666667, 41.42, 41.57333333333333, 41.44833333333333, 41.60666666666667, 41.675, 41.25666666666667, 41.62833333333333, 41.795, 41.64833333333333, 41.64, 41.91833333333334, 41.833333333333336, 41.638333333333335, 41.681666666666665, 41.69, 41.708333333333336, 41.81166666666667, 41.48166666666667, 41.51166666666666, 41.47, 41.3, 41.42, 41.615, 41.626666666666665, 42.10166666666667, 42.053333333333335, 42.14833333333333, 41.96333333333333, 42.3, 42.10666666666667, 41.748333333333335, 41.635, 41.855, 42.05833333333333, 41.965, 41.47666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.233, Test loss: 2.123, Test accuracy: 23.85 

Round   0, Global train loss: 2.233, Global test loss: 2.127, Global test accuracy: 24.82 

Round   1, Train loss: 2.035, Test loss: 1.954, Test accuracy: 27.77 

Round   1, Global train loss: 2.035, Global test loss: 1.913, Global test accuracy: 29.68 

Round   2, Train loss: 1.885, Test loss: 1.827, Test accuracy: 33.09 

Round   2, Global train loss: 1.885, Global test loss: 1.742, Global test accuracy: 37.07 

Round   3, Train loss: 1.781, Test loss: 1.775, Test accuracy: 34.77 

Round   3, Global train loss: 1.781, Global test loss: 1.647, Global test accuracy: 40.05 

Round   4, Train loss: 1.718, Test loss: 1.695, Test accuracy: 38.11 

Round   4, Global train loss: 1.718, Global test loss: 1.550, Global test accuracy: 44.14 

Round   5, Train loss: 1.650, Test loss: 1.644, Test accuracy: 39.99 

Round   5, Global train loss: 1.650, Global test loss: 1.507, Global test accuracy: 46.20 

Round   6, Train loss: 1.568, Test loss: 1.631, Test accuracy: 40.71 

Round   6, Global train loss: 1.568, Global test loss: 1.463, Global test accuracy: 47.44 

Round   7, Train loss: 1.539, Test loss: 1.616, Test accuracy: 41.19 

Round   7, Global train loss: 1.539, Global test loss: 1.427, Global test accuracy: 48.74 

Round   8, Train loss: 1.487, Test loss: 1.595, Test accuracy: 42.05 

Round   8, Global train loss: 1.487, Global test loss: 1.377, Global test accuracy: 50.98 

Round   9, Train loss: 1.445, Test loss: 1.558, Test accuracy: 43.78 

Round   9, Global train loss: 1.445, Global test loss: 1.350, Global test accuracy: 52.18 

Round  10, Train loss: 1.396, Test loss: 1.548, Test accuracy: 44.39 

Round  10, Global train loss: 1.396, Global test loss: 1.330, Global test accuracy: 53.21 

Round  11, Train loss: 1.377, Test loss: 1.509, Test accuracy: 46.00 

Round  11, Global train loss: 1.377, Global test loss: 1.298, Global test accuracy: 53.99 

Round  12, Train loss: 1.325, Test loss: 1.492, Test accuracy: 46.88 

Round  12, Global train loss: 1.325, Global test loss: 1.274, Global test accuracy: 54.84 

Round  13, Train loss: 1.323, Test loss: 1.461, Test accuracy: 48.13 

Round  13, Global train loss: 1.323, Global test loss: 1.237, Global test accuracy: 55.97 

Round  14, Train loss: 1.262, Test loss: 1.464, Test accuracy: 48.31 

Round  14, Global train loss: 1.262, Global test loss: 1.225, Global test accuracy: 56.49 

Round  15, Train loss: 1.247, Test loss: 1.465, Test accuracy: 48.41 

Round  15, Global train loss: 1.247, Global test loss: 1.197, Global test accuracy: 57.52 

Round  16, Train loss: 1.169, Test loss: 1.449, Test accuracy: 49.30 

Round  16, Global train loss: 1.169, Global test loss: 1.180, Global test accuracy: 58.21 

Round  17, Train loss: 1.139, Test loss: 1.449, Test accuracy: 49.49 

Round  17, Global train loss: 1.139, Global test loss: 1.190, Global test accuracy: 58.26 

Round  18, Train loss: 1.139, Test loss: 1.439, Test accuracy: 49.98 

Round  18, Global train loss: 1.139, Global test loss: 1.161, Global test accuracy: 59.03 

Round  19, Train loss: 1.161, Test loss: 1.408, Test accuracy: 51.34 

Round  19, Global train loss: 1.161, Global test loss: 1.163, Global test accuracy: 59.35 

Round  20, Train loss: 1.110, Test loss: 1.389, Test accuracy: 52.20 

Round  20, Global train loss: 1.110, Global test loss: 1.132, Global test accuracy: 60.53 

Round  21, Train loss: 1.080, Test loss: 1.391, Test accuracy: 52.53 

Round  21, Global train loss: 1.080, Global test loss: 1.122, Global test accuracy: 60.54 

Round  22, Train loss: 1.041, Test loss: 1.384, Test accuracy: 53.08 

Round  22, Global train loss: 1.041, Global test loss: 1.134, Global test accuracy: 60.74 

Round  23, Train loss: 1.017, Test loss: 1.398, Test accuracy: 52.92 

Round  23, Global train loss: 1.017, Global test loss: 1.133, Global test accuracy: 61.08 

Round  24, Train loss: 1.031, Test loss: 1.389, Test accuracy: 53.59 

Round  24, Global train loss: 1.031, Global test loss: 1.105, Global test accuracy: 61.74 

Round  25, Train loss: 0.982, Test loss: 1.402, Test accuracy: 53.50 

Round  25, Global train loss: 0.982, Global test loss: 1.106, Global test accuracy: 61.85 

Round  26, Train loss: 0.960, Test loss: 1.403, Test accuracy: 53.54 

Round  26, Global train loss: 0.960, Global test loss: 1.125, Global test accuracy: 60.97 

Round  27, Train loss: 0.947, Test loss: 1.406, Test accuracy: 53.71 

Round  27, Global train loss: 0.947, Global test loss: 1.106, Global test accuracy: 61.40 

Round  28, Train loss: 0.934, Test loss: 1.391, Test accuracy: 54.35 

Round  28, Global train loss: 0.934, Global test loss: 1.095, Global test accuracy: 62.51 

Round  29, Train loss: 0.977, Test loss: 1.380, Test accuracy: 54.83 

Round  29, Global train loss: 0.977, Global test loss: 1.085, Global test accuracy: 62.37 

Round  30, Train loss: 0.915, Test loss: 1.381, Test accuracy: 55.05 

Round  30, Global train loss: 0.915, Global test loss: 1.095, Global test accuracy: 62.82 

Round  31, Train loss: 0.877, Test loss: 1.370, Test accuracy: 55.39 

Round  31, Global train loss: 0.877, Global test loss: 1.089, Global test accuracy: 62.59 

Round  32, Train loss: 0.929, Test loss: 1.359, Test accuracy: 55.98 

Round  32, Global train loss: 0.929, Global test loss: 1.061, Global test accuracy: 63.50 

Round  33, Train loss: 0.845, Test loss: 1.357, Test accuracy: 56.23 

Round  33, Global train loss: 0.845, Global test loss: 1.075, Global test accuracy: 63.72 

Round  34, Train loss: 0.879, Test loss: 1.369, Test accuracy: 56.24 

Round  34, Global train loss: 0.879, Global test loss: 1.059, Global test accuracy: 63.85 

Round  35, Train loss: 0.874, Test loss: 1.365, Test accuracy: 56.45 

Round  35, Global train loss: 0.874, Global test loss: 1.047, Global test accuracy: 64.37 

Round  36, Train loss: 0.848, Test loss: 1.367, Test accuracy: 56.67 

Round  36, Global train loss: 0.848, Global test loss: 1.068, Global test accuracy: 64.02 

Round  37, Train loss: 0.796, Test loss: 1.370, Test accuracy: 56.91 

Round  37, Global train loss: 0.796, Global test loss: 1.068, Global test accuracy: 64.40 

Round  38, Train loss: 0.803, Test loss: 1.372, Test accuracy: 57.07 

Round  38, Global train loss: 0.803, Global test loss: 1.071, Global test accuracy: 64.49 

Round  39, Train loss: 0.823, Test loss: 1.369, Test accuracy: 57.30 

Round  39, Global train loss: 0.823, Global test loss: 1.066, Global test accuracy: 64.48 

Round  40, Train loss: 0.764, Test loss: 1.377, Test accuracy: 57.45 

Round  40, Global train loss: 0.764, Global test loss: 1.065, Global test accuracy: 65.10 

Round  41, Train loss: 0.743, Test loss: 1.389, Test accuracy: 57.39 

Round  41, Global train loss: 0.743, Global test loss: 1.080, Global test accuracy: 64.93 

Round  42, Train loss: 0.770, Test loss: 1.401, Test accuracy: 57.53 

Round  42, Global train loss: 0.770, Global test loss: 1.080, Global test accuracy: 64.47 

Round  43, Train loss: 0.761, Test loss: 1.378, Test accuracy: 57.80 

Round  43, Global train loss: 0.761, Global test loss: 1.040, Global test accuracy: 65.33 

Round  44, Train loss: 0.761, Test loss: 1.368, Test accuracy: 58.08 

Round  44, Global train loss: 0.761, Global test loss: 1.057, Global test accuracy: 65.20 

Round  45, Train loss: 0.762, Test loss: 1.386, Test accuracy: 58.09 

Round  45, Global train loss: 0.762, Global test loss: 1.049, Global test accuracy: 65.82 

Round  46, Train loss: 0.732, Test loss: 1.388, Test accuracy: 58.22 

Round  46, Global train loss: 0.732, Global test loss: 1.046, Global test accuracy: 65.38 

Round  47, Train loss: 0.704, Test loss: 1.381, Test accuracy: 58.67 

Round  47, Global train loss: 0.704, Global test loss: 1.058, Global test accuracy: 66.13 

Round  48, Train loss: 0.715, Test loss: 1.378, Test accuracy: 58.59 

Round  48, Global train loss: 0.715, Global test loss: 1.056, Global test accuracy: 65.54 

Round  49, Train loss: 0.695, Test loss: 1.377, Test accuracy: 58.70 

Round  49, Global train loss: 0.695, Global test loss: 1.053, Global test accuracy: 66.08 

Round  50, Train loss: 0.691, Test loss: 1.382, Test accuracy: 58.72 

Round  50, Global train loss: 0.691, Global test loss: 1.070, Global test accuracy: 65.64 

Round  51, Train loss: 0.710, Test loss: 1.404, Test accuracy: 58.74 

Round  51, Global train loss: 0.710, Global test loss: 1.049, Global test accuracy: 66.35 

Round  52, Train loss: 0.675, Test loss: 1.413, Test accuracy: 58.66 

Round  52, Global train loss: 0.675, Global test loss: 1.058, Global test accuracy: 66.07 

Round  53, Train loss: 0.668, Test loss: 1.426, Test accuracy: 58.82 

Round  53, Global train loss: 0.668, Global test loss: 1.073, Global test accuracy: 66.52 

Round  54, Train loss: 0.617, Test loss: 1.412, Test accuracy: 59.30 

Round  54, Global train loss: 0.617, Global test loss: 1.065, Global test accuracy: 66.44 

Round  55, Train loss: 0.636, Test loss: 1.429, Test accuracy: 59.27 

Round  55, Global train loss: 0.636, Global test loss: 1.063, Global test accuracy: 66.69 

Round  56, Train loss: 0.619, Test loss: 1.437, Test accuracy: 59.20 

Round  56, Global train loss: 0.619, Global test loss: 1.077, Global test accuracy: 66.17 

Round  57, Train loss: 0.621, Test loss: 1.452, Test accuracy: 58.92 

Round  57, Global train loss: 0.621, Global test loss: 1.063, Global test accuracy: 66.22 

Round  58, Train loss: 0.628, Test loss: 1.461, Test accuracy: 58.73 

Round  58, Global train loss: 0.628, Global test loss: 1.061, Global test accuracy: 66.44 

Round  59, Train loss: 0.598, Test loss: 1.471, Test accuracy: 59.11 

Round  59, Global train loss: 0.598, Global test loss: 1.095, Global test accuracy: 66.40 

Round  60, Train loss: 0.626, Test loss: 1.470, Test accuracy: 59.16 

Round  60, Global train loss: 0.626, Global test loss: 1.078, Global test accuracy: 66.55 

Round  61, Train loss: 0.585, Test loss: 1.473, Test accuracy: 59.36 

Round  61, Global train loss: 0.585, Global test loss: 1.094, Global test accuracy: 66.53 

Round  62, Train loss: 0.635, Test loss: 1.454, Test accuracy: 59.86 

Round  62, Global train loss: 0.635, Global test loss: 1.082, Global test accuracy: 66.52 

Round  63, Train loss: 0.623, Test loss: 1.466, Test accuracy: 59.86 

Round  63, Global train loss: 0.623, Global test loss: 1.079, Global test accuracy: 66.71 

Round  64, Train loss: 0.551, Test loss: 1.464, Test accuracy: 60.03 

Round  64, Global train loss: 0.551, Global test loss: 1.091, Global test accuracy: 66.64 

Round  65, Train loss: 0.523, Test loss: 1.472, Test accuracy: 59.98 

Round  65, Global train loss: 0.523, Global test loss: 1.130, Global test accuracy: 66.34 

Round  66, Train loss: 0.579, Test loss: 1.465, Test accuracy: 60.28 

Round  66, Global train loss: 0.579, Global test loss: 1.104, Global test accuracy: 66.70 

Round  67, Train loss: 0.590, Test loss: 1.474, Test accuracy: 60.08 

Round  67, Global train loss: 0.590, Global test loss: 1.093, Global test accuracy: 66.46 

Round  68, Train loss: 0.556, Test loss: 1.477, Test accuracy: 59.95 

Round  68, Global train loss: 0.556, Global test loss: 1.108, Global test accuracy: 66.30 

Round  69, Train loss: 0.561, Test loss: 1.484, Test accuracy: 60.06 

Round  69, Global train loss: 0.561, Global test loss: 1.099, Global test accuracy: 67.10 

Round  70, Train loss: 0.533, Test loss: 1.474, Test accuracy: 60.06 

Round  70, Global train loss: 0.533, Global test loss: 1.111, Global test accuracy: 66.70 

Round  71, Train loss: 0.533, Test loss: 1.480, Test accuracy: 60.07 

Round  71, Global train loss: 0.533, Global test loss: 1.109, Global test accuracy: 66.53 

Round  72, Train loss: 0.542, Test loss: 1.492, Test accuracy: 59.84 

Round  72, Global train loss: 0.542, Global test loss: 1.119, Global test accuracy: 66.31 

Round  73, Train loss: 0.510, Test loss: 1.479, Test accuracy: 60.21 

Round  73, Global train loss: 0.510, Global test loss: 1.129, Global test accuracy: 67.00 

Round  74, Train loss: 0.545, Test loss: 1.470, Test accuracy: 60.37 

Round  74, Global train loss: 0.545, Global test loss: 1.094, Global test accuracy: 66.69 

Round  75, Train loss: 0.547, Test loss: 1.470, Test accuracy: 60.48 

Round  75, Global train loss: 0.547, Global test loss: 1.079, Global test accuracy: 67.39 

Round  76, Train loss: 0.535, Test loss: 1.460, Test accuracy: 60.78 

Round  76, Global train loss: 0.535, Global test loss: 1.100, Global test accuracy: 67.27 

Round  77, Train loss: 0.558, Test loss: 1.465, Test accuracy: 60.64 

Round  77, Global train loss: 0.558, Global test loss: 1.110, Global test accuracy: 66.87 

Round  78, Train loss: 0.522, Test loss: 1.470, Test accuracy: 60.79 

Round  78, Global train loss: 0.522, Global test loss: 1.087, Global test accuracy: 67.18 

Round  79, Train loss: 0.526, Test loss: 1.471, Test accuracy: 60.91 

Round  79, Global train loss: 0.526, Global test loss: 1.089, Global test accuracy: 67.69 

Round  80, Train loss: 0.480, Test loss: 1.479, Test accuracy: 60.77 

Round  80, Global train loss: 0.480, Global test loss: 1.134, Global test accuracy: 67.05 

Round  81, Train loss: 0.521, Test loss: 1.481, Test accuracy: 60.83 

Round  81, Global train loss: 0.521, Global test loss: 1.103, Global test accuracy: 67.08 

Round  82, Train loss: 0.531, Test loss: 1.512, Test accuracy: 60.38 

Round  82, Global train loss: 0.531, Global test loss: 1.094, Global test accuracy: 67.36 

Round  83, Train loss: 0.514, Test loss: 1.513, Test accuracy: 60.53 

Round  83, Global train loss: 0.514, Global test loss: 1.123, Global test accuracy: 66.73 

Round  84, Train loss: 0.477, Test loss: 1.503, Test accuracy: 60.62 

Round  84, Global train loss: 0.477, Global test loss: 1.117, Global test accuracy: 67.23 

Round  85, Train loss: 0.471, Test loss: 1.507, Test accuracy: 60.78 

Round  85, Global train loss: 0.471, Global test loss: 1.134, Global test accuracy: 67.14 

Round  86, Train loss: 0.480, Test loss: 1.514, Test accuracy: 60.93 

Round  86, Global train loss: 0.480, Global test loss: 1.121, Global test accuracy: 67.25 

Round  87, Train loss: 0.446, Test loss: 1.511, Test accuracy: 60.93 

Round  87, Global train loss: 0.446, Global test loss: 1.117, Global test accuracy: 67.46 

Round  88, Train loss: 0.503, Test loss: 1.515, Test accuracy: 61.16 

Round  88, Global train loss: 0.503, Global test loss: 1.126, Global test accuracy: 67.40 

Round  89, Train loss: 0.473, Test loss: 1.524, Test accuracy: 61.05 

Round  89, Global train loss: 0.473, Global test loss: 1.125, Global test accuracy: 67.17 

Round  90, Train loss: 0.456, Test loss: 1.519, Test accuracy: 61.10 

Round  90, Global train loss: 0.456, Global test loss: 1.120, Global test accuracy: 67.67 

Round  91, Train loss: 0.411, Test loss: 1.511, Test accuracy: 61.34 

Round  91, Global train loss: 0.411, Global test loss: 1.148, Global test accuracy: 67.26 

Round  92, Train loss: 0.481, Test loss: 1.499, Test accuracy: 61.36 

Round  92, Global train loss: 0.481, Global test loss: 1.146, Global test accuracy: 67.34 

Round  93, Train loss: 0.448, Test loss: 1.531, Test accuracy: 61.17 

Round  93, Global train loss: 0.448, Global test loss: 1.144, Global test accuracy: 67.13 

Round  94, Train loss: 0.440, Test loss: 1.540, Test accuracy: 61.12 

Round  94, Global train loss: 0.440, Global test loss: 1.169, Global test accuracy: 67.16 

Round  95, Train loss: 0.454, Test loss: 1.555, Test accuracy: 61.12 

Round  95, Global train loss: 0.454, Global test loss: 1.144, Global test accuracy: 68.45 

Round  96, Train loss: 0.446, Test loss: 1.564, Test accuracy: 61.01 

Round  96, Global train loss: 0.446, Global test loss: 1.167, Global test accuracy: 67.31 

Round  97, Train loss: 0.448, Test loss: 1.548, Test accuracy: 61.37 

Round  97, Global train loss: 0.448, Global test loss: 1.165, Global test accuracy: 67.46 

Round  98, Train loss: 0.417, Test loss: 1.528, Test accuracy: 61.77 

Round  98, Global train loss: 0.417, Global test loss: 1.169, Global test accuracy: 67.87 

Round  99, Train loss: 0.461, Test loss: 1.525, Test accuracy: 61.95 

Round  99, Global train loss: 0.461, Global test loss: 1.139, Global test accuracy: 67.65 

Final Round, Train loss: 0.351, Test loss: 1.711, Test accuracy: 61.26 

Final Round, Global train loss: 0.351, Global test loss: 1.139, Global test accuracy: 67.65 

Average accuracy final 10 rounds: 61.33083333333333 

Average global accuracy final 10 rounds: 67.53016666666666 

3453.364027261734
[1.3996696472167969, 2.4088425636291504, 3.413928270339966, 4.420943260192871, 5.42512845993042, 6.4323296546936035, 7.441100120544434, 8.447503089904785, 9.44956636428833, 10.454710006713867, 11.459614276885986, 12.467694997787476, 13.474336385726929, 14.478411436080933, 15.481841564178467, 16.488852977752686, 17.491909742355347, 18.501903295516968, 19.507856130599976, 20.519784450531006, 21.523810386657715, 22.531126260757446, 23.538840770721436, 24.544251441955566, 25.55176544189453, 26.560953855514526, 27.565232038497925, 28.560646295547485, 29.565725088119507, 30.596500873565674, 31.601721048355103, 32.60662889480591, 33.61142539978027, 34.61560082435608, 35.60765886306763, 36.61202692985535, 37.63307309150696, 38.636852979660034, 39.64160490036011, 40.64617085456848, 41.65173959732056, 42.652570962905884, 43.65549850463867, 44.66055393218994, 45.66671323776245, 46.672954082489014, 47.66888642311096, 48.675517082214355, 49.682923555374146, 50.68649625778198, 51.692721366882324, 52.686723947525024, 53.69279098510742, 54.69905948638916, 55.705379486083984, 56.71157479286194, 57.70839786529541, 58.714224100112915, 59.72197151184082, 60.72349286079407, 61.73013687133789, 62.73429250717163, 63.73783302307129, 64.74312925338745, 65.74878978729248, 66.75491428375244, 67.76111507415771, 68.7703025341034, 69.77781677246094, 70.7835042476654, 71.79204225540161, 72.79929876327515, 73.80336928367615, 74.81154584884644, 75.8144063949585, 76.8126871585846, 77.81282806396484, 78.81186628341675, 79.80808687210083, 80.81193542480469, 81.81808304786682, 82.8246955871582, 83.82934641838074, 84.83737444877625, 85.83266305923462, 86.83138799667358, 87.83801794052124, 88.84308314323425, 89.84879207611084, 90.85501551628113, 91.8570384979248, 92.85151743888855, 93.84900259971619, 94.85332441329956, 95.85953617095947, 96.86594843864441, 97.86899781227112, 98.8743507862091, 99.87786269187927, 100.88449215888977, 102.89004826545715]
[23.846666666666668, 27.766666666666666, 33.09166666666667, 34.766666666666666, 38.10666666666667, 39.98833333333334, 40.70666666666666, 41.19, 42.055, 43.781666666666666, 44.38666666666666, 45.99666666666667, 46.88, 48.12833333333333, 48.31166666666667, 48.415, 49.303333333333335, 49.48833333333334, 49.975, 51.34, 52.19833333333333, 52.528333333333336, 53.07666666666667, 52.92, 53.585, 53.5, 53.538333333333334, 53.70666666666666, 54.35, 54.83, 55.055, 55.391666666666666, 55.97666666666667, 56.233333333333334, 56.24166666666667, 56.445, 56.67166666666667, 56.905, 57.071666666666665, 57.295, 57.445, 57.388333333333335, 57.531666666666666, 57.805, 58.07666666666667, 58.085, 58.215, 58.66833333333334, 58.595, 58.70333333333333, 58.71666666666667, 58.745, 58.66166666666667, 58.821666666666665, 59.305, 59.265, 59.2, 58.925, 58.72666666666667, 59.10666666666667, 59.165, 59.361666666666665, 59.861666666666665, 59.86, 60.03333333333333, 59.98166666666667, 60.28, 60.08166666666666, 59.94833333333333, 60.056666666666665, 60.06166666666667, 60.07, 59.84, 60.211666666666666, 60.37166666666667, 60.48, 60.785, 60.64, 60.791666666666664, 60.913333333333334, 60.766666666666666, 60.825, 60.38333333333333, 60.53, 60.623333333333335, 60.78, 60.931666666666665, 60.93, 61.16166666666667, 61.05166666666667, 61.10333333333333, 61.343333333333334, 61.36, 61.166666666666664, 61.12, 61.12166666666667, 61.00833333333333, 61.36833333333333, 61.77166666666667, 61.945, 61.25833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.263, Test loss: 2.156, Test accuracy: 20.19 

Round   1, Train loss: 2.102, Test loss: 1.985, Test accuracy: 27.95 

Round   2, Train loss: 1.954, Test loss: 1.884, Test accuracy: 31.16 

Round   3, Train loss: 1.898, Test loss: 1.816, Test accuracy: 33.28 

Round   4, Train loss: 1.811, Test loss: 1.755, Test accuracy: 35.46 

Round   5, Train loss: 1.774, Test loss: 1.698, Test accuracy: 37.59 

Round   6, Train loss: 1.689, Test loss: 1.641, Test accuracy: 39.82 

Round   7, Train loss: 1.645, Test loss: 1.615, Test accuracy: 41.14 

Round   8, Train loss: 1.635, Test loss: 1.613, Test accuracy: 41.35 

Round   9, Train loss: 1.584, Test loss: 1.584, Test accuracy: 41.95 

Round  10, Train loss: 1.569, Test loss: 1.568, Test accuracy: 43.06 

Round  11, Train loss: 1.556, Test loss: 1.522, Test accuracy: 44.66 

Round  12, Train loss: 1.491, Test loss: 1.495, Test accuracy: 45.72 

Round  13, Train loss: 1.506, Test loss: 1.474, Test accuracy: 46.44 

Round  14, Train loss: 1.458, Test loss: 1.464, Test accuracy: 46.60 

Round  15, Train loss: 1.469, Test loss: 1.455, Test accuracy: 46.95 

Round  16, Train loss: 1.410, Test loss: 1.433, Test accuracy: 47.72 

Round  17, Train loss: 1.418, Test loss: 1.419, Test accuracy: 48.27 

Round  18, Train loss: 1.400, Test loss: 1.422, Test accuracy: 48.62 

Round  19, Train loss: 1.324, Test loss: 1.398, Test accuracy: 49.27 

Round  20, Train loss: 1.363, Test loss: 1.361, Test accuracy: 50.62 

Round  21, Train loss: 1.344, Test loss: 1.356, Test accuracy: 51.33 

Round  22, Train loss: 1.292, Test loss: 1.344, Test accuracy: 51.98 

Round  23, Train loss: 1.292, Test loss: 1.329, Test accuracy: 52.06 

Round  24, Train loss: 1.282, Test loss: 1.325, Test accuracy: 52.37 

Round  25, Train loss: 1.263, Test loss: 1.310, Test accuracy: 53.02 

Round  26, Train loss: 1.223, Test loss: 1.302, Test accuracy: 53.49 

Round  27, Train loss: 1.238, Test loss: 1.279, Test accuracy: 54.48 

Round  28, Train loss: 1.209, Test loss: 1.284, Test accuracy: 54.20 

Round  29, Train loss: 1.180, Test loss: 1.304, Test accuracy: 53.24 

Round  30, Train loss: 1.205, Test loss: 1.308, Test accuracy: 53.30 

Round  31, Train loss: 1.170, Test loss: 1.301, Test accuracy: 54.03 

Round  32, Train loss: 1.137, Test loss: 1.283, Test accuracy: 54.37 

Round  33, Train loss: 1.120, Test loss: 1.281, Test accuracy: 54.45 

Round  34, Train loss: 1.129, Test loss: 1.275, Test accuracy: 54.88 

Round  35, Train loss: 1.123, Test loss: 1.262, Test accuracy: 55.45 

Round  36, Train loss: 1.136, Test loss: 1.253, Test accuracy: 55.96 

Round  37, Train loss: 1.044, Test loss: 1.237, Test accuracy: 56.63 

Round  38, Train loss: 1.102, Test loss: 1.223, Test accuracy: 57.18 

Round  39, Train loss: 1.080, Test loss: 1.211, Test accuracy: 57.78 

Round  40, Train loss: 0.995, Test loss: 1.227, Test accuracy: 57.20 

Round  41, Train loss: 1.023, Test loss: 1.212, Test accuracy: 57.76 

Round  42, Train loss: 1.011, Test loss: 1.222, Test accuracy: 57.71 

Round  43, Train loss: 0.991, Test loss: 1.203, Test accuracy: 58.38 

Round  44, Train loss: 0.959, Test loss: 1.205, Test accuracy: 58.57 

Round  45, Train loss: 1.018, Test loss: 1.203, Test accuracy: 58.50 

Round  46, Train loss: 1.010, Test loss: 1.211, Test accuracy: 57.98 

Round  47, Train loss: 0.979, Test loss: 1.177, Test accuracy: 59.33 

Round  48, Train loss: 0.965, Test loss: 1.173, Test accuracy: 59.24 

Round  49, Train loss: 0.954, Test loss: 1.176, Test accuracy: 59.50 

Round  50, Train loss: 0.904, Test loss: 1.189, Test accuracy: 59.22 

Round  51, Train loss: 0.930, Test loss: 1.187, Test accuracy: 59.74 

Round  52, Train loss: 0.907, Test loss: 1.185, Test accuracy: 59.88 

Round  53, Train loss: 0.847, Test loss: 1.172, Test accuracy: 60.10 

Round  54, Train loss: 0.841, Test loss: 1.183, Test accuracy: 59.93 

Round  55, Train loss: 0.884, Test loss: 1.175, Test accuracy: 60.42 

Round  56, Train loss: 0.858, Test loss: 1.167, Test accuracy: 60.92 

Round  57, Train loss: 0.866, Test loss: 1.183, Test accuracy: 60.60 

Round  58, Train loss: 0.877, Test loss: 1.181, Test accuracy: 60.46 

Round  59, Train loss: 0.854, Test loss: 1.190, Test accuracy: 60.45 

Round  60, Train loss: 0.838, Test loss: 1.208, Test accuracy: 59.98 

Round  61, Train loss: 0.873, Test loss: 1.186, Test accuracy: 60.63 

Round  62, Train loss: 0.814, Test loss: 1.199, Test accuracy: 60.54 

Round  63, Train loss: 0.841, Test loss: 1.188, Test accuracy: 60.57 

Round  64, Train loss: 0.806, Test loss: 1.154, Test accuracy: 60.92 

Round  65, Train loss: 0.811, Test loss: 1.166, Test accuracy: 61.19 

Round  66, Train loss: 0.802, Test loss: 1.159, Test accuracy: 61.38 

Round  67, Train loss: 0.764, Test loss: 1.165, Test accuracy: 61.81 

Round  68, Train loss: 0.815, Test loss: 1.180, Test accuracy: 61.23 

Round  69, Train loss: 0.749, Test loss: 1.186, Test accuracy: 60.95 

Round  70, Train loss: 0.766, Test loss: 1.173, Test accuracy: 61.94 

Round  71, Train loss: 0.714, Test loss: 1.199, Test accuracy: 61.24 

Round  72, Train loss: 0.700, Test loss: 1.188, Test accuracy: 61.42 

Round  73, Train loss: 0.744, Test loss: 1.201, Test accuracy: 61.45 

Round  74, Train loss: 0.722, Test loss: 1.193, Test accuracy: 61.74 

Round  75, Train loss: 0.692, Test loss: 1.202, Test accuracy: 62.23 

Round  76, Train loss: 0.678, Test loss: 1.205, Test accuracy: 61.81 

Round  77, Train loss: 0.711, Test loss: 1.208, Test accuracy: 61.24 

Round  78, Train loss: 0.694, Test loss: 1.208, Test accuracy: 61.27 

Round  79, Train loss: 0.706, Test loss: 1.188, Test accuracy: 62.08 

Round  80, Train loss: 0.651, Test loss: 1.201, Test accuracy: 62.11 

Round  81, Train loss: 0.681, Test loss: 1.231, Test accuracy: 61.93 

Round  82, Train loss: 0.691, Test loss: 1.227, Test accuracy: 61.52 

Round  83, Train loss: 0.667, Test loss: 1.201, Test accuracy: 62.03 

Round  84, Train loss: 0.643, Test loss: 1.228, Test accuracy: 61.94 

Round  85, Train loss: 0.635, Test loss: 1.225, Test accuracy: 62.07 

Round  86, Train loss: 0.641, Test loss: 1.223, Test accuracy: 62.22 

Round  87, Train loss: 0.643, Test loss: 1.208, Test accuracy: 62.66 

Round  88, Train loss: 0.664, Test loss: 1.230, Test accuracy: 62.32 

Round  89, Train loss: 0.623, Test loss: 1.232, Test accuracy: 62.21 

Round  90, Train loss: 0.645, Test loss: 1.224, Test accuracy: 62.49 

Round  91, Train loss: 0.633, Test loss: 1.247, Test accuracy: 62.48 

Round  92, Train loss: 0.609, Test loss: 1.255, Test accuracy: 62.25 

Round  93, Train loss: 0.579, Test loss: 1.222, Test accuracy: 62.43 

Round  94, Train loss: 0.563, Test loss: 1.243, Test accuracy: 62.05 

Round  95, Train loss: 0.606, Test loss: 1.252, Test accuracy: 62.58 

Round  96, Train loss: 0.561, Test loss: 1.254, Test accuracy: 62.48 

Round  97, Train loss: 0.539, Test loss: 1.277, Test accuracy: 62.62 

Round  98, Train loss: 0.676, Test loss: 1.240, Test accuracy: 62.27 

Round  99, Train loss: 0.633, Test loss: 1.256, Test accuracy: 62.45 

Final Round, Train loss: 0.509, Test loss: 1.266, Test accuracy: 62.48 

Average accuracy final 10 rounds: 62.409666666666666 

1978.087975025177
[1.2612972259521484, 2.1998026371002197, 3.137249231338501, 4.0763304233551025, 5.025578260421753, 5.969166278839111, 6.919104814529419, 7.8568267822265625, 8.79644775390625, 9.736024856567383, 10.6759934425354, 11.612808465957642, 12.55042839050293, 13.488256931304932, 14.4254469871521, 15.364811182022095, 16.304996728897095, 17.241984367370605, 18.181551218032837, 19.122812271118164, 20.060551404953003, 20.999353885650635, 21.933598041534424, 22.870227336883545, 23.801589488983154, 24.74324083328247, 25.678647994995117, 26.616060495376587, 27.552356004714966, 28.491068124771118, 29.432178020477295, 30.375479698181152, 31.322359085083008, 32.25810742378235, 33.19541120529175, 34.13572144508362, 35.07382392883301, 36.01326894760132, 36.95319175720215, 37.893123626708984, 38.83312153816223, 39.76996731758118, 40.71206259727478, 41.64363765716553, 42.57828164100647, 43.51313638687134, 44.447752237319946, 45.38651895523071, 46.32499575614929, 47.25737714767456, 48.191009283065796, 49.12472891807556, 50.055196046829224, 50.98782777786255, 51.91867017745972, 52.847418546676636, 53.77908492088318, 54.710708141326904, 55.64304971694946, 56.57876634597778, 57.51132416725159, 58.4463005065918, 59.37574005126953, 60.30806350708008, 61.24308133125305, 62.176427125930786, 63.11015510559082, 64.04629802703857, 64.98154044151306, 65.91451001167297, 66.8477110862732, 67.78109669685364, 68.71624398231506, 69.65143179893494, 70.5844087600708, 71.52027988433838, 72.45388746261597, 73.3877866268158, 74.32528066635132, 75.26555156707764, 76.20383667945862, 77.14482688903809, 78.08161330223083, 79.01935958862305, 79.95858097076416, 80.89793539047241, 81.83630776405334, 82.77362990379333, 83.71199035644531, 84.6503574848175, 85.582444190979, 86.51657199859619, 87.44637894630432, 88.37685942649841, 89.30608463287354, 90.23789930343628, 91.16828656196594, 92.1044991016388, 93.04406595230103, 93.98601365089417, 95.74356818199158]
[20.191666666666666, 27.955, 31.156666666666666, 33.278333333333336, 35.46333333333333, 37.59166666666667, 39.82, 41.14, 41.35166666666667, 41.946666666666665, 43.06, 44.665, 45.718333333333334, 46.44, 46.60166666666667, 46.94833333333333, 47.723333333333336, 48.266666666666666, 48.61833333333333, 49.27333333333333, 50.615, 51.33166666666666, 51.975, 52.056666666666665, 52.37, 53.01833333333333, 53.486666666666665, 54.485, 54.20333333333333, 53.245, 53.30166666666667, 54.03, 54.36666666666667, 54.445, 54.88, 55.446666666666665, 55.96333333333333, 56.635, 57.181666666666665, 57.776666666666664, 57.2, 57.76, 57.70666666666666, 58.38166666666667, 58.56666666666667, 58.501666666666665, 57.975, 59.33166666666666, 59.23833333333334, 59.49666666666667, 59.218333333333334, 59.74166666666667, 59.87833333333333, 60.10166666666667, 59.928333333333335, 60.42333333333333, 60.92, 60.598333333333336, 60.45666666666666, 60.45166666666667, 59.983333333333334, 60.62833333333333, 60.541666666666664, 60.56666666666667, 60.92, 61.19166666666667, 61.37833333333333, 61.81166666666667, 61.23166666666667, 60.955, 61.935, 61.23833333333334, 61.416666666666664, 61.445, 61.74333333333333, 62.23, 61.815, 61.24333333333333, 61.266666666666666, 62.07666666666667, 62.10666666666667, 61.93, 61.52333333333333, 62.035, 61.94166666666667, 62.068333333333335, 62.223333333333336, 62.655, 62.32, 62.21333333333333, 62.486666666666665, 62.483333333333334, 62.25333333333333, 62.431666666666665, 62.04666666666667, 62.57833333333333, 62.483333333333334, 62.62, 62.266666666666666, 62.446666666666665, 62.483333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.211, Test loss: 2.065, Test accuracy: 23.54 

Round   1, Train loss: 1.997, Test loss: 1.880, Test accuracy: 30.92 

Round   2, Train loss: 1.847, Test loss: 1.775, Test accuracy: 34.87 

Round   3, Train loss: 1.781, Test loss: 1.683, Test accuracy: 38.62 

Round   4, Train loss: 1.694, Test loss: 1.607, Test accuracy: 40.91 

Round   5, Train loss: 1.647, Test loss: 1.555, Test accuracy: 42.85 

Round   6, Train loss: 1.601, Test loss: 1.513, Test accuracy: 44.02 

Round   7, Train loss: 1.553, Test loss: 1.478, Test accuracy: 45.87 

Round   8, Train loss: 1.506, Test loss: 1.459, Test accuracy: 46.62 

Round   9, Train loss: 1.471, Test loss: 1.432, Test accuracy: 47.81 

Round  10, Train loss: 1.433, Test loss: 1.411, Test accuracy: 48.82 

Round  11, Train loss: 1.412, Test loss: 1.369, Test accuracy: 50.33 

Round  12, Train loss: 1.389, Test loss: 1.351, Test accuracy: 50.93 

Round  13, Train loss: 1.356, Test loss: 1.337, Test accuracy: 51.80 

Round  14, Train loss: 1.335, Test loss: 1.311, Test accuracy: 52.91 

Round  15, Train loss: 1.277, Test loss: 1.302, Test accuracy: 53.25 

Round  16, Train loss: 1.262, Test loss: 1.284, Test accuracy: 53.81 

Round  17, Train loss: 1.250, Test loss: 1.265, Test accuracy: 54.48 

Round  18, Train loss: 1.200, Test loss: 1.252, Test accuracy: 55.20 

Round  19, Train loss: 1.168, Test loss: 1.249, Test accuracy: 55.55 

Round  20, Train loss: 1.141, Test loss: 1.233, Test accuracy: 56.25 

Round  21, Train loss: 1.143, Test loss: 1.217, Test accuracy: 56.96 

Round  22, Train loss: 1.095, Test loss: 1.205, Test accuracy: 57.36 

Round  23, Train loss: 1.064, Test loss: 1.202, Test accuracy: 57.48 

Round  24, Train loss: 1.064, Test loss: 1.191, Test accuracy: 58.12 

Round  25, Train loss: 1.042, Test loss: 1.204, Test accuracy: 57.85 

Round  26, Train loss: 1.042, Test loss: 1.176, Test accuracy: 58.83 

Round  27, Train loss: 0.983, Test loss: 1.181, Test accuracy: 59.03 

Round  28, Train loss: 0.949, Test loss: 1.196, Test accuracy: 58.35 

Round  29, Train loss: 0.979, Test loss: 1.185, Test accuracy: 58.96 

Round  30, Train loss: 0.898, Test loss: 1.179, Test accuracy: 59.21 

Round  31, Train loss: 0.897, Test loss: 1.188, Test accuracy: 59.49 

Round  32, Train loss: 0.871, Test loss: 1.197, Test accuracy: 59.40 

Round  33, Train loss: 0.967, Test loss: 1.186, Test accuracy: 60.16 

Round  34, Train loss: 0.854, Test loss: 1.186, Test accuracy: 60.05 

Round  35, Train loss: 0.860, Test loss: 1.193, Test accuracy: 60.23 

Round  36, Train loss: 0.864, Test loss: 1.192, Test accuracy: 60.05 

Round  37, Train loss: 0.902, Test loss: 1.165, Test accuracy: 61.16 

Round  38, Train loss: 0.849, Test loss: 1.182, Test accuracy: 60.87 

Round  39, Train loss: 0.807, Test loss: 1.175, Test accuracy: 61.08 

Round  40, Train loss: 0.776, Test loss: 1.213, Test accuracy: 60.69 

Round  41, Train loss: 0.797, Test loss: 1.176, Test accuracy: 61.43 

Round  42, Train loss: 0.745, Test loss: 1.209, Test accuracy: 61.14 

Round  43, Train loss: 0.821, Test loss: 1.176, Test accuracy: 61.73 

Round  44, Train loss: 0.739, Test loss: 1.213, Test accuracy: 60.92 

Round  45, Train loss: 0.732, Test loss: 1.204, Test accuracy: 61.09 

Round  46, Train loss: 0.738, Test loss: 1.203, Test accuracy: 61.18 

Round  47, Train loss: 0.727, Test loss: 1.202, Test accuracy: 61.51 

Round  48, Train loss: 0.730, Test loss: 1.200, Test accuracy: 61.54 

Round  49, Train loss: 0.694, Test loss: 1.232, Test accuracy: 61.48 

Round  50, Train loss: 0.707, Test loss: 1.200, Test accuracy: 61.71 

Round  51, Train loss: 0.682, Test loss: 1.240, Test accuracy: 61.78 

Round  52, Train loss: 0.688, Test loss: 1.247, Test accuracy: 61.80 

Round  53, Train loss: 0.645, Test loss: 1.244, Test accuracy: 61.98 

Round  54, Train loss: 0.657, Test loss: 1.242, Test accuracy: 62.21 

Round  55, Train loss: 0.625, Test loss: 1.262, Test accuracy: 62.12 

Round  56, Train loss: 0.608, Test loss: 1.276, Test accuracy: 61.95 

Round  57, Train loss: 0.623, Test loss: 1.263, Test accuracy: 62.05 

Round  58, Train loss: 0.602, Test loss: 1.271, Test accuracy: 62.12 

Round  59, Train loss: 0.551, Test loss: 1.284, Test accuracy: 62.12 

Round  60, Train loss: 0.571, Test loss: 1.297, Test accuracy: 62.12 

Round  61, Train loss: 0.599, Test loss: 1.313, Test accuracy: 62.48 

Round  62, Train loss: 0.509, Test loss: 1.311, Test accuracy: 62.31 

Round  63, Train loss: 0.594, Test loss: 1.288, Test accuracy: 62.77 

Round  64, Train loss: 0.585, Test loss: 1.309, Test accuracy: 62.51 

Round  65, Train loss: 0.524, Test loss: 1.341, Test accuracy: 62.26 

Round  66, Train loss: 0.525, Test loss: 1.329, Test accuracy: 62.48 

Round  67, Train loss: 0.533, Test loss: 1.327, Test accuracy: 62.31 

Round  68, Train loss: 0.544, Test loss: 1.366, Test accuracy: 61.41 

Round  69, Train loss: 0.555, Test loss: 1.350, Test accuracy: 62.56 

Round  70, Train loss: 0.464, Test loss: 1.370, Test accuracy: 62.25 

Round  71, Train loss: 0.551, Test loss: 1.358, Test accuracy: 62.12 

Round  72, Train loss: 0.532, Test loss: 1.344, Test accuracy: 62.64 

Round  73, Train loss: 0.523, Test loss: 1.370, Test accuracy: 62.09 

Round  74, Train loss: 0.539, Test loss: 1.389, Test accuracy: 62.22 

Round  75, Train loss: 0.519, Test loss: 1.413, Test accuracy: 62.16 

Round  76, Train loss: 0.479, Test loss: 1.412, Test accuracy: 62.00 

Round  77, Train loss: 0.426, Test loss: 1.430, Test accuracy: 62.15 

Round  78, Train loss: 0.522, Test loss: 1.419, Test accuracy: 62.91 

Round  79, Train loss: 0.498, Test loss: 1.410, Test accuracy: 62.74 

Round  80, Train loss: 0.480, Test loss: 1.416, Test accuracy: 62.91 

Round  81, Train loss: 0.508, Test loss: 1.388, Test accuracy: 62.47 

Round  82, Train loss: 0.477, Test loss: 1.410, Test accuracy: 62.70 

Round  83, Train loss: 0.447, Test loss: 1.399, Test accuracy: 62.89 

Round  84, Train loss: 0.455, Test loss: 1.430, Test accuracy: 62.70 

Round  85, Train loss: 0.409, Test loss: 1.426, Test accuracy: 62.63 

Round  86, Train loss: 0.421, Test loss: 1.478, Test accuracy: 62.19 

Round  87, Train loss: 0.439, Test loss: 1.488, Test accuracy: 62.29 

Round  88, Train loss: 0.433, Test loss: 1.485, Test accuracy: 62.35 

Round  89, Train loss: 0.406, Test loss: 1.493, Test accuracy: 61.96 

Round  90, Train loss: 0.447, Test loss: 1.511, Test accuracy: 62.09 

Round  91, Train loss: 0.441, Test loss: 1.486, Test accuracy: 62.27 

Round  92, Train loss: 0.434, Test loss: 1.522, Test accuracy: 62.42 

Round  93, Train loss: 0.429, Test loss: 1.508, Test accuracy: 61.94 

Round  94, Train loss: 0.419, Test loss: 1.488, Test accuracy: 62.35 

Round  95, Train loss: 0.476, Test loss: 1.492, Test accuracy: 62.44 

Round  96, Train loss: 0.407, Test loss: 1.536, Test accuracy: 62.82 

Round  97, Train loss: 0.419, Test loss: 1.525, Test accuracy: 62.88 

Round  98, Train loss: 0.399, Test loss: 1.542, Test accuracy: 62.58 

Round  99, Train loss: 0.413, Test loss: 1.546, Test accuracy: 62.88 

Final Round, Train loss: 0.318, Test loss: 1.575, Test accuracy: 62.47 

Average accuracy final 10 rounds: 62.46666666666667 

2181.5166161060333
[1.3940134048461914, 2.5664145946502686, 3.600034475326538, 4.635284185409546, 5.6704652309417725, 6.705831050872803, 7.742094278335571, 8.779372453689575, 9.815118312835693, 10.848108530044556, 11.881092071533203, 12.915398359298706, 13.949086904525757, 14.983534097671509, 16.01602005958557, 17.052123546600342, 18.088430881500244, 19.127342462539673, 20.15912699699402, 21.1911678314209, 22.22221565246582, 23.393304347991943, 24.56608271598816, 25.73358416557312, 26.904709577560425, 28.075764179229736, 29.243566274642944, 30.407238960266113, 31.57857394218445, 32.74590444564819, 33.91030287742615, 35.066681146621704, 36.237942695617676, 37.40628147125244, 38.50429654121399, 39.605961084365845, 40.70229959487915, 41.87053322792053, 43.01431751251221, 44.16188907623291, 45.30739498138428, 46.448004961013794, 47.59537863731384, 48.751145124435425, 49.90610980987549, 51.060327768325806, 52.21438932418823, 53.23041033744812, 54.23123097419739, 55.23229670524597, 56.24023509025574, 57.24628806114197, 58.245924949645996, 59.24372720718384, 60.24342656135559, 61.24486494064331, 62.24527668952942, 63.26448106765747, 64.2721815109253, 65.28156208992004, 66.28794693946838, 67.29511213302612, 68.30078482627869, 69.48483037948608, 70.62953805923462, 71.78881359100342, 72.95011329650879, 74.15184140205383, 75.34395551681519, 76.50362491607666, 77.64581871032715, 78.76858115196228, 79.91882658004761, 81.13393568992615, 82.34067893028259, 83.61955261230469, 84.82321119308472, 86.071204662323, 87.30402207374573, 88.59463477134705, 89.8847439289093, 91.17030096054077, 92.46344542503357, 93.75542116165161, 95.06299328804016, 96.37076020240784, 97.66569638252258, 98.93499302864075, 100.08890128135681, 101.24121451377869, 102.39225625991821, 103.53846073150635, 104.72195792198181, 105.9095230102539, 107.09743022918701, 108.28625512123108, 109.4818320274353, 110.66473269462585, 111.85246419906616, 113.03805375099182, 114.97884893417358]
[23.54, 30.921666666666667, 34.86833333333333, 38.61833333333333, 40.915, 42.85, 44.02166666666667, 45.87166666666667, 46.625, 47.80833333333333, 48.821666666666665, 50.33166666666666, 50.92666666666667, 51.795, 52.91, 53.25333333333333, 53.815, 54.47666666666667, 55.196666666666665, 55.55, 56.25, 56.95666666666666, 57.35666666666667, 57.47666666666667, 58.125, 57.85166666666667, 58.82666666666667, 59.035, 58.35166666666667, 58.95666666666666, 59.211666666666666, 59.486666666666665, 59.403333333333336, 60.15833333333333, 60.053333333333335, 60.22666666666667, 60.04666666666667, 61.15833333333333, 60.873333333333335, 61.07833333333333, 60.693333333333335, 61.43333333333333, 61.14333333333333, 61.725, 60.92, 61.08833333333333, 61.17666666666667, 61.50666666666667, 61.541666666666664, 61.48166666666667, 61.71, 61.78, 61.80166666666667, 61.97666666666667, 62.21, 62.123333333333335, 61.95166666666667, 62.04666666666667, 62.125, 62.115, 62.12, 62.483333333333334, 62.30833333333333, 62.77333333333333, 62.513333333333335, 62.25666666666667, 62.475, 62.31, 61.415, 62.556666666666665, 62.25, 62.12, 62.638333333333335, 62.09, 62.218333333333334, 62.165, 62.0, 62.145, 62.905, 62.736666666666665, 62.905, 62.465, 62.70166666666667, 62.891666666666666, 62.695, 62.63333333333333, 62.193333333333335, 62.288333333333334, 62.35166666666667, 61.95666666666666, 62.095, 62.266666666666666, 62.41833333333334, 61.935, 62.35, 62.44, 62.82333333333333, 62.88333333333333, 62.575, 62.88, 62.47]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 237, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54992 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 354, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53519 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 237, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 56247 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 504, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53517 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52198 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54431 is out of bounds for axis 0 with size 50000
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 18, in <module>
    from sklearn.cluster import KMeans
ModuleNotFoundError: No module named 'sklearn'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.123, Test loss: 1.936, Test accuracy: 25.93 

Round   0, Global train loss: 1.123, Global test loss: 2.317, Global test accuracy: 15.36 

Round   1, Train loss: 0.974, Test loss: 2.040, Test accuracy: 36.19 

Round   1, Global train loss: 0.974, Global test loss: 3.055, Global test accuracy: 15.00 

Round   2, Train loss: 0.935, Test loss: 1.200, Test accuracy: 52.70 

Round   2, Global train loss: 0.935, Global test loss: 2.301, Global test accuracy: 22.65 

Round   3, Train loss: 0.792, Test loss: 1.165, Test accuracy: 50.83 

Round   3, Global train loss: 0.792, Global test loss: 2.288, Global test accuracy: 13.62 

Round   4, Train loss: 0.781, Test loss: 0.911, Test accuracy: 62.21 

Round   4, Global train loss: 0.781, Global test loss: 2.096, Global test accuracy: 23.06 

Round   5, Train loss: 0.686, Test loss: 0.845, Test accuracy: 64.23 

Round   5, Global train loss: 0.686, Global test loss: 2.009, Global test accuracy: 28.93 

Round   6, Train loss: 0.676, Test loss: 0.846, Test accuracy: 63.19 

Round   6, Global train loss: 0.676, Global test loss: 2.154, Global test accuracy: 16.83 

Round   7, Train loss: 0.656, Test loss: 0.831, Test accuracy: 64.93 

Round   7, Global train loss: 0.656, Global test loss: 2.277, Global test accuracy: 16.40 

Round   8, Train loss: 0.704, Test loss: 0.812, Test accuracy: 65.33 

Round   8, Global train loss: 0.704, Global test loss: 2.318, Global test accuracy: 16.11 

Round   9, Train loss: 0.664, Test loss: 0.815, Test accuracy: 64.89 

Round   9, Global train loss: 0.664, Global test loss: 2.257, Global test accuracy: 16.48 

Round  10, Train loss: 0.598, Test loss: 0.780, Test accuracy: 67.66 

Round  10, Global train loss: 0.598, Global test loss: 2.153, Global test accuracy: 24.66 

Round  11, Train loss: 0.650, Test loss: 0.792, Test accuracy: 68.17 

Round  11, Global train loss: 0.650, Global test loss: 2.137, Global test accuracy: 24.69 

Traceback (most recent call last):
  File "main_fedrep.py", line 237, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 682, in train
    batch_loss.append(loss.item())
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.133, Test loss: 1.998, Test accuracy: 25.38 

Round   0, Global train loss: 1.133, Global test loss: 2.366, Global test accuracy: 14.95 

Round   1, Train loss: 0.944, Test loss: 1.644, Test accuracy: 39.01 

Round   1, Global train loss: 0.944, Global test loss: 2.262, Global test accuracy: 21.18 

Round   2, Train loss: 0.838, Test loss: 1.521, Test accuracy: 45.09 

Round   2, Global train loss: 0.838, Global test loss: 2.188, Global test accuracy: 26.27 

Round   3, Train loss: 0.877, Test loss: 1.368, Test accuracy: 52.99 

Round   3, Global train loss: 0.877, Global test loss: 2.169, Global test accuracy: 34.48 

Round   4, Train loss: 0.712, Test loss: 1.401, Test accuracy: 53.37 

Round   4, Global train loss: 0.712, Global test loss: 2.386, Global test accuracy: 28.81 

Round   5, Train loss: 0.747, Test loss: 0.914, Test accuracy: 64.04 

Round   5, Global train loss: 0.747, Global test loss: 1.765, Global test accuracy: 40.13 

Round   6, Train loss: 0.723, Test loss: 0.797, Test accuracy: 66.79 

Round   6, Global train loss: 0.723, Global test loss: 1.728, Global test accuracy: 36.80 

Round   7, Train loss: 0.743, Test loss: 0.752, Test accuracy: 68.54 

Round   7, Global train loss: 0.743, Global test loss: 1.631, Global test accuracy: 42.61 

Round   8, Train loss: 0.689, Test loss: 0.757, Test accuracy: 70.04 

Round   8, Global train loss: 0.689, Global test loss: 1.900, Global test accuracy: 36.51 

Round   9, Train loss: 0.612, Test loss: 0.724, Test accuracy: 70.75 

Round   9, Global train loss: 0.612, Global test loss: 1.796, Global test accuracy: 38.43 

Round  10, Train loss: 0.632, Test loss: 0.696, Test accuracy: 72.11 

Round  10, Global train loss: 0.632, Global test loss: 1.647, Global test accuracy: 43.44 

Round  11, Train loss: 0.664, Test loss: 0.685, Test accuracy: 73.04 

Round  11, Global train loss: 0.664, Global test loss: 1.519, Global test accuracy: 48.68 

Round  12, Train loss: 0.658, Test loss: 0.711, Test accuracy: 72.60 

Round  12, Global train loss: 0.658, Global test loss: 1.795, Global test accuracy: 37.94 

Round  13, Train loss: 0.624, Test loss: 0.662, Test accuracy: 73.97 

Round  13, Global train loss: 0.624, Global test loss: 1.501, Global test accuracy: 49.11 

Round  14, Train loss: 0.595, Test loss: 0.592, Test accuracy: 75.38 

Round  14, Global train loss: 0.595, Global test loss: 1.648, Global test accuracy: 43.77 

Round  15, Train loss: 0.605, Test loss: 0.587, Test accuracy: 75.69 

Round  15, Global train loss: 0.605, Global test loss: 1.707, Global test accuracy: 45.22 

Round  16, Train loss: 0.590, Test loss: 0.582, Test accuracy: 75.96 

Round  16, Global train loss: 0.590, Global test loss: 1.482, Global test accuracy: 47.75 

Round  17, Train loss: 0.595, Test loss: 0.564, Test accuracy: 77.18 

Round  17, Global train loss: 0.595, Global test loss: 1.441, Global test accuracy: 49.16 

Round  18, Train loss: 0.581, Test loss: 0.559, Test accuracy: 77.46 

Round  18, Global train loss: 0.581, Global test loss: 1.785, Global test accuracy: 45.18 

Round  19, Train loss: 0.524, Test loss: 0.564, Test accuracy: 77.09 

Round  19, Global train loss: 0.524, Global test loss: 1.523, Global test accuracy: 47.46 

Round  20, Train loss: 0.498, Test loss: 0.554, Test accuracy: 77.59 

Round  20, Global train loss: 0.498, Global test loss: 1.601, Global test accuracy: 46.82 

Round  21, Train loss: 0.496, Test loss: 0.541, Test accuracy: 78.02 

Round  21, Global train loss: 0.496, Global test loss: 1.795, Global test accuracy: 47.18 

Round  22, Train loss: 0.545, Test loss: 0.536, Test accuracy: 78.37 

Round  22, Global train loss: 0.545, Global test loss: 1.566, Global test accuracy: 46.27 

Round  23, Train loss: 0.605, Test loss: 0.529, Test accuracy: 78.59 

Round  23, Global train loss: 0.605, Global test loss: 1.527, Global test accuracy: 46.72 

Round  24, Train loss: 0.462, Test loss: 0.529, Test accuracy: 78.99 

Round  24, Global train loss: 0.462, Global test loss: 1.499, Global test accuracy: 49.62 

Round  25, Train loss: 0.486, Test loss: 0.526, Test accuracy: 79.42 

Round  25, Global train loss: 0.486, Global test loss: 1.397, Global test accuracy: 51.36 

Round  26, Train loss: 0.510, Test loss: 0.530, Test accuracy: 78.99 

Round  26, Global train loss: 0.510, Global test loss: 1.430, Global test accuracy: 50.48 

Round  27, Train loss: 0.555, Test loss: 0.531, Test accuracy: 78.91 

Round  27, Global train loss: 0.555, Global test loss: 1.407, Global test accuracy: 50.47 

Round  28, Train loss: 0.552, Test loss: 0.525, Test accuracy: 79.16 

Round  28, Global train loss: 0.552, Global test loss: 1.419, Global test accuracy: 49.99 

Round  29, Train loss: 0.418, Test loss: 0.554, Test accuracy: 78.21 

Round  29, Global train loss: 0.418, Global test loss: 1.506, Global test accuracy: 49.66 

Round  30, Train loss: 0.414, Test loss: 0.535, Test accuracy: 79.00 

Round  30, Global train loss: 0.414, Global test loss: 1.310, Global test accuracy: 56.11 

Round  31, Train loss: 0.417, Test loss: 0.533, Test accuracy: 79.05 

Round  31, Global train loss: 0.417, Global test loss: 1.413, Global test accuracy: 53.46 

Round  32, Train loss: 0.416, Test loss: 0.511, Test accuracy: 79.95 

Round  32, Global train loss: 0.416, Global test loss: 1.510, Global test accuracy: 53.10 

Round  33, Train loss: 0.380, Test loss: 0.509, Test accuracy: 80.01 

Round  33, Global train loss: 0.380, Global test loss: 1.308, Global test accuracy: 57.76 

Round  34, Train loss: 0.504, Test loss: 0.497, Test accuracy: 80.65 

Round  34, Global train loss: 0.504, Global test loss: 1.333, Global test accuracy: 55.98 

Round  35, Train loss: 0.424, Test loss: 0.514, Test accuracy: 80.06 

Round  35, Global train loss: 0.424, Global test loss: 1.235, Global test accuracy: 58.39 

Round  36, Train loss: 0.468, Test loss: 0.513, Test accuracy: 80.40 

Round  36, Global train loss: 0.468, Global test loss: 1.286, Global test accuracy: 56.71 

Round  37, Train loss: 0.412, Test loss: 0.511, Test accuracy: 80.66 

Round  37, Global train loss: 0.412, Global test loss: 1.416, Global test accuracy: 53.30 

Round  38, Train loss: 0.477, Test loss: 0.524, Test accuracy: 80.08 

Round  38, Global train loss: 0.477, Global test loss: 1.231, Global test accuracy: 59.11 

Round  39, Train loss: 0.403, Test loss: 0.521, Test accuracy: 80.28 

Round  39, Global train loss: 0.403, Global test loss: 1.338, Global test accuracy: 56.24 

Round  40, Train loss: 0.353, Test loss: 0.503, Test accuracy: 81.03 

Round  40, Global train loss: 0.353, Global test loss: 1.301, Global test accuracy: 58.65 

Round  41, Train loss: 0.355, Test loss: 0.503, Test accuracy: 81.27 

Round  41, Global train loss: 0.355, Global test loss: 1.528, Global test accuracy: 50.89 

Round  42, Train loss: 0.348, Test loss: 0.507, Test accuracy: 81.23 

Round  42, Global train loss: 0.348, Global test loss: 1.332, Global test accuracy: 57.29 

Round  43, Train loss: 0.316, Test loss: 0.504, Test accuracy: 81.34 

Round  43, Global train loss: 0.316, Global test loss: 1.255, Global test accuracy: 58.79 

Round  44, Train loss: 0.401, Test loss: 0.496, Test accuracy: 81.59 

Round  44, Global train loss: 0.401, Global test loss: 1.119, Global test accuracy: 61.76 

Round  45, Train loss: 0.417, Test loss: 0.489, Test accuracy: 81.84 

Round  45, Global train loss: 0.417, Global test loss: 1.225, Global test accuracy: 58.84 

Round  46, Train loss: 0.311, Test loss: 0.484, Test accuracy: 81.98 

Round  46, Global train loss: 0.311, Global test loss: 1.295, Global test accuracy: 59.76 

Round  47, Train loss: 0.454, Test loss: 0.479, Test accuracy: 82.12 

Round  47, Global train loss: 0.454, Global test loss: 1.419, Global test accuracy: 53.01 

Round  48, Train loss: 0.370, Test loss: 0.475, Test accuracy: 82.53 

Round  48, Global train loss: 0.370, Global test loss: 1.310, Global test accuracy: 57.54 

Round  49, Train loss: 0.349, Test loss: 0.480, Test accuracy: 82.42 

Round  49, Global train loss: 0.349, Global test loss: 1.758, Global test accuracy: 50.54 

Round  50, Train loss: 0.344, Test loss: 0.507, Test accuracy: 81.57 

Round  50, Global train loss: 0.344, Global test loss: 1.274, Global test accuracy: 58.21 

Round  51, Train loss: 0.345, Test loss: 0.497, Test accuracy: 82.04 

Round  51, Global train loss: 0.345, Global test loss: 1.446, Global test accuracy: 54.64 

Round  52, Train loss: 0.349, Test loss: 0.480, Test accuracy: 82.58 

Round  52, Global train loss: 0.349, Global test loss: 1.234, Global test accuracy: 59.33 

Round  53, Train loss: 0.335, Test loss: 0.487, Test accuracy: 82.46 

Round  53, Global train loss: 0.335, Global test loss: 1.094, Global test accuracy: 63.17 

Round  54, Train loss: 0.318, Test loss: 0.507, Test accuracy: 81.79 

Round  54, Global train loss: 0.318, Global test loss: 1.190, Global test accuracy: 61.53 

Round  55, Train loss: 0.277, Test loss: 0.507, Test accuracy: 81.91 

Round  55, Global train loss: 0.277, Global test loss: 1.286, Global test accuracy: 58.32 

Round  56, Train loss: 0.354, Test loss: 0.512, Test accuracy: 81.64 

Round  56, Global train loss: 0.354, Global test loss: 1.324, Global test accuracy: 57.23 

Round  57, Train loss: 0.329, Test loss: 0.508, Test accuracy: 81.94 

Round  57, Global train loss: 0.329, Global test loss: 1.240, Global test accuracy: 59.46 

Round  58, Train loss: 0.324, Test loss: 0.489, Test accuracy: 82.56 

Round  58, Global train loss: 0.324, Global test loss: 1.185, Global test accuracy: 60.64 

Round  59, Train loss: 0.307, Test loss: 0.486, Test accuracy: 82.70 

Round  59, Global train loss: 0.307, Global test loss: 1.215, Global test accuracy: 61.22 

Round  60, Train loss: 0.285, Test loss: 0.503, Test accuracy: 82.63 

Round  60, Global train loss: 0.285, Global test loss: 1.190, Global test accuracy: 62.65 

Round  61, Train loss: 0.322, Test loss: 0.496, Test accuracy: 82.67 

Round  61, Global train loss: 0.322, Global test loss: 1.292, Global test accuracy: 58.17 

Round  62, Train loss: 0.269, Test loss: 0.484, Test accuracy: 83.00 

Round  62, Global train loss: 0.269, Global test loss: 1.113, Global test accuracy: 63.61 

Round  63, Train loss: 0.309, Test loss: 0.484, Test accuracy: 83.16 

Round  63, Global train loss: 0.309, Global test loss: 1.216, Global test accuracy: 60.60 

Round  64, Train loss: 0.362, Test loss: 0.490, Test accuracy: 82.99 

Round  64, Global train loss: 0.362, Global test loss: 1.411, Global test accuracy: 55.94 

Round  65, Train loss: 0.228, Test loss: 0.470, Test accuracy: 83.60 

Round  65, Global train loss: 0.228, Global test loss: 1.303, Global test accuracy: 60.98 

Round  66, Train loss: 0.277, Test loss: 0.479, Test accuracy: 83.58 

Round  66, Global train loss: 0.277, Global test loss: 1.161, Global test accuracy: 63.76 

Round  67, Train loss: 0.293, Test loss: 0.492, Test accuracy: 83.24 

Round  67, Global train loss: 0.293, Global test loss: 1.445, Global test accuracy: 56.58 

Round  68, Train loss: 0.302, Test loss: 0.479, Test accuracy: 83.52 

Round  68, Global train loss: 0.302, Global test loss: 1.160, Global test accuracy: 61.89 

Round  69, Train loss: 0.278, Test loss: 0.483, Test accuracy: 83.59 

Round  69, Global train loss: 0.278, Global test loss: 1.339, Global test accuracy: 58.36 

Round  70, Train loss: 0.260, Test loss: 0.507, Test accuracy: 83.06 

Round  70, Global train loss: 0.260, Global test loss: 1.447, Global test accuracy: 57.01 

Round  71, Train loss: 0.262, Test loss: 0.509, Test accuracy: 83.18 

Round  71, Global train loss: 0.262, Global test loss: 1.297, Global test accuracy: 60.57 

Round  72, Train loss: 0.215, Test loss: 0.493, Test accuracy: 83.72 

Round  72, Global train loss: 0.215, Global test loss: 1.130, Global test accuracy: 64.23 

Round  73, Train loss: 0.287, Test loss: 0.498, Test accuracy: 83.28 

Round  73, Global train loss: 0.287, Global test loss: 1.257, Global test accuracy: 60.56 

Round  74, Train loss: 0.246, Test loss: 0.481, Test accuracy: 83.84 

Round  74, Global train loss: 0.246, Global test loss: 1.336, Global test accuracy: 59.56 

Round  75, Train loss: 0.257, Test loss: 0.495, Test accuracy: 83.52 

Round  75, Global train loss: 0.257, Global test loss: 1.202, Global test accuracy: 62.96 

Round  76, Train loss: 0.251, Test loss: 0.495, Test accuracy: 83.48 

Round  76, Global train loss: 0.251, Global test loss: 1.482, Global test accuracy: 58.12 

Round  77, Train loss: 0.255, Test loss: 0.497, Test accuracy: 83.70 

Round  77, Global train loss: 0.255, Global test loss: 1.253, Global test accuracy: 62.41 

Round  78, Train loss: 0.212, Test loss: 0.497, Test accuracy: 83.74 

Round  78, Global train loss: 0.212, Global test loss: 1.296, Global test accuracy: 62.68 

Round  79, Train loss: 0.259, Test loss: 0.479, Test accuracy: 83.95 

Round  79, Global train loss: 0.259, Global test loss: 1.218, Global test accuracy: 62.53 

Round  80, Train loss: 0.217, Test loss: 0.512, Test accuracy: 82.99 

Round  80, Global train loss: 0.217, Global test loss: 1.296, Global test accuracy: 61.93 

Round  81, Train loss: 0.289, Test loss: 0.506, Test accuracy: 83.52 

Round  81, Global train loss: 0.289, Global test loss: 1.155, Global test accuracy: 63.66 

Round  82, Train loss: 0.196, Test loss: 0.503, Test accuracy: 83.67 

Round  82, Global train loss: 0.196, Global test loss: 1.129, Global test accuracy: 64.52 

Round  83, Train loss: 0.218, Test loss: 0.491, Test accuracy: 84.12 

Round  83, Global train loss: 0.218, Global test loss: 1.404, Global test accuracy: 58.66 

Round  84, Train loss: 0.277, Test loss: 0.496, Test accuracy: 83.96 

Round  84, Global train loss: 0.277, Global test loss: 1.230, Global test accuracy: 61.38 

Round  85, Train loss: 0.225, Test loss: 0.490, Test accuracy: 84.05 

Round  85, Global train loss: 0.225, Global test loss: 1.460, Global test accuracy: 58.39 

Round  86, Train loss: 0.222, Test loss: 0.505, Test accuracy: 83.91 

Round  86, Global train loss: 0.222, Global test loss: 1.296, Global test accuracy: 61.10 

Round  87, Train loss: 0.203, Test loss: 0.502, Test accuracy: 84.16 

Round  87, Global train loss: 0.203, Global test loss: 1.478, Global test accuracy: 59.33 

Round  88, Train loss: 0.203, Test loss: 0.498, Test accuracy: 84.25 

Round  88, Global train loss: 0.203, Global test loss: 1.271, Global test accuracy: 64.10 

Round  89, Train loss: 0.211, Test loss: 0.502, Test accuracy: 83.92 

Round  89, Global train loss: 0.211, Global test loss: 1.304, Global test accuracy: 63.16 

Round  90, Train loss: 0.164, Test loss: 0.512, Test accuracy: 83.79 

Round  90, Global train loss: 0.164, Global test loss: 1.332, Global test accuracy: 63.36 

Round  91, Train loss: 0.274, Test loss: 0.519, Test accuracy: 83.77 

Round  91, Global train loss: 0.274, Global test loss: 1.270, Global test accuracy: 62.31 

Round  92, Train loss: 0.228, Test loss: 0.510, Test accuracy: 84.04 

Round  92, Global train loss: 0.228, Global test loss: 1.458, Global test accuracy: 59.84 

Round  93, Train loss: 0.186, Test loss: 0.508, Test accuracy: 84.19 

Round  93, Global train loss: 0.186, Global test loss: 1.248, Global test accuracy: 62.07 

Round  94, Train loss: 0.158, Test loss: 0.493, Test accuracy: 84.66 

Round  94, Global train loss: 0.158, Global test loss: 1.509, Global test accuracy: 59.07 

Round  95, Train loss: 0.261, Test loss: 0.505, Test accuracy: 84.28 

Round  95, Global train loss: 0.261, Global test loss: 1.437, Global test accuracy: 58.93 

Round  96, Train loss: 0.197, Test loss: 0.506, Test accuracy: 84.41 

Round  96, Global train loss: 0.197, Global test loss: 1.584, Global test accuracy: 58.16 

Round  97, Train loss: 0.220, Test loss: 0.506, Test accuracy: 84.42 

Round  97, Global train loss: 0.220, Global test loss: 1.731, Global test accuracy: 56.36 

Round  98, Train loss: 0.199, Test loss: 0.500, Test accuracy: 84.41 

Round  98, Global train loss: 0.199, Global test loss: 1.276, Global test accuracy: 62.11 

Round  99, Train loss: 0.210, Test loss: 0.509, Test accuracy: 84.24 

Round  99, Global train loss: 0.210, Global test loss: 1.190, Global test accuracy: 63.18 

Final Round, Train loss: 0.170, Test loss: 0.546, Test accuracy: 84.31 

Final Round, Global train loss: 0.170, Global test loss: 1.190, Global test accuracy: 63.18 

Average accuracy final 10 rounds: 84.22 

Average global accuracy final 10 rounds: 60.53777777777778 

1642.2921442985535
[1.3841865062713623, 2.512662887573242, 3.633185386657715, 4.759441375732422, 5.891201972961426, 7.02735161781311, 8.166226148605347, 9.302679538726807, 10.413006067276001, 11.546401023864746, 12.691011905670166, 13.834249019622803, 14.975878953933716, 16.12107229232788, 17.265312671661377, 18.430400848388672, 19.58908987045288, 20.75281834602356, 21.91059374809265, 23.075018644332886, 24.237796783447266, 25.391573429107666, 26.555130004882812, 27.719260454177856, 28.887518167495728, 30.04073452949524, 31.199530124664307, 32.35765051841736, 33.515514850616455, 34.67626953125, 35.836872577667236, 36.99534749984741, 38.153804302215576, 39.31138253211975, 40.464823961257935, 41.628228187561035, 42.78530430793762, 43.94817137718201, 45.112555503845215, 46.27526021003723, 47.432326316833496, 48.573758363723755, 49.71905708312988, 50.85980272293091, 52.01257276535034, 53.15682935714722, 54.30032014846802, 55.4476592540741, 56.607786893844604, 57.77309989929199, 58.93305683135986, 60.0810124874115, 61.244351387023926, 62.41871476173401, 63.601147413253784, 64.79079127311707, 65.98303508758545, 67.17586660385132, 68.35955786705017, 69.54143047332764, 70.7297830581665, 71.91004800796509, 73.10261702537537, 74.29830265045166, 75.48677706718445, 76.66923022270203, 77.85310339927673, 79.03153944015503, 80.22517776489258, 81.40743851661682, 82.57710671424866, 83.75099992752075, 84.91439723968506, 86.08160924911499, 87.24761390686035, 88.41806101799011, 89.58845233917236, 90.7593183517456, 91.93234658241272, 93.11410617828369, 94.28093957901001, 95.44518828392029, 96.61566591262817, 97.79125380516052, 98.95566821098328, 100.12130880355835, 101.29372477531433, 102.45951819419861, 103.6184949874878, 104.8052225112915, 105.99253463745117, 107.1721625328064, 108.36187195777893, 109.55980038642883, 110.75725078582764, 111.94773364067078, 113.13679099082947, 114.33547186851501, 115.52021169662476, 116.7057409286499, 119.03360986709595]
[25.383333333333333, 39.01111111111111, 45.08888888888889, 52.98888888888889, 53.36666666666667, 64.04444444444445, 66.79444444444445, 68.54444444444445, 70.03888888888889, 70.75, 72.11111111111111, 73.04444444444445, 72.6, 73.97222222222223, 75.38333333333334, 75.68888888888888, 75.95555555555555, 77.17777777777778, 77.45555555555555, 77.08888888888889, 77.58888888888889, 78.02222222222223, 78.36666666666666, 78.58888888888889, 78.99444444444444, 79.41666666666667, 78.9888888888889, 78.90555555555555, 79.16111111111111, 78.21111111111111, 79.0, 79.05, 79.95, 80.00555555555556, 80.65, 80.05555555555556, 80.4, 80.66111111111111, 80.07777777777778, 80.28333333333333, 81.02777777777777, 81.26666666666667, 81.23333333333333, 81.34444444444445, 81.58888888888889, 81.84444444444445, 81.98333333333333, 82.11666666666666, 82.53333333333333, 82.41666666666667, 81.57222222222222, 82.03888888888889, 82.57777777777778, 82.45555555555555, 81.78888888888889, 81.91111111111111, 81.63888888888889, 81.94444444444444, 82.56111111111112, 82.7, 82.62777777777778, 82.67222222222222, 83.0, 83.16111111111111, 82.99444444444444, 83.6, 83.58333333333333, 83.24444444444444, 83.51666666666667, 83.58888888888889, 83.06111111111112, 83.18333333333334, 83.72222222222223, 83.28333333333333, 83.83888888888889, 83.52222222222223, 83.48333333333333, 83.7, 83.7388888888889, 83.95, 82.9888888888889, 83.52222222222223, 83.67222222222222, 84.11666666666666, 83.96111111111111, 84.05, 83.91111111111111, 84.15555555555555, 84.25, 83.91666666666667, 83.79444444444445, 83.76666666666667, 84.03888888888889, 84.18888888888888, 84.65555555555555, 84.27777777777777, 84.41111111111111, 84.41666666666667, 84.41111111111111, 84.2388888888889, 84.31111111111112]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.285, Test loss: 2.230, Test accuracy: 17.89 

Round   1, Train loss: 2.161, Test loss: 2.023, Test accuracy: 27.14 

Round   2, Train loss: 2.032, Test loss: 1.917, Test accuracy: 30.28 

Round   3, Train loss: 1.920, Test loss: 1.853, Test accuracy: 32.72 

Round   4, Train loss: 1.832, Test loss: 1.764, Test accuracy: 35.89 

Round   5, Train loss: 1.764, Test loss: 1.706, Test accuracy: 37.06 

Round   6, Train loss: 1.709, Test loss: 1.670, Test accuracy: 38.80 

Round   7, Train loss: 1.667, Test loss: 1.634, Test accuracy: 40.34 

Round   8, Train loss: 1.635, Test loss: 1.592, Test accuracy: 41.59 

Round   9, Train loss: 1.598, Test loss: 1.560, Test accuracy: 42.71 

Round  10, Train loss: 1.536, Test loss: 1.537, Test accuracy: 43.71 

Round  11, Train loss: 1.516, Test loss: 1.513, Test accuracy: 45.07 

Round  12, Train loss: 1.500, Test loss: 1.488, Test accuracy: 45.69 

Round  13, Train loss: 1.459, Test loss: 1.480, Test accuracy: 46.07 

Round  14, Train loss: 1.451, Test loss: 1.461, Test accuracy: 46.81 

Round  15, Train loss: 1.443, Test loss: 1.448, Test accuracy: 47.56 

Round  16, Train loss: 1.409, Test loss: 1.451, Test accuracy: 47.66 

Round  17, Train loss: 1.422, Test loss: 1.433, Test accuracy: 48.30 

Round  18, Train loss: 1.389, Test loss: 1.397, Test accuracy: 49.60 

Round  19, Train loss: 1.375, Test loss: 1.386, Test accuracy: 49.96 

Round  20, Train loss: 1.357, Test loss: 1.371, Test accuracy: 50.84 

Round  21, Train loss: 1.323, Test loss: 1.368, Test accuracy: 50.91 

Round  22, Train loss: 1.306, Test loss: 1.350, Test accuracy: 51.45 

Round  23, Train loss: 1.282, Test loss: 1.331, Test accuracy: 52.21 

Round  24, Train loss: 1.251, Test loss: 1.333, Test accuracy: 52.31 

Round  25, Train loss: 1.233, Test loss: 1.314, Test accuracy: 53.06 

Round  26, Train loss: 1.253, Test loss: 1.313, Test accuracy: 53.33 

Round  27, Train loss: 1.264, Test loss: 1.285, Test accuracy: 54.65 

Round  28, Train loss: 1.215, Test loss: 1.304, Test accuracy: 53.86 

Round  29, Train loss: 1.186, Test loss: 1.285, Test accuracy: 54.64 

Round  30, Train loss: 1.130, Test loss: 1.288, Test accuracy: 54.28 

Round  31, Train loss: 1.151, Test loss: 1.258, Test accuracy: 55.35 

Round  32, Train loss: 1.151, Test loss: 1.241, Test accuracy: 56.19 

Round  33, Train loss: 1.112, Test loss: 1.230, Test accuracy: 56.47 

Round  34, Train loss: 1.121, Test loss: 1.220, Test accuracy: 57.09 

Round  35, Train loss: 1.085, Test loss: 1.226, Test accuracy: 57.39 

Round  36, Train loss: 1.115, Test loss: 1.219, Test accuracy: 57.60 

Round  37, Train loss: 1.061, Test loss: 1.199, Test accuracy: 58.20 

Round  38, Train loss: 1.042, Test loss: 1.191, Test accuracy: 58.45 

Round  39, Train loss: 1.025, Test loss: 1.197, Test accuracy: 58.06 

Round  40, Train loss: 1.045, Test loss: 1.201, Test accuracy: 58.15 

Round  41, Train loss: 1.029, Test loss: 1.186, Test accuracy: 58.65 

Round  42, Train loss: 1.050, Test loss: 1.185, Test accuracy: 59.23 

Round  43, Train loss: 0.977, Test loss: 1.183, Test accuracy: 59.11 

Round  44, Train loss: 1.014, Test loss: 1.192, Test accuracy: 58.87 

Round  45, Train loss: 0.981, Test loss: 1.183, Test accuracy: 59.79 

Round  46, Train loss: 0.983, Test loss: 1.185, Test accuracy: 59.67 

Round  47, Train loss: 0.965, Test loss: 1.172, Test accuracy: 59.66 

Round  48, Train loss: 0.938, Test loss: 1.159, Test accuracy: 60.00 

Round  49, Train loss: 0.975, Test loss: 1.153, Test accuracy: 60.29 

Round  50, Train loss: 0.931, Test loss: 1.142, Test accuracy: 60.66 

Round  51, Train loss: 0.916, Test loss: 1.152, Test accuracy: 60.48 

Round  52, Train loss: 0.911, Test loss: 1.157, Test accuracy: 60.67 

Round  53, Train loss: 0.890, Test loss: 1.162, Test accuracy: 60.65 

Round  54, Train loss: 0.877, Test loss: 1.170, Test accuracy: 60.67 

Round  55, Train loss: 0.880, Test loss: 1.163, Test accuracy: 60.70 

Round  56, Train loss: 0.858, Test loss: 1.153, Test accuracy: 60.83 

Round  57, Train loss: 0.811, Test loss: 1.185, Test accuracy: 60.08 

Round  58, Train loss: 0.863, Test loss: 1.186, Test accuracy: 60.29 

Round  59, Train loss: 0.857, Test loss: 1.195, Test accuracy: 60.42 

Round  60, Train loss: 0.848, Test loss: 1.172, Test accuracy: 60.70 

Round  61, Train loss: 0.799, Test loss: 1.174, Test accuracy: 60.62 

Round  62, Train loss: 0.789, Test loss: 1.163, Test accuracy: 61.01 

Round  63, Train loss: 0.834, Test loss: 1.172, Test accuracy: 61.09 

Round  64, Train loss: 0.760, Test loss: 1.177, Test accuracy: 60.98 

Round  65, Train loss: 0.766, Test loss: 1.181, Test accuracy: 61.22 

Round  66, Train loss: 0.773, Test loss: 1.183, Test accuracy: 60.98 

Round  67, Train loss: 0.735, Test loss: 1.185, Test accuracy: 61.12 

Round  68, Train loss: 0.729, Test loss: 1.204, Test accuracy: 61.21 

Round  69, Train loss: 0.772, Test loss: 1.162, Test accuracy: 61.69 

Round  70, Train loss: 0.730, Test loss: 1.175, Test accuracy: 61.90 

Round  71, Train loss: 0.741, Test loss: 1.195, Test accuracy: 61.70 

Round  72, Train loss: 0.746, Test loss: 1.181, Test accuracy: 61.72 

Round  73, Train loss: 0.743, Test loss: 1.189, Test accuracy: 61.62 

Round  74, Train loss: 0.726, Test loss: 1.184, Test accuracy: 61.70 

Round  75, Train loss: 0.763, Test loss: 1.186, Test accuracy: 61.44 

Round  76, Train loss: 0.763, Test loss: 1.195, Test accuracy: 61.70 

Round  77, Train loss: 0.712, Test loss: 1.190, Test accuracy: 62.20 

Round  78, Train loss: 0.641, Test loss: 1.193, Test accuracy: 61.99 

Round  79, Train loss: 0.730, Test loss: 1.186, Test accuracy: 62.00 

Round  80, Train loss: 0.656, Test loss: 1.178, Test accuracy: 62.26 

Round  81, Train loss: 0.667, Test loss: 1.204, Test accuracy: 61.92 

Round  82, Train loss: 0.657, Test loss: 1.195, Test accuracy: 62.27 

Round  83, Train loss: 0.697, Test loss: 1.214, Test accuracy: 61.95 

Round  84, Train loss: 0.663, Test loss: 1.217, Test accuracy: 62.30 

Round  85, Train loss: 0.659, Test loss: 1.223, Test accuracy: 62.10 

Round  86, Train loss: 0.666, Test loss: 1.210, Test accuracy: 62.60 

Round  87, Train loss: 0.608, Test loss: 1.208, Test accuracy: 62.55 

Round  88, Train loss: 0.618, Test loss: 1.240, Test accuracy: 62.32 

Round  89, Train loss: 0.609, Test loss: 1.261, Test accuracy: 62.07 

Round  90, Train loss: 0.645, Test loss: 1.243, Test accuracy: 62.63 

Round  91, Train loss: 0.671, Test loss: 1.245, Test accuracy: 62.05 

Round  92, Train loss: 0.618, Test loss: 1.280, Test accuracy: 61.98 

Round  93, Train loss: 0.606, Test loss: 1.234, Test accuracy: 62.13 

Round  94, Train loss: 0.636, Test loss: 1.258, Test accuracy: 62.47 

Round  95, Train loss: 0.592, Test loss: 1.241, Test accuracy: 62.71 

Round  96, Train loss: 0.654, Test loss: 1.263, Test accuracy: 62.25 

Round  97, Train loss: 0.554, Test loss: 1.284, Test accuracy: 61.85 

Round  98, Train loss: 0.566, Test loss: 1.289, Test accuracy: 62.14 

Round  99, Train loss: 0.582, Test loss: 1.256, Test accuracy: 62.13 

Final Round, Train loss: 0.513, Test loss: 1.265, Test accuracy: 62.40 

Average accuracy final 10 rounds: 62.235 

2234.060107946396
[1.3277359008789062, 2.417482614517212, 3.5147950649261475, 4.60839581489563, 5.709510564804077, 6.811692237854004, 7.90868878364563, 9.015716791152954, 10.11358380317688, 11.221843242645264, 12.336708068847656, 13.432828187942505, 14.537271976470947, 15.639048099517822, 16.73820686340332, 17.84095811843872, 18.944258213043213, 20.03695583343506, 21.139198541641235, 22.227400064468384, 23.320839166641235, 24.417381286621094, 25.51654863357544, 26.60979151725769, 27.71080780029297, 28.724185943603516, 29.72004270553589, 30.718636512756348, 31.71294093132019, 32.707990884780884, 33.70763182640076, 34.70954895019531, 35.711464166641235, 36.71444034576416, 37.71541905403137, 38.71226453781128, 39.710538387298584, 40.712217569351196, 41.70223259925842, 42.70621609687805, 43.7063090801239, 44.70139956474304, 45.701677083969116, 46.69629406929016, 47.70205760002136, 48.699594020843506, 49.70474720001221, 50.70689296722412, 51.70780801773071, 52.705944538116455, 53.70889115333557, 54.70366668701172, 55.70463752746582, 56.7056941986084, 57.70055890083313, 58.70222592353821, 59.70799493789673, 60.70552372932434, 61.69984316825867, 62.70099329948425, 63.69531989097595, 64.69213390350342, 65.68343949317932, 66.6837112903595, 67.68493008613586, 68.67182469367981, 69.66716456413269, 70.65335655212402, 71.65015840530396, 72.64655661582947, 73.64126992225647, 74.63186764717102, 75.62261056900024, 76.61100387573242, 77.60679745674133, 78.59516835212708, 79.58564400672913, 80.5958092212677, 81.59627437591553, 82.59801745414734, 83.59803676605225, 84.59764337539673, 85.61177968978882, 86.61218190193176, 87.61541199684143, 88.6155481338501, 89.64932298660278, 90.68326783180237, 91.72091674804688, 92.74902153015137, 93.82960200309753, 94.92543315887451, 96.00988674163818, 97.10284399986267, 98.18763256072998, 99.26843428611755, 100.35819935798645, 101.44025540351868, 102.5317907333374, 103.62096095085144, 105.59962487220764]
[17.888333333333332, 27.14, 30.281666666666666, 32.72, 35.891666666666666, 37.056666666666665, 38.795, 40.34, 41.593333333333334, 42.71, 43.70666666666666, 45.06666666666667, 45.69166666666667, 46.06666666666667, 46.81166666666667, 47.55833333333333, 47.65833333333333, 48.305, 49.596666666666664, 49.958333333333336, 50.836666666666666, 50.91166666666667, 51.45166666666667, 52.208333333333336, 52.31333333333333, 53.065, 53.325, 54.65, 53.858333333333334, 54.638333333333335, 54.28333333333333, 55.35333333333333, 56.185, 56.471666666666664, 57.095, 57.39333333333333, 57.60166666666667, 58.205, 58.445, 58.06166666666667, 58.15, 58.651666666666664, 59.22833333333333, 59.10666666666667, 58.865, 59.788333333333334, 59.666666666666664, 59.66, 59.998333333333335, 60.291666666666664, 60.656666666666666, 60.483333333333334, 60.666666666666664, 60.651666666666664, 60.67333333333333, 60.70166666666667, 60.83, 60.07833333333333, 60.29333333333334, 60.425, 60.705, 60.625, 61.00833333333333, 61.085, 60.98, 61.22, 60.97833333333333, 61.11666666666667, 61.208333333333336, 61.685, 61.89666666666667, 61.70333333333333, 61.721666666666664, 61.61666666666667, 61.705, 61.435, 61.7, 62.20333333333333, 61.98833333333334, 62.00333333333333, 62.25666666666667, 61.916666666666664, 62.26833333333333, 61.946666666666665, 62.295, 62.105, 62.605, 62.545, 62.321666666666665, 62.068333333333335, 62.63, 62.053333333333335, 61.985, 62.13, 62.47, 62.708333333333336, 62.251666666666665, 61.85333333333333, 62.14, 62.12833333333333, 62.4]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.204, Test loss: 2.065, Test accuracy: 24.27 

Round   1, Train loss: 2.010, Test loss: 1.923, Test accuracy: 30.27 

Round   2, Train loss: 1.899, Test loss: 1.780, Test accuracy: 35.47 

Round   3, Train loss: 1.781, Test loss: 1.687, Test accuracy: 38.25 

Round   4, Train loss: 1.707, Test loss: 1.633, Test accuracy: 39.81 

Round   5, Train loss: 1.660, Test loss: 1.568, Test accuracy: 42.33 

Round   6, Train loss: 1.603, Test loss: 1.540, Test accuracy: 44.16 

Round   7, Train loss: 1.536, Test loss: 1.494, Test accuracy: 45.82 

Round   8, Train loss: 1.520, Test loss: 1.493, Test accuracy: 45.86 

Round   9, Train loss: 1.471, Test loss: 1.432, Test accuracy: 48.17 

Round  10, Train loss: 1.420, Test loss: 1.414, Test accuracy: 48.47 

Round  11, Train loss: 1.424, Test loss: 1.386, Test accuracy: 50.23 

Round  12, Train loss: 1.371, Test loss: 1.363, Test accuracy: 50.67 

Round  13, Train loss: 1.341, Test loss: 1.353, Test accuracy: 51.51 

Round  14, Train loss: 1.314, Test loss: 1.322, Test accuracy: 52.43 

Round  15, Train loss: 1.341, Test loss: 1.304, Test accuracy: 52.84 

Round  16, Train loss: 1.280, Test loss: 1.282, Test accuracy: 53.80 

Round  17, Train loss: 1.206, Test loss: 1.280, Test accuracy: 54.23 

Round  18, Train loss: 1.213, Test loss: 1.257, Test accuracy: 54.96 

Round  19, Train loss: 1.180, Test loss: 1.246, Test accuracy: 55.73 

Round  20, Train loss: 1.129, Test loss: 1.234, Test accuracy: 56.39 

Round  21, Train loss: 1.118, Test loss: 1.217, Test accuracy: 56.72 

Round  22, Train loss: 1.089, Test loss: 1.223, Test accuracy: 56.91 

Round  23, Train loss: 1.066, Test loss: 1.208, Test accuracy: 57.67 

Round  24, Train loss: 1.064, Test loss: 1.210, Test accuracy: 57.21 

Round  25, Train loss: 1.041, Test loss: 1.196, Test accuracy: 58.34 

Round  26, Train loss: 0.993, Test loss: 1.213, Test accuracy: 57.71 

Round  27, Train loss: 0.988, Test loss: 1.197, Test accuracy: 58.74 

Round  28, Train loss: 0.973, Test loss: 1.205, Test accuracy: 58.85 

Round  29, Train loss: 0.978, Test loss: 1.197, Test accuracy: 58.69 

Round  30, Train loss: 0.951, Test loss: 1.204, Test accuracy: 58.90 

Round  31, Train loss: 0.928, Test loss: 1.199, Test accuracy: 59.21 

Round  32, Train loss: 0.921, Test loss: 1.193, Test accuracy: 59.87 

Round  33, Train loss: 0.857, Test loss: 1.212, Test accuracy: 59.62 

Round  34, Train loss: 0.842, Test loss: 1.192, Test accuracy: 59.89 

Round  35, Train loss: 0.835, Test loss: 1.207, Test accuracy: 59.80 

Round  36, Train loss: 0.831, Test loss: 1.215, Test accuracy: 60.49 

Round  37, Train loss: 0.849, Test loss: 1.207, Test accuracy: 60.09 

Round  38, Train loss: 0.881, Test loss: 1.204, Test accuracy: 60.06 

Round  39, Train loss: 0.818, Test loss: 1.200, Test accuracy: 60.48 

Round  40, Train loss: 0.767, Test loss: 1.208, Test accuracy: 60.64 

Round  41, Train loss: 0.780, Test loss: 1.214, Test accuracy: 60.40 

Round  42, Train loss: 0.773, Test loss: 1.213, Test accuracy: 60.86 

Round  43, Train loss: 0.718, Test loss: 1.255, Test accuracy: 60.03 

Round  44, Train loss: 0.734, Test loss: 1.252, Test accuracy: 60.44 

Round  45, Train loss: 0.702, Test loss: 1.250, Test accuracy: 60.52 

Round  46, Train loss: 0.731, Test loss: 1.220, Test accuracy: 61.23 

Round  47, Train loss: 0.689, Test loss: 1.242, Test accuracy: 60.95 

Round  48, Train loss: 0.671, Test loss: 1.261, Test accuracy: 61.24 

Round  49, Train loss: 0.633, Test loss: 1.281, Test accuracy: 60.77 

Round  50, Train loss: 0.668, Test loss: 1.305, Test accuracy: 61.00 

Round  51, Train loss: 0.702, Test loss: 1.283, Test accuracy: 61.00 

Round  52, Train loss: 0.646, Test loss: 1.282, Test accuracy: 61.18 

Round  53, Train loss: 0.678, Test loss: 1.289, Test accuracy: 61.44 

Round  54, Train loss: 0.632, Test loss: 1.280, Test accuracy: 61.24 

Round  55, Train loss: 0.653, Test loss: 1.313, Test accuracy: 61.41 

Round  56, Train loss: 0.653, Test loss: 1.315, Test accuracy: 61.25 

Round  57, Train loss: 0.617, Test loss: 1.318, Test accuracy: 61.50 

Round  58, Train loss: 0.570, Test loss: 1.364, Test accuracy: 61.32 

Round  59, Train loss: 0.612, Test loss: 1.316, Test accuracy: 61.51 

Round  60, Train loss: 0.582, Test loss: 1.357, Test accuracy: 61.07 

Round  61, Train loss: 0.580, Test loss: 1.348, Test accuracy: 61.59 

Round  62, Train loss: 0.573, Test loss: 1.341, Test accuracy: 61.57 

Round  63, Train loss: 0.528, Test loss: 1.384, Test accuracy: 61.11 

Round  64, Train loss: 0.518, Test loss: 1.380, Test accuracy: 61.50 

Round  65, Train loss: 0.534, Test loss: 1.400, Test accuracy: 61.32 

Round  66, Train loss: 0.581, Test loss: 1.395, Test accuracy: 61.24 

Round  67, Train loss: 0.514, Test loss: 1.410, Test accuracy: 61.06 

Round  68, Train loss: 0.538, Test loss: 1.417, Test accuracy: 61.27 

Round  69, Train loss: 0.499, Test loss: 1.432, Test accuracy: 61.79 

Round  70, Train loss: 0.529, Test loss: 1.428, Test accuracy: 61.02 

Round  71, Train loss: 0.487, Test loss: 1.457, Test accuracy: 61.30 

Round  72, Train loss: 0.458, Test loss: 1.471, Test accuracy: 61.19 

Round  73, Train loss: 0.469, Test loss: 1.465, Test accuracy: 61.27 

Round  74, Train loss: 0.494, Test loss: 1.461, Test accuracy: 61.54 

Round  75, Train loss: 0.465, Test loss: 1.448, Test accuracy: 61.39 

Round  76, Train loss: 0.453, Test loss: 1.458, Test accuracy: 61.65 

Round  77, Train loss: 0.431, Test loss: 1.509, Test accuracy: 61.52 

Round  78, Train loss: 0.485, Test loss: 1.485, Test accuracy: 61.21 

Round  79, Train loss: 0.436, Test loss: 1.538, Test accuracy: 61.30 

Round  80, Train loss: 0.457, Test loss: 1.554, Test accuracy: 60.97 

Round  81, Train loss: 0.421, Test loss: 1.560, Test accuracy: 61.06 

Round  82, Train loss: 0.472, Test loss: 1.544, Test accuracy: 60.93 

Round  83, Train loss: 0.486, Test loss: 1.533, Test accuracy: 61.34 

Round  84, Train loss: 0.451, Test loss: 1.558, Test accuracy: 61.00 

Round  85, Train loss: 0.456, Test loss: 1.543, Test accuracy: 61.38 

Round  86, Train loss: 0.501, Test loss: 1.568, Test accuracy: 61.32 

Round  87, Train loss: 0.429, Test loss: 1.529, Test accuracy: 61.48 

Round  88, Train loss: 0.488, Test loss: 1.560, Test accuracy: 61.40 

Round  89, Train loss: 0.445, Test loss: 1.602, Test accuracy: 61.24 

Round  90, Train loss: 0.404, Test loss: 1.590, Test accuracy: 61.36 

Round  91, Train loss: 0.423, Test loss: 1.539, Test accuracy: 61.27 

Round  92, Train loss: 0.392, Test loss: 1.585, Test accuracy: 61.35 

Round  93, Train loss: 0.383, Test loss: 1.624, Test accuracy: 61.60 

Round  94, Train loss: 0.384, Test loss: 1.635, Test accuracy: 61.65 

Round  95, Train loss: 0.440, Test loss: 1.596, Test accuracy: 61.19 

Round  96, Train loss: 0.351, Test loss: 1.651, Test accuracy: 61.50 

Round  97, Train loss: 0.389, Test loss: 1.658, Test accuracy: 61.09 

Round  98, Train loss: 0.396, Test loss: 1.626, Test accuracy: 61.44 

Round  99, Train loss: 0.368, Test loss: 1.684, Test accuracy: 61.11 

Final Round, Train loss: 0.323, Test loss: 1.693, Test accuracy: 61.01 

Average accuracy final 10 rounds: 61.354499999999994 

2144.549221277237
[1.421419382095337, 2.6139066219329834, 3.6600916385650635, 4.74504017829895, 5.83903694152832, 6.919524192810059, 7.987534284591675, 9.028585195541382, 10.071781396865845, 11.171489477157593, 12.246649980545044, 13.315767288208008, 14.402595281600952, 19.735239505767822, 20.74830913543701, 21.76162052154541, 22.77318000793457, 23.788278341293335, 24.801318168640137, 25.81744956970215, 26.83296251296997, 27.881223440170288, 28.927148580551147, 29.97541093826294, 31.006531238555908, 32.0230770111084, 33.03856086730957, 34.05407381057739, 35.069687366485596, 36.08390188217163, 37.10645627975464, 38.12036943435669, 39.1365647315979, 40.155694246292114, 41.16866993904114, 42.20071363449097, 43.217488288879395, 44.23319435119629, 45.24675416946411, 46.26701641082764, 47.31939101219177, 48.37635374069214, 49.41633439064026, 50.4602484703064, 51.4883930683136, 52.503031730651855, 53.51271390914917, 54.524978160858154, 55.53698110580444, 56.55107140541077, 57.56407022476196, 58.57572364807129, 59.5886435508728, 60.59802293777466, 61.61096405982971, 62.62133002281189, 63.63500690460205, 64.6461238861084, 65.6581244468689, 66.66794633865356, 67.67813396453857, 68.69092559814453, 69.70226311683655, 70.71138381958008, 71.72184801101685, 72.73561453819275, 73.74642515182495, 74.75784492492676, 75.7884681224823, 76.83546328544617, 77.85060930252075, 78.87072944641113, 79.87882113456726, 80.87198090553284, 81.86703324317932, 82.86019277572632, 83.86411595344543, 84.8577446937561, 85.856036901474, 86.85529232025146, 87.85313868522644, 88.84960985183716, 89.84530115127563, 90.84133219718933, 91.83489942550659, 92.8294107913971, 93.82354259490967, 94.82669687271118, 95.83082389831543, 96.82492518424988, 97.8194944858551, 98.81558799743652, 99.81249785423279, 100.81262159347534, 101.80710220336914, 102.80205130577087, 103.79664301872253, 104.79350781440735, 105.7905330657959, 106.78500175476074, 108.54847478866577]
[24.265, 30.27, 35.47, 38.248333333333335, 39.81166666666667, 42.325, 44.15833333333333, 45.821666666666665, 45.86, 48.17333333333333, 48.46666666666667, 50.235, 50.66833333333334, 51.513333333333335, 52.43, 52.843333333333334, 53.805, 54.23166666666667, 54.958333333333336, 55.73166666666667, 56.39, 56.718333333333334, 56.91166666666667, 57.67166666666667, 57.211666666666666, 58.345, 57.71333333333333, 58.73833333333334, 58.848333333333336, 58.69, 58.89833333333333, 59.211666666666666, 59.865, 59.61666666666667, 59.891666666666666, 59.79666666666667, 60.48833333333334, 60.093333333333334, 60.06166666666667, 60.47833333333333, 60.64, 60.395, 60.861666666666665, 60.026666666666664, 60.443333333333335, 60.52333333333333, 61.233333333333334, 60.95, 61.245, 60.77166666666667, 61.0, 61.001666666666665, 61.17666666666667, 61.43833333333333, 61.236666666666665, 61.415, 61.25333333333333, 61.498333333333335, 61.321666666666665, 61.51, 61.07, 61.58833333333333, 61.571666666666665, 61.10666666666667, 61.5, 61.321666666666665, 61.236666666666665, 61.06333333333333, 61.27, 61.788333333333334, 61.02, 61.295, 61.193333333333335, 61.275, 61.53666666666667, 61.39, 61.64833333333333, 61.52166666666667, 61.208333333333336, 61.29666666666667, 60.96666666666667, 61.06333333333333, 60.92666666666667, 61.34, 61.00333333333333, 61.385, 61.32, 61.48166666666667, 61.403333333333336, 61.236666666666665, 61.36, 61.27, 61.348333333333336, 61.596666666666664, 61.645, 61.185, 61.5, 61.09166666666667, 61.43833333333333, 61.11, 61.01]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.225, Test loss: 2.105, Test accuracy: 21.67 

Round   1, Train loss: 2.026, Test loss: 1.959, Test accuracy: 28.47 

Round   2, Train loss: 1.892, Test loss: 1.865, Test accuracy: 31.73 

Round   3, Train loss: 1.830, Test loss: 1.812, Test accuracy: 34.00 

Round   4, Train loss: 1.721, Test loss: 1.767, Test accuracy: 35.61 

Round   5, Train loss: 1.734, Test loss: 1.755, Test accuracy: 36.63 

Round   6, Train loss: 1.695, Test loss: 1.724, Test accuracy: 37.57 

Round   7, Train loss: 1.600, Test loss: 1.711, Test accuracy: 38.03 

Round   8, Train loss: 1.499, Test loss: 1.719, Test accuracy: 38.55 

Round   9, Train loss: 1.513, Test loss: 1.706, Test accuracy: 39.03 

Round  10, Train loss: 1.472, Test loss: 1.693, Test accuracy: 39.63 

Round  11, Train loss: 1.371, Test loss: 1.692, Test accuracy: 39.96 

Round  12, Train loss: 1.367, Test loss: 1.691, Test accuracy: 39.96 

Round  13, Train loss: 1.386, Test loss: 1.705, Test accuracy: 40.35 

Round  14, Train loss: 1.402, Test loss: 1.703, Test accuracy: 40.48 

Round  15, Train loss: 1.279, Test loss: 1.700, Test accuracy: 40.70 

Round  16, Train loss: 1.232, Test loss: 1.727, Test accuracy: 40.99 

Round  17, Train loss: 1.241, Test loss: 1.750, Test accuracy: 40.85 

Round  18, Train loss: 1.212, Test loss: 1.730, Test accuracy: 41.63 

Round  19, Train loss: 1.139, Test loss: 1.770, Test accuracy: 41.48 

Round  20, Train loss: 1.070, Test loss: 1.797, Test accuracy: 41.81 

Round  21, Train loss: 1.082, Test loss: 1.823, Test accuracy: 41.61 

Round  22, Train loss: 1.032, Test loss: 1.864, Test accuracy: 41.94 

Round  23, Train loss: 1.089, Test loss: 1.909, Test accuracy: 41.89 

Round  24, Train loss: 1.069, Test loss: 1.891, Test accuracy: 41.87 

Round  25, Train loss: 0.986, Test loss: 1.945, Test accuracy: 41.83 

Round  26, Train loss: 0.876, Test loss: 1.986, Test accuracy: 41.62 

Round  27, Train loss: 0.912, Test loss: 2.026, Test accuracy: 41.59 

Round  28, Train loss: 0.824, Test loss: 2.072, Test accuracy: 41.49 

Round  29, Train loss: 0.753, Test loss: 2.063, Test accuracy: 41.71 

Round  30, Train loss: 0.777, Test loss: 2.131, Test accuracy: 41.59 

Round  31, Train loss: 0.768, Test loss: 2.158, Test accuracy: 41.98 

Round  32, Train loss: 0.664, Test loss: 2.208, Test accuracy: 41.86 

Round  33, Train loss: 0.708, Test loss: 2.232, Test accuracy: 42.28 

Round  34, Train loss: 0.633, Test loss: 2.270, Test accuracy: 42.38 

Round  35, Train loss: 0.621, Test loss: 2.311, Test accuracy: 41.94 

Round  36, Train loss: 0.652, Test loss: 2.327, Test accuracy: 41.80 

Round  37, Train loss: 0.580, Test loss: 2.382, Test accuracy: 41.91 

Round  38, Train loss: 0.576, Test loss: 2.399, Test accuracy: 42.05 

Round  39, Train loss: 0.567, Test loss: 2.431, Test accuracy: 41.88 

Round  40, Train loss: 0.528, Test loss: 2.457, Test accuracy: 42.13 

Round  41, Train loss: 0.519, Test loss: 2.477, Test accuracy: 42.12 

Round  42, Train loss: 0.520, Test loss: 2.456, Test accuracy: 42.47 

Round  43, Train loss: 0.466, Test loss: 2.478, Test accuracy: 42.12 

Round  44, Train loss: 0.466, Test loss: 2.544, Test accuracy: 41.99 

Round  45, Train loss: 0.472, Test loss: 2.626, Test accuracy: 42.05 

Round  46, Train loss: 0.441, Test loss: 2.654, Test accuracy: 42.20 

Round  47, Train loss: 0.410, Test loss: 2.691, Test accuracy: 42.54 

Round  48, Train loss: 0.397, Test loss: 2.732, Test accuracy: 42.33 

Round  49, Train loss: 0.414, Test loss: 2.806, Test accuracy: 42.34 

Round  50, Train loss: 0.352, Test loss: 2.868, Test accuracy: 42.34 

Round  51, Train loss: 0.365, Test loss: 2.859, Test accuracy: 42.24 

Round  52, Train loss: 0.394, Test loss: 2.905, Test accuracy: 42.23 

Round  53, Train loss: 0.380, Test loss: 2.926, Test accuracy: 42.09 

Round  54, Train loss: 0.317, Test loss: 2.953, Test accuracy: 42.17 

Round  55, Train loss: 0.336, Test loss: 3.002, Test accuracy: 42.25 

Round  56, Train loss: 0.379, Test loss: 3.010, Test accuracy: 42.41 

Round  57, Train loss: 0.329, Test loss: 3.082, Test accuracy: 42.56 

Round  58, Train loss: 0.284, Test loss: 3.139, Test accuracy: 42.27 

Round  59, Train loss: 0.281, Test loss: 3.147, Test accuracy: 42.52 

Round  60, Train loss: 0.331, Test loss: 3.199, Test accuracy: 42.58 

Round  61, Train loss: 0.280, Test loss: 3.231, Test accuracy: 42.60 

Round  62, Train loss: 0.230, Test loss: 3.229, Test accuracy: 42.26 

Round  63, Train loss: 0.277, Test loss: 3.260, Test accuracy: 42.53 

Round  64, Train loss: 0.298, Test loss: 3.309, Test accuracy: 42.59 

Round  65, Train loss: 0.250, Test loss: 3.315, Test accuracy: 42.58 

Round  66, Train loss: 0.265, Test loss: 3.332, Test accuracy: 42.53 

Round  67, Train loss: 0.232, Test loss: 3.367, Test accuracy: 42.25 

Round  68, Train loss: 0.218, Test loss: 3.377, Test accuracy: 42.62 

Round  69, Train loss: 0.210, Test loss: 3.419, Test accuracy: 42.43 

Round  70, Train loss: 0.231, Test loss: 3.414, Test accuracy: 42.78 

Round  71, Train loss: 0.200, Test loss: 3.492, Test accuracy: 42.28 

Round  72, Train loss: 0.195, Test loss: 3.493, Test accuracy: 42.26 

Round  73, Train loss: 0.226, Test loss: 3.533, Test accuracy: 42.54 

Round  74, Train loss: 0.211, Test loss: 3.517, Test accuracy: 42.79 

Round  75, Train loss: 0.210, Test loss: 3.539, Test accuracy: 42.62 

Round  76, Train loss: 0.195, Test loss: 3.583, Test accuracy: 42.61 

Round  77, Train loss: 0.200, Test loss: 3.567, Test accuracy: 42.76 

Round  78, Train loss: 0.215, Test loss: 3.661, Test accuracy: 42.44 

Round  79, Train loss: 0.156, Test loss: 3.710, Test accuracy: 42.74 

Round  80, Train loss: 0.193, Test loss: 3.703, Test accuracy: 42.96 

Round  81, Train loss: 0.167, Test loss: 3.760, Test accuracy: 42.87 

Round  82, Train loss: 0.171, Test loss: 3.842, Test accuracy: 42.78 

Round  83, Train loss: 0.205, Test loss: 3.746, Test accuracy: 42.71 

Round  84, Train loss: 0.160, Test loss: 3.778, Test accuracy: 42.76 

Round  85, Train loss: 0.146, Test loss: 3.822, Test accuracy: 42.57 

Round  86, Train loss: 0.150, Test loss: 3.840, Test accuracy: 42.60 

Round  87, Train loss: 0.168, Test loss: 3.855, Test accuracy: 42.65 

Round  88, Train loss: 0.155, Test loss: 3.856, Test accuracy: 42.70 

Round  89, Train loss: 0.125, Test loss: 3.944, Test accuracy: 42.45 

Round  90, Train loss: 0.130, Test loss: 3.910, Test accuracy: 42.70 

Round  91, Train loss: 0.169, Test loss: 3.893, Test accuracy: 42.87 

Round  92, Train loss: 0.139, Test loss: 3.904, Test accuracy: 43.02 

Round  93, Train loss: 0.123, Test loss: 3.868, Test accuracy: 43.27 

Round  94, Train loss: 0.165, Test loss: 3.887, Test accuracy: 43.05 

Round  95, Train loss: 0.156, Test loss: 3.919, Test accuracy: 42.86 

Round  96, Train loss: 0.105, Test loss: 3.972, Test accuracy: 42.85 

Round  97, Train loss: 0.175, Test loss: 3.971, Test accuracy: 42.73 

Round  98, Train loss: 0.128, Test loss: 4.029, Test accuracy: 42.66 

Round  99, Train loss: 0.113, Test loss: 4.093, Test accuracy: 42.65 

Final Round, Train loss: 0.097, Test loss: 4.328, Test accuracy: 43.01 

Average accuracy final 10 rounds: 42.865 

2231.336713552475
[1.3304238319396973, 2.4322867393493652, 3.5476365089416504, 4.795315265655518, 6.0762269496917725, 7.342791795730591, 8.590996265411377, 9.843725204467773, 11.086467742919922, 12.334092140197754, 13.590384721755981, 14.837871551513672, 16.083794593811035, 17.333787202835083, 18.530807495117188, 19.77526545524597, 21.032931327819824, 22.291165590286255, 23.547566413879395, 24.800355911254883, 26.058130979537964, 27.299522161483765, 28.538647890090942, 29.790956258773804, 31.033756256103516, 32.276204347610474, 33.52128887176514, 34.7653443813324, 36.0078558921814, 37.25276064872742, 38.49959421157837, 39.75206208229065, 41.00242066383362, 42.24808096885681, 43.49965047836304, 44.747740030288696, 45.98686456680298, 47.225762605667114, 48.47893667221069, 49.742642641067505, 50.97808599472046, 52.22487187385559, 53.467204570770264, 54.71454334259033, 55.9639310836792, 57.216195583343506, 58.481571435928345, 59.752849817276, 61.00271034240723, 62.25337195396423, 63.51900291442871, 64.76980996131897, 66.0211112499237, 67.28166270256042, 68.54003953933716, 69.79621267318726, 71.04152846336365, 72.29684019088745, 73.56320929527283, 74.82069230079651, 76.10153460502625, 77.3603904247284, 78.6125020980835, 79.85984134674072, 81.11162424087524, 82.36591410636902, 83.61317491531372, 84.86127877235413, 86.1153678894043, 87.36397528648376, 88.61575889587402, 89.86212801933289, 91.11060357093811, 92.36115789413452, 93.60886979103088, 94.86379837989807, 96.11482858657837, 97.36033821105957, 98.607492685318, 99.8533353805542, 101.10254859924316, 102.3474543094635, 103.59087109565735, 104.83994626998901, 106.09079551696777, 107.33740210533142, 108.5689902305603, 109.80458211898804, 111.04170346260071, 112.27787804603577, 113.51797318458557, 114.76122212409973, 116.0022337436676, 117.23760962486267, 118.476149559021, 119.71525239944458, 120.95678758621216, 122.19762015342712, 123.44538450241089, 124.69109463691711, 127.00450348854065]
[21.671666666666667, 28.471666666666668, 31.733333333333334, 34.00333333333333, 35.611666666666665, 36.63333333333333, 37.568333333333335, 38.028333333333336, 38.553333333333335, 39.03, 39.62833333333333, 39.96333333333333, 39.961666666666666, 40.35, 40.47666666666667, 40.705, 40.99333333333333, 40.85333333333333, 41.63166666666667, 41.47833333333333, 41.80833333333333, 41.611666666666665, 41.935, 41.89333333333333, 41.86666666666667, 41.833333333333336, 41.61833333333333, 41.59, 41.49333333333333, 41.708333333333336, 41.59, 41.97666666666667, 41.86, 42.28333333333333, 42.37833333333333, 41.935, 41.79666666666667, 41.90833333333333, 42.05166666666667, 41.88166666666667, 42.13, 42.11833333333333, 42.473333333333336, 42.12166666666667, 41.99, 42.04833333333333, 42.2, 42.54, 42.325, 42.34166666666667, 42.343333333333334, 42.245, 42.23, 42.085, 42.17333333333333, 42.248333333333335, 42.41, 42.565, 42.26833333333333, 42.516666666666666, 42.575, 42.6, 42.25666666666667, 42.53333333333333, 42.59166666666667, 42.58, 42.528333333333336, 42.24666666666667, 42.61833333333333, 42.428333333333335, 42.776666666666664, 42.278333333333336, 42.26166666666666, 42.54333333333334, 42.788333333333334, 42.623333333333335, 42.608333333333334, 42.755, 42.443333333333335, 42.74, 42.95666666666666, 42.865, 42.776666666666664, 42.70666666666666, 42.76166666666666, 42.568333333333335, 42.60166666666667, 42.645, 42.70333333333333, 42.445, 42.696666666666665, 42.87166666666667, 43.02333333333333, 43.265, 43.04833333333333, 42.85666666666667, 42.85333333333333, 42.72666666666667, 42.66, 42.64833333333333, 43.00666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 1.404, Test loss: 1.841, Test accuracy: 34.58
Round   1, Train loss: 1.212, Test loss: 1.858, Test accuracy: 32.29
Round   2, Train loss: 1.089, Test loss: 1.881, Test accuracy: 32.08
Round   3, Train loss: 1.008, Test loss: 1.893, Test accuracy: 32.29
Round   4, Train loss: 0.982, Test loss: 2.023, Test accuracy: 26.53
Round   5, Train loss: 0.916, Test loss: 2.005, Test accuracy: 26.89
Round   6, Train loss: 0.860, Test loss: 1.995, Test accuracy: 27.04
Round   7, Train loss: 0.837, Test loss: 1.979, Test accuracy: 27.94
Round   8, Train loss: 0.789, Test loss: 2.021, Test accuracy: 26.53
Round   9, Train loss: 0.764, Test loss: 2.067, Test accuracy: 24.90
Round  10, Train loss: 0.698, Test loss: 2.056, Test accuracy: 26.11
Round  11, Train loss: 0.670, Test loss: 2.046, Test accuracy: 26.97
Round  12, Train loss: 0.658, Test loss: 2.043, Test accuracy: 26.86
Round  13, Train loss: 0.670, Test loss: 2.041, Test accuracy: 27.53
Round  14, Train loss: 0.601, Test loss: 2.029, Test accuracy: 28.57
Round  15, Train loss: 0.553, Test loss: 2.017, Test accuracy: 29.31
Round  16, Train loss: 0.590, Test loss: 2.022, Test accuracy: 29.20
Round  17, Train loss: 0.545, Test loss: 2.010, Test accuracy: 30.64
Round  18, Train loss: 0.536, Test loss: 2.011, Test accuracy: 29.82
Round  19, Train loss: 0.501, Test loss: 2.006, Test accuracy: 29.70
Round  20, Train loss: 0.493, Test loss: 1.994, Test accuracy: 30.12
Round  21, Train loss: 0.444, Test loss: 1.987, Test accuracy: 31.19
Round  22, Train loss: 0.498, Test loss: 1.982, Test accuracy: 30.88
Round  23, Train loss: 0.463, Test loss: 1.978, Test accuracy: 31.67
Round  24, Train loss: 0.420, Test loss: 1.966, Test accuracy: 32.84
Round  25, Train loss: 0.468, Test loss: 1.958, Test accuracy: 33.27
Round  26, Train loss: 0.415, Test loss: 1.943, Test accuracy: 34.28
Round  27, Train loss: 0.411, Test loss: 1.939, Test accuracy: 34.55
Round  28, Train loss: 0.371, Test loss: 1.932, Test accuracy: 34.48
Round  29, Train loss: 0.384, Test loss: 1.926, Test accuracy: 34.78
Round  30, Train loss: 0.411, Test loss: 1.929, Test accuracy: 33.72
Round  31, Train loss: 0.344, Test loss: 1.920, Test accuracy: 34.83
Round  32, Train loss: 0.366, Test loss: 1.914, Test accuracy: 35.91
Round  33, Train loss: 0.350, Test loss: 1.904, Test accuracy: 36.75
Round  34, Train loss: 0.335, Test loss: 1.901, Test accuracy: 36.85
Round  35, Train loss: 0.324, Test loss: 1.893, Test accuracy: 37.23
Round  36, Train loss: 0.339, Test loss: 1.888, Test accuracy: 37.32
Round  37, Train loss: 0.327, Test loss: 1.885, Test accuracy: 37.03
Round  38, Train loss: 0.350, Test loss: 1.875, Test accuracy: 38.17
Round  39, Train loss: 0.294, Test loss: 1.867, Test accuracy: 38.48
Round  40, Train loss: 0.311, Test loss: 1.857, Test accuracy: 39.11
Round  41, Train loss: 0.281, Test loss: 1.851, Test accuracy: 39.19
Round  42, Train loss: 0.266, Test loss: 1.839, Test accuracy: 39.70
Round  43, Train loss: 0.274, Test loss: 1.843, Test accuracy: 39.72
Round  44, Train loss: 0.257, Test loss: 1.845, Test accuracy: 39.09
Round  45, Train loss: 0.281, Test loss: 1.837, Test accuracy: 39.13
Round  46, Train loss: 0.257, Test loss: 1.829, Test accuracy: 40.53
Round  47, Train loss: 0.268, Test loss: 1.836, Test accuracy: 39.57
Round  48, Train loss: 0.261, Test loss: 1.826, Test accuracy: 40.12
Round  49, Train loss: 0.248, Test loss: 1.819, Test accuracy: 40.14
Round  50, Train loss: 0.252, Test loss: 1.817, Test accuracy: 40.33
Round  51, Train loss: 0.241, Test loss: 1.811, Test accuracy: 40.47
Round  52, Train loss: 0.231, Test loss: 1.819, Test accuracy: 39.78
Round  53, Train loss: 0.236, Test loss: 1.812, Test accuracy: 40.25
Round  54, Train loss: 0.232, Test loss: 1.811, Test accuracy: 40.18
Round  55, Train loss: 0.222, Test loss: 1.803, Test accuracy: 41.04
Round  56, Train loss: 0.216, Test loss: 1.796, Test accuracy: 41.34
Round  57, Train loss: 0.228, Test loss: 1.791, Test accuracy: 40.96
Round  58, Train loss: 0.215, Test loss: 1.786, Test accuracy: 41.53
Round  59, Train loss: 0.223, Test loss: 1.782, Test accuracy: 41.58
Round  60, Train loss: 0.213, Test loss: 1.781, Test accuracy: 41.66
Round  61, Train loss: 0.198, Test loss: 1.780, Test accuracy: 41.40
Round  62, Train loss: 0.199, Test loss: 1.784, Test accuracy: 40.92
Round  63, Train loss: 0.229, Test loss: 1.780, Test accuracy: 40.70
Round  64, Train loss: 0.198, Test loss: 1.780, Test accuracy: 40.64
Round  65, Train loss: 0.195, Test loss: 1.773, Test accuracy: 41.20
Round  66, Train loss: 0.196, Test loss: 1.764, Test accuracy: 42.17
Round  67, Train loss: 0.189, Test loss: 1.759, Test accuracy: 41.73
Round  68, Train loss: 0.189, Test loss: 1.753, Test accuracy: 41.83
Round  69, Train loss: 0.196, Test loss: 1.741, Test accuracy: 42.62
Round  70, Train loss: 0.189, Test loss: 1.750, Test accuracy: 41.89
Round  71, Train loss: 0.181, Test loss: 1.748, Test accuracy: 42.31
Round  72, Train loss: 0.182, Test loss: 1.734, Test accuracy: 43.22
Round  73, Train loss: 0.194, Test loss: 1.737, Test accuracy: 42.69
Round  74, Train loss: 0.178, Test loss: 1.725, Test accuracy: 43.42
Round  75, Train loss: 0.188, Test loss: 1.729, Test accuracy: 43.12
Round  76, Train loss: 0.189, Test loss: 1.725, Test accuracy: 43.56
Round  77, Train loss: 0.179, Test loss: 1.721, Test accuracy: 43.68
Round  78, Train loss: 0.169, Test loss: 1.718, Test accuracy: 43.61
Round  79, Train loss: 0.178, Test loss: 1.722, Test accuracy: 43.05
Round  80, Train loss: 0.187, Test loss: 1.715, Test accuracy: 43.54
Round  81, Train loss: 0.170, Test loss: 1.720, Test accuracy: 42.76
Round  82, Train loss: 0.164, Test loss: 1.714, Test accuracy: 42.92
Round  83, Train loss: 0.163, Test loss: 1.709, Test accuracy: 43.18
Round  84, Train loss: 0.170, Test loss: 1.701, Test accuracy: 43.59
Round  85, Train loss: 0.161, Test loss: 1.702, Test accuracy: 43.57
Round  86, Train loss: 0.159, Test loss: 1.693, Test accuracy: 43.98
Round  87, Train loss: 0.165, Test loss: 1.695, Test accuracy: 43.93
Round  88, Train loss: 0.170, Test loss: 1.687, Test accuracy: 44.33
Round  89, Train loss: 0.163, Test loss: 1.700, Test accuracy: 44.10
Round  90, Train loss: 0.164, Test loss: 1.695, Test accuracy: 44.48
Round  91, Train loss: 0.151, Test loss: 1.688, Test accuracy: 44.88
Round  92, Train loss: 0.152, Test loss: 1.680, Test accuracy: 45.28
Round  93, Train loss: 0.153, Test loss: 1.679, Test accuracy: 45.10
Round  94, Train loss: 0.149, Test loss: 1.675, Test accuracy: 45.28
Round  95, Train loss: 0.158, Test loss: 1.685, Test accuracy: 44.31
Round  96, Train loss: 0.152, Test loss: 1.667, Test accuracy: 45.48
Round  97, Train loss: 0.151, Test loss: 1.665, Test accuracy: 45.30
Round  98, Train loss: 0.150, Test loss: 1.674, Test accuracy: 44.54
Round  99, Train loss: 0.145, Test loss: 1.675, Test accuracy: 44.34
Final Round, Train loss: 0.150, Test loss: 1.672, Test accuracy: 44.93
Average accuracy final 10 rounds: 44.90016666666667
8445.137856006622
[]
[34.58166666666666, 32.291666666666664, 32.08166666666666, 32.29, 26.53, 26.885, 27.041666666666668, 27.94333333333333, 26.526666666666667, 24.89666666666667, 26.108333333333334, 26.97, 26.855, 27.526666666666667, 28.57, 29.308333333333334, 29.198333333333334, 30.641666666666666, 29.823333333333334, 29.698333333333334, 30.125, 31.19333333333333, 30.876666666666665, 31.668333333333333, 32.84, 33.27166666666667, 34.28, 34.55, 34.47833333333333, 34.78333333333333, 33.723333333333336, 34.83166666666666, 35.905, 36.75, 36.848333333333336, 37.233333333333334, 37.321666666666665, 37.031666666666666, 38.166666666666664, 38.485, 39.11333333333334, 39.18833333333333, 39.695, 39.721666666666664, 39.093333333333334, 39.13166666666667, 40.53, 39.57, 40.11833333333333, 40.14, 40.33, 40.465, 39.78, 40.25, 40.17666666666667, 41.04333333333334, 41.345, 40.95666666666666, 41.53333333333333, 41.575, 41.66, 41.395, 40.92166666666667, 40.705, 40.641666666666666, 41.20166666666667, 42.17166666666667, 41.735, 41.833333333333336, 42.615, 41.88666666666666, 42.306666666666665, 43.215, 42.68833333333333, 43.41833333333334, 43.115, 43.56333333333333, 43.68333333333333, 43.60666666666667, 43.05166666666667, 43.54333333333334, 42.763333333333335, 42.91833333333334, 43.181666666666665, 43.595, 43.571666666666665, 43.97666666666667, 43.92666666666667, 44.32666666666667, 44.10166666666667, 44.485, 44.885, 45.278333333333336, 45.098333333333336, 45.281666666666666, 44.31333333333333, 45.48, 45.29666666666667, 44.541666666666664, 44.34166666666667, 44.93333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.907, Test loss: 2.087, Test accuracy: 19.26
Round   0: Global train loss: 1.907, Global test loss: 2.291, Global test accuracy: 10.11
Round   1, Train loss: 1.615, Test loss: 1.934, Test accuracy: 27.55
Round   1: Global train loss: 1.615, Global test loss: 2.277, Global test accuracy: 13.81
Round   2, Train loss: 1.353, Test loss: 1.808, Test accuracy: 34.09
Round   2: Global train loss: 1.353, Global test loss: 2.259, Global test accuracy: 18.95
Round   3, Train loss: 1.088, Test loss: 1.683, Test accuracy: 39.21
Round   3: Global train loss: 1.088, Global test loss: 2.237, Global test accuracy: 21.25
Round   4, Train loss: 1.094, Test loss: 1.621, Test accuracy: 42.60
Round   4: Global train loss: 1.094, Global test loss: 2.213, Global test accuracy: 26.84
Round   5, Train loss: 0.704, Test loss: 1.570, Test accuracy: 44.09
Round   5: Global train loss: 0.704, Global test loss: 2.202, Global test accuracy: 26.65
Round   6, Train loss: 0.500, Test loss: 1.519, Test accuracy: 46.13
Round   6: Global train loss: 0.500, Global test loss: 2.170, Global test accuracy: 30.94
Round   7, Train loss: 1.187, Test loss: 1.520, Test accuracy: 46.10
Round   7: Global train loss: 1.187, Global test loss: 2.148, Global test accuracy: 33.73
Round   8, Train loss: 0.436, Test loss: 1.443, Test accuracy: 48.91
Round   8: Global train loss: 0.436, Global test loss: 2.116, Global test accuracy: 36.77
Round   9, Train loss: 0.419, Test loss: 1.457, Test accuracy: 48.85
Round   9: Global train loss: 0.419, Global test loss: 2.090, Global test accuracy: 38.48
Round  10, Train loss: 0.263, Test loss: 1.424, Test accuracy: 49.50
Round  10: Global train loss: 0.263, Global test loss: 2.062, Global test accuracy: 39.60
Round  11, Train loss: -0.158, Test loss: 1.387, Test accuracy: 50.67
Round  11: Global train loss: -0.158, Global test loss: 2.034, Global test accuracy: 41.52
Round  12, Train loss: -0.207, Test loss: 1.370, Test accuracy: 51.38
Round  12: Global train loss: -0.207, Global test loss: 2.001, Global test accuracy: 43.06
Round  13, Train loss: -0.258, Test loss: 1.354, Test accuracy: 52.31
Round  13: Global train loss: -0.258, Global test loss: 1.968, Global test accuracy: 43.77
Round  14, Train loss: -0.780, Test loss: 1.337, Test accuracy: 53.08
Round  14: Global train loss: -0.780, Global test loss: 1.938, Global test accuracy: 45.52
Round  15, Train loss: -1.139, Test loss: 1.350, Test accuracy: 52.84
Round  15: Global train loss: -1.139, Global test loss: 1.911, Global test accuracy: 45.25
Round  16, Train loss: -1.076, Test loss: 1.341, Test accuracy: 53.50
Round  16: Global train loss: -1.076, Global test loss: 1.873, Global test accuracy: 46.98
Round  17, Train loss: -0.537, Test loss: 1.340, Test accuracy: 53.87
Round  17: Global train loss: -0.537, Global test loss: 1.837, Global test accuracy: 48.65
Round  18, Train loss: -0.989, Test loss: 1.351, Test accuracy: 53.39
Round  18: Global train loss: -0.989, Global test loss: 1.799, Global test accuracy: 49.88
Round  19, Train loss: -1.135, Test loss: 1.337, Test accuracy: 53.88
Round  19: Global train loss: -1.135, Global test loss: 1.777, Global test accuracy: 50.08
Round  20, Train loss: -1.168, Test loss: 1.316, Test accuracy: 54.83
Round  20: Global train loss: -1.168, Global test loss: 1.753, Global test accuracy: 50.63
Round  21, Train loss: -2.025, Test loss: 1.317, Test accuracy: 55.15
Round  21: Global train loss: -2.025, Global test loss: 1.721, Global test accuracy: 51.56
Round  22, Train loss: -1.636, Test loss: 1.305, Test accuracy: 55.64
Round  22: Global train loss: -1.636, Global test loss: 1.693, Global test accuracy: 52.65
Round  23, Train loss: -0.226, Test loss: 1.313, Test accuracy: 55.10
Round  23: Global train loss: -0.226, Global test loss: 1.685, Global test accuracy: 52.43
Round  24, Train loss: -2.266, Test loss: 1.303, Test accuracy: 55.83
Round  24: Global train loss: -2.266, Global test loss: 1.667, Global test accuracy: 52.27
Round  25, Train loss: -2.505, Test loss: 1.286, Test accuracy: 55.86
Round  25: Global train loss: -2.505, Global test loss: 1.649, Global test accuracy: 52.53
Round  26, Train loss: -2.328, Test loss: 1.285, Test accuracy: 55.86
Round  26: Global train loss: -2.328, Global test loss: 1.634, Global test accuracy: 52.96
Round  27, Train loss: -2.083, Test loss: 1.288, Test accuracy: 55.87
Round  27: Global train loss: -2.083, Global test loss: 1.619, Global test accuracy: 53.13
Round  28, Train loss: -2.344, Test loss: 1.294, Test accuracy: 55.77
Round  28: Global train loss: -2.344, Global test loss: 1.595, Global test accuracy: 53.70
Round  29, Train loss: -2.701, Test loss: 1.305, Test accuracy: 55.67
Round  29: Global train loss: -2.701, Global test loss: 1.582, Global test accuracy: 53.97
Round  30, Train loss: -3.185, Test loss: 1.301, Test accuracy: 56.26
Round  30: Global train loss: -3.185, Global test loss: 1.561, Global test accuracy: 54.67
Round  31, Train loss: -2.526, Test loss: 1.291, Test accuracy: 56.59
Round  31: Global train loss: -2.526, Global test loss: 1.543, Global test accuracy: 55.17
Round  32, Train loss: -2.346, Test loss: 1.296, Test accuracy: 56.67
Round  32: Global train loss: -2.346, Global test loss: 1.525, Global test accuracy: 55.61
Round  33, Train loss: -2.474, Test loss: 1.294, Test accuracy: 57.04
Round  33: Global train loss: -2.474, Global test loss: 1.498, Global test accuracy: 56.66
Round  34, Train loss: -2.876, Test loss: 1.300, Test accuracy: 57.14
Round  34: Global train loss: -2.876, Global test loss: 1.470, Global test accuracy: 57.76
Round  35, Train loss: -3.298, Test loss: 1.285, Test accuracy: 57.77
Round  35: Global train loss: -3.298, Global test loss: 1.441, Global test accuracy: 58.61
Round  36, Train loss: -2.798, Test loss: 1.282, Test accuracy: 57.91
Round  36: Global train loss: -2.798, Global test loss: 1.429, Global test accuracy: 58.99
Round  37, Train loss: -2.266, Test loss: 1.283, Test accuracy: 57.62
Round  37: Global train loss: -2.266, Global test loss: 1.422, Global test accuracy: 58.95
Round  38, Train loss: -2.683, Test loss: 1.282, Test accuracy: 57.52
Round  38: Global train loss: -2.683, Global test loss: 1.400, Global test accuracy: 59.77
Round  39, Train loss: -2.804, Test loss: 1.287, Test accuracy: 57.43
Round  39: Global train loss: -2.804, Global test loss: 1.382, Global test accuracy: 60.02
Round  40, Train loss: -3.019, Test loss: 1.292, Test accuracy: 57.42
Round  40: Global train loss: -3.019, Global test loss: 1.372, Global test accuracy: 60.27
Round  41, Train loss: -3.546, Test loss: 1.292, Test accuracy: 57.42
Round  41: Global train loss: -3.546, Global test loss: 1.354, Global test accuracy: 60.73
Round  42, Train loss: -3.076, Test loss: 1.285, Test accuracy: 57.64
Round  42: Global train loss: -3.076, Global test loss: 1.333, Global test accuracy: 61.45
Round  43, Train loss: -3.109, Test loss: 1.290, Test accuracy: 57.60
Round  43: Global train loss: -3.109, Global test loss: 1.319, Global test accuracy: 61.69
Round  44, Train loss: -3.549, Test loss: 1.286, Test accuracy: 57.65
Round  44: Global train loss: -3.549, Global test loss: 1.308, Global test accuracy: 62.10
Round  45, Train loss: -3.850, Test loss: 1.281, Test accuracy: 57.95
Round  45: Global train loss: -3.850, Global test loss: 1.296, Global test accuracy: 62.36
Round  46, Train loss: -4.138, Test loss: 1.289, Test accuracy: 58.02
Round  46: Global train loss: -4.138, Global test loss: 1.280, Global test accuracy: 62.49
Round  47, Train loss: -4.197, Test loss: 1.283, Test accuracy: 58.53
Round  47: Global train loss: -4.197, Global test loss: 1.261, Global test accuracy: 62.90
Round  48, Train loss: -3.279, Test loss: 1.276, Test accuracy: 58.34
Round  48: Global train loss: -3.279, Global test loss: 1.255, Global test accuracy: 62.98
Round  49, Train loss: -3.295, Test loss: 1.275, Test accuracy: 58.41
Round  49: Global train loss: -3.295, Global test loss: 1.248, Global test accuracy: 62.69
Round  50, Train loss: -3.623, Test loss: 1.275, Test accuracy: 58.71
Round  50: Global train loss: -3.623, Global test loss: 1.232, Global test accuracy: 63.22
Round  51, Train loss: -3.744, Test loss: 1.292, Test accuracy: 58.16
Round  51: Global train loss: -3.744, Global test loss: 1.223, Global test accuracy: 63.62
Round  52, Train loss: -3.750, Test loss: 1.271, Test accuracy: 58.71
Round  52: Global train loss: -3.750, Global test loss: 1.213, Global test accuracy: 63.57
Round  53, Train loss: -3.753, Test loss: 1.257, Test accuracy: 58.92
Round  53: Global train loss: -3.753, Global test loss: 1.207, Global test accuracy: 63.92
Round  54, Train loss: -3.739, Test loss: 1.259, Test accuracy: 59.08
Round  54: Global train loss: -3.739, Global test loss: 1.195, Global test accuracy: 64.14
Round  55, Train loss: -4.376, Test loss: 1.272, Test accuracy: 58.89
Round  55: Global train loss: -4.376, Global test loss: 1.186, Global test accuracy: 64.42
Round  56, Train loss: -4.458, Test loss: 1.267, Test accuracy: 59.28
Round  56: Global train loss: -4.458, Global test loss: 1.176, Global test accuracy: 64.73
Round  57, Train loss: -3.760, Test loss: 1.263, Test accuracy: 59.39
Round  57: Global train loss: -3.760, Global test loss: 1.168, Global test accuracy: 64.91
Round  58, Train loss: -4.594, Test loss: 1.278, Test accuracy: 59.10
Round  58: Global train loss: -4.594, Global test loss: 1.162, Global test accuracy: 64.77
Round  59, Train loss: -3.803, Test loss: 1.265, Test accuracy: 59.62
Round  59: Global train loss: -3.803, Global test loss: 1.155, Global test accuracy: 64.86
Round  60, Train loss: -3.897, Test loss: 1.272, Test accuracy: 59.30
Round  60: Global train loss: -3.897, Global test loss: 1.148, Global test accuracy: 65.11
Round  61, Train loss: -3.830, Test loss: 1.289, Test accuracy: 58.89
Round  61: Global train loss: -3.830, Global test loss: 1.141, Global test accuracy: 65.08
Round  62, Train loss: -4.358, Test loss: 1.304, Test accuracy: 58.50
Round  62: Global train loss: -4.358, Global test loss: 1.135, Global test accuracy: 65.21
Round  63, Train loss: -3.868, Test loss: 1.293, Test accuracy: 58.82
Round  63: Global train loss: -3.868, Global test loss: 1.130, Global test accuracy: 65.30
Round  64, Train loss: -4.107, Test loss: 1.297, Test accuracy: 59.09
Round  64: Global train loss: -4.107, Global test loss: 1.121, Global test accuracy: 65.47
Round  65, Train loss: -4.594, Test loss: 1.287, Test accuracy: 59.56
Round  65: Global train loss: -4.594, Global test loss: 1.110, Global test accuracy: 65.81
Round  66, Train loss: -3.856, Test loss: 1.286, Test accuracy: 59.33
Round  66: Global train loss: -3.856, Global test loss: 1.103, Global test accuracy: 65.42
Round  67, Train loss: -3.844, Test loss: 1.272, Test accuracy: 59.58
Round  67: Global train loss: -3.844, Global test loss: 1.095, Global test accuracy: 65.71
Round  68, Train loss: -4.209, Test loss: 1.260, Test accuracy: 59.91
Round  68: Global train loss: -4.209, Global test loss: 1.089, Global test accuracy: 65.94
Round  69, Train loss: -3.958, Test loss: 1.257, Test accuracy: 59.91
Round  69: Global train loss: -3.958, Global test loss: 1.082, Global test accuracy: 65.97
Round  70, Train loss: -4.289, Test loss: 1.271, Test accuracy: 59.77
Round  70: Global train loss: -4.289, Global test loss: 1.073, Global test accuracy: 66.11
Round  71, Train loss: -4.517, Test loss: 1.263, Test accuracy: 59.91
Round  71: Global train loss: -4.517, Global test loss: 1.065, Global test accuracy: 66.28
Round  72, Train loss: -4.414, Test loss: 1.257, Test accuracy: 59.94
Round  72: Global train loss: -4.414, Global test loss: 1.057, Global test accuracy: 66.47
Round  73, Train loss: -3.831, Test loss: 1.258, Test accuracy: 59.75
Round  73: Global train loss: -3.831, Global test loss: 1.055, Global test accuracy: 66.81
Round  74, Train loss: -4.198, Test loss: 1.267, Test accuracy: 59.98
Round  74: Global train loss: -4.198, Global test loss: 1.048, Global test accuracy: 66.94
Round  75, Train loss: -3.924, Test loss: 1.268, Test accuracy: 60.14
Round  75: Global train loss: -3.924, Global test loss: 1.040, Global test accuracy: 67.08
Round  76, Train loss: -4.185, Test loss: 1.251, Test accuracy: 60.74
Round  76: Global train loss: -4.185, Global test loss: 1.035, Global test accuracy: 67.28
Round  77, Train loss: -4.336, Test loss: 1.258, Test accuracy: 60.85
Round  77: Global train loss: -4.336, Global test loss: 1.026, Global test accuracy: 67.30
Round  78, Train loss: -4.000, Test loss: 1.254, Test accuracy: 60.83
Round  78: Global train loss: -4.000, Global test loss: 1.016, Global test accuracy: 67.53
Round  79, Train loss: -4.212, Test loss: 1.246, Test accuracy: 60.99
Round  79: Global train loss: -4.212, Global test loss: 1.009, Global test accuracy: 67.75
Round  80, Train loss: -4.118, Test loss: 1.245, Test accuracy: 60.77
Round  80: Global train loss: -4.118, Global test loss: 1.001, Global test accuracy: 68.05
Round  81, Train loss: -3.938, Test loss: 1.245, Test accuracy: 60.82
Round  81: Global train loss: -3.938, Global test loss: 0.996, Global test accuracy: 68.31
Round  82, Train loss: -4.325, Test loss: 1.251, Test accuracy: 60.74
Round  82: Global train loss: -4.325, Global test loss: 0.990, Global test accuracy: 68.12
Round  83, Train loss: -4.431, Test loss: 1.248, Test accuracy: 60.89
Round  83: Global train loss: -4.431, Global test loss: 0.984, Global test accuracy: 68.21
Round  84, Train loss: -4.262, Test loss: 1.250, Test accuracy: 60.95
Round  84: Global train loss: -4.262, Global test loss: 0.978, Global test accuracy: 68.33
Round  85, Train loss: -4.314, Test loss: 1.248, Test accuracy: 61.20
Round  85: Global train loss: -4.314, Global test loss: 0.972, Global test accuracy: 68.49
Round  86, Train loss: -4.008, Test loss: 1.250, Test accuracy: 61.05
Round  86: Global train loss: -4.008, Global test loss: 0.968, Global test accuracy: 68.66
Round  87, Train loss: -4.654, Test loss: 1.263, Test accuracy: 61.02
Round  87: Global train loss: -4.654, Global test loss: 0.962, Global test accuracy: 68.77
Round  88, Train loss: -3.848, Test loss: 1.258, Test accuracy: 60.80
Round  88: Global train loss: -3.848, Global test loss: 0.959, Global test accuracy: 68.91
Round  89, Train loss: -4.394, Test loss: 1.261, Test accuracy: 60.78
Round  89: Global train loss: -4.394, Global test loss: 0.955, Global test accuracy: 69.02
Round  90, Train loss: -4.526, Test loss: 1.246, Test accuracy: 61.08
Round  90: Global train loss: -4.526, Global test loss: 0.951, Global test accuracy: 69.02
Round  91, Train loss: -4.218, Test loss: 1.236, Test accuracy: 61.41
Round  91: Global train loss: -4.218, Global test loss: 0.944, Global test accuracy: 69.19
Round  92, Train loss: -4.094, Test loss: 1.230, Test accuracy: 61.46
Round  92: Global train loss: -4.094, Global test loss: 0.941, Global test accuracy: 69.32
Round  93, Train loss: -4.185, Test loss: 1.245, Test accuracy: 61.18
Round  93: Global train loss: -4.185, Global test loss: 0.938, Global test accuracy: 69.42
Round  94, Train loss: -4.307, Test loss: 1.246, Test accuracy: 61.36
Round  94: Global train loss: -4.307, Global test loss: 0.933, Global test accuracy: 69.53
Round  95, Train loss: -4.075, Test loss: 1.237, Test accuracy: 61.74
Round  95: Global train loss: -4.075, Global test loss: 0.928, Global test accuracy: 69.53
Round  96, Train loss: -3.766, Test loss: 1.228, Test accuracy: 61.99
Round  96: Global train loss: -3.766, Global test loss: 0.925, Global test accuracy: 69.96
Round  97, Train loss: -4.133, Test loss: 1.234, Test accuracy: 61.62
Round  97: Global train loss: -4.133, Global test loss: 0.922, Global test accuracy: 70.10
Round  98, Train loss: -3.884, Test loss: 1.211, Test accuracy: 62.08
Round  98: Global train loss: -3.884, Global test loss: 0.920, Global test accuracy: 69.89
Round  99, Train loss: -3.986, Test loss: 1.213, Test accuracy: 62.09
Round  99: Global train loss: -3.986, Global test loss: 0.917, Global test accuracy: 69.95
Final Round: Train loss: 0.855, Test loss: 1.019, Test accuracy: 65.89
Final Round: Global train loss: 0.855, Global test loss: 0.897, Global test accuracy: 70.48
Average accuracy final 10 rounds: 61.599500000000006
Average global accuracy final 10 rounds: 69.59183333333334
8608.780770540237
[]
[19.26, 27.548333333333332, 34.09166666666667, 39.211666666666666, 42.605, 44.086666666666666, 46.12833333333333, 46.098333333333336, 48.91, 48.85166666666667, 49.50333333333333, 50.67333333333333, 51.38, 52.306666666666665, 53.08166666666666, 52.843333333333334, 53.498333333333335, 53.865, 53.388333333333335, 53.87833333333333, 54.82833333333333, 55.14666666666667, 55.64, 55.1, 55.83166666666666, 55.861666666666665, 55.86333333333334, 55.873333333333335, 55.76833333333333, 55.67333333333333, 56.26166666666666, 56.595, 56.67333333333333, 57.038333333333334, 57.13666666666666, 57.775, 57.90833333333333, 57.61833333333333, 57.515, 57.43, 57.416666666666664, 57.42, 57.641666666666666, 57.605, 57.653333333333336, 57.946666666666665, 58.02, 58.535, 58.34, 58.406666666666666, 58.71, 58.165, 58.711666666666666, 58.916666666666664, 59.08, 58.888333333333335, 59.276666666666664, 59.39, 59.098333333333336, 59.615, 59.29666666666667, 58.888333333333335, 58.49666666666667, 58.81666666666667, 59.093333333333334, 59.56, 59.32666666666667, 59.58166666666666, 59.91, 59.906666666666666, 59.765, 59.913333333333334, 59.935, 59.748333333333335, 59.98166666666667, 60.13666666666666, 60.74333333333333, 60.848333333333336, 60.82666666666667, 60.98833333333334, 60.77, 60.82, 60.74, 60.88666666666666, 60.95166666666667, 61.20166666666667, 61.053333333333335, 61.02166666666667, 60.795, 60.776666666666664, 61.08, 61.415, 61.458333333333336, 61.18333333333333, 61.358333333333334, 61.736666666666665, 61.986666666666665, 61.61666666666667, 62.075, 62.085, 65.89333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.304, Test loss: 2.303, Test accuracy: 10.00 

Round   0, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.00 

Round   1, Train loss: 2.304, Test loss: 2.303, Test accuracy: 10.00 

Round   1, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.00 

Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.00 

Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.00 

Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.00 

Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.00 

Round   4, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.00 

Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.00 

Round   5, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.00 

Round   5, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 10.00 

Round   6, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.00 

Round   6, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 10.00 

Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.00 

Round   7, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.00 

Round   8, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.00 

Round   8, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.00 

Round   9, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.00 

Round   9, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.00 

Round  10, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.00 

Round  10, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 10.00 

Round  11, Train loss: 2.300, Test loss: 2.300, Test accuracy: 10.00 

Round  11, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 10.00 

Round  12, Train loss: 2.301, Test loss: 2.300, Test accuracy: 10.00 

Round  12, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 10.00 

Round  13, Train loss: 2.300, Test loss: 2.299, Test accuracy: 10.00 

Round  13, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 10.00 

Round  14, Train loss: 2.298, Test loss: 2.299, Test accuracy: 10.00 

Round  14, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 10.00 

Round  15, Train loss: 2.298, Test loss: 2.299, Test accuracy: 10.00 

Round  15, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 10.00 

Round  16, Train loss: 2.299, Test loss: 2.299, Test accuracy: 10.00 

Round  16, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 10.00 

Round  17, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.00 

Round  17, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.00 

Round  18, Train loss: 2.297, Test loss: 2.298, Test accuracy: 10.00 

Round  18, Global train loss: 2.297, Global test loss: 2.297, Global test accuracy: 10.00 

Round  19, Train loss: 2.297, Test loss: 2.297, Test accuracy: 10.00 

Round  19, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 10.00 

Round  20, Train loss: 2.297, Test loss: 2.297, Test accuracy: 10.00 

Round  20, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 10.00 

Round  21, Train loss: 2.296, Test loss: 2.296, Test accuracy: 10.00 

Round  21, Global train loss: 2.296, Global test loss: 2.295, Global test accuracy: 10.00 

Round  22, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.00 

Round  22, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.00 

Round  23, Train loss: 2.296, Test loss: 2.295, Test accuracy: 10.00 

Round  23, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 10.00 

Round  24, Train loss: 2.296, Test loss: 2.295, Test accuracy: 10.00 

Round  24, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 10.00 

Round  25, Train loss: 2.296, Test loss: 2.295, Test accuracy: 10.00 

Round  25, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 10.00 

Round  26, Train loss: 2.295, Test loss: 2.294, Test accuracy: 10.00 

Round  26, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 10.00 

Round  27, Train loss: 2.296, Test loss: 2.294, Test accuracy: 10.00 

Round  27, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 10.00 

Round  28, Train loss: 2.294, Test loss: 2.293, Test accuracy: 10.00 

Round  28, Global train loss: 2.294, Global test loss: 2.292, Global test accuracy: 10.00 

Round  29, Train loss: 2.293, Test loss: 2.293, Test accuracy: 10.00 

Round  29, Global train loss: 2.293, Global test loss: 2.292, Global test accuracy: 10.00 

Round  30, Train loss: 2.294, Test loss: 2.292, Test accuracy: 10.00 

Round  30, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 10.00 

Round  31, Train loss: 2.294, Test loss: 2.292, Test accuracy: 10.01 

Round  31, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 10.00 

Round  32, Train loss: 2.292, Test loss: 2.292, Test accuracy: 10.03 

Round  32, Global train loss: 2.292, Global test loss: 2.290, Global test accuracy: 10.04 

Round  33, Train loss: 2.292, Test loss: 2.291, Test accuracy: 10.05 

Round  33, Global train loss: 2.292, Global test loss: 2.290, Global test accuracy: 10.04 

Round  34, Train loss: 2.291, Test loss: 2.290, Test accuracy: 10.09 

Round  34, Global train loss: 2.291, Global test loss: 2.289, Global test accuracy: 10.07 

Round  35, Train loss: 2.292, Test loss: 2.290, Test accuracy: 10.08 

Round  35, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 10.11 

Round  36, Train loss: 2.291, Test loss: 2.290, Test accuracy: 10.11 

Round  36, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 10.15 

Round  37, Train loss: 2.290, Test loss: 2.289, Test accuracy: 10.15 

Round  37, Global train loss: 2.290, Global test loss: 2.288, Global test accuracy: 10.16 

Round  38, Train loss: 2.290, Test loss: 2.289, Test accuracy: 10.27 

Round  38, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 10.55 

Round  39, Train loss: 2.290, Test loss: 2.289, Test accuracy: 10.38 

Round  39, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 10.84 

Round  40, Train loss: 2.287, Test loss: 2.288, Test accuracy: 10.58 

Round  40, Global train loss: 2.287, Global test loss: 2.286, Global test accuracy: 10.99 

Round  41, Train loss: 2.287, Test loss: 2.288, Test accuracy: 10.86 

Round  41, Global train loss: 2.287, Global test loss: 2.285, Global test accuracy: 11.59 

Round  42, Train loss: 2.287, Test loss: 2.287, Test accuracy: 11.13 

Round  42, Global train loss: 2.287, Global test loss: 2.284, Global test accuracy: 11.81 

Round  43, Train loss: 2.287, Test loss: 2.286, Test accuracy: 11.44 

Round  43, Global train loss: 2.287, Global test loss: 2.284, Global test accuracy: 12.14 

Round  44, Train loss: 2.287, Test loss: 2.285, Test accuracy: 11.83 

Round  44, Global train loss: 2.287, Global test loss: 2.283, Global test accuracy: 12.40 

Round  45, Train loss: 2.283, Test loss: 2.285, Test accuracy: 12.27 

Round  45, Global train loss: 2.283, Global test loss: 2.282, Global test accuracy: 13.14 

Round  46, Train loss: 2.282, Test loss: 2.284, Test accuracy: 12.43 

Round  46, Global train loss: 2.282, Global test loss: 2.282, Global test accuracy: 13.28 

Round  47, Train loss: 2.285, Test loss: 2.283, Test accuracy: 12.91 

Round  47, Global train loss: 2.285, Global test loss: 2.281, Global test accuracy: 13.26 

Round  48, Train loss: 2.285, Test loss: 2.282, Test accuracy: 12.99 

Round  48, Global train loss: 2.285, Global test loss: 2.280, Global test accuracy: 13.24 

Round  49, Train loss: 2.283, Test loss: 2.282, Test accuracy: 13.33 

Round  49, Global train loss: 2.283, Global test loss: 2.279, Global test accuracy: 13.80 

Round  50, Train loss: 2.282, Test loss: 2.280, Test accuracy: 13.61 

Round  50, Global train loss: 2.282, Global test loss: 2.278, Global test accuracy: 13.86 

Round  51, Train loss: 2.281, Test loss: 2.279, Test accuracy: 13.75 

Round  51, Global train loss: 2.281, Global test loss: 2.277, Global test accuracy: 13.87 

Round  52, Train loss: 2.281, Test loss: 2.279, Test accuracy: 13.88 

Round  52, Global train loss: 2.281, Global test loss: 2.276, Global test accuracy: 14.33 

Round  53, Train loss: 2.281, Test loss: 2.277, Test accuracy: 14.19 

Round  53, Global train loss: 2.281, Global test loss: 2.275, Global test accuracy: 14.49 

Round  54, Train loss: 2.281, Test loss: 2.276, Test accuracy: 14.44 

Round  54, Global train loss: 2.281, Global test loss: 2.273, Global test accuracy: 14.65 

Round  55, Train loss: 2.280, Test loss: 2.275, Test accuracy: 14.40 

Round  55, Global train loss: 2.280, Global test loss: 2.273, Global test accuracy: 14.71 

Round  56, Train loss: 2.276, Test loss: 2.274, Test accuracy: 14.49 

Round  56, Global train loss: 2.276, Global test loss: 2.271, Global test accuracy: 14.69 

Round  57, Train loss: 2.278, Test loss: 2.273, Test accuracy: 14.64 

Round  57, Global train loss: 2.278, Global test loss: 2.271, Global test accuracy: 14.77 

Round  58, Train loss: 2.279, Test loss: 2.272, Test accuracy: 14.63 

Round  58, Global train loss: 2.279, Global test loss: 2.269, Global test accuracy: 14.88 

Round  59, Train loss: 2.273, Test loss: 2.271, Test accuracy: 14.72 

Round  59, Global train loss: 2.273, Global test loss: 2.269, Global test accuracy: 14.87 

Round  60, Train loss: 2.278, Test loss: 2.270, Test accuracy: 14.84 

Round  60, Global train loss: 2.278, Global test loss: 2.268, Global test accuracy: 15.06 

Round  61, Train loss: 2.274, Test loss: 2.269, Test accuracy: 15.19 

Round  61, Global train loss: 2.274, Global test loss: 2.266, Global test accuracy: 15.15 

Round  62, Train loss: 2.275, Test loss: 2.268, Test accuracy: 15.18 

Round  62, Global train loss: 2.275, Global test loss: 2.265, Global test accuracy: 15.22 

Round  63, Train loss: 2.271, Test loss: 2.266, Test accuracy: 15.08 

Round  63, Global train loss: 2.271, Global test loss: 2.263, Global test accuracy: 14.84 

Round  64, Train loss: 2.276, Test loss: 2.265, Test accuracy: 15.31 

Round  64, Global train loss: 2.276, Global test loss: 2.261, Global test accuracy: 15.59 

Round  65, Train loss: 2.271, Test loss: 2.263, Test accuracy: 15.76 

Round  65, Global train loss: 2.271, Global test loss: 2.261, Global test accuracy: 16.53 

Round  66, Train loss: 2.277, Test loss: 2.262, Test accuracy: 15.70 

Round  66, Global train loss: 2.277, Global test loss: 2.258, Global test accuracy: 15.61 

Round  67, Train loss: 2.270, Test loss: 2.260, Test accuracy: 15.42 

Round  67, Global train loss: 2.270, Global test loss: 2.257, Global test accuracy: 15.20 

Round  68, Train loss: 2.270, Test loss: 2.259, Test accuracy: 15.60 

Round  68, Global train loss: 2.270, Global test loss: 2.255, Global test accuracy: 15.96 

Round  69, Train loss: 2.266, Test loss: 2.258, Test accuracy: 15.86 

Round  69, Global train loss: 2.266, Global test loss: 2.255, Global test accuracy: 16.36 

Round  70, Train loss: 2.267, Test loss: 2.256, Test accuracy: 15.81 

Round  70, Global train loss: 2.267, Global test loss: 2.254, Global test accuracy: 16.23 

Round  71, Train loss: 2.268, Test loss: 2.256, Test accuracy: 16.14 

Round  71, Global train loss: 2.268, Global test loss: 2.254, Global test accuracy: 16.76 

Round  72, Train loss: 2.264, Test loss: 2.255, Test accuracy: 16.38 

Round  72, Global train loss: 2.264, Global test loss: 2.253, Global test accuracy: 17.01 

Round  73, Train loss: 2.266, Test loss: 2.254, Test accuracy: 16.67 

Round  73, Global train loss: 2.266, Global test loss: 2.252, Global test accuracy: 16.97 

Round  74, Train loss: 2.266, Test loss: 2.253, Test accuracy: 16.75 

Round  74, Global train loss: 2.266, Global test loss: 2.251, Global test accuracy: 17.02 

Round  75, Train loss: 2.268, Test loss: 2.252, Test accuracy: 16.76 

Round  75, Global train loss: 2.268, Global test loss: 2.250, Global test accuracy: 17.18 

Round  76, Train loss: 2.266, Test loss: 2.252, Test accuracy: 17.17 

Round  76, Global train loss: 2.266, Global test loss: 2.251, Global test accuracy: 18.15 

Round  77, Train loss: 2.264, Test loss: 2.251, Test accuracy: 17.74 

Round  77, Global train loss: 2.264, Global test loss: 2.250, Global test accuracy: 18.57 

Round  78, Train loss: 2.262, Test loss: 2.250, Test accuracy: 17.68 

Round  78, Global train loss: 2.262, Global test loss: 2.248, Global test accuracy: 18.63 

Round  79, Train loss: 2.262, Test loss: 2.249, Test accuracy: 17.76 

Round  79, Global train loss: 2.262, Global test loss: 2.247, Global test accuracy: 18.74 

Round  80, Train loss: 2.261, Test loss: 2.249, Test accuracy: 17.86 

Round  80, Global train loss: 2.261, Global test loss: 2.246, Global test accuracy: 18.66 

Round  81, Train loss: 2.259, Test loss: 2.247, Test accuracy: 18.24 

Round  81, Global train loss: 2.259, Global test loss: 2.245, Global test accuracy: 18.87 

Round  82, Train loss: 2.257, Test loss: 2.246, Test accuracy: 18.32 

Round  82, Global train loss: 2.257, Global test loss: 2.243, Global test accuracy: 18.87 

Round  83, Train loss: 2.260, Test loss: 2.246, Test accuracy: 18.72 

Round  83, Global train loss: 2.260, Global test loss: 2.243, Global test accuracy: 19.32 

Round  84, Train loss: 2.258, Test loss: 2.245, Test accuracy: 19.12 

Round  84, Global train loss: 2.258, Global test loss: 2.242, Global test accuracy: 19.79 

Round  85, Train loss: 2.261, Test loss: 2.243, Test accuracy: 19.48 

Round  85, Global train loss: 2.261, Global test loss: 2.241, Global test accuracy: 19.89 

Round  86, Train loss: 2.253, Test loss: 2.243, Test accuracy: 19.73 

Round  86, Global train loss: 2.253, Global test loss: 2.242, Global test accuracy: 20.29 

Round  87, Train loss: 2.254, Test loss: 2.242, Test accuracy: 19.79 

Round  87, Global train loss: 2.254, Global test loss: 2.239, Global test accuracy: 20.40 

Round  88, Train loss: 2.252, Test loss: 2.241, Test accuracy: 19.89 

Round  88, Global train loss: 2.252, Global test loss: 2.238, Global test accuracy: 20.34 

Round  89, Train loss: 2.257, Test loss: 2.241, Test accuracy: 19.99 

Round  89, Global train loss: 2.257, Global test loss: 2.239, Global test accuracy: 20.79 

Round  90, Train loss: 2.250, Test loss: 2.240, Test accuracy: 20.08 

Round  90, Global train loss: 2.250, Global test loss: 2.238, Global test accuracy: 20.83 

Round  91, Train loss: 2.254, Test loss: 2.238, Test accuracy: 20.11 

Round  91, Global train loss: 2.254, Global test loss: 2.235, Global test accuracy: 20.72 

Round  92, Train loss: 2.253, Test loss: 2.237, Test accuracy: 20.18 

Round  92, Global train loss: 2.253, Global test loss: 2.233, Global test accuracy: 20.46 

Round  93, Train loss: 2.251, Test loss: 2.236, Test accuracy: 20.31 

Round  93, Global train loss: 2.251, Global test loss: 2.233, Global test accuracy: 21.14 

Round  94, Train loss: 2.248, Test loss: 2.236, Test accuracy: 20.43 

Round  94, Global train loss: 2.248, Global test loss: 2.233, Global test accuracy: 20.82 

Round  95, Train loss: 2.245, Test loss: 2.235, Test accuracy: 20.41 

Round  95, Global train loss: 2.245, Global test loss: 2.231, Global test accuracy: 21.01 

Round  96, Train loss: nan, Test loss: nan, Test accuracy: 19.93 

Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round  97, Train loss: nan, Test loss: nan, Test accuracy: 17.43 

Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round  98, Train loss: nan, Test loss: nan, Test accuracy: 15.95 

Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round  99, Train loss: nan, Test loss: nan, Test accuracy: 14.28 

Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 100, Train loss: nan, Test loss: nan, Test accuracy: 13.77 

Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 101, Train loss: nan, Test loss: nan, Test accuracy: 12.16 

Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 102, Train loss: nan, Test loss: nan, Test accuracy: 11.54 

Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 103, Train loss: nan, Test loss: nan, Test accuracy: 10.49 

Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 104, Train loss: nan, Test loss: nan, Test accuracy: 10.49 

Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 105, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 106, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 107, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 108, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 109, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 110, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 111, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 112, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 113, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 114, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 115, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 116, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 117, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 118, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 119, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 120, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 121, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 122, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 123, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 124, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 125, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 126, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 127, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 128, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 129, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 130, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 131, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 132, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 133, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 134, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 135, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 136, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 137, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 138, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 139, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 140, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 141, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 142, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 143, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 144, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 145, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 146, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 147, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 148, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 149, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 150, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 151, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 152, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 153, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 154, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 155, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 156, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 157, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 158, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 159, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 160, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 161, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 162, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 163, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 164, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 165, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 166, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 167, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 168, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 169, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 170, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 171, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 172, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 173, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 174, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 175, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 176, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 177, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 178, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 179, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 180, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 181, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 182, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 183, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 184, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 185, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 186, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 187, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 188, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 189, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 190, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 191, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 192, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 193, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 194, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 195, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 196, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 197, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 198, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 199, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 200, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 201, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 202, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 203, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 204, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 205, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 206, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 207, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 208, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 209, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 210, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 211, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 212, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 213, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 214, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 215, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 216, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 217, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 218, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 219, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 220, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 221, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 222, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 223, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 224, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 225, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 226, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 227, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 228, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 229, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 230, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 231, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 232, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 233, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 234, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 235, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 236, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 237, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 238, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 239, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 240, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 241, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 242, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 243, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 244, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 245, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 246, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 247, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 248, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 249, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 250, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 251, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 252, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 253, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 254, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 255, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 256, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 257, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 258, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 259, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 260, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 261, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 262, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 263, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 264, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 265, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 266, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 267, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 268, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 269, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 270, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 271, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 272, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 273, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 274, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 275, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 276, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 277, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 278, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 279, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 280, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 281, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 282, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 283, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 284, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 285, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 286, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 287, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 288, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 289, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 290, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 291, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 292, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 293, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 294, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 295, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 296, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 297, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 298, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 299, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

9100.784269809723
[1.5366737842559814, 2.8666718006134033, 4.291596174240112, 5.712687253952026, 7.121082305908203, 8.561850786209106, 9.963548421859741, 11.386133909225464, 12.807939291000366, 14.236128807067871, 15.677605867385864, 17.092681884765625, 18.537366151809692, 19.970504999160767, 21.39927864074707, 22.81325078010559, 24.27887463569641, 25.677322387695312, 27.059053897857666, 28.519235849380493, 29.945374011993408, 31.3674418926239, 32.83940863609314, 34.282021045684814, 35.720353841781616, 37.1489052772522, 38.56895613670349, 40.02143979072571, 41.437506437301636, 42.88278007507324, 44.32652235031128, 45.74311542510986, 47.15468168258667, 48.58602595329285, 50.025062799453735, 51.44630694389343, 52.87824892997742, 54.29046678543091, 55.71108675003052, 57.11770749092102, 58.53237271308899, 59.949806928634644, 61.36241841316223, 62.78919172286987, 64.21219110488892, 65.66727328300476, 67.07689619064331, 68.47737956047058, 69.90794587135315, 71.27786684036255, 72.6406774520874, 74.00808644294739, 75.35545015335083, 76.74322962760925, 78.15264058113098, 79.5107274055481, 80.87064671516418, 82.26103448867798, 83.67616033554077, 85.03740239143372, 86.46554613113403, 87.8666684627533, 89.27410769462585, 90.61653566360474, 92.05212903022766, 93.46179938316345, 94.88125038146973, 96.30057001113892, 97.67337417602539, 99.07886099815369, 100.51066255569458, 101.89711093902588, 103.27853298187256, 104.6461775302887, 106.00914025306702, 107.40325832366943, 108.76241326332092, 110.12538266181946, 111.55125451087952, 112.93300867080688, 114.29872679710388, 115.67339301109314, 117.03759837150574, 118.44807457923889, 119.80142664909363, 121.16758728027344, 122.55375266075134, 123.92812490463257, 125.30699849128723, 126.74473071098328, 128.1160225868225, 129.51276874542236, 130.90729880332947, 132.31913876533508, 133.71625566482544, 135.0581135749817, 136.40203046798706, 137.7490735054016, 139.11020588874817, 140.469806432724, 141.8232913017273, 143.21946740150452, 144.5806279182434, 145.95236945152283, 147.31610870361328, 148.6856427192688, 150.01800060272217, 151.38005805015564, 152.73196363449097, 154.11093759536743, 155.46598625183105, 156.8154661655426, 158.17094802856445, 159.58304119110107, 160.97983288764954, 162.32864427566528, 163.71656942367554, 165.16622161865234, 166.5486524105072, 167.9546926021576, 169.392169713974, 170.79185438156128, 172.18830728530884, 173.6298999786377, 175.04240822792053, 176.45056676864624, 177.82756733894348, 179.17625999450684, 180.59902453422546, 182.04035639762878, 183.40609121322632, 184.78882002830505, 186.13038682937622, 187.53691697120667, 188.95886516571045, 190.32410550117493, 191.72816801071167, 193.17569541931152, 194.53741097450256, 195.89475083351135, 197.3132348060608, 198.69652318954468, 200.10125756263733, 201.50333714485168, 202.87056255340576, 204.2127435207367, 205.68615818023682, 207.10349869728088, 208.49085402488708, 209.8346767425537, 211.19778108596802, 212.54568600654602, 213.88362884521484, 215.27079939842224, 216.57141208648682, 217.91670274734497, 219.22988200187683, 220.63422417640686, 222.01510643959045, 223.34762525558472, 224.70903491973877, 226.08807682991028, 227.41700387001038, 228.75091934204102, 230.07661175727844, 231.3936107158661, 232.76183485984802, 234.14400362968445, 235.5312077999115, 236.90484261512756, 238.294673204422, 239.548166513443, 240.87534070014954, 242.21238541603088, 243.56258034706116, 244.9276978969574, 246.29232692718506, 247.6572458744049, 249.03148913383484, 250.3860421180725, 251.7495219707489, 253.0948486328125, 254.4532287120819, 255.81922149658203, 257.1822278499603, 258.52848529815674, 259.9061665534973, 261.25897002220154, 262.63507604599, 264.0041913986206, 265.37489557266235, 266.7563588619232, 268.1334090232849, 269.5022859573364, 270.88102746009827, 272.25850534439087, 273.6333131790161, 275.00999999046326, 276.38831853866577, 277.75269627571106, 279.1220872402191, 280.48793601989746, 281.8483214378357, 283.2181046009064, 284.5776424407959, 285.9363293647766, 287.3025395870209, 288.65756845474243, 290.0255491733551, 291.39824748039246, 292.7623836994171, 294.13360929489136, 295.516104221344, 296.88176822662354, 298.2340714931488, 299.6056830883026, 300.96766567230225, 302.3284282684326, 303.6959025859833, 305.06145763397217, 306.4335262775421, 307.80629444122314, 309.1821711063385, 310.5514323711395, 311.9095392227173, 313.2703113555908, 314.648353099823, 316.0184667110443, 317.39555191993713, 318.77214097976685, 320.1519134044647, 321.53074073791504, 322.9155695438385, 324.30657482147217, 325.6908071041107, 327.0730028152466, 328.4523696899414, 329.8202953338623, 331.1945950984955, 332.5621292591095, 333.9388048648834, 335.30636072158813, 336.68026900291443, 338.0519745349884, 339.4270586967468, 340.8039698600769, 342.17240357398987, 343.55408549308777, 344.9418113231659, 346.31665658950806, 347.6968195438385, 349.06084299087524, 350.4318106174469, 351.80166935920715, 353.16631293296814, 354.5272116661072, 355.90533995628357, 357.26539635658264, 358.63330936431885, 360.0023498535156, 361.3679368495941, 362.736839056015, 364.11169695854187, 365.47385478019714, 366.8289496898651, 368.1890730857849, 369.5562047958374, 370.91749477386475, 372.2844030857086, 373.66935110092163, 375.0382168292999, 376.4067587852478, 377.7815010547638, 379.13549304008484, 380.5147624015808, 381.8878102302551, 383.2606327533722, 384.620076417923, 385.9638042449951, 387.3309271335602, 388.68965220451355, 390.04551339149475, 391.4046332836151, 392.7597978115082, 394.1084122657776, 395.4734926223755, 396.8374309539795, 398.1966907978058, 399.5667374134064, 400.9224154949188, 402.29253339767456, 403.66390800476074, 405.0249807834625, 406.39503479003906, 407.7553596496582, 409.11796164512634, 410.4799335002899, 411.8493700027466, 413.2160656452179, 414.5786862373352, 417.3153831958771]
[10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.005, 10.0275, 10.05, 10.085, 10.08, 10.1075, 10.155, 10.27, 10.375, 10.58, 10.855, 11.135, 11.4425, 11.83, 12.2725, 12.43, 12.91, 12.9925, 13.33, 13.61, 13.7525, 13.885, 14.1875, 14.4375, 14.405, 14.4875, 14.6425, 14.6275, 14.725, 14.84, 15.185, 15.18, 15.0775, 15.3075, 15.7575, 15.6975, 15.42, 15.5975, 15.8575, 15.8125, 16.145, 16.3825, 16.6725, 16.7525, 16.7575, 17.1725, 17.7375, 17.675, 17.7575, 17.8625, 18.2375, 18.3175, 18.72, 19.1175, 19.475, 19.7275, 19.7875, 19.89, 19.99, 20.0825, 20.1075, 20.175, 20.3125, 20.425, 20.415, 19.9275, 17.425, 15.9475, 14.28, 13.7675, 12.16, 11.535, 10.49, 10.49, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.251, Test loss: 2.070, Test accuracy: 26.21
Round   1, Train loss: 2.056, Test loss: 1.869, Test accuracy: 32.59
Round   2, Train loss: 1.915, Test loss: 1.713, Test accuracy: 37.52
Round   3, Train loss: 1.824, Test loss: 1.625, Test accuracy: 40.85
Round   4, Train loss: 1.695, Test loss: 1.529, Test accuracy: 44.74
Round   5, Train loss: 1.677, Test loss: 1.501, Test accuracy: 45.09
Round   6, Train loss: 1.616, Test loss: 1.458, Test accuracy: 46.88
Round   7, Train loss: 1.587, Test loss: 1.407, Test accuracy: 49.52
Round   8, Train loss: 1.557, Test loss: 1.394, Test accuracy: 49.55
Round   9, Train loss: 1.488, Test loss: 1.355, Test accuracy: 50.94
Round  10, Train loss: 1.490, Test loss: 1.331, Test accuracy: 51.76
Round  11, Train loss: 1.391, Test loss: 1.306, Test accuracy: 52.59
Round  12, Train loss: 1.372, Test loss: 1.267, Test accuracy: 54.07
Round  13, Train loss: 1.340, Test loss: 1.250, Test accuracy: 55.33
Round  14, Train loss: 1.285, Test loss: 1.230, Test accuracy: 56.03
Round  15, Train loss: 1.331, Test loss: 1.189, Test accuracy: 57.79
Round  16, Train loss: 1.236, Test loss: 1.202, Test accuracy: 57.29
Round  17, Train loss: 1.204, Test loss: 1.172, Test accuracy: 58.86
Round  18, Train loss: 1.235, Test loss: 1.138, Test accuracy: 59.43
Round  19, Train loss: 1.179, Test loss: 1.124, Test accuracy: 59.66
Round  20, Train loss: 1.138, Test loss: 1.124, Test accuracy: 60.09
Round  21, Train loss: 1.132, Test loss: 1.112, Test accuracy: 61.01
Round  22, Train loss: 1.082, Test loss: 1.113, Test accuracy: 60.77
Round  23, Train loss: 1.080, Test loss: 1.083, Test accuracy: 61.55
Round  24, Train loss: 1.058, Test loss: 1.080, Test accuracy: 61.91
Round  25, Train loss: 1.038, Test loss: 1.074, Test accuracy: 62.04
Round  26, Train loss: 1.012, Test loss: 1.078, Test accuracy: 61.88
Round  27, Train loss: 0.943, Test loss: 1.078, Test accuracy: 62.35
Round  28, Train loss: 0.942, Test loss: 1.065, Test accuracy: 62.81
Round  29, Train loss: 0.964, Test loss: 1.054, Test accuracy: 63.91
Round  30, Train loss: 0.902, Test loss: 1.054, Test accuracy: 63.51
Round  31, Train loss: 0.918, Test loss: 1.063, Test accuracy: 63.11
Round  32, Train loss: 0.848, Test loss: 1.053, Test accuracy: 64.09
Round  33, Train loss: 0.901, Test loss: 1.038, Test accuracy: 64.20
Round  34, Train loss: 0.874, Test loss: 1.032, Test accuracy: 64.86
Round  35, Train loss: 0.872, Test loss: 1.016, Test accuracy: 65.03
Round  36, Train loss: 0.812, Test loss: 1.015, Test accuracy: 65.66
Round  37, Train loss: 0.837, Test loss: 1.007, Test accuracy: 65.11
Round  38, Train loss: 0.784, Test loss: 1.016, Test accuracy: 65.02
Round  39, Train loss: 0.780, Test loss: 1.017, Test accuracy: 65.49
Round  40, Train loss: 0.785, Test loss: 1.016, Test accuracy: 65.27
Round  41, Train loss: 0.727, Test loss: 1.017, Test accuracy: 65.49
Round  42, Train loss: 0.755, Test loss: 1.031, Test accuracy: 65.42
Round  43, Train loss: 0.740, Test loss: 1.027, Test accuracy: 65.47
Round  44, Train loss: 0.737, Test loss: 1.030, Test accuracy: 65.88
Round  45, Train loss: 0.701, Test loss: 1.013, Test accuracy: 65.92
Round  46, Train loss: 0.690, Test loss: 1.032, Test accuracy: 66.04
Round  47, Train loss: 0.683, Test loss: 1.030, Test accuracy: 66.47
Round  48, Train loss: 0.670, Test loss: 1.010, Test accuracy: 66.66
Round  49, Train loss: 0.668, Test loss: 1.010, Test accuracy: 66.53
Round  50, Train loss: 0.618, Test loss: 1.023, Test accuracy: 66.28
Round  51, Train loss: 0.668, Test loss: 1.016, Test accuracy: 66.47
Round  52, Train loss: 0.641, Test loss: 1.025, Test accuracy: 67.14
Round  53, Train loss: 0.615, Test loss: 1.023, Test accuracy: 67.02
Round  54, Train loss: 0.625, Test loss: 1.019, Test accuracy: 66.84
Round  55, Train loss: 0.604, Test loss: 1.045, Test accuracy: 66.84
Round  56, Train loss: 0.592, Test loss: 1.042, Test accuracy: 66.89
Round  57, Train loss: 0.575, Test loss: 1.043, Test accuracy: 67.14
Round  58, Train loss: 0.637, Test loss: 1.014, Test accuracy: 66.92
Round  59, Train loss: 0.653, Test loss: 1.003, Test accuracy: 67.63
Round  60, Train loss: 0.569, Test loss: 1.026, Test accuracy: 67.45
Round  61, Train loss: 0.556, Test loss: 1.015, Test accuracy: 67.39
Round  62, Train loss: 0.564, Test loss: 1.050, Test accuracy: 66.76
Round  63, Train loss: 0.503, Test loss: 1.042, Test accuracy: 66.92
Round  64, Train loss: 0.578, Test loss: 1.036, Test accuracy: 67.14
Round  65, Train loss: 0.502, Test loss: 1.051, Test accuracy: 67.33
Round  66, Train loss: 0.483, Test loss: 1.088, Test accuracy: 66.48
Round  67, Train loss: 0.545, Test loss: 1.038, Test accuracy: 67.34
Round  68, Train loss: 0.533, Test loss: 1.044, Test accuracy: 67.78
Round  69, Train loss: 0.514, Test loss: 1.033, Test accuracy: 67.51
Round  70, Train loss: 0.468, Test loss: 1.083, Test accuracy: 67.59
Round  71, Train loss: 0.448, Test loss: 1.072, Test accuracy: 67.94
Round  72, Train loss: 0.494, Test loss: 1.048, Test accuracy: 67.75
Round  73, Train loss: 0.532, Test loss: 1.059, Test accuracy: 67.37
Round  74, Train loss: 0.486, Test loss: 1.079, Test accuracy: 67.92
Round  75, Train loss: 0.457, Test loss: 1.074, Test accuracy: 68.02
Round  76, Train loss: 0.495, Test loss: 1.043, Test accuracy: 68.32
Round  77, Train loss: 0.475, Test loss: 1.065, Test accuracy: 68.13
Round  78, Train loss: 0.444, Test loss: 1.072, Test accuracy: 67.86
Round  79, Train loss: 0.425, Test loss: 1.076, Test accuracy: 67.99
Round  80, Train loss: 0.401, Test loss: 1.105, Test accuracy: 67.83
Round  81, Train loss: 0.500, Test loss: 1.052, Test accuracy: 68.43
Round  82, Train loss: 0.457, Test loss: 1.039, Test accuracy: 68.39
Round  83, Train loss: 0.432, Test loss: 1.040, Test accuracy: 68.66
Round  84, Train loss: 0.457, Test loss: 1.054, Test accuracy: 68.27
Round  85, Train loss: 0.430, Test loss: 1.070, Test accuracy: 68.33
Round  86, Train loss: 0.414, Test loss: 1.070, Test accuracy: 68.86
Round  87, Train loss: 0.408, Test loss: 1.099, Test accuracy: 68.22
Round  88, Train loss: 0.416, Test loss: 1.060, Test accuracy: 68.69
Round  89, Train loss: 0.443, Test loss: 1.057, Test accuracy: 68.79
Round  90, Train loss: 0.391, Test loss: 1.064, Test accuracy: 68.89
Round  91, Train loss: 0.426, Test loss: 1.052, Test accuracy: 69.28
Round  92, Train loss: 0.386, Test loss: 1.076, Test accuracy: 68.71
Round  93, Train loss: 0.381, Test loss: 1.095, Test accuracy: 68.58
Round  94, Train loss: 0.410, Test loss: 1.084, Test accuracy: 68.94
Round  95, Train loss: 0.408, Test loss: 1.085, Test accuracy: 68.83
Round  96, Train loss: 0.402, Test loss: 1.087, Test accuracy: 68.51
Round  97, Train loss: 0.410, Test loss: 1.078, Test accuracy: 68.93
Round  98, Train loss: 0.348, Test loss: 1.101, Test accuracy: 69.09
Round  99, Train loss: 0.372, Test loss: 1.117, Test accuracy: 68.36
Final Round, Train loss: 0.356, Test loss: 1.067, Test accuracy: 69.52
Average accuracy final 10 rounds: 68.81099999999999
2654.6636390686035
[3.391216993331909, 6.510867595672607, 9.627904176712036, 12.758474349975586, 15.405935764312744, 18.05504059791565, 20.710093021392822, 23.37925386428833, 26.018192529678345, 28.668302536010742, 31.316418647766113, 33.96986103057861, 36.617799043655396, 39.26742959022522, 41.911357402801514, 44.52457666397095, 47.16256785392761, 49.79901170730591, 52.448546171188354, 55.09377598762512, 57.75675129890442, 60.41999816894531, 63.062193155288696, 65.70619559288025, 68.3485734462738, 70.98742985725403, 73.62404823303223, 76.26889777183533, 78.92654967308044, 81.58596634864807, 84.21492099761963, 86.87558031082153, 89.51191544532776, 92.14417147636414, 94.76355409622192, 97.3886559009552, 100.02300000190735, 102.65499973297119, 105.28667116165161, 107.93743228912354, 110.58640336990356, 113.25687456130981, 116.2213888168335, 119.17467761039734, 122.14585423469543, 125.09185647964478, 128.04859399795532, 131.0196168422699, 133.98932790756226, 136.91852140426636, 139.86138105392456, 142.80178880691528, 145.472261428833, 148.13433814048767, 150.8150827884674, 153.4735119342804, 156.14671564102173, 158.80476355552673, 161.7787573337555, 164.74611735343933, 167.72637343406677, 170.36703634262085, 173.00465631484985, 175.65180325508118, 178.29632782936096, 180.94431042671204, 183.59679007530212, 186.24911618232727, 188.8688509464264, 191.50142908096313, 194.13728070259094, 196.78619360923767, 199.41509628295898, 202.05049777030945, 204.6861605644226, 207.32844591140747, 209.95688319206238, 212.59271621704102, 215.2356195449829, 217.86853075027466, 220.5061902999878, 223.14267492294312, 225.78175926208496, 228.39532113075256, 231.01201033592224, 233.65180730819702, 236.2832372188568, 238.91540837287903, 241.54478931427002, 244.15342235565186, 246.77290630340576, 249.39795637130737, 252.04078316688538, 254.6743528842926, 257.31438660621643, 259.94207286834717, 262.58381605148315, 265.2210736274719, 267.85962772369385, 270.49908685684204, 273.4669134616852]
[26.2125, 32.5875, 37.5225, 40.85, 44.7425, 45.085, 46.8775, 49.5225, 49.545, 50.935, 51.76, 52.5925, 54.07, 55.325, 56.0275, 57.79, 57.2875, 58.8575, 59.43, 59.665, 60.095, 61.01, 60.7725, 61.555, 61.9125, 62.0425, 61.88, 62.3475, 62.815, 63.9075, 63.5125, 63.11, 64.0875, 64.2025, 64.865, 65.025, 65.66, 65.11, 65.02, 65.4925, 65.2675, 65.49, 65.42, 65.4675, 65.875, 65.925, 66.04, 66.47, 66.6575, 66.535, 66.2775, 66.47, 67.1425, 67.0175, 66.8375, 66.845, 66.8875, 67.1375, 66.9175, 67.63, 67.455, 67.3875, 66.7625, 66.925, 67.145, 67.335, 66.485, 67.345, 67.7825, 67.5125, 67.59, 67.935, 67.755, 67.37, 67.925, 68.0225, 68.3225, 68.1325, 67.855, 67.9925, 67.83, 68.4325, 68.39, 68.6625, 68.2725, 68.33, 68.865, 68.22, 68.695, 68.79, 68.8875, 69.28, 68.71, 68.58, 68.94, 68.825, 68.5125, 68.9275, 69.0875, 68.36, 69.5225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.304, Test loss: 2.247, Test accuracy: 18.90
Round   1, Train loss: 2.193, Test loss: 2.068, Test accuracy: 25.72
Round   2, Train loss: 2.038, Test loss: 1.962, Test accuracy: 29.33
Round   3, Train loss: 1.958, Test loss: 1.875, Test accuracy: 31.92
Round   4, Train loss: 1.872, Test loss: 1.794, Test accuracy: 34.85
Round   5, Train loss: 1.784, Test loss: 1.736, Test accuracy: 36.92
Round   6, Train loss: 1.746, Test loss: 1.670, Test accuracy: 39.51
Round   7, Train loss: 1.686, Test loss: 1.632, Test accuracy: 40.80
Round   8, Train loss: 1.652, Test loss: 1.601, Test accuracy: 42.07
Round   9, Train loss: 1.632, Test loss: 1.553, Test accuracy: 43.90
Round  10, Train loss: 1.592, Test loss: 1.518, Test accuracy: 44.68
Round  11, Train loss: 1.553, Test loss: 1.488, Test accuracy: 45.92
Round  12, Train loss: 1.521, Test loss: 1.472, Test accuracy: 46.67
Round  13, Train loss: 1.493, Test loss: 1.453, Test accuracy: 47.45
Round  14, Train loss: 1.457, Test loss: 1.455, Test accuracy: 48.02
Round  15, Train loss: 1.465, Test loss: 1.426, Test accuracy: 48.77
Round  16, Train loss: 1.425, Test loss: 1.419, Test accuracy: 49.14
Round  17, Train loss: 1.410, Test loss: 1.394, Test accuracy: 49.99
Round  18, Train loss: 1.391, Test loss: 1.371, Test accuracy: 50.89
Round  19, Train loss: 1.403, Test loss: 1.357, Test accuracy: 51.98
Round  20, Train loss: 1.363, Test loss: 1.344, Test accuracy: 51.98
Round  21, Train loss: 1.326, Test loss: 1.331, Test accuracy: 52.19
Round  22, Train loss: 1.333, Test loss: 1.308, Test accuracy: 52.93
Round  23, Train loss: 1.302, Test loss: 1.310, Test accuracy: 52.91
Round  24, Train loss: 1.315, Test loss: 1.282, Test accuracy: 53.99
Round  25, Train loss: 1.242, Test loss: 1.265, Test accuracy: 54.53
Round  26, Train loss: 1.273, Test loss: 1.249, Test accuracy: 55.12
Round  27, Train loss: 1.238, Test loss: 1.248, Test accuracy: 55.31
Round  28, Train loss: 1.217, Test loss: 1.238, Test accuracy: 55.95
Round  29, Train loss: 1.206, Test loss: 1.229, Test accuracy: 56.38
Round  30, Train loss: 1.212, Test loss: 1.209, Test accuracy: 57.17
Round  31, Train loss: 1.170, Test loss: 1.198, Test accuracy: 57.31
Round  32, Train loss: 1.165, Test loss: 1.180, Test accuracy: 57.87
Round  33, Train loss: 1.190, Test loss: 1.167, Test accuracy: 58.48
Round  34, Train loss: 1.140, Test loss: 1.172, Test accuracy: 58.16
Round  35, Train loss: 1.149, Test loss: 1.169, Test accuracy: 58.61
Round  36, Train loss: 1.126, Test loss: 1.153, Test accuracy: 58.74
Round  37, Train loss: 1.099, Test loss: 1.143, Test accuracy: 59.20
Round  38, Train loss: 1.072, Test loss: 1.135, Test accuracy: 59.62
Round  39, Train loss: 1.070, Test loss: 1.125, Test accuracy: 59.88
Round  40, Train loss: 1.060, Test loss: 1.109, Test accuracy: 60.86
Round  41, Train loss: 1.073, Test loss: 1.121, Test accuracy: 60.25
Round  42, Train loss: 1.031, Test loss: 1.126, Test accuracy: 59.65
Round  43, Train loss: 1.080, Test loss: 1.111, Test accuracy: 60.48
Round  44, Train loss: 1.021, Test loss: 1.112, Test accuracy: 60.66
Round  45, Train loss: 1.005, Test loss: 1.096, Test accuracy: 61.27
Round  46, Train loss: 1.046, Test loss: 1.086, Test accuracy: 61.74
Round  47, Train loss: 1.003, Test loss: 1.079, Test accuracy: 62.37
Round  48, Train loss: 0.973, Test loss: 1.081, Test accuracy: 62.13
Round  49, Train loss: 0.994, Test loss: 1.066, Test accuracy: 62.44
Round  50, Train loss: 0.983, Test loss: 1.063, Test accuracy: 62.76
Round  51, Train loss: 0.924, Test loss: 1.063, Test accuracy: 62.81
Round  52, Train loss: 0.971, Test loss: 1.058, Test accuracy: 63.23
Round  53, Train loss: 0.955, Test loss: 1.057, Test accuracy: 62.80
Round  54, Train loss: 0.939, Test loss: 1.061, Test accuracy: 62.75
Round  55, Train loss: 0.906, Test loss: 1.069, Test accuracy: 61.91
Round  56, Train loss: 0.910, Test loss: 1.064, Test accuracy: 62.03
Round  57, Train loss: 0.928, Test loss: 1.046, Test accuracy: 63.16
Round  58, Train loss: 0.884, Test loss: 1.054, Test accuracy: 63.06
Round  59, Train loss: 0.909, Test loss: 1.041, Test accuracy: 63.44
Round  60, Train loss: 0.870, Test loss: 1.034, Test accuracy: 63.41
Round  61, Train loss: 0.845, Test loss: 1.032, Test accuracy: 63.29
Round  62, Train loss: 0.851, Test loss: 1.036, Test accuracy: 63.27
Round  63, Train loss: 0.873, Test loss: 1.038, Test accuracy: 63.18
Round  64, Train loss: 0.874, Test loss: 1.035, Test accuracy: 63.65
Round  65, Train loss: 0.837, Test loss: 1.039, Test accuracy: 63.55
Round  66, Train loss: 0.814, Test loss: 1.008, Test accuracy: 64.75
Round  67, Train loss: 0.833, Test loss: 1.013, Test accuracy: 64.89
Round  68, Train loss: 0.834, Test loss: 1.000, Test accuracy: 65.17
Round  69, Train loss: 0.808, Test loss: 0.998, Test accuracy: 65.44
Round  70, Train loss: 0.781, Test loss: 1.014, Test accuracy: 64.78
Round  71, Train loss: 0.819, Test loss: 1.005, Test accuracy: 64.86
Round  72, Train loss: 0.805, Test loss: 0.994, Test accuracy: 65.48
Round  73, Train loss: 0.786, Test loss: 0.999, Test accuracy: 65.58
Round  74, Train loss: 0.806, Test loss: 0.995, Test accuracy: 65.78
Round  75, Train loss: 0.769, Test loss: 0.987, Test accuracy: 65.72
Round  76, Train loss: 0.797, Test loss: 0.984, Test accuracy: 66.02
Round  77, Train loss: 0.768, Test loss: 0.979, Test accuracy: 66.11
Round  78, Train loss: 0.755, Test loss: 0.990, Test accuracy: 65.83
Round  79, Train loss: 0.767, Test loss: 0.975, Test accuracy: 66.25
Round  80, Train loss: 0.742, Test loss: 0.976, Test accuracy: 65.95
Round  81, Train loss: 0.769, Test loss: 0.981, Test accuracy: 66.21
Round  82, Train loss: 0.717, Test loss: 0.971, Test accuracy: 66.72
Round  83, Train loss: 0.725, Test loss: 0.971, Test accuracy: 66.46
Round  84, Train loss: 0.716, Test loss: 0.969, Test accuracy: 66.65
Round  85, Train loss: 0.720, Test loss: 0.969, Test accuracy: 66.78
Round  86, Train loss: 0.679, Test loss: 0.978, Test accuracy: 66.63
Round  87, Train loss: 0.698, Test loss: 0.980, Test accuracy: 66.67
Round  88, Train loss: 0.723, Test loss: 0.986, Test accuracy: 66.44
Round  89, Train loss: 0.687, Test loss: 0.983, Test accuracy: 66.31
Round  90, Train loss: 0.667, Test loss: 0.985, Test accuracy: 66.47
Round  91, Train loss: 0.686, Test loss: 0.974, Test accuracy: 66.64
Round  92, Train loss: 0.684, Test loss: 0.985, Test accuracy: 66.12
Round  93, Train loss: 0.676, Test loss: 0.969, Test accuracy: 67.17
Round  94, Train loss: 0.700, Test loss: 0.958, Test accuracy: 67.28
Round  95, Train loss: 0.640, Test loss: 0.963, Test accuracy: 67.19
Round  96, Train loss: 0.647, Test loss: 0.977, Test accuracy: 66.70
Round  97, Train loss: 0.688, Test loss: 0.965, Test accuracy: 67.03
Round  98, Train loss: 0.635, Test loss: 0.972, Test accuracy: 67.14
Round  99, Train loss: 0.654, Test loss: 0.967, Test accuracy: 67.03
Final Round, Train loss: 0.557, Test loss: 0.964, Test accuracy: 67.29
Average accuracy final 10 rounds: 66.87875
1787.9132134914398
[1.6487712860107422, 2.9931936264038086, 4.352847576141357, 5.681507349014282, 7.004239559173584, 8.337772846221924, 9.656641960144043, 10.949867725372314, 12.293158531188965, 13.641947031021118, 14.952504396438599, 16.314541816711426, 17.646867752075195, 18.96052646636963, 20.305304527282715, 21.624603986740112, 22.92322087287903, 24.249229192733765, 25.574799060821533, 26.875356674194336, 28.206486225128174, 29.556062936782837, 30.883919715881348, 32.210134983062744, 33.53318643569946, 34.85143041610718, 36.20713663101196, 37.55802369117737, 38.88243341445923, 40.23321747779846, 41.5705041885376, 42.875213861465454, 44.21522903442383, 45.57059836387634, 46.871559858322144, 48.1946485042572, 49.53984594345093, 50.8627655506134, 52.213727951049805, 53.56584167480469, 54.893717527389526, 56.243042945861816, 57.603447675704956, 58.9409601688385, 60.292325496673584, 61.647855281829834, 62.96824312210083, 64.29674100875854, 65.63834547996521, 66.97518134117126, 68.31615352630615, 69.66333174705505, 71.02466917037964, 72.36432194709778, 73.69875812530518, 75.03395104408264, 76.37981724739075, 77.72036457061768, 79.06035327911377, 80.40453624725342, 81.7296953201294, 83.06903409957886, 84.40116167068481, 85.70294737815857, 86.92602252960205, 88.15572428703308, 89.46191883087158, 90.79407835006714, 92.12492537498474, 93.4205846786499, 94.75116395950317, 96.07460689544678, 97.3797960281372, 98.58751320838928, 99.78058099746704, 100.97178339958191, 102.1799008846283, 103.3711428642273, 104.56792187690735, 105.77842330932617, 106.97197484970093, 108.1678192615509, 109.37505674362183, 110.56742310523987, 111.76087307929993, 112.96550512313843, 114.15499758720398, 115.35108423233032, 116.55656051635742, 117.74582052230835, 118.94256353378296, 120.1340684890747, 121.32332563400269, 122.53498864173889, 123.73027658462524, 124.9255063533783, 126.12433052062988, 127.3152494430542, 128.51235818862915, 129.71503710746765, 131.6641001701355]
[18.9, 25.7225, 29.33, 31.9225, 34.8475, 36.9225, 39.51, 40.7975, 42.0675, 43.9, 44.6775, 45.92, 46.6725, 47.4475, 48.0175, 48.7725, 49.1375, 49.99, 50.8875, 51.98, 51.975, 52.1925, 52.9325, 52.915, 53.995, 54.5325, 55.12, 55.3125, 55.9525, 56.385, 57.1725, 57.31, 57.8675, 58.475, 58.1625, 58.61, 58.74, 59.1975, 59.62, 59.88, 60.86, 60.25, 59.6525, 60.48, 60.6625, 61.27, 61.745, 62.3725, 62.1325, 62.4375, 62.7625, 62.815, 63.2325, 62.795, 62.7475, 61.915, 62.0325, 63.165, 63.0575, 63.435, 63.415, 63.2925, 63.2675, 63.1775, 63.6475, 63.55, 64.75, 64.8925, 65.175, 65.445, 64.775, 64.8575, 65.48, 65.5775, 65.78, 65.7225, 66.0225, 66.115, 65.825, 66.2525, 65.9525, 66.2125, 66.715, 66.4575, 66.6525, 66.7775, 66.6325, 66.67, 66.435, 66.3125, 66.475, 66.6375, 66.12, 67.1725, 67.2825, 67.1925, 66.705, 67.0325, 67.145, 67.025, 67.29]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.303, Test loss: 2.216, Test accuracy: 20.67
Round   1, Train loss: 2.158, Test loss: 2.023, Test accuracy: 27.61
Round   2, Train loss: 2.023, Test loss: 1.934, Test accuracy: 30.50
Round   3, Train loss: 1.934, Test loss: 1.825, Test accuracy: 34.42
Round   4, Train loss: 1.844, Test loss: 1.757, Test accuracy: 36.93
Round   5, Train loss: 1.775, Test loss: 1.713, Test accuracy: 38.43
Round   6, Train loss: 1.722, Test loss: 1.652, Test accuracy: 40.42
Round   7, Train loss: 1.687, Test loss: 1.628, Test accuracy: 40.94
Round   8, Train loss: 1.679, Test loss: 1.580, Test accuracy: 43.20
Round   9, Train loss: 1.624, Test loss: 1.578, Test accuracy: 42.69
Round  10, Train loss: 1.628, Test loss: 1.524, Test accuracy: 44.66
Round  11, Train loss: 1.588, Test loss: 1.495, Test accuracy: 45.52
Round  12, Train loss: 1.541, Test loss: 1.466, Test accuracy: 47.01
Round  13, Train loss: 1.503, Test loss: 1.448, Test accuracy: 47.82
Round  14, Train loss: 1.483, Test loss: 1.430, Test accuracy: 48.55
Round  15, Train loss: 1.496, Test loss: 1.408, Test accuracy: 48.82
Round  16, Train loss: 1.457, Test loss: 1.398, Test accuracy: 49.99
Round  17, Train loss: 1.431, Test loss: 1.367, Test accuracy: 50.53
Round  18, Train loss: 1.413, Test loss: 1.354, Test accuracy: 51.73
Round  19, Train loss: 1.397, Test loss: 1.351, Test accuracy: 52.09
Round  20, Train loss: 1.394, Test loss: 1.320, Test accuracy: 53.24
Round  21, Train loss: 1.347, Test loss: 1.300, Test accuracy: 54.14
Round  22, Train loss: 1.329, Test loss: 1.280, Test accuracy: 54.72
Round  23, Train loss: 1.287, Test loss: 1.268, Test accuracy: 55.14
Round  24, Train loss: 1.280, Test loss: 1.255, Test accuracy: 55.77
Round  25, Train loss: 1.260, Test loss: 1.257, Test accuracy: 55.33
Round  26, Train loss: 1.256, Test loss: 1.241, Test accuracy: 56.14
Round  27, Train loss: 1.263, Test loss: 1.227, Test accuracy: 56.44
Round  28, Train loss: 1.259, Test loss: 1.217, Test accuracy: 56.74
Round  29, Train loss: 1.213, Test loss: 1.204, Test accuracy: 57.15
Round  30, Train loss: 1.173, Test loss: 1.211, Test accuracy: 57.52
Round  31, Train loss: 1.160, Test loss: 1.182, Test accuracy: 58.47
Round  32, Train loss: 1.170, Test loss: 1.173, Test accuracy: 59.00
Round  33, Train loss: 1.154, Test loss: 1.169, Test accuracy: 58.65
Round  34, Train loss: 1.164, Test loss: 1.142, Test accuracy: 59.83
Round  35, Train loss: 1.120, Test loss: 1.147, Test accuracy: 59.20
Round  36, Train loss: 1.120, Test loss: 1.132, Test accuracy: 60.12
Round  37, Train loss: 1.096, Test loss: 1.142, Test accuracy: 59.64
Round  38, Train loss: 1.116, Test loss: 1.117, Test accuracy: 60.49
Round  39, Train loss: 1.063, Test loss: 1.118, Test accuracy: 60.72
Round  40, Train loss: 1.079, Test loss: 1.098, Test accuracy: 61.37
Round  41, Train loss: 1.068, Test loss: 1.105, Test accuracy: 61.26
Round  42, Train loss: 1.081, Test loss: 1.100, Test accuracy: 61.38
Round  43, Train loss: 1.047, Test loss: 1.104, Test accuracy: 61.45
Round  44, Train loss: 1.041, Test loss: 1.091, Test accuracy: 61.17
Round  45, Train loss: 1.031, Test loss: 1.086, Test accuracy: 61.91
Round  46, Train loss: 1.009, Test loss: 1.093, Test accuracy: 61.79
Round  47, Train loss: 0.998, Test loss: 1.093, Test accuracy: 61.95
Round  48, Train loss: 0.959, Test loss: 1.087, Test accuracy: 62.10
Round  49, Train loss: 0.972, Test loss: 1.047, Test accuracy: 63.72
Round  50, Train loss: 0.944, Test loss: 1.046, Test accuracy: 63.60
Round  51, Train loss: 0.943, Test loss: 1.039, Test accuracy: 64.18
Round  52, Train loss: 0.946, Test loss: 1.041, Test accuracy: 63.79
Round  53, Train loss: 0.910, Test loss: 1.030, Test accuracy: 64.10
Round  54, Train loss: 0.914, Test loss: 1.029, Test accuracy: 64.32
Round  55, Train loss: 0.893, Test loss: 1.027, Test accuracy: 64.49
Round  56, Train loss: 0.896, Test loss: 1.024, Test accuracy: 64.63
Round  57, Train loss: 0.871, Test loss: 1.020, Test accuracy: 64.64
Round  58, Train loss: 0.900, Test loss: 1.029, Test accuracy: 64.50
Round  59, Train loss: 0.915, Test loss: 1.014, Test accuracy: 64.94
Round  60, Train loss: 0.906, Test loss: 1.010, Test accuracy: 64.99
Round  61, Train loss: 0.877, Test loss: 1.016, Test accuracy: 64.91
Round  62, Train loss: 0.846, Test loss: 1.015, Test accuracy: 65.04
Round  63, Train loss: 0.832, Test loss: 1.007, Test accuracy: 65.23
Round  64, Train loss: 0.833, Test loss: 1.002, Test accuracy: 65.44
Round  65, Train loss: 0.818, Test loss: 1.006, Test accuracy: 65.17
Round  66, Train loss: 0.830, Test loss: 1.003, Test accuracy: 65.56
Round  67, Train loss: 0.840, Test loss: 1.001, Test accuracy: 65.37
Round  68, Train loss: 0.849, Test loss: 1.003, Test accuracy: 65.20
Round  69, Train loss: 0.795, Test loss: 1.000, Test accuracy: 65.72
Round  70, Train loss: 0.780, Test loss: 0.989, Test accuracy: 66.05
Round  71, Train loss: 0.834, Test loss: 0.995, Test accuracy: 65.52
Round  72, Train loss: 0.756, Test loss: 1.000, Test accuracy: 65.76
Round  73, Train loss: 0.757, Test loss: 0.997, Test accuracy: 65.52
Round  74, Train loss: 0.773, Test loss: 0.993, Test accuracy: 65.75
Round  75, Train loss: 0.762, Test loss: 1.002, Test accuracy: 65.49
Round  76, Train loss: 0.782, Test loss: 0.993, Test accuracy: 65.98
Round  77, Train loss: 0.800, Test loss: 0.982, Test accuracy: 66.28
Round  78, Train loss: 0.757, Test loss: 0.979, Test accuracy: 66.61
Round  79, Train loss: 0.742, Test loss: 0.996, Test accuracy: 65.96
Round  80, Train loss: 0.713, Test loss: 0.999, Test accuracy: 65.91
Round  81, Train loss: 0.734, Test loss: 0.995, Test accuracy: 65.83
Round  82, Train loss: 0.733, Test loss: 0.987, Test accuracy: 66.03
Round  83, Train loss: 0.694, Test loss: 0.990, Test accuracy: 66.23
Round  84, Train loss: 0.677, Test loss: 1.000, Test accuracy: 65.78
Round  85, Train loss: 0.685, Test loss: 0.994, Test accuracy: 66.00
Round  86, Train loss: 0.685, Test loss: 0.983, Test accuracy: 66.28
Round  87, Train loss: 0.669, Test loss: 0.972, Test accuracy: 66.78
Round  88, Train loss: 0.659, Test loss: 0.987, Test accuracy: 66.28
Round  89, Train loss: 0.678, Test loss: 0.976, Test accuracy: 66.53
Round  90, Train loss: 0.701, Test loss: 0.965, Test accuracy: 67.08
Round  91, Train loss: 0.669, Test loss: 0.968, Test accuracy: 67.11
Round  92, Train loss: 0.725, Test loss: 0.977, Test accuracy: 66.77
Round  93, Train loss: 0.666, Test loss: 0.962, Test accuracy: 66.97
Round  94, Train loss: 0.628, Test loss: 0.958, Test accuracy: 67.37
Round  95, Train loss: 0.681, Test loss: 0.970, Test accuracy: 67.08
Round  96, Train loss: 0.659, Test loss: 0.958, Test accuracy: 67.81
Round  97, Train loss: 0.645, Test loss: 0.962, Test accuracy: 67.58
Round  98, Train loss: 0.636, Test loss: 0.957, Test accuracy: 67.46
Round  99, Train loss: 0.588, Test loss: 0.962, Test accuracy: 67.22
Final Round, Train loss: 0.564, Test loss: 0.963, Test accuracy: 67.47
Average accuracy final 10 rounds: 67.245
1787.5808215141296
[1.7045114040374756, 3.0415074825286865, 4.397817373275757, 5.7288007736206055, 7.039016246795654, 8.396040439605713, 9.760920524597168, 11.071078538894653, 12.421535015106201, 13.750141143798828, 15.05875849723816, 16.409377574920654, 17.738054513931274, 19.074547290802002, 20.429669857025146, 21.771230220794678, 23.11348247528076, 24.463698148727417, 25.820094347000122, 27.176501035690308, 28.53590178489685, 29.889007568359375, 31.246040105819702, 32.60482454299927, 33.966991901397705, 35.31806778907776, 36.68272805213928, 38.04035520553589, 39.38191032409668, 40.74312400817871, 42.096261739730835, 43.43114519119263, 44.79427218437195, 46.15520358085632, 47.500895738601685, 48.85998773574829, 50.21801710128784, 51.4126091003418, 52.61147451400757, 53.81813073158264, 55.01661419868469, 56.21754336357117, 57.426552295684814, 58.623146057128906, 59.821346044540405, 61.01857805252075, 62.21694231033325, 63.41817545890808, 64.61811828613281, 65.81904125213623, 67.0335328578949, 68.23303389549255, 69.44347357749939, 70.65883898735046, 71.85581588745117, 73.0666024684906, 74.27500009536743, 75.47600150108337, 76.69155311584473, 77.89349842071533, 79.09521460533142, 80.30345726013184, 81.5046980381012, 82.70581483840942, 83.92082023620605, 85.11958146095276, 86.33503651618958, 87.54934740066528, 88.748370885849, 89.965651512146, 91.18344950675964, 92.38312578201294, 93.59428930282593, 94.79888105392456, 96.0009834766388, 97.2142424583435, 98.41900038719177, 99.62119770050049, 100.85800313949585, 102.0621235370636, 103.26216959953308, 104.46286916732788, 105.66848921775818, 106.88472199440002, 108.0910906791687, 109.29487895965576, 110.50584650039673, 111.70528435707092, 112.90531802177429, 114.12299036979675, 115.32083010673523, 116.52434206008911, 117.73059701919556, 118.92855262756348, 120.13356184959412, 121.34280514717102, 122.54140448570251, 123.7418282032013, 124.95524787902832, 126.16196203231812, 128.1443738937378]
[20.67, 27.6125, 30.4975, 34.4175, 36.93, 38.4275, 40.4175, 40.94, 43.195, 42.685, 44.665, 45.525, 47.01, 47.8225, 48.55, 48.8225, 49.99, 50.5275, 51.725, 52.095, 53.2425, 54.1375, 54.7225, 55.14, 55.7725, 55.325, 56.1375, 56.4425, 56.74, 57.145, 57.5175, 58.465, 59.0, 58.6475, 59.8325, 59.1975, 60.1225, 59.6425, 60.4875, 60.715, 61.37, 61.2575, 61.385, 61.4475, 61.175, 61.9075, 61.7925, 61.9475, 62.0975, 63.7225, 63.6025, 64.1825, 63.7875, 64.0975, 64.32, 64.4925, 64.6325, 64.6425, 64.5, 64.9425, 64.99, 64.9125, 65.04, 65.2275, 65.445, 65.1725, 65.5625, 65.37, 65.2025, 65.72, 66.0475, 65.52, 65.7575, 65.52, 65.7475, 65.49, 65.985, 66.285, 66.605, 65.9625, 65.905, 65.835, 66.0325, 66.2325, 65.7825, 66.0, 66.28, 66.785, 66.285, 66.5275, 67.085, 67.105, 66.765, 66.97, 67.37, 67.075, 67.815, 67.5825, 67.46, 67.2225, 67.47]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.965, Test loss: 1.055, Test accuracy: 39.90 

Round   0, Global train loss: 0.965, Global test loss: 1.121, Global test accuracy: 34.55 

Round   1, Train loss: 0.762, Test loss: 0.969, Test accuracy: 51.12 

Round   1, Global train loss: 0.762, Global test loss: 1.191, Global test accuracy: 37.68 

Round   2, Train loss: 0.661, Test loss: 0.843, Test accuracy: 57.25 

Round   2, Global train loss: 0.661, Global test loss: 1.120, Global test accuracy: 38.59 

Round   3, Train loss: 0.687, Test loss: 0.822, Test accuracy: 58.29 

Round   3, Global train loss: 0.687, Global test loss: 1.160, Global test accuracy: 33.71 

Round   4, Train loss: 0.704, Test loss: 0.806, Test accuracy: 61.25 

Round   4, Global train loss: 0.704, Global test loss: 1.233, Global test accuracy: 34.51 

Round   5, Train loss: 0.623, Test loss: 0.722, Test accuracy: 65.13 

Round   5, Global train loss: 0.623, Global test loss: 1.147, Global test accuracy: 35.77 

Round   6, Train loss: 0.559, Test loss: 0.670, Test accuracy: 69.73 

Round   6, Global train loss: 0.559, Global test loss: 1.120, Global test accuracy: 38.48 

Round   7, Train loss: 0.566, Test loss: 0.648, Test accuracy: 71.42 

Round   7, Global train loss: 0.566, Global test loss: 1.160, Global test accuracy: 35.42 

Round   8, Train loss: 0.574, Test loss: 0.627, Test accuracy: 72.29 

Round   8, Global train loss: 0.574, Global test loss: 1.247, Global test accuracy: 36.02 

Round   9, Train loss: 0.488, Test loss: 0.619, Test accuracy: 73.18 

Round   9, Global train loss: 0.488, Global test loss: 1.329, Global test accuracy: 38.83 

Round  10, Train loss: 0.472, Test loss: 0.631, Test accuracy: 72.61 

Round  10, Global train loss: 0.472, Global test loss: 1.224, Global test accuracy: 37.75 

Round  11, Train loss: 0.461, Test loss: 0.619, Test accuracy: 72.19 

Round  11, Global train loss: 0.461, Global test loss: 1.133, Global test accuracy: 35.25 

Round  12, Train loss: 0.440, Test loss: 0.613, Test accuracy: 73.30 

Round  12, Global train loss: 0.440, Global test loss: 1.244, Global test accuracy: 39.15 

Round  13, Train loss: 0.335, Test loss: 0.591, Test accuracy: 74.03 

Round  13, Global train loss: 0.335, Global test loss: 1.148, Global test accuracy: 35.79 

Round  14, Train loss: 0.407, Test loss: 0.587, Test accuracy: 74.18 

Round  14, Global train loss: 0.407, Global test loss: 1.424, Global test accuracy: 31.38 

Round  15, Train loss: 0.375, Test loss: 0.594, Test accuracy: 74.58 

Round  15, Global train loss: 0.375, Global test loss: 1.271, Global test accuracy: 37.86 

Round  16, Train loss: 0.501, Test loss: 0.599, Test accuracy: 73.92 

Round  16, Global train loss: 0.501, Global test loss: 1.239, Global test accuracy: 37.50 

Round  17, Train loss: 0.439, Test loss: 0.571, Test accuracy: 75.97 

Round  17, Global train loss: 0.439, Global test loss: 1.154, Global test accuracy: 35.62 

Round  18, Train loss: 0.360, Test loss: 0.592, Test accuracy: 74.56 

Round  18, Global train loss: 0.360, Global test loss: 1.164, Global test accuracy: 34.46 

Round  19, Train loss: 0.341, Test loss: 0.626, Test accuracy: 74.94 

Round  19, Global train loss: 0.341, Global test loss: 1.387, Global test accuracy: 39.19 

Round  20, Train loss: 0.360, Test loss: 0.605, Test accuracy: 74.37 

Round  20, Global train loss: 0.360, Global test loss: 1.233, Global test accuracy: 34.83 

Round  21, Train loss: 0.401, Test loss: 0.578, Test accuracy: 76.30 

Round  21, Global train loss: 0.401, Global test loss: 1.216, Global test accuracy: 39.12 

Round  22, Train loss: 0.409, Test loss: 0.542, Test accuracy: 78.25 

Round  22, Global train loss: 0.409, Global test loss: 1.180, Global test accuracy: 34.74 

Round  23, Train loss: 0.326, Test loss: 0.545, Test accuracy: 78.09 

Round  23, Global train loss: 0.326, Global test loss: 1.289, Global test accuracy: 35.52 

Round  24, Train loss: 0.294, Test loss: 0.547, Test accuracy: 78.38 

Round  24, Global train loss: 0.294, Global test loss: 1.310, Global test accuracy: 37.58 

Round  25, Train loss: 0.343, Test loss: 0.545, Test accuracy: 78.62 

Round  25, Global train loss: 0.343, Global test loss: 1.159, Global test accuracy: 38.30 

Round  26, Train loss: 0.298, Test loss: 0.556, Test accuracy: 79.12 

Round  26, Global train loss: 0.298, Global test loss: 1.220, Global test accuracy: 32.45 

Round  27, Train loss: 0.432, Test loss: 0.555, Test accuracy: 79.03 

Round  27, Global train loss: 0.432, Global test loss: 1.159, Global test accuracy: 33.82 

Round  28, Train loss: 0.429, Test loss: 0.559, Test accuracy: 79.46 

Round  28, Global train loss: 0.429, Global test loss: 1.204, Global test accuracy: 34.77 

Round  29, Train loss: 0.279, Test loss: 0.552, Test accuracy: 79.88 

Round  29, Global train loss: 0.279, Global test loss: 1.220, Global test accuracy: 35.42 

Round  30, Train loss: 0.320, Test loss: 0.561, Test accuracy: 79.72 

Round  30, Global train loss: 0.320, Global test loss: 1.308, Global test accuracy: 30.61 

Round  31, Train loss: 0.346, Test loss: 0.558, Test accuracy: 79.62 

Round  31, Global train loss: 0.346, Global test loss: 1.426, Global test accuracy: 38.34 

Round  32, Train loss: 0.251, Test loss: 0.556, Test accuracy: 79.92 

Round  32, Global train loss: 0.251, Global test loss: 1.352, Global test accuracy: 38.59 

Round  33, Train loss: 0.227, Test loss: 0.588, Test accuracy: 79.62 

Round  33, Global train loss: 0.227, Global test loss: 1.261, Global test accuracy: 35.35 

Round  34, Train loss: 0.231, Test loss: 0.614, Test accuracy: 79.32 

Round  34, Global train loss: 0.231, Global test loss: 1.449, Global test accuracy: 38.51 

Round  35, Train loss: 0.227, Test loss: 0.623, Test accuracy: 79.28 

Round  35, Global train loss: 0.227, Global test loss: 1.298, Global test accuracy: 33.23 

Round  36, Train loss: 0.219, Test loss: 0.622, Test accuracy: 79.42 

Round  36, Global train loss: 0.219, Global test loss: 1.354, Global test accuracy: 35.96 

Round  37, Train loss: 0.271, Test loss: 0.613, Test accuracy: 79.85 

Round  37, Global train loss: 0.271, Global test loss: 1.228, Global test accuracy: 39.27 

Round  38, Train loss: 0.148, Test loss: 0.611, Test accuracy: 79.85 

Round  38, Global train loss: 0.148, Global test loss: 1.307, Global test accuracy: 38.98 

Round  39, Train loss: 0.188, Test loss: 0.610, Test accuracy: 79.96 

Round  39, Global train loss: 0.188, Global test loss: 1.171, Global test accuracy: 38.60 

Round  40, Train loss: 0.239, Test loss: 0.596, Test accuracy: 80.33 

Round  40, Global train loss: 0.239, Global test loss: 1.476, Global test accuracy: 32.97 

Round  41, Train loss: 0.182, Test loss: 0.615, Test accuracy: 80.72 

Round  41, Global train loss: 0.182, Global test loss: 1.207, Global test accuracy: 36.90 

Round  42, Train loss: 0.210, Test loss: 0.618, Test accuracy: 80.71 

Round  42, Global train loss: 0.210, Global test loss: 1.578, Global test accuracy: 37.70 

Round  43, Train loss: 0.153, Test loss: 0.644, Test accuracy: 80.03 

Round  43, Global train loss: 0.153, Global test loss: 1.417, Global test accuracy: 38.28 

Round  44, Train loss: 0.190, Test loss: 0.660, Test accuracy: 80.12 

Round  44, Global train loss: 0.190, Global test loss: 1.328, Global test accuracy: 32.98 

Round  45, Train loss: 0.201, Test loss: 0.662, Test accuracy: 79.96 

Round  45, Global train loss: 0.201, Global test loss: 1.276, Global test accuracy: 38.32 

Round  46, Train loss: 0.228, Test loss: 0.671, Test accuracy: 80.25 

Round  46, Global train loss: 0.228, Global test loss: 1.213, Global test accuracy: 33.64 

Round  47, Train loss: 0.177, Test loss: 0.674, Test accuracy: 80.26 

Round  47, Global train loss: 0.177, Global test loss: 1.285, Global test accuracy: 34.65 

Round  48, Train loss: 0.197, Test loss: 0.690, Test accuracy: 80.06 

Round  48, Global train loss: 0.197, Global test loss: 1.302, Global test accuracy: 37.59 

Round  49, Train loss: 0.146, Test loss: 0.686, Test accuracy: 80.24 

Round  49, Global train loss: 0.146, Global test loss: 1.175, Global test accuracy: 39.59 

Round  50, Train loss: 0.170, Test loss: 0.691, Test accuracy: 80.06 

Round  50, Global train loss: 0.170, Global test loss: 1.286, Global test accuracy: 37.23 

Round  51, Train loss: 0.195, Test loss: 0.717, Test accuracy: 79.62 

Round  51, Global train loss: 0.195, Global test loss: 1.405, Global test accuracy: 36.98 

Round  52, Train loss: 0.184, Test loss: 0.700, Test accuracy: 79.86 

Round  52, Global train loss: 0.184, Global test loss: 1.310, Global test accuracy: 38.17 

Round  53, Train loss: 0.094, Test loss: 0.720, Test accuracy: 79.94 

Round  53, Global train loss: 0.094, Global test loss: 1.347, Global test accuracy: 32.33 

Round  54, Train loss: 0.110, Test loss: 0.734, Test accuracy: 79.90 

Round  54, Global train loss: 0.110, Global test loss: 1.704, Global test accuracy: 38.23 

Round  55, Train loss: 0.126, Test loss: 0.761, Test accuracy: 80.00 

Round  55, Global train loss: 0.126, Global test loss: 1.453, Global test accuracy: 38.67 

Round  56, Train loss: 0.153, Test loss: 0.765, Test accuracy: 80.19 

Round  56, Global train loss: 0.153, Global test loss: 1.322, Global test accuracy: 34.70 

Round  57, Train loss: 0.192, Test loss: 0.773, Test accuracy: 80.04 

Round  57, Global train loss: 0.192, Global test loss: 1.232, Global test accuracy: 36.15 

Round  58, Train loss: 0.123, Test loss: 0.762, Test accuracy: 80.49 

Round  58, Global train loss: 0.123, Global test loss: 1.421, Global test accuracy: 31.67 

Round  59, Train loss: 0.142, Test loss: 0.779, Test accuracy: 80.07 

Round  59, Global train loss: 0.142, Global test loss: 1.486, Global test accuracy: 38.44 

Round  60, Train loss: 0.095, Test loss: 0.734, Test accuracy: 80.40 

Round  60, Global train loss: 0.095, Global test loss: 1.938, Global test accuracy: 37.91 

Round  61, Train loss: 0.155, Test loss: 0.743, Test accuracy: 80.44 

Round  61, Global train loss: 0.155, Global test loss: 1.214, Global test accuracy: 39.23 

Round  62, Train loss: 0.089, Test loss: 0.772, Test accuracy: 80.06 

Round  62, Global train loss: 0.089, Global test loss: 1.578, Global test accuracy: 33.31 

Round  63, Train loss: 0.100, Test loss: 0.776, Test accuracy: 80.49 

Round  63, Global train loss: 0.100, Global test loss: 1.380, Global test accuracy: 33.98 

Round  64, Train loss: 0.139, Test loss: 0.795, Test accuracy: 80.32 

Round  64, Global train loss: 0.139, Global test loss: 1.305, Global test accuracy: 37.40 

Round  65, Train loss: 0.119, Test loss: 0.749, Test accuracy: 80.91 

Round  65, Global train loss: 0.119, Global test loss: 1.468, Global test accuracy: 34.31 

Round  66, Train loss: 0.098, Test loss: 0.767, Test accuracy: 80.65 

Round  66, Global train loss: 0.098, Global test loss: 1.723, Global test accuracy: 39.08 

Round  67, Train loss: 0.113, Test loss: 0.757, Test accuracy: 81.24 

Round  67, Global train loss: 0.113, Global test loss: 1.447, Global test accuracy: 37.57 

Round  68, Train loss: 0.072, Test loss: 0.784, Test accuracy: 81.07 

Round  68, Global train loss: 0.072, Global test loss: 1.220, Global test accuracy: 37.51 

Round  69, Train loss: 0.120, Test loss: 0.811, Test accuracy: 81.17 

Round  69, Global train loss: 0.120, Global test loss: 1.348, Global test accuracy: 33.27 

Round  70, Train loss: 0.083, Test loss: 0.812, Test accuracy: 81.22 

Round  70, Global train loss: 0.083, Global test loss: 1.645, Global test accuracy: 39.48 

Round  71, Train loss: 0.089, Test loss: 0.821, Test accuracy: 80.85 

Round  71, Global train loss: 0.089, Global test loss: 1.300, Global test accuracy: 38.20 

Round  72, Train loss: 0.100, Test loss: 0.801, Test accuracy: 80.58 

Round  72, Global train loss: 0.100, Global test loss: 1.340, Global test accuracy: 33.28 

Round  73, Train loss: 0.063, Test loss: 0.795, Test accuracy: 80.61 

Round  73, Global train loss: 0.063, Global test loss: 1.264, Global test accuracy: 37.56 

Round  74, Train loss: 0.105, Test loss: 0.806, Test accuracy: 80.63 

Round  74, Global train loss: 0.105, Global test loss: 1.200, Global test accuracy: 37.11 

Round  75, Train loss: 0.110, Test loss: 0.835, Test accuracy: 80.56 

Round  75, Global train loss: 0.110, Global test loss: 1.190, Global test accuracy: 37.67 

Round  76, Train loss: 0.118, Test loss: 0.817, Test accuracy: 80.89 

Round  76, Global train loss: 0.118, Global test loss: 1.903, Global test accuracy: 39.49 

Round  77, Train loss: 0.040, Test loss: 0.832, Test accuracy: 80.92 

Round  77, Global train loss: 0.040, Global test loss: 1.367, Global test accuracy: 39.21 

Round  78, Train loss: 0.076, Test loss: 0.841, Test accuracy: 80.89 

Round  78, Global train loss: 0.076, Global test loss: 1.351, Global test accuracy: 37.54 

Round  79, Train loss: 0.073, Test loss: 0.856, Test accuracy: 80.71 

Round  79, Global train loss: 0.073, Global test loss: 1.253, Global test accuracy: 35.70 

Round  80, Train loss: 0.060, Test loss: 0.856, Test accuracy: 80.82 

Round  80, Global train loss: 0.060, Global test loss: 1.575, Global test accuracy: 39.18 

Round  81, Train loss: 0.069, Test loss: 0.856, Test accuracy: 80.92 

Round  81, Global train loss: 0.069, Global test loss: 1.501, Global test accuracy: 37.86 

Round  82, Train loss: 0.086, Test loss: 0.876, Test accuracy: 80.66 

Round  82, Global train loss: 0.086, Global test loss: 1.282, Global test accuracy: 38.21 

Round  83, Train loss: 0.082, Test loss: 0.866, Test accuracy: 80.73 

Round  83, Global train loss: 0.082, Global test loss: 1.648, Global test accuracy: 38.12 

Round  84, Train loss: 0.044, Test loss: 0.886, Test accuracy: 80.95 

Round  84, Global train loss: 0.044, Global test loss: 1.453, Global test accuracy: 34.39 

Round  85, Train loss: 0.068, Test loss: 0.884, Test accuracy: 81.03 

Round  85, Global train loss: 0.068, Global test loss: 1.494, Global test accuracy: 38.69 

Round  86, Train loss: 0.056, Test loss: 0.903, Test accuracy: 80.78 

Round  86, Global train loss: 0.056, Global test loss: 1.929, Global test accuracy: 38.93 

Round  87, Train loss: 0.075, Test loss: 0.899, Test accuracy: 80.85 

Round  87, Global train loss: 0.075, Global test loss: 1.275, Global test accuracy: 35.97 

Round  88, Train loss: 0.080, Test loss: 0.893, Test accuracy: 80.71 

Round  88, Global train loss: 0.080, Global test loss: 1.407, Global test accuracy: 37.52 

Round  89, Train loss: 0.056, Test loss: 0.910, Test accuracy: 80.77 

Round  89, Global train loss: 0.056, Global test loss: 1.676, Global test accuracy: 35.31 

Round  90, Train loss: 0.060, Test loss: 0.891, Test accuracy: 81.17 

Round  90, Global train loss: 0.060, Global test loss: 1.298, Global test accuracy: 36.45 

Round  91, Train loss: 0.042, Test loss: 0.940, Test accuracy: 80.74 

Round  91, Global train loss: 0.042, Global test loss: 1.303, Global test accuracy: 37.32 

Round  92, Train loss: 0.053, Test loss: 0.939, Test accuracy: 80.49 

Round  92, Global train loss: 0.053, Global test loss: 1.250, Global test accuracy: 37.53 

Round  93, Train loss: 0.060, Test loss: 0.919, Test accuracy: 80.79 

Round  93, Global train loss: 0.060, Global test loss: 1.651, Global test accuracy: 32.43 

Round  94, Train loss: 0.045, Test loss: 0.941, Test accuracy: 80.85 

Round  94, Global train loss: 0.045, Global test loss: 1.294, Global test accuracy: 37.87 

Round  95, Train loss: 0.079, Test loss: 0.944, Test accuracy: 80.67 

Round  95, Global train loss: 0.079, Global test loss: 1.240, Global test accuracy: 38.52 

Round  96, Train loss: 0.083, Test loss: 0.970, Test accuracy: 80.33 

Round  96, Global train loss: 0.083, Global test loss: 1.399, Global test accuracy: 39.56 

Round  97, Train loss: 0.058, Test loss: 0.992, Test accuracy: 80.29 

Round  97, Global train loss: 0.058, Global test loss: 1.638, Global test accuracy: 34.48 

Round  98, Train loss: 0.049, Test loss: 0.959, Test accuracy: 80.71 

Round  98, Global train loss: 0.049, Global test loss: 1.820, Global test accuracy: 39.21 

Round  99, Train loss: 0.061, Test loss: 0.964, Test accuracy: 80.68 

Round  99, Global train loss: 0.061, Global test loss: 1.232, Global test accuracy: 37.86 

Final Round, Train loss: 0.054, Test loss: 0.976, Test accuracy: 81.25 

Final Round, Global train loss: 0.054, Global test loss: 1.232, Global test accuracy: 37.86 

Average accuracy final 10 rounds: 80.67249999999999 

Average global accuracy final 10 rounds: 37.1225 

1264.9675703048706
[1.4278230667114258, 2.61007022857666, 3.7999649047851562, 4.974779367446899, 6.151605606079102, 7.33823037147522, 8.525569915771484, 9.715970516204834, 10.906156301498413, 12.099567890167236, 13.290263652801514, 14.475017070770264, 15.666982650756836, 16.853055715560913, 18.039665937423706, 19.222195386886597, 20.40958333015442, 21.59206199645996, 22.702178478240967, 23.879872798919678, 25.064852237701416, 26.239757776260376, 27.4243905544281, 28.444967031478882, 29.463196992874146, 30.476447105407715, 31.486693143844604, 32.50047206878662, 33.51507377624512, 34.52866268157959, 35.54560136795044, 36.56482672691345, 37.58888864517212, 38.61366248130798, 39.6379508972168, 40.655882358551025, 41.67352604866028, 42.69095826148987, 43.70682668685913, 44.71861934661865, 45.732476472854614, 46.755919456481934, 47.7720730304718, 48.79042100906372, 49.82924675941467, 50.86553382873535, 51.90370440483093, 52.93635010719299, 53.96812844276428, 54.9875123500824, 56.0052707195282, 57.01976943016052, 58.03326201438904, 59.048551082611084, 60.06442093849182, 61.0916588306427, 62.123873710632324, 63.161821365356445, 64.19920134544373, 65.23739433288574, 66.26770305633545, 67.28304076194763, 68.30396342277527, 69.32795739173889, 70.34507608413696, 71.36438226699829, 72.37878251075745, 73.40110802650452, 74.42320203781128, 75.44296884536743, 76.46796464920044, 77.49544715881348, 78.51929306983948, 79.53841042518616, 80.5622923374176, 81.58001518249512, 82.59758353233337, 83.6084337234497, 84.6172547340393, 85.64035677909851, 86.66614699363708, 87.70356464385986, 88.72226691246033, 89.74390077590942, 90.76665091514587, 91.79068279266357, 92.81519794464111, 93.83190512657166, 94.8580584526062, 95.88319325447083, 96.90546822547913, 97.92650389671326, 98.9456639289856, 99.9648175239563, 100.98220038414001, 102.00087857246399, 103.02547359466553, 104.08137226104736, 105.10222887992859, 106.12454962730408, 108.16347575187683]
[39.9, 51.125, 57.25, 58.291666666666664, 61.25, 65.13333333333334, 69.73333333333333, 71.41666666666667, 72.29166666666667, 73.18333333333334, 72.60833333333333, 72.19166666666666, 73.3, 74.03333333333333, 74.18333333333334, 74.575, 73.925, 75.96666666666667, 74.55833333333334, 74.94166666666666, 74.36666666666666, 76.3, 78.25, 78.09166666666667, 78.375, 78.61666666666666, 79.11666666666666, 79.025, 79.45833333333333, 79.875, 79.71666666666667, 79.61666666666666, 79.925, 79.61666666666666, 79.31666666666666, 79.275, 79.41666666666667, 79.85, 79.85, 79.95833333333333, 80.33333333333333, 80.71666666666667, 80.70833333333333, 80.025, 80.11666666666666, 79.95833333333333, 80.25, 80.25833333333334, 80.05833333333334, 80.24166666666666, 80.05833333333334, 79.61666666666666, 79.85833333333333, 79.94166666666666, 79.9, 80.0, 80.19166666666666, 80.04166666666667, 80.49166666666666, 80.06666666666666, 80.4, 80.44166666666666, 80.05833333333334, 80.49166666666666, 80.31666666666666, 80.90833333333333, 80.65, 81.24166666666666, 81.06666666666666, 81.16666666666667, 81.21666666666667, 80.85, 80.575, 80.60833333333333, 80.63333333333334, 80.55833333333334, 80.89166666666667, 80.91666666666667, 80.89166666666667, 80.70833333333333, 80.81666666666666, 80.925, 80.65833333333333, 80.73333333333333, 80.95, 81.025, 80.78333333333333, 80.85, 80.70833333333333, 80.76666666666667, 81.16666666666667, 80.74166666666666, 80.49166666666666, 80.79166666666667, 80.85, 80.675, 80.325, 80.29166666666667, 80.70833333333333, 80.68333333333334, 81.25]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.905, Test loss: 1.039, Test accuracy: 44.62 

Round   0, Global train loss: 0.905, Global test loss: 1.122, Global test accuracy: 37.32 

Round   1, Train loss: 0.766, Test loss: 0.976, Test accuracy: 48.84 

Round   1, Global train loss: 0.766, Global test loss: 1.154, Global test accuracy: 35.80 

Round   2, Train loss: 0.717, Test loss: 0.936, Test accuracy: 54.82 

Round   2, Global train loss: 0.717, Global test loss: 1.234, Global test accuracy: 36.07 

Round   3, Train loss: 0.615, Test loss: 0.930, Test accuracy: 56.54 

Round   3, Global train loss: 0.615, Global test loss: 1.280, Global test accuracy: 36.67 

Round   4, Train loss: 0.745, Test loss: 0.773, Test accuracy: 64.29 

Round   4, Global train loss: 0.745, Global test loss: 1.165, Global test accuracy: 39.21 

Round   5, Train loss: 0.605, Test loss: 0.834, Test accuracy: 64.52 

Round   5, Global train loss: 0.605, Global test loss: 1.522, Global test accuracy: 38.67 

Round   6, Train loss: 0.645, Test loss: 0.759, Test accuracy: 67.12 

Round   6, Global train loss: 0.645, Global test loss: 1.360, Global test accuracy: 39.62 

Round   7, Train loss: 0.575, Test loss: 0.647, Test accuracy: 70.29 

Round   7, Global train loss: 0.575, Global test loss: 1.137, Global test accuracy: 39.03 

Round   8, Train loss: 0.553, Test loss: 0.602, Test accuracy: 73.55 

Round   8, Global train loss: 0.553, Global test loss: 1.548, Global test accuracy: 34.89 

Round   9, Train loss: 0.586, Test loss: 0.594, Test accuracy: 74.21 

Round   9, Global train loss: 0.586, Global test loss: 1.352, Global test accuracy: 36.08 

Round  10, Train loss: 0.549, Test loss: 0.598, Test accuracy: 74.01 

Round  10, Global train loss: 0.549, Global test loss: 1.280, Global test accuracy: 34.20 

Round  11, Train loss: 0.616, Test loss: 0.581, Test accuracy: 74.77 

Round  11, Global train loss: 0.616, Global test loss: 1.439, Global test accuracy: 34.88 

Round  12, Train loss: 0.589, Test loss: 0.576, Test accuracy: 75.39 

Round  12, Global train loss: 0.589, Global test loss: 1.133, Global test accuracy: 40.34 

Round  13, Train loss: 0.594, Test loss: 0.566, Test accuracy: 75.90 

Round  13, Global train loss: 0.594, Global test loss: 1.498, Global test accuracy: 39.94 

Round  14, Train loss: 0.552, Test loss: 0.552, Test accuracy: 76.72 

Round  14, Global train loss: 0.552, Global test loss: 1.236, Global test accuracy: 41.00 

Round  15, Train loss: 0.556, Test loss: 0.550, Test accuracy: 77.01 

Round  15, Global train loss: 0.556, Global test loss: 1.284, Global test accuracy: 35.75 

Round  16, Train loss: 0.482, Test loss: 0.542, Test accuracy: 77.53 

Round  16, Global train loss: 0.482, Global test loss: 1.301, Global test accuracy: 36.58 

Round  17, Train loss: 0.523, Test loss: 0.541, Test accuracy: 77.58 

Round  17, Global train loss: 0.523, Global test loss: 1.296, Global test accuracy: 36.88 

Round  18, Train loss: 0.494, Test loss: 0.542, Test accuracy: 77.74 

Round  18, Global train loss: 0.494, Global test loss: 1.561, Global test accuracy: 38.93 

Round  19, Train loss: 0.565, Test loss: 0.547, Test accuracy: 77.72 

Round  19, Global train loss: 0.565, Global test loss: 1.444, Global test accuracy: 38.40 

Round  20, Train loss: 0.477, Test loss: 0.547, Test accuracy: 77.53 

Round  20, Global train loss: 0.477, Global test loss: 1.348, Global test accuracy: 39.73 

Round  21, Train loss: 0.422, Test loss: 0.539, Test accuracy: 77.74 

Round  21, Global train loss: 0.422, Global test loss: 1.514, Global test accuracy: 40.19 

Round  22, Train loss: 0.527, Test loss: 0.555, Test accuracy: 76.97 

Round  22, Global train loss: 0.527, Global test loss: 1.387, Global test accuracy: 37.34 

Round  23, Train loss: 0.449, Test loss: 0.546, Test accuracy: 77.63 

Round  23, Global train loss: 0.449, Global test loss: 1.226, Global test accuracy: 39.74 

Round  24, Train loss: 0.485, Test loss: 0.534, Test accuracy: 78.13 

Round  24, Global train loss: 0.485, Global test loss: 1.340, Global test accuracy: 37.69 

Round  25, Train loss: 0.534, Test loss: 0.527, Test accuracy: 78.73 

Round  25, Global train loss: 0.534, Global test loss: 1.164, Global test accuracy: 40.48 

Round  26, Train loss: 0.491, Test loss: 0.520, Test accuracy: 78.90 

Round  26, Global train loss: 0.491, Global test loss: 1.330, Global test accuracy: 36.86 

Round  27, Train loss: 0.473, Test loss: 0.517, Test accuracy: 79.29 

Round  27, Global train loss: 0.473, Global test loss: 1.228, Global test accuracy: 40.70 

Round  28, Train loss: 0.412, Test loss: 0.502, Test accuracy: 79.77 

Round  28, Global train loss: 0.412, Global test loss: 1.592, Global test accuracy: 38.59 

Round  29, Train loss: 0.444, Test loss: 0.500, Test accuracy: 79.93 

Round  29, Global train loss: 0.444, Global test loss: 1.322, Global test accuracy: 37.16 

Round  30, Train loss: 0.384, Test loss: 0.504, Test accuracy: 79.93 

Round  30, Global train loss: 0.384, Global test loss: 1.590, Global test accuracy: 37.27 

Round  31, Train loss: 0.450, Test loss: 0.492, Test accuracy: 80.33 

Round  31, Global train loss: 0.450, Global test loss: 1.547, Global test accuracy: 41.17 

Round  32, Train loss: 0.421, Test loss: 0.501, Test accuracy: 80.16 

Round  32, Global train loss: 0.421, Global test loss: 1.309, Global test accuracy: 40.38 

Round  33, Train loss: 0.442, Test loss: 0.496, Test accuracy: 80.29 

Round  33, Global train loss: 0.442, Global test loss: 1.429, Global test accuracy: 41.18 

Round  34, Train loss: 0.418, Test loss: 0.504, Test accuracy: 80.15 

Round  34, Global train loss: 0.418, Global test loss: 1.608, Global test accuracy: 38.60 

Round  35, Train loss: 0.422, Test loss: 0.495, Test accuracy: 80.39 

Round  35, Global train loss: 0.422, Global test loss: 1.497, Global test accuracy: 40.12 

Round  36, Train loss: 0.426, Test loss: 0.505, Test accuracy: 80.34 

Round  36, Global train loss: 0.426, Global test loss: 1.478, Global test accuracy: 40.23 

Round  37, Train loss: 0.418, Test loss: 0.501, Test accuracy: 80.34 

Round  37, Global train loss: 0.418, Global test loss: 1.391, Global test accuracy: 37.64 

Round  38, Train loss: 0.394, Test loss: 0.493, Test accuracy: 80.49 

Round  38, Global train loss: 0.394, Global test loss: 1.443, Global test accuracy: 40.67 

Round  39, Train loss: 0.470, Test loss: 0.505, Test accuracy: 80.22 

Round  39, Global train loss: 0.470, Global test loss: 1.812, Global test accuracy: 35.12 

Round  40, Train loss: 0.346, Test loss: 0.507, Test accuracy: 80.33 

Round  40, Global train loss: 0.346, Global test loss: 1.397, Global test accuracy: 40.65 

Round  41, Train loss: 0.404, Test loss: 0.494, Test accuracy: 80.66 

Round  41, Global train loss: 0.404, Global test loss: 1.458, Global test accuracy: 36.28 

Round  42, Train loss: 0.371, Test loss: 0.481, Test accuracy: 81.11 

Round  42, Global train loss: 0.371, Global test loss: 1.459, Global test accuracy: 41.24 

Round  43, Train loss: 0.366, Test loss: 0.486, Test accuracy: 81.37 

Round  43, Global train loss: 0.366, Global test loss: 1.620, Global test accuracy: 39.83 

Round  44, Train loss: 0.352, Test loss: 0.486, Test accuracy: 81.43 

Round  44, Global train loss: 0.352, Global test loss: 1.954, Global test accuracy: 38.23 

Round  45, Train loss: 0.360, Test loss: 0.490, Test accuracy: 81.45 

Round  45, Global train loss: 0.360, Global test loss: 1.356, Global test accuracy: 37.84 

Round  46, Train loss: 0.367, Test loss: 0.487, Test accuracy: 81.31 

Round  46, Global train loss: 0.367, Global test loss: 1.395, Global test accuracy: 41.67 

Round  47, Train loss: 0.367, Test loss: 0.489, Test accuracy: 81.47 

Round  47, Global train loss: 0.367, Global test loss: 1.414, Global test accuracy: 39.86 

Round  48, Train loss: 0.402, Test loss: 0.477, Test accuracy: 81.85 

Round  48, Global train loss: 0.402, Global test loss: 1.385, Global test accuracy: 39.47 

Round  49, Train loss: 0.341, Test loss: 0.475, Test accuracy: 81.88 

Round  49, Global train loss: 0.341, Global test loss: 1.858, Global test accuracy: 39.48 

Round  50, Train loss: 0.356, Test loss: 0.486, Test accuracy: 81.61 

Round  50, Global train loss: 0.356, Global test loss: 1.558, Global test accuracy: 40.37 

Round  51, Train loss: 0.338, Test loss: 0.492, Test accuracy: 81.67 

Round  51, Global train loss: 0.338, Global test loss: 1.758, Global test accuracy: 40.38 

Round  52, Train loss: 0.295, Test loss: 0.493, Test accuracy: 81.57 

Round  52, Global train loss: 0.295, Global test loss: 1.658, Global test accuracy: 41.42 

Round  53, Train loss: 0.362, Test loss: 0.500, Test accuracy: 81.71 

Round  53, Global train loss: 0.362, Global test loss: 1.359, Global test accuracy: 39.48 

Round  54, Train loss: 0.315, Test loss: 0.505, Test accuracy: 81.52 

Round  54, Global train loss: 0.315, Global test loss: 1.453, Global test accuracy: 38.73 

Round  55, Train loss: 0.383, Test loss: 0.491, Test accuracy: 81.72 

Round  55, Global train loss: 0.383, Global test loss: 1.406, Global test accuracy: 38.76 

Round  56, Train loss: 0.289, Test loss: 0.496, Test accuracy: 81.67 

Round  56, Global train loss: 0.289, Global test loss: 1.611, Global test accuracy: 41.57 

Round  57, Train loss: 0.390, Test loss: 0.478, Test accuracy: 82.32 

Round  57, Global train loss: 0.390, Global test loss: 1.522, Global test accuracy: 37.73 

Round  58, Train loss: 0.304, Test loss: 0.482, Test accuracy: 82.38 

Round  58, Global train loss: 0.304, Global test loss: 2.343, Global test accuracy: 41.12 

Round  59, Train loss: 0.312, Test loss: 0.472, Test accuracy: 82.67 

Round  59, Global train loss: 0.312, Global test loss: 1.829, Global test accuracy: 39.16 

Round  60, Train loss: 0.339, Test loss: 0.467, Test accuracy: 82.86 

Round  60, Global train loss: 0.339, Global test loss: 1.464, Global test accuracy: 39.29 

Round  61, Train loss: 0.281, Test loss: 0.487, Test accuracy: 82.21 

Round  61, Global train loss: 0.281, Global test loss: 1.587, Global test accuracy: 39.59 

Round  62, Train loss: 0.267, Test loss: 0.479, Test accuracy: 82.58 

Round  62, Global train loss: 0.267, Global test loss: 1.564, Global test accuracy: 41.14 

Round  63, Train loss: 0.282, Test loss: 0.495, Test accuracy: 82.20 

Round  63, Global train loss: 0.282, Global test loss: 1.927, Global test accuracy: 39.41 

Round  64, Train loss: 0.274, Test loss: 0.494, Test accuracy: 82.09 

Round  64, Global train loss: 0.274, Global test loss: 1.468, Global test accuracy: 39.71 

Round  65, Train loss: 0.292, Test loss: 0.492, Test accuracy: 82.11 

Round  65, Global train loss: 0.292, Global test loss: 1.590, Global test accuracy: 40.41 

Round  66, Train loss: 0.266, Test loss: 0.497, Test accuracy: 82.11 

Round  66, Global train loss: 0.266, Global test loss: 1.486, Global test accuracy: 42.67 

Round  67, Train loss: 0.302, Test loss: 0.504, Test accuracy: 81.83 

Round  67, Global train loss: 0.302, Global test loss: 1.602, Global test accuracy: 42.68 

Round  68, Train loss: 0.323, Test loss: 0.507, Test accuracy: 81.73 

Round  68, Global train loss: 0.323, Global test loss: 1.883, Global test accuracy: 36.71 

Round  69, Train loss: 0.261, Test loss: 0.500, Test accuracy: 82.05 

Round  69, Global train loss: 0.261, Global test loss: 1.917, Global test accuracy: 37.87 

Round  70, Train loss: 0.246, Test loss: 0.501, Test accuracy: 82.18 

Round  70, Global train loss: 0.246, Global test loss: 1.769, Global test accuracy: 39.64 

Round  71, Train loss: 0.328, Test loss: 0.497, Test accuracy: 82.54 

Round  71, Global train loss: 0.328, Global test loss: 1.434, Global test accuracy: 39.48 

Round  72, Train loss: 0.277, Test loss: 0.499, Test accuracy: 82.52 

Round  72, Global train loss: 0.277, Global test loss: 1.727, Global test accuracy: 40.86 

Round  73, Train loss: 0.263, Test loss: 0.494, Test accuracy: 82.67 

Round  73, Global train loss: 0.263, Global test loss: 1.755, Global test accuracy: 38.15 

Round  74, Train loss: 0.220, Test loss: 0.502, Test accuracy: 82.44 

Round  74, Global train loss: 0.220, Global test loss: 1.826, Global test accuracy: 39.30 

Round  75, Train loss: 0.362, Test loss: 0.501, Test accuracy: 82.49 

Round  75, Global train loss: 0.362, Global test loss: 1.501, Global test accuracy: 41.02 

Round  76, Train loss: 0.304, Test loss: 0.490, Test accuracy: 82.83 

Round  76, Global train loss: 0.304, Global test loss: 1.471, Global test accuracy: 38.23 

Round  77, Train loss: 0.235, Test loss: 0.496, Test accuracy: 82.67 

Round  77, Global train loss: 0.235, Global test loss: 1.553, Global test accuracy: 40.80 

Round  78, Train loss: 0.232, Test loss: 0.491, Test accuracy: 82.92 

Round  78, Global train loss: 0.232, Global test loss: 1.555, Global test accuracy: 40.12 

Round  79, Train loss: 0.272, Test loss: 0.500, Test accuracy: 82.58 

Round  79, Global train loss: 0.272, Global test loss: 1.915, Global test accuracy: 42.48 

Round  80, Train loss: 0.205, Test loss: 0.504, Test accuracy: 82.63 

Round  80, Global train loss: 0.205, Global test loss: 2.252, Global test accuracy: 43.14 

Round  81, Train loss: 0.314, Test loss: 0.504, Test accuracy: 82.60 

Round  81, Global train loss: 0.314, Global test loss: 1.528, Global test accuracy: 42.24 

Round  82, Train loss: 0.331, Test loss: 0.508, Test accuracy: 82.67 

Round  82, Global train loss: 0.331, Global test loss: 1.623, Global test accuracy: 40.28 

Round  83, Train loss: 0.240, Test loss: 0.503, Test accuracy: 82.87 

Round  83, Global train loss: 0.240, Global test loss: 1.685, Global test accuracy: 38.65 

Round  84, Train loss: 0.224, Test loss: 0.505, Test accuracy: 82.54 

Round  84, Global train loss: 0.224, Global test loss: 1.623, Global test accuracy: 39.29 

Round  85, Train loss: 0.274, Test loss: 0.498, Test accuracy: 82.44 

Round  85, Global train loss: 0.274, Global test loss: 1.855, Global test accuracy: 40.52 

Round  86, Train loss: 0.333, Test loss: 0.507, Test accuracy: 82.38 

Round  86, Global train loss: 0.333, Global test loss: 1.563, Global test accuracy: 39.14 

Round  87, Train loss: 0.225, Test loss: 0.525, Test accuracy: 82.06 

Round  87, Global train loss: 0.225, Global test loss: 1.624, Global test accuracy: 38.56 

Round  88, Train loss: 0.215, Test loss: 0.546, Test accuracy: 81.85 

Round  88, Global train loss: 0.215, Global test loss: 1.589, Global test accuracy: 40.05 

Round  89, Train loss: 0.226, Test loss: 0.529, Test accuracy: 82.54 

Round  89, Global train loss: 0.226, Global test loss: 1.725, Global test accuracy: 40.25 

Round  90, Train loss: 0.275, Test loss: 0.528, Test accuracy: 82.66 

Round  90, Global train loss: 0.275, Global test loss: 1.820, Global test accuracy: 42.57 

Round  91, Train loss: 0.234, Test loss: 0.523, Test accuracy: 82.00 

Round  91, Global train loss: 0.234, Global test loss: 2.452, Global test accuracy: 41.62 

Round  92, Train loss: 0.282, Test loss: 0.530, Test accuracy: 82.18 

Round  92, Global train loss: 0.282, Global test loss: 1.739, Global test accuracy: 39.36 

Round  93, Train loss: 0.267, Test loss: 0.520, Test accuracy: 82.42 

Round  93, Global train loss: 0.267, Global test loss: 1.683, Global test accuracy: 42.84 

Round  94, Train loss: 0.229, Test loss: 0.517, Test accuracy: 82.77 

Round  94, Global train loss: 0.229, Global test loss: 1.955, Global test accuracy: 43.13 

Round  95, Train loss: 0.207, Test loss: 0.518, Test accuracy: 83.00 

Round  95, Global train loss: 0.207, Global test loss: 1.844, Global test accuracy: 42.37 

Round  96, Train loss: 0.240, Test loss: 0.518, Test accuracy: 82.86 

Round  96, Global train loss: 0.240, Global test loss: 1.508, Global test accuracy: 40.53 

Round  97, Train loss: 0.232, Test loss: 0.512, Test accuracy: 82.66 

Round  97, Global train loss: 0.232, Global test loss: 1.696, Global test accuracy: 42.77 

Round  98, Train loss: 0.260, Test loss: 0.518, Test accuracy: 82.71 

Round  98, Global train loss: 0.260, Global test loss: 1.578, Global test accuracy: 40.35 

Round  99, Train loss: 0.219, Test loss: 0.505, Test accuracy: 82.72 

Round  99, Global train loss: 0.219, Global test loss: 1.788, Global test accuracy: 41.83 

Final Round, Train loss: 0.183, Test loss: 0.585, Test accuracy: 82.83 

Final Round, Global train loss: 0.183, Global test loss: 1.788, Global test accuracy: 41.83 

Average accuracy final 10 rounds: 82.59749999999998 

Average global accuracy final 10 rounds: 41.735833333333325 

1299.0341801643372
[1.4136595726013184, 2.555959701538086, 3.70005202293396, 4.877057313919067, 6.05763053894043, 7.23851203918457, 8.514435052871704, 9.693271160125732, 10.876097679138184, 12.056430101394653, 13.233677625656128, 14.416636228561401, 15.59741497039795, 16.772165536880493, 17.95469617843628, 19.133273363113403, 20.320720911026, 21.506773471832275, 22.68581199645996, 23.860824823379517, 25.024349689483643, 26.19184398651123, 27.36655020713806, 28.540284633636475, 29.710453271865845, 30.885844469070435, 32.05654001235962, 33.226828813552856, 34.40903854370117, 35.583348989486694, 36.76107740402222, 37.93502974510193, 39.11172795295715, 40.290971755981445, 41.465496301651, 42.64035964012146, 43.826205253601074, 45.00919008255005, 46.18731451034546, 47.361982107162476, 48.53847408294678, 49.710753440856934, 50.885703563690186, 52.06529378890991, 53.24968433380127, 54.42208695411682, 55.59842801094055, 56.77740478515625, 57.95222544670105, 59.12928652763367, 60.306135416030884, 61.47935771942139, 62.66123962402344, 63.83714723587036, 65.01719284057617, 66.19469356536865, 67.37672853469849, 68.55839228630066, 69.73547315597534, 70.91760921478271, 72.10105848312378, 73.28565049171448, 74.458016872406, 75.63361763954163, 76.81295895576477, 77.98746752738953, 79.16191840171814, 80.33697438240051, 81.51660585403442, 82.69971060752869, 83.88085699081421, 85.06028628349304, 86.23637819290161, 87.32712435722351, 88.3396360874176, 89.3547215461731, 90.36841773986816, 91.38451957702637, 92.39936351776123, 93.41793298721313, 94.4312093257904, 95.44393491744995, 96.45737266540527, 97.47327089309692, 98.48705530166626, 99.50268888473511, 100.51673436164856, 101.5318169593811, 102.54800653457642, 103.56232953071594, 104.57875156402588, 105.59526038169861, 106.61680316925049, 107.63820910453796, 108.66318798065186, 109.68952560424805, 110.71048259735107, 111.73672556877136, 112.7542839050293, 113.77577924728394, 115.8034155368805]
[44.625, 48.84166666666667, 54.81666666666667, 56.541666666666664, 64.29166666666667, 64.51666666666667, 67.125, 70.29166666666667, 73.55, 74.20833333333333, 74.00833333333334, 74.76666666666667, 75.39166666666667, 75.9, 76.725, 77.00833333333334, 77.525, 77.575, 77.74166666666666, 77.725, 77.525, 77.74166666666666, 76.96666666666667, 77.63333333333334, 78.13333333333334, 78.73333333333333, 78.9, 79.29166666666667, 79.76666666666667, 79.93333333333334, 79.93333333333334, 80.33333333333333, 80.15833333333333, 80.29166666666667, 80.15, 80.39166666666667, 80.34166666666667, 80.34166666666667, 80.49166666666666, 80.225, 80.33333333333333, 80.65833333333333, 81.10833333333333, 81.36666666666666, 81.43333333333334, 81.45, 81.30833333333334, 81.46666666666667, 81.85, 81.88333333333334, 81.60833333333333, 81.66666666666667, 81.56666666666666, 81.70833333333333, 81.51666666666667, 81.71666666666667, 81.66666666666667, 82.31666666666666, 82.38333333333334, 82.675, 82.85833333333333, 82.20833333333333, 82.58333333333333, 82.2, 82.09166666666667, 82.10833333333333, 82.10833333333333, 81.825, 81.73333333333333, 82.05, 82.18333333333334, 82.54166666666667, 82.51666666666667, 82.675, 82.44166666666666, 82.49166666666666, 82.83333333333333, 82.675, 82.91666666666667, 82.575, 82.63333333333334, 82.6, 82.675, 82.86666666666666, 82.54166666666667, 82.44166666666666, 82.38333333333334, 82.05833333333334, 81.85, 82.54166666666667, 82.65833333333333, 82.0, 82.18333333333334, 82.425, 82.76666666666667, 83.0, 82.85833333333333, 82.65833333333333, 82.70833333333333, 82.71666666666667, 82.825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.045, Test loss: 1.102, Test accuracy: 36.81 

Round   1, Train loss: 0.939, Test loss: 1.129, Test accuracy: 38.74 

Round   2, Train loss: 0.881, Test loss: 1.019, Test accuracy: 47.27 

Round   3, Train loss: 0.767, Test loss: 1.006, Test accuracy: 50.67 

Round   4, Train loss: 0.709, Test loss: 0.991, Test accuracy: 51.44 

Round   5, Train loss: 0.759, Test loss: 0.914, Test accuracy: 55.30 

Round   6, Train loss: 0.728, Test loss: 0.800, Test accuracy: 60.47 

Round   7, Train loss: 0.661, Test loss: 0.742, Test accuracy: 65.10 

Round   8, Train loss: 0.619, Test loss: 0.744, Test accuracy: 64.83 

Round   9, Train loss: 0.615, Test loss: 0.678, Test accuracy: 67.30 

Round  10, Train loss: 0.613, Test loss: 0.714, Test accuracy: 66.87 

Round  11, Train loss: 0.639, Test loss: 0.668, Test accuracy: 69.21 

Round  12, Train loss: 0.573, Test loss: 0.661, Test accuracy: 70.31 

Round  13, Train loss: 0.546, Test loss: 0.609, Test accuracy: 72.09 

Round  14, Train loss: 0.530, Test loss: 0.582, Test accuracy: 74.24 

Round  15, Train loss: 0.545, Test loss: 0.566, Test accuracy: 75.26 

Round  16, Train loss: 0.534, Test loss: 0.554, Test accuracy: 75.59 

Round  17, Train loss: 0.578, Test loss: 0.537, Test accuracy: 76.75 

Round  18, Train loss: 0.534, Test loss: 0.523, Test accuracy: 77.67 

Round  19, Train loss: 0.510, Test loss: 0.520, Test accuracy: 77.51 

Round  20, Train loss: 0.527, Test loss: 0.520, Test accuracy: 77.82 

Round  21, Train loss: 0.468, Test loss: 0.513, Test accuracy: 78.39 

Round  22, Train loss: 0.541, Test loss: 0.506, Test accuracy: 78.64 

Round  23, Train loss: 0.515, Test loss: 0.502, Test accuracy: 78.78 

Round  24, Train loss: 0.493, Test loss: 0.492, Test accuracy: 79.12 

Round  25, Train loss: 0.534, Test loss: 0.493, Test accuracy: 79.47 

Round  26, Train loss: 0.438, Test loss: 0.492, Test accuracy: 79.44 

Round  27, Train loss: 0.483, Test loss: 0.480, Test accuracy: 79.72 

Round  28, Train loss: 0.513, Test loss: 0.485, Test accuracy: 79.20 

Round  29, Train loss: 0.473, Test loss: 0.477, Test accuracy: 79.53 

Round  30, Train loss: 0.529, Test loss: 0.475, Test accuracy: 80.09 

Round  31, Train loss: 0.422, Test loss: 0.459, Test accuracy: 80.76 

Round  32, Train loss: 0.463, Test loss: 0.456, Test accuracy: 80.90 

Round  33, Train loss: 0.483, Test loss: 0.459, Test accuracy: 80.60 

Round  34, Train loss: 0.477, Test loss: 0.466, Test accuracy: 80.42 

Round  35, Train loss: 0.451, Test loss: 0.458, Test accuracy: 80.79 

Round  36, Train loss: 0.403, Test loss: 0.444, Test accuracy: 81.34 

Round  37, Train loss: 0.435, Test loss: 0.439, Test accuracy: 81.82 

Round  38, Train loss: 0.378, Test loss: 0.438, Test accuracy: 81.77 

Round  39, Train loss: 0.380, Test loss: 0.439, Test accuracy: 81.50 

Round  40, Train loss: 0.399, Test loss: 0.428, Test accuracy: 82.16 

Round  41, Train loss: 0.455, Test loss: 0.438, Test accuracy: 81.70 

Round  42, Train loss: 0.479, Test loss: 0.435, Test accuracy: 81.98 

Round  43, Train loss: 0.395, Test loss: 0.433, Test accuracy: 82.28 

Round  44, Train loss: 0.446, Test loss: 0.432, Test accuracy: 82.23 

Round  45, Train loss: 0.339, Test loss: 0.422, Test accuracy: 82.50 

Round  46, Train loss: 0.370, Test loss: 0.422, Test accuracy: 82.58 

Round  47, Train loss: 0.385, Test loss: 0.415, Test accuracy: 83.10 

Round  48, Train loss: 0.388, Test loss: 0.421, Test accuracy: 83.00 

Round  49, Train loss: 0.335, Test loss: 0.424, Test accuracy: 82.61 

Round  50, Train loss: 0.418, Test loss: 0.410, Test accuracy: 83.00 

Round  51, Train loss: 0.413, Test loss: 0.404, Test accuracy: 83.14 

Round  52, Train loss: 0.408, Test loss: 0.406, Test accuracy: 83.33 

Round  53, Train loss: 0.295, Test loss: 0.409, Test accuracy: 82.87 

Round  54, Train loss: 0.386, Test loss: 0.405, Test accuracy: 83.34 

Round  55, Train loss: 0.337, Test loss: 0.403, Test accuracy: 83.51 

Round  56, Train loss: 0.314, Test loss: 0.409, Test accuracy: 83.46 

Round  57, Train loss: 0.339, Test loss: 0.399, Test accuracy: 83.70 

Round  58, Train loss: 0.292, Test loss: 0.401, Test accuracy: 83.42 

Round  59, Train loss: 0.352, Test loss: 0.398, Test accuracy: 83.73 

Round  60, Train loss: 0.402, Test loss: 0.393, Test accuracy: 84.17 

Round  61, Train loss: 0.342, Test loss: 0.393, Test accuracy: 84.07 

Round  62, Train loss: 0.361, Test loss: 0.390, Test accuracy: 84.22 

Round  63, Train loss: 0.365, Test loss: 0.387, Test accuracy: 84.25 

Round  64, Train loss: 0.314, Test loss: 0.392, Test accuracy: 83.99 

Round  65, Train loss: 0.330, Test loss: 0.384, Test accuracy: 84.64 

Round  66, Train loss: 0.356, Test loss: 0.389, Test accuracy: 84.06 

Round  67, Train loss: 0.315, Test loss: 0.388, Test accuracy: 84.01 

Round  68, Train loss: 0.363, Test loss: 0.385, Test accuracy: 84.25 

Round  69, Train loss: 0.297, Test loss: 0.386, Test accuracy: 84.43 

Round  70, Train loss: 0.290, Test loss: 0.390, Test accuracy: 84.26 

Round  71, Train loss: 0.270, Test loss: 0.387, Test accuracy: 84.47 

Round  72, Train loss: 0.323, Test loss: 0.381, Test accuracy: 84.53 

Round  73, Train loss: 0.250, Test loss: 0.389, Test accuracy: 84.32 

Round  74, Train loss: 0.292, Test loss: 0.389, Test accuracy: 84.42 

Round  75, Train loss: 0.281, Test loss: 0.383, Test accuracy: 84.31 

Round  76, Train loss: 0.325, Test loss: 0.379, Test accuracy: 84.78 

Round  77, Train loss: 0.217, Test loss: 0.379, Test accuracy: 84.77 

Round  78, Train loss: 0.315, Test loss: 0.375, Test accuracy: 85.02 

Round  79, Train loss: 0.296, Test loss: 0.376, Test accuracy: 85.08 

Round  80, Train loss: 0.276, Test loss: 0.385, Test accuracy: 84.81 

Round  81, Train loss: 0.299, Test loss: 0.375, Test accuracy: 85.12 

Round  82, Train loss: 0.231, Test loss: 0.378, Test accuracy: 85.11 

Round  83, Train loss: 0.281, Test loss: 0.376, Test accuracy: 84.79 

Round  84, Train loss: 0.257, Test loss: 0.370, Test accuracy: 85.28 

Round  85, Train loss: 0.261, Test loss: 0.383, Test accuracy: 84.85 

Round  86, Train loss: 0.229, Test loss: 0.382, Test accuracy: 84.85 

Round  87, Train loss: 0.263, Test loss: 0.375, Test accuracy: 84.86 

Round  88, Train loss: 0.234, Test loss: 0.378, Test accuracy: 85.43 

Round  89, Train loss: 0.319, Test loss: 0.370, Test accuracy: 85.70 

Round  90, Train loss: 0.222, Test loss: 0.376, Test accuracy: 85.33 

Round  91, Train loss: 0.245, Test loss: 0.376, Test accuracy: 85.56 

Round  92, Train loss: 0.205, Test loss: 0.376, Test accuracy: 85.47 

Round  93, Train loss: 0.271, Test loss: 0.374, Test accuracy: 85.65 

Round  94, Train loss: 0.260, Test loss: 0.371, Test accuracy: 85.83 

Round  95, Train loss: 0.222, Test loss: 0.390, Test accuracy: 85.16 

Round  96, Train loss: 0.233, Test loss: 0.385, Test accuracy: 85.48 

Round  97, Train loss: 0.256, Test loss: 0.375, Test accuracy: 85.66 

Round  98, Train loss: 0.216, Test loss: 0.368, Test accuracy: 85.80 

Round  99, Train loss: 0.227, Test loss: 0.373, Test accuracy: 85.33 

Final Round, Train loss: 0.210, Test loss: 0.377, Test accuracy: 85.76 

Average accuracy final 10 rounds: 85.52666666666666 

943.7484905719757
[1.3009612560272217, 2.3456428050994873, 3.394474506378174, 4.442238092422485, 5.489205598831177, 6.53584361076355, 7.581589221954346, 8.627515316009521, 9.677824020385742, 10.724499464035034, 11.777747631072998, 12.825876474380493, 13.872684955596924, 14.917826175689697, 15.966173648834229, 16.98021936416626, 17.995350122451782, 19.010651111602783, 20.031586170196533, 21.04910397529602, 22.065792083740234, 23.081255674362183, 24.118056297302246, 25.13053607940674, 26.157315969467163, 27.165915966033936, 28.183246850967407, 29.195776224136353, 30.21437096595764, 31.22411346435547, 32.241771936416626, 33.25701594352722, 34.27337718009949, 35.287023067474365, 36.327900409698486, 37.37494611740112, 38.41756248474121, 39.46416926383972, 40.50888419151306, 41.55661869049072, 42.60311412811279, 43.65085291862488, 44.696396827697754, 45.742135763168335, 46.79074430465698, 47.838643074035645, 48.88570857048035, 49.931926012039185, 50.98214936256409, 52.02621340751648, 53.03699827194214, 54.05355787277222, 55.0827362537384, 56.125842571258545, 57.16685223579407, 58.20571231842041, 59.252032995224, 60.2942578792572, 61.333401679992676, 62.38113594055176, 63.43076205253601, 64.47386598587036, 65.51765370368958, 66.56971859931946, 67.61247777938843, 68.65978860855103, 69.70179486274719, 70.74428343772888, 71.78972387313843, 72.83803677558899, 73.89569926261902, 74.94767999649048, 75.99455118179321, 77.03728771209717, 78.08531093597412, 79.1257004737854, 80.17172145843506, 81.20858097076416, 82.25031876564026, 83.2919328212738, 84.33361005783081, 85.37891840934753, 86.3921446800232, 87.40263724327087, 88.41282725334167, 89.42702651023865, 90.44108462333679, 91.45847654342651, 92.47209334373474, 93.48292636871338, 94.494713306427, 95.5070948600769, 96.51996803283691, 97.56206607818604, 98.61194586753845, 99.6606957912445, 100.70665645599365, 101.7511854171753, 102.80267763137817, 103.84978413581848, 105.71806406974792]
[36.80833333333333, 38.74166666666667, 47.275, 50.675, 51.44166666666667, 55.3, 60.46666666666667, 65.1, 64.825, 67.3, 66.86666666666666, 69.20833333333333, 70.30833333333334, 72.09166666666667, 74.24166666666666, 75.25833333333334, 75.59166666666667, 76.75, 77.66666666666667, 77.50833333333334, 77.81666666666666, 78.39166666666667, 78.64166666666667, 78.78333333333333, 79.125, 79.475, 79.44166666666666, 79.71666666666667, 79.2, 79.525, 80.09166666666667, 80.75833333333334, 80.9, 80.6, 80.425, 80.79166666666667, 81.34166666666667, 81.81666666666666, 81.76666666666667, 81.5, 82.15833333333333, 81.7, 81.98333333333333, 82.275, 82.23333333333333, 82.5, 82.575, 83.1, 83.0, 82.60833333333333, 83.0, 83.14166666666667, 83.33333333333333, 82.86666666666666, 83.34166666666667, 83.50833333333334, 83.45833333333333, 83.7, 83.425, 83.73333333333333, 84.16666666666667, 84.06666666666666, 84.21666666666667, 84.25, 83.99166666666666, 84.64166666666667, 84.05833333333334, 84.00833333333334, 84.25, 84.43333333333334, 84.25833333333334, 84.46666666666667, 84.525, 84.31666666666666, 84.425, 84.30833333333334, 84.78333333333333, 84.76666666666667, 85.01666666666667, 85.08333333333333, 84.80833333333334, 85.11666666666666, 85.10833333333333, 84.79166666666667, 85.275, 84.85, 84.85, 84.85833333333333, 85.43333333333334, 85.7, 85.33333333333333, 85.55833333333334, 85.475, 85.65, 85.825, 85.15833333333333, 85.48333333333333, 85.65833333333333, 85.8, 85.325, 85.75833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.958, Test loss: 1.115, Test accuracy: 33.85 

Round   1, Train loss: 0.800, Test loss: 1.116, Test accuracy: 40.19 

Round   2, Train loss: 0.723, Test loss: 1.184, Test accuracy: 41.34 

Round   3, Train loss: 0.745, Test loss: 1.039, Test accuracy: 47.38 

Round   4, Train loss: 0.670, Test loss: 1.114, Test accuracy: 42.59 

Round   5, Train loss: 0.618, Test loss: 1.022, Test accuracy: 48.32 

Round   6, Train loss: 0.599, Test loss: 0.951, Test accuracy: 53.12 

Round   7, Train loss: 0.603, Test loss: 0.936, Test accuracy: 54.53 

Round   8, Train loss: 0.503, Test loss: 0.833, Test accuracy: 61.78 

Round   9, Train loss: 0.573, Test loss: 0.817, Test accuracy: 61.05 

Round  10, Train loss: 0.549, Test loss: 0.862, Test accuracy: 60.61 

Round  11, Train loss: 0.543, Test loss: 0.712, Test accuracy: 66.42 

Round  12, Train loss: 0.564, Test loss: 0.737, Test accuracy: 65.43 

Round  13, Train loss: 0.574, Test loss: 0.732, Test accuracy: 65.93 

Round  14, Train loss: 0.491, Test loss: 0.689, Test accuracy: 68.32 

Round  15, Train loss: 0.498, Test loss: 0.659, Test accuracy: 69.95 

Round  16, Train loss: 0.537, Test loss: 0.657, Test accuracy: 69.91 

Round  17, Train loss: 0.554, Test loss: 0.623, Test accuracy: 71.88 

Round  18, Train loss: 0.507, Test loss: 0.610, Test accuracy: 73.09 

Round  19, Train loss: 0.444, Test loss: 0.625, Test accuracy: 72.69 

Round  20, Train loss: 0.527, Test loss: 0.588, Test accuracy: 73.71 

Round  21, Train loss: 0.411, Test loss: 0.613, Test accuracy: 72.99 

Round  22, Train loss: 0.493, Test loss: 0.571, Test accuracy: 75.18 

Round  23, Train loss: 0.426, Test loss: 0.555, Test accuracy: 75.92 

Round  24, Train loss: 0.428, Test loss: 0.558, Test accuracy: 76.08 

Round  25, Train loss: 0.431, Test loss: 0.562, Test accuracy: 75.57 

Round  26, Train loss: 0.368, Test loss: 0.537, Test accuracy: 76.71 

Round  27, Train loss: 0.356, Test loss: 0.527, Test accuracy: 77.31 

Round  28, Train loss: 0.372, Test loss: 0.522, Test accuracy: 77.37 

Round  29, Train loss: 0.375, Test loss: 0.513, Test accuracy: 78.24 

Round  30, Train loss: 0.394, Test loss: 0.507, Test accuracy: 78.37 

Round  31, Train loss: 0.320, Test loss: 0.485, Test accuracy: 79.20 

Round  32, Train loss: 0.373, Test loss: 0.489, Test accuracy: 79.04 

Round  33, Train loss: 0.310, Test loss: 0.484, Test accuracy: 79.33 

Round  34, Train loss: 0.340, Test loss: 0.488, Test accuracy: 79.62 

Round  35, Train loss: 0.332, Test loss: 0.476, Test accuracy: 80.20 

Round  36, Train loss: 0.359, Test loss: 0.478, Test accuracy: 80.11 

Round  37, Train loss: 0.387, Test loss: 0.475, Test accuracy: 80.11 

Round  38, Train loss: 0.315, Test loss: 0.448, Test accuracy: 81.41 

Round  39, Train loss: 0.370, Test loss: 0.463, Test accuracy: 81.12 

Round  40, Train loss: 0.317, Test loss: 0.459, Test accuracy: 81.33 

Round  41, Train loss: 0.301, Test loss: 0.446, Test accuracy: 81.69 

Round  42, Train loss: 0.339, Test loss: 0.461, Test accuracy: 81.30 

Round  43, Train loss: 0.342, Test loss: 0.456, Test accuracy: 81.53 

Round  44, Train loss: 0.294, Test loss: 0.460, Test accuracy: 81.26 

Round  45, Train loss: 0.255, Test loss: 0.447, Test accuracy: 82.24 

Round  46, Train loss: 0.258, Test loss: 0.447, Test accuracy: 81.83 

Round  47, Train loss: 0.262, Test loss: 0.450, Test accuracy: 81.68 

Round  48, Train loss: 0.266, Test loss: 0.438, Test accuracy: 82.33 

Round  49, Train loss: 0.286, Test loss: 0.443, Test accuracy: 82.07 

Round  50, Train loss: 0.250, Test loss: 0.463, Test accuracy: 81.59 

Round  51, Train loss: 0.279, Test loss: 0.437, Test accuracy: 82.38 

Round  52, Train loss: 0.225, Test loss: 0.455, Test accuracy: 82.22 

Round  53, Train loss: 0.244, Test loss: 0.431, Test accuracy: 82.80 

Round  54, Train loss: 0.302, Test loss: 0.443, Test accuracy: 82.23 

Round  55, Train loss: 0.261, Test loss: 0.429, Test accuracy: 82.65 

Round  56, Train loss: 0.292, Test loss: 0.434, Test accuracy: 82.90 

Round  57, Train loss: 0.250, Test loss: 0.425, Test accuracy: 83.52 

Round  58, Train loss: 0.245, Test loss: 0.411, Test accuracy: 83.84 

Round  59, Train loss: 0.254, Test loss: 0.431, Test accuracy: 83.00 

Round  60, Train loss: 0.230, Test loss: 0.427, Test accuracy: 83.21 

Round  61, Train loss: 0.277, Test loss: 0.428, Test accuracy: 82.97 

Round  62, Train loss: 0.189, Test loss: 0.435, Test accuracy: 83.59 

Round  63, Train loss: 0.259, Test loss: 0.428, Test accuracy: 83.28 

Round  64, Train loss: 0.233, Test loss: 0.433, Test accuracy: 83.25 

Round  65, Train loss: 0.250, Test loss: 0.418, Test accuracy: 83.72 

Round  66, Train loss: 0.234, Test loss: 0.415, Test accuracy: 83.90 

Round  67, Train loss: 0.261, Test loss: 0.415, Test accuracy: 84.09 

Round  68, Train loss: 0.211, Test loss: 0.440, Test accuracy: 83.26 

Round  69, Train loss: 0.246, Test loss: 0.427, Test accuracy: 84.18 

Round  70, Train loss: 0.186, Test loss: 0.423, Test accuracy: 84.07 

Round  71, Train loss: 0.198, Test loss: 0.420, Test accuracy: 84.62 

Round  72, Train loss: 0.271, Test loss: 0.425, Test accuracy: 84.02 

Round  73, Train loss: 0.200, Test loss: 0.433, Test accuracy: 84.06 

Round  74, Train loss: 0.180, Test loss: 0.426, Test accuracy: 84.38 

Round  75, Train loss: 0.179, Test loss: 0.420, Test accuracy: 84.56 

Round  76, Train loss: 0.194, Test loss: 0.432, Test accuracy: 84.39 

Round  77, Train loss: 0.222, Test loss: 0.434, Test accuracy: 83.92 

Round  78, Train loss: 0.188, Test loss: 0.435, Test accuracy: 84.47 

Round  79, Train loss: 0.212, Test loss: 0.428, Test accuracy: 84.68 

Round  80, Train loss: 0.228, Test loss: 0.425, Test accuracy: 84.40 

Round  81, Train loss: 0.220, Test loss: 0.430, Test accuracy: 84.16 

Round  82, Train loss: 0.188, Test loss: 0.414, Test accuracy: 84.50 

Round  83, Train loss: 0.141, Test loss: 0.428, Test accuracy: 84.75 

Round  84, Train loss: 0.195, Test loss: 0.417, Test accuracy: 84.52 

Round  85, Train loss: 0.145, Test loss: 0.422, Test accuracy: 84.64 

Round  86, Train loss: 0.168, Test loss: 0.435, Test accuracy: 84.33 

Round  87, Train loss: 0.181, Test loss: 0.421, Test accuracy: 84.92 

Round  88, Train loss: 0.160, Test loss: 0.430, Test accuracy: 84.86 

Round  89, Train loss: 0.237, Test loss: 0.428, Test accuracy: 84.51 

Round  90, Train loss: 0.209, Test loss: 0.431, Test accuracy: 84.89 

Round  91, Train loss: 0.148, Test loss: 0.437, Test accuracy: 84.67 

Round  92, Train loss: 0.164, Test loss: 0.443, Test accuracy: 84.79 

Round  93, Train loss: 0.214, Test loss: 0.442, Test accuracy: 84.65 

Round  94, Train loss: 0.214, Test loss: 0.448, Test accuracy: 84.43 

Round  95, Train loss: 0.222, Test loss: 0.441, Test accuracy: 84.62 

Round  96, Train loss: 0.140, Test loss: 0.449, Test accuracy: 84.57 

Round  97, Train loss: 0.162, Test loss: 0.434, Test accuracy: 84.69 

Round  98, Train loss: 0.169, Test loss: 0.443, Test accuracy: 84.58 

Round  99, Train loss: 0.151, Test loss: 0.462, Test accuracy: 84.39 

Final Round, Train loss: 0.152, Test loss: 0.430, Test accuracy: 85.40 

Average accuracy final 10 rounds: 84.62916666666666 

998.9166803359985
[1.4085090160369873, 2.5901942253112793, 3.7717905044555664, 4.947521924972534, 6.118747711181641, 7.290632009506226, 8.460937738418579, 9.631439924240112, 10.80173134803772, 11.972764015197754, 13.143340349197388, 14.319161176681519, 15.48944902420044, 16.666112661361694, 17.84020686149597, 19.013636112213135, 20.186256408691406, 21.356943130493164, 22.51974129676819, 23.68726873397827, 24.85544514656067, 26.030997276306152, 27.2097384929657, 28.386578798294067, 29.568939924240112, 30.74777579307556, 31.923850774765015, 33.1014986038208, 34.280696868896484, 35.4580352306366, 36.63169550895691, 37.810856342315674, 38.98966145515442, 40.16828417778015, 41.348604679107666, 42.528897523880005, 43.70624852180481, 44.88362789154053, 46.066184520721436, 47.24885034561157, 48.427539587020874, 49.60358738899231, 50.781264305114746, 51.95056676864624, 53.12036156654358, 54.29147005081177, 55.464146852493286, 56.636860370635986, 57.809197425842285, 58.989086866378784, 60.1667594909668, 61.34241342544556, 62.51440668106079, 63.684160232543945, 64.85339522361755, 66.02288722991943, 67.19371128082275, 68.36122226715088, 69.5300886631012, 70.69880676269531, 71.86772060394287, 73.03893041610718, 74.20987057685852, 75.37513065338135, 76.53219056129456, 77.69051885604858, 78.84786319732666, 80.00457167625427, 81.15838146209717, 82.32045388221741, 83.32124352455139, 84.32499241828918, 85.32964396476746, 86.32960605621338, 87.33585214614868, 88.33821392059326, 89.33717966079712, 90.34098935127258, 91.34441900253296, 92.34354186058044, 93.34468626976013, 94.34829211235046, 95.35561943054199, 96.3576648235321, 97.35460209846497, 98.35438799858093, 99.35573053359985, 100.35546255111694, 101.35947227478027, 102.36605167388916, 103.37003636360168, 104.37153267860413, 105.3788423538208, 106.38334035873413, 107.38468432426453, 108.39137482643127, 109.39714431762695, 110.3999547958374, 111.40350580215454, 112.41079640388489, 114.19190549850464]
[33.85, 40.19166666666667, 41.34166666666667, 47.375, 42.59166666666667, 48.31666666666667, 53.125, 54.53333333333333, 61.78333333333333, 61.05, 60.608333333333334, 66.41666666666667, 65.43333333333334, 65.93333333333334, 68.31666666666666, 69.95, 69.90833333333333, 71.875, 73.09166666666667, 72.69166666666666, 73.70833333333333, 72.99166666666666, 75.18333333333334, 75.925, 76.08333333333333, 75.56666666666666, 76.70833333333333, 77.30833333333334, 77.36666666666666, 78.24166666666666, 78.36666666666666, 79.2, 79.04166666666667, 79.33333333333333, 79.61666666666666, 80.2, 80.10833333333333, 80.10833333333333, 81.40833333333333, 81.125, 81.325, 81.69166666666666, 81.3, 81.53333333333333, 81.25833333333334, 82.24166666666666, 81.83333333333333, 81.68333333333334, 82.325, 82.06666666666666, 81.59166666666667, 82.375, 82.21666666666667, 82.8, 82.23333333333333, 82.65, 82.9, 83.51666666666667, 83.84166666666667, 83.0, 83.20833333333333, 82.96666666666667, 83.59166666666667, 83.28333333333333, 83.25, 83.725, 83.9, 84.09166666666667, 83.25833333333334, 84.18333333333334, 84.06666666666666, 84.61666666666666, 84.01666666666667, 84.05833333333334, 84.38333333333334, 84.55833333333334, 84.39166666666667, 83.925, 84.46666666666667, 84.68333333333334, 84.4, 84.15833333333333, 84.5, 84.75, 84.51666666666667, 84.64166666666667, 84.33333333333333, 84.91666666666667, 84.85833333333333, 84.50833333333334, 84.89166666666667, 84.675, 84.79166666666667, 84.65, 84.43333333333334, 84.61666666666666, 84.56666666666666, 84.69166666666666, 84.58333333333333, 84.39166666666667, 85.4]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 7939 (global); Percentage 2.58 (7939/307387 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.897, Test loss: 1.004, Test accuracy: 44.82 

Round   1, Train loss: 0.788, Test loss: 0.998, Test accuracy: 49.98 

Round   2, Train loss: 0.663, Test loss: 0.866, Test accuracy: 56.74 

Round   3, Train loss: 0.684, Test loss: 0.819, Test accuracy: 61.35 

Round   4, Train loss: 0.624, Test loss: 0.712, Test accuracy: 66.10 

Round   5, Train loss: 0.595, Test loss: 0.717, Test accuracy: 68.38 

Round   6, Train loss: 0.646, Test loss: 0.665, Test accuracy: 70.31 

Round   7, Train loss: 0.531, Test loss: 0.604, Test accuracy: 73.97 

Round   8, Train loss: 0.517, Test loss: 0.598, Test accuracy: 74.66 

Round   9, Train loss: 0.476, Test loss: 0.582, Test accuracy: 75.58 

Round  10, Train loss: 0.510, Test loss: 0.591, Test accuracy: 75.45 

Round  11, Train loss: 0.429, Test loss: 0.578, Test accuracy: 75.89 

Round  12, Train loss: 0.453, Test loss: 0.566, Test accuracy: 76.25 

Round  13, Train loss: 0.478, Test loss: 0.557, Test accuracy: 76.60 

Round  14, Train loss: 0.443, Test loss: 0.572, Test accuracy: 76.40 

Round  15, Train loss: 0.445, Test loss: 0.580, Test accuracy: 76.40 

Round  16, Train loss: 0.446, Test loss: 0.562, Test accuracy: 77.18 

Round  17, Train loss: 0.374, Test loss: 0.571, Test accuracy: 77.35 

Round  18, Train loss: 0.350, Test loss: 0.579, Test accuracy: 77.18 

Round  19, Train loss: 0.451, Test loss: 0.572, Test accuracy: 78.09 

Round  20, Train loss: 0.431, Test loss: 0.572, Test accuracy: 78.17 

Round  21, Train loss: 0.364, Test loss: 0.563, Test accuracy: 78.89 

Round  22, Train loss: 0.332, Test loss: 0.557, Test accuracy: 79.35 

Round  23, Train loss: 0.293, Test loss: 0.581, Test accuracy: 79.03 

Round  24, Train loss: 0.268, Test loss: 0.597, Test accuracy: 78.69 

Round  25, Train loss: 0.336, Test loss: 0.603, Test accuracy: 78.65 

Round  26, Train loss: 0.322, Test loss: 0.618, Test accuracy: 78.72 

Round  27, Train loss: 0.256, Test loss: 0.618, Test accuracy: 79.00 

Round  28, Train loss: 0.272, Test loss: 0.610, Test accuracy: 79.43 

Round  29, Train loss: 0.247, Test loss: 0.644, Test accuracy: 78.63 

Round  30, Train loss: 0.250, Test loss: 0.634, Test accuracy: 78.80 

Round  31, Train loss: 0.228, Test loss: 0.630, Test accuracy: 79.47 

Round  32, Train loss: 0.217, Test loss: 0.615, Test accuracy: 79.69 

Round  33, Train loss: 0.208, Test loss: 0.623, Test accuracy: 79.47 

Round  34, Train loss: 0.267, Test loss: 0.646, Test accuracy: 79.83 

Round  35, Train loss: 0.227, Test loss: 0.666, Test accuracy: 79.70 

Round  36, Train loss: 0.276, Test loss: 0.695, Test accuracy: 79.24 

Round  37, Train loss: 0.175, Test loss: 0.687, Test accuracy: 79.59 

Round  38, Train loss: 0.250, Test loss: 0.675, Test accuracy: 79.72 

Round  39, Train loss: 0.202, Test loss: 0.685, Test accuracy: 79.69 

Round  40, Train loss: 0.198, Test loss: 0.700, Test accuracy: 79.79 

Round  41, Train loss: 0.217, Test loss: 0.685, Test accuracy: 80.09 

Round  42, Train loss: 0.163, Test loss: 0.702, Test accuracy: 79.91 

Round  43, Train loss: 0.192, Test loss: 0.721, Test accuracy: 79.40 

Round  44, Train loss: 0.182, Test loss: 0.728, Test accuracy: 78.99 

Round  45, Train loss: 0.158, Test loss: 0.723, Test accuracy: 79.43 

Round  46, Train loss: 0.198, Test loss: 0.722, Test accuracy: 79.84 

Round  47, Train loss: 0.136, Test loss: 0.746, Test accuracy: 79.74 

Round  48, Train loss: 0.152, Test loss: 0.738, Test accuracy: 80.22 

Round  49, Train loss: 0.118, Test loss: 0.748, Test accuracy: 80.22 

Round  50, Train loss: 0.171, Test loss: 0.757, Test accuracy: 80.24 

Round  51, Train loss: 0.114, Test loss: 0.771, Test accuracy: 80.52 

Round  52, Train loss: 0.164, Test loss: 0.767, Test accuracy: 80.55 

Round  53, Train loss: 0.133, Test loss: 0.817, Test accuracy: 80.24 

Round  54, Train loss: 0.092, Test loss: 0.831, Test accuracy: 80.43 

Round  55, Train loss: 0.107, Test loss: 0.852, Test accuracy: 80.51 

Round  56, Train loss: 0.128, Test loss: 0.828, Test accuracy: 80.47 

Round  57, Train loss: 0.147, Test loss: 0.821, Test accuracy: 80.29 

Round  58, Train loss: 0.114, Test loss: 0.845, Test accuracy: 79.82 

Round  59, Train loss: 0.102, Test loss: 0.835, Test accuracy: 80.21 

Round  60, Train loss: 0.115, Test loss: 0.865, Test accuracy: 79.99 

Round  61, Train loss: 0.105, Test loss: 0.868, Test accuracy: 79.46 

Round  62, Train loss: 0.101, Test loss: 0.857, Test accuracy: 80.11 

Round  63, Train loss: 0.079, Test loss: 0.901, Test accuracy: 79.68 

Round  64, Train loss: 0.072, Test loss: 0.936, Test accuracy: 79.32 

Round  65, Train loss: 0.089, Test loss: 0.913, Test accuracy: 79.70 

Round  66, Train loss: 0.097, Test loss: 0.901, Test accuracy: 79.99 

Round  67, Train loss: 0.082, Test loss: 0.918, Test accuracy: 79.83 

Round  68, Train loss: 0.076, Test loss: 0.924, Test accuracy: 79.70 

Round  69, Train loss: 0.091, Test loss: 0.916, Test accuracy: 79.96 

Round  70, Train loss: 0.075, Test loss: 0.926, Test accuracy: 80.17 

Round  71, Train loss: 0.097, Test loss: 0.902, Test accuracy: 80.45 

Round  72, Train loss: 0.078, Test loss: 0.909, Test accuracy: 80.51 

Round  73, Train loss: 0.064, Test loss: 0.940, Test accuracy: 80.55 

Round  74, Train loss: 0.065, Test loss: 0.955, Test accuracy: 80.48 

Round  75, Train loss: 0.075, Test loss: 0.936, Test accuracy: 80.17 

Round  76, Train loss: 0.057, Test loss: 0.937, Test accuracy: 80.15 

Round  77, Train loss: 0.093, Test loss: 0.942, Test accuracy: 80.34 

Round  78, Train loss: 0.082, Test loss: 0.983, Test accuracy: 80.17 

Round  79, Train loss: 0.091, Test loss: 0.972, Test accuracy: 80.32 

Round  80, Train loss: 0.084, Test loss: 0.975, Test accuracy: 80.34 

Round  81, Train loss: 0.057, Test loss: 0.968, Test accuracy: 80.69 

Round  82, Train loss: 0.051, Test loss: 0.987, Test accuracy: 80.51 

Round  83, Train loss: 0.064, Test loss: 1.009, Test accuracy: 80.47 

Round  84, Train loss: 0.056, Test loss: 1.021, Test accuracy: 80.23 

Round  85, Train loss: 0.072, Test loss: 1.038, Test accuracy: 79.89 

Round  86, Train loss: 0.063, Test loss: 1.025, Test accuracy: 80.01 

Round  87, Train loss: 0.067, Test loss: 0.993, Test accuracy: 80.35 

Round  88, Train loss: 0.038, Test loss: 1.023, Test accuracy: 80.22 

Round  89, Train loss: 0.050, Test loss: 1.012, Test accuracy: 80.48 

Round  90, Train loss: 0.056, Test loss: 0.989, Test accuracy: 80.53 

Round  91, Train loss: 0.034, Test loss: 1.020, Test accuracy: 80.45 

Round  92, Train loss: 0.034, Test loss: 1.032, Test accuracy: 80.67 

Round  93, Train loss: 0.073, Test loss: 1.039, Test accuracy: 80.56 

Round  94, Train loss: 0.050, Test loss: 1.023, Test accuracy: 80.67 

Round  95, Train loss: 0.038, Test loss: 1.062, Test accuracy: 80.57 

Round  96, Train loss: 0.057, Test loss: 1.042, Test accuracy: 80.53 

Round  97, Train loss: 0.051, Test loss: 1.048, Test accuracy: 80.86 

Round  98, Train loss: 0.044, Test loss: 1.062, Test accuracy: 80.73 

Round  99, Train loss: 0.040, Test loss: 1.053, Test accuracy: 80.97 

Final Round, Train loss: 0.042, Test loss: 1.089, Test accuracy: 80.88 

Average accuracy final 10 rounds: 80.65416666666665 

937.005669593811
[1.4098539352416992, 2.4158058166503906, 3.4216089248657227, 4.428410291671753, 5.435052156448364, 6.444446563720703, 7.449214220046997, 8.453679084777832, 9.461769104003906, 10.471785545349121, 11.47831654548645, 12.487751007080078, 13.496224880218506, 14.50154709815979, 15.513338565826416, 16.524377584457397, 17.532420873641968, 18.54372811317444, 19.55321502685547, 20.563546657562256, 21.572382926940918, 22.583727836608887, 23.589641571044922, 24.59340190887451, 25.604132413864136, 26.614034175872803, 27.621422052383423, 28.631205081939697, 29.638116359710693, 30.64224410057068, 31.648173570632935, 32.65319585800171, 33.66315054893494, 34.67139720916748, 35.677531003952026, 36.68837881088257, 37.696349143981934, 38.70346188545227, 39.718096017837524, 40.72945237159729, 41.73827648162842, 42.74496626853943, 43.756500482559204, 44.76528453826904, 45.771955490112305, 46.78225326538086, 47.78989362716675, 48.8039186000824, 49.81739830970764, 50.82647705078125, 51.833635091781616, 52.839104652404785, 53.84592628479004, 54.851417541503906, 55.86699151992798, 56.87211847305298, 57.87715530395508, 58.89114189147949, 59.89658546447754, 60.90212035179138, 61.91001081466675, 62.91647124290466, 63.92545127868652, 64.93907070159912, 65.95753574371338, 66.96382594108582, 67.97474455833435, 68.98419380187988, 69.99170589447021, 71.00402092933655, 72.01293110847473, 73.01830720901489, 74.02392864227295, 75.03561449050903, 76.0446400642395, 77.04865407943726, 78.06041860580444, 79.06732559204102, 80.0919041633606, 81.1044397354126, 82.11010408401489, 83.11414766311646, 84.11846542358398, 85.12079882621765, 86.12780928611755, 87.13494729995728, 88.13496279716492, 89.14336657524109, 90.15049648284912, 91.15417838096619, 92.16356301307678, 93.17098498344421, 94.1776933670044, 95.1859622001648, 96.19542694091797, 97.20220470428467, 98.2130823135376, 99.22365021705627, 100.23233580589294, 101.2394073009491, 103.20514512062073]
[44.81666666666667, 49.975, 56.74166666666667, 61.35, 66.1, 68.38333333333334, 70.30833333333334, 73.96666666666667, 74.65833333333333, 75.58333333333333, 75.45, 75.89166666666667, 76.25, 76.6, 76.4, 76.4, 77.18333333333334, 77.35, 77.18333333333334, 78.09166666666667, 78.16666666666667, 78.89166666666667, 79.35, 79.03333333333333, 78.69166666666666, 78.65, 78.725, 79.0, 79.43333333333334, 78.63333333333334, 78.8, 79.46666666666667, 79.69166666666666, 79.475, 79.825, 79.7, 79.24166666666666, 79.59166666666667, 79.71666666666667, 79.69166666666666, 79.79166666666667, 80.09166666666667, 79.90833333333333, 79.4, 78.99166666666666, 79.43333333333334, 79.84166666666667, 79.74166666666666, 80.225, 80.21666666666667, 80.24166666666666, 80.51666666666667, 80.55, 80.24166666666666, 80.43333333333334, 80.50833333333334, 80.475, 80.29166666666667, 79.81666666666666, 80.20833333333333, 79.99166666666666, 79.45833333333333, 80.10833333333333, 79.68333333333334, 79.31666666666666, 79.7, 79.99166666666666, 79.825, 79.7, 79.95833333333333, 80.175, 80.45, 80.50833333333334, 80.55, 80.48333333333333, 80.16666666666667, 80.15, 80.34166666666667, 80.16666666666667, 80.31666666666666, 80.34166666666667, 80.69166666666666, 80.50833333333334, 80.475, 80.23333333333333, 79.89166666666667, 80.00833333333334, 80.35, 80.21666666666667, 80.48333333333333, 80.53333333333333, 80.45, 80.66666666666667, 80.55833333333334, 80.675, 80.56666666666666, 80.53333333333333, 80.85833333333333, 80.73333333333333, 80.96666666666667, 80.875]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 0.635, Test loss: 1.104, Test accuracy: 35.71
Round   1, Train loss: 0.511, Test loss: 1.117, Test accuracy: 44.25
Round   2, Train loss: 0.527, Test loss: 1.125, Test accuracy: 44.76
Round   3, Train loss: 0.478, Test loss: 1.065, Test accuracy: 48.25
Round   4, Train loss: 0.470, Test loss: 1.113, Test accuracy: 49.14
Round   5, Train loss: 0.377, Test loss: 1.126, Test accuracy: 52.38
Round   6, Train loss: 0.438, Test loss: 1.145, Test accuracy: 52.02
Round   7, Train loss: 0.410, Test loss: 1.041, Test accuracy: 53.57
Round   8, Train loss: 0.353, Test loss: 1.046, Test accuracy: 56.03
Round   9, Train loss: 0.366, Test loss: 1.034, Test accuracy: 56.10
Round  10, Train loss: 0.411, Test loss: 0.972, Test accuracy: 58.49
Round  11, Train loss: 0.325, Test loss: 0.962, Test accuracy: 61.03
Round  12, Train loss: 0.317, Test loss: 0.951, Test accuracy: 63.16
Round  13, Train loss: 0.294, Test loss: 0.939, Test accuracy: 63.92
Round  14, Train loss: 0.328, Test loss: 0.937, Test accuracy: 63.89
Round  15, Train loss: 0.319, Test loss: 0.929, Test accuracy: 64.01
Round  16, Train loss: 0.324, Test loss: 0.927, Test accuracy: 63.96
Round  17, Train loss: 0.249, Test loss: 0.913, Test accuracy: 65.23
Round  18, Train loss: 0.296, Test loss: 0.910, Test accuracy: 64.68
Round  19, Train loss: 0.280, Test loss: 0.893, Test accuracy: 67.00
Round  20, Train loss: 0.270, Test loss: 0.888, Test accuracy: 66.84
Round  21, Train loss: 0.199, Test loss: 0.875, Test accuracy: 67.02
Round  22, Train loss: 0.302, Test loss: 0.872, Test accuracy: 67.54
Round  23, Train loss: 0.243, Test loss: 0.862, Test accuracy: 67.94
Round  24, Train loss: 0.251, Test loss: 0.859, Test accuracy: 68.42
Round  25, Train loss: 0.225, Test loss: 0.859, Test accuracy: 68.07
Round  26, Train loss: 0.234, Test loss: 0.858, Test accuracy: 67.62
Round  27, Train loss: 0.243, Test loss: 0.849, Test accuracy: 67.28
Round  28, Train loss: 0.223, Test loss: 0.844, Test accuracy: 67.62
Round  29, Train loss: 0.191, Test loss: 0.829, Test accuracy: 68.51
Round  30, Train loss: 0.228, Test loss: 0.825, Test accuracy: 68.90
Round  31, Train loss: 0.176, Test loss: 0.828, Test accuracy: 68.10
Round  32, Train loss: 0.241, Test loss: 0.815, Test accuracy: 68.26
Round  33, Train loss: 0.152, Test loss: 0.809, Test accuracy: 68.62
Round  34, Train loss: 0.190, Test loss: 0.809, Test accuracy: 68.72
Round  35, Train loss: 0.212, Test loss: 0.805, Test accuracy: 68.59
Round  36, Train loss: 0.129, Test loss: 0.792, Test accuracy: 69.70
Round  37, Train loss: 0.175, Test loss: 0.791, Test accuracy: 69.91
Round  38, Train loss: 0.158, Test loss: 0.785, Test accuracy: 69.91
Round  39, Train loss: 0.141, Test loss: 0.786, Test accuracy: 69.69
Round  40, Train loss: 0.203, Test loss: 0.794, Test accuracy: 68.48
Round  41, Train loss: 0.143, Test loss: 0.790, Test accuracy: 67.78
Round  42, Train loss: 0.156, Test loss: 0.782, Test accuracy: 69.26
Round  43, Train loss: 0.132, Test loss: 0.779, Test accuracy: 68.90
Round  44, Train loss: 0.119, Test loss: 0.773, Test accuracy: 69.18
Round  45, Train loss: 0.153, Test loss: 0.769, Test accuracy: 69.52
Round  46, Train loss: 0.103, Test loss: 0.772, Test accuracy: 69.36
Round  47, Train loss: 0.123, Test loss: 0.774, Test accuracy: 68.20
Round  48, Train loss: 0.122, Test loss: 0.763, Test accuracy: 69.06
Round  49, Train loss: 0.118, Test loss: 0.758, Test accuracy: 69.45
Round  50, Train loss: 0.165, Test loss: 0.751, Test accuracy: 70.67
Round  51, Train loss: 0.132, Test loss: 0.749, Test accuracy: 70.33
Round  52, Train loss: 0.137, Test loss: 0.749, Test accuracy: 69.70
Round  53, Train loss: 0.154, Test loss: 0.745, Test accuracy: 69.93
Round  54, Train loss: 0.087, Test loss: 0.750, Test accuracy: 69.18
Round  55, Train loss: 0.078, Test loss: 0.738, Test accuracy: 69.99
Round  56, Train loss: 0.107, Test loss: 0.747, Test accuracy: 69.02
Round  57, Train loss: 0.080, Test loss: 0.742, Test accuracy: 69.16
Round  58, Train loss: 0.078, Test loss: 0.729, Test accuracy: 70.01
Round  59, Train loss: 0.132, Test loss: 0.733, Test accuracy: 69.98
Round  60, Train loss: 0.128, Test loss: 0.725, Test accuracy: 70.28
Round  61, Train loss: 0.082, Test loss: 0.714, Test accuracy: 71.16
Round  62, Train loss: 0.078, Test loss: 0.713, Test accuracy: 70.97
Round  63, Train loss: 0.126, Test loss: 0.720, Test accuracy: 70.62
Round  64, Train loss: 0.087, Test loss: 0.716, Test accuracy: 70.88
Round  65, Train loss: 0.090, Test loss: 0.714, Test accuracy: 70.43
Round  66, Train loss: 0.084, Test loss: 0.723, Test accuracy: 69.53
Round  67, Train loss: 0.084, Test loss: 0.712, Test accuracy: 70.38
Round  68, Train loss: 0.118, Test loss: 0.714, Test accuracy: 69.92
Round  69, Train loss: 0.123, Test loss: 0.712, Test accuracy: 70.27
Round  70, Train loss: 0.076, Test loss: 0.709, Test accuracy: 70.66
Round  71, Train loss: 0.093, Test loss: 0.727, Test accuracy: 68.92
Round  72, Train loss: 0.097, Test loss: 0.715, Test accuracy: 69.53
Round  73, Train loss: 0.071, Test loss: 0.711, Test accuracy: 69.18
Round  74, Train loss: 0.094, Test loss: 0.699, Test accuracy: 70.00
Round  75, Train loss: 0.079, Test loss: 0.706, Test accuracy: 69.63
Round  76, Train loss: 0.097, Test loss: 0.700, Test accuracy: 69.88
Round  77, Train loss: 0.088, Test loss: 0.703, Test accuracy: 69.67
Round  78, Train loss: 0.077, Test loss: 0.702, Test accuracy: 69.47
Round  79, Train loss: 0.085, Test loss: 0.705, Test accuracy: 69.11
Round  80, Train loss: 0.056, Test loss: 0.699, Test accuracy: 69.28
Round  81, Train loss: 0.071, Test loss: 0.700, Test accuracy: 68.93
Round  82, Train loss: 0.057, Test loss: 0.691, Test accuracy: 69.62
Round  83, Train loss: 0.056, Test loss: 0.693, Test accuracy: 69.50
Round  84, Train loss: 0.090, Test loss: 0.699, Test accuracy: 69.03
Round  85, Train loss: 0.064, Test loss: 0.696, Test accuracy: 69.35
Round  86, Train loss: 0.051, Test loss: 0.694, Test accuracy: 69.68
Round  87, Train loss: 0.056, Test loss: 0.691, Test accuracy: 70.01
Round  88, Train loss: 0.064, Test loss: 0.701, Test accuracy: 68.42
Round  89, Train loss: 0.093, Test loss: 0.707, Test accuracy: 68.42
Round  90, Train loss: 0.088, Test loss: 0.693, Test accuracy: 68.84
Round  91, Train loss: 0.053, Test loss: 0.683, Test accuracy: 69.72
Round  92, Train loss: 0.069, Test loss: 0.686, Test accuracy: 69.08
Round  93, Train loss: 0.068, Test loss: 0.677, Test accuracy: 70.50
Round  94, Train loss: 0.069, Test loss: 0.673, Test accuracy: 70.71
Round  95, Train loss: 0.056, Test loss: 0.668, Test accuracy: 71.21
Round  96, Train loss: 0.064, Test loss: 0.676, Test accuracy: 70.99
Round  97, Train loss: 0.071, Test loss: 0.686, Test accuracy: 70.17
Round  98, Train loss: 0.080, Test loss: 0.677, Test accuracy: 70.97
Round  99, Train loss: 0.076, Test loss: 0.683, Test accuracy: 70.70
Final Round, Train loss: 0.058, Test loss: 0.697, Test accuracy: 69.34
Average accuracy final 10 rounds: 70.29083333333332
1770.3711578845978
[]
[35.708333333333336, 44.25, 44.75833333333333, 48.25, 49.141666666666666, 52.375, 52.016666666666666, 53.56666666666667, 56.03333333333333, 56.1, 58.49166666666667, 61.03333333333333, 63.15833333333333, 63.925, 63.891666666666666, 64.00833333333334, 63.958333333333336, 65.23333333333333, 64.68333333333334, 67.0, 66.84166666666667, 67.01666666666667, 67.54166666666667, 67.94166666666666, 68.41666666666667, 68.06666666666666, 67.625, 67.28333333333333, 67.625, 68.50833333333334, 68.9, 68.1, 68.25833333333334, 68.625, 68.71666666666667, 68.59166666666667, 69.7, 69.90833333333333, 69.90833333333333, 69.69166666666666, 68.48333333333333, 67.775, 69.25833333333334, 68.9, 69.18333333333334, 69.51666666666667, 69.35833333333333, 68.2, 69.05833333333334, 69.45, 70.66666666666667, 70.33333333333333, 69.7, 69.93333333333334, 69.18333333333334, 69.99166666666666, 69.01666666666667, 69.15833333333333, 70.00833333333334, 69.98333333333333, 70.275, 71.15833333333333, 70.975, 70.625, 70.88333333333334, 70.43333333333334, 69.53333333333333, 70.375, 69.91666666666667, 70.26666666666667, 70.65833333333333, 68.91666666666667, 69.525, 69.18333333333334, 70.0, 69.63333333333334, 69.875, 69.66666666666667, 69.46666666666667, 69.10833333333333, 69.275, 68.93333333333334, 69.61666666666666, 69.5, 69.025, 69.35, 69.68333333333334, 70.00833333333334, 68.41666666666667, 68.41666666666667, 68.84166666666667, 69.725, 69.08333333333333, 70.5, 70.70833333333333, 71.20833333333333, 70.99166666666666, 70.175, 70.975, 70.7, 69.34166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
Round   0, Train loss: 0.910, Test loss: 0.999, Test accuracy: 42.65
Round   0: Global train loss: 0.910, Global test loss: 1.099, Global test accuracy: 32.88
Round   1, Train loss: 0.738, Test loss: 0.964, Test accuracy: 46.02
Round   1: Global train loss: 0.738, Global test loss: 1.099, Global test accuracy: 33.17
Round   2, Train loss: 0.680, Test loss: 0.900, Test accuracy: 50.87
Round   2: Global train loss: 0.680, Global test loss: 1.099, Global test accuracy: 32.36
Round   3, Train loss: 0.522, Test loss: 0.844, Test accuracy: 55.77
Round   3: Global train loss: 0.522, Global test loss: 1.099, Global test accuracy: 32.94
Round   4, Train loss: 0.551, Test loss: 0.824, Test accuracy: 57.69
Round   4: Global train loss: 0.551, Global test loss: 1.099, Global test accuracy: 32.63
Round   5, Train loss: 0.027, Test loss: 0.794, Test accuracy: 59.42
Round   5: Global train loss: 0.027, Global test loss: 1.098, Global test accuracy: 32.88
Round   6, Train loss: 0.527, Test loss: 0.766, Test accuracy: 61.67
Round   6: Global train loss: 0.527, Global test loss: 1.098, Global test accuracy: 32.15
Round   7, Train loss: -0.160, Test loss: 0.748, Test accuracy: 62.32
Round   7: Global train loss: -0.160, Global test loss: 1.098, Global test accuracy: 32.70
Round   8, Train loss: 0.067, Test loss: 0.748, Test accuracy: 62.12
Round   8: Global train loss: 0.067, Global test loss: 1.099, Global test accuracy: 32.02
Round   9, Train loss: -0.426, Test loss: 0.707, Test accuracy: 64.42
Round   9: Global train loss: -0.426, Global test loss: 1.098, Global test accuracy: 32.46
Round  10, Train loss: -0.359, Test loss: 0.730, Test accuracy: 63.94
Round  10: Global train loss: -0.359, Global test loss: 1.099, Global test accuracy: 32.29
Round  11, Train loss: -0.697, Test loss: 0.715, Test accuracy: 65.83
Round  11: Global train loss: -0.697, Global test loss: 1.096, Global test accuracy: 33.46
Round  12, Train loss: -0.085, Test loss: 0.681, Test accuracy: 69.01
Round  12: Global train loss: -0.085, Global test loss: 1.095, Global test accuracy: 35.67
Round  13, Train loss: -0.804, Test loss: 0.664, Test accuracy: 70.38
Round  13: Global train loss: -0.804, Global test loss: 1.095, Global test accuracy: 35.85
Round  14, Train loss: -0.311, Test loss: 0.670, Test accuracy: 69.91
Round  14: Global train loss: -0.311, Global test loss: 1.095, Global test accuracy: 35.91
Round  15, Train loss: -1.058, Test loss: 0.664, Test accuracy: 70.44
Round  15: Global train loss: -1.058, Global test loss: 1.094, Global test accuracy: 36.52
Round  16, Train loss: -0.899, Test loss: 0.657, Test accuracy: 70.50
Round  16: Global train loss: -0.899, Global test loss: 1.094, Global test accuracy: 36.83
Round  17, Train loss: -1.202, Test loss: 0.656, Test accuracy: 70.22
Round  17: Global train loss: -1.202, Global test loss: 1.093, Global test accuracy: 37.62
Round  18, Train loss: -1.190, Test loss: 0.662, Test accuracy: 70.29
Round  18: Global train loss: -1.190, Global test loss: 1.093, Global test accuracy: 37.90
Round  19, Train loss: -0.888, Test loss: 0.662, Test accuracy: 70.36
Round  19: Global train loss: -0.888, Global test loss: 1.093, Global test accuracy: 38.17
Round  20, Train loss: -1.027, Test loss: 0.659, Test accuracy: 70.92
Round  20: Global train loss: -1.027, Global test loss: 1.093, Global test accuracy: 38.08
Round  21, Train loss: -1.325, Test loss: 0.652, Test accuracy: 71.81
Round  21: Global train loss: -1.325, Global test loss: 1.092, Global test accuracy: 38.05
Round  22, Train loss: -1.170, Test loss: 0.635, Test accuracy: 72.63
Round  22: Global train loss: -1.170, Global test loss: 1.092, Global test accuracy: 38.18
Round  23, Train loss: -1.767, Test loss: 0.630, Test accuracy: 72.88
Round  23: Global train loss: -1.767, Global test loss: 1.091, Global test accuracy: 38.13
Round  24, Train loss: -1.481, Test loss: 0.639, Test accuracy: 72.48
Round  24: Global train loss: -1.481, Global test loss: 1.091, Global test accuracy: 38.50
Round  25, Train loss: -1.625, Test loss: 0.613, Test accuracy: 74.60
Round  25: Global train loss: -1.625, Global test loss: 1.090, Global test accuracy: 38.67
Round  26, Train loss: -1.522, Test loss: 0.599, Test accuracy: 75.04
Round  26: Global train loss: -1.522, Global test loss: 1.090, Global test accuracy: 38.58
Round  27, Train loss: -2.424, Test loss: 0.606, Test accuracy: 75.32
Round  27: Global train loss: -2.424, Global test loss: 1.089, Global test accuracy: 38.55
Round  28, Train loss: -1.942, Test loss: 0.602, Test accuracy: 75.09
Round  28: Global train loss: -1.942, Global test loss: 1.089, Global test accuracy: 38.37
Round  29, Train loss: -1.533, Test loss: 0.598, Test accuracy: 75.23
Round  29: Global train loss: -1.533, Global test loss: 1.088, Global test accuracy: 39.07
Round  30, Train loss: -2.304, Test loss: 0.594, Test accuracy: 75.58
Round  30: Global train loss: -2.304, Global test loss: 1.088, Global test accuracy: 38.77
Round  31, Train loss: -2.209, Test loss: 0.574, Test accuracy: 76.67
Round  31: Global train loss: -2.209, Global test loss: 1.088, Global test accuracy: 38.42
Round  32, Train loss: -2.393, Test loss: 0.589, Test accuracy: 75.90
Round  32: Global train loss: -2.393, Global test loss: 1.088, Global test accuracy: 38.77
Round  33, Train loss: -2.258, Test loss: 0.591, Test accuracy: 75.96
Round  33: Global train loss: -2.258, Global test loss: 1.087, Global test accuracy: 38.88
Round  34, Train loss: -2.756, Test loss: 0.622, Test accuracy: 75.00
Round  34: Global train loss: -2.756, Global test loss: 1.087, Global test accuracy: 38.90
Round  35, Train loss: -3.007, Test loss: 0.608, Test accuracy: 75.10
Round  35: Global train loss: -3.007, Global test loss: 1.087, Global test accuracy: 39.23
Round  36, Train loss: -2.295, Test loss: 0.610, Test accuracy: 75.77
Round  36: Global train loss: -2.295, Global test loss: 1.086, Global test accuracy: 39.11
Round  37, Train loss: -2.561, Test loss: 0.608, Test accuracy: 76.18
Round  37: Global train loss: -2.561, Global test loss: 1.086, Global test accuracy: 39.13
Round  38, Train loss: -2.488, Test loss: 0.600, Test accuracy: 76.03
Round  38: Global train loss: -2.488, Global test loss: 1.086, Global test accuracy: 39.17
Round  39, Train loss: -2.458, Test loss: 0.575, Test accuracy: 76.92
Round  39: Global train loss: -2.458, Global test loss: 1.086, Global test accuracy: 39.01
Round  40, Train loss: -2.848, Test loss: 0.562, Test accuracy: 77.51
Round  40: Global train loss: -2.848, Global test loss: 1.086, Global test accuracy: 39.23
Round  41, Train loss: -2.566, Test loss: 0.569, Test accuracy: 77.36
Round  41: Global train loss: -2.566, Global test loss: 1.087, Global test accuracy: 39.21
Round  42, Train loss: -3.249, Test loss: 0.574, Test accuracy: 77.02
Round  42: Global train loss: -3.249, Global test loss: 1.086, Global test accuracy: 39.23
Round  43, Train loss: -3.506, Test loss: 0.574, Test accuracy: 77.08
Round  43: Global train loss: -3.506, Global test loss: 1.086, Global test accuracy: 39.19
Round  44, Train loss: -3.341, Test loss: 0.583, Test accuracy: 77.19
Round  44: Global train loss: -3.341, Global test loss: 1.086, Global test accuracy: 39.08
Round  45, Train loss: -3.651, Test loss: 0.589, Test accuracy: 76.93
Round  45: Global train loss: -3.651, Global test loss: 1.086, Global test accuracy: 38.82
Round  46, Train loss: -3.833, Test loss: 0.602, Test accuracy: 76.71
Round  46: Global train loss: -3.833, Global test loss: 1.086, Global test accuracy: 38.87
Round  47, Train loss: -4.500, Test loss: 0.594, Test accuracy: 77.42
Round  47: Global train loss: -4.500, Global test loss: 1.086, Global test accuracy: 38.77
Round  48, Train loss: -2.985, Test loss: 0.587, Test accuracy: 77.49
Round  48: Global train loss: -2.985, Global test loss: 1.086, Global test accuracy: 38.78
Round  49, Train loss: -3.278, Test loss: 0.586, Test accuracy: 77.22
Round  49: Global train loss: -3.278, Global test loss: 1.086, Global test accuracy: 38.82
Round  50, Train loss: -3.466, Test loss: 0.581, Test accuracy: 77.89
Round  50: Global train loss: -3.466, Global test loss: 1.087, Global test accuracy: 38.63
Round  51, Train loss: -3.965, Test loss: 0.573, Test accuracy: 78.12
Round  51: Global train loss: -3.965, Global test loss: 1.087, Global test accuracy: 38.20
Round  52, Train loss: -4.883, Test loss: 0.569, Test accuracy: 78.67
Round  52: Global train loss: -4.883, Global test loss: 1.087, Global test accuracy: 38.14
Round  53, Train loss: -4.278, Test loss: 0.575, Test accuracy: 78.25
Round  53: Global train loss: -4.278, Global test loss: 1.086, Global test accuracy: 38.14
Round  54, Train loss: -4.584, Test loss: 0.574, Test accuracy: 77.87
Round  54: Global train loss: -4.584, Global test loss: 1.086, Global test accuracy: 38.51
Round  55, Train loss: -3.850, Test loss: 0.594, Test accuracy: 77.29
Round  55: Global train loss: -3.850, Global test loss: 1.086, Global test accuracy: 38.07
Round  56, Train loss: -3.762, Test loss: 0.593, Test accuracy: 77.53
Round  56: Global train loss: -3.762, Global test loss: 1.086, Global test accuracy: 38.52
Round  57, Train loss: -3.585, Test loss: 0.606, Test accuracy: 77.03
Round  57: Global train loss: -3.585, Global test loss: 1.084, Global test accuracy: 38.93
Round  58, Train loss: -4.543, Test loss: 0.593, Test accuracy: 77.58
Round  58: Global train loss: -4.543, Global test loss: 1.084, Global test accuracy: 38.67
Round  59, Train loss: -3.263, Test loss: 0.601, Test accuracy: 76.85
Round  59: Global train loss: -3.263, Global test loss: 1.084, Global test accuracy: 38.59
Round  60, Train loss: -4.482, Test loss: 0.622, Test accuracy: 76.89
Round  60: Global train loss: -4.482, Global test loss: 1.085, Global test accuracy: 38.61
Round  61, Train loss: -4.356, Test loss: 0.608, Test accuracy: 77.55
Round  61: Global train loss: -4.356, Global test loss: 1.084, Global test accuracy: 38.83
Round  62, Train loss: -4.062, Test loss: 0.609, Test accuracy: 77.86
Round  62: Global train loss: -4.062, Global test loss: 1.086, Global test accuracy: 38.27
Round  63, Train loss: -4.152, Test loss: 0.633, Test accuracy: 77.44
Round  63: Global train loss: -4.152, Global test loss: 1.085, Global test accuracy: 38.69
Round  64, Train loss: -3.975, Test loss: 0.661, Test accuracy: 77.19
Round  64: Global train loss: -3.975, Global test loss: 1.085, Global test accuracy: 38.67
Round  65, Train loss: -4.937, Test loss: 0.642, Test accuracy: 77.26
Round  65: Global train loss: -4.937, Global test loss: 1.086, Global test accuracy: 38.42
Round  66, Train loss: -3.976, Test loss: 0.651, Test accuracy: 76.56
Round  66: Global train loss: -3.976, Global test loss: 1.086, Global test accuracy: 38.44
Round  67, Train loss: -5.099, Test loss: 0.621, Test accuracy: 77.82
Round  67: Global train loss: -5.099, Global test loss: 1.087, Global test accuracy: 38.62
Round  68, Train loss: -4.643, Test loss: 0.634, Test accuracy: 77.17
Round  68: Global train loss: -4.643, Global test loss: 1.087, Global test accuracy: 38.55
Round  69, Train loss: -5.288, Test loss: 0.621, Test accuracy: 78.19
Round  69: Global train loss: -5.288, Global test loss: 1.087, Global test accuracy: 38.98
Round  70, Train loss: -4.791, Test loss: 0.596, Test accuracy: 78.40
Round  70: Global train loss: -4.791, Global test loss: 1.086, Global test accuracy: 38.91
Round  71, Train loss: -4.223, Test loss: 0.589, Test accuracy: 78.31
Round  71: Global train loss: -4.223, Global test loss: 1.086, Global test accuracy: 38.95
Round  72, Train loss: -5.740, Test loss: 0.621, Test accuracy: 77.61
Round  72: Global train loss: -5.740, Global test loss: 1.086, Global test accuracy: 39.35
Round  73, Train loss: -5.222, Test loss: 0.616, Test accuracy: 77.98
Round  73: Global train loss: -5.222, Global test loss: 1.086, Global test accuracy: 39.09
Round  74, Train loss: -4.521, Test loss: 0.640, Test accuracy: 77.12
Round  74: Global train loss: -4.521, Global test loss: 1.086, Global test accuracy: 39.12
Round  75, Train loss: -5.354, Test loss: 0.665, Test accuracy: 77.11
Round  75: Global train loss: -5.354, Global test loss: 1.085, Global test accuracy: 39.20
Round  76, Train loss: -5.605, Test loss: 0.646, Test accuracy: 77.53
Round  76: Global train loss: -5.605, Global test loss: 1.084, Global test accuracy: 39.40
Round  77, Train loss: -4.721, Test loss: 0.637, Test accuracy: 77.51
Round  77: Global train loss: -4.721, Global test loss: 1.084, Global test accuracy: 39.06
Round  78, Train loss: -4.090, Test loss: 0.644, Test accuracy: 76.98
Round  78: Global train loss: -4.090, Global test loss: 1.083, Global test accuracy: 39.42
Round  79, Train loss: -5.637, Test loss: 0.670, Test accuracy: 76.86
Round  79: Global train loss: -5.637, Global test loss: 1.084, Global test accuracy: 39.46
Round  80, Train loss: -5.588, Test loss: 0.667, Test accuracy: 76.57
Round  80: Global train loss: -5.588, Global test loss: 1.085, Global test accuracy: 39.68
Round  81, Train loss: -6.163, Test loss: 0.644, Test accuracy: 77.50
Round  81: Global train loss: -6.163, Global test loss: 1.088, Global test accuracy: 39.62
Round  82, Train loss: -5.047, Test loss: 0.646, Test accuracy: 77.40
Round  82: Global train loss: -5.047, Global test loss: 1.087, Global test accuracy: 39.39
Round  83, Train loss: -5.328, Test loss: 0.615, Test accuracy: 78.34
Round  83: Global train loss: -5.328, Global test loss: 1.087, Global test accuracy: 39.36
Round  84, Train loss: -5.663, Test loss: 0.609, Test accuracy: 78.79
Round  84: Global train loss: -5.663, Global test loss: 1.089, Global test accuracy: 39.67
Round  85, Train loss: -5.281, Test loss: 0.613, Test accuracy: 78.64
Round  85: Global train loss: -5.281, Global test loss: 1.088, Global test accuracy: 39.63
Round  86, Train loss: -4.716, Test loss: 0.629, Test accuracy: 78.28
Round  86: Global train loss: -4.716, Global test loss: 1.088, Global test accuracy: 39.51
Round  87, Train loss: -5.556, Test loss: 0.639, Test accuracy: 77.58
Round  87: Global train loss: -5.556, Global test loss: 1.087, Global test accuracy: 39.71
Round  88, Train loss: -5.669, Test loss: 0.638, Test accuracy: 78.40
Round  88: Global train loss: -5.669, Global test loss: 1.086, Global test accuracy: 39.80
Round  89, Train loss: -5.928, Test loss: 0.632, Test accuracy: 78.76
Round  89: Global train loss: -5.928, Global test loss: 1.088, Global test accuracy: 39.80
Round  90, Train loss: -5.323, Test loss: 0.651, Test accuracy: 78.26
Round  90: Global train loss: -5.323, Global test loss: 1.090, Global test accuracy: 39.52
Round  91, Train loss: -5.374, Test loss: 0.610, Test accuracy: 78.81
Round  91: Global train loss: -5.374, Global test loss: 1.088, Global test accuracy: 39.48
Round  92, Train loss: -5.802, Test loss: 0.619, Test accuracy: 78.03
Round  92: Global train loss: -5.802, Global test loss: 1.089, Global test accuracy: 39.76
Round  93, Train loss: -5.048, Test loss: 0.626, Test accuracy: 77.74
Round  93: Global train loss: -5.048, Global test loss: 1.088, Global test accuracy: 40.18
Round  94, Train loss: -5.777, Test loss: 0.642, Test accuracy: 77.83
Round  94: Global train loss: -5.777, Global test loss: 1.089, Global test accuracy: 39.85
Round  95, Train loss: -5.663, Test loss: 0.618, Test accuracy: 77.72
Round  95: Global train loss: -5.663, Global test loss: 1.091, Global test accuracy: 39.95
Round  96, Train loss: -5.121, Test loss: 0.626, Test accuracy: 78.08
Round  96: Global train loss: -5.121, Global test loss: 1.093, Global test accuracy: 40.02
Round  97, Train loss: -4.801, Test loss: 0.615, Test accuracy: 78.69
Round  97: Global train loss: -4.801, Global test loss: 1.091, Global test accuracy: 39.98
Round  98, Train loss: -4.720, Test loss: 0.614, Test accuracy: 78.30
Round  98: Global train loss: -4.720, Global test loss: 1.089, Global test accuracy: 39.90
Round  99, Train loss: -5.252, Test loss: 0.625, Test accuracy: 78.11
Round  99: Global train loss: -5.252, Global test loss: 1.088, Global test accuracy: 39.76
Final Round: Train loss: 0.530, Test loss: 0.536, Test accuracy: 78.06
Final Round: Global train loss: 0.530, Global test loss: 1.090, Global test accuracy: 39.87
Average accuracy final 10 rounds: 78.1575
Average global accuracy final 10 rounds: 39.84
1718.725626707077
[]
[42.65, 46.016666666666666, 50.86666666666667, 55.775, 57.69166666666667, 59.416666666666664, 61.675, 62.31666666666667, 62.11666666666667, 64.41666666666667, 63.94166666666667, 65.83333333333333, 69.00833333333334, 70.38333333333334, 69.90833333333333, 70.44166666666666, 70.5, 70.225, 70.29166666666667, 70.35833333333333, 70.925, 71.80833333333334, 72.63333333333334, 72.88333333333334, 72.48333333333333, 74.6, 75.04166666666667, 75.31666666666666, 75.09166666666667, 75.23333333333333, 75.575, 76.675, 75.9, 75.95833333333333, 75.0, 75.1, 75.76666666666667, 76.18333333333334, 76.03333333333333, 76.925, 77.50833333333334, 77.35833333333333, 77.01666666666667, 77.08333333333333, 77.19166666666666, 76.93333333333334, 76.70833333333333, 77.41666666666667, 77.49166666666666, 77.21666666666667, 77.89166666666667, 78.11666666666666, 78.675, 78.25, 77.86666666666666, 77.29166666666667, 77.53333333333333, 77.025, 77.575, 76.85, 76.89166666666667, 77.55, 77.85833333333333, 77.44166666666666, 77.19166666666666, 77.25833333333334, 76.55833333333334, 77.81666666666666, 77.175, 78.19166666666666, 78.4, 78.30833333333334, 77.60833333333333, 77.98333333333333, 77.11666666666666, 77.10833333333333, 77.53333333333333, 77.50833333333334, 76.98333333333333, 76.85833333333333, 76.56666666666666, 77.5, 77.4, 78.34166666666667, 78.79166666666667, 78.64166666666667, 78.275, 77.575, 78.4, 78.75833333333334, 78.25833333333334, 78.80833333333334, 78.03333333333333, 77.74166666666666, 77.825, 77.725, 78.08333333333333, 78.69166666666666, 78.3, 78.10833333333333, 78.05833333333334]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

python: can't open file 'main_fedpac_k.py': [Errno 2] No such file or directory
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307384
307387
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.299, Test accuracy: 16.44 

Round   0, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 16.18 

Round   1, Train loss: 2.296, Test loss: 2.294, Test accuracy: 16.96 

Round   1, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 16.50 

Round   2, Train loss: 2.286, Test loss: 2.281, Test accuracy: 16.57 

Round   2, Global train loss: 2.286, Global test loss: 2.275, Global test accuracy: 13.84 

Round   3, Train loss: 2.258, Test loss: 2.259, Test accuracy: 19.82 

Round   3, Global train loss: 2.258, Global test loss: 2.240, Global test accuracy: 22.30 

Round   4, Train loss: 2.206, Test loss: 2.223, Test accuracy: 28.05 

Round   4, Global train loss: 2.206, Global test loss: 2.183, Global test accuracy: 37.28 

Round   5, Train loss: 2.260, Test loss: 2.232, Test accuracy: 27.75 

Round   5, Global train loss: 2.260, Global test loss: 2.258, Global test accuracy: 33.15 

Round   6, Train loss: 2.137, Test loss: 2.177, Test accuracy: 33.58 

Round   6, Global train loss: 2.137, Global test loss: 2.146, Global test accuracy: 39.84 

Round   7, Train loss: 2.144, Test loss: 2.124, Test accuracy: 41.48 

Round   7, Global train loss: 2.144, Global test loss: 2.099, Global test accuracy: 54.68 

Round   8, Train loss: 1.984, Test loss: 2.075, Test accuracy: 44.19 

Round   8, Global train loss: 1.984, Global test loss: 1.988, Global test accuracy: 60.05 

Round   9, Train loss: 1.964, Test loss: 2.032, Test accuracy: 47.01 

Round   9, Global train loss: 1.964, Global test loss: 1.911, Global test accuracy: 66.80 

Round  10, Train loss: 2.070, Test loss: 2.003, Test accuracy: 48.77 

Round  10, Global train loss: 2.070, Global test loss: 2.088, Global test accuracy: 55.77 

Round  11, Train loss: 1.888, Test loss: 1.974, Test accuracy: 52.00 

Round  11, Global train loss: 1.888, Global test loss: 1.877, Global test accuracy: 65.92 

Round  12, Train loss: 1.977, Test loss: 1.940, Test accuracy: 56.35 

Round  12, Global train loss: 1.977, Global test loss: 1.984, Global test accuracy: 57.78 

Round  13, Train loss: 1.830, Test loss: 1.920, Test accuracy: 57.94 

Round  13, Global train loss: 1.830, Global test loss: 1.858, Global test accuracy: 62.62 

Round  14, Train loss: 1.756, Test loss: 1.901, Test accuracy: 59.93 

Round  14, Global train loss: 1.756, Global test loss: 1.821, Global test accuracy: 67.50 

Round  15, Train loss: 1.760, Test loss: 1.876, Test accuracy: 62.31 

Round  15, Global train loss: 1.760, Global test loss: 1.777, Global test accuracy: 71.73 

Round  16, Train loss: 1.907, Test loss: 1.844, Test accuracy: 65.97 

Round  16, Global train loss: 1.907, Global test loss: 1.890, Global test accuracy: 66.67 

Round  17, Train loss: 1.778, Test loss: 1.821, Test accuracy: 68.03 

Round  17, Global train loss: 1.778, Global test loss: 1.814, Global test accuracy: 65.85 

Round  18, Train loss: 1.703, Test loss: 1.810, Test accuracy: 68.89 

Round  18, Global train loss: 1.703, Global test loss: 1.818, Global test accuracy: 64.95 

Round  19, Train loss: 1.778, Test loss: 1.788, Test accuracy: 70.79 

Round  19, Global train loss: 1.778, Global test loss: 1.834, Global test accuracy: 66.24 

Round  20, Train loss: 1.668, Test loss: 1.778, Test accuracy: 71.72 

Round  20, Global train loss: 1.668, Global test loss: 1.784, Global test accuracy: 67.91 

Round  21, Train loss: 1.695, Test loss: 1.765, Test accuracy: 73.14 

Round  21, Global train loss: 1.695, Global test loss: 1.783, Global test accuracy: 70.23 

Round  22, Train loss: 1.675, Test loss: 1.758, Test accuracy: 73.35 

Round  22, Global train loss: 1.675, Global test loss: 1.788, Global test accuracy: 68.01 

Round  23, Train loss: 1.631, Test loss: 1.745, Test accuracy: 74.23 

Round  23, Global train loss: 1.631, Global test loss: 1.710, Global test accuracy: 77.24 

Round  24, Train loss: 1.639, Test loss: 1.740, Test accuracy: 74.61 

Round  24, Global train loss: 1.639, Global test loss: 1.772, Global test accuracy: 71.49 

Round  25, Train loss: 1.651, Test loss: 1.731, Test accuracy: 75.39 

Round  25, Global train loss: 1.651, Global test loss: 1.753, Global test accuracy: 72.72 

Round  26, Train loss: 1.611, Test loss: 1.728, Test accuracy: 75.47 

Round  26, Global train loss: 1.611, Global test loss: 1.729, Global test accuracy: 75.27 

Round  27, Train loss: 1.595, Test loss: 1.723, Test accuracy: 75.63 

Round  27, Global train loss: 1.595, Global test loss: 1.699, Global test accuracy: 78.11 

Round  28, Train loss: 1.609, Test loss: 1.719, Test accuracy: 76.06 

Round  28, Global train loss: 1.609, Global test loss: 1.701, Global test accuracy: 78.74 

Round  29, Train loss: 1.618, Test loss: 1.715, Test accuracy: 76.31 

Round  29, Global train loss: 1.618, Global test loss: 1.725, Global test accuracy: 76.36 

Round  30, Train loss: 1.592, Test loss: 1.712, Test accuracy: 76.54 

Round  30, Global train loss: 1.592, Global test loss: 1.710, Global test accuracy: 76.82 

Round  31, Train loss: 1.572, Test loss: 1.705, Test accuracy: 77.19 

Round  31, Global train loss: 1.572, Global test loss: 1.672, Global test accuracy: 80.72 

Round  32, Train loss: 1.595, Test loss: 1.703, Test accuracy: 77.42 

Round  32, Global train loss: 1.595, Global test loss: 1.705, Global test accuracy: 78.70 

Round  33, Train loss: 1.542, Test loss: 1.698, Test accuracy: 77.96 

Round  33, Global train loss: 1.542, Global test loss: 1.635, Global test accuracy: 84.82 

Round  34, Train loss: 1.573, Test loss: 1.696, Test accuracy: 78.07 

Round  34, Global train loss: 1.573, Global test loss: 1.692, Global test accuracy: 77.86 

Round  35, Train loss: 1.619, Test loss: 1.693, Test accuracy: 78.29 

Round  35, Global train loss: 1.619, Global test loss: 1.762, Global test accuracy: 70.31 

Round  36, Train loss: 1.576, Test loss: 1.692, Test accuracy: 78.30 

Round  36, Global train loss: 1.576, Global test loss: 1.741, Global test accuracy: 72.09 

Round  37, Train loss: 1.599, Test loss: 1.690, Test accuracy: 78.50 

Round  37, Global train loss: 1.599, Global test loss: 1.752, Global test accuracy: 71.26 

Round  38, Train loss: 1.574, Test loss: 1.687, Test accuracy: 78.78 

Round  38, Global train loss: 1.574, Global test loss: 1.693, Global test accuracy: 78.27 

Round  39, Train loss: 1.525, Test loss: 1.686, Test accuracy: 78.87 

Round  39, Global train loss: 1.525, Global test loss: 1.651, Global test accuracy: 82.30 

Round  40, Train loss: 1.545, Test loss: 1.685, Test accuracy: 78.91 

Round  40, Global train loss: 1.545, Global test loss: 1.676, Global test accuracy: 79.98 

Round  41, Train loss: 1.543, Test loss: 1.685, Test accuracy: 78.87 

Round  41, Global train loss: 1.543, Global test loss: 1.713, Global test accuracy: 74.32 

Round  42, Train loss: 1.533, Test loss: 1.682, Test accuracy: 79.03 

Round  42, Global train loss: 1.533, Global test loss: 1.659, Global test accuracy: 81.90 

Round  43, Train loss: 1.553, Test loss: 1.682, Test accuracy: 79.08 

Round  43, Global train loss: 1.553, Global test loss: 1.687, Global test accuracy: 77.84 

Round  44, Train loss: 1.493, Test loss: 1.680, Test accuracy: 79.18 

Round  44, Global train loss: 1.493, Global test loss: 1.601, Global test accuracy: 87.65 

Round  45, Train loss: 1.547, Test loss: 1.678, Test accuracy: 79.44 

Round  45, Global train loss: 1.547, Global test loss: 1.693, Global test accuracy: 77.24 

Round  46, Train loss: 1.529, Test loss: 1.677, Test accuracy: 79.50 

Round  46, Global train loss: 1.529, Global test loss: 1.663, Global test accuracy: 81.19 

Round  47, Train loss: 1.561, Test loss: 1.676, Test accuracy: 79.61 

Round  47, Global train loss: 1.561, Global test loss: 1.736, Global test accuracy: 72.80 

Round  48, Train loss: 1.568, Test loss: 1.675, Test accuracy: 79.66 

Round  48, Global train loss: 1.568, Global test loss: 1.709, Global test accuracy: 76.97 

Round  49, Train loss: 1.547, Test loss: 1.673, Test accuracy: 79.95 

Round  49, Global train loss: 1.547, Global test loss: 1.683, Global test accuracy: 78.45 

Round  50, Train loss: 1.501, Test loss: 1.672, Test accuracy: 79.89 

Round  50, Global train loss: 1.501, Global test loss: 1.626, Global test accuracy: 84.65 

Round  51, Train loss: 1.564, Test loss: 1.672, Test accuracy: 79.93 

Round  51, Global train loss: 1.564, Global test loss: 1.732, Global test accuracy: 73.94 

Round  52, Train loss: 1.540, Test loss: 1.671, Test accuracy: 79.97 

Round  52, Global train loss: 1.540, Global test loss: 1.685, Global test accuracy: 78.39 

Round  53, Train loss: 1.501, Test loss: 1.671, Test accuracy: 79.96 

Round  53, Global train loss: 1.501, Global test loss: 1.617, Global test accuracy: 85.44 

Round  54, Train loss: 1.527, Test loss: 1.671, Test accuracy: 79.94 

Round  54, Global train loss: 1.527, Global test loss: 1.673, Global test accuracy: 79.30 

Round  55, Train loss: 1.529, Test loss: 1.671, Test accuracy: 79.95 

Round  55, Global train loss: 1.529, Global test loss: 1.647, Global test accuracy: 82.33 

Round  56, Train loss: 1.516, Test loss: 1.670, Test accuracy: 80.01 

Round  56, Global train loss: 1.516, Global test loss: 1.639, Global test accuracy: 83.12 

Round  57, Train loss: 1.554, Test loss: 1.670, Test accuracy: 79.95 

Round  57, Global train loss: 1.554, Global test loss: 1.708, Global test accuracy: 76.01 

Round  58, Train loss: 1.546, Test loss: 1.668, Test accuracy: 80.26 

Round  58, Global train loss: 1.546, Global test loss: 1.706, Global test accuracy: 76.16 

Round  59, Train loss: 1.533, Test loss: 1.668, Test accuracy: 80.28 

Round  59, Global train loss: 1.533, Global test loss: 1.677, Global test accuracy: 79.06 

Round  60, Train loss: 1.525, Test loss: 1.667, Test accuracy: 80.28 

Round  60, Global train loss: 1.525, Global test loss: 1.666, Global test accuracy: 80.06 

Round  61, Train loss: 1.512, Test loss: 1.667, Test accuracy: 80.34 

Round  61, Global train loss: 1.512, Global test loss: 1.637, Global test accuracy: 83.53 

Round  62, Train loss: 1.493, Test loss: 1.666, Test accuracy: 80.35 

Round  62, Global train loss: 1.493, Global test loss: 1.615, Global test accuracy: 85.65 

Round  63, Train loss: 1.549, Test loss: 1.666, Test accuracy: 80.36 

Round  63, Global train loss: 1.549, Global test loss: 1.732, Global test accuracy: 73.28 

Round  64, Train loss: 1.498, Test loss: 1.666, Test accuracy: 80.38 

Round  64, Global train loss: 1.498, Global test loss: 1.617, Global test accuracy: 85.48 

Round  65, Train loss: 1.539, Test loss: 1.666, Test accuracy: 80.39 

Round  65, Global train loss: 1.539, Global test loss: 1.679, Global test accuracy: 79.10 

Round  66, Train loss: 1.535, Test loss: 1.666, Test accuracy: 80.35 

Round  66, Global train loss: 1.535, Global test loss: 1.692, Global test accuracy: 77.31 

Round  67, Train loss: 1.495, Test loss: 1.665, Test accuracy: 80.39 

Round  67, Global train loss: 1.495, Global test loss: 1.620, Global test accuracy: 85.54 

Round  68, Train loss: 1.534, Test loss: 1.665, Test accuracy: 80.39 

Round  68, Global train loss: 1.534, Global test loss: 1.699, Global test accuracy: 76.50 

Round  69, Train loss: 1.501, Test loss: 1.665, Test accuracy: 80.40 

Round  69, Global train loss: 1.501, Global test loss: 1.623, Global test accuracy: 84.83 

Round  70, Train loss: 1.510, Test loss: 1.665, Test accuracy: 80.39 

Round  70, Global train loss: 1.510, Global test loss: 1.638, Global test accuracy: 82.96 

Round  71, Train loss: 1.497, Test loss: 1.665, Test accuracy: 80.39 

Round  71, Global train loss: 1.497, Global test loss: 1.639, Global test accuracy: 82.99 

Round  72, Train loss: 1.559, Test loss: 1.664, Test accuracy: 80.39 

Round  72, Global train loss: 1.559, Global test loss: 1.727, Global test accuracy: 74.33 

Round  73, Train loss: 1.512, Test loss: 1.664, Test accuracy: 80.41 

Round  73, Global train loss: 1.512, Global test loss: 1.638, Global test accuracy: 83.19 

Round  74, Train loss: 1.548, Test loss: 1.664, Test accuracy: 80.42 

Round  74, Global train loss: 1.548, Global test loss: 1.724, Global test accuracy: 73.43 

Round  75, Train loss: 1.525, Test loss: 1.664, Test accuracy: 80.43 

Round  75, Global train loss: 1.525, Global test loss: 1.663, Global test accuracy: 80.91 

Round  76, Train loss: 1.492, Test loss: 1.664, Test accuracy: 80.42 

Round  76, Global train loss: 1.492, Global test loss: 1.622, Global test accuracy: 84.79 

Round  77, Train loss: 1.521, Test loss: 1.663, Test accuracy: 80.42 

Round  77, Global train loss: 1.521, Global test loss: 1.669, Global test accuracy: 80.11 

Round  78, Train loss: 1.576, Test loss: 1.663, Test accuracy: 80.41 

Round  78, Global train loss: 1.576, Global test loss: 1.747, Global test accuracy: 72.16 

Round  79, Train loss: 1.535, Test loss: 1.663, Test accuracy: 80.39 

Round  79, Global train loss: 1.535, Global test loss: 1.702, Global test accuracy: 76.27 

Round  80, Train loss: 1.495, Test loss: 1.663, Test accuracy: 80.41 

Round  80, Global train loss: 1.495, Global test loss: 1.617, Global test accuracy: 85.29 

Round  81, Train loss: 1.484, Test loss: 1.663, Test accuracy: 80.39 

Round  81, Global train loss: 1.484, Global test loss: 1.598, Global test accuracy: 87.40 

Round  82, Train loss: 1.535, Test loss: 1.663, Test accuracy: 80.39 

Round  82, Global train loss: 1.535, Global test loss: 1.707, Global test accuracy: 75.09 

Round  83, Train loss: 1.533, Test loss: 1.663, Test accuracy: 80.36 

Round  83, Global train loss: 1.533, Global test loss: 1.697, Global test accuracy: 76.75 

Round  84, Train loss: 1.520, Test loss: 1.663, Test accuracy: 80.36 

Round  84, Global train loss: 1.520, Global test loss: 1.672, Global test accuracy: 79.35 

Round  85, Train loss: 1.560, Test loss: 1.663, Test accuracy: 80.36 

Round  85, Global train loss: 1.560, Global test loss: 1.702, Global test accuracy: 76.64 

Round  86, Train loss: 1.494, Test loss: 1.663, Test accuracy: 80.37 

Round  86, Global train loss: 1.494, Global test loss: 1.614, Global test accuracy: 85.69 

Round  87, Train loss: 1.522, Test loss: 1.663, Test accuracy: 80.40 

Round  87, Global train loss: 1.522, Global test loss: 1.675, Global test accuracy: 79.03 

Round  88, Train loss: 1.523, Test loss: 1.663, Test accuracy: 80.42 

Round  88, Global train loss: 1.523, Global test loss: 1.655, Global test accuracy: 81.35 

Round  89, Train loss: 1.545, Test loss: 1.663, Test accuracy: 80.41 

Round  89, Global train loss: 1.545, Global test loss: 1.710, Global test accuracy: 75.38 

Round  90, Train loss: 1.521, Test loss: 1.663, Test accuracy: 80.38 

Round  90, Global train loss: 1.521, Global test loss: 1.673, Global test accuracy: 79.03 

Round  91, Train loss: 1.521, Test loss: 1.663, Test accuracy: 80.37 

Round  91, Global train loss: 1.521, Global test loss: 1.665, Global test accuracy: 80.25 

Round  92, Train loss: 1.519, Test loss: 1.663, Test accuracy: 80.34 

Round  92, Global train loss: 1.519, Global test loss: 1.669, Global test accuracy: 79.98 

Round  93, Train loss: 1.520, Test loss: 1.663, Test accuracy: 80.33 

Round  93, Global train loss: 1.520, Global test loss: 1.679, Global test accuracy: 78.36 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.522, Test loss: 1.663, Test accuracy: 80.35 

Round  94, Global train loss: 1.522, Global test loss: 1.663, Global test accuracy: 80.42 

Round  95, Train loss: 1.503, Test loss: 1.663, Test accuracy: 80.37 

Round  95, Global train loss: 1.503, Global test loss: 1.639, Global test accuracy: 83.05 

Round  96, Train loss: 1.517, Test loss: 1.663, Test accuracy: 80.37 

Round  96, Global train loss: 1.517, Global test loss: 1.670, Global test accuracy: 79.58 

Round  97, Train loss: 1.503, Test loss: 1.663, Test accuracy: 80.38 

Round  97, Global train loss: 1.503, Global test loss: 1.632, Global test accuracy: 83.60 

Round  98, Train loss: 1.535, Test loss: 1.663, Test accuracy: 80.38 

Round  98, Global train loss: 1.535, Global test loss: 1.693, Global test accuracy: 76.85 

Round  99, Train loss: 1.486, Test loss: 1.663, Test accuracy: 80.34 

Round  99, Global train loss: 1.486, Global test loss: 1.590, Global test accuracy: 88.06 

Final Round, Train loss: 1.523, Test loss: 1.662, Test accuracy: 80.35 

Final Round, Global train loss: 1.523, Global test loss: 1.590, Global test accuracy: 88.06 

Average accuracy final 10 rounds: 80.36175 

Average global accuracy final 10 rounds: 80.919 

2108.165360212326
[0.8355996608734131, 1.5845181941986084, 2.334195375442505, 3.087692975997925, 3.8513028621673584, 4.6018407344818115, 5.350720643997192, 6.101351737976074, 6.851322889328003, 7.603301763534546, 8.347714185714722, 9.089219093322754, 9.837637186050415, 10.5889413356781, 11.33078384399414, 12.080727815628052, 12.827370882034302, 13.574724197387695, 14.330241203308105, 15.07575511932373, 15.830928564071655, 16.584962844848633, 17.334152698516846, 18.083654165267944, 18.830393075942993, 19.58276081085205, 20.33340835571289, 21.084737539291382, 21.838294506072998, 22.58740997314453, 23.334511756896973, 24.088112592697144, 24.837799072265625, 25.58543372154236, 26.330329656600952, 27.078204870224, 27.829265594482422, 28.581746339797974, 29.328667640686035, 30.080888986587524, 30.83284020423889, 31.588260889053345, 32.35140037536621, 33.09996771812439, 33.851526498794556, 34.60350728034973, 35.3541202545166, 36.10665678977966, 36.857775926589966, 37.60702466964722, 38.35429906845093, 39.08951807022095, 39.87700629234314, 40.638922452926636, 41.442710638046265, 42.24369239807129, 43.04380798339844, 43.84220266342163, 44.649057149887085, 45.456358432769775, 46.25765538215637, 47.06411957740784, 47.86730194091797, 48.66986536979675, 49.47037124633789, 50.26891756057739, 51.07203793525696, 51.868653774261475, 52.67118430137634, 53.473209381103516, 54.27626943588257, 55.074970960617065, 55.873136043548584, 56.6790132522583, 57.48073959350586, 58.275704860687256, 59.07356023788452, 59.86854648590088, 60.67164587974548, 61.47897386550903, 62.27925395965576, 63.07581615447998, 63.88023591041565, 64.68689322471619, 65.48319816589355, 66.27619409561157, 67.08249616622925, 67.88472557067871, 68.68357872962952, 69.48618292808533, 70.28708100318909, 71.0872118473053, 71.8855562210083, 72.684077501297, 73.48223328590393, 74.28362941741943, 75.0856351852417, 75.88178610801697, 76.68109893798828, 77.48627781867981, 79.08658790588379]
[16.44, 16.9575, 16.57, 19.8175, 28.05, 27.7475, 33.5825, 41.475, 44.185, 47.005, 48.765, 51.9975, 56.3525, 57.9375, 59.93, 62.3075, 65.975, 68.0325, 68.8875, 70.7875, 71.72, 73.1425, 73.3525, 74.235, 74.615, 75.39, 75.4725, 75.63, 76.055, 76.305, 76.5425, 77.19, 77.42, 77.9625, 78.07, 78.2875, 78.295, 78.495, 78.785, 78.8725, 78.9125, 78.8725, 79.025, 79.08, 79.1825, 79.4425, 79.5025, 79.605, 79.655, 79.95, 79.89, 79.9325, 79.965, 79.96, 79.9425, 79.955, 80.0125, 79.9525, 80.2575, 80.28, 80.2825, 80.34, 80.3475, 80.3575, 80.3775, 80.385, 80.3525, 80.3875, 80.3925, 80.3975, 80.39, 80.385, 80.385, 80.4125, 80.4225, 80.43, 80.4175, 80.42, 80.4075, 80.3925, 80.4075, 80.39, 80.3875, 80.355, 80.365, 80.36, 80.37, 80.4, 80.415, 80.405, 80.38, 80.3675, 80.3425, 80.335, 80.3475, 80.3675, 80.3725, 80.3825, 80.3775, 80.345, 80.35]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.300, Test accuracy: 17.13 

Round   0, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 16.93 

Round   1, Train loss: 2.298, Test loss: 2.297, Test accuracy: 27.57 

Round   1, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 30.96 

Round   2, Train loss: 2.291, Test loss: 2.291, Test accuracy: 37.55 

Round   2, Global train loss: 2.291, Global test loss: 2.287, Global test accuracy: 48.12 

Round   3, Train loss: 2.274, Test loss: 2.271, Test accuracy: 38.08 

Round   3, Global train loss: 2.274, Global test loss: 2.255, Global test accuracy: 47.82 

Round   4, Train loss: 2.149, Test loss: 2.161, Test accuracy: 43.07 

Round   4, Global train loss: 2.149, Global test loss: 2.039, Global test accuracy: 48.06 

Round   5, Train loss: 1.962, Test loss: 2.058, Test accuracy: 48.75 

Round   5, Global train loss: 1.962, Global test loss: 1.922, Global test accuracy: 56.42 

Round   6, Train loss: 1.892, Test loss: 2.012, Test accuracy: 52.02 

Round   6, Global train loss: 1.892, Global test loss: 1.900, Global test accuracy: 57.02 

Round   7, Train loss: 1.881, Test loss: 2.002, Test accuracy: 52.32 

Round   7, Global train loss: 1.881, Global test loss: 1.892, Global test accuracy: 57.33 

Round   8, Train loss: 1.874, Test loss: 1.963, Test accuracy: 53.69 

Round   8, Global train loss: 1.874, Global test loss: 1.887, Global test accuracy: 57.41 

Round   9, Train loss: 1.851, Test loss: 1.915, Test accuracy: 56.05 

Round   9, Global train loss: 1.851, Global test loss: 1.860, Global test accuracy: 60.17 

Round  10, Train loss: 1.817, Test loss: 1.885, Test accuracy: 59.14 

Round  10, Global train loss: 1.817, Global test loss: 1.804, Global test accuracy: 67.45 

Round  11, Train loss: 1.743, Test loss: 1.843, Test accuracy: 63.74 

Round  11, Global train loss: 1.743, Global test loss: 1.711, Global test accuracy: 78.05 

Round  12, Train loss: 1.668, Test loss: 1.803, Test accuracy: 68.14 

Round  12, Global train loss: 1.668, Global test loss: 1.668, Global test accuracy: 81.39 

Round  13, Train loss: 1.625, Test loss: 1.752, Test accuracy: 73.67 

Round  13, Global train loss: 1.625, Global test loss: 1.628, Global test accuracy: 86.09 

Round  14, Train loss: 1.609, Test loss: 1.715, Test accuracy: 77.32 

Round  14, Global train loss: 1.609, Global test loss: 1.602, Global test accuracy: 87.97 

Round  15, Train loss: 1.577, Test loss: 1.699, Test accuracy: 78.67 

Round  15, Global train loss: 1.577, Global test loss: 1.588, Global test accuracy: 88.62 

Round  16, Train loss: 1.574, Test loss: 1.675, Test accuracy: 80.74 

Round  16, Global train loss: 1.574, Global test loss: 1.579, Global test accuracy: 89.53 

Round  17, Train loss: 1.553, Test loss: 1.668, Test accuracy: 81.26 

Round  17, Global train loss: 1.553, Global test loss: 1.574, Global test accuracy: 89.67 

Round  18, Train loss: 1.546, Test loss: 1.643, Test accuracy: 83.70 

Round  18, Global train loss: 1.546, Global test loss: 1.569, Global test accuracy: 90.14 

Round  19, Train loss: 1.542, Test loss: 1.635, Test accuracy: 84.32 

Round  19, Global train loss: 1.542, Global test loss: 1.564, Global test accuracy: 90.82 

Round  20, Train loss: 1.546, Test loss: 1.629, Test accuracy: 84.74 

Round  20, Global train loss: 1.546, Global test loss: 1.561, Global test accuracy: 90.83 

Round  21, Train loss: 1.544, Test loss: 1.614, Test accuracy: 86.08 

Round  21, Global train loss: 1.544, Global test loss: 1.560, Global test accuracy: 91.02 

Round  22, Train loss: 1.540, Test loss: 1.609, Test accuracy: 86.48 

Round  22, Global train loss: 1.540, Global test loss: 1.557, Global test accuracy: 91.03 

Round  23, Train loss: 1.532, Test loss: 1.569, Test accuracy: 90.03 

Round  23, Global train loss: 1.532, Global test loss: 1.555, Global test accuracy: 91.27 

Round  24, Train loss: 1.524, Test loss: 1.566, Test accuracy: 90.27 

Round  24, Global train loss: 1.524, Global test loss: 1.554, Global test accuracy: 91.52 

Round  25, Train loss: 1.527, Test loss: 1.564, Test accuracy: 90.35 

Round  25, Global train loss: 1.527, Global test loss: 1.552, Global test accuracy: 91.34 

Round  26, Train loss: 1.533, Test loss: 1.563, Test accuracy: 90.49 

Round  26, Global train loss: 1.533, Global test loss: 1.552, Global test accuracy: 91.54 

Round  27, Train loss: 1.525, Test loss: 1.561, Test accuracy: 90.58 

Round  27, Global train loss: 1.525, Global test loss: 1.550, Global test accuracy: 91.51 

Round  28, Train loss: 1.522, Test loss: 1.560, Test accuracy: 90.72 

Round  28, Global train loss: 1.522, Global test loss: 1.549, Global test accuracy: 91.62 

Round  29, Train loss: 1.518, Test loss: 1.559, Test accuracy: 90.83 

Round  29, Global train loss: 1.518, Global test loss: 1.548, Global test accuracy: 91.53 

Round  30, Train loss: 1.517, Test loss: 1.556, Test accuracy: 91.08 

Round  30, Global train loss: 1.517, Global test loss: 1.546, Global test accuracy: 91.78 

Round  31, Train loss: 1.513, Test loss: 1.555, Test accuracy: 91.11 

Round  31, Global train loss: 1.513, Global test loss: 1.546, Global test accuracy: 91.87 

Round  32, Train loss: 1.514, Test loss: 1.553, Test accuracy: 91.12 

Round  32, Global train loss: 1.514, Global test loss: 1.544, Global test accuracy: 92.03 

Round  33, Train loss: 1.515, Test loss: 1.551, Test accuracy: 91.38 

Round  33, Global train loss: 1.515, Global test loss: 1.544, Global test accuracy: 92.13 

Round  34, Train loss: 1.511, Test loss: 1.550, Test accuracy: 91.57 

Round  34, Global train loss: 1.511, Global test loss: 1.542, Global test accuracy: 92.21 

Round  35, Train loss: 1.508, Test loss: 1.548, Test accuracy: 91.70 

Round  35, Global train loss: 1.508, Global test loss: 1.541, Global test accuracy: 92.33 

Round  36, Train loss: 1.512, Test loss: 1.547, Test accuracy: 91.86 

Round  36, Global train loss: 1.512, Global test loss: 1.541, Global test accuracy: 92.38 

Round  37, Train loss: 1.507, Test loss: 1.546, Test accuracy: 91.91 

Round  37, Global train loss: 1.507, Global test loss: 1.540, Global test accuracy: 92.43 

Round  38, Train loss: 1.506, Test loss: 1.546, Test accuracy: 91.83 

Round  38, Global train loss: 1.506, Global test loss: 1.541, Global test accuracy: 92.54 

Round  39, Train loss: 1.506, Test loss: 1.546, Test accuracy: 91.91 

Round  39, Global train loss: 1.506, Global test loss: 1.539, Global test accuracy: 92.51 

Round  40, Train loss: 1.506, Test loss: 1.545, Test accuracy: 91.84 

Round  40, Global train loss: 1.506, Global test loss: 1.538, Global test accuracy: 92.53 

Round  41, Train loss: 1.506, Test loss: 1.545, Test accuracy: 91.87 

Round  41, Global train loss: 1.506, Global test loss: 1.537, Global test accuracy: 92.85 

Round  42, Train loss: 1.503, Test loss: 1.545, Test accuracy: 91.88 

Round  42, Global train loss: 1.503, Global test loss: 1.536, Global test accuracy: 92.95 

Round  43, Train loss: 1.501, Test loss: 1.544, Test accuracy: 92.03 

Round  43, Global train loss: 1.501, Global test loss: 1.536, Global test accuracy: 92.91 

Round  44, Train loss: 1.503, Test loss: 1.544, Test accuracy: 91.99 

Round  44, Global train loss: 1.503, Global test loss: 1.536, Global test accuracy: 92.65 

Round  45, Train loss: 1.497, Test loss: 1.543, Test accuracy: 92.13 

Round  45, Global train loss: 1.497, Global test loss: 1.534, Global test accuracy: 93.08 

Round  46, Train loss: 1.496, Test loss: 1.541, Test accuracy: 92.28 

Round  46, Global train loss: 1.496, Global test loss: 1.535, Global test accuracy: 92.91 

Round  47, Train loss: 1.503, Test loss: 1.540, Test accuracy: 92.51 

Round  47, Global train loss: 1.503, Global test loss: 1.533, Global test accuracy: 93.14 

Round  48, Train loss: 1.496, Test loss: 1.539, Test accuracy: 92.65 

Round  48, Global train loss: 1.496, Global test loss: 1.532, Global test accuracy: 93.33 

Round  49, Train loss: 1.496, Test loss: 1.539, Test accuracy: 92.58 

Round  49, Global train loss: 1.496, Global test loss: 1.532, Global test accuracy: 93.25 

Round  50, Train loss: 1.500, Test loss: 1.538, Test accuracy: 92.65 

Round  50, Global train loss: 1.500, Global test loss: 1.531, Global test accuracy: 93.31 

Round  51, Train loss: 1.499, Test loss: 1.537, Test accuracy: 92.70 

Round  51, Global train loss: 1.499, Global test loss: 1.529, Global test accuracy: 93.46 

Round  52, Train loss: 1.502, Test loss: 1.536, Test accuracy: 92.73 

Round  52, Global train loss: 1.502, Global test loss: 1.530, Global test accuracy: 93.39 

Round  53, Train loss: 1.498, Test loss: 1.536, Test accuracy: 92.78 

Round  53, Global train loss: 1.498, Global test loss: 1.529, Global test accuracy: 93.38 

Round  54, Train loss: 1.493, Test loss: 1.535, Test accuracy: 92.89 

Round  54, Global train loss: 1.493, Global test loss: 1.528, Global test accuracy: 93.66 

Round  55, Train loss: 1.495, Test loss: 1.535, Test accuracy: 92.97 

Round  55, Global train loss: 1.495, Global test loss: 1.529, Global test accuracy: 93.49 

Round  56, Train loss: 1.494, Test loss: 1.534, Test accuracy: 93.06 

Round  56, Global train loss: 1.494, Global test loss: 1.528, Global test accuracy: 93.66 

Round  57, Train loss: 1.494, Test loss: 1.534, Test accuracy: 93.17 

Round  57, Global train loss: 1.494, Global test loss: 1.527, Global test accuracy: 93.80 

Round  58, Train loss: 1.494, Test loss: 1.533, Test accuracy: 93.25 

Round  58, Global train loss: 1.494, Global test loss: 1.526, Global test accuracy: 93.70 

Round  59, Train loss: 1.495, Test loss: 1.533, Test accuracy: 93.21 

Round  59, Global train loss: 1.495, Global test loss: 1.527, Global test accuracy: 93.69 

Round  60, Train loss: 1.494, Test loss: 1.532, Test accuracy: 93.20 

Round  60, Global train loss: 1.494, Global test loss: 1.527, Global test accuracy: 93.69 

Round  61, Train loss: 1.496, Test loss: 1.531, Test accuracy: 93.28 

Round  61, Global train loss: 1.496, Global test loss: 1.527, Global test accuracy: 93.81 

Round  62, Train loss: 1.491, Test loss: 1.532, Test accuracy: 93.18 

Round  62, Global train loss: 1.491, Global test loss: 1.526, Global test accuracy: 93.87 

Round  63, Train loss: 1.493, Test loss: 1.532, Test accuracy: 93.22 

Round  63, Global train loss: 1.493, Global test loss: 1.526, Global test accuracy: 93.69 

Round  64, Train loss: 1.489, Test loss: 1.531, Test accuracy: 93.28 

Round  64, Global train loss: 1.489, Global test loss: 1.527, Global test accuracy: 93.64 

Round  65, Train loss: 1.493, Test loss: 1.531, Test accuracy: 93.31 

Round  65, Global train loss: 1.493, Global test loss: 1.526, Global test accuracy: 93.92 

Round  66, Train loss: 1.492, Test loss: 1.531, Test accuracy: 93.33 

Round  66, Global train loss: 1.492, Global test loss: 1.525, Global test accuracy: 93.95 

Round  67, Train loss: 1.491, Test loss: 1.531, Test accuracy: 93.38 

Round  67, Global train loss: 1.491, Global test loss: 1.526, Global test accuracy: 93.78 

Round  68, Train loss: 1.488, Test loss: 1.530, Test accuracy: 93.47 

Round  68, Global train loss: 1.488, Global test loss: 1.525, Global test accuracy: 93.89 

Round  69, Train loss: 1.490, Test loss: 1.530, Test accuracy: 93.52 

Round  69, Global train loss: 1.490, Global test loss: 1.525, Global test accuracy: 93.79 

Round  70, Train loss: 1.491, Test loss: 1.529, Test accuracy: 93.60 

Round  70, Global train loss: 1.491, Global test loss: 1.524, Global test accuracy: 94.08 

Round  71, Train loss: 1.489, Test loss: 1.529, Test accuracy: 93.62 

Round  71, Global train loss: 1.489, Global test loss: 1.523, Global test accuracy: 94.28 

Round  72, Train loss: 1.484, Test loss: 1.528, Test accuracy: 93.67 

Round  72, Global train loss: 1.484, Global test loss: 1.523, Global test accuracy: 94.20 

Round  73, Train loss: 1.489, Test loss: 1.528, Test accuracy: 93.72 

Round  73, Global train loss: 1.489, Global test loss: 1.523, Global test accuracy: 94.03 

Round  74, Train loss: 1.491, Test loss: 1.528, Test accuracy: 93.71 

Round  74, Global train loss: 1.491, Global test loss: 1.523, Global test accuracy: 94.12 

Round  75, Train loss: 1.489, Test loss: 1.527, Test accuracy: 93.68 

Round  75, Global train loss: 1.489, Global test loss: 1.523, Global test accuracy: 94.00 

Round  76, Train loss: 1.487, Test loss: 1.527, Test accuracy: 93.75 

Round  76, Global train loss: 1.487, Global test loss: 1.522, Global test accuracy: 94.13 

Round  77, Train loss: 1.483, Test loss: 1.526, Test accuracy: 93.83 

Round  77, Global train loss: 1.483, Global test loss: 1.522, Global test accuracy: 94.16 

Round  78, Train loss: 1.485, Test loss: 1.527, Test accuracy: 93.73 

Round  78, Global train loss: 1.485, Global test loss: 1.523, Global test accuracy: 94.17 

Round  79, Train loss: 1.483, Test loss: 1.526, Test accuracy: 93.73 

Round  79, Global train loss: 1.483, Global test loss: 1.522, Global test accuracy: 94.12 

Round  80, Train loss: 1.489, Test loss: 1.526, Test accuracy: 93.72 

Round  80, Global train loss: 1.489, Global test loss: 1.522, Global test accuracy: 94.18 

Round  81, Train loss: 1.484, Test loss: 1.526, Test accuracy: 93.72 

Round  81, Global train loss: 1.484, Global test loss: 1.522, Global test accuracy: 94.22 

Round  82, Train loss: 1.483, Test loss: 1.526, Test accuracy: 93.72 

Round  82, Global train loss: 1.483, Global test loss: 1.521, Global test accuracy: 94.28 

Round  83, Train loss: 1.481, Test loss: 1.526, Test accuracy: 93.73 

Round  83, Global train loss: 1.481, Global test loss: 1.521, Global test accuracy: 94.34 

Round  84, Train loss: 1.484, Test loss: 1.526, Test accuracy: 93.74 

Round  84, Global train loss: 1.484, Global test loss: 1.521, Global test accuracy: 94.22 

Round  85, Train loss: 1.489, Test loss: 1.525, Test accuracy: 93.74 

Round  85, Global train loss: 1.489, Global test loss: 1.521, Global test accuracy: 94.32 

Round  86, Train loss: 1.484, Test loss: 1.525, Test accuracy: 93.75 

Round  86, Global train loss: 1.484, Global test loss: 1.521, Global test accuracy: 94.27 

Round  87, Train loss: 1.482, Test loss: 1.525, Test accuracy: 93.77 

Round  87, Global train loss: 1.482, Global test loss: 1.521, Global test accuracy: 94.21 

Round  88, Train loss: 1.482, Test loss: 1.525, Test accuracy: 93.73 

Round  88, Global train loss: 1.482, Global test loss: 1.520, Global test accuracy: 94.35 

Round  89, Train loss: 1.482, Test loss: 1.525, Test accuracy: 93.76 

Round  89, Global train loss: 1.482, Global test loss: 1.521, Global test accuracy: 94.27 

Round  90, Train loss: 1.483, Test loss: 1.525, Test accuracy: 93.77 

Round  90, Global train loss: 1.483, Global test loss: 1.520, Global test accuracy: 94.45 

Round  91, Train loss: 1.483, Test loss: 1.524, Test accuracy: 93.84 

Round  91, Global train loss: 1.483, Global test loss: 1.520, Global test accuracy: 94.51 

Round  92, Train loss: 1.487, Test loss: 1.524, Test accuracy: 93.90 

Round  92, Global train loss: 1.487, Global test loss: 1.520, Global test accuracy: 94.38 

Round  93, Train loss: 1.483, Test loss: 1.524, Test accuracy: 93.92 

Round  93, Global train loss: 1.483, Global test loss: 1.521, Global test accuracy: 94.34 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.480, Test loss: 1.523, Test accuracy: 93.97 

Round  94, Global train loss: 1.480, Global test loss: 1.520, Global test accuracy: 94.42 

Round  95, Train loss: 1.485, Test loss: 1.523, Test accuracy: 93.97 

Round  95, Global train loss: 1.485, Global test loss: 1.521, Global test accuracy: 94.30 

Round  96, Train loss: 1.480, Test loss: 1.523, Test accuracy: 93.94 

Round  96, Global train loss: 1.480, Global test loss: 1.519, Global test accuracy: 94.58 

Round  97, Train loss: 1.484, Test loss: 1.523, Test accuracy: 93.96 

Round  97, Global train loss: 1.484, Global test loss: 1.520, Global test accuracy: 94.40 

Round  98, Train loss: 1.490, Test loss: 1.524, Test accuracy: 93.88 

Round  98, Global train loss: 1.490, Global test loss: 1.520, Global test accuracy: 94.42 

Round  99, Train loss: 1.482, Test loss: 1.524, Test accuracy: 93.92 

Round  99, Global train loss: 1.482, Global test loss: 1.519, Global test accuracy: 94.66 

Final Round, Train loss: 1.482, Test loss: 1.523, Test accuracy: 94.08 

Final Round, Global train loss: 1.482, Global test loss: 1.519, Global test accuracy: 94.66 

Average accuracy final 10 rounds: 93.90666666666667 

Average global accuracy final 10 rounds: 94.445 

902.440863609314
[0.8706974983215332, 1.6337158679962158, 2.3895342350006104, 3.1409213542938232, 3.898002862930298, 4.653726816177368, 5.4068522453308105, 6.155097246170044, 6.800069093704224, 7.450962781906128, 8.09106731414795, 8.736228227615356, 9.383744716644287, 10.01888370513916, 10.664761066436768, 11.312366485595703, 11.948491334915161, 12.59598708152771, 13.242346286773682, 13.886168241500854, 14.534566640853882, 15.184273481369019, 15.824649333953857, 16.470887899398804, 17.124910354614258, 17.762377500534058, 18.414175033569336, 19.068559169769287, 19.70612120628357, 20.358710289001465, 21.010528087615967, 21.648688077926636, 22.302201509475708, 22.952276706695557, 23.597309112548828, 24.250380992889404, 24.903517484664917, 25.549514293670654, 26.20813822746277, 26.858492374420166, 27.50499653816223, 28.152613878250122, 28.807862997055054, 29.450304746627808, 30.093129634857178, 30.743393182754517, 31.396703720092773, 32.03622913360596, 32.691269397735596, 33.34561562538147, 33.98407459259033, 34.63578128814697, 35.28372931480408, 35.92406439781189, 36.57233762741089, 37.22589564323425, 37.86478304862976, 38.515586614608765, 39.16550254821777, 39.81177520751953, 40.45845818519592, 41.1038293838501, 41.74457049369812, 42.387794733047485, 43.0533971786499, 43.698869466781616, 44.33725166320801, 44.99091935157776, 45.64238905906677, 46.279078006744385, 46.93131375312805, 47.58245015144348, 48.23124885559082, 48.88110280036926, 49.536768436431885, 50.182437896728516, 50.828099727630615, 51.474461793899536, 52.11971831321716, 52.76824641227722, 53.41747188568115, 54.069409132003784, 54.71498417854309, 55.36205291748047, 56.019296407699585, 56.666020154953, 57.31776285171509, 57.97271251678467, 58.61862063407898, 59.26128530502319, 59.91285014152527, 60.56709384918213, 61.20573902130127, 61.85770225524902, 62.501851081848145, 63.14770174026489, 63.797605991363525, 64.44561624526978, 65.10168695449829, 65.75303959846497, 67.05821442604065]
[17.133333333333333, 27.566666666666666, 37.55, 38.083333333333336, 43.06666666666667, 48.75, 52.016666666666666, 52.31666666666667, 53.69166666666667, 56.05, 59.141666666666666, 63.74166666666667, 68.14166666666667, 73.66666666666667, 77.31666666666666, 78.66666666666667, 80.74166666666666, 81.25833333333334, 83.7, 84.31666666666666, 84.74166666666666, 86.075, 86.48333333333333, 90.03333333333333, 90.26666666666667, 90.35, 90.49166666666666, 90.575, 90.725, 90.825, 91.075, 91.10833333333333, 91.11666666666666, 91.38333333333334, 91.56666666666666, 91.7, 91.85833333333333, 91.90833333333333, 91.825, 91.90833333333333, 91.84166666666667, 91.86666666666666, 91.875, 92.025, 91.99166666666666, 92.13333333333334, 92.275, 92.50833333333334, 92.65, 92.575, 92.65, 92.7, 92.73333333333333, 92.78333333333333, 92.89166666666667, 92.975, 93.05833333333334, 93.16666666666667, 93.25, 93.20833333333333, 93.2, 93.28333333333333, 93.18333333333334, 93.21666666666667, 93.28333333333333, 93.30833333333334, 93.325, 93.375, 93.46666666666667, 93.51666666666667, 93.6, 93.61666666666666, 93.66666666666667, 93.71666666666667, 93.70833333333333, 93.68333333333334, 93.75, 93.83333333333333, 93.73333333333333, 93.73333333333333, 93.71666666666667, 93.725, 93.71666666666667, 93.73333333333333, 93.74166666666666, 93.74166666666666, 93.75, 93.76666666666667, 93.73333333333333, 93.75833333333334, 93.76666666666667, 93.84166666666667, 93.9, 93.91666666666667, 93.975, 93.96666666666667, 93.94166666666666, 93.95833333333333, 93.88333333333334, 93.91666666666667, 94.08333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.300, Test accuracy: 16.64 

Round   1, Train loss: 2.300, Test loss: 2.299, Test accuracy: 26.22 

Round   2, Train loss: 2.298, Test loss: 2.296, Test accuracy: 35.90 

Round   3, Train loss: 2.295, Test loss: 2.293, Test accuracy: 40.47 

Round   4, Train loss: 2.291, Test loss: 2.287, Test accuracy: 45.62 

Round   5, Train loss: 2.284, Test loss: 2.278, Test accuracy: 45.69 

Round   6, Train loss: 2.274, Test loss: 2.261, Test accuracy: 44.92 

Round   7, Train loss: 2.239, Test loss: 2.204, Test accuracy: 40.42 

Round   8, Train loss: 2.155, Test loss: 2.099, Test accuracy: 46.72 

Round   9, Train loss: 2.039, Test loss: 1.998, Test accuracy: 59.47 

Round  10, Train loss: 1.924, Test loss: 1.914, Test accuracy: 62.33 

Round  11, Train loss: 1.863, Test loss: 1.854, Test accuracy: 66.15 

Round  12, Train loss: 1.804, Test loss: 1.811, Test accuracy: 69.13 

Round  13, Train loss: 1.751, Test loss: 1.767, Test accuracy: 73.18 

Round  14, Train loss: 1.729, Test loss: 1.736, Test accuracy: 75.72 

Round  15, Train loss: 1.689, Test loss: 1.712, Test accuracy: 77.74 

Round  16, Train loss: 1.685, Test loss: 1.690, Test accuracy: 79.79 

Round  17, Train loss: 1.665, Test loss: 1.682, Test accuracy: 80.21 

Round  18, Train loss: 1.656, Test loss: 1.672, Test accuracy: 80.90 

Round  19, Train loss: 1.644, Test loss: 1.668, Test accuracy: 81.19 

Round  20, Train loss: 1.652, Test loss: 1.665, Test accuracy: 81.28 

Round  21, Train loss: 1.654, Test loss: 1.656, Test accuracy: 82.15 

Round  22, Train loss: 1.645, Test loss: 1.652, Test accuracy: 82.37 

Round  23, Train loss: 1.640, Test loss: 1.650, Test accuracy: 82.49 

Round  24, Train loss: 1.636, Test loss: 1.644, Test accuracy: 83.06 

Round  25, Train loss: 1.623, Test loss: 1.642, Test accuracy: 83.01 

Round  26, Train loss: 1.632, Test loss: 1.639, Test accuracy: 83.35 

Round  27, Train loss: 1.616, Test loss: 1.636, Test accuracy: 83.88 

Round  28, Train loss: 1.605, Test loss: 1.628, Test accuracy: 84.62 

Round  29, Train loss: 1.595, Test loss: 1.621, Test accuracy: 85.42 

Round  30, Train loss: 1.592, Test loss: 1.616, Test accuracy: 85.99 

Round  31, Train loss: 1.578, Test loss: 1.613, Test accuracy: 86.26 

Round  32, Train loss: 1.576, Test loss: 1.611, Test accuracy: 86.25 

Round  33, Train loss: 1.566, Test loss: 1.599, Test accuracy: 87.64 

Round  34, Train loss: 1.564, Test loss: 1.590, Test accuracy: 88.57 

Round  35, Train loss: 1.563, Test loss: 1.587, Test accuracy: 88.66 

Round  36, Train loss: 1.553, Test loss: 1.581, Test accuracy: 89.17 

Round  37, Train loss: 1.548, Test loss: 1.580, Test accuracy: 89.28 

Round  38, Train loss: 1.554, Test loss: 1.579, Test accuracy: 89.31 

Round  39, Train loss: 1.544, Test loss: 1.581, Test accuracy: 88.99 

Round  40, Train loss: 1.537, Test loss: 1.578, Test accuracy: 89.24 

Round  41, Train loss: 1.533, Test loss: 1.577, Test accuracy: 89.31 

Round  42, Train loss: 1.532, Test loss: 1.574, Test accuracy: 89.63 

Round  43, Train loss: 1.541, Test loss: 1.573, Test accuracy: 89.58 

Round  44, Train loss: 1.538, Test loss: 1.575, Test accuracy: 89.50 

Round  45, Train loss: 1.533, Test loss: 1.574, Test accuracy: 89.47 

Round  46, Train loss: 1.532, Test loss: 1.573, Test accuracy: 89.62 

Round  47, Train loss: 1.525, Test loss: 1.570, Test accuracy: 89.88 

Round  48, Train loss: 1.525, Test loss: 1.568, Test accuracy: 90.01 

Round  49, Train loss: 1.531, Test loss: 1.568, Test accuracy: 89.94 

Round  50, Train loss: 1.528, Test loss: 1.568, Test accuracy: 89.93 

Round  51, Train loss: 1.523, Test loss: 1.566, Test accuracy: 90.12 

Round  52, Train loss: 1.527, Test loss: 1.565, Test accuracy: 90.18 

Round  53, Train loss: 1.522, Test loss: 1.565, Test accuracy: 90.33 

Round  54, Train loss: 1.523, Test loss: 1.563, Test accuracy: 90.44 

Round  55, Train loss: 1.516, Test loss: 1.562, Test accuracy: 90.50 

Round  56, Train loss: 1.520, Test loss: 1.563, Test accuracy: 90.42 

Round  57, Train loss: 1.519, Test loss: 1.562, Test accuracy: 90.49 

Round  58, Train loss: 1.517, Test loss: 1.562, Test accuracy: 90.46 

Round  59, Train loss: 1.518, Test loss: 1.561, Test accuracy: 90.49 

Round  60, Train loss: 1.517, Test loss: 1.561, Test accuracy: 90.46 

Round  61, Train loss: 1.513, Test loss: 1.560, Test accuracy: 90.62 

Round  62, Train loss: 1.512, Test loss: 1.561, Test accuracy: 90.47 

Round  63, Train loss: 1.505, Test loss: 1.559, Test accuracy: 90.65 

Round  64, Train loss: 1.514, Test loss: 1.559, Test accuracy: 90.59 

Round  65, Train loss: 1.512, Test loss: 1.559, Test accuracy: 90.56 

Round  66, Train loss: 1.509, Test loss: 1.558, Test accuracy: 90.67 

Round  67, Train loss: 1.512, Test loss: 1.558, Test accuracy: 90.69 

Round  68, Train loss: 1.503, Test loss: 1.557, Test accuracy: 90.82 

Round  69, Train loss: 1.507, Test loss: 1.557, Test accuracy: 90.75 

Round  70, Train loss: 1.512, Test loss: 1.558, Test accuracy: 90.68 

Round  71, Train loss: 1.508, Test loss: 1.557, Test accuracy: 90.67 

Round  72, Train loss: 1.510, Test loss: 1.557, Test accuracy: 90.61 

Round  73, Train loss: 1.510, Test loss: 1.557, Test accuracy: 90.72 

Round  74, Train loss: 1.506, Test loss: 1.556, Test accuracy: 90.84 

Round  75, Train loss: 1.506, Test loss: 1.555, Test accuracy: 90.88 

Round  76, Train loss: 1.509, Test loss: 1.556, Test accuracy: 90.86 

Round  77, Train loss: 1.507, Test loss: 1.556, Test accuracy: 90.91 

Round  78, Train loss: 1.508, Test loss: 1.556, Test accuracy: 90.83 

Round  79, Train loss: 1.505, Test loss: 1.555, Test accuracy: 90.92 

Round  80, Train loss: 1.507, Test loss: 1.556, Test accuracy: 90.65 

Round  81, Train loss: 1.500, Test loss: 1.556, Test accuracy: 90.79 

Round  82, Train loss: 1.505, Test loss: 1.556, Test accuracy: 90.75 

Round  83, Train loss: 1.502, Test loss: 1.556, Test accuracy: 90.90 

Round  84, Train loss: 1.508, Test loss: 1.555, Test accuracy: 90.87 

Round  85, Train loss: 1.504, Test loss: 1.554, Test accuracy: 90.96 

Round  86, Train loss: 1.500, Test loss: 1.554, Test accuracy: 90.88 

Round  87, Train loss: 1.503, Test loss: 1.554, Test accuracy: 91.07 

Round  88, Train loss: 1.506, Test loss: 1.554, Test accuracy: 90.97 

Round  89, Train loss: 1.503, Test loss: 1.555, Test accuracy: 90.92 

Round  90, Train loss: 1.502, Test loss: 1.555, Test accuracy: 90.89 

Round  91, Train loss: 1.502, Test loss: 1.555, Test accuracy: 90.92 

Round  92, Train loss: 1.497, Test loss: 1.554, Test accuracy: 91.04 

Round  93, Train loss: 1.502, Test loss: 1.553, Test accuracy: 90.97 

Round  94, Train loss: 1.500, Test loss: 1.553, Test accuracy: 91.08 

Round  95, Train loss: 1.500, Test loss: 1.553, Test accuracy: 91.16 

Round  96, Train loss: 1.498, Test loss: 1.553, Test accuracy: 90.96 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 1.500, Test loss: 1.553, Test accuracy: 90.95 

Round  98, Train loss: 1.502, Test loss: 1.553, Test accuracy: 91.03 

Round  99, Train loss: 1.500, Test loss: 1.553, Test accuracy: 91.08 

Final Round, Train loss: 1.500, Test loss: 1.554, Test accuracy: 90.91 

Average accuracy final 10 rounds: 91.00666666666667 

671.8655269145966
[0.8284201622009277, 1.5808742046356201, 2.2562928199768066, 2.9397847652435303, 3.6191065311431885, 4.302206039428711, 4.987476348876953, 5.671797752380371, 6.351391315460205, 7.03665566444397, 7.717416763305664, 8.39799427986145, 9.077645301818848, 9.759738206863403, 10.44271731376648, 11.12128758430481, 11.803993463516235, 12.48388934135437, 13.165602445602417, 13.846582412719727, 14.524216413497925, 15.206094741821289, 15.884248495101929, 16.567954301834106, 17.24797010421753, 17.92730212211609, 18.606048107147217, 19.285614728927612, 19.967992544174194, 20.647236108779907, 21.333252906799316, 22.010868549346924, 22.69489288330078, 23.36824917793274, 24.050581455230713, 24.727408170700073, 25.40682101249695, 26.086579084396362, 26.765724182128906, 27.444292783737183, 28.120254278182983, 28.803292989730835, 29.476354360580444, 30.162471771240234, 30.84123706817627, 31.52344560623169, 32.20387816429138, 32.89117169380188, 33.57138419151306, 34.24808740615845, 34.93130302429199, 35.602009534835815, 36.28995728492737, 36.965280532836914, 37.65488910675049, 38.33257532119751, 39.01177763938904, 39.6927330493927, 40.37038969993591, 41.054131269454956, 41.73199510574341, 42.42032861709595, 43.09664511680603, 43.774433612823486, 44.45280051231384, 45.1314058303833, 45.81408381462097, 46.502036333084106, 47.18502712249756, 47.86600708961487, 48.551276206970215, 49.22857451438904, 49.90937685966492, 50.586830377578735, 51.27281308174133, 51.95215702056885, 52.638993978500366, 53.321682929992676, 54.00330424308777, 54.68397092819214, 55.36530613899231, 56.0462327003479, 56.72940945625305, 57.40892052650452, 58.09569525718689, 58.78016400337219, 59.458574295043945, 60.13758063316345, 60.804977893829346, 61.4850697517395, 62.156519651412964, 62.82826900482178, 63.500186920166016, 64.17297697067261, 64.8459644317627, 65.51599311828613, 66.19143486022949, 66.86851000785828, 67.5429892539978, 68.22444200515747, 69.43384718894958]
[16.641666666666666, 26.216666666666665, 35.9, 40.46666666666667, 45.61666666666667, 45.69166666666667, 44.916666666666664, 40.425, 46.71666666666667, 59.46666666666667, 62.333333333333336, 66.15, 69.13333333333334, 73.18333333333334, 75.725, 77.74166666666666, 79.79166666666667, 80.20833333333333, 80.9, 81.19166666666666, 81.275, 82.15, 82.36666666666666, 82.49166666666666, 83.05833333333334, 83.00833333333334, 83.35, 83.875, 84.61666666666666, 85.41666666666667, 85.99166666666666, 86.25833333333334, 86.25, 87.64166666666667, 88.56666666666666, 88.65833333333333, 89.175, 89.275, 89.30833333333334, 88.99166666666666, 89.24166666666666, 89.30833333333334, 89.63333333333334, 89.58333333333333, 89.5, 89.46666666666667, 89.625, 89.875, 90.00833333333334, 89.94166666666666, 89.93333333333334, 90.11666666666666, 90.18333333333334, 90.325, 90.44166666666666, 90.5, 90.41666666666667, 90.49166666666666, 90.45833333333333, 90.49166666666666, 90.45833333333333, 90.625, 90.475, 90.65, 90.59166666666667, 90.55833333333334, 90.675, 90.69166666666666, 90.81666666666666, 90.75, 90.68333333333334, 90.66666666666667, 90.60833333333333, 90.725, 90.84166666666667, 90.875, 90.85833333333333, 90.90833333333333, 90.83333333333333, 90.91666666666667, 90.65, 90.79166666666667, 90.75, 90.9, 90.86666666666666, 90.95833333333333, 90.88333333333334, 91.06666666666666, 90.975, 90.925, 90.89166666666667, 90.91666666666667, 91.04166666666667, 90.975, 91.075, 91.15833333333333, 90.95833333333333, 90.95, 91.025, 91.075, 90.90833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.299, Test accuracy: 12.93 

Round   1, Train loss: 2.295, Test loss: 2.292, Test accuracy: 29.68 

Round   2, Train loss: 2.282, Test loss: 2.271, Test accuracy: 30.24 

Round   3, Train loss: 2.221, Test loss: 2.180, Test accuracy: 41.55 

Round   4, Train loss: 2.081, Test loss: 2.054, Test accuracy: 45.21 

Round   5, Train loss: 1.983, Test loss: 1.970, Test accuracy: 51.17 

Round   6, Train loss: 1.880, Test loss: 1.892, Test accuracy: 58.70 

Round   7, Train loss: 1.779, Test loss: 1.820, Test accuracy: 66.76 

Round   8, Train loss: 1.725, Test loss: 1.762, Test accuracy: 72.41 

Round   9, Train loss: 1.686, Test loss: 1.723, Test accuracy: 75.53 

Round  10, Train loss: 1.672, Test loss: 1.682, Test accuracy: 79.67 

Round  11, Train loss: 1.647, Test loss: 1.663, Test accuracy: 81.44 

Round  12, Train loss: 1.645, Test loss: 1.649, Test accuracy: 82.56 

Round  13, Train loss: 1.626, Test loss: 1.643, Test accuracy: 82.97 

Round  14, Train loss: 1.625, Test loss: 1.638, Test accuracy: 83.39 

Round  15, Train loss: 1.616, Test loss: 1.636, Test accuracy: 83.52 

Round  16, Train loss: 1.625, Test loss: 1.630, Test accuracy: 83.97 

Round  17, Train loss: 1.620, Test loss: 1.628, Test accuracy: 84.05 

Round  18, Train loss: 1.607, Test loss: 1.626, Test accuracy: 84.12 

Round  19, Train loss: 1.610, Test loss: 1.625, Test accuracy: 84.23 

Round  20, Train loss: 1.602, Test loss: 1.624, Test accuracy: 84.44 

Round  21, Train loss: 1.603, Test loss: 1.622, Test accuracy: 84.46 

Round  22, Train loss: 1.608, Test loss: 1.621, Test accuracy: 84.56 

Round  23, Train loss: 1.606, Test loss: 1.620, Test accuracy: 84.51 

Round  24, Train loss: 1.592, Test loss: 1.619, Test accuracy: 84.54 

Round  25, Train loss: 1.597, Test loss: 1.617, Test accuracy: 84.77 

Round  26, Train loss: 1.590, Test loss: 1.617, Test accuracy: 84.84 

Round  27, Train loss: 1.603, Test loss: 1.617, Test accuracy: 84.91 

Round  28, Train loss: 1.599, Test loss: 1.616, Test accuracy: 84.89 

Round  29, Train loss: 1.594, Test loss: 1.616, Test accuracy: 84.89 

Round  30, Train loss: 1.586, Test loss: 1.616, Test accuracy: 84.95 

Round  31, Train loss: 1.589, Test loss: 1.615, Test accuracy: 84.95 

Round  32, Train loss: 1.594, Test loss: 1.615, Test accuracy: 85.00 

Round  33, Train loss: 1.591, Test loss: 1.615, Test accuracy: 84.91 

Round  34, Train loss: 1.600, Test loss: 1.615, Test accuracy: 84.96 

Round  35, Train loss: 1.585, Test loss: 1.615, Test accuracy: 84.96 

Round  36, Train loss: 1.589, Test loss: 1.614, Test accuracy: 85.02 

Round  37, Train loss: 1.589, Test loss: 1.614, Test accuracy: 85.00 

Round  38, Train loss: 1.589, Test loss: 1.614, Test accuracy: 85.10 

Round  39, Train loss: 1.592, Test loss: 1.612, Test accuracy: 85.14 

Round  40, Train loss: 1.587, Test loss: 1.613, Test accuracy: 85.12 

Round  41, Train loss: 1.586, Test loss: 1.613, Test accuracy: 85.13 

Round  42, Train loss: 1.584, Test loss: 1.612, Test accuracy: 85.14 

Round  43, Train loss: 1.583, Test loss: 1.612, Test accuracy: 85.17 

Round  44, Train loss: 1.585, Test loss: 1.612, Test accuracy: 85.19 

Round  45, Train loss: 1.582, Test loss: 1.612, Test accuracy: 85.25 

Round  46, Train loss: 1.580, Test loss: 1.612, Test accuracy: 85.19 

Round  47, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.29 

Round  48, Train loss: 1.590, Test loss: 1.611, Test accuracy: 85.30 

Round  49, Train loss: 1.578, Test loss: 1.611, Test accuracy: 85.28 

Round  50, Train loss: 1.589, Test loss: 1.611, Test accuracy: 85.33 

Round  51, Train loss: 1.584, Test loss: 1.611, Test accuracy: 85.22 

Round  52, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.28 

Round  53, Train loss: 1.586, Test loss: 1.610, Test accuracy: 85.33 

Round  54, Train loss: 1.589, Test loss: 1.610, Test accuracy: 85.30 

Round  55, Train loss: 1.578, Test loss: 1.610, Test accuracy: 85.31 

Round  56, Train loss: 1.584, Test loss: 1.610, Test accuracy: 85.37 

Round  57, Train loss: 1.578, Test loss: 1.610, Test accuracy: 85.42 

Round  58, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.38 

Round  59, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.35 

Round  60, Train loss: 1.575, Test loss: 1.609, Test accuracy: 85.37 

Round  61, Train loss: 1.578, Test loss: 1.609, Test accuracy: 85.42 

Round  62, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.42 

Round  63, Train loss: 1.573, Test loss: 1.609, Test accuracy: 85.45 

Round  64, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.41 

Round  65, Train loss: 1.590, Test loss: 1.609, Test accuracy: 85.31 

Round  66, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.37 

Round  67, Train loss: 1.584, Test loss: 1.609, Test accuracy: 85.37 

Round  68, Train loss: 1.583, Test loss: 1.609, Test accuracy: 85.35 

Round  69, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.45 

Round  70, Train loss: 1.582, Test loss: 1.608, Test accuracy: 85.46 

Round  71, Train loss: 1.580, Test loss: 1.608, Test accuracy: 85.53 

Round  72, Train loss: 1.580, Test loss: 1.607, Test accuracy: 85.57 

Round  73, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.62 

Round  74, Train loss: 1.563, Test loss: 1.606, Test accuracy: 85.73 

Round  75, Train loss: 1.569, Test loss: 1.606, Test accuracy: 85.83 

Round  76, Train loss: 1.567, Test loss: 1.606, Test accuracy: 85.78 

Round  77, Train loss: 1.563, Test loss: 1.604, Test accuracy: 85.88 

Round  78, Train loss: 1.574, Test loss: 1.604, Test accuracy: 85.95 

Round  79, Train loss: 1.562, Test loss: 1.603, Test accuracy: 86.01 

Round  80, Train loss: 1.568, Test loss: 1.603, Test accuracy: 85.96 

Round  81, Train loss: 1.551, Test loss: 1.601, Test accuracy: 86.13 

Round  82, Train loss: 1.571, Test loss: 1.602, Test accuracy: 86.20 

Round  83, Train loss: 1.562, Test loss: 1.602, Test accuracy: 86.15 

Round  84, Train loss: 1.569, Test loss: 1.601, Test accuracy: 86.21 

Round  85, Train loss: 1.577, Test loss: 1.602, Test accuracy: 86.15 

Round  86, Train loss: 1.553, Test loss: 1.601, Test accuracy: 86.19 

Round  87, Train loss: 1.569, Test loss: 1.601, Test accuracy: 86.16 

Round  88, Train loss: 1.538, Test loss: 1.600, Test accuracy: 86.25 

Round  89, Train loss: 1.555, Test loss: 1.600, Test accuracy: 86.19 

Round  90, Train loss: 1.549, Test loss: 1.600, Test accuracy: 86.28 

Round  91, Train loss: 1.573, Test loss: 1.600, Test accuracy: 86.28 

Round  92, Train loss: 1.558, Test loss: 1.599, Test accuracy: 86.35 

Round  93, Train loss: 1.555, Test loss: 1.599, Test accuracy: 86.38 

Round  94, Train loss: 1.565, Test loss: 1.599, Test accuracy: 86.37 

Round  95, Train loss: 1.568, Test loss: 1.598, Test accuracy: 86.49 

Round  96, Train loss: 1.551, Test loss: 1.598, Test accuracy: 86.46 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 1.551, Test loss: 1.598, Test accuracy: 86.51 

Round  98, Train loss: 1.553, Test loss: 1.597, Test accuracy: 86.54 

Round  99, Train loss: 1.559, Test loss: 1.596, Test accuracy: 86.72 

Final Round, Train loss: 1.552, Test loss: 1.592, Test accuracy: 87.15 

Average accuracy final 10 rounds: 86.43750000000001 

688.5101847648621
[0.8061695098876953, 1.5195808410644531, 2.2590489387512207, 3.009612560272217, 3.758556842803955, 4.506368398666382, 5.251147031784058, 6.00141167640686, 6.74948787689209, 7.499538421630859, 8.248693704605103, 8.991097211837769, 9.741246938705444, 10.483635187149048, 11.228221416473389, 11.971707344055176, 12.715337991714478, 13.461381435394287, 14.201412200927734, 14.94640588760376, 15.716569423675537, 16.526384115219116, 17.18184781074524, 17.83115315437317, 18.49340057373047, 19.14484453201294, 19.78812289237976, 20.444938898086548, 21.09439182281494, 21.749844074249268, 22.403055429458618, 23.056220054626465, 23.711101531982422, 24.371881246566772, 25.03167414665222, 25.686737775802612, 26.34176254272461, 27.00040888786316, 27.655211448669434, 28.307394981384277, 28.969502687454224, 29.63071846961975, 30.280019521713257, 30.93987536430359, 31.60033631324768, 32.25731325149536, 32.91072702407837, 33.56332778930664, 34.21190571784973, 34.86906433105469, 35.526167154312134, 36.17361283302307, 36.83070707321167, 37.48408317565918, 38.13920068740845, 38.78706765174866, 39.44002294540405, 40.090794801712036, 40.737573862075806, 41.39773869514465, 42.047592878341675, 42.68906903266907, 43.343236684799194, 43.99772810935974, 44.64776825904846, 45.40807247161865, 46.16866397857666, 46.92659902572632, 47.68542194366455, 48.447818994522095, 49.200361490249634, 49.95525407791138, 50.71217894554138, 51.466997146606445, 52.22749471664429, 52.989712953567505, 53.746742725372314, 54.50406622886658, 55.26313614845276, 56.01879167556763, 56.77999997138977, 57.54423379898071, 58.303027391433716, 59.060178995132446, 59.81424045562744, 60.57519316673279, 61.33608269691467, 62.09181475639343, 62.84661650657654, 63.598387241363525, 64.35198593139648, 65.10608720779419, 65.8605408668518, 66.61901235580444, 67.37372255325317, 68.02795481681824, 68.67596888542175, 69.33012700080872, 69.98802280426025, 70.6348078250885, 71.78760814666748]
[12.933333333333334, 29.675, 30.241666666666667, 41.55, 45.208333333333336, 51.175, 58.7, 66.75833333333334, 72.40833333333333, 75.53333333333333, 79.675, 81.44166666666666, 82.55833333333334, 82.96666666666667, 83.39166666666667, 83.51666666666667, 83.96666666666667, 84.05, 84.125, 84.23333333333333, 84.44166666666666, 84.45833333333333, 84.55833333333334, 84.50833333333334, 84.54166666666667, 84.76666666666667, 84.84166666666667, 84.90833333333333, 84.89166666666667, 84.89166666666667, 84.95, 84.95, 85.0, 84.90833333333333, 84.95833333333333, 84.95833333333333, 85.01666666666667, 85.0, 85.1, 85.14166666666667, 85.125, 85.13333333333334, 85.14166666666667, 85.16666666666667, 85.19166666666666, 85.25, 85.19166666666666, 85.29166666666667, 85.3, 85.28333333333333, 85.33333333333333, 85.225, 85.28333333333333, 85.325, 85.3, 85.30833333333334, 85.36666666666666, 85.41666666666667, 85.38333333333334, 85.35, 85.36666666666666, 85.425, 85.41666666666667, 85.45, 85.40833333333333, 85.30833333333334, 85.36666666666666, 85.36666666666666, 85.35, 85.45, 85.45833333333333, 85.525, 85.56666666666666, 85.61666666666666, 85.73333333333333, 85.825, 85.78333333333333, 85.875, 85.95, 86.00833333333334, 85.95833333333333, 86.13333333333334, 86.2, 86.15, 86.20833333333333, 86.15, 86.19166666666666, 86.15833333333333, 86.25, 86.19166666666666, 86.275, 86.28333333333333, 86.35, 86.38333333333334, 86.36666666666666, 86.49166666666666, 86.45833333333333, 86.50833333333334, 86.54166666666667, 86.71666666666667, 87.15]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.299, Test accuracy: 9.81 

Round   1, Train loss: 2.295, Test loss: 2.293, Test accuracy: 20.72 

Round   2, Train loss: 2.279, Test loss: 2.271, Test accuracy: 21.09 

Round   3, Train loss: 2.249, Test loss: 2.234, Test accuracy: 26.47 

Round   4, Train loss: 2.174, Test loss: 2.166, Test accuracy: 38.14 

Round   5, Train loss: 2.081, Test loss: 2.072, Test accuracy: 47.25 

Round   6, Train loss: 1.969, Test loss: 1.981, Test accuracy: 55.27 

Round   7, Train loss: 1.841, Test loss: 1.912, Test accuracy: 65.04 

Round   8, Train loss: 1.786, Test loss: 1.841, Test accuracy: 69.37 

Round   9, Train loss: 1.745, Test loss: 1.787, Test accuracy: 72.62 

Round  10, Train loss: 1.663, Test loss: 1.773, Test accuracy: 73.08 

Round  11, Train loss: 1.686, Test loss: 1.741, Test accuracy: 75.03 

Round  12, Train loss: 1.656, Test loss: 1.722, Test accuracy: 76.47 

Round  13, Train loss: 1.624, Test loss: 1.714, Test accuracy: 76.95 

Round  14, Train loss: 1.609, Test loss: 1.708, Test accuracy: 77.22 

Round  15, Train loss: 1.629, Test loss: 1.703, Test accuracy: 77.45 

Round  16, Train loss: 1.608, Test loss: 1.699, Test accuracy: 77.69 

Round  17, Train loss: 1.606, Test loss: 1.697, Test accuracy: 77.88 

Round  18, Train loss: 1.595, Test loss: 1.695, Test accuracy: 77.83 

Round  19, Train loss: 1.586, Test loss: 1.693, Test accuracy: 78.00 

Round  20, Train loss: 1.604, Test loss: 1.688, Test accuracy: 78.45 

Round  21, Train loss: 1.577, Test loss: 1.687, Test accuracy: 78.49 

Round  22, Train loss: 1.585, Test loss: 1.686, Test accuracy: 78.47 

Round  23, Train loss: 1.582, Test loss: 1.686, Test accuracy: 78.48 

Round  24, Train loss: 1.593, Test loss: 1.685, Test accuracy: 78.53 

Round  25, Train loss: 1.580, Test loss: 1.685, Test accuracy: 78.46 

Round  26, Train loss: 1.577, Test loss: 1.685, Test accuracy: 78.36 

Round  27, Train loss: 1.575, Test loss: 1.685, Test accuracy: 78.39 

Round  28, Train loss: 1.577, Test loss: 1.684, Test accuracy: 78.46 

Round  29, Train loss: 1.572, Test loss: 1.684, Test accuracy: 78.46 

Round  30, Train loss: 1.572, Test loss: 1.683, Test accuracy: 78.42 

Round  31, Train loss: 1.563, Test loss: 1.673, Test accuracy: 79.54 

Round  32, Train loss: 1.524, Test loss: 1.659, Test accuracy: 81.82 

Round  33, Train loss: 1.523, Test loss: 1.649, Test accuracy: 82.74 

Round  34, Train loss: 1.507, Test loss: 1.642, Test accuracy: 83.40 

Round  35, Train loss: 1.500, Test loss: 1.637, Test accuracy: 83.88 

Round  36, Train loss: 1.511, Test loss: 1.629, Test accuracy: 84.63 

Round  37, Train loss: 1.506, Test loss: 1.624, Test accuracy: 85.38 

Round  38, Train loss: 1.493, Test loss: 1.622, Test accuracy: 85.31 

Round  39, Train loss: 1.485, Test loss: 1.620, Test accuracy: 85.53 

Round  40, Train loss: 1.488, Test loss: 1.619, Test accuracy: 85.44 

Round  41, Train loss: 1.485, Test loss: 1.618, Test accuracy: 85.53 

Round  42, Train loss: 1.487, Test loss: 1.618, Test accuracy: 85.47 

Round  43, Train loss: 1.493, Test loss: 1.616, Test accuracy: 85.52 

Round  44, Train loss: 1.487, Test loss: 1.616, Test accuracy: 85.52 

Round  45, Train loss: 1.484, Test loss: 1.616, Test accuracy: 85.52 

Round  46, Train loss: 1.485, Test loss: 1.615, Test accuracy: 85.61 

Round  47, Train loss: 1.486, Test loss: 1.615, Test accuracy: 85.67 

Round  48, Train loss: 1.477, Test loss: 1.614, Test accuracy: 85.67 

Round  49, Train loss: 1.488, Test loss: 1.614, Test accuracy: 85.58 

Round  50, Train loss: 1.489, Test loss: 1.613, Test accuracy: 85.64 

Round  51, Train loss: 1.481, Test loss: 1.613, Test accuracy: 85.69 

Round  52, Train loss: 1.482, Test loss: 1.612, Test accuracy: 85.65 

Round  53, Train loss: 1.480, Test loss: 1.612, Test accuracy: 85.68 

Round  54, Train loss: 1.485, Test loss: 1.612, Test accuracy: 85.66 

Round  55, Train loss: 1.486, Test loss: 1.612, Test accuracy: 85.72 

Round  56, Train loss: 1.486, Test loss: 1.611, Test accuracy: 85.75 

Round  57, Train loss: 1.483, Test loss: 1.611, Test accuracy: 85.67 

Round  58, Train loss: 1.482, Test loss: 1.611, Test accuracy: 85.78 

Round  59, Train loss: 1.483, Test loss: 1.611, Test accuracy: 85.69 

Round  60, Train loss: 1.482, Test loss: 1.611, Test accuracy: 85.63 

Round  61, Train loss: 1.483, Test loss: 1.611, Test accuracy: 85.78 

Round  62, Train loss: 1.484, Test loss: 1.611, Test accuracy: 85.72 

Round  63, Train loss: 1.484, Test loss: 1.611, Test accuracy: 85.73 

Round  64, Train loss: 1.484, Test loss: 1.611, Test accuracy: 85.73 

Round  65, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.67 

Round  66, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.71 

Round  67, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.69 

Round  68, Train loss: 1.480, Test loss: 1.610, Test accuracy: 85.69 

Round  69, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.72 

Round  70, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.67 

Round  71, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.72 

Round  72, Train loss: 1.480, Test loss: 1.610, Test accuracy: 85.70 

Round  73, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.67 

Round  74, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.65 

Round  75, Train loss: 1.486, Test loss: 1.610, Test accuracy: 85.65 

Round  76, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.67 

Round  77, Train loss: 1.481, Test loss: 1.610, Test accuracy: 85.67 

Round  78, Train loss: 1.481, Test loss: 1.610, Test accuracy: 85.67 

Round  79, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.66 

Round  80, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.64 

Round  81, Train loss: 1.482, Test loss: 1.610, Test accuracy: 85.65 

Round  82, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.72 

Round  83, Train loss: 1.482, Test loss: 1.610, Test accuracy: 85.72 

Round  84, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.70 

Round  85, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.72 

Round  86, Train loss: 1.482, Test loss: 1.610, Test accuracy: 85.69 

Round  87, Train loss: 1.481, Test loss: 1.610, Test accuracy: 85.67 

Round  88, Train loss: 1.481, Test loss: 1.610, Test accuracy: 85.62 

Round  89, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.62 

Round  90, Train loss: 1.484, Test loss: 1.609, Test accuracy: 85.68 

Round  91, Train loss: 1.478, Test loss: 1.609, Test accuracy: 85.71 

Round  92, Train loss: 1.480, Test loss: 1.609, Test accuracy: 85.71 

Round  93, Train loss: 1.480, Test loss: 1.609, Test accuracy: 85.70 

Round  94, Train loss: 1.481, Test loss: 1.609, Test accuracy: 85.67 

Round  95, Train loss: 1.480, Test loss: 1.609, Test accuracy: 85.72 

Round  96, Train loss: 1.481, Test loss: 1.609, Test accuracy: 85.66 

Round  97, Train loss: 1.479, Test loss: 1.609, Test accuracy: 85.72 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 1.484, Test loss: 1.609, Test accuracy: 85.70 

Round  99, Train loss: 1.483, Test loss: 1.609, Test accuracy: 85.70 

Final Round, Train loss: 1.481, Test loss: 1.609, Test accuracy: 85.67 

Average accuracy final 10 rounds: 85.69666666666666 

725.2032697200775
[0.8952627182006836, 1.6957783699035645, 2.5038020610809326, 3.323000907897949, 4.094758749008179, 4.859174013137817, 5.630231857299805, 6.399234771728516, 7.163597345352173, 7.934328079223633, 8.69733190536499, 9.458141326904297, 10.22089958190918, 10.987489938735962, 11.748006582260132, 12.511425495147705, 13.272780418395996, 14.034607410430908, 14.798208713531494, 15.551633358001709, 16.311479806900024, 17.066861152648926, 17.82231068611145, 18.54719090461731, 19.28226590156555, 20.02179718017578, 20.742634534835815, 21.461727142333984, 22.19013738632202, 22.915400743484497, 23.681495189666748, 24.448994159698486, 25.211061239242554, 25.970027446746826, 26.736559867858887, 27.496475219726562, 28.26299548149109, 29.025734186172485, 29.790919065475464, 30.554016590118408, 31.31709051132202, 32.084226846694946, 32.849984645843506, 33.61026382446289, 34.37408113479614, 35.136029958724976, 35.896583557128906, 36.6560845375061, 37.41767477989197, 38.18197441101074, 38.941144943237305, 39.69689083099365, 40.44187355041504, 41.18695688247681, 41.935351848602295, 42.67730450630188, 43.41784381866455, 44.166494369506836, 44.91233992576599, 45.652092933654785, 46.397329568862915, 47.14816474914551, 47.8983850479126, 48.64562726020813, 49.38654446601868, 50.12947201728821, 50.86373448371887, 51.60953116416931, 52.355180740356445, 53.09939646720886, 53.85795283317566, 54.623464584350586, 55.38299870491028, 56.144296407699585, 56.905216693878174, 57.6628315448761, 58.42604160308838, 59.18472099304199, 59.94461464881897, 60.70698380470276, 61.46656608581543, 62.2250337600708, 62.985721826553345, 63.74002289772034, 64.50499486923218, 65.26466274261475, 66.02658677101135, 66.78217220306396, 67.53727173805237, 68.29415726661682, 69.05239868164062, 69.80689764022827, 70.571608543396, 71.33178067207336, 72.09131336212158, 72.85097002983093, 73.61159133911133, 74.37021350860596, 75.13132524490356, 75.88795375823975, 77.30201172828674]
[9.808333333333334, 20.716666666666665, 21.091666666666665, 26.466666666666665, 38.141666666666666, 47.25, 55.266666666666666, 65.04166666666667, 69.36666666666666, 72.625, 73.08333333333333, 75.025, 76.46666666666667, 76.95, 77.21666666666667, 77.45, 77.69166666666666, 77.88333333333334, 77.825, 78.0, 78.45, 78.49166666666666, 78.46666666666667, 78.48333333333333, 78.525, 78.45833333333333, 78.35833333333333, 78.39166666666667, 78.45833333333333, 78.45833333333333, 78.425, 79.54166666666667, 81.81666666666666, 82.74166666666666, 83.4, 83.875, 84.63333333333334, 85.38333333333334, 85.30833333333334, 85.525, 85.44166666666666, 85.525, 85.46666666666667, 85.51666666666667, 85.51666666666667, 85.51666666666667, 85.60833333333333, 85.675, 85.66666666666667, 85.58333333333333, 85.64166666666667, 85.69166666666666, 85.65, 85.68333333333334, 85.65833333333333, 85.71666666666667, 85.75, 85.66666666666667, 85.775, 85.69166666666666, 85.63333333333334, 85.78333333333333, 85.725, 85.73333333333333, 85.73333333333333, 85.675, 85.70833333333333, 85.69166666666666, 85.69166666666666, 85.71666666666667, 85.675, 85.725, 85.7, 85.66666666666667, 85.65, 85.65, 85.66666666666667, 85.675, 85.675, 85.65833333333333, 85.64166666666667, 85.65, 85.71666666666667, 85.725, 85.7, 85.725, 85.69166666666666, 85.675, 85.625, 85.625, 85.68333333333334, 85.70833333333333, 85.70833333333333, 85.7, 85.675, 85.71666666666667, 85.65833333333333, 85.71666666666667, 85.7, 85.7, 85.66666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.703, Test loss: 2.291, Test accuracy: 32.17
Round   1, Train loss: 1.561, Test loss: 2.235, Test accuracy: 52.07
Round   2, Train loss: 1.378, Test loss: 2.102, Test accuracy: 62.74
Round   3, Train loss: 1.270, Test loss: 1.973, Test accuracy: 74.04
Round   4, Train loss: 1.225, Test loss: 1.922, Test accuracy: 77.13
Round   5, Train loss: 1.220, Test loss: 1.902, Test accuracy: 77.92
Round   6, Train loss: 1.216, Test loss: 1.886, Test accuracy: 80.02
Round   7, Train loss: 1.211, Test loss: 1.873, Test accuracy: 81.36
Round   8, Train loss: 1.197, Test loss: 1.864, Test accuracy: 81.38
Round   9, Train loss: 1.201, Test loss: 1.859, Test accuracy: 81.22
Round  10, Train loss: 1.201, Test loss: 1.856, Test accuracy: 81.12
Round  11, Train loss: 1.186, Test loss: 1.853, Test accuracy: 81.13
Round  12, Train loss: 1.187, Test loss: 1.850, Test accuracy: 80.82
Round  13, Train loss: 1.195, Test loss: 1.849, Test accuracy: 80.81
Round  14, Train loss: 1.189, Test loss: 1.848, Test accuracy: 80.73
Round  15, Train loss: 1.184, Test loss: 1.846, Test accuracy: 80.51
Round  16, Train loss: 1.183, Test loss: 1.845, Test accuracy: 80.42
Round  17, Train loss: 1.183, Test loss: 1.844, Test accuracy: 80.41
Round  18, Train loss: 1.190, Test loss: 1.844, Test accuracy: 80.26
Round  19, Train loss: 1.182, Test loss: 1.843, Test accuracy: 80.14
Round  20, Train loss: 1.187, Test loss: 1.842, Test accuracy: 80.05
Round  21, Train loss: 1.178, Test loss: 1.843, Test accuracy: 79.88
Round  22, Train loss: 1.177, Test loss: 1.842, Test accuracy: 79.66
Round  23, Train loss: 1.181, Test loss: 1.841, Test accuracy: 79.45
Round  24, Train loss: 1.183, Test loss: 1.841, Test accuracy: 79.42
Round  25, Train loss: 1.182, Test loss: 1.842, Test accuracy: 79.15
Round  26, Train loss: 1.187, Test loss: 1.843, Test accuracy: 78.93
Round  27, Train loss: 1.183, Test loss: 1.843, Test accuracy: 78.90
Round  28, Train loss: 1.180, Test loss: 1.843, Test accuracy: 78.74
Round  29, Train loss: 1.177, Test loss: 1.844, Test accuracy: 78.53
Round  30, Train loss: 1.185, Test loss: 1.846, Test accuracy: 77.98
Round  31, Train loss: 1.182, Test loss: 1.847, Test accuracy: 77.78
Round  32, Train loss: 1.183, Test loss: 1.848, Test accuracy: 77.53
Round  33, Train loss: 1.178, Test loss: 1.848, Test accuracy: 77.28
Round  34, Train loss: 1.180, Test loss: 1.849, Test accuracy: 77.14
Round  35, Train loss: 1.172, Test loss: 1.850, Test accuracy: 77.00
Round  36, Train loss: 1.175, Test loss: 1.851, Test accuracy: 76.71
Round  37, Train loss: 1.175, Test loss: 1.851, Test accuracy: 76.70
Round  38, Train loss: 1.182, Test loss: 1.852, Test accuracy: 76.62
Round  39, Train loss: 1.174, Test loss: 1.851, Test accuracy: 76.58
Round  40, Train loss: 1.179, Test loss: 1.852, Test accuracy: 76.46
Round  41, Train loss: 1.184, Test loss: 1.853, Test accuracy: 76.29
Round  42, Train loss: 1.180, Test loss: 1.854, Test accuracy: 76.04
Round  43, Train loss: 1.181, Test loss: 1.853, Test accuracy: 76.00
Round  44, Train loss: 1.177, Test loss: 1.854, Test accuracy: 75.81
Round  45, Train loss: 1.182, Test loss: 1.855, Test accuracy: 75.76
Round  46, Train loss: 1.178, Test loss: 1.855, Test accuracy: 75.47
Round  47, Train loss: 1.183, Test loss: 1.856, Test accuracy: 75.46
Round  48, Train loss: 1.183, Test loss: 1.856, Test accuracy: 75.29
Round  49, Train loss: 1.178, Test loss: 1.856, Test accuracy: 75.24
Round  50, Train loss: 1.173, Test loss: 1.856, Test accuracy: 74.90
Round  51, Train loss: 1.182, Test loss: 1.857, Test accuracy: 74.72
Round  52, Train loss: 1.177, Test loss: 1.859, Test accuracy: 74.59
Round  53, Train loss: 1.184, Test loss: 1.860, Test accuracy: 74.52
Round  54, Train loss: 1.175, Test loss: 1.860, Test accuracy: 74.46
Round  55, Train loss: 1.178, Test loss: 1.860, Test accuracy: 74.36
Round  56, Train loss: 1.176, Test loss: 1.860, Test accuracy: 74.39
Round  57, Train loss: 1.177, Test loss: 1.861, Test accuracy: 74.17
Round  58, Train loss: 1.177, Test loss: 1.862, Test accuracy: 74.05
Round  59, Train loss: 1.181, Test loss: 1.862, Test accuracy: 73.92
Round  60, Train loss: 1.172, Test loss: 1.862, Test accuracy: 73.86
Round  61, Train loss: 1.167, Test loss: 1.864, Test accuracy: 73.77
Round  62, Train loss: 1.180, Test loss: 1.864, Test accuracy: 73.58
Round  63, Train loss: 1.182, Test loss: 1.865, Test accuracy: 73.50
Round  64, Train loss: 1.173, Test loss: 1.865, Test accuracy: 73.36
Round  65, Train loss: 1.174, Test loss: 1.865, Test accuracy: 73.38
Round  66, Train loss: 1.171, Test loss: 1.865, Test accuracy: 73.42
Round  67, Train loss: 1.180, Test loss: 1.867, Test accuracy: 73.07
Round  68, Train loss: 1.176, Test loss: 1.867, Test accuracy: 73.11
Round  69, Train loss: 1.180, Test loss: 1.867, Test accuracy: 73.12
Round  70, Train loss: 1.178, Test loss: 1.867, Test accuracy: 72.98
Round  71, Train loss: 1.174, Test loss: 1.867, Test accuracy: 72.85
Round  72, Train loss: 1.173, Test loss: 1.868, Test accuracy: 72.71
Round  73, Train loss: 1.179, Test loss: 1.869, Test accuracy: 72.50
Round  74, Train loss: 1.173, Test loss: 1.869, Test accuracy: 72.52
Round  75, Train loss: 1.176, Test loss: 1.869, Test accuracy: 72.28
Round  76, Train loss: 1.171, Test loss: 1.870, Test accuracy: 72.21
Round  77, Train loss: 1.174, Test loss: 1.870, Test accuracy: 72.03
Round  78, Train loss: 1.175, Test loss: 1.871, Test accuracy: 71.77
Round  79, Train loss: 1.174, Test loss: 1.872, Test accuracy: 71.72
Round  80, Train loss: 1.176, Test loss: 1.872, Test accuracy: 71.79
Round  81, Train loss: 1.178, Test loss: 1.873, Test accuracy: 71.72
Round  82, Train loss: 1.175, Test loss: 1.874, Test accuracy: 71.57
Round  83, Train loss: 1.174, Test loss: 1.874, Test accuracy: 71.60
Round  84, Train loss: 1.173, Test loss: 1.875, Test accuracy: 71.60
Round  85, Train loss: 1.177, Test loss: 1.875, Test accuracy: 71.53
Round  86, Train loss: 1.172, Test loss: 1.875, Test accuracy: 71.43
Round  87, Train loss: 1.175, Test loss: 1.876, Test accuracy: 71.37
Round  88, Train loss: 1.171, Test loss: 1.876, Test accuracy: 71.19
Round  89, Train loss: 1.174, Test loss: 1.876, Test accuracy: 71.34
Round  90, Train loss: 1.177, Test loss: 1.879, Test accuracy: 71.28
Round  91, Train loss: 1.164, Test loss: 1.879, Test accuracy: 71.43
Round  92, Train loss: 1.163, Test loss: 1.875, Test accuracy: 72.47
Round  93, Train loss: 1.134, Test loss: 1.872, Test accuracy: 73.65
Round  94, Train loss: 1.129, Test loss: 1.869, Test accuracy: 74.22
Round  95, Train loss: 1.121, Test loss: 1.868, Test accuracy: 74.47
Round  96, Train loss: 1.125, Test loss: 1.864, Test accuracy: 75.25
Round  97, Train loss: 1.114, Test loss: 1.862, Test accuracy: 75.58
Round  98, Train loss: 1.110, Test loss: 1.862, Test accuracy: 75.62
Round  99, Train loss: 1.112, Test loss: 1.860, Test accuracy: 75.80
Final Round, Train loss: 1.115, Test loss: 1.860, Test accuracy: 76.12
Average accuracy final 10 rounds: 73.97749999999999
1302.0514051914215
[]
[32.175, 52.06666666666667, 62.74166666666667, 74.04166666666667, 77.13333333333334, 77.91666666666667, 80.01666666666667, 81.35833333333333, 81.38333333333334, 81.21666666666667, 81.11666666666666, 81.13333333333334, 80.81666666666666, 80.80833333333334, 80.73333333333333, 80.50833333333334, 80.41666666666667, 80.40833333333333, 80.25833333333334, 80.14166666666667, 80.05, 79.875, 79.65833333333333, 79.45, 79.41666666666667, 79.15, 78.93333333333334, 78.9, 78.74166666666666, 78.525, 77.98333333333333, 77.78333333333333, 77.53333333333333, 77.28333333333333, 77.14166666666667, 77.0, 76.70833333333333, 76.7, 76.61666666666666, 76.575, 76.45833333333333, 76.29166666666667, 76.04166666666667, 76.0, 75.80833333333334, 75.75833333333334, 75.475, 75.45833333333333, 75.29166666666667, 75.24166666666666, 74.9, 74.71666666666667, 74.59166666666667, 74.51666666666667, 74.45833333333333, 74.35833333333333, 74.39166666666667, 74.16666666666667, 74.05, 73.91666666666667, 73.85833333333333, 73.76666666666667, 73.58333333333333, 73.5, 73.35833333333333, 73.38333333333334, 73.41666666666667, 73.06666666666666, 73.10833333333333, 73.11666666666666, 72.98333333333333, 72.85, 72.70833333333333, 72.5, 72.51666666666667, 72.275, 72.20833333333333, 72.025, 71.76666666666667, 71.71666666666667, 71.79166666666667, 71.71666666666667, 71.56666666666666, 71.6, 71.6, 71.53333333333333, 71.43333333333334, 71.36666666666666, 71.19166666666666, 71.34166666666667, 71.275, 71.43333333333334, 72.475, 73.65, 74.225, 74.46666666666667, 75.25, 75.575, 75.625, 75.8, 76.11666666666666]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 12.33
Round   0: Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 8.18
Round   1, Train loss: 2.305, Test loss: 2.300, Test accuracy: 18.49
Round   1: Global train loss: 2.305, Global test loss: 2.302, Global test accuracy: 10.59
Round   2, Train loss: 2.288, Test loss: 2.300, Test accuracy: 18.23
Round   2: Global train loss: 2.288, Global test loss: 2.302, Global test accuracy: 11.46
Round   3, Train loss: 2.302, Test loss: 2.298, Test accuracy: 20.82
Round   3: Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.38
Round   4, Train loss: 2.301, Test loss: 2.298, Test accuracy: 17.38
Round   4: Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.06
Round   5, Train loss: 2.292, Test loss: 2.297, Test accuracy: 17.88
Round   5: Global train loss: 2.292, Global test loss: 2.301, Global test accuracy: 16.43
Round   6, Train loss: 2.272, Test loss: 2.290, Test accuracy: 22.26
Round   6: Global train loss: 2.272, Global test loss: 2.300, Global test accuracy: 20.61
Round   7, Train loss: 2.269, Test loss: 2.288, Test accuracy: 22.28
Round   7: Global train loss: 2.269, Global test loss: 2.300, Global test accuracy: 22.28
Round   8, Train loss: 2.282, Test loss: 2.290, Test accuracy: 19.68
Round   8: Global train loss: 2.282, Global test loss: 2.300, Global test accuracy: 23.77
Round   9, Train loss: 2.251, Test loss: 2.283, Test accuracy: 21.18
Round   9: Global train loss: 2.251, Global test loss: 2.299, Global test accuracy: 27.16
Round  10, Train loss: 2.264, Test loss: 2.278, Test accuracy: 21.79
Round  10: Global train loss: 2.264, Global test loss: 2.299, Global test accuracy: 30.32
Round  11, Train loss: 2.295, Test loss: 2.280, Test accuracy: 21.76
Round  11: Global train loss: 2.295, Global test loss: 2.299, Global test accuracy: 30.60
Round  12, Train loss: 2.193, Test loss: 2.261, Test accuracy: 24.03
Round  12: Global train loss: 2.193, Global test loss: 2.298, Global test accuracy: 33.77
Round  13, Train loss: 2.271, Test loss: 2.278, Test accuracy: 19.82
Round  13: Global train loss: 2.271, Global test loss: 2.299, Global test accuracy: 32.02
Round  14, Train loss: 2.263, Test loss: 2.277, Test accuracy: 18.14
Round  14: Global train loss: 2.263, Global test loss: 2.299, Global test accuracy: 29.07
Round  15, Train loss: 2.284, Test loss: 2.285, Test accuracy: 17.04
Round  15: Global train loss: 2.284, Global test loss: 2.300, Global test accuracy: 23.02
Round  16, Train loss: 2.226, Test loss: 2.287, Test accuracy: 16.78
Round  16: Global train loss: 2.226, Global test loss: 2.301, Global test accuracy: 20.32
Round  17, Train loss: 2.227, Test loss: 2.276, Test accuracy: 16.37
Round  17: Global train loss: 2.227, Global test loss: 2.301, Global test accuracy: 20.68
Round  18, Train loss: 2.168, Test loss: 2.280, Test accuracy: 14.11
Round  18: Global train loss: 2.168, Global test loss: 2.302, Global test accuracy: 13.91
Round  19, Train loss: 2.137, Test loss: 2.258, Test accuracy: 16.70
Round  19: Global train loss: 2.137, Global test loss: 2.301, Global test accuracy: 17.83
Round  20, Train loss: 1.919, Test loss: 2.247, Test accuracy: 18.55
Round  20: Global train loss: 1.919, Global test loss: 2.301, Global test accuracy: 21.68
Round  21, Train loss: 1.806, Test loss: 2.204, Test accuracy: 24.86
Round  21: Global train loss: 1.806, Global test loss: 2.299, Global test accuracy: 30.50
Round  22, Train loss: 2.148, Test loss: 2.200, Test accuracy: 25.98
Round  22: Global train loss: 2.148, Global test loss: 2.299, Global test accuracy: 33.29
Round  23, Train loss: 1.706, Test loss: 2.161, Test accuracy: 32.19
Round  23: Global train loss: 1.706, Global test loss: 2.297, Global test accuracy: 40.07
Round  24, Train loss: 1.537, Test loss: 2.140, Test accuracy: 34.28
Round  24: Global train loss: 1.537, Global test loss: 2.296, Global test accuracy: 46.82
Round  25, Train loss: 1.526, Test loss: 2.119, Test accuracy: 36.64
Round  25: Global train loss: 1.526, Global test loss: 2.294, Global test accuracy: 50.43
Round  26, Train loss: 0.817, Test loss: 2.054, Test accuracy: 43.57
Round  26: Global train loss: 0.817, Global test loss: 2.287, Global test accuracy: 54.35
Round  27, Train loss: 1.507, Test loss: 2.083, Test accuracy: 39.77
Round  27: Global train loss: 1.507, Global test loss: 2.286, Global test accuracy: 54.67
Round  28, Train loss: 1.579, Test loss: 2.180, Test accuracy: 28.88
Round  28: Global train loss: 1.579, Global test loss: 2.293, Global test accuracy: 48.52
Round  29, Train loss: 0.883, Test loss: 2.119, Test accuracy: 36.07
Round  29: Global train loss: 0.883, Global test loss: 2.291, Global test accuracy: 50.47
Round  30, Train loss: 0.860, Test loss: 2.082, Test accuracy: 38.11
Round  30: Global train loss: 0.860, Global test loss: 2.286, Global test accuracy: 55.30
Round  31, Train loss: 1.473, Test loss: 2.117, Test accuracy: 34.02
Round  31: Global train loss: 1.473, Global test loss: 2.290, Global test accuracy: 53.53
Round  32, Train loss: 1.725, Test loss: 2.224, Test accuracy: 22.11
Round  32: Global train loss: 1.725, Global test loss: 2.298, Global test accuracy: 30.76
Round  33, Train loss: 1.103, Test loss: 2.104, Test accuracy: 37.78
Round  33: Global train loss: 1.103, Global test loss: 2.292, Global test accuracy: 49.97
Round  34, Train loss: 0.866, Test loss: 2.110, Test accuracy: 37.60
Round  34: Global train loss: 0.866, Global test loss: 2.292, Global test accuracy: 53.71
Round  35, Train loss: 1.290, Test loss: 2.107, Test accuracy: 37.27
Round  35: Global train loss: 1.290, Global test loss: 2.292, Global test accuracy: 52.42
Round  36, Train loss: 1.336, Test loss: 2.098, Test accuracy: 37.88
Round  36: Global train loss: 1.336, Global test loss: 2.291, Global test accuracy: 54.91
Round  37, Train loss: 0.756, Test loss: 2.059, Test accuracy: 40.47
Round  37: Global train loss: 0.756, Global test loss: 2.289, Global test accuracy: 57.16
Round  38, Train loss: 0.589, Test loss: 2.080, Test accuracy: 38.12
Round  38: Global train loss: 0.589, Global test loss: 2.289, Global test accuracy: 58.37
Round  39, Train loss: -0.705, Test loss: 1.981, Test accuracy: 48.29
Round  39: Global train loss: -0.705, Global test loss: 2.272, Global test accuracy: 66.29
Round  40, Train loss: 1.091, Test loss: 2.034, Test accuracy: 43.07
Round  40: Global train loss: 1.091, Global test loss: 2.277, Global test accuracy: 66.27
Round  41, Train loss: 0.534, Test loss: 2.036, Test accuracy: 44.46
Round  41: Global train loss: 0.534, Global test loss: 2.274, Global test accuracy: 66.74
Round  42, Train loss: 1.604, Test loss: 2.124, Test accuracy: 36.54
Round  42: Global train loss: 1.604, Global test loss: 2.285, Global test accuracy: 56.93
Round  43, Train loss: -0.240, Test loss: 2.052, Test accuracy: 43.10
Round  43: Global train loss: -0.240, Global test loss: 2.274, Global test accuracy: 65.47
Round  44, Train loss: 0.264, Test loss: 2.056, Test accuracy: 40.85
Round  44: Global train loss: 0.264, Global test loss: 2.271, Global test accuracy: 63.00
Round  45, Train loss: -0.097, Test loss: 2.045, Test accuracy: 43.02
Round  45: Global train loss: -0.097, Global test loss: 2.266, Global test accuracy: 63.57
Round  46, Train loss: 0.718, Test loss: 2.063, Test accuracy: 39.54
Round  46: Global train loss: 0.718, Global test loss: 2.272, Global test accuracy: 62.87
Round  47, Train loss: 0.064, Test loss: 2.054, Test accuracy: 40.29
Round  47: Global train loss: 0.064, Global test loss: 2.272, Global test accuracy: 63.77
Round  48, Train loss: 1.234, Test loss: 2.112, Test accuracy: 34.31
Round  48: Global train loss: 1.234, Global test loss: 2.285, Global test accuracy: 59.09
Round  49, Train loss: 0.057, Test loss: 2.078, Test accuracy: 38.14
Round  49: Global train loss: 0.057, Global test loss: 2.285, Global test accuracy: 58.68
Round  50, Train loss: -0.247, Test loss: 2.009, Test accuracy: 45.27
Round  50: Global train loss: -0.247, Global test loss: 2.276, Global test accuracy: 60.04
Round  51, Train loss: -1.780, Test loss: 1.944, Test accuracy: 51.99
Round  51: Global train loss: -1.780, Global test loss: 2.250, Global test accuracy: 61.94
Round  52, Train loss: -2.026, Test loss: 1.932, Test accuracy: 54.21
Round  52: Global train loss: -2.026, Global test loss: 2.209, Global test accuracy: 64.28
Round  53, Train loss: 0.546, Test loss: 1.989, Test accuracy: 49.15
Round  53: Global train loss: 0.546, Global test loss: 2.213, Global test accuracy: 62.24
Round  54, Train loss: -0.675, Test loss: 1.985, Test accuracy: 50.77
Round  54: Global train loss: -0.675, Global test loss: 2.206, Global test accuracy: 63.61
Round  55, Train loss: 0.263, Test loss: 2.018, Test accuracy: 47.20
Round  55: Global train loss: 0.263, Global test loss: 2.208, Global test accuracy: 61.10
Round  56, Train loss: 0.489, Test loss: 2.021, Test accuracy: 45.43
Round  56: Global train loss: 0.489, Global test loss: 2.211, Global test accuracy: 59.38
Round  57, Train loss: 0.454, Test loss: 2.063, Test accuracy: 40.36
Round  57: Global train loss: 0.454, Global test loss: 2.234, Global test accuracy: 57.58
Round  58, Train loss: -0.534, Test loss: 2.044, Test accuracy: 42.61
Round  58: Global train loss: -0.534, Global test loss: 2.237, Global test accuracy: 58.32
Round  59, Train loss: 0.027, Test loss: 2.014, Test accuracy: 45.42
Round  59: Global train loss: 0.027, Global test loss: 2.236, Global test accuracy: 57.52
Round  60, Train loss: -1.098, Test loss: 2.012, Test accuracy: 44.56
Round  60: Global train loss: -1.098, Global test loss: 2.234, Global test accuracy: 56.49
Round  61, Train loss: -0.489, Test loss: 1.993, Test accuracy: 47.98
Round  61: Global train loss: -0.489, Global test loss: 2.236, Global test accuracy: 55.11
Round  62, Train loss: -0.838, Test loss: 1.946, Test accuracy: 52.85
Round  62: Global train loss: -0.838, Global test loss: 2.217, Global test accuracy: 55.94
Round  63, Train loss: -3.267, Test loss: 1.880, Test accuracy: 59.80
Round  63: Global train loss: -3.267, Global test loss: 2.165, Global test accuracy: 60.35
Round  64, Train loss: -1.952, Test loss: 1.911, Test accuracy: 56.08
Round  64: Global train loss: -1.952, Global test loss: 2.154, Global test accuracy: 60.05
Round  65, Train loss: -2.432, Test loss: 1.868, Test accuracy: 60.19
Round  65: Global train loss: -2.432, Global test loss: 2.116, Global test accuracy: 61.12
Round  66, Train loss: 0.056, Test loss: 1.907, Test accuracy: 56.27
Round  66: Global train loss: 0.056, Global test loss: 2.136, Global test accuracy: 59.86
Round  67, Train loss: -2.553, Test loss: 1.838, Test accuracy: 63.41
Round  67: Global train loss: -2.553, Global test loss: 2.071, Global test accuracy: 63.68
Round  68, Train loss: -0.347, Test loss: 1.854, Test accuracy: 62.13
Round  68: Global train loss: -0.347, Global test loss: 2.045, Global test accuracy: 64.08
Round  69, Train loss: -0.427, Test loss: 1.858, Test accuracy: 62.38
Round  69: Global train loss: -0.427, Global test loss: 2.052, Global test accuracy: 62.62
Round  70, Train loss: -0.350, Test loss: 1.881, Test accuracy: 61.54
Round  70: Global train loss: -0.350, Global test loss: 2.071, Global test accuracy: 55.88
Round  71, Train loss: -1.063, Test loss: 1.844, Test accuracy: 64.84
Round  71: Global train loss: -1.063, Global test loss: 2.045, Global test accuracy: 58.44
Round  72, Train loss: -1.961, Test loss: 1.794, Test accuracy: 68.74
Round  72: Global train loss: -1.961, Global test loss: 2.006, Global test accuracy: 62.12
Round  73, Train loss: -0.787, Test loss: 1.809, Test accuracy: 67.14
Round  73: Global train loss: -0.787, Global test loss: 1.998, Global test accuracy: 61.66
Round  74, Train loss: -0.056, Test loss: 1.792, Test accuracy: 68.28
Round  74: Global train loss: -0.056, Global test loss: 1.988, Global test accuracy: 62.52
Round  75, Train loss: -1.261, Test loss: 1.759, Test accuracy: 72.10
Round  75: Global train loss: -1.261, Global test loss: 1.957, Global test accuracy: 63.92
Round  76, Train loss: -0.647, Test loss: 1.770, Test accuracy: 70.83
Round  76: Global train loss: -0.647, Global test loss: 1.950, Global test accuracy: 62.55
Round  77, Train loss: -0.189, Test loss: 1.772, Test accuracy: 70.42
Round  77: Global train loss: -0.189, Global test loss: 1.948, Global test accuracy: 62.94
Round  78, Train loss: -1.924, Test loss: 1.724, Test accuracy: 74.52
Round  78: Global train loss: -1.924, Global test loss: 1.922, Global test accuracy: 66.05
Round  79, Train loss: 0.069, Test loss: 1.736, Test accuracy: 73.54
Round  79: Global train loss: 0.069, Global test loss: 1.923, Global test accuracy: 66.28
Round  80, Train loss: -0.342, Test loss: 1.725, Test accuracy: 74.87
Round  80: Global train loss: -0.342, Global test loss: 1.912, Global test accuracy: 67.64
Round  81, Train loss: -1.893, Test loss: 1.703, Test accuracy: 76.58
Round  81: Global train loss: -1.893, Global test loss: 1.886, Global test accuracy: 69.17
Round  82, Train loss: -0.098, Test loss: 1.702, Test accuracy: 76.45
Round  82: Global train loss: -0.098, Global test loss: 1.883, Global test accuracy: 69.33
Round  83, Train loss: -0.707, Test loss: 1.692, Test accuracy: 77.47
Round  83: Global train loss: -0.707, Global test loss: 1.868, Global test accuracy: 70.53
Round  84, Train loss: -0.432, Test loss: 1.696, Test accuracy: 77.29
Round  84: Global train loss: -0.432, Global test loss: 1.863, Global test accuracy: 71.86
Round  85, Train loss: -2.189, Test loss: 1.682, Test accuracy: 78.44
Round  85: Global train loss: -2.189, Global test loss: 1.842, Global test accuracy: 72.69
Round  86, Train loss: -0.524, Test loss: 1.685, Test accuracy: 78.38
Round  86: Global train loss: -0.524, Global test loss: 1.849, Global test accuracy: 72.94
Round  87, Train loss: -0.887, Test loss: 1.684, Test accuracy: 78.28
Round  87: Global train loss: -0.887, Global test loss: 1.841, Global test accuracy: 73.70
Round  88, Train loss: -0.796, Test loss: 1.684, Test accuracy: 78.36
Round  88: Global train loss: -0.796, Global test loss: 1.827, Global test accuracy: 75.24
Round  89, Train loss: -1.006, Test loss: 1.688, Test accuracy: 77.87
Round  89: Global train loss: -1.006, Global test loss: 1.817, Global test accuracy: 75.95
Round  90, Train loss: 0.071, Test loss: 1.685, Test accuracy: 78.16
Round  90: Global train loss: 0.071, Global test loss: 1.811, Global test accuracy: 75.94
Round  91, Train loss: -1.596, Test loss: 1.676, Test accuracy: 78.85
Round  91: Global train loss: -1.596, Global test loss: 1.797, Global test accuracy: 76.27
Round  92, Train loss: -0.512, Test loss: 1.672, Test accuracy: 79.18
Round  92: Global train loss: -0.512, Global test loss: 1.787, Global test accuracy: 76.68
Round  93, Train loss: -1.120, Test loss: 1.670, Test accuracy: 79.34
Round  93: Global train loss: -1.120, Global test loss: 1.782, Global test accuracy: 77.02
Round  94, Train loss: -1.292, Test loss: 1.671, Test accuracy: 79.25
Round  94: Global train loss: -1.292, Global test loss: 1.771, Global test accuracy: 77.45
Round  95, Train loss: -0.582, Test loss: 1.668, Test accuracy: 79.59
Round  95: Global train loss: -0.582, Global test loss: 1.768, Global test accuracy: 77.67
Round  96, Train loss: -0.877, Test loss: 1.666, Test accuracy: 79.71
Round  96: Global train loss: -0.877, Global test loss: 1.761, Global test accuracy: 77.97
Round  97, Train loss: -0.824, Test loss: 1.668, Test accuracy: 79.58
Round  97: Global train loss: -0.824, Global test loss: 1.756, Global test accuracy: 78.33
Round  98, Train loss: -0.679, Test loss: 1.668, Test accuracy: 79.61
Round  98: Global train loss: -0.679, Global test loss: 1.751, Global test accuracy: 78.63
Round  99, Train loss: -0.451, Test loss: 1.666, Test accuracy: 79.92/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99: Global train loss: -0.451, Global test loss: 1.746, Global test accuracy: 78.99
Final Round: Train loss: 1.662, Test loss: 1.667, Test accuracy: 81.38
Final Round: Global train loss: 1.662, Global test loss: 1.731, Global test accuracy: 79.78
Average accuracy final 10 rounds: 79.31916666666666
Average global accuracy final 10 rounds: 77.49666666666667
1226.409749031067
[]
[12.333333333333334, 18.491666666666667, 18.233333333333334, 20.816666666666666, 17.375, 17.883333333333333, 22.258333333333333, 22.283333333333335, 19.675, 21.175, 21.791666666666668, 21.758333333333333, 24.033333333333335, 19.825, 18.141666666666666, 17.041666666666668, 16.783333333333335, 16.366666666666667, 14.108333333333333, 16.7, 18.55, 24.858333333333334, 25.975, 32.19166666666667, 34.28333333333333, 36.641666666666666, 43.56666666666667, 39.766666666666666, 28.875, 36.06666666666667, 38.108333333333334, 34.016666666666666, 22.108333333333334, 37.78333333333333, 37.6, 37.275, 37.875, 40.46666666666667, 38.11666666666667, 48.291666666666664, 43.06666666666667, 44.458333333333336, 36.541666666666664, 43.1, 40.85, 43.016666666666666, 39.541666666666664, 40.291666666666664, 34.30833333333333, 38.141666666666666, 45.266666666666666, 51.99166666666667, 54.208333333333336, 49.15, 50.766666666666666, 47.2, 45.43333333333333, 40.358333333333334, 42.608333333333334, 45.416666666666664, 44.55833333333333, 47.975, 52.85, 59.8, 56.075, 60.19166666666667, 56.275, 63.40833333333333, 62.13333333333333, 62.375, 61.541666666666664, 64.84166666666667, 68.74166666666666, 67.14166666666667, 68.28333333333333, 72.1, 70.83333333333333, 70.41666666666667, 74.51666666666667, 73.54166666666667, 74.86666666666666, 76.58333333333333, 76.45, 77.475, 77.29166666666667, 78.44166666666666, 78.38333333333334, 78.275, 78.35833333333333, 77.86666666666666, 78.15833333333333, 78.85, 79.18333333333334, 79.34166666666667, 79.25, 79.59166666666667, 79.70833333333333, 79.58333333333333, 79.60833333333333, 79.91666666666667, 81.38333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.88 

Round   0, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.89 

Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.92 

Round   1, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.84 

Round   2, Train loss: 2.303, Test loss: 2.302, Test accuracy: 11.92 

Round   2, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 11.84 

Round   3, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.95 

Round   3, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.92 

Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.99 

Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.03 

Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.06 

Round   5, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.16 

Round   6, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.09 

Round   6, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.21 

Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.15 

Round   7, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.27 

Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.16 

Round   8, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.27 

Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.18 

Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.33 

Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.24 

Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.44 

Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.34 

Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.53 

Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.46 

Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.64 

Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.54 

Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.72 

Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.61 

Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.78 

Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.74 

Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89 

Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.77 

Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.93 

Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88 

Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.93 

Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.91 

Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.03 

Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.96 

Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.05 

Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.99 

Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.12 

Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.09 

Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.33 

Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.15 

Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.35 

Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.21 

Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.43 

Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.44 

Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.63 

Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.51 

Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.66 

Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.58 

Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.66 

Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.62 

Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.72 

Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.62 

Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.66 

Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.62 

Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.62 

Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.62 

Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.53 

Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.65 

Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.63 

Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.67 

Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.65 

Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.69 

Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.84 

Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.73 

Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.77 

Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.77 

Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.86 

Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.79 

Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.88 

Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.81 

Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.89 

Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.82 

Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.91 

Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.85 

Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.93 

Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.87 

Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.96 

Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.89 

Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.97 

Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.86 

Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.96 

Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.95 

Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.07 

Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.98 

Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.07 

Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.98 

Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.08 

Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.99 

Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.07 

Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.09 

Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.10 

Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.07 

Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.17 

Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.12 

Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.28 

Round  50, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.14 

Round  50, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.25 

Round  51, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.18 

Round  51, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.26 

Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.14 

Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.14 

Round  53, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.16 

Round  53, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.16 

Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.18 

Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.29 

Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.18 

Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.32 

Round  56, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.23 

Round  56, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.43 

Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.28 

Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.50 

Round  58, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.38 

Round  58, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.51 

Round  59, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.37 

Round  59, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.49 

Round  60, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.40 

Round  60, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.50 

Round  61, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.40 

Round  61, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.52 

Round  62, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.50 

Round  62, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.57 

Round  63, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.54 

Round  63, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.61 

Round  64, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.61 

Round  64, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.66 

Round  65, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.67 

Round  65, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.73 

Round  66, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.72 

Round  66, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.79 

Round  67, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.75 

Round  67, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.87 

Round  68, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.72 

Round  68, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.90 

Round  69, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.78 

Round  69, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.89 

Round  70, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.84 

Round  70, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.97 

Round  71, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.96 

Round  71, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.07 

Round  72, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.02 

Round  72, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.12 

Round  73, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.00 

Round  73, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.12 

Round  74, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.97 

Round  74, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.08 

Round  75, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.95 

Round  75, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.12 

Round  76, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.01 

Round  76, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.12 

Round  77, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.08 

Round  77, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.18 

Round  78, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.09 

Round  78, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.14 

Round  79, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.12 

Round  79, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.18 

Round  80, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.13 

Round  80, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.16 

Round  81, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.21 

Round  81, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.18 

Round  82, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.14 

Round  82, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.16 

Round  83, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.21 

Round  83, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.41 

Round  84, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.32 

Round  84, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.49 

Round  85, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.40 

Round  85, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.62 

Round  86, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.41 

Round  86, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.62 

Round  87, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.49 

Round  87, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.66 

Round  88, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.52 

Round  88, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.67 

Round  89, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.55 

Round  89, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.70 

Round  90, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.56 

Round  90, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.68 

Round  91, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.62 

Round  91, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.68 

Round  92, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.62 

Round  92, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.78 

Round  93, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.65 

Round  93, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.80 

Round  94, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.71 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.82 

Round  95, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.72 

Round  95, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.82 

Round  96, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.72 

Round  96, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.82 

Round  97, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.76 

Round  97, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.82 

Round  98, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.77 

Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.85 

Round  99, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.85 

Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.88 

Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.91 

Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.88 

Average accuracy final 10 rounds: 15.696666666666665 

Average global accuracy final 10 rounds: 15.796666666666665 

1069.9331767559052
[0.9948136806488037, 1.9044773578643799, 2.8056466579437256, 3.709818124771118, 4.617498874664307, 5.51695990562439, 6.425658464431763, 7.3310866355896, 8.232603311538696, 9.130309820175171, 10.02309775352478, 10.917522192001343, 11.819297552108765, 12.713798522949219, 13.609523057937622, 14.511911153793335, 15.405327796936035, 16.30492925643921, 17.207752227783203, 18.102900505065918, 18.994044065475464, 19.87519884109497, 20.770816326141357, 21.670939922332764, 22.56002140045166, 23.4436297416687, 24.33533787727356, 25.222499132156372, 26.116400718688965, 27.00336480140686, 27.903281688690186, 28.801918268203735, 29.699143409729004, 30.59444570541382, 31.49195957183838, 32.38708972930908, 33.28420662879944, 34.182488679885864, 35.08455944061279, 35.98224759101868, 36.877622842788696, 37.77047514915466, 38.668336391448975, 39.56378626823425, 40.4574134349823, 41.351677656173706, 42.249478816986084, 43.14258027076721, 44.03402376174927, 44.93423914909363, 45.83326768875122, 46.72789549827576, 47.61736345291138, 48.511921882629395, 49.40498900413513, 50.29779839515686, 51.179293394088745, 52.07563924789429, 52.97710871696472, 53.85505127906799, 54.74189043045044, 55.6303985118866, 56.5206139087677, 57.403796911239624, 58.26931977272034, 59.151588678359985, 60.027730226516724, 60.89597034454346, 61.77942943572998, 62.66111779212952, 63.55032229423523, 64.4267828464508, 65.29331350326538, 66.17112445831299, 67.06339764595032, 67.93639802932739, 68.81863355636597, 69.69332885742188, 70.58620476722717, 71.47169256210327, 72.35263085365295, 73.24271893501282, 74.12374138832092, 75.02087759971619, 75.90391206741333, 76.7930679321289, 77.68400549888611, 78.57566666603088, 79.46834015846252, 80.36047530174255, 81.25004434585571, 82.13960456848145, 83.02544474601746, 83.90783476829529, 84.79281759262085, 85.66969680786133, 86.55340313911438, 87.43167781829834, 88.31696844100952, 89.20176100730896, 90.97389841079712]
[11.883333333333333, 11.916666666666666, 11.916666666666666, 11.95, 11.991666666666667, 12.058333333333334, 12.091666666666667, 12.15, 12.158333333333333, 12.175, 12.241666666666667, 12.341666666666667, 12.458333333333334, 12.541666666666666, 12.608333333333333, 12.741666666666667, 12.766666666666667, 12.875, 12.908333333333333, 12.958333333333334, 12.991666666666667, 13.091666666666667, 13.15, 13.208333333333334, 13.441666666666666, 13.508333333333333, 13.583333333333334, 13.625, 13.625, 13.625, 13.616666666666667, 13.65, 13.666666666666666, 13.691666666666666, 13.733333333333333, 13.766666666666667, 13.791666666666666, 13.808333333333334, 13.816666666666666, 13.85, 13.866666666666667, 13.891666666666667, 13.858333333333333, 13.95, 13.983333333333333, 13.983333333333333, 13.991666666666667, 14.091666666666667, 14.066666666666666, 14.116666666666667, 14.141666666666667, 14.175, 14.141666666666667, 14.158333333333333, 14.175, 14.183333333333334, 14.233333333333333, 14.283333333333333, 14.375, 14.366666666666667, 14.4, 14.4, 14.5, 14.541666666666666, 14.608333333333333, 14.666666666666666, 14.725, 14.75, 14.716666666666667, 14.783333333333333, 14.841666666666667, 14.958333333333334, 15.016666666666667, 15.0, 14.966666666666667, 14.95, 15.008333333333333, 15.083333333333334, 15.091666666666667, 15.116666666666667, 15.133333333333333, 15.208333333333334, 15.141666666666667, 15.208333333333334, 15.316666666666666, 15.4, 15.408333333333333, 15.491666666666667, 15.516666666666667, 15.55, 15.558333333333334, 15.625, 15.616666666666667, 15.65, 15.708333333333334, 15.716666666666667, 15.716666666666667, 15.758333333333333, 15.766666666666667, 15.85, 15.908333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.299, Test accuracy: 11.69
Round   1, Train loss: 2.298, Test loss: 2.294, Test accuracy: 26.81
Round   2, Train loss: 2.293, Test loss: 2.280, Test accuracy: 25.44
Round   3, Train loss: 2.282, Test loss: 2.203, Test accuracy: 29.62
Round   4, Train loss: 2.213, Test loss: 2.037, Test accuracy: 46.44
Round   5, Train loss: 2.031, Test loss: 1.866, Test accuracy: 64.92
Round   6, Train loss: 1.897, Test loss: 1.818, Test accuracy: 66.38
Round   7, Train loss: 1.865, Test loss: 1.762, Test accuracy: 73.38
Round   8, Train loss: 1.784, Test loss: 1.684, Test accuracy: 81.71
Round   9, Train loss: 1.734, Test loss: 1.658, Test accuracy: 82.61
Round  10, Train loss: 1.680, Test loss: 1.649, Test accuracy: 83.10
Round  11, Train loss: 1.684, Test loss: 1.638, Test accuracy: 83.75
Round  12, Train loss: 1.711, Test loss: 1.613, Test accuracy: 86.87
Round  13, Train loss: 1.612, Test loss: 1.594, Test accuracy: 88.33
Round  14, Train loss: 1.593, Test loss: 1.583, Test accuracy: 89.22
Round  15, Train loss: 1.610, Test loss: 1.574, Test accuracy: 89.94
Round  16, Train loss: 1.578, Test loss: 1.569, Test accuracy: 90.13
Round  17, Train loss: 1.565, Test loss: 1.565, Test accuracy: 90.56
Round  18, Train loss: 1.549, Test loss: 1.562, Test accuracy: 90.81
Round  19, Train loss: 1.540, Test loss: 1.560, Test accuracy: 90.97
Round  20, Train loss: 1.546, Test loss: 1.557, Test accuracy: 91.28
Round  21, Train loss: 1.549, Test loss: 1.554, Test accuracy: 91.44
Round  22, Train loss: 1.540, Test loss: 1.554, Test accuracy: 91.45
Round  23, Train loss: 1.531, Test loss: 1.551, Test accuracy: 91.41
Round  24, Train loss: 1.529, Test loss: 1.549, Test accuracy: 91.77
Round  25, Train loss: 1.528, Test loss: 1.548, Test accuracy: 91.77
Round  26, Train loss: 1.525, Test loss: 1.549, Test accuracy: 91.59
Round  27, Train loss: 1.520, Test loss: 1.548, Test accuracy: 91.65
Round  28, Train loss: 1.520, Test loss: 1.546, Test accuracy: 91.91
Round  29, Train loss: 1.527, Test loss: 1.545, Test accuracy: 92.20
Round  30, Train loss: 1.519, Test loss: 1.545, Test accuracy: 92.03
Round  31, Train loss: 1.513, Test loss: 1.542, Test accuracy: 92.35
Round  32, Train loss: 1.518, Test loss: 1.543, Test accuracy: 92.21
Round  33, Train loss: 1.516, Test loss: 1.542, Test accuracy: 92.42
Round  34, Train loss: 1.513, Test loss: 1.542, Test accuracy: 92.30
Round  35, Train loss: 1.510, Test loss: 1.541, Test accuracy: 92.42
Round  36, Train loss: 1.507, Test loss: 1.540, Test accuracy: 92.62
Round  37, Train loss: 1.508, Test loss: 1.538, Test accuracy: 92.74
Round  38, Train loss: 1.506, Test loss: 1.537, Test accuracy: 92.69
Round  39, Train loss: 1.508, Test loss: 1.537, Test accuracy: 92.77
Round  40, Train loss: 1.504, Test loss: 1.537, Test accuracy: 92.83
Round  41, Train loss: 1.507, Test loss: 1.536, Test accuracy: 92.83
Round  42, Train loss: 1.503, Test loss: 1.536, Test accuracy: 92.89
Round  43, Train loss: 1.505, Test loss: 1.536, Test accuracy: 92.99
Round  44, Train loss: 1.500, Test loss: 1.536, Test accuracy: 92.96
Round  45, Train loss: 1.500, Test loss: 1.535, Test accuracy: 92.97
Round  46, Train loss: 1.500, Test loss: 1.534, Test accuracy: 93.28
Round  47, Train loss: 1.499, Test loss: 1.533, Test accuracy: 93.12
Round  48, Train loss: 1.498, Test loss: 1.533, Test accuracy: 93.28
Round  49, Train loss: 1.502, Test loss: 1.533, Test accuracy: 93.28
Round  50, Train loss: 1.496, Test loss: 1.532, Test accuracy: 93.28
Round  51, Train loss: 1.496, Test loss: 1.531, Test accuracy: 93.23
Round  52, Train loss: 1.497, Test loss: 1.531, Test accuracy: 93.37
Round  53, Train loss: 1.494, Test loss: 1.530, Test accuracy: 93.41
Round  54, Train loss: 1.498, Test loss: 1.530, Test accuracy: 93.53
Round  55, Train loss: 1.497, Test loss: 1.531, Test accuracy: 93.38
Round  56, Train loss: 1.496, Test loss: 1.529, Test accuracy: 93.53
Round  57, Train loss: 1.496, Test loss: 1.531, Test accuracy: 93.60
Round  58, Train loss: 1.498, Test loss: 1.530, Test accuracy: 93.46
Round  59, Train loss: 1.491, Test loss: 1.529, Test accuracy: 93.55
Round  60, Train loss: 1.494, Test loss: 1.529, Test accuracy: 93.45
Round  61, Train loss: 1.493, Test loss: 1.528, Test accuracy: 93.61
Round  62, Train loss: 1.493, Test loss: 1.528, Test accuracy: 93.52
Round  63, Train loss: 1.493, Test loss: 1.527, Test accuracy: 93.72
Round  64, Train loss: 1.490, Test loss: 1.528, Test accuracy: 93.82
Round  65, Train loss: 1.490, Test loss: 1.528, Test accuracy: 93.65
Round  66, Train loss: 1.492, Test loss: 1.527, Test accuracy: 93.73
Round  67, Train loss: 1.494, Test loss: 1.527, Test accuracy: 93.79
Round  68, Train loss: 1.492, Test loss: 1.526, Test accuracy: 93.92
Round  69, Train loss: 1.490, Test loss: 1.526, Test accuracy: 94.01
Round  70, Train loss: 1.489, Test loss: 1.525, Test accuracy: 94.05
Round  71, Train loss: 1.490, Test loss: 1.525, Test accuracy: 93.78
Round  72, Train loss: 1.491, Test loss: 1.525, Test accuracy: 94.03
Round  73, Train loss: 1.491, Test loss: 1.525, Test accuracy: 94.01
Round  74, Train loss: 1.489, Test loss: 1.524, Test accuracy: 93.99
Round  75, Train loss: 1.485, Test loss: 1.524, Test accuracy: 94.08
Round  76, Train loss: 1.491, Test loss: 1.524, Test accuracy: 94.11
Round  77, Train loss: 1.487, Test loss: 1.523, Test accuracy: 94.23
Round  78, Train loss: 1.488, Test loss: 1.523, Test accuracy: 94.26
Round  79, Train loss: 1.489, Test loss: 1.524, Test accuracy: 94.12
Round  80, Train loss: 1.487, Test loss: 1.523, Test accuracy: 94.12
Round  81, Train loss: 1.486, Test loss: 1.524, Test accuracy: 94.14
Round  82, Train loss: 1.486, Test loss: 1.523, Test accuracy: 94.08
Round  83, Train loss: 1.486, Test loss: 1.523, Test accuracy: 94.25
Round  84, Train loss: 1.488, Test loss: 1.522, Test accuracy: 94.30
Round  85, Train loss: 1.486, Test loss: 1.522, Test accuracy: 94.29
Round  86, Train loss: 1.483, Test loss: 1.522, Test accuracy: 94.28
Round  87, Train loss: 1.485, Test loss: 1.522, Test accuracy: 94.17
Round  88, Train loss: 1.485, Test loss: 1.522, Test accuracy: 94.23
Round  89, Train loss: 1.483, Test loss: 1.522, Test accuracy: 94.13
Round  90, Train loss: 1.485, Test loss: 1.522, Test accuracy: 94.22
Round  91, Train loss: 1.486, Test loss: 1.522, Test accuracy: 94.21
Round  92, Train loss: 1.483, Test loss: 1.521, Test accuracy: 94.33
Round  93, Train loss: 1.485, Test loss: 1.521, Test accuracy: 94.34
Round  94, Train loss: 1.484, Test loss: 1.522, Test accuracy: 94.24
Round  95, Train loss: 1.483, Test loss: 1.521, Test accuracy: 94.22
Round  96, Train loss: 1.485, Test loss: 1.522, Test accuracy: 94.22
Round  97, Train loss: 1.484, Test loss: 1.520, Test accuracy: 94.33
Round  98, Train loss: 1.484, Test loss: 1.520, Test accuracy: 94.46
Round  99, Train loss: 1.483, Test loss: 1.520, Test accuracy: 94.25
Final Round, Train loss: 1.483, Test loss: 1.520, Test accuracy: 94.33
Average accuracy final 10 rounds: 94.28249999999998
1374.7758951187134
[2.167560577392578, 4.187452554702759, 6.206372261047363, 8.13611626625061, 9.943491220474243, 11.772181987762451, 13.592665433883667, 15.436760902404785, 17.27616286277771, 19.093870401382446, 20.911854028701782, 22.732297897338867, 24.55814242362976, 26.38431692123413, 28.206812620162964, 30.027338981628418, 31.854971170425415, 33.697856187820435, 35.54997277259827, 37.37813901901245, 39.20071816444397, 41.04007935523987, 42.883721113204956, 44.733420610427856, 46.584619998931885, 48.43765997886658, 50.29206037521362, 52.10723614692688, 53.93441128730774, 55.78668999671936, 57.63603591918945, 59.468424558639526, 61.31986904144287, 63.152496337890625, 64.99055433273315, 66.82794094085693, 68.65861868858337, 70.48722243309021, 72.30945110321045, 74.13305735588074, 75.96314215660095, 77.79056596755981, 79.60504674911499, 81.433518409729, 83.26300358772278, 85.11545038223267, 86.96679377555847, 88.82262873649597, 90.67402935028076, 92.52325916290283, 94.3672091960907, 96.2171847820282, 98.04463958740234, 99.86787247657776, 101.69869089126587, 103.55353808403015, 105.38002967834473, 107.20612812042236, 109.03360509872437, 110.86336755752563, 112.68400692939758, 114.49940347671509, 116.33822679519653, 118.19349145889282, 120.04941439628601, 121.878253698349, 123.70870542526245, 125.53497529029846, 127.35242772102356, 129.17893409729004, 131.02926921844482, 132.85492777824402, 134.67797875404358, 136.50734400749207, 138.35346460342407, 140.18091106414795, 142.3080062866211, 144.4326298236847, 146.2587070465088, 148.0845308303833, 149.915531873703, 151.72329950332642, 153.53722476959229, 155.34362626075745, 157.1577010154724, 158.95815896987915, 160.77066588401794, 162.58046746253967, 164.39083075523376, 166.19850039482117, 168.01381993293762, 169.8220202922821, 171.60012221336365, 173.37460494041443, 175.1544268131256, 176.93489265441895, 178.7523193359375, 180.5451443195343, 182.35528755187988, 184.13167524337769, 185.9490625858307]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[11.691666666666666, 26.808333333333334, 25.441666666666666, 29.625, 46.44166666666667, 64.91666666666667, 66.38333333333334, 73.375, 81.70833333333333, 82.60833333333333, 83.1, 83.75, 86.86666666666666, 88.33333333333333, 89.21666666666667, 89.94166666666666, 90.13333333333334, 90.55833333333334, 90.80833333333334, 90.975, 91.28333333333333, 91.44166666666666, 91.45, 91.40833333333333, 91.76666666666667, 91.76666666666667, 91.59166666666667, 91.65, 91.90833333333333, 92.2, 92.025, 92.35, 92.20833333333333, 92.425, 92.3, 92.425, 92.61666666666666, 92.74166666666666, 92.69166666666666, 92.76666666666667, 92.825, 92.83333333333333, 92.89166666666667, 92.99166666666666, 92.95833333333333, 92.975, 93.275, 93.11666666666666, 93.275, 93.275, 93.275, 93.23333333333333, 93.36666666666666, 93.40833333333333, 93.525, 93.38333333333334, 93.525, 93.6, 93.45833333333333, 93.55, 93.45, 93.60833333333333, 93.51666666666667, 93.725, 93.81666666666666, 93.65, 93.73333333333333, 93.79166666666667, 93.91666666666667, 94.00833333333334, 94.05, 93.775, 94.025, 94.00833333333334, 93.99166666666666, 94.075, 94.10833333333333, 94.23333333333333, 94.25833333333334, 94.125, 94.125, 94.14166666666667, 94.075, 94.25, 94.3, 94.29166666666667, 94.275, 94.16666666666667, 94.23333333333333, 94.13333333333334, 94.225, 94.20833333333333, 94.325, 94.34166666666667, 94.24166666666666, 94.21666666666667, 94.225, 94.33333333333333, 94.45833333333333, 94.25, 94.33333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.322, Test loss: 2.302, Test accuracy: 8.77
Round   1, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.67
Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.30
Round   3, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.06
Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.51
Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.42
Round   6, Train loss: 2.303, Test loss: 2.302, Test accuracy: 7.34
Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.85
Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.67
Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.75
Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.73
Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.08
Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.15
Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.23
Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.12
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.87
Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.61
Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.71
Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.00
Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.02
Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.01
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.13
Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.26
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.30
Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.35
Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.34
Round  26, Train loss: 2.301, Test loss: 2.302, Test accuracy: 7.69
Round  27, Train loss: 2.301, Test loss: 2.302, Test accuracy: 7.69
Round  28, Train loss: 2.302, Test loss: 2.303, Test accuracy: 7.58
Round  29, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.78
Round  30, Train loss: 2.302, Test loss: 2.303, Test accuracy: 7.41
Round  31, Train loss: 2.302, Test loss: 2.303, Test accuracy: 7.43
Round  32, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.37
Round  33, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.33
Round  34, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.41
Round  35, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.32
Round  36, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.29
Round  37, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.04
Round  38, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.32
Round  39, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.45
Round  40, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.54
Round  41, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.55
Round  42, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.40
Round  43, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.39
Round  44, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.27
Round  45, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.33
Round  46, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.28
Round  47, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.43
Round  48, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.30
Round  49, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.29
Round  50, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.30
Round  51, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.55
Round  52, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.38
Round  53, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.32
Round  54, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.19
Round  55, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.23
Round  56, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.35
Round  57, Train loss: 2.299, Test loss: 2.303, Test accuracy: 7.36
Round  58, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.45
Round  59, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.33
Round  60, Train loss: 2.299, Test loss: 2.303, Test accuracy: 7.43
Round  61, Train loss: 2.299, Test loss: 2.303, Test accuracy: 7.64
Round  62, Train loss: 2.299, Test loss: 2.303, Test accuracy: 7.59
Round  63, Train loss: 2.299, Test loss: 2.303, Test accuracy: 7.58
Round  64, Train loss: 2.299, Test loss: 2.304, Test accuracy: 7.62
Round  65, Train loss: 2.300, Test loss: 2.304, Test accuracy: 7.62
Round  66, Train loss: 2.299, Test loss: 2.304, Test accuracy: 7.69
Round  67, Train loss: 2.299, Test loss: 2.304, Test accuracy: 7.83
Round  68, Train loss: 2.298, Test loss: 2.304, Test accuracy: 7.79
Round  69, Train loss: 2.298, Test loss: 2.304, Test accuracy: 7.77
Round  70, Train loss: 2.298, Test loss: 2.304, Test accuracy: 7.80
Round  71, Train loss: 2.298, Test loss: 2.304, Test accuracy: 7.99
Round  72, Train loss: 2.299, Test loss: 2.304, Test accuracy: 8.08
Round  73, Train loss: 2.298, Test loss: 2.304, Test accuracy: 8.13
Round  74, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.21
Round  75, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.35
Round  76, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.37
Round  77, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.42
Round  78, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.59
Round  79, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.57
Round  80, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.61
Round  81, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.68
Round  82, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.74
Round  83, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.73
Round  84, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.79
Round  85, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.81
Round  86, Train loss: 2.296, Test loss: 2.305, Test accuracy: 8.87
Round  87, Train loss: 2.297, Test loss: 2.306, Test accuracy: 8.91
Round  88, Train loss: 2.296, Test loss: 2.306, Test accuracy: 8.91
Round  89, Train loss: 2.296, Test loss: 2.306, Test accuracy: 8.95
Round  90, Train loss: 2.296, Test loss: 2.306, Test accuracy: 9.00
Round  91, Train loss: 2.294, Test loss: 2.306, Test accuracy: 8.99
Round  92, Train loss: 2.296, Test loss: 2.306, Test accuracy: 8.95
Round  93, Train loss: 2.297, Test loss: 2.306, Test accuracy: 8.94
Round  94, Train loss: 2.295, Test loss: 2.306, Test accuracy: 8.95
Round  95, Train loss: 2.296, Test loss: 2.307, Test accuracy: 8.98/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 2.296, Test loss: 2.307, Test accuracy: 9.06
Round  97, Train loss: 2.293, Test loss: 2.307, Test accuracy: 9.06
Round  98, Train loss: 2.296, Test loss: 2.307, Test accuracy: 9.06
Round  99, Train loss: 2.294, Test loss: 2.307, Test accuracy: 9.05
Final Round, Train loss: 2.294, Test loss: 2.308, Test accuracy: 9.04
Average accuracy final 10 rounds: 9.0045
1348.3984599113464
[1.0372323989868164, 1.9125072956085205, 2.7745518684387207, 3.6463794708251953, 4.511243581771851, 5.38986873626709, 6.256757974624634, 7.156548976898193, 7.932474374771118, 8.794819355010986, 9.65129566192627, 10.51368761062622, 11.374762296676636, 12.24022364616394, 13.106030225753784, 13.974013805389404, 14.842626333236694, 15.708485126495361, 16.575552463531494, 17.437601804733276, 18.306498527526855, 19.17211604118347, 20.039499521255493, 20.902031898498535, 21.76968026161194, 22.633233308792114, 23.500731945037842, 24.36290168762207, 25.231526374816895, 26.09182596206665, 26.958958625793457, 27.82048773765564, 28.68865990638733, 29.549376487731934, 30.419408321380615, 31.279245615005493, 32.14899182319641, 33.00840640068054, 33.87479281425476, 34.73414659500122, 35.604206562042236, 36.46461892127991, 37.33252811431885, 38.19390630722046, 39.06109595298767, 39.918835163116455, 40.78565979003906, 41.645100116729736, 42.51009678840637, 43.37534999847412, 44.23873043060303, 45.095316648483276, 45.95754909515381, 46.81556510925293, 47.680503606796265, 48.53944659233093, 49.40231657028198, 50.260008573532104, 51.12319493293762, 51.98584032058716, 52.848642110824585, 53.70542311668396, 54.56607532501221, 55.420079708099365, 56.28310036659241, 57.13651132583618, 57.99785614013672, 58.85055112838745, 59.70962429046631, 60.56537342071533, 61.42164969444275, 62.27472805976868, 63.13483953475952, 63.98958230018616, 64.85216999053955, 65.70507311820984, 66.56238341331482, 67.42151498794556, 68.28436779975891, 69.14706325531006, 70.0064845085144, 70.87285852432251, 71.73210263252258, 72.59786462783813, 73.4579131603241, 74.33195090293884, 75.1945686340332, 76.06310057640076, 76.92774200439453, 77.79499506950378, 78.65703582763672, 79.5246012210846, 80.38877606391907, 81.25526309013367, 82.11486768722534, 82.98404717445374, 83.84589171409607, 84.71115970611572, 85.57690834999084, 86.44857406616211, 87.7477195262909]
[8.7725, 8.67, 8.2975, 8.06, 7.51, 7.4225, 7.345, 6.8475, 6.6725, 6.7525, 6.73, 7.0825, 7.1475, 7.2275, 7.125, 6.87, 6.6125, 6.715, 7.0, 7.0175, 7.01, 7.1325, 7.26, 7.305, 7.3475, 7.34, 7.6875, 7.6875, 7.585, 7.7775, 7.405, 7.435, 7.365, 7.3325, 7.405, 7.315, 7.2925, 7.0375, 7.3225, 7.445, 7.535, 7.5475, 7.3975, 7.39, 7.2725, 7.33, 7.275, 7.4325, 7.305, 7.2875, 7.3025, 7.5475, 7.385, 7.315, 7.19, 7.23, 7.3525, 7.3625, 7.45, 7.33, 7.4275, 7.645, 7.5875, 7.585, 7.625, 7.6175, 7.69, 7.835, 7.7925, 7.7675, 7.7975, 7.985, 8.0825, 8.13, 8.2125, 8.345, 8.3725, 8.4225, 8.585, 8.5725, 8.6075, 8.68, 8.745, 8.735, 8.795, 8.815, 8.8725, 8.915, 8.9125, 8.95, 8.9975, 8.9875, 8.9475, 8.9375, 8.9525, 8.9825, 9.0625, 9.0625, 9.06, 9.055, 9.045]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.301, Test accuracy: 12.09
Round   1, Train loss: 2.316, Test loss: 2.298, Test accuracy: 14.86
Round   2, Train loss: 2.308, Test loss: 2.292, Test accuracy: 19.79
Round   3, Train loss: 2.296, Test loss: 2.281, Test accuracy: 25.61
Round   4, Train loss: 2.273, Test loss: 2.250, Test accuracy: 26.62
Round   5, Train loss: 2.235, Test loss: 2.199, Test accuracy: 33.48
Round   6, Train loss: 2.183, Test loss: 2.145, Test accuracy: 43.23
Round   7, Train loss: 2.121, Test loss: 2.074, Test accuracy: 46.75
Round   8, Train loss: 2.047, Test loss: 2.027, Test accuracy: 50.49
Round   9, Train loss: 2.018, Test loss: 1.996, Test accuracy: 55.17
Round  10, Train loss: 2.004, Test loss: 1.971, Test accuracy: 58.96
Round  11, Train loss: 1.987, Test loss: 1.942, Test accuracy: 62.26
Round  12, Train loss: 1.947, Test loss: 1.913, Test accuracy: 63.95
Round  13, Train loss: 1.915, Test loss: 1.887, Test accuracy: 66.59
Round  14, Train loss: 1.893, Test loss: 1.864, Test accuracy: 68.11
Round  15, Train loss: 1.873, Test loss: 1.842, Test accuracy: 70.89
Round  16, Train loss: 1.861, Test loss: 1.821, Test accuracy: 72.85
Round  17, Train loss: 1.827, Test loss: 1.806, Test accuracy: 74.50
Round  18, Train loss: 1.822, Test loss: 1.786, Test accuracy: 76.45
Round  19, Train loss: 1.794, Test loss: 1.773, Test accuracy: 79.27
Round  20, Train loss: 1.778, Test loss: 1.751, Test accuracy: 82.89
Round  21, Train loss: 1.771, Test loss: 1.719, Test accuracy: 86.25
Round  22, Train loss: 1.722, Test loss: 1.700, Test accuracy: 87.71
Round  23, Train loss: 1.710, Test loss: 1.684, Test accuracy: 89.00
Round  24, Train loss: 1.696, Test loss: 1.669, Test accuracy: 89.91
Round  25, Train loss: 1.692, Test loss: 1.653, Test accuracy: 90.57
Round  26, Train loss: 1.685, Test loss: 1.639, Test accuracy: 90.94
Round  27, Train loss: 1.672, Test loss: 1.631, Test accuracy: 91.29
Round  28, Train loss: 1.655, Test loss: 1.626, Test accuracy: 91.48
Round  29, Train loss: 1.650, Test loss: 1.621, Test accuracy: 91.76
Round  30, Train loss: 1.639, Test loss: 1.618, Test accuracy: 91.98
Round  31, Train loss: 1.642, Test loss: 1.613, Test accuracy: 92.16
Round  32, Train loss: 1.624, Test loss: 1.611, Test accuracy: 92.33
Round  33, Train loss: 1.622, Test loss: 1.608, Test accuracy: 92.44
Round  34, Train loss: 1.632, Test loss: 1.600, Test accuracy: 92.64
Round  35, Train loss: 1.616, Test loss: 1.600, Test accuracy: 92.70
Round  36, Train loss: 1.618, Test loss: 1.595, Test accuracy: 92.85
Round  37, Train loss: 1.611, Test loss: 1.591, Test accuracy: 93.07
Round  38, Train loss: 1.604, Test loss: 1.589, Test accuracy: 93.22
Round  39, Train loss: 1.615, Test loss: 1.584, Test accuracy: 93.36
Round  40, Train loss: 1.596, Test loss: 1.585, Test accuracy: 93.42
Round  41, Train loss: 1.598, Test loss: 1.582, Test accuracy: 93.51
Round  42, Train loss: 1.601, Test loss: 1.577, Test accuracy: 93.68
Round  43, Train loss: 1.592, Test loss: 1.577, Test accuracy: 93.85
Round  44, Train loss: 1.577, Test loss: 1.578, Test accuracy: 93.86
Round  45, Train loss: 1.579, Test loss: 1.576, Test accuracy: 93.82
Round  46, Train loss: 1.589, Test loss: 1.572, Test accuracy: 93.98
Round  47, Train loss: 1.583, Test loss: 1.570, Test accuracy: 94.00
Round  48, Train loss: 1.573, Test loss: 1.571, Test accuracy: 94.12
Round  49, Train loss: 1.573, Test loss: 1.569, Test accuracy: 94.15
Round  50, Train loss: 1.581, Test loss: 1.566, Test accuracy: 94.24
Round  51, Train loss: 1.566, Test loss: 1.566, Test accuracy: 94.40
Round  52, Train loss: 1.572, Test loss: 1.564, Test accuracy: 94.38
Round  53, Train loss: 1.578, Test loss: 1.561, Test accuracy: 94.41
Round  54, Train loss: 1.561, Test loss: 1.563, Test accuracy: 94.37
Round  55, Train loss: 1.565, Test loss: 1.562, Test accuracy: 94.40
Round  56, Train loss: 1.562, Test loss: 1.560, Test accuracy: 94.48
Round  57, Train loss: 1.560, Test loss: 1.559, Test accuracy: 94.56
Round  58, Train loss: 1.561, Test loss: 1.557, Test accuracy: 94.63
Round  59, Train loss: 1.553, Test loss: 1.557, Test accuracy: 94.70
Round  60, Train loss: 1.553, Test loss: 1.558, Test accuracy: 94.67
Round  61, Train loss: 1.553, Test loss: 1.556, Test accuracy: 94.72
Round  62, Train loss: 1.548, Test loss: 1.556, Test accuracy: 94.78
Round  63, Train loss: 1.544, Test loss: 1.556, Test accuracy: 94.77
Round  64, Train loss: 1.552, Test loss: 1.553, Test accuracy: 94.79
Round  65, Train loss: 1.544, Test loss: 1.554, Test accuracy: 94.82
Round  66, Train loss: 1.545, Test loss: 1.552, Test accuracy: 94.95
Round  67, Train loss: 1.545, Test loss: 1.552, Test accuracy: 94.93
Round  68, Train loss: 1.546, Test loss: 1.551, Test accuracy: 94.94
Round  69, Train loss: 1.550, Test loss: 1.548, Test accuracy: 94.98
Round  70, Train loss: 1.539, Test loss: 1.550, Test accuracy: 94.98
Round  71, Train loss: 1.541, Test loss: 1.549, Test accuracy: 95.02
Round  72, Train loss: 1.538, Test loss: 1.549, Test accuracy: 95.01
Round  73, Train loss: 1.539, Test loss: 1.548, Test accuracy: 95.09
Round  74, Train loss: 1.543, Test loss: 1.546, Test accuracy: 95.07
Round  75, Train loss: 1.535, Test loss: 1.547, Test accuracy: 95.18
Round  76, Train loss: 1.541, Test loss: 1.544, Test accuracy: 95.17
Round  77, Train loss: 1.539, Test loss: 1.544, Test accuracy: 95.18
Round  78, Train loss: 1.537, Test loss: 1.544, Test accuracy: 95.18
Round  79, Train loss: 1.531, Test loss: 1.546, Test accuracy: 95.28
Round  80, Train loss: 1.538, Test loss: 1.543, Test accuracy: 95.27
Round  81, Train loss: 1.532, Test loss: 1.543, Test accuracy: 95.39
Round  82, Train loss: 1.533, Test loss: 1.542, Test accuracy: 95.43
Round  83, Train loss: 1.534, Test loss: 1.541, Test accuracy: 95.42
Round  84, Train loss: 1.528, Test loss: 1.542, Test accuracy: 95.39
Round  85, Train loss: 1.530, Test loss: 1.541, Test accuracy: 95.42
Round  86, Train loss: 1.527, Test loss: 1.541, Test accuracy: 95.43
Round  87, Train loss: 1.524, Test loss: 1.541, Test accuracy: 95.54
Round  88, Train loss: 1.523, Test loss: 1.541, Test accuracy: 95.55
Round  89, Train loss: 1.523, Test loss: 1.540, Test accuracy: 95.64
Round  90, Train loss: 1.523, Test loss: 1.540, Test accuracy: 95.63
Round  91, Train loss: 1.525, Test loss: 1.539, Test accuracy: 95.68
Round  92, Train loss: 1.522, Test loss: 1.539, Test accuracy: 95.70
Round  93, Train loss: 1.520, Test loss: 1.539, Test accuracy: 95.72/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.521, Test loss: 1.540, Test accuracy: 95.72
Round  95, Train loss: 1.527, Test loss: 1.536, Test accuracy: 95.72
Round  96, Train loss: 1.521, Test loss: 1.537, Test accuracy: 95.73
Round  97, Train loss: 1.520, Test loss: 1.537, Test accuracy: 95.74
Round  98, Train loss: 1.519, Test loss: 1.537, Test accuracy: 95.74
Round  99, Train loss: 1.515, Test loss: 1.538, Test accuracy: 95.78
Final Round, Train loss: 1.495, Test loss: 1.532, Test accuracy: 95.77
Average accuracy final 10 rounds: 95.71475000000001
1321.87154006958
[0.9835460186004639, 1.8451042175292969, 2.7020857334136963, 3.5609114170074463, 4.418253183364868, 5.279950380325317, 6.140278577804565, 6.997110605239868, 7.8564605712890625, 8.724778175354004, 9.581104516983032, 10.447983503341675, 11.311675310134888, 12.177855730056763, 13.044528722763062, 13.905204772949219, 14.777242183685303, 15.638170957565308, 16.505834579467773, 17.366734266281128, 18.224811792373657, 19.08313012123108, 19.949543714523315, 20.808690071105957, 21.67358088493347, 22.52927327156067, 23.392956495285034, 24.247897148132324, 25.110564947128296, 25.962742567062378, 26.825227737426758, 27.67964816093445, 28.543838024139404, 29.39844250679016, 30.2702898979187, 31.12434673309326, 32.00019454956055, 32.85400581359863, 33.731001138687134, 34.58234477043152, 35.4597430229187, 36.31439661979675, 37.1915009021759, 38.04280161857605, 38.91740441322327, 39.77880668640137, 40.65808391571045, 41.51383185386658, 42.39049172401428, 43.24687314033508, 44.12210130691528, 44.97822904586792, 45.84621858596802, 46.70668292045593, 47.60627102851868, 48.394174337387085, 49.19890284538269, 49.98069620132446, 50.78219127655029, 51.56544876098633, 52.35594177246094, 53.14208769798279, 53.930503606796265, 54.71644067764282, 55.50067067146301, 56.2905957698822, 57.0777792930603, 57.860004901885986, 58.64324593544006, 59.42679190635681, 60.21247363090515, 60.997589111328125, 61.78375506401062, 62.58319139480591, 63.36949419975281, 64.16598176956177, 64.94766449928284, 65.73461675643921, 66.51329731941223, 67.30468320846558, 68.08885979652405, 68.87180423736572, 69.65102982521057, 70.43931293487549, 71.2244622707367, 72.0147910118103, 72.81078290939331, 73.59855556488037, 74.38722133636475, 75.17268538475037, 75.96339344978333, 76.74326825141907, 77.53306317329407, 78.31861138343811, 79.10362577438354, 79.88593435287476, 80.6758246421814, 81.46194529533386, 82.24741220474243, 83.03685474395752, 84.35057497024536]
[12.0925, 14.8625, 19.7925, 25.6125, 26.6225, 33.475, 43.2275, 46.7475, 50.4925, 55.175, 58.96, 62.26, 63.955, 66.5925, 68.105, 70.8875, 72.85, 74.4975, 76.4525, 79.265, 82.89, 86.2525, 87.71, 88.9975, 89.9075, 90.5725, 90.94, 91.29, 91.48, 91.76, 91.9775, 92.1625, 92.335, 92.44, 92.635, 92.6975, 92.85, 93.0675, 93.2225, 93.3575, 93.415, 93.51, 93.6775, 93.85, 93.8625, 93.8225, 93.985, 94.0025, 94.125, 94.15, 94.2425, 94.3975, 94.375, 94.405, 94.3725, 94.3975, 94.48, 94.5575, 94.6325, 94.7025, 94.6725, 94.7225, 94.7825, 94.765, 94.7875, 94.8175, 94.9475, 94.9275, 94.945, 94.9775, 94.98, 95.0225, 95.01, 95.095, 95.07, 95.1775, 95.165, 95.18, 95.1825, 95.275, 95.2675, 95.385, 95.43, 95.42, 95.385, 95.42, 95.4325, 95.54, 95.545, 95.635, 95.6275, 95.68, 95.6975, 95.715, 95.72, 95.7175, 95.7275, 95.74, 95.7425, 95.78, 95.7675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.226, Test loss: 2.194, Test accuracy: 35.85 

Round   0, Global train loss: 2.226, Global test loss: 2.283, Global test accuracy: 28.64 

Round   1, Train loss: 1.767, Test loss: 2.051, Test accuracy: 38.91 

Round   1, Global train loss: 1.767, Global test loss: 2.243, Global test accuracy: 17.64 

Round   2, Train loss: 1.867, Test loss: 1.895, Test accuracy: 63.51 

Round   2, Global train loss: 1.867, Global test loss: 2.161, Global test accuracy: 39.24 

Round   3, Train loss: 1.660, Test loss: 1.835, Test accuracy: 65.17 

Round   3, Global train loss: 1.660, Global test loss: 2.202, Global test accuracy: 27.46 

Round   4, Train loss: 1.589, Test loss: 1.699, Test accuracy: 80.38 

Round   4, Global train loss: 1.589, Global test loss: 2.126, Global test accuracy: 37.63 

Round   5, Train loss: 1.712, Test loss: 1.651, Test accuracy: 84.39 

Round   5, Global train loss: 1.712, Global test loss: 2.203, Global test accuracy: 23.41 

Round   6, Train loss: 1.645, Test loss: 1.636, Test accuracy: 84.60 

Round   6, Global train loss: 1.645, Global test loss: 2.196, Global test accuracy: 20.68 

Round   7, Train loss: 1.748, Test loss: 1.634, Test accuracy: 84.65 

Round   7, Global train loss: 1.748, Global test loss: 2.217, Global test accuracy: 25.37 

Round   8, Train loss: 1.638, Test loss: 1.612, Test accuracy: 86.45 

Round   8, Global train loss: 1.638, Global test loss: 2.098, Global test accuracy: 34.37 

Round   9, Train loss: 1.554, Test loss: 1.603, Test accuracy: 86.94 

Round   9, Global train loss: 1.554, Global test loss: 2.243, Global test accuracy: 15.43 

Round  10, Train loss: 1.589, Test loss: 1.599, Test accuracy: 86.98 

Round  10, Global train loss: 1.589, Global test loss: 2.098, Global test accuracy: 34.99 

Round  11, Train loss: 1.579, Test loss: 1.598, Test accuracy: 87.01 

Round  11, Global train loss: 1.579, Global test loss: 2.083, Global test accuracy: 40.40 

Round  12, Train loss: 1.591, Test loss: 1.597, Test accuracy: 87.08 

Round  12, Global train loss: 1.591, Global test loss: 2.187, Global test accuracy: 22.09 

Round  13, Train loss: 1.636, Test loss: 1.597, Test accuracy: 87.08 

Round  13, Global train loss: 1.636, Global test loss: 2.212, Global test accuracy: 19.72 

Round  14, Train loss: 1.683, Test loss: 1.596, Test accuracy: 87.11 

Round  14, Global train loss: 1.683, Global test loss: 2.180, Global test accuracy: 30.12 

Round  15, Train loss: 1.533, Test loss: 1.594, Test accuracy: 87.17 

Round  15, Global train loss: 1.533, Global test loss: 2.110, Global test accuracy: 34.68 

Round  16, Train loss: 1.523, Test loss: 1.594, Test accuracy: 87.12 

Round  16, Global train loss: 1.523, Global test loss: 2.156, Global test accuracy: 25.77 

Round  17, Train loss: 1.553, Test loss: 1.581, Test accuracy: 88.44 

Round  17, Global train loss: 1.553, Global test loss: 2.141, Global test accuracy: 30.95 

Round  18, Train loss: 1.524, Test loss: 1.581, Test accuracy: 88.52 

Round  18, Global train loss: 1.524, Global test loss: 2.080, Global test accuracy: 49.48 

Round  19, Train loss: 1.608, Test loss: 1.568, Test accuracy: 90.02 

Round  19, Global train loss: 1.608, Global test loss: 2.201, Global test accuracy: 28.93 

Round  20, Train loss: 1.526, Test loss: 1.566, Test accuracy: 90.04 

Round  20, Global train loss: 1.526, Global test loss: 2.153, Global test accuracy: 27.99 

Round  21, Train loss: 1.576, Test loss: 1.566, Test accuracy: 90.06 

Round  21, Global train loss: 1.576, Global test loss: 2.115, Global test accuracy: 38.40 

Round  22, Train loss: 1.528, Test loss: 1.565, Test accuracy: 90.08 

Round  22, Global train loss: 1.528, Global test loss: 2.200, Global test accuracy: 23.88 

Round  23, Train loss: 1.467, Test loss: 1.565, Test accuracy: 90.05 

Round  23, Global train loss: 1.467, Global test loss: 2.125, Global test accuracy: 39.06 

Round  24, Train loss: 1.530, Test loss: 1.564, Test accuracy: 90.02 

Round  24, Global train loss: 1.530, Global test loss: 2.100, Global test accuracy: 36.98 

Round  25, Train loss: 1.580, Test loss: 1.564, Test accuracy: 90.06 

Round  25, Global train loss: 1.580, Global test loss: 2.178, Global test accuracy: 28.22 

Round  26, Train loss: 1.578, Test loss: 1.564, Test accuracy: 89.98 

Round  26, Global train loss: 1.578, Global test loss: 2.064, Global test accuracy: 45.32 

Round  27, Train loss: 1.523, Test loss: 1.564, Test accuracy: 89.97 

Round  27, Global train loss: 1.523, Global test loss: 2.154, Global test accuracy: 38.33 

Round  28, Train loss: 1.521, Test loss: 1.564, Test accuracy: 90.03 

Round  28, Global train loss: 1.521, Global test loss: 2.070, Global test accuracy: 40.58 

Round  29, Train loss: 1.518, Test loss: 1.551, Test accuracy: 91.50 

Round  29, Global train loss: 1.518, Global test loss: 2.102, Global test accuracy: 42.62 

Round  30, Train loss: 1.465, Test loss: 1.551, Test accuracy: 91.53 

Round  30, Global train loss: 1.465, Global test loss: 2.115, Global test accuracy: 31.77 

Round  31, Train loss: 1.476, Test loss: 1.547, Test accuracy: 91.66 

Round  31, Global train loss: 1.476, Global test loss: 2.014, Global test accuracy: 45.28 

Round  32, Train loss: 1.604, Test loss: 1.534, Test accuracy: 93.13 

Round  32, Global train loss: 1.604, Global test loss: 2.153, Global test accuracy: 27.53 

Round  33, Train loss: 1.525, Test loss: 1.534, Test accuracy: 93.14 

Round  33, Global train loss: 1.525, Global test loss: 2.155, Global test accuracy: 27.01 

Round  34, Train loss: 1.469, Test loss: 1.532, Test accuracy: 93.17 

Round  34, Global train loss: 1.469, Global test loss: 2.069, Global test accuracy: 38.96 

Round  35, Train loss: 1.467, Test loss: 1.532, Test accuracy: 93.21 

Round  35, Global train loss: 1.467, Global test loss: 2.150, Global test accuracy: 27.62 

Round  36, Train loss: 1.522, Test loss: 1.532, Test accuracy: 93.23 

Round  36, Global train loss: 1.522, Global test loss: 2.213, Global test accuracy: 21.08 

Round  37, Train loss: 1.520, Test loss: 1.532, Test accuracy: 93.22 

Round  37, Global train loss: 1.520, Global test loss: 2.206, Global test accuracy: 24.67 

Round  38, Train loss: 1.468, Test loss: 1.532, Test accuracy: 93.20 

Round  38, Global train loss: 1.468, Global test loss: 2.025, Global test accuracy: 42.92 

Round  39, Train loss: 1.519, Test loss: 1.532, Test accuracy: 93.20 

Round  39, Global train loss: 1.519, Global test loss: 2.038, Global test accuracy: 48.63 

Round  40, Train loss: 1.519, Test loss: 1.532, Test accuracy: 93.21 

Round  40, Global train loss: 1.519, Global test loss: 2.124, Global test accuracy: 34.42 

Round  41, Train loss: 1.518, Test loss: 1.532, Test accuracy: 93.23 

Round  41, Global train loss: 1.518, Global test loss: 2.076, Global test accuracy: 39.93 

Round  42, Train loss: 1.518, Test loss: 1.532, Test accuracy: 93.26 

Round  42, Global train loss: 1.518, Global test loss: 2.160, Global test accuracy: 22.79 

Round  43, Train loss: 1.525, Test loss: 1.531, Test accuracy: 93.30 

Round  43, Global train loss: 1.525, Global test loss: 2.135, Global test accuracy: 34.67 

Round  44, Train loss: 1.521, Test loss: 1.531, Test accuracy: 93.32 

Round  44, Global train loss: 1.521, Global test loss: 2.131, Global test accuracy: 36.33 

Round  45, Train loss: 1.523, Test loss: 1.531, Test accuracy: 93.32 

Round  45, Global train loss: 1.523, Global test loss: 2.088, Global test accuracy: 36.12 

Round  46, Train loss: 1.521, Test loss: 1.531, Test accuracy: 93.32 

Round  46, Global train loss: 1.521, Global test loss: 2.080, Global test accuracy: 51.90 

Round  47, Train loss: 1.467, Test loss: 1.531, Test accuracy: 93.33 

Round  47, Global train loss: 1.467, Global test loss: 2.254, Global test accuracy: 16.47 

Round  48, Train loss: 1.520, Test loss: 1.531, Test accuracy: 93.37 

Round  48, Global train loss: 1.520, Global test loss: 2.188, Global test accuracy: 22.87 

Round  49, Train loss: 1.467, Test loss: 1.531, Test accuracy: 93.34 

Round  49, Global train loss: 1.467, Global test loss: 2.261, Global test accuracy: 13.63 

Round  50, Train loss: 1.465, Test loss: 1.531, Test accuracy: 93.33 

Round  50, Global train loss: 1.465, Global test loss: 2.210, Global test accuracy: 20.98 

Round  51, Train loss: 1.464, Test loss: 1.531, Test accuracy: 93.34 

Round  51, Global train loss: 1.464, Global test loss: 2.152, Global test accuracy: 30.38 

Round  52, Train loss: 1.517, Test loss: 1.531, Test accuracy: 93.33 

Round  52, Global train loss: 1.517, Global test loss: 2.146, Global test accuracy: 31.12 

Round  53, Train loss: 1.521, Test loss: 1.531, Test accuracy: 93.34 

Round  53, Global train loss: 1.521, Global test loss: 1.993, Global test accuracy: 50.54 

Round  54, Train loss: 1.466, Test loss: 1.531, Test accuracy: 93.37 

Round  54, Global train loss: 1.466, Global test loss: 2.080, Global test accuracy: 36.88 

Round  55, Train loss: 1.506, Test loss: 1.519, Test accuracy: 94.66 

Round  55, Global train loss: 1.506, Global test loss: 2.205, Global test accuracy: 24.13 

Round  56, Train loss: 1.523, Test loss: 1.519, Test accuracy: 94.64 

Round  56, Global train loss: 1.523, Global test loss: 2.103, Global test accuracy: 34.14 

Round  57, Train loss: 1.466, Test loss: 1.519, Test accuracy: 94.64 

Round  57, Global train loss: 1.466, Global test loss: 2.143, Global test accuracy: 31.74 

Round  58, Train loss: 1.465, Test loss: 1.519, Test accuracy: 94.65 

Round  58, Global train loss: 1.465, Global test loss: 2.082, Global test accuracy: 41.95 

Round  59, Train loss: 1.522, Test loss: 1.519, Test accuracy: 94.65 

Round  59, Global train loss: 1.522, Global test loss: 2.115, Global test accuracy: 37.10 

Round  60, Train loss: 1.464, Test loss: 1.519, Test accuracy: 94.64 

Round  60, Global train loss: 1.464, Global test loss: 2.037, Global test accuracy: 48.17 

Round  61, Train loss: 1.474, Test loss: 1.517, Test accuracy: 94.70 

Round  61, Global train loss: 1.474, Global test loss: 2.186, Global test accuracy: 20.34 

Round  62, Train loss: 1.467, Test loss: 1.518, Test accuracy: 94.69 

Round  62, Global train loss: 1.467, Global test loss: 2.194, Global test accuracy: 24.22 

Round  63, Train loss: 1.525, Test loss: 1.517, Test accuracy: 94.77 

Round  63, Global train loss: 1.525, Global test loss: 2.152, Global test accuracy: 29.91 

Round  64, Train loss: 1.467, Test loss: 1.517, Test accuracy: 94.77 

Round  64, Global train loss: 1.467, Global test loss: 2.076, Global test accuracy: 34.95 

Round  65, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.77 

Round  65, Global train loss: 1.522, Global test loss: 2.191, Global test accuracy: 24.84 

Round  66, Train loss: 1.469, Test loss: 1.517, Test accuracy: 94.70 

Round  66, Global train loss: 1.469, Global test loss: 2.277, Global test accuracy: 16.18 

Round  67, Train loss: 1.464, Test loss: 1.517, Test accuracy: 94.70 

Round  67, Global train loss: 1.464, Global test loss: 2.062, Global test accuracy: 38.28 

Round  68, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.71 

Round  68, Global train loss: 1.522, Global test loss: 2.109, Global test accuracy: 39.24 

Round  69, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.71 

Round  69, Global train loss: 1.522, Global test loss: 2.092, Global test accuracy: 44.51 

Round  70, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.72 

Round  70, Global train loss: 1.464, Global test loss: 2.058, Global test accuracy: 43.92 

Round  71, Train loss: 1.468, Test loss: 1.516, Test accuracy: 94.72 

Round  71, Global train loss: 1.468, Global test loss: 2.254, Global test accuracy: 17.24 

Round  72, Train loss: 1.523, Test loss: 1.516, Test accuracy: 94.73 

Round  72, Global train loss: 1.523, Global test loss: 2.102, Global test accuracy: 31.70 

Round  73, Train loss: 1.465, Test loss: 1.516, Test accuracy: 94.74 

Round  73, Global train loss: 1.465, Global test loss: 2.162, Global test accuracy: 22.64 

Round  74, Train loss: 1.462, Test loss: 1.516, Test accuracy: 94.74 

Round  74, Global train loss: 1.462, Global test loss: 2.081, Global test accuracy: 40.91 

Round  75, Train loss: 1.466, Test loss: 1.516, Test accuracy: 94.73 

Round  75, Global train loss: 1.466, Global test loss: 2.261, Global test accuracy: 13.91 

Round  76, Train loss: 1.465, Test loss: 1.516, Test accuracy: 94.73 

Round  76, Global train loss: 1.465, Global test loss: 2.144, Global test accuracy: 27.88 

Round  77, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.73 

Round  77, Global train loss: 1.464, Global test loss: 2.076, Global test accuracy: 45.16 

Round  78, Train loss: 1.462, Test loss: 1.516, Test accuracy: 94.73 

Round  78, Global train loss: 1.462, Global test loss: 2.097, Global test accuracy: 41.56 

Round  79, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.74 

Round  79, Global train loss: 1.522, Global test loss: 2.169, Global test accuracy: 28.43 

Round  80, Train loss: 1.521, Test loss: 1.516, Test accuracy: 94.74 

Round  80, Global train loss: 1.521, Global test loss: 2.093, Global test accuracy: 38.70 

Round  81, Train loss: 1.523, Test loss: 1.516, Test accuracy: 94.74 

Round  81, Global train loss: 1.523, Global test loss: 2.115, Global test accuracy: 41.20 

Round  82, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.75 

Round  82, Global train loss: 1.464, Global test loss: 2.108, Global test accuracy: 35.20 

Round  83, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.70 

Round  83, Global train loss: 1.522, Global test loss: 2.259, Global test accuracy: 19.65 

Round  84, Train loss: 1.522, Test loss: 1.517, Test accuracy: 94.67 

Round  84, Global train loss: 1.522, Global test loss: 2.157, Global test accuracy: 29.08 

Round  85, Train loss: 1.466, Test loss: 1.516, Test accuracy: 94.70 

Round  85, Global train loss: 1.466, Global test loss: 2.114, Global test accuracy: 33.76 

Round  86, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.70 

Round  86, Global train loss: 1.464, Global test loss: 2.044, Global test accuracy: 42.33 

Round  87, Train loss: 1.462, Test loss: 1.516, Test accuracy: 94.70 

Round  87, Global train loss: 1.462, Global test loss: 2.038, Global test accuracy: 45.34 

Round  88, Train loss: 1.465, Test loss: 1.516, Test accuracy: 94.70 

Round  88, Global train loss: 1.465, Global test loss: 2.188, Global test accuracy: 28.00 

Round  89, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.72 

Round  89, Global train loss: 1.464, Global test loss: 2.193, Global test accuracy: 21.00 

Round  90, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.72 

Round  90, Global train loss: 1.464, Global test loss: 2.202, Global test accuracy: 25.42 

Round  91, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.71 

Round  91, Global train loss: 1.464, Global test loss: 2.128, Global test accuracy: 32.64 

Round  92, Train loss: 1.521, Test loss: 1.516, Test accuracy: 94.67 

Round  92, Global train loss: 1.521, Global test loss: 2.206, Global test accuracy: 21.62 

Round  93, Train loss: 1.465, Test loss: 1.516, Test accuracy: 94.67 

Round  93, Global train loss: 1.465, Global test loss: 2.161, Global test accuracy: 24.97 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.70 

Round  94, Global train loss: 1.464, Global test loss: 2.032, Global test accuracy: 46.91 

Round  95, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.70 

Round  95, Global train loss: 1.464, Global test loss: 2.103, Global test accuracy: 36.86 

Round  96, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.69 

Round  96, Global train loss: 1.464, Global test loss: 2.036, Global test accuracy: 41.34 

Round  97, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.69 

Round  97, Global train loss: 1.464, Global test loss: 2.124, Global test accuracy: 29.43 

Round  98, Train loss: 1.466, Test loss: 1.516, Test accuracy: 94.68 

Round  98, Global train loss: 1.466, Global test loss: 2.077, Global test accuracy: 42.17 

Round  99, Train loss: 1.466, Test loss: 1.516, Test accuracy: 94.68 

Round  99, Global train loss: 1.466, Global test loss: 2.183, Global test accuracy: 23.57 

Final Round, Train loss: 1.482, Test loss: 1.516, Test accuracy: 94.71 

Final Round, Global train loss: 1.482, Global test loss: 2.183, Global test accuracy: 23.57 

Average accuracy final 10 rounds: 94.6925 

Average global accuracy final 10 rounds: 32.49333333333334 

949.7276291847229
[0.8246917724609375, 1.5636649131774902, 2.300971031188965, 3.0324037075042725, 3.770650625228882, 4.515578746795654, 5.252824783325195, 5.990726947784424, 6.742670297622681, 7.480889797210693, 8.217970132827759, 8.968200445175171, 9.707202196121216, 10.444408893585205, 11.192769050598145, 11.92981481552124, 12.66389536857605, 13.416408777236938, 14.156615495681763, 14.891223192214966, 15.640198469161987, 16.37914729118347, 17.116074323654175, 17.864531993865967, 18.605958938598633, 19.344679594039917, 20.088707208633423, 20.828411102294922, 21.566513538360596, 22.319811582565308, 23.055299520492554, 23.794392347335815, 24.546466827392578, 25.282646656036377, 26.02553701400757, 26.77276301383972, 27.512327909469604, 28.252593755722046, 29.004347562789917, 29.744654655456543, 30.485228776931763, 31.235632181167603, 31.973567724227905, 32.7116482257843, 33.45187306404114, 34.19294333457947, 34.938727378845215, 35.68393516540527, 36.419413328170776, 37.160619020462036, 37.89783811569214, 38.63385486602783, 39.376805782318115, 40.11631536483765, 40.852635860443115, 41.60102677345276, 42.3389036655426, 43.080830574035645, 43.82691526412964, 44.56602668762207, 45.30404996871948, 46.04948711395264, 46.79249978065491, 47.5294668674469, 48.27772521972656, 49.016003131866455, 49.75576996803284, 50.50100231170654, 51.242483615875244, 51.99053597450256, 52.72710728645325, 53.468756914138794, 54.21456480026245, 54.95151662826538, 55.693074464797974, 56.43904781341553, 57.18124222755432, 57.92129611968994, 58.6706120967865, 59.411147594451904, 60.145620584487915, 60.89630365371704, 61.63397574424744, 62.37221908569336, 63.12007403373718, 63.86166000366211, 64.61085724830627, 65.35291719436646, 66.09031796455383, 66.83171439170837, 67.57030153274536, 68.30934524536133, 69.05807852745056, 69.79678797721863, 70.53563451766968, 71.28788542747498, 72.02949023246765, 72.76687288284302, 73.51042985916138, 74.24630856513977, 75.73575139045715]
[35.85, 38.90833333333333, 63.50833333333333, 65.16666666666667, 80.38333333333334, 84.39166666666667, 84.6, 84.65, 86.45, 86.94166666666666, 86.98333333333333, 87.00833333333334, 87.08333333333333, 87.075, 87.10833333333333, 87.175, 87.125, 88.44166666666666, 88.51666666666667, 90.01666666666667, 90.04166666666667, 90.05833333333334, 90.075, 90.05, 90.01666666666667, 90.05833333333334, 89.98333333333333, 89.975, 90.025, 91.5, 91.525, 91.65833333333333, 93.13333333333334, 93.14166666666667, 93.16666666666667, 93.20833333333333, 93.23333333333333, 93.225, 93.2, 93.2, 93.20833333333333, 93.23333333333333, 93.25833333333334, 93.3, 93.31666666666666, 93.31666666666666, 93.31666666666666, 93.325, 93.36666666666666, 93.34166666666667, 93.33333333333333, 93.34166666666667, 93.33333333333333, 93.34166666666667, 93.36666666666666, 94.65833333333333, 94.64166666666667, 94.64166666666667, 94.65, 94.65, 94.64166666666667, 94.7, 94.69166666666666, 94.76666666666667, 94.76666666666667, 94.76666666666667, 94.7, 94.7, 94.70833333333333, 94.70833333333333, 94.725, 94.71666666666667, 94.73333333333333, 94.74166666666666, 94.74166666666666, 94.73333333333333, 94.73333333333333, 94.73333333333333, 94.73333333333333, 94.74166666666666, 94.74166666666666, 94.74166666666666, 94.75, 94.7, 94.675, 94.7, 94.7, 94.7, 94.7, 94.71666666666667, 94.71666666666667, 94.70833333333333, 94.675, 94.675, 94.7, 94.7, 94.69166666666666, 94.69166666666666, 94.68333333333334, 94.68333333333334, 94.70833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.233, Test loss: 2.223, Test accuracy: 16.96 

Round   0, Global train loss: 2.233, Global test loss: 2.293, Global test accuracy: 8.33 

Round   1, Train loss: 1.959, Test loss: 2.053, Test accuracy: 44.71 

Round   1, Global train loss: 1.959, Global test loss: 2.257, Global test accuracy: 25.62 

Round   2, Train loss: 1.735, Test loss: 1.843, Test accuracy: 64.03 

Round   2, Global train loss: 1.735, Global test loss: 2.156, Global test accuracy: 30.84 

Round   3, Train loss: 1.618, Test loss: 1.792, Test accuracy: 68.63 

Round   3, Global train loss: 1.618, Global test loss: 2.166, Global test accuracy: 28.12 

Round   4, Train loss: 1.584, Test loss: 1.719, Test accuracy: 75.52 

Round   4, Global train loss: 1.584, Global test loss: 2.098, Global test accuracy: 35.96 

Round   5, Train loss: 1.520, Test loss: 1.646, Test accuracy: 83.82 

Round   5, Global train loss: 1.520, Global test loss: 1.934, Global test accuracy: 55.75 

Round   6, Train loss: 1.538, Test loss: 1.611, Test accuracy: 86.28 

Round   6, Global train loss: 1.538, Global test loss: 1.896, Global test accuracy: 57.19 

Round   7, Train loss: 1.555, Test loss: 1.546, Test accuracy: 93.23 

Round   7, Global train loss: 1.555, Global test loss: 1.819, Global test accuracy: 68.10 

Round   8, Train loss: 1.502, Test loss: 1.543, Test accuracy: 93.43 

Round   8, Global train loss: 1.502, Global test loss: 1.833, Global test accuracy: 63.26 

Round   9, Train loss: 1.513, Test loss: 1.542, Test accuracy: 93.49 

Round   9, Global train loss: 1.513, Global test loss: 1.794, Global test accuracy: 72.58 

Round  10, Train loss: 1.504, Test loss: 1.540, Test accuracy: 93.50 

Round  10, Global train loss: 1.504, Global test loss: 1.760, Global test accuracy: 72.84 

Round  11, Train loss: 1.506, Test loss: 1.539, Test accuracy: 93.65 

Round  11, Global train loss: 1.506, Global test loss: 1.705, Global test accuracy: 78.91 

Round  12, Train loss: 1.496, Test loss: 1.506, Test accuracy: 96.37 

Round  12, Global train loss: 1.496, Global test loss: 1.778, Global test accuracy: 69.91 

Round  13, Train loss: 1.506, Test loss: 1.506, Test accuracy: 96.26 

Round  13, Global train loss: 1.506, Global test loss: 1.698, Global test accuracy: 79.10 

Round  14, Train loss: 1.491, Test loss: 1.505, Test accuracy: 96.30 

Round  14, Global train loss: 1.491, Global test loss: 1.663, Global test accuracy: 83.16 

Round  15, Train loss: 1.491, Test loss: 1.506, Test accuracy: 96.14 

Round  15, Global train loss: 1.491, Global test loss: 1.710, Global test accuracy: 77.04 

Round  16, Train loss: 1.490, Test loss: 1.506, Test accuracy: 96.20 

Round  16, Global train loss: 1.490, Global test loss: 1.667, Global test accuracy: 82.10 

Round  17, Train loss: 1.506, Test loss: 1.503, Test accuracy: 96.38 

Round  17, Global train loss: 1.506, Global test loss: 1.649, Global test accuracy: 84.88 

Round  18, Train loss: 1.495, Test loss: 1.501, Test accuracy: 96.64 

Round  18, Global train loss: 1.495, Global test loss: 1.642, Global test accuracy: 85.33 

Round  19, Train loss: 1.488, Test loss: 1.502, Test accuracy: 96.54 

Round  19, Global train loss: 1.488, Global test loss: 1.682, Global test accuracy: 79.93 

Round  20, Train loss: 1.495, Test loss: 1.501, Test accuracy: 96.61 

Round  20, Global train loss: 1.495, Global test loss: 1.696, Global test accuracy: 77.72 

Round  21, Train loss: 1.488, Test loss: 1.500, Test accuracy: 96.70 

Round  21, Global train loss: 1.488, Global test loss: 1.638, Global test accuracy: 84.62 

Round  22, Train loss: 1.488, Test loss: 1.499, Test accuracy: 96.68 

Round  22, Global train loss: 1.488, Global test loss: 1.662, Global test accuracy: 82.33 

Round  23, Train loss: 1.485, Test loss: 1.499, Test accuracy: 96.69 

Round  23, Global train loss: 1.485, Global test loss: 1.652, Global test accuracy: 82.82 

Round  24, Train loss: 1.489, Test loss: 1.500, Test accuracy: 96.66 

Round  24, Global train loss: 1.489, Global test loss: 1.639, Global test accuracy: 84.54 

Round  25, Train loss: 1.485, Test loss: 1.499, Test accuracy: 96.66 

Round  25, Global train loss: 1.485, Global test loss: 1.637, Global test accuracy: 84.20 

Round  26, Train loss: 1.485, Test loss: 1.500, Test accuracy: 96.53 

Round  26, Global train loss: 1.485, Global test loss: 1.629, Global test accuracy: 84.97 

Round  27, Train loss: 1.488, Test loss: 1.499, Test accuracy: 96.63 

Round  27, Global train loss: 1.488, Global test loss: 1.650, Global test accuracy: 82.80 

Round  28, Train loss: 1.483, Test loss: 1.499, Test accuracy: 96.66 

Round  28, Global train loss: 1.483, Global test loss: 1.627, Global test accuracy: 85.16 

Round  29, Train loss: 1.485, Test loss: 1.499, Test accuracy: 96.64 

Round  29, Global train loss: 1.485, Global test loss: 1.604, Global test accuracy: 87.72 

Round  30, Train loss: 1.483, Test loss: 1.498, Test accuracy: 96.67 

Round  30, Global train loss: 1.483, Global test loss: 1.677, Global test accuracy: 79.06 

Round  31, Train loss: 1.487, Test loss: 1.498, Test accuracy: 96.62 

Round  31, Global train loss: 1.487, Global test loss: 1.622, Global test accuracy: 85.77 

Round  32, Train loss: 1.477, Test loss: 1.498, Test accuracy: 96.59 

Round  32, Global train loss: 1.477, Global test loss: 1.645, Global test accuracy: 82.97 

Round  33, Train loss: 1.481, Test loss: 1.498, Test accuracy: 96.69 

Round  33, Global train loss: 1.481, Global test loss: 1.652, Global test accuracy: 82.27 

Round  34, Train loss: 1.485, Test loss: 1.497, Test accuracy: 96.83 

Round  34, Global train loss: 1.485, Global test loss: 1.608, Global test accuracy: 87.38 

Round  35, Train loss: 1.478, Test loss: 1.497, Test accuracy: 96.79 

Round  35, Global train loss: 1.478, Global test loss: 1.593, Global test accuracy: 88.39 

Round  36, Train loss: 1.479, Test loss: 1.498, Test accuracy: 96.62 

Round  36, Global train loss: 1.479, Global test loss: 1.619, Global test accuracy: 85.84 

Round  37, Train loss: 1.478, Test loss: 1.499, Test accuracy: 96.60 

Round  37, Global train loss: 1.478, Global test loss: 1.607, Global test accuracy: 87.33 

Round  38, Train loss: 1.477, Test loss: 1.498, Test accuracy: 96.58 

Round  38, Global train loss: 1.477, Global test loss: 1.610, Global test accuracy: 86.74 

Round  39, Train loss: 1.476, Test loss: 1.498, Test accuracy: 96.56 

Round  39, Global train loss: 1.476, Global test loss: 1.600, Global test accuracy: 87.50 

Round  40, Train loss: 1.479, Test loss: 1.499, Test accuracy: 96.54 

Round  40, Global train loss: 1.479, Global test loss: 1.622, Global test accuracy: 85.47 

Round  41, Train loss: 1.477, Test loss: 1.499, Test accuracy: 96.53 

Round  41, Global train loss: 1.477, Global test loss: 1.612, Global test accuracy: 86.38 

Round  42, Train loss: 1.478, Test loss: 1.499, Test accuracy: 96.47 

Round  42, Global train loss: 1.478, Global test loss: 1.631, Global test accuracy: 84.51 

Round  43, Train loss: 1.478, Test loss: 1.498, Test accuracy: 96.64 

Round  43, Global train loss: 1.478, Global test loss: 1.587, Global test accuracy: 88.92 

Round  44, Train loss: 1.480, Test loss: 1.497, Test accuracy: 96.71 

Round  44, Global train loss: 1.480, Global test loss: 1.703, Global test accuracy: 76.21 

Round  45, Train loss: 1.478, Test loss: 1.496, Test accuracy: 96.73 

Round  45, Global train loss: 1.478, Global test loss: 1.622, Global test accuracy: 84.83 

Round  46, Train loss: 1.478, Test loss: 1.496, Test accuracy: 96.77 

Round  46, Global train loss: 1.478, Global test loss: 1.615, Global test accuracy: 85.82 

Round  47, Train loss: 1.477, Test loss: 1.497, Test accuracy: 96.80 

Round  47, Global train loss: 1.477, Global test loss: 1.608, Global test accuracy: 86.17 

Round  48, Train loss: 1.476, Test loss: 1.497, Test accuracy: 96.78 

Round  48, Global train loss: 1.476, Global test loss: 1.585, Global test accuracy: 89.00 

Round  49, Train loss: 1.477, Test loss: 1.496, Test accuracy: 96.85 

Round  49, Global train loss: 1.477, Global test loss: 1.602, Global test accuracy: 86.95 

Round  50, Train loss: 1.476, Test loss: 1.497, Test accuracy: 96.80 

Round  50, Global train loss: 1.476, Global test loss: 1.602, Global test accuracy: 87.06 

Round  51, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.83 

Round  51, Global train loss: 1.474, Global test loss: 1.620, Global test accuracy: 84.88 

Round  52, Train loss: 1.478, Test loss: 1.496, Test accuracy: 96.84 

Round  52, Global train loss: 1.478, Global test loss: 1.604, Global test accuracy: 86.98 

Round  53, Train loss: 1.476, Test loss: 1.496, Test accuracy: 96.79 

Round  53, Global train loss: 1.476, Global test loss: 1.619, Global test accuracy: 85.45 

Round  54, Train loss: 1.473, Test loss: 1.496, Test accuracy: 96.85 

Round  54, Global train loss: 1.473, Global test loss: 1.626, Global test accuracy: 84.22 

Round  55, Train loss: 1.475, Test loss: 1.496, Test accuracy: 96.85 

Round  55, Global train loss: 1.475, Global test loss: 1.620, Global test accuracy: 84.83 

Round  56, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.82 

Round  56, Global train loss: 1.474, Global test loss: 1.593, Global test accuracy: 87.68 

Round  57, Train loss: 1.474, Test loss: 1.495, Test accuracy: 96.85 

Round  57, Global train loss: 1.474, Global test loss: 1.601, Global test accuracy: 86.98 

Round  58, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.78 

Round  58, Global train loss: 1.473, Global test loss: 1.579, Global test accuracy: 89.50 

Round  59, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.77 

Round  59, Global train loss: 1.473, Global test loss: 1.611, Global test accuracy: 85.92 

Round  60, Train loss: 1.479, Test loss: 1.495, Test accuracy: 96.87 

Round  60, Global train loss: 1.479, Global test loss: 1.569, Global test accuracy: 90.46 

Round  61, Train loss: 1.476, Test loss: 1.494, Test accuracy: 96.93 

Round  61, Global train loss: 1.476, Global test loss: 1.615, Global test accuracy: 85.29 

Round  62, Train loss: 1.472, Test loss: 1.494, Test accuracy: 96.97 

Round  62, Global train loss: 1.472, Global test loss: 1.611, Global test accuracy: 85.97 

Round  63, Train loss: 1.475, Test loss: 1.495, Test accuracy: 96.92 

Round  63, Global train loss: 1.475, Global test loss: 1.600, Global test accuracy: 87.12 

Round  64, Train loss: 1.476, Test loss: 1.495, Test accuracy: 96.85 

Round  64, Global train loss: 1.476, Global test loss: 1.585, Global test accuracy: 88.37 

Round  65, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.88 

Round  65, Global train loss: 1.470, Global test loss: 1.576, Global test accuracy: 89.55 

Round  66, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.92 

Round  66, Global train loss: 1.471, Global test loss: 1.585, Global test accuracy: 88.71 

Round  67, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.88 

Round  67, Global train loss: 1.473, Global test loss: 1.569, Global test accuracy: 90.17 

Round  68, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.88 

Round  68, Global train loss: 1.471, Global test loss: 1.568, Global test accuracy: 90.33 

Round  69, Train loss: 1.476, Test loss: 1.494, Test accuracy: 97.00 

Round  69, Global train loss: 1.476, Global test loss: 1.594, Global test accuracy: 87.47 

Round  70, Train loss: 1.473, Test loss: 1.494, Test accuracy: 96.99 

Round  70, Global train loss: 1.473, Global test loss: 1.577, Global test accuracy: 89.50 

Round  71, Train loss: 1.471, Test loss: 1.494, Test accuracy: 97.01 

Round  71, Global train loss: 1.471, Global test loss: 1.572, Global test accuracy: 89.71 

Round  72, Train loss: 1.474, Test loss: 1.494, Test accuracy: 96.94 

Round  72, Global train loss: 1.474, Global test loss: 1.562, Global test accuracy: 91.08 

Round  73, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.99 

Round  73, Global train loss: 1.470, Global test loss: 1.579, Global test accuracy: 88.97 

Round  74, Train loss: 1.472, Test loss: 1.494, Test accuracy: 96.96 

Round  74, Global train loss: 1.472, Global test loss: 1.601, Global test accuracy: 87.10 

Round  75, Train loss: 1.472, Test loss: 1.494, Test accuracy: 96.98 

Round  75, Global train loss: 1.472, Global test loss: 1.587, Global test accuracy: 88.51 

Round  76, Train loss: 1.474, Test loss: 1.494, Test accuracy: 97.02 

Round  76, Global train loss: 1.474, Global test loss: 1.591, Global test accuracy: 88.03 

Round  77, Train loss: 1.474, Test loss: 1.493, Test accuracy: 97.04 

Round  77, Global train loss: 1.474, Global test loss: 1.567, Global test accuracy: 90.42 

Round  78, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.07 

Round  78, Global train loss: 1.469, Global test loss: 1.598, Global test accuracy: 87.14 

Round  79, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.06 

Round  79, Global train loss: 1.471, Global test loss: 1.591, Global test accuracy: 87.85 

Round  80, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.12 

Round  80, Global train loss: 1.470, Global test loss: 1.605, Global test accuracy: 86.51 

Round  81, Train loss: 1.473, Test loss: 1.493, Test accuracy: 97.11 

Round  81, Global train loss: 1.473, Global test loss: 1.591, Global test accuracy: 87.92 

Round  82, Train loss: 1.473, Test loss: 1.493, Test accuracy: 97.12 

Round  82, Global train loss: 1.473, Global test loss: 1.586, Global test accuracy: 88.64 

Round  83, Train loss: 1.472, Test loss: 1.493, Test accuracy: 97.12 

Round  83, Global train loss: 1.472, Global test loss: 1.575, Global test accuracy: 89.91 

Round  84, Train loss: 1.474, Test loss: 1.493, Test accuracy: 97.16 

Round  84, Global train loss: 1.474, Global test loss: 1.572, Global test accuracy: 89.97 

Round  85, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.16 

Round  85, Global train loss: 1.470, Global test loss: 1.582, Global test accuracy: 88.60 

Round  86, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.13 

Round  86, Global train loss: 1.471, Global test loss: 1.573, Global test accuracy: 89.45 

Round  87, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.17 

Round  87, Global train loss: 1.471, Global test loss: 1.578, Global test accuracy: 89.47 

Round  88, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.13 

Round  88, Global train loss: 1.470, Global test loss: 1.582, Global test accuracy: 88.85 

Round  89, Train loss: 1.471, Test loss: 1.492, Test accuracy: 97.14 

Round  89, Global train loss: 1.471, Global test loss: 1.573, Global test accuracy: 89.60 

Round  90, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.09 

Round  90, Global train loss: 1.470, Global test loss: 1.580, Global test accuracy: 88.97 

Round  91, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.09 

Round  91, Global train loss: 1.471, Global test loss: 1.589, Global test accuracy: 88.13 

Round  92, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.14 

Round  92, Global train loss: 1.469, Global test loss: 1.568, Global test accuracy: 90.17 

Round  93, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.12 

Round  93, Global train loss: 1.470, Global test loss: 1.559, Global test accuracy: 91.08 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.08 

Round  94, Global train loss: 1.470, Global test loss: 1.579, Global test accuracy: 88.89 

Round  95, Train loss: 1.472, Test loss: 1.493, Test accuracy: 97.09 

Round  95, Global train loss: 1.472, Global test loss: 1.585, Global test accuracy: 88.47 

Round  96, Train loss: 1.470, Test loss: 1.492, Test accuracy: 97.08 

Round  96, Global train loss: 1.470, Global test loss: 1.590, Global test accuracy: 88.09 

Round  97, Train loss: 1.469, Test loss: 1.492, Test accuracy: 97.10 

Round  97, Global train loss: 1.469, Global test loss: 1.604, Global test accuracy: 86.14 

Round  98, Train loss: 1.471, Test loss: 1.492, Test accuracy: 97.12 

Round  98, Global train loss: 1.471, Global test loss: 1.568, Global test accuracy: 90.17 

Round  99, Train loss: 1.472, Test loss: 1.492, Test accuracy: 97.13 

Round  99, Global train loss: 1.472, Global test loss: 1.573, Global test accuracy: 89.78 

Final Round, Train loss: 1.469, Test loss: 1.491, Test accuracy: 97.27 

Final Round, Global train loss: 1.469, Global test loss: 1.573, Global test accuracy: 89.78 

Average accuracy final 10 rounds: 97.10583333333334 

Average global accuracy final 10 rounds: 88.99083333333334 

940.6393694877625
[0.8495545387268066, 1.5989887714385986, 2.353464126586914, 3.1047637462615967, 3.8526265621185303, 4.607360601425171, 5.357455730438232, 6.111208200454712, 6.869455814361572, 7.6169373989105225, 8.368174076080322, 9.12178921699524, 9.873459100723267, 10.624867916107178, 11.380621194839478, 12.128995418548584, 12.881164789199829, 13.632563829421997, 14.379873752593994, 15.129658460617065, 15.872927904129028, 16.614463806152344, 17.369447708129883, 18.10988759994507, 18.852219343185425, 19.59948444366455, 20.336974620819092, 21.08057475090027, 21.83209252357483, 22.578015565872192, 23.328310012817383, 24.07796621322632, 24.822295904159546, 25.57413625717163, 26.329962491989136, 27.07239031791687, 27.820801496505737, 28.56352972984314, 29.30432677268982, 30.05466055870056, 30.797905206680298, 31.50767159461975, 32.23776841163635, 32.941234827041626, 33.654696464538574, 34.37002158164978, 35.089149713516235, 35.8142511844635, 36.56772589683533, 37.31328749656677, 38.0635142326355, 38.81698417663574, 39.56486988067627, 40.30910611152649, 41.064525842666626, 41.81228971481323, 42.55870270729065, 43.31342625617981, 44.05543327331543, 44.80208683013916, 45.560176849365234, 46.30375099182129, 47.046382427215576, 47.80505633354187, 48.54774022102356, 49.3031485080719, 50.06130504608154, 50.80450654029846, 51.46443462371826, 52.12601447105408, 52.76889395713806, 53.41200804710388, 54.05748701095581, 54.69651198387146, 55.333810806274414, 55.99091553688049, 56.635117530822754, 57.27971863746643, 57.93502449989319, 58.58136296272278, 59.224441051483154, 59.87535309791565, 60.52038764953613, 61.15942668914795, 61.81259536743164, 62.451908111572266, 63.094576835632324, 63.7451856136322, 64.38158297538757, 65.01901984214783, 65.67355799674988, 66.31081914901733, 66.95319390296936, 67.60431575775146, 68.26004338264465, 68.90269732475281, 69.5466058254242, 70.19917011260986, 70.83591556549072, 71.48204493522644, 72.78256869316101]
[16.958333333333332, 44.708333333333336, 64.025, 68.63333333333334, 75.51666666666667, 83.81666666666666, 86.275, 93.23333333333333, 93.43333333333334, 93.49166666666666, 93.5, 93.65, 96.36666666666666, 96.25833333333334, 96.3, 96.14166666666667, 96.2, 96.38333333333334, 96.64166666666667, 96.54166666666667, 96.60833333333333, 96.7, 96.68333333333334, 96.69166666666666, 96.65833333333333, 96.65833333333333, 96.53333333333333, 96.63333333333334, 96.65833333333333, 96.64166666666667, 96.675, 96.625, 96.59166666666667, 96.69166666666666, 96.825, 96.79166666666667, 96.625, 96.6, 96.575, 96.55833333333334, 96.54166666666667, 96.525, 96.46666666666667, 96.64166666666667, 96.70833333333333, 96.73333333333333, 96.76666666666667, 96.8, 96.78333333333333, 96.85, 96.8, 96.83333333333333, 96.84166666666667, 96.79166666666667, 96.85, 96.85, 96.81666666666666, 96.85, 96.78333333333333, 96.76666666666667, 96.86666666666666, 96.93333333333334, 96.975, 96.925, 96.85, 96.875, 96.925, 96.875, 96.875, 97.0, 96.99166666666666, 97.00833333333334, 96.94166666666666, 96.99166666666666, 96.95833333333333, 96.98333333333333, 97.01666666666667, 97.04166666666667, 97.06666666666666, 97.05833333333334, 97.11666666666666, 97.10833333333333, 97.11666666666666, 97.11666666666666, 97.15833333333333, 97.15833333333333, 97.13333333333334, 97.16666666666667, 97.13333333333334, 97.14166666666667, 97.09166666666667, 97.09166666666667, 97.14166666666667, 97.11666666666666, 97.08333333333333, 97.09166666666667, 97.08333333333333, 97.1, 97.125, 97.13333333333334, 97.26666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.302, Test accuracy: 7.92 

Round   1, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.13 

Round   2, Train loss: 2.301, Test loss: 2.300, Test accuracy: 15.88 

Round   3, Train loss: 2.299, Test loss: 2.299, Test accuracy: 20.87 

Round   4, Train loss: 2.298, Test loss: 2.297, Test accuracy: 26.80 

Round   5, Train loss: 2.296, Test loss: 2.295, Test accuracy: 31.08 

Round   6, Train loss: 2.294, Test loss: 2.293, Test accuracy: 34.98 

Round   7, Train loss: 2.292, Test loss: 2.290, Test accuracy: 37.00 

Round   8, Train loss: 2.288, Test loss: 2.285, Test accuracy: 37.48 

Round   9, Train loss: 2.281, Test loss: 2.277, Test accuracy: 33.80 

Round  10, Train loss: 2.268, Test loss: 2.261, Test accuracy: 27.70 

Round  11, Train loss: 2.253, Test loss: 2.238, Test accuracy: 30.10 

Round  12, Train loss: 2.215, Test loss: 2.205, Test accuracy: 32.28 

Round  13, Train loss: 2.170, Test loss: 2.159, Test accuracy: 37.42 

Round  14, Train loss: 2.128, Test loss: 2.117, Test accuracy: 42.47 

Round  15, Train loss: 2.091, Test loss: 2.076, Test accuracy: 44.67 

Round  16, Train loss: 2.035, Test loss: 2.035, Test accuracy: 46.80 

Round  17, Train loss: 2.014, Test loss: 1.995, Test accuracy: 50.87 

Round  18, Train loss: 1.963, Test loss: 1.954, Test accuracy: 55.48 

Round  19, Train loss: 1.938, Test loss: 1.918, Test accuracy: 58.63 

Round  20, Train loss: 1.889, Test loss: 1.886, Test accuracy: 62.55 

Round  21, Train loss: 1.851, Test loss: 1.854, Test accuracy: 65.88 

Round  22, Train loss: 1.810, Test loss: 1.821, Test accuracy: 68.70 

Round  23, Train loss: 1.789, Test loss: 1.789, Test accuracy: 71.73 

Round  24, Train loss: 1.761, Test loss: 1.764, Test accuracy: 73.83 

Round  25, Train loss: 1.733, Test loss: 1.742, Test accuracy: 76.12 

Round  26, Train loss: 1.715, Test loss: 1.730, Test accuracy: 76.78 

Round  27, Train loss: 1.704, Test loss: 1.712, Test accuracy: 78.70 

Round  28, Train loss: 1.686, Test loss: 1.700, Test accuracy: 79.37 

Round  29, Train loss: 1.673, Test loss: 1.692, Test accuracy: 79.83 

Round  30, Train loss: 1.685, Test loss: 1.686, Test accuracy: 80.12 

Round  31, Train loss: 1.657, Test loss: 1.681, Test accuracy: 80.73 

Round  32, Train loss: 1.649, Test loss: 1.675, Test accuracy: 80.90 

Round  33, Train loss: 1.658, Test loss: 1.670, Test accuracy: 81.22 

Round  34, Train loss: 1.636, Test loss: 1.667, Test accuracy: 81.32 

Round  35, Train loss: 1.648, Test loss: 1.666, Test accuracy: 81.43 

Round  36, Train loss: 1.635, Test loss: 1.662, Test accuracy: 81.60 

Round  37, Train loss: 1.640, Test loss: 1.663, Test accuracy: 81.43 

Round  38, Train loss: 1.637, Test loss: 1.660, Test accuracy: 81.70 

Round  39, Train loss: 1.639, Test loss: 1.656, Test accuracy: 82.05 

Round  40, Train loss: 1.631, Test loss: 1.652, Test accuracy: 82.37 

Round  41, Train loss: 1.617, Test loss: 1.651, Test accuracy: 82.25 

Round  42, Train loss: 1.637, Test loss: 1.649, Test accuracy: 82.27 

Round  43, Train loss: 1.608, Test loss: 1.648, Test accuracy: 82.28 

Round  44, Train loss: 1.618, Test loss: 1.646, Test accuracy: 82.37 

Round  45, Train loss: 1.604, Test loss: 1.645, Test accuracy: 82.38 

Round  46, Train loss: 1.625, Test loss: 1.645, Test accuracy: 82.47 

Round  47, Train loss: 1.604, Test loss: 1.645, Test accuracy: 82.53 

Round  48, Train loss: 1.620, Test loss: 1.644, Test accuracy: 82.73 

Round  49, Train loss: 1.614, Test loss: 1.643, Test accuracy: 82.77 

Round  50, Train loss: 1.616, Test loss: 1.642, Test accuracy: 82.92 

Round  51, Train loss: 1.607, Test loss: 1.642, Test accuracy: 82.88 

Round  52, Train loss: 1.589, Test loss: 1.641, Test accuracy: 83.07 

Round  53, Train loss: 1.622, Test loss: 1.640, Test accuracy: 83.03 

Round  54, Train loss: 1.601, Test loss: 1.640, Test accuracy: 83.05 

Round  55, Train loss: 1.604, Test loss: 1.640, Test accuracy: 82.80 

Round  56, Train loss: 1.594, Test loss: 1.639, Test accuracy: 83.12 

Round  57, Train loss: 1.622, Test loss: 1.639, Test accuracy: 82.93 

Round  58, Train loss: 1.595, Test loss: 1.638, Test accuracy: 82.95 

Round  59, Train loss: 1.591, Test loss: 1.637, Test accuracy: 82.95 

Round  60, Train loss: 1.600, Test loss: 1.637, Test accuracy: 83.18 

Round  61, Train loss: 1.598, Test loss: 1.637, Test accuracy: 83.00 

Round  62, Train loss: 1.598, Test loss: 1.636, Test accuracy: 83.08 

Round  63, Train loss: 1.588, Test loss: 1.638, Test accuracy: 82.93 

Round  64, Train loss: 1.596, Test loss: 1.638, Test accuracy: 82.92 

Round  65, Train loss: 1.593, Test loss: 1.637, Test accuracy: 82.88 

Round  66, Train loss: 1.600, Test loss: 1.636, Test accuracy: 83.02 

Round  67, Train loss: 1.569, Test loss: 1.636, Test accuracy: 82.97 

Round  68, Train loss: 1.591, Test loss: 1.635, Test accuracy: 83.18 

Round  69, Train loss: 1.578, Test loss: 1.632, Test accuracy: 83.47 

Round  70, Train loss: 1.573, Test loss: 1.631, Test accuracy: 83.52 

Round  71, Train loss: 1.597, Test loss: 1.630, Test accuracy: 83.60 

Round  72, Train loss: 1.587, Test loss: 1.630, Test accuracy: 83.62 

Round  73, Train loss: 1.577, Test loss: 1.629, Test accuracy: 83.77 

Round  74, Train loss: 1.591, Test loss: 1.629, Test accuracy: 83.83 

Round  75, Train loss: 1.585, Test loss: 1.630, Test accuracy: 83.63 

Round  76, Train loss: 1.553, Test loss: 1.630, Test accuracy: 83.63 

Round  77, Train loss: 1.552, Test loss: 1.630, Test accuracy: 83.47 

Round  78, Train loss: 1.589, Test loss: 1.630, Test accuracy: 83.40 

Round  79, Train loss: 1.593, Test loss: 1.629, Test accuracy: 83.47 

Round  80, Train loss: 1.577, Test loss: 1.628, Test accuracy: 83.62 

Round  81, Train loss: 1.578, Test loss: 1.628, Test accuracy: 83.53 

Round  82, Train loss: 1.568, Test loss: 1.627, Test accuracy: 84.00 

Round  83, Train loss: 1.590, Test loss: 1.627, Test accuracy: 84.05 

Round  84, Train loss: 1.590, Test loss: 1.627, Test accuracy: 83.93 

Round  85, Train loss: 1.574, Test loss: 1.627, Test accuracy: 83.88 

Round  86, Train loss: 1.559, Test loss: 1.626, Test accuracy: 83.87 

Round  87, Train loss: 1.589, Test loss: 1.626, Test accuracy: 83.95 

Round  88, Train loss: 1.577, Test loss: 1.626, Test accuracy: 83.80 

Round  89, Train loss: 1.578, Test loss: 1.625, Test accuracy: 84.15 

Round  90, Train loss: 1.598, Test loss: 1.625, Test accuracy: 84.00 

Round  91, Train loss: 1.550, Test loss: 1.625, Test accuracy: 84.10 

Round  92, Train loss: 1.576, Test loss: 1.624, Test accuracy: 84.33 

Round  93, Train loss: 1.579, Test loss: 1.624, Test accuracy: 84.27 

Round  94, Train loss: 1.559, Test loss: 1.623, Test accuracy: 84.27 

Round  95, Train loss: 1.594, Test loss: 1.623, Test accuracy: 84.38 

Round  96, Train loss: 1.573, Test loss: 1.622, Test accuracy: 84.35 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 1.569, Test loss: 1.622, Test accuracy: 84.25 

Round  98, Train loss: 1.550, Test loss: 1.622, Test accuracy: 84.33 

Round  99, Train loss: 1.582, Test loss: 1.622, Test accuracy: 84.22 

Final Round, Train loss: 1.570, Test loss: 1.621, Test accuracy: 84.52 

Average accuracy final 10 rounds: 84.25 

362.12333941459656
[0.487257719039917, 0.8807661533355713, 1.2864437103271484, 1.6951119899749756, 2.1076161861419678, 2.5192348957061768, 2.9374043941497803, 3.3588149547576904, 3.7692418098449707, 4.177155494689941, 4.585704803466797, 4.994160413742065, 5.407101392745972, 5.820838689804077, 6.2397754192352295, 6.646289587020874, 7.059704542160034, 7.464690208435059, 7.838522434234619, 8.205855131149292, 8.581127405166626, 8.959025144577026, 9.335022211074829, 9.705747842788696, 10.072954893112183, 10.439764499664307, 10.806702613830566, 11.179539442062378, 11.552603006362915, 11.928205490112305, 12.300095319747925, 12.666860342025757, 13.038381099700928, 13.4075186252594, 13.775705337524414, 14.14623475074768, 14.517895460128784, 14.892140865325928, 15.2618989944458, 15.632987976074219, 16.00127625465393, 16.370402574539185, 16.73993945121765, 17.115269899368286, 17.490248680114746, 17.865710496902466, 18.23459553718567, 18.601896286010742, 18.973117351531982, 19.342201232910156, 19.7140371799469, 20.088976860046387, 20.46270179748535, 20.834066152572632, 21.204919576644897, 21.57714557647705, 21.945763111114502, 22.315165042877197, 22.685413360595703, 23.04484486579895, 23.415126085281372, 23.781002521514893, 24.15441060066223, 24.522000789642334, 24.889681339263916, 25.25770378112793, 25.62743377685547, 25.997681856155396, 26.369651317596436, 26.738161325454712, 27.106258630752563, 27.475406646728516, 27.846681594848633, 28.217375993728638, 28.58841371536255, 28.960510730743408, 29.327869415283203, 29.696969509124756, 30.069279193878174, 30.438191413879395, 30.805190801620483, 31.17475962638855, 31.543932914733887, 31.914803504943848, 32.28459358215332, 32.65467953681946, 33.02501606941223, 33.39382839202881, 33.76118445396423, 34.12858772277832, 34.50068664550781, 34.8686957359314, 35.24222683906555, 35.609458208084106, 35.98339557647705, 36.35342812538147, 36.72155261039734, 37.095008850097656, 37.46636176109314, 37.84040975570679, 38.55731248855591]
[7.916666666666667, 12.133333333333333, 15.883333333333333, 20.866666666666667, 26.8, 31.083333333333332, 34.983333333333334, 37.0, 37.483333333333334, 33.8, 27.7, 30.1, 32.28333333333333, 37.416666666666664, 42.46666666666667, 44.666666666666664, 46.8, 50.86666666666667, 55.483333333333334, 58.63333333333333, 62.55, 65.88333333333334, 68.7, 71.73333333333333, 73.83333333333333, 76.11666666666666, 76.78333333333333, 78.7, 79.36666666666666, 79.83333333333333, 80.11666666666666, 80.73333333333333, 80.9, 81.21666666666667, 81.31666666666666, 81.43333333333334, 81.6, 81.43333333333334, 81.7, 82.05, 82.36666666666666, 82.25, 82.26666666666667, 82.28333333333333, 82.36666666666666, 82.38333333333334, 82.46666666666667, 82.53333333333333, 82.73333333333333, 82.76666666666667, 82.91666666666667, 82.88333333333334, 83.06666666666666, 83.03333333333333, 83.05, 82.8, 83.11666666666666, 82.93333333333334, 82.95, 82.95, 83.18333333333334, 83.0, 83.08333333333333, 82.93333333333334, 82.91666666666667, 82.88333333333334, 83.01666666666667, 82.96666666666667, 83.18333333333334, 83.46666666666667, 83.51666666666667, 83.6, 83.61666666666666, 83.76666666666667, 83.83333333333333, 83.63333333333334, 83.63333333333334, 83.46666666666667, 83.4, 83.46666666666667, 83.61666666666666, 83.53333333333333, 84.0, 84.05, 83.93333333333334, 83.88333333333334, 83.86666666666666, 83.95, 83.8, 84.15, 84.0, 84.1, 84.33333333333333, 84.26666666666667, 84.26666666666667, 84.38333333333334, 84.35, 84.25, 84.33333333333333, 84.21666666666667, 84.51666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.300, Test accuracy: 18.80 

Round   1, Train loss: 2.299, Test loss: 2.298, Test accuracy: 30.00 

Round   2, Train loss: 2.296, Test loss: 2.294, Test accuracy: 44.45 

Round   3, Train loss: 2.291, Test loss: 2.289, Test accuracy: 50.58 

Round   4, Train loss: 2.283, Test loss: 2.280, Test accuracy: 52.38 

Round   5, Train loss: 2.264, Test loss: 2.251, Test accuracy: 46.17 

Round   6, Train loss: 2.186, Test loss: 2.175, Test accuracy: 46.83 

Round   7, Train loss: 2.093, Test loss: 2.068, Test accuracy: 48.38 

Round   8, Train loss: 1.979, Test loss: 1.982, Test accuracy: 54.12 

Round   9, Train loss: 1.913, Test loss: 1.920, Test accuracy: 59.23 

Round  10, Train loss: 1.856, Test loss: 1.875, Test accuracy: 62.22 

Round  11, Train loss: 1.817, Test loss: 1.829, Test accuracy: 66.78 

Round  12, Train loss: 1.788, Test loss: 1.798, Test accuracy: 69.48 

Round  13, Train loss: 1.759, Test loss: 1.780, Test accuracy: 70.98 

Round  14, Train loss: 1.743, Test loss: 1.761, Test accuracy: 72.47 

Round  15, Train loss: 1.737, Test loss: 1.748, Test accuracy: 73.35 

Round  16, Train loss: 1.717, Test loss: 1.744, Test accuracy: 73.58 

Round  17, Train loss: 1.725, Test loss: 1.737, Test accuracy: 73.95 

Round  18, Train loss: 1.708, Test loss: 1.735, Test accuracy: 73.95 

Round  19, Train loss: 1.707, Test loss: 1.731, Test accuracy: 74.07 

Round  20, Train loss: 1.709, Test loss: 1.726, Test accuracy: 74.43 

Round  21, Train loss: 1.697, Test loss: 1.724, Test accuracy: 74.65 

Round  22, Train loss: 1.689, Test loss: 1.723, Test accuracy: 74.62 

Round  23, Train loss: 1.695, Test loss: 1.721, Test accuracy: 74.92 

Round  24, Train loss: 1.683, Test loss: 1.720, Test accuracy: 74.85 

Round  25, Train loss: 1.680, Test loss: 1.719, Test accuracy: 74.73 

Round  26, Train loss: 1.687, Test loss: 1.718, Test accuracy: 74.80 

Round  27, Train loss: 1.692, Test loss: 1.718, Test accuracy: 74.80 

Round  28, Train loss: 1.699, Test loss: 1.718, Test accuracy: 74.82 

Round  29, Train loss: 1.688, Test loss: 1.716, Test accuracy: 74.97 

Round  30, Train loss: 1.675, Test loss: 1.715, Test accuracy: 74.80 

Round  31, Train loss: 1.675, Test loss: 1.715, Test accuracy: 75.05 

Round  32, Train loss: 1.674, Test loss: 1.716, Test accuracy: 74.90 

Round  33, Train loss: 1.689, Test loss: 1.714, Test accuracy: 75.02 

Round  34, Train loss: 1.671, Test loss: 1.712, Test accuracy: 75.37 

Round  35, Train loss: 1.690, Test loss: 1.712, Test accuracy: 75.40 

Round  36, Train loss: 1.682, Test loss: 1.712, Test accuracy: 75.33 

Round  37, Train loss: 1.687, Test loss: 1.711, Test accuracy: 75.50 

Round  38, Train loss: 1.679, Test loss: 1.711, Test accuracy: 75.45 

Round  39, Train loss: 1.668, Test loss: 1.709, Test accuracy: 75.68 

Round  40, Train loss: 1.675, Test loss: 1.708, Test accuracy: 75.73 

Round  41, Train loss: 1.668, Test loss: 1.707, Test accuracy: 75.82 

Round  42, Train loss: 1.667, Test loss: 1.707, Test accuracy: 75.83 

Round  43, Train loss: 1.652, Test loss: 1.706, Test accuracy: 76.05 

Round  44, Train loss: 1.668, Test loss: 1.705, Test accuracy: 76.15 

Round  45, Train loss: 1.669, Test loss: 1.706, Test accuracy: 75.90 

Round  46, Train loss: 1.657, Test loss: 1.705, Test accuracy: 76.05 

Round  47, Train loss: 1.629, Test loss: 1.704, Test accuracy: 76.13 

Round  48, Train loss: 1.661, Test loss: 1.701, Test accuracy: 76.60 

Round  49, Train loss: 1.637, Test loss: 1.700, Test accuracy: 76.67 

Round  50, Train loss: 1.674, Test loss: 1.700, Test accuracy: 76.70 

Round  51, Train loss: 1.643, Test loss: 1.699, Test accuracy: 76.72 

Round  52, Train loss: 1.653, Test loss: 1.694, Test accuracy: 77.13 

Round  53, Train loss: 1.640, Test loss: 1.692, Test accuracy: 77.42 

Round  54, Train loss: 1.643, Test loss: 1.690, Test accuracy: 77.82 

Round  55, Train loss: 1.628, Test loss: 1.688, Test accuracy: 77.90 

Round  56, Train loss: 1.623, Test loss: 1.684, Test accuracy: 78.27 

Round  57, Train loss: 1.603, Test loss: 1.680, Test accuracy: 78.60 

Round  58, Train loss: 1.647, Test loss: 1.679, Test accuracy: 78.63 

Round  59, Train loss: 1.612, Test loss: 1.675, Test accuracy: 79.15 

Round  60, Train loss: 1.607, Test loss: 1.672, Test accuracy: 79.33 

Round  61, Train loss: 1.619, Test loss: 1.668, Test accuracy: 79.70 

Round  62, Train loss: 1.610, Test loss: 1.664, Test accuracy: 80.27 

Round  63, Train loss: 1.593, Test loss: 1.662, Test accuracy: 80.42 

Round  64, Train loss: 1.602, Test loss: 1.659, Test accuracy: 80.80 

Round  65, Train loss: 1.567, Test loss: 1.655, Test accuracy: 81.10 

Round  66, Train loss: 1.593, Test loss: 1.654, Test accuracy: 81.27 

Round  67, Train loss: 1.581, Test loss: 1.649, Test accuracy: 81.72 

Round  68, Train loss: 1.583, Test loss: 1.646, Test accuracy: 81.97 

Round  69, Train loss: 1.585, Test loss: 1.646, Test accuracy: 81.95 

Round  70, Train loss: 1.600, Test loss: 1.638, Test accuracy: 82.78 

Round  71, Train loss: 1.562, Test loss: 1.637, Test accuracy: 83.13 

Round  72, Train loss: 1.579, Test loss: 1.631, Test accuracy: 83.73 

Round  73, Train loss: 1.566, Test loss: 1.629, Test accuracy: 83.85 

Round  74, Train loss: 1.546, Test loss: 1.627, Test accuracy: 83.97 

Round  75, Train loss: 1.573, Test loss: 1.619, Test accuracy: 84.82 

Round  76, Train loss: 1.567, Test loss: 1.615, Test accuracy: 85.35 

Round  77, Train loss: 1.524, Test loss: 1.611, Test accuracy: 85.63 

Round  78, Train loss: 1.555, Test loss: 1.604, Test accuracy: 86.35 

Round  79, Train loss: 1.524, Test loss: 1.601, Test accuracy: 86.57 

Round  80, Train loss: 1.533, Test loss: 1.598, Test accuracy: 86.87 

Round  81, Train loss: 1.526, Test loss: 1.597, Test accuracy: 86.87 

Round  82, Train loss: 1.529, Test loss: 1.597, Test accuracy: 86.75 

Round  83, Train loss: 1.550, Test loss: 1.590, Test accuracy: 87.70 

Round  84, Train loss: 1.504, Test loss: 1.589, Test accuracy: 87.73 

Round  85, Train loss: 1.531, Test loss: 1.580, Test accuracy: 88.67 

Round  86, Train loss: 1.512, Test loss: 1.576, Test accuracy: 89.38 

Round  87, Train loss: 1.494, Test loss: 1.574, Test accuracy: 89.33 

Round  88, Train loss: 1.516, Test loss: 1.572, Test accuracy: 89.35 

Round  89, Train loss: 1.506, Test loss: 1.570, Test accuracy: 89.58 

Round  90, Train loss: 1.513, Test loss: 1.570, Test accuracy: 89.72 

Round  91, Train loss: 1.524, Test loss: 1.570, Test accuracy: 89.63 

Round  92, Train loss: 1.496, Test loss: 1.569, Test accuracy: 89.72 

Round  93, Train loss: 1.513, Test loss: 1.568, Test accuracy: 89.75 

Round  94, Train loss: 1.510, Test loss: 1.568, Test accuracy: 89.77 

Round  95, Train loss: 1.495, Test loss: 1.566, Test accuracy: 89.98 

Round  96, Train loss: 1.486, Test loss: 1.566, Test accuracy: 90.15 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 1.498, Test loss: 1.565, Test accuracy: 90.08 

Round  98, Train loss: 1.494, Test loss: 1.565, Test accuracy: 90.05 

Round  99, Train loss: 1.494, Test loss: 1.565, Test accuracy: 90.07 

Final Round, Train loss: 1.500, Test loss: 1.566, Test accuracy: 89.92 

Average accuracy final 10 rounds: 89.89166666666665 

407.7642397880554
[0.5409283638000488, 0.9951682090759277, 1.4496374130249023, 1.9033453464508057, 2.3562018871307373, 2.806838274002075, 3.26350736618042, 3.720792055130005, 4.171487808227539, 4.621882200241089, 5.0731751918792725, 5.525545358657837, 5.979192495346069, 6.434047222137451, 6.8924572467803955, 7.34738564491272, 7.79965353012085, 8.25204086303711, 8.703572750091553, 9.159320831298828, 9.615415334701538, 10.067798614501953, 10.520601511001587, 10.97398591041565, 11.42657208442688, 11.878801584243774, 12.335839033126831, 12.790773868560791, 13.244305849075317, 13.696988582611084, 14.150573015213013, 14.605364561080933, 15.05793309211731, 15.519176721572876, 15.976807832717896, 16.428571224212646, 16.880488395690918, 17.333880186080933, 17.78596782684326, 18.24272394180298, 18.698538303375244, 19.151719570159912, 19.60590934753418, 20.05986785888672, 20.51058340072632, 20.963435173034668, 21.421310901641846, 21.88039255142212, 22.32953953742981, 22.782142877578735, 23.232200622558594, 23.681597232818604, 24.13169765472412, 24.587992429733276, 25.042754411697388, 25.494540691375732, 25.944847106933594, 26.398128271102905, 26.8487389087677, 27.302436351776123, 27.760578393936157, 28.218326330184937, 28.67420768737793, 29.127081632614136, 29.577536582946777, 30.030967473983765, 30.485533237457275, 30.943575859069824, 31.400180339813232, 31.851250886917114, 32.306663036346436, 32.761958837509155, 33.21228551864624, 33.668293952941895, 34.126468896865845, 34.58388710021973, 35.034740924835205, 35.48683047294617, 35.93685483932495, 36.388373136520386, 36.84572911262512, 37.30368518829346, 37.75614619255066, 38.206968784332275, 38.658958435058594, 39.11135506629944, 39.56113338470459, 40.01998686790466, 40.48167634010315, 40.934945821762085, 41.388474464416504, 41.83856439590454, 42.29126334190369, 42.74170732498169, 43.197608947753906, 43.65294671058655, 44.10233116149902, 44.55244588851929, 45.00233054161072, 45.451380014419556, 46.1934757232666]
[18.8, 30.0, 44.45, 50.583333333333336, 52.38333333333333, 46.166666666666664, 46.833333333333336, 48.38333333333333, 54.11666666666667, 59.233333333333334, 62.21666666666667, 66.78333333333333, 69.48333333333333, 70.98333333333333, 72.46666666666667, 73.35, 73.58333333333333, 73.95, 73.95, 74.06666666666666, 74.43333333333334, 74.65, 74.61666666666666, 74.91666666666667, 74.85, 74.73333333333333, 74.8, 74.8, 74.81666666666666, 74.96666666666667, 74.8, 75.05, 74.9, 75.01666666666667, 75.36666666666666, 75.4, 75.33333333333333, 75.5, 75.45, 75.68333333333334, 75.73333333333333, 75.81666666666666, 75.83333333333333, 76.05, 76.15, 75.9, 76.05, 76.13333333333334, 76.6, 76.66666666666667, 76.7, 76.71666666666667, 77.13333333333334, 77.41666666666667, 77.81666666666666, 77.9, 78.26666666666667, 78.6, 78.63333333333334, 79.15, 79.33333333333333, 79.7, 80.26666666666667, 80.41666666666667, 80.8, 81.1, 81.26666666666667, 81.71666666666667, 81.96666666666667, 81.95, 82.78333333333333, 83.13333333333334, 83.73333333333333, 83.85, 83.96666666666667, 84.81666666666666, 85.35, 85.63333333333334, 86.35, 86.56666666666666, 86.86666666666666, 86.86666666666666, 86.75, 87.7, 87.73333333333333, 88.66666666666667, 89.38333333333334, 89.33333333333333, 89.35, 89.58333333333333, 89.71666666666667, 89.63333333333334, 89.71666666666667, 89.75, 89.76666666666667, 89.98333333333333, 90.15, 90.08333333333333, 90.05, 90.06666666666666, 89.91666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 17.32 

Round   1, Train loss: 2.300, Test loss: 2.300, Test accuracy: 22.92 

Round   2, Train loss: 2.298, Test loss: 2.298, Test accuracy: 30.78 

Round   3, Train loss: 2.299, Test loss: 2.297, Test accuracy: 31.62 

Round   4, Train loss: 2.296, Test loss: 2.295, Test accuracy: 41.18 

Round   5, Train loss: 2.293, Test loss: 2.293, Test accuracy: 42.57 

Round   6, Train loss: 2.288, Test loss: 2.289, Test accuracy: 43.43 

Round   7, Train loss: 2.285, Test loss: 2.285, Test accuracy: 45.58 

Round   8, Train loss: 2.264, Test loss: 2.271, Test accuracy: 44.37 

Round   9, Train loss: 2.213, Test loss: 2.236, Test accuracy: 42.85 

Round  10, Train loss: 2.215, Test loss: 2.194, Test accuracy: 41.73 

Round  11, Train loss: 2.106, Test loss: 2.132, Test accuracy: 45.35 

Round  12, Train loss: 2.108, Test loss: 2.076, Test accuracy: 48.42 

Round  13, Train loss: 1.983, Test loss: 2.012, Test accuracy: 56.38 

Round  14, Train loss: 1.900, Test loss: 1.960, Test accuracy: 61.77 

Round  15, Train loss: 1.852, Test loss: 1.907, Test accuracy: 64.48 

Round  16, Train loss: 1.834, Test loss: 1.858, Test accuracy: 67.43 

Round  17, Train loss: 1.740, Test loss: 1.814, Test accuracy: 72.08 

Round  18, Train loss: 1.713, Test loss: 1.775, Test accuracy: 74.88 

Round  19, Train loss: 1.665, Test loss: 1.751, Test accuracy: 76.02 

Round  20, Train loss: 1.641, Test loss: 1.734, Test accuracy: 76.67 

Round  21, Train loss: 1.614, Test loss: 1.726, Test accuracy: 76.93 

Round  22, Train loss: 1.627, Test loss: 1.719, Test accuracy: 77.18 

Round  23, Train loss: 1.623, Test loss: 1.704, Test accuracy: 78.32 

Round  24, Train loss: 1.614, Test loss: 1.697, Test accuracy: 78.70 

Round  25, Train loss: 1.597, Test loss: 1.695, Test accuracy: 78.62 

Round  26, Train loss: 1.580, Test loss: 1.693, Test accuracy: 78.62 

Round  27, Train loss: 1.605, Test loss: 1.690, Test accuracy: 78.57 

Round  28, Train loss: 1.584, Test loss: 1.683, Test accuracy: 80.62 

Round  29, Train loss: 1.559, Test loss: 1.675, Test accuracy: 81.78 

Round  30, Train loss: 1.527, Test loss: 1.674, Test accuracy: 81.48 

Round  31, Train loss: 1.529, Test loss: 1.667, Test accuracy: 81.98 

Round  32, Train loss: 1.513, Test loss: 1.660, Test accuracy: 82.58 

Round  33, Train loss: 1.508, Test loss: 1.659, Test accuracy: 82.23 

Round  34, Train loss: 1.515, Test loss: 1.655, Test accuracy: 82.30 

Round  35, Train loss: 1.516, Test loss: 1.652, Test accuracy: 82.55 

Round  36, Train loss: 1.500, Test loss: 1.650, Test accuracy: 82.58 

Round  37, Train loss: 1.505, Test loss: 1.646, Test accuracy: 82.90 

Round  38, Train loss: 1.499, Test loss: 1.646, Test accuracy: 82.95 

Round  39, Train loss: 1.492, Test loss: 1.645, Test accuracy: 82.93 

Round  40, Train loss: 1.498, Test loss: 1.645, Test accuracy: 82.87 

Round  41, Train loss: 1.488, Test loss: 1.644, Test accuracy: 82.92 

Round  42, Train loss: 1.491, Test loss: 1.643, Test accuracy: 82.90 

Round  43, Train loss: 1.494, Test loss: 1.643, Test accuracy: 82.88 

Round  44, Train loss: 1.490, Test loss: 1.642, Test accuracy: 83.02 

Round  45, Train loss: 1.487, Test loss: 1.642, Test accuracy: 83.02 

Round  46, Train loss: 1.486, Test loss: 1.641, Test accuracy: 83.03 

Round  47, Train loss: 1.490, Test loss: 1.641, Test accuracy: 82.90 

Round  48, Train loss: 1.480, Test loss: 1.640, Test accuracy: 83.00 

Round  49, Train loss: 1.490, Test loss: 1.640, Test accuracy: 82.93 

Round  50, Train loss: 1.485, Test loss: 1.639, Test accuracy: 82.92 

Round  51, Train loss: 1.493, Test loss: 1.640, Test accuracy: 82.87 

Round  52, Train loss: 1.485, Test loss: 1.640, Test accuracy: 82.98 

Round  53, Train loss: 1.483, Test loss: 1.639, Test accuracy: 82.95 

Round  54, Train loss: 1.491, Test loss: 1.639, Test accuracy: 83.00 

Round  55, Train loss: 1.480, Test loss: 1.639, Test accuracy: 83.05 

Round  56, Train loss: 1.482, Test loss: 1.638, Test accuracy: 82.97 

Round  57, Train loss: 1.481, Test loss: 1.638, Test accuracy: 83.05 

Round  58, Train loss: 1.481, Test loss: 1.638, Test accuracy: 83.05 

Round  59, Train loss: 1.486, Test loss: 1.638, Test accuracy: 83.13 

Round  60, Train loss: 1.478, Test loss: 1.638, Test accuracy: 83.13 

Round  61, Train loss: 1.492, Test loss: 1.638, Test accuracy: 83.03 

Round  62, Train loss: 1.489, Test loss: 1.638, Test accuracy: 83.03 

Round  63, Train loss: 1.487, Test loss: 1.638, Test accuracy: 83.10 

Round  64, Train loss: 1.482, Test loss: 1.638, Test accuracy: 83.05 

Round  65, Train loss: 1.487, Test loss: 1.638, Test accuracy: 83.00 

Round  66, Train loss: 1.484, Test loss: 1.638, Test accuracy: 82.95 

Round  67, Train loss: 1.476, Test loss: 1.638, Test accuracy: 82.98 

Round  68, Train loss: 1.476, Test loss: 1.637, Test accuracy: 82.98 

Round  69, Train loss: 1.485, Test loss: 1.637, Test accuracy: 83.00 

Round  70, Train loss: 1.479, Test loss: 1.637, Test accuracy: 83.12 

Round  71, Train loss: 1.483, Test loss: 1.637, Test accuracy: 83.00 

Round  72, Train loss: 1.486, Test loss: 1.637, Test accuracy: 83.03 

Round  73, Train loss: 1.485, Test loss: 1.637, Test accuracy: 83.03 

Round  74, Train loss: 1.483, Test loss: 1.637, Test accuracy: 83.12 

Round  75, Train loss: 1.479, Test loss: 1.637, Test accuracy: 83.10 

Round  76, Train loss: 1.482, Test loss: 1.637, Test accuracy: 83.13 

Round  77, Train loss: 1.482, Test loss: 1.636, Test accuracy: 83.12 

Round  78, Train loss: 1.481, Test loss: 1.636, Test accuracy: 83.12 

Round  79, Train loss: 1.483, Test loss: 1.636, Test accuracy: 83.15 

Round  80, Train loss: 1.486, Test loss: 1.636, Test accuracy: 83.20 

Round  81, Train loss: 1.480, Test loss: 1.636, Test accuracy: 83.18 

Round  82, Train loss: 1.481, Test loss: 1.636, Test accuracy: 83.17 

Round  83, Train loss: 1.482, Test loss: 1.636, Test accuracy: 83.20 

Round  84, Train loss: 1.483, Test loss: 1.636, Test accuracy: 83.17 

Round  85, Train loss: 1.488, Test loss: 1.636, Test accuracy: 83.18 

Round  86, Train loss: 1.489, Test loss: 1.636, Test accuracy: 83.17 

Round  87, Train loss: 1.481, Test loss: 1.636, Test accuracy: 83.08 

Round  88, Train loss: 1.484, Test loss: 1.636, Test accuracy: 83.10 

Round  89, Train loss: 1.479, Test loss: 1.636, Test accuracy: 83.08 

Round  90, Train loss: 1.480, Test loss: 1.636, Test accuracy: 83.08 

Round  91, Train loss: 1.482, Test loss: 1.635, Test accuracy: 83.08 

Round  92, Train loss: 1.485, Test loss: 1.635, Test accuracy: 83.15 

Round  93, Train loss: 1.483, Test loss: 1.635, Test accuracy: 83.15 

Round  94, Train loss: 1.477, Test loss: 1.635, Test accuracy: 83.15 

Round  95, Train loss: 1.484, Test loss: 1.635, Test accuracy: 83.12 

Round  96, Train loss: 1.482, Test loss: 1.635, Test accuracy: 83.15 

Round  97, Train loss: 1.481, Test loss: 1.635, Test accuracy: 83.20 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 1.482, Test loss: 1.635, Test accuracy: 83.18 

Round  99, Train loss: 1.480, Test loss: 1.635, Test accuracy: 83.20 

Final Round, Train loss: 1.481, Test loss: 1.635, Test accuracy: 83.20 

Average accuracy final 10 rounds: 83.14666666666665 

410.8892707824707
[0.5448095798492432, 0.9981906414031982, 1.4524812698364258, 1.9109585285186768, 2.3652782440185547, 2.8234245777130127, 3.278489112854004, 3.734861373901367, 4.189744234085083, 4.642908811569214, 5.102741718292236, 5.558742046356201, 6.008871078491211, 6.465338945388794, 6.920605897903442, 7.371825695037842, 7.82998514175415, 8.28705358505249, 8.748699188232422, 9.202770233154297, 9.652282953262329, 10.109473466873169, 10.56213665008545, 11.016321420669556, 11.476469278335571, 11.931370258331299, 12.382124185562134, 12.831827402114868, 13.28219747543335, 13.740391492843628, 14.198841333389282, 14.655132055282593, 15.109740734100342, 15.559993743896484, 16.010509729385376, 16.463931560516357, 16.91633105278015, 17.36921501159668, 17.823713541030884, 18.275415658950806, 18.73323369026184, 19.186420679092407, 19.634690284729004, 20.0886390209198, 20.54794430732727, 21.0012686252594, 21.456902027130127, 21.905627727508545, 22.35375690460205, 22.80568027496338, 23.25927233695984, 23.722382307052612, 24.17821979522705, 24.62627625465393, 25.079612493515015, 25.530165433883667, 25.97908306121826, 26.440272092819214, 26.90222716331482, 27.356069803237915, 27.809022426605225, 28.25953197479248, 28.711256742477417, 29.16358709335327, 29.621794939041138, 30.084471225738525, 30.541677951812744, 30.990527391433716, 31.443198919296265, 31.895172119140625, 32.35231351852417, 32.81340479850769, 33.269262075424194, 33.72020769119263, 34.171326637268066, 34.620113372802734, 35.073755979537964, 35.526028871536255, 35.982627391815186, 36.44149827957153, 36.89650869369507, 37.35051918029785, 37.80409526824951, 38.25378942489624, 38.707932233810425, 39.16657638549805, 39.62257194519043, 40.0737738609314, 40.5228545665741, 40.981436252593994, 41.43133592605591, 41.881524324417114, 42.33999538421631, 42.79662275314331, 43.247201442718506, 43.69836139678955, 44.15268039703369, 44.60392785072327, 45.05818176269531, 45.51664876937866, 46.366199016571045]
[17.316666666666666, 22.916666666666668, 30.783333333333335, 31.616666666666667, 41.18333333333333, 42.56666666666667, 43.43333333333333, 45.583333333333336, 44.36666666666667, 42.85, 41.733333333333334, 45.35, 48.416666666666664, 56.38333333333333, 61.766666666666666, 64.48333333333333, 67.43333333333334, 72.08333333333333, 74.88333333333334, 76.01666666666667, 76.66666666666667, 76.93333333333334, 77.18333333333334, 78.31666666666666, 78.7, 78.61666666666666, 78.61666666666666, 78.56666666666666, 80.61666666666666, 81.78333333333333, 81.48333333333333, 81.98333333333333, 82.58333333333333, 82.23333333333333, 82.3, 82.55, 82.58333333333333, 82.9, 82.95, 82.93333333333334, 82.86666666666666, 82.91666666666667, 82.9, 82.88333333333334, 83.01666666666667, 83.01666666666667, 83.03333333333333, 82.9, 83.0, 82.93333333333334, 82.91666666666667, 82.86666666666666, 82.98333333333333, 82.95, 83.0, 83.05, 82.96666666666667, 83.05, 83.05, 83.13333333333334, 83.13333333333334, 83.03333333333333, 83.03333333333333, 83.1, 83.05, 83.0, 82.95, 82.98333333333333, 82.98333333333333, 83.0, 83.11666666666666, 83.0, 83.03333333333333, 83.03333333333333, 83.11666666666666, 83.1, 83.13333333333334, 83.11666666666666, 83.11666666666666, 83.15, 83.2, 83.18333333333334, 83.16666666666667, 83.2, 83.16666666666667, 83.18333333333334, 83.16666666666667, 83.08333333333333, 83.1, 83.08333333333333, 83.08333333333333, 83.08333333333333, 83.15, 83.15, 83.15, 83.11666666666666, 83.15, 83.2, 83.18333333333334, 83.2, 83.2]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.714, Test loss: 2.297, Test accuracy: 38.18
Round   1, Train loss: 1.652, Test loss: 2.283, Test accuracy: 40.68
Round   2, Train loss: 1.551, Test loss: 2.258, Test accuracy: 45.52
Round   3, Train loss: 1.490, Test loss: 2.214, Test accuracy: 50.80
Round   4, Train loss: 1.393, Test loss: 2.181, Test accuracy: 56.70
Round   5, Train loss: 1.353, Test loss: 2.146, Test accuracy: 60.77
Round   6, Train loss: 1.318, Test loss: 2.120, Test accuracy: 62.62
Round   7, Train loss: 1.308, Test loss: 2.098, Test accuracy: 65.40
Round   8, Train loss: 1.280, Test loss: 2.082, Test accuracy: 67.08
Round   9, Train loss: 1.278, Test loss: 2.068, Test accuracy: 68.38
Round  10, Train loss: 1.285, Test loss: 2.059, Test accuracy: 69.75
Round  11, Train loss: 1.260, Test loss: 2.049, Test accuracy: 70.43
Round  12, Train loss: 1.260, Test loss: 2.045, Test accuracy: 70.30
Round  13, Train loss: 1.267, Test loss: 2.039, Test accuracy: 70.70
Round  14, Train loss: 1.254, Test loss: 2.035, Test accuracy: 70.98
Round  15, Train loss: 1.253, Test loss: 2.034, Test accuracy: 70.63
Round  16, Train loss: 1.257, Test loss: 2.029, Test accuracy: 70.63
Round  17, Train loss: 1.246, Test loss: 2.025, Test accuracy: 70.62
Round  18, Train loss: 1.261, Test loss: 2.024, Test accuracy: 70.43
Round  19, Train loss: 1.251, Test loss: 2.022, Test accuracy: 69.83
Round  20, Train loss: 1.249, Test loss: 2.020, Test accuracy: 69.68
Round  21, Train loss: 1.249, Test loss: 2.019, Test accuracy: 69.67
Round  22, Train loss: 1.255, Test loss: 2.018, Test accuracy: 69.62
Round  23, Train loss: 1.254, Test loss: 2.017, Test accuracy: 69.07
Round  24, Train loss: 1.245, Test loss: 2.016, Test accuracy: 69.03
Round  25, Train loss: 1.254, Test loss: 2.016, Test accuracy: 68.95
Round  26, Train loss: 1.247, Test loss: 2.014, Test accuracy: 68.78
Round  27, Train loss: 1.247, Test loss: 2.014, Test accuracy: 68.55
Round  28, Train loss: 1.245, Test loss: 2.014, Test accuracy: 68.13
Round  29, Train loss: 1.249, Test loss: 2.013, Test accuracy: 67.85
Round  30, Train loss: 1.249, Test loss: 2.013, Test accuracy: 67.70
Round  31, Train loss: 1.248, Test loss: 2.012, Test accuracy: 67.55
Round  32, Train loss: 1.244, Test loss: 2.013, Test accuracy: 67.32
Round  33, Train loss: 1.247, Test loss: 2.012, Test accuracy: 67.27
Round  34, Train loss: 1.245, Test loss: 2.011, Test accuracy: 67.02
Round  35, Train loss: 1.248, Test loss: 2.012, Test accuracy: 66.47
Round  36, Train loss: 1.253, Test loss: 2.012, Test accuracy: 66.13
Round  37, Train loss: 1.245, Test loss: 2.012, Test accuracy: 65.78
Round  38, Train loss: 1.255, Test loss: 2.012, Test accuracy: 66.00
Round  39, Train loss: 1.250, Test loss: 2.013, Test accuracy: 65.43
Round  40, Train loss: 1.249, Test loss: 2.013, Test accuracy: 65.45
Round  41, Train loss: 1.249, Test loss: 2.011, Test accuracy: 65.43
Round  42, Train loss: 1.242, Test loss: 2.011, Test accuracy: 65.28
Round  43, Train loss: 1.247, Test loss: 2.011, Test accuracy: 65.13
Round  44, Train loss: 1.243, Test loss: 2.012, Test accuracy: 64.98
Round  45, Train loss: 1.254, Test loss: 2.011, Test accuracy: 64.80
Round  46, Train loss: 1.251, Test loss: 2.012, Test accuracy: 64.63
Round  47, Train loss: 1.241, Test loss: 2.010, Test accuracy: 64.77
Round  48, Train loss: 1.243, Test loss: 2.011, Test accuracy: 64.43
Round  49, Train loss: 1.242, Test loss: 2.011, Test accuracy: 64.23
Round  50, Train loss: 1.241, Test loss: 2.010, Test accuracy: 64.42
Round  51, Train loss: 1.244, Test loss: 2.010, Test accuracy: 64.22
Round  52, Train loss: 1.241, Test loss: 2.011, Test accuracy: 63.82
Round  53, Train loss: 1.250, Test loss: 2.012, Test accuracy: 63.58
Round  54, Train loss: 1.246, Test loss: 2.012, Test accuracy: 63.32
Round  55, Train loss: 1.241, Test loss: 2.012, Test accuracy: 63.43
Round  56, Train loss: 1.233, Test loss: 2.011, Test accuracy: 63.33
Round  57, Train loss: 1.240, Test loss: 2.012, Test accuracy: 62.88
Round  58, Train loss: 1.247, Test loss: 2.013, Test accuracy: 62.97
Round  59, Train loss: 1.242, Test loss: 2.013, Test accuracy: 62.95
Round  60, Train loss: 1.239, Test loss: 2.014, Test accuracy: 62.20
Round  61, Train loss: 1.242, Test loss: 2.014, Test accuracy: 61.85
Round  62, Train loss: 1.236, Test loss: 2.014, Test accuracy: 61.50
Round  63, Train loss: 1.249, Test loss: 2.014, Test accuracy: 61.32
Round  64, Train loss: 1.242, Test loss: 2.014, Test accuracy: 61.13
Round  65, Train loss: 1.253, Test loss: 2.014, Test accuracy: 61.12
Round  66, Train loss: 1.251, Test loss: 2.015, Test accuracy: 61.10
Round  67, Train loss: 1.247, Test loss: 2.014, Test accuracy: 61.17
Round  68, Train loss: 1.233, Test loss: 2.014, Test accuracy: 60.83
Round  69, Train loss: 1.243, Test loss: 2.015, Test accuracy: 60.55
Round  70, Train loss: 1.248, Test loss: 2.015, Test accuracy: 60.50
Round  71, Train loss: 1.241, Test loss: 2.016, Test accuracy: 60.35
Round  72, Train loss: 1.247, Test loss: 2.016, Test accuracy: 60.05
Round  73, Train loss: 1.247, Test loss: 2.017, Test accuracy: 59.93
Round  74, Train loss: 1.233, Test loss: 2.016, Test accuracy: 59.87
Round  75, Train loss: 1.246, Test loss: 2.016, Test accuracy: 59.97
Round  76, Train loss: 1.245, Test loss: 2.016, Test accuracy: 59.83
Round  77, Train loss: 1.245, Test loss: 2.017, Test accuracy: 59.80
Round  78, Train loss: 1.241, Test loss: 2.017, Test accuracy: 59.58
Round  79, Train loss: 1.246, Test loss: 2.017, Test accuracy: 59.63
Round  80, Train loss: 1.243, Test loss: 2.017, Test accuracy: 59.38
Round  81, Train loss: 1.248, Test loss: 2.018, Test accuracy: 59.25
Round  82, Train loss: 1.247, Test loss: 2.018, Test accuracy: 59.35
Round  83, Train loss: 1.248, Test loss: 2.018, Test accuracy: 59.22
Round  84, Train loss: 1.239, Test loss: 2.018, Test accuracy: 59.12
Round  85, Train loss: 1.235, Test loss: 2.018, Test accuracy: 59.10
Round  86, Train loss: 1.235, Test loss: 2.018, Test accuracy: 59.15
Round  87, Train loss: 1.227, Test loss: 2.019, Test accuracy: 58.98
Round  88, Train loss: 1.244, Test loss: 2.019, Test accuracy: 58.87
Round  89, Train loss: 1.239, Test loss: 2.020, Test accuracy: 58.85
Round  90, Train loss: 1.241, Test loss: 2.020, Test accuracy: 58.83
Round  91, Train loss: 1.242, Test loss: 2.024, Test accuracy: 58.82
Round  92, Train loss: 1.218, Test loss: 2.024, Test accuracy: 60.35
Round  93, Train loss: 1.201, Test loss: 2.018, Test accuracy: 63.18
Round  94, Train loss: 1.191, Test loss: 2.015, Test accuracy: 64.20
Round  95, Train loss: 1.195, Test loss: 2.015, Test accuracy: 64.75
Round  96, Train loss: 1.189, Test loss: 2.013, Test accuracy: 65.63
Round  97, Train loss: 1.188, Test loss: 2.009, Test accuracy: 66.10
Round  98, Train loss: 1.180, Test loss: 2.010, Test accuracy: 66.22
Round  99, Train loss: 1.183, Test loss: 2.008, Test accuracy: 66.33
Final Round, Train loss: 1.182, Test loss: 2.008, Test accuracy: 65.60
Average accuracy final 10 rounds: 63.44166666666667
665.707957983017
[]
[38.18333333333333, 40.68333333333333, 45.516666666666666, 50.8, 56.7, 60.766666666666666, 62.61666666666667, 65.4, 67.08333333333333, 68.38333333333334, 69.75, 70.43333333333334, 70.3, 70.7, 70.98333333333333, 70.63333333333334, 70.63333333333334, 70.61666666666666, 70.43333333333334, 69.83333333333333, 69.68333333333334, 69.66666666666667, 69.61666666666666, 69.06666666666666, 69.03333333333333, 68.95, 68.78333333333333, 68.55, 68.13333333333334, 67.85, 67.7, 67.55, 67.31666666666666, 67.26666666666667, 67.01666666666667, 66.46666666666667, 66.13333333333334, 65.78333333333333, 66.0, 65.43333333333334, 65.45, 65.43333333333334, 65.28333333333333, 65.13333333333334, 64.98333333333333, 64.8, 64.63333333333334, 64.76666666666667, 64.43333333333334, 64.23333333333333, 64.41666666666667, 64.21666666666667, 63.81666666666667, 63.583333333333336, 63.31666666666667, 63.43333333333333, 63.333333333333336, 62.88333333333333, 62.96666666666667, 62.95, 62.2, 61.85, 61.5, 61.31666666666667, 61.13333333333333, 61.11666666666667, 61.1, 61.166666666666664, 60.833333333333336, 60.55, 60.5, 60.35, 60.05, 59.93333333333333, 59.86666666666667, 59.96666666666667, 59.833333333333336, 59.8, 59.583333333333336, 59.63333333333333, 59.38333333333333, 59.25, 59.35, 59.21666666666667, 59.11666666666667, 59.1, 59.15, 58.983333333333334, 58.86666666666667, 58.85, 58.833333333333336, 58.81666666666667, 60.35, 63.18333333333333, 64.2, 64.75, 65.63333333333334, 66.1, 66.21666666666667, 66.33333333333333, 65.6]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.82
Round   0: Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.73
Round   1, Train loss: 2.298, Test loss: 2.302, Test accuracy: 9.70
Round   1: Global train loss: 2.298, Global test loss: 2.303, Global test accuracy: 8.77
Round   2, Train loss: 2.305, Test loss: 2.301, Test accuracy: 11.18
Round   2: Global train loss: 2.305, Global test loss: 2.303, Global test accuracy: 8.77
Round   3, Train loss: 2.303, Test loss: 2.301, Test accuracy: 12.33
Round   3: Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.77
Round   4, Train loss: 2.291, Test loss: 2.301, Test accuracy: 11.60
Round   4: Global train loss: 2.291, Global test loss: 2.302, Global test accuracy: 8.78
Round   5, Train loss: 2.296, Test loss: 2.301, Test accuracy: 11.85
Round   5: Global train loss: 2.296, Global test loss: 2.302, Global test accuracy: 8.78
Round   6, Train loss: 2.305, Test loss: 2.301, Test accuracy: 12.68
Round   6: Global train loss: 2.305, Global test loss: 2.302, Global test accuracy: 8.78
Round   7, Train loss: 2.301, Test loss: 2.299, Test accuracy: 15.38
Round   7: Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 8.80
Round   8, Train loss: 2.291, Test loss: 2.299, Test accuracy: 15.02
Round   8: Global train loss: 2.291, Global test loss: 2.301, Global test accuracy: 8.87
Round   9, Train loss: 2.293, Test loss: 2.299, Test accuracy: 14.52
Round   9: Global train loss: 2.293, Global test loss: 2.301, Global test accuracy: 8.88
Round  10, Train loss: 2.293, Test loss: 2.300, Test accuracy: 13.23
Round  10: Global train loss: 2.293, Global test loss: 2.301, Global test accuracy: 8.90
Round  11, Train loss: 2.295, Test loss: 2.298, Test accuracy: 15.38
Round  11: Global train loss: 2.295, Global test loss: 2.301, Global test accuracy: 9.17
Round  12, Train loss: 2.279, Test loss: 2.296, Test accuracy: 17.43
Round  12: Global train loss: 2.279, Global test loss: 2.300, Global test accuracy: 9.67
Round  13, Train loss: 2.276, Test loss: 2.294, Test accuracy: 18.48
Round  13: Global train loss: 2.276, Global test loss: 2.300, Global test accuracy: 11.00
Round  14, Train loss: 2.276, Test loss: 2.293, Test accuracy: 19.02
Round  14: Global train loss: 2.276, Global test loss: 2.300, Global test accuracy: 13.98
Round  15, Train loss: 2.294, Test loss: 2.294, Test accuracy: 18.02
Round  15: Global train loss: 2.294, Global test loss: 2.299, Global test accuracy: 15.32
Round  16, Train loss: 2.282, Test loss: 2.292, Test accuracy: 17.55
Round  16: Global train loss: 2.282, Global test loss: 2.299, Global test accuracy: 15.60
Round  17, Train loss: 2.240, Test loss: 2.288, Test accuracy: 19.57
Round  17: Global train loss: 2.240, Global test loss: 2.299, Global test accuracy: 17.43
Round  18, Train loss: 2.245, Test loss: 2.288, Test accuracy: 18.12
Round  18: Global train loss: 2.245, Global test loss: 2.298, Global test accuracy: 17.87
Round  19, Train loss: 2.281, Test loss: 2.287, Test accuracy: 19.02
Round  19: Global train loss: 2.281, Global test loss: 2.298, Global test accuracy: 19.30
Round  20, Train loss: 2.228, Test loss: 2.282, Test accuracy: 20.92
Round  20: Global train loss: 2.228, Global test loss: 2.297, Global test accuracy: 20.53
Round  21, Train loss: 2.266, Test loss: 2.281, Test accuracy: 20.13
Round  21: Global train loss: 2.266, Global test loss: 2.297, Global test accuracy: 22.18
Round  22, Train loss: 1.992, Test loss: 2.255, Test accuracy: 24.65
Round  22: Global train loss: 1.992, Global test loss: 2.294, Global test accuracy: 25.37
Round  23, Train loss: 2.250, Test loss: 2.258, Test accuracy: 22.52
Round  23: Global train loss: 2.250, Global test loss: 2.293, Global test accuracy: 26.28
Round  24, Train loss: 2.362, Test loss: 2.273, Test accuracy: 21.20
Round  24: Global train loss: 2.362, Global test loss: 2.294, Global test accuracy: 25.05
Round  25, Train loss: 2.141, Test loss: 2.263, Test accuracy: 22.75
Round  25: Global train loss: 2.141, Global test loss: 2.292, Global test accuracy: 26.87
Round  26, Train loss: 2.335, Test loss: 2.273, Test accuracy: 21.83
Round  26: Global train loss: 2.335, Global test loss: 2.293, Global test accuracy: 26.07
Round  27, Train loss: 2.274, Test loss: 2.280, Test accuracy: 21.00
Round  27: Global train loss: 2.274, Global test loss: 2.293, Global test accuracy: 25.82
Round  28, Train loss: 2.156, Test loss: 2.271, Test accuracy: 19.95
Round  28: Global train loss: 2.156, Global test loss: 2.293, Global test accuracy: 26.42
Round  29, Train loss: 1.929, Test loss: 2.258, Test accuracy: 21.27
Round  29: Global train loss: 1.929, Global test loss: 2.292, Global test accuracy: 27.58
Round  30, Train loss: 2.137, Test loss: 2.252, Test accuracy: 23.32
Round  30: Global train loss: 2.137, Global test loss: 2.291, Global test accuracy: 27.87
Round  31, Train loss: 2.096, Test loss: 2.264, Test accuracy: 21.37
Round  31: Global train loss: 2.096, Global test loss: 2.292, Global test accuracy: 27.08
Round  32, Train loss: 1.846, Test loss: 2.243, Test accuracy: 23.18
Round  32: Global train loss: 1.846, Global test loss: 2.290, Global test accuracy: 27.97
Round  33, Train loss: 1.955, Test loss: 2.244, Test accuracy: 24.32
Round  33: Global train loss: 1.955, Global test loss: 2.291, Global test accuracy: 29.77
Round  34, Train loss: 2.131, Test loss: 2.242, Test accuracy: 26.12
Round  34: Global train loss: 2.131, Global test loss: 2.291, Global test accuracy: 28.07
Round  35, Train loss: 2.226, Test loss: 2.255, Test accuracy: 24.85
Round  35: Global train loss: 2.226, Global test loss: 2.292, Global test accuracy: 24.03
Round  36, Train loss: 1.545, Test loss: 2.218, Test accuracy: 28.72
Round  36: Global train loss: 1.545, Global test loss: 2.289, Global test accuracy: 30.58
Round  37, Train loss: 1.886, Test loss: 2.236, Test accuracy: 25.77
Round  37: Global train loss: 1.886, Global test loss: 2.290, Global test accuracy: 28.05
Round  38, Train loss: 2.046, Test loss: 2.225, Test accuracy: 26.95
Round  38: Global train loss: 2.046, Global test loss: 2.289, Global test accuracy: 28.77
Round  39, Train loss: 1.430, Test loss: 2.211, Test accuracy: 29.67
Round  39: Global train loss: 1.430, Global test loss: 2.288, Global test accuracy: 29.40
Round  40, Train loss: 2.004, Test loss: 2.232, Test accuracy: 27.12
Round  40: Global train loss: 2.004, Global test loss: 2.290, Global test accuracy: 27.47
Round  41, Train loss: 2.368, Test loss: 2.248, Test accuracy: 23.07
Round  41: Global train loss: 2.368, Global test loss: 2.292, Global test accuracy: 23.02
Round  42, Train loss: 1.580, Test loss: 2.224, Test accuracy: 23.50
Round  42: Global train loss: 1.580, Global test loss: 2.291, Global test accuracy: 22.35
Round  43, Train loss: 1.606, Test loss: 2.191, Test accuracy: 30.50
Round  43: Global train loss: 1.606, Global test loss: 2.287, Global test accuracy: 26.88
Round  44, Train loss: 2.312, Test loss: 2.212, Test accuracy: 25.73
Round  44: Global train loss: 2.312, Global test loss: 2.289, Global test accuracy: 26.08
Round  45, Train loss: 1.533, Test loss: 2.205, Test accuracy: 26.63
Round  45: Global train loss: 1.533, Global test loss: 2.289, Global test accuracy: 25.57
Round  46, Train loss: 1.301, Test loss: 2.199, Test accuracy: 25.70
Round  46: Global train loss: 1.301, Global test loss: 2.288, Global test accuracy: 27.58
Round  47, Train loss: 1.717, Test loss: 2.167, Test accuracy: 30.22
Round  47: Global train loss: 1.717, Global test loss: 2.286, Global test accuracy: 30.82
Round  48, Train loss: 1.186, Test loss: 2.174, Test accuracy: 27.32
Round  48: Global train loss: 1.186, Global test loss: 2.286, Global test accuracy: 29.45
Round  49, Train loss: 1.387, Test loss: 2.161, Test accuracy: 28.33
Round  49: Global train loss: 1.387, Global test loss: 2.284, Global test accuracy: 29.92
Round  50, Train loss: 1.367, Test loss: 2.161, Test accuracy: 27.93
Round  50: Global train loss: 1.367, Global test loss: 2.285, Global test accuracy: 29.72
Round  51, Train loss: 1.082, Test loss: 2.117, Test accuracy: 33.53
Round  51: Global train loss: 1.082, Global test loss: 2.281, Global test accuracy: 33.38
Round  52, Train loss: 0.221, Test loss: 2.099, Test accuracy: 35.75
Round  52: Global train loss: 0.221, Global test loss: 2.274, Global test accuracy: 41.57
Round  53, Train loss: 0.447, Test loss: 2.072, Test accuracy: 38.88
Round  53: Global train loss: 0.447, Global test loss: 2.266, Global test accuracy: 44.57
Round  54, Train loss: 1.419, Test loss: 2.080, Test accuracy: 36.97
Round  54: Global train loss: 1.419, Global test loss: 2.262, Global test accuracy: 44.88
Round  55, Train loss: 1.420, Test loss: 2.076, Test accuracy: 37.73
Round  55: Global train loss: 1.420, Global test loss: 2.262, Global test accuracy: 44.75
Round  56, Train loss: 1.268, Test loss: 2.101, Test accuracy: 34.42
Round  56: Global train loss: 1.268, Global test loss: 2.260, Global test accuracy: 43.13
Round  57, Train loss: 2.221, Test loss: 2.177, Test accuracy: 26.40
Round  57: Global train loss: 2.221, Global test loss: 2.273, Global test accuracy: 38.47
Round  58, Train loss: 1.504, Test loss: 2.177, Test accuracy: 26.45
Round  58: Global train loss: 1.504, Global test loss: 2.276, Global test accuracy: 34.93
Round  59, Train loss: 1.110, Test loss: 2.164, Test accuracy: 27.70
Round  59: Global train loss: 1.110, Global test loss: 2.277, Global test accuracy: 36.27
Round  60, Train loss: 1.363, Test loss: 2.118, Test accuracy: 34.77
Round  60: Global train loss: 1.363, Global test loss: 2.274, Global test accuracy: 38.73
Round  61, Train loss: 0.878, Test loss: 2.113, Test accuracy: 34.97
Round  61: Global train loss: 0.878, Global test loss: 2.273, Global test accuracy: 39.52
Round  62, Train loss: 1.636, Test loss: 2.154, Test accuracy: 30.62
Round  62: Global train loss: 1.636, Global test loss: 2.280, Global test accuracy: 36.05
Round  63, Train loss: 0.674, Test loss: 2.114, Test accuracy: 34.58
Round  63: Global train loss: 0.674, Global test loss: 2.277, Global test accuracy: 39.00
Round  64, Train loss: -0.307, Test loss: 2.067, Test accuracy: 40.20
Round  64: Global train loss: -0.307, Global test loss: 2.269, Global test accuracy: 42.97
Round  65, Train loss: 0.801, Test loss: 2.066, Test accuracy: 39.57
Round  65: Global train loss: 0.801, Global test loss: 2.269, Global test accuracy: 43.18
Round  66, Train loss: 1.183, Test loss: 2.116, Test accuracy: 33.83
Round  66: Global train loss: 1.183, Global test loss: 2.274, Global test accuracy: 42.53
Round  67, Train loss: 0.388, Test loss: 2.107, Test accuracy: 34.77
Round  67: Global train loss: 0.388, Global test loss: 2.277, Global test accuracy: 43.03
Round  68, Train loss: -0.299, Test loss: 2.096, Test accuracy: 36.08
Round  68: Global train loss: -0.299, Global test loss: 2.275, Global test accuracy: 43.05
Round  69, Train loss: 1.007, Test loss: 2.115, Test accuracy: 33.60
Round  69: Global train loss: 1.007, Global test loss: 2.279, Global test accuracy: 34.70
Round  70, Train loss: -0.424, Test loss: 2.031, Test accuracy: 43.53
Round  70: Global train loss: -0.424, Global test loss: 2.267, Global test accuracy: 47.20
Round  71, Train loss: -0.375, Test loss: 2.061, Test accuracy: 39.12
Round  71: Global train loss: -0.375, Global test loss: 2.267, Global test accuracy: 49.30
Round  72, Train loss: -1.189, Test loss: 1.971, Test accuracy: 48.25
Round  72: Global train loss: -1.189, Global test loss: 2.244, Global test accuracy: 54.90
Round  73, Train loss: 0.141, Test loss: 2.009, Test accuracy: 44.20
Round  73: Global train loss: 0.141, Global test loss: 2.237, Global test accuracy: 53.78
Round  74, Train loss: 0.766, Test loss: 2.079, Test accuracy: 38.03
Round  74: Global train loss: 0.766, Global test loss: 2.243, Global test accuracy: 51.00
Round  75, Train loss: -0.113, Test loss: 2.028, Test accuracy: 44.40
Round  75: Global train loss: -0.113, Global test loss: 2.230, Global test accuracy: 53.18
Round  76, Train loss: -0.546, Test loss: 1.989, Test accuracy: 49.17
Round  76: Global train loss: -0.546, Global test loss: 2.213, Global test accuracy: 54.42
Round  77, Train loss: -0.725, Test loss: 1.989, Test accuracy: 47.80
Round  77: Global train loss: -0.725, Global test loss: 2.204, Global test accuracy: 54.13
Round  78, Train loss: 0.760, Test loss: 2.021, Test accuracy: 44.73
Round  78: Global train loss: 0.760, Global test loss: 2.205, Global test accuracy: 52.20
Round  79, Train loss: -2.178, Test loss: 1.936, Test accuracy: 54.07
Round  79: Global train loss: -2.178, Global test loss: 2.154, Global test accuracy: 52.53
Round  80, Train loss: -1.358, Test loss: 1.950, Test accuracy: 52.70
Round  80: Global train loss: -1.358, Global test loss: 2.130, Global test accuracy: 50.02
Round  81, Train loss: 0.832, Test loss: 2.004, Test accuracy: 49.42
Round  81: Global train loss: 0.832, Global test loss: 2.144, Global test accuracy: 49.93
Round  82, Train loss: -2.141, Test loss: 1.936, Test accuracy: 55.03
Round  82: Global train loss: -2.141, Global test loss: 2.096, Global test accuracy: 51.98
Round  83, Train loss: 0.631, Test loss: 1.951, Test accuracy: 54.60
Round  83: Global train loss: 0.631, Global test loss: 2.087, Global test accuracy: 51.32
Round  84, Train loss: 0.970, Test loss: 1.985, Test accuracy: 51.05
Round  84: Global train loss: 0.970, Global test loss: 2.091, Global test accuracy: 50.07
Round  85, Train loss: -0.735, Test loss: 1.933, Test accuracy: 54.85
Round  85: Global train loss: -0.735, Global test loss: 2.055, Global test accuracy: 51.03
Round  86, Train loss: 1.715, Test loss: 1.988, Test accuracy: 50.65
Round  86: Global train loss: 1.715, Global test loss: 2.063, Global test accuracy: 50.15
Round  87, Train loss: -0.533, Test loss: 1.952, Test accuracy: 53.63
Round  87: Global train loss: -0.533, Global test loss: 2.053, Global test accuracy: 52.63
Round  88, Train loss: -0.251, Test loss: 1.949, Test accuracy: 54.93
Round  88: Global train loss: -0.251, Global test loss: 2.035, Global test accuracy: 52.17
Round  89, Train loss: -0.826, Test loss: 1.881, Test accuracy: 59.30
Round  89: Global train loss: -0.826, Global test loss: 2.004, Global test accuracy: 53.20
Round  90, Train loss: 1.367, Test loss: 1.925, Test accuracy: 55.83
Round  90: Global train loss: 1.367, Global test loss: 2.010, Global test accuracy: 52.48
Round  91, Train loss: 0.078, Test loss: 1.911, Test accuracy: 56.98
Round  91: Global train loss: 0.078, Global test loss: 2.000, Global test accuracy: 52.87
Round  92, Train loss: 0.576, Test loss: 1.909, Test accuracy: 57.85
Round  92: Global train loss: 0.576, Global test loss: 1.995, Global test accuracy: 53.02
Round  93, Train loss: 1.310, Test loss: 1.944, Test accuracy: 55.03
Round  93: Global train loss: 1.310, Global test loss: 2.003, Global test accuracy: 52.13
Round  94, Train loss: 0.790, Test loss: 1.933, Test accuracy: 56.52
Round  94: Global train loss: 0.790, Global test loss: 1.999, Global test accuracy: 51.73
Round  95, Train loss: 0.099, Test loss: 1.890, Test accuracy: 59.60
Round  95: Global train loss: 0.099, Global test loss: 1.986, Global test accuracy: 52.43
Round  96, Train loss: 0.171, Test loss: 1.874, Test accuracy: 61.13
Round  96: Global train loss: 0.171, Global test loss: 1.979, Global test accuracy: 52.83
Round  97, Train loss: -0.726, Test loss: 1.816, Test accuracy: 66.45
Round  97: Global train loss: -0.726, Global test loss: 1.956, Global test accuracy: 54.10
Round  98, Train loss: -0.309, Test loss: 1.813, Test accuracy: 66.63
Round  98: Global train loss: -0.309, Global test loss: 1.942, Global test accuracy: 55.72
Round  99, Train loss: -0.643, Test loss: 1.776, Test accuracy: 69.77
Round  99: Global train loss: -0.643, Global test loss: 1.925, Global test accuracy: 57.03/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round: Train loss: 1.783, Test loss: 1.731, Test accuracy: 79.15
Final Round: Global train loss: 1.783, Global test loss: 1.907, Global test accuracy: 58.57
Average accuracy final 10 rounds: 60.580000000000005
Average global accuracy final 10 rounds: 53.434999999999995
631.5837082862854
[]
[8.816666666666666, 9.7, 11.183333333333334, 12.333333333333334, 11.6, 11.85, 12.683333333333334, 15.383333333333333, 15.016666666666667, 14.516666666666667, 13.233333333333333, 15.383333333333333, 17.433333333333334, 18.483333333333334, 19.016666666666666, 18.016666666666666, 17.55, 19.566666666666666, 18.116666666666667, 19.016666666666666, 20.916666666666668, 20.133333333333333, 24.65, 22.516666666666666, 21.2, 22.75, 21.833333333333332, 21.0, 19.95, 21.266666666666666, 23.316666666666666, 21.366666666666667, 23.183333333333334, 24.316666666666666, 26.116666666666667, 24.85, 28.716666666666665, 25.766666666666666, 26.95, 29.666666666666668, 27.116666666666667, 23.066666666666666, 23.5, 30.5, 25.733333333333334, 26.633333333333333, 25.7, 30.216666666666665, 27.316666666666666, 28.333333333333332, 27.933333333333334, 33.53333333333333, 35.75, 38.88333333333333, 36.96666666666667, 37.733333333333334, 34.416666666666664, 26.4, 26.45, 27.7, 34.766666666666666, 34.96666666666667, 30.616666666666667, 34.583333333333336, 40.2, 39.56666666666667, 33.833333333333336, 34.766666666666666, 36.083333333333336, 33.6, 43.53333333333333, 39.11666666666667, 48.25, 44.2, 38.03333333333333, 44.4, 49.166666666666664, 47.8, 44.733333333333334, 54.06666666666667, 52.7, 49.416666666666664, 55.03333333333333, 54.6, 51.05, 54.85, 50.65, 53.63333333333333, 54.93333333333333, 59.3, 55.833333333333336, 56.983333333333334, 57.85, 55.03333333333333, 56.516666666666666, 59.6, 61.13333333333333, 66.45, 66.63333333333334, 69.76666666666667, 79.15]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.303, Test accuracy: 13.67 

Round   0, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 13.70 

Round   1, Train loss: 2.302, Test loss: 2.303, Test accuracy: 13.90 

Round   1, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 13.93 

Round   2, Train loss: 2.302, Test loss: 2.303, Test accuracy: 13.92 

Round   2, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.02 

Round   3, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.00 

Round   3, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.10 

Round   4, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.05 

Round   4, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.22 

Round   5, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.17 

Round   5, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.45 

Round   6, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.35 

Round   6, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.62 

Round   7, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.48 

Round   7, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.87 

Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.60 

Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 15.03 

Round   9, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.87 

Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.25 

Round  10, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.95 

Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.35 

Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.20 

Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.53 

Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.22 

Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.43 

Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.33 

Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.67 

Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.57 

Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.73 

Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.57 

Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.82 

Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.72 

Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.92 

Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.83 

Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.97 

Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.90 

Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.13 

Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.03 

Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.27 

Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.12 

Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.32 

Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.38 

Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.60 

Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.43 

Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.67 

Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.43 

Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.72 

Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.58 

Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.75 

Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.67 

Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.90 

Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.88 

Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.93 

Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.93 

Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.92 

Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.98 

Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.95 

Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.98 

Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.02 

Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.03 

Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.42 

Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.17 

Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.38 

Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.33 

Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.52 

Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.47 

Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.58 

Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.45 

Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.63 

Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.55 

Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.88 

Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.65 

Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.98 

Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.78 

Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.03 

Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.92 

Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.17 

Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.07 

Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.25 

Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.10 

Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.30 

Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.40 

Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.32 

Round  42, Train loss: 2.301, Test loss: 2.302, Test accuracy: 18.37 

Round  42, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 18.47 

Round  43, Train loss: 2.301, Test loss: 2.302, Test accuracy: 18.38 

Round  43, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 18.57 

Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.45 

Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.60 

Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.57 

Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.63 

Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.63 

Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.70 

Round  47, Train loss: 2.301, Test loss: 2.302, Test accuracy: 18.68 

Round  47, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 18.88 

Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.77 

Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.00 

Round  49, Train loss: 2.301, Test loss: 2.302, Test accuracy: 18.88 

Round  49, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.15 

Round  50, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.02 

Round  50, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.33 

Round  51, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.13 

Round  51, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.35 

Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 19.18 

Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.52 

Round  53, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.17 

Round  53, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.55 

Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 19.32 

Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.62 

Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 19.38 

Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.67 

Round  56, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.55 

Round  56, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.72 

Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 19.63 

Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.78 

Round  58, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.65 

Round  58, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.83 

Round  59, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.72 

Round  59, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.90 

Round  60, Train loss: 2.302, Test loss: 2.302, Test accuracy: 19.78 

Round  60, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.97 

Round  61, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.93 

Round  61, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.03 

Round  62, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.92 

Round  62, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.03 

Round  63, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.93 

Round  63, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.03 

Round  64, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.05 

Round  64, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.07 

Round  65, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.12 

Round  65, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.18 

Round  66, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.12 

Round  66, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.25 

Round  67, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.15 

Round  67, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.27 

Round  68, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.22 

Round  68, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.35 

Round  69, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.22 

Round  69, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.38 

Round  70, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.27 

Round  70, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.37 

Round  71, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.28 

Round  71, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.38 

Round  72, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.32 

Round  72, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.53 

Round  73, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.37 

Round  73, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.55 

Round  74, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.47 

Round  74, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.58 

Round  75, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.53 

Round  75, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.60 

Round  76, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.62 

Round  76, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.70 

Round  77, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.63 

Round  77, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.70 

Round  78, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.75 

Round  78, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.82 

Round  79, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.75 

Round  79, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.90 

Round  80, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.83 

Round  80, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.95 

Round  81, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.88 

Round  81, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.97 

Round  82, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.93 

Round  82, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.07 

Round  83, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.95 

Round  83, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.05 

Round  84, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.03 

Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.20 

Round  85, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.12 

Round  85, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.23 

Round  86, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.20 

Round  86, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.28 

Round  87, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.23 

Round  87, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.27 

Round  88, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.23 

Round  88, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.25 

Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.23 

Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.35 

Round  90, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.25 

Round  90, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.28 

Round  91, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.30 

Round  91, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.35 

Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.28 

Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.40 

Round  93, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.45 

Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.40 

Round  94, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.45 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.48 

Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.47 

Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.50 

Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.50 

Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.50 

Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.45 

Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.48 

Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.50 

Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.58 

Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.48 

Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.57 

Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.58 

Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.57 

Average accuracy final 10 rounds: 21.41333333333333 

Average global accuracy final 10 rounds: 21.455 

528.0413286685944
[0.5943806171417236, 1.0897603034973145, 1.5893924236297607, 2.0896108150482178, 2.5932514667510986, 3.097294807434082, 3.598078489303589, 4.0991740226745605, 4.604261636734009, 5.110774278640747, 5.618943691253662, 6.121859312057495, 6.626470327377319, 7.1284263134002686, 7.635884046554565, 8.124279499053955, 8.557582378387451, 8.989379167556763, 9.421461582183838, 9.85852861404419, 10.29589319229126, 10.729526281356812, 11.163991451263428, 11.59464979171753, 12.030524492263794, 12.473021268844604, 12.903381824493408, 13.33982253074646, 13.773521661758423, 14.204024076461792, 14.644250392913818, 15.082537651062012, 15.513116836547852, 15.947884798049927, 16.380833625793457, 16.81848430633545, 17.25814461708069, 17.691359758377075, 18.121618032455444, 18.555020809173584, 18.991342306137085, 19.427594661712646, 19.85945415496826, 20.292837858200073, 20.722110986709595, 21.1558735370636, 21.59581208229065, 22.02640962600708, 22.458704948425293, 22.89294695854187, 23.323036670684814, 23.762162923812866, 24.202548503875732, 24.633850812911987, 25.067757606506348, 25.50174379348755, 25.93862509727478, 26.378540515899658, 26.813234090805054, 27.244619131088257, 27.676883459091187, 28.11208963394165, 28.548036336898804, 28.976233959197998, 29.40804171562195, 29.838714122772217, 30.26572871208191, 30.70605182647705, 31.13690733909607, 31.563472270965576, 31.997053861618042, 32.42821216583252, 32.86057424545288, 33.29964876174927, 33.730785608291626, 34.15605401992798, 34.590505599975586, 35.02643322944641, 35.458688735961914, 35.89207673072815, 36.323726415634155, 36.74872064590454, 37.18273878097534, 37.61933994293213, 38.046550273895264, 38.48129725456238, 38.91575026512146, 39.34010195732117, 39.78080344200134, 40.21946430206299, 40.64443397521973, 41.079018354415894, 41.51331067085266, 41.9419424533844, 42.38102412223816, 42.81586027145386, 43.23918890953064, 43.67309069633484, 44.10743975639343, 44.54224252700806, 45.406097412109375]
[13.666666666666666, 13.9, 13.916666666666666, 14.0, 14.05, 14.166666666666666, 14.35, 14.483333333333333, 14.6, 14.866666666666667, 14.95, 15.2, 15.216666666666667, 15.333333333333334, 15.566666666666666, 15.566666666666666, 15.716666666666667, 15.833333333333334, 15.9, 16.033333333333335, 16.116666666666667, 16.383333333333333, 16.433333333333334, 16.433333333333334, 16.583333333333332, 16.666666666666668, 16.883333333333333, 16.933333333333334, 16.983333333333334, 16.983333333333334, 17.033333333333335, 17.166666666666668, 17.333333333333332, 17.466666666666665, 17.45, 17.55, 17.65, 17.783333333333335, 17.916666666666668, 18.066666666666666, 18.1, 18.4, 18.366666666666667, 18.383333333333333, 18.45, 18.566666666666666, 18.633333333333333, 18.683333333333334, 18.766666666666666, 18.883333333333333, 19.016666666666666, 19.133333333333333, 19.183333333333334, 19.166666666666668, 19.316666666666666, 19.383333333333333, 19.55, 19.633333333333333, 19.65, 19.716666666666665, 19.783333333333335, 19.933333333333334, 19.916666666666668, 19.933333333333334, 20.05, 20.116666666666667, 20.116666666666667, 20.15, 20.216666666666665, 20.216666666666665, 20.266666666666666, 20.283333333333335, 20.316666666666666, 20.366666666666667, 20.466666666666665, 20.533333333333335, 20.616666666666667, 20.633333333333333, 20.75, 20.75, 20.833333333333332, 20.883333333333333, 20.933333333333334, 20.95, 21.033333333333335, 21.116666666666667, 21.2, 21.233333333333334, 21.233333333333334, 21.233333333333334, 21.25, 21.3, 21.283333333333335, 21.45, 21.45, 21.466666666666665, 21.5, 21.45, 21.5, 21.483333333333334, 21.583333333333332]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 16.95
Round   1, Train loss: 2.300, Test loss: 2.299, Test accuracy: 17.83
Round   2, Train loss: 2.298, Test loss: 2.296, Test accuracy: 22.08
Round   3, Train loss: 2.295, Test loss: 2.291, Test accuracy: 29.95
Round   4, Train loss: 2.293, Test loss: 2.281, Test accuracy: 37.17
Round   5, Train loss: 2.283, Test loss: 2.243, Test accuracy: 29.83
Round   6, Train loss: 2.237, Test loss: 2.170, Test accuracy: 30.33
Round   7, Train loss: 2.196, Test loss: 2.064, Test accuracy: 46.43
Round   8, Train loss: 2.105, Test loss: 1.976, Test accuracy: 49.78
Round   9, Train loss: 2.021, Test loss: 1.915, Test accuracy: 57.78
Round  10, Train loss: 2.005, Test loss: 1.870, Test accuracy: 60.60
Round  11, Train loss: 1.926, Test loss: 1.811, Test accuracy: 68.65
Round  12, Train loss: 1.894, Test loss: 1.744, Test accuracy: 77.53
Round  13, Train loss: 1.780, Test loss: 1.702, Test accuracy: 79.93
Round  14, Train loss: 1.710, Test loss: 1.680, Test accuracy: 80.90
Round  15, Train loss: 1.740, Test loss: 1.669, Test accuracy: 81.23
Round  16, Train loss: 1.698, Test loss: 1.656, Test accuracy: 82.25
Round  17, Train loss: 1.686, Test loss: 1.650, Test accuracy: 82.52
Round  18, Train loss: 1.680, Test loss: 1.645, Test accuracy: 82.78
Round  19, Train loss: 1.687, Test loss: 1.644, Test accuracy: 82.80
Round  20, Train loss: 1.661, Test loss: 1.638, Test accuracy: 83.25
Round  21, Train loss: 1.619, Test loss: 1.637, Test accuracy: 83.43
Round  22, Train loss: 1.635, Test loss: 1.634, Test accuracy: 83.67
Round  23, Train loss: 1.604, Test loss: 1.634, Test accuracy: 83.47
Round  24, Train loss: 1.620, Test loss: 1.630, Test accuracy: 83.78
Round  25, Train loss: 1.612, Test loss: 1.629, Test accuracy: 84.02
Round  26, Train loss: 1.593, Test loss: 1.628, Test accuracy: 83.67
Round  27, Train loss: 1.612, Test loss: 1.626, Test accuracy: 83.97
Round  28, Train loss: 1.594, Test loss: 1.625, Test accuracy: 83.95
Round  29, Train loss: 1.603, Test loss: 1.624, Test accuracy: 84.13
Round  30, Train loss: 1.604, Test loss: 1.624, Test accuracy: 84.23
Round  31, Train loss: 1.594, Test loss: 1.624, Test accuracy: 84.17
Round  32, Train loss: 1.590, Test loss: 1.623, Test accuracy: 84.32
Round  33, Train loss: 1.610, Test loss: 1.622, Test accuracy: 84.33
Round  34, Train loss: 1.596, Test loss: 1.621, Test accuracy: 84.38
Round  35, Train loss: 1.601, Test loss: 1.621, Test accuracy: 84.42
Round  36, Train loss: 1.604, Test loss: 1.620, Test accuracy: 84.52
Round  37, Train loss: 1.585, Test loss: 1.621, Test accuracy: 84.37
Round  38, Train loss: 1.589, Test loss: 1.621, Test accuracy: 84.12
Round  39, Train loss: 1.596, Test loss: 1.620, Test accuracy: 84.40
Round  40, Train loss: 1.594, Test loss: 1.618, Test accuracy: 84.50
Round  41, Train loss: 1.590, Test loss: 1.618, Test accuracy: 84.55
Round  42, Train loss: 1.592, Test loss: 1.619, Test accuracy: 84.67
Round  43, Train loss: 1.589, Test loss: 1.617, Test accuracy: 84.72
Round  44, Train loss: 1.599, Test loss: 1.616, Test accuracy: 84.73
Round  45, Train loss: 1.587, Test loss: 1.615, Test accuracy: 84.80
Round  46, Train loss: 1.587, Test loss: 1.615, Test accuracy: 84.95
Round  47, Train loss: 1.596, Test loss: 1.616, Test accuracy: 84.95
Round  48, Train loss: 1.585, Test loss: 1.616, Test accuracy: 84.83
Round  49, Train loss: 1.580, Test loss: 1.615, Test accuracy: 84.83
Round  50, Train loss: 1.593, Test loss: 1.615, Test accuracy: 85.05
Round  51, Train loss: 1.584, Test loss: 1.615, Test accuracy: 84.92
Round  52, Train loss: 1.582, Test loss: 1.614, Test accuracy: 85.00
Round  53, Train loss: 1.590, Test loss: 1.613, Test accuracy: 84.97
Round  54, Train loss: 1.587, Test loss: 1.613, Test accuracy: 85.05
Round  55, Train loss: 1.582, Test loss: 1.613, Test accuracy: 85.17
Round  56, Train loss: 1.584, Test loss: 1.614, Test accuracy: 84.92
Round  57, Train loss: 1.588, Test loss: 1.613, Test accuracy: 85.13
Round  58, Train loss: 1.585, Test loss: 1.613, Test accuracy: 85.20
Round  59, Train loss: 1.585, Test loss: 1.613, Test accuracy: 85.28
Round  60, Train loss: 1.580, Test loss: 1.612, Test accuracy: 85.23
Round  61, Train loss: 1.588, Test loss: 1.612, Test accuracy: 85.25
Round  62, Train loss: 1.580, Test loss: 1.612, Test accuracy: 85.22
Round  63, Train loss: 1.587, Test loss: 1.613, Test accuracy: 85.15
Round  64, Train loss: 1.578, Test loss: 1.612, Test accuracy: 85.17
Round  65, Train loss: 1.591, Test loss: 1.612, Test accuracy: 85.30
Round  66, Train loss: 1.584, Test loss: 1.612, Test accuracy: 85.28
Round  67, Train loss: 1.581, Test loss: 1.612, Test accuracy: 85.03
Round  68, Train loss: 1.582, Test loss: 1.612, Test accuracy: 84.97
Round  69, Train loss: 1.585, Test loss: 1.611, Test accuracy: 85.30
Round  70, Train loss: 1.584, Test loss: 1.612, Test accuracy: 85.08
Round  71, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.28
Round  72, Train loss: 1.577, Test loss: 1.611, Test accuracy: 85.18
Round  73, Train loss: 1.582, Test loss: 1.610, Test accuracy: 85.43
Round  74, Train loss: 1.576, Test loss: 1.611, Test accuracy: 85.38
Round  75, Train loss: 1.584, Test loss: 1.611, Test accuracy: 85.33
Round  76, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.32
Round  77, Train loss: 1.577, Test loss: 1.611, Test accuracy: 85.33
Round  78, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.33
Round  79, Train loss: 1.574, Test loss: 1.610, Test accuracy: 85.40
Round  80, Train loss: 1.583, Test loss: 1.609, Test accuracy: 85.40
Round  81, Train loss: 1.576, Test loss: 1.610, Test accuracy: 85.25
Round  82, Train loss: 1.571, Test loss: 1.609, Test accuracy: 85.30
Round  83, Train loss: 1.580, Test loss: 1.610, Test accuracy: 85.23
Round  84, Train loss: 1.573, Test loss: 1.610, Test accuracy: 85.42
Round  85, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.40
Round  86, Train loss: 1.572, Test loss: 1.609, Test accuracy: 85.40
Round  87, Train loss: 1.572, Test loss: 1.609, Test accuracy: 85.40
Round  88, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.27
Round  89, Train loss: 1.585, Test loss: 1.609, Test accuracy: 85.40
Round  90, Train loss: 1.573, Test loss: 1.609, Test accuracy: 85.40
Round  91, Train loss: 1.573, Test loss: 1.609, Test accuracy: 85.30
Round  92, Train loss: 1.575, Test loss: 1.609, Test accuracy: 85.42
Round  93, Train loss: 1.581, Test loss: 1.610, Test accuracy: 85.37
Round  94, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.30
Round  95, Train loss: 1.582, Test loss: 1.609, Test accuracy: 85.22
Round  96, Train loss: 1.577, Test loss: 1.608, Test accuracy: 85.55
Round  97, Train loss: 1.573, Test loss: 1.608, Test accuracy: 85.47
Round  98, Train loss: 1.572, Test loss: 1.608, Test accuracy: 85.38
Round  99, Train loss: 1.581, Test loss: 1.608, Test accuracy: 85.57
Final Round, Train loss: 1.576, Test loss: 1.608, Test accuracy: 85.55
Average accuracy final 10 rounds: 85.39666666666668
829.7833926677704
[1.3175828456878662, 2.5213565826416016, 3.7290055751800537, 4.935198783874512, 6.142941236495972, 7.352817535400391, 8.559576988220215, 9.766870021820068, 10.970908641815186, 12.18018364906311, 13.386073112487793, 14.596026420593262, 15.8047776222229, 17.019051551818848, 18.238057136535645, 19.45146155357361, 20.66543459892273, 21.881502866744995, 23.101394653320312, 24.30892038345337, 25.519550323486328, 26.732839584350586, 27.94517707824707, 29.16144871711731, 30.37321400642395, 31.587527990341187, 32.79794645309448, 33.99982285499573, 35.203943729400635, 36.401506185531616, 37.60238528251648, 38.808349609375, 40.00529956817627, 41.20328903198242, 42.414450883865356, 43.60931372642517, 44.80514478683472, 46.01205134391785, 47.21974182128906, 48.42436385154724, 49.62986421585083, 50.83704686164856, 52.03833484649658, 53.2457640171051, 54.462522745132446, 55.693543910980225, 56.78769540786743, 57.89002513885498, 58.98585796356201, 60.08248043060303, 61.18365693092346, 62.27333331108093, 63.369062185287476, 64.46041941642761, 65.5516185760498, 66.64344835281372, 67.73383951187134, 68.82485842704773, 69.91894721984863, 71.0078992843628, 72.10449171066284, 73.19590878486633, 74.29359483718872, 75.38107943534851, 76.47196769714355, 77.56156086921692, 78.65332508087158, 79.74778842926025, 80.84054207801819, 81.932373046875, 83.02109742164612, 84.11371088027954, 85.20077300071716, 86.28850412368774, 87.37902402877808, 88.47108674049377, 89.56728315353394, 90.66284847259521, 91.75548195838928, 92.8502471446991, 93.94390630722046, 95.03262257575989, 96.1230206489563, 97.21535181999207, 98.3050045967102, 99.39742636680603, 100.49477005004883, 101.58608675003052, 102.67570781707764, 103.76417469978333, 104.85070943832397, 105.93842220306396, 107.02647089958191, 108.11950922012329, 109.21083617210388, 110.30034923553467, 111.39379453659058, 112.4861650466919, 113.57774186134338, 114.66959691047668, 115.7630786895752]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[16.95, 17.833333333333332, 22.083333333333332, 29.95, 37.166666666666664, 29.833333333333332, 30.333333333333332, 46.43333333333333, 49.78333333333333, 57.78333333333333, 60.6, 68.65, 77.53333333333333, 79.93333333333334, 80.9, 81.23333333333333, 82.25, 82.51666666666667, 82.78333333333333, 82.8, 83.25, 83.43333333333334, 83.66666666666667, 83.46666666666667, 83.78333333333333, 84.01666666666667, 83.66666666666667, 83.96666666666667, 83.95, 84.13333333333334, 84.23333333333333, 84.16666666666667, 84.31666666666666, 84.33333333333333, 84.38333333333334, 84.41666666666667, 84.51666666666667, 84.36666666666666, 84.11666666666666, 84.4, 84.5, 84.55, 84.66666666666667, 84.71666666666667, 84.73333333333333, 84.8, 84.95, 84.95, 84.83333333333333, 84.83333333333333, 85.05, 84.91666666666667, 85.0, 84.96666666666667, 85.05, 85.16666666666667, 84.91666666666667, 85.13333333333334, 85.2, 85.28333333333333, 85.23333333333333, 85.25, 85.21666666666667, 85.15, 85.16666666666667, 85.3, 85.28333333333333, 85.03333333333333, 84.96666666666667, 85.3, 85.08333333333333, 85.28333333333333, 85.18333333333334, 85.43333333333334, 85.38333333333334, 85.33333333333333, 85.31666666666666, 85.33333333333333, 85.33333333333333, 85.4, 85.4, 85.25, 85.3, 85.23333333333333, 85.41666666666667, 85.4, 85.4, 85.4, 85.26666666666667, 85.4, 85.4, 85.3, 85.41666666666667, 85.36666666666666, 85.3, 85.21666666666667, 85.55, 85.46666666666667, 85.38333333333334, 85.56666666666666, 85.55]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.313, Test loss: 2.303, Test accuracy: 9.73
Round   1, Train loss: 2.293, Test loss: 2.303, Test accuracy: 9.22
Round   2, Train loss: 2.286, Test loss: 2.303, Test accuracy: 12.23
Round   3, Train loss: 2.270, Test loss: 2.304, Test accuracy: 11.87
Round   4, Train loss: 2.225, Test loss: 2.306, Test accuracy: 11.22
Round   5, Train loss: 2.187, Test loss: 2.312, Test accuracy: 10.43
Round   6, Train loss: 2.154, Test loss: 2.318, Test accuracy: 10.57
Round   7, Train loss: 2.134, Test loss: 2.322, Test accuracy: 10.33
Round   8, Train loss: 2.116, Test loss: 2.320, Test accuracy: 10.60
Round   9, Train loss: 2.117, Test loss: 2.325, Test accuracy: 10.45
Round  10, Train loss: 2.104, Test loss: 2.324, Test accuracy: 10.35
Round  11, Train loss: 2.101, Test loss: 2.322, Test accuracy: 10.65
Round  12, Train loss: 2.097, Test loss: 2.322, Test accuracy: 11.02
Round  13, Train loss: 2.096, Test loss: 2.323, Test accuracy: 10.88
Round  14, Train loss: 2.104, Test loss: 2.322, Test accuracy: 10.93
Round  15, Train loss: 2.105, Test loss: 2.324, Test accuracy: 10.52
Round  16, Train loss: 2.093, Test loss: 2.324, Test accuracy: 10.48
Round  17, Train loss: 2.103, Test loss: 2.324, Test accuracy: 10.32
Round  18, Train loss: 2.090, Test loss: 2.324, Test accuracy: 10.42
Round  19, Train loss: 2.085, Test loss: 2.324, Test accuracy: 10.88
Round  20, Train loss: 2.082, Test loss: 2.324, Test accuracy: 10.62
Round  21, Train loss: 2.100, Test loss: 2.321, Test accuracy: 10.60
Round  22, Train loss: 2.087, Test loss: 2.320, Test accuracy: 10.70
Round  23, Train loss: 2.089, Test loss: 2.320, Test accuracy: 10.77
Round  24, Train loss: 2.085, Test loss: 2.320, Test accuracy: 10.87
Round  25, Train loss: 2.085, Test loss: 2.320, Test accuracy: 10.70
Round  26, Train loss: 2.073, Test loss: 2.321, Test accuracy: 10.52
Round  27, Train loss: 2.074, Test loss: 2.322, Test accuracy: 10.83
Round  28, Train loss: 2.071, Test loss: 2.322, Test accuracy: 10.60
Round  29, Train loss: 2.086, Test loss: 2.321, Test accuracy: 10.13
Round  30, Train loss: 2.072, Test loss: 2.321, Test accuracy: 10.78
Round  31, Train loss: 2.063, Test loss: 2.322, Test accuracy: 10.97
Round  32, Train loss: 2.080, Test loss: 2.323, Test accuracy: 10.98
Round  33, Train loss: 2.083, Test loss: 2.324, Test accuracy: 11.00
Round  34, Train loss: 2.068, Test loss: 2.324, Test accuracy: 10.23
Round  35, Train loss: 2.077, Test loss: 2.323, Test accuracy: 10.85
Round  36, Train loss: 2.054, Test loss: 2.324, Test accuracy: 10.38
Round  37, Train loss: 2.060, Test loss: 2.325, Test accuracy: 10.50
Round  38, Train loss: 2.062, Test loss: 2.324, Test accuracy: 10.58
Round  39, Train loss: 2.073, Test loss: 2.324, Test accuracy: 10.73
Round  40, Train loss: 2.065, Test loss: 2.323, Test accuracy: 10.98
Round  41, Train loss: 2.063, Test loss: 2.323, Test accuracy: 11.05
Round  42, Train loss: 2.056, Test loss: 2.324, Test accuracy: 11.12
Round  43, Train loss: 2.069, Test loss: 2.323, Test accuracy: 10.85
Round  44, Train loss: 2.045, Test loss: 2.323, Test accuracy: 10.85
Round  45, Train loss: 2.049, Test loss: 2.324, Test accuracy: 10.67
Round  46, Train loss: 2.044, Test loss: 2.324, Test accuracy: 10.35
Round  47, Train loss: 2.023, Test loss: 2.325, Test accuracy: 10.57
Round  48, Train loss: 2.041, Test loss: 2.324, Test accuracy: 10.40
Round  49, Train loss: 2.016, Test loss: 2.325, Test accuracy: 10.18
Round  50, Train loss: 2.020, Test loss: 2.325, Test accuracy: 10.37
Round  51, Train loss: 2.032, Test loss: 2.323, Test accuracy: 10.78
Round  52, Train loss: 2.035, Test loss: 2.323, Test accuracy: 10.80
Round  53, Train loss: 2.006, Test loss: 2.323, Test accuracy: 10.95
Round  54, Train loss: 2.014, Test loss: 2.323, Test accuracy: 10.90
Round  55, Train loss: 2.034, Test loss: 2.322, Test accuracy: 10.95
Round  56, Train loss: 2.000, Test loss: 2.321, Test accuracy: 11.13
Round  57, Train loss: 1.948, Test loss: 2.322, Test accuracy: 11.37
Round  58, Train loss: 2.030, Test loss: 2.322, Test accuracy: 11.22
Round  59, Train loss: 1.978, Test loss: 2.321, Test accuracy: 11.25
Round  60, Train loss: 1.981, Test loss: 2.325, Test accuracy: 10.63
Round  61, Train loss: 1.998, Test loss: 2.324, Test accuracy: 10.90
Round  62, Train loss: 1.956, Test loss: 2.325, Test accuracy: 10.72
Round  63, Train loss: 1.990, Test loss: 2.320, Test accuracy: 11.35
Round  64, Train loss: 1.994, Test loss: 2.318, Test accuracy: 11.72
Round  65, Train loss: 1.987, Test loss: 2.316, Test accuracy: 12.25
Round  66, Train loss: 1.954, Test loss: 2.317, Test accuracy: 11.83
Round  67, Train loss: 1.978, Test loss: 2.317, Test accuracy: 11.87
Round  68, Train loss: 1.932, Test loss: 2.317, Test accuracy: 12.35
Round  69, Train loss: 1.920, Test loss: 2.321, Test accuracy: 11.60
Round  70, Train loss: 1.932, Test loss: 2.317, Test accuracy: 12.02
Round  71, Train loss: 1.914, Test loss: 2.316, Test accuracy: 12.15
Round  72, Train loss: 1.922, Test loss: 2.319, Test accuracy: 11.62
Round  73, Train loss: 1.947, Test loss: 2.322, Test accuracy: 11.55
Round  74, Train loss: 1.921, Test loss: 2.321, Test accuracy: 11.80
Round  75, Train loss: 1.907, Test loss: 2.319, Test accuracy: 11.80
Round  76, Train loss: 1.891, Test loss: 2.319, Test accuracy: 12.18
Round  77, Train loss: 1.917, Test loss: 2.320, Test accuracy: 12.15
Round  78, Train loss: 1.936, Test loss: 2.321, Test accuracy: 11.88
Round  79, Train loss: 1.877, Test loss: 2.321, Test accuracy: 11.90
Round  80, Train loss: 1.882, Test loss: 2.325, Test accuracy: 11.28
Round  81, Train loss: 1.902, Test loss: 2.321, Test accuracy: 11.83
Round  82, Train loss: 1.887, Test loss: 2.320, Test accuracy: 11.45
Round  83, Train loss: 1.893, Test loss: 2.325, Test accuracy: 11.32
Round  84, Train loss: 1.848, Test loss: 2.325, Test accuracy: 11.15
Round  85, Train loss: 1.917, Test loss: 2.323, Test accuracy: 11.68
Round  86, Train loss: 1.885, Test loss: 2.321, Test accuracy: 12.08
Round  87, Train loss: 1.869, Test loss: 2.319, Test accuracy: 12.07
Round  88, Train loss: 1.822, Test loss: 2.320, Test accuracy: 12.15
Round  89, Train loss: 1.839, Test loss: 2.323, Test accuracy: 11.65
Round  90, Train loss: 1.863, Test loss: 2.325, Test accuracy: 11.42
Round  91, Train loss: 1.854, Test loss: 2.331, Test accuracy: 11.17
Round  92, Train loss: 1.805, Test loss: 2.325, Test accuracy: 11.90
Round  93, Train loss: 1.867, Test loss: 2.326, Test accuracy: 11.50
Round  94, Train loss: 1.837, Test loss: 2.324, Test accuracy: 11.90/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.785, Test loss: 2.335, Test accuracy: 10.45
Round  96, Train loss: 1.793, Test loss: 2.328, Test accuracy: 11.47
Round  97, Train loss: 1.798, Test loss: 2.336, Test accuracy: 10.17
Round  98, Train loss: 1.832, Test loss: 2.335, Test accuracy: 10.20
Round  99, Train loss: 1.815, Test loss: 2.327, Test accuracy: 11.42
Final Round, Train loss: 1.813, Test loss: 2.330, Test accuracy: 11.15
Average accuracy final 10 rounds: 11.158333333333331
471.0575635433197
[0.6999070644378662, 1.2360563278198242, 1.7638609409332275, 2.3034448623657227, 2.83735728263855, 3.377251386642456, 3.9183390140533447, 4.448573112487793, 4.990302801132202, 5.5271875858306885, 6.059247732162476, 6.601284742355347, 7.1352698802948, 7.662715911865234, 8.203510284423828, 8.734521389007568, 9.27754807472229, 9.8175630569458, 10.351679563522339, 10.89294695854187, 11.429709434509277, 11.955450773239136, 12.489805698394775, 13.017563819885254, 13.549735069274902, 14.083766222000122, 14.616534233093262, 15.154390811920166, 15.697277545928955, 16.23258876800537, 16.770776748657227, 17.305510997772217, 17.836382389068604, 18.379928827285767, 18.909826040267944, 19.449625730514526, 19.990978002548218, 20.523940801620483, 21.063032627105713, 21.617457628250122, 22.16897678375244, 22.72280478477478, 23.264734745025635, 23.804062604904175, 24.353899717330933, 24.91107177734375, 25.472792387008667, 26.02631449699402, 26.57662343978882, 27.13553810119629, 27.686363220214844, 28.24358320236206, 28.80463218688965, 29.35599446296692, 29.89916491508484, 30.45125937461853, 30.999982357025146, 31.54755187034607, 32.11416506767273, 32.66439437866211, 33.21868371963501, 33.76393532752991, 34.31136655807495, 34.86933183670044, 35.43145251274109, 35.97667956352234, 36.5320029258728, 37.08379054069519, 37.63001275062561, 38.18554663658142, 38.74057960510254, 39.28331542015076, 39.84030294418335, 40.38691759109497, 40.940372705459595, 41.504600524902344, 42.057801485061646, 42.597639083862305, 43.151001930236816, 43.69190168380737, 44.250115633010864, 44.81216073036194, 45.362719774246216, 45.92165756225586, 46.47290802001953, 47.0115020275116, 47.56760311126709, 48.12758493423462, 48.69152331352234, 49.24516248703003, 49.79222130775452, 50.34759020805359, 50.90437412261963, 51.46375250816345, 52.02765607833862, 52.58043456077576, 53.12325620651245, 53.67786931991577, 54.234883546829224, 54.792855978012085, 55.61679530143738]
[9.733333333333333, 9.216666666666667, 12.233333333333333, 11.866666666666667, 11.216666666666667, 10.433333333333334, 10.566666666666666, 10.333333333333334, 10.6, 10.45, 10.35, 10.65, 11.016666666666667, 10.883333333333333, 10.933333333333334, 10.516666666666667, 10.483333333333333, 10.316666666666666, 10.416666666666666, 10.883333333333333, 10.616666666666667, 10.6, 10.7, 10.766666666666667, 10.866666666666667, 10.7, 10.516666666666667, 10.833333333333334, 10.6, 10.133333333333333, 10.783333333333333, 10.966666666666667, 10.983333333333333, 11.0, 10.233333333333333, 10.85, 10.383333333333333, 10.5, 10.583333333333334, 10.733333333333333, 10.983333333333333, 11.05, 11.116666666666667, 10.85, 10.85, 10.666666666666666, 10.35, 10.566666666666666, 10.4, 10.183333333333334, 10.366666666666667, 10.783333333333333, 10.8, 10.95, 10.9, 10.95, 11.133333333333333, 11.366666666666667, 11.216666666666667, 11.25, 10.633333333333333, 10.9, 10.716666666666667, 11.35, 11.716666666666667, 12.25, 11.833333333333334, 11.866666666666667, 12.35, 11.6, 12.016666666666667, 12.15, 11.616666666666667, 11.55, 11.8, 11.8, 12.183333333333334, 12.15, 11.883333333333333, 11.9, 11.283333333333333, 11.833333333333334, 11.45, 11.316666666666666, 11.15, 11.683333333333334, 12.083333333333334, 12.066666666666666, 12.15, 11.65, 11.416666666666666, 11.166666666666666, 11.9, 11.5, 11.9, 10.45, 11.466666666666667, 10.166666666666666, 10.2, 11.416666666666666, 11.15]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.320, Test loss: 2.301, Test accuracy: 25.00
Round   1, Train loss: 2.315, Test loss: 2.298, Test accuracy: 32.26
Round   2, Train loss: 2.307, Test loss: 2.293, Test accuracy: 28.00
Round   3, Train loss: 2.298, Test loss: 2.285, Test accuracy: 25.46
Round   4, Train loss: 2.286, Test loss: 2.272, Test accuracy: 26.93
Round   5, Train loss: 2.265, Test loss: 2.241, Test accuracy: 40.95
Round   6, Train loss: 2.218, Test loss: 2.177, Test accuracy: 46.63
Round   7, Train loss: 2.155, Test loss: 2.105, Test accuracy: 51.16
Round   8, Train loss: 2.079, Test loss: 2.047, Test accuracy: 58.78
Round   9, Train loss: 2.013, Test loss: 1.987, Test accuracy: 65.05
Round  10, Train loss: 1.975, Test loss: 1.927, Test accuracy: 68.66
Round  11, Train loss: 1.919, Test loss: 1.886, Test accuracy: 72.38
Round  12, Train loss: 1.882, Test loss: 1.855, Test accuracy: 74.39
Round  13, Train loss: 1.862, Test loss: 1.823, Test accuracy: 75.50
Round  14, Train loss: 1.841, Test loss: 1.801, Test accuracy: 77.23
Round  15, Train loss: 1.811, Test loss: 1.783, Test accuracy: 79.27
Round  16, Train loss: 1.789, Test loss: 1.769, Test accuracy: 80.83
Round  17, Train loss: 1.785, Test loss: 1.750, Test accuracy: 81.88
Round  18, Train loss: 1.771, Test loss: 1.737, Test accuracy: 82.58
Round  19, Train loss: 1.766, Test loss: 1.726, Test accuracy: 83.22
Round  20, Train loss: 1.756, Test loss: 1.712, Test accuracy: 83.70
Round  21, Train loss: 1.739, Test loss: 1.703, Test accuracy: 84.09
Round  22, Train loss: 1.735, Test loss: 1.696, Test accuracy: 84.56
Round  23, Train loss: 1.717, Test loss: 1.693, Test accuracy: 85.15
Round  24, Train loss: 1.724, Test loss: 1.676, Test accuracy: 85.88
Round  25, Train loss: 1.707, Test loss: 1.669, Test accuracy: 87.00
Round  26, Train loss: 1.687, Test loss: 1.660, Test accuracy: 88.43
Round  27, Train loss: 1.679, Test loss: 1.650, Test accuracy: 89.58
Round  28, Train loss: 1.668, Test loss: 1.644, Test accuracy: 90.36
Round  29, Train loss: 1.672, Test loss: 1.632, Test accuracy: 90.97
Round  30, Train loss: 1.651, Test loss: 1.628, Test accuracy: 91.58
Round  31, Train loss: 1.638, Test loss: 1.621, Test accuracy: 91.90
Round  32, Train loss: 1.640, Test loss: 1.617, Test accuracy: 92.24
Round  33, Train loss: 1.635, Test loss: 1.612, Test accuracy: 92.32
Round  34, Train loss: 1.636, Test loss: 1.603, Test accuracy: 92.65
Round  35, Train loss: 1.620, Test loss: 1.603, Test accuracy: 92.93
Round  36, Train loss: 1.617, Test loss: 1.599, Test accuracy: 93.08
Round  37, Train loss: 1.618, Test loss: 1.593, Test accuracy: 93.20
Round  38, Train loss: 1.611, Test loss: 1.590, Test accuracy: 93.33
Round  39, Train loss: 1.609, Test loss: 1.586, Test accuracy: 93.45
Round  40, Train loss: 1.600, Test loss: 1.586, Test accuracy: 93.58
Round  41, Train loss: 1.614, Test loss: 1.579, Test accuracy: 93.65
Round  42, Train loss: 1.596, Test loss: 1.578, Test accuracy: 93.69
Round  43, Train loss: 1.593, Test loss: 1.577, Test accuracy: 93.82
Round  44, Train loss: 1.586, Test loss: 1.578, Test accuracy: 93.86
Round  45, Train loss: 1.585, Test loss: 1.576, Test accuracy: 93.98
Round  46, Train loss: 1.592, Test loss: 1.571, Test accuracy: 94.08
Round  47, Train loss: 1.580, Test loss: 1.571, Test accuracy: 94.16
Round  48, Train loss: 1.578, Test loss: 1.569, Test accuracy: 94.31
Round  49, Train loss: 1.582, Test loss: 1.566, Test accuracy: 94.43
Round  50, Train loss: 1.574, Test loss: 1.566, Test accuracy: 94.50
Round  51, Train loss: 1.578, Test loss: 1.562, Test accuracy: 94.54
Round  52, Train loss: 1.569, Test loss: 1.564, Test accuracy: 94.59
Round  53, Train loss: 1.573, Test loss: 1.559, Test accuracy: 94.70
Round  54, Train loss: 1.574, Test loss: 1.557, Test accuracy: 94.71
Round  55, Train loss: 1.563, Test loss: 1.558, Test accuracy: 94.77
Round  56, Train loss: 1.559, Test loss: 1.558, Test accuracy: 94.83
Round  57, Train loss: 1.558, Test loss: 1.557, Test accuracy: 94.83
Round  58, Train loss: 1.568, Test loss: 1.553, Test accuracy: 94.94
Round  59, Train loss: 1.557, Test loss: 1.554, Test accuracy: 95.08
Round  60, Train loss: 1.553, Test loss: 1.554, Test accuracy: 95.12
Round  61, Train loss: 1.551, Test loss: 1.553, Test accuracy: 95.03
Round  62, Train loss: 1.559, Test loss: 1.550, Test accuracy: 95.22
Round  63, Train loss: 1.546, Test loss: 1.551, Test accuracy: 95.25
Round  64, Train loss: 1.548, Test loss: 1.550, Test accuracy: 95.26
Round  65, Train loss: 1.545, Test loss: 1.550, Test accuracy: 95.36
Round  66, Train loss: 1.548, Test loss: 1.547, Test accuracy: 95.40
Round  67, Train loss: 1.548, Test loss: 1.547, Test accuracy: 95.39
Round  68, Train loss: 1.551, Test loss: 1.545, Test accuracy: 95.42
Round  69, Train loss: 1.550, Test loss: 1.544, Test accuracy: 95.44
Round  70, Train loss: 1.544, Test loss: 1.544, Test accuracy: 95.59
Round  71, Train loss: 1.542, Test loss: 1.543, Test accuracy: 95.58
Round  72, Train loss: 1.539, Test loss: 1.544, Test accuracy: 95.68
Round  73, Train loss: 1.542, Test loss: 1.543, Test accuracy: 95.56
Round  74, Train loss: 1.536, Test loss: 1.544, Test accuracy: 95.56
Round  75, Train loss: 1.538, Test loss: 1.542, Test accuracy: 95.59
Round  76, Train loss: 1.537, Test loss: 1.541, Test accuracy: 95.60
Round  77, Train loss: 1.534, Test loss: 1.541, Test accuracy: 95.70
Round  78, Train loss: 1.534, Test loss: 1.540, Test accuracy: 95.79
Round  79, Train loss: 1.535, Test loss: 1.540, Test accuracy: 95.82
Round  80, Train loss: 1.535, Test loss: 1.538, Test accuracy: 95.87
Round  81, Train loss: 1.531, Test loss: 1.539, Test accuracy: 95.93
Round  82, Train loss: 1.533, Test loss: 1.538, Test accuracy: 95.90
Round  83, Train loss: 1.527, Test loss: 1.539, Test accuracy: 95.81
Round  84, Train loss: 1.530, Test loss: 1.538, Test accuracy: 95.93
Round  85, Train loss: 1.530, Test loss: 1.537, Test accuracy: 95.95
Round  86, Train loss: 1.527, Test loss: 1.537, Test accuracy: 95.92
Round  87, Train loss: 1.527, Test loss: 1.535, Test accuracy: 96.05
Round  88, Train loss: 1.523, Test loss: 1.536, Test accuracy: 96.05
Round  89, Train loss: 1.523, Test loss: 1.536, Test accuracy: 96.03
Round  90, Train loss: 1.526, Test loss: 1.535, Test accuracy: 96.01
Round  91, Train loss: 1.526, Test loss: 1.534, Test accuracy: 96.09
Round  92, Train loss: 1.522, Test loss: 1.535, Test accuracy: 96.06
Round  93, Train loss: 1.525, Test loss: 1.533, Test accuracy: 96.13/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.524, Test loss: 1.533, Test accuracy: 96.14
Round  95, Train loss: 1.525, Test loss: 1.533, Test accuracy: 96.17
Round  96, Train loss: 1.521, Test loss: 1.533, Test accuracy: 96.20
Round  97, Train loss: 1.518, Test loss: 1.533, Test accuracy: 96.23
Round  98, Train loss: 1.518, Test loss: 1.533, Test accuracy: 96.16
Round  99, Train loss: 1.521, Test loss: 1.532, Test accuracy: 96.21
Final Round, Train loss: 1.492, Test loss: 1.527, Test accuracy: 96.19
Average accuracy final 10 rounds: 96.13950000000001
1471.431144952774
[1.1460802555084229, 2.0705361366271973, 3.0132689476013184, 3.9391472339630127, 4.8765130043029785, 5.792395830154419, 6.7264440059661865, 7.648383140563965, 8.572468996047974, 9.484440565109253, 10.402430772781372, 11.32544732093811, 12.242138862609863, 13.172514200210571, 14.092060565948486, 15.030566453933716, 15.947115182876587, 16.877132177352905, 17.79289221763611, 18.724231004714966, 19.64321780204773, 20.56780457496643, 21.447191953659058, 22.318434238433838, 23.191564083099365, 24.09431219100952, 25.005008935928345, 25.897939682006836, 26.815091371536255, 27.72545289993286, 28.668410778045654, 29.60618305206299, 30.56253218650818, 31.52226686477661, 32.50209164619446, 33.46902680397034, 34.440619468688965, 35.40021085739136, 36.38477921485901, 37.343905448913574, 38.304216146469116, 39.260101556777954, 40.2099027633667, 41.17352247238159, 42.14783692359924, 43.11487030982971, 44.0678927898407, 45.046053647994995, 46.013792753219604, 47.003984212875366, 47.97490119934082, 48.935513496398926, 49.894468784332275, 50.87414836883545, 51.84810400009155, 52.82928204536438, 53.7781023979187, 54.74153733253479, 55.720720529556274, 56.691333532333374, 57.66341805458069, 58.61129927635193, 59.58330702781677, 60.54603338241577, 61.53348398208618, 62.50351071357727, 63.47103404998779, 64.43972945213318, 65.40046381950378, 66.37534070014954, 67.35055351257324, 68.307053565979, 69.26864171028137, 70.25115036964417, 71.21641182899475, 72.20555281639099, 73.1544759273529, 74.13631892204285, 75.10501718521118, 76.0917739868164, 77.05904245376587, 78.01295924186707, 78.983553647995, 79.9528694152832, 80.9199366569519, 81.88628244400024, 82.85202765464783, 83.80795502662659, 84.78981614112854, 85.76802706718445, 86.75260472297668, 87.70956754684448, 88.68230652809143, 89.64753079414368, 90.61155939102173, 91.57351803779602, 92.5350694656372, 93.51421093940735, 94.48152875900269, 95.46036291122437, 96.88942527770996]
[25.0, 32.255, 28.0, 25.465, 26.93, 40.955, 46.635, 51.1625, 58.7775, 65.0525, 68.66, 72.3825, 74.3875, 75.5025, 77.2325, 79.27, 80.83, 81.88, 82.5775, 83.2225, 83.705, 84.09, 84.555, 85.15, 85.8775, 86.995, 88.4325, 89.5825, 90.365, 90.97, 91.585, 91.9025, 92.2375, 92.32, 92.65, 92.9275, 93.085, 93.2, 93.325, 93.455, 93.585, 93.6525, 93.6925, 93.8175, 93.8575, 93.9825, 94.085, 94.1625, 94.315, 94.4325, 94.5, 94.5375, 94.5925, 94.6975, 94.71, 94.77, 94.825, 94.835, 94.945, 95.0825, 95.1225, 95.03, 95.215, 95.25, 95.26, 95.3625, 95.3975, 95.3875, 95.415, 95.435, 95.59, 95.585, 95.68, 95.5625, 95.565, 95.5875, 95.6025, 95.6975, 95.7925, 95.8175, 95.87, 95.9325, 95.9, 95.805, 95.9325, 95.955, 95.92, 96.045, 96.045, 96.0325, 96.0125, 96.09, 96.06, 96.1325, 96.135, 96.1675, 96.2025, 96.23, 96.1575, 96.2075, 96.195]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.149, Test loss: 2.033, Test accuracy: 43.95 

Round   0, Global train loss: 2.149, Global test loss: 2.106, Global test accuracy: 31.91 

Round   1, Train loss: 1.670, Test loss: 1.899, Test accuracy: 56.06 

Round   1, Global train loss: 1.670, Global test loss: 2.068, Global test accuracy: 37.10 

Round   2, Train loss: 1.629, Test loss: 1.826, Test accuracy: 65.72 

Round   2, Global train loss: 1.629, Global test loss: 2.065, Global test accuracy: 38.30 

Round   3, Train loss: 1.547, Test loss: 1.802, Test accuracy: 67.42 

Round   3, Global train loss: 1.547, Global test loss: 2.055, Global test accuracy: 39.39 

Round   4, Train loss: 1.598, Test loss: 1.750, Test accuracy: 73.06 

Round   4, Global train loss: 1.598, Global test loss: 2.064, Global test accuracy: 38.76 

Round   5, Train loss: 1.559, Test loss: 1.722, Test accuracy: 74.79 

Round   5, Global train loss: 1.559, Global test loss: 2.049, Global test accuracy: 39.31 

Round   6, Train loss: 1.594, Test loss: 1.686, Test accuracy: 78.60 

Round   6, Global train loss: 1.594, Global test loss: 2.048, Global test accuracy: 40.17 

Round   7, Train loss: 1.584, Test loss: 1.674, Test accuracy: 80.03 

Round   7, Global train loss: 1.584, Global test loss: 2.022, Global test accuracy: 43.12 

Round   8, Train loss: 1.662, Test loss: 1.609, Test accuracy: 86.61 

Round   8, Global train loss: 1.662, Global test loss: 2.080, Global test accuracy: 36.13 

Round   9, Train loss: 1.590, Test loss: 1.597, Test accuracy: 87.12 

Round   9, Global train loss: 1.590, Global test loss: 2.094, Global test accuracy: 35.27 

Round  10, Train loss: 1.581, Test loss: 1.596, Test accuracy: 87.05 

Round  10, Global train loss: 1.581, Global test loss: 2.059, Global test accuracy: 38.98 

Round  11, Train loss: 1.566, Test loss: 1.582, Test accuracy: 88.43 

Round  11, Global train loss: 1.566, Global test loss: 2.045, Global test accuracy: 40.33 

Round  12, Train loss: 1.578, Test loss: 1.582, Test accuracy: 88.41 

Round  12, Global train loss: 1.578, Global test loss: 2.058, Global test accuracy: 39.44 

Round  13, Train loss: 1.585, Test loss: 1.580, Test accuracy: 88.60 

Round  13, Global train loss: 1.585, Global test loss: 2.071, Global test accuracy: 36.81 

Round  14, Train loss: 1.582, Test loss: 1.579, Test accuracy: 88.64 

Round  14, Global train loss: 1.582, Global test loss: 2.048, Global test accuracy: 39.69 

Round  15, Train loss: 1.581, Test loss: 1.578, Test accuracy: 88.64 

Round  15, Global train loss: 1.581, Global test loss: 2.065, Global test accuracy: 37.40 

Round  16, Train loss: 1.578, Test loss: 1.578, Test accuracy: 88.67 

Round  16, Global train loss: 1.578, Global test loss: 2.061, Global test accuracy: 38.22 

Round  17, Train loss: 1.520, Test loss: 1.578, Test accuracy: 88.65 

Round  17, Global train loss: 1.520, Global test loss: 2.051, Global test accuracy: 39.88 

Round  18, Train loss: 1.469, Test loss: 1.577, Test accuracy: 88.63 

Round  18, Global train loss: 1.469, Global test loss: 2.047, Global test accuracy: 39.96 

Round  19, Train loss: 1.469, Test loss: 1.577, Test accuracy: 88.65 

Round  19, Global train loss: 1.469, Global test loss: 2.076, Global test accuracy: 37.81 

Round  20, Train loss: 1.482, Test loss: 1.562, Test accuracy: 90.18 

Round  20, Global train loss: 1.482, Global test loss: 2.064, Global test accuracy: 38.89 

Round  21, Train loss: 1.468, Test loss: 1.562, Test accuracy: 90.25 

Round  21, Global train loss: 1.468, Global test loss: 2.042, Global test accuracy: 40.85 

Round  22, Train loss: 1.576, Test loss: 1.562, Test accuracy: 90.27 

Round  22, Global train loss: 1.576, Global test loss: 2.048, Global test accuracy: 40.79 

Round  23, Train loss: 1.466, Test loss: 1.562, Test accuracy: 90.27 

Round  23, Global train loss: 1.466, Global test loss: 2.049, Global test accuracy: 40.56 

Round  24, Train loss: 1.576, Test loss: 1.561, Test accuracy: 90.28 

Round  24, Global train loss: 1.576, Global test loss: 2.119, Global test accuracy: 31.99 

Round  25, Train loss: 1.465, Test loss: 1.561, Test accuracy: 90.30 

Round  25, Global train loss: 1.465, Global test loss: 2.060, Global test accuracy: 38.94 

Round  26, Train loss: 1.522, Test loss: 1.561, Test accuracy: 90.30 

Round  26, Global train loss: 1.522, Global test loss: 2.056, Global test accuracy: 39.84 

Round  27, Train loss: 1.466, Test loss: 1.561, Test accuracy: 90.31 

Round  27, Global train loss: 1.466, Global test loss: 2.080, Global test accuracy: 36.01 

Round  28, Train loss: 1.578, Test loss: 1.560, Test accuracy: 90.34 

Round  28, Global train loss: 1.578, Global test loss: 2.052, Global test accuracy: 39.04 

Round  29, Train loss: 1.573, Test loss: 1.560, Test accuracy: 90.33 

Round  29, Global train loss: 1.573, Global test loss: 2.075, Global test accuracy: 37.04 

Round  30, Train loss: 1.575, Test loss: 1.560, Test accuracy: 90.34 

Round  30, Global train loss: 1.575, Global test loss: 2.083, Global test accuracy: 35.98 

Round  31, Train loss: 1.519, Test loss: 1.560, Test accuracy: 90.31 

Round  31, Global train loss: 1.519, Global test loss: 2.061, Global test accuracy: 38.43 

Round  32, Train loss: 1.576, Test loss: 1.560, Test accuracy: 90.33 

Round  32, Global train loss: 1.576, Global test loss: 2.057, Global test accuracy: 39.58 

Round  33, Train loss: 1.572, Test loss: 1.560, Test accuracy: 90.33 

Round  33, Global train loss: 1.572, Global test loss: 2.094, Global test accuracy: 34.92 

Round  34, Train loss: 1.520, Test loss: 1.559, Test accuracy: 90.33 

Round  34, Global train loss: 1.520, Global test loss: 2.091, Global test accuracy: 35.21 

Round  35, Train loss: 1.465, Test loss: 1.559, Test accuracy: 90.33 

Round  35, Global train loss: 1.465, Global test loss: 2.058, Global test accuracy: 38.72 

Round  36, Train loss: 1.519, Test loss: 1.559, Test accuracy: 90.37 

Round  36, Global train loss: 1.519, Global test loss: 2.047, Global test accuracy: 39.71 

Round  37, Train loss: 1.627, Test loss: 1.559, Test accuracy: 90.37 

Round  37, Global train loss: 1.627, Global test loss: 2.081, Global test accuracy: 36.23 

Round  38, Train loss: 1.573, Test loss: 1.559, Test accuracy: 90.38 

Round  38, Global train loss: 1.573, Global test loss: 2.090, Global test accuracy: 36.26 

Round  39, Train loss: 1.518, Test loss: 1.558, Test accuracy: 90.41 

Round  39, Global train loss: 1.518, Global test loss: 2.051, Global test accuracy: 40.18 

Round  40, Train loss: 1.548, Test loss: 1.530, Test accuracy: 93.44 

Round  40, Global train loss: 1.548, Global test loss: 2.081, Global test accuracy: 35.67 

Round  41, Train loss: 1.575, Test loss: 1.529, Test accuracy: 93.49 

Round  41, Global train loss: 1.575, Global test loss: 2.094, Global test accuracy: 34.41 

Round  42, Train loss: 1.518, Test loss: 1.529, Test accuracy: 93.49 

Round  42, Global train loss: 1.518, Global test loss: 2.077, Global test accuracy: 36.85 

Round  43, Train loss: 1.464, Test loss: 1.529, Test accuracy: 93.49 

Round  43, Global train loss: 1.464, Global test loss: 2.075, Global test accuracy: 37.12 

Round  44, Train loss: 1.519, Test loss: 1.529, Test accuracy: 93.44 

Round  44, Global train loss: 1.519, Global test loss: 2.074, Global test accuracy: 37.37 

Round  45, Train loss: 1.518, Test loss: 1.529, Test accuracy: 93.47 

Round  45, Global train loss: 1.518, Global test loss: 2.089, Global test accuracy: 35.24 

Round  46, Train loss: 1.518, Test loss: 1.529, Test accuracy: 93.46 

Round  46, Global train loss: 1.518, Global test loss: 2.093, Global test accuracy: 35.12 

Round  47, Train loss: 1.464, Test loss: 1.529, Test accuracy: 93.45 

Round  47, Global train loss: 1.464, Global test loss: 2.032, Global test accuracy: 41.38 

Round  48, Train loss: 1.464, Test loss: 1.529, Test accuracy: 93.45 

Round  48, Global train loss: 1.464, Global test loss: 2.077, Global test accuracy: 35.94 

Round  49, Train loss: 1.519, Test loss: 1.529, Test accuracy: 93.45 

Round  49, Global train loss: 1.519, Global test loss: 2.058, Global test accuracy: 39.27 

Round  50, Train loss: 1.520, Test loss: 1.528, Test accuracy: 93.49 

Round  50, Global train loss: 1.520, Global test loss: 2.077, Global test accuracy: 36.65 

Round  51, Train loss: 1.520, Test loss: 1.528, Test accuracy: 93.51 

Round  51, Global train loss: 1.520, Global test loss: 2.042, Global test accuracy: 39.98 

Round  52, Train loss: 1.520, Test loss: 1.528, Test accuracy: 93.51 

Round  52, Global train loss: 1.520, Global test loss: 2.049, Global test accuracy: 39.70 

Round  53, Train loss: 1.519, Test loss: 1.528, Test accuracy: 93.51 

Round  53, Global train loss: 1.519, Global test loss: 2.048, Global test accuracy: 39.49 

Round  54, Train loss: 1.463, Test loss: 1.528, Test accuracy: 93.52 

Round  54, Global train loss: 1.463, Global test loss: 2.095, Global test accuracy: 34.75 

Round  55, Train loss: 1.465, Test loss: 1.528, Test accuracy: 93.53 

Round  55, Global train loss: 1.465, Global test loss: 2.046, Global test accuracy: 40.96 

Round  56, Train loss: 1.495, Test loss: 1.512, Test accuracy: 95.14 

Round  56, Global train loss: 1.495, Global test loss: 2.103, Global test accuracy: 34.36 

Round  57, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.13 

Round  57, Global train loss: 1.464, Global test loss: 2.099, Global test accuracy: 35.27 

Round  58, Train loss: 1.465, Test loss: 1.512, Test accuracy: 95.12 

Round  58, Global train loss: 1.465, Global test loss: 2.071, Global test accuracy: 37.68 

Round  59, Train loss: 1.464, Test loss: 1.513, Test accuracy: 95.12 

Round  59, Global train loss: 1.464, Global test loss: 2.023, Global test accuracy: 42.49 

Round  60, Train loss: 1.519, Test loss: 1.513, Test accuracy: 95.12 

Round  60, Global train loss: 1.519, Global test loss: 2.053, Global test accuracy: 39.41 

Round  61, Train loss: 1.465, Test loss: 1.512, Test accuracy: 95.12 

Round  61, Global train loss: 1.465, Global test loss: 2.048, Global test accuracy: 39.64 

Round  62, Train loss: 1.518, Test loss: 1.512, Test accuracy: 95.14 

Round  62, Global train loss: 1.518, Global test loss: 2.068, Global test accuracy: 38.08 

Round  63, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.14 

Round  63, Global train loss: 1.464, Global test loss: 2.085, Global test accuracy: 34.68 

Round  64, Train loss: 1.520, Test loss: 1.512, Test accuracy: 95.16 

Round  64, Global train loss: 1.520, Global test loss: 2.058, Global test accuracy: 39.23 

Round  65, Train loss: 1.465, Test loss: 1.512, Test accuracy: 95.17 

Round  65, Global train loss: 1.465, Global test loss: 2.044, Global test accuracy: 40.53 

Round  66, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.17 

Round  66, Global train loss: 1.464, Global test loss: 2.043, Global test accuracy: 40.92 

Round  67, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.17 

Round  67, Global train loss: 1.464, Global test loss: 2.050, Global test accuracy: 39.62 

Round  68, Train loss: 1.462, Test loss: 1.512, Test accuracy: 95.19 

Round  68, Global train loss: 1.462, Global test loss: 2.075, Global test accuracy: 36.80 

Round  69, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.22 

Round  69, Global train loss: 1.464, Global test loss: 2.044, Global test accuracy: 40.36 

Round  70, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.22 

Round  70, Global train loss: 1.464, Global test loss: 2.071, Global test accuracy: 37.43 

Round  71, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.22 

Round  71, Global train loss: 1.464, Global test loss: 2.072, Global test accuracy: 37.43 

Round  72, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.22 

Round  72, Global train loss: 1.463, Global test loss: 2.089, Global test accuracy: 35.14 

Round  73, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.22 

Round  73, Global train loss: 1.464, Global test loss: 2.108, Global test accuracy: 33.15 

Round  74, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.23 

Round  74, Global train loss: 1.463, Global test loss: 2.070, Global test accuracy: 37.64 

Round  75, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.23 

Round  75, Global train loss: 1.519, Global test loss: 2.054, Global test accuracy: 39.62 

Round  76, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.20 

Round  76, Global train loss: 1.519, Global test loss: 2.044, Global test accuracy: 40.88 

Round  77, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.20 

Round  77, Global train loss: 1.464, Global test loss: 2.052, Global test accuracy: 40.06 

Round  78, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.20 

Round  78, Global train loss: 1.464, Global test loss: 2.062, Global test accuracy: 39.49 

Round  79, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.20 

Round  79, Global train loss: 1.463, Global test loss: 2.081, Global test accuracy: 36.56 

Round  80, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.21 

Round  80, Global train loss: 1.464, Global test loss: 2.036, Global test accuracy: 41.54 

Round  81, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.22 

Round  81, Global train loss: 1.464, Global test loss: 2.086, Global test accuracy: 34.59 

Round  82, Train loss: 1.462, Test loss: 1.511, Test accuracy: 95.21 

Round  82, Global train loss: 1.462, Global test loss: 2.069, Global test accuracy: 37.70 

Round  83, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.21 

Round  83, Global train loss: 1.464, Global test loss: 2.052, Global test accuracy: 39.73 

Round  84, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.21 

Round  84, Global train loss: 1.463, Global test loss: 2.074, Global test accuracy: 37.37 

Round  85, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.21 

Round  85, Global train loss: 1.519, Global test loss: 2.071, Global test accuracy: 37.80 

Round  86, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.21 

Round  86, Global train loss: 1.464, Global test loss: 2.031, Global test accuracy: 42.39 

Round  87, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.22 

Round  87, Global train loss: 1.519, Global test loss: 2.065, Global test accuracy: 37.78 

Round  88, Train loss: 1.462, Test loss: 1.511, Test accuracy: 95.22 

Round  88, Global train loss: 1.462, Global test loss: 2.077, Global test accuracy: 36.55 

Round  89, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.22 

Round  89, Global train loss: 1.463, Global test loss: 2.040, Global test accuracy: 41.55 

Round  90, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.22 

Round  90, Global train loss: 1.463, Global test loss: 2.088, Global test accuracy: 35.04 

Round  91, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.22 

Round  91, Global train loss: 1.463, Global test loss: 2.035, Global test accuracy: 41.93 

Round  92, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.22 

Round  92, Global train loss: 1.464, Global test loss: 2.062, Global test accuracy: 37.94 

Round  93, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.22 

Round  93, Global train loss: 1.519, Global test loss: 2.066, Global test accuracy: 37.45 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.23 

Round  94, Global train loss: 1.464, Global test loss: 2.078, Global test accuracy: 36.42 

Round  95, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.23 

Round  95, Global train loss: 1.463, Global test loss: 2.032, Global test accuracy: 42.06 

Round  96, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.25 

Round  96, Global train loss: 1.519, Global test loss: 2.054, Global test accuracy: 39.38 

Round  97, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.24 

Round  97, Global train loss: 1.463, Global test loss: 2.067, Global test accuracy: 37.47 

Round  98, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.24 

Round  98, Global train loss: 1.464, Global test loss: 2.080, Global test accuracy: 36.57 

Round  99, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.24 

Round  99, Global train loss: 1.519, Global test loss: 2.048, Global test accuracy: 40.27 

Final Round, Train loss: 1.480, Test loss: 1.511, Test accuracy: 95.25 

Final Round, Global train loss: 1.480, Global test loss: 2.048, Global test accuracy: 40.27 

Average accuracy final 10 rounds: 95.23250000000002 

Average global accuracy final 10 rounds: 38.4525 

991.6302058696747
[0.9271705150604248, 1.7493882179260254, 2.5750880241394043, 3.4018630981445312, 4.2234649658203125, 5.0449700355529785, 5.869471311569214, 6.694937705993652, 7.519896030426025, 8.342526912689209, 9.16695499420166, 9.998734474182129, 10.821998596191406, 11.651015996932983, 12.482836723327637, 13.313130617141724, 14.14797592163086, 14.967515707015991, 15.806807279586792, 16.642619848251343, 17.47615694999695, 18.32229232788086, 19.153258323669434, 19.975351333618164, 20.78970432281494, 21.587895154953003, 22.416446924209595, 23.236096382141113, 24.014195203781128, 24.80537486076355, 25.585399389266968, 26.33113384246826, 27.08626937866211, 27.835434675216675, 28.610586404800415, 29.371957302093506, 30.124377965927124, 30.905012369155884, 31.682422876358032, 32.46120262145996, 33.24022912979126, 34.01630663871765, 34.803426027297974, 35.599507093429565, 36.38121795654297, 37.15511751174927, 37.83132004737854, 38.50787329673767, 39.18127965927124, 39.8427619934082, 40.53286290168762, 41.18438649177551, 41.8563289642334, 42.5582058429718, 43.24188947677612, 43.91457962989807, 44.595630407333374, 45.26536679267883, 45.94000291824341, 46.61456751823425, 47.29008603096008, 47.95714807510376, 48.625736951828, 49.286375522613525, 49.963807106018066, 50.62765574455261, 51.301310539245605, 51.98017621040344, 52.64897656440735, 53.323975563049316, 54.000614404678345, 54.6848464012146, 55.36757564544678, 56.046772956848145, 56.75329089164734, 57.448683738708496, 58.13221001625061, 58.80563569068909, 59.49210453033447, 60.18745470046997, 60.866419076919556, 61.55925130844116, 62.23892879486084, 62.912593364715576, 63.59093618392944, 64.26419425010681, 64.95146822929382, 65.63461375236511, 66.30789041519165, 66.99075818061829, 67.67394757270813, 68.36134362220764, 69.02740168571472, 69.71522355079651, 70.39698648452759, 71.08545207977295, 71.77266550064087, 72.45306253433228, 73.13944935798645, 73.81442141532898, 75.18319249153137]
[43.95, 56.05833333333333, 65.71666666666667, 67.425, 73.05833333333334, 74.79166666666667, 78.6, 80.03333333333333, 86.60833333333333, 87.11666666666666, 87.05, 88.43333333333334, 88.40833333333333, 88.6, 88.64166666666667, 88.64166666666667, 88.675, 88.65, 88.63333333333334, 88.65, 90.18333333333334, 90.25, 90.26666666666667, 90.26666666666667, 90.28333333333333, 90.3, 90.3, 90.30833333333334, 90.34166666666667, 90.325, 90.34166666666667, 90.30833333333334, 90.33333333333333, 90.325, 90.325, 90.33333333333333, 90.36666666666666, 90.36666666666666, 90.38333333333334, 90.40833333333333, 93.44166666666666, 93.49166666666666, 93.49166666666666, 93.49166666666666, 93.44166666666666, 93.46666666666667, 93.45833333333333, 93.45, 93.45, 93.45, 93.49166666666666, 93.50833333333334, 93.50833333333334, 93.50833333333334, 93.51666666666667, 93.525, 95.14166666666667, 95.13333333333334, 95.125, 95.11666666666666, 95.11666666666666, 95.11666666666666, 95.14166666666667, 95.14166666666667, 95.15833333333333, 95.16666666666667, 95.16666666666667, 95.16666666666667, 95.19166666666666, 95.21666666666667, 95.225, 95.21666666666667, 95.21666666666667, 95.21666666666667, 95.23333333333333, 95.23333333333333, 95.2, 95.2, 95.2, 95.2, 95.20833333333333, 95.21666666666667, 95.20833333333333, 95.20833333333333, 95.20833333333333, 95.20833333333333, 95.20833333333333, 95.21666666666667, 95.21666666666667, 95.21666666666667, 95.21666666666667, 95.21666666666667, 95.225, 95.225, 95.23333333333333, 95.23333333333333, 95.25, 95.24166666666666, 95.24166666666666, 95.24166666666666, 95.25]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 275, in <module>
    acc_test, loss_test = test_img_local_all(net_glob, args, dataset_test, dict_users_test,
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 133, in test_img_local_all
    a, b = test_img_local(net_local, dataset_test, args, user_idx=idx, idxs=dict_users_test[idx], concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 97, in test_img_local
    test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.214, Test loss: 2.075, Test accuracy: 40.03 

Round   0, Global train loss: 2.214, Global test loss: 2.122, Global test accuracy: 33.52 

Round   1, Train loss: 1.766, Test loss: 1.901, Test accuracy: 57.38 

Round   1, Global train loss: 1.766, Global test loss: 2.062, Global test accuracy: 38.43 

Round   2, Train loss: 1.616, Test loss: 1.841, Test accuracy: 62.01 

Round   2, Global train loss: 1.616, Global test loss: 2.080, Global test accuracy: 34.88 

Round   3, Train loss: 1.566, Test loss: 1.735, Test accuracy: 73.68 

Round   3, Global train loss: 1.566, Global test loss: 2.046, Global test accuracy: 39.73 

Round   4, Train loss: 1.556, Test loss: 1.624, Test accuracy: 87.39 

Round   4, Global train loss: 1.556, Global test loss: 2.038, Global test accuracy: 42.16 

Round   5, Train loss: 1.535, Test loss: 1.580, Test accuracy: 89.95 

Round   5, Global train loss: 1.535, Global test loss: 2.002, Global test accuracy: 46.35 

Round   6, Train loss: 1.488, Test loss: 1.598, Test accuracy: 86.70 

Round   6, Global train loss: 1.488, Global test loss: 2.047, Global test accuracy: 39.63 

Round   7, Train loss: 1.482, Test loss: 1.582, Test accuracy: 88.96 

Round   7, Global train loss: 1.482, Global test loss: 2.038, Global test accuracy: 41.25 

Round   8, Train loss: 1.485, Test loss: 1.552, Test accuracy: 92.02 

Round   8, Global train loss: 1.485, Global test loss: 2.000, Global test accuracy: 47.37 

Round   9, Train loss: 1.487, Test loss: 1.531, Test accuracy: 93.92 

Round   9, Global train loss: 1.487, Global test loss: 2.025, Global test accuracy: 42.00 

Round  10, Train loss: 1.474, Test loss: 1.530, Test accuracy: 94.01 

Round  10, Global train loss: 1.474, Global test loss: 2.074, Global test accuracy: 36.36 

Round  11, Train loss: 1.478, Test loss: 1.528, Test accuracy: 94.12 

Round  11, Global train loss: 1.478, Global test loss: 2.029, Global test accuracy: 42.30 

Round  12, Train loss: 1.473, Test loss: 1.528, Test accuracy: 94.06 

Round  12, Global train loss: 1.473, Global test loss: 2.042, Global test accuracy: 40.14 

Round  13, Train loss: 1.486, Test loss: 1.512, Test accuracy: 95.62 

Round  13, Global train loss: 1.486, Global test loss: 2.013, Global test accuracy: 44.72 

Round  14, Train loss: 1.470, Test loss: 1.512, Test accuracy: 95.54 

Round  14, Global train loss: 1.470, Global test loss: 2.126, Global test accuracy: 31.25 

Round  15, Train loss: 1.468, Test loss: 1.512, Test accuracy: 95.55 

Round  15, Global train loss: 1.468, Global test loss: 2.003, Global test accuracy: 45.30 

Round  16, Train loss: 1.470, Test loss: 1.511, Test accuracy: 95.60 

Round  16, Global train loss: 1.470, Global test loss: 2.064, Global test accuracy: 37.87 

Round  17, Train loss: 1.470, Test loss: 1.510, Test accuracy: 95.62 

Round  17, Global train loss: 1.470, Global test loss: 2.017, Global test accuracy: 43.64 

Round  18, Train loss: 1.470, Test loss: 1.509, Test accuracy: 95.65 

Round  18, Global train loss: 1.470, Global test loss: 2.019, Global test accuracy: 44.38 

Round  19, Train loss: 1.466, Test loss: 1.509, Test accuracy: 95.64 

Round  19, Global train loss: 1.466, Global test loss: 2.021, Global test accuracy: 43.03 

Round  20, Train loss: 1.470, Test loss: 1.509, Test accuracy: 95.62 

Round  20, Global train loss: 1.470, Global test loss: 2.017, Global test accuracy: 44.83 

Round  21, Train loss: 1.467, Test loss: 1.508, Test accuracy: 95.63 

Round  21, Global train loss: 1.467, Global test loss: 2.016, Global test accuracy: 43.20 

Round  22, Train loss: 1.470, Test loss: 1.508, Test accuracy: 95.67 

Round  22, Global train loss: 1.470, Global test loss: 2.011, Global test accuracy: 44.16 

Round  23, Train loss: 1.469, Test loss: 1.508, Test accuracy: 95.67 

Round  23, Global train loss: 1.469, Global test loss: 2.044, Global test accuracy: 40.02 

Round  24, Train loss: 1.468, Test loss: 1.508, Test accuracy: 95.67 

Round  24, Global train loss: 1.468, Global test loss: 2.057, Global test accuracy: 39.24 

Round  25, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.69 

Round  25, Global train loss: 1.467, Global test loss: 2.071, Global test accuracy: 37.06 

Round  26, Train loss: 1.468, Test loss: 1.507, Test accuracy: 95.69 

Round  26, Global train loss: 1.468, Global test loss: 2.039, Global test accuracy: 42.58 

Round  27, Train loss: 1.468, Test loss: 1.507, Test accuracy: 95.68 

Round  27, Global train loss: 1.468, Global test loss: 2.084, Global test accuracy: 36.28 

Round  28, Train loss: 1.468, Test loss: 1.507, Test accuracy: 95.63 

Round  28, Global train loss: 1.468, Global test loss: 2.026, Global test accuracy: 42.02 

Round  29, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.66 

Round  29, Global train loss: 1.467, Global test loss: 2.019, Global test accuracy: 43.58 

Round  30, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.68 

Round  30, Global train loss: 1.467, Global test loss: 2.016, Global test accuracy: 43.73 

Round  31, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.68 

Round  31, Global train loss: 1.466, Global test loss: 2.033, Global test accuracy: 41.88 

Round  32, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.66 

Round  32, Global train loss: 1.466, Global test loss: 2.066, Global test accuracy: 38.29 

Round  33, Train loss: 1.468, Test loss: 1.507, Test accuracy: 95.64 

Round  33, Global train loss: 1.468, Global test loss: 2.052, Global test accuracy: 40.27 

Round  34, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.67 

Round  34, Global train loss: 1.465, Global test loss: 2.001, Global test accuracy: 46.50 

Round  35, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.68 

Round  35, Global train loss: 1.468, Global test loss: 2.004, Global test accuracy: 45.46 

Round  36, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.68 

Round  36, Global train loss: 1.467, Global test loss: 2.078, Global test accuracy: 36.67 

Round  37, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.68 

Round  37, Global train loss: 1.468, Global test loss: 2.005, Global test accuracy: 45.04 

Round  38, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.70 

Round  38, Global train loss: 1.467, Global test loss: 1.998, Global test accuracy: 45.37 

Round  39, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.68 

Round  39, Global train loss: 1.464, Global test loss: 2.015, Global test accuracy: 43.33 

Round  40, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.69 

Round  40, Global train loss: 1.465, Global test loss: 2.031, Global test accuracy: 42.19 

Round  41, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.68 

Round  41, Global train loss: 1.468, Global test loss: 2.031, Global test accuracy: 41.88 

Round  42, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.69 

Round  42, Global train loss: 1.467, Global test loss: 2.030, Global test accuracy: 41.46 

Round  43, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.68 

Round  43, Global train loss: 1.466, Global test loss: 2.050, Global test accuracy: 40.17 

Round  44, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.68 

Round  44, Global train loss: 1.466, Global test loss: 2.011, Global test accuracy: 44.25 

Round  45, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.68 

Round  45, Global train loss: 1.466, Global test loss: 2.018, Global test accuracy: 43.84 

Round  46, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.68 

Round  46, Global train loss: 1.465, Global test loss: 2.082, Global test accuracy: 34.81 

Round  47, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.68 

Round  47, Global train loss: 1.464, Global test loss: 2.021, Global test accuracy: 43.54 

Round  48, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.67 

Round  48, Global train loss: 1.466, Global test loss: 2.013, Global test accuracy: 43.77 

Round  49, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.68 

Round  49, Global train loss: 1.465, Global test loss: 2.001, Global test accuracy: 45.58 

Round  50, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.68 

Round  50, Global train loss: 1.467, Global test loss: 2.043, Global test accuracy: 41.30 

Round  51, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.69 

Round  51, Global train loss: 1.467, Global test loss: 2.035, Global test accuracy: 40.92 

Round  52, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.68 

Round  52, Global train loss: 1.467, Global test loss: 2.018, Global test accuracy: 44.73 

Round  53, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.67 

Round  53, Global train loss: 1.465, Global test loss: 2.005, Global test accuracy: 44.88 

Round  54, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.67 

Round  54, Global train loss: 1.464, Global test loss: 2.050, Global test accuracy: 39.95 

Round  55, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.68 

Round  55, Global train loss: 1.468, Global test loss: 1.998, Global test accuracy: 46.12 

Round  56, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.66 

Round  56, Global train loss: 1.464, Global test loss: 2.051, Global test accuracy: 38.90 

Round  57, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.66 

Round  57, Global train loss: 1.465, Global test loss: 2.005, Global test accuracy: 44.88 

Round  58, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.65 

Round  58, Global train loss: 1.468, Global test loss: 2.008, Global test accuracy: 44.09 

Round  59, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.64 

Round  59, Global train loss: 1.466, Global test loss: 2.015, Global test accuracy: 44.14 

Round  60, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.64 

Round  60, Global train loss: 1.465, Global test loss: 2.011, Global test accuracy: 44.47 

Round  61, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.65 

Round  61, Global train loss: 1.468, Global test loss: 2.006, Global test accuracy: 44.62 

Round  62, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.66 

Round  62, Global train loss: 1.466, Global test loss: 2.028, Global test accuracy: 42.41 

Round  63, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.66 

Round  63, Global train loss: 1.467, Global test loss: 2.032, Global test accuracy: 40.80 

Round  64, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.66 

Round  64, Global train loss: 1.467, Global test loss: 2.028, Global test accuracy: 42.14 

Round  65, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.66 

Round  65, Global train loss: 1.466, Global test loss: 2.040, Global test accuracy: 41.23 

Round  66, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.69 

Round  66, Global train loss: 1.468, Global test loss: 2.004, Global test accuracy: 45.61 

Round  67, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.68 

Round  67, Global train loss: 1.465, Global test loss: 2.093, Global test accuracy: 35.50 

Round  68, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.68 

Round  68, Global train loss: 1.465, Global test loss: 2.042, Global test accuracy: 39.82 

Round  69, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.68 

Round  69, Global train loss: 1.465, Global test loss: 2.022, Global test accuracy: 43.98 

Round  70, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.68 

Round  70, Global train loss: 1.466, Global test loss: 2.074, Global test accuracy: 36.83 

Round  71, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.70 

Round  71, Global train loss: 1.465, Global test loss: 2.043, Global test accuracy: 40.20 

Round  72, Train loss: 1.467, Test loss: 1.505, Test accuracy: 95.69 

Round  72, Global train loss: 1.467, Global test loss: 2.033, Global test accuracy: 42.23 

Round  73, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.67 

Round  73, Global train loss: 1.466, Global test loss: 2.044, Global test accuracy: 39.81 

Round  74, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.68 

Round  74, Global train loss: 1.465, Global test loss: 2.046, Global test accuracy: 40.07 

Round  75, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.68 

Round  75, Global train loss: 1.464, Global test loss: 2.050, Global test accuracy: 39.68 

Round  76, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.68 

Round  76, Global train loss: 1.464, Global test loss: 2.014, Global test accuracy: 43.46 

Round  77, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.69 

Round  77, Global train loss: 1.465, Global test loss: 2.005, Global test accuracy: 45.01 

Round  78, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.71 

Round  78, Global train loss: 1.464, Global test loss: 2.005, Global test accuracy: 45.01 

Round  79, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.72 

Round  79, Global train loss: 1.465, Global test loss: 2.025, Global test accuracy: 42.26 

Round  80, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.71 

Round  80, Global train loss: 1.464, Global test loss: 2.021, Global test accuracy: 43.88 

Round  81, Train loss: 1.467, Test loss: 1.505, Test accuracy: 95.70 

Round  81, Global train loss: 1.467, Global test loss: 2.003, Global test accuracy: 46.38 

Round  82, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.70 

Round  82, Global train loss: 1.466, Global test loss: 2.029, Global test accuracy: 41.81 

Round  83, Train loss: 1.468, Test loss: 1.505, Test accuracy: 95.69 

Round  83, Global train loss: 1.468, Global test loss: 1.999, Global test accuracy: 45.92 

Round  84, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.69 

Round  84, Global train loss: 1.465, Global test loss: 2.033, Global test accuracy: 41.93 

Round  85, Train loss: 1.467, Test loss: 1.505, Test accuracy: 95.68 

Round  85, Global train loss: 1.467, Global test loss: 2.008, Global test accuracy: 44.84 

Round  86, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.69 

Round  86, Global train loss: 1.466, Global test loss: 2.061, Global test accuracy: 38.87 

Round  87, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.68 

Round  87, Global train loss: 1.465, Global test loss: 2.013, Global test accuracy: 43.03 

Round  88, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.68 

Round  88, Global train loss: 1.464, Global test loss: 2.016, Global test accuracy: 43.41 

Round  89, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.68 

Round  89, Global train loss: 1.466, Global test loss: 2.030, Global test accuracy: 42.24 

Round  90, Train loss: 1.467, Test loss: 1.505, Test accuracy: 95.70 

Round  90, Global train loss: 1.467, Global test loss: 2.015, Global test accuracy: 44.08 

Round  91, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.70 

Round  91, Global train loss: 1.466, Global test loss: 2.021, Global test accuracy: 43.42 

Round  92, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.70 

Round  92, Global train loss: 1.466, Global test loss: 2.032, Global test accuracy: 41.62 

Round  93, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.70 

Round  93, Global train loss: 1.465, Global test loss: 2.026, Global test accuracy: 42.85 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.70 

Round  94, Global train loss: 1.464, Global test loss: 2.037, Global test accuracy: 41.61 

Round  95, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.70 

Round  95, Global train loss: 1.465, Global test loss: 2.028, Global test accuracy: 42.13 

Round  96, Train loss: 1.468, Test loss: 1.505, Test accuracy: 95.70 

Round  96, Global train loss: 1.468, Global test loss: 2.072, Global test accuracy: 36.95 

Round  97, Train loss: 1.468, Test loss: 1.505, Test accuracy: 95.70 

Round  97, Global train loss: 1.468, Global test loss: 2.007, Global test accuracy: 44.68 

Round  98, Train loss: 1.468, Test loss: 1.505, Test accuracy: 95.70 

Round  98, Global train loss: 1.468, Global test loss: 2.016, Global test accuracy: 43.33 

Round  99, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.70 

Round  99, Global train loss: 1.466, Global test loss: 2.001, Global test accuracy: 45.27 

Final Round, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.71 

Final Round, Global train loss: 1.466, Global test loss: 2.001, Global test accuracy: 45.27 

Average accuracy final 10 rounds: 95.69999999999999 

Average global accuracy final 10 rounds: 42.59583333333333 

1021.9177551269531
[0.8993685245513916, 1.6930105686187744, 2.4915778636932373, 3.2798564434051514, 4.070462226867676, 4.869231700897217, 5.665306568145752, 6.4557061195373535, 7.23860764503479, 8.029424667358398, 8.815176486968994, 9.59731125831604, 10.37497615814209, 11.169540166854858, 11.966145515441895, 12.752490282058716, 13.543281555175781, 14.336907148361206, 15.136670112609863, 15.92072319984436, 16.705268621444702, 17.49579668045044, 18.28331232070923, 19.052167177200317, 19.83941650390625, 20.626559495925903, 21.404985666275024, 22.194485425949097, 22.983049154281616, 23.765376329421997, 24.568039655685425, 25.347493886947632, 26.13352656364441, 26.928847551345825, 27.716259717941284, 28.495057582855225, 29.289212703704834, 30.076797246932983, 30.85640597343445, 31.645941972732544, 32.426127910614014, 33.21953320503235, 34.00045108795166, 34.78326082229614, 35.57741856575012, 36.368053913116455, 37.14796471595764, 37.938342809677124, 38.72996973991394, 39.50357246398926, 40.28333830833435, 41.07811427116394, 41.872254610061646, 42.66605043411255, 43.4509551525116, 44.24251699447632, 45.031652212142944, 45.81217050552368, 46.59431982040405, 47.37577748298645, 48.15853810310364, 48.93843054771423, 49.727303981781006, 50.51279091835022, 51.29575991630554, 52.0768837928772, 52.76130986213684, 53.4447386264801, 54.11384725570679, 54.79802966117859, 55.479944467544556, 56.152928590774536, 56.84139156341553, 57.63282823562622, 58.41997814178467, 59.20552349090576, 59.991477251052856, 60.78019428253174, 61.56824254989624, 62.36038088798523, 63.16170024871826, 63.96671986579895, 64.7462649345398, 65.52310752868652, 66.24978494644165, 66.96752595901489, 67.68332982063293, 68.39751100540161, 69.12295317649841, 69.84834146499634, 70.5697033405304, 71.30922961235046, 72.0574803352356, 72.79069638252258, 73.55415868759155, 74.32537198066711, 75.0543098449707, 75.7774407863617, 76.51287961006165, 77.24255228042603, 78.71541285514832]
[40.03333333333333, 57.375, 62.00833333333333, 73.68333333333334, 87.39166666666667, 89.95, 86.7, 88.95833333333333, 92.01666666666667, 93.925, 94.00833333333334, 94.11666666666666, 94.05833333333334, 95.625, 95.54166666666667, 95.55, 95.6, 95.61666666666666, 95.65, 95.64166666666667, 95.625, 95.63333333333334, 95.675, 95.66666666666667, 95.675, 95.69166666666666, 95.69166666666666, 95.68333333333334, 95.63333333333334, 95.65833333333333, 95.68333333333334, 95.68333333333334, 95.65833333333333, 95.64166666666667, 95.66666666666667, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.7, 95.68333333333334, 95.69166666666666, 95.68333333333334, 95.69166666666666, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.675, 95.68333333333334, 95.68333333333334, 95.69166666666666, 95.68333333333334, 95.675, 95.675, 95.68333333333334, 95.65833333333333, 95.65833333333333, 95.65, 95.64166666666667, 95.64166666666667, 95.65, 95.65833333333333, 95.65833333333333, 95.65833333333333, 95.65833333333333, 95.69166666666666, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.7, 95.69166666666666, 95.675, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.69166666666666, 95.70833333333333, 95.71666666666667, 95.70833333333333, 95.7, 95.7, 95.69166666666666, 95.69166666666666, 95.68333333333334, 95.69166666666666, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.7, 95.7, 95.7, 95.7, 95.7, 95.7, 95.7, 95.7, 95.7, 95.7, 95.70833333333333]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 275, in <module>
    acc_test, loss_test = test_img_local_all(net_glob, args, dataset_test, dict_users_test,
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 133, in test_img_local_all
    a, b = test_img_local(net_local, dataset_test, args, user_idx=idx, idxs=dict_users_test[idx], concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 97, in test_img_local
    test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 44, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 44, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 44, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.225, Test loss: 2.076, Test accuracy: 25.86 

Round   0, Global train loss: 2.225, Global test loss: 2.072, Global test accuracy: 27.25 

Round   1, Train loss: 2.004, Test loss: 1.934, Test accuracy: 28.93 

Round   1, Global train loss: 2.004, Global test loss: 1.873, Global test accuracy: 31.55 

Round   2, Train loss: 1.873, Test loss: 1.824, Test accuracy: 32.31 

Round   2, Global train loss: 1.873, Global test loss: 1.718, Global test accuracy: 37.16 

Round   3, Train loss: 1.739, Test loss: 1.732, Test accuracy: 36.21 

Round   3, Global train loss: 1.739, Global test loss: 1.598, Global test accuracy: 42.63 

Round   4, Train loss: 1.666, Test loss: 1.699, Test accuracy: 37.35 

Round   4, Global train loss: 1.666, Global test loss: 1.536, Global test accuracy: 43.94 

Round   5, Train loss: 1.619, Test loss: 1.695, Test accuracy: 37.47 

Round   5, Global train loss: 1.619, Global test loss: 1.490, Global test accuracy: 45.62 

Round   6, Train loss: 1.589, Test loss: 1.649, Test accuracy: 39.58 

Round   6, Global train loss: 1.589, Global test loss: 1.456, Global test accuracy: 47.09 

Round   7, Train loss: 1.525, Test loss: 1.613, Test accuracy: 41.23 

Round   7, Global train loss: 1.525, Global test loss: 1.421, Global test accuracy: 49.22 

Round   8, Train loss: 1.483, Test loss: 1.608, Test accuracy: 41.49 

Round   8, Global train loss: 1.483, Global test loss: 1.372, Global test accuracy: 50.82 

Round   9, Train loss: 1.451, Test loss: 1.552, Test accuracy: 43.58 

Round   9, Global train loss: 1.451, Global test loss: 1.342, Global test accuracy: 52.06 

Round  10, Train loss: 1.414, Test loss: 1.520, Test accuracy: 45.11 

Round  10, Global train loss: 1.414, Global test loss: 1.317, Global test accuracy: 52.85 

Round  11, Train loss: 1.373, Test loss: 1.501, Test accuracy: 46.01 

Round  11, Global train loss: 1.373, Global test loss: 1.291, Global test accuracy: 53.68 

Round  12, Train loss: 1.329, Test loss: 1.488, Test accuracy: 46.51 

Round  12, Global train loss: 1.329, Global test loss: 1.282, Global test accuracy: 54.11 

Round  13, Train loss: 1.318, Test loss: 1.456, Test accuracy: 47.72 

Round  13, Global train loss: 1.318, Global test loss: 1.254, Global test accuracy: 55.05 

Round  14, Train loss: 1.254, Test loss: 1.455, Test accuracy: 48.30 

Round  14, Global train loss: 1.254, Global test loss: 1.234, Global test accuracy: 55.87 

Round  15, Train loss: 1.261, Test loss: 1.439, Test accuracy: 48.77 

Round  15, Global train loss: 1.261, Global test loss: 1.207, Global test accuracy: 57.45 

Round  16, Train loss: 1.213, Test loss: 1.422, Test accuracy: 49.64 

Round  16, Global train loss: 1.213, Global test loss: 1.196, Global test accuracy: 57.68 

Round  17, Train loss: 1.199, Test loss: 1.417, Test accuracy: 50.20 

Round  17, Global train loss: 1.199, Global test loss: 1.172, Global test accuracy: 58.94 

Round  18, Train loss: 1.207, Test loss: 1.391, Test accuracy: 51.32 

Round  18, Global train loss: 1.207, Global test loss: 1.153, Global test accuracy: 59.06 

Round  19, Train loss: 1.130, Test loss: 1.383, Test accuracy: 51.83 

Round  19, Global train loss: 1.130, Global test loss: 1.134, Global test accuracy: 59.99 

Round  20, Train loss: 1.103, Test loss: 1.373, Test accuracy: 52.35 

Round  20, Global train loss: 1.103, Global test loss: 1.129, Global test accuracy: 60.20 

Round  21, Train loss: 1.073, Test loss: 1.380, Test accuracy: 52.37 

Round  21, Global train loss: 1.073, Global test loss: 1.133, Global test accuracy: 60.27 

Round  22, Train loss: 1.104, Test loss: 1.364, Test accuracy: 52.84 

Round  22, Global train loss: 1.104, Global test loss: 1.111, Global test accuracy: 61.02 

Round  23, Train loss: 1.061, Test loss: 1.360, Test accuracy: 53.02 

Round  23, Global train loss: 1.061, Global test loss: 1.084, Global test accuracy: 61.79 

Round  24, Train loss: 1.026, Test loss: 1.371, Test accuracy: 53.26 

Round  24, Global train loss: 1.026, Global test loss: 1.086, Global test accuracy: 61.80 

Round  25, Train loss: 0.993, Test loss: 1.359, Test accuracy: 53.88 

Round  25, Global train loss: 0.993, Global test loss: 1.091, Global test accuracy: 62.10 

Round  26, Train loss: 0.967, Test loss: 1.353, Test accuracy: 54.30 

Round  26, Global train loss: 0.967, Global test loss: 1.081, Global test accuracy: 62.21 

Round  27, Train loss: 0.965, Test loss: 1.326, Test accuracy: 55.10 

Round  27, Global train loss: 0.965, Global test loss: 1.063, Global test accuracy: 63.15 

Round  28, Train loss: 0.959, Test loss: 1.330, Test accuracy: 55.21 

Round  28, Global train loss: 0.959, Global test loss: 1.052, Global test accuracy: 63.53 

Round  29, Train loss: 0.918, Test loss: 1.321, Test accuracy: 55.70 

Round  29, Global train loss: 0.918, Global test loss: 1.078, Global test accuracy: 62.50 

Round  30, Train loss: 0.927, Test loss: 1.338, Test accuracy: 55.66 

Round  30, Global train loss: 0.927, Global test loss: 1.070, Global test accuracy: 62.60 

Round  31, Train loss: 0.901, Test loss: 1.333, Test accuracy: 55.93 

Round  31, Global train loss: 0.901, Global test loss: 1.050, Global test accuracy: 63.69 

Round  32, Train loss: 0.879, Test loss: 1.333, Test accuracy: 56.20 

Round  32, Global train loss: 0.879, Global test loss: 1.046, Global test accuracy: 63.80 

Round  33, Train loss: 0.856, Test loss: 1.326, Test accuracy: 56.61 

Round  33, Global train loss: 0.856, Global test loss: 1.041, Global test accuracy: 64.00 

Round  34, Train loss: 0.910, Test loss: 1.318, Test accuracy: 57.27 

Round  34, Global train loss: 0.910, Global test loss: 1.015, Global test accuracy: 65.33 

Round  35, Train loss: 0.854, Test loss: 1.318, Test accuracy: 57.72 

Round  35, Global train loss: 0.854, Global test loss: 1.013, Global test accuracy: 65.54 

Round  36, Train loss: 0.820, Test loss: 1.329, Test accuracy: 57.50 

Round  36, Global train loss: 0.820, Global test loss: 1.035, Global test accuracy: 64.95 

Round  37, Train loss: 0.810, Test loss: 1.308, Test accuracy: 58.05 

Round  37, Global train loss: 0.810, Global test loss: 1.012, Global test accuracy: 65.42 

Round  38, Train loss: 0.801, Test loss: 1.329, Test accuracy: 57.88 

Round  38, Global train loss: 0.801, Global test loss: 1.033, Global test accuracy: 65.24 

Round  39, Train loss: 0.770, Test loss: 1.340, Test accuracy: 58.01 

Round  39, Global train loss: 0.770, Global test loss: 1.026, Global test accuracy: 65.21 

Round  40, Train loss: 0.768, Test loss: 1.346, Test accuracy: 57.69 

Round  40, Global train loss: 0.768, Global test loss: 1.061, Global test accuracy: 64.61 

Round  41, Train loss: 0.831, Test loss: 1.364, Test accuracy: 57.66 

Round  41, Global train loss: 0.831, Global test loss: 1.022, Global test accuracy: 65.26 

Round  42, Train loss: 0.739, Test loss: 1.347, Test accuracy: 58.20 

Round  42, Global train loss: 0.739, Global test loss: 1.020, Global test accuracy: 65.98 

Round  43, Train loss: 0.717, Test loss: 1.373, Test accuracy: 57.96 

Round  43, Global train loss: 0.717, Global test loss: 1.024, Global test accuracy: 65.77 

Round  44, Train loss: 0.722, Test loss: 1.356, Test accuracy: 58.27 

Round  44, Global train loss: 0.722, Global test loss: 1.028, Global test accuracy: 65.84 

Round  45, Train loss: 0.730, Test loss: 1.345, Test accuracy: 58.77 

Round  45, Global train loss: 0.730, Global test loss: 0.996, Global test accuracy: 67.00 

Round  46, Train loss: 0.716, Test loss: 1.344, Test accuracy: 58.79 

Round  46, Global train loss: 0.716, Global test loss: 1.020, Global test accuracy: 66.34 

Round  47, Train loss: 0.775, Test loss: 1.357, Test accuracy: 58.97 

Round  47, Global train loss: 0.775, Global test loss: 1.014, Global test accuracy: 66.56 

Round  48, Train loss: 0.659, Test loss: 1.363, Test accuracy: 59.10 

Round  48, Global train loss: 0.659, Global test loss: 1.028, Global test accuracy: 66.67 

Round  49, Train loss: 0.678, Test loss: 1.368, Test accuracy: 59.16 

Round  49, Global train loss: 0.678, Global test loss: 1.023, Global test accuracy: 66.65 

Round  50, Train loss: 0.685, Test loss: 1.380, Test accuracy: 58.87 

Round  50, Global train loss: 0.685, Global test loss: 1.011, Global test accuracy: 67.01 

Round  51, Train loss: 0.712, Test loss: 1.386, Test accuracy: 59.20 

Round  51, Global train loss: 0.712, Global test loss: 1.015, Global test accuracy: 66.96 

Round  52, Train loss: 0.700, Test loss: 1.367, Test accuracy: 59.76 

Round  52, Global train loss: 0.700, Global test loss: 0.996, Global test accuracy: 67.76 

Round  53, Train loss: 0.660, Test loss: 1.356, Test accuracy: 60.08 

Round  53, Global train loss: 0.660, Global test loss: 1.003, Global test accuracy: 67.49 

Round  54, Train loss: 0.651, Test loss: 1.363, Test accuracy: 60.13 

Round  54, Global train loss: 0.651, Global test loss: 1.011, Global test accuracy: 67.54 

Round  55, Train loss: 0.658, Test loss: 1.388, Test accuracy: 59.97 

Round  55, Global train loss: 0.658, Global test loss: 1.017, Global test accuracy: 67.47 

Round  56, Train loss: 0.624, Test loss: 1.394, Test accuracy: 59.96 

Round  56, Global train loss: 0.624, Global test loss: 1.012, Global test accuracy: 67.57 

Round  57, Train loss: 0.643, Test loss: 1.365, Test accuracy: 60.42 

Round  57, Global train loss: 0.643, Global test loss: 1.001, Global test accuracy: 67.73 

Round  58, Train loss: 0.619, Test loss: 1.362, Test accuracy: 60.74 

Round  58, Global train loss: 0.619, Global test loss: 1.010, Global test accuracy: 67.64 

Round  59, Train loss: 0.596, Test loss: 1.364, Test accuracy: 60.95 

Round  59, Global train loss: 0.596, Global test loss: 1.007, Global test accuracy: 68.22 

Round  60, Train loss: 0.621, Test loss: 1.363, Test accuracy: 61.18 

Round  60, Global train loss: 0.621, Global test loss: 1.017, Global test accuracy: 67.55 

Round  61, Train loss: 0.611, Test loss: 1.370, Test accuracy: 61.02 

Round  61, Global train loss: 0.611, Global test loss: 1.005, Global test accuracy: 68.16 

Round  62, Train loss: 0.607, Test loss: 1.364, Test accuracy: 61.10 

Round  62, Global train loss: 0.607, Global test loss: 0.982, Global test accuracy: 68.80 

Round  63, Train loss: 0.553, Test loss: 1.376, Test accuracy: 61.01 

Round  63, Global train loss: 0.553, Global test loss: 1.021, Global test accuracy: 68.19 

Round  64, Train loss: 0.580, Test loss: 1.383, Test accuracy: 61.24 

Round  64, Global train loss: 0.580, Global test loss: 1.029, Global test accuracy: 68.12 

Round  65, Train loss: 0.560, Test loss: 1.396, Test accuracy: 61.21 

Round  65, Global train loss: 0.560, Global test loss: 1.008, Global test accuracy: 68.99 

Round  66, Train loss: 0.573, Test loss: 1.383, Test accuracy: 61.53 

Round  66, Global train loss: 0.573, Global test loss: 1.028, Global test accuracy: 68.69 

Round  67, Train loss: 0.520, Test loss: 1.382, Test accuracy: 61.83 

Round  67, Global train loss: 0.520, Global test loss: 1.021, Global test accuracy: 69.14 

Round  68, Train loss: 0.552, Test loss: 1.394, Test accuracy: 61.84 

Round  68, Global train loss: 0.552, Global test loss: 1.033, Global test accuracy: 68.54 

Round  69, Train loss: 0.548, Test loss: 1.386, Test accuracy: 61.81 

Round  69, Global train loss: 0.548, Global test loss: 1.035, Global test accuracy: 68.31 

Round  70, Train loss: 0.553, Test loss: 1.395, Test accuracy: 61.83 

Round  70, Global train loss: 0.553, Global test loss: 1.023, Global test accuracy: 68.31 

Round  71, Train loss: 0.522, Test loss: 1.408, Test accuracy: 61.78 

Round  71, Global train loss: 0.522, Global test loss: 1.070, Global test accuracy: 67.57 

Round  72, Train loss: 0.519, Test loss: 1.398, Test accuracy: 62.18 

Round  72, Global train loss: 0.519, Global test loss: 1.049, Global test accuracy: 68.69 

Round  73, Train loss: 0.558, Test loss: 1.388, Test accuracy: 62.22 

Round  73, Global train loss: 0.558, Global test loss: 1.023, Global test accuracy: 68.67 

Round  74, Train loss: 0.535, Test loss: 1.388, Test accuracy: 62.38 

Round  74, Global train loss: 0.535, Global test loss: 1.041, Global test accuracy: 68.23 

Round  75, Train loss: 0.515, Test loss: 1.383, Test accuracy: 62.39 

Round  75, Global train loss: 0.515, Global test loss: 1.039, Global test accuracy: 68.57 

Round  76, Train loss: 0.526, Test loss: 1.392, Test accuracy: 62.19 

Round  76, Global train loss: 0.526, Global test loss: 1.033, Global test accuracy: 68.56 

Round  77, Train loss: 0.525, Test loss: 1.385, Test accuracy: 62.43 

Round  77, Global train loss: 0.525, Global test loss: 1.020, Global test accuracy: 68.83 

Round  78, Train loss: 0.466, Test loss: 1.385, Test accuracy: 62.56 

Round  78, Global train loss: 0.466, Global test loss: 1.038, Global test accuracy: 68.60 

Round  79, Train loss: 0.491, Test loss: 1.407, Test accuracy: 62.37 

Round  79, Global train loss: 0.491, Global test loss: 1.057, Global test accuracy: 68.14 

Round  80, Train loss: 0.515, Test loss: 1.399, Test accuracy: 62.70 

Round  80, Global train loss: 0.515, Global test loss: 1.034, Global test accuracy: 68.54 

Round  81, Train loss: 0.478, Test loss: 1.395, Test accuracy: 62.98 

Round  81, Global train loss: 0.478, Global test loss: 1.048, Global test accuracy: 68.93 

Round  82, Train loss: 0.522, Test loss: 1.387, Test accuracy: 62.77 

Round  82, Global train loss: 0.522, Global test loss: 1.046, Global test accuracy: 68.28 

Round  83, Train loss: 0.463, Test loss: 1.419, Test accuracy: 62.79 

Round  83, Global train loss: 0.463, Global test loss: 1.092, Global test accuracy: 67.90 

Round  84, Train loss: 0.518, Test loss: 1.409, Test accuracy: 62.80 

Round  84, Global train loss: 0.518, Global test loss: 1.049, Global test accuracy: 68.51 

Round  85, Train loss: 0.455, Test loss: 1.424, Test accuracy: 62.68 

Round  85, Global train loss: 0.455, Global test loss: 1.083, Global test accuracy: 68.44 

Round  86, Train loss: 0.509, Test loss: 1.437, Test accuracy: 62.46 

Round  86, Global train loss: 0.509, Global test loss: 1.049, Global test accuracy: 68.67 

Round  87, Train loss: 0.470, Test loss: 1.424, Test accuracy: 62.97 

Round  87, Global train loss: 0.470, Global test loss: 1.067, Global test accuracy: 68.89 

Round  88, Train loss: 0.449, Test loss: 1.422, Test accuracy: 63.15 

Round  88, Global train loss: 0.449, Global test loss: 1.081, Global test accuracy: 68.89 

Round  89, Train loss: 0.443, Test loss: 1.439, Test accuracy: 63.16 

Round  89, Global train loss: 0.443, Global test loss: 1.050, Global test accuracy: 69.03 

Round  90, Train loss: 0.439, Test loss: 1.438, Test accuracy: 63.11 

Round  90, Global train loss: 0.439, Global test loss: 1.084, Global test accuracy: 68.54 

Round  91, Train loss: 0.464, Test loss: 1.443, Test accuracy: 63.21 

Round  91, Global train loss: 0.464, Global test loss: 1.080, Global test accuracy: 69.23 

Round  92, Train loss: 0.449, Test loss: 1.433, Test accuracy: 63.38 

Round  92, Global train loss: 0.449, Global test loss: 1.068, Global test accuracy: 69.51 

Round  93, Train loss: 0.426, Test loss: 1.449, Test accuracy: 63.20 

Round  93, Global train loss: 0.426, Global test loss: 1.095, Global test accuracy: 69.05 

Round  94, Train loss: 0.452, Test loss: 1.466, Test accuracy: 62.84 

Round  94, Global train loss: 0.452, Global test loss: 1.077, Global test accuracy: 69.32 

Round  95, Train loss: 0.443, Test loss: 1.460, Test accuracy: 62.78 

Round  95, Global train loss: 0.443, Global test loss: 1.077, Global test accuracy: 68.95 

Round  96, Train loss: 0.434, Test loss: 1.462, Test accuracy: 63.02 

Round  96, Global train loss: 0.434, Global test loss: 1.073, Global test accuracy: 69.18 

Round  97, Train loss: 0.478, Test loss: 1.457, Test accuracy: 62.96 

Round  97, Global train loss: 0.478, Global test loss: 1.080, Global test accuracy: 68.97 

Round  98, Train loss: 0.449, Test loss: 1.451, Test accuracy: 63.23 

Round  98, Global train loss: 0.449, Global test loss: 1.100, Global test accuracy: 68.84 

Round  99, Train loss: 0.429, Test loss: 1.447, Test accuracy: 63.23 

Round  99, Global train loss: 0.429, Global test loss: 1.080, Global test accuracy: 69.74 

Final Round, Train loss: 0.335, Test loss: 1.646, Test accuracy: 62.41 

Final Round, Global train loss: 0.335, Global test loss: 1.080, Global test accuracy: 69.74 

Average accuracy final 10 rounds: 63.09525 

Average global accuracy final 10 rounds: 69.1335 

2750.518965482712
[1.4191102981567383, 2.5369367599487305, 3.648463010787964, 4.7514612674713135, 5.863318920135498, 6.9686572551727295, 8.083221673965454, 9.301065444946289, 10.444710493087769, 11.561556339263916, 12.678393840789795, 13.800625085830688, 14.902364253997803, 16.023310899734497, 17.137409210205078, 18.238019704818726, 19.335100650787354, 20.470505952835083, 21.574443101882935, 22.696884632110596, 23.874844551086426, 25.00705909729004, 26.10233163833618, 27.21260166168213, 28.336180210113525, 29.44087505340576, 30.45210075378418, 31.46401882171631, 32.47675609588623, 33.491370677948, 34.51067781448364, 35.52295899391174, 36.54135704040527, 37.56004214286804, 38.58257818222046, 39.604838848114014, 40.61521601676941, 41.62768340110779, 42.791232109069824, 43.91703820228577, 44.93145775794983, 45.94464826583862, 46.96505856513977, 47.99238181114197, 49.02038025856018, 50.192840337753296, 51.36817741394043, 52.53734350204468, 53.70948123931885, 54.867433309555054, 56.032235860824585, 57.199230909347534, 58.36499619483948, 59.55068063735962, 60.58250713348389, 61.60277271270752, 62.62712001800537, 63.64636516571045, 64.67342162132263, 65.70101952552795, 66.72135019302368, 67.7456841468811, 68.76289916038513, 69.78810405731201, 70.96031427383423, 72.10039472579956, 73.3409173488617, 74.44102478027344, 75.4507360458374, 76.65025806427002, 77.88059568405151, 79.04473185539246, 80.07260537147522, 81.09681558609009, 82.12123560905457, 83.14402031898499, 84.16152572631836, 85.18962550163269, 86.20555448532104, 87.22197556495667, 88.23032426834106, 89.24093866348267, 90.26193761825562, 91.28047776222229, 92.30439758300781, 93.40482354164124, 94.41379714012146, 95.42365288734436, 96.43995714187622, 97.4594190120697, 98.48684644699097, 99.49722528457642, 100.50860548019409, 101.53613495826721, 102.56156897544861, 103.72936248779297, 104.91617274284363, 106.1131055355072, 107.21643304824829, 108.31621313095093, 110.79237413406372]
[25.8575, 28.93, 32.3075, 36.2125, 37.3525, 37.4725, 39.58, 41.225, 41.49, 43.5775, 45.1075, 46.01, 46.51, 47.72, 48.295, 48.775, 49.64, 50.195, 51.3175, 51.825, 52.355, 52.37, 52.8375, 53.025, 53.2625, 53.885, 54.3025, 55.1025, 55.21, 55.6975, 55.6625, 55.9325, 56.2025, 56.6125, 57.2725, 57.72, 57.5025, 58.0525, 57.88, 58.0075, 57.69, 57.66, 58.2025, 57.96, 58.275, 58.775, 58.7925, 58.9725, 59.1025, 59.155, 58.865, 59.205, 59.755, 60.0825, 60.13, 59.965, 59.9625, 60.4175, 60.745, 60.9475, 61.1825, 61.015, 61.105, 61.01, 61.2425, 61.21, 61.53, 61.8275, 61.8375, 61.815, 61.8325, 61.78, 62.1825, 62.2175, 62.385, 62.3875, 62.1875, 62.4325, 62.565, 62.3675, 62.7, 62.98, 62.7675, 62.7875, 62.795, 62.6775, 62.4575, 62.97, 63.15, 63.1575, 63.1075, 63.2075, 63.3775, 63.195, 62.8425, 62.7825, 63.02, 62.96, 63.2325, 63.2275, 62.41]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.482, Test loss: 2.436, Test accuracy: 19.97 

Round   1, Train loss: 1.002, Test loss: 2.471, Test accuracy: 30.63 

Round   2, Train loss: 0.978, Test loss: 1.650, Test accuracy: 40.73 

Round   3, Train loss: 0.931, Test loss: 1.419, Test accuracy: 45.33 

Round   4, Train loss: 0.796, Test loss: 1.340, Test accuracy: 52.62 

Round   5, Train loss: 0.766, Test loss: 1.002, Test accuracy: 59.65 

Round   6, Train loss: 0.696, Test loss: 1.005, Test accuracy: 60.17 

Round   7, Train loss: 0.803, Test loss: 0.999, Test accuracy: 62.60 

Round   8, Train loss: 0.754, Test loss: 0.822, Test accuracy: 65.10 

Round   9, Train loss: 0.748, Test loss: 0.718, Test accuracy: 68.48 

Round  10, Train loss: 0.682, Test loss: 0.699, Test accuracy: 69.50 

Round  11, Train loss: 0.654, Test loss: 0.666, Test accuracy: 71.55 

Round  12, Train loss: 0.613, Test loss: 0.662, Test accuracy: 71.73 

Round  13, Train loss: 0.701, Test loss: 0.656, Test accuracy: 71.71 

Round  14, Train loss: 0.665, Test loss: 0.627, Test accuracy: 72.90 

Round  15, Train loss: 0.717, Test loss: 0.626, Test accuracy: 73.38 

Round  16, Train loss: 0.607, Test loss: 0.626, Test accuracy: 73.85 

Round  17, Train loss: 0.587, Test loss: 0.611, Test accuracy: 74.54 

Round  18, Train loss: 0.567, Test loss: 0.585, Test accuracy: 74.75 

Round  19, Train loss: 0.633, Test loss: 0.586, Test accuracy: 74.70 

Round  20, Train loss: 0.685, Test loss: 0.578, Test accuracy: 75.77 

Round  21, Train loss: 0.612, Test loss: 0.562, Test accuracy: 75.88 

Round  22, Train loss: 0.644, Test loss: 0.553, Test accuracy: 76.75 

Round  23, Train loss: 0.660, Test loss: 0.561, Test accuracy: 76.47 

Round  24, Train loss: 0.555, Test loss: 0.559, Test accuracy: 76.62 

Round  25, Train loss: 0.597, Test loss: 0.544, Test accuracy: 77.30 

Round  26, Train loss: 0.552, Test loss: 0.546, Test accuracy: 77.35 

Round  27, Train loss: 0.478, Test loss: 0.545, Test accuracy: 77.26 

Round  28, Train loss: 0.566, Test loss: 0.536, Test accuracy: 77.48 

Round  29, Train loss: 0.573, Test loss: 0.530, Test accuracy: 77.81 

Round  30, Train loss: 0.510, Test loss: 0.515, Test accuracy: 78.38 

Round  31, Train loss: 0.514, Test loss: 0.525, Test accuracy: 78.15 

Round  32, Train loss: 0.531, Test loss: 0.519, Test accuracy: 78.29 

Round  33, Train loss: 0.544, Test loss: 0.507, Test accuracy: 78.90 

Round  34, Train loss: 0.480, Test loss: 0.495, Test accuracy: 79.50 

Round  35, Train loss: 0.379, Test loss: 0.489, Test accuracy: 79.89 

Round  36, Train loss: 0.496, Test loss: 0.496, Test accuracy: 79.47 

Round  37, Train loss: 0.389, Test loss: 0.484, Test accuracy: 80.13 

Round  38, Train loss: 0.599, Test loss: 0.487, Test accuracy: 79.92 

Round  39, Train loss: 0.506, Test loss: 0.480, Test accuracy: 80.30 

Round  40, Train loss: 0.455, Test loss: 0.481, Test accuracy: 80.20 

Round  41, Train loss: 0.479, Test loss: 0.479, Test accuracy: 80.35 

Round  42, Train loss: 0.543, Test loss: 0.477, Test accuracy: 80.47 

Round  43, Train loss: 0.458, Test loss: 0.467, Test accuracy: 81.08 

Round  44, Train loss: 0.474, Test loss: 0.459, Test accuracy: 81.20 

Round  45, Train loss: 0.522, Test loss: 0.467, Test accuracy: 80.84 

Round  46, Train loss: 0.397, Test loss: 0.459, Test accuracy: 81.30 

Round  47, Train loss: 0.425, Test loss: 0.466, Test accuracy: 81.12 

Round  48, Train loss: 0.420, Test loss: 0.468, Test accuracy: 81.11 

Round  49, Train loss: 0.444, Test loss: 0.475, Test accuracy: 80.89 

Round  50, Train loss: 0.396, Test loss: 0.464, Test accuracy: 81.46 

Round  51, Train loss: 0.451, Test loss: 0.450, Test accuracy: 81.80 

Round  52, Train loss: 0.354, Test loss: 0.441, Test accuracy: 82.10 

Round  53, Train loss: 0.409, Test loss: 0.442, Test accuracy: 82.12 

Round  54, Train loss: 0.383, Test loss: 0.441, Test accuracy: 82.27 

Round  55, Train loss: 0.336, Test loss: 0.447, Test accuracy: 82.12 

Round  56, Train loss: 0.403, Test loss: 0.439, Test accuracy: 82.29 

Round  57, Train loss: 0.402, Test loss: 0.439, Test accuracy: 82.41 

Round  58, Train loss: 0.389, Test loss: 0.442, Test accuracy: 82.67 

Round  59, Train loss: 0.291, Test loss: 0.448, Test accuracy: 82.04 

Round  60, Train loss: 0.307, Test loss: 0.453, Test accuracy: 81.67 

Round  61, Train loss: 0.398, Test loss: 0.448, Test accuracy: 82.13 

Round  62, Train loss: 0.341, Test loss: 0.461, Test accuracy: 81.55 

Round  63, Train loss: 0.315, Test loss: 0.459, Test accuracy: 81.84 

Round  64, Train loss: 0.343, Test loss: 0.448, Test accuracy: 82.36 

Round  65, Train loss: 0.327, Test loss: 0.458, Test accuracy: 81.88 

Round  66, Train loss: 0.350, Test loss: 0.443, Test accuracy: 82.65 

Round  67, Train loss: 0.294, Test loss: 0.457, Test accuracy: 82.02 

Round  68, Train loss: 0.340, Test loss: 0.459, Test accuracy: 82.20 

Round  69, Train loss: 0.367, Test loss: 0.437, Test accuracy: 83.00 

Round  70, Train loss: 0.415, Test loss: 0.442, Test accuracy: 82.84 

Round  71, Train loss: 0.328, Test loss: 0.450, Test accuracy: 82.83 

Round  72, Train loss: 0.366, Test loss: 0.450, Test accuracy: 82.60 

Round  73, Train loss: 0.286, Test loss: 0.445, Test accuracy: 83.15 

Round  74, Train loss: 0.362, Test loss: 0.440, Test accuracy: 83.15 

Round  75, Train loss: 0.246, Test loss: 0.441, Test accuracy: 83.25 

Round  76, Train loss: 0.311, Test loss: 0.440, Test accuracy: 83.14 

Round  77, Train loss: 0.185, Test loss: 0.440, Test accuracy: 83.40 

Round  78, Train loss: 0.251, Test loss: 0.441, Test accuracy: 83.18 

Round  79, Train loss: 0.319, Test loss: 0.439, Test accuracy: 83.15 

Round  80, Train loss: 0.300, Test loss: 0.448, Test accuracy: 82.92 

Round  81, Train loss: 0.347, Test loss: 0.448, Test accuracy: 83.06 

Round  82, Train loss: 0.297, Test loss: 0.433, Test accuracy: 83.57 

Round  83, Train loss: 0.257, Test loss: 0.449, Test accuracy: 83.01 

Round  84, Train loss: 0.292, Test loss: 0.438, Test accuracy: 83.32 

Round  85, Train loss: 0.343, Test loss: 0.434, Test accuracy: 83.72 

Round  86, Train loss: 0.318, Test loss: 0.443, Test accuracy: 83.64 

Round  87, Train loss: 0.327, Test loss: 0.448, Test accuracy: 83.40 

Round  88, Train loss: 0.315, Test loss: 0.437, Test accuracy: 83.69 

Round  89, Train loss: 0.264, Test loss: 0.439, Test accuracy: 83.39 

Round  90, Train loss: 0.244, Test loss: 0.427, Test accuracy: 83.98 

Round  91, Train loss: 0.296, Test loss: 0.445, Test accuracy: 83.52 

Round  92, Train loss: 0.238, Test loss: 0.454, Test accuracy: 83.35 

Round  93, Train loss: 0.371, Test loss: 0.437, Test accuracy: 83.70 

Round  94, Train loss: 0.291, Test loss: 0.433, Test accuracy: 83.72 

Round  95, Train loss: 0.323, Test loss: 0.443, Test accuracy: 83.52 

Round  96, Train loss: 0.297, Test loss: 0.441, Test accuracy: 83.89 

Round  97, Train loss: 0.216, Test loss: 0.451, Test accuracy: 83.26 

Round  98, Train loss: 0.371, Test loss: 0.456, Test accuracy: 83.39 

Round  99, Train loss: 0.269, Test loss: 0.451, Test accuracy: 83.68 

Final Round, Train loss: 0.227, Test loss: 0.450, Test accuracy: 84.08 

Average accuracy final 10 rounds: 83.60041666666666 

1332.674974679947
[1.2975826263427734, 2.364497184753418, 3.4176628589630127, 4.470544338226318, 5.5329906940460205, 6.59775447845459, 7.6614601612091064, 8.719135522842407, 9.77370285987854, 10.832968711853027, 11.899566888809204, 12.963523864746094, 14.081355810165405, 15.1348135471344, 16.18940234184265, 17.28781771659851, 18.394970417022705, 19.488526344299316, 20.619014263153076, 21.678698778152466, 22.738357305526733, 23.78702211380005, 24.83754587173462, 25.89521884918213, 26.945056915283203, 27.999502182006836, 29.05882978439331, 30.10999846458435, 31.169787883758545, 32.23230719566345, 33.29000186920166, 34.3428795337677, 35.404693365097046, 36.46009111404419, 37.516154289245605, 38.575153827667236, 39.63397240638733, 40.692543745040894, 41.7528395652771, 42.81345725059509, 43.93070673942566, 45.07666230201721, 46.13828206062317, 47.196688413619995, 48.25411534309387, 49.309107542037964, 50.40505790710449, 51.55848050117493, 52.709893465042114, 53.86314153671265, 55.107569217681885, 56.259151220321655, 57.4006769657135, 58.613271713256836, 59.781388998031616, 60.889045000076294, 61.97077965736389, 63.09860825538635, 64.2313802242279, 65.36070513725281, 66.50124645233154, 67.64754295349121, 68.78577041625977, 69.92929077148438, 71.06789779663086, 72.20626306533813, 73.34915852546692, 74.48651218414307, 75.63941931724548, 76.78067827224731, 77.92126107215881, 79.05344009399414, 80.19277262687683, 81.33317160606384, 82.46215796470642, 83.59831070899963, 84.7417061328888, 85.88299584388733, 87.0232903957367, 88.16299366950989, 89.29859638214111, 90.4396243095398, 91.58955478668213, 92.72304630279541, 93.86425042152405, 95.0052444934845, 96.14259076118469, 97.28427505493164, 98.42162728309631, 99.55743050575256, 100.60662627220154, 101.7038459777832, 102.7982816696167, 103.90031909942627, 104.9993667602539, 106.0954008102417, 107.22868871688843, 108.33490681648254, 109.43111634254456, 110.53170943260193, 112.51845383644104]
[19.970833333333335, 30.633333333333333, 40.729166666666664, 45.325, 52.62083333333333, 59.645833333333336, 60.175, 62.6, 65.09583333333333, 68.48333333333333, 69.50416666666666, 71.55416666666666, 71.73333333333333, 71.7125, 72.89583333333333, 73.375, 73.85, 74.5375, 74.75, 74.69583333333334, 75.77083333333333, 75.875, 76.75416666666666, 76.47083333333333, 76.61666666666666, 77.29583333333333, 77.35416666666667, 77.2625, 77.47916666666667, 77.80833333333334, 78.38333333333334, 78.15, 78.2875, 78.89583333333333, 79.50416666666666, 79.89166666666667, 79.47083333333333, 80.12916666666666, 79.91666666666667, 80.30416666666666, 80.20416666666667, 80.34583333333333, 80.47083333333333, 81.075, 81.19583333333334, 80.84166666666667, 81.3, 81.12083333333334, 81.10833333333333, 80.89166666666667, 81.45833333333333, 81.79583333333333, 82.09583333333333, 82.125, 82.27083333333333, 82.12083333333334, 82.2875, 82.40833333333333, 82.67083333333333, 82.0375, 81.675, 82.13333333333334, 81.55, 81.84166666666667, 82.3625, 81.88333333333334, 82.65, 82.01666666666667, 82.2, 83.0, 82.84166666666667, 82.82916666666667, 82.59583333333333, 83.14583333333333, 83.15416666666667, 83.25416666666666, 83.14166666666667, 83.40416666666667, 83.18333333333334, 83.15, 82.91666666666667, 83.05833333333334, 83.56666666666666, 83.00833333333334, 83.31666666666666, 83.72083333333333, 83.64166666666667, 83.40416666666667, 83.6875, 83.3875, 83.97916666666667, 83.52083333333333, 83.34583333333333, 83.70416666666667, 83.71666666666667, 83.52083333333333, 83.8875, 83.2625, 83.3875, 83.67916666666666, 84.075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.538, Test loss: 2.035, Test accuracy: 22.84
Round   1, Train loss: 1.070, Test loss: 1.825, Test accuracy: 32.04
Round   2, Train loss: 0.942, Test loss: 1.379, Test accuracy: 43.08
Round   3, Train loss: 0.835, Test loss: 1.458, Test accuracy: 48.42
Round   4, Train loss: 0.909, Test loss: 1.057, Test accuracy: 60.12
Round   5, Train loss: 0.799, Test loss: 0.974, Test accuracy: 64.39
Round   6, Train loss: 0.801, Test loss: 0.824, Test accuracy: 67.01
Round   7, Train loss: 0.721, Test loss: 0.863, Test accuracy: 67.67
Round   8, Train loss: 0.800, Test loss: 0.741, Test accuracy: 70.81
Round   9, Train loss: 0.729, Test loss: 0.705, Test accuracy: 72.08
Round  10, Train loss: 0.674, Test loss: 0.706, Test accuracy: 72.48
Round  11, Train loss: 0.566, Test loss: 0.696, Test accuracy: 71.88
Round  12, Train loss: 0.704, Test loss: 0.673, Test accuracy: 73.11
Round  13, Train loss: 0.756, Test loss: 0.633, Test accuracy: 75.14
Round  14, Train loss: 0.700, Test loss: 0.632, Test accuracy: 75.11
Round  15, Train loss: 0.691, Test loss: 0.623, Test accuracy: 75.33
Round  16, Train loss: 0.666, Test loss: 0.585, Test accuracy: 76.41
Round  17, Train loss: 0.629, Test loss: 0.581, Test accuracy: 76.82
Round  18, Train loss: 0.636, Test loss: 0.574, Test accuracy: 77.29
Round  19, Train loss: 0.605, Test loss: 0.557, Test accuracy: 78.31
Round  20, Train loss: 0.559, Test loss: 0.538, Test accuracy: 78.57
Round  21, Train loss: 0.485, Test loss: 0.536, Test accuracy: 78.77
Round  22, Train loss: 0.636, Test loss: 0.566, Test accuracy: 77.65
Round  23, Train loss: 0.591, Test loss: 0.529, Test accuracy: 79.14
Round  24, Train loss: 0.465, Test loss: 0.523, Test accuracy: 79.41
Round  25, Train loss: 0.574, Test loss: 0.534, Test accuracy: 79.10
Round  26, Train loss: 0.516, Test loss: 0.521, Test accuracy: 79.48
Round  27, Train loss: 0.515, Test loss: 0.521, Test accuracy: 79.92
Round  28, Train loss: 0.567, Test loss: 0.514, Test accuracy: 79.96
Round  29, Train loss: 0.469, Test loss: 0.493, Test accuracy: 80.45
Round  30, Train loss: 0.583, Test loss: 0.500, Test accuracy: 80.46
Round  31, Train loss: 0.571, Test loss: 0.502, Test accuracy: 80.39
Round  32, Train loss: 0.538, Test loss: 0.495, Test accuracy: 80.27
Round  33, Train loss: 0.475, Test loss: 0.475, Test accuracy: 80.54
Round  34, Train loss: 0.480, Test loss: 0.470, Test accuracy: 81.22
Round  35, Train loss: 0.419, Test loss: 0.461, Test accuracy: 81.38
Round  36, Train loss: 0.579, Test loss: 0.469, Test accuracy: 81.51
Round  37, Train loss: 0.460, Test loss: 0.467, Test accuracy: 81.55
Round  38, Train loss: 0.422, Test loss: 0.453, Test accuracy: 82.12
Round  39, Train loss: 0.480, Test loss: 0.450, Test accuracy: 82.07
Round  40, Train loss: 0.426, Test loss: 0.449, Test accuracy: 81.95
Round  41, Train loss: 0.440, Test loss: 0.447, Test accuracy: 82.42
Round  42, Train loss: 0.474, Test loss: 0.438, Test accuracy: 82.59
Round  43, Train loss: 0.534, Test loss: 0.442, Test accuracy: 82.88
Round  44, Train loss: 0.521, Test loss: 0.448, Test accuracy: 82.70
Round  45, Train loss: 0.486, Test loss: 0.438, Test accuracy: 82.58
Round  46, Train loss: 0.438, Test loss: 0.431, Test accuracy: 82.85
Round  47, Train loss: 0.464, Test loss: 0.434, Test accuracy: 82.88
Round  48, Train loss: 0.530, Test loss: 0.439, Test accuracy: 82.59
Round  49, Train loss: 0.425, Test loss: 0.431, Test accuracy: 82.75
Round  50, Train loss: 0.426, Test loss: 0.432, Test accuracy: 82.86
Round  51, Train loss: 0.440, Test loss: 0.427, Test accuracy: 83.09
Round  52, Train loss: 0.481, Test loss: 0.424, Test accuracy: 83.26
Round  53, Train loss: 0.336, Test loss: 0.420, Test accuracy: 83.26
Round  54, Train loss: 0.417, Test loss: 0.425, Test accuracy: 83.17
Round  55, Train loss: 0.359, Test loss: 0.419, Test accuracy: 83.30
Round  56, Train loss: 0.364, Test loss: 0.419, Test accuracy: 83.10
Round  57, Train loss: 0.330, Test loss: 0.410, Test accuracy: 83.45
Round  58, Train loss: 0.311, Test loss: 0.414, Test accuracy: 83.26
Round  59, Train loss: 0.424, Test loss: 0.415, Test accuracy: 83.54
Round  60, Train loss: 0.419, Test loss: 0.415, Test accuracy: 83.62
Round  61, Train loss: 0.325, Test loss: 0.412, Test accuracy: 83.40
Round  62, Train loss: 0.316, Test loss: 0.412, Test accuracy: 83.46
Round  63, Train loss: 0.271, Test loss: 0.418, Test accuracy: 82.94
Round  64, Train loss: 0.320, Test loss: 0.414, Test accuracy: 83.33
Round  65, Train loss: 0.345, Test loss: 0.419, Test accuracy: 83.28
Round  66, Train loss: 0.364, Test loss: 0.414, Test accuracy: 83.57
Round  67, Train loss: 0.388, Test loss: 0.407, Test accuracy: 83.57
Round  68, Train loss: 0.406, Test loss: 0.410, Test accuracy: 83.77
Round  69, Train loss: 0.341, Test loss: 0.410, Test accuracy: 83.95
Round  70, Train loss: 0.336, Test loss: 0.407, Test accuracy: 84.03
Round  71, Train loss: 0.333, Test loss: 0.399, Test accuracy: 84.33
Round  72, Train loss: 0.337, Test loss: 0.406, Test accuracy: 83.95
Round  73, Train loss: 0.311, Test loss: 0.401, Test accuracy: 84.11
Round  74, Train loss: 0.269, Test loss: 0.394, Test accuracy: 84.33
Round  75, Train loss: 0.356, Test loss: 0.407, Test accuracy: 83.80
Round  76, Train loss: 0.253, Test loss: 0.397, Test accuracy: 84.23
Round  77, Train loss: 0.337, Test loss: 0.395, Test accuracy: 84.27
Round  78, Train loss: 0.327, Test loss: 0.402, Test accuracy: 84.07
Round  79, Train loss: 0.257, Test loss: 0.401, Test accuracy: 84.09
Round  80, Train loss: 0.275, Test loss: 0.399, Test accuracy: 84.20
Round  81, Train loss: 0.380, Test loss: 0.401, Test accuracy: 84.12
Round  82, Train loss: 0.243, Test loss: 0.396, Test accuracy: 84.33
Round  83, Train loss: 0.327, Test loss: 0.397, Test accuracy: 84.45
Round  84, Train loss: 0.351, Test loss: 0.395, Test accuracy: 84.30
Round  85, Train loss: 0.278, Test loss: 0.396, Test accuracy: 84.33
Round  86, Train loss: 0.313, Test loss: 0.394, Test accuracy: 84.46
Round  87, Train loss: 0.295, Test loss: 0.395, Test accuracy: 84.46
Round  88, Train loss: 0.345, Test loss: 0.394, Test accuracy: 84.37
Round  89, Train loss: 0.259, Test loss: 0.394, Test accuracy: 84.60
Round  90, Train loss: 0.236, Test loss: 0.391, Test accuracy: 84.58
Round  91, Train loss: 0.223, Test loss: 0.390, Test accuracy: 84.57
Round  92, Train loss: 0.377, Test loss: 0.394, Test accuracy: 84.50
Round  93, Train loss: 0.245, Test loss: 0.391, Test accuracy: 84.62
Round  94, Train loss: 0.264, Test loss: 0.388, Test accuracy: 84.70
Round  95, Train loss: 0.283, Test loss: 0.393, Test accuracy: 84.57
Round  96, Train loss: 0.250, Test loss: 0.389, Test accuracy: 84.73
Round  97, Train loss: 0.244, Test loss: 0.385, Test accuracy: 84.97
Round  98, Train loss: 0.278, Test loss: 0.385, Test accuracy: 85.12
Round  99, Train loss: 0.252, Test loss: 0.393, Test accuracy: 84.55
Final Round, Train loss: 0.224, Test loss: 0.385, Test accuracy: 84.95
Average accuracy final 10 rounds: 84.69041666666666
1397.4424095153809
[1.7772326469421387, 3.0446717739105225, 4.275237798690796, 5.507526874542236, 6.740595817565918, 7.9680681228637695, 9.201441049575806, 10.430102825164795, 11.658099174499512, 12.887461423873901, 14.120133399963379, 15.342477083206177, 16.571966648101807, 17.796764135360718, 19.020524978637695, 20.250532388687134, 21.47943663597107, 22.706296682357788, 23.931199073791504, 25.15928339958191, 26.386040925979614, 27.623882055282593, 28.984041690826416, 30.21211814880371, 31.44769859313965, 32.67979669570923, 33.905088663101196, 35.12991189956665, 36.3573899269104, 37.583107233047485, 38.97013330459595, 40.35189771652222, 41.73415422439575, 43.114174365997314, 44.50488305091858, 45.8801109790802, 47.24172353744507, 48.609496116638184, 49.9787483215332, 51.42153716087341, 52.6567120552063, 53.89135980606079, 55.126370906829834, 56.35935044288635, 57.5913827419281, 58.82577133178711, 60.06865954399109, 61.29329514503479, 62.52996039390564, 63.75947713851929, 64.99725103378296, 66.2253098487854, 67.46423745155334, 68.69416093826294, 69.92899131774902, 71.15790939331055, 72.39369893074036, 73.76167440414429, 74.99588465690613, 76.21587014198303, 77.5196852684021, 78.74634027481079, 79.97131681442261, 81.19571471214294, 82.43902111053467, 83.66621589660645, 84.8841381072998, 86.11442732810974, 87.34083819389343, 88.57213973999023, 89.88190531730652, 91.12094497680664, 92.36096525192261, 93.59269666671753, 94.83268094062805, 96.08331084251404, 97.31765031814575, 98.62552905082703, 99.86347889900208, 101.08914923667908, 102.32041215896606, 103.53269338607788, 104.74631524085999, 105.96287274360657, 107.18358111381531, 108.4026927947998, 109.6277871131897, 110.8503487110138, 112.12997436523438, 113.37963700294495, 114.60326504707336, 115.83391547203064, 117.16229820251465, 118.4002628326416, 119.65108394622803, 120.89033246040344, 122.1304259300232, 123.36815214157104, 124.61512565612793, 125.84623217582703, 128.03400015830994]
[22.8375, 32.0375, 43.075, 48.416666666666664, 60.11666666666667, 64.39166666666667, 67.0125, 67.675, 70.8125, 72.07916666666667, 72.47916666666667, 71.88333333333334, 73.1125, 75.14166666666667, 75.10833333333333, 75.32916666666667, 76.4125, 76.81666666666666, 77.2875, 78.30833333333334, 78.57083333333334, 78.76666666666667, 77.65, 79.14166666666667, 79.4125, 79.1, 79.47916666666667, 79.91666666666667, 79.9625, 80.44583333333334, 80.4625, 80.39166666666667, 80.26666666666667, 80.5375, 81.21666666666667, 81.375, 81.5125, 81.55416666666666, 82.11666666666666, 82.06666666666666, 81.94583333333334, 82.41666666666667, 82.5875, 82.875, 82.7, 82.575, 82.85, 82.875, 82.5875, 82.75416666666666, 82.8625, 83.0875, 83.2625, 83.25833333333334, 83.17083333333333, 83.30416666666666, 83.09583333333333, 83.44583333333334, 83.2625, 83.5375, 83.62083333333334, 83.4, 83.4625, 82.94166666666666, 83.32916666666667, 83.28333333333333, 83.56666666666666, 83.57083333333334, 83.77083333333333, 83.94583333333334, 84.03333333333333, 84.325, 83.95, 84.1125, 84.32916666666667, 83.8, 84.22916666666667, 84.26666666666667, 84.07083333333334, 84.0875, 84.2, 84.12083333333334, 84.32916666666667, 84.45, 84.3, 84.33333333333333, 84.45833333333333, 84.45833333333333, 84.37083333333334, 84.59583333333333, 84.58333333333333, 84.56666666666666, 84.49583333333334, 84.62083333333334, 84.70416666666667, 84.57083333333334, 84.72916666666667, 84.96666666666667, 85.12083333333334, 84.54583333333333, 84.95]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.264, Test loss: 2.091, Test accuracy: 25.27
Round   1, Train loss: 2.073, Test loss: 1.921, Test accuracy: 28.39
Round   2, Train loss: 1.940, Test loss: 1.761, Test accuracy: 34.94
Round   3, Train loss: 1.852, Test loss: 1.647, Test accuracy: 39.88
Round   4, Train loss: 1.755, Test loss: 1.585, Test accuracy: 42.20
Round   5, Train loss: 1.672, Test loss: 1.514, Test accuracy: 45.19
Round   6, Train loss: 1.621, Test loss: 1.462, Test accuracy: 46.85
Round   7, Train loss: 1.576, Test loss: 1.424, Test accuracy: 48.06
Round   8, Train loss: 1.502, Test loss: 1.398, Test accuracy: 49.07
Round   9, Train loss: 1.572, Test loss: 1.371, Test accuracy: 50.24
Round  10, Train loss: 1.473, Test loss: 1.353, Test accuracy: 51.34
Round  11, Train loss: 1.461, Test loss: 1.328, Test accuracy: 52.81
Round  12, Train loss: 1.427, Test loss: 1.291, Test accuracy: 54.38
Round  13, Train loss: 1.312, Test loss: 1.268, Test accuracy: 55.63
Round  14, Train loss: 1.289, Test loss: 1.256, Test accuracy: 55.29
Round  15, Train loss: 1.271, Test loss: 1.238, Test accuracy: 56.18
Round  16, Train loss: 1.305, Test loss: 1.205, Test accuracy: 57.75
Round  17, Train loss: 1.208, Test loss: 1.206, Test accuracy: 57.50
Round  18, Train loss: 1.228, Test loss: 1.188, Test accuracy: 57.90
Round  19, Train loss: 1.126, Test loss: 1.181, Test accuracy: 58.59
Round  20, Train loss: 1.085, Test loss: 1.182, Test accuracy: 59.05
Round  21, Train loss: 1.095, Test loss: 1.152, Test accuracy: 60.27
Round  22, Train loss: 1.164, Test loss: 1.139, Test accuracy: 60.31
Round  23, Train loss: 1.063, Test loss: 1.134, Test accuracy: 60.54
Round  24, Train loss: 1.007, Test loss: 1.152, Test accuracy: 59.88
Round  25, Train loss: 1.036, Test loss: 1.110, Test accuracy: 60.93
Round  26, Train loss: 1.008, Test loss: 1.122, Test accuracy: 61.32
Round  27, Train loss: 0.911, Test loss: 1.121, Test accuracy: 61.02
Round  28, Train loss: 0.971, Test loss: 1.116, Test accuracy: 61.07
Round  29, Train loss: 0.907, Test loss: 1.104, Test accuracy: 61.93
Round  30, Train loss: 0.916, Test loss: 1.094, Test accuracy: 61.91
Round  31, Train loss: 0.909, Test loss: 1.090, Test accuracy: 62.63
Round  32, Train loss: 0.887, Test loss: 1.100, Test accuracy: 62.34
Round  33, Train loss: 0.825, Test loss: 1.099, Test accuracy: 62.51
Round  34, Train loss: 0.864, Test loss: 1.091, Test accuracy: 63.66
Round  35, Train loss: 0.840, Test loss: 1.094, Test accuracy: 63.48
Round  36, Train loss: 0.834, Test loss: 1.085, Test accuracy: 63.64
Round  37, Train loss: 0.831, Test loss: 1.083, Test accuracy: 64.05
Round  38, Train loss: 0.818, Test loss: 1.067, Test accuracy: 64.84
Round  39, Train loss: 0.746, Test loss: 1.089, Test accuracy: 64.47
Round  40, Train loss: 0.762, Test loss: 1.072, Test accuracy: 64.84
Round  41, Train loss: 0.727, Test loss: 1.090, Test accuracy: 64.48
Round  42, Train loss: 0.699, Test loss: 1.087, Test accuracy: 64.50
Round  43, Train loss: 0.709, Test loss: 1.079, Test accuracy: 64.64
Round  44, Train loss: 0.759, Test loss: 1.047, Test accuracy: 65.50
Round  45, Train loss: 0.716, Test loss: 1.079, Test accuracy: 64.94
Round  46, Train loss: 0.726, Test loss: 1.082, Test accuracy: 64.99
Round  47, Train loss: 0.681, Test loss: 1.087, Test accuracy: 65.16
Round  48, Train loss: 0.698, Test loss: 1.080, Test accuracy: 64.80
Round  49, Train loss: 0.651, Test loss: 1.066, Test accuracy: 65.83
Round  50, Train loss: 0.670, Test loss: 1.063, Test accuracy: 65.70
Round  51, Train loss: 0.652, Test loss: 1.067, Test accuracy: 66.17
Round  52, Train loss: 0.608, Test loss: 1.071, Test accuracy: 66.20
Round  53, Train loss: 0.627, Test loss: 1.057, Test accuracy: 65.86
Round  54, Train loss: 0.591, Test loss: 1.071, Test accuracy: 65.96
Round  55, Train loss: 0.617, Test loss: 1.067, Test accuracy: 66.47
Round  56, Train loss: 0.646, Test loss: 1.052, Test accuracy: 67.01
Round  57, Train loss: 0.578, Test loss: 1.083, Test accuracy: 66.83
Round  58, Train loss: 0.571, Test loss: 1.111, Test accuracy: 65.98
Round  59, Train loss: 0.591, Test loss: 1.096, Test accuracy: 66.15
Round  60, Train loss: 0.554, Test loss: 1.113, Test accuracy: 66.20
Round  61, Train loss: 0.560, Test loss: 1.097, Test accuracy: 67.08
Round  62, Train loss: 0.573, Test loss: 1.080, Test accuracy: 67.37
Round  63, Train loss: 0.578, Test loss: 1.084, Test accuracy: 67.17
Round  64, Train loss: 0.539, Test loss: 1.110, Test accuracy: 66.26
Round  65, Train loss: 0.531, Test loss: 1.118, Test accuracy: 66.06
Round  66, Train loss: 0.481, Test loss: 1.142, Test accuracy: 66.27
Round  67, Train loss: 0.540, Test loss: 1.100, Test accuracy: 66.46
Round  68, Train loss: 0.517, Test loss: 1.118, Test accuracy: 67.45
Round  69, Train loss: 0.464, Test loss: 1.130, Test accuracy: 66.67
Round  70, Train loss: 0.491, Test loss: 1.114, Test accuracy: 66.89
Round  71, Train loss: 0.473, Test loss: 1.108, Test accuracy: 66.64
Round  72, Train loss: 0.511, Test loss: 1.082, Test accuracy: 67.48
Round  73, Train loss: 0.453, Test loss: 1.121, Test accuracy: 67.31
Round  74, Train loss: 0.497, Test loss: 1.137, Test accuracy: 67.28
Round  75, Train loss: 0.448, Test loss: 1.123, Test accuracy: 67.76
Round  76, Train loss: 0.479, Test loss: 1.110, Test accuracy: 67.67
Round  77, Train loss: 0.456, Test loss: 1.098, Test accuracy: 67.73
Round  78, Train loss: 0.424, Test loss: 1.131, Test accuracy: 67.53
Round  79, Train loss: 0.453, Test loss: 1.131, Test accuracy: 67.71
Round  80, Train loss: 0.422, Test loss: 1.145, Test accuracy: 67.47
Round  81, Train loss: 0.430, Test loss: 1.151, Test accuracy: 67.20
Round  82, Train loss: 0.462, Test loss: 1.108, Test accuracy: 67.75
Round  83, Train loss: 0.471, Test loss: 1.115, Test accuracy: 67.84
Round  84, Train loss: 0.377, Test loss: 1.154, Test accuracy: 67.60
Round  85, Train loss: 0.445, Test loss: 1.113, Test accuracy: 67.86
Round  86, Train loss: 0.434, Test loss: 1.123, Test accuracy: 68.12
Round  87, Train loss: 0.408, Test loss: 1.097, Test accuracy: 68.70
Round  88, Train loss: 0.390, Test loss: 1.137, Test accuracy: 68.15
Round  89, Train loss: 0.361, Test loss: 1.137, Test accuracy: 68.23
Round  90, Train loss: 0.386, Test loss: 1.147, Test accuracy: 68.06
Round  91, Train loss: 0.370, Test loss: 1.172, Test accuracy: 67.71
Round  92, Train loss: 0.395, Test loss: 1.142, Test accuracy: 67.91
Round  93, Train loss: 0.363, Test loss: 1.168, Test accuracy: 67.98
Round  94, Train loss: 0.355, Test loss: 1.153, Test accuracy: 68.17
Round  95, Train loss: 0.385, Test loss: 1.182, Test accuracy: 67.77
Round  96, Train loss: 0.430, Test loss: 1.132, Test accuracy: 67.89
Round  97, Train loss: 0.403, Test loss: 1.105, Test accuracy: 68.14
Round  98, Train loss: 0.354, Test loss: 1.164, Test accuracy: 68.31
Round  99, Train loss: 0.316, Test loss: 1.177, Test accuracy: 68.30
Final Round, Train loss: 0.363, Test loss: 1.116, Test accuracy: 69.24
Average accuracy final 10 rounds: 68.02324999999999
2750.597479581833
[3.1557068824768066, 6.0750932693481445, 8.996207237243652, 11.99158763885498, 15.036269664764404, 18.06874680519104, 21.08669424057007, 24.09367322921753, 27.08895492553711, 30.08780002593994, 33.08948636054993, 36.08073902130127, 39.07891511917114, 42.08962821960449, 45.09491729736328, 48.10346698760986, 51.11138105392456, 54.099764823913574, 57.09762144088745, 60.100292682647705, 63.098787784576416, 66.10015964508057, 69.11578464508057, 72.1210777759552, 75.1316819190979, 78.1347644329071, 81.1271755695343, 84.15519666671753, 87.19606852531433, 90.240731716156, 93.28538131713867, 96.43024849891663, 99.31505513191223, 102.14180326461792, 104.81158995628357, 107.47821617126465, 110.14270853996277, 112.81224322319031, 115.48661851882935, 118.15968775749207, 120.82984375953674, 123.50207495689392, 126.17742085456848, 128.86440443992615, 131.5370864868164, 134.2036736011505, 136.8791265487671, 139.54517769813538, 142.21532464027405, 144.88571667671204, 147.55779552459717, 150.22298741340637, 152.89497137069702, 155.56438183784485, 158.23599338531494, 160.92771172523499, 163.61914205551147, 166.3031723499298, 168.97677779197693, 171.65084743499756, 174.32348823547363, 176.99632787704468, 179.66754984855652, 182.3434247970581, 185.02039003372192, 187.70417308807373, 190.37341785430908, 193.06173491477966, 195.74580907821655, 198.4249665737152, 201.12700843811035, 203.82907390594482, 206.52038383483887, 209.21572160720825, 211.89382672309875, 214.57170343399048, 217.26639914512634, 219.96077799797058, 222.65288853645325, 225.3289339542389, 227.99943375587463, 230.6821734905243, 233.36728143692017, 236.0953347682953, 238.81860828399658, 241.57199597358704, 244.29305768013, 246.99744153022766, 249.68779373168945, 252.39989471435547, 255.11858439445496, 257.84334683418274, 260.55949997901917, 263.66851568222046, 266.7355635166168, 269.48909735679626, 272.2078559398651, 274.9168972969055, 277.6215670108795, 280.34079480171204, 283.0955419540405]
[25.265, 28.385, 34.94, 39.8775, 42.195, 45.1925, 46.8475, 48.065, 49.0725, 50.2425, 51.335, 52.81, 54.375, 55.6325, 55.29, 56.1825, 57.75, 57.5, 57.9025, 58.595, 59.055, 60.265, 60.3075, 60.5375, 59.885, 60.9325, 61.3175, 61.025, 61.0725, 61.9325, 61.91, 62.6275, 62.345, 62.5075, 63.665, 63.4825, 63.6425, 64.05, 64.8375, 64.4725, 64.8375, 64.485, 64.5025, 64.6375, 65.5, 64.9375, 64.99, 65.16, 64.8, 65.8325, 65.705, 66.175, 66.2, 65.8575, 65.96, 66.4725, 67.0125, 66.835, 65.9825, 66.1475, 66.205, 67.075, 67.37, 67.1725, 66.26, 66.0575, 66.2675, 66.4625, 67.45, 66.665, 66.89, 66.6425, 67.485, 67.31, 67.2775, 67.7625, 67.6675, 67.7275, 67.5275, 67.71, 67.4725, 67.2025, 67.7475, 67.84, 67.6025, 67.8625, 68.12, 68.7025, 68.1525, 68.23, 68.065, 67.7125, 67.905, 67.98, 68.1675, 67.7675, 67.885, 68.1375, 68.315, 68.2975, 69.2375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.306, Test accuracy: 10.31 

Round   0, Global train loss: 2.303, Global test loss: 2.306, Global test accuracy: 10.30 

Round   1, Train loss: 2.305, Test loss: 2.306, Test accuracy: 10.36 

Round   1, Global train loss: 2.305, Global test loss: 2.305, Global test accuracy: 10.33 

Round   2, Train loss: 2.304, Test loss: 2.305, Test accuracy: 10.29 

Round   2, Global train loss: 2.304, Global test loss: 2.305, Global test accuracy: 10.33 

Round   3, Train loss: 2.304, Test loss: 2.305, Test accuracy: 10.29 

Round   3, Global train loss: 2.304, Global test loss: 2.305, Global test accuracy: 10.15 

Round   4, Train loss: 2.302, Test loss: 2.305, Test accuracy: 10.28 

Round   4, Global train loss: 2.302, Global test loss: 2.304, Global test accuracy: 10.07 

Round   5, Train loss: 2.304, Test loss: 2.304, Test accuracy: 10.20 

Round   5, Global train loss: 2.304, Global test loss: 2.304, Global test accuracy: 10.02 

Round   6, Train loss: 2.301, Test loss: 2.304, Test accuracy: 10.13 

Round   6, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 10.03 

Round   7, Train loss: 2.304, Test loss: 2.304, Test accuracy: 10.08 

Round   7, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.03 

Round   8, Train loss: 2.300, Test loss: 2.303, Test accuracy: 10.06 

Round   8, Global train loss: 2.300, Global test loss: 2.303, Global test accuracy: 10.03 

Round   9, Train loss: 2.304, Test loss: 2.303, Test accuracy: 10.05 

Round   9, Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 10.02 

Round  10, Train loss: 2.300, Test loss: 2.302, Test accuracy: 10.06 

Round  10, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 10.03 

Round  11, Train loss: 2.300, Test loss: 2.302, Test accuracy: 10.07 

Round  11, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 10.02 

Round  12, Train loss: 2.300, Test loss: 2.301, Test accuracy: 10.08 

Round  12, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 10.06 

Round  13, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.08 

Round  13, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.10 

Round  14, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.09 

Round  14, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.03 

Round  15, Train loss: 2.300, Test loss: 2.300, Test accuracy: 10.08 

Round  15, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 10.03 

Round  16, Train loss: 2.298, Test loss: 2.300, Test accuracy: 10.09 

Round  16, Global train loss: 2.298, Global test loss: 2.299, Global test accuracy: 10.07 

Round  17, Train loss: 2.300, Test loss: 2.299, Test accuracy: 10.09 

Round  17, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 10.06 

Round  18, Train loss: 2.297, Test loss: 2.299, Test accuracy: 10.09 

Round  18, Global train loss: 2.297, Global test loss: 2.298, Global test accuracy: 10.08 

Round  19, Train loss: 2.299, Test loss: 2.299, Test accuracy: 10.15 

Round  19, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.23 

Round  20, Train loss: 2.298, Test loss: 2.298, Test accuracy: 10.17 

Round  20, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 10.16 

Round  21, Train loss: 2.297, Test loss: 2.297, Test accuracy: 10.25 

Round  21, Global train loss: 2.297, Global test loss: 2.297, Global test accuracy: 10.24 

Round  22, Train loss: 2.296, Test loss: 2.297, Test accuracy: 10.27 

Round  22, Global train loss: 2.296, Global test loss: 2.296, Global test accuracy: 10.11 

Round  23, Train loss: 2.298, Test loss: 2.297, Test accuracy: 10.33 

Round  23, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 10.38 

Round  24, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.42 

Round  24, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.67 

Round  25, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.55 

Round  25, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.74 

Round  26, Train loss: 2.295, Test loss: 2.295, Test accuracy: 10.66 

Round  26, Global train loss: 2.295, Global test loss: 2.294, Global test accuracy: 10.54 

Round  27, Train loss: 2.293, Test loss: 2.295, Test accuracy: 10.76 

Round  27, Global train loss: 2.293, Global test loss: 2.294, Global test accuracy: 10.75 

Round  28, Train loss: 2.294, Test loss: 2.295, Test accuracy: 10.96 

Round  28, Global train loss: 2.294, Global test loss: 2.293, Global test accuracy: 11.34 

Round  29, Train loss: 2.296, Test loss: 2.294, Test accuracy: 11.32 

Round  29, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 11.98 

Round  30, Train loss: 2.296, Test loss: 2.293, Test accuracy: 11.50 

Round  30, Global train loss: 2.296, Global test loss: 2.292, Global test accuracy: 11.72 

Round  31, Train loss: 2.293, Test loss: 2.293, Test accuracy: 11.68 

Round  31, Global train loss: 2.293, Global test loss: 2.292, Global test accuracy: 11.75 

Round  32, Train loss: 2.294, Test loss: 2.292, Test accuracy: 11.64 

Round  32, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 11.95 

Round  33, Train loss: 2.291, Test loss: 2.292, Test accuracy: 11.71 

Round  33, Global train loss: 2.291, Global test loss: 2.291, Global test accuracy: 12.07 

Round  34, Train loss: 2.291, Test loss: 2.291, Test accuracy: 11.74 

Round  34, Global train loss: 2.291, Global test loss: 2.290, Global test accuracy: 12.23 

Round  35, Train loss: 2.289, Test loss: 2.291, Test accuracy: 12.00 

Round  35, Global train loss: 2.289, Global test loss: 2.289, Global test accuracy: 12.38 

Round  36, Train loss: 2.290, Test loss: 2.290, Test accuracy: 12.14 

Round  36, Global train loss: 2.290, Global test loss: 2.289, Global test accuracy: 12.56 

Round  37, Train loss: 2.292, Test loss: 2.290, Test accuracy: 12.20 

Round  37, Global train loss: 2.292, Global test loss: 2.288, Global test accuracy: 13.06 

Round  38, Train loss: 2.290, Test loss: 2.289, Test accuracy: 12.45 

Round  38, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 12.73 

Round  39, Train loss: 2.290, Test loss: 2.289, Test accuracy: 12.55 

Round  39, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 12.85 

Round  40, Train loss: 2.288, Test loss: 2.288, Test accuracy: 12.61 

Round  40, Global train loss: 2.288, Global test loss: 2.286, Global test accuracy: 12.68 

Round  41, Train loss: 2.289, Test loss: 2.286, Test accuracy: 12.63 

Round  41, Global train loss: 2.289, Global test loss: 2.285, Global test accuracy: 12.77 

Round  42, Train loss: 2.286, Test loss: 2.286, Test accuracy: 12.59 

Round  42, Global train loss: 2.286, Global test loss: 2.284, Global test accuracy: 12.62 

Round  43, Train loss: 2.287, Test loss: 2.285, Test accuracy: 12.63 

Round  43, Global train loss: 2.287, Global test loss: 2.284, Global test accuracy: 12.90 

Round  44, Train loss: 2.286, Test loss: 2.284, Test accuracy: 12.71 

Round  44, Global train loss: 2.286, Global test loss: 2.283, Global test accuracy: 12.94 

Round  45, Train loss: 2.286, Test loss: 2.284, Test accuracy: 12.95 

Round  45, Global train loss: 2.286, Global test loss: 2.282, Global test accuracy: 13.10 

Round  46, Train loss: 2.286, Test loss: 2.283, Test accuracy: 12.79 

Round  46, Global train loss: 2.286, Global test loss: 2.281, Global test accuracy: 12.96 

Round  47, Train loss: 2.283, Test loss: 2.282, Test accuracy: 12.93 

Round  47, Global train loss: 2.283, Global test loss: 2.280, Global test accuracy: 13.12 

Round  48, Train loss: 2.283, Test loss: 2.281, Test accuracy: 13.02 

Round  48, Global train loss: 2.283, Global test loss: 2.279, Global test accuracy: 13.69 

Round  49, Train loss: 2.283, Test loss: 2.281, Test accuracy: 13.40 

Round  49, Global train loss: 2.283, Global test loss: 2.279, Global test accuracy: 14.32 

Round  50, Train loss: 2.282, Test loss: 2.280, Test accuracy: 13.61 

Round  50, Global train loss: 2.282, Global test loss: 2.278, Global test accuracy: 14.38 

Round  51, Train loss: 2.282, Test loss: 2.279, Test accuracy: 13.94 

Round  51, Global train loss: 2.282, Global test loss: 2.277, Global test accuracy: 14.55 

Round  52, Train loss: 2.282, Test loss: 2.279, Test accuracy: 14.25 

Round  52, Global train loss: 2.282, Global test loss: 2.277, Global test accuracy: 14.79 

Round  53, Train loss: 2.285, Test loss: 2.278, Test accuracy: 14.32 

Round  53, Global train loss: 2.285, Global test loss: 2.276, Global test accuracy: 14.72 

Round  54, Train loss: 2.282, Test loss: 2.277, Test accuracy: 14.55 

Round  54, Global train loss: 2.282, Global test loss: 2.275, Global test accuracy: 15.13 

Round  55, Train loss: 2.281, Test loss: 2.276, Test accuracy: 14.71 

Round  55, Global train loss: 2.281, Global test loss: 2.274, Global test accuracy: 15.02 

Round  56, Train loss: 2.278, Test loss: 2.276, Test accuracy: 14.98 

Round  56, Global train loss: 2.278, Global test loss: 2.274, Global test accuracy: 15.32 

Round  57, Train loss: 2.281, Test loss: 2.275, Test accuracy: 15.15 

Round  57, Global train loss: 2.281, Global test loss: 2.273, Global test accuracy: 15.25 

Round  58, Train loss: 2.277, Test loss: 2.274, Test accuracy: 15.10 

Round  58, Global train loss: 2.277, Global test loss: 2.272, Global test accuracy: 15.30 

Round  59, Train loss: 2.277, Test loss: 2.273, Test accuracy: 15.02 

Round  59, Global train loss: 2.277, Global test loss: 2.270, Global test accuracy: 15.35 

Round  60, Train loss: 2.276, Test loss: 2.272, Test accuracy: 15.06 

Round  60, Global train loss: 2.276, Global test loss: 2.270, Global test accuracy: 15.41 

Round  61, Train loss: 2.273, Test loss: 2.271, Test accuracy: 15.19 

Round  61, Global train loss: 2.273, Global test loss: 2.269, Global test accuracy: 15.14 

Round  62, Train loss: 2.275, Test loss: 2.270, Test accuracy: 15.30 

Round  62, Global train loss: 2.275, Global test loss: 2.267, Global test accuracy: 15.35 

Round  63, Train loss: 2.273, Test loss: 2.270, Test accuracy: 15.30 

Round  63, Global train loss: 2.273, Global test loss: 2.266, Global test accuracy: 14.98 

Round  64, Train loss: 2.274, Test loss: 2.268, Test accuracy: 15.28 

Round  64, Global train loss: 2.274, Global test loss: 2.266, Global test accuracy: 15.42 

Round  65, Train loss: 2.274, Test loss: 2.266, Test accuracy: 15.45 

Round  65, Global train loss: 2.274, Global test loss: 2.264, Global test accuracy: 15.63 

Round  66, Train loss: 2.273, Test loss: 2.266, Test accuracy: 15.51 

Round  66, Global train loss: 2.273, Global test loss: 2.263, Global test accuracy: 16.05 

Round  67, Train loss: 2.269, Test loss: 2.264, Test accuracy: 15.88 

Round  67, Global train loss: 2.269, Global test loss: 2.263, Global test accuracy: 16.43 

Round  68, Train loss: 2.268, Test loss: 2.263, Test accuracy: 16.07 

Round  68, Global train loss: 2.268, Global test loss: 2.261, Global test accuracy: 16.66 

Round  69, Train loss: 2.270, Test loss: 2.262, Test accuracy: 16.35 

Round  69, Global train loss: 2.270, Global test loss: 2.260, Global test accuracy: 16.76 

Round  70, Train loss: 2.266, Test loss: 2.261, Test accuracy: 16.55 

Round  70, Global train loss: 2.266, Global test loss: 2.259, Global test accuracy: 16.89 

Round  71, Train loss: 2.271, Test loss: 2.260, Test accuracy: 16.37 

Round  71, Global train loss: 2.271, Global test loss: 2.257, Global test accuracy: 16.55 

Round  72, Train loss: 2.269, Test loss: 2.259, Test accuracy: 16.38 

Round  72, Global train loss: 2.269, Global test loss: 2.255, Global test accuracy: 16.62 

Round  73, Train loss: 2.266, Test loss: 2.257, Test accuracy: 16.57 

Round  73, Global train loss: 2.266, Global test loss: 2.255, Global test accuracy: 16.75 

Round  74, Train loss: 2.267, Test loss: 2.256, Test accuracy: 16.54 

Round  74, Global train loss: 2.267, Global test loss: 2.253, Global test accuracy: 16.54 

Round  75, Train loss: 2.268, Test loss: 2.255, Test accuracy: 16.68 

Round  75, Global train loss: 2.268, Global test loss: 2.252, Global test accuracy: 17.03 

Round  76, Train loss: 2.265, Test loss: 2.254, Test accuracy: 16.99 

Round  76, Global train loss: 2.265, Global test loss: 2.251, Global test accuracy: 17.09 

Round  77, Train loss: 2.263, Test loss: 2.253, Test accuracy: 17.21 

Round  77, Global train loss: 2.263, Global test loss: 2.249, Global test accuracy: 17.58 

Round  78, Train loss: 2.267, Test loss: 2.251, Test accuracy: 17.26 

Round  78, Global train loss: 2.267, Global test loss: 2.247, Global test accuracy: 17.45 

Round  79, Train loss: 2.263, Test loss: 2.249, Test accuracy: 17.26 

Round  79, Global train loss: 2.263, Global test loss: 2.245, Global test accuracy: 17.21 

Round  80, Train loss: 2.264, Test loss: 2.248, Test accuracy: 17.18 

Round  80, Global train loss: 2.264, Global test loss: 2.244, Global test accuracy: 17.24 

Round  81, Train loss: 2.256, Test loss: 2.247, Test accuracy: 17.08 

Round  81, Global train loss: 2.256, Global test loss: 2.243, Global test accuracy: 17.05 

Round  82, Train loss: 2.268, Test loss: 2.245, Test accuracy: 16.93 

Round  82, Global train loss: 2.268, Global test loss: 2.242, Global test accuracy: 16.61 

Round  83, Train loss: 2.259, Test loss: 2.244, Test accuracy: 17.03 

Round  83, Global train loss: 2.259, Global test loss: 2.241, Global test accuracy: 17.50 

Round  84, Train loss: 2.260, Test loss: 2.243, Test accuracy: 17.10 

Round  84, Global train loss: 2.260, Global test loss: 2.239, Global test accuracy: 17.37 

Round  85, Train loss: 2.258, Test loss: 2.242, Test accuracy: 17.34 

Round  85, Global train loss: 2.258, Global test loss: 2.238, Global test accuracy: 17.57 

Round  86, Train loss: 2.257, Test loss: 2.240, Test accuracy: 17.29 

Round  86, Global train loss: 2.257, Global test loss: 2.237, Global test accuracy: 17.52 

Round  87, Train loss: 2.256, Test loss: 2.239, Test accuracy: 17.25 

Round  87, Global train loss: 2.256, Global test loss: 2.235, Global test accuracy: 17.38 

Round  88, Train loss: 2.259, Test loss: 2.237, Test accuracy: 17.31 

Round  88, Global train loss: 2.259, Global test loss: 2.233, Global test accuracy: 17.71 

Round  89, Train loss: 2.256, Test loss: 2.235, Test accuracy: 17.54 

Round  89, Global train loss: 2.256, Global test loss: 2.231, Global test accuracy: 17.95 

Round  90, Train loss: 2.252, Test loss: 2.234, Test accuracy: 17.88 

Round  90, Global train loss: 2.252, Global test loss: 2.229, Global test accuracy: 18.30 

Round  91, Train loss: 2.259, Test loss: 2.232, Test accuracy: 18.13 

Round  91, Global train loss: 2.259, Global test loss: 2.228, Global test accuracy: 18.36 

Round  92, Train loss: 2.244, Test loss: 2.231, Test accuracy: 18.34 

Round  92, Global train loss: 2.244, Global test loss: 2.226, Global test accuracy: 18.79 

Round  93, Train loss: 2.243, Test loss: 2.229, Test accuracy: 18.55 

Round  93, Global train loss: 2.243, Global test loss: 2.224, Global test accuracy: 18.82 

Round  94, Train loss: 2.252, Test loss: 2.227, Test accuracy: 18.56 

Round  94, Global train loss: 2.252, Global test loss: 2.221, Global test accuracy: 18.77 

Round  95, Train loss: 2.254, Test loss: 2.225, Test accuracy: 18.43 

Round  95, Global train loss: 2.254, Global test loss: 2.221, Global test accuracy: 18.16 

Round  96, Train loss: 2.247, Test loss: 2.224, Test accuracy: 18.48 

Round  96, Global train loss: 2.247, Global test loss: 2.221, Global test accuracy: 18.38 

Round  97, Train loss: 2.248, Test loss: 2.223, Test accuracy: 18.54 

Round  97, Global train loss: 2.248, Global test loss: 2.220, Global test accuracy: 18.86 

Round  98, Train loss: 2.244, Test loss: 2.222, Test accuracy: 18.71 

Round  98, Global train loss: 2.244, Global test loss: 2.219, Global test accuracy: 19.25 

Round  99, Train loss: 2.239, Test loss: 2.221, Test accuracy: 18.87 

Round  99, Global train loss: 2.239, Global test loss: 2.217, Global test accuracy: 19.21 

Round 100, Train loss: 2.238, Test loss: 2.220, Test accuracy: 18.93 

Round 100, Global train loss: 2.238, Global test loss: 2.217, Global test accuracy: 19.16 

Round 101, Train loss: 2.240, Test loss: 2.219, Test accuracy: 19.18 

Round 101, Global train loss: 2.240, Global test loss: 2.218, Global test accuracy: 19.44 

Round 102, Train loss: 2.241, Test loss: 2.218, Test accuracy: 19.26 

Round 102, Global train loss: 2.241, Global test loss: 2.216, Global test accuracy: 19.39 

Round 103, Train loss: 2.243, Test loss: 2.217, Test accuracy: 19.14 

Round 103, Global train loss: 2.243, Global test loss: 2.213, Global test accuracy: 18.89 

Round 104, Train loss: 2.239, Test loss: 2.216, Test accuracy: 19.17 

Round 104, Global train loss: 2.239, Global test loss: 2.214, Global test accuracy: 19.54 

Round 105, Train loss: 2.237, Test loss: 2.215, Test accuracy: 19.03 

Round 105, Global train loss: 2.237, Global test loss: 2.211, Global test accuracy: 19.36 

Round 106, Train loss: 2.231, Test loss: 2.214, Test accuracy: 19.34 

Round 106, Global train loss: 2.231, Global test loss: 2.210, Global test accuracy: 19.38 

Round 107, Train loss: 2.231, Test loss: 2.213, Test accuracy: 19.04 

Round 107, Global train loss: 2.231, Global test loss: 2.209, Global test accuracy: 18.60 

Round 108, Train loss: 2.237, Test loss: 2.212, Test accuracy: 18.93 

Round 108, Global train loss: 2.237, Global test loss: 2.206, Global test accuracy: 18.45 

Round 109, Train loss: 2.234, Test loss: 2.210, Test accuracy: 18.86 

Round 109, Global train loss: 2.234, Global test loss: 2.204, Global test accuracy: 18.91 

Round 110, Train loss: 2.232, Test loss: 2.208, Test accuracy: 19.21 

Round 110, Global train loss: 2.232, Global test loss: 2.202, Global test accuracy: 19.09 

Round 111, Train loss: 2.236, Test loss: 2.206, Test accuracy: 19.29 

Round 111, Global train loss: 2.236, Global test loss: 2.201, Global test accuracy: 19.42 

Round 112, Train loss: 2.232, Test loss: 2.204, Test accuracy: 19.17 

Round 112, Global train loss: 2.232, Global test loss: 2.200, Global test accuracy: 19.91 

Round 113, Train loss: 2.226, Test loss: 2.203, Test accuracy: 19.49 

Round 113, Global train loss: 2.226, Global test loss: 2.197, Global test accuracy: 20.30 

Round 114, Train loss: 2.223, Test loss: 2.200, Test accuracy: 20.00 

Round 114, Global train loss: 2.223, Global test loss: 2.194, Global test accuracy: 21.55 

Round 115, Train loss: 2.229, Test loss: 2.198, Test accuracy: 20.38 

Round 115, Global train loss: 2.229, Global test loss: 2.193, Global test accuracy: 20.86 

Round 116, Train loss: 2.228, Test loss: 2.197, Test accuracy: 20.66 

Round 116, Global train loss: 2.228, Global test loss: 2.193, Global test accuracy: 21.14 

Round 117, Train loss: 2.225, Test loss: 2.196, Test accuracy: 20.77 

Round 117, Global train loss: 2.225, Global test loss: 2.191, Global test accuracy: 20.77 

Round 118, Train loss: 2.223, Test loss: 2.195, Test accuracy: 21.21 

Round 118, Global train loss: 2.223, Global test loss: 2.190, Global test accuracy: 20.72 

Round 119, Train loss: 2.215, Test loss: 2.193, Test accuracy: 21.20 

Round 119, Global train loss: 2.215, Global test loss: 2.190, Global test accuracy: 20.40 

Round 120, Train loss: 2.228, Test loss: 2.193, Test accuracy: 21.02 

Round 120, Global train loss: 2.228, Global test loss: 2.189, Global test accuracy: 21.23 

Round 121, Train loss: 2.224, Test loss: 2.192, Test accuracy: 21.17 

Round 121, Global train loss: 2.224, Global test loss: 2.188, Global test accuracy: 21.60 

Round 122, Train loss: 2.224, Test loss: 2.190, Test accuracy: 21.18 

Round 122, Global train loss: 2.224, Global test loss: 2.185, Global test accuracy: 21.32 

Round 123, Train loss: 2.232, Test loss: 2.188, Test accuracy: 21.41 

Round 123, Global train loss: 2.232, Global test loss: 2.183, Global test accuracy: 21.57 

Round 124, Train loss: 2.223, Test loss: 2.186, Test accuracy: 21.30 

Round 124, Global train loss: 2.223, Global test loss: 2.181, Global test accuracy: 21.08 

Round 125, Train loss: 2.234, Test loss: 2.185, Test accuracy: 21.12 

Round 125, Global train loss: 2.234, Global test loss: 2.181, Global test accuracy: 20.45 

Round 126, Train loss: 2.231, Test loss: 2.183, Test accuracy: 21.09 

Round 126, Global train loss: 2.231, Global test loss: 2.181, Global test accuracy: 21.22 

Round 127, Train loss: 2.218, Test loss: 2.184, Test accuracy: 20.91 

Round 127, Global train loss: 2.218, Global test loss: 2.181, Global test accuracy: 20.77 

Round 128, Train loss: 2.223, Test loss: 2.183, Test accuracy: 20.95 

Round 128, Global train loss: 2.223, Global test loss: 2.181, Global test accuracy: 21.89 

Round 129, Train loss: 2.213, Test loss: 2.182, Test accuracy: 21.39 

Round 129, Global train loss: 2.213, Global test loss: 2.180, Global test accuracy: 22.38 

Round 130, Train loss: 2.208, Test loss: 2.183, Test accuracy: 21.79 

Round 130, Global train loss: 2.208, Global test loss: 2.181, Global test accuracy: 23.12 

Round 131, Train loss: 2.210, Test loss: 2.182, Test accuracy: 22.23 

Round 131, Global train loss: 2.210, Global test loss: 2.177, Global test accuracy: 23.11 

Round 132, Train loss: 2.208, Test loss: 2.180, Test accuracy: 22.23 

Round 132, Global train loss: 2.208, Global test loss: 2.175, Global test accuracy: 23.19 

Round 133, Train loss: 2.219, Test loss: 2.178, Test accuracy: 22.04 

Round 133, Global train loss: 2.219, Global test loss: 2.172, Global test accuracy: 22.33 

Round 134, Train loss: 2.213, Test loss: 2.177, Test accuracy: 22.09 

Round 134, Global train loss: 2.213, Global test loss: 2.172, Global test accuracy: 22.20 

Round 135, Train loss: 2.223, Test loss: 2.176, Test accuracy: 21.78 

Round 135, Global train loss: 2.223, Global test loss: 2.171, Global test accuracy: 22.23 

Round 136, Train loss: 2.215, Test loss: 2.173, Test accuracy: 22.07 

Round 136, Global train loss: 2.215, Global test loss: 2.171, Global test accuracy: 22.15 

Round 137, Train loss: 2.212, Test loss: 2.173, Test accuracy: 22.11 

Round 137, Global train loss: 2.212, Global test loss: 2.171, Global test accuracy: 21.60 

Round 138, Train loss: 2.209, Test loss: 2.172, Test accuracy: 22.34 

Round 138, Global train loss: 2.209, Global test loss: 2.171, Global test accuracy: 23.54 

Round 139, Train loss: 2.208, Test loss: 2.172, Test accuracy: 22.83 

Round 139, Global train loss: 2.208, Global test loss: 2.169, Global test accuracy: 24.27 

Round 140, Train loss: 2.208, Test loss: 2.171, Test accuracy: 22.99 

Round 140, Global train loss: 2.208, Global test loss: 2.168, Global test accuracy: 24.04 

Round 141, Train loss: 2.204, Test loss: 2.169, Test accuracy: 22.98 

Round 141, Global train loss: 2.204, Global test loss: 2.165, Global test accuracy: 24.15 

Round 142, Train loss: 2.205, Test loss: 2.168, Test accuracy: 23.02 

Round 142, Global train loss: 2.205, Global test loss: 2.165, Global test accuracy: 24.29 

Round 143, Train loss: 2.203, Test loss: 2.167, Test accuracy: 23.20 

Round 143, Global train loss: 2.203, Global test loss: 2.163, Global test accuracy: 23.83 

Round 144, Train loss: 2.206, Test loss: 2.165, Test accuracy: 23.22 

Round 144, Global train loss: 2.206, Global test loss: 2.161, Global test accuracy: 23.53 

Round 145, Train loss: 2.207, Test loss: 2.164, Test accuracy: 22.96 

Round 145, Global train loss: 2.207, Global test loss: 2.161, Global test accuracy: 22.95 

Round 146, Train loss: 2.202, Test loss: 2.164, Test accuracy: 22.64 

Round 146, Global train loss: 2.202, Global test loss: 2.160, Global test accuracy: 21.99 

Round 147, Train loss: 2.211, Test loss: 2.164, Test accuracy: 22.57 

Round 147, Global train loss: 2.211, Global test loss: 2.161, Global test accuracy: 21.51 

Round 148, Train loss: 2.204, Test loss: 2.164, Test accuracy: 22.48 

Round 148, Global train loss: 2.204, Global test loss: 2.161, Global test accuracy: 22.23 

Round 149, Train loss: 2.192, Test loss: 2.163, Test accuracy: 22.27 

Round 149, Global train loss: 2.192, Global test loss: 2.160, Global test accuracy: 22.02 

Round 150, Train loss: 2.197, Test loss: 2.161, Test accuracy: 22.11 

Round 150, Global train loss: 2.197, Global test loss: 2.159, Global test accuracy: 22.97 

Round 151, Train loss: 2.202, Test loss: 2.160, Test accuracy: 22.00 

Round 151, Global train loss: 2.202, Global test loss: 2.155, Global test accuracy: 21.98 

Round 152, Train loss: 2.201, Test loss: 2.159, Test accuracy: 21.82 

Round 152, Global train loss: 2.201, Global test loss: 2.154, Global test accuracy: 22.57 

Round 153, Train loss: 2.198, Test loss: 2.158, Test accuracy: 22.14 

Round 153, Global train loss: 2.198, Global test loss: 2.153, Global test accuracy: 23.15 

Round 154, Train loss: 2.195, Test loss: 2.156, Test accuracy: 22.53 

Round 154, Global train loss: 2.195, Global test loss: 2.152, Global test accuracy: 23.43 

Round 155, Train loss: 2.192, Test loss: 2.155, Test accuracy: 22.58 

Round 155, Global train loss: 2.192, Global test loss: 2.153, Global test accuracy: 23.28 

Round 156, Train loss: 2.196, Test loss: 2.155, Test accuracy: 22.66 

Round 156, Global train loss: 2.196, Global test loss: 2.153, Global test accuracy: 23.60 

Round 157, Train loss: 2.189, Test loss: 2.156, Test accuracy: 22.84 

Round 157, Global train loss: 2.189, Global test loss: 2.154, Global test accuracy: 23.88 

Round 158, Train loss: 2.195, Test loss: 2.154, Test accuracy: 23.10 

Round 158, Global train loss: 2.195, Global test loss: 2.152, Global test accuracy: 23.90 

Round 159, Train loss: 2.198, Test loss: 2.153, Test accuracy: 23.32 

Round 159, Global train loss: 2.198, Global test loss: 2.152, Global test accuracy: 23.57 

Round 160, Train loss: 2.195, Test loss: 2.153, Test accuracy: 23.45 

Round 160, Global train loss: 2.195, Global test loss: 2.152, Global test accuracy: 23.66 

Round 161, Train loss: 2.193, Test loss: 2.154, Test accuracy: 23.40 

Round 161, Global train loss: 2.193, Global test loss: 2.152, Global test accuracy: 23.61 

Round 162, Train loss: 2.198, Test loss: 2.152, Test accuracy: 23.16 

Round 162, Global train loss: 2.198, Global test loss: 2.150, Global test accuracy: 22.94 

Round 163, Train loss: 2.190, Test loss: 2.151, Test accuracy: 23.06 

Round 163, Global train loss: 2.190, Global test loss: 2.147, Global test accuracy: 22.71 

Round 164, Train loss: 2.195, Test loss: 2.151, Test accuracy: 22.83 

Round 164, Global train loss: 2.195, Global test loss: 2.147, Global test accuracy: 22.86 

Round 165, Train loss: 2.191, Test loss: 2.149, Test accuracy: 22.53 

Round 165, Global train loss: 2.191, Global test loss: 2.144, Global test accuracy: 22.68 

Round 166, Train loss: 2.201, Test loss: 2.148, Test accuracy: 22.52 

Round 166, Global train loss: 2.201, Global test loss: 2.142, Global test accuracy: 22.59 

Round 167, Train loss: 2.201, Test loss: 2.147, Test accuracy: 22.61 

Round 167, Global train loss: 2.201, Global test loss: 2.141, Global test accuracy: 22.04 

Round 168, Train loss: 2.187, Test loss: 2.145, Test accuracy: 22.54 

Round 168, Global train loss: 2.187, Global test loss: 2.140, Global test accuracy: 22.19 

Round 169, Train loss: 2.200, Test loss: 2.144, Test accuracy: 22.43 

Round 169, Global train loss: 2.200, Global test loss: 2.139, Global test accuracy: 22.42 

Round 170, Train loss: nan, Test loss: nan, Test accuracy: 21.85 

Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 171, Train loss: nan, Test loss: nan, Test accuracy: 18.96 

Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 172, Train loss: nan, Test loss: nan, Test accuracy: 15.33 

Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 173, Train loss: nan, Test loss: nan, Test accuracy: 13.94 

Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 174, Train loss: nan, Test loss: nan, Test accuracy: 12.59 

Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 175, Train loss: nan, Test loss: nan, Test accuracy: 11.83 

Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 176, Train loss: nan, Test loss: nan, Test accuracy: 11.22 

Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 177, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 178, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 179, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 180, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 181, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 182, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 183, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 184, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 185, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 186, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 187, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 188, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 189, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 190, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 191, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 192, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 193, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 194, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 195, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 196, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 197, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 198, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 199, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 200, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 201, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 202, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 203, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 204, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 205, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 206, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 207, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 208, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 209, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 210, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 211, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 212, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 213, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 214, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 215, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 216, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 217, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 218, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 219, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 220, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 221, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 222, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 223, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 224, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 225, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 226, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 227, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 228, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 229, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 230, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 231, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 232, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 233, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 234, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 235, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 236, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 237, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 238, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 239, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 240, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 241, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 242, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 243, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 244, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 245, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 246, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 247, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 248, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 249, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 250, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 251, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 252, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 253, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 254, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 255, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 256, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 257, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 258, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 259, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 260, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 261, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 262, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 263, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 264, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 265, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 266, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 267, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 268, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 269, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 270, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 271, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 272, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 273, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 274, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 275, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 276, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 277, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 278, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 279, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 280, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 281, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 282, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 283, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 284, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 285, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 286, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 287, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 288, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 289, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 290, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 291, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 292, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 293, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 294, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 295, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 296, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 297, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 298, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 299, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

8387.27372455597
[1.5799472332000732, 2.9590110778808594, 4.329233884811401, 5.702244997024536, 7.095094680786133, 8.191386699676514, 9.30330204963684, 10.406426906585693, 11.50325608253479, 12.60305643081665, 13.707261085510254, 14.809475898742676, 16.07423162460327, 17.175675868988037, 18.281556129455566, 19.387863397598267, 20.48917055130005, 21.593072414398193, 22.86562967300415, 24.13024878501892, 25.274988174438477, 26.385865926742554, 27.482696533203125, 28.581605195999146, 29.883564710617065, 31.144898414611816, 32.241501569747925, 33.33795666694641, 34.43055868148804, 35.704681634902954, 36.81843423843384, 37.927507638931274, 39.12134790420532, 40.21552348136902, 41.30908417701721, 42.40582084655762, 43.65571403503418, 44.752843379974365, 45.8497896194458, 46.93910217285156, 48.03821301460266, 49.208263874053955, 50.30362272262573, 51.406296491622925, 52.504862546920776, 53.600750207901, 54.69611859321594, 55.796550273895264, 56.891979694366455, 58.019880294799805, 59.10879898071289, 60.209309577941895, 61.30726671218872, 62.40413498878479, 63.4994957447052, 64.7504472732544, 65.99630761146545, 67.25337076187134, 68.35615491867065, 69.61469888687134, 70.94888663291931, 72.19984221458435, 73.3113842010498, 74.42558789253235, 75.53015899658203, 76.63790345191956, 77.74983382225037, 78.84240651130676, 79.96050071716309, 81.06948900222778, 82.17128992080688, 83.279470205307, 84.53858923912048, 85.80558633804321, 86.9125828742981, 88.01049852371216, 89.29559683799744, 90.39477467536926, 91.48893141746521, 92.59484100341797, 93.70684719085693, 94.81162428855896, 95.93667602539062, 97.38315892219543, 98.53036975860596, 99.7233452796936, 100.8852117061615, 102.00164866447449, 103.11555671691895, 104.22843194007874, 105.33825516700745, 106.48626470565796, 107.74868559837341, 108.85401439666748, 109.9505455493927, 111.0504412651062, 112.14413952827454, 113.23751068115234, 114.33574604988098, 115.44058227539062, 116.55373477935791, 117.6784737110138, 118.7896237373352, 119.89177799224854, 120.99957084655762, 122.10592842102051, 123.21209144592285, 124.35245370864868, 125.46929001808167, 126.6878821849823, 127.95127940177917, 129.05915331840515, 130.3019995689392, 131.49147081375122, 132.61155104637146, 133.7974829673767, 135.14116978645325, 136.33208537101746, 137.53942275047302, 138.71384692192078, 139.90708422660828, 141.02061867713928, 142.27091789245605, 143.555645942688, 144.81692028045654, 145.9783480167389, 147.18527054786682, 148.63617420196533, 150.10281991958618, 151.36549139022827, 152.60983419418335, 153.82149648666382, 155.10188674926758, 156.38360977172852, 157.62005281448364, 158.87263321876526, 160.14959454536438, 161.372873544693, 162.58692932128906, 163.7781937122345, 165.0412402153015, 166.24933767318726, 167.48931169509888, 168.70057606697083, 169.88528561592102, 171.1505320072174, 172.35956120491028, 173.5963830947876, 174.84059596061707, 176.0498025417328, 177.2678689956665, 178.5103771686554, 179.76121282577515, 180.9481074810028, 182.17694234848022, 183.40375781059265, 184.657958984375, 185.9571976661682, 187.17194533348083, 188.45281291007996, 189.6643214225769, 190.91814470291138, 192.2047266960144, 193.5131094455719, 194.76550197601318, 196.06972241401672, 197.2650249004364, 198.4560465812683, 199.7106056213379, 200.95550990104675, 202.1896858215332, 203.3726727962494, 204.62791633605957, 205.88404059410095, 207.14396691322327, 208.40300726890564, 209.66462421417236, 210.88524556159973, 212.143150806427, 213.41259121894836, 214.6151442527771, 215.8622441291809, 217.1504909992218, 218.46747756004333, 219.728040933609, 220.97774195671082, 222.1911060810089, 223.6266360282898, 224.89619302749634, 226.13210153579712, 227.343425989151, 228.53803610801697, 229.82408833503723, 231.16935777664185, 232.40003538131714, 233.7292778491974, 235.02152562141418, 236.22997999191284, 237.52586889266968, 238.79359197616577, 240.04411005973816, 241.28820776939392, 242.51823449134827, 243.78212308883667, 244.98974871635437, 246.18818998336792, 247.3872845172882, 248.59510469436646, 249.7993562221527, 250.99711990356445, 252.2030701637268, 253.3963098526001, 254.5991826057434, 255.79814314842224, 256.9947111606598, 258.18844723701477, 259.48611879348755, 260.74522829055786, 262.0663561820984, 263.321480512619, 264.5747287273407, 265.84517884254456, 267.1193006038666, 268.39543771743774, 269.6754415035248, 270.9434027671814, 272.1414511203766, 273.34518361091614, 274.566561460495, 275.8262302875519, 277.03201389312744, 278.2306697368622, 279.42876982688904, 280.62831926345825, 281.8232841491699, 283.0157153606415, 284.2189588546753, 285.41095495224, 286.6011600494385, 287.80301094055176, 289.0066239833832, 290.1992027759552, 291.41312646865845, 292.6084041595459, 293.8050026893616, 295.0056254863739, 296.2037980556488, 297.40446519851685, 298.6037266254425, 299.86593532562256, 301.1489782333374, 302.39696168899536, 303.63544631004333, 304.8790349960327, 306.1546194553375, 307.39207220077515, 308.62908959388733, 309.88430738449097, 311.12782430648804, 312.38737988471985, 313.58693528175354, 314.778902053833, 315.96882581710815, 317.16157817840576, 318.3641984462738, 319.5651364326477, 320.76385259628296, 322.02557921409607, 323.2284743785858, 324.41429328918457, 325.6084418296814, 326.8076813220978, 327.99371695518494, 329.1818583011627, 330.37515115737915, 331.56720638275146, 332.7613983154297, 333.96247267723083, 335.17190504074097, 336.37161922454834, 337.5724356174469, 338.7684600353241, 339.9443757534027, 341.1297507286072, 342.338502407074, 343.42562222480774, 344.52130341529846, 345.77685546875, 347.001825094223, 348.19354796409607, 349.4117600917816, 350.6605443954468, 351.9149396419525, 353.0375919342041, 354.1265916824341, 355.2125005722046, 356.3266348838806, 357.4161286354065, 358.51169180870056, 359.602285861969, 361.8124873638153]
[10.31, 10.3625, 10.2875, 10.285, 10.2825, 10.2, 10.13, 10.0825, 10.0575, 10.05, 10.06, 10.075, 10.08, 10.08, 10.0925, 10.0775, 10.09, 10.0875, 10.0925, 10.1525, 10.17, 10.2525, 10.2725, 10.3275, 10.4175, 10.5525, 10.6575, 10.76, 10.955, 11.3225, 11.4975, 11.6825, 11.6425, 11.715, 11.7375, 12.0025, 12.145, 12.1975, 12.445, 12.55, 12.605, 12.6275, 12.585, 12.63, 12.705, 12.9475, 12.79, 12.93, 13.0175, 13.3975, 13.61, 13.9425, 14.2475, 14.325, 14.555, 14.705, 14.98, 15.155, 15.1, 15.0225, 15.0625, 15.185, 15.3, 15.3, 15.275, 15.4525, 15.5125, 15.8775, 16.07, 16.3525, 16.545, 16.3675, 16.38, 16.57, 16.5375, 16.675, 16.9875, 17.2125, 17.26, 17.26, 17.185, 17.0775, 16.9275, 17.0325, 17.1025, 17.335, 17.29, 17.2475, 17.31, 17.5425, 17.8775, 18.1325, 18.34, 18.55, 18.56, 18.4325, 18.48, 18.535, 18.715, 18.8675, 18.925, 19.1775, 19.2625, 19.1375, 19.1675, 19.0275, 19.3425, 19.0425, 18.9325, 18.8625, 19.21, 19.2875, 19.17, 19.4875, 19.995, 20.38, 20.66, 20.77, 21.2075, 21.205, 21.025, 21.17, 21.18, 21.41, 21.3, 21.12, 21.0875, 20.9075, 20.95, 21.395, 21.7875, 22.2325, 22.225, 22.0425, 22.095, 21.7825, 22.07, 22.1125, 22.34, 22.8275, 22.9875, 22.9775, 23.0225, 23.2, 23.2175, 22.965, 22.64, 22.575, 22.4825, 22.275, 22.105, 22.005, 21.8175, 22.1425, 22.5275, 22.5775, 22.66, 22.845, 23.1025, 23.3225, 23.445, 23.4025, 23.155, 23.0575, 22.8325, 22.5325, 22.5225, 22.605, 22.535, 22.435, 21.8475, 18.9575, 15.33, 13.9425, 12.59, 11.8325, 11.22, 10.5675, 10.5675, 10.5675, 10.5675, 10.5675, 10.5675, 10.5675, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.226, Test loss: 2.116, Test accuracy: 22.02 

Round   0, Global train loss: 2.226, Global test loss: 2.121, Global test accuracy: 22.45 

Round   1, Train loss: 2.042, Test loss: 2.003, Test accuracy: 25.61 

Round   1, Global train loss: 2.042, Global test loss: 1.953, Global test accuracy: 28.14 

Round   2, Train loss: 1.919, Test loss: 1.925, Test accuracy: 27.69 

Round   2, Global train loss: 1.919, Global test loss: 1.841, Global test accuracy: 30.87 

Round   3, Train loss: 1.805, Test loss: 1.873, Test accuracy: 29.84 

Round   3, Global train loss: 1.805, Global test loss: 1.738, Global test accuracy: 33.76 

Round   4, Train loss: 1.758, Test loss: 1.796, Test accuracy: 33.03 

Round   4, Global train loss: 1.758, Global test loss: 1.665, Global test accuracy: 38.03 

Round   5, Train loss: 1.669, Test loss: 1.718, Test accuracy: 36.33 

Round   5, Global train loss: 1.669, Global test loss: 1.608, Global test accuracy: 39.95 

Round   6, Train loss: 1.630, Test loss: 1.706, Test accuracy: 36.89 

Round   6, Global train loss: 1.630, Global test loss: 1.586, Global test accuracy: 40.62 

Round   7, Train loss: 1.561, Test loss: 1.671, Test accuracy: 38.09 

Round   7, Global train loss: 1.561, Global test loss: 1.527, Global test accuracy: 42.70 

Round   8, Train loss: 1.509, Test loss: 1.637, Test accuracy: 40.10 

Round   8, Global train loss: 1.509, Global test loss: 1.506, Global test accuracy: 43.42 

Round   9, Train loss: 1.481, Test loss: 1.595, Test accuracy: 41.56 

Round   9, Global train loss: 1.481, Global test loss: 1.480, Global test accuracy: 45.17 

Round  10, Train loss: 1.457, Test loss: 1.580, Test accuracy: 42.35 

Round  10, Global train loss: 1.457, Global test loss: 1.468, Global test accuracy: 46.60 

Round  11, Train loss: 1.400, Test loss: 1.569, Test accuracy: 42.90 

Round  11, Global train loss: 1.400, Global test loss: 1.477, Global test accuracy: 47.57 

Round  12, Train loss: 1.407, Test loss: 1.552, Test accuracy: 43.70 

Round  12, Global train loss: 1.407, Global test loss: 1.427, Global test accuracy: 47.28 

Round  13, Train loss: 1.360, Test loss: 1.519, Test accuracy: 45.25 

Round  13, Global train loss: 1.360, Global test loss: 1.404, Global test accuracy: 48.82 

Round  14, Train loss: 1.311, Test loss: 1.499, Test accuracy: 45.76 

Round  14, Global train loss: 1.311, Global test loss: 1.513, Global test accuracy: 48.18 

Round  15, Train loss: 1.286, Test loss: 1.491, Test accuracy: 46.28 

Round  15, Global train loss: 1.286, Global test loss: 1.341, Global test accuracy: 49.89 

Round  16, Train loss: 1.266, Test loss: 1.486, Test accuracy: 46.80 

Round  16, Global train loss: 1.266, Global test loss: 1.352, Global test accuracy: 50.11 

Round  17, Train loss: 1.270, Test loss: 1.453, Test accuracy: 48.72 

Round  17, Global train loss: 1.270, Global test loss: 1.343, Global test accuracy: 51.16 

Round  18, Train loss: 1.205, Test loss: 1.447, Test accuracy: 49.46 

Round  18, Global train loss: 1.205, Global test loss: 1.320, Global test accuracy: 51.28 

Round  19, Train loss: 1.170, Test loss: 1.450, Test accuracy: 49.88 

Round  19, Global train loss: 1.170, Global test loss: 1.317, Global test accuracy: 51.84 

Round  20, Train loss: 1.180, Test loss: 1.430, Test accuracy: 50.74 

Round  20, Global train loss: 1.180, Global test loss: 1.283, Global test accuracy: 52.51 

Round  21, Train loss: 1.154, Test loss: 1.424, Test accuracy: 51.36 

Round  21, Global train loss: 1.154, Global test loss: 1.358, Global test accuracy: 51.54 

Round  22, Train loss: 1.118, Test loss: 1.409, Test accuracy: 52.02 

Round  22, Global train loss: 1.118, Global test loss: 1.330, Global test accuracy: 53.27 

Round  23, Train loss: 1.104, Test loss: 1.410, Test accuracy: 52.23 

Round  23, Global train loss: 1.104, Global test loss: 1.246, Global test accuracy: 54.42 

Round  24, Train loss: 1.097, Test loss: 1.389, Test accuracy: 52.80 

Round  24, Global train loss: 1.097, Global test loss: 1.263, Global test accuracy: 54.50 

Round  25, Train loss: 1.048, Test loss: 1.383, Test accuracy: 53.22 

Round  25, Global train loss: 1.048, Global test loss: 1.231, Global test accuracy: 54.47 

Round  26, Train loss: 1.014, Test loss: 1.375, Test accuracy: 53.56 

Round  26, Global train loss: 1.014, Global test loss: 1.259, Global test accuracy: 55.58 

Round  27, Train loss: 1.046, Test loss: 1.364, Test accuracy: 53.90 

Round  27, Global train loss: 1.046, Global test loss: 1.281, Global test accuracy: 55.68 

Round  28, Train loss: 1.028, Test loss: 1.366, Test accuracy: 54.02 

Round  28, Global train loss: 1.028, Global test loss: 1.246, Global test accuracy: 56.21 

Round  29, Train loss: 1.000, Test loss: 1.373, Test accuracy: 54.16 

Round  29, Global train loss: 1.000, Global test loss: 1.240, Global test accuracy: 56.24 

Round  30, Train loss: 0.989, Test loss: 1.386, Test accuracy: 54.30 

Round  30, Global train loss: 0.989, Global test loss: 1.319, Global test accuracy: 56.79 

Round  31, Train loss: 0.957, Test loss: 1.387, Test accuracy: 54.65 

Round  31, Global train loss: 0.957, Global test loss: 1.246, Global test accuracy: 56.15 

Round  32, Train loss: 0.938, Test loss: 1.368, Test accuracy: 55.31 

Round  32, Global train loss: 0.938, Global test loss: 1.228, Global test accuracy: 56.00 

Round  33, Train loss: 0.922, Test loss: 1.373, Test accuracy: 55.12 

Round  33, Global train loss: 0.922, Global test loss: 1.227, Global test accuracy: 57.14 

Round  34, Train loss: 0.907, Test loss: 1.362, Test accuracy: 55.30 

Round  34, Global train loss: 0.907, Global test loss: 1.199, Global test accuracy: 57.00 

Round  35, Train loss: 0.896, Test loss: 1.350, Test accuracy: 55.85 

Round  35, Global train loss: 0.896, Global test loss: 1.304, Global test accuracy: 57.96 

Round  36, Train loss: 0.870, Test loss: 1.349, Test accuracy: 56.05 

Round  36, Global train loss: 0.870, Global test loss: 1.316, Global test accuracy: 58.07 

Round  37, Train loss: 0.849, Test loss: 1.348, Test accuracy: 56.35 

Round  37, Global train loss: 0.849, Global test loss: 1.205, Global test accuracy: 57.35 

Round  38, Train loss: 0.836, Test loss: 1.366, Test accuracy: 56.33 

Round  38, Global train loss: 0.836, Global test loss: 1.249, Global test accuracy: 57.62 

Round  39, Train loss: 0.841, Test loss: 1.366, Test accuracy: 56.45 

Round  39, Global train loss: 0.841, Global test loss: 1.221, Global test accuracy: 57.40 

Round  40, Train loss: 0.830, Test loss: 1.381, Test accuracy: 56.22 

Round  40, Global train loss: 0.830, Global test loss: 1.271, Global test accuracy: 57.33 

Round  41, Train loss: 0.787, Test loss: 1.383, Test accuracy: 56.36 

Round  41, Global train loss: 0.787, Global test loss: 1.225, Global test accuracy: 57.67 

Round  42, Train loss: 0.781, Test loss: 1.377, Test accuracy: 56.74 

Round  42, Global train loss: 0.781, Global test loss: 1.237, Global test accuracy: 58.91 

Round  43, Train loss: 0.821, Test loss: 1.371, Test accuracy: 57.16 

Round  43, Global train loss: 0.821, Global test loss: 1.236, Global test accuracy: 58.34 

Round  44, Train loss: 0.743, Test loss: 1.387, Test accuracy: 56.90 

Round  44, Global train loss: 0.743, Global test loss: 1.241, Global test accuracy: 59.51 

Round  45, Train loss: 0.747, Test loss: 1.386, Test accuracy: 57.18 

Round  45, Global train loss: 0.747, Global test loss: 1.206, Global test accuracy: 58.56 

Round  46, Train loss: 0.750, Test loss: 1.369, Test accuracy: 57.97 

Round  46, Global train loss: 0.750, Global test loss: 1.244, Global test accuracy: 59.29 

Round  47, Train loss: 0.771, Test loss: 1.378, Test accuracy: 58.23 

Round  47, Global train loss: 0.771, Global test loss: 1.197, Global test accuracy: 58.66 

Round  48, Train loss: 0.721, Test loss: 1.380, Test accuracy: 58.41 

Round  48, Global train loss: 0.721, Global test loss: 1.232, Global test accuracy: 59.04 

Round  49, Train loss: 0.701, Test loss: 1.374, Test accuracy: 58.43 

Round  49, Global train loss: 0.701, Global test loss: 1.203, Global test accuracy: 58.90 

Round  50, Train loss: 0.727, Test loss: 1.380, Test accuracy: 58.33 

Round  50, Global train loss: 0.727, Global test loss: 1.212, Global test accuracy: 58.32 

Round  51, Train loss: 0.732, Test loss: 1.388, Test accuracy: 58.52 

Round  51, Global train loss: 0.732, Global test loss: 1.190, Global test accuracy: 58.60 

Round  52, Train loss: 0.697, Test loss: 1.376, Test accuracy: 58.52 

Round  52, Global train loss: 0.697, Global test loss: 1.374, Global test accuracy: 59.52 

Round  53, Train loss: 0.698, Test loss: 1.377, Test accuracy: 58.80 

Round  53, Global train loss: 0.698, Global test loss: 1.249, Global test accuracy: 59.51 

Round  54, Train loss: 0.649, Test loss: 1.385, Test accuracy: 58.82 

Round  54, Global train loss: 0.649, Global test loss: 1.267, Global test accuracy: 59.91 

Round  55, Train loss: 0.692, Test loss: 1.382, Test accuracy: 59.20 

Round  55, Global train loss: 0.692, Global test loss: 1.219, Global test accuracy: 59.86 

Round  56, Train loss: 0.672, Test loss: 1.380, Test accuracy: 59.68 

Round  56, Global train loss: 0.672, Global test loss: 1.290, Global test accuracy: 59.66 

Round  57, Train loss: 0.685, Test loss: 1.384, Test accuracy: 59.47 

Round  57, Global train loss: 0.685, Global test loss: 1.263, Global test accuracy: 59.71 

Round  58, Train loss: 0.635, Test loss: 1.367, Test accuracy: 59.41 

Round  58, Global train loss: 0.635, Global test loss: 1.318, Global test accuracy: 58.95 

Round  59, Train loss: 0.712, Test loss: 1.378, Test accuracy: 59.42 

Round  59, Global train loss: 0.712, Global test loss: 1.288, Global test accuracy: 58.82 

Round  60, Train loss: 0.642, Test loss: 1.400, Test accuracy: 59.41 

Round  60, Global train loss: 0.642, Global test loss: 1.387, Global test accuracy: 60.54 

Round  61, Train loss: 0.607, Test loss: 1.397, Test accuracy: 59.52 

Round  61, Global train loss: 0.607, Global test loss: 1.264, Global test accuracy: 60.09 

Round  62, Train loss: 0.621, Test loss: 1.409, Test accuracy: 59.80 

Round  62, Global train loss: 0.621, Global test loss: 1.406, Global test accuracy: 60.77 

Round  63, Train loss: 0.573, Test loss: 1.424, Test accuracy: 59.27 

Round  63, Global train loss: 0.573, Global test loss: 1.557, Global test accuracy: 59.82 

Round  64, Train loss: 0.623, Test loss: 1.435, Test accuracy: 59.15 

Round  64, Global train loss: 0.623, Global test loss: 1.260, Global test accuracy: 59.27 

Round  65, Train loss: 0.648, Test loss: 1.430, Test accuracy: 59.59 

Round  65, Global train loss: 0.648, Global test loss: 1.251, Global test accuracy: 60.11 

Round  66, Train loss: 0.637, Test loss: 1.410, Test accuracy: 60.02 

Round  66, Global train loss: 0.637, Global test loss: 1.242, Global test accuracy: 59.77 

Round  67, Train loss: 0.621, Test loss: 1.431, Test accuracy: 59.92 

Round  67, Global train loss: 0.621, Global test loss: 1.308, Global test accuracy: 59.35 

Round  68, Train loss: 0.602, Test loss: 1.442, Test accuracy: 59.70 

Round  68, Global train loss: 0.602, Global test loss: 1.282, Global test accuracy: 60.55 

Round  69, Train loss: 0.588, Test loss: 1.453, Test accuracy: 59.64 

Round  69, Global train loss: 0.588, Global test loss: 1.268, Global test accuracy: 60.28 

Round  70, Train loss: 0.560, Test loss: 1.454, Test accuracy: 60.03 

Round  70, Global train loss: 0.560, Global test loss: 1.308, Global test accuracy: 60.22 

Round  71, Train loss: 0.542, Test loss: 1.461, Test accuracy: 59.80 

Round  71, Global train loss: 0.542, Global test loss: 1.493, Global test accuracy: 60.27 

Round  72, Train loss: 0.558, Test loss: 1.471, Test accuracy: 59.73 

Round  72, Global train loss: 0.558, Global test loss: 1.353, Global test accuracy: 60.13 

Round  73, Train loss: 0.553, Test loss: 1.452, Test accuracy: 60.15 

Round  73, Global train loss: 0.553, Global test loss: 1.288, Global test accuracy: 59.63 

Round  74, Train loss: 0.516, Test loss: 1.437, Test accuracy: 60.68 

Round  74, Global train loss: 0.516, Global test loss: 1.287, Global test accuracy: 59.95 

Round  75, Train loss: 0.557, Test loss: 1.450, Test accuracy: 60.75 

Round  75, Global train loss: 0.557, Global test loss: 1.289, Global test accuracy: 60.69 

Round  76, Train loss: 0.595, Test loss: 1.443, Test accuracy: 61.03 

Round  76, Global train loss: 0.595, Global test loss: 1.330, Global test accuracy: 60.85 

Round  77, Train loss: 0.503, Test loss: 1.433, Test accuracy: 61.27 

Round  77, Global train loss: 0.503, Global test loss: 1.336, Global test accuracy: 61.15 

Round  78, Train loss: 0.564, Test loss: 1.425, Test accuracy: 61.47 

Round  78, Global train loss: 0.564, Global test loss: 1.268, Global test accuracy: 60.20 

Round  79, Train loss: 0.550, Test loss: 1.439, Test accuracy: 61.36 

Round  79, Global train loss: 0.550, Global test loss: 1.243, Global test accuracy: 60.55 

Round  80, Train loss: 0.542, Test loss: 1.447, Test accuracy: 61.27 

Round  80, Global train loss: 0.542, Global test loss: 1.276, Global test accuracy: 60.15 

Round  81, Train loss: 0.516, Test loss: 1.459, Test accuracy: 61.29 

Round  81, Global train loss: 0.516, Global test loss: 1.333, Global test accuracy: 60.62 

Round  82, Train loss: 0.479, Test loss: 1.459, Test accuracy: 61.39 

Round  82, Global train loss: 0.479, Global test loss: 1.315, Global test accuracy: 59.87 

Round  83, Train loss: 0.536, Test loss: 1.466, Test accuracy: 61.39 

Round  83, Global train loss: 0.536, Global test loss: 1.328, Global test accuracy: 61.33 

Round  84, Train loss: 0.512, Test loss: 1.475, Test accuracy: 61.16 

Round  84, Global train loss: 0.512, Global test loss: 1.322, Global test accuracy: 60.83 

Round  85, Train loss: 0.489, Test loss: 1.468, Test accuracy: 61.50 

Round  85, Global train loss: 0.489, Global test loss: 1.342, Global test accuracy: 60.62 

Round  86, Train loss: 0.525, Test loss: 1.468, Test accuracy: 61.62 

Round  86, Global train loss: 0.525, Global test loss: 1.281, Global test accuracy: 60.67 

Round  87, Train loss: 0.498, Test loss: 1.467, Test accuracy: 61.58 

Round  87, Global train loss: 0.498, Global test loss: 1.455, Global test accuracy: 61.72 

Round  88, Train loss: 0.473, Test loss: 1.479, Test accuracy: 61.31 

Round  88, Global train loss: 0.473, Global test loss: 1.305, Global test accuracy: 60.48 

Round  89, Train loss: 0.467, Test loss: 1.490, Test accuracy: 61.34 

Round  89, Global train loss: 0.467, Global test loss: 1.306, Global test accuracy: 60.47 

Round  90, Train loss: 0.506, Test loss: 1.498, Test accuracy: 61.42 

Round  90, Global train loss: 0.506, Global test loss: 1.364, Global test accuracy: 60.40 

Round  91, Train loss: 0.449, Test loss: 1.516, Test accuracy: 61.36 

Round  91, Global train loss: 0.449, Global test loss: 1.559, Global test accuracy: 61.09 

Round  92, Train loss: 0.454, Test loss: 1.519, Test accuracy: 61.23 

Round  92, Global train loss: 0.454, Global test loss: 1.389, Global test accuracy: 61.21 

Round  93, Train loss: 0.445, Test loss: 1.507, Test accuracy: 61.25 

Round  93, Global train loss: 0.445, Global test loss: 1.345, Global test accuracy: 60.90 

Round  94, Train loss: 0.401, Test loss: 1.536, Test accuracy: 61.05 

Round  94, Global train loss: 0.401, Global test loss: 1.729, Global test accuracy: 61.10 

Round  95, Train loss: 0.506, Test loss: 1.523, Test accuracy: 61.16 

Round  95, Global train loss: 0.506, Global test loss: 1.397, Global test accuracy: 60.06 

Round  96, Train loss: 0.502, Test loss: 1.523, Test accuracy: 61.34 

Round  96, Global train loss: 0.502, Global test loss: 1.320, Global test accuracy: 60.39 

Round  97, Train loss: 0.468, Test loss: 1.529, Test accuracy: 61.49 

Round  97, Global train loss: 0.468, Global test loss: 1.389, Global test accuracy: 61.32 

Round  98, Train loss: 0.504, Test loss: 1.517, Test accuracy: 61.52 

Round  98, Global train loss: 0.504, Global test loss: 1.314, Global test accuracy: 61.18 

Round  99, Train loss: 0.457, Test loss: 1.526, Test accuracy: 61.30 

Round  99, Global train loss: 0.457, Global test loss: 1.336, Global test accuracy: 60.45 

Final Round, Train loss: 0.359, Test loss: 1.701, Test accuracy: 60.95 

Final Round, Global train loss: 0.359, Global test loss: 1.336, Global test accuracy: 60.45 

Average accuracy final 10 rounds: 61.313500000000005 

Average global accuracy final 10 rounds: 60.81025000000001 

2814.1803271770477
[1.5261149406433105, 2.7178714275360107, 3.9039902687072754, 5.094900131225586, 6.313772439956665, 7.516180038452148, 8.703750848770142, 9.906837463378906, 11.117218971252441, 12.323936700820923, 13.536064863204956, 14.73347544670105, 15.93257188796997, 17.13411283493042, 18.340087175369263, 19.54059934616089, 20.734293460845947, 21.93627166748047, 23.13929796218872, 24.341482162475586, 25.545634269714355, 26.74432945251465, 27.941843032836914, 29.1423397064209, 30.341749906539917, 31.53486466407776, 32.734214305877686, 33.93934512138367, 35.14544987678528, 36.352182388305664, 37.55443286895752, 38.752690076828, 39.9597647190094, 41.16049146652222, 42.367043018341064, 43.564690351486206, 44.76003432273865, 45.963406801223755, 47.16792964935303, 48.373656272888184, 49.56804919242859, 50.76658844947815, 51.97250771522522, 53.18004775047302, 54.38166379928589, 55.577720403671265, 56.77416944503784, 57.97993063926697, 59.17861533164978, 60.37987971305847, 61.58162879943848, 62.78263330459595, 63.978917598724365, 65.1770761013031, 66.37933659553528, 67.58310198783875, 68.7843828201294, 69.9789354801178, 71.17450404167175, 72.37673163414001, 73.5720534324646, 74.76572442054749, 75.96781063079834, 77.1630609035492, 78.36642527580261, 79.56613826751709, 80.76222920417786, 81.96261382102966, 83.15353608131409, 84.34792232513428, 85.55168437957764, 86.74924802780151, 87.94288969039917, 89.1393072605133, 90.34047746658325, 91.53937077522278, 92.74025058746338, 93.93285465240479, 95.11237382888794, 96.31196856498718, 97.52980923652649, 98.8260452747345, 100.09152674674988, 101.3566677570343, 102.62240839004517, 103.877614736557, 105.13365983963013, 106.39348316192627, 107.65592288970947, 108.83008742332458, 110.08800315856934, 111.34755373001099, 112.60923314094543, 113.86840581893921, 115.12698435783386, 116.39301204681396, 117.65776562690735, 118.9184958934784, 120.17706441879272, 121.43753576278687, 123.95437669754028]
[22.025, 25.6075, 27.6875, 29.84, 33.035, 36.3275, 36.8925, 38.09, 40.105, 41.5575, 42.3475, 42.8975, 43.705, 45.25, 45.76, 46.28, 46.8, 48.72, 49.4625, 49.88, 50.745, 51.3625, 52.02, 52.2275, 52.8025, 53.22, 53.5625, 53.8975, 54.02, 54.16, 54.305, 54.65, 55.31, 55.12, 55.295, 55.8525, 56.0475, 56.3475, 56.33, 56.455, 56.22, 56.36, 56.7425, 57.1625, 56.895, 57.18, 57.9725, 58.2275, 58.415, 58.43, 58.33, 58.5225, 58.5225, 58.7975, 58.8175, 59.2025, 59.6775, 59.4725, 59.4125, 59.42, 59.41, 59.5225, 59.7975, 59.2675, 59.145, 59.595, 60.0225, 59.9225, 59.705, 59.64, 60.0275, 59.8, 59.7325, 60.1475, 60.6825, 60.7475, 61.0325, 61.2675, 61.465, 61.3575, 61.2675, 61.29, 61.3925, 61.3875, 61.165, 61.4975, 61.6175, 61.5775, 61.315, 61.335, 61.4225, 61.36, 61.235, 61.2525, 61.0475, 61.1625, 61.3425, 61.4925, 61.52, 61.3, 60.955]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.292, Test loss: 2.259, Test accuracy: 18.64 

Round   1, Train loss: 2.188, Test loss: 2.057, Test accuracy: 23.83 

Round   2, Train loss: 2.036, Test loss: 1.976, Test accuracy: 26.25 

Round   3, Train loss: 1.934, Test loss: 1.886, Test accuracy: 28.36 

Round   4, Train loss: 1.854, Test loss: 1.846, Test accuracy: 29.75 

Round   5, Train loss: 1.814, Test loss: 1.776, Test accuracy: 32.99 

Round   6, Train loss: 1.775, Test loss: 1.725, Test accuracy: 34.70 

Round   7, Train loss: 1.713, Test loss: 1.689, Test accuracy: 36.70 

Round   8, Train loss: 1.679, Test loss: 1.673, Test accuracy: 37.87 

Round   9, Train loss: 1.636, Test loss: 1.625, Test accuracy: 39.81 

Round  10, Train loss: 1.614, Test loss: 1.611, Test accuracy: 40.19 

Round  11, Train loss: 1.598, Test loss: 1.586, Test accuracy: 40.96 

Round  12, Train loss: 1.599, Test loss: 1.547, Test accuracy: 42.78 

Round  13, Train loss: 1.547, Test loss: 1.549, Test accuracy: 42.76 

Round  14, Train loss: 1.540, Test loss: 1.504, Test accuracy: 44.69 

Round  15, Train loss: 1.498, Test loss: 1.489, Test accuracy: 45.28 

Round  16, Train loss: 1.483, Test loss: 1.481, Test accuracy: 45.52 

Round  17, Train loss: 1.468, Test loss: 1.451, Test accuracy: 46.80 

Round  18, Train loss: 1.431, Test loss: 1.436, Test accuracy: 47.51 

Round  19, Train loss: 1.437, Test loss: 1.413, Test accuracy: 48.28 

Round  20, Train loss: 1.407, Test loss: 1.394, Test accuracy: 48.93 

Round  21, Train loss: 1.390, Test loss: 1.398, Test accuracy: 49.17 

Round  22, Train loss: 1.355, Test loss: 1.374, Test accuracy: 50.38 

Round  23, Train loss: 1.348, Test loss: 1.363, Test accuracy: 50.87 

Round  24, Train loss: 1.341, Test loss: 1.353, Test accuracy: 51.29 

Round  25, Train loss: 1.321, Test loss: 1.335, Test accuracy: 52.32 

Round  26, Train loss: 1.264, Test loss: 1.325, Test accuracy: 52.35 

Round  27, Train loss: 1.291, Test loss: 1.352, Test accuracy: 51.11 

Round  28, Train loss: 1.269, Test loss: 1.335, Test accuracy: 51.85 

Round  29, Train loss: 1.291, Test loss: 1.307, Test accuracy: 52.87 

Round  30, Train loss: 1.217, Test loss: 1.294, Test accuracy: 53.66 

Round  31, Train loss: 1.232, Test loss: 1.286, Test accuracy: 53.51 

Round  32, Train loss: 1.187, Test loss: 1.301, Test accuracy: 53.26 

Round  33, Train loss: 1.186, Test loss: 1.271, Test accuracy: 54.68 

Round  34, Train loss: 1.159, Test loss: 1.273, Test accuracy: 54.51 

Round  35, Train loss: 1.138, Test loss: 1.270, Test accuracy: 54.69 

Round  36, Train loss: 1.177, Test loss: 1.264, Test accuracy: 54.90 

Round  37, Train loss: 1.096, Test loss: 1.258, Test accuracy: 55.45 

Round  38, Train loss: 1.082, Test loss: 1.224, Test accuracy: 56.80 

Round  39, Train loss: 1.063, Test loss: 1.217, Test accuracy: 57.13 

Round  40, Train loss: 1.103, Test loss: 1.212, Test accuracy: 57.28 

Round  41, Train loss: 1.093, Test loss: 1.198, Test accuracy: 57.58 

Round  42, Train loss: 1.025, Test loss: 1.216, Test accuracy: 57.03 

Round  43, Train loss: 1.043, Test loss: 1.206, Test accuracy: 57.45 

Round  44, Train loss: 1.023, Test loss: 1.210, Test accuracy: 57.90 

Round  45, Train loss: 1.002, Test loss: 1.176, Test accuracy: 58.46 

Round  46, Train loss: 1.026, Test loss: 1.188, Test accuracy: 58.35 

Round  47, Train loss: 1.032, Test loss: 1.176, Test accuracy: 58.94 

Round  48, Train loss: 0.984, Test loss: 1.163, Test accuracy: 59.18 

Round  49, Train loss: 0.958, Test loss: 1.170, Test accuracy: 59.01 

Round  50, Train loss: 0.954, Test loss: 1.174, Test accuracy: 58.94 

Round  51, Train loss: 0.950, Test loss: 1.181, Test accuracy: 58.91 

Round  52, Train loss: 0.930, Test loss: 1.198, Test accuracy: 59.17 

Round  53, Train loss: 0.932, Test loss: 1.190, Test accuracy: 59.36 

Round  54, Train loss: 0.919, Test loss: 1.155, Test accuracy: 59.80 

Round  55, Train loss: 0.963, Test loss: 1.151, Test accuracy: 60.39 

Round  56, Train loss: 0.925, Test loss: 1.159, Test accuracy: 60.58 

Round  57, Train loss: 0.891, Test loss: 1.174, Test accuracy: 60.08 

Round  58, Train loss: 0.888, Test loss: 1.164, Test accuracy: 60.60 

Round  59, Train loss: 0.904, Test loss: 1.159, Test accuracy: 60.77 

Round  60, Train loss: 0.854, Test loss: 1.157, Test accuracy: 60.94 

Round  61, Train loss: 0.861, Test loss: 1.160, Test accuracy: 60.97 

Round  62, Train loss: 0.817, Test loss: 1.162, Test accuracy: 61.02 

Round  63, Train loss: 0.837, Test loss: 1.150, Test accuracy: 61.55 

Round  64, Train loss: 0.838, Test loss: 1.143, Test accuracy: 61.65 

Round  65, Train loss: 0.803, Test loss: 1.142, Test accuracy: 61.73 

Round  66, Train loss: 0.779, Test loss: 1.150, Test accuracy: 61.66 

Round  67, Train loss: 0.804, Test loss: 1.146, Test accuracy: 61.73 

Round  68, Train loss: 0.795, Test loss: 1.141, Test accuracy: 61.76 

Round  69, Train loss: 0.816, Test loss: 1.157, Test accuracy: 61.47 

Round  70, Train loss: 0.790, Test loss: 1.146, Test accuracy: 62.02 

Round  71, Train loss: 0.832, Test loss: 1.146, Test accuracy: 62.35 

Round  72, Train loss: 0.737, Test loss: 1.139, Test accuracy: 62.30 

Round  73, Train loss: 0.736, Test loss: 1.157, Test accuracy: 61.78 

Round  74, Train loss: 0.749, Test loss: 1.157, Test accuracy: 62.13 

Round  75, Train loss: 0.796, Test loss: 1.146, Test accuracy: 62.56 

Round  76, Train loss: 0.767, Test loss: 1.153, Test accuracy: 62.15 

Round  77, Train loss: 0.709, Test loss: 1.180, Test accuracy: 61.97 

Round  78, Train loss: 0.659, Test loss: 1.167, Test accuracy: 62.38 

Round  79, Train loss: 0.760, Test loss: 1.161, Test accuracy: 62.46 

Round  80, Train loss: 0.699, Test loss: 1.151, Test accuracy: 62.94 

Round  81, Train loss: 0.707, Test loss: 1.158, Test accuracy: 63.25 

Round  82, Train loss: 0.678, Test loss: 1.146, Test accuracy: 63.14 

Round  83, Train loss: 0.740, Test loss: 1.159, Test accuracy: 63.21 

Round  84, Train loss: 0.670, Test loss: 1.167, Test accuracy: 63.27 

Round  85, Train loss: 0.707, Test loss: 1.178, Test accuracy: 63.28 

Round  86, Train loss: 0.657, Test loss: 1.150, Test accuracy: 63.62 

Round  87, Train loss: 0.704, Test loss: 1.188, Test accuracy: 62.85 

Round  88, Train loss: 0.689, Test loss: 1.164, Test accuracy: 63.46 

Round  89, Train loss: 0.621, Test loss: 1.168, Test accuracy: 63.27 

Round  90, Train loss: 0.660, Test loss: 1.183, Test accuracy: 63.31 

Round  91, Train loss: 0.654, Test loss: 1.178, Test accuracy: 63.46 

Round  92, Train loss: 0.692, Test loss: 1.190, Test accuracy: 63.37 

Round  93, Train loss: 0.653, Test loss: 1.178, Test accuracy: 63.42 

Round  94, Train loss: 0.625, Test loss: 1.185, Test accuracy: 63.46 

Round  95, Train loss: 0.624, Test loss: 1.182, Test accuracy: 63.18 

Round  96, Train loss: 0.671, Test loss: 1.175, Test accuracy: 63.55 

Round  97, Train loss: 0.626, Test loss: 1.191, Test accuracy: 63.83 

Round  98, Train loss: 0.625, Test loss: 1.220, Test accuracy: 63.46 

Round  99, Train loss: 0.603, Test loss: 1.205, Test accuracy: 63.81 

Final Round, Train loss: 0.535, Test loss: 1.217, Test accuracy: 64.03 

Average accuracy final 10 rounds: 63.48475 

1650.7109637260437
[1.3003554344177246, 2.3291378021240234, 3.3587610721588135, 4.385822296142578, 5.416116952896118, 6.4414732456207275, 7.495015382766724, 8.547472476959229, 9.601008892059326, 10.657144784927368, 11.71363878250122, 12.76239275932312, 13.813973903656006, 14.868176937103271, 15.92379379272461, 16.97332763671875, 18.027022123336792, 19.07749056816101, 20.126206636428833, 21.181105852127075, 22.23666477203369, 23.287814617156982, 24.340781688690186, 25.391992807388306, 26.444355010986328, 27.498438596725464, 28.55324149131775, 29.609721183776855, 30.664313316345215, 31.714824438095093, 32.76444435119629, 33.818812131881714, 34.87094163894653, 35.92671203613281, 36.986940145492554, 38.0370147228241, 39.08244490623474, 40.132570028305054, 41.19529676437378, 42.25082230567932, 43.30220103263855, 44.35745644569397, 45.40740084648132, 46.45454668998718, 47.505717515945435, 48.55962157249451, 49.61253643035889, 50.671653747558594, 51.72722601890564, 52.78449034690857, 53.83838391304016, 54.88904094696045, 55.941696643829346, 56.99214148521423, 57.942285776138306, 58.897329330444336, 59.85366988182068, 60.80974745750427, 61.769312143325806, 62.72557044029236, 63.67699956893921, 64.63161540031433, 65.58696293830872, 66.54297041893005, 67.49579405784607, 68.44839811325073, 69.3996353149414, 70.35289192199707, 71.30971050262451, 72.26487517356873, 73.21839499473572, 74.16791343688965, 75.12506055831909, 76.08182525634766, 77.03878021240234, 77.9934070110321, 78.94352746009827, 79.89505529403687, 80.85102558135986, 81.80406141281128, 82.7723605632782, 83.72043967247009, 84.6740951538086, 85.62948513031006, 86.59177279472351, 87.55310773849487, 88.5003023147583, 89.45846390724182, 90.4125235080719, 91.36519145965576, 92.31581807136536, 93.26775932312012, 94.21940302848816, 95.17522168159485, 96.13154911994934, 97.08383536338806, 98.03554034233093, 98.98593068122864, 99.93623733520508, 100.8862247467041, 102.66796684265137]
[18.635, 23.8275, 26.2525, 28.3575, 29.755, 32.9925, 34.705, 36.705, 37.8725, 39.81, 40.1925, 40.96, 42.785, 42.755, 44.685, 45.285, 45.5225, 46.7975, 47.5125, 48.28, 48.9325, 49.175, 50.3775, 50.865, 51.29, 52.3175, 52.3525, 51.1075, 51.8475, 52.865, 53.665, 53.51, 53.255, 54.6775, 54.51, 54.69, 54.895, 55.445, 56.7975, 57.135, 57.2775, 57.5825, 57.0325, 57.445, 57.9, 58.4575, 58.3475, 58.9425, 59.1825, 59.0075, 58.935, 58.905, 59.175, 59.3625, 59.8025, 60.3925, 60.5825, 60.08, 60.6025, 60.77, 60.94, 60.965, 61.015, 61.555, 61.6525, 61.7275, 61.6575, 61.725, 61.7575, 61.4675, 62.02, 62.3525, 62.3025, 61.785, 62.13, 62.565, 62.1475, 61.9675, 62.385, 62.4625, 62.9375, 63.2525, 63.1375, 63.2075, 63.2725, 63.2825, 63.615, 62.855, 63.46, 63.2675, 63.3125, 63.4575, 63.3675, 63.4175, 63.4625, 63.1775, 63.5525, 63.8275, 63.46, 63.8125, 64.0325]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.295, Test loss: 2.206, Test accuracy: 19.82
Round   1, Train loss: 2.156, Test loss: 2.038, Test accuracy: 24.30
Round   2, Train loss: 2.036, Test loss: 1.947, Test accuracy: 28.07
Round   3, Train loss: 1.941, Test loss: 1.871, Test accuracy: 30.93
Round   4, Train loss: 1.869, Test loss: 1.810, Test accuracy: 34.30
Round   5, Train loss: 1.814, Test loss: 1.742, Test accuracy: 35.83
Round   6, Train loss: 1.747, Test loss: 1.684, Test accuracy: 37.81
Round   7, Train loss: 1.736, Test loss: 1.634, Test accuracy: 40.02
Round   8, Train loss: 1.693, Test loss: 1.601, Test accuracy: 41.81
Round   9, Train loss: 1.643, Test loss: 1.570, Test accuracy: 42.30
Round  10, Train loss: 1.618, Test loss: 1.561, Test accuracy: 43.41
Round  11, Train loss: 1.619, Test loss: 1.504, Test accuracy: 45.54
Round  12, Train loss: 1.552, Test loss: 1.491, Test accuracy: 46.74
Round  13, Train loss: 1.537, Test loss: 1.458, Test accuracy: 47.77
Round  14, Train loss: 1.504, Test loss: 1.434, Test accuracy: 48.15
Round  15, Train loss: 1.507, Test loss: 1.410, Test accuracy: 49.01
Round  16, Train loss: 1.477, Test loss: 1.386, Test accuracy: 49.89
Round  17, Train loss: 1.433, Test loss: 1.381, Test accuracy: 50.66
Round  18, Train loss: 1.415, Test loss: 1.353, Test accuracy: 51.82
Round  19, Train loss: 1.395, Test loss: 1.351, Test accuracy: 51.65
Round  20, Train loss: 1.397, Test loss: 1.318, Test accuracy: 52.60
Round  21, Train loss: 1.355, Test loss: 1.326, Test accuracy: 52.68
Round  22, Train loss: 1.350, Test loss: 1.291, Test accuracy: 54.03
Round  23, Train loss: 1.295, Test loss: 1.308, Test accuracy: 53.38
Round  24, Train loss: 1.335, Test loss: 1.273, Test accuracy: 54.89
Round  25, Train loss: 1.274, Test loss: 1.249, Test accuracy: 56.01
Round  26, Train loss: 1.298, Test loss: 1.236, Test accuracy: 56.05
Round  27, Train loss: 1.281, Test loss: 1.228, Test accuracy: 56.28
Round  28, Train loss: 1.258, Test loss: 1.206, Test accuracy: 57.08
Round  29, Train loss: 1.217, Test loss: 1.206, Test accuracy: 57.15
Round  30, Train loss: 1.252, Test loss: 1.191, Test accuracy: 57.59
Round  31, Train loss: 1.180, Test loss: 1.198, Test accuracy: 57.20
Round  32, Train loss: 1.186, Test loss: 1.186, Test accuracy: 57.72
Round  33, Train loss: 1.183, Test loss: 1.169, Test accuracy: 58.10
Round  34, Train loss: 1.156, Test loss: 1.161, Test accuracy: 58.71
Round  35, Train loss: 1.153, Test loss: 1.147, Test accuracy: 59.43
Round  36, Train loss: 1.125, Test loss: 1.144, Test accuracy: 59.90
Round  37, Train loss: 1.116, Test loss: 1.135, Test accuracy: 59.99
Round  38, Train loss: 1.091, Test loss: 1.134, Test accuracy: 60.02
Round  39, Train loss: 1.140, Test loss: 1.104, Test accuracy: 60.97
Round  40, Train loss: 1.073, Test loss: 1.110, Test accuracy: 61.02
Round  41, Train loss: 1.105, Test loss: 1.102, Test accuracy: 61.35
Round  42, Train loss: 1.033, Test loss: 1.095, Test accuracy: 61.36
Round  43, Train loss: 1.042, Test loss: 1.096, Test accuracy: 61.13
Round  44, Train loss: 1.017, Test loss: 1.093, Test accuracy: 61.55
Round  45, Train loss: 1.011, Test loss: 1.090, Test accuracy: 61.71
Round  46, Train loss: 1.002, Test loss: 1.085, Test accuracy: 61.95
Round  47, Train loss: 1.024, Test loss: 1.073, Test accuracy: 62.45
Round  48, Train loss: 0.995, Test loss: 1.067, Test accuracy: 62.48
Round  49, Train loss: 0.974, Test loss: 1.057, Test accuracy: 62.92
Round  50, Train loss: 0.973, Test loss: 1.050, Test accuracy: 63.14
Round  51, Train loss: 0.974, Test loss: 1.051, Test accuracy: 63.01
Round  52, Train loss: 0.955, Test loss: 1.041, Test accuracy: 63.16
Round  53, Train loss: 0.952, Test loss: 1.033, Test accuracy: 63.53
Round  54, Train loss: 0.977, Test loss: 1.027, Test accuracy: 64.12
Round  55, Train loss: 0.933, Test loss: 1.038, Test accuracy: 63.44
Round  56, Train loss: 0.946, Test loss: 1.039, Test accuracy: 63.55
Round  57, Train loss: 0.903, Test loss: 1.027, Test accuracy: 63.80
Round  58, Train loss: 0.901, Test loss: 1.027, Test accuracy: 64.30
Round  59, Train loss: 0.849, Test loss: 1.030, Test accuracy: 64.06
Round  60, Train loss: 0.872, Test loss: 1.038, Test accuracy: 63.96
Round  61, Train loss: 0.881, Test loss: 1.029, Test accuracy: 63.81
Round  62, Train loss: 0.909, Test loss: 1.016, Test accuracy: 64.44
Round  63, Train loss: 0.853, Test loss: 1.012, Test accuracy: 64.58
Round  64, Train loss: 0.832, Test loss: 1.016, Test accuracy: 64.32
Round  65, Train loss: 0.864, Test loss: 1.007, Test accuracy: 64.95
Round  66, Train loss: 0.843, Test loss: 1.022, Test accuracy: 64.50
Round  67, Train loss: 0.795, Test loss: 1.005, Test accuracy: 65.51
Round  68, Train loss: 0.854, Test loss: 0.998, Test accuracy: 65.56
Round  69, Train loss: 0.807, Test loss: 1.004, Test accuracy: 65.61
Round  70, Train loss: 0.821, Test loss: 0.997, Test accuracy: 65.82
Round  71, Train loss: 0.784, Test loss: 0.990, Test accuracy: 65.45
Round  72, Train loss: 0.722, Test loss: 1.002, Test accuracy: 65.63
Round  73, Train loss: 0.771, Test loss: 0.991, Test accuracy: 65.94
Round  74, Train loss: 0.753, Test loss: 1.005, Test accuracy: 65.25
Round  75, Train loss: 0.803, Test loss: 0.995, Test accuracy: 65.91
Round  76, Train loss: 0.764, Test loss: 0.995, Test accuracy: 65.91
Round  77, Train loss: 0.777, Test loss: 0.995, Test accuracy: 65.83
Round  78, Train loss: 0.746, Test loss: 0.997, Test accuracy: 65.86
Round  79, Train loss: 0.758, Test loss: 0.980, Test accuracy: 66.50
Round  80, Train loss: 0.788, Test loss: 0.971, Test accuracy: 66.91
Round  81, Train loss: 0.723, Test loss: 0.977, Test accuracy: 66.38
Round  82, Train loss: 0.745, Test loss: 0.968, Test accuracy: 67.06
Round  83, Train loss: 0.741, Test loss: 0.971, Test accuracy: 66.65
Round  84, Train loss: 0.725, Test loss: 0.976, Test accuracy: 66.33
Round  85, Train loss: 0.767, Test loss: 0.973, Test accuracy: 66.67
Round  86, Train loss: 0.677, Test loss: 0.987, Test accuracy: 66.26
Round  87, Train loss: 0.701, Test loss: 0.983, Test accuracy: 66.53
Round  88, Train loss: 0.676, Test loss: 0.978, Test accuracy: 66.66
Round  89, Train loss: 0.648, Test loss: 0.977, Test accuracy: 66.68
Round  90, Train loss: 0.705, Test loss: 0.970, Test accuracy: 67.17
Round  91, Train loss: 0.674, Test loss: 0.969, Test accuracy: 66.89
Round  92, Train loss: 0.673, Test loss: 0.984, Test accuracy: 66.31
Round  93, Train loss: 0.673, Test loss: 0.985, Test accuracy: 66.58
Round  94, Train loss: 0.689, Test loss: 0.979, Test accuracy: 66.59
Round  95, Train loss: 0.654, Test loss: 0.968, Test accuracy: 66.86
Round  96, Train loss: 0.638, Test loss: 0.969, Test accuracy: 67.03
Round  97, Train loss: 0.670, Test loss: 0.973, Test accuracy: 66.97
Round  98, Train loss: 0.633, Test loss: 0.990, Test accuracy: 66.88
Round  99, Train loss: 0.600, Test loss: 0.977, Test accuracy: 66.98
Final Round, Train loss: 0.561, Test loss: 0.986, Test accuracy: 66.98
Average accuracy final 10 rounds: 66.82624999999999
1845.3841094970703
[1.728520154953003, 3.080855369567871, 4.417928457260132, 5.758871793746948, 7.087554216384888, 8.437259674072266, 9.862974405288696, 11.18232774734497, 12.468670129776001, 13.760276079177856, 15.049176454544067, 16.337173461914062, 17.62404179573059, 18.971008777618408, 20.256219625473022, 21.542320489883423, 22.829643487930298, 24.170535564422607, 25.520429134368896, 26.87145757675171, 28.230546951293945, 29.571030855178833, 30.921404600143433, 32.28109431266785, 33.636051177978516, 34.96775436401367, 36.31261444091797, 37.65805411338806, 38.9946985244751, 40.329474210739136, 41.67476487159729, 43.00808382034302, 44.34389901161194, 45.6920371055603, 47.036651372909546, 48.37123990058899, 49.71704030036926, 51.06159543991089, 52.41388440132141, 53.76203155517578, 55.112236738204956, 56.45098567008972, 57.79218864440918, 59.14431810379028, 60.48948931694031, 61.8194797039032, 63.153045892715454, 64.49359178543091, 65.82098126411438, 67.1529529094696, 68.498202085495, 69.83609175682068, 71.1629204750061, 72.49767565727234, 73.82543349266052, 75.11218047142029, 76.40085053443909, 77.69587635993958, 79.00421643257141, 80.28608584403992, 81.58135890960693, 82.89448142051697, 84.22874999046326, 85.57726764678955, 86.93624114990234, 88.2748670578003, 89.61081409454346, 90.96573376655579, 92.33229947090149, 93.63002514839172, 94.97077512741089, 96.34834313392639, 97.64012217521667, 98.98561120033264, 100.33817315101624, 101.67831254005432, 103.00989508628845, 104.36192274093628, 105.70194053649902, 107.04691672325134, 108.39983248710632, 109.71443605422974, 110.99408721923828, 112.28755211830139, 113.61977577209473, 114.96213102340698, 116.25274705886841, 117.57149267196655, 118.88735508918762, 120.17551779747009, 121.47344946861267, 122.77436232566833, 124.0753858089447, 125.36396861076355, 126.70511388778687, 128.00178861618042, 129.28301286697388, 130.59494829177856, 131.89682531356812, 133.18628096580505, 135.25688314437866]
[19.815, 24.305, 28.0725, 30.925, 34.2975, 35.8325, 37.8075, 40.0225, 41.815, 42.2975, 43.4125, 45.5375, 46.7425, 47.7725, 48.15, 49.005, 49.89, 50.655, 51.82, 51.6475, 52.5975, 52.6825, 54.0275, 53.3825, 54.89, 56.01, 56.045, 56.2825, 57.08, 57.15, 57.5875, 57.195, 57.7225, 58.105, 58.7125, 59.43, 59.9, 59.995, 60.015, 60.965, 61.02, 61.3475, 61.36, 61.13, 61.55, 61.7125, 61.955, 62.4475, 62.4825, 62.9225, 63.14, 63.0075, 63.165, 63.53, 64.1175, 63.4425, 63.545, 63.8, 64.295, 64.06, 63.9625, 63.81, 64.4425, 64.58, 64.3175, 64.955, 64.495, 65.5075, 65.5575, 65.615, 65.8225, 65.455, 65.6325, 65.945, 65.2475, 65.9125, 65.9075, 65.8275, 65.8625, 66.505, 66.91, 66.375, 67.0625, 66.6475, 66.3275, 66.675, 66.2575, 66.53, 66.66, 66.68, 67.1675, 66.89, 66.3075, 66.5825, 66.595, 66.8575, 67.0325, 66.9725, 66.875, 66.9825, 66.9775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.274, Test loss: 2.130, Test accuracy: 22.23
Round   1, Train loss: 2.072, Test loss: 1.928, Test accuracy: 29.49
Round   2, Train loss: 1.970, Test loss: 1.805, Test accuracy: 31.50
Round   3, Train loss: 1.863, Test loss: 1.747, Test accuracy: 34.90
Round   4, Train loss: 1.801, Test loss: 1.655, Test accuracy: 36.84
Round   5, Train loss: 1.700, Test loss: 1.609, Test accuracy: 40.22
Round   6, Train loss: 1.634, Test loss: 1.571, Test accuracy: 43.24
Round   7, Train loss: 1.606, Test loss: 1.560, Test accuracy: 40.51
Round   8, Train loss: 1.571, Test loss: 1.534, Test accuracy: 41.87
Round   9, Train loss: 1.485, Test loss: 1.489, Test accuracy: 46.74
Round  10, Train loss: 1.510, Test loss: 1.525, Test accuracy: 46.85
Round  11, Train loss: 1.510, Test loss: 1.572, Test accuracy: 44.27
Round  12, Train loss: 1.412, Test loss: 1.420, Test accuracy: 47.67
Round  13, Train loss: 1.366, Test loss: 1.414, Test accuracy: 49.74
Round  14, Train loss: 1.428, Test loss: 1.377, Test accuracy: 50.56
Round  15, Train loss: 1.294, Test loss: 1.356, Test accuracy: 51.35
Round  16, Train loss: 1.297, Test loss: 1.350, Test accuracy: 49.85
Round  17, Train loss: 1.286, Test loss: 1.344, Test accuracy: 51.74
Round  18, Train loss: 1.203, Test loss: 1.344, Test accuracy: 50.42
Round  19, Train loss: 1.190, Test loss: 1.336, Test accuracy: 53.74
Round  20, Train loss: 1.135, Test loss: 1.352, Test accuracy: 53.27
Round  21, Train loss: 1.225, Test loss: 1.353, Test accuracy: 50.69
Round  22, Train loss: 1.124, Test loss: 1.265, Test accuracy: 55.12
Round  23, Train loss: 1.096, Test loss: 1.298, Test accuracy: 56.23
Round  24, Train loss: 1.149, Test loss: 1.339, Test accuracy: 55.61
Round  25, Train loss: 1.067, Test loss: 1.260, Test accuracy: 55.79
Round  26, Train loss: 1.075, Test loss: 1.240, Test accuracy: 54.13
Round  27, Train loss: 1.035, Test loss: 1.236, Test accuracy: 54.28
Round  28, Train loss: 0.989, Test loss: 1.214, Test accuracy: 57.18
Round  29, Train loss: 0.965, Test loss: 1.275, Test accuracy: 57.92
Round  30, Train loss: 0.991, Test loss: 1.209, Test accuracy: 57.44
Round  31, Train loss: 0.963, Test loss: 1.204, Test accuracy: 57.58
Round  32, Train loss: 0.926, Test loss: 1.360, Test accuracy: 57.85
Round  33, Train loss: 0.905, Test loss: 1.208, Test accuracy: 58.53
Round  34, Train loss: 0.900, Test loss: 1.282, Test accuracy: 54.99
Round  35, Train loss: 0.887, Test loss: 1.201, Test accuracy: 58.91
Round  36, Train loss: 0.882, Test loss: 1.243, Test accuracy: 59.55
Round  37, Train loss: 0.835, Test loss: 1.187, Test accuracy: 57.76
Round  38, Train loss: 0.838, Test loss: 1.220, Test accuracy: 58.41
Round  39, Train loss: 0.784, Test loss: 1.216, Test accuracy: 59.19
Round  40, Train loss: 0.802, Test loss: 1.289, Test accuracy: 56.02
Round  41, Train loss: 0.802, Test loss: 1.244, Test accuracy: 60.13
Round  42, Train loss: 0.773, Test loss: 1.195, Test accuracy: 59.78
Round  43, Train loss: 0.722, Test loss: 1.221, Test accuracy: 60.09
Round  44, Train loss: 0.724, Test loss: 1.392, Test accuracy: 59.98
Round  45, Train loss: 0.745, Test loss: 1.364, Test accuracy: 60.01
Round  46, Train loss: 0.745, Test loss: 1.232, Test accuracy: 59.70
Round  47, Train loss: 0.714, Test loss: 1.227, Test accuracy: 59.42
Round  48, Train loss: 0.715, Test loss: 1.232, Test accuracy: 57.74
Round  49, Train loss: 0.689, Test loss: 1.228, Test accuracy: 58.27
Round  50, Train loss: 0.654, Test loss: 1.228, Test accuracy: 59.94
Round  51, Train loss: 0.673, Test loss: 1.293, Test accuracy: 61.27
Round  52, Train loss: 0.668, Test loss: 1.319, Test accuracy: 60.73
Round  53, Train loss: 0.608, Test loss: 1.233, Test accuracy: 58.23
Round  54, Train loss: 0.707, Test loss: 1.296, Test accuracy: 61.06
Round  55, Train loss: 0.612, Test loss: 1.310, Test accuracy: 61.41
Round  56, Train loss: 0.602, Test loss: 1.250, Test accuracy: 61.83
Round  57, Train loss: 0.625, Test loss: 1.305, Test accuracy: 61.51
Round  58, Train loss: 0.621, Test loss: 1.244, Test accuracy: 58.77
Round  59, Train loss: 0.568, Test loss: 1.312, Test accuracy: 61.85
Round  60, Train loss: 0.622, Test loss: 1.241, Test accuracy: 58.91
Round  61, Train loss: 0.575, Test loss: 1.337, Test accuracy: 61.81
Round  62, Train loss: 0.552, Test loss: 1.246, Test accuracy: 61.33
Round  63, Train loss: 0.575, Test loss: 1.230, Test accuracy: 59.51
Round  64, Train loss: 0.534, Test loss: 1.271, Test accuracy: 61.33
Round  65, Train loss: 0.583, Test loss: 1.332, Test accuracy: 57.67
Round  66, Train loss: 0.596, Test loss: 1.224, Test accuracy: 59.56
Round  67, Train loss: 0.556, Test loss: 1.232, Test accuracy: 59.79
Round  68, Train loss: 0.518, Test loss: 1.218, Test accuracy: 60.40
Round  69, Train loss: 0.523, Test loss: 1.294, Test accuracy: 61.42
Round  70, Train loss: 0.577, Test loss: 1.391, Test accuracy: 57.72
Round  71, Train loss: 0.501, Test loss: 1.615, Test accuracy: 56.98
Round  72, Train loss: 0.523, Test loss: 1.236, Test accuracy: 59.74
Round  73, Train loss: 0.496, Test loss: 1.365, Test accuracy: 62.26
Round  74, Train loss: 0.499, Test loss: 1.249, Test accuracy: 60.38
Round  75, Train loss: 0.480, Test loss: 1.230, Test accuracy: 60.27
Round  76, Train loss: 0.512, Test loss: 1.295, Test accuracy: 61.88
Round  77, Train loss: 0.456, Test loss: 1.347, Test accuracy: 61.13
Round  78, Train loss: 0.447, Test loss: 1.303, Test accuracy: 61.65
Round  79, Train loss: 0.455, Test loss: 1.318, Test accuracy: 62.42
Round  80, Train loss: 0.472, Test loss: 1.304, Test accuracy: 62.30
Round  81, Train loss: 0.474, Test loss: 1.442, Test accuracy: 62.26
Round  82, Train loss: 0.468, Test loss: 1.461, Test accuracy: 58.38
Round  83, Train loss: 0.460, Test loss: 1.430, Test accuracy: 58.20
Round  84, Train loss: 0.425, Test loss: 1.308, Test accuracy: 60.31
Round  85, Train loss: 0.473, Test loss: 1.319, Test accuracy: 62.13
Round  86, Train loss: 0.422, Test loss: 1.426, Test accuracy: 62.74
Round  87, Train loss: 0.428, Test loss: 1.417, Test accuracy: 63.03
Round  88, Train loss: 0.462, Test loss: 1.285, Test accuracy: 62.55
Round  89, Train loss: 0.403, Test loss: 1.297, Test accuracy: 62.69
Round  90, Train loss: 0.388, Test loss: 1.414, Test accuracy: 59.26
Round  91, Train loss: 0.435, Test loss: 1.575, Test accuracy: 62.97
Round  92, Train loss: 0.383, Test loss: 1.479, Test accuracy: 62.87
Round  93, Train loss: 0.425, Test loss: 1.285, Test accuracy: 61.16
Round  94, Train loss: 0.370, Test loss: 1.459, Test accuracy: 62.95
Round  95, Train loss: 0.434, Test loss: 1.260, Test accuracy: 60.44
Round  96, Train loss: 0.403, Test loss: 1.296, Test accuracy: 61.04
Round  97, Train loss: 0.359, Test loss: 1.459, Test accuracy: 63.05
Round  98, Train loss: 0.448, Test loss: 1.441, Test accuracy: 63.10
Round  99, Train loss: 0.369, Test loss: 1.365, Test accuracy: 60.27
Final Round, Train loss: 0.386, Test loss: 1.297, Test accuracy: 63.12
Average accuracy final 10 rounds: 61.711
2744.9555053710938
[3.263240098953247, 6.242027521133423, 9.22866678237915, 12.21421217918396, 15.20934247970581, 18.22723889350891, 21.243773937225342, 24.238218545913696, 27.235559225082397, 30.237459659576416, 33.24103856086731, 36.24674439430237, 39.23945331573486, 42.241843700408936, 45.23610877990723, 48.22881531715393, 51.21450924873352, 54.2017936706543, 57.20739197731018, 60.201233863830566, 63.15887260437012, 66.11473298072815, 69.08624911308289, 71.96938252449036, 74.93054914474487, 77.90841436386108, 80.91554427146912, 83.89285469055176, 86.86769366264343, 89.87470889091492, 92.90247750282288, 95.9341402053833, 98.94833111763, 102.00098657608032, 105.0184121131897, 108.03647422790527, 111.0763828754425, 114.08436918258667, 117.0988097190857, 119.83531951904297, 122.56092596054077, 125.29561686515808, 128.02351331710815, 130.73790073394775, 133.45431518554688, 136.1857566833496, 138.93084502220154, 141.65531635284424, 144.3733639717102, 147.07602715492249, 149.7743844985962, 152.47281050682068, 155.1787292957306, 157.86657524108887, 160.57773566246033, 163.3105607032776, 166.0291006565094, 168.75678324699402, 171.4687795639038, 174.1828863620758, 176.89643096923828, 179.6093249320984, 182.32577562332153, 185.02183318138123, 187.72178626060486, 190.4342052936554, 193.1649775505066, 195.88747715950012, 198.58345866203308, 201.2999782562256, 204.0094645023346, 206.70716094970703, 209.4079246520996, 212.11520099639893, 214.83325052261353, 217.56013107299805, 220.29109120368958, 223.01408624649048, 225.7233808040619, 228.44534134864807, 231.17122721672058, 233.88971877098083, 236.6157352924347, 239.3444857597351, 242.11374688148499, 244.8527193069458, 247.56012988090515, 250.26007890701294, 252.97513556480408, 255.65195751190186, 258.29340410232544, 260.95833230018616, 263.6014277935028, 266.2496283054352, 268.8960978984833, 271.5598568916321, 274.2014994621277, 276.8551170825958, 279.5174503326416, 282.1660852432251, 284.85024309158325]
[22.235, 29.49, 31.5, 34.9, 36.84, 40.22, 43.245, 40.51, 41.8675, 46.7375, 46.855, 44.2725, 47.675, 49.7375, 50.565, 51.35, 49.8475, 51.7375, 50.42, 53.745, 53.275, 50.6875, 55.12, 56.235, 55.6125, 55.7925, 54.135, 54.2775, 57.18, 57.9175, 57.4375, 57.5825, 57.8475, 58.53, 54.9925, 58.91, 59.5525, 57.7625, 58.405, 59.1925, 56.0175, 60.1275, 59.785, 60.0875, 59.9775, 60.005, 59.7025, 59.425, 57.745, 58.2675, 59.94, 61.27, 60.7325, 58.2275, 61.065, 61.41, 61.825, 61.505, 58.775, 61.8525, 58.9075, 61.8075, 61.325, 59.505, 61.3325, 57.67, 59.5575, 59.7875, 60.4025, 61.4225, 57.715, 56.9775, 59.7425, 62.2625, 60.38, 60.265, 61.88, 61.135, 61.645, 62.4225, 62.2975, 62.255, 58.38, 58.205, 60.315, 62.1275, 62.74, 63.0325, 62.5525, 62.6925, 59.255, 62.9725, 62.87, 61.1575, 62.955, 60.4375, 61.0375, 63.055, 63.105, 60.265, 63.115]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

8577.309713602066
[1.485741376876831, 2.744899272918701, 4.0171897411346436, 5.2887701988220215, 6.5664427280426025, 7.842285633087158, 9.123176574707031, 10.399111270904541, 11.674952983856201, 12.960114479064941, 14.24113917350769, 15.514020204544067, 16.792462587356567, 18.072309494018555, 19.356674671173096, 20.65300703048706, 21.942466497421265, 23.219088315963745, 24.498382329940796, 25.78818941116333, 27.126450777053833, 28.482134103775024, 29.80755925178528, 30.94739294052124, 32.0838668346405, 33.213802099227905, 34.34742331504822, 35.47528147697449, 36.607290506362915, 37.74225068092346, 38.871485233306885, 40.00357151031494, 41.13218140602112, 42.266507625579834, 43.39836025238037, 44.52700734138489, 45.65699863433838, 46.78714346885681, 47.916757106781006, 49.0539767742157, 50.18342041969299, 51.31398677825928, 52.44231700897217, 53.572458267211914, 54.709097385406494, 55.83798146247864, 56.97412467002869, 58.1089301109314, 59.24448776245117, 60.382853507995605, 61.5191969871521, 62.65430188179016, 63.94381809234619, 65.2445125579834, 66.54146647453308, 67.8311996459961, 69.12142157554626, 70.41027426719666, 71.70022082328796, 72.9973840713501, 74.28480553627014, 75.58466672897339, 76.87322044372559, 78.16670989990234, 79.45878577232361, 80.751629114151, 82.04472398757935, 83.34120035171509, 84.63696527481079, 85.92908239364624, 87.21844792366028, 88.51182389259338, 89.8072657585144, 91.09816932678223, 92.38774871826172, 93.68523144721985, 94.9725067615509, 96.2614688873291, 97.5617983341217, 98.85002613067627, 100.14062976837158, 101.42951226234436, 102.72252607345581, 104.01110935211182, 105.29905939102173, 106.58839392662048, 107.87774181365967, 109.15863990783691, 110.44008207321167, 111.73624873161316, 113.03523206710815, 114.32317018508911, 115.61774778366089, 116.91839694976807, 118.21519160270691, 119.51080226898193, 120.80143904685974, 122.10232257843018, 123.39872765541077, 124.68954992294312, 125.98655080795288, 127.14839148521423, 128.27553057670593, 129.40251994132996, 130.5359287261963, 131.66687297821045, 132.80131363868713, 133.93269681930542, 135.07957863807678, 136.21258234977722, 137.34742832183838, 138.4829661846161, 139.61910724639893, 140.7517647743225, 141.8848237991333, 143.02041721343994, 144.15608167648315, 145.29504656791687, 146.43497848510742, 147.5745804309845, 148.71475195884705, 149.86005520820618, 150.99963784217834, 152.1475875377655, 153.2957272529602, 154.4400475025177, 155.58038902282715, 156.72151708602905, 157.85483646392822, 158.98894357681274, 160.13889908790588, 161.2827169895172, 162.42356872558594, 163.5614035129547, 164.70228791236877, 165.8424301147461, 166.97750616073608, 168.27100133895874, 169.5670166015625, 170.86306762695312, 172.16578650474548, 173.46242547035217, 174.76121520996094, 176.05752086639404, 177.3651626110077, 178.66634273529053, 179.9574203491211, 181.2546854019165, 182.55120849609375, 183.85409569740295, 185.15963888168335, 186.45968341827393, 187.75650644302368, 189.05585169792175, 190.3621380329132, 191.66328835487366, 192.95547366142273, 194.2488079071045, 195.5497224330902, 196.8382396697998, 198.1353521347046, 199.4284472465515, 200.73005986213684, 202.01779675483704, 203.31353902816772, 204.6056694984436, 205.90590357780457, 207.20264196395874, 208.49584460258484, 209.78547739982605, 211.0833854675293, 212.37583184242249, 213.6658592224121, 214.96589756011963, 216.2643985748291, 217.56467366218567, 218.86812353134155, 220.16650485992432, 221.45921516418457, 222.76074504852295, 224.0616579055786, 225.3714394569397, 226.66041088104248, 227.9583010673523, 229.2522292137146, 230.56107187271118, 231.85001015663147, 233.14968466758728, 234.44547533988953, 235.7438805103302, 237.03145122528076, 238.32166814804077, 239.61258792877197, 240.91118621826172, 242.20537161827087, 243.50193738937378, 244.7912974357605, 246.08311319351196, 247.37755036354065, 248.67571187019348, 249.9677448272705, 251.2578203678131, 252.55065274238586, 253.8489465713501, 255.14387798309326, 256.4354622364044, 257.72601890563965, 259.0217807292938, 260.31906747817993, 261.61546206474304, 262.9091110229492, 264.1998145580292, 265.4873926639557, 266.7937150001526, 268.07720613479614, 269.3590352535248, 270.6519844532013, 271.95335602760315, 273.25963973999023, 274.56332659721375, 275.8627486228943, 277.16447591781616, 278.46968030929565, 279.777058839798, 281.0788941383362, 282.37751269340515, 283.67891788482666, 284.98358392715454, 286.2817680835724, 287.5815098285675, 288.8834869861603, 290.179536819458, 291.4765086174011, 292.7760624885559, 294.07407903671265, 295.3730366230011, 296.6776113510132, 297.9796071052551, 299.27089953422546, 300.5591583251953, 301.85632848739624, 303.15325927734375, 304.4522032737732, 305.73877120018005, 307.03686022758484, 308.33849477767944, 309.64660906791687, 310.94708156585693, 312.24617290496826, 313.5526101589203, 314.8529198169708, 316.1472885608673, 317.4438180923462, 318.74423813819885, 320.0419547557831, 321.34821462631226, 322.6591022014618, 323.9643428325653, 325.29720878601074, 326.60826110839844, 327.926833152771, 329.23938274383545, 330.54946303367615, 331.8572156429291, 333.16129517555237, 334.46772837638855, 335.77862548828125, 337.0844693183899, 338.40298986434937, 339.72027492523193, 341.03036308288574, 342.33647656440735, 343.64236664772034, 344.95042753219604, 346.2524616718292, 347.56371784210205, 348.8700225353241, 350.17079877853394, 351.4764573574066, 352.7880494594574, 354.09473395347595, 355.4006118774414, 356.7060601711273, 358.02151226997375, 359.3521406650543, 360.6692008972168, 362.01570558547974, 363.32181453704834, 364.62473797798157, 365.9180586338043, 367.06447649002075, 368.2049448490143, 369.3532109260559, 370.48913979530334, 371.6216514110565, 372.75670742988586, 373.8974394798279, 375.0262415409088, 376.1585569381714, 377.28912687301636, 379.5690977573395]
[10.145, 10.1825, 10.175, 10.1725, 10.195, 10.2325, 10.185, 10.2025, 10.3625, 10.47, 10.5175, 10.525, 10.555, 10.6725, 10.9, 10.9625, 11.1025, 11.2775, 11.4825, 11.5775, 11.6475, 11.7925, 11.8, 11.8775, 11.935, 11.975, 12.1975, 12.5225, 12.67, 12.8675, 12.8725, 12.9825, 13.21, 13.455, 13.5625, 13.6175, 14.0475, 14.2175, 14.3375, 14.5425, 14.6825, 14.73, 14.6975, 14.8275, 14.895, 14.92, 14.7775, 14.8325, 14.86, 14.665, 14.46, 14.6225, 14.57, 14.3525, 14.6425, 14.7225, 14.3925, 14.49, 14.2175, 14.2175, 14.2825, 14.3, 14.0925, 14.0375, 14.0975, 14.2775, 14.31, 14.2125, 14.4325, 14.53, 14.46, 14.4, 14.5525, 14.49, 14.7175, 14.97, 15.595, 15.69, 15.905, 16.0025, 15.905, 15.9425, 15.955, 15.995, 16.06, 16.2325, 16.52, 16.51, 16.4675, 16.3525, 16.6875, 17.2175, 17.27, 17.4975, 17.235, 17.4325, 17.3625, 17.57, 17.83, 18.01, 18.19, 18.2175, 18.3975, 18.51, 18.5925, 18.735, 18.695, 18.945, 18.91, 18.93, 18.72, 18.845, 18.955, 19.02, 18.97, 18.8925, 18.76, 18.725, 18.785, 18.9425, 19.21, 19.0575, 18.9625, 19.12, 19.1175, 19.2375, 19.2475, 19.3075, 19.32, 19.4225, 19.5525, 19.4825, 19.5575, 19.795, 19.8975, 19.9525, 19.81, 19.7225, 19.93, 20.16, 20.175, 20.08, 20.085, 20.1975, 20.42, 20.41, 20.5375, 20.6525, 20.325, 17.21, 15.0175, 12.705, 12.1775, 12.1775, 11.6825, 11.6825, 11.6825, 11.1275, 10.5725, 10.5725, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.401, Test loss: 1.788, Test accuracy: 59.47
Final Round, Global train loss: 0.401, Global test loss: 2.005, Global test accuracy: 52.12
Average accuracy final 10 rounds: 59.404999999999994 

Average global accuracy final 10 rounds: 51.954 

3806.202492237091
[1.4103562831878662, 2.8207125663757324, 3.84812331199646, 4.8755340576171875, 5.915058851242065, 6.954583644866943, 7.980641841888428, 9.006700038909912, 10.021193265914917, 11.035686492919922, 12.068202495574951, 13.10071849822998, 14.127964496612549, 15.155210494995117, 16.18198871612549, 17.20876693725586, 18.233819723129272, 19.258872509002686, 20.28331232070923, 21.30775213241577, 22.326284885406494, 23.344817638397217, 24.359041690826416, 25.373265743255615, 26.39162802696228, 27.409990310668945, 28.43360996246338, 29.457229614257812, 30.478211641311646, 31.49919366836548, 32.5236496925354, 33.54810571670532, 34.74046516418457, 35.93282461166382, 37.12054896354675, 38.30827331542969, 39.50035810470581, 40.692442893981934, 41.87731909751892, 43.06219530105591, 44.25386309623718, 45.44553089141846, 46.63901710510254, 47.83250331878662, 49.02420401573181, 50.215904712677, 51.41365671157837, 52.611408710479736, 53.80345797538757, 54.99550724029541, 56.18899345397949, 57.382479667663574, 58.56874942779541, 59.755019187927246, 60.93324041366577, 62.1114616394043, 63.50636672973633, 64.90127182006836, 66.09338927268982, 67.28550672531128, 68.4775779247284, 69.66964912414551, 70.86016345024109, 72.05067777633667, 73.24474787712097, 74.43881797790527, 75.62815642356873, 76.81749486923218, 78.00550436973572, 79.19351387023926, 80.37496829032898, 81.5564227104187, 82.74044060707092, 83.92445850372314, 85.10388922691345, 86.28331995010376, 87.46390080451965, 88.64448165893555, 89.83645987510681, 91.02843809127808, 92.22466683387756, 93.42089557647705, 94.61356902122498, 95.8062424659729, 96.99738430976868, 98.18852615356445, 99.37472581863403, 100.56092548370361, 101.75235080718994, 102.94377613067627, 103.98852300643921, 105.03326988220215, 106.06097555160522, 107.0886812210083, 108.11950039863586, 109.15031957626343, 110.17915034294128, 111.20798110961914, 112.24747848510742, 113.2869758605957, 114.3122808933258, 115.33758592605591, 116.35810875892639, 117.37863159179688, 118.41368818283081, 119.44874477386475, 120.4829568862915, 121.51716899871826, 122.55376386642456, 123.59035873413086, 124.62695288658142, 125.66354703903198, 126.7045967578888, 127.7456464767456, 128.78423738479614, 129.82282829284668, 130.84497380256653, 131.86711931228638, 132.89633297920227, 133.92554664611816, 134.95406770706177, 135.98258876800537, 137.0094497203827, 138.03631067276, 139.0750434398651, 140.11377620697021, 141.147287607193, 142.18079900741577, 143.20634508132935, 144.23189115524292, 145.25620651245117, 146.28052186965942, 147.31233859062195, 148.34415531158447, 149.37252497673035, 150.40089464187622, 151.42366790771484, 152.44644117355347, 153.47789764404297, 154.50935411453247, 155.5418345928192, 156.57431507110596, 157.6096076965332, 158.64490032196045, 159.67160058021545, 160.69830083847046, 161.71725630760193, 162.7362117767334, 163.76321482658386, 164.79021787643433, 165.83077263832092, 166.87132740020752, 167.9028925895691, 168.93445777893066, 169.96851587295532, 171.00257396697998, 172.03388929367065, 173.06520462036133, 174.08740210533142, 175.1095995903015, 176.1280755996704, 177.1465516090393, 178.17060828208923, 179.19466495513916, 180.24916195869446, 181.30365896224976, 182.34293937683105, 183.38221979141235, 184.4109969139099, 185.43977403640747, 186.4700973033905, 187.50042057037354, 188.53010892868042, 189.5597972869873, 190.59654116630554, 191.63328504562378, 192.65882754325867, 193.68437004089355, 194.7184956073761, 195.75262117385864, 196.78460574150085, 197.81659030914307, 198.84740614891052, 199.87822198867798, 200.90804028511047, 201.93785858154297, 202.9671778678894, 203.99649715423584, 205.03166961669922, 206.0668420791626, 207.0929183959961, 208.1189947128296, 209.1379165649414, 210.15683841705322, 211.19127869606018, 212.22571897506714, 213.2516348361969, 214.27755069732666, 215.3056116104126, 216.33367252349854, 218.41060042381287, 220.4875283241272]
[22.855, 22.855, 26.25, 26.25, 29.12, 29.12, 32.0675, 32.0675, 33.8425, 33.8425, 35.5725, 35.5725, 37.395, 37.395, 37.5975, 37.5975, 38.88, 38.88, 40.5225, 40.5225, 41.83, 41.83, 43.4925, 43.4925, 44.06, 44.06, 44.5275, 44.5275, 45.2, 45.2, 45.065, 45.065, 46.1925, 46.1925, 47.635, 47.635, 47.885, 47.885, 48.62, 48.62, 49.105, 49.105, 49.58, 49.58, 50.0875, 50.0875, 49.775, 49.775, 50.37, 50.37, 50.355, 50.355, 51.59, 51.59, 52.115, 52.115, 52.5575, 52.5575, 52.89, 52.89, 53.095, 53.095, 53.1575, 53.1575, 53.4725, 53.4725, 53.34, 53.34, 54.0, 54.0, 53.6575, 53.6575, 54.3125, 54.3125, 54.3025, 54.3025, 54.7125, 54.7125, 54.94, 54.94, 55.29, 55.29, 55.6475, 55.6475, 55.8925, 55.8925, 55.91, 55.91, 55.915, 55.915, 55.9075, 55.9075, 56.0075, 56.0075, 56.295, 56.295, 56.5125, 56.5125, 56.2375, 56.2375, 56.415, 56.415, 56.335, 56.335, 56.7375, 56.7375, 56.445, 56.445, 56.7225, 56.7225, 57.0775, 57.0775, 57.495, 57.495, 57.5625, 57.5625, 57.4175, 57.4175, 57.7325, 57.7325, 57.7625, 57.7625, 57.61, 57.61, 57.935, 57.935, 58.015, 58.015, 57.6625, 57.6625, 58.005, 58.005, 58.08, 58.08, 58.5825, 58.5825, 58.28, 58.28, 58.055, 58.055, 58.11, 58.11, 58.3375, 58.3375, 58.51, 58.51, 58.205, 58.205, 58.2325, 58.2325, 58.3425, 58.3425, 58.225, 58.225, 58.645, 58.645, 58.665, 58.665, 58.8075, 58.8075, 58.765, 58.765, 58.7075, 58.7075, 58.885, 58.885, 59.065, 59.065, 59.2925, 59.2925, 59.3675, 59.3675, 59.335, 59.335, 59.44, 59.44, 59.3275, 59.3275, 59.22, 59.22, 58.8725, 58.8725, 59.2225, 59.2225, 59.28, 59.28, 59.475, 59.475, 59.8175, 59.8175, 59.615, 59.615, 59.3375, 59.3375, 59.635, 59.635, 59.345, 59.345, 59.45, 59.45, 59.465, 59.465]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.564, Test loss: 1.276, Test accuracy: 62.48
Average accuracy final 10 rounds: 61.81399999999999 

2834.7167434692383
[1.303173542022705, 2.60634708404541, 3.697333574295044, 4.788320064544678, 5.880782604217529, 6.973245143890381, 8.063112020492554, 9.152978897094727, 10.250566720962524, 11.348154544830322, 12.451720476150513, 13.555286407470703, 14.658189058303833, 15.761091709136963, 16.871233701705933, 17.981375694274902, 19.08536672592163, 20.18935775756836, 21.292253732681274, 22.39514970779419, 23.50356698036194, 24.611984252929688, 25.72049832344055, 26.829012393951416, 27.920594215393066, 29.012176036834717, 30.11156988143921, 31.2109637260437, 32.320247411727905, 33.42953109741211, 34.53812265396118, 35.646714210510254, 36.7444269657135, 37.84213972091675, 38.94390916824341, 40.04567861557007, 41.14974308013916, 42.25380754470825, 43.35553979873657, 44.45727205276489, 45.559945821762085, 46.66261959075928, 47.76466464996338, 48.86670970916748, 49.97204613685608, 51.07738256454468, 52.182865381240845, 53.28834819793701, 54.39263129234314, 55.49691438674927, 56.59559679031372, 57.694279193878174, 58.79957675933838, 59.904874324798584, 61.003485441207886, 62.10209655761719, 63.20792603492737, 64.31375551223755, 65.413006067276, 66.51225662231445, 67.60877656936646, 68.70529651641846, 69.8093318939209, 70.91336727142334, 72.01797461509705, 73.12258195877075, 74.22619295120239, 75.32980394363403, 76.42993116378784, 77.53005838394165, 78.63365817070007, 79.7372579574585, 80.83457255363464, 81.93188714981079, 83.03059577941895, 84.1293044090271, 85.23208928108215, 86.3348741531372, 87.43859052658081, 88.54230690002441, 89.64873123168945, 90.75515556335449, 91.85815167427063, 92.96114778518677, 94.06618428230286, 95.17122077941895, 96.27441835403442, 97.3776159286499, 98.48252558708191, 99.58743524551392, 100.68532085418701, 101.78320646286011, 102.88624906539917, 103.98929166793823, 105.09053444862366, 106.19177722930908, 107.29398393630981, 108.39619064331055, 109.50513672828674, 110.61408281326294, 111.72143387794495, 112.82878494262695, 113.92883062362671, 115.02887630462646, 116.12373757362366, 117.21859884262085, 118.29504013061523, 119.37148141860962, 120.45957374572754, 121.54766607284546, 122.64174342155457, 123.73582077026367, 124.82341647148132, 125.91101217269897, 127.00205183029175, 128.09309148788452, 129.1905641555786, 130.2880368232727, 131.3716104030609, 132.45518398284912, 133.53407764434814, 134.61297130584717, 135.70780038833618, 136.8026294708252, 137.90061807632446, 138.99860668182373, 140.0915207862854, 141.18443489074707, 142.27764868736267, 143.37086248397827, 144.46412086486816, 145.55737924575806, 146.6420295238495, 147.72667980194092, 148.81221532821655, 149.8977508544922, 150.99053740501404, 152.0833239555359, 153.17531728744507, 154.26731061935425, 155.35667300224304, 156.44603538513184, 157.53682136535645, 158.62760734558105, 159.7237048149109, 160.81980228424072, 161.90821623802185, 162.99663019180298, 164.0750606060028, 165.15349102020264, 166.24876379966736, 167.34403657913208, 168.43208575248718, 169.52013492584229, 170.6099922657013, 171.6998496055603, 172.78053426742554, 173.86121892929077, 174.94454264640808, 176.0278663635254, 177.10611701011658, 178.18436765670776, 179.2678759098053, 180.35138416290283, 181.43557953834534, 182.51977491378784, 183.60711240768433, 184.6944499015808, 185.75211453437805, 186.8097791671753, 187.89998579025269, 188.99019241333008, 190.08377194404602, 191.17735147476196, 192.26120519638062, 193.34505891799927, 194.42711973190308, 195.50918054580688, 196.59417128562927, 197.67916202545166, 198.76909494400024, 199.85902786254883, 200.9480037689209, 202.03697967529297, 203.140465259552, 204.24395084381104, 205.3401038646698, 206.43625688552856, 207.5316460132599, 208.6270351409912, 209.77459001541138, 210.92214488983154, 212.011488199234, 213.10083150863647, 214.19216990470886, 215.28350830078125, 216.37354350090027, 217.4635787010193, 218.55577182769775, 219.64796495437622, 221.61017632484436, 223.5723876953125]
[16.6825, 16.6825, 22.1225, 22.1225, 24.88, 24.88, 28.035, 28.035, 29.8175, 29.8175, 32.4725, 32.4725, 33.4575, 33.4575, 35.35, 35.35, 36.475, 36.475, 38.6175, 38.6175, 39.59, 39.59, 40.535, 40.535, 42.6725, 42.6725, 43.515, 43.515, 45.02, 45.02, 45.525, 45.525, 45.5325, 45.5325, 46.305, 46.305, 46.9475, 46.9475, 47.305, 47.305, 48.72, 48.72, 49.065, 49.065, 49.7125, 49.7125, 49.95, 49.95, 50.035, 50.035, 50.92, 50.92, 51.1425, 51.1425, 51.3325, 51.3325, 52.58, 52.58, 53.2325, 53.2325, 53.8825, 53.8825, 54.09, 54.09, 53.9575, 53.9575, 54.13, 54.13, 54.8375, 54.8375, 55.125, 55.125, 54.4225, 54.4225, 55.215, 55.215, 55.49, 55.49, 56.615, 56.615, 57.0175, 57.0175, 57.565, 57.565, 56.8925, 56.8925, 57.015, 57.015, 57.3175, 57.3175, 57.74, 57.74, 58.1175, 58.1175, 57.9075, 57.9075, 58.795, 58.795, 58.53, 58.53, 58.365, 58.365, 58.2875, 58.2875, 58.575, 58.575, 59.2225, 59.2225, 59.61, 59.61, 59.2425, 59.2425, 60.025, 60.025, 59.855, 59.855, 60.235, 60.235, 59.825, 59.825, 59.8675, 59.8675, 60.19, 60.19, 59.3625, 59.3625, 59.6725, 59.6725, 59.815, 59.815, 60.3125, 60.3125, 60.7625, 60.7625, 60.1675, 60.1675, 60.54, 60.54, 60.6825, 60.6825, 60.9225, 60.9225, 60.785, 60.785, 60.745, 60.745, 61.33, 61.33, 60.8975, 60.8975, 61.0325, 61.0325, 61.48, 61.48, 61.375, 61.375, 60.7925, 60.7925, 61.0125, 61.0125, 60.8925, 60.8925, 60.8375, 60.8375, 61.1375, 61.1375, 61.555, 61.555, 61.78, 61.78, 62.1, 62.1, 62.1075, 62.1075, 61.97, 61.97, 62.2875, 62.2875, 61.915, 61.915, 62.36, 62.36, 61.97, 61.97, 62.0775, 62.0775, 61.825, 61.825, 61.7225, 61.7225, 61.71, 61.71, 61.8575, 61.8575, 61.3825, 61.3825, 61.6325, 61.6325, 61.6025, 61.6025, 62.485, 62.485]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.569, Test loss: 0.940, Test accuracy: 68.32
Average accuracy final 10 rounds: 67.60549999999999
2980.5951182842255
[1.7271485328674316, 3.4542970657348633, 4.8097310066223145, 6.165164947509766, 7.52192497253418, 8.878684997558594, 10.176389455795288, 11.474093914031982, 12.69073224067688, 13.907370567321777, 15.117257118225098, 16.327143669128418, 17.55336856842041, 18.779593467712402, 20.009201526641846, 21.23880958557129, 22.4594247341156, 23.680039882659912, 24.893428087234497, 26.106816291809082, 27.31518006324768, 28.52354383468628, 29.74140453338623, 30.95926523208618, 32.18032217025757, 33.401379108428955, 34.625807762145996, 35.85023641586304, 37.07111668586731, 38.29199695587158, 39.50806474685669, 40.7241325378418, 41.93400692939758, 43.14388132095337, 44.352375984191895, 45.56087064743042, 46.770241498947144, 47.97961235046387, 49.20195508003235, 50.42429780960083, 51.64830017089844, 52.872302532196045, 54.0909206867218, 55.30953884124756, 56.52425003051758, 57.7389612197876, 58.95216774940491, 60.16537427902222, 61.389695167541504, 62.61401605606079, 63.83119058609009, 65.04836511611938, 66.54362964630127, 68.03889417648315, 69.31364679336548, 70.5883994102478, 71.85724830627441, 73.12609720230103, 74.3931155204773, 75.66013383865356, 77.04806399345398, 78.4359941482544, 79.83298182487488, 81.22996950149536, 82.61608529090881, 84.00220108032227, 85.38938164710999, 86.7765622138977, 88.15789246559143, 89.53922271728516, 90.92038941383362, 92.30155611038208, 93.69348573684692, 95.08541536331177, 96.4968810081482, 97.90834665298462, 99.29953169822693, 100.69071674346924, 102.06774234771729, 103.44476795196533, 104.83795118331909, 106.23113441467285, 107.62728071212769, 109.02342700958252, 110.41905927658081, 111.8146915435791, 113.19967031478882, 114.58464908599854, 115.97184443473816, 117.35903978347778, 118.74043130874634, 120.12182283401489, 121.53217911720276, 122.94253540039062, 124.33033490180969, 125.71813440322876, 127.10532116889954, 128.4925079345703, 129.90069794654846, 131.3088879585266, 132.71173000335693, 134.11457204818726, 135.5298614501953, 136.94515085220337, 138.33025693893433, 139.71536302566528, 141.12251353263855, 142.52966403961182, 143.93001341819763, 145.33036279678345, 146.7158544063568, 148.10134601593018, 149.47388911247253, 150.8464322090149, 152.23758935928345, 153.628746509552, 155.01362705230713, 156.39850759506226, 157.7894265651703, 159.18034553527832, 160.57839250564575, 161.97643947601318, 163.3835608959198, 164.79068231582642, 166.18853116035461, 167.5863800048828, 168.98994398117065, 170.3935079574585, 171.77253222465515, 173.1515564918518, 174.5265679359436, 175.9015793800354, 177.3087284564972, 178.71587753295898, 180.11583495140076, 181.51579236984253, 182.89730024337769, 184.27880811691284, 185.65163278579712, 187.0244574546814, 188.41258668899536, 189.80071592330933, 191.19078922271729, 192.58086252212524, 193.97538948059082, 195.3699164390564, 196.76017332077026, 198.15043020248413, 199.54839372634888, 200.94635725021362, 202.34793329238892, 203.7495093345642, 205.145281791687, 206.54105424880981, 207.93421125411987, 209.32736825942993, 210.72432708740234, 212.12128591537476, 213.51959872245789, 214.91791152954102, 216.18254470825195, 217.4471778869629, 218.72277808189392, 219.99837827682495, 221.2564079761505, 222.51443767547607, 223.79884219169617, 225.08324670791626, 226.35573959350586, 227.62823247909546, 228.89521861076355, 230.16220474243164, 231.44617867469788, 232.7301526069641, 234.01997637748718, 235.30980014801025, 236.58406972885132, 237.85833930969238, 239.142893075943, 240.4274468421936, 241.71715021133423, 243.00685358047485, 244.29757809638977, 245.5883026123047, 246.87036681175232, 248.15243101119995, 249.44020700454712, 250.7279829978943, 252.01466059684753, 253.30133819580078, 254.57932710647583, 255.85731601715088, 257.13224482536316, 258.40717363357544, 259.6980812549591, 260.9889888763428, 262.26764965057373, 263.5463104248047, 264.82836961746216, 266.11042881011963, 268.1782636642456, 270.2460985183716]
[17.935, 17.935, 23.4625, 23.4625, 25.7, 25.7, 28.525, 28.525, 30.91, 30.91, 32.76, 32.76, 34.41, 34.41, 36.565, 36.565, 38.245, 38.245, 39.975, 39.975, 41.1025, 41.1025, 42.6925, 42.6925, 43.26, 43.26, 44.5225, 44.5225, 45.7625, 45.7625, 46.2375, 46.2375, 46.6225, 46.6225, 47.905, 47.905, 48.2425, 48.2425, 48.785, 48.785, 49.45, 49.45, 50.5325, 50.5325, 52.105, 52.105, 52.7475, 52.7475, 53.255, 53.255, 54.0575, 54.0575, 54.3875, 54.3875, 54.8, 54.8, 54.52, 54.52, 55.1975, 55.1975, 55.695, 55.695, 56.025, 56.025, 56.72, 56.72, 56.9625, 56.9625, 57.6925, 57.6925, 58.065, 58.065, 58.5325, 58.5325, 58.5725, 58.5725, 59.245, 59.245, 60.11, 60.11, 59.765, 59.765, 60.1425, 60.1425, 60.605, 60.605, 60.1425, 60.1425, 60.7425, 60.7425, 61.165, 61.165, 61.0475, 61.0475, 61.265, 61.265, 61.7725, 61.7725, 62.2725, 62.2725, 62.555, 62.555, 62.785, 62.785, 62.2525, 62.2525, 62.83, 62.83, 63.21, 63.21, 63.67, 63.67, 63.5925, 63.5925, 63.62, 63.62, 63.665, 63.665, 63.4425, 63.4425, 63.37, 63.37, 63.89, 63.89, 64.075, 64.075, 63.525, 63.525, 64.5425, 64.5425, 64.145, 64.145, 65.2525, 65.2525, 65.43, 65.43, 65.185, 65.185, 64.9125, 64.9125, 65.245, 65.245, 65.48, 65.48, 65.4975, 65.4975, 65.78, 65.78, 65.5075, 65.5075, 65.8575, 65.8575, 66.0925, 66.0925, 66.3025, 66.3025, 66.075, 66.075, 66.145, 66.145, 65.8775, 65.8775, 65.485, 65.485, 66.715, 66.715, 66.5825, 66.5825, 66.8175, 66.8175, 66.925, 66.925, 67.4875, 67.4875, 67.3025, 67.3025, 67.4525, 67.4525, 67.72, 67.72, 67.6575, 67.6575, 67.7625, 67.7625, 66.8875, 66.8875, 67.165, 67.165, 67.4675, 67.4675, 67.915, 67.915, 67.7975, 67.7975, 67.6275, 67.6275, 67.825, 67.825, 67.95, 67.95, 68.3175, 68.3175]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.457, Test loss: 1.784, Test accuracy: 51.37
Average accuracy final 10 rounds: 50.74975
3003.1093814373016
[3.3148739337921143, 6.484703302383423, 9.651582717895508, 12.790904760360718, 15.930892705917358, 19.075708389282227, 22.22748112678528, 25.373470067977905, 28.52538537979126, 31.684412479400635, 34.80570316314697, 37.91144013404846, 41.03539276123047, 44.152857542037964, 47.26487588882446, 50.37461876869202, 53.51289486885071, 56.664106130599976, 59.797101974487305, 62.908578634262085, 66.0356330871582, 69.17241406440735, 72.30268096923828, 75.41245007514954, 78.54417705535889, 81.6632342338562, 84.79641771316528, 87.89321660995483, 91.03000044822693, 94.16279006004333, 97.28887009620667, 100.40565037727356, 103.52657675743103, 106.67107152938843, 109.79726362228394, 112.89790844917297, 116.01703381538391, 119.13936138153076, 122.24703335762024, 125.3530170917511, 128.4655203819275, 131.60604619979858, 134.7142345905304, 137.8163092136383, 140.9409523010254, 144.06522369384766, 147.19932889938354, 150.31482553482056, 153.45426869392395, 156.60268115997314, 159.7372908592224, 162.85434556007385, 166.00344228744507, 169.09723782539368, 172.23464608192444, 175.35742473602295, 178.47722744941711, 181.59724044799805, 184.73695921897888, 187.85209345817566, 190.980220079422, 194.0940659046173, 197.21887588500977, 200.34067606925964, 203.46736192703247, 206.58075046539307, 209.7134759426117, 212.8284478187561, 215.95961570739746, 219.05800080299377, 222.15899109840393, 225.2504105567932, 228.37404370307922, 231.45221543312073, 234.5606825351715, 237.66454648971558, 240.73277497291565, 243.80193758010864, 246.87829160690308, 249.95831656455994, 253.05714416503906, 256.1559591293335, 259.25396251678467, 262.36156153678894, 265.4876549243927, 268.5927834510803, 271.6981608867645, 274.8113634586334, 277.9279022216797, 281.035053730011, 284.1610224246979, 287.28430438041687, 290.405611038208, 293.51259779930115, 296.62190890312195, 299.74413084983826, 302.85972356796265, 306.0077986717224, 309.1412687301636, 312.26705169677734, 315.38077664375305]
[22.3225, 26.38, 29.8825, 32.54, 34.19, 35.0275, 37.6375, 38.975, 39.9325, 40.42, 40.2875, 41.225, 41.725, 42.1, 43.045, 43.29, 44.18, 45.47, 45.4, 45.275, 45.5125, 45.5075, 45.955, 46.53, 46.96, 47.24, 47.5925, 47.605, 47.5375, 47.785, 47.9675, 48.7325, 47.84, 48.47, 48.43, 48.25, 49.105, 48.8075, 48.69, 48.96, 48.15, 48.7875, 48.07, 48.835, 48.715, 49.3225, 48.485, 48.7975, 49.575, 49.3, 49.5825, 49.7225, 49.7275, 49.84, 49.585, 48.9825, 49.0475, 49.6975, 49.62, 50.26, 49.825, 49.195, 50.3, 48.9425, 49.29, 50.615, 50.7225, 50.96, 49.935, 50.705, 50.2925, 49.6225, 49.875, 49.9525, 50.235, 50.515, 50.43, 50.3, 50.0925, 50.7525, 50.685, 50.2175, 50.83, 50.725, 50.4325, 50.685, 50.33, 50.9825, 50.9, 50.53, 50.795, 50.905, 51.5275, 50.6275, 50.4925, 50.775, 51.1575, 50.5, 49.835, 50.8825, 51.37]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

8511.822862386703
[1.536494255065918, 2.8441474437713623, 4.147979736328125, 5.4704296588897705, 6.787309646606445, 8.107546329498291, 9.415691137313843, 10.720038175582886, 12.023712158203125, 13.33666181564331, 14.647384643554688, 15.973679065704346, 17.291126012802124, 18.61319661140442, 19.92956829071045, 21.258206129074097, 22.57543420791626, 23.833186626434326, 25.13597273826599, 26.40947699546814, 27.668770790100098, 28.93672204017639, 30.202754497528076, 31.528791427612305, 32.851622104644775, 34.17412090301514, 35.497366189956665, 36.821112394332886, 38.146554946899414, 39.472700357437134, 40.79137587547302, 42.104170083999634, 43.41874957084656, 44.786866664886475, 46.1008095741272, 47.397380352020264, 48.702818155288696, 50.00516104698181, 51.30347204208374, 52.4342520236969, 53.56792950630188, 54.701032400131226, 55.83570170402527, 56.96623349189758, 58.09909248352051, 59.232168674468994, 60.37393116950989, 61.505433082580566, 62.638630867004395, 63.78113532066345, 64.91675043106079, 66.05036807060242, 67.18156456947327, 68.3127646446228, 69.44355344772339, 70.57434630393982, 71.70961236953735, 72.84779810905457, 73.98166275024414, 75.11250638961792, 76.40184426307678, 77.69412469863892, 78.97775268554688, 80.28442049026489, 81.58721876144409, 82.8927891254425, 84.1874418258667, 85.48524975776672, 86.7841866016388, 88.08459854125977, 89.41636681556702, 90.71981430053711, 92.01081562042236, 93.30609083175659, 94.59989786148071, 95.90216565132141, 97.21155285835266, 98.5072283744812, 99.80181407928467, 101.14076900482178, 102.43490433692932, 103.74066758155823, 105.05892825126648, 106.36528658866882, 107.67023968696594, 108.96433448791504, 110.2587947845459, 111.54630708694458, 112.67903470993042, 113.81421208381653, 114.94395089149475, 116.07601881027222, 117.2090539932251, 118.33640909194946, 119.46630764007568, 120.59809756278992, 121.72826910018921, 122.8636429309845, 123.99015951156616, 125.11376905441284, 126.26054906845093, 127.38557076454163, 128.5119662284851, 129.63653111457825, 130.78834581375122, 131.9178283214569, 133.20529508590698, 134.48638367652893, 135.60767364501953, 136.73786568641663, 137.86733627319336, 139.0333697795868, 140.161297082901, 141.29520845413208, 142.42220401763916, 143.72467136383057, 145.02896857261658, 146.32993698120117, 147.63269805908203, 148.93000411987305, 150.23445343971252, 151.53219652175903, 152.8256812095642, 154.12369585037231, 155.42259335517883, 156.72547316551208, 158.02842211723328, 159.33097577095032, 160.63313388824463, 161.94130182266235, 163.24094557762146, 164.5465371608734, 165.84789395332336, 167.1503758430481, 168.46542382240295, 169.75829195976257, 171.06279373168945, 172.36431574821472, 173.67534685134888, 174.98763060569763, 176.30824947357178, 177.62406420707703, 178.92827916145325, 180.25503778457642, 181.57226729393005, 182.8747799396515, 184.18368005752563, 185.49934673309326, 186.8130145072937, 188.10880184173584, 189.41483879089355, 190.73135614395142, 192.0326292514801, 193.3449068069458, 194.66023445129395, 195.96618843078613, 197.29121923446655, 198.59328746795654, 199.91705679893494, 201.23747301101685, 202.54845118522644, 203.86655044555664, 205.1738612651825, 206.49444150924683, 207.80069255828857, 209.09605884552002, 210.42354202270508, 211.72896146774292, 213.04459595680237, 214.35717368125916, 215.66204690933228, 216.96109342575073, 218.27440977096558, 219.60039281845093, 220.90322041511536, 222.0414752960205, 223.18993282318115, 224.32430458068848, 225.45921397209167, 226.59602069854736, 227.7261667251587, 228.87079977989197, 230.0230906009674, 231.1632857322693, 232.2988579273224, 233.4388620853424, 234.58494806289673, 235.72284841537476, 236.85320115089417, 237.9920518398285, 239.13616228103638, 240.27797031402588, 241.41044282913208, 242.5502734184265, 243.69017958641052, 244.8219838142395, 245.94961857795715, 247.0997667312622, 248.3892204761505, 249.69473004341125, 250.9899127483368, 252.28822469711304, 253.59399819374084, 254.89498448371887, 256.19083619117737, 257.33192920684814, 258.4654150009155, 259.60165333747864, 260.7373695373535, 261.87186098098755, 263.00863909721375, 264.14071321487427, 265.2751851081848, 266.4121518135071, 267.54679799079895, 268.67920303344727, 269.81427359580994, 270.94811511039734, 272.0872781276703, 273.2264106273651, 274.36075043678284, 275.49741315841675, 276.6380081176758, 277.77421259880066, 278.90766072273254, 280.04980754852295, 281.1882691383362, 282.3223898410797, 283.45340633392334, 284.587917804718, 285.7181398868561, 286.8506135940552, 287.980295419693, 289.11872243881226, 290.2596778869629, 291.3973169326782, 292.5236828327179, 293.6481120586395, 294.76851892471313, 295.89084029197693, 297.0169789791107, 298.1330554485321, 299.2570834159851, 300.3743143081665, 301.49648427963257, 302.61800622940063, 303.7306156158447, 304.8444654941559, 305.9600462913513, 307.08204650878906, 308.20448994636536, 309.3265264034271, 310.4427251815796, 311.56107568740845, 312.6801509857178, 313.7992043495178, 314.92015957832336, 316.0358533859253, 317.15184450149536, 318.2709276676178, 319.3886013031006, 320.50452709198, 321.6209990978241, 322.7368485927582, 323.8539671897888, 324.9701852798462, 326.08923149108887, 327.36311197280884, 328.6432797908783, 329.9097955226898, 331.181125164032, 332.4512197971344, 333.7268989086151, 335.0079264640808, 336.29453778266907, 337.5779049396515, 338.85915446281433, 340.1420478820801, 341.42553782463074, 342.7071120738983, 343.9915544986725, 345.27061128616333, 346.3851537704468, 347.497695684433, 348.6100718975067, 349.72536611557007, 350.8357219696045, 351.9496409893036, 353.06003046035767, 354.16905546188354, 355.27776551246643, 356.38718581199646, 357.49633622169495, 358.6079351902008, 359.72220635414124, 360.8317084312439, 361.94117736816406, 363.04803371429443, 364.1602509021759, 365.26891803741455, 367.4866783618927]
[10.0325, 10.0825, 10.165, 10.18, 10.3075, 10.35, 10.465, 10.5225, 10.5075, 10.45, 10.48, 10.405, 10.535, 10.5925, 10.6725, 10.6775, 10.5975, 10.61, 10.6175, 10.6575, 10.8075, 10.8425, 10.9175, 10.95, 10.965, 11.165, 11.5, 11.56, 11.475, 11.3475, 11.32, 11.3325, 11.5375, 11.66, 11.725, 11.765, 11.745, 11.98, 12.085, 12.0275, 12.19, 12.2425, 12.39, 12.535, 12.53, 12.5675, 12.68, 12.9175, 13.2025, 13.4925, 13.7825, 14.165, 14.3075, 14.4475, 14.525, 14.615, 14.7025, 15.03, 15.4525, 15.2575, 15.3575, 15.64, 15.92, 16.105, 16.035, 16.2325, 16.32, 16.1725, 16.23, 16.3275, 16.3175, 16.44, 16.5525, 16.58, 16.6, 16.6025, 16.8625, 17.08, 16.8375, 16.83, 16.975, 16.8775, 16.8025, 17.005, 17.045, 17.1975, 17.195, 17.2525, 17.6425, 17.6725, 17.91, 18.025, 18.065, 17.96, 17.9275, 17.8475, 17.745, 17.485, 17.3775, 17.1575, 16.8975, 16.61, 16.4975, 16.2925, 16.6275, 16.915, 17.355, 17.55, 17.74, 17.8125, 17.7, 18.2225, 18.4425, 18.6575, 18.8025, 19.065, 19.075, 19.16, 19.315, 19.45, 19.67, 19.8225, 19.975, 20.1675, 20.43, 20.475, 20.5075, 20.5675, 20.67, 20.5025, 20.59, 20.43, 20.5075, 20.3925, 20.3675, 20.595, 20.5925, 20.7975, 20.8275, 20.7725, 20.815, 20.775, 20.74, 21.075, 21.04, 21.1325, 21.07, 21.1975, 21.09, 21.1, 21.2175, 21.56, 21.4275, 21.3575, 21.32, 21.22, 21.235, 21.215, 21.2025, 21.2475, 21.3425, 21.2275, 21.2525, 21.21, 21.1625, 21.05, 21.1475, 21.3775, 21.32, 21.42, 21.56, 21.5525, 21.62, 21.81, 21.935, 22.1325, 22.0975, 21.965, 21.8775, 21.805, 21.8075, 21.835, 21.9175, 21.7575, 21.93, 22.0575, 22.01, 22.0, 21.9025, 21.885, 21.8525, 21.925, 21.815, 21.835, 21.9375, 21.9325, 21.915, 21.855, 21.8725, 21.75, 21.7225, 21.705, 21.68, 21.6025, 21.5775, 21.52, 21.6375, 21.605, 21.7625, 21.8675, 22.0525, 22.25, 22.2525, 22.1375, 21.34, 17.71, 14.1925, 12.3625, 11.1475, 11.1475, 11.1475, 11.1475, 11.1475, 11.1475, 11.1475, 11.1475, 10.57, 10.57, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.431, Test loss: 1.926, Test accuracy: 57.06
Final Round, Global train loss: 0.431, Global test loss: 2.361, Global test accuracy: 41.89
Average accuracy final 10 rounds: 56.869499999999995 

Average global accuracy final 10 rounds: 41.857 

3766.8201570510864
[1.4019052982330322, 2.8038105964660645, 3.970139980316162, 5.13646936416626, 6.30764365196228, 7.478817939758301, 8.646282434463501, 9.813746929168701, 10.977843999862671, 12.14194107055664, 13.309732675552368, 14.477524280548096, 15.646687030792236, 16.815849781036377, 17.98593235015869, 19.156014919281006, 20.327718496322632, 21.499422073364258, 22.673827409744263, 23.848232746124268, 25.020301342010498, 26.19236993789673, 27.35956859588623, 28.526767253875732, 29.69344925880432, 30.86013126373291, 32.03506779670715, 33.2100043296814, 34.39239311218262, 35.57478189468384, 36.75466465950012, 37.934547424316406, 39.1150963306427, 40.295645236968994, 41.474524974823, 42.653404712677, 43.832003355026245, 45.01060199737549, 46.19065260887146, 47.37070322036743, 48.54936456680298, 49.728025913238525, 50.909175872802734, 52.09032583236694, 53.27357053756714, 54.456815242767334, 55.63516926765442, 56.813523292541504, 57.996837854385376, 59.18015241622925, 60.36315941810608, 61.54616641998291, 62.728527307510376, 63.91088819503784, 65.0885763168335, 66.26626443862915, 67.44935750961304, 68.63245058059692, 69.81478428840637, 70.99711799621582, 72.18615746498108, 73.37519693374634, 74.56477189064026, 75.75434684753418, 76.93809342384338, 78.12184000015259, 79.29902243614197, 80.47620487213135, 81.65851926803589, 82.84083366394043, 84.02674007415771, 85.212646484375, 86.39305996894836, 87.57347345352173, 88.75289225578308, 89.93231105804443, 91.11184811592102, 92.29138517379761, 93.46289300918579, 94.63440084457397, 95.81652975082397, 96.99865865707397, 98.1781256198883, 99.35759258270264, 100.53804993629456, 101.71850728988647, 102.90224385261536, 104.08598041534424, 105.26310086250305, 106.44022130966187, 107.59697580337524, 108.75373029708862, 109.91065526008606, 111.0675802230835, 112.2179024219513, 113.36822462081909, 114.53412508964539, 115.70002555847168, 116.8684630393982, 118.0369005203247, 119.03935766220093, 120.04181480407715, 121.04736065864563, 122.05290651321411, 123.05761694908142, 124.06232738494873, 125.06459355354309, 126.06685972213745, 127.0677056312561, 128.06855154037476, 129.07013034820557, 130.07170915603638, 131.07140851020813, 132.07110786437988, 133.06854391098022, 134.06597995758057, 135.23131227493286, 136.39664459228516, 137.56314873695374, 138.72965288162231, 139.8931884765625, 141.05672407150269, 142.22286987304688, 143.38901567459106, 144.48677372932434, 145.58453178405762, 146.74049592018127, 147.89646005630493, 149.06929421424866, 150.24212837219238, 151.40902185440063, 152.5759153366089, 153.74713730812073, 154.91835927963257, 156.09872221946716, 157.27908515930176, 158.44917511940002, 159.6192650794983, 160.78360843658447, 161.94795179367065, 163.11377668380737, 164.2796015739441, 165.4502673149109, 166.62093305587769, 167.79247403144836, 168.96401500701904, 170.14279055595398, 171.32156610488892, 172.49180722236633, 173.66204833984375, 174.8366732597351, 176.01129817962646, 177.1877794265747, 178.36426067352295, 179.54783940315247, 180.73141813278198, 181.89787483215332, 183.06433153152466, 184.23714995384216, 185.40996837615967, 186.58384704589844, 187.7577257156372, 188.92227506637573, 190.08682441711426, 191.2573311328888, 192.42783784866333, 193.59588384628296, 194.7639298439026, 195.9305477142334, 197.0971655845642, 198.2670440673828, 199.43692255020142, 200.6020872592926, 201.7672519683838, 202.7765760421753, 203.7859001159668, 204.79416394233704, 205.80242776870728, 206.81278681755066, 207.82314586639404, 208.8311686515808, 209.83919143676758, 210.84681296348572, 211.85443449020386, 212.86363887786865, 213.87284326553345, 214.88204979896545, 215.89125633239746, 216.898832321167, 217.90640830993652, 218.91602730751038, 219.92564630508423, 220.9358446598053, 221.94604301452637, 222.95564818382263, 223.9652533531189, 224.97288250923157, 225.98051166534424, 226.98892974853516, 227.99734783172607, 230.01526045799255, 232.03317308425903]
[22.8175, 22.8175, 25.07, 25.07, 28.3, 28.3, 30.4525, 30.4525, 31.66, 31.66, 34.22, 34.22, 36.0875, 36.0875, 37.265, 37.265, 38.585, 38.585, 39.6875, 39.6875, 40.89, 40.89, 41.33, 41.33, 41.885, 41.885, 42.9975, 42.9975, 44.1325, 44.1325, 44.8475, 44.8475, 45.2475, 45.2475, 46.175, 46.175, 46.71, 46.71, 47.01, 47.01, 47.305, 47.305, 48.345, 48.345, 49.1425, 49.1425, 49.3925, 49.3925, 49.445, 49.445, 49.545, 49.545, 49.72, 49.72, 50.3825, 50.3825, 50.5825, 50.5825, 50.705, 50.705, 50.99, 50.99, 50.9975, 50.9975, 51.7625, 51.7625, 51.625, 51.625, 51.895, 51.895, 51.955, 51.955, 52.0, 52.0, 52.1975, 52.1975, 52.57, 52.57, 53.215, 53.215, 53.1675, 53.1675, 53.08, 53.08, 53.135, 53.135, 53.2625, 53.2625, 53.165, 53.165, 53.455, 53.455, 53.645, 53.645, 53.695, 53.695, 53.775, 53.775, 53.9875, 53.9875, 54.42, 54.42, 54.795, 54.795, 55.05, 55.05, 54.59, 54.59, 54.405, 54.405, 54.5525, 54.5525, 54.16, 54.16, 54.63, 54.63, 54.7375, 54.7375, 54.695, 54.695, 54.6525, 54.6525, 54.625, 54.625, 54.5675, 54.5675, 54.76, 54.76, 55.0775, 55.0775, 55.4275, 55.4275, 55.475, 55.475, 55.355, 55.355, 55.47, 55.47, 55.4025, 55.4025, 55.2825, 55.2825, 55.3925, 55.3925, 55.3825, 55.3825, 55.62, 55.62, 55.86, 55.86, 55.795, 55.795, 55.8225, 55.8225, 55.8725, 55.8725, 55.695, 55.695, 56.0525, 56.0525, 56.59, 56.59, 56.42, 56.42, 56.5625, 56.5625, 56.4675, 56.4675, 56.3925, 56.3925, 56.415, 56.415, 56.675, 56.675, 56.545, 56.545, 56.7375, 56.7375, 56.6825, 56.6825, 56.7275, 56.7275, 56.96, 56.96, 56.835, 56.835, 56.6775, 56.6775, 56.9, 56.9, 56.75, 56.75, 57.055, 57.055, 56.96, 56.96, 56.9075, 56.9075, 56.9225, 56.9225, 57.0575, 57.0575]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.516, Test loss: 1.328, Test accuracy: 61.53
Average accuracy final 10 rounds: 61.2435 

2751.312468767166
[1.3693809509277344, 2.7387619018554688, 3.861083745956421, 4.983405590057373, 6.103696584701538, 7.223987579345703, 8.346612453460693, 9.469237327575684, 10.59262752532959, 11.716017723083496, 12.835236072540283, 13.95445442199707, 15.065553426742554, 16.176652431488037, 17.286686182022095, 18.396719932556152, 19.520000457763672, 20.64328098297119, 21.770265340805054, 22.897249698638916, 24.02309823036194, 25.14894676208496, 26.276208639144897, 27.403470516204834, 28.52935028076172, 29.655230045318604, 30.784271001815796, 31.91331195831299, 33.03894376754761, 34.16457557678223, 35.2861442565918, 36.40771293640137, 37.53316831588745, 38.658623695373535, 39.78567934036255, 40.91273498535156, 42.039294719696045, 43.16585445404053, 44.289292097091675, 45.41272974014282, 46.54316568374634, 47.67360162734985, 48.80189847946167, 49.930195331573486, 51.053510904312134, 52.17682647705078, 53.303152084350586, 54.42947769165039, 55.54971218109131, 56.66994667053223, 57.78954839706421, 58.90915012359619, 60.03199553489685, 61.15484094619751, 62.279417753219604, 63.4039945602417, 64.52370953559875, 65.64342451095581, 66.77133178710938, 67.89923906326294, 69.02299332618713, 70.14674758911133, 71.27809882164001, 72.4094500541687, 73.53190851211548, 74.65436697006226, 75.78024888038635, 76.90613079071045, 78.06695890426636, 79.22778701782227, 80.35265016555786, 81.47751331329346, 82.60080623626709, 83.72409915924072, 84.84626126289368, 85.96842336654663, 87.09343910217285, 88.21845483779907, 89.341468334198, 90.46448183059692, 91.59832501411438, 92.73216819763184, 93.85410737991333, 94.97604656219482, 96.10624742507935, 97.23644828796387, 98.35179710388184, 99.4671459197998, 100.58773708343506, 101.70832824707031, 102.78697562217712, 103.86562299728394, 104.99563074111938, 106.12563848495483, 107.24572777748108, 108.36581707000732, 109.50073075294495, 110.63564443588257, 111.7625641822815, 112.88948392868042, 114.0185604095459, 115.14763689041138, 116.27587103843689, 117.4041051864624, 118.52773427963257, 119.65136337280273, 120.78077459335327, 121.91018581390381, 123.03982210159302, 124.16945838928223, 125.30128049850464, 126.43310260772705, 127.56044435501099, 128.68778610229492, 129.81283855438232, 130.93789100646973, 132.0658824443817, 133.1938738822937, 134.3177149295807, 135.44155597686768, 136.56636810302734, 137.691180229187, 138.8152985572815, 139.93941688537598, 141.07053065299988, 142.20164442062378, 143.32813334465027, 144.45462226867676, 145.58382940292358, 146.7130365371704, 147.8377640247345, 148.96249151229858, 150.0923445224762, 151.2221975326538, 152.35643553733826, 153.4906735420227, 154.61616849899292, 155.74166345596313, 156.86325335502625, 157.98484325408936, 159.11335468292236, 160.24186611175537, 161.37293124198914, 162.5039963722229, 163.6382372379303, 164.7724781036377, 165.8995463848114, 167.0266146659851, 168.15380930900574, 169.28100395202637, 170.4119963645935, 171.54298877716064, 172.67660689353943, 173.8102250099182, 174.89442896842957, 175.97863292694092, 177.06748032569885, 178.1563277244568, 179.2337532043457, 180.31117868423462, 181.44199538230896, 182.5728120803833, 183.69657969474792, 184.82034730911255, 185.94413805007935, 187.06792879104614, 188.19066739082336, 189.3134059906006, 190.43685483932495, 191.56030368804932, 192.69890213012695, 193.8375005722046, 194.9604151248932, 196.0833296775818, 197.2149519920349, 198.34657430648804, 199.47162127494812, 200.5966682434082, 201.7151234149933, 202.83357858657837, 203.96152067184448, 205.0894627571106, 206.21288967132568, 207.33631658554077, 208.46200561523438, 209.58769464492798, 210.71477389335632, 211.84185314178467, 212.97240042686462, 214.10294771194458, 215.22785639762878, 216.352765083313, 217.47605276107788, 218.59934043884277, 219.72442293167114, 220.8495054244995, 221.97963500022888, 223.10976457595825, 224.23639726638794, 225.36302995681763, 227.32978463172913, 229.29653930664062]
[17.16, 17.16, 21.17, 21.17, 23.8825, 23.8825, 25.635, 25.635, 28.4025, 28.4025, 30.3625, 30.3625, 32.895, 32.895, 33.9825, 33.9825, 35.5775, 35.5775, 37.0825, 37.0825, 38.7, 38.7, 40.1775, 40.1775, 41.68, 41.68, 43.555, 43.555, 44.0125, 44.0125, 44.945, 44.945, 45.7175, 45.7175, 46.2, 46.2, 46.73, 46.73, 47.005, 47.005, 48.09, 48.09, 48.5775, 48.5775, 49.505, 49.505, 49.5675, 49.5675, 50.6025, 50.6025, 51.4325, 51.4325, 51.7675, 51.7675, 51.675, 51.675, 51.9575, 51.9575, 51.875, 51.875, 52.8825, 52.8825, 52.4575, 52.4575, 53.51, 53.51, 53.36, 53.36, 53.6275, 53.6275, 54.505, 54.505, 54.9975, 54.9975, 55.4175, 55.4175, 55.29, 55.29, 55.7975, 55.7975, 55.0625, 55.0625, 56.0425, 56.0425, 55.7925, 55.7925, 56.5375, 56.5375, 56.6775, 56.6775, 56.5375, 56.5375, 56.87, 56.87, 57.42, 57.42, 58.105, 58.105, 58.12, 58.12, 58.33, 58.33, 58.3725, 58.3725, 58.4675, 58.4675, 58.595, 58.595, 58.545, 58.545, 58.37, 58.37, 58.56, 58.56, 58.57, 58.57, 58.305, 58.305, 58.8525, 58.8525, 59.06, 59.06, 58.835, 58.835, 58.4075, 58.4075, 59.0175, 59.0175, 58.92, 58.92, 59.5825, 59.5825, 59.08, 59.08, 59.89, 59.89, 60.24, 60.24, 60.595, 60.595, 59.8375, 59.8375, 60.145, 60.145, 60.19, 60.19, 59.78, 59.78, 60.295, 60.295, 60.265, 60.265, 60.0125, 60.0125, 60.5975, 60.5975, 60.6475, 60.6475, 60.655, 60.655, 60.9, 60.9, 60.9275, 60.9275, 60.7675, 60.7675, 60.86, 60.86, 61.08, 61.08, 61.165, 61.165, 60.9325, 60.9325, 60.9175, 60.9175, 61.1025, 61.1025, 60.875, 60.875, 61.1725, 61.1725, 61.22, 61.22, 60.8925, 60.8925, 61.2575, 61.2575, 61.205, 61.205, 60.86, 60.86, 61.4725, 61.4725, 61.7875, 61.7875, 61.2, 61.2, 61.3675, 61.3675, 61.53, 61.53]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.576, Test loss: 0.985, Test accuracy: 67.58
Average accuracy final 10 rounds: 66.87
2890.6669466495514
[1.6954216957092285, 3.390843391418457, 4.748420476913452, 6.105997562408447, 7.449983596801758, 8.793969631195068, 10.134435653686523, 11.474901676177979, 12.767045974731445, 14.059190273284912, 15.350731134414673, 16.642271995544434, 17.93394947052002, 19.225626945495605, 20.5541250705719, 21.882623195648193, 23.19308114051819, 24.503539085388184, 25.814303636550903, 27.125068187713623, 28.427993059158325, 29.730917930603027, 31.0334050655365, 32.33589220046997, 33.67974352836609, 35.02359485626221, 36.402496337890625, 37.78139781951904, 39.11125588417053, 40.44111394882202, 41.771294593811035, 43.10147523880005, 44.43796896934509, 45.77446269989014, 47.120556354522705, 48.46665000915527, 49.821486949920654, 51.176323890686035, 52.51737380027771, 53.858423709869385, 55.19658088684082, 56.534738063812256, 57.87108087539673, 59.2074236869812, 60.549336671829224, 61.891249656677246, 63.23366641998291, 64.57608318328857, 65.91502165794373, 67.25396013259888, 68.59502696990967, 69.93609380722046, 71.2757339477539, 72.61537408828735, 73.95817756652832, 75.30098104476929, 76.63903665542603, 77.97709226608276, 79.32956671714783, 80.68204116821289, 82.03960657119751, 83.39717197418213, 84.74254631996155, 86.08792066574097, 87.4309229850769, 88.77392530441284, 90.1245949268341, 91.47526454925537, 92.82697749137878, 94.1786904335022, 95.46858835220337, 96.75848627090454, 98.08613181114197, 99.4137773513794, 100.71320247650146, 102.01262760162354, 103.30858492851257, 104.60454225540161, 105.95408153533936, 107.3036208152771, 108.5918915271759, 109.8801622390747, 111.17917728424072, 112.47819232940674, 113.76779842376709, 115.05740451812744, 116.39177370071411, 117.72614288330078, 119.01223301887512, 120.29832315444946, 121.65419483184814, 123.01006650924683, 124.36092829704285, 125.71179008483887, 127.06172895431519, 128.4116678237915, 129.75673294067383, 131.10179805755615, 132.45914888381958, 133.816499710083, 135.1803777217865, 136.54425573349, 137.89700841903687, 139.24976110458374, 140.6140012741089, 141.97824144363403, 143.33124136924744, 144.68424129486084, 146.03119611740112, 147.3781509399414, 148.72270441055298, 150.06725788116455, 151.4061577320099, 152.74505758285522, 154.07911467552185, 155.41317176818848, 156.7548315525055, 158.0964913368225, 159.43363070487976, 160.770770072937, 162.11395001411438, 163.45712995529175, 164.79125452041626, 166.12537908554077, 167.47193908691406, 168.81849908828735, 170.16558361053467, 171.51266813278198, 172.85867953300476, 174.20469093322754, 175.57440185546875, 176.94411277770996, 178.30219721794128, 179.6602816581726, 181.0375702381134, 182.4148588180542, 183.76139211654663, 185.10792541503906, 186.47495555877686, 187.84198570251465, 189.21151423454285, 190.58104276657104, 191.9572069644928, 193.33337116241455, 194.6958041191101, 196.05823707580566, 197.41468143463135, 198.77112579345703, 200.12734484672546, 201.4835638999939, 202.84864377975464, 204.21372365951538, 205.58043837547302, 206.94715309143066, 208.2986342906952, 209.65011548995972, 211.00434684753418, 212.35857820510864, 213.71430492401123, 215.07003164291382, 216.43990063667297, 217.80976963043213, 219.16043376922607, 220.51109790802002, 221.8587236404419, 223.20634937286377, 224.56656527519226, 225.92678117752075, 227.28093957901, 228.63509798049927, 229.99600958824158, 231.3569211959839, 232.71940398216248, 234.08188676834106, 235.44072461128235, 236.79956245422363, 238.1680850982666, 239.53660774230957, 240.90608072280884, 242.2755537033081, 243.63560271263123, 244.99565172195435, 246.36515498161316, 247.73465824127197, 249.10239434242249, 250.470130443573, 251.8318099975586, 253.1934895515442, 254.5462749004364, 255.8990602493286, 257.25253677368164, 258.60601329803467, 259.9577534198761, 261.30949354171753, 262.6767683029175, 264.04404306411743, 265.3985493183136, 266.75305557250977, 268.1073715686798, 269.46168756484985, 271.5380177497864, 273.6143479347229]
[14.8925, 14.8925, 21.7875, 21.7875, 23.7825, 23.7825, 25.8575, 25.8575, 28.7125, 28.7125, 30.4825, 30.4825, 32.86, 32.86, 34.6, 34.6, 36.2675, 36.2675, 37.4825, 37.4825, 38.9425, 38.9425, 40.5275, 40.5275, 42.535, 42.535, 43.275, 43.275, 44.4775, 44.4775, 45.0275, 45.0275, 47.0275, 47.0275, 47.815, 47.815, 49.0675, 49.0675, 48.9025, 48.9025, 49.905, 49.905, 50.4475, 50.4475, 51.2875, 51.2875, 52.2025, 52.2025, 52.435, 52.435, 53.1925, 53.1925, 53.42, 53.42, 54.405, 54.405, 54.33, 54.33, 55.0125, 55.0125, 55.56, 55.56, 55.5975, 55.5975, 56.3125, 56.3125, 56.4875, 56.4875, 56.225, 56.225, 57.195, 57.195, 57.205, 57.205, 57.575, 57.575, 58.0975, 58.0975, 58.3425, 58.3425, 58.9925, 58.9925, 59.32, 59.32, 59.3925, 59.3925, 59.8075, 59.8075, 60.035, 60.035, 59.92, 59.92, 60.8125, 60.8125, 60.6025, 60.6025, 61.535, 61.535, 61.075, 61.075, 61.185, 61.185, 61.095, 61.095, 61.9325, 61.9325, 62.3225, 62.3225, 63.015, 63.015, 62.5975, 62.5975, 62.41, 62.41, 62.7925, 62.7925, 63.4575, 63.4575, 63.85, 63.85, 63.555, 63.555, 63.66, 63.66, 63.745, 63.745, 63.49, 63.49, 63.5475, 63.5475, 64.145, 64.145, 64.825, 64.825, 64.5425, 64.5425, 64.7, 64.7, 64.7625, 64.7625, 65.17, 65.17, 65.37, 65.37, 65.1575, 65.1575, 65.4375, 65.4375, 65.655, 65.655, 66.0025, 66.0025, 65.7675, 65.7675, 65.8275, 65.8275, 66.2125, 66.2125, 66.32, 66.32, 66.12, 66.12, 66.5175, 66.5175, 66.605, 66.605, 66.0025, 66.0025, 66.1825, 66.1825, 66.31, 66.31, 65.8325, 65.8325, 66.34, 66.34, 66.2225, 66.2225, 65.9575, 65.9575, 66.45, 66.45, 67.085, 67.085, 66.7875, 66.7875, 66.5425, 66.5425, 66.275, 66.275, 66.955, 66.955, 67.425, 67.425, 67.12, 67.12, 66.9325, 66.9325, 67.1275, 67.1275, 67.58, 67.58]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.467, Test loss: 2.128, Test accuracy: 42.92
Average accuracy final 10 rounds: 41.994249999999994
2747.1748592853546
[3.118896961212158, 6.047323703765869, 8.962646007537842, 11.93749737739563, 14.908758640289307, 17.888784885406494, 20.8774516582489, 23.531768321990967, 26.203502655029297, 29.174450874328613, 32.154229402542114, 35.13469171524048, 38.125659465789795, 41.09543013572693, 44.066123247146606, 47.04986810684204, 50.03478813171387, 53.012311697006226, 55.966267585754395, 58.929336071014404, 61.895408153533936, 64.86822009086609, 67.83535861968994, 70.80949449539185, 73.7786602973938, 76.73951029777527, 79.71956372261047, 82.69202184677124, 85.66664028167725, 88.6363251209259, 91.60839056968689, 94.57943844795227, 97.56076407432556, 100.53503489494324, 103.50625443458557, 106.47484421730042, 109.30116295814514, 111.9809558391571, 114.66895723342896, 117.36960029602051, 120.05492949485779, 122.734783411026, 125.42882657051086, 128.16635870933533, 130.88910841941833, 133.6281726360321, 136.35394430160522, 139.0862283706665, 141.80748963356018, 144.56067848205566, 147.29523181915283, 150.02285766601562, 152.75558805465698, 155.48269963264465, 158.21542525291443, 160.9481828212738, 163.67360711097717, 166.41310167312622, 169.14411330223083, 171.87492609024048, 174.60412669181824, 177.34423518180847, 180.07791018486023, 182.80571007728577, 185.54059386253357, 188.27841687202454, 191.0124225616455, 193.74718475341797, 196.47612476348877, 199.20952987670898, 201.93980741500854, 204.6764621734619, 207.40421652793884, 210.12612915039062, 212.86228966712952, 215.55484175682068, 218.29606771469116, 220.99696946144104, 223.73098945617676, 226.46436214447021, 229.1920735836029, 231.91092443466187, 234.64681673049927, 237.37052273750305, 240.09626626968384, 242.82514262199402, 245.55311608314514, 248.28569221496582, 251.01371574401855, 253.74834084510803, 256.48142671585083, 259.2078380584717, 261.93917536735535, 264.6800434589386, 267.40667057037354, 270.1291706562042, 272.86636900901794, 275.5981066226959, 278.325884103775, 281.06196784973145, 283.798953294754]
[20.9425, 24.1875, 26.1425, 28.37, 30.56, 32.0025, 32.625, 34.3075, 33.715, 35.16, 34.65, 34.965, 36.2475, 35.9725, 36.455, 36.94, 37.7575, 38.01, 37.6225, 38.66, 37.975, 39.07, 37.8225, 38.4375, 39.0525, 38.9475, 39.5, 39.86, 39.6025, 40.4275, 40.3775, 40.805, 39.715, 39.47, 40.0575, 40.0725, 40.0175, 41.4425, 39.9725, 39.7825, 40.4325, 40.5225, 40.45, 42.0575, 41.2475, 40.1725, 40.9575, 40.7875, 40.77, 41.0675, 41.5625, 41.7225, 41.165, 40.4625, 41.535, 41.095, 41.205, 41.2175, 41.795, 41.2675, 41.9375, 41.035, 42.4075, 41.7775, 42.765, 41.6575, 41.23, 41.1125, 41.79, 40.945, 42.91, 42.3425, 42.8175, 41.8825, 41.2075, 42.05, 42.135, 42.9875, 40.9275, 41.75, 42.49, 41.485, 41.635, 42.44, 40.4675, 41.8475, 41.245, 41.8575, 41.31, 41.265, 42.03, 42.5825, 41.5275, 41.615, 42.7475, 42.905, 41.5, 41.6475, 41.175, 42.2125, 42.925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

8265.518300294876
[1.520085096359253, 2.836699962615967, 4.151113510131836, 5.4602086544036865, 6.794298410415649, 8.12129521369934, 9.458529472351074, 10.788928747177124, 12.119317293167114, 13.445854663848877, 14.778692483901978, 16.111433744430542, 17.442452430725098, 18.770874500274658, 20.092725038528442, 21.25323462486267, 22.408291816711426, 23.56707787513733, 24.722899198532104, 25.877843379974365, 27.034082889556885, 28.190402269363403, 29.34356927871704, 30.49909210205078, 31.651632070541382, 32.80010390281677, 33.947200775146484, 35.09439468383789, 36.23727989196777, 37.387213945388794, 38.52877593040466, 39.67848181724548, 40.81857752799988, 41.9735062122345, 43.11172080039978, 44.26024270057678, 45.39847493171692, 46.54915928840637, 47.69601559638977, 48.84306716918945, 49.99011588096619, 51.140387296676636, 52.288633823394775, 53.43347096443176, 54.5828001499176, 55.72958302497864, 56.87835717201233, 58.01650333404541, 59.16241669654846, 60.308658838272095, 61.45161962509155, 62.5966215133667, 63.743969202041626, 64.88819193840027, 66.03173780441284, 67.17373132705688, 68.31118369102478, 69.45331072807312, 70.5999686717987, 71.7427237033844, 72.89055609703064, 74.03457450866699, 75.17633104324341, 76.31767535209656, 77.46242427825928, 78.59548807144165, 79.7369019985199, 80.87300133705139, 82.01254606246948, 83.14551568031311, 84.2871561050415, 85.4258074760437, 86.56791996955872, 87.70518612861633, 88.84873223304749, 89.9964029788971, 91.14238667488098, 92.29031682014465, 93.43446445465088, 94.58063125610352, 95.72549271583557, 96.86719822883606, 98.01953840255737, 99.15744733810425, 100.3039071559906, 101.44952917098999, 102.5949501991272, 103.73904085159302, 104.88599610328674, 106.03110933303833, 107.17935538291931, 108.32423543930054, 109.46807670593262, 110.6050055027008, 111.7485740184784, 112.89556813240051, 114.03900074958801, 115.18397879600525, 116.3279938697815, 117.4718930721283, 118.61535835266113, 119.76056170463562, 120.89391899108887, 122.02831935882568, 123.16240906715393, 124.29885172843933, 125.42721962928772, 126.56416893005371, 127.69931483268738, 128.83517146110535, 129.97009778022766, 131.10356426239014, 132.23671674728394, 133.3699550628662, 134.502836227417, 135.64107942581177, 136.77890348434448, 137.91526794433594, 139.04827117919922, 140.1901412010193, 141.33179187774658, 142.47703099250793, 143.6126959323883, 144.75742292404175, 145.90082955360413, 147.04804849624634, 148.1923599243164, 149.33547377586365, 150.48764944076538, 151.62607216835022, 152.76893043518066, 153.9082851409912, 155.05346131324768, 156.18362975120544, 157.33107829093933, 158.46882104873657, 159.6178216934204, 160.75239539146423, 161.89469027519226, 163.02894186973572, 164.16430616378784, 165.29629755020142, 166.43356466293335, 167.5638289451599, 168.69814085960388, 169.83834791183472, 170.9846224784851, 172.12193512916565, 173.2759828567505, 174.41755747795105, 175.55382990837097, 176.70390915870667, 177.84854006767273, 178.98851442337036, 180.12522554397583, 181.26704716682434, 182.40160250663757, 183.5415279865265, 184.6855034828186, 185.8218696117401, 186.9513533115387, 188.0744149684906, 189.20764541625977, 190.33551406860352, 191.4638922214508, 192.59555101394653, 193.7223436832428, 194.85236501693726, 195.98293590545654, 197.11373257637024, 198.24572014808655, 199.36749958992004, 200.49504733085632, 201.62616610527039, 202.7482831478119, 203.88051056861877, 205.01687693595886, 206.13914370536804, 207.2724244594574, 208.4029357433319, 209.56977152824402, 210.7006676197052, 211.82893800735474, 212.9551751613617, 214.07719016075134, 215.1989471912384, 216.3205029964447, 217.4501039981842, 218.57439517974854, 219.69768524169922, 220.8245656490326, 221.955717086792, 223.083988904953, 224.2178931236267, 225.34697222709656, 226.47070908546448, 227.5979516506195, 228.72780108451843, 229.85911178588867, 230.99283599853516, 232.11291766166687, 233.23627281188965, 234.35878992080688, 235.47861003875732, 236.61183285713196, 237.74533772468567, 238.8683750629425, 240.00202107429504, 241.12902927398682, 242.2551975250244, 243.38706970214844, 244.5245397090912, 245.65569496154785, 246.78018355369568, 247.91126108169556, 249.03689765930176, 250.16856956481934, 251.30135321617126, 252.43363428115845, 253.55950212478638, 254.69138026237488, 255.8230757713318, 256.9546024799347, 258.0819401741028, 259.2070028781891, 260.3377182483673, 261.4617350101471, 262.5913381576538, 263.7281754016876, 264.85189962387085, 265.975643157959, 267.09824323654175, 268.2259955406189, 269.3544590473175, 270.49076652526855, 271.6151854991913, 272.73552942276, 273.8564145565033, 274.98448395729065, 276.11223888397217, 277.24464893341064, 278.3690814971924, 279.49518179893494, 280.61703395843506, 281.7399435043335, 282.8717563152313, 284.001930475235, 285.12770438194275, 286.249144077301, 287.3696377277374, 288.49374294281006, 289.6223945617676, 290.7512352466583, 291.87170600891113, 292.9956171512604, 294.1189558506012, 295.2467200756073, 296.37330055236816, 297.5018594264984, 298.6296670436859, 299.7621591091156, 300.8906123638153, 302.02207469940186, 303.15619587898254, 304.2998011112213, 305.4432945251465, 306.58644580841064, 307.7188153266907, 308.85812425613403, 310.0004131793976, 311.13833236694336, 312.2726345062256, 313.40932631492615, 314.5535886287689, 315.6859521865845, 316.813547372818, 317.9482808113098, 319.09100222587585, 320.2341969013214, 321.3732559680939, 322.5090026855469, 323.64805030822754, 324.7918438911438, 325.93137550354004, 327.0643587112427, 328.20554065704346, 329.3463022708893, 330.48565912246704, 331.61818528175354, 332.757107257843, 333.8981235027313, 335.04249477386475, 336.17947268486023, 337.31883454322815, 338.4653298854828, 339.6031770706177, 340.743750333786, 341.8815448284149, 343.02504897117615, 344.16671204566956, 346.45104479789734]
[10.6025, 10.7075, 10.7875, 10.86, 10.9975, 11.1825, 11.29, 11.475, 11.5025, 11.62, 11.6525, 11.8075, 12.08, 12.1375, 12.1875, 12.1225, 12.335, 12.32, 12.395, 12.495, 12.52, 12.53, 12.5525, 12.675, 12.6875, 12.7625, 12.72, 12.8475, 12.9625, 12.9125, 12.7975, 12.6675, 12.7075, 12.7025, 12.6775, 12.465, 12.61, 12.365, 12.3625, 12.5325, 12.5925, 12.705, 12.9375, 13.19, 13.34, 13.2375, 13.3925, 13.41, 13.4425, 13.3275, 13.4125, 13.39, 13.365, 13.465, 13.425, 13.48, 13.525, 13.6, 13.7, 13.5925, 13.5375, 13.4875, 13.35, 13.29, 13.3425, 13.2525, 13.3075, 13.25, 13.3175, 13.4075, 13.625, 13.815, 13.925, 14.05, 14.24, 14.3225, 14.475, 14.4425, 14.395, 14.525, 14.495, 14.58, 14.6675, 14.96, 15.1725, 15.395, 15.7, 15.7775, 15.8175, 15.8875, 15.9675, 15.975, 16.0775, 16.04, 16.0875, 16.1425, 16.165, 16.23, 16.2225, 16.3025, 16.275, 16.2675, 16.43, 16.6025, 16.79, 17.0275, 17.43, 17.4625, 17.605, 17.5375, 17.8175, 18.0775, 18.275, 18.515, 18.485, 18.9, 18.9025, 19.2025, 19.1825, 19.1425, 19.13, 19.015, 19.09, 18.9875, 18.9575, 18.85, 18.96, 18.9875, 19.05, 19.185, 19.195, 19.1975, 19.13, 18.9325, 18.8825, 18.8475, 18.935, 18.8725, 18.6225, 18.5425, 18.445, 18.4425, 18.4525, 18.4175, 18.455, 18.34, 18.345, 18.5625, 18.6375, 18.6425, 18.61, 18.755, 18.8875, 18.8475, 18.835, 18.925, 18.9175, 19.015, 19.13, 19.2175, 19.3575, 19.3625, 19.275, 19.24, 19.0425, 18.9275, 19.005, 19.175, 19.13, 19.1575, 19.225, 19.225, 19.2275, 19.3925, 19.355, 19.3375, 19.4275, 19.635, 19.625, 19.595, 19.71, 19.6375, 19.7025, 19.815, 19.8, 19.735, 19.8375, 19.81, 19.7575, 19.7625, 19.78, 19.73, 19.8325, 19.7375, 19.805, 19.865, 19.8925, 20.04, 20.0, 19.865, 19.3625, 16.4525, 14.9, 12.87, 11.9625, 10.9275, 10.9275, 10.9275, 10.9275, 10.4675, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.478, Test loss: 2.007, Test accuracy: 55.20
Final Round, Global train loss: 0.478, Global test loss: 2.797, Global test accuracy: 29.68
Average accuracy final 10 rounds: 54.876999999999995 

Average global accuracy final 10 rounds: 28.704749999999997 

3843.357641220093
[1.4760682582855225, 2.952136516571045, 4.210308313369751, 5.468480110168457, 6.7303924560546875, 7.992304801940918, 9.252727270126343, 10.513149738311768, 11.766606092453003, 13.020062446594238, 14.283827781677246, 15.547593116760254, 16.808379888534546, 18.069166660308838, 19.341209650039673, 20.613252639770508, 21.870585203170776, 23.127917766571045, 24.387206554412842, 25.64649534225464, 26.90003204345703, 28.153568744659424, 29.405746698379517, 30.65792465209961, 31.915905237197876, 33.17388582229614, 34.43118953704834, 35.68849325180054, 36.9406464099884, 38.19279956817627, 39.44203519821167, 40.69127082824707, 41.9458384513855, 43.200406074523926, 44.45091438293457, 45.701422691345215, 46.943214893341064, 48.185007095336914, 49.43272376060486, 50.6804404258728, 51.932488441467285, 53.18453645706177, 54.43576741218567, 55.68699836730957, 56.93623685836792, 58.18547534942627, 59.43746590614319, 60.68945646286011, 61.939634799957275, 63.18981313705444, 64.44006991386414, 65.69032669067383, 66.93862724304199, 68.18692779541016, 69.43781232833862, 70.68869686126709, 71.94011974334717, 73.19154262542725, 74.44820952415466, 75.70487642288208, 76.95879626274109, 78.2127161026001, 79.46321105957031, 80.71370601654053, 81.96554446220398, 83.21738290786743, 84.47061491012573, 85.72384691238403, 86.97438859939575, 88.22493028640747, 89.47974634170532, 90.73456239700317, 91.98017168045044, 93.2257809638977, 94.476069688797, 95.72635841369629, 96.9756965637207, 98.22503471374512, 99.47606635093689, 100.72709798812866, 101.97327995300293, 103.2194619178772, 104.45350241661072, 105.68754291534424, 106.92648696899414, 108.16543102264404, 109.40877795219421, 110.65212488174438, 111.89798426628113, 113.14384365081787, 114.38813304901123, 115.63242244720459, 116.87993836402893, 118.12745428085327, 119.37461447715759, 120.62177467346191, 121.86399579048157, 123.10621690750122, 124.35350108146667, 125.60078525543213, 126.85216331481934, 128.10354137420654, 129.34751200675964, 130.59148263931274, 131.83769822120667, 133.0839138031006, 134.3276722431183, 135.571430683136, 136.81792497634888, 138.06441926956177, 139.3028450012207, 140.54127073287964, 141.7732253074646, 143.00517988204956, 144.23744130134583, 145.4697027206421, 146.70670866966248, 147.94371461868286, 149.1817924976349, 150.4198703765869, 151.6612937450409, 152.90271711349487, 154.13947939872742, 155.37624168395996, 156.61630964279175, 157.85637760162354, 159.0958161354065, 160.33525466918945, 161.57529973983765, 162.81534481048584, 164.05645966529846, 165.29757452011108, 166.53306913375854, 167.768563747406, 169.0059928894043, 170.2434220314026, 171.3636510372162, 172.48388004302979, 173.66846632957458, 174.85305261611938, 175.98708319664001, 177.12111377716064, 178.29784035682678, 179.47456693649292, 180.66868019104004, 181.86279344558716, 183.05911374092102, 184.25543403625488, 185.4569730758667, 186.65851211547852, 187.83772587776184, 189.01693964004517, 190.1975634098053, 191.37818717956543, 192.5641803741455, 193.7501735687256, 194.93038058280945, 196.1105875968933, 197.30550074577332, 198.50041389465332, 199.69312858581543, 200.88584327697754, 202.0756196975708, 203.26539611816406, 204.45397758483887, 205.64255905151367, 206.82478380203247, 208.00700855255127, 209.19097018241882, 210.37493181228638, 211.57441568374634, 212.7738995552063, 213.96444439888, 215.1549892425537, 216.34408712387085, 217.533185005188, 218.73649311065674, 219.9398012161255, 221.12307810783386, 222.30635499954224, 223.3446707725525, 224.38298654556274, 225.41338181495667, 226.4437770843506, 227.46979069709778, 228.49580430984497, 229.52993750572205, 230.56407070159912, 231.58948016166687, 232.61488962173462, 233.64856576919556, 234.6822419166565, 235.71352076530457, 236.74479961395264, 237.76789593696594, 238.79099225997925, 239.81972002983093, 240.84844779968262, 241.8753912448883, 242.902334690094, 244.95160102844238, 247.00086736679077]
[18.7975, 18.7975, 22.0525, 22.0525, 23.9075, 23.9075, 26.725, 26.725, 27.865, 27.865, 29.6525, 29.6525, 32.2825, 32.2825, 33.83, 33.83, 36.5075, 36.5075, 38.8825, 38.8825, 39.115, 39.115, 39.4325, 39.4325, 40.73, 40.73, 41.36, 41.36, 42.2925, 42.2925, 42.9, 42.9, 43.7025, 43.7025, 44.3575, 44.3575, 44.62, 44.62, 45.08, 45.08, 46.285, 46.285, 46.0775, 46.0775, 46.705, 46.705, 47.45, 47.45, 48.165, 48.165, 47.87, 47.87, 48.26, 48.26, 48.6525, 48.6525, 48.5875, 48.5875, 49.0475, 49.0475, 49.615, 49.615, 49.8475, 49.8475, 49.8125, 49.8125, 50.465, 50.465, 50.105, 50.105, 50.39, 50.39, 51.115, 51.115, 51.0325, 51.0325, 51.3625, 51.3625, 51.235, 51.235, 51.015, 51.015, 50.98, 50.98, 51.2325, 51.2325, 51.47, 51.47, 51.3575, 51.3575, 52.0375, 52.0375, 52.1725, 52.1725, 52.495, 52.495, 52.65, 52.65, 52.7675, 52.7675, 52.8, 52.8, 52.6625, 52.6625, 53.01, 53.01, 52.9225, 52.9225, 52.88, 52.88, 53.02, 53.02, 52.7725, 52.7725, 52.9475, 52.9475, 53.23, 53.23, 53.4675, 53.4675, 53.53, 53.53, 53.4175, 53.4175, 53.5975, 53.5975, 53.7875, 53.7875, 53.7775, 53.7775, 53.4925, 53.4925, 53.67, 53.67, 53.82, 53.82, 54.1525, 54.1525, 54.0975, 54.0975, 54.0775, 54.0775, 54.075, 54.075, 53.775, 53.775, 53.76, 53.76, 53.93, 53.93, 54.18, 54.18, 54.3, 54.3, 54.2275, 54.2275, 54.2325, 54.2325, 54.345, 54.345, 54.4775, 54.4775, 54.7675, 54.7675, 54.93, 54.93, 54.7125, 54.7125, 54.91, 54.91, 55.2175, 55.2175, 54.9075, 54.9075, 54.8575, 54.8575, 55.0475, 55.0475, 54.67, 54.67, 54.825, 54.825, 54.61, 54.61, 54.97, 54.97, 54.78, 54.78, 55.155, 55.155, 54.8075, 54.8075, 55.2475, 55.2475, 54.91, 54.91, 54.6425, 54.6425, 54.8225, 54.8225, 55.205, 55.205]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.562, Test loss: 1.270, Test accuracy: 61.88
Average accuracy final 10 rounds: 61.674499999999995 

2740.704345226288
[1.3390369415283203, 2.6780738830566406, 3.7574167251586914, 4.836759567260742, 5.919121026992798, 7.0014824867248535, 8.093337774276733, 9.185193061828613, 10.27661919593811, 11.368045330047607, 12.451342821121216, 13.534640312194824, 14.61716079711914, 15.699681282043457, 16.780986309051514, 17.86229133605957, 18.947853088378906, 20.033414840698242, 21.115429162979126, 22.19744348526001, 23.28159260749817, 24.365741729736328, 25.455677032470703, 26.545612335205078, 27.62754511833191, 28.70947790145874, 29.796175956726074, 30.882874011993408, 31.96978187561035, 33.056689739227295, 34.14392328262329, 35.23115682601929, 36.319608211517334, 37.40805959701538, 38.499356269836426, 39.59065294265747, 40.67622923851013, 41.76180553436279, 42.84557104110718, 43.92933654785156, 45.01286554336548, 46.096394538879395, 47.1690559387207, 48.24171733856201, 49.32000923156738, 50.398301124572754, 51.4818000793457, 52.56529903411865, 53.64411544799805, 54.72293186187744, 55.80403113365173, 56.885130405426025, 57.96721839904785, 59.04930639266968, 60.11882281303406, 61.18833923339844, 62.26620054244995, 63.344061851501465, 64.42699527740479, 65.5099287033081, 66.58311748504639, 67.65630626678467, 68.73668909072876, 69.81707191467285, 70.89550232887268, 71.97393274307251, 73.05554461479187, 74.13715648651123, 75.22587466239929, 76.31459283828735, 77.40482664108276, 78.49506044387817, 79.57845497131348, 80.66184949874878, 81.74827551841736, 82.83470153808594, 83.91216778755188, 84.98963403701782, 86.03773307800293, 87.08583211898804, 88.13312888145447, 89.1804256439209, 90.22126889228821, 91.26211214065552, 92.30822968482971, 93.3543472290039, 94.39845275878906, 95.44255828857422, 96.49442839622498, 97.54629850387573, 98.6019401550293, 99.65758180618286, 100.71105623245239, 101.76453065872192, 102.83796238899231, 103.9113941192627, 104.97693133354187, 106.04246854782104, 107.11876463890076, 108.19506072998047, 109.25934839248657, 110.32363605499268, 111.39083552360535, 112.45803499221802, 113.51479959487915, 114.57156419754028, 115.64607286453247, 116.72058153152466, 117.78788471221924, 118.85518789291382, 119.91822648048401, 120.9812650680542, 122.05352687835693, 123.12578868865967, 124.19318842887878, 125.2605881690979, 126.33412051200867, 127.40765285491943, 128.47112679481506, 129.5346007347107, 130.59490275382996, 131.65520477294922, 132.72812795639038, 133.80105113983154, 134.87531685829163, 135.9495825767517, 137.01198959350586, 138.07439661026, 139.14255714416504, 140.21071767807007, 141.2729480266571, 142.33517837524414, 143.40454626083374, 144.47391414642334, 145.5507938861847, 146.62767362594604, 147.70075607299805, 148.77383852005005, 149.741516828537, 150.70919513702393, 151.67898869514465, 152.64878225326538, 153.7095582485199, 154.7703342437744, 155.82928705215454, 156.88823986053467, 157.9550702571869, 159.0219006538391, 160.08548045158386, 161.1490602493286, 162.2111930847168, 163.27332592010498, 164.3420991897583, 165.41087245941162, 166.47242426872253, 167.53397607803345, 168.5935618877411, 169.65314769744873, 170.71365666389465, 171.77416563034058, 172.8347451686859, 173.89532470703125, 174.95771622657776, 176.02010774612427, 177.08257341384888, 178.1450390815735, 179.20341181755066, 180.26178455352783, 181.327068567276, 182.39235258102417, 183.4568862915039, 184.52142000198364, 185.5864818096161, 186.65154361724854, 187.71592831611633, 188.78031301498413, 189.83828282356262, 190.8962526321411, 191.95589447021484, 193.01553630828857, 194.07435822486877, 195.13318014144897, 196.19184041023254, 197.2505006790161, 198.31888341903687, 199.38726615905762, 200.44680213928223, 201.50633811950684, 202.55078649520874, 203.59523487091064, 204.63881087303162, 205.6823868751526, 206.87244153022766, 208.06249618530273, 209.20807361602783, 210.35365104675293, 211.50071144104004, 212.64777183532715, 213.7875235080719, 214.92727518081665, 216.97610902786255, 219.02494287490845]
[15.42, 15.42, 18.1225, 18.1225, 19.5375, 19.5375, 21.3475, 21.3475, 23.505, 23.505, 25.785, 25.785, 27.97, 27.97, 30.72, 30.72, 32.8625, 32.8625, 34.1825, 34.1825, 35.055, 35.055, 36.995, 36.995, 39.2025, 39.2025, 40.295, 40.295, 42.025, 42.025, 42.5775, 42.5775, 43.3525, 43.3525, 44.3525, 44.3525, 45.355, 45.355, 46.445, 46.445, 46.9, 46.9, 47.76, 47.76, 48.6225, 48.6225, 49.2525, 49.2525, 49.825, 49.825, 51.0275, 51.0275, 50.57, 50.57, 50.26, 50.26, 51.4775, 51.4775, 51.295, 51.295, 52.6575, 52.6575, 53.2025, 53.2025, 52.7825, 52.7825, 53.89, 53.89, 53.4175, 53.4175, 54.8425, 54.8425, 54.235, 54.235, 54.6975, 54.6975, 54.4, 54.4, 55.07, 55.07, 55.6625, 55.6625, 55.53, 55.53, 55.975, 55.975, 56.5025, 56.5025, 57.1325, 57.1325, 57.3325, 57.3325, 56.7575, 56.7575, 56.7325, 56.7325, 58.135, 58.135, 57.8425, 57.8425, 57.62, 57.62, 58.1425, 58.1425, 58.145, 58.145, 57.9225, 57.9225, 58.42, 58.42, 58.92, 58.92, 59.525, 59.525, 59.43, 59.43, 59.3875, 59.3875, 59.7975, 59.7975, 59.3725, 59.3725, 59.87, 59.87, 59.755, 59.755, 60.01, 60.01, 59.7925, 59.7925, 60.37, 60.37, 60.26, 60.26, 59.86, 59.86, 60.36, 60.36, 60.0325, 60.0325, 60.0725, 60.0725, 60.3225, 60.3225, 60.745, 60.745, 60.515, 60.515, 60.755, 60.755, 60.5725, 60.5725, 60.895, 60.895, 60.85, 60.85, 60.985, 60.985, 61.12, 61.12, 60.98, 60.98, 60.755, 60.755, 61.0175, 61.0175, 61.295, 61.295, 61.165, 61.165, 61.46, 61.46, 61.875, 61.875, 61.1925, 61.1925, 61.3475, 61.3475, 61.4875, 61.4875, 61.3725, 61.3725, 61.245, 61.245, 61.295, 61.295, 61.9275, 61.9275, 61.515, 61.515, 61.89, 61.89, 61.66, 61.66, 61.685, 61.685, 62.0625, 62.0625, 62.0925, 62.0925, 61.88, 61.88]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.616, Test loss: 1.017, Test accuracy: 66.18
Average accuracy final 10 rounds: 65.74000000000001
2914.256604909897
[1.8307459354400635, 3.661491870880127, 5.095559358596802, 6.529626846313477, 7.977986812591553, 9.426346778869629, 10.86652660369873, 12.306706428527832, 13.711790800094604, 15.116875171661377, 16.524646043777466, 17.932416915893555, 19.340453386306763, 20.74848985671997, 22.161109685897827, 23.573729515075684, 24.96754503250122, 26.361360549926758, 27.76847743988037, 29.175594329833984, 30.57692837715149, 31.978262424468994, 33.36891531944275, 34.759568214416504, 36.16746377944946, 37.57535934448242, 38.97891044616699, 40.38246154785156, 41.78490948677063, 43.1873574256897, 44.602725982666016, 46.018094539642334, 47.431381940841675, 48.844669342041016, 50.26306939125061, 51.681469440460205, 53.091002225875854, 54.500535011291504, 55.90404558181763, 57.30755615234375, 58.72114372253418, 60.13473129272461, 61.55732178688049, 62.97991228103638, 64.38849711418152, 65.79708194732666, 67.19999074935913, 68.6028995513916, 70.01216697692871, 71.42143440246582, 72.83177280426025, 74.24211120605469, 75.66005730628967, 77.07800340652466, 78.49388933181763, 79.9097752571106, 81.31689620018005, 82.72401714324951, 84.1347188949585, 85.54542064666748, 86.9673216342926, 88.38922262191772, 89.79886937141418, 91.20851612091064, 92.61905932426453, 94.02960252761841, 95.44224667549133, 96.85489082336426, 98.26748943328857, 99.68008804321289, 101.09545946121216, 102.51083087921143, 103.92748856544495, 105.34414625167847, 106.76145505905151, 108.17876386642456, 109.5907506942749, 111.00273752212524, 112.41250681877136, 113.82227611541748, 115.20959854125977, 116.59692096710205, 118.01099514961243, 119.4250693321228, 120.83725357055664, 122.24943780899048, 123.66282224655151, 125.07620668411255, 126.48635292053223, 127.8964991569519, 129.3119761943817, 130.72745323181152, 132.13331484794617, 133.5391764640808, 134.94627332687378, 136.35337018966675, 137.76193380355835, 139.17049741744995, 140.574120759964, 141.97774410247803, 143.3760085105896, 144.77427291870117, 146.17023301124573, 147.56619310379028, 148.98734378814697, 150.40849447250366, 151.84983611106873, 153.2911777496338, 154.7209312915802, 156.1506848335266, 157.574640750885, 158.9985966682434, 160.40574264526367, 161.81288862228394, 163.23280000686646, 164.65271139144897, 166.07742857933044, 167.5021457672119, 168.926509141922, 170.35087251663208, 171.76119756698608, 173.1715226173401, 174.5768804550171, 175.9822382926941, 177.40099143981934, 178.81974458694458, 180.2256679534912, 181.63159132003784, 183.040940284729, 184.45028924942017, 185.8627905845642, 187.27529191970825, 188.6980197429657, 190.12074756622314, 191.54464316368103, 192.96853876113892, 194.3841986656189, 195.79985857009888, 197.21839237213135, 198.63692617416382, 200.06127858161926, 201.4856309890747, 202.89866733551025, 204.3117036819458, 205.74585962295532, 207.18001556396484, 208.5983922481537, 210.01676893234253, 211.44393229484558, 212.87109565734863, 214.2895896434784, 215.70808362960815, 217.1327793598175, 218.55747509002686, 219.99151706695557, 221.42555904388428, 222.84616017341614, 224.266761302948, 225.66343641281128, 227.06011152267456, 228.44191098213196, 229.82371044158936, 231.21448302268982, 232.60525560379028, 233.99032497406006, 235.37539434432983, 236.75943040847778, 238.14346647262573, 239.53343057632446, 240.9233946800232, 242.31845259666443, 243.71351051330566, 245.07576298713684, 246.43801546096802, 247.8058955669403, 249.1737756729126, 250.53365755081177, 251.89353942871094, 253.14316725730896, 254.39279508590698, 255.64314460754395, 256.8934941291809, 258.1393005847931, 259.3851070404053, 260.6300368309021, 261.8749666213989, 263.11434864997864, 264.35373067855835, 265.61354899406433, 266.8733673095703, 268.1152403354645, 269.35711336135864, 270.5914511680603, 271.82578897476196, 273.06522130966187, 274.30465364456177, 275.5428874492645, 276.7811212539673, 278.03028655052185, 279.2794518470764, 281.29446816444397, 283.3094844818115]
[16.3425, 16.3425, 17.5275, 17.5275, 19.0625, 19.0625, 20.7075, 20.7075, 22.8575, 22.8575, 24.7575, 24.7575, 26.4825, 26.4825, 28.425, 28.425, 30.985, 30.985, 32.2025, 32.2025, 33.8925, 33.8925, 35.83, 35.83, 37.0025, 37.0025, 38.355, 38.355, 41.035, 41.035, 42.285, 42.285, 43.8475, 43.8475, 44.675, 44.675, 45.8925, 45.8925, 46.8075, 46.8075, 47.715, 47.715, 48.2175, 48.2175, 48.305, 48.305, 49.31, 49.31, 49.6425, 49.6425, 50.8975, 50.8975, 50.9375, 50.9375, 51.9675, 51.9675, 52.535, 52.535, 52.1425, 52.1425, 52.3675, 52.3675, 53.2525, 53.2525, 54.58, 54.58, 54.4925, 54.4925, 55.2725, 55.2725, 54.8925, 54.8925, 55.5625, 55.5625, 56.94, 56.94, 56.7325, 56.7325, 57.0675, 57.0675, 57.335, 57.335, 58.21, 58.21, 58.1275, 58.1275, 59.21, 59.21, 59.135, 59.135, 59.6525, 59.6525, 59.83, 59.83, 60.11, 60.11, 59.7075, 59.7075, 60.4875, 60.4875, 59.545, 59.545, 60.3125, 60.3125, 60.7525, 60.7525, 61.2275, 61.2275, 61.38, 61.38, 61.7275, 61.7275, 61.72, 61.72, 61.55, 61.55, 61.915, 61.915, 62.0875, 62.0875, 62.15, 62.15, 62.235, 62.235, 61.9, 61.9, 62.0225, 62.0225, 62.55, 62.55, 62.7125, 62.7125, 63.2025, 63.2025, 62.8275, 62.8275, 63.1825, 63.1825, 63.27, 63.27, 63.3175, 63.3175, 63.7025, 63.7025, 64.18, 64.18, 63.3225, 63.3225, 63.87, 63.87, 63.3175, 63.3175, 63.3175, 63.3175, 62.98, 62.98, 64.0025, 64.0025, 64.8125, 64.8125, 64.835, 64.835, 64.635, 64.635, 64.815, 64.815, 65.175, 65.175, 65.25, 65.25, 65.37, 65.37, 65.095, 65.095, 65.5, 65.5, 65.3225, 65.3225, 65.235, 65.235, 65.6225, 65.6225, 65.6525, 65.6525, 65.28, 65.28, 65.4625, 65.4625, 65.8075, 65.8075, 65.7925, 65.7925, 66.07, 66.07, 65.8475, 65.8475, 65.79, 65.79, 66.075, 66.075, 66.1825, 66.1825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.505, Test loss: 2.580, Test accuracy: 28.88
Average accuracy final 10 rounds: 28.261999999999997
3012.61816573143
[3.208202600479126, 6.375471115112305, 9.540911197662354, 12.71178412437439, 15.907310962677002, 19.088868379592896, 22.261946439743042, 25.42436456680298, 28.588874340057373, 31.760315418243408, 34.940162658691406, 38.11503267288208, 41.2773232460022, 44.45621991157532, 47.634395360946655, 50.81046938896179, 53.977062940597534, 57.17219519615173, 60.36995077133179, 63.570566177368164, 66.77269840240479, 69.96391916275024, 73.16417169570923, 76.3590784072876, 79.5535192489624, 82.56765913963318, 85.75383615493774, 88.95013403892517, 91.98584127426147, 95.1202085018158, 98.31142258644104, 101.50726866722107, 104.69563055038452, 107.88819336891174, 111.05165123939514, 114.23594880104065, 117.41876173019409, 120.60406851768494, 123.8021981716156, 126.98864603042603, 130.17924880981445, 133.36981678009033, 136.57198071479797, 139.77171897888184, 142.97172927856445, 146.16479396820068, 149.35736680030823, 152.54998588562012, 155.74257707595825, 158.92952728271484, 162.10776233673096, 165.30089807510376, 168.4860520362854, 171.68186950683594, 174.82416415214539, 178.03419613838196, 181.2580282688141, 184.47948288917542, 187.70829105377197, 190.91578650474548, 194.13671851158142, 197.34163427352905, 200.54455995559692, 203.75206851959229, 206.94972038269043, 210.18451070785522, 213.4057743549347, 216.6295564174652, 219.8576204776764, 223.07624053955078, 226.27609491348267, 229.48571634292603, 232.68718004226685, 235.89714646339417, 239.10668897628784, 242.31584072113037, 245.54689955711365, 248.77720046043396, 252.00874376296997, 255.24083518981934, 258.4740357398987, 261.7016062736511, 264.92149901390076, 268.1508278846741, 271.3806879520416, 274.61048221588135, 277.85261607170105, 281.08943700790405, 284.3190071582794, 287.542920589447, 290.76687002182007, 294.00751638412476, 297.2425603866577, 300.4740102291107, 303.7165858745575, 306.94245958328247, 310.16091871261597, 313.38996052742004, 316.6188359260559, 319.85730481147766, 323.0890097618103]
[16.5375, 18.78, 20.8875, 21.315, 22.575, 22.86, 23.355, 24.23, 24.0625, 24.5075, 24.8925, 25.06, 25.255, 25.7425, 25.525, 25.9225, 26.0975, 26.5, 25.78, 26.565, 26.4225, 26.2675, 26.9325, 27.275, 27.24, 27.1325, 27.6175, 26.98, 27.2525, 26.83, 27.0875, 27.5475, 27.105, 27.9175, 27.4975, 28.0175, 26.6325, 27.2775, 27.365, 27.1875, 27.99, 26.9925, 28.07, 27.375, 27.68, 27.7275, 28.1725, 28.2525, 28.42, 28.145, 28.3775, 28.065, 28.3175, 27.905, 27.835, 28.6975, 27.74, 28.44, 27.6625, 28.0425, 27.575, 28.47, 28.835, 28.2075, 28.2975, 28.2325, 28.0875, 27.4775, 27.805, 27.3775, 28.15, 28.165, 28.065, 28.1575, 28.1225, 28.3425, 28.3325, 28.5225, 28.2925, 28.5525, 28.36, 27.5625, 27.8625, 28.4825, 28.2675, 28.3475, 28.355, 28.12, 28.4275, 28.5725, 28.395, 27.9, 27.04, 28.43, 27.775, 28.3525, 28.5025, 28.94, 28.3925, 28.8925, 28.8775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Average accuracy final 10 rounds: 6.666666666666669 

Average global accuracy final 10 rounds: 6.666666666666669 

4620.9944105148315
[1.8240880966186523, 3.503126382827759, 5.185196399688721, 6.8684611320495605, 8.54378366470337, 10.230250358581543, 11.910008192062378, 13.587097644805908, 15.27407455444336, 16.947431564331055, 18.632518768310547, 20.311813831329346, 21.98664093017578, 23.677729845046997, 25.361376762390137, 27.057058334350586, 28.725354433059692, 30.384779453277588, 32.03814458847046, 33.599584102630615, 35.15728425979614, 36.716864347457886, 38.396644830703735, 40.076645612716675, 41.76205492019653, 43.448445320129395, 45.15754747390747, 46.849555015563965, 48.53151869773865, 50.213032484054565, 51.74965310096741, 53.375576972961426, 55.005775928497314, 56.63761043548584, 58.2693555355072, 59.903889417648315, 61.535669565200806, 63.045682191848755, 64.55353474617004, 66.06239008903503, 67.600332736969, 69.23320198059082, 70.85890197753906, 72.49623203277588, 74.12690663337708, 75.75844240188599, 77.39425206184387, 79.03144383430481, 80.66537356376648, 82.29835534095764, 83.92959475517273, 85.56084752082825, 87.19418668746948, 88.82265853881836, 90.45428037643433, 92.08936977386475, 93.72261905670166, 95.3507707118988, 96.97244358062744, 98.59380269050598, 100.23516488075256, 101.85880708694458, 103.48763227462769, 105.13232517242432, 106.76578688621521, 108.40022826194763, 110.03270435333252, 111.66188740730286, 113.29690146446228, 114.93341255187988, 116.57112860679626, 118.19740891456604, 119.83095860481262, 121.46013450622559, 123.09128880500793, 124.72329640388489, 126.35723519325256, 127.99143266677856, 129.62574434280396, 131.2573902606964, 132.88626384735107, 134.51266241073608, 136.13542222976685, 137.7647294998169, 139.39803075790405, 141.0290904045105, 142.66234016418457, 144.29719066619873, 145.93337869644165, 147.5639624595642, 149.2013761997223, 150.83423376083374, 152.46561288833618, 154.097829580307, 155.72233653068542, 157.34886837005615, 158.9791955947876, 160.61595177650452, 162.2498815059662, 163.88816261291504, 165.52169179916382, 167.15522170066833, 168.79185605049133, 170.4240906238556, 172.0557668209076, 173.68579077720642, 175.3263132572174, 176.9605212211609, 178.59136533737183, 180.2253932952881, 181.85585474967957, 183.48487758636475, 185.11538124084473, 186.73890614509583, 188.36016535758972, 189.98435378074646, 191.61811184883118, 193.24664163589478, 194.88438248634338, 196.5141246318817, 198.14324593544006, 199.77951216697693, 201.40993976593018, 203.04364442825317, 204.67557382583618, 206.3082573413849, 207.94737362861633, 209.57061123847961, 211.13266611099243, 212.7370285987854, 214.24348211288452, 215.74927139282227, 217.25550365447998, 218.76028871536255, 220.26380467414856, 221.76699709892273, 223.27256989479065, 224.9045214653015, 226.5343210697174, 228.14626264572144, 229.79186940193176, 231.41119384765625, 233.0260829925537, 234.66375279426575, 236.28516101837158, 237.9011993408203, 239.51305055618286, 241.13630151748657, 242.75185227394104, 244.36841750144958, 245.9918727874756, 247.63123774528503, 249.2528235912323, 250.86556386947632, 252.49464917182922, 254.11410856246948, 255.74827885627747, 257.37184882164, 258.98177790641785, 260.62601613998413, 262.27007031440735, 263.8832881450653, 265.4952006340027, 267.12736916542053, 268.7359631061554, 270.34840655326843, 271.9590411186218, 273.572705745697, 275.19129252433777, 276.8034017086029, 278.4336655139923, 280.07307410240173, 281.71349000930786, 283.33287525177, 284.95609188079834, 286.5723147392273, 288.2030782699585, 289.8381085395813, 291.4633469581604, 293.0944468975067, 294.7251296043396, 296.3462436199188, 297.9863471984863, 299.6167688369751, 301.26085114479065, 302.88786458969116, 304.50550866127014, 306.1195089817047, 307.737934589386, 309.35757660865784, 310.97643542289734, 312.602991104126, 314.23219633102417, 315.8472099304199, 317.46424293518066, 319.08770990371704, 320.7023663520813, 322.3193271160126, 323.93345952033997, 325.5481984615326, 327.1695206165314, 328.7876570224762, 330.4100513458252, 332.0410530567169, 333.6610977649689, 335.28009605407715, 336.9026598930359, 338.52354168891907, 340.1335165500641, 341.7613134384155, 343.38107800483704, 345.00823950767517, 346.6255578994751, 348.23847484588623, 349.8625478744507, 351.47711396217346, 353.0905294418335, 354.7155797481537, 356.33232617378235, 357.95201897621155, 359.5690219402313, 361.1876468658447, 362.8036963939667, 364.4252841472626, 366.0406115055084, 367.6512842178345, 369.2643105983734, 370.8811535835266, 372.49483489990234, 374.1032283306122, 375.7214665412903, 377.33778858184814, 378.9502303600311, 380.56827783584595, 382.181663274765, 383.79788732528687, 385.4198491573334, 387.0279335975647, 388.64229464530945, 390.25520730018616, 391.8637890815735, 393.4734642505646, 395.0859303474426, 396.70412611961365, 398.3283429145813, 399.94458746910095, 401.5612826347351, 403.1754033565521, 404.79464840888977, 406.4104552268982, 408.02897119522095, 409.6393735408783, 411.25480031967163, 412.8751890659332, 414.49981713294983, 416.1195697784424, 417.7384583950043, 419.3524522781372, 420.9701282978058, 422.58742356300354, 424.20627188682556, 425.8182260990143, 427.43279004096985, 429.04742336273193, 430.66272711753845, 432.2788107395172, 433.89949345588684, 435.5182237625122, 437.14052724838257, 438.762925863266, 440.3716974258423, 441.9827551841736, 443.5994884967804, 445.2091648578644, 446.82107043266296, 448.4409689903259, 450.0604290962219, 451.6769289970398, 453.3038856983185, 454.92571663856506, 456.5550422668457, 458.17591667175293, 459.79085302352905, 461.4027214050293, 463.01512813568115, 464.6219997406006, 466.22858691215515, 467.8428430557251, 469.45784759521484, 471.07091426849365, 472.6804802417755, 474.29947543144226, 475.90962982177734, 477.5222499370575, 479.1347198486328, 480.74997305870056, 482.36603260040283, 483.98095655441284, 485.5930118560791, 487.21232557296753, 489.8968942165375]
[15.883333333333333, 16.016666666666666, 14.383333333333333, 10.516666666666667, 8.766666666666667, 8.691666666666666, 10.233333333333333, 9.066666666666666, 8.508333333333333, 8.266666666666667, 8.266666666666667, 8.558333333333334, 8.558333333333334, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667, 6.666666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.187, Test loss: 0.622, Test accuracy: 81.52
Final Round, Global train loss: 0.187, Global test loss: 1.387, Global test accuracy: 56.97
Average accuracy final 10 rounds: 81.38166666666666 

Average global accuracy final 10 rounds: 55.63333333333333 

935.2046849727631
[0.9702215194702148, 1.9404430389404297, 2.659834861755371, 3.3792266845703125, 4.0935046672821045, 4.8077826499938965, 5.526701211929321, 6.245619773864746, 6.966451168060303, 7.687282562255859, 8.414312839508057, 9.141343116760254, 9.866028785705566, 10.590714454650879, 11.312368154525757, 12.034021854400635, 12.75257134437561, 13.471120834350586, 14.188371419906616, 14.905622005462646, 15.63606071472168, 16.366499423980713, 17.09631609916687, 17.826132774353027, 18.54747176170349, 19.268810749053955, 19.98863959312439, 20.708468437194824, 21.430214881896973, 22.15196132659912, 22.876448392868042, 23.600935459136963, 24.329416036605835, 25.057896614074707, 25.778270959854126, 26.498645305633545, 27.21818518638611, 27.937725067138672, 28.656904220581055, 29.376083374023438, 30.097423553466797, 30.818763732910156, 31.538695096969604, 32.25862646102905, 32.97666883468628, 33.694711208343506, 34.42073392868042, 35.146756649017334, 35.87383151054382, 36.60090637207031, 37.3262460231781, 38.05158567428589, 38.7721586227417, 39.49273157119751, 40.21435475349426, 40.935977935791016, 41.65697956085205, 42.377981185913086, 43.100502014160156, 43.82302284240723, 44.55500078201294, 45.28697872161865, 46.007779121398926, 46.7285795211792, 47.449466943740845, 48.17035436630249, 48.88828229904175, 49.606210231781006, 50.33214068412781, 51.05807113647461, 51.784324169158936, 52.51057720184326, 53.23796772956848, 53.9653582572937, 54.68381142616272, 55.40226459503174, 56.12584447860718, 56.84942436218262, 57.57119560241699, 58.29296684265137, 59.01945352554321, 59.74594020843506, 60.46843671798706, 61.19093322753906, 61.91601800918579, 62.64110279083252, 63.36787724494934, 64.09465169906616, 64.81746029853821, 65.54026889801025, 66.26003885269165, 66.97980880737305, 67.70099949836731, 68.42219018936157, 69.1403443813324, 69.85849857330322, 70.57718992233276, 71.2958812713623, 72.02424168586731, 72.75260210037231, 73.48103332519531, 74.20946455001831, 74.93148517608643, 75.65350580215454, 76.36974239349365, 77.08597898483276, 77.80512261390686, 78.52426624298096, 79.24690771102905, 79.96954917907715, 80.69137406349182, 81.4131989479065, 82.13634777069092, 82.85949659347534, 83.58231925964355, 84.30514192581177, 85.02645015716553, 85.74775838851929, 86.46382260322571, 87.17988681793213, 87.8978819847107, 88.61587715148926, 89.33149576187134, 90.04711437225342, 90.76006293296814, 91.47301149368286, 92.19752836227417, 92.92204523086548, 93.64407229423523, 94.36609935760498, 95.08143448829651, 95.79676961898804, 96.51119661331177, 97.2256236076355, 97.93714427947998, 98.64866495132446, 99.36971473693848, 100.09076452255249, 100.81195187568665, 101.5331392288208, 102.25286626815796, 102.97259330749512, 103.69008350372314, 104.40757369995117, 105.1213858127594, 105.83519792556763, 106.55244588851929, 107.26969385147095, 107.98613595962524, 108.70257806777954, 109.41997909545898, 110.13738012313843, 110.85707950592041, 111.57677888870239, 112.29495811462402, 113.01313734054565, 113.73137259483337, 114.4496078491211, 115.1674394607544, 115.8852710723877, 116.60135078430176, 117.31743049621582, 118.0334119796753, 118.74939346313477, 119.46968412399292, 120.18997478485107, 120.9145941734314, 121.63921356201172, 122.3625373840332, 123.08586120605469, 123.799232006073, 124.51260280609131, 125.23229694366455, 125.9519910812378, 126.66926336288452, 127.38653564453125, 128.1049826145172, 128.82342958450317, 129.53785467147827, 130.25227975845337, 130.96843647956848, 131.6845932006836, 132.40200519561768, 133.11941719055176, 133.83502650260925, 134.55063581466675, 135.27377462387085, 135.99691343307495, 136.7119402885437, 137.42696714401245, 138.14638113975525, 138.86579513549805, 139.58215308189392, 140.2985110282898, 141.0195620059967, 141.7406129837036, 142.46613693237305, 143.19166088104248, 143.90642833709717, 144.62119579315186, 146.07178783416748, 147.5223798751831]
[29.3, 29.3, 36.483333333333334, 36.483333333333334, 44.06666666666667, 44.06666666666667, 57.3, 57.3, 56.68333333333333, 56.68333333333333, 59.6, 59.6, 65.43333333333334, 65.43333333333334, 66.06666666666666, 66.06666666666666, 66.05, 66.05, 67.41666666666667, 67.41666666666667, 68.45, 68.45, 71.63333333333334, 71.63333333333334, 71.61666666666666, 71.61666666666666, 71.4, 71.4, 72.26666666666667, 72.26666666666667, 73.63333333333334, 73.63333333333334, 73.48333333333333, 73.48333333333333, 74.2, 74.2, 74.78333333333333, 74.78333333333333, 75.68333333333334, 75.68333333333334, 76.65, 76.65, 76.45, 76.45, 76.51666666666667, 76.51666666666667, 76.68333333333334, 76.68333333333334, 76.15, 76.15, 76.78333333333333, 76.78333333333333, 76.41666666666667, 76.41666666666667, 76.3, 76.3, 75.71666666666667, 75.71666666666667, 76.08333333333333, 76.08333333333333, 76.33333333333333, 76.33333333333333, 77.03333333333333, 77.03333333333333, 77.23333333333333, 77.23333333333333, 78.68333333333334, 78.68333333333334, 78.66666666666667, 78.66666666666667, 77.7, 77.7, 78.36666666666666, 78.36666666666666, 77.86666666666666, 77.86666666666666, 77.63333333333334, 77.63333333333334, 78.2, 78.2, 77.95, 77.95, 78.31666666666666, 78.31666666666666, 78.63333333333334, 78.63333333333334, 79.01666666666667, 79.01666666666667, 79.73333333333333, 79.73333333333333, 79.5, 79.5, 79.93333333333334, 79.93333333333334, 79.86666666666666, 79.86666666666666, 80.45, 80.45, 79.78333333333333, 79.78333333333333, 80.38333333333334, 80.38333333333334, 81.01666666666667, 81.01666666666667, 80.3, 80.3, 79.93333333333334, 79.93333333333334, 79.88333333333334, 79.88333333333334, 79.8, 79.8, 80.26666666666667, 80.26666666666667, 81.0, 81.0, 80.86666666666666, 80.86666666666666, 80.43333333333334, 80.43333333333334, 80.58333333333333, 80.58333333333333, 80.33333333333333, 80.33333333333333, 80.11666666666666, 80.11666666666666, 80.73333333333333, 80.73333333333333, 80.2, 80.2, 80.58333333333333, 80.58333333333333, 81.45, 81.45, 81.33333333333333, 81.33333333333333, 82.03333333333333, 82.03333333333333, 81.75, 81.75, 81.56666666666666, 81.56666666666666, 80.71666666666667, 80.71666666666667, 80.88333333333334, 80.88333333333334, 81.18333333333334, 81.18333333333334, 81.08333333333333, 81.08333333333333, 80.73333333333333, 80.73333333333333, 80.95, 80.95, 80.66666666666667, 80.66666666666667, 81.21666666666667, 81.21666666666667, 81.36666666666666, 81.36666666666666, 81.3, 81.3, 81.61666666666666, 81.61666666666666, 81.61666666666666, 81.61666666666666, 81.71666666666667, 81.71666666666667, 81.6, 81.6, 81.66666666666667, 81.66666666666667, 81.5, 81.5, 81.51666666666667, 81.51666666666667, 81.55, 81.55, 81.7, 81.7, 81.53333333333333, 81.53333333333333, 81.4, 81.4, 81.91666666666667, 81.91666666666667, 81.63333333333334, 81.63333333333334, 81.36666666666666, 81.36666666666666, 81.13333333333334, 81.13333333333334, 81.13333333333334, 81.13333333333334, 81.03333333333333, 81.03333333333333, 81.13333333333334, 81.13333333333334, 81.53333333333333, 81.53333333333333, 81.51666666666667, 81.51666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.260, Test loss: 0.447, Test accuracy: 82.75
Average accuracy final 10 rounds: 82.37500000000001 

725.7818765640259
[0.9347860813140869, 1.8695721626281738, 2.521191358566284, 3.1728105545043945, 3.8217668533325195, 4.4707231521606445, 5.114622592926025, 5.758522033691406, 6.408040523529053, 7.057559013366699, 7.7075114250183105, 8.357463836669922, 9.006571054458618, 9.655678272247314, 10.307716131210327, 10.95975399017334, 11.61273980140686, 12.26572561264038, 12.916786670684814, 13.567847728729248, 14.219558954238892, 14.871270179748535, 15.521831750869751, 16.172393321990967, 16.8188955783844, 17.465397834777832, 18.113282680511475, 18.761167526245117, 19.414029598236084, 20.06689167022705, 20.722166538238525, 21.37744140625, 22.036529064178467, 22.695616722106934, 23.353058099746704, 24.010499477386475, 24.66299033164978, 25.315481185913086, 25.964216709136963, 26.61295223236084, 27.262479305267334, 27.912006378173828, 28.562927722930908, 29.21384906768799, 29.87110185623169, 30.52835464477539, 31.182875871658325, 31.83739709854126, 32.493603229522705, 33.14980936050415, 33.804970264434814, 34.46013116836548, 35.11177349090576, 35.763415813446045, 36.417335510253906, 37.07125520706177, 37.72399044036865, 38.37672567367554, 39.03140044212341, 39.68607521057129, 40.34163308143616, 40.997190952301025, 41.64838910102844, 42.29958724975586, 42.95330357551575, 43.607019901275635, 44.263123512268066, 44.9192271232605, 45.57563304901123, 46.23203897476196, 46.89150333404541, 47.55096769332886, 48.212016344070435, 48.87306499481201, 49.52441048622131, 50.175755977630615, 50.823336124420166, 51.47091627120972, 52.1190230846405, 52.76712989807129, 53.41355872154236, 54.05998754501343, 54.70759844779968, 55.35520935058594, 56.00386166572571, 56.65251398086548, 57.30087661743164, 57.9492392539978, 58.598756551742554, 59.248273849487305, 59.90231490135193, 60.55635595321655, 61.20403289794922, 61.851709842681885, 62.50129771232605, 63.150885581970215, 63.80520415306091, 64.45952272415161, 65.10034084320068, 65.74115896224976, 66.38990044593811, 67.03864192962646, 67.68979501724243, 68.3409481048584, 68.98714017868042, 69.63333225250244, 70.28359508514404, 70.93385791778564, 71.58709692955017, 72.2403359413147, 72.88668513298035, 73.533034324646, 74.18367719650269, 74.83432006835938, 75.48015308380127, 76.12598609924316, 76.77066826820374, 77.4153504371643, 78.06703662872314, 78.71872282028198, 79.37155294418335, 80.02438306808472, 80.67613315582275, 81.32788324356079, 81.97943496704102, 82.63098669052124, 83.27557134628296, 83.92015600204468, 84.56646728515625, 85.21277856826782, 85.86173105239868, 86.51068353652954, 87.15843796730042, 87.80619239807129, 88.45426845550537, 89.10234451293945, 89.7497889995575, 90.39723348617554, 91.04415106773376, 91.69106864929199, 92.33933234214783, 92.98759603500366, 93.62468886375427, 94.26178169250488, 94.91197156906128, 95.56216144561768, 96.21361637115479, 96.8650712966919, 97.50812792778015, 98.15118455886841, 98.79724192619324, 99.44329929351807, 100.09310173988342, 100.74290418624878, 101.39199638366699, 102.0410885810852, 102.6889705657959, 103.33685255050659, 103.98619866371155, 104.6355447769165, 105.28973293304443, 105.94392108917236, 106.58614778518677, 107.22837448120117, 107.871830701828, 108.51528692245483, 109.15863370895386, 109.80198049545288, 110.44671320915222, 111.09144592285156, 111.74037671089172, 112.38930749893188, 113.03501534461975, 113.68072319030762, 114.32590103149414, 114.97107887268066, 115.62534284591675, 116.27960681915283, 116.928466796875, 117.57732677459717, 118.22167730331421, 118.86602783203125, 119.51390838623047, 120.16178894042969, 120.8092143535614, 121.45663976669312, 122.10616755485535, 122.75569534301758, 123.40243530273438, 124.04917526245117, 124.6958167552948, 125.34245824813843, 125.98952984809875, 126.63660144805908, 127.28961038589478, 127.94261932373047, 128.59074211120605, 129.23886489868164, 129.89103937149048, 130.54321384429932, 131.70186400413513, 132.86051416397095]
[23.4, 23.4, 27.366666666666667, 27.366666666666667, 37.9, 37.9, 46.1, 46.1, 51.6, 51.6, 58.61666666666667, 58.61666666666667, 60.13333333333333, 60.13333333333333, 62.0, 62.0, 65.0, 65.0, 69.21666666666667, 69.21666666666667, 69.36666666666666, 69.36666666666666, 71.4, 71.4, 71.15, 71.15, 71.63333333333334, 71.63333333333334, 70.58333333333333, 70.58333333333333, 71.2, 71.2, 71.0, 71.0, 72.96666666666667, 72.96666666666667, 72.91666666666667, 72.91666666666667, 73.1, 73.1, 73.91666666666667, 73.91666666666667, 74.51666666666667, 74.51666666666667, 74.83333333333333, 74.83333333333333, 75.61666666666666, 75.61666666666666, 75.08333333333333, 75.08333333333333, 75.4, 75.4, 76.06666666666666, 76.06666666666666, 77.2, 77.2, 76.71666666666667, 76.71666666666667, 75.78333333333333, 75.78333333333333, 75.76666666666667, 75.76666666666667, 76.35, 76.35, 76.96666666666667, 76.96666666666667, 77.06666666666666, 77.06666666666666, 77.9, 77.9, 77.45, 77.45, 77.33333333333333, 77.33333333333333, 77.7, 77.7, 77.66666666666667, 77.66666666666667, 78.16666666666667, 78.16666666666667, 78.36666666666666, 78.36666666666666, 77.76666666666667, 77.76666666666667, 78.3, 78.3, 78.11666666666666, 78.11666666666666, 78.91666666666667, 78.91666666666667, 79.26666666666667, 79.26666666666667, 79.18333333333334, 79.18333333333334, 78.15, 78.15, 78.9, 78.9, 79.3, 79.3, 79.3, 79.3, 79.13333333333334, 79.13333333333334, 79.15, 79.15, 79.75, 79.75, 79.83333333333333, 79.83333333333333, 79.78333333333333, 79.78333333333333, 79.86666666666666, 79.86666666666666, 80.11666666666666, 80.11666666666666, 79.9, 79.9, 80.55, 80.55, 80.48333333333333, 80.48333333333333, 81.26666666666667, 81.26666666666667, 80.7, 80.7, 80.05, 80.05, 80.56666666666666, 80.56666666666666, 80.66666666666667, 80.66666666666667, 81.31666666666666, 81.31666666666666, 80.98333333333333, 80.98333333333333, 81.51666666666667, 81.51666666666667, 81.0, 81.0, 81.18333333333334, 81.18333333333334, 81.3, 81.3, 81.91666666666667, 81.91666666666667, 82.16666666666667, 82.16666666666667, 82.05, 82.05, 81.76666666666667, 81.76666666666667, 81.46666666666667, 81.46666666666667, 81.81666666666666, 81.81666666666666, 82.16666666666667, 82.16666666666667, 81.65, 81.65, 81.85, 81.85, 81.68333333333334, 81.68333333333334, 82.16666666666667, 82.16666666666667, 81.86666666666666, 81.86666666666666, 82.18333333333334, 82.18333333333334, 82.55, 82.55, 82.43333333333334, 82.43333333333334, 82.55, 82.55, 82.38333333333334, 82.38333333333334, 82.05, 82.05, 82.11666666666666, 82.11666666666666, 82.35, 82.35, 82.43333333333334, 82.43333333333334, 82.61666666666666, 82.61666666666666, 82.88333333333334, 82.88333333333334, 82.4, 82.4, 81.73333333333333, 81.73333333333333, 82.15, 82.15, 82.43333333333334, 82.43333333333334, 82.63333333333334, 82.63333333333334, 82.75, 82.75]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.252, Test loss: 0.406, Test accuracy: 83.67
Average accuracy final 10 rounds: 83.375
806.5922844409943
[1.1283767223358154, 2.256753444671631, 3.045353889465332, 3.833954334259033, 4.620360374450684, 5.406766414642334, 6.198688507080078, 6.990610599517822, 7.791613340377808, 8.592616081237793, 9.383745193481445, 10.174874305725098, 10.967488527297974, 11.76010274887085, 12.548872232437134, 13.337641716003418, 14.126051425933838, 14.914461135864258, 15.697726726531982, 16.480992317199707, 17.27141785621643, 18.061843395233154, 18.855019330978394, 19.648195266723633, 20.441410064697266, 21.2346248626709, 22.031156301498413, 22.827687740325928, 23.62082815170288, 24.413968563079834, 25.207314252853394, 26.000659942626953, 26.797099590301514, 27.593539237976074, 28.390087127685547, 29.18663501739502, 29.981423139572144, 30.776211261749268, 31.56693983078003, 32.35766839981079, 33.14816236495972, 33.93865633010864, 34.734890937805176, 35.53112554550171, 36.32742214202881, 37.12371873855591, 37.947556257247925, 38.77139377593994, 39.601072788238525, 40.43075180053711, 41.26296663284302, 42.095181465148926, 42.91885328292847, 43.74252510070801, 44.574206590652466, 45.405888080596924, 46.237062215805054, 47.068236351013184, 47.891879081726074, 48.715521812438965, 49.54133415222168, 50.367146492004395, 51.187596559524536, 52.00804662704468, 52.83839440345764, 53.668742179870605, 54.49291515350342, 55.31708812713623, 56.15503215789795, 56.99297618865967, 57.82723784446716, 58.66149950027466, 59.49662160873413, 60.3317437171936, 61.170984983444214, 62.010226249694824, 62.839738607406616, 63.66925096511841, 64.42350220680237, 65.17775344848633, 65.9380292892456, 66.69830513000488, 67.45314311981201, 68.20798110961914, 68.95908308029175, 69.71018505096436, 70.45971083641052, 71.20923662185669, 71.96699690818787, 72.72475719451904, 73.48031044006348, 74.23586368560791, 74.98709392547607, 75.73832416534424, 76.48566842079163, 77.23301267623901, 77.98200345039368, 78.73099422454834, 79.48221564292908, 80.23343706130981, 80.9855043888092, 81.7375717163086, 82.49532651901245, 83.25308132171631, 84.00372910499573, 84.75437688827515, 85.50551414489746, 86.25665140151978, 87.00727367401123, 87.75789594650269, 88.51780247688293, 89.27770900726318, 90.02755045890808, 90.77739191055298, 91.53381609916687, 92.29024028778076, 93.04023790359497, 93.79023551940918, 94.5417947769165, 95.29335403442383, 96.05256843566895, 96.81178283691406, 97.56751036643982, 98.32323789596558, 99.08358573913574, 99.84393358230591, 100.5974633693695, 101.3509931564331, 102.09698534011841, 102.84297752380371, 103.59313726425171, 104.3432970046997, 105.10187363624573, 105.86045026779175, 106.62022829055786, 107.38000631332397, 108.13783264160156, 108.89565896987915, 109.64461851119995, 110.39357805252075, 111.15001058578491, 111.90644311904907, 112.6623113155365, 113.41817951202393, 114.17931532859802, 114.94045114517212, 115.69276094436646, 116.44507074356079, 117.20324754714966, 117.96142435073853, 118.70491290092468, 119.44840145111084, 120.19728875160217, 120.9461760520935, 121.70467782020569, 122.46317958831787, 123.22291088104248, 123.98264217376709, 124.73732256889343, 125.49200296401978, 126.24755120277405, 127.00309944152832, 127.75018692016602, 128.4972743988037, 129.24855947494507, 129.99984455108643, 130.75330877304077, 131.50677299499512, 132.2627305984497, 133.0186882019043, 133.76842141151428, 134.51815462112427, 135.2660903930664, 136.01402616500854, 136.76627039909363, 137.5185146331787, 138.27815532684326, 139.0377960205078, 139.79843997955322, 140.55908393859863, 141.31408548355103, 142.06908702850342, 142.82891273498535, 143.58873844146729, 144.3481788635254, 145.1076192855835, 145.8633952140808, 146.61917114257812, 147.37046194076538, 148.12175273895264, 148.85849738121033, 149.59524202346802, 150.33769845962524, 151.08015489578247, 151.83324337005615, 152.58633184432983, 153.3373692035675, 154.08840656280518, 154.8426103591919, 155.5968141555786, 156.80930137634277, 158.02178859710693]
[16.85, 16.85, 29.083333333333332, 29.083333333333332, 32.0, 32.0, 44.983333333333334, 44.983333333333334, 53.31666666666667, 53.31666666666667, 58.0, 58.0, 59.71666666666667, 59.71666666666667, 63.75, 63.75, 65.18333333333334, 65.18333333333334, 70.01666666666667, 70.01666666666667, 71.36666666666666, 71.36666666666666, 72.11666666666666, 72.11666666666666, 72.26666666666667, 72.26666666666667, 73.01666666666667, 73.01666666666667, 73.21666666666667, 73.21666666666667, 73.08333333333333, 73.08333333333333, 74.26666666666667, 74.26666666666667, 74.38333333333334, 74.38333333333334, 75.38333333333334, 75.38333333333334, 75.51666666666667, 75.51666666666667, 75.96666666666667, 75.96666666666667, 75.63333333333334, 75.63333333333334, 77.0, 77.0, 76.38333333333334, 76.38333333333334, 76.36666666666666, 76.36666666666666, 77.08333333333333, 77.08333333333333, 76.93333333333334, 76.93333333333334, 77.73333333333333, 77.73333333333333, 77.91666666666667, 77.91666666666667, 77.85, 77.85, 78.75, 78.75, 77.76666666666667, 77.76666666666667, 77.88333333333334, 77.88333333333334, 77.83333333333333, 77.83333333333333, 78.8, 78.8, 79.48333333333333, 79.48333333333333, 80.11666666666666, 80.11666666666666, 79.51666666666667, 79.51666666666667, 79.86666666666666, 79.86666666666666, 80.23333333333333, 80.23333333333333, 80.53333333333333, 80.53333333333333, 80.45, 80.45, 80.65, 80.65, 80.43333333333334, 80.43333333333334, 80.91666666666667, 80.91666666666667, 80.75, 80.75, 80.95, 80.95, 81.58333333333333, 81.58333333333333, 81.5, 81.5, 81.4, 81.4, 81.23333333333333, 81.23333333333333, 81.83333333333333, 81.83333333333333, 81.93333333333334, 81.93333333333334, 81.46666666666667, 81.46666666666667, 81.91666666666667, 81.91666666666667, 81.83333333333333, 81.83333333333333, 82.3, 82.3, 82.05, 82.05, 82.18333333333334, 82.18333333333334, 82.35, 82.35, 82.0, 82.0, 82.6, 82.6, 82.38333333333334, 82.38333333333334, 82.0, 82.0, 82.46666666666667, 82.46666666666667, 82.56666666666666, 82.56666666666666, 82.36666666666666, 82.36666666666666, 82.96666666666667, 82.96666666666667, 82.6, 82.6, 82.76666666666667, 82.76666666666667, 82.65, 82.65, 83.21666666666667, 83.21666666666667, 82.73333333333333, 82.73333333333333, 83.11666666666666, 83.11666666666666, 82.8, 82.8, 82.9, 82.9, 83.08333333333333, 83.08333333333333, 82.85, 82.85, 83.03333333333333, 83.03333333333333, 82.65, 82.65, 83.25, 83.25, 83.46666666666667, 83.46666666666667, 82.91666666666667, 82.91666666666667, 83.38333333333334, 83.38333333333334, 83.6, 83.6, 83.66666666666667, 83.66666666666667, 83.41666666666667, 83.41666666666667, 83.33333333333333, 83.33333333333333, 83.3, 83.3, 83.36666666666666, 83.36666666666666, 83.28333333333333, 83.28333333333333, 83.73333333333333, 83.73333333333333, 83.48333333333333, 83.48333333333333, 83.05, 83.05, 83.36666666666666, 83.36666666666666, 83.6, 83.6, 83.56666666666666, 83.56666666666666, 83.33333333333333, 83.33333333333333, 83.11666666666666, 83.11666666666666, 83.21666666666667, 83.21666666666667, 83.66666666666667, 83.66666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.166, Test loss: 1.249, Test accuracy: 62.37
Average accuracy final 10 rounds: 58.095000000000006
1197.7319991588593
[2.0248169898986816, 3.7653143405914307, 5.519567012786865, 7.268853425979614, 9.020774126052856, 10.77847957611084, 12.532241106033325, 14.270355463027954, 16.063021421432495, 17.872920751571655, 19.681619882583618, 21.500120162963867, 23.317639112472534, 25.127758026123047, 26.950184106826782, 28.763217449188232, 30.584075450897217, 32.40484619140625, 34.22641968727112, 36.04639744758606, 37.86002802848816, 39.672778367996216, 41.496960401535034, 43.31315898895264, 45.12872529029846, 46.927589654922485, 48.73576903343201, 50.544108867645264, 52.291035175323486, 54.03685903549194, 55.783066272735596, 57.53433895111084, 59.28149151802063, 61.04763317108154, 62.797324657440186, 64.55218148231506, 66.30353355407715, 68.05177068710327, 69.78665709495544, 71.58731985092163, 73.3889970779419, 75.19071793556213, 76.99714207649231, 78.8099217414856, 80.60059189796448, 82.40756559371948, 84.20529079437256, 85.82914733886719, 87.45624685287476, 89.07277607917786, 90.69447016716003, 92.31180024147034, 93.93744707107544, 95.54995083808899, 97.17560887336731, 98.79974126815796, 100.4169249534607, 102.0358533859253, 103.66296672821045, 105.27482914924622, 106.88386487960815, 108.49197292327881, 110.10304474830627, 111.7162754535675, 113.32243990898132, 114.93322134017944, 116.54493594169617, 118.16275978088379, 119.77766585350037, 121.39035773277283, 123.00020146369934, 124.61600542068481, 126.22477078437805, 127.84941267967224, 129.46516847610474, 131.0811312198639, 132.69703698158264, 134.30799651145935, 135.92127013206482, 137.5470154285431, 139.17049765586853, 140.79449272155762, 142.41232109069824, 144.02791929244995, 145.64411735534668, 147.2499623298645, 148.857816696167, 150.4690465927124, 152.28920936584473, 154.15027928352356, 155.9875135421753, 157.7606599330902, 159.36736869812012, 160.94457030296326, 162.50651264190674, 164.05947542190552, 165.6052589416504, 167.16984510421753, 168.74041533470154, 170.30429029464722, 171.8557846546173]
[24.083333333333332, 25.066666666666666, 31.933333333333334, 33.18333333333333, 30.766666666666666, 39.28333333333333, 32.43333333333333, 36.35, 37.03333333333333, 33.766666666666666, 34.85, 43.53333333333333, 43.8, 42.18333333333333, 43.06666666666667, 37.166666666666664, 38.81666666666667, 45.233333333333334, 45.483333333333334, 46.6, 45.95, 46.3, 43.56666666666667, 49.2, 49.0, 32.7, 47.36666666666667, 38.38333333333333, 47.25, 48.28333333333333, 53.0, 42.28333333333333, 53.85, 52.05, 49.03333333333333, 49.71666666666667, 52.2, 57.18333333333333, 55.733333333333334, 49.4, 47.6, 55.3, 52.46666666666667, 48.85, 48.266666666666666, 48.55, 55.233333333333334, 51.31666666666667, 47.55, 47.35, 53.38333333333333, 55.78333333333333, 57.78333333333333, 52.4, 52.8, 56.7, 57.416666666666664, 55.93333333333333, 55.85, 51.0, 45.56666666666667, 56.833333333333336, 51.766666666666666, 57.85, 52.93333333333333, 52.95, 48.583333333333336, 55.96666666666667, 55.233333333333334, 53.71666666666667, 54.81666666666667, 51.63333333333333, 53.733333333333334, 56.31666666666667, 57.13333333333333, 52.8, 60.166666666666664, 56.016666666666666, 57.35, 56.016666666666666, 55.13333333333333, 56.86666666666667, 59.05, 55.983333333333334, 56.21666666666667, 59.733333333333334, 56.5, 54.11666666666667, 54.25, 56.416666666666664, 59.56666666666667, 56.68333333333333, 56.46666666666667, 57.63333333333333, 57.78333333333333, 55.45, 58.483333333333334, 59.3, 59.68333333333333, 59.9, 62.36666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

15202.42624092102
[5.138664484024048, 10.192073822021484, 15.288769006729126, 20.468899965286255, 25.37892770767212, 29.951939821243286, 34.7100396156311, 39.268126010894775, 43.80042004585266, 48.32150173187256, 53.00677418708801, 57.73503613471985, 62.340744733810425, 66.88715052604675, 71.44356417655945, 76.41396427154541, 81.0278799533844, 85.59545016288757, 90.59616875648499, 95.27313542366028, 99.78539800643921, 104.77652931213379, 109.80801391601562, 114.77644801139832, 119.80826759338379, 124.8351378440857, 129.7631766796112, 134.83433389663696, 139.9629852771759, 144.90968990325928, 149.68880558013916, 154.71363997459412, 159.82515335083008, 164.81580924987793, 169.77650547027588, 174.78285598754883, 179.74774932861328, 184.697856426239, 189.605464220047, 194.51472115516663, 199.4290804862976, 204.38309621810913, 209.30903148651123, 213.83348751068115, 218.3353977203369, 222.83465003967285, 227.36250400543213, 231.8818302154541, 236.5685338973999, 241.3080015182495, 245.91288900375366, 251.9399971961975, 258.51150703430176, 263.9185719490051, 269.3085844516754, 274.7987685203552, 279.9155421257019, 285.1232306957245, 290.28671741485596, 295.57786536216736, 301.00686597824097, 306.40069913864136, 311.83913135528564, 317.3128774166107, 322.7955570220947, 328.124902009964, 333.3682460784912, 338.5377109050751, 343.79142808914185, 348.6424551010132, 353.2078375816345, 357.73797130584717, 362.3147625923157, 366.8932478427887, 371.58552646636963, 376.3040518760681, 381.060644865036, 385.6174919605255, 390.19385027885437, 394.8144693374634, 399.45781207084656, 404.09811878204346, 408.8417308330536, 413.5352189540863, 418.1539144515991, 422.7326729297638, 427.30526995658875, 431.86138105392456, 436.426472902298, 440.9784517288208, 445.51748752593994, 450.12999725341797, 454.8144347667694, 459.5606508255005, 464.05244183540344, 468.56031346321106, 473.3344900608063, 478.11884355545044, 482.863440990448, 489.2474801540375, 495.11525869369507, 500.93278193473816, 505.5186846256256, 510.0469002723694, 514.5743799209595, 519.6678669452667, 524.7602028846741, 529.3338181972504, 533.8468260765076, 538.7966909408569, 543.8965239524841, 549.0314292907715, 554.1635217666626, 558.6907331943512, 563.2183372974396, 567.7902321815491, 572.350953578949, 577.4718036651611, 582.5678570270538, 587.7050759792328, 592.6753177642822, 597.7505028247833, 602.7227685451508, 607.7829766273499, 612.7134342193604, 617.348108291626, 621.9212203025818, 626.9297032356262, 632.0810034275055, 637.2144479751587, 642.4605829715729, 647.7190117835999, 652.9549267292023, 658.189466714859, 663.0464029312134, 668.2913289070129, 673.6460137367249, 678.3756959438324, 682.9559934139252, 687.5782463550568, 692.174284696579, 696.7986295223236, 701.447333574295, 706.0731987953186, 710.6689774990082, 715.2636296749115, 719.9284579753876, 724.5739905834198, 729.1789226531982, 733.8009538650513, 738.4122641086578, 743.0751957893372, 747.6934270858765, 752.3218722343445, 756.8750824928284, 761.4751710891724, 766.0899767875671, 770.7219479084015, 775.2655172348022, 779.9731931686401, 784.7075369358063, 789.9376318454742, 795.0392739772797, 799.7851226329803, 804.9822702407837, 809.6160264015198, 814.210652589798, 818.7657747268677, 823.3107485771179, 828.0017387866974, 832.5720818042755, 837.1247985363007, 841.7295889854431, 846.6716666221619, 851.2431993484497, 855.7995672225952, 860.6052639484406, 865.2309436798096, 869.8873474597931, 874.5657067298889, 879.165931224823, 884.3037385940552, 889.7063927650452, 895.0912053585052, 900.5140569210052, 905.9585537910461, 911.287693977356, 916.723066329956, 922.1212246417999, 926.866293668747, 931.5868737697601, 936.854633808136, 941.5541627407074, 946.2825767993927, 951.0389020442963, 955.7785832881927, 960.5113730430603, 965.2191076278687, 969.9594976902008, 975.2098479270935, 979.9307599067688, 984.673410654068, 989.3964731693268, 994.7241575717926, 1000.0001201629639, 1004.723340511322, 1009.438643693924, 1014.1663477420807, 1018.9157085418701, 1023.6713111400604, 1028.452764749527, 1033.200377702713, 1038.1992073059082, 1042.886677980423, 1047.8171260356903, 1052.9538369178772, 1058.1764814853668, 1063.4457378387451, 1068.1851007938385, 1072.9508368968964, 1077.6960117816925, 1082.443966627121, 1087.1357371807098, 1091.8702001571655, 1096.5687446594238, 1101.2892315387726, 1106.0334236621857, 1110.7630598545074, 1115.486650466919, 1120.17387342453, 1125.0566589832306, 1129.8290441036224, 1134.9035453796387, 1139.793321609497, 1144.5753078460693, 1149.4935548305511, 1154.8905456066132, 1160.2461359500885, 1165.6290278434753, 1170.9360210895538, 1176.279628753662, 1181.6402397155762, 1186.3885009288788, 1191.1179192066193, 1195.8302102088928, 1200.6035630702972, 1205.3978929519653, 1210.1849081516266, 1214.9594721794128, 1219.7375266551971, 1224.5105991363525, 1229.2589962482452, 1233.9905214309692, 1238.7552552223206, 1243.5651338100433, 1248.3969156742096, 1253.249785900116, 1258.0467782020569, 1262.8381445407867, 1267.6861839294434, 1272.4510028362274, 1277.2043721675873, 1282.2782125473022, 1287.5053131580353, 1292.2851750850677, 1297.3859648704529, 1302.1893482208252, 1306.8928527832031, 1311.9327065944672, 1317.2777028083801, 1322.4882566928864, 1327.8151149749756, 1333.054892539978, 1338.2881870269775, 1343.1085419654846, 1347.9171051979065, 1353.2144048213959, 1358.4247179031372, 1363.2639374732971, 1368.1407856941223, 1373.0133712291718, 1377.7907416820526, 1383.3409383296967, 1388.7373402118683, 1393.8817856311798, 1398.6777141094208, 1403.4966280460358, 1408.236277103424, 1413.0424304008484, 1417.9145019054413, 1422.714589357376, 1428.0086452960968, 1433.2611207962036, 1438.6280100345612, 1444.0223977565765, 1449.3002576828003, 1454.6754760742188, 1459.9302577972412, 1465.3212614059448, 1470.029706954956, 1472.3903086185455]
[9.025, 9.235, 9.4225, 9.615, 9.7025, 9.7225, 9.8175, 9.86, 9.885, 9.895, 9.9025, 9.9325, 9.95, 9.96, 9.965, 10.03, 10.06, 10.115, 10.1575, 10.2575, 10.2575, 10.2925, 10.295, 10.325, 10.295, 10.3, 10.4375, 10.5625, 10.5875, 10.5325, 10.58, 10.545, 10.6225, 10.605, 10.605, 10.635, 10.665, 10.645, 10.6575, 10.67, 10.61, 10.6425, 10.595, 10.5375, 10.5275, 10.485, 10.51, 10.455, 10.4475, 10.5, 10.515, 10.515, 10.5725, 10.6125, 10.5675, 10.5825, 10.6825, 10.7075, 10.65, 10.6925, 10.7225, 10.8275, 10.9125, 10.8725, 10.805, 10.8475, 10.9525, 11.0, 11.1175, 11.3775, 11.3675, 11.6175, 11.69, 11.7725, 11.7125, 11.92, 11.8625, 11.98, 12.2675, 12.515, 12.64, 12.8575, 12.9475, 13.1, 13.435, 13.6825, 13.8075, 14.2175, 14.255, 14.33, 14.635, 14.9825, 15.2975, 15.9575, 16.28, 16.4775, 17.15, 17.7125, 17.7625, 17.65, 18.22, 18.345, 18.13, 18.125, 18.0875, 18.355, 18.3475, 18.565, 18.445, 18.9875, 19.275, 19.1675, 18.77, 18.465, 18.425, 18.4575, 18.705, 18.76, 18.9175, 19.16, 19.3725, 19.285, 19.3325, 19.58, 19.675, 19.69, 19.975, 20.355, 20.445, 20.27, 20.1925, 20.55, 20.3675, 20.24, 20.335, 19.66, 19.62, 18.8875, 16.3875, 13.9475, 12.9325, 10.9675, 10.6225, 10.6225, 10.6225, 10.6225, 10.6225, 10.6225, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.151, Test loss: 0.512, Test accuracy: 84.88
Final Round, Global train loss: 0.151, Global test loss: 1.138, Global test accuracy: 66.42
Average accuracy final 10 rounds: 85.11666666666666 

Average global accuracy final 10 rounds: 65.60666666666667 

1813.0075147151947
[1.8104956150054932, 3.6209912300109863, 5.146742343902588, 6.6724934577941895, 8.191572427749634, 9.710651397705078, 11.184337377548218, 12.658023357391357, 13.91959834098816, 15.181173324584961, 16.422291040420532, 17.663408756256104, 18.918845891952515, 20.174283027648926, 21.450454711914062, 22.7266263961792, 24.014482975006104, 25.302339553833008, 26.639000177383423, 27.975660800933838, 29.238482236862183, 30.501303672790527, 31.778831005096436, 33.056358337402344, 34.35552263259888, 35.65468692779541, 36.942363023757935, 38.23003911972046, 39.51930332183838, 40.8085675239563, 42.08184862136841, 43.35512971878052, 44.81383538246155, 46.27254104614258, 47.7461142539978, 49.21968746185303, 50.55057716369629, 51.88146686553955, 53.16559863090515, 54.44973039627075, 55.75190305709839, 57.054075717926025, 58.34303689002991, 59.63199806213379, 60.91368079185486, 62.19536352157593, 63.4900426864624, 64.78472185134888, 66.09631872177124, 67.4079155921936, 68.70735359191895, 70.00679159164429, 71.29561972618103, 72.58444786071777, 73.87784695625305, 75.17124605178833, 76.46390843391418, 77.75657081604004, 79.01987838745117, 80.2831859588623, 81.5414400100708, 82.7996940612793, 84.05635595321655, 85.31301784515381, 86.5727481842041, 87.8324785232544, 89.0836718082428, 90.3348650932312, 91.59281706809998, 92.85076904296875, 94.11998295783997, 95.38919687271118, 96.64869356155396, 97.90819025039673, 99.17223572731018, 100.43628120422363, 101.70104908943176, 102.96581697463989, 104.23134899139404, 105.4968810081482, 106.7615396976471, 108.026198387146, 109.43770694732666, 110.84921550750732, 112.30276322364807, 113.75631093978882, 115.21865129470825, 116.68099164962769, 118.13469934463501, 119.58840703964233, 121.04385185241699, 122.49929666519165, 123.96266913414001, 125.42604160308838, 126.8794162273407, 128.33279085159302, 129.77616596221924, 131.21954107284546, 132.67343831062317, 134.12733554840088, 135.57848048210144, 137.029625415802, 138.48101711273193, 139.93240880966187, 141.3815667629242, 142.83072471618652, 144.2858226299286, 145.74092054367065, 147.18868589401245, 148.63645124435425, 150.0830307006836, 151.52961015701294, 152.9940962791443, 154.45858240127563, 155.91137552261353, 157.36416864395142, 158.8162875175476, 160.2684063911438, 161.70450353622437, 163.14060068130493, 164.3997802734375, 165.65895986557007, 166.91205620765686, 168.16515254974365, 169.41996026039124, 170.67476797103882, 171.9241168498993, 173.17346572875977, 174.42652869224548, 175.6795916557312, 176.9333837032318, 178.18717575073242, 179.454931974411, 180.7226881980896, 181.97557425498962, 183.22846031188965, 184.4815230369568, 185.73458576202393, 186.98123025894165, 188.22787475585938, 189.50037026405334, 190.77286577224731, 192.02981972694397, 193.28677368164062, 194.53417944908142, 195.78158521652222, 197.03474068641663, 198.28789615631104, 199.53695249557495, 200.78600883483887, 202.03634333610535, 203.28667783737183, 204.54011869430542, 205.793559551239, 207.05082297325134, 208.30808639526367, 209.56514739990234, 210.82220840454102, 212.0758411884308, 213.32947397232056, 214.58914375305176, 215.84881353378296, 217.10093021392822, 218.3530468940735, 219.61561751365662, 220.87818813323975, 222.13802647590637, 223.397864818573, 224.6532301902771, 225.9085955619812, 227.1582806110382, 228.40796566009521, 229.6560139656067, 230.90406227111816, 232.15947794914246, 233.41489362716675, 234.67813420295715, 235.94137477874756, 237.19737672805786, 238.45337867736816, 239.70548248291016, 240.95758628845215, 242.23122119903564, 243.50485610961914, 244.76482677459717, 246.0247974395752, 247.29412269592285, 248.5634479522705, 249.83022451400757, 251.09700107574463, 252.36035799980164, 253.62371492385864, 254.8873269557953, 256.15093898773193, 257.40672612190247, 258.662513256073, 259.9215006828308, 261.1804881095886, 262.4384009838104, 263.6963138580322, 265.79888248443604, 267.90145111083984]
[23.683333333333334, 23.683333333333334, 45.68333333333333, 45.68333333333333, 54.11666666666667, 54.11666666666667, 59.666666666666664, 59.666666666666664, 60.725, 60.725, 65.25, 65.25, 71.55833333333334, 71.55833333333334, 72.28333333333333, 72.28333333333333, 72.475, 72.475, 73.25833333333334, 73.25833333333334, 74.26666666666667, 74.26666666666667, 76.26666666666667, 76.26666666666667, 77.19166666666666, 77.19166666666666, 78.025, 78.025, 77.925, 77.925, 78.5, 78.5, 78.31666666666666, 78.31666666666666, 79.04166666666667, 79.04166666666667, 79.33333333333333, 79.33333333333333, 79.60833333333333, 79.60833333333333, 80.025, 80.025, 80.45833333333333, 80.45833333333333, 80.55, 80.55, 80.625, 80.625, 80.91666666666667, 80.91666666666667, 81.125, 81.125, 81.7, 81.7, 82.05, 82.05, 82.25833333333334, 82.25833333333334, 82.30833333333334, 82.30833333333334, 82.09166666666667, 82.09166666666667, 81.58333333333333, 81.58333333333333, 81.50833333333334, 81.50833333333334, 82.09166666666667, 82.09166666666667, 81.78333333333333, 81.78333333333333, 81.86666666666666, 81.86666666666666, 81.725, 81.725, 81.875, 81.875, 82.1, 82.1, 81.83333333333333, 81.83333333333333, 82.15, 82.15, 82.05833333333334, 82.05833333333334, 82.08333333333333, 82.08333333333333, 82.59166666666667, 82.59166666666667, 83.14166666666667, 83.14166666666667, 82.925, 82.925, 82.88333333333334, 82.88333333333334, 83.25833333333334, 83.25833333333334, 83.10833333333333, 83.10833333333333, 83.39166666666667, 83.39166666666667, 83.425, 83.425, 83.33333333333333, 83.33333333333333, 83.075, 83.075, 83.33333333333333, 83.33333333333333, 83.25833333333334, 83.25833333333334, 83.26666666666667, 83.26666666666667, 82.98333333333333, 82.98333333333333, 83.525, 83.525, 84.0, 84.0, 84.18333333333334, 84.18333333333334, 84.24166666666666, 84.24166666666666, 84.05833333333334, 84.05833333333334, 83.80833333333334, 83.80833333333334, 83.74166666666666, 83.74166666666666, 83.69166666666666, 83.69166666666666, 83.95833333333333, 83.95833333333333, 83.5, 83.5, 83.58333333333333, 83.58333333333333, 83.35, 83.35, 83.68333333333334, 83.68333333333334, 84.50833333333334, 84.50833333333334, 84.25, 84.25, 84.00833333333334, 84.00833333333334, 84.075, 84.075, 83.94166666666666, 83.94166666666666, 84.05833333333334, 84.05833333333334, 84.225, 84.225, 84.40833333333333, 84.40833333333333, 84.86666666666666, 84.86666666666666, 84.95, 84.95, 84.96666666666667, 84.96666666666667, 84.95833333333333, 84.95833333333333, 84.975, 84.975, 84.89166666666667, 84.89166666666667, 85.08333333333333, 85.08333333333333, 85.00833333333334, 85.00833333333334, 85.21666666666667, 85.21666666666667, 85.575, 85.575, 85.5, 85.5, 85.54166666666667, 85.54166666666667, 85.33333333333333, 85.33333333333333, 85.30833333333334, 85.30833333333334, 85.08333333333333, 85.08333333333333, 85.28333333333333, 85.28333333333333, 85.225, 85.225, 85.03333333333333, 85.03333333333333, 84.675, 84.675, 85.075, 85.075, 85.01666666666667, 85.01666666666667, 85.13333333333334, 85.13333333333334, 84.88333333333334, 84.88333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.208, Test loss: 0.365, Test accuracy: 86.27
Average accuracy final 10 rounds: 85.96250000000002 

1520.9704575538635
[1.529860019683838, 3.059720039367676, 4.410574913024902, 5.761429786682129, 7.136934041976929, 8.512438297271729, 9.887417316436768, 11.262396335601807, 12.63185429573059, 14.001312255859375, 15.37178659439087, 16.742260932922363, 18.12438654899597, 19.50651216506958, 20.8830144405365, 22.259516716003418, 23.629842042922974, 25.00016736984253, 26.3774094581604, 27.75465154647827, 29.234434127807617, 30.714216709136963, 32.21174240112305, 33.70926809310913, 35.078847885131836, 36.44842767715454, 37.823065757751465, 39.19770383834839, 40.57488965988159, 41.952075481414795, 43.33518171310425, 44.7182879447937, 46.11292839050293, 47.50756883621216, 48.89682388305664, 50.28607892990112, 51.66576099395752, 53.045443058013916, 54.40933966636658, 55.77323627471924, 57.13048839569092, 58.4877405166626, 59.85409617424011, 61.22045183181763, 62.599640130996704, 63.97882843017578, 65.35113978385925, 66.72345113754272, 68.0903058052063, 69.45716047286987, 70.82843828201294, 72.199716091156, 73.57502555847168, 74.95033502578735, 76.32977056503296, 77.70920610427856, 79.0727891921997, 80.43637228012085, 81.80610871315002, 83.1758451461792, 84.5665979385376, 85.957350730896, 87.32600831985474, 88.69466590881348, 90.06582617759705, 91.43698644638062, 92.81022357940674, 94.18346071243286, 95.54426860809326, 96.90507650375366, 98.28274369239807, 99.66041088104248, 101.03747606277466, 102.41454124450684, 103.79628705978394, 105.17803287506104, 106.55105495452881, 107.92407703399658, 109.30373311042786, 110.68338918685913, 112.05631470680237, 113.4292402267456, 114.80399680137634, 116.17875337600708, 117.45326614379883, 118.72777891159058, 119.9934675693512, 121.25915622711182, 122.53347396850586, 123.8077917098999, 125.08313369750977, 126.35847568511963, 127.63069939613342, 128.90292310714722, 130.1786985397339, 131.45447397232056, 132.73197174072266, 134.00946950912476, 135.28055620193481, 136.55164289474487, 137.8238673210144, 139.09609174728394, 140.4487361907959, 141.80138063430786, 143.15101218223572, 144.50064373016357, 145.92021679878235, 147.33978986740112, 148.81826066970825, 150.29673147201538, 151.6831557750702, 153.069580078125, 154.46237921714783, 155.85517835617065, 157.2075490951538, 158.55991983413696, 159.94690656661987, 161.33389329910278, 162.6981074810028, 164.06232166290283, 165.50281286239624, 166.94330406188965, 168.42591977119446, 169.90853548049927, 171.25401091575623, 172.59948635101318, 174.056622505188, 175.5137586593628, 177.03197622299194, 178.5501937866211, 179.93730211257935, 181.3244104385376, 182.67842626571655, 184.0324420928955, 185.41474652290344, 186.79705095291138, 188.17727518081665, 189.55749940872192, 190.92029929161072, 192.2830991744995, 193.6564486026764, 195.02979803085327, 196.40106344223022, 197.77232885360718, 199.15012955665588, 200.5279302597046, 201.914155960083, 203.30038166046143, 204.67712378501892, 206.05386590957642, 207.44038558006287, 208.82690525054932, 210.1943359375, 211.56176662445068, 212.93881797790527, 214.31586933135986, 215.68078589439392, 217.04570245742798, 218.4137032032013, 219.7817039489746, 221.16040968894958, 222.53911542892456, 223.91237497329712, 225.28563451766968, 226.66422939300537, 228.04282426834106, 229.42971110343933, 230.8165979385376, 232.21316814422607, 233.60973834991455, 234.99264931678772, 236.3755602836609, 237.77724242210388, 239.17892456054688, 240.56203293800354, 241.9451413154602, 243.34345245361328, 244.74176359176636, 246.15547132492065, 247.56917905807495, 248.95760846138, 250.34603786468506, 251.73008704185486, 253.11413621902466, 254.51188588142395, 255.90963554382324, 257.323117017746, 258.7365984916687, 260.1013767719269, 261.46615505218506, 262.83416175842285, 264.20216846466064, 265.60955452919006, 267.0169405937195, 268.39495277404785, 269.7729649543762, 271.1672840118408, 272.5616030693054, 273.95573902130127, 275.3498749732971, 277.3806805610657, 279.41148614883423]
[22.016666666666666, 22.016666666666666, 35.333333333333336, 35.333333333333336, 45.2, 45.2, 53.80833333333333, 53.80833333333333, 56.21666666666667, 56.21666666666667, 58.041666666666664, 58.041666666666664, 63.525, 63.525, 68.55833333333334, 68.55833333333334, 70.1, 70.1, 69.6, 69.6, 70.95, 70.95, 71.775, 71.775, 71.18333333333334, 71.18333333333334, 73.31666666666666, 73.31666666666666, 76.04166666666667, 76.04166666666667, 77.11666666666666, 77.11666666666666, 77.95833333333333, 77.95833333333333, 78.05, 78.05, 78.275, 78.275, 78.5, 78.5, 79.7, 79.7, 79.90833333333333, 79.90833333333333, 79.70833333333333, 79.70833333333333, 79.8, 79.8, 80.50833333333334, 80.50833333333334, 81.10833333333333, 81.10833333333333, 80.88333333333334, 80.88333333333334, 80.63333333333334, 80.63333333333334, 81.14166666666667, 81.14166666666667, 81.38333333333334, 81.38333333333334, 81.75, 81.75, 81.475, 81.475, 82.125, 82.125, 81.75833333333334, 81.75833333333334, 82.025, 82.025, 82.13333333333334, 82.13333333333334, 82.08333333333333, 82.08333333333333, 83.125, 83.125, 82.61666666666666, 82.61666666666666, 82.625, 82.625, 82.8, 82.8, 82.725, 82.725, 82.625, 82.625, 83.11666666666666, 83.11666666666666, 83.55, 83.55, 83.45, 83.45, 84.025, 84.025, 84.08333333333333, 84.08333333333333, 83.91666666666667, 83.91666666666667, 83.96666666666667, 83.96666666666667, 83.71666666666667, 83.71666666666667, 84.125, 84.125, 84.08333333333333, 84.08333333333333, 84.275, 84.275, 84.325, 84.325, 84.28333333333333, 84.28333333333333, 84.45833333333333, 84.45833333333333, 84.35833333333333, 84.35833333333333, 84.45833333333333, 84.45833333333333, 84.41666666666667, 84.41666666666667, 84.75, 84.75, 84.85833333333333, 84.85833333333333, 84.625, 84.625, 85.025, 85.025, 85.09166666666667, 85.09166666666667, 84.91666666666667, 84.91666666666667, 85.14166666666667, 85.14166666666667, 84.88333333333334, 84.88333333333334, 84.85833333333333, 84.85833333333333, 85.29166666666667, 85.29166666666667, 85.525, 85.525, 85.125, 85.125, 85.79166666666667, 85.79166666666667, 84.93333333333334, 84.93333333333334, 85.125, 85.125, 85.38333333333334, 85.38333333333334, 85.33333333333333, 85.33333333333333, 85.65833333333333, 85.65833333333333, 85.99166666666666, 85.99166666666666, 85.4, 85.4, 85.425, 85.425, 85.75, 85.75, 85.81666666666666, 85.81666666666666, 85.65, 85.65, 85.45, 85.45, 85.875, 85.875, 85.425, 85.425, 86.025, 86.025, 85.93333333333334, 85.93333333333334, 86.04166666666667, 86.04166666666667, 86.11666666666666, 86.11666666666666, 86.10833333333333, 86.10833333333333, 86.30833333333334, 86.30833333333334, 85.975, 85.975, 85.375, 85.375, 85.66666666666667, 85.66666666666667, 85.78333333333333, 85.78333333333333, 86.075, 86.075, 86.19166666666666, 86.19166666666666, 86.025, 86.025, 86.26666666666667, 86.26666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.189, Test loss: 0.327, Test accuracy: 87.18
Average accuracy final 10 rounds: 87.00666666666667
1719.5799264907837
[2.2669458389282227, 4.533891677856445, 6.313511848449707, 8.093132019042969, 9.833614826202393, 11.574097633361816, 13.336055994033813, 15.09801435470581, 16.86099863052368, 18.623982906341553, 20.36741805076599, 22.11085319519043, 23.901658296585083, 25.692463397979736, 27.46403670310974, 29.235610008239746, 30.995617628097534, 32.75562524795532, 34.521700382232666, 36.28777551651001, 38.058411836624146, 39.82904815673828, 41.572784423828125, 43.31652069091797, 45.06937861442566, 46.82223653793335, 48.59782099723816, 50.37340545654297, 52.161412715911865, 53.94941997528076, 55.737289905548096, 57.52515983581543, 59.311198234558105, 61.09723663330078, 62.90228080749512, 64.70732498168945, 66.49467873573303, 68.28203248977661, 70.06787323951721, 71.85371398925781, 73.63450741767883, 75.41530084609985, 77.1972188949585, 78.97913694381714, 80.76583313941956, 82.55252933502197, 84.32819151878357, 86.10385370254517, 87.88832139968872, 89.67278909683228, 91.45508122444153, 93.23737335205078, 95.01427745819092, 96.79118156433105, 98.59409022331238, 100.3969988822937, 102.16486644744873, 103.93273401260376, 105.70451283454895, 107.47629165649414, 109.27809953689575, 111.07990741729736, 112.86427116394043, 114.6486349105835, 116.41496300697327, 118.18129110336304, 119.96028685569763, 121.73928260803223, 123.51081085205078, 125.28233909606934, 127.04929661750793, 128.81625413894653, 130.59127163887024, 132.36628913879395, 134.14362978935242, 135.9209704399109, 137.68470859527588, 139.44844675064087, 141.2397232055664, 143.03099966049194, 144.82947850227356, 146.62795734405518, 148.4072539806366, 150.18655061721802, 151.9730830192566, 153.75961542129517, 155.54275226593018, 157.32588911056519, 159.1148943901062, 160.90389966964722, 162.68398571014404, 164.46407175064087, 166.2279667854309, 167.99186182022095, 169.77329397201538, 171.55472612380981, 173.35176372528076, 175.1488013267517, 176.9009771347046, 178.65315294265747, 180.43143153190613, 182.20971012115479, 183.99821734428406, 185.78672456741333, 187.5502951145172, 189.3138656616211, 191.04861402511597, 192.78336238861084, 194.57919192314148, 196.37502145767212, 198.17861127853394, 199.98220109939575, 201.7576515674591, 203.53310203552246, 205.32336378097534, 207.11362552642822, 208.93204641342163, 210.75046730041504, 212.5453643798828, 214.3402614593506, 216.13117671012878, 217.92209196090698, 219.72209858894348, 221.52210521697998, 223.26250433921814, 225.0029034614563, 226.76006960868835, 228.5172357559204, 230.2968249320984, 232.07641410827637, 233.84998869895935, 235.62356328964233, 237.3919460773468, 239.16032886505127, 240.71879124641418, 242.2772536277771, 243.83437871932983, 245.39150381088257, 246.93872332572937, 248.48594284057617, 250.03349709510803, 251.5810513496399, 253.13081288337708, 254.68057441711426, 256.23776865005493, 257.7949628829956, 259.32730507850647, 260.85964727401733, 262.39706158638, 263.9344758987427, 265.4868357181549, 267.03919553756714, 268.5873849391937, 270.1355743408203, 271.6776969432831, 273.21981954574585, 274.78535294532776, 276.35088634490967, 277.9049334526062, 279.45898056030273, 281.01801681518555, 282.57705307006836, 284.12646889686584, 285.67588472366333, 287.2217507362366, 288.7676167488098, 290.3289294242859, 291.89024209976196, 293.4388253688812, 294.9874086380005, 296.5205810070038, 298.0537533760071, 299.6129972934723, 301.1722412109375, 302.72594118118286, 304.2796411514282, 305.83365058898926, 307.3876600265503, 309.09158301353455, 310.7955060005188, 312.50340962409973, 314.21131324768066, 315.912801027298, 317.6142888069153, 319.3081512451172, 321.0020136833191, 322.71287870407104, 324.423743724823, 326.12446880340576, 327.8251938819885, 329.4340908527374, 331.0429878234863, 332.58756017684937, 334.1321325302124, 335.69692492485046, 337.2617173194885, 338.8124566078186, 340.3631958961487, 341.89791798591614, 343.4326400756836, 345.50197291374207, 347.57130575180054]
[20.65, 20.65, 39.21666666666667, 39.21666666666667, 53.391666666666666, 53.391666666666666, 56.95, 56.95, 64.625, 64.625, 70.20833333333333, 70.20833333333333, 69.19166666666666, 69.19166666666666, 71.74166666666666, 71.74166666666666, 71.13333333333334, 71.13333333333334, 75.58333333333333, 75.58333333333333, 76.96666666666667, 76.96666666666667, 76.91666666666667, 76.91666666666667, 77.65833333333333, 77.65833333333333, 78.05, 78.05, 78.74166666666666, 78.74166666666666, 79.35, 79.35, 79.68333333333334, 79.68333333333334, 80.25, 80.25, 80.6, 80.6, 80.4, 80.4, 80.95833333333333, 80.95833333333333, 80.59166666666667, 80.59166666666667, 80.98333333333333, 80.98333333333333, 81.29166666666667, 81.29166666666667, 81.46666666666667, 81.46666666666667, 81.63333333333334, 81.63333333333334, 82.03333333333333, 82.03333333333333, 82.21666666666667, 82.21666666666667, 82.55, 82.55, 82.33333333333333, 82.33333333333333, 81.95, 81.95, 82.675, 82.675, 82.46666666666667, 82.46666666666667, 83.33333333333333, 83.33333333333333, 83.58333333333333, 83.58333333333333, 83.35833333333333, 83.35833333333333, 83.475, 83.475, 84.1, 84.1, 84.1, 84.1, 84.01666666666667, 84.01666666666667, 83.74166666666666, 83.74166666666666, 83.95, 83.95, 84.24166666666666, 84.24166666666666, 84.58333333333333, 84.58333333333333, 84.675, 84.675, 84.68333333333334, 84.68333333333334, 84.90833333333333, 84.90833333333333, 84.70833333333333, 84.70833333333333, 84.8, 84.8, 84.54166666666667, 84.54166666666667, 84.86666666666666, 84.86666666666666, 85.14166666666667, 85.14166666666667, 85.50833333333334, 85.50833333333334, 85.65833333333333, 85.65833333333333, 85.68333333333334, 85.68333333333334, 85.71666666666667, 85.71666666666667, 85.89166666666667, 85.89166666666667, 85.95, 85.95, 86.025, 86.025, 85.79166666666667, 85.79166666666667, 86.04166666666667, 86.04166666666667, 85.925, 85.925, 86.225, 86.225, 85.825, 85.825, 85.76666666666667, 85.76666666666667, 86.15833333333333, 86.15833333333333, 86.55833333333334, 86.55833333333334, 86.58333333333333, 86.58333333333333, 86.25, 86.25, 86.16666666666667, 86.16666666666667, 86.325, 86.325, 86.73333333333333, 86.73333333333333, 86.15833333333333, 86.15833333333333, 86.30833333333334, 86.30833333333334, 86.33333333333333, 86.33333333333333, 86.59166666666667, 86.59166666666667, 86.73333333333333, 86.73333333333333, 86.55833333333334, 86.55833333333334, 86.83333333333333, 86.83333333333333, 86.74166666666666, 86.74166666666666, 86.95, 86.95, 86.8, 86.8, 86.73333333333333, 86.73333333333333, 86.81666666666666, 86.81666666666666, 86.88333333333334, 86.88333333333334, 86.975, 86.975, 86.85, 86.85, 87.125, 87.125, 86.9, 86.9, 86.55, 86.55, 86.89166666666667, 86.89166666666667, 86.975, 86.975, 87.34166666666667, 87.34166666666667, 86.99166666666666, 86.99166666666666, 87.16666666666667, 87.16666666666667, 86.93333333333334, 86.93333333333334, 86.84166666666667, 86.84166666666667, 87.00833333333334, 87.00833333333334, 86.88333333333334, 86.88333333333334, 87.03333333333333, 87.03333333333333, 87.18333333333334, 87.18333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.155, Test loss: 0.990, Test accuracy: 69.47
Average accuracy final 10 rounds: 63.991666666666674
2515.0785512924194
[4.161343097686768, 8.117095947265625, 12.018908739089966, 15.931092023849487, 19.849371433258057, 23.807730674743652, 27.72716522216797, 31.67065691947937, 35.58034038543701, 39.53915476799011, 43.45312976837158, 47.36795783042908, 51.28205347061157, 55.199414014816284, 59.11503529548645, 63.0035765171051, 66.91819214820862, 70.78178429603577, 74.60686945915222, 78.31766438484192, 82.06874251365662, 85.81103253364563, 89.69158744812012, 93.54508638381958, 97.38719916343689, 101.21452951431274, 105.06924724578857, 108.91899609565735, 112.78214955329895, 116.51426720619202, 120.24173951148987, 123.98823165893555, 127.7531418800354, 131.50834012031555, 135.35641384124756, 139.21918606758118, 142.9731569290161, 146.82678651809692, 150.711487531662, 154.60739183425903, 158.46028184890747, 162.18794441223145, 165.94320273399353, 169.6779236793518, 173.57131266593933, 177.4678394794464, 181.3361668586731, 185.26804852485657, 189.25591897964478, 193.1868109703064, 197.06912922859192, 200.97066831588745, 204.84711265563965, 208.6485903263092, 212.39854168891907, 216.04150295257568, 219.7188560962677, 223.49117851257324, 227.231787443161, 230.99947476387024, 234.77939653396606, 238.4518642425537, 242.2322542667389, 246.01384472846985, 249.72905349731445, 253.5114622116089, 257.2600419521332, 260.9600234031677, 264.68994879722595, 268.42260551452637, 271.9173729419708, 275.42875504493713, 278.9190492630005, 282.44244623184204, 285.5826258659363, 288.74706864356995, 291.8759300708771, 295.00239658355713, 298.12441182136536, 301.2751965522766, 304.43345737457275, 307.5625755786896, 310.6928448677063, 313.8212296962738, 316.9573040008545, 320.08604669570923, 323.2180950641632, 326.33634877204895, 329.47288727760315, 332.6011176109314, 335.72727060317993, 338.854195356369, 341.9666049480438, 345.0841705799103, 348.2090575695038, 351.2654855251312, 354.32179069519043, 357.4205892086029, 360.5333139896393, 363.6139237880707, 366.17045426368713]
[22.783333333333335, 30.608333333333334, 30.633333333333333, 39.208333333333336, 37.666666666666664, 41.475, 44.30833333333333, 42.09166666666667, 49.391666666666666, 49.19166666666667, 51.766666666666666, 41.6, 44.74166666666667, 49.65833333333333, 44.06666666666667, 53.68333333333333, 55.733333333333334, 48.858333333333334, 46.69166666666667, 54.541666666666664, 47.75833333333333, 56.63333333333333, 53.15, 60.291666666666664, 60.63333333333333, 54.958333333333336, 61.44166666666667, 58.30833333333333, 58.85, 58.266666666666666, 60.875, 51.30833333333333, 53.458333333333336, 54.175, 54.225, 57.65, 58.083333333333336, 64.375, 63.166666666666664, 58.05, 63.0, 58.291666666666664, 56.99166666666667, 62.475, 63.333333333333336, 59.88333333333333, 66.6, 65.41666666666667, 61.95, 66.35, 52.65, 62.975, 63.666666666666664, 65.15, 60.475, 63.90833333333333, 66.83333333333333, 64.08333333333333, 59.125, 58.8, 67.24166666666666, 62.28333333333333, 61.15, 62.93333333333333, 64.73333333333333, 61.25833333333333, 66.15, 62.983333333333334, 66.49166666666666, 63.891666666666666, 65.975, 67.29166666666667, 66.74166666666666, 66.43333333333334, 61.041666666666664, 67.89166666666667, 67.38333333333334, 68.88333333333334, 62.2, 59.18333333333333, 58.4, 62.35, 67.225, 60.666666666666664, 60.625, 62.75, 64.79166666666667, 65.90833333333333, 66.68333333333334, 64.6, 55.983333333333334, 65.2, 65.96666666666667, 64.23333333333333, 64.73333333333333, 66.975, 64.05833333333334, 65.525, 61.88333333333333, 65.35833333333333, 69.46666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 15.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 15.00
Average accuracy final 10 rounds: 15.0 

Average global accuracy final 10 rounds: 15.0 

5071.844216108322
[1.825524091720581, 3.440183401107788, 5.03402042388916, 6.627469062805176, 8.228342771530151, 9.793432474136353, 11.382448196411133, 12.976348400115967, 14.592743158340454, 16.0117826461792, 17.42351245880127, 18.827515602111816, 20.223567962646484, 21.670145750045776, 23.08726191520691, 24.52298641204834, 25.913421869277954, 27.413968563079834, 28.879634141921997, 30.327072143554688, 31.76059341430664, 33.201136350631714, 34.635645151138306, 36.10853958129883, 37.52259945869446, 38.9280424118042, 40.36030960083008, 41.792949199676514, 43.2799072265625, 44.71761083602905, 46.15867567062378, 47.57931971549988, 48.996222496032715, 50.4013786315918, 51.858278036117554, 53.31080770492554, 54.79376983642578, 56.277912855148315, 57.703460454940796, 59.15933012962341, 60.60886526107788, 62.058678150177, 63.48404884338379, 65.07279014587402, 66.52338075637817, 67.96178722381592, 69.39459252357483, 70.78823280334473, 72.2136721611023, 73.62691307067871, 75.12073612213135, 76.52533030509949, 77.95466637611389, 79.38286256790161, 80.79778909683228, 82.19950246810913, 83.62514877319336, 85.0430645942688, 86.4586033821106, 87.87810182571411, 89.28963279724121, 90.70177245140076, 92.12044954299927, 93.55469512939453, 95.05991244316101, 96.51468801498413, 97.9734423160553, 99.43573188781738, 100.85385179519653, 102.28860545158386, 103.75104475021362, 105.16930317878723, 106.58926177024841, 108.0133900642395, 109.3966817855835, 110.80343413352966, 112.22085738182068, 113.63328099250793, 115.03746247291565, 116.4608063697815, 117.87512612342834, 119.31835174560547, 120.92411351203918, 122.36894869804382, 123.8412880897522, 125.30953860282898, 126.76863741874695, 128.18529605865479, 129.63572025299072, 131.07529473304749, 132.52127361297607, 133.94778060913086, 135.4272518157959, 136.85132384300232, 138.25868272781372, 139.68384981155396, 141.13158011436462, 142.5490438938141, 143.96437096595764, 145.3866250514984, 146.8390510082245, 148.2386600971222, 149.6579623222351, 151.0808572769165, 152.51179909706116, 153.92877531051636, 155.35339832305908, 156.79629468917847, 158.21268439292908, 159.64911603927612, 161.07383513450623, 162.5024378299713, 163.9360749721527, 165.4205355644226, 166.84207463264465, 168.30208086967468, 169.73413562774658, 171.17270803451538, 172.60450100898743, 174.0334632396698, 175.4584674835205, 176.89478707313538, 178.31529688835144, 179.72822952270508, 181.1651201248169, 182.59391617774963, 184.00070190429688, 185.4243974685669, 186.83695769309998, 188.26067209243774, 189.68680119514465, 191.1134693622589, 192.5258686542511, 193.94637894630432, 195.36353158950806, 196.8096079826355, 198.22599577903748, 199.6229705810547, 201.11185550689697, 202.60501909255981, 204.06591892242432, 205.5021092891693, 206.9189281463623, 208.3509304523468, 209.77407145500183, 211.16854047775269, 212.58540153503418, 214.01726841926575, 215.44655060768127, 216.88849449157715, 218.3052897453308, 219.72190833091736, 221.1410596370697, 222.56477546691895, 223.9647238254547, 225.39507484436035, 226.88682651519775, 228.35141038894653, 229.83206272125244, 231.3160276412964, 232.80267429351807, 234.28931665420532, 235.8142991065979, 237.31837439537048, 238.79913663864136, 240.29805755615234, 241.81284999847412, 243.288334608078, 244.7745246887207, 246.26823663711548, 247.78327798843384, 249.29880952835083, 250.9988877773285, 252.72930192947388, 254.32745695114136, 255.94620060920715, 257.55500054359436, 259.1402542591095, 260.72254610061646, 262.31776881217957, 264.02573466300964, 265.76329016685486, 267.4170174598694, 268.99908447265625, 270.52704882621765, 272.0947585105896, 273.88388299942017, 275.5805416107178, 277.28313875198364, 278.9783458709717, 280.7097544670105, 282.4085991382599, 284.1515738964081, 285.87260341644287, 287.6042127609253, 289.155730009079, 290.87804675102234, 292.63766741752625, 294.2192621231079, 295.86727023124695, 297.47528982162476, 299.08784651756287, 300.80022287368774, 302.54082202911377, 304.33351254463196, 305.99651885032654, 307.57560539245605, 309.17544174194336, 310.82170486450195, 312.4354782104492, 313.99484610557556, 315.64708256721497, 317.31250190734863, 318.9280662536621, 320.5314197540283, 322.0759708881378, 323.84343123435974, 325.63190937042236, 327.35794949531555, 329.1518642902374, 330.8650863170624, 332.49959230422974, 334.05455780029297, 335.636513710022, 337.2135396003723, 338.83069252967834, 340.39504194259644, 341.9978840351105, 343.58315229415894, 345.22385454177856, 346.89040637016296, 348.5360953807831, 350.2547142505646, 351.948992729187, 353.63389110565186, 355.33009004592896, 357.01449847221375, 358.7173602581024, 360.4073648452759, 362.10716366767883, 363.86476945877075, 365.585905790329, 367.26993799209595, 368.9799563884735, 370.7421090602875, 372.4389650821686, 374.11966013908386, 375.8427610397339, 377.58233761787415, 379.2961332798004, 381.0044856071472, 382.7282061576843, 384.48965764045715, 386.05765557289124, 387.6027178764343, 389.2006013393402, 390.85821318626404, 392.41003465652466, 393.976016998291, 395.56717586517334, 397.24836778640747, 398.9219241142273, 400.5890567302704, 402.3043351173401, 404.0796813964844, 405.8438329696655, 407.5142147541046, 409.15037202835083, 410.73567175865173, 412.31743836402893, 413.8611807823181, 415.4267132282257, 417.015745639801, 418.6574954986572, 420.38880825042725, 422.1195216178894, 423.7615873813629, 425.45161414146423, 427.2017948627472, 428.9075539112091, 430.566232919693, 432.08998680114746, 433.6649634838104, 435.2816843986511, 436.9152717590332, 438.4954352378845, 440.1077754497528, 441.7157952785492, 443.34982085227966, 444.9144468307495, 446.50406289100647, 448.0808057785034, 449.6651220321655, 451.22539806365967, 452.8015456199646, 454.4023292064667, 455.9952538013458, 457.5131299495697, 459.1296899318695, 460.73136591911316, 463.4121632575989]
[11.666666666666666, 11.666666666666666, 21.666666666666668, 20.0, 16.666666666666668, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.116, Test loss: 0.486, Test accuracy: 86.93
Final Round, Global train loss: 0.116, Global test loss: 1.349, Global test accuracy: 63.53
Average accuracy final 10 rounds: 86.79222222222222 

Average global accuracy final 10 rounds: 63.42333333333333 

3312.2975890636444
[2.7555899620056152, 5.5111799240112305, 7.663044691085815, 9.8149094581604, 11.987767457962036, 14.160625457763672, 16.475638151168823, 18.790650844573975, 21.18889093399048, 23.587131023406982, 25.96315360069275, 28.339176177978516, 30.804638385772705, 33.270100593566895, 35.633827924728394, 37.99755525588989, 40.36030411720276, 42.723052978515625, 45.10869646072388, 47.49433994293213, 49.92839169502258, 52.36244344711304, 54.61593222618103, 56.86942100524902, 59.05317735671997, 61.23693370819092, 63.392616987228394, 65.54830026626587, 67.73310136795044, 69.91790246963501, 72.16664099693298, 74.41537952423096, 76.51560759544373, 78.6158356666565, 80.8602831363678, 83.1047306060791, 85.35307359695435, 87.60141658782959, 89.85179924964905, 92.1021819114685, 94.45643854141235, 96.8106951713562, 99.15646529197693, 101.50223541259766, 103.77400922775269, 106.04578304290771, 108.30374956130981, 110.56171607971191, 112.8525071144104, 115.14329814910889, 117.44097423553467, 119.73865032196045, 122.05524373054504, 124.37183713912964, 126.67649340629578, 128.9811496734619, 131.30777621269226, 133.6344027519226, 136.00418996810913, 138.37397718429565, 140.67803978919983, 142.982102394104, 145.3115255832672, 147.64094877243042, 149.91664052009583, 152.19233226776123, 154.50386953353882, 156.8154067993164, 159.09442377090454, 161.37344074249268, 163.55224561691284, 165.731050491333, 167.83928656578064, 169.94752264022827, 172.11817741394043, 174.2888321876526, 176.4092185497284, 178.5296049118042, 180.6776478290558, 182.82569074630737, 184.98044109344482, 187.13519144058228, 189.2760488986969, 191.41690635681152, 193.572185754776, 195.72746515274048, 197.96343612670898, 200.1994071006775, 202.36757135391235, 204.53573560714722, 206.68846440315247, 208.84119319915771, 211.0700249671936, 213.2988567352295, 215.44095921516418, 217.58306169509888, 219.71460151672363, 221.8461413383484, 223.9317021369934, 226.01726293563843, 228.1639485359192, 230.31063413619995, 232.48164105415344, 234.65264797210693, 236.82540035247803, 238.99815273284912, 241.1304702758789, 243.2627878189087, 245.39048171043396, 247.51817560195923, 249.68285465240479, 251.84753370285034, 254.19888854026794, 256.55024337768555, 258.9084975719452, 261.26675176620483, 263.57617473602295, 265.88559770584106, 268.21472358703613, 270.5438494682312, 272.8923578262329, 275.2408661842346, 277.52740144729614, 279.81393671035767, 282.0949139595032, 284.3758912086487, 286.76665234565735, 289.157413482666, 291.4621367454529, 293.76686000823975, 296.0416009426117, 298.31634187698364, 300.6668472290039, 303.01735258102417, 305.2512581348419, 307.48516368865967, 309.6058249473572, 311.7264862060547, 313.85045528411865, 315.9744243621826, 318.1096234321594, 320.24482250213623, 322.3657076358795, 324.4865927696228, 326.6413097381592, 328.79602670669556, 330.9519610404968, 333.1078953742981, 335.2455940246582, 337.3832926750183, 339.5206820964813, 341.65807151794434, 343.8243148326874, 345.9905581474304, 348.3410358428955, 350.6915135383606, 353.03407049179077, 355.37662744522095, 357.6946430206299, 360.0126585960388, 362.36161255836487, 364.7105665206909, 367.0336289405823, 369.35669136047363, 371.68198251724243, 374.00727367401123, 376.3271496295929, 378.64702558517456, 380.8102135658264, 382.97340154647827, 385.1422281265259, 387.3110547065735, 389.45718359947205, 391.6033124923706, 393.72552251815796, 395.8477325439453, 397.99403285980225, 400.1403331756592, 402.29385805130005, 404.4473829269409, 406.62246918678284, 408.79755544662476, 410.97046089172363, 413.1433663368225, 415.31324768066406, 417.4831290245056, 419.66868805885315, 421.8542470932007, 424.24202060699463, 426.6297941207886, 428.77619552612305, 430.9225969314575, 433.1724753379822, 435.42235374450684, 437.5825343132019, 439.742714881897, 441.8775577545166, 444.01240062713623, 446.1603569984436, 448.308313369751, 450.6905641555786, 453.07281494140625]
[25.2, 25.2, 46.11666666666667, 46.11666666666667, 56.05555555555556, 56.05555555555556, 64.33888888888889, 64.33888888888889, 67.96666666666667, 67.96666666666667, 68.32777777777778, 68.32777777777778, 69.57222222222222, 69.57222222222222, 70.47222222222223, 70.47222222222223, 72.93333333333334, 72.93333333333334, 74.98333333333333, 74.98333333333333, 79.19444444444444, 79.19444444444444, 79.6, 79.6, 80.05555555555556, 80.05555555555556, 80.57777777777778, 80.57777777777778, 80.07222222222222, 80.07222222222222, 80.47222222222223, 80.47222222222223, 81.0, 81.0, 80.53888888888889, 80.53888888888889, 81.31666666666666, 81.31666666666666, 81.36111111111111, 81.36111111111111, 81.82222222222222, 81.82222222222222, 81.97777777777777, 81.97777777777777, 82.68888888888888, 82.68888888888888, 83.01666666666667, 83.01666666666667, 83.25555555555556, 83.25555555555556, 83.57222222222222, 83.57222222222222, 84.07222222222222, 84.07222222222222, 84.31111111111112, 84.31111111111112, 84.16666666666667, 84.16666666666667, 83.84444444444445, 83.84444444444445, 84.38888888888889, 84.38888888888889, 84.52222222222223, 84.52222222222223, 84.39444444444445, 84.39444444444445, 84.90555555555555, 84.90555555555555, 84.73333333333333, 84.73333333333333, 85.04444444444445, 85.04444444444445, 84.95555555555555, 84.95555555555555, 85.21666666666667, 85.21666666666667, 85.27222222222223, 85.27222222222223, 84.88888888888889, 84.88888888888889, 84.31666666666666, 84.31666666666666, 84.85555555555555, 84.85555555555555, 85.35, 85.35, 85.95, 85.95, 85.66666666666667, 85.66666666666667, 85.60555555555555, 85.60555555555555, 85.70555555555555, 85.70555555555555, 85.80555555555556, 85.80555555555556, 85.91111111111111, 85.91111111111111, 85.96666666666667, 85.96666666666667, 85.9888888888889, 85.9888888888889, 85.55555555555556, 85.55555555555556, 85.72777777777777, 85.72777777777777, 86.09444444444445, 86.09444444444445, 86.1, 86.1, 86.47777777777777, 86.47777777777777, 86.21666666666667, 86.21666666666667, 86.32222222222222, 86.32222222222222, 86.08888888888889, 86.08888888888889, 86.52777777777777, 86.52777777777777, 86.56111111111112, 86.56111111111112, 86.14444444444445, 86.14444444444445, 86.18333333333334, 86.18333333333334, 86.51666666666667, 86.51666666666667, 86.32222222222222, 86.32222222222222, 86.08888888888889, 86.08888888888889, 86.16666666666667, 86.16666666666667, 86.1, 86.1, 86.37222222222222, 86.37222222222222, 86.46111111111111, 86.46111111111111, 86.45, 86.45, 86.76666666666667, 86.76666666666667, 86.7611111111111, 86.7611111111111, 86.63888888888889, 86.63888888888889, 86.68888888888888, 86.68888888888888, 86.62777777777778, 86.62777777777778, 86.55555555555556, 86.55555555555556, 86.92222222222222, 86.92222222222222, 86.93333333333334, 86.93333333333334, 86.72777777777777, 86.72777777777777, 86.48333333333333, 86.48333333333333, 86.48333333333333, 86.48333333333333, 86.83333333333333, 86.83333333333333, 86.62222222222222, 86.62222222222222, 86.72777777777777, 86.72777777777777, 86.81666666666666, 86.81666666666666, 86.72222222222223, 86.72222222222223, 86.9, 86.9, 86.99444444444444, 86.99444444444444, 86.87777777777778, 86.87777777777778, 86.67222222222222, 86.67222222222222, 86.53333333333333, 86.53333333333333, 86.64444444444445, 86.64444444444445, 86.8, 86.8, 87.12222222222222, 87.12222222222222, 87.24444444444444, 87.24444444444444, 86.91111111111111, 86.91111111111111, 86.63888888888889, 86.63888888888889, 86.68888888888888, 86.68888888888888, 86.66666666666667, 86.66666666666667, 86.92777777777778, 86.92777777777778]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.171, Test loss: 0.351, Test accuracy: 87.67
Average accuracy final 10 rounds: 87.72277777777778 

2623.6088433265686
[2.452488660812378, 4.904977321624756, 7.097949504852295, 9.290921688079834, 11.505953311920166, 13.720984935760498, 15.922779083251953, 18.124573230743408, 20.271840572357178, 22.419107913970947, 24.59571099281311, 26.772314071655273, 28.977144718170166, 31.18197536468506, 33.35524845123291, 35.52852153778076, 37.692381143569946, 39.85624074935913, 42.05208134651184, 44.24792194366455, 46.41289258003235, 48.57786321640015, 50.77048945426941, 52.96311569213867, 55.15131878852844, 57.33952188491821, 59.53638815879822, 61.73325443267822, 63.933125257492065, 66.13299608230591, 68.29011130332947, 70.44722652435303, 72.60909175872803, 74.77095699310303, 76.91779732704163, 79.06463766098022, 81.24844408035278, 83.43225049972534, 85.50944995880127, 87.5866494178772, 89.6452043056488, 91.70375919342041, 93.75995135307312, 95.81614351272583, 97.88850903511047, 99.96087455749512, 102.02958059310913, 104.09828662872314, 106.32780361175537, 108.5573205947876, 110.63830542564392, 112.71929025650024, 114.76405429840088, 116.80881834030151, 118.87849354743958, 120.94816875457764, 123.03136372566223, 125.11455869674683, 127.20469689369202, 129.2948350906372, 131.35728693008423, 133.41973876953125, 135.47835159301758, 137.5369644165039, 139.64080905914307, 141.74465370178223, 143.98353505134583, 146.22241640090942, 148.4464201927185, 150.6704239845276, 152.88029265403748, 155.09016132354736, 157.327388048172, 159.56461477279663, 161.82959055900574, 164.09456634521484, 166.3101360797882, 168.52570581436157, 170.73100519180298, 172.93630456924438, 175.1362476348877, 177.336190700531, 179.5337929725647, 181.7313952445984, 183.84963726997375, 185.96787929534912, 188.17788934707642, 190.3878993988037, 192.57110929489136, 194.754319190979, 196.94422364234924, 199.13412809371948, 201.21603870391846, 203.29794931411743, 205.3956081867218, 207.49326705932617, 209.56040477752686, 211.62754249572754, 213.64965677261353, 215.6717710494995, 217.92268896102905, 220.1736068725586, 222.43232226371765, 224.6910376548767, 226.92344450950623, 229.15585136413574, 231.3728265762329, 233.58980178833008, 235.80873918533325, 238.02767658233643, 240.32463192939758, 242.62158727645874, 244.89431643486023, 247.16704559326172, 249.393798828125, 251.62055206298828, 253.84385418891907, 256.06715631484985, 258.38657903671265, 260.70600175857544, 263.0533103942871, 265.4006190299988, 267.6356439590454, 269.87066888809204, 272.0682475566864, 274.26582622528076, 276.4862678050995, 278.7067093849182, 280.9414417743683, 283.17617416381836, 285.30795884132385, 287.43974351882935, 289.6278030872345, 291.81586265563965, 293.9497911930084, 296.0837197303772, 298.27994871139526, 300.47617769241333, 302.77170062065125, 305.06722354888916, 307.3075621128082, 309.5479006767273, 311.7991592884064, 314.05041790008545, 316.3011963367462, 318.551974773407, 320.7750577926636, 322.99814081192017, 325.2119755744934, 327.42581033706665, 329.6175465583801, 331.8092827796936, 334.04389572143555, 336.2785086631775, 338.46812319755554, 340.6577377319336, 342.9091172218323, 345.16049671173096, 347.46148347854614, 349.7624702453613, 351.9847548007965, 354.2070393562317, 356.4202506542206, 358.6334619522095, 360.82270193099976, 363.01194190979004, 365.24698781967163, 367.4820337295532, 369.7837278842926, 372.085422039032, 374.2787277698517, 376.4720335006714, 378.72124695777893, 380.9704604148865, 383.1501133441925, 385.32976627349854, 387.5059051513672, 389.68204402923584, 391.85701608657837, 394.0319881439209, 396.20191764831543, 398.37184715270996, 400.6005082130432, 402.82916927337646, 405.04588198661804, 407.2625946998596, 409.51158380508423, 411.76057291030884, 413.8614196777344, 415.9622664451599, 418.08924651145935, 420.2162265777588, 422.311753988266, 424.4072813987732, 426.50734210014343, 428.6074028015137, 430.66495180130005, 432.7225008010864, 434.79939675331116, 436.8762927055359, 439.05035376548767, 441.22441482543945]
[23.06111111111111, 23.06111111111111, 37.71666666666667, 37.71666666666667, 47.0, 47.0, 60.416666666666664, 60.416666666666664, 60.794444444444444, 60.794444444444444, 64.80555555555556, 64.80555555555556, 67.06111111111112, 67.06111111111112, 73.31111111111112, 73.31111111111112, 75.17222222222222, 75.17222222222222, 75.8, 75.8, 75.00555555555556, 75.00555555555556, 74.63333333333334, 74.63333333333334, 76.39444444444445, 76.39444444444445, 76.38888888888889, 76.38888888888889, 76.32777777777778, 76.32777777777778, 78.13333333333334, 78.13333333333334, 77.36111111111111, 77.36111111111111, 77.89444444444445, 77.89444444444445, 77.73333333333333, 77.73333333333333, 78.48333333333333, 78.48333333333333, 80.52222222222223, 80.52222222222223, 80.50555555555556, 80.50555555555556, 81.81111111111112, 81.81111111111112, 82.03333333333333, 82.03333333333333, 81.67222222222222, 81.67222222222222, 82.56111111111112, 82.56111111111112, 82.79444444444445, 82.79444444444445, 83.38333333333334, 83.38333333333334, 83.62222222222222, 83.62222222222222, 83.91666666666667, 83.91666666666667, 84.02222222222223, 84.02222222222223, 83.75, 83.75, 83.58333333333333, 83.58333333333333, 84.2611111111111, 84.2611111111111, 84.42222222222222, 84.42222222222222, 84.38333333333334, 84.38333333333334, 84.75555555555556, 84.75555555555556, 84.88333333333334, 84.88333333333334, 85.37222222222222, 85.37222222222222, 85.20555555555555, 85.20555555555555, 85.46111111111111, 85.46111111111111, 85.37777777777778, 85.37777777777778, 85.51666666666667, 85.51666666666667, 85.61666666666666, 85.61666666666666, 85.39444444444445, 85.39444444444445, 85.7611111111111, 85.7611111111111, 85.90555555555555, 85.90555555555555, 85.96111111111111, 85.96111111111111, 86.22777777777777, 86.22777777777777, 85.78888888888889, 85.78888888888889, 86.00555555555556, 86.00555555555556, 86.08888888888889, 86.08888888888889, 86.2388888888889, 86.2388888888889, 86.44444444444444, 86.44444444444444, 86.28888888888889, 86.28888888888889, 86.37222222222222, 86.37222222222222, 86.66111111111111, 86.66111111111111, 86.43333333333334, 86.43333333333334, 86.57777777777778, 86.57777777777778, 86.23333333333333, 86.23333333333333, 86.63333333333334, 86.63333333333334, 86.96666666666667, 86.96666666666667, 86.85, 86.85, 86.45, 86.45, 86.47777777777777, 86.47777777777777, 86.84444444444445, 86.84444444444445, 86.7611111111111, 86.7611111111111, 86.70555555555555, 86.70555555555555, 86.88333333333334, 86.88333333333334, 87.4, 87.4, 87.24444444444444, 87.24444444444444, 87.10555555555555, 87.10555555555555, 87.17777777777778, 87.17777777777778, 87.2611111111111, 87.2611111111111, 87.22777777777777, 87.22777777777777, 86.97777777777777, 86.97777777777777, 87.09444444444445, 87.09444444444445, 86.84444444444445, 86.84444444444445, 87.07222222222222, 87.07222222222222, 87.1, 87.1, 87.0111111111111, 87.0111111111111, 87.47777777777777, 87.47777777777777, 87.35, 87.35, 87.73333333333333, 87.73333333333333, 87.61111111111111, 87.61111111111111, 87.84444444444445, 87.84444444444445, 87.71666666666667, 87.71666666666667, 87.38333333333334, 87.38333333333334, 87.77222222222223, 87.77222222222223, 87.98333333333333, 87.98333333333333, 88.14444444444445, 88.14444444444445, 87.66666666666667, 87.66666666666667, 88.17777777777778, 88.17777777777778, 87.56666666666666, 87.56666666666666, 87.93888888888888, 87.93888888888888, 87.86111111111111, 87.86111111111111, 87.67222222222222, 87.67222222222222, 87.23333333333333, 87.23333333333333, 87.56111111111112, 87.56111111111112, 87.40555555555555, 87.40555555555555, 87.66666666666667, 87.66666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.164, Test loss: 0.316, Test accuracy: 88.33
Average accuracy final 10 rounds: 88.16000000000001
3078.903269290924
[3.43105411529541, 6.86210823059082, 9.654853582382202, 12.447598934173584, 15.266595840454102, 18.08559274673462, 20.911497831344604, 23.73740291595459, 26.622219800949097, 29.507036685943604, 32.35266900062561, 35.19830131530762, 38.246243476867676, 41.294185638427734, 44.172035694122314, 47.049885749816895, 49.88654088973999, 52.723196029663086, 55.63222622871399, 58.54125642776489, 61.646404504776, 64.75155258178711, 67.8307740688324, 70.90999555587769, 73.89302349090576, 76.87605142593384, 79.85862231254578, 82.84119319915771, 85.78086543083191, 88.7205376625061, 91.6606674194336, 94.60079717636108, 97.36280846595764, 100.1248197555542, 103.19740319252014, 106.26998662948608, 109.29677867889404, 112.323570728302, 115.43876194953918, 118.55395317077637, 121.642502784729, 124.73105239868164, 127.7968819141388, 130.86271142959595, 133.68064618110657, 136.4985809326172, 139.2854311466217, 142.07228136062622, 144.83486700057983, 147.59745264053345, 150.6823742389679, 153.76729583740234, 156.81298065185547, 159.8586654663086, 162.8693118095398, 165.879958152771, 168.8966999053955, 171.91344165802002, 174.95661425590515, 177.99978685379028, 181.0530126094818, 184.10623836517334, 187.16485476493835, 190.22347116470337, 193.19041681289673, 196.1573624610901, 199.17900252342224, 202.2006425857544, 205.2361295223236, 208.27161645889282, 211.35953545570374, 214.44745445251465, 217.47693037986755, 220.50640630722046, 223.5983190536499, 226.69023180007935, 229.74507546424866, 232.79991912841797, 235.83663368225098, 238.87334823608398, 241.9054470062256, 244.9375457763672, 247.92391681671143, 250.91028785705566, 253.8482961654663, 256.78630447387695, 259.85566234588623, 262.9250202178955, 266.0219147205353, 269.11880922317505, 272.0732789039612, 275.0277485847473, 278.1193416118622, 281.21093463897705, 284.3397936820984, 287.4686527252197, 290.38971757888794, 293.31078243255615, 296.26355624198914, 299.2163300514221, 301.996155500412, 304.77598094940186, 307.5756027698517, 310.3752245903015, 313.26596784591675, 316.156711101532, 319.0993537902832, 322.0419964790344, 325.0848925113678, 328.1277885437012, 331.02298974990845, 333.9181909561157, 336.827525138855, 339.73685932159424, 342.653058052063, 345.56925678253174, 348.61611700057983, 351.66297721862793, 354.6282241344452, 357.59347105026245, 360.54029965400696, 363.48712825775146, 366.55824398994446, 369.62935972213745, 372.3770749568939, 375.1247901916504, 377.9493329524994, 380.7738757133484, 383.5515992641449, 386.3293228149414, 389.07452154159546, 391.8197202682495, 394.6202621459961, 397.4208040237427, 400.19505977630615, 402.96931552886963, 405.7188928127289, 408.46847009658813, 411.1829237937927, 413.8973774909973, 416.8427109718323, 419.78804445266724, 422.76985454559326, 425.7516646385193, 428.75495958328247, 431.75825452804565, 434.71726417541504, 437.6762738227844, 440.66099095344543, 443.64570808410645, 446.61137318611145, 449.57703828811646, 452.54278922080994, 455.5085401535034, 458.5576500892639, 461.6067600250244, 464.6492111682892, 467.69166231155396, 470.7121422290802, 473.73262214660645, 476.8088872432709, 479.8851523399353, 482.88535022735596, 485.8855481147766, 488.9030957221985, 491.92064332962036, 494.8751919269562, 497.829740524292, 500.7580189704895, 503.686297416687, 506.70045948028564, 509.7146215438843, 512.6581790447235, 515.6017365455627, 518.5467872619629, 521.491837978363, 524.396698474884, 527.301558971405, 530.2947525978088, 533.2879462242126, 536.2660987377167, 539.2442512512207, 542.1515114307404, 545.05877161026, 551.9382610321045, 558.817750453949, 561.522711277008, 564.2276721000671, 566.9659278392792, 569.7041835784912, 572.484044790268, 575.2639060020447, 577.9523270130157, 580.6407480239868, 583.3836712837219, 586.126594543457, 588.8866364955902, 591.6466784477234, 594.3685100078583, 597.0903415679932, 599.418535232544, 601.7467288970947]
[24.455555555555556, 24.455555555555556, 41.78333333333333, 41.78333333333333, 45.9, 45.9, 53.48888888888889, 53.48888888888889, 65.5, 65.5, 68.68888888888888, 68.68888888888888, 72.95, 72.95, 75.35, 75.35, 77.50555555555556, 77.50555555555556, 77.78333333333333, 77.78333333333333, 78.39444444444445, 78.39444444444445, 79.26666666666667, 79.26666666666667, 79.44444444444444, 79.44444444444444, 80.41111111111111, 80.41111111111111, 80.56111111111112, 80.56111111111112, 80.61111111111111, 80.61111111111111, 81.00555555555556, 81.00555555555556, 81.36111111111111, 81.36111111111111, 81.7388888888889, 81.7388888888889, 82.34444444444445, 82.34444444444445, 82.68888888888888, 82.68888888888888, 83.20555555555555, 83.20555555555555, 83.38888888888889, 83.38888888888889, 83.42222222222222, 83.42222222222222, 83.63333333333334, 83.63333333333334, 83.67777777777778, 83.67777777777778, 83.69444444444444, 83.69444444444444, 83.85, 83.85, 82.94444444444444, 82.94444444444444, 83.66666666666667, 83.66666666666667, 83.83333333333333, 83.83333333333333, 84.36666666666666, 84.36666666666666, 84.53888888888889, 84.53888888888889, 84.63888888888889, 84.63888888888889, 84.92777777777778, 84.92777777777778, 85.10555555555555, 85.10555555555555, 85.4888888888889, 85.4888888888889, 85.13888888888889, 85.13888888888889, 85.57777777777778, 85.57777777777778, 85.70555555555555, 85.70555555555555, 85.85, 85.85, 86.03888888888889, 86.03888888888889, 86.21666666666667, 86.21666666666667, 86.16111111111111, 86.16111111111111, 86.3, 86.3, 86.30555555555556, 86.30555555555556, 86.42777777777778, 86.42777777777778, 86.15, 86.15, 86.31666666666666, 86.31666666666666, 86.63888888888889, 86.63888888888889, 87.16666666666667, 87.16666666666667, 87.21111111111111, 87.21111111111111, 86.93888888888888, 86.93888888888888, 87.06111111111112, 87.06111111111112, 86.81111111111112, 86.81111111111112, 86.5, 86.5, 87.04444444444445, 87.04444444444445, 87.22222222222223, 87.22222222222223, 87.03333333333333, 87.03333333333333, 87.25, 87.25, 87.35, 87.35, 87.58333333333333, 87.58333333333333, 87.27222222222223, 87.27222222222223, 87.28333333333333, 87.28333333333333, 87.66666666666667, 87.66666666666667, 87.68888888888888, 87.68888888888888, 87.58333333333333, 87.58333333333333, 87.60555555555555, 87.60555555555555, 87.62777777777778, 87.62777777777778, 87.7611111111111, 87.7611111111111, 87.87777777777778, 87.87777777777778, 88.03888888888889, 88.03888888888889, 87.88333333333334, 87.88333333333334, 87.80555555555556, 87.80555555555556, 87.82222222222222, 87.82222222222222, 87.74444444444444, 87.74444444444444, 87.78888888888889, 87.78888888888889, 87.89444444444445, 87.89444444444445, 87.88888888888889, 87.88888888888889, 87.92222222222222, 87.92222222222222, 87.96111111111111, 87.96111111111111, 88.12777777777778, 88.12777777777778, 88.06666666666666, 88.06666666666666, 88.05, 88.05, 88.06666666666666, 88.06666666666666, 88.08333333333333, 88.08333333333333, 88.25555555555556, 88.25555555555556, 88.00555555555556, 88.00555555555556, 87.83888888888889, 87.83888888888889, 88.06111111111112, 88.06111111111112, 88.2611111111111, 88.2611111111111, 88.21111111111111, 88.21111111111111, 88.31666666666666, 88.31666666666666, 88.07777777777778, 88.07777777777778, 88.2, 88.2, 87.99444444444444, 87.99444444444444, 88.19444444444444, 88.19444444444444, 88.15555555555555, 88.15555555555555, 88.18888888888888, 88.18888888888888, 88.0, 88.0, 88.33333333333333, 88.33333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 504, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52919 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

15688.674664735794
[5.827646732330322, 11.39458441734314, 17.002675533294678, 22.134471893310547, 27.227495431900024, 32.432830810546875, 37.601070404052734, 42.754311084747314, 48.01337718963623, 53.77482748031616, 59.0474636554718, 64.24152255058289, 69.9435887336731, 75.49882960319519, 80.81296515464783, 86.07580828666687, 91.38474106788635, 96.54609370231628, 102.19974040985107, 107.525226354599, 112.91224837303162, 118.31194972991943, 123.82647919654846, 129.49412751197815, 134.93729877471924, 140.17554593086243, 145.48230814933777, 150.75237393379211, 155.9614496231079, 161.11458897590637, 166.1779351234436, 171.41135334968567, 176.55455446243286, 181.835955619812, 187.20027136802673, 192.5702383518219, 197.7369668483734, 202.9365303516388, 208.05954670906067, 213.29978942871094, 218.5703580379486, 223.7251751422882, 228.9262068271637, 234.17949438095093, 239.43106484413147, 244.62192749977112, 249.90646839141846, 255.17663145065308, 260.44806456565857, 265.7453155517578, 271.1618139743805, 276.55494260787964, 281.8341472148895, 287.1367783546448, 292.3327133655548, 297.55403995513916, 302.77400517463684, 307.89657068252563, 313.0878026485443, 318.2332580089569, 323.15178179740906, 327.84944462776184, 332.5752272605896, 337.2934217453003, 342.0302963256836, 346.737783908844, 351.44411420822144, 356.1513686180115, 360.874897480011, 365.59845423698425, 370.3240737915039, 375.0416738986969, 379.786593914032, 384.51270818710327, 389.2234642505646, 393.9637522697449, 398.70672249794006, 403.442973613739, 408.18641686439514, 412.9261693954468, 417.6494953632355, 422.37775182724, 427.1008503437042, 431.82659244537354, 436.57830286026, 441.3218491077423, 446.0218448638916, 450.72367811203003, 455.43486309051514, 460.16983461380005, 464.8963623046875, 469.62523126602173, 474.35985374450684, 479.0642535686493, 483.78184509277344, 488.53568410873413, 493.27536034584045, 498.00619745254517, 502.72585701942444, 507.439822435379, 512.1856832504272, 516.9344499111176, 521.6849195957184, 526.4276692867279, 531.1638615131378, 535.8869705200195, 540.6158437728882, 545.34428191185, 550.089583158493, 554.8340282440186, 559.5792164802551, 564.302925825119, 569.0264263153076, 573.7528855800629, 578.4810733795166, 583.2131493091583, 587.9331843852997, 592.6357507705688, 597.3409526348114, 602.0455303192139, 606.7531478404999, 611.4740722179413, 616.1771566867828, 620.8873043060303, 625.5910305976868, 630.3006374835968, 635.0005810260773, 639.734299659729, 644.4561307430267, 649.1547210216522, 653.9062089920044, 658.6288859844208, 663.335047006607, 668.0741355419159, 672.8056795597076, 677.5310795307159, 682.2495775222778, 686.961980342865, 691.6670649051666, 696.3586006164551, 701.0558788776398, 705.7531282901764, 710.4515447616577, 715.1665110588074, 719.8820586204529, 724.5918009281158, 729.3027155399323, 734.0008907318115, 738.6902143955231, 743.373966217041, 748.1036922931671, 752.8247961997986, 757.5516741275787, 762.2846009731293, 767.0279388427734, 771.759331703186, 776.475572347641, 781.128143787384, 785.7954769134521, 790.5018410682678, 795.194005727768, 799.8648006916046, 804.5101687908173, 809.1544878482819, 813.8232169151306, 818.47225689888, 823.1459188461304, 827.8097891807556, 832.4974691867828, 837.1325349807739, 841.7812802791595, 846.4418215751648, 851.1029937267303, 855.7468464374542, 860.422892332077, 865.1167194843292, 869.7718601226807, 874.428475856781, 879.0966484546661, 883.7872650623322, 888.4818217754364, 893.1338496208191, 897.7613258361816, 902.3839626312256, 907.0351638793945, 911.6593322753906, 916.4059548377991, 921.1224303245544, 925.8377494812012, 930.5367367267609, 935.2295167446136, 939.9662799835205, 944.6418397426605, 949.2855916023254, 953.925621509552, 958.5452880859375, 963.2120733261108, 967.8547792434692, 972.5113832950592, 977.1948759555817, 981.8599882125854, 986.5881583690643, 991.2898645401001, 995.9767422676086, 1000.6381070613861, 1005.2879965305328, 1009.9416859149933, 1014.6211936473846, 1019.2763757705688, 1023.9270262718201, 1028.5610690116882, 1033.2074892520905, 1037.8715765476227, 1042.5186440944672, 1047.1640493869781, 1051.7833156585693, 1056.402487039566, 1061.0431833267212, 1065.700389623642, 1070.3718605041504, 1075.0265963077545, 1079.6652328968048, 1084.302553653717, 1088.9544296264648, 1093.6290180683136, 1098.2925724983215, 1102.9486765861511, 1107.6304216384888, 1112.2847957611084, 1116.9282097816467, 1121.5657889842987, 1126.2224805355072, 1130.8786289691925, 1135.501823425293, 1140.1699571609497, 1144.8212242126465, 1149.4773671627045, 1154.1505198478699, 1158.7996871471405, 1163.4746716022491, 1168.126360654831, 1172.7578711509705, 1177.4128851890564, 1182.077006816864, 1186.7452569007874, 1191.3953206539154, 1196.0666229724884, 1200.737633228302, 1205.3993077278137, 1210.0552303791046, 1214.7527318000793, 1219.4291915893555, 1224.0805928707123, 1228.7473437786102, 1233.4186561107635, 1238.0559453964233, 1242.6736252307892, 1247.3180165290833, 1252.003663301468, 1256.6959981918335, 1261.34601521492, 1265.9946446418762, 1270.6560230255127, 1275.3438301086426, 1280.000477552414, 1284.7146978378296, 1289.4105677604675, 1294.060604095459, 1298.7473061084747, 1303.4236619472504, 1308.064103603363, 1312.7072756290436, 1317.3539097309113, 1322.0039145946503, 1326.6902613639832, 1331.3423926830292, 1336.008074760437, 1340.6862428188324, 1345.3803715705872, 1350.0656683444977, 1354.7711112499237, 1359.4509446620941, 1364.1110236644745, 1368.7615509033203, 1373.4145956039429, 1378.0734386444092, 1382.7224736213684, 1387.3529467582703, 1392.0037236213684, 1396.6708035469055, 1401.3425917625427, 1406.003802061081, 1410.6738431453705, 1415.3350710868835, 1420.002252817154, 1424.685435295105, 1429.3680756092072, 1434.0190987586975, 1438.677065372467, 1443.3311975002289, 1445.6604642868042]
[9.9275, 9.9475, 9.9, 9.8975, 9.9175, 9.9225, 9.9375, 9.96, 9.99, 9.9925, 9.9775, 9.9925, 10.0025, 10.015, 10.025, 10.1375, 10.195, 10.4575, 10.8275, 10.9625, 11.05, 11.1125, 11.6625, 12.06, 12.915, 13.4975, 13.985, 14.0425, 14.32, 14.395, 14.4, 14.7425, 14.63, 14.73, 14.9225, 14.955, 15.02, 15.08, 15.0725, 15.255, 15.2775, 15.385, 15.33, 15.555, 15.6875, 16.035, 16.285, 16.24, 16.225, 16.355, 16.4725, 16.635, 16.805, 16.7875, 16.95, 16.9275, 16.675, 16.785, 16.9075, 16.9625, 16.945, 16.99, 17.01, 16.8675, 16.9425, 16.665, 16.4275, 16.51, 16.76, 16.875, 17.13, 17.1525, 17.075, 16.9625, 17.1375, 16.94, 16.88, 16.9425, 16.815, 16.9425, 17.02, 17.05, 17.0075, 16.935, 17.335, 17.58, 17.5775, 17.525, 17.71, 17.65, 17.555, 17.7125, 17.705, 18.08, 18.2525, 18.2225, 18.365, 18.405, 18.4275, 18.6775, 18.395, 15.7125, 13.5, 12.56, 10.75, 10.3675, 10.3675, 10.3675, 10.3675, 10.3675, 10.3675, 10.3675, 10.3675, 10.3675, 10.3675, 10.3675, 10.3675, 10.3675, 10.3675, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.124, Test loss: 0.457, Test accuracy: 87.33
Final Round, Global train loss: 0.124, Global test loss: 1.272, Global test accuracy: 62.88
Average accuracy final 10 rounds: 87.35666666666665 

Average global accuracy final 10 rounds: 61.50666666666667 

3581.9861438274384
[2.8927981853485107, 5.7855963706970215, 8.563468933105469, 11.341341495513916, 13.83297061920166, 16.324599742889404, 18.799326181411743, 21.274052619934082, 28.056262969970703, 34.838473320007324, 37.37090611457825, 39.90333890914917, 42.39123272895813, 44.87912654876709, 47.4325590133667, 49.98599147796631, 52.49631595611572, 55.00664043426514, 57.537224531173706, 60.067808628082275, 62.593308448791504, 65.11880826950073, 67.61302947998047, 70.1072506904602, 72.61947011947632, 75.13168954849243, 77.66353273391724, 80.19537591934204, 82.69964623451233, 85.20391654968262, 87.71140003204346, 90.2188835144043, 92.74314832687378, 95.26741313934326, 97.77189183235168, 100.27637052536011, 102.81425356864929, 105.35213661193848, 107.84722828865051, 110.34231996536255, 112.88487935066223, 115.42743873596191, 117.93869161605835, 120.44994449615479, 122.98026466369629, 125.5105848312378, 128.05388474464417, 130.59718465805054, 133.09897136688232, 135.6007580757141, 138.14211130142212, 140.68346452713013, 143.17659497261047, 145.66972541809082, 148.20612478256226, 150.7425241470337, 153.23119115829468, 155.71985816955566, 158.24669480323792, 160.77353143692017, 163.28727793693542, 165.80102443695068, 168.33165574073792, 170.86228704452515, 173.4057161808014, 175.94914531707764, 178.46786665916443, 180.98658800125122, 183.52521586418152, 186.06384372711182, 188.57688546180725, 191.08992719650269, 193.60809993743896, 196.12627267837524, 198.64954233169556, 201.17281198501587, 203.70173525810242, 206.23065853118896, 208.72837281227112, 211.22608709335327, 213.73639059066772, 216.24669408798218, 218.76638317108154, 221.2860722541809, 223.8078486919403, 226.3296251296997, 228.82381510734558, 231.31800508499146, 233.84381103515625, 236.36961698532104, 238.88214302062988, 241.39466905593872, 244.10592436790466, 246.8171796798706, 249.55346632003784, 252.28975296020508, 255.1417977809906, 257.9938426017761, 260.89827251434326, 263.8027024269104, 266.7004849910736, 269.5982675552368, 272.4944133758545, 275.39055919647217, 278.27283215522766, 281.15510511398315, 283.98489928245544, 286.81469345092773, 289.33627367019653, 291.85785388946533, 294.34452295303345, 296.83119201660156, 299.3704116344452, 301.9096312522888, 304.396507024765, 306.8833827972412, 309.4242305755615, 311.96507835388184, 314.47002243995667, 316.9749665260315, 319.49453139305115, 322.0140962600708, 324.5238606929779, 327.033625125885, 329.554080247879, 332.07453536987305, 334.61101508140564, 337.14749479293823, 339.65763092041016, 342.1677670478821, 344.6849763393402, 347.20218563079834, 349.70062351226807, 352.1990613937378, 354.71066308021545, 357.2222647666931, 359.75175499916077, 362.2812452316284, 364.80334663391113, 367.32544803619385, 369.85332012176514, 372.3811922073364, 374.91573882102966, 377.4502854347229, 379.953017950058, 382.45575046539307, 384.99002051353455, 387.524290561676, 390.0423352718353, 392.56037998199463, 395.0872621536255, 397.61414432525635, 400.1086676120758, 402.60319089889526, 405.11354088783264, 407.62389087677, 410.15248799324036, 412.6810851097107, 415.1673336029053, 417.65358209609985, 420.17227149009705, 422.69096088409424, 425.1916835308075, 427.69240617752075, 430.2234785556793, 432.7545509338379, 435.2460219860077, 437.7374930381775, 440.24508714675903, 442.7526812553406, 445.2584729194641, 447.76426458358765, 450.2612624168396, 452.75826025009155, 455.2591543197632, 457.7600483894348, 460.247599363327, 462.73515033721924, 465.26688146591187, 467.7986125946045, 470.34257555007935, 472.8865385055542, 475.4285457134247, 477.97055292129517, 480.5254054069519, 483.08025789260864, 485.614461183548, 488.1486644744873, 490.6877419948578, 493.22681951522827, 495.76728677749634, 498.3077540397644, 500.85314321517944, 503.3985323905945, 505.93895506858826, 508.47937774658203, 511.0053188800812, 513.5312600135803, 516.0730860233307, 518.614912033081, 520.741265296936, 522.867618560791]
[36.425, 36.425, 49.2875, 49.2875, 54.0125, 54.0125, 57.7875, 57.7875, 69.05416666666666, 69.05416666666666, 74.60833333333333, 74.60833333333333, 76.52916666666667, 76.52916666666667, 76.78333333333333, 76.78333333333333, 76.20416666666667, 76.20416666666667, 77.33333333333333, 77.33333333333333, 77.57916666666667, 77.57916666666667, 80.64166666666667, 80.64166666666667, 81.6, 81.6, 81.8875, 81.8875, 82.47916666666667, 82.47916666666667, 82.3625, 82.3625, 82.675, 82.675, 82.8625, 82.8625, 82.70416666666667, 82.70416666666667, 82.9625, 82.9625, 83.08333333333333, 83.08333333333333, 83.05416666666666, 83.05416666666666, 83.05833333333334, 83.05833333333334, 83.47083333333333, 83.47083333333333, 84.35, 84.35, 84.65, 84.65, 84.3375, 84.3375, 84.76666666666667, 84.76666666666667, 85.0625, 85.0625, 85.0, 85.0, 85.04166666666667, 85.04166666666667, 85.1875, 85.1875, 85.12083333333334, 85.12083333333334, 85.12083333333334, 85.12083333333334, 84.95416666666667, 84.95416666666667, 85.30416666666666, 85.30416666666666, 85.64166666666667, 85.64166666666667, 85.5375, 85.5375, 85.20833333333333, 85.20833333333333, 84.81666666666666, 84.81666666666666, 85.54166666666667, 85.54166666666667, 85.775, 85.775, 85.72083333333333, 85.72083333333333, 85.7375, 85.7375, 85.7875, 85.7875, 85.6625, 85.6625, 85.67916666666666, 85.67916666666666, 85.9625, 85.9625, 85.725, 85.725, 85.64166666666667, 85.64166666666667, 85.9125, 85.9125, 85.97083333333333, 85.97083333333333, 86.0625, 86.0625, 86.42083333333333, 86.42083333333333, 86.42916666666666, 86.42916666666666, 86.3, 86.3, 86.27916666666667, 86.27916666666667, 86.55833333333334, 86.55833333333334, 86.65, 86.65, 86.51666666666667, 86.51666666666667, 86.6625, 86.6625, 86.66666666666667, 86.66666666666667, 86.65416666666667, 86.65416666666667, 86.75, 86.75, 86.89166666666667, 86.89166666666667, 86.9, 86.9, 86.975, 86.975, 87.02083333333333, 87.02083333333333, 86.79166666666667, 86.79166666666667, 86.79166666666667, 86.79166666666667, 86.6375, 86.6375, 86.90833333333333, 86.90833333333333, 87.09166666666667, 87.09166666666667, 86.98333333333333, 86.98333333333333, 86.2875, 86.2875, 86.72916666666667, 86.72916666666667, 86.4, 86.4, 86.62916666666666, 86.62916666666666, 86.76666666666667, 86.76666666666667, 86.69166666666666, 86.69166666666666, 86.98333333333333, 86.98333333333333, 87.3625, 87.3625, 87.11666666666666, 87.11666666666666, 86.79583333333333, 86.79583333333333, 86.85, 86.85, 87.0125, 87.0125, 87.62083333333334, 87.62083333333334, 87.2875, 87.2875, 87.225, 87.225, 87.07916666666667, 87.07916666666667, 86.875, 86.875, 87.0625, 87.0625, 86.9625, 86.9625, 87.72083333333333, 87.72083333333333, 87.925, 87.925, 87.8125, 87.8125, 87.9125, 87.9125, 87.32916666666667, 87.32916666666667, 87.06666666666666, 87.06666666666666, 86.9, 86.9, 87.33333333333333, 87.33333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.206, Test loss: 0.389, Test accuracy: 86.06
Average accuracy final 10 rounds: 85.50750000000001 

1480.8260583877563
[1.5820932388305664, 3.164186477661133, 4.543870210647583, 5.923553943634033, 7.263312816619873, 8.603071689605713, 9.942088603973389, 11.281105518341064, 12.652203798294067, 14.02330207824707, 15.378214597702026, 16.733127117156982, 18.082983255386353, 19.432839393615723, 20.77800750732422, 22.123175621032715, 23.48574423789978, 24.848312854766846, 26.22072458267212, 27.593136310577393, 28.957968711853027, 30.322801113128662, 31.68198585510254, 33.041170597076416, 34.41767764091492, 35.79418468475342, 37.16002106666565, 38.52585744857788, 39.8985652923584, 41.271273136138916, 42.5004506111145, 43.72962808609009, 44.93640351295471, 46.143178939819336, 47.375513315200806, 48.607847690582275, 49.839269399642944, 51.07069110870361, 52.29931998252869, 53.52794885635376, 54.75292372703552, 55.977898597717285, 57.21045517921448, 58.44301176071167, 59.68089938163757, 60.91878700256348, 62.14191460609436, 63.365042209625244, 64.58384132385254, 65.80264043807983, 67.03326034545898, 68.26388025283813, 69.5018093585968, 70.73973846435547, 71.96434426307678, 73.1889500617981, 74.41921615600586, 75.64948225021362, 76.87321448326111, 78.0969467163086, 79.32582879066467, 80.55471086502075, 81.8024206161499, 83.05013036727905, 84.28502345085144, 85.51991653442383, 86.74039554595947, 87.96087455749512, 89.19552540779114, 90.43017625808716, 91.66491794586182, 92.89965963363647, 94.12841463088989, 95.35716962814331, 96.58428287506104, 97.81139612197876, 99.03356885910034, 100.25574159622192, 101.47537970542908, 102.69501781463623, 103.93223524093628, 105.16945266723633, 106.41020393371582, 107.65095520019531, 108.86748051643372, 110.08400583267212, 111.31367182731628, 112.54333782196045, 113.78063678741455, 115.01793575286865, 116.24325942993164, 117.46858310699463, 118.67851519584656, 119.88844728469849, 121.11401891708374, 122.339590549469, 123.57994556427002, 124.82030057907104, 126.07362985610962, 127.3269591331482, 128.54406881332397, 129.76117849349976, 130.99087858200073, 132.2205786705017, 133.45598483085632, 134.69139099121094, 135.9308145046234, 137.1702380180359, 138.4053373336792, 139.6404366493225, 140.86490845680237, 142.08938026428223, 143.31771659851074, 144.54605293273926, 145.78118538856506, 147.01631784439087, 148.23796248435974, 149.4596071243286, 150.68142414093018, 151.90324115753174, 153.13120198249817, 154.3591628074646, 155.59170007705688, 156.82423734664917, 158.06533575057983, 159.3064341545105, 160.5222487449646, 161.7380633354187, 162.98200750350952, 164.22595167160034, 165.46311783790588, 166.70028400421143, 167.9446406364441, 169.18899726867676, 170.40801429748535, 171.62703132629395, 172.85244512557983, 174.07785892486572, 175.31684160232544, 176.55582427978516, 177.80182027816772, 179.0478162765503, 180.2825756072998, 181.51733493804932, 182.73617362976074, 183.95501232147217, 185.16666293144226, 186.37831354141235, 187.61231136322021, 188.84630918502808, 190.08001446723938, 191.31371974945068, 192.53746438026428, 193.76120901107788, 194.97852730751038, 196.19584560394287, 197.41925430297852, 198.64266300201416, 199.89069032669067, 201.1387176513672, 202.36225032806396, 203.58578300476074, 204.80368900299072, 206.0215950012207, 207.2466492652893, 208.4717035293579, 209.71256256103516, 210.9534215927124, 212.19297170639038, 213.43252182006836, 214.6454689502716, 215.85841608047485, 217.08769059181213, 218.3169651031494, 219.56025290489197, 220.80354070663452, 222.04503083229065, 223.28652095794678, 224.50953197479248, 225.73254299163818, 226.95450901985168, 228.17647504806519, 229.40126776695251, 230.62606048583984, 231.87517762184143, 233.12429475784302, 234.3519744873047, 235.57965421676636, 236.80033135414124, 238.0210084915161, 239.23207259178162, 240.44313669204712, 241.68354725837708, 242.92395782470703, 244.1509940624237, 245.37803030014038, 246.5941960811615, 247.81036186218262, 249.0354962348938, 250.26063060760498, 252.1998255252838, 254.13902044296265]
[22.108333333333334, 22.108333333333334, 27.441666666666666, 27.441666666666666, 43.31666666666667, 43.31666666666667, 53.15, 53.15, 57.9, 57.9, 61.458333333333336, 61.458333333333336, 60.958333333333336, 60.958333333333336, 66.38333333333334, 66.38333333333334, 64.38333333333334, 64.38333333333334, 68.51666666666667, 68.51666666666667, 67.03333333333333, 67.03333333333333, 71.55, 71.55, 71.41666666666667, 71.41666666666667, 75.775, 75.775, 76.9, 76.9, 77.03333333333333, 77.03333333333333, 77.35, 77.35, 76.875, 76.875, 78.19166666666666, 78.19166666666666, 78.41666666666667, 78.41666666666667, 78.75833333333334, 78.75833333333334, 78.86666666666666, 78.86666666666666, 79.04166666666667, 79.04166666666667, 79.225, 79.225, 79.48333333333333, 79.48333333333333, 80.48333333333333, 80.48333333333333, 80.275, 80.275, 80.45, 80.45, 80.84166666666667, 80.84166666666667, 80.99166666666666, 80.99166666666666, 80.98333333333333, 80.98333333333333, 81.275, 81.275, 81.38333333333334, 81.38333333333334, 81.80833333333334, 81.80833333333334, 81.725, 81.725, 82.3, 82.3, 82.51666666666667, 82.51666666666667, 82.825, 82.825, 82.63333333333334, 82.63333333333334, 82.78333333333333, 82.78333333333333, 83.01666666666667, 83.01666666666667, 82.175, 82.175, 82.94166666666666, 82.94166666666666, 82.85, 82.85, 82.88333333333334, 82.88333333333334, 82.89166666666667, 82.89166666666667, 83.75833333333334, 83.75833333333334, 83.09166666666667, 83.09166666666667, 83.44166666666666, 83.44166666666666, 83.48333333333333, 83.48333333333333, 83.33333333333333, 83.33333333333333, 83.61666666666666, 83.61666666666666, 83.325, 83.325, 83.475, 83.475, 83.95833333333333, 83.95833333333333, 83.98333333333333, 83.98333333333333, 84.025, 84.025, 84.20833333333333, 84.20833333333333, 83.98333333333333, 83.98333333333333, 83.94166666666666, 83.94166666666666, 84.25, 84.25, 84.075, 84.075, 84.40833333333333, 84.40833333333333, 84.5, 84.5, 84.225, 84.225, 84.675, 84.675, 84.36666666666666, 84.36666666666666, 84.75, 84.75, 84.75, 84.75, 84.5, 84.5, 84.74166666666666, 84.74166666666666, 84.61666666666666, 84.61666666666666, 84.40833333333333, 84.40833333333333, 84.825, 84.825, 84.65833333333333, 84.65833333333333, 84.625, 84.625, 84.5, 84.5, 84.71666666666667, 84.71666666666667, 85.03333333333333, 85.03333333333333, 85.01666666666667, 85.01666666666667, 84.99166666666666, 84.99166666666666, 85.15833333333333, 85.15833333333333, 85.05, 85.05, 85.13333333333334, 85.13333333333334, 85.65, 85.65, 85.675, 85.675, 85.325, 85.325, 85.29166666666667, 85.29166666666667, 85.3, 85.3, 85.2, 85.2, 85.54166666666667, 85.54166666666667, 85.75833333333334, 85.75833333333334, 85.75, 85.75, 85.7, 85.7, 85.26666666666667, 85.26666666666667, 85.39166666666667, 85.39166666666667, 85.38333333333334, 85.38333333333334, 85.15833333333333, 85.15833333333333, 85.35, 85.35, 85.775, 85.775, 86.05833333333334, 86.05833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.190, Test loss: 0.353, Test accuracy: 86.59
Average accuracy final 10 rounds: 86.36
1750.2051441669464
[2.1292097568511963, 4.258419513702393, 6.031327724456787, 7.804235935211182, 9.515947103500366, 11.22765827178955, 12.96187686920166, 14.69609546661377, 16.442957639694214, 18.189819812774658, 19.923135995864868, 21.656452178955078, 23.336259365081787, 25.016066551208496, 26.742575645446777, 28.46908473968506, 30.219398975372314, 31.96971321105957, 33.71577191352844, 35.461830615997314, 37.19882583618164, 38.93582105636597, 40.689876079559326, 42.443931102752686, 44.21557116508484, 45.98721122741699, 47.68009543418884, 49.37297964096069, 51.09432911872864, 52.81567859649658, 54.5547308921814, 56.29378318786621, 58.03383421897888, 59.77388525009155, 61.49781823158264, 63.22175121307373, 64.94513773918152, 66.6685242652893, 68.40599822998047, 70.14347219467163, 71.88981533050537, 73.63615846633911, 75.34643316268921, 77.0567078590393, 78.79593324661255, 80.53515863418579, 82.30677700042725, 84.0783953666687, 85.83051085472107, 87.58262634277344, 89.31462240219116, 91.04661846160889, 92.70720720291138, 94.36779594421387, 95.9729311466217, 97.57806634902954, 99.17746305465698, 100.77685976028442, 102.36083555221558, 103.94481134414673, 105.55722570419312, 107.1696400642395, 108.74755907058716, 110.32547807693481, 111.89276814460754, 113.46005821228027, 115.02690482139587, 116.59375143051147, 118.35812473297119, 120.12249803543091, 121.88990068435669, 123.65730333328247, 125.39978408813477, 127.14226484298706, 128.87657284736633, 130.6108808517456, 132.3594319820404, 134.1079831123352, 135.86807656288147, 137.62817001342773, 139.34704852104187, 141.065927028656, 142.80450868606567, 144.54309034347534, 146.29617190361023, 148.04925346374512, 149.8115963935852, 151.5739393234253, 153.30405044555664, 155.034161567688, 156.80321717262268, 158.57227277755737, 160.34727144241333, 162.1222701072693, 163.86477708816528, 165.60728406906128, 167.33560967445374, 169.0639352798462, 170.82746863365173, 172.59100198745728, 174.34640383720398, 176.10180568695068, 177.85488057136536, 179.60795545578003, 181.35615229606628, 183.10434913635254, 184.8804042339325, 186.65645933151245, 188.39645981788635, 190.13646030426025, 191.87694382667542, 193.61742734909058, 195.3858072757721, 197.1541872024536, 198.92121005058289, 200.68823289871216, 202.42060828208923, 204.1529836654663, 205.87519025802612, 207.59739685058594, 209.36882066726685, 211.14024448394775, 212.90033888816833, 214.66043329238892, 216.39713072776794, 218.13382816314697, 219.88940930366516, 221.64499044418335, 223.41188859939575, 225.17878675460815, 226.90877962112427, 228.63877248764038, 230.37411618232727, 232.10945987701416, 233.8693037033081, 235.62914752960205, 237.37751030921936, 239.12587308883667, 240.85327339172363, 242.5806736946106, 244.31886339187622, 246.05705308914185, 247.82835388183594, 249.59965467453003, 251.36586093902588, 253.13206720352173, 254.8601188659668, 256.58817052841187, 258.3524103164673, 260.1166501045227, 261.87578773498535, 263.634925365448, 265.2342894077301, 266.8336534500122, 268.4386553764343, 270.04365730285645, 271.64283084869385, 273.24200439453125, 274.85140204429626, 276.4607996940613, 278.0559198856354, 279.6510400772095, 281.2608494758606, 282.8706588745117, 284.46749544143677, 286.0643320083618, 287.65984439849854, 289.25535678863525, 290.83927369117737, 292.4231905937195, 294.0185408592224, 295.61389112472534, 297.2248797416687, 298.83586835861206, 300.43229389190674, 302.0287194252014, 303.58655548095703, 305.14439153671265, 306.7245559692383, 308.3047204017639, 309.8853838443756, 311.4660472869873, 313.17843532562256, 314.8908233642578, 316.61841559410095, 318.3460078239441, 320.07678055763245, 321.8075532913208, 323.52976179122925, 325.2519702911377, 326.9783504009247, 328.70473051071167, 330.4379880428314, 332.1712455749512, 333.8996231555939, 335.6280007362366, 337.3547086715698, 339.0814166069031, 340.81621408462524, 342.5510115623474, 344.713374376297, 346.8757371902466]
[27.633333333333333, 27.633333333333333, 34.275, 34.275, 51.6, 51.6, 55.84166666666667, 55.84166666666667, 59.583333333333336, 59.583333333333336, 63.95, 63.95, 67.34166666666667, 67.34166666666667, 72.93333333333334, 72.93333333333334, 73.9, 73.9, 74.53333333333333, 74.53333333333333, 75.875, 75.875, 76.59166666666667, 76.59166666666667, 76.9, 76.9, 77.35833333333333, 77.35833333333333, 77.79166666666667, 77.79166666666667, 78.68333333333334, 78.68333333333334, 79.23333333333333, 79.23333333333333, 79.81666666666666, 79.81666666666666, 79.71666666666667, 79.71666666666667, 80.06666666666666, 80.06666666666666, 80.125, 80.125, 80.375, 80.375, 79.96666666666667, 79.96666666666667, 80.525, 80.525, 80.65833333333333, 80.65833333333333, 80.74166666666666, 80.74166666666666, 81.4, 81.4, 81.93333333333334, 81.93333333333334, 81.80833333333334, 81.80833333333334, 81.96666666666667, 81.96666666666667, 82.85833333333333, 82.85833333333333, 82.99166666666666, 82.99166666666666, 83.39166666666667, 83.39166666666667, 83.14166666666667, 83.14166666666667, 82.81666666666666, 82.81666666666666, 83.44166666666666, 83.44166666666666, 83.775, 83.775, 83.81666666666666, 83.81666666666666, 83.81666666666666, 83.81666666666666, 83.625, 83.625, 83.55833333333334, 83.55833333333334, 84.35833333333333, 84.35833333333333, 84.125, 84.125, 84.05833333333334, 84.05833333333334, 84.34166666666667, 84.34166666666667, 84.40833333333333, 84.40833333333333, 84.5, 84.5, 84.825, 84.825, 84.85833333333333, 84.85833333333333, 84.90833333333333, 84.90833333333333, 85.075, 85.075, 85.14166666666667, 85.14166666666667, 85.00833333333334, 85.00833333333334, 84.95, 84.95, 85.05, 85.05, 85.46666666666667, 85.46666666666667, 85.375, 85.375, 84.94166666666666, 84.94166666666666, 85.06666666666666, 85.06666666666666, 85.375, 85.375, 85.4, 85.4, 85.2, 85.2, 85.13333333333334, 85.13333333333334, 85.24166666666666, 85.24166666666666, 85.63333333333334, 85.63333333333334, 85.65, 85.65, 85.70833333333333, 85.70833333333333, 85.30833333333334, 85.30833333333334, 85.09166666666667, 85.09166666666667, 85.79166666666667, 85.79166666666667, 85.54166666666667, 85.54166666666667, 85.575, 85.575, 85.59166666666667, 85.59166666666667, 86.08333333333333, 86.08333333333333, 85.79166666666667, 85.79166666666667, 85.8, 85.8, 85.85, 85.85, 85.74166666666666, 85.74166666666666, 85.95833333333333, 85.95833333333333, 85.94166666666666, 85.94166666666666, 85.98333333333333, 85.98333333333333, 86.06666666666666, 86.06666666666666, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.49166666666666, 86.49166666666666, 86.25, 86.25, 86.08333333333333, 86.08333333333333, 85.825, 85.825, 86.25, 86.25, 86.025, 86.025, 86.10833333333333, 86.10833333333333, 86.51666666666667, 86.51666666666667, 86.24166666666666, 86.24166666666666, 86.55833333333334, 86.55833333333334, 86.125, 86.125, 86.49166666666666, 86.49166666666666, 86.5, 86.5, 86.38333333333334, 86.38333333333334, 86.29166666666667, 86.29166666666667, 86.38333333333334, 86.38333333333334, 86.59166666666667, 86.59166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.163, Test loss: 1.107, Test accuracy: 67.00
Average accuracy final 10 rounds: 62.57583333333334
2533.151615381241
[4.0599353313446045, 7.866822004318237, 11.6489839553833, 15.413770914077759, 19.20076894760132, 22.98893141746521, 26.784745454788208, 30.568190574645996, 34.38127279281616, 38.16540193557739, 41.94868612289429, 45.72576951980591, 49.51455783843994, 53.285635471343994, 57.098387479782104, 60.89214468002319, 64.67715358734131, 68.42748951911926, 72.19004249572754, 75.99386477470398, 79.78859686851501, 83.6102557182312, 87.44411659240723, 91.2435405254364, 95.0652871131897, 98.86540174484253, 102.6829993724823, 106.48557925224304, 110.28159356117249, 114.08981513977051, 117.88473224639893, 121.6901695728302, 125.49115705490112, 129.29063415527344, 133.09063243865967, 136.89483332633972, 140.6956865787506, 144.50749492645264, 148.31120800971985, 152.1244113445282, 155.50925374031067, 158.91897296905518, 162.33734583854675, 165.74136233329773, 169.15087461471558, 172.85995388031006, 176.3111801147461, 179.7855806350708, 183.20562624931335, 186.64453053474426, 190.11678051948547, 193.56377363204956, 196.9976577758789, 200.46083283424377, 203.9324028491974, 207.37129426002502, 210.8046567440033, 214.25896954536438, 217.6599943637848, 221.09337091445923, 224.4866020679474, 227.89870977401733, 231.28208351135254, 234.7052297592163, 238.11246824264526, 241.50182461738586, 244.9379436969757, 248.36756825447083, 251.7936885356903, 255.22664380073547, 258.65128993988037, 262.0890259742737, 265.52161264419556, 268.95407938957214, 272.3570430278778, 275.74881386756897, 279.16030526161194, 282.5615646839142, 285.97280979156494, 289.3913354873657, 292.8119595050812, 296.22812032699585, 299.66730761528015, 303.456148147583, 306.97302889823914, 310.4974527359009, 314.0191562175751, 317.5466516017914, 321.01805806159973, 324.4976327419281, 327.98511147499084, 331.4762017726898, 334.9634130001068, 338.4581718444824, 341.9490222930908, 345.4300172328949, 348.969064950943, 352.5101821422577, 356.0142493247986, 359.5215890407562, 362.43292570114136]
[30.433333333333334, 23.275, 27.616666666666667, 34.99166666666667, 32.725, 35.125, 30.066666666666666, 45.516666666666666, 40.541666666666664, 41.55833333333333, 44.09166666666667, 50.36666666666667, 48.40833333333333, 43.983333333333334, 46.9, 49.61666666666667, 51.65, 50.06666666666667, 52.88333333333333, 53.84166666666667, 50.916666666666664, 52.708333333333336, 57.141666666666666, 52.38333333333333, 55.7, 58.69166666666667, 51.333333333333336, 56.65, 58.475, 54.875, 56.425, 51.90833333333333, 58.925, 54.325, 52.025, 52.3, 58.94166666666667, 55.5, 58.5, 59.608333333333334, 58.65, 59.34166666666667, 55.9, 60.46666666666667, 57.175, 58.00833333333333, 57.94166666666667, 59.925, 59.81666666666667, 60.666666666666664, 56.083333333333336, 61.99166666666667, 55.541666666666664, 64.225, 59.891666666666666, 57.233333333333334, 61.99166666666667, 62.05833333333333, 61.325, 62.425, 61.56666666666667, 62.208333333333336, 61.96666666666667, 59.53333333333333, 62.733333333333334, 65.15, 65.56666666666666, 48.583333333333336, 62.05, 61.03333333333333, 65.94166666666666, 55.141666666666666, 62.71666666666667, 60.833333333333336, 57.18333333333333, 63.34166666666667, 63.25833333333333, 61.81666666666667, 64.43333333333334, 59.40833333333333, 64.6, 60.608333333333334, 61.583333333333336, 64.31666666666666, 63.96666666666667, 62.425, 62.19166666666667, 62.375, 65.20833333333333, 62.74166666666667, 66.5, 65.74166666666666, 58.958333333333336, 63.56666666666667, 62.55833333333333, 62.7, 62.333333333333336, 59.74166666666667, 59.266666666666666, 64.39166666666667, 67.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 3.33
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33
Average accuracy final 10 rounds: 3.3333333333333344 

Average global accuracy final 10 rounds: 3.3333333333333344 

4766.456553697586
[1.9226164817810059, 3.39577317237854, 4.866724252700806, 6.323439121246338, 7.785709619522095, 9.25093412399292, 10.70998477935791, 12.182496070861816, 13.648576021194458, 15.10399866104126, 16.559138298034668, 18.02366018295288, 19.478310108184814, 20.92971444129944, 22.369974851608276, 23.8930447101593, 25.35525941848755, 26.80632972717285, 28.27178120613098, 29.728735208511353, 31.176888465881348, 32.64409518241882, 34.09426832199097, 35.54601168632507, 37.0128870010376, 38.468968868255615, 39.938615560531616, 41.40446066856384, 42.88303303718567, 44.350913763046265, 45.823344230651855, 47.28653383255005, 48.76422691345215, 50.24337291717529, 51.68975281715393, 53.14211821556091, 54.60282826423645, 56.07981872558594, 57.54817581176758, 59.005234241485596, 60.460495471954346, 61.908692598342896, 63.35677790641785, 64.82026934623718, 66.30060267448425, 67.76201438903809, 69.22881150245667, 70.68329811096191, 72.12882876396179, 73.57855534553528, 75.02703619003296, 76.46837139129639, 77.92123556137085, 79.37912487983704, 80.83823823928833, 82.31216502189636, 83.78122425079346, 85.25161170959473, 86.70473027229309, 88.15776634216309, 89.61637377738953, 91.0810809135437, 92.54272556304932, 93.9957115650177, 95.4479570388794, 96.91863965988159, 98.38925695419312, 99.86296272277832, 101.32791376113892, 102.77403426170349, 104.23313903808594, 105.6812813282013, 107.14491534233093, 108.60609340667725, 110.06074523925781, 111.51471138000488, 112.97305560112, 114.45079231262207, 115.90951871871948, 117.36450219154358, 118.81871938705444, 120.27564430236816, 121.74890375137329, 123.21602463722229, 124.67639541625977, 126.13644623756409, 127.57558488845825, 129.03986859321594, 130.5104203224182, 131.97973227500916, 133.42048573493958, 134.86667442321777, 136.31932830810547, 137.77168035507202, 139.25702953338623, 140.8728265762329, 142.4377417564392, 144.06133484840393, 145.67446446418762, 147.28692054748535, 148.84912037849426, 150.40973019599915, 151.96341276168823, 153.56556582450867, 155.17981505393982, 156.79245972633362, 158.46508622169495, 160.09489727020264, 161.71915316581726, 163.31926274299622, 164.76216650009155, 166.19560647010803, 167.63035011291504, 169.06668710708618, 170.50038623809814, 171.9439766407013, 173.38741993904114, 174.82245230674744, 176.25309491157532, 177.68388414382935, 179.13139581680298, 180.5644609928131, 181.99683094024658, 183.43616199493408, 184.87606811523438, 186.31565308570862, 187.75559854507446, 189.1890206336975, 190.64917922019958, 192.08451771736145, 193.51411080360413, 194.95477318763733, 196.3888964653015, 197.82144689559937, 199.26099848747253, 200.688640832901, 202.12680864334106, 203.5690987110138, 205.0168538093567, 206.44753766059875, 207.87195873260498, 209.2899363040924, 210.71872091293335, 212.14992594718933, 213.58047103881836, 215.00678825378418, 216.44645404815674, 217.90344738960266, 219.3710355758667, 220.8094882965088, 222.25337958335876, 223.6914918422699, 225.13160300254822, 226.56060647964478, 227.99798893928528, 229.46125483512878, 230.9008288383484, 232.34649634361267, 233.7891664505005, 235.22088265419006, 236.6560549736023, 238.09596061706543, 239.5387008190155, 240.9687478542328, 242.40368843078613, 243.83525609970093, 245.27669739723206, 246.7189347743988, 248.15628290176392, 249.59231400489807, 251.03231000900269, 252.4708034992218, 253.9018256664276, 255.3330361843109, 256.77051305770874, 258.2053792476654, 259.6390426158905, 261.0766382217407, 262.51196002960205, 263.9460217952728, 265.39074897766113, 266.8391649723053, 268.28459310531616, 269.73110032081604, 271.15538692474365, 272.5743260383606, 273.99697518348694, 275.44040298461914, 276.88643312454224, 278.32160568237305, 279.7621760368347, 281.1971399784088, 282.63792991638184, 284.06751465797424, 285.51335287094116, 286.94449520111084, 288.37348079681396, 289.79736161231995, 291.2289183139801, 292.66945695877075, 294.10012912750244, 295.53490114212036, 296.9669075012207, 298.3966498374939, 299.8337621688843, 301.26286721229553, 302.70748591423035, 304.12811517715454, 305.55550050735474, 306.985714673996, 308.42506766319275, 309.86413764953613, 311.2993416786194, 312.75604224205017, 314.20051431655884, 315.62818574905396, 317.07831835746765, 318.5301241874695, 319.96585392951965, 321.3923714160919, 322.8282935619354, 324.2667520046234, 325.70223212242126, 327.137047290802, 328.57260751724243, 330.01571226119995, 331.4440610408783, 332.86868953704834, 334.30875635147095, 335.7541856765747, 337.1800389289856, 338.6233460903168, 340.0865797996521, 341.53941202163696, 342.98169898986816, 344.44784927368164, 345.89058446884155, 347.32918214797974, 348.78801941871643, 350.2085087299347, 351.64253401756287, 353.0981526374817, 354.569384098053, 355.99603390693665, 357.42769837379456, 358.9039521217346, 360.3456883430481, 361.7710053920746, 363.2434813976288, 364.68002700805664, 366.1028814315796, 367.5428144931793, 368.9856207370758, 370.4168658256531, 371.8519251346588, 373.31096482276917, 374.7356653213501, 376.17809104919434, 377.6382281780243, 379.0600755214691, 380.5032591819763, 381.93930101394653, 383.37552976608276, 384.80333614349365, 386.248295545578, 387.6880147457123, 389.1080093383789, 390.5520534515381, 392.01490354537964, 393.4475076198578, 394.87865591049194, 396.30895376205444, 397.7329752445221, 399.1466407775879, 400.56920862197876, 401.9907569885254, 403.4187514781952, 404.83766627311707, 406.25297832489014, 407.6789638996124, 409.1109974384308, 410.5329475402832, 411.9420335292816, 413.36813044548035, 414.7981963157654, 416.20975732803345, 417.62509322166443, 419.05583906173706, 420.4798662662506, 421.90299463272095, 423.3191044330597, 424.7405607700348, 426.1650743484497, 427.5889310836792, 429.0014147758484, 430.4225363731384, 431.8423647880554, 433.27028584480286, 434.6970090866089, 436.1381437778473, 438.4984257221222]
[10.091666666666667, 9.125, 9.033333333333333, 8.708333333333334, 7.116666666666666, 5.75, 5.091666666666667, 6.758333333333334, 6.758333333333334, 6.758333333333334, 5.391666666666667, 5.391666666666667, 5.391666666666667, 3.816666666666667, 3.816666666666667, 3.816666666666667, 3.816666666666667, 3.816666666666667, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.122, Test loss: 0.418, Test accuracy: 88.16
Final Round, Global train loss: 0.122, Global test loss: 1.427, Global test accuracy: 64.42
Average accuracy final 10 rounds: 87.6 

Average global accuracy final 10 rounds: 67.46 

4592.658370733261
[3.8329367637634277, 7.6658735275268555, 11.283259630203247, 14.900645732879639, 18.530495405197144, 22.16034507751465, 25.7821307182312, 29.403916358947754, 33.055365800857544, 36.706815242767334, 40.34996271133423, 43.99311017990112, 47.629270792007446, 51.26543140411377, 54.910648822784424, 58.55586624145508, 62.18956661224365, 65.82326698303223, 69.4542727470398, 73.08527851104736, 76.72591519355774, 80.36655187606812, 83.99927592277527, 87.63199996948242, 91.25143432617188, 94.87086868286133, 98.49188494682312, 102.11290121078491, 105.75951266288757, 109.40612411499023, 113.04289484024048, 116.67966556549072, 120.30174160003662, 123.92381763458252, 127.54951000213623, 131.17520236968994, 134.79641461372375, 138.41762685775757, 142.0456600189209, 145.67369318008423, 149.2882115840912, 152.90272998809814, 156.5185627937317, 160.13439559936523, 163.75515794754028, 167.37592029571533, 170.9940905570984, 174.61226081848145, 178.23175811767578, 181.85125541687012, 185.48749828338623, 189.12374114990234, 192.74620532989502, 196.3686695098877, 199.99394178390503, 203.61921405792236, 207.25538849830627, 210.89156293869019, 214.5272409915924, 218.16291904449463, 221.7903187274933, 225.41771841049194, 229.0438323020935, 232.66994619369507, 236.30343747138977, 239.93692874908447, 243.53191661834717, 247.12690448760986, 250.71359205245972, 254.30027961730957, 257.880229473114, 261.46017932891846, 265.0505061149597, 268.640832901001, 272.2304456233978, 275.8200583457947, 279.3973104953766, 282.9745626449585, 286.56757974624634, 290.1605968475342, 293.7317605018616, 297.30292415618896, 300.8775632381439, 304.4522023200989, 308.05378580093384, 311.6553692817688, 315.245760679245, 318.8361520767212, 322.41946268081665, 326.0027732849121, 329.60701990127563, 333.21126651763916, 336.8304214477539, 340.44957637786865, 344.066166639328, 347.68275690078735, 351.30783581733704, 354.9329147338867, 358.5336112976074, 362.1343078613281, 365.76925015449524, 369.40419244766235, 373.0289800167084, 376.6537675857544, 380.27602458000183, 383.89828157424927, 387.4722957611084, 391.04630994796753, 394.65751671791077, 398.268723487854, 401.8872675895691, 405.5058116912842, 409.1132700443268, 412.7207283973694, 416.3240444660187, 419.92736053466797, 423.54525876045227, 427.1631569862366, 430.7934637069702, 434.42377042770386, 438.0741753578186, 441.72458028793335, 445.35762214660645, 448.99066400527954, 452.624534368515, 456.2584047317505, 459.9133150577545, 463.56822538375854, 467.2138161659241, 470.8594069480896, 474.50792145729065, 478.1564359664917, 481.7738151550293, 485.3911943435669, 488.96241331100464, 492.5336322784424, 495.63855385780334, 498.7434754371643, 501.8284487724304, 504.91342210769653, 508.0174083709717, 511.1213946342468, 514.2161221504211, 517.3108496665955, 520.4443686008453, 523.5778875350952, 526.6948437690735, 529.8118000030518, 532.9084236621857, 536.0050473213196, 539.0863356590271, 542.1676239967346, 545.2552552223206, 548.3428864479065, 551.444118976593, 554.5453515052795, 557.6572864055634, 560.7692213058472, 563.8749809265137, 566.9807405471802, 570.0728607177734, 573.1649808883667, 576.2903318405151, 579.4156827926636, 582.5378713607788, 585.660059928894, 588.7725384235382, 591.8850169181824, 595.002649307251, 598.1202816963196, 601.2402586936951, 604.3602356910706, 607.4405753612518, 610.5209150314331, 613.6775271892548, 616.8341393470764, 619.9749774932861, 623.1158156394958, 626.2390632629395, 629.362310886383, 632.4783599376678, 635.5944089889526, 638.7968621253967, 641.9993152618408, 645.1143276691437, 648.2293400764465, 651.3954021930695, 654.5614643096924, 657.7032518386841, 660.8450393676758, 663.9965307712555, 667.1480221748352, 670.3151814937592, 673.4823408126831, 676.611734867096, 679.7411289215088, 682.9088017940521, 686.0764746665955, 689.2550911903381, 692.4337077140808, 694.5547597408295, 696.6758117675781]
[42.95, 42.95, 48.54666666666667, 48.54666666666667, 62.35333333333333, 62.35333333333333, 64.59333333333333, 64.59333333333333, 70.16333333333333, 70.16333333333333, 75.02, 75.02, 76.71333333333334, 76.71333333333334, 78.31333333333333, 78.31333333333333, 77.85666666666667, 77.85666666666667, 78.23, 78.23, 79.95333333333333, 79.95333333333333, 81.16333333333333, 81.16333333333333, 81.99333333333334, 81.99333333333334, 82.65666666666667, 82.65666666666667, 83.37666666666667, 83.37666666666667, 83.57333333333334, 83.57333333333334, 83.78333333333333, 83.78333333333333, 83.82333333333334, 83.82333333333334, 83.3, 83.3, 83.37333333333333, 83.37333333333333, 83.96666666666667, 83.96666666666667, 84.89333333333333, 84.89333333333333, 85.23666666666666, 85.23666666666666, 85.08333333333333, 85.08333333333333, 85.35666666666667, 85.35666666666667, 85.33, 85.33, 85.32666666666667, 85.32666666666667, 85.27666666666667, 85.27666666666667, 84.85666666666667, 84.85666666666667, 84.89666666666666, 84.89666666666666, 84.89666666666666, 84.89666666666666, 84.75, 84.75, 85.34666666666666, 85.34666666666666, 85.48333333333333, 85.48333333333333, 85.68333333333334, 85.68333333333334, 85.70333333333333, 85.70333333333333, 85.6, 85.6, 86.15666666666667, 86.15666666666667, 85.99333333333334, 85.99333333333334, 86.21666666666667, 86.21666666666667, 86.22, 86.22, 85.83333333333333, 85.83333333333333, 86.01666666666667, 86.01666666666667, 86.05, 86.05, 86.8, 86.8, 86.73, 86.73, 86.82333333333334, 86.82333333333334, 86.71, 86.71, 86.72333333333333, 86.72333333333333, 86.69333333333333, 86.69333333333333, 86.83666666666667, 86.83666666666667, 86.61333333333333, 86.61333333333333, 86.64, 86.64, 86.57, 86.57, 86.62666666666667, 86.62666666666667, 86.7, 86.7, 86.93333333333334, 86.93333333333334, 87.14333333333333, 87.14333333333333, 87.17, 87.17, 87.29, 87.29, 87.26666666666667, 87.26666666666667, 87.35, 87.35, 87.13333333333334, 87.13333333333334, 87.33666666666667, 87.33666666666667, 87.47666666666667, 87.47666666666667, 87.52333333333333, 87.52333333333333, 87.69, 87.69, 87.48666666666666, 87.48666666666666, 87.59666666666666, 87.59666666666666, 87.88333333333334, 87.88333333333334, 87.50333333333333, 87.50333333333333, 87.56666666666666, 87.56666666666666, 87.50333333333333, 87.50333333333333, 87.46, 87.46, 87.42666666666666, 87.42666666666666, 87.39666666666666, 87.39666666666666, 87.82, 87.82, 87.52666666666667, 87.52666666666667, 87.72333333333333, 87.72333333333333, 87.59666666666666, 87.59666666666666, 87.72333333333333, 87.72333333333333, 87.66, 87.66, 87.36, 87.36, 87.47, 87.47, 87.61333333333333, 87.61333333333333, 87.72333333333333, 87.72333333333333, 87.74, 87.74, 87.59666666666666, 87.59666666666666, 87.68666666666667, 87.68666666666667, 87.73, 87.73, 87.82333333333334, 87.82333333333334, 87.86666666666666, 87.86666666666666, 87.89, 87.89, 87.58, 87.58, 87.37333333333333, 87.37333333333333, 87.46666666666667, 87.46666666666667, 87.29333333333334, 87.29333333333334, 87.55666666666667, 87.55666666666667, 87.63, 87.63, 87.52, 87.52, 88.16333333333333, 88.16333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.393, Test loss: 0.786, Test accuracy: 75.85
Average accuracy final 10 rounds: 75.5365 

4812.05218911171
[4.701592683792114, 9.403185367584229, 13.847635269165039, 18.29208517074585, 22.78153395652771, 27.27098274230957, 31.777145862579346, 36.28330898284912, 40.73685145378113, 45.190393924713135, 49.64849805831909, 54.10660219192505, 58.63007640838623, 63.15355062484741, 67.62985157966614, 72.10615253448486, 76.5251784324646, 80.94420433044434, 85.38261413574219, 89.82102394104004, 94.24723148345947, 98.6734390258789, 103.06984806060791, 107.46625709533691, 111.86992192268372, 116.27358675003052, 120.66112875938416, 125.0486707687378, 129.49227213859558, 133.93587350845337, 138.34110689163208, 142.7463402748108, 147.1656572818756, 151.58497428894043, 156.01772022247314, 160.45046615600586, 164.84090089797974, 169.2313356399536, 173.58934235572815, 177.94734907150269, 182.35270404815674, 186.7580590248108, 191.1518750190735, 195.54569101333618, 199.93623781204224, 204.3267846107483, 208.7258656024933, 213.12494659423828, 217.20902466773987, 221.29310274124146, 225.37340021133423, 229.453697681427, 233.4936912059784, 237.53368473052979, 241.89737248420715, 246.26106023788452, 250.62025260925293, 254.97944498062134, 259.33822536468506, 263.6970057487488, 267.7539749145508, 271.8109440803528, 276.21519565582275, 280.6194472312927, 284.9784348011017, 289.33742237091064, 293.702189207077, 298.0669560432434, 302.48097348213196, 306.8949909210205, 311.3183376789093, 315.7416844367981, 320.09907364845276, 324.4564628601074, 328.8301258087158, 333.2037887573242, 337.5793545246124, 341.95492029190063, 346.3286201953888, 350.70232009887695, 355.03925013542175, 359.37618017196655, 363.76043224334717, 368.1446843147278, 372.57463455200195, 377.0045847892761, 381.38259768486023, 385.76061058044434, 390.1558721065521, 394.5511336326599, 398.64447140693665, 402.7378091812134, 407.1018509864807, 411.46589279174805, 415.8214478492737, 420.1770029067993, 424.57513070106506, 428.9732584953308, 433.3439049720764, 437.714551448822, 442.0659465789795, 446.41734170913696, 450.7834346294403, 455.14952754974365, 459.5671606063843, 463.9847936630249, 468.41136407852173, 472.83793449401855, 477.25766158103943, 481.6773886680603, 486.0313274860382, 490.3852663040161, 494.7825403213501, 499.1798143386841, 503.5697476863861, 507.95968103408813, 512.3202152252197, 516.6807494163513, 521.0407366752625, 525.4007239341736, 529.7696371078491, 534.1385502815247, 538.5598344802856, 542.9811186790466, 547.3501834869385, 551.7192482948303, 556.068843126297, 560.4184379577637, 564.7795066833496, 569.1405754089355, 573.5064306259155, 577.8722858428955, 582.2620847225189, 586.6518836021423, 591.014853477478, 595.3778233528137, 599.7390773296356, 604.1003313064575, 608.4583296775818, 612.816328048706, 617.1943345069885, 621.572340965271, 625.9566202163696, 630.3408994674683, 634.7531132698059, 639.1653270721436, 643.595162153244, 648.0249972343445, 652.4303386211395, 656.8356800079346, 660.8620836734772, 664.8884873390198, 668.8593957424164, 672.830304145813, 676.9079241752625, 680.9855442047119, 685.0524740219116, 689.1194038391113, 693.1008474826813, 697.0822911262512, 701.05815076828, 705.0340104103088, 709.0903606414795, 713.1467108726501, 717.1867153644562, 721.2267198562622, 725.212545633316, 729.1983714103699, 733.3045411109924, 737.410710811615, 741.4724435806274, 745.5341763496399, 749.5912902355194, 753.6484041213989, 757.6371591091156, 761.6259140968323, 765.6640372276306, 769.702160358429, 773.72070479393, 777.7392492294312, 781.7192487716675, 785.6992483139038, 789.6955740451813, 793.6918997764587, 797.698881149292, 801.7058625221252, 805.713148355484, 809.7204341888428, 813.7238450050354, 817.727255821228, 821.7346580028534, 825.7420601844788, 829.7568707466125, 833.7716813087463, 837.764030456543, 841.7563796043396, 845.7562832832336, 849.7561869621277, 853.9327282905579, 858.109269618988, 860.0503842830658, 861.9914989471436]
[30.7675, 30.7675, 39.4525, 39.4525, 43.365, 43.365, 47.06, 47.06, 50.655, 50.655, 52.6675, 52.6675, 54.835, 54.835, 56.6775, 56.6775, 58.7325, 58.7325, 60.4375, 60.4375, 61.7875, 61.7875, 62.695, 62.695, 63.2225, 63.2225, 64.32, 64.32, 64.785, 64.785, 65.44, 65.44, 66.43, 66.43, 67.3225, 67.3225, 67.5275, 67.5275, 68.2225, 68.2225, 68.9375, 68.9375, 69.54, 69.54, 70.5425, 70.5425, 70.3025, 70.3025, 70.5025, 70.5025, 70.885, 70.885, 70.765, 70.765, 70.9525, 70.9525, 71.075, 71.075, 71.2375, 71.2375, 71.8525, 71.8525, 72.4125, 72.4125, 72.23, 72.23, 71.565, 71.565, 72.305, 72.305, 71.8825, 71.8825, 72.025, 72.025, 72.59, 72.59, 72.355, 72.355, 72.7825, 72.7825, 73.0375, 73.0375, 73.1775, 73.1775, 73.2075, 73.2075, 73.565, 73.565, 73.9325, 73.9325, 73.495, 73.495, 74.2625, 74.2625, 73.95, 73.95, 73.8875, 73.8875, 74.36, 74.36, 74.69, 74.69, 74.6675, 74.6675, 74.7275, 74.7275, 74.345, 74.345, 74.42, 74.42, 74.4975, 74.4975, 74.7175, 74.7175, 74.8875, 74.8875, 74.535, 74.535, 74.5375, 74.5375, 74.5925, 74.5925, 74.215, 74.215, 75.0175, 75.0175, 74.745, 74.745, 75.165, 75.165, 75.4875, 75.4875, 74.815, 74.815, 74.92, 74.92, 74.8, 74.8, 75.1475, 75.1475, 75.315, 75.315, 75.32, 75.32, 74.7225, 74.7225, 75.095, 75.095, 75.465, 75.465, 74.9775, 74.9775, 75.495, 75.495, 75.285, 75.285, 75.365, 75.365, 75.375, 75.375, 74.985, 74.985, 75.2875, 75.2875, 75.3725, 75.3725, 75.4875, 75.4875, 75.8625, 75.8625, 74.935, 74.935, 75.355, 75.355, 75.315, 75.315, 75.585, 75.585, 75.485, 75.485, 75.5325, 75.5325, 75.195, 75.195, 75.5175, 75.5175, 75.4825, 75.4825, 75.545, 75.545, 75.7925, 75.7925, 75.55, 75.55, 75.835, 75.835, 75.2225, 75.2225, 75.6925, 75.6925, 75.8475, 75.8475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.411, Test loss: 0.712, Test accuracy: 77.03
Average accuracy final 10 rounds: 76.55775000000001
5401.598746538162
[6.1236701011657715, 12.247340202331543, 17.948312759399414, 23.649285316467285, 29.239200115203857, 34.82911491394043, 40.36192226409912, 45.89472961425781, 51.64509439468384, 57.39545917510986, 62.6439266204834, 67.89239406585693, 73.30572414398193, 78.71905422210693, 83.98086214065552, 89.2426700592041, 94.38716793060303, 99.53166580200195, 104.67470741271973, 109.8177490234375, 114.96302771568298, 120.10830640792847, 125.26213550567627, 130.41596460342407, 135.55979299545288, 140.7036213874817, 145.8853120803833, 151.0670027732849, 156.2658863067627, 161.46476984024048, 166.64534950256348, 171.82592916488647, 177.02201509475708, 182.21810102462769, 187.3988320827484, 192.57956314086914, 197.75292778015137, 202.9262924194336, 208.11087036132812, 213.29544830322266, 218.43688464164734, 223.57832098007202, 228.79441857337952, 234.010516166687, 239.20528388023376, 244.40005159378052, 249.59628224372864, 254.79251289367676, 260.0449764728546, 265.29744005203247, 270.5734784603119, 275.8495168685913, 281.1472840309143, 286.4450511932373, 291.7343466281891, 297.02364206314087, 302.2842411994934, 307.54484033584595, 312.764155626297, 317.98347091674805, 323.21989130973816, 328.45631170272827, 333.64102482795715, 338.82573795318604, 344.05893898010254, 349.29214000701904, 354.54892349243164, 359.80570697784424, 365.10840129852295, 370.41109561920166, 375.6495006084442, 380.88790559768677, 386.0651822090149, 391.242458820343, 396.482684135437, 401.722909450531, 406.8903388977051, 412.05776834487915, 417.3005564212799, 422.54334449768066, 427.7606031894684, 432.9778618812561, 438.1720333099365, 443.36620473861694, 448.53465509414673, 453.7031054496765, 458.8673598766327, 464.03161430358887, 469.2120113372803, 474.3924083709717, 479.6427102088928, 484.89301204681396, 490.1475455760956, 495.4020791053772, 500.59223198890686, 505.7823848724365, 510.97222995758057, 516.1620750427246, 521.3842601776123, 526.6064453125, 531.8579542636871, 537.1094632148743, 542.3491861820221, 547.5889091491699, 552.7698175907135, 557.9507260322571, 563.1654863357544, 568.3802466392517, 573.6391978263855, 578.8981490135193, 584.1098258495331, 589.3215026855469, 594.5276219844818, 599.7337412834167, 604.9029450416565, 610.0721487998962, 615.2893092632294, 620.5064697265625, 625.7267551422119, 630.9470405578613, 636.099570274353, 641.2520999908447, 646.4466075897217, 651.6411151885986, 656.8766207695007, 662.1121263504028, 667.3145365715027, 672.5169467926025, 677.774575471878, 683.0322041511536, 688.2184228897095, 693.4046416282654, 698.6151418685913, 703.8256421089172, 708.9983282089233, 714.1710143089294, 719.343502998352, 724.5159916877747, 729.6698729991913, 734.8237543106079, 740.5367951393127, 746.2498359680176, 751.4575235843658, 756.6652112007141, 761.8568134307861, 767.0484156608582, 772.213593006134, 777.3787703514099, 782.4966809749603, 787.6145915985107, 792.7993099689484, 797.984028339386, 803.2553939819336, 808.5267596244812, 813.7826945781708, 819.0386295318604, 824.233892917633, 829.4291563034058, 834.6513321399689, 839.873507976532, 845.059818983078, 850.246129989624, 855.4181530475616, 860.5901761054993, 865.7535228729248, 870.9168696403503, 876.1436800956726, 881.3704905509949, 886.6597521305084, 891.949013710022, 897.1876304149628, 902.4262471199036, 907.6778862476349, 912.9295253753662, 918.095588684082, 923.2616519927979, 928.7124087810516, 934.1631655693054, 939.7768869400024, 945.3906083106995, 950.7470285892487, 956.1034488677979, 961.8028726577759, 967.5022964477539, 973.0785844326019, 978.65487241745, 984.2397043704987, 989.8245363235474, 995.4916467666626, 1001.1587572097778, 1006.7687542438507, 1012.3787512779236, 1017.9241824150085, 1023.4696135520935, 1029.0826573371887, 1034.695701122284, 1040.3726794719696, 1046.0496578216553, 1051.6716120243073, 1057.2935662269592, 1059.4328982830048, 1061.5722303390503]
[34.26, 34.26, 42.465, 42.465, 46.755, 46.755, 50.125, 50.125, 53.215, 53.215, 55.7675, 55.7675, 57.5775, 57.5775, 59.19, 59.19, 61.1475, 61.1475, 62.195, 62.195, 63.9025, 63.9025, 64.24, 64.24, 64.9775, 64.9775, 65.8625, 65.8625, 67.6475, 67.6475, 67.8075, 67.8075, 68.78, 68.78, 69.2575, 69.2575, 69.6025, 69.6025, 70.2475, 70.2475, 70.5725, 70.5725, 70.7375, 70.7375, 71.01, 71.01, 71.955, 71.955, 72.05, 72.05, 72.055, 72.055, 72.7075, 72.7075, 72.515, 72.515, 72.91, 72.91, 73.1125, 73.1125, 73.375, 73.375, 73.3825, 73.3825, 73.8125, 73.8125, 73.81, 73.81, 73.6675, 73.6675, 73.6325, 73.6325, 74.1275, 74.1275, 74.7725, 74.7725, 74.745, 74.745, 74.99, 74.99, 75.08, 75.08, 74.4375, 74.4375, 75.0725, 75.0725, 74.58, 74.58, 74.865, 74.865, 75.1725, 75.1725, 75.29, 75.29, 75.2825, 75.2825, 74.965, 74.965, 74.88, 74.88, 74.33, 74.33, 75.1125, 75.1125, 75.34, 75.34, 75.4625, 75.4625, 75.43, 75.43, 75.615, 75.615, 76.0525, 76.0525, 76.06, 76.06, 76.2825, 76.2825, 76.0325, 76.0325, 76.1225, 76.1225, 76.205, 76.205, 76.0475, 76.0475, 76.235, 76.235, 76.27, 76.27, 76.085, 76.085, 76.3475, 76.3475, 76.0025, 76.0025, 75.9475, 75.9475, 75.835, 75.835, 75.8975, 75.8975, 76.08, 76.08, 76.385, 76.385, 76.3325, 76.3325, 76.4775, 76.4775, 76.5575, 76.5575, 76.3375, 76.3375, 76.4925, 76.4925, 76.38, 76.38, 76.5175, 76.5175, 76.6725, 76.6725, 76.95, 76.95, 76.41, 76.41, 76.675, 76.675, 76.345, 76.345, 76.1, 76.1, 76.295, 76.295, 76.1425, 76.1425, 76.6, 76.6, 76.2575, 76.2575, 76.1075, 76.1075, 76.6775, 76.6775, 76.7, 76.7, 76.2075, 76.2075, 76.795, 76.795, 76.6575, 76.6575, 76.605, 76.605, 76.625, 76.625, 76.485, 76.485, 76.7175, 76.7175, 77.0325, 77.0325]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.311, Test loss: 0.782, Test accuracy: 78.30
Average accuracy final 10 rounds: 77.65825000000001
8257.453884363174
[12.690705299377441, 25.43014359474182, 36.879204988479614, 48.51527690887451, 60.027456521987915, 71.45104646682739, 82.96655678749084, 94.5383608341217, 106.13788676261902, 117.69881415367126, 129.49901270866394, 141.308940410614, 153.1862154006958, 164.92226123809814, 177.3202531337738, 189.80065774917603, 202.568457365036, 214.3317461013794, 225.9854519367218, 238.16659355163574, 250.39165353775024, 262.66484570503235, 274.8114740848541, 286.82700991630554, 298.58262515068054, 310.49633836746216, 322.3132290840149, 334.9021499156952, 348.1485695838928, 359.71328592300415, 371.37737703323364, 383.0249137878418, 394.9690103530884, 406.52246022224426, 418.11487078666687, 429.6224801540375, 441.31155824661255, 452.9977960586548, 464.48429703712463, 475.91741490364075, 487.43187618255615, 498.9429597854614, 510.398264169693, 522.1541845798492, 533.8588707447052, 545.3597621917725, 556.8214380741119, 568.2832446098328, 579.7976505756378, 591.2838666439056, 602.7641735076904, 614.3193819522858, 625.863664150238, 637.3556699752808, 649.2262024879456, 661.9991140365601, 675.306512594223, 688.4369809627533, 701.5401225090027, 713.7425670623779, 725.6035301685333, 737.4042267799377, 749.754798412323, 762.3235759735107, 774.739061832428, 786.7161350250244, 799.0224504470825, 811.2536990642548, 823.2261328697205, 835.5218670368195, 848.383951663971, 860.9727046489716, 873.0656719207764, 884.6451230049133, 896.7185969352722, 908.7425563335419, 920.6967813968658, 932.5570466518402, 944.4067742824554, 956.2993121147156, 968.024158000946, 979.7430222034454, 991.5935833454132, 1003.3939979076385, 1015.0186142921448, 1026.791808605194, 1038.8525910377502, 1050.838965177536, 1062.8251769542694, 1074.6741564273834, 1086.4804656505585, 1098.1860411167145, 1109.7358031272888, 1121.6991991996765, 1133.58438539505, 1145.5654616355896, 1157.44065284729, 1169.2979137897491, 1181.3652379512787, 1193.5294258594513, 1196.792396068573]
[38.9375, 46.575, 51.1025, 56.38, 59.7225, 61.6025, 62.795, 64.5025, 65.71, 66.7925, 67.1025, 68.3325, 68.8, 69.6775, 70.035, 70.8075, 71.0325, 71.68, 71.895, 72.33, 72.545, 73.09, 73.7, 73.61, 73.8775, 74.06, 73.665, 74.6525, 75.2525, 74.84, 74.4275, 75.1675, 75.825, 74.875, 75.26, 75.925, 75.8075, 75.54, 75.9725, 75.4575, 75.8725, 76.29, 76.2, 76.195, 76.1125, 76.14, 75.9475, 76.6475, 76.365, 76.13, 76.5825, 76.385, 76.6325, 76.48, 76.84, 76.38, 76.8625, 76.685, 76.8975, 76.8875, 76.8325, 77.0175, 76.9225, 76.9025, 76.8825, 76.8225, 77.5325, 76.8, 77.0325, 76.7875, 77.015, 77.165, 76.7725, 77.4, 77.4075, 77.3375, 77.48, 77.445, 77.78, 77.65, 77.655, 77.5275, 77.705, 76.7925, 77.4575, 77.6, 77.4425, 77.5225, 77.7125, 78.1575, 77.415, 77.59, 77.0475, 77.985, 77.895, 77.715, 78.045, 77.905, 77.4675, 77.5175, 78.295]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.301, Test accuracy: 11.38
Round   0, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 11.47
Round   1, Train loss: 2.303, Test loss: 2.301, Test accuracy: 11.63
Round   1, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 11.60
Round   2, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.83
Round   2, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.81
Round   3, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.96
Round   3, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 12.44
Round   4, Train loss: 2.302, Test loss: 2.300, Test accuracy: 11.96
Round   4, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 12.31
Round   5, Train loss: 2.301, Test loss: 2.300, Test accuracy: 12.09
Round   5, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 12.21
Round   6, Train loss: 2.301, Test loss: 2.300, Test accuracy: 12.38
Round   6, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 12.78
Round   7, Train loss: 2.300, Test loss: 2.299, Test accuracy: 12.68
Round   7, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 13.18
Round   8, Train loss: 2.300, Test loss: 2.299, Test accuracy: 12.76
Round   8, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 12.94
Round   9, Train loss: 2.300, Test loss: 2.299, Test accuracy: 12.77
Round   9, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 13.05
Round  10, Train loss: 2.300, Test loss: 2.299, Test accuracy: 12.87
Round  10, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 13.37
Round  11, Train loss: 2.299, Test loss: 2.299, Test accuracy: 13.00
Round  11, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 13.24
Round  12, Train loss: 2.299, Test loss: 2.298, Test accuracy: 13.09
Round  12, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 13.80
Round  13, Train loss: 2.299, Test loss: 2.298, Test accuracy: 13.01
Round  13, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 13.26
Round  14, Train loss: 2.298, Test loss: 2.298, Test accuracy: 13.01
Round  14, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 13.44
Round  15, Train loss: 2.298, Test loss: 2.297, Test accuracy: 13.05
Round  15, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 13.69
Round  16, Train loss: 2.298, Test loss: 2.297, Test accuracy: 13.03
Round  16, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 13.40
Round  17, Train loss: 2.298, Test loss: 2.297, Test accuracy: 13.48
Round  17, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 14.11
Round  18, Train loss: 2.298, Test loss: 2.297, Test accuracy: 13.52
Round  18, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 14.35
Round  19, Train loss: 2.297, Test loss: 2.296, Test accuracy: 13.74
Round  19, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 14.69
Round  20, Train loss: 2.297, Test loss: 2.296, Test accuracy: 14.14
Round  20, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 14.67
Round  21, Train loss: 2.297, Test loss: 2.296, Test accuracy: 14.59
Round  21, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 15.21
Round  22, Train loss: 2.296, Test loss: 2.295, Test accuracy: 14.74
Round  22, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 15.81
Round  23, Train loss: 2.296, Test loss: 2.295, Test accuracy: 14.80
Round  23, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 15.81
Round  24, Train loss: 2.296, Test loss: 2.295, Test accuracy: 14.98
Round  24, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 15.89
Round  25, Train loss: 2.296, Test loss: 2.295, Test accuracy: 15.22
Round  25, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 16.33
Round  26, Train loss: 2.295, Test loss: 2.294, Test accuracy: 15.56
Round  26, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 16.12
Round  27, Train loss: 2.295, Test loss: 2.294, Test accuracy: 15.71
Round  27, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 16.21
Round  28, Train loss: 2.296, Test loss: 2.294, Test accuracy: 15.72
Round  28, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 16.38
Round  29, Train loss: 2.295, Test loss: 2.293, Test accuracy: 15.96
Round  29, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 16.53
Round  30, Train loss: 2.294, Test loss: 2.293, Test accuracy: 16.23
Round  30, Global train loss: 2.294, Global test loss: 2.292, Global test accuracy: 17.03
Round  31, Train loss: 2.294, Test loss: 2.293, Test accuracy: 16.38
Round  31, Global train loss: 2.294, Global test loss: 2.292, Global test accuracy: 16.85
Round  32, Train loss: 2.294, Test loss: 2.292, Test accuracy: 16.58
Round  32, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 16.85
Round  33, Train loss: 2.294, Test loss: 2.292, Test accuracy: 16.52
Round  33, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 16.46
Round  34, Train loss: 2.293, Test loss: 2.292, Test accuracy: 16.27
Round  34, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 16.17
Round  35, Train loss: 2.293, Test loss: 2.291, Test accuracy: 16.10
Round  35, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 16.17
Round  36, Train loss: 2.293, Test loss: 2.291, Test accuracy: 16.34
Round  36, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 16.65
Round  37, Train loss: 2.293, Test loss: 2.290, Test accuracy: 16.15
Round  37, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 16.32
Round  38, Train loss: 2.293, Test loss: 2.290, Test accuracy: 15.82
Round  38, Global train loss: 2.293, Global test loss: 2.289, Global test accuracy: 15.98
Round  39, Train loss: 2.292, Test loss: 2.290, Test accuracy: 15.76
Round  39, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 15.82
Round  40, Train loss: 2.292, Test loss: 2.289, Test accuracy: 15.74
Round  40, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 15.82
Round  41, Train loss: 2.292, Test loss: 2.289, Test accuracy: 15.65
Round  41, Global train loss: 2.292, Global test loss: 2.288, Global test accuracy: 16.06
Round  42, Train loss: 2.291, Test loss: 2.289, Test accuracy: 15.64
Round  42, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 15.98
Round  43, Train loss: 2.291, Test loss: 2.288, Test accuracy: 15.63
Round  43, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 16.18
Round  44, Train loss: 2.291, Test loss: 2.288, Test accuracy: 15.55
Round  44, Global train loss: 2.291, Global test loss: 2.287, Global test accuracy: 15.30
Round  45, Train loss: 2.291, Test loss: 2.287, Test accuracy: 15.19
Round  45, Global train loss: 2.291, Global test loss: 2.286, Global test accuracy: 14.58
Round  46, Train loss: 2.290, Test loss: 2.287, Test accuracy: 14.85
Round  46, Global train loss: 2.290, Global test loss: 2.286, Global test accuracy: 14.67
Round  47, Train loss: 2.290, Test loss: 2.287, Test accuracy: 14.84
Round  47, Global train loss: 2.290, Global test loss: 2.285, Global test accuracy: 15.16
Round  48, Train loss: 2.290, Test loss: 2.286, Test accuracy: 14.91
Round  48, Global train loss: 2.290, Global test loss: 2.285, Global test accuracy: 14.96
Round  49, Train loss: 2.289, Test loss: 2.286, Test accuracy: 15.05
Round  49, Global train loss: 2.289, Global test loss: 2.285, Global test accuracy: 14.85
Round  50, Train loss: 2.288, Test loss: 2.285, Test accuracy: 15.02
Round  50, Global train loss: 2.288, Global test loss: 2.284, Global test accuracy: 14.83
Round  51, Train loss: 2.289, Test loss: 2.285, Test accuracy: 15.01
Round  51, Global train loss: 2.289, Global test loss: 2.284, Global test accuracy: 14.73
Round  52, Train loss: 2.288, Test loss: 2.284, Test accuracy: 15.45
Round  52, Global train loss: 2.288, Global test loss: 2.284, Global test accuracy: 15.82
Round  53, Train loss: 2.287, Test loss: 2.284, Test accuracy: 16.13
Round  53, Global train loss: 2.287, Global test loss: 2.283, Global test accuracy: 16.39
Round  54, Train loss: 2.287, Test loss: 2.284, Test accuracy: 16.33
Round  54, Global train loss: 2.287, Global test loss: 2.283, Global test accuracy: 16.54
Round  55, Train loss: 2.287, Test loss: 2.283, Test accuracy: 16.50
Round  55, Global train loss: 2.287, Global test loss: 2.282, Global test accuracy: 16.55
Round  56, Train loss: 2.286, Test loss: 2.283, Test accuracy: 16.67
Round  56, Global train loss: 2.286, Global test loss: 2.282, Global test accuracy: 16.98
Round  57, Train loss: 2.286, Test loss: 2.283, Test accuracy: 16.52
Round  57, Global train loss: 2.286, Global test loss: 2.281, Global test accuracy: 17.51
Round  58, Train loss: 2.285, Test loss: 2.282, Test accuracy: 16.92
Round  58, Global train loss: 2.285, Global test loss: 2.280, Global test accuracy: 16.75
Round  59, Train loss: 2.286, Test loss: 2.282, Test accuracy: 17.03
Round  59, Global train loss: 2.286, Global test loss: 2.280, Global test accuracy: 17.31
Round  60, Train loss: 2.284, Test loss: 2.281, Test accuracy: 16.81
Round  60, Global train loss: 2.284, Global test loss: 2.279, Global test accuracy: 17.06
Round  61, Train loss: 2.284, Test loss: 2.281, Test accuracy: 16.78
Round  61, Global train loss: 2.284, Global test loss: 2.279, Global test accuracy: 17.28
Round  62, Train loss: 2.283, Test loss: 2.280, Test accuracy: 16.86
Round  62, Global train loss: 2.283, Global test loss: 2.278, Global test accuracy: 16.69
Round  63, Train loss: 2.282, Test loss: 2.279, Test accuracy: 16.87
Round  63, Global train loss: 2.282, Global test loss: 2.278, Global test accuracy: 16.87
Round  64, Train loss: 2.283, Test loss: 2.279, Test accuracy: 17.37
Round  64, Global train loss: 2.283, Global test loss: 2.277, Global test accuracy: 17.98
Round  65, Train loss: 2.283, Test loss: 2.278, Test accuracy: 18.16
Round  65, Global train loss: 2.283, Global test loss: 2.277, Global test accuracy: 20.24
Round  66, Train loss: 2.283, Test loss: 2.278, Test accuracy: 18.29
Round  66, Global train loss: 2.283, Global test loss: 2.277, Global test accuracy: 20.48
Round  67, Train loss: 2.282, Test loss: 2.277, Test accuracy: 18.82
Round  67, Global train loss: 2.282, Global test loss: 2.276, Global test accuracy: 19.62
Round  68, Train loss: 2.282, Test loss: 2.277, Test accuracy: 19.18
Round  68, Global train loss: 2.282, Global test loss: 2.275, Global test accuracy: 20.37
Round  69, Train loss: 2.281, Test loss: 2.276, Test accuracy: 19.70
Round  69, Global train loss: 2.281, Global test loss: 2.275, Global test accuracy: 20.88
Round  70, Train loss: 2.281, Test loss: 2.276, Test accuracy: 20.25
Round  70, Global train loss: 2.281, Global test loss: 2.274, Global test accuracy: 21.36
Round  71, Train loss: 2.280, Test loss: 2.275, Test accuracy: 20.20
Round  71, Global train loss: 2.280, Global test loss: 2.274, Global test accuracy: 20.83
Round  72, Train loss: 2.280, Test loss: 2.275, Test accuracy: 19.94
Round  72, Global train loss: 2.280, Global test loss: 2.273, Global test accuracy: 19.62
Round  73, Train loss: 2.279, Test loss: 2.274, Test accuracy: 19.59
Round  73, Global train loss: 2.279, Global test loss: 2.272, Global test accuracy: 19.18
Round  74, Train loss: 2.278, Test loss: 2.273, Test accuracy: 19.22
Round  74, Global train loss: 2.278, Global test loss: 2.271, Global test accuracy: 18.55
Round  75, Train loss: 2.278, Test loss: 2.272, Test accuracy: 18.95
Round  75, Global train loss: 2.278, Global test loss: 2.270, Global test accuracy: 18.29
Round  76, Train loss: 2.279, Test loss: 2.271, Test accuracy: 18.27
Round  76, Global train loss: 2.279, Global test loss: 2.269, Global test accuracy: 17.36
Round  77, Train loss: 2.276, Test loss: 2.270, Test accuracy: 17.91
Round  77, Global train loss: 2.276, Global test loss: 2.268, Global test accuracy: 17.99
Round  78, Train loss: 2.277, Test loss: 2.270, Test accuracy: 17.60
Round  78, Global train loss: 2.277, Global test loss: 2.267, Global test accuracy: 16.89
Round  79, Train loss: 2.276, Test loss: 2.269, Test accuracy: 17.30
Round  79, Global train loss: 2.276, Global test loss: 2.267, Global test accuracy: 17.49
Round  80, Train loss: 2.275, Test loss: 2.268, Test accuracy: 17.53
Round  80, Global train loss: 2.275, Global test loss: 2.265, Global test accuracy: 17.08
Round  81, Train loss: 2.275, Test loss: 2.267, Test accuracy: 17.91
Round  81, Global train loss: 2.275, Global test loss: 2.265, Global test accuracy: 17.87
Round  82, Train loss: 2.274, Test loss: 2.266, Test accuracy: 18.08
Round  82, Global train loss: 2.274, Global test loss: 2.264, Global test accuracy: 19.35
Round  83, Train loss: 2.274, Test loss: 2.265, Test accuracy: 17.97
Round  83, Global train loss: 2.274, Global test loss: 2.263, Global test accuracy: 18.13
Round  84, Train loss: 2.273, Test loss: 2.264, Test accuracy: 17.64
Round  84, Global train loss: 2.273, Global test loss: 2.262, Global test accuracy: 17.47
Round  85, Train loss: 2.274, Test loss: 2.264, Test accuracy: 17.54
Round  85, Global train loss: 2.274, Global test loss: 2.261, Global test accuracy: 16.93
Round  86, Train loss: 2.272, Test loss: 2.263, Test accuracy: 17.18
Round  86, Global train loss: 2.272, Global test loss: 2.261, Global test accuracy: 16.43
Round  87, Train loss: 2.271, Test loss: 2.261, Test accuracy: 16.76
Round  87, Global train loss: 2.271, Global test loss: 2.260, Global test accuracy: 16.75
Round  88, Train loss: 2.270, Test loss: 2.261, Test accuracy: 16.44
Round  88, Global train loss: 2.270, Global test loss: 2.258, Global test accuracy: 16.05
Round  89, Train loss: 2.270, Test loss: 2.259, Test accuracy: 16.90
Round  89, Global train loss: 2.270, Global test loss: 2.257, Global test accuracy: 17.94
Round  90, Train loss: 2.269, Test loss: 2.258, Test accuracy: 17.29
Round  90, Global train loss: 2.269, Global test loss: 2.256, Global test accuracy: 17.66
Round  91, Train loss: 2.267, Test loss: 2.257, Test accuracy: 17.61
Round  91, Global train loss: 2.267, Global test loss: 2.255, Global test accuracy: 18.45
Round  92, Train loss: 2.268, Test loss: 2.256, Test accuracy: 18.10
Round  92, Global train loss: 2.268, Global test loss: 2.254, Global test accuracy: 17.96
Round  93, Train loss: 2.267, Test loss: 2.255, Test accuracy: 18.27
Round  93, Global train loss: 2.267, Global test loss: 2.253, Global test accuracy: 17.71
Round  94, Train loss: 2.266, Test loss: 2.254, Test accuracy: 18.11
Round  94, Global train loss: 2.266, Global test loss: 2.251, Global test accuracy: 18.04
Round  95, Train loss: 2.265, Test loss: 2.252, Test accuracy: 18.31
Round  95, Global train loss: 2.265, Global test loss: 2.249, Global test accuracy: 18.06
Round  96, Train loss: 2.265, Test loss: 2.251, Test accuracy: 18.09
Round  96, Global train loss: 2.265, Global test loss: 2.249, Global test accuracy: 18.21
Round  97, Train loss: 2.266, Test loss: 2.251, Test accuracy: 18.32
Round  97, Global train loss: 2.266, Global test loss: 2.248, Global test accuracy: 19.40
Round  98, Train loss: 2.262, Test loss: 2.250, Test accuracy: 18.47
Round  98, Global train loss: 2.262, Global test loss: 2.247, Global test accuracy: 20.05
Round  99, Train loss: 2.261, Test loss: 2.249, Test accuracy: 18.70
Round  99, Global train loss: 2.261, Global test loss: 2.247, Global test accuracy: 20.73
Round 100, Train loss: 2.261, Test loss: 2.248, Test accuracy: 19.67
Round 100, Global train loss: 2.261, Global test loss: 2.246, Global test accuracy: 21.34
Round 101, Train loss: 2.259, Test loss: 2.247, Test accuracy: 19.65
Round 101, Global train loss: 2.259, Global test loss: 2.244, Global test accuracy: 20.72
Round 102, Train loss: 2.261, Test loss: 2.246, Test accuracy: 20.02
Round 102, Global train loss: 2.261, Global test loss: 2.243, Global test accuracy: 20.82
Round 103, Train loss: 2.259, Test loss: 2.244, Test accuracy: 19.70
Round 103, Global train loss: 2.259, Global test loss: 2.240, Global test accuracy: 19.41
Round 104, Train loss: 2.256, Test loss: 2.243, Test accuracy: 19.16
Round 104, Global train loss: 2.256, Global test loss: 2.239, Global test accuracy: 18.18
Round 105, Train loss: 2.259, Test loss: 2.242, Test accuracy: 19.12
Round 105, Global train loss: 2.259, Global test loss: 2.237, Global test accuracy: 19.41
Round 106, Train loss: 2.255, Test loss: 2.240, Test accuracy: 19.27
Round 106, Global train loss: 2.255, Global test loss: 2.235, Global test accuracy: 19.23
Round 107, Train loss: 2.255, Test loss: 2.239, Test accuracy: 19.35
Round 107, Global train loss: 2.255, Global test loss: 2.235, Global test accuracy: 18.80
Round 108, Train loss: 2.255, Test loss: 2.237, Test accuracy: 19.67
Round 108, Global train loss: 2.255, Global test loss: 2.234, Global test accuracy: 19.24
Round 109, Train loss: 2.253, Test loss: 2.237, Test accuracy: 19.52
Round 109, Global train loss: 2.253, Global test loss: 2.234, Global test accuracy: 19.74
Round 110, Train loss: 2.255, Test loss: 2.235, Test accuracy: 19.53
Round 110, Global train loss: 2.255, Global test loss: 2.232, Global test accuracy: 19.33
Round 111, Train loss: 2.251, Test loss: 2.233, Test accuracy: 19.52
Round 111, Global train loss: 2.251, Global test loss: 2.230, Global test accuracy: 19.64
Round 112, Train loss: 2.252, Test loss: 2.232, Test accuracy: 19.52
Round 112, Global train loss: 2.252, Global test loss: 2.229, Global test accuracy: 19.26
Round 113, Train loss: 2.249, Test loss: 2.231, Test accuracy: 19.72
Round 113, Global train loss: 2.249, Global test loss: 2.228, Global test accuracy: 19.91
Round 114, Train loss: 2.249, Test loss: 2.230, Test accuracy: 19.97
Round 114, Global train loss: 2.249, Global test loss: 2.227, Global test accuracy: 20.84
Round 115, Train loss: 2.248, Test loss: 2.229, Test accuracy: 20.59
Round 115, Global train loss: 2.248, Global test loss: 2.226, Global test accuracy: 21.95
Round 116, Train loss: 2.247, Test loss: 2.228, Test accuracy: 21.07
Round 116, Global train loss: 2.247, Global test loss: 2.226, Global test accuracy: 22.33
Round 117, Train loss: 2.246, Test loss: 2.227, Test accuracy: 21.00
Round 117, Global train loss: 2.246, Global test loss: 2.225, Global test accuracy: 22.50
Round 118, Train loss: 2.247, Test loss: 2.226, Test accuracy: 21.42
Round 118, Global train loss: 2.247, Global test loss: 2.222, Global test accuracy: 22.77
Round 119, Train loss: 2.244, Test loss: 2.225, Test accuracy: 21.87
Round 119, Global train loss: 2.244, Global test loss: 2.222, Global test accuracy: 24.16
Round 120, Train loss: 2.244, Test loss: 2.223, Test accuracy: 22.14
Round 120, Global train loss: 2.244, Global test loss: 2.219, Global test accuracy: 24.17
Round 121, Train loss: 2.242, Test loss: 2.222, Test accuracy: 22.22
Round 121, Global train loss: 2.242, Global test loss: 2.218, Global test accuracy: 23.73
Round 122, Train loss: 2.241, Test loss: 2.220, Test accuracy: 22.82
Round 122, Global train loss: 2.241, Global test loss: 2.218, Global test accuracy: 23.46
Round 123, Train loss: 2.239, Test loss: 2.219, Test accuracy: 22.67
Round 123, Global train loss: 2.239, Global test loss: 2.216, Global test accuracy: 23.19
Round 124, Train loss: 2.241, Test loss: 2.218, Test accuracy: 22.55
Round 124, Global train loss: 2.241, Global test loss: 2.215, Global test accuracy: 22.26
Round 125, Train loss: 2.239, Test loss: 2.216, Test accuracy: 22.55
Round 125, Global train loss: 2.239, Global test loss: 2.213, Global test accuracy: 22.40
Round 126, Train loss: 2.240, Test loss: 2.216, Test accuracy: 22.35
Round 126, Global train loss: 2.240, Global test loss: 2.212, Global test accuracy: 22.41
Round 127, Train loss: 2.237, Test loss: 2.215, Test accuracy: 22.62
Round 127, Global train loss: 2.237, Global test loss: 2.211, Global test accuracy: 23.70
Round 128, Train loss: 2.236, Test loss: 2.213, Test accuracy: 22.91
Round 128, Global train loss: 2.236, Global test loss: 2.210, Global test accuracy: 23.83
Round 129, Train loss: 2.235, Test loss: 2.213, Test accuracy: 23.80
Round 129, Global train loss: 2.235, Global test loss: 2.209, Global test accuracy: 24.06
Round 130, Train loss: 2.234, Test loss: 2.212, Test accuracy: 23.62
Round 130, Global train loss: 2.234, Global test loss: 2.208, Global test accuracy: 24.30
Round 131, Train loss: 2.235, Test loss: 2.211, Test accuracy: 23.62
Round 131, Global train loss: 2.235, Global test loss: 2.207, Global test accuracy: 24.38
Round 132, Train loss: 2.234, Test loss: 2.210, Test accuracy: 23.52
Round 132, Global train loss: 2.234, Global test loss: 2.205, Global test accuracy: 24.09
Round 133, Train loss: 2.233, Test loss: 2.208, Test accuracy: 23.63
Round 133, Global train loss: 2.233, Global test loss: 2.204, Global test accuracy: 24.61
Round 134, Train loss: 2.234, Test loss: 2.207, Test accuracy: 23.39
Round 134, Global train loss: 2.234, Global test loss: 2.203, Global test accuracy: 23.54
Round 135, Train loss: 2.228, Test loss: 2.205, Test accuracy: 23.23
Round 135, Global train loss: 2.228, Global test loss: 2.201, Global test accuracy: 22.79
Round 136, Train loss: 2.231, Test loss: 2.204, Test accuracy: 22.79
Round 136, Global train loss: 2.231, Global test loss: 2.201, Global test accuracy: 23.39
Round 137, Train loss: 2.227, Test loss: 2.203, Test accuracy: 23.19
Round 137, Global train loss: 2.227, Global test loss: 2.199, Global test accuracy: 24.15
Round 138, Train loss: 2.225, Test loss: 2.202, Test accuracy: 23.36
Round 138, Global train loss: 2.225, Global test loss: 2.198, Global test accuracy: 23.78
Round 139, Train loss: 2.226, Test loss: 2.201, Test accuracy: 23.50
Round 139, Global train loss: 2.226, Global test loss: 2.196, Global test accuracy: 24.23
Round 140, Train loss: 2.226, Test loss: 2.198, Test accuracy: 23.46
Round 140, Global train loss: 2.226, Global test loss: 2.193, Global test accuracy: 23.43
Round 141, Train loss: 2.225, Test loss: 2.197, Test accuracy: 23.45
Round 141, Global train loss: 2.225, Global test loss: 2.191, Global test accuracy: 23.56
Round 142, Train loss: 2.223, Test loss: 2.194, Test accuracy: 23.43
Round 142, Global train loss: 2.223, Global test loss: 2.190, Global test accuracy: 23.97
Round 143, Train loss: 2.221, Test loss: 2.192, Test accuracy: 23.28
Round 143, Global train loss: 2.221, Global test loss: 2.188, Global test accuracy: 23.71
Round 144, Train loss: 2.221, Test loss: 2.191, Test accuracy: 23.32
Round 144, Global train loss: 2.221, Global test loss: 2.187, Global test accuracy: 24.57
Round 145, Train loss: 2.221, Test loss: 2.190, Test accuracy: 23.27
Round 145, Global train loss: 2.221, Global test loss: 2.183, Global test accuracy: 24.16
Round 146, Train loss: 2.220, Test loss: 2.188, Test accuracy: 23.18
Round 146, Global train loss: 2.220, Global test loss: 2.182, Global test accuracy: 23.75
Round 147, Train loss: 2.220, Test loss: 2.186, Test accuracy: 23.43
Round 147, Global train loss: 2.220, Global test loss: 2.182, Global test accuracy: 24.29
Round 148, Train loss: 2.224, Test loss: 2.185, Test accuracy: 23.31
Round 148, Global train loss: 2.224, Global test loss: 2.180, Global test accuracy: 23.63
Round 149, Train loss: 2.219, Test loss: 2.183, Test accuracy: 23.14
Round 149, Global train loss: 2.219, Global test loss: 2.180, Global test accuracy: 24.19
Round 150, Train loss: 2.218, Test loss: 2.183, Test accuracy: 23.02
Round 150, Global train loss: 2.218, Global test loss: 2.179, Global test accuracy: 23.60
Round 151, Train loss: 2.219, Test loss: 2.181, Test accuracy: 22.89
Round 151, Global train loss: 2.219, Global test loss: 2.178, Global test accuracy: 23.41
Round 152, Train loss: 2.222, Test loss: 2.180, Test accuracy: 22.79
Round 152, Global train loss: 2.222, Global test loss: 2.178, Global test accuracy: 24.09
Round 153, Train loss: 2.220, Test loss: 2.180, Test accuracy: 23.13
Round 153, Global train loss: 2.220, Global test loss: 2.177, Global test accuracy: 23.99
Round 154, Train loss: 2.214, Test loss: 2.180, Test accuracy: 23.10
Round 154, Global train loss: 2.214, Global test loss: 2.179, Global test accuracy: 23.93
Round 155, Train loss: 2.213, Test loss: 2.180, Test accuracy: 23.45
Round 155, Global train loss: 2.213, Global test loss: 2.179, Global test accuracy: 23.57
Round 156, Train loss: 2.215, Test loss: 2.179, Test accuracy: 23.26
Round 156, Global train loss: 2.215, Global test loss: 2.178, Global test accuracy: 24.77
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 22.62
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 18.74
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 16.74
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 14.05
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 12.77
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 12.77
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 11.42
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 11.42
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 11.42
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 11.42
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 11.42
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 11.42
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 10.65
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 10.65
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

15252.353248596191
[5.411004543304443, 10.64197587966919, 15.84936237335205, 21.09080672264099, 26.374541759490967, 31.592419147491455, 36.657349586486816, 41.78077578544617, 46.86871123313904, 51.83802556991577, 56.93104577064514, 62.05228590965271, 67.22431969642639, 72.38832330703735, 77.57266283035278, 82.85862922668457, 87.89339447021484, 93.02571296691895, 98.35317468643188, 103.6612901687622, 109.17417025566101, 114.65133428573608, 120.0932035446167, 125.70793294906616, 131.18503212928772, 136.5875051021576, 141.87391805648804, 147.26743745803833, 152.69880390167236, 158.15339016914368, 163.64291858673096, 169.14087510108948, 174.56634044647217, 179.99420475959778, 185.46695613861084, 190.86700987815857, 196.45783352851868, 201.86612153053284, 207.21874928474426, 212.64315915107727, 217.69781303405762, 223.00872492790222, 228.39872288703918, 233.86148142814636, 239.2833330631256, 244.70992612838745, 250.15067958831787, 255.62807869911194, 260.9504506587982, 266.26142954826355, 271.69843196868896, 277.23612117767334, 282.5770993232727, 287.9969091415405, 293.41648840904236, 298.6911430358887, 304.1434142589569, 309.5847272872925, 314.8674964904785, 320.04205656051636, 325.6130826473236, 331.04473996162415, 336.45137190818787, 341.8700656890869, 347.3419647216797, 352.8533043861389, 358.29752683639526, 363.8246943950653, 369.3219118118286, 374.7924702167511, 380.2009108066559, 385.66003704071045, 391.1033504009247, 396.62369418144226, 401.9861869812012, 407.5038604736328, 412.8857853412628, 418.34044313430786, 423.68606209754944, 429.1156075000763, 434.48181796073914, 439.8983688354492, 445.3871932029724, 450.79543137550354, 456.1051404476166, 461.2979826927185, 466.826110124588, 472.5568890571594, 478.1538596153259, 483.85696840286255, 489.46569442749023, 495.0745413303375, 500.64196586608887, 506.05279445648193, 511.5196645259857, 517.1623079776764, 522.6764440536499, 528.1414396762848, 533.6007299423218, 539.0346009731293, 544.4730961322784, 549.9642910957336, 555.2990474700928, 560.6483182907104, 566.144467830658, 571.6885297298431, 577.2700526714325, 582.9888212680817, 588.5543313026428, 593.9891192913055, 598.8654799461365, 603.6592605113983, 608.420508146286, 613.2670567035675, 618.0737464427948, 622.8899686336517, 627.7184071540833, 632.4959638118744, 637.3372368812561, 642.2157616615295, 647.5432846546173, 652.4573271274567, 657.3907513618469, 662.2312111854553, 667.1087484359741, 672.5176079273224, 677.576587677002, 682.795511007309, 688.0099010467529, 693.0642776489258, 698.0154092311859, 703.0906121730804, 708.25523853302, 713.3459265232086, 718.8523559570312, 724.235044002533, 729.6634163856506, 735.0742363929749, 740.5322062969208, 745.5602107048035, 750.4240810871124, 755.2675976753235, 760.3180756568909, 765.3663864135742, 770.4282639026642, 775.3456988334656, 780.3599076271057, 785.3381087779999, 790.3099637031555, 795.3400452136993, 800.32217669487, 805.433541059494, 810.4036810398102, 815.3258929252625, 820.373327255249, 825.470287322998, 830.3664288520813, 835.327131986618, 840.2694373130798, 845.2453989982605, 850.1863946914673, 855.2394866943359, 860.174388885498, 865.1649839878082, 870.3348026275635, 875.5472576618195, 880.7263140678406, 885.9246542453766, 891.3681349754333, 896.896164894104, 902.5468292236328, 908.163013458252, 913.8198001384735, 919.3477213382721, 925.0516879558563, 930.1325356960297, 935.1561608314514, 941.0013539791107, 946.0865559577942, 951.299608707428, 956.4251186847687, 961.5188364982605, 966.5776286125183, 972.1504652500153, 977.7632625102997, 983.4070427417755, 988.5481514930725, 994.17791223526, 999.2113742828369, 1004.1674098968506, 1009.3896021842957, 1014.4816999435425, 1019.3678617477417, 1024.212569952011, 1028.9971759319305, 1033.7202894687653, 1038.4472846984863, 1043.1918194293976, 1047.9785113334656, 1052.781482219696, 1057.5843169689178, 1062.2678577899933, 1066.8891561031342, 1071.6930096149445, 1076.3481044769287, 1081.4876351356506, 1086.574753522873, 1091.6647791862488, 1096.8131358623505, 1101.9728472232819, 1106.9852738380432, 1111.6774814128876, 1116.4028692245483, 1121.1953859329224, 1126.069036245346, 1130.7883372306824, 1135.4432272911072, 1140.1752183437347, 1144.8453333377838, 1150.0180475711823, 1154.7575109004974, 1159.5170593261719, 1164.2609746456146, 1169.156273841858, 1173.9753692150116, 1178.6874563694, 1183.3834772109985, 1188.0552411079407, 1192.9945259094238, 1198.1048147678375, 1202.8728249073029, 1207.474820613861, 1212.2443392276764, 1216.9805624485016, 1221.6542284488678, 1226.3228507041931, 1230.990092754364, 1236.0578229427338, 1241.2846157550812, 1246.0913434028625, 1251.3536694049835, 1256.3333106040955, 1260.9668254852295, 1265.6075809001923, 1270.3063607215881, 1275.419618844986, 1280.5440871715546, 1285.6754117012024, 1290.6566162109375, 1295.5038180351257, 1300.2626202106476, 1304.9541957378387, 1309.644379377365, 1314.340573310852, 1318.992802143097, 1323.635278224945, 1328.8442723751068, 1333.9955425262451, 1338.9762160778046, 1343.5653398036957, 1348.2280514240265, 1352.746912240982, 1357.3480966091156, 1361.9441499710083, 1367.1024718284607, 1372.2266511917114, 1376.8885343074799, 1381.6383681297302, 1386.2905945777893, 1390.9245884418488, 1395.529569864273, 1400.220852136612, 1405.0357019901276, 1409.748143196106, 1414.4571447372437, 1419.1920008659363, 1423.9477684497833, 1428.7837054729462, 1433.4379658699036, 1438.2524681091309, 1443.225192785263, 1447.9473843574524, 1453.244359254837, 1458.2703530788422, 1462.9274864196777, 1467.6474568843842, 1472.4548468589783, 1477.177140712738, 1481.9995198249817, 1486.8567953109741, 1491.6731541156769, 1496.405550956726, 1501.0130641460419, 1505.6058366298676, 1510.308823108673, 1514.8245906829834, 1519.546220779419, 1524.2574787139893, 1528.9565937519073, 1533.6196205615997, 1536.0000433921814]
[11.385, 11.6325, 11.8325, 11.9575, 11.9625, 12.09, 12.3775, 12.6825, 12.7625, 12.77, 12.865, 13.0, 13.085, 13.005, 13.01, 13.0475, 13.0275, 13.48, 13.5225, 13.745, 14.1425, 14.59, 14.7425, 14.8025, 14.98, 15.2175, 15.5575, 15.7125, 15.7225, 15.9575, 16.2275, 16.3775, 16.5825, 16.525, 16.2675, 16.1, 16.3425, 16.1475, 15.825, 15.76, 15.7375, 15.65, 15.64, 15.63, 15.5475, 15.1925, 14.8525, 14.8375, 14.91, 15.05, 15.015, 15.0075, 15.4525, 16.1325, 16.3325, 16.5, 16.6725, 16.52, 16.92, 17.03, 16.8075, 16.7775, 16.86, 16.87, 17.3725, 18.165, 18.2875, 18.825, 19.175, 19.7, 20.2475, 20.1975, 19.9375, 19.585, 19.2175, 18.945, 18.2675, 17.905, 17.6, 17.3, 17.53, 17.91, 18.0825, 17.9725, 17.635, 17.54, 17.1775, 16.7625, 16.44, 16.8975, 17.29, 17.6075, 18.1, 18.27, 18.115, 18.31, 18.0875, 18.3225, 18.4725, 18.6975, 19.67, 19.65, 20.0175, 19.7025, 19.16, 19.12, 19.27, 19.3525, 19.6725, 19.5225, 19.53, 19.5175, 19.5175, 19.7225, 19.97, 20.595, 21.07, 21.005, 21.4175, 21.87, 22.145, 22.2225, 22.825, 22.6725, 22.55, 22.545, 22.3525, 22.625, 22.915, 23.7975, 23.6175, 23.62, 23.5225, 23.6325, 23.395, 23.235, 22.7925, 23.19, 23.3575, 23.5025, 23.4575, 23.455, 23.425, 23.2775, 23.32, 23.275, 23.1825, 23.425, 23.3125, 23.1425, 23.025, 22.89, 22.7875, 23.13, 23.1025, 23.45, 23.2575, 22.625, 18.7375, 16.7375, 14.0525, 12.7725, 12.7725, 11.42, 11.42, 11.42, 11.42, 11.42, 11.42, 10.65, 10.65, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.849, Test loss: 1.854, Test accuracy: 27.07
Round   0, Global train loss: 0.849, Global test loss: 2.271, Global test accuracy: 11.67
Round   1, Train loss: 0.723, Test loss: 1.451, Test accuracy: 48.41
Round   1, Global train loss: 0.723, Global test loss: 2.168, Global test accuracy: 23.84
Round   2, Train loss: 0.646, Test loss: 1.190, Test accuracy: 58.96
Round   2, Global train loss: 0.646, Global test loss: 2.039, Global test accuracy: 33.34
Round   3, Train loss: 0.557, Test loss: 0.897, Test accuracy: 68.05
Round   3, Global train loss: 0.557, Global test loss: 2.103, Global test accuracy: 35.11
Round   4, Train loss: 0.618, Test loss: 0.784, Test accuracy: 72.14
Round   4, Global train loss: 0.618, Global test loss: 2.037, Global test accuracy: 35.35
Round   5, Train loss: 0.543, Test loss: 0.731, Test accuracy: 74.65
Round   5, Global train loss: 0.543, Global test loss: 2.096, Global test accuracy: 36.42
Round   6, Train loss: 0.518, Test loss: 0.655, Test accuracy: 75.86
Round   6, Global train loss: 0.518, Global test loss: 1.832, Global test accuracy: 40.92
Round   7, Train loss: 0.455, Test loss: 0.652, Test accuracy: 75.34
Round   7, Global train loss: 0.455, Global test loss: 1.512, Global test accuracy: 46.05
Round   8, Train loss: 0.482, Test loss: 0.657, Test accuracy: 76.58
Round   8, Global train loss: 0.482, Global test loss: 1.723, Global test accuracy: 42.49
Round   9, Train loss: 0.455, Test loss: 0.537, Test accuracy: 79.83
Round   9, Global train loss: 0.455, Global test loss: 1.584, Global test accuracy: 45.19
Round  10, Train loss: 0.456, Test loss: 0.415, Test accuracy: 83.49
Round  10, Global train loss: 0.456, Global test loss: 1.498, Global test accuracy: 47.70
Round  11, Train loss: 0.453, Test loss: 0.403, Test accuracy: 84.17
Round  11, Global train loss: 0.453, Global test loss: 1.299, Global test accuracy: 54.46
Round  12, Train loss: 0.413, Test loss: 0.394, Test accuracy: 84.50
Round  12, Global train loss: 0.413, Global test loss: 1.365, Global test accuracy: 52.14
Round  13, Train loss: 0.353, Test loss: 0.392, Test accuracy: 84.71
Round  13, Global train loss: 0.353, Global test loss: 1.243, Global test accuracy: 58.14
Round  14, Train loss: 0.365, Test loss: 0.393, Test accuracy: 84.81
Round  14, Global train loss: 0.365, Global test loss: 1.248, Global test accuracy: 56.49
Round  15, Train loss: 0.350, Test loss: 0.384, Test accuracy: 85.13
Round  15, Global train loss: 0.350, Global test loss: 1.430, Global test accuracy: 51.69
Round  16, Train loss: 0.379, Test loss: 0.379, Test accuracy: 85.24
Round  16, Global train loss: 0.379, Global test loss: 1.118, Global test accuracy: 61.28
Round  17, Train loss: 0.317, Test loss: 0.382, Test accuracy: 85.15
Round  17, Global train loss: 0.317, Global test loss: 1.354, Global test accuracy: 55.00
Round  18, Train loss: 0.397, Test loss: 0.370, Test accuracy: 85.66
Round  18, Global train loss: 0.397, Global test loss: 1.337, Global test accuracy: 52.26
Round  19, Train loss: 0.373, Test loss: 0.371, Test accuracy: 85.88
Round  19, Global train loss: 0.373, Global test loss: 1.262, Global test accuracy: 57.13
Round  20, Train loss: 0.420, Test loss: 0.362, Test accuracy: 86.29
Round  20, Global train loss: 0.420, Global test loss: 1.413, Global test accuracy: 51.38
Round  21, Train loss: 0.355, Test loss: 0.360, Test accuracy: 86.37
Round  21, Global train loss: 0.355, Global test loss: 1.140, Global test accuracy: 58.40
Round  22, Train loss: 0.345, Test loss: 0.354, Test accuracy: 86.49
Round  22, Global train loss: 0.345, Global test loss: 1.247, Global test accuracy: 56.33
Round  23, Train loss: 0.311, Test loss: 0.362, Test accuracy: 86.35
Round  23, Global train loss: 0.311, Global test loss: 0.975, Global test accuracy: 66.19
Round  24, Train loss: 0.340, Test loss: 0.364, Test accuracy: 86.22
Round  24, Global train loss: 0.340, Global test loss: 1.209, Global test accuracy: 58.92
Round  25, Train loss: 0.273, Test loss: 0.377, Test accuracy: 85.75
Round  25, Global train loss: 0.273, Global test loss: 1.207, Global test accuracy: 59.62
Round  26, Train loss: 0.299, Test loss: 0.370, Test accuracy: 86.09
Round  26, Global train loss: 0.299, Global test loss: 1.186, Global test accuracy: 60.64
Round  27, Train loss: 0.273, Test loss: 0.359, Test accuracy: 86.49
Round  27, Global train loss: 0.273, Global test loss: 1.900, Global test accuracy: 44.62
Round  28, Train loss: 0.285, Test loss: 0.373, Test accuracy: 86.15
Round  28, Global train loss: 0.285, Global test loss: 1.426, Global test accuracy: 56.27
Round  29, Train loss: 0.284, Test loss: 0.363, Test accuracy: 86.59
Round  29, Global train loss: 0.284, Global test loss: 1.109, Global test accuracy: 61.74
Round  30, Train loss: 0.263, Test loss: 0.364, Test accuracy: 86.52
Round  30, Global train loss: 0.263, Global test loss: 0.975, Global test accuracy: 66.89
Round  31, Train loss: 0.216, Test loss: 0.358, Test accuracy: 86.77
Round  31, Global train loss: 0.216, Global test loss: 1.359, Global test accuracy: 57.00
Round  32, Train loss: 0.289, Test loss: 0.348, Test accuracy: 87.16
Round  32, Global train loss: 0.289, Global test loss: 1.113, Global test accuracy: 61.48
Round  33, Train loss: 0.306, Test loss: 0.349, Test accuracy: 87.08
Round  33, Global train loss: 0.306, Global test loss: 1.414, Global test accuracy: 55.66
Round  34, Train loss: 0.267, Test loss: 0.356, Test accuracy: 87.19
Round  34, Global train loss: 0.267, Global test loss: 1.953, Global test accuracy: 47.43
Round  35, Train loss: 0.266, Test loss: 0.354, Test accuracy: 87.28
Round  35, Global train loss: 0.266, Global test loss: 1.029, Global test accuracy: 64.16
Round  36, Train loss: 0.224, Test loss: 0.353, Test accuracy: 87.37
Round  36, Global train loss: 0.224, Global test loss: 1.069, Global test accuracy: 65.01
Round  37, Train loss: 0.246, Test loss: 0.348, Test accuracy: 87.40
Round  37, Global train loss: 0.246, Global test loss: 0.991, Global test accuracy: 66.52
Round  38, Train loss: 0.244, Test loss: 0.352, Test accuracy: 87.48
Round  38, Global train loss: 0.244, Global test loss: 1.237, Global test accuracy: 61.24
Round  39, Train loss: 0.210, Test loss: 0.354, Test accuracy: 87.33
Round  39, Global train loss: 0.210, Global test loss: 1.184, Global test accuracy: 61.96
Round  40, Train loss: 0.229, Test loss: 0.351, Test accuracy: 87.42
Round  40, Global train loss: 0.229, Global test loss: 1.308, Global test accuracy: 57.75
Round  41, Train loss: 0.239, Test loss: 0.349, Test accuracy: 87.63
Round  41, Global train loss: 0.239, Global test loss: 0.948, Global test accuracy: 67.48
Round  42, Train loss: 0.220, Test loss: 0.349, Test accuracy: 87.81
Round  42, Global train loss: 0.220, Global test loss: 1.368, Global test accuracy: 57.99
Round  43, Train loss: 0.244, Test loss: 0.356, Test accuracy: 87.60
Round  43, Global train loss: 0.244, Global test loss: 0.975, Global test accuracy: 66.60
Round  44, Train loss: 0.271, Test loss: 0.357, Test accuracy: 87.49
Round  44, Global train loss: 0.271, Global test loss: 1.148, Global test accuracy: 59.79
Round  45, Train loss: 0.223, Test loss: 0.351, Test accuracy: 87.80
Round  45, Global train loss: 0.223, Global test loss: 0.997, Global test accuracy: 66.34
Round  46, Train loss: 0.219, Test loss: 0.352, Test accuracy: 87.85
Round  46, Global train loss: 0.219, Global test loss: 1.391, Global test accuracy: 57.64
Round  47, Train loss: 0.190, Test loss: 0.363, Test accuracy: 87.56
Round  47, Global train loss: 0.190, Global test loss: 1.235, Global test accuracy: 61.98
Round  48, Train loss: 0.245, Test loss: 0.359, Test accuracy: 87.63
Round  48, Global train loss: 0.245, Global test loss: 1.130, Global test accuracy: 62.01
Round  49, Train loss: 0.184, Test loss: 0.364, Test accuracy: 87.59
Round  49, Global train loss: 0.184, Global test loss: 1.406, Global test accuracy: 58.28
Round  50, Train loss: 0.255, Test loss: 0.364, Test accuracy: 87.49
Round  50, Global train loss: 0.255, Global test loss: 1.013, Global test accuracy: 66.39
Round  51, Train loss: 0.216, Test loss: 0.375, Test accuracy: 87.28
Round  51, Global train loss: 0.216, Global test loss: 1.081, Global test accuracy: 64.33
Round  52, Train loss: 0.174, Test loss: 0.374, Test accuracy: 87.33
Round  52, Global train loss: 0.174, Global test loss: 1.028, Global test accuracy: 65.80
Round  53, Train loss: 0.232, Test loss: 0.372, Test accuracy: 87.66
Round  53, Global train loss: 0.232, Global test loss: 1.482, Global test accuracy: 54.74
Round  54, Train loss: 0.167, Test loss: 0.369, Test accuracy: 87.75
Round  54, Global train loss: 0.167, Global test loss: 1.676, Global test accuracy: 54.71
Round  55, Train loss: 0.212, Test loss: 0.367, Test accuracy: 87.76
Round  55, Global train loss: 0.212, Global test loss: 1.016, Global test accuracy: 65.58
Round  56, Train loss: 0.199, Test loss: 0.358, Test accuracy: 88.14
Round  56, Global train loss: 0.199, Global test loss: 0.992, Global test accuracy: 67.01
Round  57, Train loss: 0.172, Test loss: 0.359, Test accuracy: 88.03
Round  57, Global train loss: 0.172, Global test loss: 1.559, Global test accuracy: 55.35
Round  58, Train loss: 0.248, Test loss: 0.354, Test accuracy: 88.23
Round  58, Global train loss: 0.248, Global test loss: 1.139, Global test accuracy: 64.32
Round  59, Train loss: 0.212, Test loss: 0.351, Test accuracy: 88.28
Round  59, Global train loss: 0.212, Global test loss: 1.469, Global test accuracy: 58.84
Round  60, Train loss: 0.194, Test loss: 0.353, Test accuracy: 88.25
Round  60, Global train loss: 0.194, Global test loss: 1.241, Global test accuracy: 61.76
Round  61, Train loss: 0.150, Test loss: 0.364, Test accuracy: 87.95
Round  61, Global train loss: 0.150, Global test loss: 1.069, Global test accuracy: 65.47
Round  62, Train loss: 0.192, Test loss: 0.374, Test accuracy: 87.66
Round  62, Global train loss: 0.192, Global test loss: 1.152, Global test accuracy: 65.22
Round  63, Train loss: 0.253, Test loss: 0.365, Test accuracy: 88.13
Round  63, Global train loss: 0.253, Global test loss: 0.951, Global test accuracy: 68.07
Round  64, Train loss: 0.172, Test loss: 0.375, Test accuracy: 87.79
Round  64, Global train loss: 0.172, Global test loss: 1.190, Global test accuracy: 63.88
Round  65, Train loss: 0.184, Test loss: 0.380, Test accuracy: 87.72
Round  65, Global train loss: 0.184, Global test loss: 1.109, Global test accuracy: 65.19
Round  66, Train loss: 0.153, Test loss: 0.373, Test accuracy: 87.99
Round  66, Global train loss: 0.153, Global test loss: 1.303, Global test accuracy: 61.48
Round  67, Train loss: 0.165, Test loss: 0.367, Test accuracy: 88.31
Round  67, Global train loss: 0.165, Global test loss: 1.166, Global test accuracy: 64.60
Round  68, Train loss: 0.176, Test loss: 0.369, Test accuracy: 88.28
Round  68, Global train loss: 0.176, Global test loss: 1.190, Global test accuracy: 63.53
Round  69, Train loss: 0.156, Test loss: 0.362, Test accuracy: 88.51
Round  69, Global train loss: 0.156, Global test loss: 1.568, Global test accuracy: 57.85
Round  70, Train loss: 0.221, Test loss: 0.356, Test accuracy: 88.55
Round  70, Global train loss: 0.221, Global test loss: 0.990, Global test accuracy: 66.44
Round  71, Train loss: 0.131, Test loss: 0.360, Test accuracy: 88.49
Round  71, Global train loss: 0.131, Global test loss: 1.133, Global test accuracy: 65.37
Round  72, Train loss: 0.202, Test loss: 0.367, Test accuracy: 88.39
Round  72, Global train loss: 0.202, Global test loss: 1.269, Global test accuracy: 61.10
Round  73, Train loss: 0.172, Test loss: 0.373, Test accuracy: 88.38
Round  73, Global train loss: 0.172, Global test loss: 1.136, Global test accuracy: 63.67
Round  74, Train loss: 0.189, Test loss: 0.373, Test accuracy: 88.62
Round  74, Global train loss: 0.189, Global test loss: 1.103, Global test accuracy: 65.71
Round  75, Train loss: 0.228, Test loss: 0.382, Test accuracy: 88.26
Round  75, Global train loss: 0.228, Global test loss: 1.080, Global test accuracy: 65.12
Round  76, Train loss: 0.170, Test loss: 0.377, Test accuracy: 88.29
Round  76, Global train loss: 0.170, Global test loss: 1.072, Global test accuracy: 65.05
Round  77, Train loss: 0.173, Test loss: 0.383, Test accuracy: 88.24
Round  77, Global train loss: 0.173, Global test loss: 1.028, Global test accuracy: 65.83
Round  78, Train loss: 0.171, Test loss: 0.384, Test accuracy: 88.24
Round  78, Global train loss: 0.171, Global test loss: 1.436, Global test accuracy: 58.76
Round  79, Train loss: 0.151, Test loss: 0.388, Test accuracy: 88.13
Round  79, Global train loss: 0.151, Global test loss: 1.208, Global test accuracy: 63.15
Round  80, Train loss: 0.214, Test loss: 0.386, Test accuracy: 88.20
Round  80, Global train loss: 0.214, Global test loss: 1.255, Global test accuracy: 61.78
Round  81, Train loss: 0.175, Test loss: 0.381, Test accuracy: 88.37
Round  81, Global train loss: 0.175, Global test loss: 1.250, Global test accuracy: 63.26
Round  82, Train loss: 0.198, Test loss: 0.390, Test accuracy: 87.95
Round  82, Global train loss: 0.198, Global test loss: 1.125, Global test accuracy: 63.35
Round  83, Train loss: 0.164, Test loss: 0.373, Test accuracy: 88.21
Round  83, Global train loss: 0.164, Global test loss: 1.260, Global test accuracy: 61.18
Round  84, Train loss: 0.144, Test loss: 0.393, Test accuracy: 87.81
Round  84, Global train loss: 0.144, Global test loss: 1.329, Global test accuracy: 61.08
Round  85, Train loss: 0.134, Test loss: 0.387, Test accuracy: 88.00
Round  85, Global train loss: 0.134, Global test loss: 1.185, Global test accuracy: 64.14
Round  86, Train loss: 0.123, Test loss: 0.379, Test accuracy: 88.32
Round  86, Global train loss: 0.123, Global test loss: 1.094, Global test accuracy: 67.20
Round  87, Train loss: 0.168, Test loss: 0.382, Test accuracy: 88.20
Round  87, Global train loss: 0.168, Global test loss: 1.226, Global test accuracy: 64.66
Round  88, Train loss: 0.179, Test loss: 0.379, Test accuracy: 88.39
Round  88, Global train loss: 0.179, Global test loss: 1.249, Global test accuracy: 63.53
Round  89, Train loss: 0.188, Test loss: 0.375, Test accuracy: 88.67
Round  89, Global train loss: 0.188, Global test loss: 0.891, Global test accuracy: 70.26
Round  90, Train loss: 0.145, Test loss: 0.370, Test accuracy: 88.82
Round  90, Global train loss: 0.145, Global test loss: 0.967, Global test accuracy: 68.59
Round  91, Train loss: 0.175, Test loss: 0.384, Test accuracy: 88.46
Round  91, Global train loss: 0.175, Global test loss: 1.061, Global test accuracy: 65.69
Round  92, Train loss: 0.147, Test loss: 0.378, Test accuracy: 88.68
Round  92, Global train loss: 0.147, Global test loss: 1.240, Global test accuracy: 62.99
Round  93, Train loss: 0.147, Test loss: 0.383, Test accuracy: 88.62
Round  93, Global train loss: 0.147, Global test loss: 1.124, Global test accuracy: 66.86
Round  94, Train loss: 0.112, Test loss: 0.375, Test accuracy: 88.87
Round  94, Global train loss: 0.112, Global test loss: 1.720, Global test accuracy: 58.33
Round  95, Train loss: 0.166, Test loss: 0.376, Test accuracy: 88.87
Round  95, Global train loss: 0.166, Global test loss: 1.101, Global test accuracy: 65.76
Round  96, Train loss: 0.140, Test loss: 0.379, Test accuracy: 88.78
Round  96, Global train loss: 0.140, Global test loss: 1.879, Global test accuracy: 53.00
Round  97, Train loss: 0.177, Test loss: 0.387, Test accuracy: 88.69
Round  97, Global train loss: 0.177, Global test loss: 1.164, Global test accuracy: 62.77
Round  98, Train loss: 0.115, Test loss: 0.372, Test accuracy: 88.89
Round  98, Global train loss: 0.115, Global test loss: 1.039, Global test accuracy: 68.20
Round  99, Train loss: 0.154, Test loss: 0.370, Test accuracy: 88.77
Round  99, Global train loss: 0.154, Global test loss: 1.038, Global test accuracy: 67.51
Final Round, Train loss: 0.101, Test loss: 0.439, Test accuracy: 88.42
Final Round, Global train loss: 0.101, Global test loss: 1.038, Global test accuracy: 67.51
Average accuracy final 10 rounds: 88.74527777777777 

Average global accuracy final 10 rounds: 63.97055555555556 

6230.263583183289
[4.68251895904541, 9.36503791809082, 13.799575090408325, 18.23411226272583, 22.719890356063843, 27.205668449401855, 31.680883646011353, 36.15609884262085, 40.652623891830444, 45.14914894104004, 49.60560464859009, 54.06206035614014, 58.60299205780029, 63.14392375946045, 67.58192610740662, 72.01992845535278, 76.45450806617737, 80.88908767700195, 85.33506059646606, 89.78103351593018, 94.1210024356842, 98.46097135543823, 102.71297883987427, 106.9649863243103, 111.28725457191467, 115.60952281951904, 119.9910216331482, 124.37252044677734, 128.78362584114075, 133.19473123550415, 137.61444067955017, 142.0341501235962, 146.61448740959167, 151.19482469558716, 155.6342785358429, 160.07373237609863, 164.42566633224487, 168.7776002883911, 173.1755256652832, 177.5734510421753, 181.97920632362366, 186.38496160507202, 190.79082894325256, 195.1966962814331, 199.57061862945557, 203.94454097747803, 208.64807438850403, 213.35160779953003, 218.05838561058044, 222.76516342163086, 227.16428804397583, 231.5634126663208, 235.87231540679932, 240.18121814727783, 244.5773823261261, 248.97354650497437, 253.45161938667297, 257.9296922683716, 262.37365460395813, 266.8176169395447, 271.3271164894104, 275.8366160392761, 280.29115200042725, 284.74568796157837, 289.3171513080597, 293.888614654541, 298.22438859939575, 302.5601625442505, 307.122855424881, 311.6855483055115, 316.30722880363464, 320.9289093017578, 325.27221632003784, 329.61552333831787, 334.22874999046326, 338.84197664260864, 343.3304977416992, 347.8190188407898, 352.3108675479889, 356.802716255188, 361.01598381996155, 365.2292513847351, 369.7767324447632, 374.32421350479126, 378.8684136867523, 383.4126138687134, 387.7571711540222, 392.10172843933105, 396.43574142456055, 400.76975440979004, 404.96873331069946, 409.1677122116089, 413.4077401161194, 417.6477680206299, 421.87193393707275, 426.0960998535156, 430.7161030769348, 435.336106300354, 439.73052763938904, 444.1249489784241, 448.5238997936249, 452.9228506088257, 457.23826122283936, 461.553671836853, 465.7935664653778, 470.0334610939026, 474.5722849369049, 479.1111087799072, 483.60470056533813, 488.09829235076904, 492.7049083709717, 497.3115243911743, 501.8724582195282, 506.4333920478821, 511.0076279640198, 515.5818638801575, 520.1522145271301, 524.7225651741028, 529.3681008815765, 534.0136365890503, 538.706681728363, 543.3997268676758, 548.0431413650513, 552.6865558624268, 557.2873165607452, 561.8880772590637, 566.4704685211182, 571.0528597831726, 575.5847311019897, 580.1166024208069, 584.3871269226074, 588.657651424408, 593.301696062088, 597.9457406997681, 602.5365328788757, 607.1273250579834, 611.3371257781982, 615.5469264984131, 619.8412954807281, 624.1356644630432, 628.3857293128967, 632.6357941627502, 636.8467807769775, 641.0577673912048, 645.3125243186951, 649.5672812461853, 653.7980411052704, 658.0288009643555, 662.2484302520752, 666.4680595397949, 670.684602022171, 674.9011445045471, 679.1195843219757, 683.3380241394043, 687.4622936248779, 691.5865631103516, 695.6990587711334, 699.8115544319153, 704.024552822113, 708.2375512123108, 712.4582467079163, 716.6789422035217, 721.0362129211426, 725.3934836387634, 729.6313898563385, 733.8692960739136, 738.01988530159, 742.1704745292664, 746.3959021568298, 750.6213297843933, 754.950273513794, 759.2792172431946, 763.547741651535, 767.8162660598755, 772.256020784378, 776.6957755088806, 780.9172282218933, 785.138680934906, 789.6785047054291, 794.2183284759521, 798.4228565692902, 802.6273846626282, 807.0910685062408, 811.5547523498535, 816.0669069290161, 820.5790615081787, 825.0800757408142, 829.5810899734497, 834.207026720047, 838.8329634666443, 843.5486164093018, 848.2642693519592, 852.4659373760223, 856.6676054000854, 860.7778594493866, 864.8881134986877, 869.0985202789307, 873.3089270591736, 877.4374678134918, 881.5660085678101, 883.8968169689178, 886.2276253700256]
[27.072222222222223, 27.072222222222223, 48.40555555555556, 48.40555555555556, 58.96111111111111, 58.96111111111111, 68.05277777777778, 68.05277777777778, 72.14166666666667, 72.14166666666667, 74.64722222222223, 74.64722222222223, 75.85833333333333, 75.85833333333333, 75.33611111111111, 75.33611111111111, 76.575, 76.575, 79.825, 79.825, 83.49166666666666, 83.49166666666666, 84.175, 84.175, 84.50277777777778, 84.50277777777778, 84.71388888888889, 84.71388888888889, 84.81111111111112, 84.81111111111112, 85.13333333333334, 85.13333333333334, 85.2388888888889, 85.2388888888889, 85.15, 85.15, 85.66388888888889, 85.66388888888889, 85.875, 85.875, 86.29166666666667, 86.29166666666667, 86.37222222222222, 86.37222222222222, 86.4888888888889, 86.4888888888889, 86.35277777777777, 86.35277777777777, 86.22222222222223, 86.22222222222223, 85.74722222222222, 85.74722222222222, 86.09444444444445, 86.09444444444445, 86.49166666666666, 86.49166666666666, 86.15, 86.15, 86.58888888888889, 86.58888888888889, 86.51944444444445, 86.51944444444445, 86.77222222222223, 86.77222222222223, 87.16388888888889, 87.16388888888889, 87.075, 87.075, 87.19166666666666, 87.19166666666666, 87.27777777777777, 87.27777777777777, 87.37222222222222, 87.37222222222222, 87.4, 87.4, 87.48055555555555, 87.48055555555555, 87.32777777777778, 87.32777777777778, 87.41666666666667, 87.41666666666667, 87.63055555555556, 87.63055555555556, 87.80833333333334, 87.80833333333334, 87.59722222222223, 87.59722222222223, 87.49166666666666, 87.49166666666666, 87.79722222222222, 87.79722222222222, 87.85, 87.85, 87.55833333333334, 87.55833333333334, 87.62777777777778, 87.62777777777778, 87.59166666666667, 87.59166666666667, 87.4888888888889, 87.4888888888889, 87.27777777777777, 87.27777777777777, 87.33055555555555, 87.33055555555555, 87.66111111111111, 87.66111111111111, 87.74722222222222, 87.74722222222222, 87.7611111111111, 87.7611111111111, 88.1361111111111, 88.1361111111111, 88.025, 88.025, 88.22777777777777, 88.22777777777777, 88.27777777777777, 88.27777777777777, 88.25277777777778, 88.25277777777778, 87.94722222222222, 87.94722222222222, 87.66388888888889, 87.66388888888889, 88.13055555555556, 88.13055555555556, 87.79444444444445, 87.79444444444445, 87.72222222222223, 87.72222222222223, 87.99444444444444, 87.99444444444444, 88.31388888888888, 88.31388888888888, 88.275, 88.275, 88.50555555555556, 88.50555555555556, 88.55, 88.55, 88.48611111111111, 88.48611111111111, 88.3861111111111, 88.3861111111111, 88.37777777777778, 88.37777777777778, 88.625, 88.625, 88.25555555555556, 88.25555555555556, 88.29444444444445, 88.29444444444445, 88.24166666666666, 88.24166666666666, 88.24166666666666, 88.24166666666666, 88.13055555555556, 88.13055555555556, 88.19722222222222, 88.19722222222222, 88.37222222222222, 88.37222222222222, 87.95, 87.95, 88.20555555555555, 88.20555555555555, 87.80555555555556, 87.80555555555556, 87.99722222222222, 87.99722222222222, 88.31666666666666, 88.31666666666666, 88.2, 88.2, 88.3861111111111, 88.3861111111111, 88.67222222222222, 88.67222222222222, 88.81944444444444, 88.81944444444444, 88.45833333333333, 88.45833333333333, 88.68055555555556, 88.68055555555556, 88.625, 88.625, 88.87222222222222, 88.87222222222222, 88.87222222222222, 88.87222222222222, 88.775, 88.775, 88.68888888888888, 88.68888888888888, 88.89444444444445, 88.89444444444445, 88.76666666666667, 88.76666666666667, 88.41944444444445, 88.41944444444445]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.182, Test loss: 1.915, Test accuracy: 28.98
Round   1, Train loss: 1.888, Test loss: 1.660, Test accuracy: 39.26
Round   2, Train loss: 1.696, Test loss: 1.547, Test accuracy: 43.65
Round   3, Train loss: 1.587, Test loss: 1.475, Test accuracy: 46.56
Round   4, Train loss: 1.518, Test loss: 1.398, Test accuracy: 49.95
Round   5, Train loss: 1.445, Test loss: 1.333, Test accuracy: 52.84
Round   6, Train loss: 1.390, Test loss: 1.277, Test accuracy: 54.87
Round   7, Train loss: 1.341, Test loss: 1.205, Test accuracy: 57.59
Round   8, Train loss: 1.264, Test loss: 1.169, Test accuracy: 58.87
Round   9, Train loss: 1.222, Test loss: 1.146, Test accuracy: 59.77
Round  10, Train loss: 1.196, Test loss: 1.121, Test accuracy: 60.93
Round  11, Train loss: 1.171, Test loss: 1.102, Test accuracy: 61.50
Round  12, Train loss: 1.131, Test loss: 1.091, Test accuracy: 61.76
Round  13, Train loss: 1.115, Test loss: 1.030, Test accuracy: 63.70
Round  14, Train loss: 1.055, Test loss: 1.030, Test accuracy: 64.07
Round  15, Train loss: 1.046, Test loss: 0.980, Test accuracy: 65.66
Round  16, Train loss: 1.001, Test loss: 0.983, Test accuracy: 65.60
Round  17, Train loss: 1.006, Test loss: 0.956, Test accuracy: 66.82
Round  18, Train loss: 0.973, Test loss: 0.951, Test accuracy: 66.50
Round  19, Train loss: 0.939, Test loss: 0.941, Test accuracy: 67.44
Round  20, Train loss: 0.930, Test loss: 0.918, Test accuracy: 68.18
Round  21, Train loss: 0.907, Test loss: 0.927, Test accuracy: 67.64
Round  22, Train loss: 0.888, Test loss: 0.905, Test accuracy: 68.68
Round  23, Train loss: 0.892, Test loss: 0.900, Test accuracy: 68.36
Round  24, Train loss: 0.882, Test loss: 0.893, Test accuracy: 68.88
Round  25, Train loss: 0.872, Test loss: 0.876, Test accuracy: 69.77
Round  26, Train loss: 0.843, Test loss: 0.865, Test accuracy: 70.17
Round  27, Train loss: 0.828, Test loss: 0.857, Test accuracy: 70.31
Round  28, Train loss: 0.826, Test loss: 0.843, Test accuracy: 70.69
Round  29, Train loss: 0.795, Test loss: 0.839, Test accuracy: 71.02
Round  30, Train loss: 0.777, Test loss: 0.838, Test accuracy: 70.99
Round  31, Train loss: 0.807, Test loss: 0.829, Test accuracy: 71.30
Round  32, Train loss: 0.794, Test loss: 0.821, Test accuracy: 71.53
Round  33, Train loss: 0.757, Test loss: 0.817, Test accuracy: 71.65
Round  34, Train loss: 0.749, Test loss: 0.814, Test accuracy: 72.21
Round  35, Train loss: 0.746, Test loss: 0.809, Test accuracy: 72.19
Round  36, Train loss: 0.769, Test loss: 0.821, Test accuracy: 72.26
Round  37, Train loss: 0.752, Test loss: 0.818, Test accuracy: 72.10
Round  38, Train loss: 0.722, Test loss: 0.823, Test accuracy: 72.06
Round  39, Train loss: 0.736, Test loss: 0.804, Test accuracy: 72.52
Round  40, Train loss: 0.716, Test loss: 0.795, Test accuracy: 73.02
Round  41, Train loss: 0.681, Test loss: 0.801, Test accuracy: 72.88
Round  42, Train loss: 0.678, Test loss: 0.802, Test accuracy: 72.95
Round  43, Train loss: 0.717, Test loss: 0.806, Test accuracy: 72.59
Round  44, Train loss: 0.692, Test loss: 0.806, Test accuracy: 72.61
Round  45, Train loss: 0.672, Test loss: 0.789, Test accuracy: 73.25
Round  46, Train loss: 0.678, Test loss: 0.792, Test accuracy: 73.04
Round  47, Train loss: 0.652, Test loss: 0.795, Test accuracy: 73.20
Round  48, Train loss: 0.702, Test loss: 0.778, Test accuracy: 73.86
Round  49, Train loss: 0.649, Test loss: 0.772, Test accuracy: 74.33
Round  50, Train loss: 0.671, Test loss: 0.764, Test accuracy: 74.38
Round  51, Train loss: 0.644, Test loss: 0.769, Test accuracy: 74.19
Round  52, Train loss: 0.642, Test loss: 0.764, Test accuracy: 74.44
Round  53, Train loss: 0.634, Test loss: 0.760, Test accuracy: 74.63
Round  54, Train loss: 0.636, Test loss: 0.775, Test accuracy: 73.97
Round  55, Train loss: 0.614, Test loss: 0.775, Test accuracy: 73.94
Round  56, Train loss: 0.637, Test loss: 0.766, Test accuracy: 74.53
Round  57, Train loss: 0.614, Test loss: 0.791, Test accuracy: 73.77
Round  58, Train loss: 0.630, Test loss: 0.769, Test accuracy: 74.39
Round  59, Train loss: 0.596, Test loss: 0.778, Test accuracy: 74.39
Round  60, Train loss: 0.615, Test loss: 0.768, Test accuracy: 74.88
Round  61, Train loss: 0.591, Test loss: 0.784, Test accuracy: 74.54
Round  62, Train loss: 0.583, Test loss: 0.770, Test accuracy: 74.89
Round  63, Train loss: 0.612, Test loss: 0.762, Test accuracy: 75.13
Round  64, Train loss: 0.591, Test loss: 0.767, Test accuracy: 75.15
Round  65, Train loss: 0.585, Test loss: 0.777, Test accuracy: 74.67
Round  66, Train loss: 0.593, Test loss: 0.757, Test accuracy: 75.20
Round  67, Train loss: 0.566, Test loss: 0.754, Test accuracy: 75.19
Round  68, Train loss: 0.570, Test loss: 0.759, Test accuracy: 75.22
Round  69, Train loss: 0.580, Test loss: 0.759, Test accuracy: 75.38
Round  70, Train loss: 0.580, Test loss: 0.752, Test accuracy: 75.53
Round  71, Train loss: 0.566, Test loss: 0.765, Test accuracy: 75.27
Round  72, Train loss: 0.560, Test loss: 0.760, Test accuracy: 75.53
Round  73, Train loss: 0.541, Test loss: 0.762, Test accuracy: 75.37
Round  74, Train loss: 0.564, Test loss: 0.768, Test accuracy: 74.88
Round  75, Train loss: 0.539, Test loss: 0.760, Test accuracy: 75.16
Round  76, Train loss: 0.552, Test loss: 0.752, Test accuracy: 75.58
Round  77, Train loss: 0.553, Test loss: 0.755, Test accuracy: 75.57
Round  78, Train loss: 0.551, Test loss: 0.743, Test accuracy: 75.70
Round  79, Train loss: 0.533, Test loss: 0.764, Test accuracy: 75.03
Round  80, Train loss: 0.500, Test loss: 0.764, Test accuracy: 75.35
Round  81, Train loss: 0.555, Test loss: 0.771, Test accuracy: 75.32
Round  82, Train loss: 0.549, Test loss: 0.756, Test accuracy: 75.57
Round  83, Train loss: 0.541, Test loss: 0.766, Test accuracy: 75.41
Round  84, Train loss: 0.555, Test loss: 0.761, Test accuracy: 75.62
Round  85, Train loss: 0.519, Test loss: 0.772, Test accuracy: 75.67
Round  86, Train loss: 0.493, Test loss: 0.768, Test accuracy: 75.69
Round  87, Train loss: 0.521, Test loss: 0.760, Test accuracy: 75.93
Round  88, Train loss: 0.528, Test loss: 0.764, Test accuracy: 75.98
Round  89, Train loss: 0.530, Test loss: 0.760, Test accuracy: 75.61
Round  90, Train loss: 0.503, Test loss: 0.761, Test accuracy: 76.13
Round  91, Train loss: 0.509, Test loss: 0.760, Test accuracy: 76.13
Round  92, Train loss: 0.495, Test loss: 0.772, Test accuracy: 75.99
Round  93, Train loss: 0.521, Test loss: 0.773, Test accuracy: 76.08
Round  94, Train loss: 0.495, Test loss: 0.775, Test accuracy: 75.93
Round  95, Train loss: 0.479, Test loss: 0.790, Test accuracy: 75.96
Round  96, Train loss: 0.473, Test loss: 0.800, Test accuracy: 75.50
Round  97, Train loss: 0.488, Test loss: 0.783, Test accuracy: 75.78
Round  98, Train loss: 0.491, Test loss: 0.785, Test accuracy: 75.62
Round  99, Train loss: 0.486, Test loss: 0.779, Test accuracy: 76.04
Final Round, Train loss: 0.412, Test loss: 0.769, Test accuracy: 76.12
Average accuracy final 10 rounds: 75.91675000000001 

5374.410232543945
[4.4410011768341064, 8.882002353668213, 13.111287593841553, 17.340572834014893, 21.60257840156555, 25.86458396911621, 30.14181923866272, 34.41905450820923, 38.65966796875, 42.90028142929077, 47.34352970123291, 51.78677797317505, 56.018364906311035, 60.24995183944702, 64.48324847221375, 68.71654510498047, 73.00238466262817, 77.28822422027588, 81.9493637084961, 86.61050319671631, 91.36762928962708, 96.12475538253784, 100.89726519584656, 105.66977500915527, 110.31812262535095, 114.96647024154663, 119.65616083145142, 124.3458514213562, 128.9912552833557, 133.63665914535522, 138.14057517051697, 142.6444911956787, 146.99258708953857, 151.34068298339844, 156.14576125144958, 160.95083951950073, 165.95804858207703, 170.96525764465332, 175.92714381217957, 180.8890299797058, 185.65877604484558, 190.42852210998535, 195.23509073257446, 200.04165935516357, 204.66588401794434, 209.2901086807251, 214.0010540485382, 218.71199941635132, 223.5822651386261, 228.45253086090088, 233.39496850967407, 238.33740615844727, 243.23986864089966, 248.14233112335205, 253.02901554107666, 257.91569995880127, 262.85569500923157, 267.79569005966187, 272.68495750427246, 277.57422494888306, 282.22627544403076, 286.87832593917847, 291.5596709251404, 296.2410159111023, 300.93289613723755, 305.6247763633728, 310.1928014755249, 314.760826587677, 319.43253231048584, 324.1042380332947, 329.0181601047516, 333.9320821762085, 338.81048130989075, 343.688880443573, 348.44318652153015, 353.1974925994873, 358.08133125305176, 362.9651699066162, 367.87198066711426, 372.7787914276123, 377.72208857536316, 382.665385723114, 387.35163140296936, 392.0378770828247, 397.03793120384216, 402.0379853248596, 406.821026802063, 411.60406827926636, 416.5061228275299, 421.40817737579346, 426.33289790153503, 431.2576184272766, 436.0540716648102, 440.85052490234375, 445.6615891456604, 450.47265338897705, 455.35817313194275, 460.24369287490845, 465.10193490982056, 469.96017694473267, 474.8836236000061, 479.80707025527954, 484.71143531799316, 489.6158003807068, 494.5329840183258, 499.4501676559448, 504.2967188358307, 509.14327001571655, 514.0608232021332, 518.9783763885498, 523.7127499580383, 528.4471235275269, 533.3544285297394, 538.2617335319519, 543.1436052322388, 548.0254769325256, 552.784098148346, 557.5427193641663, 562.242374420166, 566.9420294761658, 571.6473252773285, 576.3526210784912, 581.1754870414734, 585.9983530044556, 590.8283064365387, 595.6582598686218, 600.442581653595, 605.2269034385681, 609.8672630786896, 614.507622718811, 619.1732518672943, 623.8388810157776, 628.5392825603485, 633.2396841049194, 638.0597808361053, 642.8798775672913, 647.6651256084442, 652.4503736495972, 657.1919825077057, 661.9335913658142, 666.6072278022766, 671.280864238739, 675.7055933475494, 680.1303224563599, 684.649756193161, 689.1691899299622, 693.5548431873322, 697.9404964447021, 702.2946903705597, 706.6488842964172, 711.030219078064, 715.4115538597107, 719.8836100101471, 724.3556661605835, 728.8611161708832, 733.3665661811829, 737.8041460514069, 742.2417259216309, 746.6256544589996, 751.0095829963684, 755.3348717689514, 759.6601605415344, 764.0563213825226, 768.4524822235107, 772.8212521076202, 777.1900219917297, 781.5553181171417, 785.9206142425537, 790.3236010074615, 794.7265877723694, 799.1985769271851, 803.6705660820007, 808.2648346424103, 812.8591032028198, 817.4685657024384, 822.0780282020569, 826.5396292209625, 831.0012302398682, 835.4369597434998, 839.8726892471313, 844.4157321453094, 848.9587750434875, 853.4866125583649, 858.0144500732422, 862.5289285182953, 867.0434069633484, 871.42529296875, 875.8071789741516, 880.190484046936, 884.5737891197205, 888.957174539566, 893.3405599594116, 897.7366940975189, 902.1328282356262, 906.4674098491669, 910.8019914627075, 915.117832660675, 919.4336738586426, 923.8109300136566, 928.1881861686707, 930.2543728351593, 932.320559501648]
[28.98, 28.98, 39.2625, 39.2625, 43.65, 43.65, 46.5625, 46.5625, 49.95, 49.95, 52.8425, 52.8425, 54.865, 54.865, 57.585, 57.585, 58.87, 58.87, 59.77, 59.77, 60.9325, 60.9325, 61.5, 61.5, 61.755, 61.755, 63.6975, 63.6975, 64.07, 64.07, 65.6625, 65.6625, 65.6, 65.6, 66.82, 66.82, 66.5025, 66.5025, 67.4375, 67.4375, 68.1825, 68.1825, 67.6425, 67.6425, 68.6825, 68.6825, 68.3575, 68.3575, 68.8775, 68.8775, 69.7725, 69.7725, 70.175, 70.175, 70.3125, 70.3125, 70.685, 70.685, 71.0175, 71.0175, 70.99, 70.99, 71.2975, 71.2975, 71.525, 71.525, 71.65, 71.65, 72.2075, 72.2075, 72.185, 72.185, 72.2575, 72.2575, 72.0975, 72.0975, 72.055, 72.055, 72.52, 72.52, 73.0225, 73.0225, 72.88, 72.88, 72.955, 72.955, 72.5875, 72.5875, 72.6075, 72.6075, 73.2525, 73.2525, 73.04, 73.04, 73.205, 73.205, 73.8625, 73.8625, 74.3275, 74.3275, 74.38, 74.38, 74.19, 74.19, 74.44, 74.44, 74.63, 74.63, 73.965, 73.965, 73.9425, 73.9425, 74.5275, 74.5275, 73.7675, 73.7675, 74.39, 74.39, 74.395, 74.395, 74.875, 74.875, 74.5425, 74.5425, 74.89, 74.89, 75.13, 75.13, 75.1525, 75.1525, 74.6725, 74.6725, 75.2, 75.2, 75.1925, 75.1925, 75.225, 75.225, 75.3825, 75.3825, 75.535, 75.535, 75.27, 75.27, 75.53, 75.53, 75.37, 75.37, 74.8825, 74.8825, 75.1575, 75.1575, 75.585, 75.585, 75.5675, 75.5675, 75.7, 75.7, 75.0275, 75.0275, 75.3525, 75.3525, 75.3225, 75.3225, 75.5725, 75.5725, 75.41, 75.41, 75.6225, 75.6225, 75.67, 75.67, 75.6875, 75.6875, 75.93, 75.93, 75.9825, 75.9825, 75.6125, 75.6125, 76.13, 76.13, 76.13, 76.13, 75.9925, 75.9925, 76.0775, 76.0775, 75.9275, 75.9275, 75.96, 75.96, 75.5025, 75.5025, 75.785, 75.785, 75.62, 75.62, 76.0425, 76.0425, 76.1225, 76.1225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.219, Test loss: 1.904, Test accuracy: 33.44
Round   1, Train loss: 1.869, Test loss: 1.646, Test accuracy: 42.89
Round   2, Train loss: 1.687, Test loss: 1.541, Test accuracy: 46.77
Round   3, Train loss: 1.593, Test loss: 1.448, Test accuracy: 49.73
Round   4, Train loss: 1.506, Test loss: 1.394, Test accuracy: 52.53
Round   5, Train loss: 1.430, Test loss: 1.315, Test accuracy: 55.18
Round   6, Train loss: 1.374, Test loss: 1.242, Test accuracy: 56.96
Round   7, Train loss: 1.307, Test loss: 1.212, Test accuracy: 58.53
Round   8, Train loss: 1.244, Test loss: 1.157, Test accuracy: 60.80
Round   9, Train loss: 1.239, Test loss: 1.099, Test accuracy: 61.95
Round  10, Train loss: 1.187, Test loss: 1.087, Test accuracy: 63.20
Round  11, Train loss: 1.163, Test loss: 1.053, Test accuracy: 63.97
Round  12, Train loss: 1.137, Test loss: 1.006, Test accuracy: 65.29
Round  13, Train loss: 1.089, Test loss: 1.000, Test accuracy: 65.78
Round  14, Train loss: 1.063, Test loss: 0.992, Test accuracy: 66.67
Round  15, Train loss: 1.047, Test loss: 0.975, Test accuracy: 66.76
Round  16, Train loss: 1.024, Test loss: 0.935, Test accuracy: 68.01
Round  17, Train loss: 0.995, Test loss: 0.924, Test accuracy: 68.46
Round  18, Train loss: 0.980, Test loss: 0.903, Test accuracy: 69.05
Round  19, Train loss: 0.951, Test loss: 0.910, Test accuracy: 68.45
Round  20, Train loss: 0.927, Test loss: 0.907, Test accuracy: 68.71
Round  21, Train loss: 0.924, Test loss: 0.871, Test accuracy: 69.69
Round  22, Train loss: 0.886, Test loss: 0.875, Test accuracy: 69.95
Round  23, Train loss: 0.903, Test loss: 0.856, Test accuracy: 70.35
Round  24, Train loss: 0.873, Test loss: 0.851, Test accuracy: 70.85
Round  25, Train loss: 0.858, Test loss: 0.841, Test accuracy: 71.03
Round  26, Train loss: 0.843, Test loss: 0.821, Test accuracy: 72.00
Round  27, Train loss: 0.860, Test loss: 0.811, Test accuracy: 72.32
Round  28, Train loss: 0.816, Test loss: 0.804, Test accuracy: 72.58
Round  29, Train loss: 0.813, Test loss: 0.799, Test accuracy: 73.08
Round  30, Train loss: 0.805, Test loss: 0.798, Test accuracy: 72.87
Round  31, Train loss: 0.785, Test loss: 0.781, Test accuracy: 73.50
Round  32, Train loss: 0.806, Test loss: 0.774, Test accuracy: 73.70
Round  33, Train loss: 0.781, Test loss: 0.768, Test accuracy: 73.94
Round  34, Train loss: 0.758, Test loss: 0.775, Test accuracy: 73.62
Round  35, Train loss: 0.770, Test loss: 0.769, Test accuracy: 74.02
Round  36, Train loss: 0.750, Test loss: 0.768, Test accuracy: 73.82
Round  37, Train loss: 0.741, Test loss: 0.760, Test accuracy: 74.16
Round  38, Train loss: 0.744, Test loss: 0.755, Test accuracy: 74.35
Round  39, Train loss: 0.712, Test loss: 0.759, Test accuracy: 74.21
Round  40, Train loss: 0.698, Test loss: 0.746, Test accuracy: 74.53
Round  41, Train loss: 0.722, Test loss: 0.744, Test accuracy: 74.75
Round  42, Train loss: 0.691, Test loss: 0.742, Test accuracy: 74.91
Round  43, Train loss: 0.675, Test loss: 0.743, Test accuracy: 74.69
Round  44, Train loss: 0.702, Test loss: 0.745, Test accuracy: 74.76
Round  45, Train loss: 0.720, Test loss: 0.732, Test accuracy: 74.97
Round  46, Train loss: 0.664, Test loss: 0.735, Test accuracy: 75.14
Round  47, Train loss: 0.662, Test loss: 0.743, Test accuracy: 75.26
Round  48, Train loss: 0.663, Test loss: 0.733, Test accuracy: 75.04
Round  49, Train loss: 0.680, Test loss: 0.727, Test accuracy: 75.69
Round  50, Train loss: 0.649, Test loss: 0.730, Test accuracy: 75.56
Round  51, Train loss: 0.673, Test loss: 0.734, Test accuracy: 75.17
Round  52, Train loss: 0.655, Test loss: 0.736, Test accuracy: 75.58
Round  53, Train loss: 0.635, Test loss: 0.726, Test accuracy: 75.80
Round  54, Train loss: 0.625, Test loss: 0.733, Test accuracy: 75.51
Round  55, Train loss: 0.619, Test loss: 0.726, Test accuracy: 75.80
Round  56, Train loss: 0.613, Test loss: 0.725, Test accuracy: 75.78
Round  57, Train loss: 0.632, Test loss: 0.717, Test accuracy: 75.91
Round  58, Train loss: 0.636, Test loss: 0.720, Test accuracy: 75.63
Round  59, Train loss: 0.622, Test loss: 0.719, Test accuracy: 75.75
Round  60, Train loss: 0.598, Test loss: 0.723, Test accuracy: 76.03
Round  61, Train loss: 0.599, Test loss: 0.722, Test accuracy: 75.89
Round  62, Train loss: 0.599, Test loss: 0.720, Test accuracy: 76.10
Round  63, Train loss: 0.614, Test loss: 0.720, Test accuracy: 75.88
Round  64, Train loss: 0.608, Test loss: 0.717, Test accuracy: 76.26
Round  65, Train loss: 0.589, Test loss: 0.710, Test accuracy: 76.41
Round  66, Train loss: 0.583, Test loss: 0.716, Test accuracy: 76.24
Round  67, Train loss: 0.579, Test loss: 0.712, Test accuracy: 76.35
Round  68, Train loss: 0.596, Test loss: 0.705, Test accuracy: 76.72
Round  69, Train loss: 0.572, Test loss: 0.714, Test accuracy: 76.35
Round  70, Train loss: 0.568, Test loss: 0.716, Test accuracy: 76.40
Round  71, Train loss: 0.551, Test loss: 0.715, Test accuracy: 76.39
Round  72, Train loss: 0.563, Test loss: 0.720, Test accuracy: 76.15
Round  73, Train loss: 0.551, Test loss: 0.720, Test accuracy: 76.26
Round  74, Train loss: 0.562, Test loss: 0.709, Test accuracy: 76.91
Round  75, Train loss: 0.568, Test loss: 0.713, Test accuracy: 76.64
Round  76, Train loss: 0.556, Test loss: 0.711, Test accuracy: 76.88
Round  77, Train loss: 0.578, Test loss: 0.705, Test accuracy: 76.90
Round  78, Train loss: 0.563, Test loss: 0.699, Test accuracy: 77.11
Round  79, Train loss: 0.538, Test loss: 0.704, Test accuracy: 77.04
Round  80, Train loss: 0.545, Test loss: 0.706, Test accuracy: 77.00
Round  81, Train loss: 0.504, Test loss: 0.714, Test accuracy: 77.14
Round  82, Train loss: 0.559, Test loss: 0.710, Test accuracy: 76.72
Round  83, Train loss: 0.549, Test loss: 0.706, Test accuracy: 76.96
Round  84, Train loss: 0.549, Test loss: 0.718, Test accuracy: 76.55
Round  85, Train loss: 0.548, Test loss: 0.697, Test accuracy: 76.91
Round  86, Train loss: 0.532, Test loss: 0.700, Test accuracy: 76.85
Round  87, Train loss: 0.516, Test loss: 0.711, Test accuracy: 76.89
Round  88, Train loss: 0.525, Test loss: 0.702, Test accuracy: 77.12
Round  89, Train loss: 0.495, Test loss: 0.710, Test accuracy: 76.89
Round  90, Train loss: 0.508, Test loss: 0.705, Test accuracy: 77.08
Round  91, Train loss: 0.520, Test loss: 0.717, Test accuracy: 76.52
Round  92, Train loss: 0.537, Test loss: 0.712, Test accuracy: 76.58
Round  93, Train loss: 0.499, Test loss: 0.709, Test accuracy: 76.91
Round  94, Train loss: 0.524, Test loss: 0.711, Test accuracy: 77.16
Round  95, Train loss: 0.521, Test loss: 0.710, Test accuracy: 77.11
Round  96, Train loss: 0.515, Test loss: 0.699, Test accuracy: 76.98
Round  97, Train loss: 0.516, Test loss: 0.700, Test accuracy: 77.27
Round  98, Train loss: 0.490, Test loss: 0.700, Test accuracy: 77.01
Round  99, Train loss: 0.496, Test loss: 0.709, Test accuracy: 77.00
Final Round, Train loss: 0.399, Test loss: 0.706, Test accuracy: 77.17
Average accuracy final 10 rounds: 76.96249999999999
6503.081434726715
[5.976212978363037, 11.952425956726074, 17.95141863822937, 23.950411319732666, 29.996586799621582, 36.0427622795105, 42.04504156112671, 48.04732084274292, 54.00823950767517, 59.96915817260742, 66.07207250595093, 72.17498683929443, 78.17348337173462, 84.1719799041748, 90.33329248428345, 96.49460506439209, 102.55857849121094, 108.62255191802979, 114.61524844169617, 120.60794496536255, 126.48293328285217, 132.3579216003418, 137.83346462249756, 143.30900764465332, 148.82296180725098, 154.33691596984863, 160.50093412399292, 166.6649522781372, 172.80970358848572, 178.95445489883423, 185.08621549606323, 191.21797609329224, 197.39061856269836, 203.5632610321045, 209.7455928325653, 215.92792463302612, 222.02151346206665, 228.11510229110718, 233.8804874420166, 239.64587259292603, 245.37369513511658, 251.10151767730713, 256.77071475982666, 262.4399118423462, 268.1957333087921, 273.95155477523804, 279.7525932788849, 285.55363178253174, 291.6541004180908, 297.7545690536499, 303.4490671157837, 309.1435651779175, 314.90316915512085, 320.6627731323242, 326.4842448234558, 332.3057165145874, 338.1716148853302, 344.037513256073, 349.72623801231384, 355.4149627685547, 361.52257895469666, 367.6301951408386, 373.6719810962677, 379.7137670516968, 386.0733919143677, 392.4330167770386, 399.1221122741699, 405.81120777130127, 412.4981048107147, 419.1850018501282, 425.724365234375, 432.2637286186218, 438.13226532936096, 444.0008020401001, 450.5832288265228, 457.16565561294556, 463.7310161590576, 470.2963767051697, 476.8334527015686, 483.37052869796753, 489.962842464447, 496.5551562309265, 503.1897521018982, 509.8243479728699, 516.273752450943, 522.7231569290161, 529.223569393158, 535.7239818572998, 542.3634796142578, 549.0029773712158, 555.5055868625641, 562.0081963539124, 568.5753335952759, 575.1424708366394, 581.6324768066406, 588.1224827766418, 594.7216267585754, 601.320770740509, 607.9058899879456, 614.4910092353821, 621.1160109043121, 627.7410125732422, 634.3918058872223, 641.0425992012024, 647.7860250473022, 654.5294508934021, 661.2157275676727, 667.9020042419434, 674.6989905834198, 681.4959769248962, 688.3051664829254, 695.1143560409546, 701.8144638538361, 708.5145716667175, 715.3340425491333, 722.1535134315491, 728.8153462409973, 735.4771790504456, 742.2454617023468, 749.013744354248, 755.6238529682159, 762.2339615821838, 769.0400500297546, 775.8461384773254, 782.634467124939, 789.4227957725525, 796.2269146442413, 803.0310335159302, 809.6662285327911, 816.3014235496521, 822.6989412307739, 829.0964589118958, 835.8187634944916, 842.5410680770874, 849.2192914485931, 855.8975148200989, 862.4176259040833, 868.9377369880676, 875.5022552013397, 882.0667734146118, 888.6786391735077, 895.2905049324036, 902.0280911922455, 908.7656774520874, 915.4869337081909, 922.2081899642944, 928.9395966529846, 935.6710033416748, 942.3768229484558, 949.0826425552368, 955.7463073730469, 962.4099721908569, 969.0541045665741, 975.6982369422913, 982.4140050411224, 989.1297731399536, 995.8732538223267, 1002.6167345046997, 1009.308361530304, 1015.9999885559082, 1022.6546142101288, 1029.3092398643494, 1036.017007112503, 1042.7247743606567, 1049.4270479679108, 1056.1293215751648, 1062.7964046001434, 1069.463487625122, 1076.1606657505035, 1082.857843875885, 1089.6025450229645, 1096.347246170044, 1103.0934340953827, 1109.8396220207214, 1116.564198732376, 1123.2887754440308, 1130.0248730182648, 1136.7609705924988, 1143.5139002799988, 1150.2668299674988, 1157.0135617256165, 1163.7602934837341, 1170.4804260730743, 1177.2005586624146, 1183.9118692874908, 1190.6231799125671, 1197.3327865600586, 1204.04239320755, 1210.7840776443481, 1217.5257620811462, 1224.3065502643585, 1231.0873384475708, 1237.9229745864868, 1244.7586107254028, 1251.5255444049835, 1258.2924780845642, 1265.0161151885986, 1271.739752292633, 1278.5498690605164, 1285.3599858283997, 1287.9123392105103, 1290.4646925926208]
[33.435, 33.435, 42.8925, 42.8925, 46.7675, 46.7675, 49.7325, 49.7325, 52.5275, 52.5275, 55.18, 55.18, 56.96, 56.96, 58.53, 58.53, 60.805, 60.805, 61.9475, 61.9475, 63.2, 63.2, 63.965, 63.965, 65.2925, 65.2925, 65.78, 65.78, 66.67, 66.67, 66.76, 66.76, 68.01, 68.01, 68.46, 68.46, 69.045, 69.045, 68.455, 68.455, 68.71, 68.71, 69.695, 69.695, 69.95, 69.95, 70.3525, 70.3525, 70.8525, 70.8525, 71.035, 71.035, 71.9975, 71.9975, 72.32, 72.32, 72.575, 72.575, 73.0825, 73.0825, 72.8725, 72.8725, 73.505, 73.505, 73.7025, 73.7025, 73.9425, 73.9425, 73.625, 73.625, 74.02, 74.02, 73.82, 73.82, 74.16, 74.16, 74.3475, 74.3475, 74.2125, 74.2125, 74.5275, 74.5275, 74.7475, 74.7475, 74.9125, 74.9125, 74.6925, 74.6925, 74.7575, 74.7575, 74.9675, 74.9675, 75.1375, 75.1375, 75.2625, 75.2625, 75.04, 75.04, 75.685, 75.685, 75.5625, 75.5625, 75.1675, 75.1675, 75.585, 75.585, 75.7975, 75.7975, 75.5125, 75.5125, 75.8025, 75.8025, 75.785, 75.785, 75.91, 75.91, 75.6275, 75.6275, 75.7475, 75.7475, 76.03, 76.03, 75.89, 75.89, 76.1, 76.1, 75.8825, 75.8825, 76.2625, 76.2625, 76.4075, 76.4075, 76.2375, 76.2375, 76.3475, 76.3475, 76.7225, 76.7225, 76.35, 76.35, 76.4025, 76.4025, 76.39, 76.39, 76.15, 76.15, 76.2575, 76.2575, 76.905, 76.905, 76.645, 76.645, 76.875, 76.875, 76.9025, 76.9025, 77.1125, 77.1125, 77.0375, 77.0375, 77.0, 77.0, 77.14, 77.14, 76.72, 76.72, 76.9575, 76.9575, 76.55, 76.55, 76.9075, 76.9075, 76.85, 76.85, 76.89, 76.89, 77.12, 77.12, 76.895, 76.895, 77.08, 77.08, 76.5225, 76.5225, 76.5775, 76.5775, 76.91, 76.91, 77.1575, 77.1575, 77.11, 77.11, 76.98, 76.98, 77.2725, 77.2725, 77.0125, 77.0125, 77.0025, 77.0025, 77.17, 77.17]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.180, Test loss: 1.758, Test accuracy: 38.53
Round   1, Train loss: 1.779, Test loss: 1.458, Test accuracy: 47.88
Round   2, Train loss: 1.614, Test loss: 1.343, Test accuracy: 52.08
Round   3, Train loss: 1.502, Test loss: 1.250, Test accuracy: 55.73
Round   4, Train loss: 1.387, Test loss: 1.180, Test accuracy: 58.31
Round   5, Train loss: 1.333, Test loss: 1.108, Test accuracy: 60.75
Round   6, Train loss: 1.249, Test loss: 1.059, Test accuracy: 63.20
Round   7, Train loss: 1.185, Test loss: 1.029, Test accuracy: 63.72
Round   8, Train loss: 1.158, Test loss: 0.995, Test accuracy: 65.28
Round   9, Train loss: 1.113, Test loss: 0.949, Test accuracy: 66.54
Round  10, Train loss: 1.065, Test loss: 0.942, Test accuracy: 67.08
Round  11, Train loss: 1.025, Test loss: 0.917, Test accuracy: 68.10
Round  12, Train loss: 0.979, Test loss: 0.893, Test accuracy: 69.60
Round  13, Train loss: 0.990, Test loss: 0.876, Test accuracy: 69.62
Round  14, Train loss: 0.945, Test loss: 0.868, Test accuracy: 70.11
Round  15, Train loss: 0.913, Test loss: 0.852, Test accuracy: 70.29
Round  16, Train loss: 0.887, Test loss: 0.831, Test accuracy: 71.40
Round  17, Train loss: 0.862, Test loss: 0.819, Test accuracy: 71.35
Round  18, Train loss: 0.839, Test loss: 0.810, Test accuracy: 71.88
Round  19, Train loss: 0.817, Test loss: 0.809, Test accuracy: 72.52
Round  20, Train loss: 0.830, Test loss: 0.788, Test accuracy: 72.82
Round  21, Train loss: 0.771, Test loss: 0.799, Test accuracy: 72.65
Round  22, Train loss: 0.787, Test loss: 0.798, Test accuracy: 72.45
Round  23, Train loss: 0.760, Test loss: 0.784, Test accuracy: 73.59
Round  24, Train loss: 0.748, Test loss: 0.781, Test accuracy: 73.96
Round  25, Train loss: 0.738, Test loss: 0.775, Test accuracy: 74.05
Round  26, Train loss: 0.727, Test loss: 0.780, Test accuracy: 73.75
Round  27, Train loss: 0.695, Test loss: 0.775, Test accuracy: 73.70
Round  28, Train loss: 0.707, Test loss: 0.752, Test accuracy: 74.22
Round  29, Train loss: 0.720, Test loss: 0.745, Test accuracy: 74.78
Round  30, Train loss: 0.681, Test loss: 0.767, Test accuracy: 73.84
Round  31, Train loss: 0.664, Test loss: 0.744, Test accuracy: 75.11
Round  32, Train loss: 0.651, Test loss: 0.755, Test accuracy: 74.92
Round  33, Train loss: 0.691, Test loss: 0.737, Test accuracy: 75.49
Round  34, Train loss: 0.617, Test loss: 0.755, Test accuracy: 75.28
Round  35, Train loss: 0.640, Test loss: 0.737, Test accuracy: 75.25
Round  36, Train loss: 0.627, Test loss: 0.756, Test accuracy: 75.45
Round  37, Train loss: 0.608, Test loss: 0.753, Test accuracy: 75.66
Round  38, Train loss: 0.630, Test loss: 0.749, Test accuracy: 75.54
Round  39, Train loss: 0.591, Test loss: 0.749, Test accuracy: 76.09
Round  40, Train loss: 0.616, Test loss: 0.741, Test accuracy: 76.02
Round  41, Train loss: 0.582, Test loss: 0.744, Test accuracy: 76.17
Round  42, Train loss: 0.570, Test loss: 0.747, Test accuracy: 75.92
Round  43, Train loss: 0.565, Test loss: 0.739, Test accuracy: 76.20
Round  44, Train loss: 0.571, Test loss: 0.748, Test accuracy: 75.57
Round  45, Train loss: 0.553, Test loss: 0.756, Test accuracy: 76.08
Round  46, Train loss: 0.566, Test loss: 0.737, Test accuracy: 76.11
Round  47, Train loss: 0.533, Test loss: 0.756, Test accuracy: 76.22
Round  48, Train loss: 0.527, Test loss: 0.747, Test accuracy: 76.35
Round  49, Train loss: 0.530, Test loss: 0.760, Test accuracy: 76.68
Round  50, Train loss: 0.550, Test loss: 0.749, Test accuracy: 76.45
Round  51, Train loss: 0.538, Test loss: 0.761, Test accuracy: 76.21
Round  52, Train loss: 0.506, Test loss: 0.747, Test accuracy: 76.44
Round  53, Train loss: 0.521, Test loss: 0.740, Test accuracy: 76.63
Round  54, Train loss: 0.586, Test loss: 0.726, Test accuracy: 76.83
Round  55, Train loss: 0.524, Test loss: 0.732, Test accuracy: 76.47
Round  56, Train loss: 0.519, Test loss: 0.735, Test accuracy: 76.75
Round  57, Train loss: 0.505, Test loss: 0.744, Test accuracy: 76.76
Round  58, Train loss: 0.553, Test loss: 0.722, Test accuracy: 76.82
Round  59, Train loss: 0.503, Test loss: 0.747, Test accuracy: 76.65
Round  60, Train loss: 0.501, Test loss: 0.736, Test accuracy: 76.50
Round  61, Train loss: 0.498, Test loss: 0.750, Test accuracy: 76.84
Round  62, Train loss: 0.496, Test loss: 0.737, Test accuracy: 76.91
Round  63, Train loss: 0.482, Test loss: 0.744, Test accuracy: 76.73
Round  64, Train loss: 0.446, Test loss: 0.750, Test accuracy: 76.79
Round  65, Train loss: 0.448, Test loss: 0.750, Test accuracy: 76.73
Round  66, Train loss: 0.494, Test loss: 0.739, Test accuracy: 76.85
Round  67, Train loss: 0.491, Test loss: 0.732, Test accuracy: 77.06
Round  68, Train loss: 0.470, Test loss: 0.752, Test accuracy: 76.92
Round  69, Train loss: 0.432, Test loss: 0.757, Test accuracy: 77.12
Round  70, Train loss: 0.474, Test loss: 0.739, Test accuracy: 76.71
Round  71, Train loss: 0.460, Test loss: 0.756, Test accuracy: 76.91
Round  72, Train loss: 0.437, Test loss: 0.752, Test accuracy: 77.05
Round  73, Train loss: 0.428, Test loss: 0.756, Test accuracy: 77.36
Round  74, Train loss: 0.474, Test loss: 0.740, Test accuracy: 77.23
Round  75, Train loss: 0.485, Test loss: 0.735, Test accuracy: 77.74
Round  76, Train loss: 0.451, Test loss: 0.738, Test accuracy: 77.76
Round  77, Train loss: 0.432, Test loss: 0.751, Test accuracy: 77.60
Round  78, Train loss: 0.453, Test loss: 0.741, Test accuracy: 77.45
Round  79, Train loss: 0.423, Test loss: 0.744, Test accuracy: 77.47
Round  80, Train loss: 0.446, Test loss: 0.731, Test accuracy: 78.01
Round  81, Train loss: 0.430, Test loss: 0.747, Test accuracy: 77.13
Round  82, Train loss: 0.422, Test loss: 0.746, Test accuracy: 77.48
Round  83, Train loss: 0.404, Test loss: 0.764, Test accuracy: 77.56
Round  84, Train loss: 0.452, Test loss: 0.745, Test accuracy: 77.53
Round  85, Train loss: 0.428, Test loss: 0.744, Test accuracy: 77.74
Round  86, Train loss: 0.420, Test loss: 0.748, Test accuracy: 77.24
Round  87, Train loss: 0.431, Test loss: 0.751, Test accuracy: 77.81
Round  88, Train loss: 0.407, Test loss: 0.735, Test accuracy: 77.93
Round  89, Train loss: 0.423, Test loss: 0.738, Test accuracy: 77.78
Round  90, Train loss: 0.421, Test loss: 0.751, Test accuracy: 77.44
Round  91, Train loss: 0.388, Test loss: 0.746, Test accuracy: 77.74
Round  92, Train loss: 0.401, Test loss: 0.755, Test accuracy: 77.46
Round  93, Train loss: 0.447, Test loss: 0.733, Test accuracy: 77.58
Round  94, Train loss: 0.392, Test loss: 0.764, Test accuracy: 77.70
Round  95, Train loss: 0.373, Test loss: 0.768, Test accuracy: 77.45
Round  96, Train loss: 0.399, Test loss: 0.762, Test accuracy: 77.45
Round  97, Train loss: 0.356, Test loss: 0.766, Test accuracy: 77.91
Round  98, Train loss: 0.438, Test loss: 0.751, Test accuracy: 77.76
Round  99, Train loss: 0.403, Test loss: 0.762, Test accuracy: 77.74
Final Round, Train loss: 0.310, Test loss: 0.766, Test accuracy: 78.19
Average accuracy final 10 rounds: 77.62275
9137.956445932388
[13.953087329864502, 27.250203132629395, 40.384379386901855, 54.37002205848694, 68.72841835021973, 83.1047956943512, 97.4068853855133, 111.60332345962524, 125.15745997428894, 138.61904859542847, 152.15467429161072, 165.37668704986572, 178.21385836601257, 191.51144313812256, 205.35760164260864, 219.2588210105896, 233.04241609573364, 247.0635530948639, 261.23811078071594, 275.3281216621399, 289.01191210746765, 302.5511665344238, 316.4195964336395, 330.7157289981842, 343.6380121707916, 356.457218170166, 369.38568091392517, 382.25790786743164, 395.1149525642395, 408.0410041809082, 421.21348547935486, 434.39558815956116, 447.1256036758423, 459.7883870601654, 472.40926241874695, 484.9900562763214, 497.8030996322632, 510.38065242767334, 522.9361484050751, 535.6883487701416, 547.8699135780334, 560.1922652721405, 572.3600664138794, 584.5198903083801, 596.781848192215, 608.9552137851715, 621.0459542274475, 633.1115233898163, 645.1579103469849, 658.3330070972443, 671.5210134983063, 684.8456971645355, 698.2322888374329, 711.5086386203766, 724.6282732486725, 737.8219676017761, 751.0232956409454, 764.3740389347076, 777.5030858516693, 789.6200172901154, 801.7137784957886, 813.8085808753967, 825.9045217037201, 838.0407361984253, 850.1888473033905, 862.4420402050018, 874.7035386562347, 886.9500737190247, 899.2482886314392, 911.5606319904327, 923.8529419898987, 936.0803143978119, 948.3706760406494, 960.6501588821411, 973.8594362735748, 987.8583235740662, 1002.0198674201965, 1016.2215237617493, 1030.352534532547, 1044.4640815258026, 1058.5700912475586, 1072.5770320892334, 1086.64320063591, 1100.7330749034882, 1114.8485498428345, 1128.8761970996857, 1142.8102271556854, 1156.7252132892609, 1170.6855080127716, 1184.6438012123108, 1198.4628574848175, 1212.3668146133423, 1226.2007322311401, 1240.022676229477, 1253.5796446800232, 1267.0771505832672, 1280.4507331848145, 1293.948566198349, 1307.353251695633, 1320.7982845306396, 1324.184820175171]
[38.5325, 47.885, 52.08, 55.725, 58.31, 60.7475, 63.195, 63.7225, 65.2825, 66.5375, 67.085, 68.1025, 69.6025, 69.6225, 70.105, 70.2925, 71.4, 71.3475, 71.875, 72.515, 72.82, 72.6475, 72.455, 73.5875, 73.9625, 74.0475, 73.755, 73.7025, 74.215, 74.7825, 73.84, 75.115, 74.9175, 75.4875, 75.275, 75.245, 75.45, 75.66, 75.5425, 76.09, 76.0225, 76.175, 75.9225, 76.2025, 75.57, 76.08, 76.105, 76.215, 76.3475, 76.6775, 76.45, 76.21, 76.44, 76.63, 76.825, 76.4725, 76.75, 76.7575, 76.8175, 76.65, 76.505, 76.84, 76.9125, 76.735, 76.7925, 76.735, 76.85, 77.06, 76.915, 77.1175, 76.7125, 76.9075, 77.0475, 77.3625, 77.235, 77.7425, 77.76, 77.6, 77.4475, 77.475, 78.0075, 77.1275, 77.485, 77.5575, 77.5325, 77.74, 77.2375, 77.8075, 77.93, 77.775, 77.445, 77.74, 77.46, 77.5775, 77.7025, 77.4525, 77.45, 77.905, 77.7575, 77.7375, 78.19]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.264, Test loss: 2.306, Test accuracy: 6.65
Round   0, Global train loss: 2.264, Global test loss: 2.309, Global test accuracy: 6.66
Round   1, Train loss: 2.249, Test loss: 2.301, Test accuracy: 6.71
Round   1, Global train loss: 2.249, Global test loss: 2.306, Global test accuracy: 6.68
Round   2, Train loss: 2.251, Test loss: 2.296, Test accuracy: 7.78
Round   2, Global train loss: 2.251, Global test loss: 2.303, Global test accuracy: 7.28
Round   3, Train loss: 2.219, Test loss: 2.291, Test accuracy: 11.18
Round   3, Global train loss: 2.219, Global test loss: 2.297, Global test accuracy: 10.55
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 11.00
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 12.33
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 10.89
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 14.18
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 14.91
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 16.37
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 16.37
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 15.12
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 15.12
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 100, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 101, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 102, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 103, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 104, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 105, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 106, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 107, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 108, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 109, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 110, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 111, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 112, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 113, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 114, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 115, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 116, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 117, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 118, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 119, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 120, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 121, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 122, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 123, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 124, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 125, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 126, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 127, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 128, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 129, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 130, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 131, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 132, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 133, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 134, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Average accuracy final 10 rounds: 16.666666666666664 

Average global accuracy final 10 rounds: 16.666666666666664 

5206.311261177063
[1.9856061935424805, 3.682511329650879, 5.4246251583099365, 7.146123647689819, 8.870962858200073, 10.545229434967041, 12.281015157699585, 14.037184953689575, 15.787543058395386, 17.51681613922119, 19.27007484436035, 21.005268096923828, 22.745697259902954, 24.49092388153076, 26.228833436965942, 27.97103190422058, 29.717033624649048, 31.44117283821106, 33.19360685348511, 34.93103885650635, 36.66079640388489, 38.37788367271423, 40.08431839942932, 41.83212733268738, 43.566622495651245, 45.29108715057373, 47.02922296524048, 48.77063870429993, 50.51605153083801, 52.088741064071655, 53.80568885803223, 55.53314542770386, 57.24527716636658, 58.956600189208984, 60.68419623374939, 62.33060002326965, 63.98477530479431, 65.71738696098328, 67.456698179245, 69.19228649139404, 70.92913842201233, 72.67413258552551, 74.41572999954224, 76.06838130950928, 77.82183265686035, 79.56054782867432, 81.28961420059204, 83.02922630310059, 84.75253248214722, 86.49040031433105, 88.2276623249054, 89.95912265777588, 91.70356583595276, 93.4378879070282, 95.15903615951538, 97.18166089057922, 98.91185712814331, 100.56917333602905, 102.2317898273468, 103.89353680610657, 105.57570004463196, 107.27271723747253, 108.9558732509613, 110.64582061767578, 112.3238775730133, 113.98126888275146, 115.65167546272278, 117.33160209655762, 119.03543400764465, 120.71254539489746, 122.37771773338318, 124.05670380592346, 125.7276520729065, 127.39965963363647, 129.06739616394043, 130.74642038345337, 132.4336416721344, 134.0926821231842, 135.78054642677307, 137.4748318195343, 139.23740148544312, 140.91357421875, 142.62858128547668, 144.30848336219788, 145.98349118232727, 147.66071796417236, 149.35055661201477, 151.03119230270386, 152.7023570537567, 154.52199172973633, 156.26690220832825, 157.92917919158936, 159.60560941696167, 161.29328274726868, 163.0018970966339, 164.6952404975891, 166.36678981781006, 168.08150053024292, 169.77113270759583, 171.43335032463074, 173.11990356445312, 174.81874895095825, 176.47383618354797, 178.1630470752716, 179.8502323627472, 181.5396957397461, 183.21507906913757, 184.8897624015808, 186.57271814346313, 188.2778558731079, 189.9325249195099, 191.60942029953003, 193.29473423957825, 194.9678189754486, 196.72179102897644, 198.38767647743225, 200.0714020729065, 201.72545313835144, 203.36869382858276, 205.0510983467102, 206.72867560386658, 208.38977193832397, 210.06226205825806, 211.70332169532776, 213.36548805236816, 215.03765416145325, 216.71459412574768, 218.42888832092285, 220.11749482154846, 221.77490305900574, 223.47127294540405, 225.15315985679626, 226.81797003746033, 228.48389387130737, 230.17428016662598, 231.87325739860535, 233.5495765209198, 235.04751467704773, 236.57858538627625, 238.2351062297821, 239.92487716674805, 241.6432981491089, 243.34758496284485, 245.03866338729858, 246.72616291046143, 248.43989515304565, 250.18621563911438, 251.86908769607544, 253.5526716709137, 255.2712845802307, 256.9739987850189, 258.6588535308838, 260.35493063926697, 262.0635666847229, 263.77733755111694, 265.46770572662354, 267.1955819129944, 268.9037027359009, 270.5803883075714, 272.25802659988403, 273.9709415435791, 275.65835404396057, 277.3589015007019, 279.02662420272827, 280.77049684524536, 282.5309867858887, 284.26855969429016, 286.02974700927734, 287.7150013446808, 289.2020728588104, 290.71030139923096, 292.2150399684906, 293.7208454608917, 295.20678663253784, 296.68728828430176, 298.1684412956238, 299.68424248695374, 301.1601188182831, 302.64796805381775, 304.14684891700745, 305.65497040748596, 307.13934087753296, 308.6285095214844, 310.14730310440063, 311.66498351097107, 313.17161679267883, 314.67150259017944, 316.1937584877014, 317.7137544155121, 319.22825050354004, 320.7243916988373, 322.2647681236267, 323.79543948173523, 325.2978744506836, 326.79173374176025, 328.3289818763733, 329.8308753967285, 331.346244096756, 332.85582423210144, 334.3947341442108, 335.9302611351013, 337.450829744339, 338.9594609737396, 340.5026216506958, 342.02483892440796, 343.531902551651, 345.0365068912506, 346.5732135772705, 348.0857284069061, 349.59212946891785, 351.11086893081665, 352.64923214912415, 354.1458342075348, 355.65416955947876, 357.1583216190338, 358.6912684440613, 360.21225214004517, 361.6974904537201, 363.18259835243225, 364.6822876930237, 366.17219734191895, 367.65170526504517, 369.11968970298767, 370.6220655441284, 372.1193599700928, 373.6131024360657, 375.11601996421814, 376.62784457206726, 378.1233878135681, 379.6134510040283, 381.0945522785187, 382.6168689727783, 384.1216175556183, 385.6138744354248, 387.119943857193, 388.6281621456146, 390.1277480125427, 391.62303829193115, 393.1410286426544, 394.65568685531616, 396.14351892471313, 397.64163088798523, 399.13935565948486, 400.644731760025, 402.1489796638489, 403.65194272994995, 405.1502523422241, 406.6538817882538, 408.16095328330994, 409.65182065963745, 411.1662619113922, 412.6596784591675, 414.20112919807434, 415.7249343395233, 417.2309467792511, 418.7260720729828, 420.24291157722473, 421.7881963253021, 423.3002030849457, 424.8157181739807, 426.3316285610199, 427.85728120803833, 429.3586480617523, 430.89568758010864, 432.41393971443176, 433.9401926994324, 435.4365222454071, 436.9720163345337, 438.4889221191406, 439.9842870235443, 441.4932334423065, 443.0018346309662, 444.54841351509094, 446.02595686912537, 447.5434482097626, 449.068372964859, 450.55936002731323, 452.06085658073425, 453.59833908081055, 455.1235523223877, 456.63974237442017, 458.1616039276123, 459.68284606933594, 461.2043604850769, 462.7545247077942, 464.30332946777344, 465.78905844688416, 467.3214077949524, 468.8600368499756, 470.3856670856476, 471.8924481868744, 473.4273760318756, 474.93303775787354, 476.4358956813812, 477.98006439208984, 479.49437189102173, 480.9880678653717, 482.543598651886, 484.058221578598, 485.5514702796936, 488.1165750026703]
[6.65, 6.708333333333333, 7.775, 11.175, 11.0, 12.333333333333334, 10.891666666666667, 14.183333333333334, 14.908333333333333, 16.366666666666667, 16.366666666666667, 15.116666666666667, 15.116666666666667, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668]
