nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.227, Test loss: 2.082, Test accuracy: 25.67 

Round   0, Global train loss: 2.227, Global test loss: 2.083, Global test accuracy: 26.66 

Round   1, Train loss: 2.010, Test loss: 1.965, Test accuracy: 29.06 

Round   1, Global train loss: 2.010, Global test loss: 1.923, Global test accuracy: 31.22 

Round   2, Train loss: 1.890, Test loss: 1.878, Test accuracy: 31.46 

Round   2, Global train loss: 1.890, Global test loss: 1.787, Global test accuracy: 35.40 

Round   3, Train loss: 1.850, Test loss: 1.838, Test accuracy: 32.56 

Round   3, Global train loss: 1.850, Global test loss: 1.733, Global test accuracy: 37.27 

Round   4, Train loss: 1.748, Test loss: 1.815, Test accuracy: 33.73 

Round   4, Global train loss: 1.748, Global test loss: 1.697, Global test accuracy: 38.77 

Round   5, Train loss: 1.779, Test loss: 1.798, Test accuracy: 34.55 

Round   5, Global train loss: 1.779, Global test loss: 1.724, Global test accuracy: 38.71 

Round   6, Train loss: 1.702, Test loss: 1.778, Test accuracy: 35.07 

Round   6, Global train loss: 1.702, Global test loss: 1.681, Global test accuracy: 38.89 

Round   7, Train loss: 1.602, Test loss: 1.771, Test accuracy: 35.41 

Round   7, Global train loss: 1.602, Global test loss: 1.600, Global test accuracy: 41.74 

Round   8, Train loss: 1.543, Test loss: 1.760, Test accuracy: 35.98 

Round   8, Global train loss: 1.543, Global test loss: 1.573, Global test accuracy: 42.58 

Round   9, Train loss: 1.531, Test loss: 1.743, Test accuracy: 36.46 

Round   9, Global train loss: 1.531, Global test loss: 1.571, Global test accuracy: 42.81 

Round  10, Train loss: 1.470, Test loss: 1.764, Test accuracy: 36.67 

Round  10, Global train loss: 1.470, Global test loss: 1.560, Global test accuracy: 41.79 

Round  11, Train loss: 1.506, Test loss: 1.747, Test accuracy: 37.48 

Round  11, Global train loss: 1.506, Global test loss: 1.586, Global test accuracy: 41.95 

Round  12, Train loss: 1.530, Test loss: 1.738, Test accuracy: 38.26 

Round  12, Global train loss: 1.530, Global test loss: 1.615, Global test accuracy: 42.35 

Round  13, Train loss: 1.517, Test loss: 1.726, Test accuracy: 39.02 

Round  13, Global train loss: 1.517, Global test loss: 1.546, Global test accuracy: 43.80 

Round  14, Train loss: 1.470, Test loss: 1.700, Test accuracy: 39.90 

Round  14, Global train loss: 1.470, Global test loss: 1.536, Global test accuracy: 43.80 

Round  15, Train loss: 1.365, Test loss: 1.719, Test accuracy: 39.65 

Round  15, Global train loss: 1.365, Global test loss: 1.527, Global test accuracy: 44.70 

Round  16, Train loss: 1.311, Test loss: 1.761, Test accuracy: 38.96 

Round  16, Global train loss: 1.311, Global test loss: 1.522, Global test accuracy: 43.90 

Round  17, Train loss: 1.273, Test loss: 1.780, Test accuracy: 38.94 

Round  17, Global train loss: 1.273, Global test loss: 1.487, Global test accuracy: 45.93 

Round  18, Train loss: 1.172, Test loss: 1.785, Test accuracy: 39.31 

Round  18, Global train loss: 1.172, Global test loss: 1.470, Global test accuracy: 46.25 

Round  19, Train loss: 1.220, Test loss: 1.763, Test accuracy: 40.36 

Round  19, Global train loss: 1.220, Global test loss: 1.470, Global test accuracy: 45.79 

Round  20, Train loss: 1.127, Test loss: 1.780, Test accuracy: 40.17 

Round  20, Global train loss: 1.127, Global test loss: 1.469, Global test accuracy: 46.36 

Round  21, Train loss: 1.005, Test loss: 1.816, Test accuracy: 40.29 

Round  21, Global train loss: 1.005, Global test loss: 1.479, Global test accuracy: 46.02 

Round  22, Train loss: 1.281, Test loss: 1.817, Test accuracy: 40.87 

Round  22, Global train loss: 1.281, Global test loss: 1.527, Global test accuracy: 45.15 

Round  23, Train loss: 1.038, Test loss: 1.824, Test accuracy: 40.91 

Round  23, Global train loss: 1.038, Global test loss: 1.457, Global test accuracy: 47.82 

Round  24, Train loss: 0.956, Test loss: 1.847, Test accuracy: 41.02 

Round  24, Global train loss: 0.956, Global test loss: 1.451, Global test accuracy: 48.19 

Round  25, Train loss: 1.015, Test loss: 1.892, Test accuracy: 40.86 

Round  25, Global train loss: 1.015, Global test loss: 1.502, Global test accuracy: 45.63 

Round  26, Train loss: 1.002, Test loss: 1.910, Test accuracy: 40.81 

Round  26, Global train loss: 1.002, Global test loss: 1.474, Global test accuracy: 46.68 

Round  27, Train loss: 1.091, Test loss: 1.931, Test accuracy: 40.96 

Round  27, Global train loss: 1.091, Global test loss: 1.502, Global test accuracy: 45.18 

Round  28, Train loss: 0.978, Test loss: 1.966, Test accuracy: 40.86 

Round  28, Global train loss: 0.978, Global test loss: 1.479, Global test accuracy: 46.12 

Round  29, Train loss: 0.979, Test loss: 1.991, Test accuracy: 40.87 

Round  29, Global train loss: 0.979, Global test loss: 1.443, Global test accuracy: 47.96 

Round  30, Train loss: 0.841, Test loss: 2.028, Test accuracy: 40.79 

Round  30, Global train loss: 0.841, Global test loss: 1.468, Global test accuracy: 47.35 

Round  31, Train loss: 1.005, Test loss: 2.067, Test accuracy: 40.47 

Round  31, Global train loss: 1.005, Global test loss: 1.476, Global test accuracy: 46.05 

Round  32, Train loss: 0.758, Test loss: 2.079, Test accuracy: 40.69 

Round  32, Global train loss: 0.758, Global test loss: 1.471, Global test accuracy: 48.34 

Round  33, Train loss: 0.829, Test loss: 2.128, Test accuracy: 40.55 

Round  33, Global train loss: 0.829, Global test loss: 1.461, Global test accuracy: 47.41 

Round  34, Train loss: 0.832, Test loss: 2.152, Test accuracy: 40.72 

Round  34, Global train loss: 0.832, Global test loss: 1.475, Global test accuracy: 46.63 

Round  35, Train loss: 0.795, Test loss: 2.215, Test accuracy: 40.62 

Round  35, Global train loss: 0.795, Global test loss: 1.486, Global test accuracy: 47.74 

Round  36, Train loss: 0.802, Test loss: 2.234, Test accuracy: 41.09 

Round  36, Global train loss: 0.802, Global test loss: 1.464, Global test accuracy: 49.02 

Round  37, Train loss: 0.807, Test loss: 2.271, Test accuracy: 41.21 

Round  37, Global train loss: 0.807, Global test loss: 1.462, Global test accuracy: 48.94 

Round  38, Train loss: 0.684, Test loss: 2.277, Test accuracy: 41.47 

Round  38, Global train loss: 0.684, Global test loss: 1.503, Global test accuracy: 46.90 

Round  39, Train loss: 0.742, Test loss: 2.325, Test accuracy: 41.27 

Round  39, Global train loss: 0.742, Global test loss: 1.475, Global test accuracy: 48.13 

Round  40, Train loss: 0.646, Test loss: 2.333, Test accuracy: 41.50 

Round  40, Global train loss: 0.646, Global test loss: 1.492, Global test accuracy: 49.16 

Round  41, Train loss: 0.601, Test loss: 2.407, Test accuracy: 41.26 

Round  41, Global train loss: 0.601, Global test loss: 1.495, Global test accuracy: 46.38 

Round  42, Train loss: 0.619, Test loss: 2.406, Test accuracy: 41.67 

Round  42, Global train loss: 0.619, Global test loss: 1.469, Global test accuracy: 49.20 

Round  43, Train loss: 0.666, Test loss: 2.432, Test accuracy: 41.84 

Round  43, Global train loss: 0.666, Global test loss: 1.455, Global test accuracy: 47.89 

Round  44, Train loss: 0.677, Test loss: 2.466, Test accuracy: 41.66 

Round  44, Global train loss: 0.677, Global test loss: 1.469, Global test accuracy: 46.63 

Round  45, Train loss: 0.672, Test loss: 2.476, Test accuracy: 41.89 

Round  45, Global train loss: 0.672, Global test loss: 1.501, Global test accuracy: 47.07 

Round  46, Train loss: 0.435, Test loss: 2.522, Test accuracy: 41.87 

Round  46, Global train loss: 0.435, Global test loss: 1.520, Global test accuracy: 47.39 

Round  47, Train loss: 0.514, Test loss: 2.607, Test accuracy: 41.45 

Round  47, Global train loss: 0.514, Global test loss: 1.559, Global test accuracy: 48.11 

Round  48, Train loss: 0.426, Test loss: 2.635, Test accuracy: 41.43 

Round  48, Global train loss: 0.426, Global test loss: 1.503, Global test accuracy: 47.74 

Round  49, Train loss: 0.483, Test loss: 2.688, Test accuracy: 41.39 

Round  49, Global train loss: 0.483, Global test loss: 1.512, Global test accuracy: 47.23 

Round  50, Train loss: 0.447, Test loss: 2.692, Test accuracy: 41.41 

Round  50, Global train loss: 0.447, Global test loss: 1.519, Global test accuracy: 48.84 

Round  51, Train loss: 0.507, Test loss: 2.695, Test accuracy: 41.58 

Round  51, Global train loss: 0.507, Global test loss: 1.492, Global test accuracy: 46.96 

Round  52, Train loss: 0.420, Test loss: 2.732, Test accuracy: 41.61 

Round  52, Global train loss: 0.420, Global test loss: 1.554, Global test accuracy: 46.77 

Round  53, Train loss: 0.428, Test loss: 2.774, Test accuracy: 41.37 

Round  53, Global train loss: 0.428, Global test loss: 1.497, Global test accuracy: 47.44 

Round  54, Train loss: 0.396, Test loss: 2.838, Test accuracy: 41.30 

Round  54, Global train loss: 0.396, Global test loss: 1.570, Global test accuracy: 48.10 

Round  55, Train loss: 0.522, Test loss: 2.863, Test accuracy: 41.38 

Round  55, Global train loss: 0.522, Global test loss: 1.480, Global test accuracy: 48.26 

Round  56, Train loss: 0.372, Test loss: 2.919, Test accuracy: 41.45 

Round  56, Global train loss: 0.372, Global test loss: 1.575, Global test accuracy: 48.04 

Round  57, Train loss: 0.413, Test loss: 2.929, Test accuracy: 41.48 

Round  57, Global train loss: 0.413, Global test loss: 1.590, Global test accuracy: 48.26 

Round  58, Train loss: 0.405, Test loss: 2.951, Test accuracy: 41.11 

Round  58, Global train loss: 0.405, Global test loss: 1.561, Global test accuracy: 47.60 

Round  59, Train loss: 0.344, Test loss: 2.987, Test accuracy: 41.30 

Round  59, Global train loss: 0.344, Global test loss: 1.561, Global test accuracy: 48.65 

Round  60, Train loss: 0.363, Test loss: 3.057, Test accuracy: 41.31 

Round  60, Global train loss: 0.363, Global test loss: 1.570, Global test accuracy: 47.75 

Round  61, Train loss: 0.336, Test loss: 3.079, Test accuracy: 41.13 

Round  61, Global train loss: 0.336, Global test loss: 1.591, Global test accuracy: 47.31 

Round  62, Train loss: 0.361, Test loss: 3.091, Test accuracy: 41.43 

Round  62, Global train loss: 0.361, Global test loss: 1.537, Global test accuracy: 48.99 

Round  63, Train loss: 0.353, Test loss: 3.131, Test accuracy: 41.40 

Round  63, Global train loss: 0.353, Global test loss: 1.537, Global test accuracy: 47.99 

Round  64, Train loss: 0.299, Test loss: 3.212, Test accuracy: 41.12 

Round  64, Global train loss: 0.299, Global test loss: 1.581, Global test accuracy: 48.04 

Round  65, Train loss: 0.285, Test loss: 3.236, Test accuracy: 41.42 

Round  65, Global train loss: 0.285, Global test loss: 1.588, Global test accuracy: 47.69 

Round  66, Train loss: 0.272, Test loss: 3.245, Test accuracy: 41.57 

Round  66, Global train loss: 0.272, Global test loss: 1.556, Global test accuracy: 46.81 

Round  67, Train loss: 0.335, Test loss: 3.331, Test accuracy: 41.45 

Round  67, Global train loss: 0.335, Global test loss: 1.531, Global test accuracy: 46.65 

Round  68, Train loss: 0.287, Test loss: 3.362, Test accuracy: 41.61 

Round  68, Global train loss: 0.287, Global test loss: 1.627, Global test accuracy: 48.89 

Round  69, Train loss: 0.300, Test loss: 3.417, Test accuracy: 41.67 

Round  69, Global train loss: 0.300, Global test loss: 1.617, Global test accuracy: 46.33 

Round  70, Train loss: 0.272, Test loss: 3.400, Test accuracy: 41.26 

Round  70, Global train loss: 0.272, Global test loss: 1.561, Global test accuracy: 48.78 

Round  71, Train loss: 0.283, Test loss: 3.324, Test accuracy: 41.63 

Round  71, Global train loss: 0.283, Global test loss: 1.602, Global test accuracy: 46.30 

Round  72, Train loss: 0.233, Test loss: 3.396, Test accuracy: 41.80 

Round  72, Global train loss: 0.233, Global test loss: 1.565, Global test accuracy: 46.20 

Round  73, Train loss: 0.286, Test loss: 3.475, Test accuracy: 41.65 

Round  73, Global train loss: 0.286, Global test loss: 1.541, Global test accuracy: 47.37 

Round  74, Train loss: 0.249, Test loss: 3.501, Test accuracy: 41.64 

Round  74, Global train loss: 0.249, Global test loss: 1.562, Global test accuracy: 48.09 

Round  75, Train loss: 0.286, Test loss: 3.481, Test accuracy: 41.92 

Round  75, Global train loss: 0.286, Global test loss: 1.579, Global test accuracy: 47.41 

Round  76, Train loss: 0.258, Test loss: 3.535, Test accuracy: 41.83 

Round  76, Global train loss: 0.258, Global test loss: 1.537, Global test accuracy: 46.61 

Round  77, Train loss: 0.242, Test loss: 3.634, Test accuracy: 41.64 

Round  77, Global train loss: 0.242, Global test loss: 1.575, Global test accuracy: 48.66 

Round  78, Train loss: 0.234, Test loss: 3.617, Test accuracy: 41.68 

Round  78, Global train loss: 0.234, Global test loss: 1.533, Global test accuracy: 47.05 

Round  79, Train loss: 0.250, Test loss: 3.625, Test accuracy: 41.69 

Round  79, Global train loss: 0.250, Global test loss: 1.555, Global test accuracy: 49.80 

Round  80, Train loss: 0.177, Test loss: 3.699, Test accuracy: 41.71 

Round  80, Global train loss: 0.177, Global test loss: 1.625, Global test accuracy: 45.63 

Round  81, Train loss: 0.221, Test loss: 3.661, Test accuracy: 41.81 

Round  81, Global train loss: 0.221, Global test loss: 1.539, Global test accuracy: 47.81 

Round  82, Train loss: 0.173, Test loss: 3.772, Test accuracy: 41.48 

Round  82, Global train loss: 0.173, Global test loss: 1.641, Global test accuracy: 47.77 

Round  83, Train loss: 0.194, Test loss: 3.783, Test accuracy: 41.51 

Round  83, Global train loss: 0.194, Global test loss: 1.574, Global test accuracy: 46.82 

Round  84, Train loss: 0.201, Test loss: 3.836, Test accuracy: 41.47 

Round  84, Global train loss: 0.201, Global test loss: 1.653, Global test accuracy: 46.96 

Round  85, Train loss: 0.222, Test loss: 3.820, Test accuracy: 41.30 

Round  85, Global train loss: 0.222, Global test loss: 1.573, Global test accuracy: 47.81 

Round  86, Train loss: 0.230, Test loss: 3.843, Test accuracy: 41.42 

Round  86, Global train loss: 0.230, Global test loss: 1.587, Global test accuracy: 48.02 

Round  87, Train loss: 0.152, Test loss: 3.874, Test accuracy: 41.62 

Round  87, Global train loss: 0.152, Global test loss: 1.551, Global test accuracy: 45.95 

Round  88, Train loss: 0.182, Test loss: 3.812, Test accuracy: 41.63 

Round  88, Global train loss: 0.182, Global test loss: 1.622, Global test accuracy: 48.55 

Round  89, Train loss: 0.167, Test loss: 3.815, Test accuracy: 42.10 

Round  89, Global train loss: 0.167, Global test loss: 1.688, Global test accuracy: 48.44 

Round  90, Train loss: 0.148, Test loss: 3.841, Test accuracy: 42.05 

Round  90, Global train loss: 0.148, Global test loss: 1.760, Global test accuracy: 45.91 

Round  91, Train loss: 0.223, Test loss: 3.878, Test accuracy: 42.15 

Round  91, Global train loss: 0.223, Global test loss: 1.722, Global test accuracy: 47.41 

Round  92, Train loss: 0.220, Test loss: 3.872, Test accuracy: 41.96 

Round  92, Global train loss: 0.220, Global test loss: 1.593, Global test accuracy: 48.70 

Round  93, Train loss: 0.156, Test loss: 3.822, Test accuracy: 42.30 

Round  93, Global train loss: 0.156, Global test loss: 1.621, Global test accuracy: 47.30 

Round  94, Train loss: 0.153, Test loss: 3.937, Test accuracy: 42.11 

Round  94, Global train loss: 0.153, Global test loss: 1.637, Global test accuracy: 48.25 

Round  95, Train loss: 0.192, Test loss: 3.979, Test accuracy: 41.75 

Round  95, Global train loss: 0.192, Global test loss: 1.690, Global test accuracy: 49.84 

Round  96, Train loss: 0.173, Test loss: 4.046, Test accuracy: 41.63 

Round  96, Global train loss: 0.173, Global test loss: 1.566, Global test accuracy: 46.50 

Round  97, Train loss: 0.210, Test loss: 3.975, Test accuracy: 41.85 

Round  97, Global train loss: 0.210, Global test loss: 1.647, Global test accuracy: 47.86 

Round  98, Train loss: 0.129, Test loss: 3.970, Test accuracy: 42.06 

Round  98, Global train loss: 0.129, Global test loss: 1.603, Global test accuracy: 47.30 

Round  99, Train loss: 0.142, Test loss: 4.026, Test accuracy: 41.97 

Round  99, Global train loss: 0.142, Global test loss: 1.642, Global test accuracy: 46.92 

Final Round, Train loss: 0.154, Test loss: 4.276, Test accuracy: 41.48 

Final Round, Global train loss: 0.154, Global test loss: 1.642, Global test accuracy: 46.92 

Average accuracy final 10 rounds: 41.98333333333333 

Average global accuracy final 10 rounds: 47.59833333333333 

3442.9786751270294
[1.4078669548034668, 2.5854790210723877, 3.7604782581329346, 4.9379494190216064, 6.107939958572388, 7.272692918777466, 8.441473722457886, 9.610953569412231, 10.776527166366577, 11.945287942886353, 13.09828233718872, 14.240771293640137, 15.379443883895874, 16.51565194129944, 17.647135972976685, 18.78426456451416, 19.917388200759888, 21.04986071586609, 22.1911039352417, 23.330859422683716, 24.475985050201416, 25.62272810935974, 26.770379781723022, 27.91938614845276, 28.92496967315674, 29.91848063468933, 30.91738724708557, 31.911640644073486, 32.9051673412323, 33.904743671417236, 34.89936327934265, 35.89938306808472, 36.892348289489746, 37.886805295944214, 38.89080810546875, 39.88302779197693, 40.88598895072937, 41.891775608062744, 42.88705539703369, 43.89154505729675, 44.880207777023315, 45.86846470832825, 46.86656141281128, 47.86129117012024, 48.86022734642029, 49.8576717376709, 50.853628158569336, 51.85283637046814, 52.848334550857544, 53.84715819358826, 54.843159437179565, 55.83540678024292, 56.83438205718994, 57.82746148109436, 58.82485389709473, 59.82559657096863, 60.835541009902954, 61.834484815597534, 62.82829928398132, 63.82154130935669, 64.82110047340393, 65.81278324127197, 66.81351971626282, 67.80993962287903, 68.80279040336609, 69.80295658111572, 70.79630255699158, 71.79446291923523, 72.79431056976318, 73.78757691383362, 74.7860472202301, 75.77963304519653, 76.77489566802979, 77.77491116523743, 78.77174401283264, 79.77115821838379, 80.76643657684326, 81.75693535804749, 82.75815868377686, 83.7512423992157, 84.75070524215698, 85.74408078193665, 86.73797965049744, 87.73900723457336, 88.73198390007019, 89.73365044593811, 90.72552514076233, 91.71859574317932, 92.71919536590576, 93.71364903450012, 94.71454524993896, 95.71265602111816, 96.71064782142639, 97.70477986335754, 98.70005249977112, 99.70440006256104, 100.70353436470032, 101.69878911972046, 102.70150899887085, 103.69736647605896, 105.6984293460846]
[25.67, 29.063333333333333, 31.461666666666666, 32.565, 33.735, 34.54666666666667, 35.07333333333333, 35.406666666666666, 35.98166666666667, 36.46, 36.666666666666664, 37.483333333333334, 38.25833333333333, 39.01833333333333, 39.89666666666667, 39.645, 38.958333333333336, 38.94, 39.306666666666665, 40.36, 40.166666666666664, 40.29, 40.87, 40.90833333333333, 41.02333333333333, 40.861666666666665, 40.815, 40.95666666666666, 40.861666666666665, 40.86666666666667, 40.79, 40.47, 40.69166666666667, 40.545, 40.723333333333336, 40.625, 41.09, 41.211666666666666, 41.465, 41.275, 41.498333333333335, 41.25666666666667, 41.675, 41.83833333333333, 41.665, 41.891666666666666, 41.87166666666667, 41.445, 41.431666666666665, 41.388333333333335, 41.41166666666667, 41.583333333333336, 41.61, 41.373333333333335, 41.295, 41.38166666666667, 41.455, 41.47833333333333, 41.108333333333334, 41.303333333333335, 41.306666666666665, 41.126666666666665, 41.428333333333335, 41.39833333333333, 41.12166666666667, 41.42, 41.57333333333333, 41.44833333333333, 41.60666666666667, 41.675, 41.25666666666667, 41.62833333333333, 41.795, 41.64833333333333, 41.64, 41.91833333333334, 41.833333333333336, 41.638333333333335, 41.681666666666665, 41.69, 41.708333333333336, 41.81166666666667, 41.48166666666667, 41.51166666666666, 41.47, 41.3, 41.42, 41.615, 41.626666666666665, 42.10166666666667, 42.053333333333335, 42.14833333333333, 41.96333333333333, 42.3, 42.10666666666667, 41.748333333333335, 41.635, 41.855, 42.05833333333333, 41.965, 41.47666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.233, Test loss: 2.123, Test accuracy: 23.85 

Round   0, Global train loss: 2.233, Global test loss: 2.127, Global test accuracy: 24.82 

Round   1, Train loss: 2.035, Test loss: 1.954, Test accuracy: 27.77 

Round   1, Global train loss: 2.035, Global test loss: 1.913, Global test accuracy: 29.68 

Round   2, Train loss: 1.885, Test loss: 1.827, Test accuracy: 33.09 

Round   2, Global train loss: 1.885, Global test loss: 1.742, Global test accuracy: 37.07 

Round   3, Train loss: 1.781, Test loss: 1.775, Test accuracy: 34.77 

Round   3, Global train loss: 1.781, Global test loss: 1.647, Global test accuracy: 40.05 

Round   4, Train loss: 1.718, Test loss: 1.695, Test accuracy: 38.11 

Round   4, Global train loss: 1.718, Global test loss: 1.550, Global test accuracy: 44.14 

Round   5, Train loss: 1.650, Test loss: 1.644, Test accuracy: 39.99 

Round   5, Global train loss: 1.650, Global test loss: 1.507, Global test accuracy: 46.20 

Round   6, Train loss: 1.568, Test loss: 1.631, Test accuracy: 40.71 

Round   6, Global train loss: 1.568, Global test loss: 1.463, Global test accuracy: 47.44 

Round   7, Train loss: 1.539, Test loss: 1.616, Test accuracy: 41.19 

Round   7, Global train loss: 1.539, Global test loss: 1.427, Global test accuracy: 48.74 

Round   8, Train loss: 1.487, Test loss: 1.595, Test accuracy: 42.05 

Round   8, Global train loss: 1.487, Global test loss: 1.377, Global test accuracy: 50.98 

Round   9, Train loss: 1.445, Test loss: 1.558, Test accuracy: 43.78 

Round   9, Global train loss: 1.445, Global test loss: 1.350, Global test accuracy: 52.18 

Round  10, Train loss: 1.396, Test loss: 1.548, Test accuracy: 44.39 

Round  10, Global train loss: 1.396, Global test loss: 1.330, Global test accuracy: 53.21 

Round  11, Train loss: 1.377, Test loss: 1.509, Test accuracy: 46.00 

Round  11, Global train loss: 1.377, Global test loss: 1.298, Global test accuracy: 53.99 

Round  12, Train loss: 1.325, Test loss: 1.492, Test accuracy: 46.88 

Round  12, Global train loss: 1.325, Global test loss: 1.274, Global test accuracy: 54.84 

Round  13, Train loss: 1.323, Test loss: 1.461, Test accuracy: 48.13 

Round  13, Global train loss: 1.323, Global test loss: 1.237, Global test accuracy: 55.97 

Round  14, Train loss: 1.262, Test loss: 1.464, Test accuracy: 48.31 

Round  14, Global train loss: 1.262, Global test loss: 1.225, Global test accuracy: 56.49 

Round  15, Train loss: 1.247, Test loss: 1.465, Test accuracy: 48.41 

Round  15, Global train loss: 1.247, Global test loss: 1.197, Global test accuracy: 57.52 

Round  16, Train loss: 1.169, Test loss: 1.449, Test accuracy: 49.30 

Round  16, Global train loss: 1.169, Global test loss: 1.180, Global test accuracy: 58.21 

Round  17, Train loss: 1.139, Test loss: 1.449, Test accuracy: 49.49 

Round  17, Global train loss: 1.139, Global test loss: 1.190, Global test accuracy: 58.26 

Round  18, Train loss: 1.139, Test loss: 1.439, Test accuracy: 49.98 

Round  18, Global train loss: 1.139, Global test loss: 1.161, Global test accuracy: 59.03 

Round  19, Train loss: 1.161, Test loss: 1.408, Test accuracy: 51.34 

Round  19, Global train loss: 1.161, Global test loss: 1.163, Global test accuracy: 59.35 

Round  20, Train loss: 1.110, Test loss: 1.389, Test accuracy: 52.20 

Round  20, Global train loss: 1.110, Global test loss: 1.132, Global test accuracy: 60.53 

Round  21, Train loss: 1.080, Test loss: 1.391, Test accuracy: 52.53 

Round  21, Global train loss: 1.080, Global test loss: 1.122, Global test accuracy: 60.54 

Round  22, Train loss: 1.041, Test loss: 1.384, Test accuracy: 53.08 

Round  22, Global train loss: 1.041, Global test loss: 1.134, Global test accuracy: 60.74 

Round  23, Train loss: 1.017, Test loss: 1.398, Test accuracy: 52.92 

Round  23, Global train loss: 1.017, Global test loss: 1.133, Global test accuracy: 61.08 

Round  24, Train loss: 1.031, Test loss: 1.389, Test accuracy: 53.59 

Round  24, Global train loss: 1.031, Global test loss: 1.105, Global test accuracy: 61.74 

Round  25, Train loss: 0.982, Test loss: 1.402, Test accuracy: 53.50 

Round  25, Global train loss: 0.982, Global test loss: 1.106, Global test accuracy: 61.85 

Round  26, Train loss: 0.960, Test loss: 1.403, Test accuracy: 53.54 

Round  26, Global train loss: 0.960, Global test loss: 1.125, Global test accuracy: 60.97 

Round  27, Train loss: 0.947, Test loss: 1.406, Test accuracy: 53.71 

Round  27, Global train loss: 0.947, Global test loss: 1.106, Global test accuracy: 61.40 

Round  28, Train loss: 0.934, Test loss: 1.391, Test accuracy: 54.35 

Round  28, Global train loss: 0.934, Global test loss: 1.095, Global test accuracy: 62.51 

Round  29, Train loss: 0.977, Test loss: 1.380, Test accuracy: 54.83 

Round  29, Global train loss: 0.977, Global test loss: 1.085, Global test accuracy: 62.37 

Round  30, Train loss: 0.915, Test loss: 1.381, Test accuracy: 55.05 

Round  30, Global train loss: 0.915, Global test loss: 1.095, Global test accuracy: 62.82 

Round  31, Train loss: 0.877, Test loss: 1.370, Test accuracy: 55.39 

Round  31, Global train loss: 0.877, Global test loss: 1.089, Global test accuracy: 62.59 

Round  32, Train loss: 0.929, Test loss: 1.359, Test accuracy: 55.98 

Round  32, Global train loss: 0.929, Global test loss: 1.061, Global test accuracy: 63.50 

Round  33, Train loss: 0.845, Test loss: 1.357, Test accuracy: 56.23 

Round  33, Global train loss: 0.845, Global test loss: 1.075, Global test accuracy: 63.72 

Round  34, Train loss: 0.879, Test loss: 1.369, Test accuracy: 56.24 

Round  34, Global train loss: 0.879, Global test loss: 1.059, Global test accuracy: 63.85 

Round  35, Train loss: 0.874, Test loss: 1.365, Test accuracy: 56.45 

Round  35, Global train loss: 0.874, Global test loss: 1.047, Global test accuracy: 64.37 

Round  36, Train loss: 0.848, Test loss: 1.367, Test accuracy: 56.67 

Round  36, Global train loss: 0.848, Global test loss: 1.068, Global test accuracy: 64.02 

Round  37, Train loss: 0.796, Test loss: 1.370, Test accuracy: 56.91 

Round  37, Global train loss: 0.796, Global test loss: 1.068, Global test accuracy: 64.40 

Round  38, Train loss: 0.803, Test loss: 1.372, Test accuracy: 57.07 

Round  38, Global train loss: 0.803, Global test loss: 1.071, Global test accuracy: 64.49 

Round  39, Train loss: 0.823, Test loss: 1.369, Test accuracy: 57.30 

Round  39, Global train loss: 0.823, Global test loss: 1.066, Global test accuracy: 64.48 

Round  40, Train loss: 0.764, Test loss: 1.377, Test accuracy: 57.45 

Round  40, Global train loss: 0.764, Global test loss: 1.065, Global test accuracy: 65.10 

Round  41, Train loss: 0.743, Test loss: 1.389, Test accuracy: 57.39 

Round  41, Global train loss: 0.743, Global test loss: 1.080, Global test accuracy: 64.93 

Round  42, Train loss: 0.770, Test loss: 1.401, Test accuracy: 57.53 

Round  42, Global train loss: 0.770, Global test loss: 1.080, Global test accuracy: 64.47 

Round  43, Train loss: 0.761, Test loss: 1.378, Test accuracy: 57.80 

Round  43, Global train loss: 0.761, Global test loss: 1.040, Global test accuracy: 65.33 

Round  44, Train loss: 0.761, Test loss: 1.368, Test accuracy: 58.08 

Round  44, Global train loss: 0.761, Global test loss: 1.057, Global test accuracy: 65.20 

Round  45, Train loss: 0.762, Test loss: 1.386, Test accuracy: 58.09 

Round  45, Global train loss: 0.762, Global test loss: 1.049, Global test accuracy: 65.82 

Round  46, Train loss: 0.732, Test loss: 1.388, Test accuracy: 58.22 

Round  46, Global train loss: 0.732, Global test loss: 1.046, Global test accuracy: 65.38 

Round  47, Train loss: 0.704, Test loss: 1.381, Test accuracy: 58.67 

Round  47, Global train loss: 0.704, Global test loss: 1.058, Global test accuracy: 66.13 

Round  48, Train loss: 0.715, Test loss: 1.378, Test accuracy: 58.59 

Round  48, Global train loss: 0.715, Global test loss: 1.056, Global test accuracy: 65.54 

Round  49, Train loss: 0.695, Test loss: 1.377, Test accuracy: 58.70 

Round  49, Global train loss: 0.695, Global test loss: 1.053, Global test accuracy: 66.08 

Round  50, Train loss: 0.691, Test loss: 1.382, Test accuracy: 58.72 

Round  50, Global train loss: 0.691, Global test loss: 1.070, Global test accuracy: 65.64 

Round  51, Train loss: 0.710, Test loss: 1.404, Test accuracy: 58.74 

Round  51, Global train loss: 0.710, Global test loss: 1.049, Global test accuracy: 66.35 

Round  52, Train loss: 0.675, Test loss: 1.413, Test accuracy: 58.66 

Round  52, Global train loss: 0.675, Global test loss: 1.058, Global test accuracy: 66.07 

Round  53, Train loss: 0.668, Test loss: 1.426, Test accuracy: 58.82 

Round  53, Global train loss: 0.668, Global test loss: 1.073, Global test accuracy: 66.52 

Round  54, Train loss: 0.617, Test loss: 1.412, Test accuracy: 59.30 

Round  54, Global train loss: 0.617, Global test loss: 1.065, Global test accuracy: 66.44 

Round  55, Train loss: 0.636, Test loss: 1.429, Test accuracy: 59.27 

Round  55, Global train loss: 0.636, Global test loss: 1.063, Global test accuracy: 66.69 

Round  56, Train loss: 0.619, Test loss: 1.437, Test accuracy: 59.20 

Round  56, Global train loss: 0.619, Global test loss: 1.077, Global test accuracy: 66.17 

Round  57, Train loss: 0.621, Test loss: 1.452, Test accuracy: 58.92 

Round  57, Global train loss: 0.621, Global test loss: 1.063, Global test accuracy: 66.22 

Round  58, Train loss: 0.628, Test loss: 1.461, Test accuracy: 58.73 

Round  58, Global train loss: 0.628, Global test loss: 1.061, Global test accuracy: 66.44 

Round  59, Train loss: 0.598, Test loss: 1.471, Test accuracy: 59.11 

Round  59, Global train loss: 0.598, Global test loss: 1.095, Global test accuracy: 66.40 

Round  60, Train loss: 0.626, Test loss: 1.470, Test accuracy: 59.16 

Round  60, Global train loss: 0.626, Global test loss: 1.078, Global test accuracy: 66.55 

Round  61, Train loss: 0.585, Test loss: 1.473, Test accuracy: 59.36 

Round  61, Global train loss: 0.585, Global test loss: 1.094, Global test accuracy: 66.53 

Round  62, Train loss: 0.635, Test loss: 1.454, Test accuracy: 59.86 

Round  62, Global train loss: 0.635, Global test loss: 1.082, Global test accuracy: 66.52 

Round  63, Train loss: 0.623, Test loss: 1.466, Test accuracy: 59.86 

Round  63, Global train loss: 0.623, Global test loss: 1.079, Global test accuracy: 66.71 

Round  64, Train loss: 0.551, Test loss: 1.464, Test accuracy: 60.03 

Round  64, Global train loss: 0.551, Global test loss: 1.091, Global test accuracy: 66.64 

Round  65, Train loss: 0.523, Test loss: 1.472, Test accuracy: 59.98 

Round  65, Global train loss: 0.523, Global test loss: 1.130, Global test accuracy: 66.34 

Round  66, Train loss: 0.579, Test loss: 1.465, Test accuracy: 60.28 

Round  66, Global train loss: 0.579, Global test loss: 1.104, Global test accuracy: 66.70 

Round  67, Train loss: 0.590, Test loss: 1.474, Test accuracy: 60.08 

Round  67, Global train loss: 0.590, Global test loss: 1.093, Global test accuracy: 66.46 

Round  68, Train loss: 0.556, Test loss: 1.477, Test accuracy: 59.95 

Round  68, Global train loss: 0.556, Global test loss: 1.108, Global test accuracy: 66.30 

Round  69, Train loss: 0.561, Test loss: 1.484, Test accuracy: 60.06 

Round  69, Global train loss: 0.561, Global test loss: 1.099, Global test accuracy: 67.10 

Round  70, Train loss: 0.533, Test loss: 1.474, Test accuracy: 60.06 

Round  70, Global train loss: 0.533, Global test loss: 1.111, Global test accuracy: 66.70 

Round  71, Train loss: 0.533, Test loss: 1.480, Test accuracy: 60.07 

Round  71, Global train loss: 0.533, Global test loss: 1.109, Global test accuracy: 66.53 

Round  72, Train loss: 0.542, Test loss: 1.492, Test accuracy: 59.84 

Round  72, Global train loss: 0.542, Global test loss: 1.119, Global test accuracy: 66.31 

Round  73, Train loss: 0.510, Test loss: 1.479, Test accuracy: 60.21 

Round  73, Global train loss: 0.510, Global test loss: 1.129, Global test accuracy: 67.00 

Round  74, Train loss: 0.545, Test loss: 1.470, Test accuracy: 60.37 

Round  74, Global train loss: 0.545, Global test loss: 1.094, Global test accuracy: 66.69 

Round  75, Train loss: 0.547, Test loss: 1.470, Test accuracy: 60.48 

Round  75, Global train loss: 0.547, Global test loss: 1.079, Global test accuracy: 67.39 

Round  76, Train loss: 0.535, Test loss: 1.460, Test accuracy: 60.78 

Round  76, Global train loss: 0.535, Global test loss: 1.100, Global test accuracy: 67.27 

Round  77, Train loss: 0.558, Test loss: 1.465, Test accuracy: 60.64 

Round  77, Global train loss: 0.558, Global test loss: 1.110, Global test accuracy: 66.87 

Round  78, Train loss: 0.522, Test loss: 1.470, Test accuracy: 60.79 

Round  78, Global train loss: 0.522, Global test loss: 1.087, Global test accuracy: 67.18 

Round  79, Train loss: 0.526, Test loss: 1.471, Test accuracy: 60.91 

Round  79, Global train loss: 0.526, Global test loss: 1.089, Global test accuracy: 67.69 

Round  80, Train loss: 0.480, Test loss: 1.479, Test accuracy: 60.77 

Round  80, Global train loss: 0.480, Global test loss: 1.134, Global test accuracy: 67.05 

Round  81, Train loss: 0.521, Test loss: 1.481, Test accuracy: 60.83 

Round  81, Global train loss: 0.521, Global test loss: 1.103, Global test accuracy: 67.08 

Round  82, Train loss: 0.531, Test loss: 1.512, Test accuracy: 60.38 

Round  82, Global train loss: 0.531, Global test loss: 1.094, Global test accuracy: 67.36 

Round  83, Train loss: 0.514, Test loss: 1.513, Test accuracy: 60.53 

Round  83, Global train loss: 0.514, Global test loss: 1.123, Global test accuracy: 66.73 

Round  84, Train loss: 0.477, Test loss: 1.503, Test accuracy: 60.62 

Round  84, Global train loss: 0.477, Global test loss: 1.117, Global test accuracy: 67.23 

Round  85, Train loss: 0.471, Test loss: 1.507, Test accuracy: 60.78 

Round  85, Global train loss: 0.471, Global test loss: 1.134, Global test accuracy: 67.14 

Round  86, Train loss: 0.480, Test loss: 1.514, Test accuracy: 60.93 

Round  86, Global train loss: 0.480, Global test loss: 1.121, Global test accuracy: 67.25 

Round  87, Train loss: 0.446, Test loss: 1.511, Test accuracy: 60.93 

Round  87, Global train loss: 0.446, Global test loss: 1.117, Global test accuracy: 67.46 

Round  88, Train loss: 0.503, Test loss: 1.515, Test accuracy: 61.16 

Round  88, Global train loss: 0.503, Global test loss: 1.126, Global test accuracy: 67.40 

Round  89, Train loss: 0.473, Test loss: 1.524, Test accuracy: 61.05 

Round  89, Global train loss: 0.473, Global test loss: 1.125, Global test accuracy: 67.17 

Round  90, Train loss: 0.456, Test loss: 1.519, Test accuracy: 61.10 

Round  90, Global train loss: 0.456, Global test loss: 1.120, Global test accuracy: 67.67 

Round  91, Train loss: 0.411, Test loss: 1.511, Test accuracy: 61.34 

Round  91, Global train loss: 0.411, Global test loss: 1.148, Global test accuracy: 67.26 

Round  92, Train loss: 0.481, Test loss: 1.499, Test accuracy: 61.36 

Round  92, Global train loss: 0.481, Global test loss: 1.146, Global test accuracy: 67.34 

Round  93, Train loss: 0.448, Test loss: 1.531, Test accuracy: 61.17 

Round  93, Global train loss: 0.448, Global test loss: 1.144, Global test accuracy: 67.13 

Round  94, Train loss: 0.440, Test loss: 1.540, Test accuracy: 61.12 

Round  94, Global train loss: 0.440, Global test loss: 1.169, Global test accuracy: 67.16 

Round  95, Train loss: 0.454, Test loss: 1.555, Test accuracy: 61.12 

Round  95, Global train loss: 0.454, Global test loss: 1.144, Global test accuracy: 68.45 

Round  96, Train loss: 0.446, Test loss: 1.564, Test accuracy: 61.01 

Round  96, Global train loss: 0.446, Global test loss: 1.167, Global test accuracy: 67.31 

Round  97, Train loss: 0.448, Test loss: 1.548, Test accuracy: 61.37 

Round  97, Global train loss: 0.448, Global test loss: 1.165, Global test accuracy: 67.46 

Round  98, Train loss: 0.417, Test loss: 1.528, Test accuracy: 61.77 

Round  98, Global train loss: 0.417, Global test loss: 1.169, Global test accuracy: 67.87 

Round  99, Train loss: 0.461, Test loss: 1.525, Test accuracy: 61.95 

Round  99, Global train loss: 0.461, Global test loss: 1.139, Global test accuracy: 67.65 

Final Round, Train loss: 0.351, Test loss: 1.711, Test accuracy: 61.26 

Final Round, Global train loss: 0.351, Global test loss: 1.139, Global test accuracy: 67.65 

Average accuracy final 10 rounds: 61.33083333333333 

Average global accuracy final 10 rounds: 67.53016666666666 

3453.364027261734
[1.3996696472167969, 2.4088425636291504, 3.413928270339966, 4.420943260192871, 5.42512845993042, 6.4323296546936035, 7.441100120544434, 8.447503089904785, 9.44956636428833, 10.454710006713867, 11.459614276885986, 12.467694997787476, 13.474336385726929, 14.478411436080933, 15.481841564178467, 16.488852977752686, 17.491909742355347, 18.501903295516968, 19.507856130599976, 20.519784450531006, 21.523810386657715, 22.531126260757446, 23.538840770721436, 24.544251441955566, 25.55176544189453, 26.560953855514526, 27.565232038497925, 28.560646295547485, 29.565725088119507, 30.596500873565674, 31.601721048355103, 32.60662889480591, 33.61142539978027, 34.61560082435608, 35.60765886306763, 36.61202692985535, 37.63307309150696, 38.636852979660034, 39.64160490036011, 40.64617085456848, 41.65173959732056, 42.652570962905884, 43.65549850463867, 44.66055393218994, 45.66671323776245, 46.672954082489014, 47.66888642311096, 48.675517082214355, 49.682923555374146, 50.68649625778198, 51.692721366882324, 52.686723947525024, 53.69279098510742, 54.69905948638916, 55.705379486083984, 56.71157479286194, 57.70839786529541, 58.714224100112915, 59.72197151184082, 60.72349286079407, 61.73013687133789, 62.73429250717163, 63.73783302307129, 64.74312925338745, 65.74878978729248, 66.75491428375244, 67.76111507415771, 68.7703025341034, 69.77781677246094, 70.7835042476654, 71.79204225540161, 72.79929876327515, 73.80336928367615, 74.81154584884644, 75.8144063949585, 76.8126871585846, 77.81282806396484, 78.81186628341675, 79.80808687210083, 80.81193542480469, 81.81808304786682, 82.8246955871582, 83.82934641838074, 84.83737444877625, 85.83266305923462, 86.83138799667358, 87.83801794052124, 88.84308314323425, 89.84879207611084, 90.85501551628113, 91.8570384979248, 92.85151743888855, 93.84900259971619, 94.85332441329956, 95.85953617095947, 96.86594843864441, 97.86899781227112, 98.8743507862091, 99.87786269187927, 100.88449215888977, 102.89004826545715]
[23.846666666666668, 27.766666666666666, 33.09166666666667, 34.766666666666666, 38.10666666666667, 39.98833333333334, 40.70666666666666, 41.19, 42.055, 43.781666666666666, 44.38666666666666, 45.99666666666667, 46.88, 48.12833333333333, 48.31166666666667, 48.415, 49.303333333333335, 49.48833333333334, 49.975, 51.34, 52.19833333333333, 52.528333333333336, 53.07666666666667, 52.92, 53.585, 53.5, 53.538333333333334, 53.70666666666666, 54.35, 54.83, 55.055, 55.391666666666666, 55.97666666666667, 56.233333333333334, 56.24166666666667, 56.445, 56.67166666666667, 56.905, 57.071666666666665, 57.295, 57.445, 57.388333333333335, 57.531666666666666, 57.805, 58.07666666666667, 58.085, 58.215, 58.66833333333334, 58.595, 58.70333333333333, 58.71666666666667, 58.745, 58.66166666666667, 58.821666666666665, 59.305, 59.265, 59.2, 58.925, 58.72666666666667, 59.10666666666667, 59.165, 59.361666666666665, 59.861666666666665, 59.86, 60.03333333333333, 59.98166666666667, 60.28, 60.08166666666666, 59.94833333333333, 60.056666666666665, 60.06166666666667, 60.07, 59.84, 60.211666666666666, 60.37166666666667, 60.48, 60.785, 60.64, 60.791666666666664, 60.913333333333334, 60.766666666666666, 60.825, 60.38333333333333, 60.53, 60.623333333333335, 60.78, 60.931666666666665, 60.93, 61.16166666666667, 61.05166666666667, 61.10333333333333, 61.343333333333334, 61.36, 61.166666666666664, 61.12, 61.12166666666667, 61.00833333333333, 61.36833333333333, 61.77166666666667, 61.945, 61.25833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.263, Test loss: 2.156, Test accuracy: 20.19 

Round   1, Train loss: 2.102, Test loss: 1.985, Test accuracy: 27.95 

Round   2, Train loss: 1.954, Test loss: 1.884, Test accuracy: 31.16 

Round   3, Train loss: 1.898, Test loss: 1.816, Test accuracy: 33.28 

Round   4, Train loss: 1.811, Test loss: 1.755, Test accuracy: 35.46 

Round   5, Train loss: 1.774, Test loss: 1.698, Test accuracy: 37.59 

Round   6, Train loss: 1.689, Test loss: 1.641, Test accuracy: 39.82 

Round   7, Train loss: 1.645, Test loss: 1.615, Test accuracy: 41.14 

Round   8, Train loss: 1.635, Test loss: 1.613, Test accuracy: 41.35 

Round   9, Train loss: 1.584, Test loss: 1.584, Test accuracy: 41.95 

Round  10, Train loss: 1.569, Test loss: 1.568, Test accuracy: 43.06 

Round  11, Train loss: 1.556, Test loss: 1.522, Test accuracy: 44.66 

Round  12, Train loss: 1.491, Test loss: 1.495, Test accuracy: 45.72 

Round  13, Train loss: 1.506, Test loss: 1.474, Test accuracy: 46.44 

Round  14, Train loss: 1.458, Test loss: 1.464, Test accuracy: 46.60 

Round  15, Train loss: 1.469, Test loss: 1.455, Test accuracy: 46.95 

Round  16, Train loss: 1.410, Test loss: 1.433, Test accuracy: 47.72 

Round  17, Train loss: 1.418, Test loss: 1.419, Test accuracy: 48.27 

Round  18, Train loss: 1.400, Test loss: 1.422, Test accuracy: 48.62 

Round  19, Train loss: 1.324, Test loss: 1.398, Test accuracy: 49.27 

Round  20, Train loss: 1.363, Test loss: 1.361, Test accuracy: 50.62 

Round  21, Train loss: 1.344, Test loss: 1.356, Test accuracy: 51.33 

Round  22, Train loss: 1.292, Test loss: 1.344, Test accuracy: 51.98 

Round  23, Train loss: 1.292, Test loss: 1.329, Test accuracy: 52.06 

Round  24, Train loss: 1.282, Test loss: 1.325, Test accuracy: 52.37 

Round  25, Train loss: 1.263, Test loss: 1.310, Test accuracy: 53.02 

Round  26, Train loss: 1.223, Test loss: 1.302, Test accuracy: 53.49 

Round  27, Train loss: 1.238, Test loss: 1.279, Test accuracy: 54.48 

Round  28, Train loss: 1.209, Test loss: 1.284, Test accuracy: 54.20 

Round  29, Train loss: 1.180, Test loss: 1.304, Test accuracy: 53.24 

Round  30, Train loss: 1.205, Test loss: 1.308, Test accuracy: 53.30 

Round  31, Train loss: 1.170, Test loss: 1.301, Test accuracy: 54.03 

Round  32, Train loss: 1.137, Test loss: 1.283, Test accuracy: 54.37 

Round  33, Train loss: 1.120, Test loss: 1.281, Test accuracy: 54.45 

Round  34, Train loss: 1.129, Test loss: 1.275, Test accuracy: 54.88 

Round  35, Train loss: 1.123, Test loss: 1.262, Test accuracy: 55.45 

Round  36, Train loss: 1.136, Test loss: 1.253, Test accuracy: 55.96 

Round  37, Train loss: 1.044, Test loss: 1.237, Test accuracy: 56.63 

Round  38, Train loss: 1.102, Test loss: 1.223, Test accuracy: 57.18 

Round  39, Train loss: 1.080, Test loss: 1.211, Test accuracy: 57.78 

Round  40, Train loss: 0.995, Test loss: 1.227, Test accuracy: 57.20 

Round  41, Train loss: 1.023, Test loss: 1.212, Test accuracy: 57.76 

Round  42, Train loss: 1.011, Test loss: 1.222, Test accuracy: 57.71 

Round  43, Train loss: 0.991, Test loss: 1.203, Test accuracy: 58.38 

Round  44, Train loss: 0.959, Test loss: 1.205, Test accuracy: 58.57 

Round  45, Train loss: 1.018, Test loss: 1.203, Test accuracy: 58.50 

Round  46, Train loss: 1.010, Test loss: 1.211, Test accuracy: 57.98 

Round  47, Train loss: 0.979, Test loss: 1.177, Test accuracy: 59.33 

Round  48, Train loss: 0.965, Test loss: 1.173, Test accuracy: 59.24 

Round  49, Train loss: 0.954, Test loss: 1.176, Test accuracy: 59.50 

Round  50, Train loss: 0.904, Test loss: 1.189, Test accuracy: 59.22 

Round  51, Train loss: 0.930, Test loss: 1.187, Test accuracy: 59.74 

Round  52, Train loss: 0.907, Test loss: 1.185, Test accuracy: 59.88 

Round  53, Train loss: 0.847, Test loss: 1.172, Test accuracy: 60.10 

Round  54, Train loss: 0.841, Test loss: 1.183, Test accuracy: 59.93 

Round  55, Train loss: 0.884, Test loss: 1.175, Test accuracy: 60.42 

Round  56, Train loss: 0.858, Test loss: 1.167, Test accuracy: 60.92 

Round  57, Train loss: 0.866, Test loss: 1.183, Test accuracy: 60.60 

Round  58, Train loss: 0.877, Test loss: 1.181, Test accuracy: 60.46 

Round  59, Train loss: 0.854, Test loss: 1.190, Test accuracy: 60.45 

Round  60, Train loss: 0.838, Test loss: 1.208, Test accuracy: 59.98 

Round  61, Train loss: 0.873, Test loss: 1.186, Test accuracy: 60.63 

Round  62, Train loss: 0.814, Test loss: 1.199, Test accuracy: 60.54 

Round  63, Train loss: 0.841, Test loss: 1.188, Test accuracy: 60.57 

Round  64, Train loss: 0.806, Test loss: 1.154, Test accuracy: 60.92 

Round  65, Train loss: 0.811, Test loss: 1.166, Test accuracy: 61.19 

Round  66, Train loss: 0.802, Test loss: 1.159, Test accuracy: 61.38 

Round  67, Train loss: 0.764, Test loss: 1.165, Test accuracy: 61.81 

Round  68, Train loss: 0.815, Test loss: 1.180, Test accuracy: 61.23 

Round  69, Train loss: 0.749, Test loss: 1.186, Test accuracy: 60.95 

Round  70, Train loss: 0.766, Test loss: 1.173, Test accuracy: 61.94 

Round  71, Train loss: 0.714, Test loss: 1.199, Test accuracy: 61.24 

Round  72, Train loss: 0.700, Test loss: 1.188, Test accuracy: 61.42 

Round  73, Train loss: 0.744, Test loss: 1.201, Test accuracy: 61.45 

Round  74, Train loss: 0.722, Test loss: 1.193, Test accuracy: 61.74 

Round  75, Train loss: 0.692, Test loss: 1.202, Test accuracy: 62.23 

Round  76, Train loss: 0.678, Test loss: 1.205, Test accuracy: 61.81 

Round  77, Train loss: 0.711, Test loss: 1.208, Test accuracy: 61.24 

Round  78, Train loss: 0.694, Test loss: 1.208, Test accuracy: 61.27 

Round  79, Train loss: 0.706, Test loss: 1.188, Test accuracy: 62.08 

Round  80, Train loss: 0.651, Test loss: 1.201, Test accuracy: 62.11 

Round  81, Train loss: 0.681, Test loss: 1.231, Test accuracy: 61.93 

Round  82, Train loss: 0.691, Test loss: 1.227, Test accuracy: 61.52 

Round  83, Train loss: 0.667, Test loss: 1.201, Test accuracy: 62.03 

Round  84, Train loss: 0.643, Test loss: 1.228, Test accuracy: 61.94 

Round  85, Train loss: 0.635, Test loss: 1.225, Test accuracy: 62.07 

Round  86, Train loss: 0.641, Test loss: 1.223, Test accuracy: 62.22 

Round  87, Train loss: 0.643, Test loss: 1.208, Test accuracy: 62.66 

Round  88, Train loss: 0.664, Test loss: 1.230, Test accuracy: 62.32 

Round  89, Train loss: 0.623, Test loss: 1.232, Test accuracy: 62.21 

Round  90, Train loss: 0.645, Test loss: 1.224, Test accuracy: 62.49 

Round  91, Train loss: 0.633, Test loss: 1.247, Test accuracy: 62.48 

Round  92, Train loss: 0.609, Test loss: 1.255, Test accuracy: 62.25 

Round  93, Train loss: 0.579, Test loss: 1.222, Test accuracy: 62.43 

Round  94, Train loss: 0.563, Test loss: 1.243, Test accuracy: 62.05 

Round  95, Train loss: 0.606, Test loss: 1.252, Test accuracy: 62.58 

Round  96, Train loss: 0.561, Test loss: 1.254, Test accuracy: 62.48 

Round  97, Train loss: 0.539, Test loss: 1.277, Test accuracy: 62.62 

Round  98, Train loss: 0.676, Test loss: 1.240, Test accuracy: 62.27 

Round  99, Train loss: 0.633, Test loss: 1.256, Test accuracy: 62.45 

Final Round, Train loss: 0.509, Test loss: 1.266, Test accuracy: 62.48 

Average accuracy final 10 rounds: 62.409666666666666 

1978.087975025177
[1.2612972259521484, 2.1998026371002197, 3.137249231338501, 4.0763304233551025, 5.025578260421753, 5.969166278839111, 6.919104814529419, 7.8568267822265625, 8.79644775390625, 9.736024856567383, 10.6759934425354, 11.612808465957642, 12.55042839050293, 13.488256931304932, 14.4254469871521, 15.364811182022095, 16.304996728897095, 17.241984367370605, 18.181551218032837, 19.122812271118164, 20.060551404953003, 20.999353885650635, 21.933598041534424, 22.870227336883545, 23.801589488983154, 24.74324083328247, 25.678647994995117, 26.616060495376587, 27.552356004714966, 28.491068124771118, 29.432178020477295, 30.375479698181152, 31.322359085083008, 32.25810742378235, 33.19541120529175, 34.13572144508362, 35.07382392883301, 36.01326894760132, 36.95319175720215, 37.893123626708984, 38.83312153816223, 39.76996731758118, 40.71206259727478, 41.64363765716553, 42.57828164100647, 43.51313638687134, 44.447752237319946, 45.38651895523071, 46.32499575614929, 47.25737714767456, 48.191009283065796, 49.12472891807556, 50.055196046829224, 50.98782777786255, 51.91867017745972, 52.847418546676636, 53.77908492088318, 54.710708141326904, 55.64304971694946, 56.57876634597778, 57.51132416725159, 58.4463005065918, 59.37574005126953, 60.30806350708008, 61.24308133125305, 62.176427125930786, 63.11015510559082, 64.04629802703857, 64.98154044151306, 65.91451001167297, 66.8477110862732, 67.78109669685364, 68.71624398231506, 69.65143179893494, 70.5844087600708, 71.52027988433838, 72.45388746261597, 73.3877866268158, 74.32528066635132, 75.26555156707764, 76.20383667945862, 77.14482688903809, 78.08161330223083, 79.01935958862305, 79.95858097076416, 80.89793539047241, 81.83630776405334, 82.77362990379333, 83.71199035644531, 84.6503574848175, 85.582444190979, 86.51657199859619, 87.44637894630432, 88.37685942649841, 89.30608463287354, 90.23789930343628, 91.16828656196594, 92.1044991016388, 93.04406595230103, 93.98601365089417, 95.74356818199158]
[20.191666666666666, 27.955, 31.156666666666666, 33.278333333333336, 35.46333333333333, 37.59166666666667, 39.82, 41.14, 41.35166666666667, 41.946666666666665, 43.06, 44.665, 45.718333333333334, 46.44, 46.60166666666667, 46.94833333333333, 47.723333333333336, 48.266666666666666, 48.61833333333333, 49.27333333333333, 50.615, 51.33166666666666, 51.975, 52.056666666666665, 52.37, 53.01833333333333, 53.486666666666665, 54.485, 54.20333333333333, 53.245, 53.30166666666667, 54.03, 54.36666666666667, 54.445, 54.88, 55.446666666666665, 55.96333333333333, 56.635, 57.181666666666665, 57.776666666666664, 57.2, 57.76, 57.70666666666666, 58.38166666666667, 58.56666666666667, 58.501666666666665, 57.975, 59.33166666666666, 59.23833333333334, 59.49666666666667, 59.218333333333334, 59.74166666666667, 59.87833333333333, 60.10166666666667, 59.928333333333335, 60.42333333333333, 60.92, 60.598333333333336, 60.45666666666666, 60.45166666666667, 59.983333333333334, 60.62833333333333, 60.541666666666664, 60.56666666666667, 60.92, 61.19166666666667, 61.37833333333333, 61.81166666666667, 61.23166666666667, 60.955, 61.935, 61.23833333333334, 61.416666666666664, 61.445, 61.74333333333333, 62.23, 61.815, 61.24333333333333, 61.266666666666666, 62.07666666666667, 62.10666666666667, 61.93, 61.52333333333333, 62.035, 61.94166666666667, 62.068333333333335, 62.223333333333336, 62.655, 62.32, 62.21333333333333, 62.486666666666665, 62.483333333333334, 62.25333333333333, 62.431666666666665, 62.04666666666667, 62.57833333333333, 62.483333333333334, 62.62, 62.266666666666666, 62.446666666666665, 62.483333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.211, Test loss: 2.065, Test accuracy: 23.54 

Round   1, Train loss: 1.997, Test loss: 1.880, Test accuracy: 30.92 

Round   2, Train loss: 1.847, Test loss: 1.775, Test accuracy: 34.87 

Round   3, Train loss: 1.781, Test loss: 1.683, Test accuracy: 38.62 

Round   4, Train loss: 1.694, Test loss: 1.607, Test accuracy: 40.91 

Round   5, Train loss: 1.647, Test loss: 1.555, Test accuracy: 42.85 

Round   6, Train loss: 1.601, Test loss: 1.513, Test accuracy: 44.02 

Round   7, Train loss: 1.553, Test loss: 1.478, Test accuracy: 45.87 

Round   8, Train loss: 1.506, Test loss: 1.459, Test accuracy: 46.62 

Round   9, Train loss: 1.471, Test loss: 1.432, Test accuracy: 47.81 

Round  10, Train loss: 1.433, Test loss: 1.411, Test accuracy: 48.82 

Round  11, Train loss: 1.412, Test loss: 1.369, Test accuracy: 50.33 

Round  12, Train loss: 1.389, Test loss: 1.351, Test accuracy: 50.93 

Round  13, Train loss: 1.356, Test loss: 1.337, Test accuracy: 51.80 

Round  14, Train loss: 1.335, Test loss: 1.311, Test accuracy: 52.91 

Round  15, Train loss: 1.277, Test loss: 1.302, Test accuracy: 53.25 

Round  16, Train loss: 1.262, Test loss: 1.284, Test accuracy: 53.81 

Round  17, Train loss: 1.250, Test loss: 1.265, Test accuracy: 54.48 

Round  18, Train loss: 1.200, Test loss: 1.252, Test accuracy: 55.20 

Round  19, Train loss: 1.168, Test loss: 1.249, Test accuracy: 55.55 

Round  20, Train loss: 1.141, Test loss: 1.233, Test accuracy: 56.25 

Round  21, Train loss: 1.143, Test loss: 1.217, Test accuracy: 56.96 

Round  22, Train loss: 1.095, Test loss: 1.205, Test accuracy: 57.36 

Round  23, Train loss: 1.064, Test loss: 1.202, Test accuracy: 57.48 

Round  24, Train loss: 1.064, Test loss: 1.191, Test accuracy: 58.12 

Round  25, Train loss: 1.042, Test loss: 1.204, Test accuracy: 57.85 

Round  26, Train loss: 1.042, Test loss: 1.176, Test accuracy: 58.83 

Round  27, Train loss: 0.983, Test loss: 1.181, Test accuracy: 59.03 

Round  28, Train loss: 0.949, Test loss: 1.196, Test accuracy: 58.35 

Round  29, Train loss: 0.979, Test loss: 1.185, Test accuracy: 58.96 

Round  30, Train loss: 0.898, Test loss: 1.179, Test accuracy: 59.21 

Round  31, Train loss: 0.897, Test loss: 1.188, Test accuracy: 59.49 

Round  32, Train loss: 0.871, Test loss: 1.197, Test accuracy: 59.40 

Round  33, Train loss: 0.967, Test loss: 1.186, Test accuracy: 60.16 

Round  34, Train loss: 0.854, Test loss: 1.186, Test accuracy: 60.05 

Round  35, Train loss: 0.860, Test loss: 1.193, Test accuracy: 60.23 

Round  36, Train loss: 0.864, Test loss: 1.192, Test accuracy: 60.05 

Round  37, Train loss: 0.902, Test loss: 1.165, Test accuracy: 61.16 

Round  38, Train loss: 0.849, Test loss: 1.182, Test accuracy: 60.87 

Round  39, Train loss: 0.807, Test loss: 1.175, Test accuracy: 61.08 

Round  40, Train loss: 0.776, Test loss: 1.213, Test accuracy: 60.69 

Round  41, Train loss: 0.797, Test loss: 1.176, Test accuracy: 61.43 

Round  42, Train loss: 0.745, Test loss: 1.209, Test accuracy: 61.14 

Round  43, Train loss: 0.821, Test loss: 1.176, Test accuracy: 61.73 

Round  44, Train loss: 0.739, Test loss: 1.213, Test accuracy: 60.92 

Round  45, Train loss: 0.732, Test loss: 1.204, Test accuracy: 61.09 

Round  46, Train loss: 0.738, Test loss: 1.203, Test accuracy: 61.18 

Round  47, Train loss: 0.727, Test loss: 1.202, Test accuracy: 61.51 

Round  48, Train loss: 0.730, Test loss: 1.200, Test accuracy: 61.54 

Round  49, Train loss: 0.694, Test loss: 1.232, Test accuracy: 61.48 

Round  50, Train loss: 0.707, Test loss: 1.200, Test accuracy: 61.71 

Round  51, Train loss: 0.682, Test loss: 1.240, Test accuracy: 61.78 

Round  52, Train loss: 0.688, Test loss: 1.247, Test accuracy: 61.80 

Round  53, Train loss: 0.645, Test loss: 1.244, Test accuracy: 61.98 

Round  54, Train loss: 0.657, Test loss: 1.242, Test accuracy: 62.21 

Round  55, Train loss: 0.625, Test loss: 1.262, Test accuracy: 62.12 

Round  56, Train loss: 0.608, Test loss: 1.276, Test accuracy: 61.95 

Round  57, Train loss: 0.623, Test loss: 1.263, Test accuracy: 62.05 

Round  58, Train loss: 0.602, Test loss: 1.271, Test accuracy: 62.12 

Round  59, Train loss: 0.551, Test loss: 1.284, Test accuracy: 62.12 

Round  60, Train loss: 0.571, Test loss: 1.297, Test accuracy: 62.12 

Round  61, Train loss: 0.599, Test loss: 1.313, Test accuracy: 62.48 

Round  62, Train loss: 0.509, Test loss: 1.311, Test accuracy: 62.31 

Round  63, Train loss: 0.594, Test loss: 1.288, Test accuracy: 62.77 

Round  64, Train loss: 0.585, Test loss: 1.309, Test accuracy: 62.51 

Round  65, Train loss: 0.524, Test loss: 1.341, Test accuracy: 62.26 

Round  66, Train loss: 0.525, Test loss: 1.329, Test accuracy: 62.48 

Round  67, Train loss: 0.533, Test loss: 1.327, Test accuracy: 62.31 

Round  68, Train loss: 0.544, Test loss: 1.366, Test accuracy: 61.41 

Round  69, Train loss: 0.555, Test loss: 1.350, Test accuracy: 62.56 

Round  70, Train loss: 0.464, Test loss: 1.370, Test accuracy: 62.25 

Round  71, Train loss: 0.551, Test loss: 1.358, Test accuracy: 62.12 

Round  72, Train loss: 0.532, Test loss: 1.344, Test accuracy: 62.64 

Round  73, Train loss: 0.523, Test loss: 1.370, Test accuracy: 62.09 

Round  74, Train loss: 0.539, Test loss: 1.389, Test accuracy: 62.22 

Round  75, Train loss: 0.519, Test loss: 1.413, Test accuracy: 62.16 

Round  76, Train loss: 0.479, Test loss: 1.412, Test accuracy: 62.00 

Round  77, Train loss: 0.426, Test loss: 1.430, Test accuracy: 62.15 

Round  78, Train loss: 0.522, Test loss: 1.419, Test accuracy: 62.91 

Round  79, Train loss: 0.498, Test loss: 1.410, Test accuracy: 62.74 

Round  80, Train loss: 0.480, Test loss: 1.416, Test accuracy: 62.91 

Round  81, Train loss: 0.508, Test loss: 1.388, Test accuracy: 62.47 

Round  82, Train loss: 0.477, Test loss: 1.410, Test accuracy: 62.70 

Round  83, Train loss: 0.447, Test loss: 1.399, Test accuracy: 62.89 

Round  84, Train loss: 0.455, Test loss: 1.430, Test accuracy: 62.70 

Round  85, Train loss: 0.409, Test loss: 1.426, Test accuracy: 62.63 

Round  86, Train loss: 0.421, Test loss: 1.478, Test accuracy: 62.19 

Round  87, Train loss: 0.439, Test loss: 1.488, Test accuracy: 62.29 

Round  88, Train loss: 0.433, Test loss: 1.485, Test accuracy: 62.35 

Round  89, Train loss: 0.406, Test loss: 1.493, Test accuracy: 61.96 

Round  90, Train loss: 0.447, Test loss: 1.511, Test accuracy: 62.09 

Round  91, Train loss: 0.441, Test loss: 1.486, Test accuracy: 62.27 

Round  92, Train loss: 0.434, Test loss: 1.522, Test accuracy: 62.42 

Round  93, Train loss: 0.429, Test loss: 1.508, Test accuracy: 61.94 

Round  94, Train loss: 0.419, Test loss: 1.488, Test accuracy: 62.35 

Round  95, Train loss: 0.476, Test loss: 1.492, Test accuracy: 62.44 

Round  96, Train loss: 0.407, Test loss: 1.536, Test accuracy: 62.82 

Round  97, Train loss: 0.419, Test loss: 1.525, Test accuracy: 62.88 

Round  98, Train loss: 0.399, Test loss: 1.542, Test accuracy: 62.58 

Round  99, Train loss: 0.413, Test loss: 1.546, Test accuracy: 62.88 

Final Round, Train loss: 0.318, Test loss: 1.575, Test accuracy: 62.47 

Average accuracy final 10 rounds: 62.46666666666667 

2181.5166161060333
[1.3940134048461914, 2.5664145946502686, 3.600034475326538, 4.635284185409546, 5.6704652309417725, 6.705831050872803, 7.742094278335571, 8.779372453689575, 9.815118312835693, 10.848108530044556, 11.881092071533203, 12.915398359298706, 13.949086904525757, 14.983534097671509, 16.01602005958557, 17.052123546600342, 18.088430881500244, 19.127342462539673, 20.15912699699402, 21.1911678314209, 22.22221565246582, 23.393304347991943, 24.56608271598816, 25.73358416557312, 26.904709577560425, 28.075764179229736, 29.243566274642944, 30.407238960266113, 31.57857394218445, 32.74590444564819, 33.91030287742615, 35.066681146621704, 36.237942695617676, 37.40628147125244, 38.50429654121399, 39.605961084365845, 40.70229959487915, 41.87053322792053, 43.01431751251221, 44.16188907623291, 45.30739498138428, 46.448004961013794, 47.59537863731384, 48.751145124435425, 49.90610980987549, 51.060327768325806, 52.21438932418823, 53.23041033744812, 54.23123097419739, 55.23229670524597, 56.24023509025574, 57.24628806114197, 58.245924949645996, 59.24372720718384, 60.24342656135559, 61.24486494064331, 62.24527668952942, 63.26448106765747, 64.2721815109253, 65.28156208992004, 66.28794693946838, 67.29511213302612, 68.30078482627869, 69.48483037948608, 70.62953805923462, 71.78881359100342, 72.95011329650879, 74.15184140205383, 75.34395551681519, 76.50362491607666, 77.64581871032715, 78.76858115196228, 79.91882658004761, 81.13393568992615, 82.34067893028259, 83.61955261230469, 84.82321119308472, 86.071204662323, 87.30402207374573, 88.59463477134705, 89.8847439289093, 91.17030096054077, 92.46344542503357, 93.75542116165161, 95.06299328804016, 96.37076020240784, 97.66569638252258, 98.93499302864075, 100.08890128135681, 101.24121451377869, 102.39225625991821, 103.53846073150635, 104.72195792198181, 105.9095230102539, 107.09743022918701, 108.28625512123108, 109.4818320274353, 110.66473269462585, 111.85246419906616, 113.03805375099182, 114.97884893417358]
[23.54, 30.921666666666667, 34.86833333333333, 38.61833333333333, 40.915, 42.85, 44.02166666666667, 45.87166666666667, 46.625, 47.80833333333333, 48.821666666666665, 50.33166666666666, 50.92666666666667, 51.795, 52.91, 53.25333333333333, 53.815, 54.47666666666667, 55.196666666666665, 55.55, 56.25, 56.95666666666666, 57.35666666666667, 57.47666666666667, 58.125, 57.85166666666667, 58.82666666666667, 59.035, 58.35166666666667, 58.95666666666666, 59.211666666666666, 59.486666666666665, 59.403333333333336, 60.15833333333333, 60.053333333333335, 60.22666666666667, 60.04666666666667, 61.15833333333333, 60.873333333333335, 61.07833333333333, 60.693333333333335, 61.43333333333333, 61.14333333333333, 61.725, 60.92, 61.08833333333333, 61.17666666666667, 61.50666666666667, 61.541666666666664, 61.48166666666667, 61.71, 61.78, 61.80166666666667, 61.97666666666667, 62.21, 62.123333333333335, 61.95166666666667, 62.04666666666667, 62.125, 62.115, 62.12, 62.483333333333334, 62.30833333333333, 62.77333333333333, 62.513333333333335, 62.25666666666667, 62.475, 62.31, 61.415, 62.556666666666665, 62.25, 62.12, 62.638333333333335, 62.09, 62.218333333333334, 62.165, 62.0, 62.145, 62.905, 62.736666666666665, 62.905, 62.465, 62.70166666666667, 62.891666666666666, 62.695, 62.63333333333333, 62.193333333333335, 62.288333333333334, 62.35166666666667, 61.95666666666666, 62.095, 62.266666666666666, 62.41833333333334, 61.935, 62.35, 62.44, 62.82333333333333, 62.88333333333333, 62.575, 62.88, 62.47]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 237, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54992 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 354, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53519 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 237, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 56247 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 504, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53517 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52198 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54431 is out of bounds for axis 0 with size 50000
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 18, in <module>
    from sklearn.cluster import KMeans
ModuleNotFoundError: No module named 'sklearn'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.123, Test loss: 1.936, Test accuracy: 25.93 

Round   0, Global train loss: 1.123, Global test loss: 2.317, Global test accuracy: 15.36 

Round   1, Train loss: 0.974, Test loss: 2.040, Test accuracy: 36.19 

Round   1, Global train loss: 0.974, Global test loss: 3.055, Global test accuracy: 15.00 

Round   2, Train loss: 0.935, Test loss: 1.200, Test accuracy: 52.70 

Round   2, Global train loss: 0.935, Global test loss: 2.301, Global test accuracy: 22.65 

Round   3, Train loss: 0.792, Test loss: 1.165, Test accuracy: 50.83 

Round   3, Global train loss: 0.792, Global test loss: 2.288, Global test accuracy: 13.62 

Round   4, Train loss: 0.781, Test loss: 0.911, Test accuracy: 62.21 

Round   4, Global train loss: 0.781, Global test loss: 2.096, Global test accuracy: 23.06 

Round   5, Train loss: 0.686, Test loss: 0.845, Test accuracy: 64.23 

Round   5, Global train loss: 0.686, Global test loss: 2.009, Global test accuracy: 28.93 

Round   6, Train loss: 0.676, Test loss: 0.846, Test accuracy: 63.19 

Round   6, Global train loss: 0.676, Global test loss: 2.154, Global test accuracy: 16.83 

Round   7, Train loss: 0.656, Test loss: 0.831, Test accuracy: 64.93 

Round   7, Global train loss: 0.656, Global test loss: 2.277, Global test accuracy: 16.40 

Round   8, Train loss: 0.704, Test loss: 0.812, Test accuracy: 65.33 

Round   8, Global train loss: 0.704, Global test loss: 2.318, Global test accuracy: 16.11 

Round   9, Train loss: 0.664, Test loss: 0.815, Test accuracy: 64.89 

Round   9, Global train loss: 0.664, Global test loss: 2.257, Global test accuracy: 16.48 

Round  10, Train loss: 0.598, Test loss: 0.780, Test accuracy: 67.66 

Round  10, Global train loss: 0.598, Global test loss: 2.153, Global test accuracy: 24.66 

Round  11, Train loss: 0.650, Test loss: 0.792, Test accuracy: 68.17 

Round  11, Global train loss: 0.650, Global test loss: 2.137, Global test accuracy: 24.69 

Traceback (most recent call last):
  File "main_fedrep.py", line 237, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 682, in train
    batch_loss.append(loss.item())
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.133, Test loss: 1.998, Test accuracy: 25.38 

Round   0, Global train loss: 1.133, Global test loss: 2.366, Global test accuracy: 14.95 

Round   1, Train loss: 0.944, Test loss: 1.644, Test accuracy: 39.01 

Round   1, Global train loss: 0.944, Global test loss: 2.262, Global test accuracy: 21.18 

Round   2, Train loss: 0.838, Test loss: 1.521, Test accuracy: 45.09 

Round   2, Global train loss: 0.838, Global test loss: 2.188, Global test accuracy: 26.27 

Round   3, Train loss: 0.877, Test loss: 1.368, Test accuracy: 52.99 

Round   3, Global train loss: 0.877, Global test loss: 2.169, Global test accuracy: 34.48 

Round   4, Train loss: 0.712, Test loss: 1.401, Test accuracy: 53.37 

Round   4, Global train loss: 0.712, Global test loss: 2.386, Global test accuracy: 28.81 

Round   5, Train loss: 0.747, Test loss: 0.914, Test accuracy: 64.04 

Round   5, Global train loss: 0.747, Global test loss: 1.765, Global test accuracy: 40.13 

Round   6, Train loss: 0.723, Test loss: 0.797, Test accuracy: 66.79 

Round   6, Global train loss: 0.723, Global test loss: 1.728, Global test accuracy: 36.80 

Round   7, Train loss: 0.743, Test loss: 0.752, Test accuracy: 68.54 

Round   7, Global train loss: 0.743, Global test loss: 1.631, Global test accuracy: 42.61 

Round   8, Train loss: 0.689, Test loss: 0.757, Test accuracy: 70.04 

Round   8, Global train loss: 0.689, Global test loss: 1.900, Global test accuracy: 36.51 

Round   9, Train loss: 0.612, Test loss: 0.724, Test accuracy: 70.75 

Round   9, Global train loss: 0.612, Global test loss: 1.796, Global test accuracy: 38.43 

Round  10, Train loss: 0.632, Test loss: 0.696, Test accuracy: 72.11 

Round  10, Global train loss: 0.632, Global test loss: 1.647, Global test accuracy: 43.44 

Round  11, Train loss: 0.664, Test loss: 0.685, Test accuracy: 73.04 

Round  11, Global train loss: 0.664, Global test loss: 1.519, Global test accuracy: 48.68 

Round  12, Train loss: 0.658, Test loss: 0.711, Test accuracy: 72.60 

Round  12, Global train loss: 0.658, Global test loss: 1.795, Global test accuracy: 37.94 

Round  13, Train loss: 0.624, Test loss: 0.662, Test accuracy: 73.97 

Round  13, Global train loss: 0.624, Global test loss: 1.501, Global test accuracy: 49.11 

Round  14, Train loss: 0.595, Test loss: 0.592, Test accuracy: 75.38 

Round  14, Global train loss: 0.595, Global test loss: 1.648, Global test accuracy: 43.77 

Round  15, Train loss: 0.605, Test loss: 0.587, Test accuracy: 75.69 

Round  15, Global train loss: 0.605, Global test loss: 1.707, Global test accuracy: 45.22 

Round  16, Train loss: 0.590, Test loss: 0.582, Test accuracy: 75.96 

Round  16, Global train loss: 0.590, Global test loss: 1.482, Global test accuracy: 47.75 

Round  17, Train loss: 0.595, Test loss: 0.564, Test accuracy: 77.18 

Round  17, Global train loss: 0.595, Global test loss: 1.441, Global test accuracy: 49.16 

Round  18, Train loss: 0.581, Test loss: 0.559, Test accuracy: 77.46 

Round  18, Global train loss: 0.581, Global test loss: 1.785, Global test accuracy: 45.18 

Round  19, Train loss: 0.524, Test loss: 0.564, Test accuracy: 77.09 

Round  19, Global train loss: 0.524, Global test loss: 1.523, Global test accuracy: 47.46 

Round  20, Train loss: 0.498, Test loss: 0.554, Test accuracy: 77.59 

Round  20, Global train loss: 0.498, Global test loss: 1.601, Global test accuracy: 46.82 

Round  21, Train loss: 0.496, Test loss: 0.541, Test accuracy: 78.02 

Round  21, Global train loss: 0.496, Global test loss: 1.795, Global test accuracy: 47.18 

Round  22, Train loss: 0.545, Test loss: 0.536, Test accuracy: 78.37 

Round  22, Global train loss: 0.545, Global test loss: 1.566, Global test accuracy: 46.27 

Round  23, Train loss: 0.605, Test loss: 0.529, Test accuracy: 78.59 

Round  23, Global train loss: 0.605, Global test loss: 1.527, Global test accuracy: 46.72 

Round  24, Train loss: 0.462, Test loss: 0.529, Test accuracy: 78.99 

Round  24, Global train loss: 0.462, Global test loss: 1.499, Global test accuracy: 49.62 

Round  25, Train loss: 0.486, Test loss: 0.526, Test accuracy: 79.42 

Round  25, Global train loss: 0.486, Global test loss: 1.397, Global test accuracy: 51.36 

Round  26, Train loss: 0.510, Test loss: 0.530, Test accuracy: 78.99 

Round  26, Global train loss: 0.510, Global test loss: 1.430, Global test accuracy: 50.48 

Round  27, Train loss: 0.555, Test loss: 0.531, Test accuracy: 78.91 

Round  27, Global train loss: 0.555, Global test loss: 1.407, Global test accuracy: 50.47 

Round  28, Train loss: 0.552, Test loss: 0.525, Test accuracy: 79.16 

Round  28, Global train loss: 0.552, Global test loss: 1.419, Global test accuracy: 49.99 

Round  29, Train loss: 0.418, Test loss: 0.554, Test accuracy: 78.21 

Round  29, Global train loss: 0.418, Global test loss: 1.506, Global test accuracy: 49.66 

Round  30, Train loss: 0.414, Test loss: 0.535, Test accuracy: 79.00 

Round  30, Global train loss: 0.414, Global test loss: 1.310, Global test accuracy: 56.11 

Round  31, Train loss: 0.417, Test loss: 0.533, Test accuracy: 79.05 

Round  31, Global train loss: 0.417, Global test loss: 1.413, Global test accuracy: 53.46 

Round  32, Train loss: 0.416, Test loss: 0.511, Test accuracy: 79.95 

Round  32, Global train loss: 0.416, Global test loss: 1.510, Global test accuracy: 53.10 

Round  33, Train loss: 0.380, Test loss: 0.509, Test accuracy: 80.01 

Round  33, Global train loss: 0.380, Global test loss: 1.308, Global test accuracy: 57.76 

Round  34, Train loss: 0.504, Test loss: 0.497, Test accuracy: 80.65 

Round  34, Global train loss: 0.504, Global test loss: 1.333, Global test accuracy: 55.98 

Round  35, Train loss: 0.424, Test loss: 0.514, Test accuracy: 80.06 

Round  35, Global train loss: 0.424, Global test loss: 1.235, Global test accuracy: 58.39 

Round  36, Train loss: 0.468, Test loss: 0.513, Test accuracy: 80.40 

Round  36, Global train loss: 0.468, Global test loss: 1.286, Global test accuracy: 56.71 

Round  37, Train loss: 0.412, Test loss: 0.511, Test accuracy: 80.66 

Round  37, Global train loss: 0.412, Global test loss: 1.416, Global test accuracy: 53.30 

Round  38, Train loss: 0.477, Test loss: 0.524, Test accuracy: 80.08 

Round  38, Global train loss: 0.477, Global test loss: 1.231, Global test accuracy: 59.11 

Round  39, Train loss: 0.403, Test loss: 0.521, Test accuracy: 80.28 

Round  39, Global train loss: 0.403, Global test loss: 1.338, Global test accuracy: 56.24 

Round  40, Train loss: 0.353, Test loss: 0.503, Test accuracy: 81.03 

Round  40, Global train loss: 0.353, Global test loss: 1.301, Global test accuracy: 58.65 

Round  41, Train loss: 0.355, Test loss: 0.503, Test accuracy: 81.27 

Round  41, Global train loss: 0.355, Global test loss: 1.528, Global test accuracy: 50.89 

Round  42, Train loss: 0.348, Test loss: 0.507, Test accuracy: 81.23 

Round  42, Global train loss: 0.348, Global test loss: 1.332, Global test accuracy: 57.29 

Round  43, Train loss: 0.316, Test loss: 0.504, Test accuracy: 81.34 

Round  43, Global train loss: 0.316, Global test loss: 1.255, Global test accuracy: 58.79 

Round  44, Train loss: 0.401, Test loss: 0.496, Test accuracy: 81.59 

Round  44, Global train loss: 0.401, Global test loss: 1.119, Global test accuracy: 61.76 

Round  45, Train loss: 0.417, Test loss: 0.489, Test accuracy: 81.84 

Round  45, Global train loss: 0.417, Global test loss: 1.225, Global test accuracy: 58.84 

Round  46, Train loss: 0.311, Test loss: 0.484, Test accuracy: 81.98 

Round  46, Global train loss: 0.311, Global test loss: 1.295, Global test accuracy: 59.76 

Round  47, Train loss: 0.454, Test loss: 0.479, Test accuracy: 82.12 

Round  47, Global train loss: 0.454, Global test loss: 1.419, Global test accuracy: 53.01 

Round  48, Train loss: 0.370, Test loss: 0.475, Test accuracy: 82.53 

Round  48, Global train loss: 0.370, Global test loss: 1.310, Global test accuracy: 57.54 

Round  49, Train loss: 0.349, Test loss: 0.480, Test accuracy: 82.42 

Round  49, Global train loss: 0.349, Global test loss: 1.758, Global test accuracy: 50.54 

Round  50, Train loss: 0.344, Test loss: 0.507, Test accuracy: 81.57 

Round  50, Global train loss: 0.344, Global test loss: 1.274, Global test accuracy: 58.21 

Round  51, Train loss: 0.345, Test loss: 0.497, Test accuracy: 82.04 

Round  51, Global train loss: 0.345, Global test loss: 1.446, Global test accuracy: 54.64 

Round  52, Train loss: 0.349, Test loss: 0.480, Test accuracy: 82.58 

Round  52, Global train loss: 0.349, Global test loss: 1.234, Global test accuracy: 59.33 

Round  53, Train loss: 0.335, Test loss: 0.487, Test accuracy: 82.46 

Round  53, Global train loss: 0.335, Global test loss: 1.094, Global test accuracy: 63.17 

Round  54, Train loss: 0.318, Test loss: 0.507, Test accuracy: 81.79 

Round  54, Global train loss: 0.318, Global test loss: 1.190, Global test accuracy: 61.53 

Round  55, Train loss: 0.277, Test loss: 0.507, Test accuracy: 81.91 

Round  55, Global train loss: 0.277, Global test loss: 1.286, Global test accuracy: 58.32 

Round  56, Train loss: 0.354, Test loss: 0.512, Test accuracy: 81.64 

Round  56, Global train loss: 0.354, Global test loss: 1.324, Global test accuracy: 57.23 

Round  57, Train loss: 0.329, Test loss: 0.508, Test accuracy: 81.94 

Round  57, Global train loss: 0.329, Global test loss: 1.240, Global test accuracy: 59.46 

Round  58, Train loss: 0.324, Test loss: 0.489, Test accuracy: 82.56 

Round  58, Global train loss: 0.324, Global test loss: 1.185, Global test accuracy: 60.64 

Round  59, Train loss: 0.307, Test loss: 0.486, Test accuracy: 82.70 

Round  59, Global train loss: 0.307, Global test loss: 1.215, Global test accuracy: 61.22 

Round  60, Train loss: 0.285, Test loss: 0.503, Test accuracy: 82.63 

Round  60, Global train loss: 0.285, Global test loss: 1.190, Global test accuracy: 62.65 

Round  61, Train loss: 0.322, Test loss: 0.496, Test accuracy: 82.67 

Round  61, Global train loss: 0.322, Global test loss: 1.292, Global test accuracy: 58.17 

Round  62, Train loss: 0.269, Test loss: 0.484, Test accuracy: 83.00 

Round  62, Global train loss: 0.269, Global test loss: 1.113, Global test accuracy: 63.61 

Round  63, Train loss: 0.309, Test loss: 0.484, Test accuracy: 83.16 

Round  63, Global train loss: 0.309, Global test loss: 1.216, Global test accuracy: 60.60 

Round  64, Train loss: 0.362, Test loss: 0.490, Test accuracy: 82.99 

Round  64, Global train loss: 0.362, Global test loss: 1.411, Global test accuracy: 55.94 

Round  65, Train loss: 0.228, Test loss: 0.470, Test accuracy: 83.60 

Round  65, Global train loss: 0.228, Global test loss: 1.303, Global test accuracy: 60.98 

Round  66, Train loss: 0.277, Test loss: 0.479, Test accuracy: 83.58 

Round  66, Global train loss: 0.277, Global test loss: 1.161, Global test accuracy: 63.76 

Round  67, Train loss: 0.293, Test loss: 0.492, Test accuracy: 83.24 

Round  67, Global train loss: 0.293, Global test loss: 1.445, Global test accuracy: 56.58 

Round  68, Train loss: 0.302, Test loss: 0.479, Test accuracy: 83.52 

Round  68, Global train loss: 0.302, Global test loss: 1.160, Global test accuracy: 61.89 

Round  69, Train loss: 0.278, Test loss: 0.483, Test accuracy: 83.59 

Round  69, Global train loss: 0.278, Global test loss: 1.339, Global test accuracy: 58.36 

Round  70, Train loss: 0.260, Test loss: 0.507, Test accuracy: 83.06 

Round  70, Global train loss: 0.260, Global test loss: 1.447, Global test accuracy: 57.01 

Round  71, Train loss: 0.262, Test loss: 0.509, Test accuracy: 83.18 

Round  71, Global train loss: 0.262, Global test loss: 1.297, Global test accuracy: 60.57 

Round  72, Train loss: 0.215, Test loss: 0.493, Test accuracy: 83.72 

Round  72, Global train loss: 0.215, Global test loss: 1.130, Global test accuracy: 64.23 

Round  73, Train loss: 0.287, Test loss: 0.498, Test accuracy: 83.28 

Round  73, Global train loss: 0.287, Global test loss: 1.257, Global test accuracy: 60.56 

Round  74, Train loss: 0.246, Test loss: 0.481, Test accuracy: 83.84 

Round  74, Global train loss: 0.246, Global test loss: 1.336, Global test accuracy: 59.56 

Round  75, Train loss: 0.257, Test loss: 0.495, Test accuracy: 83.52 

Round  75, Global train loss: 0.257, Global test loss: 1.202, Global test accuracy: 62.96 

Round  76, Train loss: 0.251, Test loss: 0.495, Test accuracy: 83.48 

Round  76, Global train loss: 0.251, Global test loss: 1.482, Global test accuracy: 58.12 

Round  77, Train loss: 0.255, Test loss: 0.497, Test accuracy: 83.70 

Round  77, Global train loss: 0.255, Global test loss: 1.253, Global test accuracy: 62.41 

Round  78, Train loss: 0.212, Test loss: 0.497, Test accuracy: 83.74 

Round  78, Global train loss: 0.212, Global test loss: 1.296, Global test accuracy: 62.68 

Round  79, Train loss: 0.259, Test loss: 0.479, Test accuracy: 83.95 

Round  79, Global train loss: 0.259, Global test loss: 1.218, Global test accuracy: 62.53 

Round  80, Train loss: 0.217, Test loss: 0.512, Test accuracy: 82.99 

Round  80, Global train loss: 0.217, Global test loss: 1.296, Global test accuracy: 61.93 

Round  81, Train loss: 0.289, Test loss: 0.506, Test accuracy: 83.52 

Round  81, Global train loss: 0.289, Global test loss: 1.155, Global test accuracy: 63.66 

Round  82, Train loss: 0.196, Test loss: 0.503, Test accuracy: 83.67 

Round  82, Global train loss: 0.196, Global test loss: 1.129, Global test accuracy: 64.52 

Round  83, Train loss: 0.218, Test loss: 0.491, Test accuracy: 84.12 

Round  83, Global train loss: 0.218, Global test loss: 1.404, Global test accuracy: 58.66 

Round  84, Train loss: 0.277, Test loss: 0.496, Test accuracy: 83.96 

Round  84, Global train loss: 0.277, Global test loss: 1.230, Global test accuracy: 61.38 

Round  85, Train loss: 0.225, Test loss: 0.490, Test accuracy: 84.05 

Round  85, Global train loss: 0.225, Global test loss: 1.460, Global test accuracy: 58.39 

Round  86, Train loss: 0.222, Test loss: 0.505, Test accuracy: 83.91 

Round  86, Global train loss: 0.222, Global test loss: 1.296, Global test accuracy: 61.10 

Round  87, Train loss: 0.203, Test loss: 0.502, Test accuracy: 84.16 

Round  87, Global train loss: 0.203, Global test loss: 1.478, Global test accuracy: 59.33 

Round  88, Train loss: 0.203, Test loss: 0.498, Test accuracy: 84.25 

Round  88, Global train loss: 0.203, Global test loss: 1.271, Global test accuracy: 64.10 

Round  89, Train loss: 0.211, Test loss: 0.502, Test accuracy: 83.92 

Round  89, Global train loss: 0.211, Global test loss: 1.304, Global test accuracy: 63.16 

Round  90, Train loss: 0.164, Test loss: 0.512, Test accuracy: 83.79 

Round  90, Global train loss: 0.164, Global test loss: 1.332, Global test accuracy: 63.36 

Round  91, Train loss: 0.274, Test loss: 0.519, Test accuracy: 83.77 

Round  91, Global train loss: 0.274, Global test loss: 1.270, Global test accuracy: 62.31 

Round  92, Train loss: 0.228, Test loss: 0.510, Test accuracy: 84.04 

Round  92, Global train loss: 0.228, Global test loss: 1.458, Global test accuracy: 59.84 

Round  93, Train loss: 0.186, Test loss: 0.508, Test accuracy: 84.19 

Round  93, Global train loss: 0.186, Global test loss: 1.248, Global test accuracy: 62.07 

Round  94, Train loss: 0.158, Test loss: 0.493, Test accuracy: 84.66 

Round  94, Global train loss: 0.158, Global test loss: 1.509, Global test accuracy: 59.07 

Round  95, Train loss: 0.261, Test loss: 0.505, Test accuracy: 84.28 

Round  95, Global train loss: 0.261, Global test loss: 1.437, Global test accuracy: 58.93 

Round  96, Train loss: 0.197, Test loss: 0.506, Test accuracy: 84.41 

Round  96, Global train loss: 0.197, Global test loss: 1.584, Global test accuracy: 58.16 

Round  97, Train loss: 0.220, Test loss: 0.506, Test accuracy: 84.42 

Round  97, Global train loss: 0.220, Global test loss: 1.731, Global test accuracy: 56.36 

Round  98, Train loss: 0.199, Test loss: 0.500, Test accuracy: 84.41 

Round  98, Global train loss: 0.199, Global test loss: 1.276, Global test accuracy: 62.11 

Round  99, Train loss: 0.210, Test loss: 0.509, Test accuracy: 84.24 

Round  99, Global train loss: 0.210, Global test loss: 1.190, Global test accuracy: 63.18 

Final Round, Train loss: 0.170, Test loss: 0.546, Test accuracy: 84.31 

Final Round, Global train loss: 0.170, Global test loss: 1.190, Global test accuracy: 63.18 

Average accuracy final 10 rounds: 84.22 

Average global accuracy final 10 rounds: 60.53777777777778 

1642.2921442985535
[1.3841865062713623, 2.512662887573242, 3.633185386657715, 4.759441375732422, 5.891201972961426, 7.02735161781311, 8.166226148605347, 9.302679538726807, 10.413006067276001, 11.546401023864746, 12.691011905670166, 13.834249019622803, 14.975878953933716, 16.12107229232788, 17.265312671661377, 18.430400848388672, 19.58908987045288, 20.75281834602356, 21.91059374809265, 23.075018644332886, 24.237796783447266, 25.391573429107666, 26.555130004882812, 27.719260454177856, 28.887518167495728, 30.04073452949524, 31.199530124664307, 32.35765051841736, 33.515514850616455, 34.67626953125, 35.836872577667236, 36.99534749984741, 38.153804302215576, 39.31138253211975, 40.464823961257935, 41.628228187561035, 42.78530430793762, 43.94817137718201, 45.112555503845215, 46.27526021003723, 47.432326316833496, 48.573758363723755, 49.71905708312988, 50.85980272293091, 52.01257276535034, 53.15682935714722, 54.30032014846802, 55.4476592540741, 56.607786893844604, 57.77309989929199, 58.93305683135986, 60.0810124874115, 61.244351387023926, 62.41871476173401, 63.601147413253784, 64.79079127311707, 65.98303508758545, 67.17586660385132, 68.35955786705017, 69.54143047332764, 70.7297830581665, 71.91004800796509, 73.10261702537537, 74.29830265045166, 75.48677706718445, 76.66923022270203, 77.85310339927673, 79.03153944015503, 80.22517776489258, 81.40743851661682, 82.57710671424866, 83.75099992752075, 84.91439723968506, 86.08160924911499, 87.24761390686035, 88.41806101799011, 89.58845233917236, 90.7593183517456, 91.93234658241272, 93.11410617828369, 94.28093957901001, 95.44518828392029, 96.61566591262817, 97.79125380516052, 98.95566821098328, 100.12130880355835, 101.29372477531433, 102.45951819419861, 103.6184949874878, 104.8052225112915, 105.99253463745117, 107.1721625328064, 108.36187195777893, 109.55980038642883, 110.75725078582764, 111.94773364067078, 113.13679099082947, 114.33547186851501, 115.52021169662476, 116.7057409286499, 119.03360986709595]
[25.383333333333333, 39.01111111111111, 45.08888888888889, 52.98888888888889, 53.36666666666667, 64.04444444444445, 66.79444444444445, 68.54444444444445, 70.03888888888889, 70.75, 72.11111111111111, 73.04444444444445, 72.6, 73.97222222222223, 75.38333333333334, 75.68888888888888, 75.95555555555555, 77.17777777777778, 77.45555555555555, 77.08888888888889, 77.58888888888889, 78.02222222222223, 78.36666666666666, 78.58888888888889, 78.99444444444444, 79.41666666666667, 78.9888888888889, 78.90555555555555, 79.16111111111111, 78.21111111111111, 79.0, 79.05, 79.95, 80.00555555555556, 80.65, 80.05555555555556, 80.4, 80.66111111111111, 80.07777777777778, 80.28333333333333, 81.02777777777777, 81.26666666666667, 81.23333333333333, 81.34444444444445, 81.58888888888889, 81.84444444444445, 81.98333333333333, 82.11666666666666, 82.53333333333333, 82.41666666666667, 81.57222222222222, 82.03888888888889, 82.57777777777778, 82.45555555555555, 81.78888888888889, 81.91111111111111, 81.63888888888889, 81.94444444444444, 82.56111111111112, 82.7, 82.62777777777778, 82.67222222222222, 83.0, 83.16111111111111, 82.99444444444444, 83.6, 83.58333333333333, 83.24444444444444, 83.51666666666667, 83.58888888888889, 83.06111111111112, 83.18333333333334, 83.72222222222223, 83.28333333333333, 83.83888888888889, 83.52222222222223, 83.48333333333333, 83.7, 83.7388888888889, 83.95, 82.9888888888889, 83.52222222222223, 83.67222222222222, 84.11666666666666, 83.96111111111111, 84.05, 83.91111111111111, 84.15555555555555, 84.25, 83.91666666666667, 83.79444444444445, 83.76666666666667, 84.03888888888889, 84.18888888888888, 84.65555555555555, 84.27777777777777, 84.41111111111111, 84.41666666666667, 84.41111111111111, 84.2388888888889, 84.31111111111112]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.285, Test loss: 2.230, Test accuracy: 17.89 

Round   1, Train loss: 2.161, Test loss: 2.023, Test accuracy: 27.14 

Round   2, Train loss: 2.032, Test loss: 1.917, Test accuracy: 30.28 

Round   3, Train loss: 1.920, Test loss: 1.853, Test accuracy: 32.72 

Round   4, Train loss: 1.832, Test loss: 1.764, Test accuracy: 35.89 

Round   5, Train loss: 1.764, Test loss: 1.706, Test accuracy: 37.06 

Round   6, Train loss: 1.709, Test loss: 1.670, Test accuracy: 38.80 

Round   7, Train loss: 1.667, Test loss: 1.634, Test accuracy: 40.34 

Round   8, Train loss: 1.635, Test loss: 1.592, Test accuracy: 41.59 

Round   9, Train loss: 1.598, Test loss: 1.560, Test accuracy: 42.71 

Round  10, Train loss: 1.536, Test loss: 1.537, Test accuracy: 43.71 

Round  11, Train loss: 1.516, Test loss: 1.513, Test accuracy: 45.07 

Round  12, Train loss: 1.500, Test loss: 1.488, Test accuracy: 45.69 

Round  13, Train loss: 1.459, Test loss: 1.480, Test accuracy: 46.07 

Round  14, Train loss: 1.451, Test loss: 1.461, Test accuracy: 46.81 

Round  15, Train loss: 1.443, Test loss: 1.448, Test accuracy: 47.56 

Round  16, Train loss: 1.409, Test loss: 1.451, Test accuracy: 47.66 

Round  17, Train loss: 1.422, Test loss: 1.433, Test accuracy: 48.30 

Round  18, Train loss: 1.389, Test loss: 1.397, Test accuracy: 49.60 

Round  19, Train loss: 1.375, Test loss: 1.386, Test accuracy: 49.96 

Round  20, Train loss: 1.357, Test loss: 1.371, Test accuracy: 50.84 

Round  21, Train loss: 1.323, Test loss: 1.368, Test accuracy: 50.91 

Round  22, Train loss: 1.306, Test loss: 1.350, Test accuracy: 51.45 

Round  23, Train loss: 1.282, Test loss: 1.331, Test accuracy: 52.21 

Round  24, Train loss: 1.251, Test loss: 1.333, Test accuracy: 52.31 

Round  25, Train loss: 1.233, Test loss: 1.314, Test accuracy: 53.06 

Round  26, Train loss: 1.253, Test loss: 1.313, Test accuracy: 53.33 

Round  27, Train loss: 1.264, Test loss: 1.285, Test accuracy: 54.65 

Round  28, Train loss: 1.215, Test loss: 1.304, Test accuracy: 53.86 

Round  29, Train loss: 1.186, Test loss: 1.285, Test accuracy: 54.64 

Round  30, Train loss: 1.130, Test loss: 1.288, Test accuracy: 54.28 

Round  31, Train loss: 1.151, Test loss: 1.258, Test accuracy: 55.35 

Round  32, Train loss: 1.151, Test loss: 1.241, Test accuracy: 56.19 

Round  33, Train loss: 1.112, Test loss: 1.230, Test accuracy: 56.47 

Round  34, Train loss: 1.121, Test loss: 1.220, Test accuracy: 57.09 

Round  35, Train loss: 1.085, Test loss: 1.226, Test accuracy: 57.39 

Round  36, Train loss: 1.115, Test loss: 1.219, Test accuracy: 57.60 

Round  37, Train loss: 1.061, Test loss: 1.199, Test accuracy: 58.20 

Round  38, Train loss: 1.042, Test loss: 1.191, Test accuracy: 58.45 

Round  39, Train loss: 1.025, Test loss: 1.197, Test accuracy: 58.06 

Round  40, Train loss: 1.045, Test loss: 1.201, Test accuracy: 58.15 

Round  41, Train loss: 1.029, Test loss: 1.186, Test accuracy: 58.65 

Round  42, Train loss: 1.050, Test loss: 1.185, Test accuracy: 59.23 

Round  43, Train loss: 0.977, Test loss: 1.183, Test accuracy: 59.11 

Round  44, Train loss: 1.014, Test loss: 1.192, Test accuracy: 58.87 

Round  45, Train loss: 0.981, Test loss: 1.183, Test accuracy: 59.79 

Round  46, Train loss: 0.983, Test loss: 1.185, Test accuracy: 59.67 

Round  47, Train loss: 0.965, Test loss: 1.172, Test accuracy: 59.66 

Round  48, Train loss: 0.938, Test loss: 1.159, Test accuracy: 60.00 

Round  49, Train loss: 0.975, Test loss: 1.153, Test accuracy: 60.29 

Round  50, Train loss: 0.931, Test loss: 1.142, Test accuracy: 60.66 

Round  51, Train loss: 0.916, Test loss: 1.152, Test accuracy: 60.48 

Round  52, Train loss: 0.911, Test loss: 1.157, Test accuracy: 60.67 

Round  53, Train loss: 0.890, Test loss: 1.162, Test accuracy: 60.65 

Round  54, Train loss: 0.877, Test loss: 1.170, Test accuracy: 60.67 

Round  55, Train loss: 0.880, Test loss: 1.163, Test accuracy: 60.70 

Round  56, Train loss: 0.858, Test loss: 1.153, Test accuracy: 60.83 

Round  57, Train loss: 0.811, Test loss: 1.185, Test accuracy: 60.08 

Round  58, Train loss: 0.863, Test loss: 1.186, Test accuracy: 60.29 

Round  59, Train loss: 0.857, Test loss: 1.195, Test accuracy: 60.42 

Round  60, Train loss: 0.848, Test loss: 1.172, Test accuracy: 60.70 

Round  61, Train loss: 0.799, Test loss: 1.174, Test accuracy: 60.62 

Round  62, Train loss: 0.789, Test loss: 1.163, Test accuracy: 61.01 

Round  63, Train loss: 0.834, Test loss: 1.172, Test accuracy: 61.09 

Round  64, Train loss: 0.760, Test loss: 1.177, Test accuracy: 60.98 

Round  65, Train loss: 0.766, Test loss: 1.181, Test accuracy: 61.22 

Round  66, Train loss: 0.773, Test loss: 1.183, Test accuracy: 60.98 

Round  67, Train loss: 0.735, Test loss: 1.185, Test accuracy: 61.12 

Round  68, Train loss: 0.729, Test loss: 1.204, Test accuracy: 61.21 

Round  69, Train loss: 0.772, Test loss: 1.162, Test accuracy: 61.69 

Round  70, Train loss: 0.730, Test loss: 1.175, Test accuracy: 61.90 

Round  71, Train loss: 0.741, Test loss: 1.195, Test accuracy: 61.70 

Round  72, Train loss: 0.746, Test loss: 1.181, Test accuracy: 61.72 

Round  73, Train loss: 0.743, Test loss: 1.189, Test accuracy: 61.62 

Round  74, Train loss: 0.726, Test loss: 1.184, Test accuracy: 61.70 

Round  75, Train loss: 0.763, Test loss: 1.186, Test accuracy: 61.44 

Round  76, Train loss: 0.763, Test loss: 1.195, Test accuracy: 61.70 

Round  77, Train loss: 0.712, Test loss: 1.190, Test accuracy: 62.20 

Round  78, Train loss: 0.641, Test loss: 1.193, Test accuracy: 61.99 

Round  79, Train loss: 0.730, Test loss: 1.186, Test accuracy: 62.00 

Round  80, Train loss: 0.656, Test loss: 1.178, Test accuracy: 62.26 

Round  81, Train loss: 0.667, Test loss: 1.204, Test accuracy: 61.92 

Round  82, Train loss: 0.657, Test loss: 1.195, Test accuracy: 62.27 

Round  83, Train loss: 0.697, Test loss: 1.214, Test accuracy: 61.95 

Round  84, Train loss: 0.663, Test loss: 1.217, Test accuracy: 62.30 

Round  85, Train loss: 0.659, Test loss: 1.223, Test accuracy: 62.10 

Round  86, Train loss: 0.666, Test loss: 1.210, Test accuracy: 62.60 

Round  87, Train loss: 0.608, Test loss: 1.208, Test accuracy: 62.55 

Round  88, Train loss: 0.618, Test loss: 1.240, Test accuracy: 62.32 

Round  89, Train loss: 0.609, Test loss: 1.261, Test accuracy: 62.07 

Round  90, Train loss: 0.645, Test loss: 1.243, Test accuracy: 62.63 

Round  91, Train loss: 0.671, Test loss: 1.245, Test accuracy: 62.05 

Round  92, Train loss: 0.618, Test loss: 1.280, Test accuracy: 61.98 

Round  93, Train loss: 0.606, Test loss: 1.234, Test accuracy: 62.13 

Round  94, Train loss: 0.636, Test loss: 1.258, Test accuracy: 62.47 

Round  95, Train loss: 0.592, Test loss: 1.241, Test accuracy: 62.71 

Round  96, Train loss: 0.654, Test loss: 1.263, Test accuracy: 62.25 

Round  97, Train loss: 0.554, Test loss: 1.284, Test accuracy: 61.85 

Round  98, Train loss: 0.566, Test loss: 1.289, Test accuracy: 62.14 

Round  99, Train loss: 0.582, Test loss: 1.256, Test accuracy: 62.13 

Final Round, Train loss: 0.513, Test loss: 1.265, Test accuracy: 62.40 

Average accuracy final 10 rounds: 62.235 

2234.060107946396
[1.3277359008789062, 2.417482614517212, 3.5147950649261475, 4.60839581489563, 5.709510564804077, 6.811692237854004, 7.90868878364563, 9.015716791152954, 10.11358380317688, 11.221843242645264, 12.336708068847656, 13.432828187942505, 14.537271976470947, 15.639048099517822, 16.73820686340332, 17.84095811843872, 18.944258213043213, 20.03695583343506, 21.139198541641235, 22.227400064468384, 23.320839166641235, 24.417381286621094, 25.51654863357544, 26.60979151725769, 27.71080780029297, 28.724185943603516, 29.72004270553589, 30.718636512756348, 31.71294093132019, 32.707990884780884, 33.70763182640076, 34.70954895019531, 35.711464166641235, 36.71444034576416, 37.71541905403137, 38.71226453781128, 39.710538387298584, 40.712217569351196, 41.70223259925842, 42.70621609687805, 43.7063090801239, 44.70139956474304, 45.701677083969116, 46.69629406929016, 47.70205760002136, 48.699594020843506, 49.70474720001221, 50.70689296722412, 51.70780801773071, 52.705944538116455, 53.70889115333557, 54.70366668701172, 55.70463752746582, 56.7056941986084, 57.70055890083313, 58.70222592353821, 59.70799493789673, 60.70552372932434, 61.69984316825867, 62.70099329948425, 63.69531989097595, 64.69213390350342, 65.68343949317932, 66.6837112903595, 67.68493008613586, 68.67182469367981, 69.66716456413269, 70.65335655212402, 71.65015840530396, 72.64655661582947, 73.64126992225647, 74.63186764717102, 75.62261056900024, 76.61100387573242, 77.60679745674133, 78.59516835212708, 79.58564400672913, 80.5958092212677, 81.59627437591553, 82.59801745414734, 83.59803676605225, 84.59764337539673, 85.61177968978882, 86.61218190193176, 87.61541199684143, 88.6155481338501, 89.64932298660278, 90.68326783180237, 91.72091674804688, 92.74902153015137, 93.82960200309753, 94.92543315887451, 96.00988674163818, 97.10284399986267, 98.18763256072998, 99.26843428611755, 100.35819935798645, 101.44025540351868, 102.5317907333374, 103.62096095085144, 105.59962487220764]
[17.888333333333332, 27.14, 30.281666666666666, 32.72, 35.891666666666666, 37.056666666666665, 38.795, 40.34, 41.593333333333334, 42.71, 43.70666666666666, 45.06666666666667, 45.69166666666667, 46.06666666666667, 46.81166666666667, 47.55833333333333, 47.65833333333333, 48.305, 49.596666666666664, 49.958333333333336, 50.836666666666666, 50.91166666666667, 51.45166666666667, 52.208333333333336, 52.31333333333333, 53.065, 53.325, 54.65, 53.858333333333334, 54.638333333333335, 54.28333333333333, 55.35333333333333, 56.185, 56.471666666666664, 57.095, 57.39333333333333, 57.60166666666667, 58.205, 58.445, 58.06166666666667, 58.15, 58.651666666666664, 59.22833333333333, 59.10666666666667, 58.865, 59.788333333333334, 59.666666666666664, 59.66, 59.998333333333335, 60.291666666666664, 60.656666666666666, 60.483333333333334, 60.666666666666664, 60.651666666666664, 60.67333333333333, 60.70166666666667, 60.83, 60.07833333333333, 60.29333333333334, 60.425, 60.705, 60.625, 61.00833333333333, 61.085, 60.98, 61.22, 60.97833333333333, 61.11666666666667, 61.208333333333336, 61.685, 61.89666666666667, 61.70333333333333, 61.721666666666664, 61.61666666666667, 61.705, 61.435, 61.7, 62.20333333333333, 61.98833333333334, 62.00333333333333, 62.25666666666667, 61.916666666666664, 62.26833333333333, 61.946666666666665, 62.295, 62.105, 62.605, 62.545, 62.321666666666665, 62.068333333333335, 62.63, 62.053333333333335, 61.985, 62.13, 62.47, 62.708333333333336, 62.251666666666665, 61.85333333333333, 62.14, 62.12833333333333, 62.4]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.204, Test loss: 2.065, Test accuracy: 24.27 

Round   1, Train loss: 2.010, Test loss: 1.923, Test accuracy: 30.27 

Round   2, Train loss: 1.899, Test loss: 1.780, Test accuracy: 35.47 

Round   3, Train loss: 1.781, Test loss: 1.687, Test accuracy: 38.25 

Round   4, Train loss: 1.707, Test loss: 1.633, Test accuracy: 39.81 

Round   5, Train loss: 1.660, Test loss: 1.568, Test accuracy: 42.33 

Round   6, Train loss: 1.603, Test loss: 1.540, Test accuracy: 44.16 

Round   7, Train loss: 1.536, Test loss: 1.494, Test accuracy: 45.82 

Round   8, Train loss: 1.520, Test loss: 1.493, Test accuracy: 45.86 

Round   9, Train loss: 1.471, Test loss: 1.432, Test accuracy: 48.17 

Round  10, Train loss: 1.420, Test loss: 1.414, Test accuracy: 48.47 

Round  11, Train loss: 1.424, Test loss: 1.386, Test accuracy: 50.23 

Round  12, Train loss: 1.371, Test loss: 1.363, Test accuracy: 50.67 

Round  13, Train loss: 1.341, Test loss: 1.353, Test accuracy: 51.51 

Round  14, Train loss: 1.314, Test loss: 1.322, Test accuracy: 52.43 

Round  15, Train loss: 1.341, Test loss: 1.304, Test accuracy: 52.84 

Round  16, Train loss: 1.280, Test loss: 1.282, Test accuracy: 53.80 

Round  17, Train loss: 1.206, Test loss: 1.280, Test accuracy: 54.23 

Round  18, Train loss: 1.213, Test loss: 1.257, Test accuracy: 54.96 

Round  19, Train loss: 1.180, Test loss: 1.246, Test accuracy: 55.73 

Round  20, Train loss: 1.129, Test loss: 1.234, Test accuracy: 56.39 

Round  21, Train loss: 1.118, Test loss: 1.217, Test accuracy: 56.72 

Round  22, Train loss: 1.089, Test loss: 1.223, Test accuracy: 56.91 

Round  23, Train loss: 1.066, Test loss: 1.208, Test accuracy: 57.67 

Round  24, Train loss: 1.064, Test loss: 1.210, Test accuracy: 57.21 

Round  25, Train loss: 1.041, Test loss: 1.196, Test accuracy: 58.34 

Round  26, Train loss: 0.993, Test loss: 1.213, Test accuracy: 57.71 

Round  27, Train loss: 0.988, Test loss: 1.197, Test accuracy: 58.74 

Round  28, Train loss: 0.973, Test loss: 1.205, Test accuracy: 58.85 

Round  29, Train loss: 0.978, Test loss: 1.197, Test accuracy: 58.69 

Round  30, Train loss: 0.951, Test loss: 1.204, Test accuracy: 58.90 

Round  31, Train loss: 0.928, Test loss: 1.199, Test accuracy: 59.21 

Round  32, Train loss: 0.921, Test loss: 1.193, Test accuracy: 59.87 

Round  33, Train loss: 0.857, Test loss: 1.212, Test accuracy: 59.62 

Round  34, Train loss: 0.842, Test loss: 1.192, Test accuracy: 59.89 

Round  35, Train loss: 0.835, Test loss: 1.207, Test accuracy: 59.80 

Round  36, Train loss: 0.831, Test loss: 1.215, Test accuracy: 60.49 

Round  37, Train loss: 0.849, Test loss: 1.207, Test accuracy: 60.09 

Round  38, Train loss: 0.881, Test loss: 1.204, Test accuracy: 60.06 

Round  39, Train loss: 0.818, Test loss: 1.200, Test accuracy: 60.48 

Round  40, Train loss: 0.767, Test loss: 1.208, Test accuracy: 60.64 

Round  41, Train loss: 0.780, Test loss: 1.214, Test accuracy: 60.40 

Round  42, Train loss: 0.773, Test loss: 1.213, Test accuracy: 60.86 

Round  43, Train loss: 0.718, Test loss: 1.255, Test accuracy: 60.03 

Round  44, Train loss: 0.734, Test loss: 1.252, Test accuracy: 60.44 

Round  45, Train loss: 0.702, Test loss: 1.250, Test accuracy: 60.52 

Round  46, Train loss: 0.731, Test loss: 1.220, Test accuracy: 61.23 

Round  47, Train loss: 0.689, Test loss: 1.242, Test accuracy: 60.95 

Round  48, Train loss: 0.671, Test loss: 1.261, Test accuracy: 61.24 

Round  49, Train loss: 0.633, Test loss: 1.281, Test accuracy: 60.77 

Round  50, Train loss: 0.668, Test loss: 1.305, Test accuracy: 61.00 

Round  51, Train loss: 0.702, Test loss: 1.283, Test accuracy: 61.00 

Round  52, Train loss: 0.646, Test loss: 1.282, Test accuracy: 61.18 

Round  53, Train loss: 0.678, Test loss: 1.289, Test accuracy: 61.44 

Round  54, Train loss: 0.632, Test loss: 1.280, Test accuracy: 61.24 

Round  55, Train loss: 0.653, Test loss: 1.313, Test accuracy: 61.41 

Round  56, Train loss: 0.653, Test loss: 1.315, Test accuracy: 61.25 

Round  57, Train loss: 0.617, Test loss: 1.318, Test accuracy: 61.50 

Round  58, Train loss: 0.570, Test loss: 1.364, Test accuracy: 61.32 

Round  59, Train loss: 0.612, Test loss: 1.316, Test accuracy: 61.51 

Round  60, Train loss: 0.582, Test loss: 1.357, Test accuracy: 61.07 

Round  61, Train loss: 0.580, Test loss: 1.348, Test accuracy: 61.59 

Round  62, Train loss: 0.573, Test loss: 1.341, Test accuracy: 61.57 

Round  63, Train loss: 0.528, Test loss: 1.384, Test accuracy: 61.11 

Round  64, Train loss: 0.518, Test loss: 1.380, Test accuracy: 61.50 

Round  65, Train loss: 0.534, Test loss: 1.400, Test accuracy: 61.32 

Round  66, Train loss: 0.581, Test loss: 1.395, Test accuracy: 61.24 

Round  67, Train loss: 0.514, Test loss: 1.410, Test accuracy: 61.06 

Round  68, Train loss: 0.538, Test loss: 1.417, Test accuracy: 61.27 

Round  69, Train loss: 0.499, Test loss: 1.432, Test accuracy: 61.79 

Round  70, Train loss: 0.529, Test loss: 1.428, Test accuracy: 61.02 

Round  71, Train loss: 0.487, Test loss: 1.457, Test accuracy: 61.30 

Round  72, Train loss: 0.458, Test loss: 1.471, Test accuracy: 61.19 

Round  73, Train loss: 0.469, Test loss: 1.465, Test accuracy: 61.27 

Round  74, Train loss: 0.494, Test loss: 1.461, Test accuracy: 61.54 

Round  75, Train loss: 0.465, Test loss: 1.448, Test accuracy: 61.39 

Round  76, Train loss: 0.453, Test loss: 1.458, Test accuracy: 61.65 

Round  77, Train loss: 0.431, Test loss: 1.509, Test accuracy: 61.52 

Round  78, Train loss: 0.485, Test loss: 1.485, Test accuracy: 61.21 

Round  79, Train loss: 0.436, Test loss: 1.538, Test accuracy: 61.30 

Round  80, Train loss: 0.457, Test loss: 1.554, Test accuracy: 60.97 

Round  81, Train loss: 0.421, Test loss: 1.560, Test accuracy: 61.06 

Round  82, Train loss: 0.472, Test loss: 1.544, Test accuracy: 60.93 

Round  83, Train loss: 0.486, Test loss: 1.533, Test accuracy: 61.34 

Round  84, Train loss: 0.451, Test loss: 1.558, Test accuracy: 61.00 

Round  85, Train loss: 0.456, Test loss: 1.543, Test accuracy: 61.38 

Round  86, Train loss: 0.501, Test loss: 1.568, Test accuracy: 61.32 

Round  87, Train loss: 0.429, Test loss: 1.529, Test accuracy: 61.48 

Round  88, Train loss: 0.488, Test loss: 1.560, Test accuracy: 61.40 

Round  89, Train loss: 0.445, Test loss: 1.602, Test accuracy: 61.24 

Round  90, Train loss: 0.404, Test loss: 1.590, Test accuracy: 61.36 

Round  91, Train loss: 0.423, Test loss: 1.539, Test accuracy: 61.27 

Round  92, Train loss: 0.392, Test loss: 1.585, Test accuracy: 61.35 

Round  93, Train loss: 0.383, Test loss: 1.624, Test accuracy: 61.60 

Round  94, Train loss: 0.384, Test loss: 1.635, Test accuracy: 61.65 

Round  95, Train loss: 0.440, Test loss: 1.596, Test accuracy: 61.19 

Round  96, Train loss: 0.351, Test loss: 1.651, Test accuracy: 61.50 

Round  97, Train loss: 0.389, Test loss: 1.658, Test accuracy: 61.09 

Round  98, Train loss: 0.396, Test loss: 1.626, Test accuracy: 61.44 

Round  99, Train loss: 0.368, Test loss: 1.684, Test accuracy: 61.11 

Final Round, Train loss: 0.323, Test loss: 1.693, Test accuracy: 61.01 

Average accuracy final 10 rounds: 61.354499999999994 

2144.549221277237
[1.421419382095337, 2.6139066219329834, 3.6600916385650635, 4.74504017829895, 5.83903694152832, 6.919524192810059, 7.987534284591675, 9.028585195541382, 10.071781396865845, 11.171489477157593, 12.246649980545044, 13.315767288208008, 14.402595281600952, 19.735239505767822, 20.74830913543701, 21.76162052154541, 22.77318000793457, 23.788278341293335, 24.801318168640137, 25.81744956970215, 26.83296251296997, 27.881223440170288, 28.927148580551147, 29.97541093826294, 31.006531238555908, 32.0230770111084, 33.03856086730957, 34.05407381057739, 35.069687366485596, 36.08390188217163, 37.10645627975464, 38.12036943435669, 39.1365647315979, 40.155694246292114, 41.16866993904114, 42.20071363449097, 43.217488288879395, 44.23319435119629, 45.24675416946411, 46.26701641082764, 47.31939101219177, 48.37635374069214, 49.41633439064026, 50.4602484703064, 51.4883930683136, 52.503031730651855, 53.51271390914917, 54.524978160858154, 55.53698110580444, 56.55107140541077, 57.56407022476196, 58.57572364807129, 59.5886435508728, 60.59802293777466, 61.61096405982971, 62.62133002281189, 63.63500690460205, 64.6461238861084, 65.6581244468689, 66.66794633865356, 67.67813396453857, 68.69092559814453, 69.70226311683655, 70.71138381958008, 71.72184801101685, 72.73561453819275, 73.74642515182495, 74.75784492492676, 75.7884681224823, 76.83546328544617, 77.85060930252075, 78.87072944641113, 79.87882113456726, 80.87198090553284, 81.86703324317932, 82.86019277572632, 83.86411595344543, 84.8577446937561, 85.856036901474, 86.85529232025146, 87.85313868522644, 88.84960985183716, 89.84530115127563, 90.84133219718933, 91.83489942550659, 92.8294107913971, 93.82354259490967, 94.82669687271118, 95.83082389831543, 96.82492518424988, 97.8194944858551, 98.81558799743652, 99.81249785423279, 100.81262159347534, 101.80710220336914, 102.80205130577087, 103.79664301872253, 104.79350781440735, 105.7905330657959, 106.78500175476074, 108.54847478866577]
[24.265, 30.27, 35.47, 38.248333333333335, 39.81166666666667, 42.325, 44.15833333333333, 45.821666666666665, 45.86, 48.17333333333333, 48.46666666666667, 50.235, 50.66833333333334, 51.513333333333335, 52.43, 52.843333333333334, 53.805, 54.23166666666667, 54.958333333333336, 55.73166666666667, 56.39, 56.718333333333334, 56.91166666666667, 57.67166666666667, 57.211666666666666, 58.345, 57.71333333333333, 58.73833333333334, 58.848333333333336, 58.69, 58.89833333333333, 59.211666666666666, 59.865, 59.61666666666667, 59.891666666666666, 59.79666666666667, 60.48833333333334, 60.093333333333334, 60.06166666666667, 60.47833333333333, 60.64, 60.395, 60.861666666666665, 60.026666666666664, 60.443333333333335, 60.52333333333333, 61.233333333333334, 60.95, 61.245, 60.77166666666667, 61.0, 61.001666666666665, 61.17666666666667, 61.43833333333333, 61.236666666666665, 61.415, 61.25333333333333, 61.498333333333335, 61.321666666666665, 61.51, 61.07, 61.58833333333333, 61.571666666666665, 61.10666666666667, 61.5, 61.321666666666665, 61.236666666666665, 61.06333333333333, 61.27, 61.788333333333334, 61.02, 61.295, 61.193333333333335, 61.275, 61.53666666666667, 61.39, 61.64833333333333, 61.52166666666667, 61.208333333333336, 61.29666666666667, 60.96666666666667, 61.06333333333333, 60.92666666666667, 61.34, 61.00333333333333, 61.385, 61.32, 61.48166666666667, 61.403333333333336, 61.236666666666665, 61.36, 61.27, 61.348333333333336, 61.596666666666664, 61.645, 61.185, 61.5, 61.09166666666667, 61.43833333333333, 61.11, 61.01]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.225, Test loss: 2.105, Test accuracy: 21.67 

Round   1, Train loss: 2.026, Test loss: 1.959, Test accuracy: 28.47 

Round   2, Train loss: 1.892, Test loss: 1.865, Test accuracy: 31.73 

Round   3, Train loss: 1.830, Test loss: 1.812, Test accuracy: 34.00 

Round   4, Train loss: 1.721, Test loss: 1.767, Test accuracy: 35.61 

Round   5, Train loss: 1.734, Test loss: 1.755, Test accuracy: 36.63 

Round   6, Train loss: 1.695, Test loss: 1.724, Test accuracy: 37.57 

Round   7, Train loss: 1.600, Test loss: 1.711, Test accuracy: 38.03 

Round   8, Train loss: 1.499, Test loss: 1.719, Test accuracy: 38.55 

Round   9, Train loss: 1.513, Test loss: 1.706, Test accuracy: 39.03 

Round  10, Train loss: 1.472, Test loss: 1.693, Test accuracy: 39.63 

Round  11, Train loss: 1.371, Test loss: 1.692, Test accuracy: 39.96 

Round  12, Train loss: 1.367, Test loss: 1.691, Test accuracy: 39.96 

Round  13, Train loss: 1.386, Test loss: 1.705, Test accuracy: 40.35 

Round  14, Train loss: 1.402, Test loss: 1.703, Test accuracy: 40.48 

Round  15, Train loss: 1.279, Test loss: 1.700, Test accuracy: 40.70 

Round  16, Train loss: 1.232, Test loss: 1.727, Test accuracy: 40.99 

Round  17, Train loss: 1.241, Test loss: 1.750, Test accuracy: 40.85 

Round  18, Train loss: 1.212, Test loss: 1.730, Test accuracy: 41.63 

Round  19, Train loss: 1.139, Test loss: 1.770, Test accuracy: 41.48 

Round  20, Train loss: 1.070, Test loss: 1.797, Test accuracy: 41.81 

Round  21, Train loss: 1.082, Test loss: 1.823, Test accuracy: 41.61 

Round  22, Train loss: 1.032, Test loss: 1.864, Test accuracy: 41.94 

Round  23, Train loss: 1.089, Test loss: 1.909, Test accuracy: 41.89 

Round  24, Train loss: 1.069, Test loss: 1.891, Test accuracy: 41.87 

Round  25, Train loss: 0.986, Test loss: 1.945, Test accuracy: 41.83 

Round  26, Train loss: 0.876, Test loss: 1.986, Test accuracy: 41.62 

Round  27, Train loss: 0.912, Test loss: 2.026, Test accuracy: 41.59 

Round  28, Train loss: 0.824, Test loss: 2.072, Test accuracy: 41.49 

Round  29, Train loss: 0.753, Test loss: 2.063, Test accuracy: 41.71 

Round  30, Train loss: 0.777, Test loss: 2.131, Test accuracy: 41.59 

Round  31, Train loss: 0.768, Test loss: 2.158, Test accuracy: 41.98 

Round  32, Train loss: 0.664, Test loss: 2.208, Test accuracy: 41.86 

Round  33, Train loss: 0.708, Test loss: 2.232, Test accuracy: 42.28 

Round  34, Train loss: 0.633, Test loss: 2.270, Test accuracy: 42.38 

Round  35, Train loss: 0.621, Test loss: 2.311, Test accuracy: 41.94 

Round  36, Train loss: 0.652, Test loss: 2.327, Test accuracy: 41.80 

Round  37, Train loss: 0.580, Test loss: 2.382, Test accuracy: 41.91 

Round  38, Train loss: 0.576, Test loss: 2.399, Test accuracy: 42.05 

Round  39, Train loss: 0.567, Test loss: 2.431, Test accuracy: 41.88 

Round  40, Train loss: 0.528, Test loss: 2.457, Test accuracy: 42.13 

Round  41, Train loss: 0.519, Test loss: 2.477, Test accuracy: 42.12 

Round  42, Train loss: 0.520, Test loss: 2.456, Test accuracy: 42.47 

Round  43, Train loss: 0.466, Test loss: 2.478, Test accuracy: 42.12 

Round  44, Train loss: 0.466, Test loss: 2.544, Test accuracy: 41.99 

Round  45, Train loss: 0.472, Test loss: 2.626, Test accuracy: 42.05 

Round  46, Train loss: 0.441, Test loss: 2.654, Test accuracy: 42.20 

Round  47, Train loss: 0.410, Test loss: 2.691, Test accuracy: 42.54 

Round  48, Train loss: 0.397, Test loss: 2.732, Test accuracy: 42.33 

Round  49, Train loss: 0.414, Test loss: 2.806, Test accuracy: 42.34 

Round  50, Train loss: 0.352, Test loss: 2.868, Test accuracy: 42.34 

Round  51, Train loss: 0.365, Test loss: 2.859, Test accuracy: 42.24 

Round  52, Train loss: 0.394, Test loss: 2.905, Test accuracy: 42.23 

Round  53, Train loss: 0.380, Test loss: 2.926, Test accuracy: 42.09 

Round  54, Train loss: 0.317, Test loss: 2.953, Test accuracy: 42.17 

Round  55, Train loss: 0.336, Test loss: 3.002, Test accuracy: 42.25 

Round  56, Train loss: 0.379, Test loss: 3.010, Test accuracy: 42.41 

Round  57, Train loss: 0.329, Test loss: 3.082, Test accuracy: 42.56 

Round  58, Train loss: 0.284, Test loss: 3.139, Test accuracy: 42.27 

Round  59, Train loss: 0.281, Test loss: 3.147, Test accuracy: 42.52 

Round  60, Train loss: 0.331, Test loss: 3.199, Test accuracy: 42.58 

Round  61, Train loss: 0.280, Test loss: 3.231, Test accuracy: 42.60 

Round  62, Train loss: 0.230, Test loss: 3.229, Test accuracy: 42.26 

Round  63, Train loss: 0.277, Test loss: 3.260, Test accuracy: 42.53 

Round  64, Train loss: 0.298, Test loss: 3.309, Test accuracy: 42.59 

Round  65, Train loss: 0.250, Test loss: 3.315, Test accuracy: 42.58 

Round  66, Train loss: 0.265, Test loss: 3.332, Test accuracy: 42.53 

Round  67, Train loss: 0.232, Test loss: 3.367, Test accuracy: 42.25 

Round  68, Train loss: 0.218, Test loss: 3.377, Test accuracy: 42.62 

Round  69, Train loss: 0.210, Test loss: 3.419, Test accuracy: 42.43 

Round  70, Train loss: 0.231, Test loss: 3.414, Test accuracy: 42.78 

Round  71, Train loss: 0.200, Test loss: 3.492, Test accuracy: 42.28 

Round  72, Train loss: 0.195, Test loss: 3.493, Test accuracy: 42.26 

Round  73, Train loss: 0.226, Test loss: 3.533, Test accuracy: 42.54 

Round  74, Train loss: 0.211, Test loss: 3.517, Test accuracy: 42.79 

Round  75, Train loss: 0.210, Test loss: 3.539, Test accuracy: 42.62 

Round  76, Train loss: 0.195, Test loss: 3.583, Test accuracy: 42.61 

Round  77, Train loss: 0.200, Test loss: 3.567, Test accuracy: 42.76 

Round  78, Train loss: 0.215, Test loss: 3.661, Test accuracy: 42.44 

Round  79, Train loss: 0.156, Test loss: 3.710, Test accuracy: 42.74 

Round  80, Train loss: 0.193, Test loss: 3.703, Test accuracy: 42.96 

Round  81, Train loss: 0.167, Test loss: 3.760, Test accuracy: 42.87 

Round  82, Train loss: 0.171, Test loss: 3.842, Test accuracy: 42.78 

Round  83, Train loss: 0.205, Test loss: 3.746, Test accuracy: 42.71 

Round  84, Train loss: 0.160, Test loss: 3.778, Test accuracy: 42.76 

Round  85, Train loss: 0.146, Test loss: 3.822, Test accuracy: 42.57 

Round  86, Train loss: 0.150, Test loss: 3.840, Test accuracy: 42.60 

Round  87, Train loss: 0.168, Test loss: 3.855, Test accuracy: 42.65 

Round  88, Train loss: 0.155, Test loss: 3.856, Test accuracy: 42.70 

Round  89, Train loss: 0.125, Test loss: 3.944, Test accuracy: 42.45 

Round  90, Train loss: 0.130, Test loss: 3.910, Test accuracy: 42.70 

Round  91, Train loss: 0.169, Test loss: 3.893, Test accuracy: 42.87 

Round  92, Train loss: 0.139, Test loss: 3.904, Test accuracy: 43.02 

Round  93, Train loss: 0.123, Test loss: 3.868, Test accuracy: 43.27 

Round  94, Train loss: 0.165, Test loss: 3.887, Test accuracy: 43.05 

Round  95, Train loss: 0.156, Test loss: 3.919, Test accuracy: 42.86 

Round  96, Train loss: 0.105, Test loss: 3.972, Test accuracy: 42.85 

Round  97, Train loss: 0.175, Test loss: 3.971, Test accuracy: 42.73 

Round  98, Train loss: 0.128, Test loss: 4.029, Test accuracy: 42.66 

Round  99, Train loss: 0.113, Test loss: 4.093, Test accuracy: 42.65 

Final Round, Train loss: 0.097, Test loss: 4.328, Test accuracy: 43.01 

Average accuracy final 10 rounds: 42.865 

2231.336713552475
[1.3304238319396973, 2.4322867393493652, 3.5476365089416504, 4.795315265655518, 6.0762269496917725, 7.342791795730591, 8.590996265411377, 9.843725204467773, 11.086467742919922, 12.334092140197754, 13.590384721755981, 14.837871551513672, 16.083794593811035, 17.333787202835083, 18.530807495117188, 19.77526545524597, 21.032931327819824, 22.291165590286255, 23.547566413879395, 24.800355911254883, 26.058130979537964, 27.299522161483765, 28.538647890090942, 29.790956258773804, 31.033756256103516, 32.276204347610474, 33.52128887176514, 34.7653443813324, 36.0078558921814, 37.25276064872742, 38.49959421157837, 39.75206208229065, 41.00242066383362, 42.24808096885681, 43.49965047836304, 44.747740030288696, 45.98686456680298, 47.225762605667114, 48.47893667221069, 49.742642641067505, 50.97808599472046, 52.22487187385559, 53.467204570770264, 54.71454334259033, 55.9639310836792, 57.216195583343506, 58.481571435928345, 59.752849817276, 61.00271034240723, 62.25337195396423, 63.51900291442871, 64.76980996131897, 66.0211112499237, 67.28166270256042, 68.54003953933716, 69.79621267318726, 71.04152846336365, 72.29684019088745, 73.56320929527283, 74.82069230079651, 76.10153460502625, 77.3603904247284, 78.6125020980835, 79.85984134674072, 81.11162424087524, 82.36591410636902, 83.61317491531372, 84.86127877235413, 86.1153678894043, 87.36397528648376, 88.61575889587402, 89.86212801933289, 91.11060357093811, 92.36115789413452, 93.60886979103088, 94.86379837989807, 96.11482858657837, 97.36033821105957, 98.607492685318, 99.8533353805542, 101.10254859924316, 102.3474543094635, 103.59087109565735, 104.83994626998901, 106.09079551696777, 107.33740210533142, 108.5689902305603, 109.80458211898804, 111.04170346260071, 112.27787804603577, 113.51797318458557, 114.76122212409973, 116.0022337436676, 117.23760962486267, 118.476149559021, 119.71525239944458, 120.95678758621216, 122.19762015342712, 123.44538450241089, 124.69109463691711, 127.00450348854065]
[21.671666666666667, 28.471666666666668, 31.733333333333334, 34.00333333333333, 35.611666666666665, 36.63333333333333, 37.568333333333335, 38.028333333333336, 38.553333333333335, 39.03, 39.62833333333333, 39.96333333333333, 39.961666666666666, 40.35, 40.47666666666667, 40.705, 40.99333333333333, 40.85333333333333, 41.63166666666667, 41.47833333333333, 41.80833333333333, 41.611666666666665, 41.935, 41.89333333333333, 41.86666666666667, 41.833333333333336, 41.61833333333333, 41.59, 41.49333333333333, 41.708333333333336, 41.59, 41.97666666666667, 41.86, 42.28333333333333, 42.37833333333333, 41.935, 41.79666666666667, 41.90833333333333, 42.05166666666667, 41.88166666666667, 42.13, 42.11833333333333, 42.473333333333336, 42.12166666666667, 41.99, 42.04833333333333, 42.2, 42.54, 42.325, 42.34166666666667, 42.343333333333334, 42.245, 42.23, 42.085, 42.17333333333333, 42.248333333333335, 42.41, 42.565, 42.26833333333333, 42.516666666666666, 42.575, 42.6, 42.25666666666667, 42.53333333333333, 42.59166666666667, 42.58, 42.528333333333336, 42.24666666666667, 42.61833333333333, 42.428333333333335, 42.776666666666664, 42.278333333333336, 42.26166666666666, 42.54333333333334, 42.788333333333334, 42.623333333333335, 42.608333333333334, 42.755, 42.443333333333335, 42.74, 42.95666666666666, 42.865, 42.776666666666664, 42.70666666666666, 42.76166666666666, 42.568333333333335, 42.60166666666667, 42.645, 42.70333333333333, 42.445, 42.696666666666665, 42.87166666666667, 43.02333333333333, 43.265, 43.04833333333333, 42.85666666666667, 42.85333333333333, 42.72666666666667, 42.66, 42.64833333333333, 43.00666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 1.404, Test loss: 1.841, Test accuracy: 34.58
Round   1, Train loss: 1.212, Test loss: 1.858, Test accuracy: 32.29
Round   2, Train loss: 1.089, Test loss: 1.881, Test accuracy: 32.08
Round   3, Train loss: 1.008, Test loss: 1.893, Test accuracy: 32.29
Round   4, Train loss: 0.982, Test loss: 2.023, Test accuracy: 26.53
Round   5, Train loss: 0.916, Test loss: 2.005, Test accuracy: 26.89
Round   6, Train loss: 0.860, Test loss: 1.995, Test accuracy: 27.04
Round   7, Train loss: 0.837, Test loss: 1.979, Test accuracy: 27.94
Round   8, Train loss: 0.789, Test loss: 2.021, Test accuracy: 26.53
Round   9, Train loss: 0.764, Test loss: 2.067, Test accuracy: 24.90
Round  10, Train loss: 0.698, Test loss: 2.056, Test accuracy: 26.11
Round  11, Train loss: 0.670, Test loss: 2.046, Test accuracy: 26.97
Round  12, Train loss: 0.658, Test loss: 2.043, Test accuracy: 26.86
Round  13, Train loss: 0.670, Test loss: 2.041, Test accuracy: 27.53
Round  14, Train loss: 0.601, Test loss: 2.029, Test accuracy: 28.57
Round  15, Train loss: 0.553, Test loss: 2.017, Test accuracy: 29.31
Round  16, Train loss: 0.590, Test loss: 2.022, Test accuracy: 29.20
Round  17, Train loss: 0.545, Test loss: 2.010, Test accuracy: 30.64
Round  18, Train loss: 0.536, Test loss: 2.011, Test accuracy: 29.82
Round  19, Train loss: 0.501, Test loss: 2.006, Test accuracy: 29.70
Round  20, Train loss: 0.493, Test loss: 1.994, Test accuracy: 30.12
Round  21, Train loss: 0.444, Test loss: 1.987, Test accuracy: 31.19
Round  22, Train loss: 0.498, Test loss: 1.982, Test accuracy: 30.88
Round  23, Train loss: 0.463, Test loss: 1.978, Test accuracy: 31.67
Round  24, Train loss: 0.420, Test loss: 1.966, Test accuracy: 32.84
Round  25, Train loss: 0.468, Test loss: 1.958, Test accuracy: 33.27
Round  26, Train loss: 0.415, Test loss: 1.943, Test accuracy: 34.28
Round  27, Train loss: 0.411, Test loss: 1.939, Test accuracy: 34.55
Round  28, Train loss: 0.371, Test loss: 1.932, Test accuracy: 34.48
Round  29, Train loss: 0.384, Test loss: 1.926, Test accuracy: 34.78
Round  30, Train loss: 0.411, Test loss: 1.929, Test accuracy: 33.72
Round  31, Train loss: 0.344, Test loss: 1.920, Test accuracy: 34.83
Round  32, Train loss: 0.366, Test loss: 1.914, Test accuracy: 35.91
Round  33, Train loss: 0.350, Test loss: 1.904, Test accuracy: 36.75
Round  34, Train loss: 0.335, Test loss: 1.901, Test accuracy: 36.85
Round  35, Train loss: 0.324, Test loss: 1.893, Test accuracy: 37.23
Round  36, Train loss: 0.339, Test loss: 1.888, Test accuracy: 37.32
Round  37, Train loss: 0.327, Test loss: 1.885, Test accuracy: 37.03
Round  38, Train loss: 0.350, Test loss: 1.875, Test accuracy: 38.17
Round  39, Train loss: 0.294, Test loss: 1.867, Test accuracy: 38.48
Round  40, Train loss: 0.311, Test loss: 1.857, Test accuracy: 39.11
Round  41, Train loss: 0.281, Test loss: 1.851, Test accuracy: 39.19
Round  42, Train loss: 0.266, Test loss: 1.839, Test accuracy: 39.70
Round  43, Train loss: 0.274, Test loss: 1.843, Test accuracy: 39.72
Round  44, Train loss: 0.257, Test loss: 1.845, Test accuracy: 39.09
Round  45, Train loss: 0.281, Test loss: 1.837, Test accuracy: 39.13
Round  46, Train loss: 0.257, Test loss: 1.829, Test accuracy: 40.53
Round  47, Train loss: 0.268, Test loss: 1.836, Test accuracy: 39.57
Round  48, Train loss: 0.261, Test loss: 1.826, Test accuracy: 40.12
Round  49, Train loss: 0.248, Test loss: 1.819, Test accuracy: 40.14
Round  50, Train loss: 0.252, Test loss: 1.817, Test accuracy: 40.33
Round  51, Train loss: 0.241, Test loss: 1.811, Test accuracy: 40.47
Round  52, Train loss: 0.231, Test loss: 1.819, Test accuracy: 39.78
Round  53, Train loss: 0.236, Test loss: 1.812, Test accuracy: 40.25
Round  54, Train loss: 0.232, Test loss: 1.811, Test accuracy: 40.18
Round  55, Train loss: 0.222, Test loss: 1.803, Test accuracy: 41.04
Round  56, Train loss: 0.216, Test loss: 1.796, Test accuracy: 41.34
Round  57, Train loss: 0.228, Test loss: 1.791, Test accuracy: 40.96
Round  58, Train loss: 0.215, Test loss: 1.786, Test accuracy: 41.53
Round  59, Train loss: 0.223, Test loss: 1.782, Test accuracy: 41.58
Round  60, Train loss: 0.213, Test loss: 1.781, Test accuracy: 41.66
Round  61, Train loss: 0.198, Test loss: 1.780, Test accuracy: 41.40
Round  62, Train loss: 0.199, Test loss: 1.784, Test accuracy: 40.92
Round  63, Train loss: 0.229, Test loss: 1.780, Test accuracy: 40.70
Round  64, Train loss: 0.198, Test loss: 1.780, Test accuracy: 40.64
Round  65, Train loss: 0.195, Test loss: 1.773, Test accuracy: 41.20
Round  66, Train loss: 0.196, Test loss: 1.764, Test accuracy: 42.17
Round  67, Train loss: 0.189, Test loss: 1.759, Test accuracy: 41.73
Round  68, Train loss: 0.189, Test loss: 1.753, Test accuracy: 41.83
Round  69, Train loss: 0.196, Test loss: 1.741, Test accuracy: 42.62
Round  70, Train loss: 0.189, Test loss: 1.750, Test accuracy: 41.89
Round  71, Train loss: 0.181, Test loss: 1.748, Test accuracy: 42.31
Round  72, Train loss: 0.182, Test loss: 1.734, Test accuracy: 43.22
Round  73, Train loss: 0.194, Test loss: 1.737, Test accuracy: 42.69
Round  74, Train loss: 0.178, Test loss: 1.725, Test accuracy: 43.42
Round  75, Train loss: 0.188, Test loss: 1.729, Test accuracy: 43.12
Round  76, Train loss: 0.189, Test loss: 1.725, Test accuracy: 43.56
Round  77, Train loss: 0.179, Test loss: 1.721, Test accuracy: 43.68
Round  78, Train loss: 0.169, Test loss: 1.718, Test accuracy: 43.61
Round  79, Train loss: 0.178, Test loss: 1.722, Test accuracy: 43.05
Round  80, Train loss: 0.187, Test loss: 1.715, Test accuracy: 43.54
Round  81, Train loss: 0.170, Test loss: 1.720, Test accuracy: 42.76
Round  82, Train loss: 0.164, Test loss: 1.714, Test accuracy: 42.92
Round  83, Train loss: 0.163, Test loss: 1.709, Test accuracy: 43.18
Round  84, Train loss: 0.170, Test loss: 1.701, Test accuracy: 43.59
Round  85, Train loss: 0.161, Test loss: 1.702, Test accuracy: 43.57
Round  86, Train loss: 0.159, Test loss: 1.693, Test accuracy: 43.98
Round  87, Train loss: 0.165, Test loss: 1.695, Test accuracy: 43.93
Round  88, Train loss: 0.170, Test loss: 1.687, Test accuracy: 44.33
Round  89, Train loss: 0.163, Test loss: 1.700, Test accuracy: 44.10
Round  90, Train loss: 0.164, Test loss: 1.695, Test accuracy: 44.48
Round  91, Train loss: 0.151, Test loss: 1.688, Test accuracy: 44.88
Round  92, Train loss: 0.152, Test loss: 1.680, Test accuracy: 45.28
Round  93, Train loss: 0.153, Test loss: 1.679, Test accuracy: 45.10
Round  94, Train loss: 0.149, Test loss: 1.675, Test accuracy: 45.28
Round  95, Train loss: 0.158, Test loss: 1.685, Test accuracy: 44.31
Round  96, Train loss: 0.152, Test loss: 1.667, Test accuracy: 45.48
Round  97, Train loss: 0.151, Test loss: 1.665, Test accuracy: 45.30
Round  98, Train loss: 0.150, Test loss: 1.674, Test accuracy: 44.54
Round  99, Train loss: 0.145, Test loss: 1.675, Test accuracy: 44.34
Final Round, Train loss: 0.150, Test loss: 1.672, Test accuracy: 44.93
Average accuracy final 10 rounds: 44.90016666666667
8445.137856006622
[]
[34.58166666666666, 32.291666666666664, 32.08166666666666, 32.29, 26.53, 26.885, 27.041666666666668, 27.94333333333333, 26.526666666666667, 24.89666666666667, 26.108333333333334, 26.97, 26.855, 27.526666666666667, 28.57, 29.308333333333334, 29.198333333333334, 30.641666666666666, 29.823333333333334, 29.698333333333334, 30.125, 31.19333333333333, 30.876666666666665, 31.668333333333333, 32.84, 33.27166666666667, 34.28, 34.55, 34.47833333333333, 34.78333333333333, 33.723333333333336, 34.83166666666666, 35.905, 36.75, 36.848333333333336, 37.233333333333334, 37.321666666666665, 37.031666666666666, 38.166666666666664, 38.485, 39.11333333333334, 39.18833333333333, 39.695, 39.721666666666664, 39.093333333333334, 39.13166666666667, 40.53, 39.57, 40.11833333333333, 40.14, 40.33, 40.465, 39.78, 40.25, 40.17666666666667, 41.04333333333334, 41.345, 40.95666666666666, 41.53333333333333, 41.575, 41.66, 41.395, 40.92166666666667, 40.705, 40.641666666666666, 41.20166666666667, 42.17166666666667, 41.735, 41.833333333333336, 42.615, 41.88666666666666, 42.306666666666665, 43.215, 42.68833333333333, 43.41833333333334, 43.115, 43.56333333333333, 43.68333333333333, 43.60666666666667, 43.05166666666667, 43.54333333333334, 42.763333333333335, 42.91833333333334, 43.181666666666665, 43.595, 43.571666666666665, 43.97666666666667, 43.92666666666667, 44.32666666666667, 44.10166666666667, 44.485, 44.885, 45.278333333333336, 45.098333333333336, 45.281666666666666, 44.31333333333333, 45.48, 45.29666666666667, 44.541666666666664, 44.34166666666667, 44.93333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.907, Test loss: 2.087, Test accuracy: 19.26
Round   0: Global train loss: 1.907, Global test loss: 2.291, Global test accuracy: 10.11
Round   1, Train loss: 1.615, Test loss: 1.934, Test accuracy: 27.55
Round   1: Global train loss: 1.615, Global test loss: 2.277, Global test accuracy: 13.81
Round   2, Train loss: 1.353, Test loss: 1.808, Test accuracy: 34.09
Round   2: Global train loss: 1.353, Global test loss: 2.259, Global test accuracy: 18.95
Round   3, Train loss: 1.088, Test loss: 1.683, Test accuracy: 39.21
Round   3: Global train loss: 1.088, Global test loss: 2.237, Global test accuracy: 21.25
Round   4, Train loss: 1.094, Test loss: 1.621, Test accuracy: 42.60
Round   4: Global train loss: 1.094, Global test loss: 2.213, Global test accuracy: 26.84
Round   5, Train loss: 0.704, Test loss: 1.570, Test accuracy: 44.09
Round   5: Global train loss: 0.704, Global test loss: 2.202, Global test accuracy: 26.65
Round   6, Train loss: 0.500, Test loss: 1.519, Test accuracy: 46.13
Round   6: Global train loss: 0.500, Global test loss: 2.170, Global test accuracy: 30.94
Round   7, Train loss: 1.187, Test loss: 1.520, Test accuracy: 46.10
Round   7: Global train loss: 1.187, Global test loss: 2.148, Global test accuracy: 33.73
Round   8, Train loss: 0.436, Test loss: 1.443, Test accuracy: 48.91
Round   8: Global train loss: 0.436, Global test loss: 2.116, Global test accuracy: 36.77
Round   9, Train loss: 0.419, Test loss: 1.457, Test accuracy: 48.85
Round   9: Global train loss: 0.419, Global test loss: 2.090, Global test accuracy: 38.48
Round  10, Train loss: 0.263, Test loss: 1.424, Test accuracy: 49.50
Round  10: Global train loss: 0.263, Global test loss: 2.062, Global test accuracy: 39.60
Round  11, Train loss: -0.158, Test loss: 1.387, Test accuracy: 50.67
Round  11: Global train loss: -0.158, Global test loss: 2.034, Global test accuracy: 41.52
Round  12, Train loss: -0.207, Test loss: 1.370, Test accuracy: 51.38
Round  12: Global train loss: -0.207, Global test loss: 2.001, Global test accuracy: 43.06
Round  13, Train loss: -0.258, Test loss: 1.354, Test accuracy: 52.31
Round  13: Global train loss: -0.258, Global test loss: 1.968, Global test accuracy: 43.77
Round  14, Train loss: -0.780, Test loss: 1.337, Test accuracy: 53.08
Round  14: Global train loss: -0.780, Global test loss: 1.938, Global test accuracy: 45.52
Round  15, Train loss: -1.139, Test loss: 1.350, Test accuracy: 52.84
Round  15: Global train loss: -1.139, Global test loss: 1.911, Global test accuracy: 45.25
Round  16, Train loss: -1.076, Test loss: 1.341, Test accuracy: 53.50
Round  16: Global train loss: -1.076, Global test loss: 1.873, Global test accuracy: 46.98
Round  17, Train loss: -0.537, Test loss: 1.340, Test accuracy: 53.87
Round  17: Global train loss: -0.537, Global test loss: 1.837, Global test accuracy: 48.65
Round  18, Train loss: -0.989, Test loss: 1.351, Test accuracy: 53.39
Round  18: Global train loss: -0.989, Global test loss: 1.799, Global test accuracy: 49.88
Round  19, Train loss: -1.135, Test loss: 1.337, Test accuracy: 53.88
Round  19: Global train loss: -1.135, Global test loss: 1.777, Global test accuracy: 50.08
Round  20, Train loss: -1.168, Test loss: 1.316, Test accuracy: 54.83
Round  20: Global train loss: -1.168, Global test loss: 1.753, Global test accuracy: 50.63
Round  21, Train loss: -2.025, Test loss: 1.317, Test accuracy: 55.15
Round  21: Global train loss: -2.025, Global test loss: 1.721, Global test accuracy: 51.56
Round  22, Train loss: -1.636, Test loss: 1.305, Test accuracy: 55.64
Round  22: Global train loss: -1.636, Global test loss: 1.693, Global test accuracy: 52.65
Round  23, Train loss: -0.226, Test loss: 1.313, Test accuracy: 55.10
Round  23: Global train loss: -0.226, Global test loss: 1.685, Global test accuracy: 52.43
Round  24, Train loss: -2.266, Test loss: 1.303, Test accuracy: 55.83
Round  24: Global train loss: -2.266, Global test loss: 1.667, Global test accuracy: 52.27
Round  25, Train loss: -2.505, Test loss: 1.286, Test accuracy: 55.86
Round  25: Global train loss: -2.505, Global test loss: 1.649, Global test accuracy: 52.53
Round  26, Train loss: -2.328, Test loss: 1.285, Test accuracy: 55.86
Round  26: Global train loss: -2.328, Global test loss: 1.634, Global test accuracy: 52.96
Round  27, Train loss: -2.083, Test loss: 1.288, Test accuracy: 55.87
Round  27: Global train loss: -2.083, Global test loss: 1.619, Global test accuracy: 53.13
Round  28, Train loss: -2.344, Test loss: 1.294, Test accuracy: 55.77
Round  28: Global train loss: -2.344, Global test loss: 1.595, Global test accuracy: 53.70
Round  29, Train loss: -2.701, Test loss: 1.305, Test accuracy: 55.67
Round  29: Global train loss: -2.701, Global test loss: 1.582, Global test accuracy: 53.97
Round  30, Train loss: -3.185, Test loss: 1.301, Test accuracy: 56.26
Round  30: Global train loss: -3.185, Global test loss: 1.561, Global test accuracy: 54.67
Round  31, Train loss: -2.526, Test loss: 1.291, Test accuracy: 56.59
Round  31: Global train loss: -2.526, Global test loss: 1.543, Global test accuracy: 55.17
Round  32, Train loss: -2.346, Test loss: 1.296, Test accuracy: 56.67
Round  32: Global train loss: -2.346, Global test loss: 1.525, Global test accuracy: 55.61
Round  33, Train loss: -2.474, Test loss: 1.294, Test accuracy: 57.04
Round  33: Global train loss: -2.474, Global test loss: 1.498, Global test accuracy: 56.66
Round  34, Train loss: -2.876, Test loss: 1.300, Test accuracy: 57.14
Round  34: Global train loss: -2.876, Global test loss: 1.470, Global test accuracy: 57.76
Round  35, Train loss: -3.298, Test loss: 1.285, Test accuracy: 57.77
Round  35: Global train loss: -3.298, Global test loss: 1.441, Global test accuracy: 58.61
Round  36, Train loss: -2.798, Test loss: 1.282, Test accuracy: 57.91
Round  36: Global train loss: -2.798, Global test loss: 1.429, Global test accuracy: 58.99
Round  37, Train loss: -2.266, Test loss: 1.283, Test accuracy: 57.62
Round  37: Global train loss: -2.266, Global test loss: 1.422, Global test accuracy: 58.95
Round  38, Train loss: -2.683, Test loss: 1.282, Test accuracy: 57.52
Round  38: Global train loss: -2.683, Global test loss: 1.400, Global test accuracy: 59.77
Round  39, Train loss: -2.804, Test loss: 1.287, Test accuracy: 57.43
Round  39: Global train loss: -2.804, Global test loss: 1.382, Global test accuracy: 60.02
Round  40, Train loss: -3.019, Test loss: 1.292, Test accuracy: 57.42
Round  40: Global train loss: -3.019, Global test loss: 1.372, Global test accuracy: 60.27
Round  41, Train loss: -3.546, Test loss: 1.292, Test accuracy: 57.42
Round  41: Global train loss: -3.546, Global test loss: 1.354, Global test accuracy: 60.73
Round  42, Train loss: -3.076, Test loss: 1.285, Test accuracy: 57.64
Round  42: Global train loss: -3.076, Global test loss: 1.333, Global test accuracy: 61.45
Round  43, Train loss: -3.109, Test loss: 1.290, Test accuracy: 57.60
Round  43: Global train loss: -3.109, Global test loss: 1.319, Global test accuracy: 61.69
Round  44, Train loss: -3.549, Test loss: 1.286, Test accuracy: 57.65
Round  44: Global train loss: -3.549, Global test loss: 1.308, Global test accuracy: 62.10
Round  45, Train loss: -3.850, Test loss: 1.281, Test accuracy: 57.95
Round  45: Global train loss: -3.850, Global test loss: 1.296, Global test accuracy: 62.36
Round  46, Train loss: -4.138, Test loss: 1.289, Test accuracy: 58.02
Round  46: Global train loss: -4.138, Global test loss: 1.280, Global test accuracy: 62.49
Round  47, Train loss: -4.197, Test loss: 1.283, Test accuracy: 58.53
Round  47: Global train loss: -4.197, Global test loss: 1.261, Global test accuracy: 62.90
Round  48, Train loss: -3.279, Test loss: 1.276, Test accuracy: 58.34
Round  48: Global train loss: -3.279, Global test loss: 1.255, Global test accuracy: 62.98
Round  49, Train loss: -3.295, Test loss: 1.275, Test accuracy: 58.41
Round  49: Global train loss: -3.295, Global test loss: 1.248, Global test accuracy: 62.69
Round  50, Train loss: -3.623, Test loss: 1.275, Test accuracy: 58.71
Round  50: Global train loss: -3.623, Global test loss: 1.232, Global test accuracy: 63.22
Round  51, Train loss: -3.744, Test loss: 1.292, Test accuracy: 58.16
Round  51: Global train loss: -3.744, Global test loss: 1.223, Global test accuracy: 63.62
Round  52, Train loss: -3.750, Test loss: 1.271, Test accuracy: 58.71
Round  52: Global train loss: -3.750, Global test loss: 1.213, Global test accuracy: 63.57
Round  53, Train loss: -3.753, Test loss: 1.257, Test accuracy: 58.92
Round  53: Global train loss: -3.753, Global test loss: 1.207, Global test accuracy: 63.92
Round  54, Train loss: -3.739, Test loss: 1.259, Test accuracy: 59.08
Round  54: Global train loss: -3.739, Global test loss: 1.195, Global test accuracy: 64.14
Round  55, Train loss: -4.376, Test loss: 1.272, Test accuracy: 58.89
Round  55: Global train loss: -4.376, Global test loss: 1.186, Global test accuracy: 64.42
Round  56, Train loss: -4.458, Test loss: 1.267, Test accuracy: 59.28
Round  56: Global train loss: -4.458, Global test loss: 1.176, Global test accuracy: 64.73
Round  57, Train loss: -3.760, Test loss: 1.263, Test accuracy: 59.39
Round  57: Global train loss: -3.760, Global test loss: 1.168, Global test accuracy: 64.91
Round  58, Train loss: -4.594, Test loss: 1.278, Test accuracy: 59.10
Round  58: Global train loss: -4.594, Global test loss: 1.162, Global test accuracy: 64.77
Round  59, Train loss: -3.803, Test loss: 1.265, Test accuracy: 59.62
Round  59: Global train loss: -3.803, Global test loss: 1.155, Global test accuracy: 64.86
Round  60, Train loss: -3.897, Test loss: 1.272, Test accuracy: 59.30
Round  60: Global train loss: -3.897, Global test loss: 1.148, Global test accuracy: 65.11
Round  61, Train loss: -3.830, Test loss: 1.289, Test accuracy: 58.89
Round  61: Global train loss: -3.830, Global test loss: 1.141, Global test accuracy: 65.08
Round  62, Train loss: -4.358, Test loss: 1.304, Test accuracy: 58.50
Round  62: Global train loss: -4.358, Global test loss: 1.135, Global test accuracy: 65.21
Round  63, Train loss: -3.868, Test loss: 1.293, Test accuracy: 58.82
Round  63: Global train loss: -3.868, Global test loss: 1.130, Global test accuracy: 65.30
Round  64, Train loss: -4.107, Test loss: 1.297, Test accuracy: 59.09
Round  64: Global train loss: -4.107, Global test loss: 1.121, Global test accuracy: 65.47
Round  65, Train loss: -4.594, Test loss: 1.287, Test accuracy: 59.56
Round  65: Global train loss: -4.594, Global test loss: 1.110, Global test accuracy: 65.81
Round  66, Train loss: -3.856, Test loss: 1.286, Test accuracy: 59.33
Round  66: Global train loss: -3.856, Global test loss: 1.103, Global test accuracy: 65.42
Round  67, Train loss: -3.844, Test loss: 1.272, Test accuracy: 59.58
Round  67: Global train loss: -3.844, Global test loss: 1.095, Global test accuracy: 65.71
Round  68, Train loss: -4.209, Test loss: 1.260, Test accuracy: 59.91
Round  68: Global train loss: -4.209, Global test loss: 1.089, Global test accuracy: 65.94
Round  69, Train loss: -3.958, Test loss: 1.257, Test accuracy: 59.91
Round  69: Global train loss: -3.958, Global test loss: 1.082, Global test accuracy: 65.97
Round  70, Train loss: -4.289, Test loss: 1.271, Test accuracy: 59.77
Round  70: Global train loss: -4.289, Global test loss: 1.073, Global test accuracy: 66.11
Round  71, Train loss: -4.517, Test loss: 1.263, Test accuracy: 59.91
Round  71: Global train loss: -4.517, Global test loss: 1.065, Global test accuracy: 66.28
Round  72, Train loss: -4.414, Test loss: 1.257, Test accuracy: 59.94
Round  72: Global train loss: -4.414, Global test loss: 1.057, Global test accuracy: 66.47
Round  73, Train loss: -3.831, Test loss: 1.258, Test accuracy: 59.75
Round  73: Global train loss: -3.831, Global test loss: 1.055, Global test accuracy: 66.81
Round  74, Train loss: -4.198, Test loss: 1.267, Test accuracy: 59.98
Round  74: Global train loss: -4.198, Global test loss: 1.048, Global test accuracy: 66.94
Round  75, Train loss: -3.924, Test loss: 1.268, Test accuracy: 60.14
Round  75: Global train loss: -3.924, Global test loss: 1.040, Global test accuracy: 67.08
Round  76, Train loss: -4.185, Test loss: 1.251, Test accuracy: 60.74
Round  76: Global train loss: -4.185, Global test loss: 1.035, Global test accuracy: 67.28
Round  77, Train loss: -4.336, Test loss: 1.258, Test accuracy: 60.85
Round  77: Global train loss: -4.336, Global test loss: 1.026, Global test accuracy: 67.30
Round  78, Train loss: -4.000, Test loss: 1.254, Test accuracy: 60.83
Round  78: Global train loss: -4.000, Global test loss: 1.016, Global test accuracy: 67.53
Round  79, Train loss: -4.212, Test loss: 1.246, Test accuracy: 60.99
Round  79: Global train loss: -4.212, Global test loss: 1.009, Global test accuracy: 67.75
Round  80, Train loss: -4.118, Test loss: 1.245, Test accuracy: 60.77
Round  80: Global train loss: -4.118, Global test loss: 1.001, Global test accuracy: 68.05
Round  81, Train loss: -3.938, Test loss: 1.245, Test accuracy: 60.82
Round  81: Global train loss: -3.938, Global test loss: 0.996, Global test accuracy: 68.31
Round  82, Train loss: -4.325, Test loss: 1.251, Test accuracy: 60.74
Round  82: Global train loss: -4.325, Global test loss: 0.990, Global test accuracy: 68.12
Round  83, Train loss: -4.431, Test loss: 1.248, Test accuracy: 60.89
Round  83: Global train loss: -4.431, Global test loss: 0.984, Global test accuracy: 68.21
Round  84, Train loss: -4.262, Test loss: 1.250, Test accuracy: 60.95
Round  84: Global train loss: -4.262, Global test loss: 0.978, Global test accuracy: 68.33
Round  85, Train loss: -4.314, Test loss: 1.248, Test accuracy: 61.20
Round  85: Global train loss: -4.314, Global test loss: 0.972, Global test accuracy: 68.49
Round  86, Train loss: -4.008, Test loss: 1.250, Test accuracy: 61.05
Round  86: Global train loss: -4.008, Global test loss: 0.968, Global test accuracy: 68.66
Round  87, Train loss: -4.654, Test loss: 1.263, Test accuracy: 61.02
Round  87: Global train loss: -4.654, Global test loss: 0.962, Global test accuracy: 68.77
Round  88, Train loss: -3.848, Test loss: 1.258, Test accuracy: 60.80
Round  88: Global train loss: -3.848, Global test loss: 0.959, Global test accuracy: 68.91
Round  89, Train loss: -4.394, Test loss: 1.261, Test accuracy: 60.78
Round  89: Global train loss: -4.394, Global test loss: 0.955, Global test accuracy: 69.02
Round  90, Train loss: -4.526, Test loss: 1.246, Test accuracy: 61.08
Round  90: Global train loss: -4.526, Global test loss: 0.951, Global test accuracy: 69.02
Round  91, Train loss: -4.218, Test loss: 1.236, Test accuracy: 61.41
Round  91: Global train loss: -4.218, Global test loss: 0.944, Global test accuracy: 69.19
Round  92, Train loss: -4.094, Test loss: 1.230, Test accuracy: 61.46
Round  92: Global train loss: -4.094, Global test loss: 0.941, Global test accuracy: 69.32
Round  93, Train loss: -4.185, Test loss: 1.245, Test accuracy: 61.18
Round  93: Global train loss: -4.185, Global test loss: 0.938, Global test accuracy: 69.42
Round  94, Train loss: -4.307, Test loss: 1.246, Test accuracy: 61.36
Round  94: Global train loss: -4.307, Global test loss: 0.933, Global test accuracy: 69.53
Round  95, Train loss: -4.075, Test loss: 1.237, Test accuracy: 61.74
Round  95: Global train loss: -4.075, Global test loss: 0.928, Global test accuracy: 69.53
Round  96, Train loss: -3.766, Test loss: 1.228, Test accuracy: 61.99
Round  96: Global train loss: -3.766, Global test loss: 0.925, Global test accuracy: 69.96
Round  97, Train loss: -4.133, Test loss: 1.234, Test accuracy: 61.62
Round  97: Global train loss: -4.133, Global test loss: 0.922, Global test accuracy: 70.10
Round  98, Train loss: -3.884, Test loss: 1.211, Test accuracy: 62.08
Round  98: Global train loss: -3.884, Global test loss: 0.920, Global test accuracy: 69.89
Round  99, Train loss: -3.986, Test loss: 1.213, Test accuracy: 62.09
Round  99: Global train loss: -3.986, Global test loss: 0.917, Global test accuracy: 69.95
Final Round: Train loss: 0.855, Test loss: 1.019, Test accuracy: 65.89
Final Round: Global train loss: 0.855, Global test loss: 0.897, Global test accuracy: 70.48
Average accuracy final 10 rounds: 61.599500000000006
Average global accuracy final 10 rounds: 69.59183333333334
8608.780770540237
[]
[19.26, 27.548333333333332, 34.09166666666667, 39.211666666666666, 42.605, 44.086666666666666, 46.12833333333333, 46.098333333333336, 48.91, 48.85166666666667, 49.50333333333333, 50.67333333333333, 51.38, 52.306666666666665, 53.08166666666666, 52.843333333333334, 53.498333333333335, 53.865, 53.388333333333335, 53.87833333333333, 54.82833333333333, 55.14666666666667, 55.64, 55.1, 55.83166666666666, 55.861666666666665, 55.86333333333334, 55.873333333333335, 55.76833333333333, 55.67333333333333, 56.26166666666666, 56.595, 56.67333333333333, 57.038333333333334, 57.13666666666666, 57.775, 57.90833333333333, 57.61833333333333, 57.515, 57.43, 57.416666666666664, 57.42, 57.641666666666666, 57.605, 57.653333333333336, 57.946666666666665, 58.02, 58.535, 58.34, 58.406666666666666, 58.71, 58.165, 58.711666666666666, 58.916666666666664, 59.08, 58.888333333333335, 59.276666666666664, 59.39, 59.098333333333336, 59.615, 59.29666666666667, 58.888333333333335, 58.49666666666667, 58.81666666666667, 59.093333333333334, 59.56, 59.32666666666667, 59.58166666666666, 59.91, 59.906666666666666, 59.765, 59.913333333333334, 59.935, 59.748333333333335, 59.98166666666667, 60.13666666666666, 60.74333333333333, 60.848333333333336, 60.82666666666667, 60.98833333333334, 60.77, 60.82, 60.74, 60.88666666666666, 60.95166666666667, 61.20166666666667, 61.053333333333335, 61.02166666666667, 60.795, 60.776666666666664, 61.08, 61.415, 61.458333333333336, 61.18333333333333, 61.358333333333334, 61.736666666666665, 61.986666666666665, 61.61666666666667, 62.075, 62.085, 65.89333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.304, Test loss: 2.303, Test accuracy: 10.00 

Round   0, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.00 

Round   1, Train loss: 2.304, Test loss: 2.303, Test accuracy: 10.00 

Round   1, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.00 

Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.00 

Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.00 

Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.00 

Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.00 

Round   4, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.00 

Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.00 

Round   5, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.00 

Round   5, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 10.00 

Round   6, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.00 

Round   6, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 10.00 

Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.00 

Round   7, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.00 

Round   8, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.00 

Round   8, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.00 

Round   9, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.00 

Round   9, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.00 

Round  10, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.00 

Round  10, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 10.00 

Round  11, Train loss: 2.300, Test loss: 2.300, Test accuracy: 10.00 

Round  11, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 10.00 

Round  12, Train loss: 2.301, Test loss: 2.300, Test accuracy: 10.00 

Round  12, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 10.00 

Round  13, Train loss: 2.300, Test loss: 2.299, Test accuracy: 10.00 

Round  13, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 10.00 

Round  14, Train loss: 2.298, Test loss: 2.299, Test accuracy: 10.00 

Round  14, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 10.00 

Round  15, Train loss: 2.298, Test loss: 2.299, Test accuracy: 10.00 

Round  15, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 10.00 

Round  16, Train loss: 2.299, Test loss: 2.299, Test accuracy: 10.00 

Round  16, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 10.00 

Round  17, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.00 

Round  17, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.00 

Round  18, Train loss: 2.297, Test loss: 2.298, Test accuracy: 10.00 

Round  18, Global train loss: 2.297, Global test loss: 2.297, Global test accuracy: 10.00 

Round  19, Train loss: 2.297, Test loss: 2.297, Test accuracy: 10.00 

Round  19, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 10.00 

Round  20, Train loss: 2.297, Test loss: 2.297, Test accuracy: 10.00 

Round  20, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 10.00 

Round  21, Train loss: 2.296, Test loss: 2.296, Test accuracy: 10.00 

Round  21, Global train loss: 2.296, Global test loss: 2.295, Global test accuracy: 10.00 

Round  22, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.00 

Round  22, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.00 

Round  23, Train loss: 2.296, Test loss: 2.295, Test accuracy: 10.00 

Round  23, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 10.00 

Round  24, Train loss: 2.296, Test loss: 2.295, Test accuracy: 10.00 

Round  24, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 10.00 

Round  25, Train loss: 2.296, Test loss: 2.295, Test accuracy: 10.00 

Round  25, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 10.00 

Round  26, Train loss: 2.295, Test loss: 2.294, Test accuracy: 10.00 

Round  26, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 10.00 

Round  27, Train loss: 2.296, Test loss: 2.294, Test accuracy: 10.00 

Round  27, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 10.00 

Round  28, Train loss: 2.294, Test loss: 2.293, Test accuracy: 10.00 

Round  28, Global train loss: 2.294, Global test loss: 2.292, Global test accuracy: 10.00 

Round  29, Train loss: 2.293, Test loss: 2.293, Test accuracy: 10.00 

Round  29, Global train loss: 2.293, Global test loss: 2.292, Global test accuracy: 10.00 

Round  30, Train loss: 2.294, Test loss: 2.292, Test accuracy: 10.00 

Round  30, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 10.00 

Round  31, Train loss: 2.294, Test loss: 2.292, Test accuracy: 10.01 

Round  31, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 10.00 

Round  32, Train loss: 2.292, Test loss: 2.292, Test accuracy: 10.03 

Round  32, Global train loss: 2.292, Global test loss: 2.290, Global test accuracy: 10.04 

Round  33, Train loss: 2.292, Test loss: 2.291, Test accuracy: 10.05 

Round  33, Global train loss: 2.292, Global test loss: 2.290, Global test accuracy: 10.04 

Round  34, Train loss: 2.291, Test loss: 2.290, Test accuracy: 10.09 

Round  34, Global train loss: 2.291, Global test loss: 2.289, Global test accuracy: 10.07 

Round  35, Train loss: 2.292, Test loss: 2.290, Test accuracy: 10.08 

Round  35, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 10.11 

Round  36, Train loss: 2.291, Test loss: 2.290, Test accuracy: 10.11 

Round  36, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 10.15 

Round  37, Train loss: 2.290, Test loss: 2.289, Test accuracy: 10.15 

Round  37, Global train loss: 2.290, Global test loss: 2.288, Global test accuracy: 10.16 

Round  38, Train loss: 2.290, Test loss: 2.289, Test accuracy: 10.27 

Round  38, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 10.55 

Round  39, Train loss: 2.290, Test loss: 2.289, Test accuracy: 10.38 

Round  39, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 10.84 

Round  40, Train loss: 2.287, Test loss: 2.288, Test accuracy: 10.58 

Round  40, Global train loss: 2.287, Global test loss: 2.286, Global test accuracy: 10.99 

Round  41, Train loss: 2.287, Test loss: 2.288, Test accuracy: 10.86 

Round  41, Global train loss: 2.287, Global test loss: 2.285, Global test accuracy: 11.59 

Round  42, Train loss: 2.287, Test loss: 2.287, Test accuracy: 11.13 

Round  42, Global train loss: 2.287, Global test loss: 2.284, Global test accuracy: 11.81 

Round  43, Train loss: 2.287, Test loss: 2.286, Test accuracy: 11.44 

Round  43, Global train loss: 2.287, Global test loss: 2.284, Global test accuracy: 12.14 

Round  44, Train loss: 2.287, Test loss: 2.285, Test accuracy: 11.83 

Round  44, Global train loss: 2.287, Global test loss: 2.283, Global test accuracy: 12.40 

Round  45, Train loss: 2.283, Test loss: 2.285, Test accuracy: 12.27 

Round  45, Global train loss: 2.283, Global test loss: 2.282, Global test accuracy: 13.14 

Round  46, Train loss: 2.282, Test loss: 2.284, Test accuracy: 12.43 

Round  46, Global train loss: 2.282, Global test loss: 2.282, Global test accuracy: 13.28 

Round  47, Train loss: 2.285, Test loss: 2.283, Test accuracy: 12.91 

Round  47, Global train loss: 2.285, Global test loss: 2.281, Global test accuracy: 13.26 

Round  48, Train loss: 2.285, Test loss: 2.282, Test accuracy: 12.99 

Round  48, Global train loss: 2.285, Global test loss: 2.280, Global test accuracy: 13.24 

Round  49, Train loss: 2.283, Test loss: 2.282, Test accuracy: 13.33 

Round  49, Global train loss: 2.283, Global test loss: 2.279, Global test accuracy: 13.80 

Round  50, Train loss: 2.282, Test loss: 2.280, Test accuracy: 13.61 

Round  50, Global train loss: 2.282, Global test loss: 2.278, Global test accuracy: 13.86 

Round  51, Train loss: 2.281, Test loss: 2.279, Test accuracy: 13.75 

Round  51, Global train loss: 2.281, Global test loss: 2.277, Global test accuracy: 13.87 

Round  52, Train loss: 2.281, Test loss: 2.279, Test accuracy: 13.88 

Round  52, Global train loss: 2.281, Global test loss: 2.276, Global test accuracy: 14.33 

Round  53, Train loss: 2.281, Test loss: 2.277, Test accuracy: 14.19 

Round  53, Global train loss: 2.281, Global test loss: 2.275, Global test accuracy: 14.49 

Round  54, Train loss: 2.281, Test loss: 2.276, Test accuracy: 14.44 

Round  54, Global train loss: 2.281, Global test loss: 2.273, Global test accuracy: 14.65 

Round  55, Train loss: 2.280, Test loss: 2.275, Test accuracy: 14.40 

Round  55, Global train loss: 2.280, Global test loss: 2.273, Global test accuracy: 14.71 

Round  56, Train loss: 2.276, Test loss: 2.274, Test accuracy: 14.49 

Round  56, Global train loss: 2.276, Global test loss: 2.271, Global test accuracy: 14.69 

Round  57, Train loss: 2.278, Test loss: 2.273, Test accuracy: 14.64 

Round  57, Global train loss: 2.278, Global test loss: 2.271, Global test accuracy: 14.77 

Round  58, Train loss: 2.279, Test loss: 2.272, Test accuracy: 14.63 

Round  58, Global train loss: 2.279, Global test loss: 2.269, Global test accuracy: 14.88 

Round  59, Train loss: 2.273, Test loss: 2.271, Test accuracy: 14.72 

Round  59, Global train loss: 2.273, Global test loss: 2.269, Global test accuracy: 14.87 

Round  60, Train loss: 2.278, Test loss: 2.270, Test accuracy: 14.84 

Round  60, Global train loss: 2.278, Global test loss: 2.268, Global test accuracy: 15.06 

Round  61, Train loss: 2.274, Test loss: 2.269, Test accuracy: 15.19 

Round  61, Global train loss: 2.274, Global test loss: 2.266, Global test accuracy: 15.15 

Round  62, Train loss: 2.275, Test loss: 2.268, Test accuracy: 15.18 

Round  62, Global train loss: 2.275, Global test loss: 2.265, Global test accuracy: 15.22 

Round  63, Train loss: 2.271, Test loss: 2.266, Test accuracy: 15.08 

Round  63, Global train loss: 2.271, Global test loss: 2.263, Global test accuracy: 14.84 

Round  64, Train loss: 2.276, Test loss: 2.265, Test accuracy: 15.31 

Round  64, Global train loss: 2.276, Global test loss: 2.261, Global test accuracy: 15.59 

Round  65, Train loss: 2.271, Test loss: 2.263, Test accuracy: 15.76 

Round  65, Global train loss: 2.271, Global test loss: 2.261, Global test accuracy: 16.53 

Round  66, Train loss: 2.277, Test loss: 2.262, Test accuracy: 15.70 

Round  66, Global train loss: 2.277, Global test loss: 2.258, Global test accuracy: 15.61 

Round  67, Train loss: 2.270, Test loss: 2.260, Test accuracy: 15.42 

Round  67, Global train loss: 2.270, Global test loss: 2.257, Global test accuracy: 15.20 

Round  68, Train loss: 2.270, Test loss: 2.259, Test accuracy: 15.60 

Round  68, Global train loss: 2.270, Global test loss: 2.255, Global test accuracy: 15.96 

Round  69, Train loss: 2.266, Test loss: 2.258, Test accuracy: 15.86 

Round  69, Global train loss: 2.266, Global test loss: 2.255, Global test accuracy: 16.36 

Round  70, Train loss: 2.267, Test loss: 2.256, Test accuracy: 15.81 

Round  70, Global train loss: 2.267, Global test loss: 2.254, Global test accuracy: 16.23 

Round  71, Train loss: 2.268, Test loss: 2.256, Test accuracy: 16.14 

Round  71, Global train loss: 2.268, Global test loss: 2.254, Global test accuracy: 16.76 

Round  72, Train loss: 2.264, Test loss: 2.255, Test accuracy: 16.38 

Round  72, Global train loss: 2.264, Global test loss: 2.253, Global test accuracy: 17.01 

Round  73, Train loss: 2.266, Test loss: 2.254, Test accuracy: 16.67 

Round  73, Global train loss: 2.266, Global test loss: 2.252, Global test accuracy: 16.97 

Round  74, Train loss: 2.266, Test loss: 2.253, Test accuracy: 16.75 

Round  74, Global train loss: 2.266, Global test loss: 2.251, Global test accuracy: 17.02 

Round  75, Train loss: 2.268, Test loss: 2.252, Test accuracy: 16.76 

Round  75, Global train loss: 2.268, Global test loss: 2.250, Global test accuracy: 17.18 

Round  76, Train loss: 2.266, Test loss: 2.252, Test accuracy: 17.17 

Round  76, Global train loss: 2.266, Global test loss: 2.251, Global test accuracy: 18.15 

Round  77, Train loss: 2.264, Test loss: 2.251, Test accuracy: 17.74 

Round  77, Global train loss: 2.264, Global test loss: 2.250, Global test accuracy: 18.57 

Round  78, Train loss: 2.262, Test loss: 2.250, Test accuracy: 17.68 

Round  78, Global train loss: 2.262, Global test loss: 2.248, Global test accuracy: 18.63 

Round  79, Train loss: 2.262, Test loss: 2.249, Test accuracy: 17.76 

Round  79, Global train loss: 2.262, Global test loss: 2.247, Global test accuracy: 18.74 

Round  80, Train loss: 2.261, Test loss: 2.249, Test accuracy: 17.86 

Round  80, Global train loss: 2.261, Global test loss: 2.246, Global test accuracy: 18.66 

Round  81, Train loss: 2.259, Test loss: 2.247, Test accuracy: 18.24 

Round  81, Global train loss: 2.259, Global test loss: 2.245, Global test accuracy: 18.87 

Round  82, Train loss: 2.257, Test loss: 2.246, Test accuracy: 18.32 

Round  82, Global train loss: 2.257, Global test loss: 2.243, Global test accuracy: 18.87 

Round  83, Train loss: 2.260, Test loss: 2.246, Test accuracy: 18.72 

Round  83, Global train loss: 2.260, Global test loss: 2.243, Global test accuracy: 19.32 

Round  84, Train loss: 2.258, Test loss: 2.245, Test accuracy: 19.12 

Round  84, Global train loss: 2.258, Global test loss: 2.242, Global test accuracy: 19.79 

Round  85, Train loss: 2.261, Test loss: 2.243, Test accuracy: 19.48 

Round  85, Global train loss: 2.261, Global test loss: 2.241, Global test accuracy: 19.89 

Round  86, Train loss: 2.253, Test loss: 2.243, Test accuracy: 19.73 

Round  86, Global train loss: 2.253, Global test loss: 2.242, Global test accuracy: 20.29 

Round  87, Train loss: 2.254, Test loss: 2.242, Test accuracy: 19.79 

Round  87, Global train loss: 2.254, Global test loss: 2.239, Global test accuracy: 20.40 

Round  88, Train loss: 2.252, Test loss: 2.241, Test accuracy: 19.89 

Round  88, Global train loss: 2.252, Global test loss: 2.238, Global test accuracy: 20.34 

Round  89, Train loss: 2.257, Test loss: 2.241, Test accuracy: 19.99 

Round  89, Global train loss: 2.257, Global test loss: 2.239, Global test accuracy: 20.79 

Round  90, Train loss: 2.250, Test loss: 2.240, Test accuracy: 20.08 

Round  90, Global train loss: 2.250, Global test loss: 2.238, Global test accuracy: 20.83 

Round  91, Train loss: 2.254, Test loss: 2.238, Test accuracy: 20.11 

Round  91, Global train loss: 2.254, Global test loss: 2.235, Global test accuracy: 20.72 

Round  92, Train loss: 2.253, Test loss: 2.237, Test accuracy: 20.18 

Round  92, Global train loss: 2.253, Global test loss: 2.233, Global test accuracy: 20.46 

Round  93, Train loss: 2.251, Test loss: 2.236, Test accuracy: 20.31 

Round  93, Global train loss: 2.251, Global test loss: 2.233, Global test accuracy: 21.14 

Round  94, Train loss: 2.248, Test loss: 2.236, Test accuracy: 20.43 

Round  94, Global train loss: 2.248, Global test loss: 2.233, Global test accuracy: 20.82 

Round  95, Train loss: 2.245, Test loss: 2.235, Test accuracy: 20.41 

Round  95, Global train loss: 2.245, Global test loss: 2.231, Global test accuracy: 21.01 

Round  96, Train loss: nan, Test loss: nan, Test accuracy: 19.93 

Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round  97, Train loss: nan, Test loss: nan, Test accuracy: 17.43 

Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round  98, Train loss: nan, Test loss: nan, Test accuracy: 15.95 

Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round  99, Train loss: nan, Test loss: nan, Test accuracy: 14.28 

Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 100, Train loss: nan, Test loss: nan, Test accuracy: 13.77 

Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 101, Train loss: nan, Test loss: nan, Test accuracy: 12.16 

Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 102, Train loss: nan, Test loss: nan, Test accuracy: 11.54 

Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 103, Train loss: nan, Test loss: nan, Test accuracy: 10.49 

Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 104, Train loss: nan, Test loss: nan, Test accuracy: 10.49 

Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 105, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 106, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 107, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 108, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 109, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 110, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 111, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 112, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 113, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 114, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 115, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 116, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 117, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 118, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 119, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 120, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 121, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 122, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 123, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 124, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 125, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 126, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 127, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 128, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 129, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 130, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 131, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 132, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 133, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 134, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 135, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 136, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 137, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 138, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 139, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 140, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 141, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 142, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 143, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 144, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 145, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 146, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 147, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 148, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 149, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 150, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 151, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 152, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 153, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 154, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 155, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 156, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 157, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 158, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 159, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 160, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 161, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 162, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 163, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 164, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 165, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 166, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 167, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 168, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 169, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 170, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 171, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 172, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 173, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 174, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 175, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 176, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 177, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 178, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 179, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 180, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 181, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 182, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 183, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 184, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 185, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 186, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 187, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 188, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 189, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 190, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 191, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 192, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 193, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 194, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 195, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 196, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 197, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 198, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 199, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 200, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 201, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 202, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 203, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 204, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 205, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 206, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 207, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 208, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 209, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 210, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 211, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 212, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 213, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 214, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 215, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 216, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 217, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 218, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 219, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 220, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 221, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 222, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 223, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 224, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 225, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 226, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 227, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 228, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 229, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 230, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 231, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 232, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 233, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 234, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 235, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 236, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 237, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 238, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 239, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 240, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 241, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 242, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 243, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 244, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 245, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 246, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 247, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 248, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 249, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 250, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 251, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 252, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 253, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 254, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 255, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 256, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 257, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 258, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 259, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 260, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 261, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 262, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 263, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 264, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 265, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 266, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 267, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 268, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 269, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 270, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 271, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 272, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 273, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 274, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 275, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 276, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 277, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 278, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 279, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 280, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 281, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 282, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 283, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 284, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 285, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 286, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 287, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 288, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 289, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 290, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 291, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 292, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 293, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 294, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 295, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 296, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 297, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 298, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 299, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

9100.784269809723
[1.5366737842559814, 2.8666718006134033, 4.291596174240112, 5.712687253952026, 7.121082305908203, 8.561850786209106, 9.963548421859741, 11.386133909225464, 12.807939291000366, 14.236128807067871, 15.677605867385864, 17.092681884765625, 18.537366151809692, 19.970504999160767, 21.39927864074707, 22.81325078010559, 24.27887463569641, 25.677322387695312, 27.059053897857666, 28.519235849380493, 29.945374011993408, 31.3674418926239, 32.83940863609314, 34.282021045684814, 35.720353841781616, 37.1489052772522, 38.56895613670349, 40.02143979072571, 41.437506437301636, 42.88278007507324, 44.32652235031128, 45.74311542510986, 47.15468168258667, 48.58602595329285, 50.025062799453735, 51.44630694389343, 52.87824892997742, 54.29046678543091, 55.71108675003052, 57.11770749092102, 58.53237271308899, 59.949806928634644, 61.36241841316223, 62.78919172286987, 64.21219110488892, 65.66727328300476, 67.07689619064331, 68.47737956047058, 69.90794587135315, 71.27786684036255, 72.6406774520874, 74.00808644294739, 75.35545015335083, 76.74322962760925, 78.15264058113098, 79.5107274055481, 80.87064671516418, 82.26103448867798, 83.67616033554077, 85.03740239143372, 86.46554613113403, 87.8666684627533, 89.27410769462585, 90.61653566360474, 92.05212903022766, 93.46179938316345, 94.88125038146973, 96.30057001113892, 97.67337417602539, 99.07886099815369, 100.51066255569458, 101.89711093902588, 103.27853298187256, 104.6461775302887, 106.00914025306702, 107.40325832366943, 108.76241326332092, 110.12538266181946, 111.55125451087952, 112.93300867080688, 114.29872679710388, 115.67339301109314, 117.03759837150574, 118.44807457923889, 119.80142664909363, 121.16758728027344, 122.55375266075134, 123.92812490463257, 125.30699849128723, 126.74473071098328, 128.1160225868225, 129.51276874542236, 130.90729880332947, 132.31913876533508, 133.71625566482544, 135.0581135749817, 136.40203046798706, 137.7490735054016, 139.11020588874817, 140.469806432724, 141.8232913017273, 143.21946740150452, 144.5806279182434, 145.95236945152283, 147.31610870361328, 148.6856427192688, 150.01800060272217, 151.38005805015564, 152.73196363449097, 154.11093759536743, 155.46598625183105, 156.8154661655426, 158.17094802856445, 159.58304119110107, 160.97983288764954, 162.32864427566528, 163.71656942367554, 165.16622161865234, 166.5486524105072, 167.9546926021576, 169.392169713974, 170.79185438156128, 172.18830728530884, 173.6298999786377, 175.04240822792053, 176.45056676864624, 177.82756733894348, 179.17625999450684, 180.59902453422546, 182.04035639762878, 183.40609121322632, 184.78882002830505, 186.13038682937622, 187.53691697120667, 188.95886516571045, 190.32410550117493, 191.72816801071167, 193.17569541931152, 194.53741097450256, 195.89475083351135, 197.3132348060608, 198.69652318954468, 200.10125756263733, 201.50333714485168, 202.87056255340576, 204.2127435207367, 205.68615818023682, 207.10349869728088, 208.49085402488708, 209.8346767425537, 211.19778108596802, 212.54568600654602, 213.88362884521484, 215.27079939842224, 216.57141208648682, 217.91670274734497, 219.22988200187683, 220.63422417640686, 222.01510643959045, 223.34762525558472, 224.70903491973877, 226.08807682991028, 227.41700387001038, 228.75091934204102, 230.07661175727844, 231.3936107158661, 232.76183485984802, 234.14400362968445, 235.5312077999115, 236.90484261512756, 238.294673204422, 239.548166513443, 240.87534070014954, 242.21238541603088, 243.56258034706116, 244.9276978969574, 246.29232692718506, 247.6572458744049, 249.03148913383484, 250.3860421180725, 251.7495219707489, 253.0948486328125, 254.4532287120819, 255.81922149658203, 257.1822278499603, 258.52848529815674, 259.9061665534973, 261.25897002220154, 262.63507604599, 264.0041913986206, 265.37489557266235, 266.7563588619232, 268.1334090232849, 269.5022859573364, 270.88102746009827, 272.25850534439087, 273.6333131790161, 275.00999999046326, 276.38831853866577, 277.75269627571106, 279.1220872402191, 280.48793601989746, 281.8483214378357, 283.2181046009064, 284.5776424407959, 285.9363293647766, 287.3025395870209, 288.65756845474243, 290.0255491733551, 291.39824748039246, 292.7623836994171, 294.13360929489136, 295.516104221344, 296.88176822662354, 298.2340714931488, 299.6056830883026, 300.96766567230225, 302.3284282684326, 303.6959025859833, 305.06145763397217, 306.4335262775421, 307.80629444122314, 309.1821711063385, 310.5514323711395, 311.9095392227173, 313.2703113555908, 314.648353099823, 316.0184667110443, 317.39555191993713, 318.77214097976685, 320.1519134044647, 321.53074073791504, 322.9155695438385, 324.30657482147217, 325.6908071041107, 327.0730028152466, 328.4523696899414, 329.8202953338623, 331.1945950984955, 332.5621292591095, 333.9388048648834, 335.30636072158813, 336.68026900291443, 338.0519745349884, 339.4270586967468, 340.8039698600769, 342.17240357398987, 343.55408549308777, 344.9418113231659, 346.31665658950806, 347.6968195438385, 349.06084299087524, 350.4318106174469, 351.80166935920715, 353.16631293296814, 354.5272116661072, 355.90533995628357, 357.26539635658264, 358.63330936431885, 360.0023498535156, 361.3679368495941, 362.736839056015, 364.11169695854187, 365.47385478019714, 366.8289496898651, 368.1890730857849, 369.5562047958374, 370.91749477386475, 372.2844030857086, 373.66935110092163, 375.0382168292999, 376.4067587852478, 377.7815010547638, 379.13549304008484, 380.5147624015808, 381.8878102302551, 383.2606327533722, 384.620076417923, 385.9638042449951, 387.3309271335602, 388.68965220451355, 390.04551339149475, 391.4046332836151, 392.7597978115082, 394.1084122657776, 395.4734926223755, 396.8374309539795, 398.1966907978058, 399.5667374134064, 400.9224154949188, 402.29253339767456, 403.66390800476074, 405.0249807834625, 406.39503479003906, 407.7553596496582, 409.11796164512634, 410.4799335002899, 411.8493700027466, 413.2160656452179, 414.5786862373352, 417.3153831958771]
[10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.005, 10.0275, 10.05, 10.085, 10.08, 10.1075, 10.155, 10.27, 10.375, 10.58, 10.855, 11.135, 11.4425, 11.83, 12.2725, 12.43, 12.91, 12.9925, 13.33, 13.61, 13.7525, 13.885, 14.1875, 14.4375, 14.405, 14.4875, 14.6425, 14.6275, 14.725, 14.84, 15.185, 15.18, 15.0775, 15.3075, 15.7575, 15.6975, 15.42, 15.5975, 15.8575, 15.8125, 16.145, 16.3825, 16.6725, 16.7525, 16.7575, 17.1725, 17.7375, 17.675, 17.7575, 17.8625, 18.2375, 18.3175, 18.72, 19.1175, 19.475, 19.7275, 19.7875, 19.89, 19.99, 20.0825, 20.1075, 20.175, 20.3125, 20.425, 20.415, 19.9275, 17.425, 15.9475, 14.28, 13.7675, 12.16, 11.535, 10.49, 10.49, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.251, Test loss: 2.070, Test accuracy: 26.21
Round   1, Train loss: 2.056, Test loss: 1.869, Test accuracy: 32.59
Round   2, Train loss: 1.915, Test loss: 1.713, Test accuracy: 37.52
Round   3, Train loss: 1.824, Test loss: 1.625, Test accuracy: 40.85
Round   4, Train loss: 1.695, Test loss: 1.529, Test accuracy: 44.74
Round   5, Train loss: 1.677, Test loss: 1.501, Test accuracy: 45.09
Round   6, Train loss: 1.616, Test loss: 1.458, Test accuracy: 46.88
Round   7, Train loss: 1.587, Test loss: 1.407, Test accuracy: 49.52
Round   8, Train loss: 1.557, Test loss: 1.394, Test accuracy: 49.55
Round   9, Train loss: 1.488, Test loss: 1.355, Test accuracy: 50.94
Round  10, Train loss: 1.490, Test loss: 1.331, Test accuracy: 51.76
Round  11, Train loss: 1.391, Test loss: 1.306, Test accuracy: 52.59
Round  12, Train loss: 1.372, Test loss: 1.267, Test accuracy: 54.07
Round  13, Train loss: 1.340, Test loss: 1.250, Test accuracy: 55.33
Round  14, Train loss: 1.285, Test loss: 1.230, Test accuracy: 56.03
Round  15, Train loss: 1.331, Test loss: 1.189, Test accuracy: 57.79
Round  16, Train loss: 1.236, Test loss: 1.202, Test accuracy: 57.29
Round  17, Train loss: 1.204, Test loss: 1.172, Test accuracy: 58.86
Round  18, Train loss: 1.235, Test loss: 1.138, Test accuracy: 59.43
Round  19, Train loss: 1.179, Test loss: 1.124, Test accuracy: 59.66
Round  20, Train loss: 1.138, Test loss: 1.124, Test accuracy: 60.09
Round  21, Train loss: 1.132, Test loss: 1.112, Test accuracy: 61.01
Round  22, Train loss: 1.082, Test loss: 1.113, Test accuracy: 60.77
Round  23, Train loss: 1.080, Test loss: 1.083, Test accuracy: 61.55
Round  24, Train loss: 1.058, Test loss: 1.080, Test accuracy: 61.91
Round  25, Train loss: 1.038, Test loss: 1.074, Test accuracy: 62.04
Round  26, Train loss: 1.012, Test loss: 1.078, Test accuracy: 61.88
Round  27, Train loss: 0.943, Test loss: 1.078, Test accuracy: 62.35
Round  28, Train loss: 0.942, Test loss: 1.065, Test accuracy: 62.81
Round  29, Train loss: 0.964, Test loss: 1.054, Test accuracy: 63.91
Round  30, Train loss: 0.902, Test loss: 1.054, Test accuracy: 63.51
Round  31, Train loss: 0.918, Test loss: 1.063, Test accuracy: 63.11
Round  32, Train loss: 0.848, Test loss: 1.053, Test accuracy: 64.09
Round  33, Train loss: 0.901, Test loss: 1.038, Test accuracy: 64.20
Round  34, Train loss: 0.874, Test loss: 1.032, Test accuracy: 64.86
Round  35, Train loss: 0.872, Test loss: 1.016, Test accuracy: 65.03
Round  36, Train loss: 0.812, Test loss: 1.015, Test accuracy: 65.66
Round  37, Train loss: 0.837, Test loss: 1.007, Test accuracy: 65.11
Round  38, Train loss: 0.784, Test loss: 1.016, Test accuracy: 65.02
Round  39, Train loss: 0.780, Test loss: 1.017, Test accuracy: 65.49
Round  40, Train loss: 0.785, Test loss: 1.016, Test accuracy: 65.27
Round  41, Train loss: 0.727, Test loss: 1.017, Test accuracy: 65.49
Round  42, Train loss: 0.755, Test loss: 1.031, Test accuracy: 65.42
Round  43, Train loss: 0.740, Test loss: 1.027, Test accuracy: 65.47
Round  44, Train loss: 0.737, Test loss: 1.030, Test accuracy: 65.88
Round  45, Train loss: 0.701, Test loss: 1.013, Test accuracy: 65.92
Round  46, Train loss: 0.690, Test loss: 1.032, Test accuracy: 66.04
Round  47, Train loss: 0.683, Test loss: 1.030, Test accuracy: 66.47
Round  48, Train loss: 0.670, Test loss: 1.010, Test accuracy: 66.66
Round  49, Train loss: 0.668, Test loss: 1.010, Test accuracy: 66.53
Round  50, Train loss: 0.618, Test loss: 1.023, Test accuracy: 66.28
Round  51, Train loss: 0.668, Test loss: 1.016, Test accuracy: 66.47
Round  52, Train loss: 0.641, Test loss: 1.025, Test accuracy: 67.14
Round  53, Train loss: 0.615, Test loss: 1.023, Test accuracy: 67.02
Round  54, Train loss: 0.625, Test loss: 1.019, Test accuracy: 66.84
Round  55, Train loss: 0.604, Test loss: 1.045, Test accuracy: 66.84
Round  56, Train loss: 0.592, Test loss: 1.042, Test accuracy: 66.89
Round  57, Train loss: 0.575, Test loss: 1.043, Test accuracy: 67.14
Round  58, Train loss: 0.637, Test loss: 1.014, Test accuracy: 66.92
Round  59, Train loss: 0.653, Test loss: 1.003, Test accuracy: 67.63
Round  60, Train loss: 0.569, Test loss: 1.026, Test accuracy: 67.45
Round  61, Train loss: 0.556, Test loss: 1.015, Test accuracy: 67.39
Round  62, Train loss: 0.564, Test loss: 1.050, Test accuracy: 66.76
Round  63, Train loss: 0.503, Test loss: 1.042, Test accuracy: 66.92
Round  64, Train loss: 0.578, Test loss: 1.036, Test accuracy: 67.14
Round  65, Train loss: 0.502, Test loss: 1.051, Test accuracy: 67.33
Round  66, Train loss: 0.483, Test loss: 1.088, Test accuracy: 66.48
Round  67, Train loss: 0.545, Test loss: 1.038, Test accuracy: 67.34
Round  68, Train loss: 0.533, Test loss: 1.044, Test accuracy: 67.78
Round  69, Train loss: 0.514, Test loss: 1.033, Test accuracy: 67.51
Round  70, Train loss: 0.468, Test loss: 1.083, Test accuracy: 67.59
Round  71, Train loss: 0.448, Test loss: 1.072, Test accuracy: 67.94
Round  72, Train loss: 0.494, Test loss: 1.048, Test accuracy: 67.75
Round  73, Train loss: 0.532, Test loss: 1.059, Test accuracy: 67.37
Round  74, Train loss: 0.486, Test loss: 1.079, Test accuracy: 67.92
Round  75, Train loss: 0.457, Test loss: 1.074, Test accuracy: 68.02
Round  76, Train loss: 0.495, Test loss: 1.043, Test accuracy: 68.32
Round  77, Train loss: 0.475, Test loss: 1.065, Test accuracy: 68.13
Round  78, Train loss: 0.444, Test loss: 1.072, Test accuracy: 67.86
Round  79, Train loss: 0.425, Test loss: 1.076, Test accuracy: 67.99
Round  80, Train loss: 0.401, Test loss: 1.105, Test accuracy: 67.83
Round  81, Train loss: 0.500, Test loss: 1.052, Test accuracy: 68.43
Round  82, Train loss: 0.457, Test loss: 1.039, Test accuracy: 68.39
Round  83, Train loss: 0.432, Test loss: 1.040, Test accuracy: 68.66
Round  84, Train loss: 0.457, Test loss: 1.054, Test accuracy: 68.27
Round  85, Train loss: 0.430, Test loss: 1.070, Test accuracy: 68.33
Round  86, Train loss: 0.414, Test loss: 1.070, Test accuracy: 68.86
Round  87, Train loss: 0.408, Test loss: 1.099, Test accuracy: 68.22
Round  88, Train loss: 0.416, Test loss: 1.060, Test accuracy: 68.69
Round  89, Train loss: 0.443, Test loss: 1.057, Test accuracy: 68.79
Round  90, Train loss: 0.391, Test loss: 1.064, Test accuracy: 68.89
Round  91, Train loss: 0.426, Test loss: 1.052, Test accuracy: 69.28
Round  92, Train loss: 0.386, Test loss: 1.076, Test accuracy: 68.71
Round  93, Train loss: 0.381, Test loss: 1.095, Test accuracy: 68.58
Round  94, Train loss: 0.410, Test loss: 1.084, Test accuracy: 68.94
Round  95, Train loss: 0.408, Test loss: 1.085, Test accuracy: 68.83
Round  96, Train loss: 0.402, Test loss: 1.087, Test accuracy: 68.51
Round  97, Train loss: 0.410, Test loss: 1.078, Test accuracy: 68.93
Round  98, Train loss: 0.348, Test loss: 1.101, Test accuracy: 69.09
Round  99, Train loss: 0.372, Test loss: 1.117, Test accuracy: 68.36
Final Round, Train loss: 0.356, Test loss: 1.067, Test accuracy: 69.52
Average accuracy final 10 rounds: 68.81099999999999
2654.6636390686035
[3.391216993331909, 6.510867595672607, 9.627904176712036, 12.758474349975586, 15.405935764312744, 18.05504059791565, 20.710093021392822, 23.37925386428833, 26.018192529678345, 28.668302536010742, 31.316418647766113, 33.96986103057861, 36.617799043655396, 39.26742959022522, 41.911357402801514, 44.52457666397095, 47.16256785392761, 49.79901170730591, 52.448546171188354, 55.09377598762512, 57.75675129890442, 60.41999816894531, 63.062193155288696, 65.70619559288025, 68.3485734462738, 70.98742985725403, 73.62404823303223, 76.26889777183533, 78.92654967308044, 81.58596634864807, 84.21492099761963, 86.87558031082153, 89.51191544532776, 92.14417147636414, 94.76355409622192, 97.3886559009552, 100.02300000190735, 102.65499973297119, 105.28667116165161, 107.93743228912354, 110.58640336990356, 113.25687456130981, 116.2213888168335, 119.17467761039734, 122.14585423469543, 125.09185647964478, 128.04859399795532, 131.0196168422699, 133.98932790756226, 136.91852140426636, 139.86138105392456, 142.80178880691528, 145.472261428833, 148.13433814048767, 150.8150827884674, 153.4735119342804, 156.14671564102173, 158.80476355552673, 161.7787573337555, 164.74611735343933, 167.72637343406677, 170.36703634262085, 173.00465631484985, 175.65180325508118, 178.29632782936096, 180.94431042671204, 183.59679007530212, 186.24911618232727, 188.8688509464264, 191.50142908096313, 194.13728070259094, 196.78619360923767, 199.41509628295898, 202.05049777030945, 204.6861605644226, 207.32844591140747, 209.95688319206238, 212.59271621704102, 215.2356195449829, 217.86853075027466, 220.5061902999878, 223.14267492294312, 225.78175926208496, 228.39532113075256, 231.01201033592224, 233.65180730819702, 236.2832372188568, 238.91540837287903, 241.54478931427002, 244.15342235565186, 246.77290630340576, 249.39795637130737, 252.04078316688538, 254.6743528842926, 257.31438660621643, 259.94207286834717, 262.58381605148315, 265.2210736274719, 267.85962772369385, 270.49908685684204, 273.4669134616852]
[26.2125, 32.5875, 37.5225, 40.85, 44.7425, 45.085, 46.8775, 49.5225, 49.545, 50.935, 51.76, 52.5925, 54.07, 55.325, 56.0275, 57.79, 57.2875, 58.8575, 59.43, 59.665, 60.095, 61.01, 60.7725, 61.555, 61.9125, 62.0425, 61.88, 62.3475, 62.815, 63.9075, 63.5125, 63.11, 64.0875, 64.2025, 64.865, 65.025, 65.66, 65.11, 65.02, 65.4925, 65.2675, 65.49, 65.42, 65.4675, 65.875, 65.925, 66.04, 66.47, 66.6575, 66.535, 66.2775, 66.47, 67.1425, 67.0175, 66.8375, 66.845, 66.8875, 67.1375, 66.9175, 67.63, 67.455, 67.3875, 66.7625, 66.925, 67.145, 67.335, 66.485, 67.345, 67.7825, 67.5125, 67.59, 67.935, 67.755, 67.37, 67.925, 68.0225, 68.3225, 68.1325, 67.855, 67.9925, 67.83, 68.4325, 68.39, 68.6625, 68.2725, 68.33, 68.865, 68.22, 68.695, 68.79, 68.8875, 69.28, 68.71, 68.58, 68.94, 68.825, 68.5125, 68.9275, 69.0875, 68.36, 69.5225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.304, Test loss: 2.247, Test accuracy: 18.90
Round   1, Train loss: 2.193, Test loss: 2.068, Test accuracy: 25.72
Round   2, Train loss: 2.038, Test loss: 1.962, Test accuracy: 29.33
Round   3, Train loss: 1.958, Test loss: 1.875, Test accuracy: 31.92
Round   4, Train loss: 1.872, Test loss: 1.794, Test accuracy: 34.85
Round   5, Train loss: 1.784, Test loss: 1.736, Test accuracy: 36.92
Round   6, Train loss: 1.746, Test loss: 1.670, Test accuracy: 39.51
Round   7, Train loss: 1.686, Test loss: 1.632, Test accuracy: 40.80
Round   8, Train loss: 1.652, Test loss: 1.601, Test accuracy: 42.07
Round   9, Train loss: 1.632, Test loss: 1.553, Test accuracy: 43.90
Round  10, Train loss: 1.592, Test loss: 1.518, Test accuracy: 44.68
Round  11, Train loss: 1.553, Test loss: 1.488, Test accuracy: 45.92
Round  12, Train loss: 1.521, Test loss: 1.472, Test accuracy: 46.67
Round  13, Train loss: 1.493, Test loss: 1.453, Test accuracy: 47.45
Round  14, Train loss: 1.457, Test loss: 1.455, Test accuracy: 48.02
Round  15, Train loss: 1.465, Test loss: 1.426, Test accuracy: 48.77
Round  16, Train loss: 1.425, Test loss: 1.419, Test accuracy: 49.14
Round  17, Train loss: 1.410, Test loss: 1.394, Test accuracy: 49.99
Round  18, Train loss: 1.391, Test loss: 1.371, Test accuracy: 50.89
Round  19, Train loss: 1.403, Test loss: 1.357, Test accuracy: 51.98
Round  20, Train loss: 1.363, Test loss: 1.344, Test accuracy: 51.98
Round  21, Train loss: 1.326, Test loss: 1.331, Test accuracy: 52.19
Round  22, Train loss: 1.333, Test loss: 1.308, Test accuracy: 52.93
Round  23, Train loss: 1.302, Test loss: 1.310, Test accuracy: 52.91
Round  24, Train loss: 1.315, Test loss: 1.282, Test accuracy: 53.99
Round  25, Train loss: 1.242, Test loss: 1.265, Test accuracy: 54.53
Round  26, Train loss: 1.273, Test loss: 1.249, Test accuracy: 55.12
Round  27, Train loss: 1.238, Test loss: 1.248, Test accuracy: 55.31
Round  28, Train loss: 1.217, Test loss: 1.238, Test accuracy: 55.95
Round  29, Train loss: 1.206, Test loss: 1.229, Test accuracy: 56.38
Round  30, Train loss: 1.212, Test loss: 1.209, Test accuracy: 57.17
Round  31, Train loss: 1.170, Test loss: 1.198, Test accuracy: 57.31
Round  32, Train loss: 1.165, Test loss: 1.180, Test accuracy: 57.87
Round  33, Train loss: 1.190, Test loss: 1.167, Test accuracy: 58.48
Round  34, Train loss: 1.140, Test loss: 1.172, Test accuracy: 58.16
Round  35, Train loss: 1.149, Test loss: 1.169, Test accuracy: 58.61
Round  36, Train loss: 1.126, Test loss: 1.153, Test accuracy: 58.74
Round  37, Train loss: 1.099, Test loss: 1.143, Test accuracy: 59.20
Round  38, Train loss: 1.072, Test loss: 1.135, Test accuracy: 59.62
Round  39, Train loss: 1.070, Test loss: 1.125, Test accuracy: 59.88
Round  40, Train loss: 1.060, Test loss: 1.109, Test accuracy: 60.86
Round  41, Train loss: 1.073, Test loss: 1.121, Test accuracy: 60.25
Round  42, Train loss: 1.031, Test loss: 1.126, Test accuracy: 59.65
Round  43, Train loss: 1.080, Test loss: 1.111, Test accuracy: 60.48
Round  44, Train loss: 1.021, Test loss: 1.112, Test accuracy: 60.66
Round  45, Train loss: 1.005, Test loss: 1.096, Test accuracy: 61.27
Round  46, Train loss: 1.046, Test loss: 1.086, Test accuracy: 61.74
Round  47, Train loss: 1.003, Test loss: 1.079, Test accuracy: 62.37
Round  48, Train loss: 0.973, Test loss: 1.081, Test accuracy: 62.13
Round  49, Train loss: 0.994, Test loss: 1.066, Test accuracy: 62.44
Round  50, Train loss: 0.983, Test loss: 1.063, Test accuracy: 62.76
Round  51, Train loss: 0.924, Test loss: 1.063, Test accuracy: 62.81
Round  52, Train loss: 0.971, Test loss: 1.058, Test accuracy: 63.23
Round  53, Train loss: 0.955, Test loss: 1.057, Test accuracy: 62.80
Round  54, Train loss: 0.939, Test loss: 1.061, Test accuracy: 62.75
Round  55, Train loss: 0.906, Test loss: 1.069, Test accuracy: 61.91
Round  56, Train loss: 0.910, Test loss: 1.064, Test accuracy: 62.03
Round  57, Train loss: 0.928, Test loss: 1.046, Test accuracy: 63.16
Round  58, Train loss: 0.884, Test loss: 1.054, Test accuracy: 63.06
Round  59, Train loss: 0.909, Test loss: 1.041, Test accuracy: 63.44
Round  60, Train loss: 0.870, Test loss: 1.034, Test accuracy: 63.41
Round  61, Train loss: 0.845, Test loss: 1.032, Test accuracy: 63.29
Round  62, Train loss: 0.851, Test loss: 1.036, Test accuracy: 63.27
Round  63, Train loss: 0.873, Test loss: 1.038, Test accuracy: 63.18
Round  64, Train loss: 0.874, Test loss: 1.035, Test accuracy: 63.65
Round  65, Train loss: 0.837, Test loss: 1.039, Test accuracy: 63.55
Round  66, Train loss: 0.814, Test loss: 1.008, Test accuracy: 64.75
Round  67, Train loss: 0.833, Test loss: 1.013, Test accuracy: 64.89
Round  68, Train loss: 0.834, Test loss: 1.000, Test accuracy: 65.17
Round  69, Train loss: 0.808, Test loss: 0.998, Test accuracy: 65.44
Round  70, Train loss: 0.781, Test loss: 1.014, Test accuracy: 64.78
Round  71, Train loss: 0.819, Test loss: 1.005, Test accuracy: 64.86
Round  72, Train loss: 0.805, Test loss: 0.994, Test accuracy: 65.48
Round  73, Train loss: 0.786, Test loss: 0.999, Test accuracy: 65.58
Round  74, Train loss: 0.806, Test loss: 0.995, Test accuracy: 65.78
Round  75, Train loss: 0.769, Test loss: 0.987, Test accuracy: 65.72
Round  76, Train loss: 0.797, Test loss: 0.984, Test accuracy: 66.02
Round  77, Train loss: 0.768, Test loss: 0.979, Test accuracy: 66.11
Round  78, Train loss: 0.755, Test loss: 0.990, Test accuracy: 65.83
Round  79, Train loss: 0.767, Test loss: 0.975, Test accuracy: 66.25
Round  80, Train loss: 0.742, Test loss: 0.976, Test accuracy: 65.95
Round  81, Train loss: 0.769, Test loss: 0.981, Test accuracy: 66.21
Round  82, Train loss: 0.717, Test loss: 0.971, Test accuracy: 66.72
Round  83, Train loss: 0.725, Test loss: 0.971, Test accuracy: 66.46
Round  84, Train loss: 0.716, Test loss: 0.969, Test accuracy: 66.65
Round  85, Train loss: 0.720, Test loss: 0.969, Test accuracy: 66.78
Round  86, Train loss: 0.679, Test loss: 0.978, Test accuracy: 66.63
Round  87, Train loss: 0.698, Test loss: 0.980, Test accuracy: 66.67
Round  88, Train loss: 0.723, Test loss: 0.986, Test accuracy: 66.44
Round  89, Train loss: 0.687, Test loss: 0.983, Test accuracy: 66.31
Round  90, Train loss: 0.667, Test loss: 0.985, Test accuracy: 66.47
Round  91, Train loss: 0.686, Test loss: 0.974, Test accuracy: 66.64
Round  92, Train loss: 0.684, Test loss: 0.985, Test accuracy: 66.12
Round  93, Train loss: 0.676, Test loss: 0.969, Test accuracy: 67.17
Round  94, Train loss: 0.700, Test loss: 0.958, Test accuracy: 67.28
Round  95, Train loss: 0.640, Test loss: 0.963, Test accuracy: 67.19
Round  96, Train loss: 0.647, Test loss: 0.977, Test accuracy: 66.70
Round  97, Train loss: 0.688, Test loss: 0.965, Test accuracy: 67.03
Round  98, Train loss: 0.635, Test loss: 0.972, Test accuracy: 67.14
Round  99, Train loss: 0.654, Test loss: 0.967, Test accuracy: 67.03
Final Round, Train loss: 0.557, Test loss: 0.964, Test accuracy: 67.29
Average accuracy final 10 rounds: 66.87875
1787.9132134914398
[1.6487712860107422, 2.9931936264038086, 4.352847576141357, 5.681507349014282, 7.004239559173584, 8.337772846221924, 9.656641960144043, 10.949867725372314, 12.293158531188965, 13.641947031021118, 14.952504396438599, 16.314541816711426, 17.646867752075195, 18.96052646636963, 20.305304527282715, 21.624603986740112, 22.92322087287903, 24.249229192733765, 25.574799060821533, 26.875356674194336, 28.206486225128174, 29.556062936782837, 30.883919715881348, 32.210134983062744, 33.53318643569946, 34.85143041610718, 36.20713663101196, 37.55802369117737, 38.88243341445923, 40.23321747779846, 41.5705041885376, 42.875213861465454, 44.21522903442383, 45.57059836387634, 46.871559858322144, 48.1946485042572, 49.53984594345093, 50.8627655506134, 52.213727951049805, 53.56584167480469, 54.893717527389526, 56.243042945861816, 57.603447675704956, 58.9409601688385, 60.292325496673584, 61.647855281829834, 62.96824312210083, 64.29674100875854, 65.63834547996521, 66.97518134117126, 68.31615352630615, 69.66333174705505, 71.02466917037964, 72.36432194709778, 73.69875812530518, 75.03395104408264, 76.37981724739075, 77.72036457061768, 79.06035327911377, 80.40453624725342, 81.7296953201294, 83.06903409957886, 84.40116167068481, 85.70294737815857, 86.92602252960205, 88.15572428703308, 89.46191883087158, 90.79407835006714, 92.12492537498474, 93.4205846786499, 94.75116395950317, 96.07460689544678, 97.3797960281372, 98.58751320838928, 99.78058099746704, 100.97178339958191, 102.1799008846283, 103.3711428642273, 104.56792187690735, 105.77842330932617, 106.97197484970093, 108.1678192615509, 109.37505674362183, 110.56742310523987, 111.76087307929993, 112.96550512313843, 114.15499758720398, 115.35108423233032, 116.55656051635742, 117.74582052230835, 118.94256353378296, 120.1340684890747, 121.32332563400269, 122.53498864173889, 123.73027658462524, 124.9255063533783, 126.12433052062988, 127.3152494430542, 128.51235818862915, 129.71503710746765, 131.6641001701355]
[18.9, 25.7225, 29.33, 31.9225, 34.8475, 36.9225, 39.51, 40.7975, 42.0675, 43.9, 44.6775, 45.92, 46.6725, 47.4475, 48.0175, 48.7725, 49.1375, 49.99, 50.8875, 51.98, 51.975, 52.1925, 52.9325, 52.915, 53.995, 54.5325, 55.12, 55.3125, 55.9525, 56.385, 57.1725, 57.31, 57.8675, 58.475, 58.1625, 58.61, 58.74, 59.1975, 59.62, 59.88, 60.86, 60.25, 59.6525, 60.48, 60.6625, 61.27, 61.745, 62.3725, 62.1325, 62.4375, 62.7625, 62.815, 63.2325, 62.795, 62.7475, 61.915, 62.0325, 63.165, 63.0575, 63.435, 63.415, 63.2925, 63.2675, 63.1775, 63.6475, 63.55, 64.75, 64.8925, 65.175, 65.445, 64.775, 64.8575, 65.48, 65.5775, 65.78, 65.7225, 66.0225, 66.115, 65.825, 66.2525, 65.9525, 66.2125, 66.715, 66.4575, 66.6525, 66.7775, 66.6325, 66.67, 66.435, 66.3125, 66.475, 66.6375, 66.12, 67.1725, 67.2825, 67.1925, 66.705, 67.0325, 67.145, 67.025, 67.29]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.303, Test loss: 2.216, Test accuracy: 20.67
Round   1, Train loss: 2.158, Test loss: 2.023, Test accuracy: 27.61
Round   2, Train loss: 2.023, Test loss: 1.934, Test accuracy: 30.50
Round   3, Train loss: 1.934, Test loss: 1.825, Test accuracy: 34.42
Round   4, Train loss: 1.844, Test loss: 1.757, Test accuracy: 36.93
Round   5, Train loss: 1.775, Test loss: 1.713, Test accuracy: 38.43
Round   6, Train loss: 1.722, Test loss: 1.652, Test accuracy: 40.42
Round   7, Train loss: 1.687, Test loss: 1.628, Test accuracy: 40.94
Round   8, Train loss: 1.679, Test loss: 1.580, Test accuracy: 43.20
Round   9, Train loss: 1.624, Test loss: 1.578, Test accuracy: 42.69
Round  10, Train loss: 1.628, Test loss: 1.524, Test accuracy: 44.66
Round  11, Train loss: 1.588, Test loss: 1.495, Test accuracy: 45.52
Round  12, Train loss: 1.541, Test loss: 1.466, Test accuracy: 47.01
Round  13, Train loss: 1.503, Test loss: 1.448, Test accuracy: 47.82
Round  14, Train loss: 1.483, Test loss: 1.430, Test accuracy: 48.55
Round  15, Train loss: 1.496, Test loss: 1.408, Test accuracy: 48.82
Round  16, Train loss: 1.457, Test loss: 1.398, Test accuracy: 49.99
Round  17, Train loss: 1.431, Test loss: 1.367, Test accuracy: 50.53
Round  18, Train loss: 1.413, Test loss: 1.354, Test accuracy: 51.73
Round  19, Train loss: 1.397, Test loss: 1.351, Test accuracy: 52.09
Round  20, Train loss: 1.394, Test loss: 1.320, Test accuracy: 53.24
Round  21, Train loss: 1.347, Test loss: 1.300, Test accuracy: 54.14
Round  22, Train loss: 1.329, Test loss: 1.280, Test accuracy: 54.72
Round  23, Train loss: 1.287, Test loss: 1.268, Test accuracy: 55.14
Round  24, Train loss: 1.280, Test loss: 1.255, Test accuracy: 55.77
Round  25, Train loss: 1.260, Test loss: 1.257, Test accuracy: 55.33
Round  26, Train loss: 1.256, Test loss: 1.241, Test accuracy: 56.14
Round  27, Train loss: 1.263, Test loss: 1.227, Test accuracy: 56.44
Round  28, Train loss: 1.259, Test loss: 1.217, Test accuracy: 56.74
Round  29, Train loss: 1.213, Test loss: 1.204, Test accuracy: 57.15
Round  30, Train loss: 1.173, Test loss: 1.211, Test accuracy: 57.52
Round  31, Train loss: 1.160, Test loss: 1.182, Test accuracy: 58.47
Round  32, Train loss: 1.170, Test loss: 1.173, Test accuracy: 59.00
Round  33, Train loss: 1.154, Test loss: 1.169, Test accuracy: 58.65
Round  34, Train loss: 1.164, Test loss: 1.142, Test accuracy: 59.83
Round  35, Train loss: 1.120, Test loss: 1.147, Test accuracy: 59.20
Round  36, Train loss: 1.120, Test loss: 1.132, Test accuracy: 60.12
Round  37, Train loss: 1.096, Test loss: 1.142, Test accuracy: 59.64
Round  38, Train loss: 1.116, Test loss: 1.117, Test accuracy: 60.49
Round  39, Train loss: 1.063, Test loss: 1.118, Test accuracy: 60.72
Round  40, Train loss: 1.079, Test loss: 1.098, Test accuracy: 61.37
Round  41, Train loss: 1.068, Test loss: 1.105, Test accuracy: 61.26
Round  42, Train loss: 1.081, Test loss: 1.100, Test accuracy: 61.38
Round  43, Train loss: 1.047, Test loss: 1.104, Test accuracy: 61.45
Round  44, Train loss: 1.041, Test loss: 1.091, Test accuracy: 61.17
Round  45, Train loss: 1.031, Test loss: 1.086, Test accuracy: 61.91
Round  46, Train loss: 1.009, Test loss: 1.093, Test accuracy: 61.79
Round  47, Train loss: 0.998, Test loss: 1.093, Test accuracy: 61.95
Round  48, Train loss: 0.959, Test loss: 1.087, Test accuracy: 62.10
Round  49, Train loss: 0.972, Test loss: 1.047, Test accuracy: 63.72
Round  50, Train loss: 0.944, Test loss: 1.046, Test accuracy: 63.60
Round  51, Train loss: 0.943, Test loss: 1.039, Test accuracy: 64.18
Round  52, Train loss: 0.946, Test loss: 1.041, Test accuracy: 63.79
Round  53, Train loss: 0.910, Test loss: 1.030, Test accuracy: 64.10
Round  54, Train loss: 0.914, Test loss: 1.029, Test accuracy: 64.32
Round  55, Train loss: 0.893, Test loss: 1.027, Test accuracy: 64.49
Round  56, Train loss: 0.896, Test loss: 1.024, Test accuracy: 64.63
Round  57, Train loss: 0.871, Test loss: 1.020, Test accuracy: 64.64
Round  58, Train loss: 0.900, Test loss: 1.029, Test accuracy: 64.50
Round  59, Train loss: 0.915, Test loss: 1.014, Test accuracy: 64.94
Round  60, Train loss: 0.906, Test loss: 1.010, Test accuracy: 64.99
Round  61, Train loss: 0.877, Test loss: 1.016, Test accuracy: 64.91
Round  62, Train loss: 0.846, Test loss: 1.015, Test accuracy: 65.04
Round  63, Train loss: 0.832, Test loss: 1.007, Test accuracy: 65.23
Round  64, Train loss: 0.833, Test loss: 1.002, Test accuracy: 65.44
Round  65, Train loss: 0.818, Test loss: 1.006, Test accuracy: 65.17
Round  66, Train loss: 0.830, Test loss: 1.003, Test accuracy: 65.56
Round  67, Train loss: 0.840, Test loss: 1.001, Test accuracy: 65.37
Round  68, Train loss: 0.849, Test loss: 1.003, Test accuracy: 65.20
Round  69, Train loss: 0.795, Test loss: 1.000, Test accuracy: 65.72
Round  70, Train loss: 0.780, Test loss: 0.989, Test accuracy: 66.05
Round  71, Train loss: 0.834, Test loss: 0.995, Test accuracy: 65.52
Round  72, Train loss: 0.756, Test loss: 1.000, Test accuracy: 65.76
Round  73, Train loss: 0.757, Test loss: 0.997, Test accuracy: 65.52
Round  74, Train loss: 0.773, Test loss: 0.993, Test accuracy: 65.75
Round  75, Train loss: 0.762, Test loss: 1.002, Test accuracy: 65.49
Round  76, Train loss: 0.782, Test loss: 0.993, Test accuracy: 65.98
Round  77, Train loss: 0.800, Test loss: 0.982, Test accuracy: 66.28
Round  78, Train loss: 0.757, Test loss: 0.979, Test accuracy: 66.61
Round  79, Train loss: 0.742, Test loss: 0.996, Test accuracy: 65.96
Round  80, Train loss: 0.713, Test loss: 0.999, Test accuracy: 65.91
Round  81, Train loss: 0.734, Test loss: 0.995, Test accuracy: 65.83
Round  82, Train loss: 0.733, Test loss: 0.987, Test accuracy: 66.03
Round  83, Train loss: 0.694, Test loss: 0.990, Test accuracy: 66.23
Round  84, Train loss: 0.677, Test loss: 1.000, Test accuracy: 65.78
Round  85, Train loss: 0.685, Test loss: 0.994, Test accuracy: 66.00
Round  86, Train loss: 0.685, Test loss: 0.983, Test accuracy: 66.28
Round  87, Train loss: 0.669, Test loss: 0.972, Test accuracy: 66.78
Round  88, Train loss: 0.659, Test loss: 0.987, Test accuracy: 66.28
Round  89, Train loss: 0.678, Test loss: 0.976, Test accuracy: 66.53
Round  90, Train loss: 0.701, Test loss: 0.965, Test accuracy: 67.08
Round  91, Train loss: 0.669, Test loss: 0.968, Test accuracy: 67.11
Round  92, Train loss: 0.725, Test loss: 0.977, Test accuracy: 66.77
Round  93, Train loss: 0.666, Test loss: 0.962, Test accuracy: 66.97
Round  94, Train loss: 0.628, Test loss: 0.958, Test accuracy: 67.37
Round  95, Train loss: 0.681, Test loss: 0.970, Test accuracy: 67.08
Round  96, Train loss: 0.659, Test loss: 0.958, Test accuracy: 67.81
Round  97, Train loss: 0.645, Test loss: 0.962, Test accuracy: 67.58
Round  98, Train loss: 0.636, Test loss: 0.957, Test accuracy: 67.46
Round  99, Train loss: 0.588, Test loss: 0.962, Test accuracy: 67.22
Final Round, Train loss: 0.564, Test loss: 0.963, Test accuracy: 67.47
Average accuracy final 10 rounds: 67.245
1787.5808215141296
[1.7045114040374756, 3.0415074825286865, 4.397817373275757, 5.7288007736206055, 7.039016246795654, 8.396040439605713, 9.760920524597168, 11.071078538894653, 12.421535015106201, 13.750141143798828, 15.05875849723816, 16.409377574920654, 17.738054513931274, 19.074547290802002, 20.429669857025146, 21.771230220794678, 23.11348247528076, 24.463698148727417, 25.820094347000122, 27.176501035690308, 28.53590178489685, 29.889007568359375, 31.246040105819702, 32.60482454299927, 33.966991901397705, 35.31806778907776, 36.68272805213928, 38.04035520553589, 39.38191032409668, 40.74312400817871, 42.096261739730835, 43.43114519119263, 44.79427218437195, 46.15520358085632, 47.500895738601685, 48.85998773574829, 50.21801710128784, 51.4126091003418, 52.61147451400757, 53.81813073158264, 55.01661419868469, 56.21754336357117, 57.426552295684814, 58.623146057128906, 59.821346044540405, 61.01857805252075, 62.21694231033325, 63.41817545890808, 64.61811828613281, 65.81904125213623, 67.0335328578949, 68.23303389549255, 69.44347357749939, 70.65883898735046, 71.85581588745117, 73.0666024684906, 74.27500009536743, 75.47600150108337, 76.69155311584473, 77.89349842071533, 79.09521460533142, 80.30345726013184, 81.5046980381012, 82.70581483840942, 83.92082023620605, 85.11958146095276, 86.33503651618958, 87.54934740066528, 88.748370885849, 89.965651512146, 91.18344950675964, 92.38312578201294, 93.59428930282593, 94.79888105392456, 96.0009834766388, 97.2142424583435, 98.41900038719177, 99.62119770050049, 100.85800313949585, 102.0621235370636, 103.26216959953308, 104.46286916732788, 105.66848921775818, 106.88472199440002, 108.0910906791687, 109.29487895965576, 110.50584650039673, 111.70528435707092, 112.90531802177429, 114.12299036979675, 115.32083010673523, 116.52434206008911, 117.73059701919556, 118.92855262756348, 120.13356184959412, 121.34280514717102, 122.54140448570251, 123.7418282032013, 124.95524787902832, 126.16196203231812, 128.1443738937378]
[20.67, 27.6125, 30.4975, 34.4175, 36.93, 38.4275, 40.4175, 40.94, 43.195, 42.685, 44.665, 45.525, 47.01, 47.8225, 48.55, 48.8225, 49.99, 50.5275, 51.725, 52.095, 53.2425, 54.1375, 54.7225, 55.14, 55.7725, 55.325, 56.1375, 56.4425, 56.74, 57.145, 57.5175, 58.465, 59.0, 58.6475, 59.8325, 59.1975, 60.1225, 59.6425, 60.4875, 60.715, 61.37, 61.2575, 61.385, 61.4475, 61.175, 61.9075, 61.7925, 61.9475, 62.0975, 63.7225, 63.6025, 64.1825, 63.7875, 64.0975, 64.32, 64.4925, 64.6325, 64.6425, 64.5, 64.9425, 64.99, 64.9125, 65.04, 65.2275, 65.445, 65.1725, 65.5625, 65.37, 65.2025, 65.72, 66.0475, 65.52, 65.7575, 65.52, 65.7475, 65.49, 65.985, 66.285, 66.605, 65.9625, 65.905, 65.835, 66.0325, 66.2325, 65.7825, 66.0, 66.28, 66.785, 66.285, 66.5275, 67.085, 67.105, 66.765, 66.97, 67.37, 67.075, 67.815, 67.5825, 67.46, 67.2225, 67.47]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.965, Test loss: 1.055, Test accuracy: 39.90 

Round   0, Global train loss: 0.965, Global test loss: 1.121, Global test accuracy: 34.55 

Round   1, Train loss: 0.762, Test loss: 0.969, Test accuracy: 51.12 

Round   1, Global train loss: 0.762, Global test loss: 1.191, Global test accuracy: 37.68 

Round   2, Train loss: 0.661, Test loss: 0.843, Test accuracy: 57.25 

Round   2, Global train loss: 0.661, Global test loss: 1.120, Global test accuracy: 38.59 

Round   3, Train loss: 0.687, Test loss: 0.822, Test accuracy: 58.29 

Round   3, Global train loss: 0.687, Global test loss: 1.160, Global test accuracy: 33.71 

Round   4, Train loss: 0.704, Test loss: 0.806, Test accuracy: 61.25 

Round   4, Global train loss: 0.704, Global test loss: 1.233, Global test accuracy: 34.51 

Round   5, Train loss: 0.623, Test loss: 0.722, Test accuracy: 65.13 

Round   5, Global train loss: 0.623, Global test loss: 1.147, Global test accuracy: 35.77 

Round   6, Train loss: 0.559, Test loss: 0.670, Test accuracy: 69.73 

Round   6, Global train loss: 0.559, Global test loss: 1.120, Global test accuracy: 38.48 

Round   7, Train loss: 0.566, Test loss: 0.648, Test accuracy: 71.42 

Round   7, Global train loss: 0.566, Global test loss: 1.160, Global test accuracy: 35.42 

Round   8, Train loss: 0.574, Test loss: 0.627, Test accuracy: 72.29 

Round   8, Global train loss: 0.574, Global test loss: 1.247, Global test accuracy: 36.02 

Round   9, Train loss: 0.488, Test loss: 0.619, Test accuracy: 73.18 

Round   9, Global train loss: 0.488, Global test loss: 1.329, Global test accuracy: 38.83 

Round  10, Train loss: 0.472, Test loss: 0.631, Test accuracy: 72.61 

Round  10, Global train loss: 0.472, Global test loss: 1.224, Global test accuracy: 37.75 

Round  11, Train loss: 0.461, Test loss: 0.619, Test accuracy: 72.19 

Round  11, Global train loss: 0.461, Global test loss: 1.133, Global test accuracy: 35.25 

Round  12, Train loss: 0.440, Test loss: 0.613, Test accuracy: 73.30 

Round  12, Global train loss: 0.440, Global test loss: 1.244, Global test accuracy: 39.15 

Round  13, Train loss: 0.335, Test loss: 0.591, Test accuracy: 74.03 

Round  13, Global train loss: 0.335, Global test loss: 1.148, Global test accuracy: 35.79 

Round  14, Train loss: 0.407, Test loss: 0.587, Test accuracy: 74.18 

Round  14, Global train loss: 0.407, Global test loss: 1.424, Global test accuracy: 31.38 

Round  15, Train loss: 0.375, Test loss: 0.594, Test accuracy: 74.58 

Round  15, Global train loss: 0.375, Global test loss: 1.271, Global test accuracy: 37.86 

Round  16, Train loss: 0.501, Test loss: 0.599, Test accuracy: 73.92 

Round  16, Global train loss: 0.501, Global test loss: 1.239, Global test accuracy: 37.50 

Round  17, Train loss: 0.439, Test loss: 0.571, Test accuracy: 75.97 

Round  17, Global train loss: 0.439, Global test loss: 1.154, Global test accuracy: 35.62 

Round  18, Train loss: 0.360, Test loss: 0.592, Test accuracy: 74.56 

Round  18, Global train loss: 0.360, Global test loss: 1.164, Global test accuracy: 34.46 

Round  19, Train loss: 0.341, Test loss: 0.626, Test accuracy: 74.94 

Round  19, Global train loss: 0.341, Global test loss: 1.387, Global test accuracy: 39.19 

Round  20, Train loss: 0.360, Test loss: 0.605, Test accuracy: 74.37 

Round  20, Global train loss: 0.360, Global test loss: 1.233, Global test accuracy: 34.83 

Round  21, Train loss: 0.401, Test loss: 0.578, Test accuracy: 76.30 

Round  21, Global train loss: 0.401, Global test loss: 1.216, Global test accuracy: 39.12 

Round  22, Train loss: 0.409, Test loss: 0.542, Test accuracy: 78.25 

Round  22, Global train loss: 0.409, Global test loss: 1.180, Global test accuracy: 34.74 

Round  23, Train loss: 0.326, Test loss: 0.545, Test accuracy: 78.09 

Round  23, Global train loss: 0.326, Global test loss: 1.289, Global test accuracy: 35.52 

Round  24, Train loss: 0.294, Test loss: 0.547, Test accuracy: 78.38 

Round  24, Global train loss: 0.294, Global test loss: 1.310, Global test accuracy: 37.58 

Round  25, Train loss: 0.343, Test loss: 0.545, Test accuracy: 78.62 

Round  25, Global train loss: 0.343, Global test loss: 1.159, Global test accuracy: 38.30 

Round  26, Train loss: 0.298, Test loss: 0.556, Test accuracy: 79.12 

Round  26, Global train loss: 0.298, Global test loss: 1.220, Global test accuracy: 32.45 

Round  27, Train loss: 0.432, Test loss: 0.555, Test accuracy: 79.03 

Round  27, Global train loss: 0.432, Global test loss: 1.159, Global test accuracy: 33.82 

Round  28, Train loss: 0.429, Test loss: 0.559, Test accuracy: 79.46 

Round  28, Global train loss: 0.429, Global test loss: 1.204, Global test accuracy: 34.77 

Round  29, Train loss: 0.279, Test loss: 0.552, Test accuracy: 79.88 

Round  29, Global train loss: 0.279, Global test loss: 1.220, Global test accuracy: 35.42 

Round  30, Train loss: 0.320, Test loss: 0.561, Test accuracy: 79.72 

Round  30, Global train loss: 0.320, Global test loss: 1.308, Global test accuracy: 30.61 

Round  31, Train loss: 0.346, Test loss: 0.558, Test accuracy: 79.62 

Round  31, Global train loss: 0.346, Global test loss: 1.426, Global test accuracy: 38.34 

Round  32, Train loss: 0.251, Test loss: 0.556, Test accuracy: 79.92 

Round  32, Global train loss: 0.251, Global test loss: 1.352, Global test accuracy: 38.59 

Round  33, Train loss: 0.227, Test loss: 0.588, Test accuracy: 79.62 

Round  33, Global train loss: 0.227, Global test loss: 1.261, Global test accuracy: 35.35 

Round  34, Train loss: 0.231, Test loss: 0.614, Test accuracy: 79.32 

Round  34, Global train loss: 0.231, Global test loss: 1.449, Global test accuracy: 38.51 

Round  35, Train loss: 0.227, Test loss: 0.623, Test accuracy: 79.28 

Round  35, Global train loss: 0.227, Global test loss: 1.298, Global test accuracy: 33.23 

Round  36, Train loss: 0.219, Test loss: 0.622, Test accuracy: 79.42 

Round  36, Global train loss: 0.219, Global test loss: 1.354, Global test accuracy: 35.96 

Round  37, Train loss: 0.271, Test loss: 0.613, Test accuracy: 79.85 

Round  37, Global train loss: 0.271, Global test loss: 1.228, Global test accuracy: 39.27 

Round  38, Train loss: 0.148, Test loss: 0.611, Test accuracy: 79.85 

Round  38, Global train loss: 0.148, Global test loss: 1.307, Global test accuracy: 38.98 

Round  39, Train loss: 0.188, Test loss: 0.610, Test accuracy: 79.96 

Round  39, Global train loss: 0.188, Global test loss: 1.171, Global test accuracy: 38.60 

Round  40, Train loss: 0.239, Test loss: 0.596, Test accuracy: 80.33 

Round  40, Global train loss: 0.239, Global test loss: 1.476, Global test accuracy: 32.97 

Round  41, Train loss: 0.182, Test loss: 0.615, Test accuracy: 80.72 

Round  41, Global train loss: 0.182, Global test loss: 1.207, Global test accuracy: 36.90 

Round  42, Train loss: 0.210, Test loss: 0.618, Test accuracy: 80.71 

Round  42, Global train loss: 0.210, Global test loss: 1.578, Global test accuracy: 37.70 

Round  43, Train loss: 0.153, Test loss: 0.644, Test accuracy: 80.03 

Round  43, Global train loss: 0.153, Global test loss: 1.417, Global test accuracy: 38.28 

Round  44, Train loss: 0.190, Test loss: 0.660, Test accuracy: 80.12 

Round  44, Global train loss: 0.190, Global test loss: 1.328, Global test accuracy: 32.98 

Round  45, Train loss: 0.201, Test loss: 0.662, Test accuracy: 79.96 

Round  45, Global train loss: 0.201, Global test loss: 1.276, Global test accuracy: 38.32 

Round  46, Train loss: 0.228, Test loss: 0.671, Test accuracy: 80.25 

Round  46, Global train loss: 0.228, Global test loss: 1.213, Global test accuracy: 33.64 

Round  47, Train loss: 0.177, Test loss: 0.674, Test accuracy: 80.26 

Round  47, Global train loss: 0.177, Global test loss: 1.285, Global test accuracy: 34.65 

Round  48, Train loss: 0.197, Test loss: 0.690, Test accuracy: 80.06 

Round  48, Global train loss: 0.197, Global test loss: 1.302, Global test accuracy: 37.59 

Round  49, Train loss: 0.146, Test loss: 0.686, Test accuracy: 80.24 

Round  49, Global train loss: 0.146, Global test loss: 1.175, Global test accuracy: 39.59 

Round  50, Train loss: 0.170, Test loss: 0.691, Test accuracy: 80.06 

Round  50, Global train loss: 0.170, Global test loss: 1.286, Global test accuracy: 37.23 

Round  51, Train loss: 0.195, Test loss: 0.717, Test accuracy: 79.62 

Round  51, Global train loss: 0.195, Global test loss: 1.405, Global test accuracy: 36.98 

Round  52, Train loss: 0.184, Test loss: 0.700, Test accuracy: 79.86 

Round  52, Global train loss: 0.184, Global test loss: 1.310, Global test accuracy: 38.17 

Round  53, Train loss: 0.094, Test loss: 0.720, Test accuracy: 79.94 

Round  53, Global train loss: 0.094, Global test loss: 1.347, Global test accuracy: 32.33 

Round  54, Train loss: 0.110, Test loss: 0.734, Test accuracy: 79.90 

Round  54, Global train loss: 0.110, Global test loss: 1.704, Global test accuracy: 38.23 

Round  55, Train loss: 0.126, Test loss: 0.761, Test accuracy: 80.00 

Round  55, Global train loss: 0.126, Global test loss: 1.453, Global test accuracy: 38.67 

Round  56, Train loss: 0.153, Test loss: 0.765, Test accuracy: 80.19 

Round  56, Global train loss: 0.153, Global test loss: 1.322, Global test accuracy: 34.70 

Round  57, Train loss: 0.192, Test loss: 0.773, Test accuracy: 80.04 

Round  57, Global train loss: 0.192, Global test loss: 1.232, Global test accuracy: 36.15 

Round  58, Train loss: 0.123, Test loss: 0.762, Test accuracy: 80.49 

Round  58, Global train loss: 0.123, Global test loss: 1.421, Global test accuracy: 31.67 

Round  59, Train loss: 0.142, Test loss: 0.779, Test accuracy: 80.07 

Round  59, Global train loss: 0.142, Global test loss: 1.486, Global test accuracy: 38.44 

Round  60, Train loss: 0.095, Test loss: 0.734, Test accuracy: 80.40 

Round  60, Global train loss: 0.095, Global test loss: 1.938, Global test accuracy: 37.91 

Round  61, Train loss: 0.155, Test loss: 0.743, Test accuracy: 80.44 

Round  61, Global train loss: 0.155, Global test loss: 1.214, Global test accuracy: 39.23 

Round  62, Train loss: 0.089, Test loss: 0.772, Test accuracy: 80.06 

Round  62, Global train loss: 0.089, Global test loss: 1.578, Global test accuracy: 33.31 

Round  63, Train loss: 0.100, Test loss: 0.776, Test accuracy: 80.49 

Round  63, Global train loss: 0.100, Global test loss: 1.380, Global test accuracy: 33.98 

Round  64, Train loss: 0.139, Test loss: 0.795, Test accuracy: 80.32 

Round  64, Global train loss: 0.139, Global test loss: 1.305, Global test accuracy: 37.40 

Round  65, Train loss: 0.119, Test loss: 0.749, Test accuracy: 80.91 

Round  65, Global train loss: 0.119, Global test loss: 1.468, Global test accuracy: 34.31 

Round  66, Train loss: 0.098, Test loss: 0.767, Test accuracy: 80.65 

Round  66, Global train loss: 0.098, Global test loss: 1.723, Global test accuracy: 39.08 

Round  67, Train loss: 0.113, Test loss: 0.757, Test accuracy: 81.24 

Round  67, Global train loss: 0.113, Global test loss: 1.447, Global test accuracy: 37.57 

Round  68, Train loss: 0.072, Test loss: 0.784, Test accuracy: 81.07 

Round  68, Global train loss: 0.072, Global test loss: 1.220, Global test accuracy: 37.51 

Round  69, Train loss: 0.120, Test loss: 0.811, Test accuracy: 81.17 

Round  69, Global train loss: 0.120, Global test loss: 1.348, Global test accuracy: 33.27 

Round  70, Train loss: 0.083, Test loss: 0.812, Test accuracy: 81.22 

Round  70, Global train loss: 0.083, Global test loss: 1.645, Global test accuracy: 39.48 

Round  71, Train loss: 0.089, Test loss: 0.821, Test accuracy: 80.85 

Round  71, Global train loss: 0.089, Global test loss: 1.300, Global test accuracy: 38.20 

Round  72, Train loss: 0.100, Test loss: 0.801, Test accuracy: 80.58 

Round  72, Global train loss: 0.100, Global test loss: 1.340, Global test accuracy: 33.28 

Round  73, Train loss: 0.063, Test loss: 0.795, Test accuracy: 80.61 

Round  73, Global train loss: 0.063, Global test loss: 1.264, Global test accuracy: 37.56 

Round  74, Train loss: 0.105, Test loss: 0.806, Test accuracy: 80.63 

Round  74, Global train loss: 0.105, Global test loss: 1.200, Global test accuracy: 37.11 

Round  75, Train loss: 0.110, Test loss: 0.835, Test accuracy: 80.56 

Round  75, Global train loss: 0.110, Global test loss: 1.190, Global test accuracy: 37.67 

Round  76, Train loss: 0.118, Test loss: 0.817, Test accuracy: 80.89 

Round  76, Global train loss: 0.118, Global test loss: 1.903, Global test accuracy: 39.49 

Round  77, Train loss: 0.040, Test loss: 0.832, Test accuracy: 80.92 

Round  77, Global train loss: 0.040, Global test loss: 1.367, Global test accuracy: 39.21 

Round  78, Train loss: 0.076, Test loss: 0.841, Test accuracy: 80.89 

Round  78, Global train loss: 0.076, Global test loss: 1.351, Global test accuracy: 37.54 

Round  79, Train loss: 0.073, Test loss: 0.856, Test accuracy: 80.71 

Round  79, Global train loss: 0.073, Global test loss: 1.253, Global test accuracy: 35.70 

Round  80, Train loss: 0.060, Test loss: 0.856, Test accuracy: 80.82 

Round  80, Global train loss: 0.060, Global test loss: 1.575, Global test accuracy: 39.18 

Round  81, Train loss: 0.069, Test loss: 0.856, Test accuracy: 80.92 

Round  81, Global train loss: 0.069, Global test loss: 1.501, Global test accuracy: 37.86 

Round  82, Train loss: 0.086, Test loss: 0.876, Test accuracy: 80.66 

Round  82, Global train loss: 0.086, Global test loss: 1.282, Global test accuracy: 38.21 

Round  83, Train loss: 0.082, Test loss: 0.866, Test accuracy: 80.73 

Round  83, Global train loss: 0.082, Global test loss: 1.648, Global test accuracy: 38.12 

Round  84, Train loss: 0.044, Test loss: 0.886, Test accuracy: 80.95 

Round  84, Global train loss: 0.044, Global test loss: 1.453, Global test accuracy: 34.39 

Round  85, Train loss: 0.068, Test loss: 0.884, Test accuracy: 81.03 

Round  85, Global train loss: 0.068, Global test loss: 1.494, Global test accuracy: 38.69 

Round  86, Train loss: 0.056, Test loss: 0.903, Test accuracy: 80.78 

Round  86, Global train loss: 0.056, Global test loss: 1.929, Global test accuracy: 38.93 

Round  87, Train loss: 0.075, Test loss: 0.899, Test accuracy: 80.85 

Round  87, Global train loss: 0.075, Global test loss: 1.275, Global test accuracy: 35.97 

Round  88, Train loss: 0.080, Test loss: 0.893, Test accuracy: 80.71 

Round  88, Global train loss: 0.080, Global test loss: 1.407, Global test accuracy: 37.52 

Round  89, Train loss: 0.056, Test loss: 0.910, Test accuracy: 80.77 

Round  89, Global train loss: 0.056, Global test loss: 1.676, Global test accuracy: 35.31 

Round  90, Train loss: 0.060, Test loss: 0.891, Test accuracy: 81.17 

Round  90, Global train loss: 0.060, Global test loss: 1.298, Global test accuracy: 36.45 

Round  91, Train loss: 0.042, Test loss: 0.940, Test accuracy: 80.74 

Round  91, Global train loss: 0.042, Global test loss: 1.303, Global test accuracy: 37.32 

Round  92, Train loss: 0.053, Test loss: 0.939, Test accuracy: 80.49 

Round  92, Global train loss: 0.053, Global test loss: 1.250, Global test accuracy: 37.53 

Round  93, Train loss: 0.060, Test loss: 0.919, Test accuracy: 80.79 

Round  93, Global train loss: 0.060, Global test loss: 1.651, Global test accuracy: 32.43 

Round  94, Train loss: 0.045, Test loss: 0.941, Test accuracy: 80.85 

Round  94, Global train loss: 0.045, Global test loss: 1.294, Global test accuracy: 37.87 

Round  95, Train loss: 0.079, Test loss: 0.944, Test accuracy: 80.67 

Round  95, Global train loss: 0.079, Global test loss: 1.240, Global test accuracy: 38.52 

Round  96, Train loss: 0.083, Test loss: 0.970, Test accuracy: 80.33 

Round  96, Global train loss: 0.083, Global test loss: 1.399, Global test accuracy: 39.56 

Round  97, Train loss: 0.058, Test loss: 0.992, Test accuracy: 80.29 

Round  97, Global train loss: 0.058, Global test loss: 1.638, Global test accuracy: 34.48 

Round  98, Train loss: 0.049, Test loss: 0.959, Test accuracy: 80.71 

Round  98, Global train loss: 0.049, Global test loss: 1.820, Global test accuracy: 39.21 

Round  99, Train loss: 0.061, Test loss: 0.964, Test accuracy: 80.68 

Round  99, Global train loss: 0.061, Global test loss: 1.232, Global test accuracy: 37.86 

Final Round, Train loss: 0.054, Test loss: 0.976, Test accuracy: 81.25 

Final Round, Global train loss: 0.054, Global test loss: 1.232, Global test accuracy: 37.86 

Average accuracy final 10 rounds: 80.67249999999999 

Average global accuracy final 10 rounds: 37.1225 

1264.9675703048706
[1.4278230667114258, 2.61007022857666, 3.7999649047851562, 4.974779367446899, 6.151605606079102, 7.33823037147522, 8.525569915771484, 9.715970516204834, 10.906156301498413, 12.099567890167236, 13.290263652801514, 14.475017070770264, 15.666982650756836, 16.853055715560913, 18.039665937423706, 19.222195386886597, 20.40958333015442, 21.59206199645996, 22.702178478240967, 23.879872798919678, 25.064852237701416, 26.239757776260376, 27.4243905544281, 28.444967031478882, 29.463196992874146, 30.476447105407715, 31.486693143844604, 32.50047206878662, 33.51507377624512, 34.52866268157959, 35.54560136795044, 36.56482672691345, 37.58888864517212, 38.61366248130798, 39.6379508972168, 40.655882358551025, 41.67352604866028, 42.69095826148987, 43.70682668685913, 44.71861934661865, 45.732476472854614, 46.755919456481934, 47.7720730304718, 48.79042100906372, 49.82924675941467, 50.86553382873535, 51.90370440483093, 52.93635010719299, 53.96812844276428, 54.9875123500824, 56.0052707195282, 57.01976943016052, 58.03326201438904, 59.048551082611084, 60.06442093849182, 61.0916588306427, 62.123873710632324, 63.161821365356445, 64.19920134544373, 65.23739433288574, 66.26770305633545, 67.28304076194763, 68.30396342277527, 69.32795739173889, 70.34507608413696, 71.36438226699829, 72.37878251075745, 73.40110802650452, 74.42320203781128, 75.44296884536743, 76.46796464920044, 77.49544715881348, 78.51929306983948, 79.53841042518616, 80.5622923374176, 81.58001518249512, 82.59758353233337, 83.6084337234497, 84.6172547340393, 85.64035677909851, 86.66614699363708, 87.70356464385986, 88.72226691246033, 89.74390077590942, 90.76665091514587, 91.79068279266357, 92.81519794464111, 93.83190512657166, 94.8580584526062, 95.88319325447083, 96.90546822547913, 97.92650389671326, 98.9456639289856, 99.9648175239563, 100.98220038414001, 102.00087857246399, 103.02547359466553, 104.08137226104736, 105.10222887992859, 106.12454962730408, 108.16347575187683]
[39.9, 51.125, 57.25, 58.291666666666664, 61.25, 65.13333333333334, 69.73333333333333, 71.41666666666667, 72.29166666666667, 73.18333333333334, 72.60833333333333, 72.19166666666666, 73.3, 74.03333333333333, 74.18333333333334, 74.575, 73.925, 75.96666666666667, 74.55833333333334, 74.94166666666666, 74.36666666666666, 76.3, 78.25, 78.09166666666667, 78.375, 78.61666666666666, 79.11666666666666, 79.025, 79.45833333333333, 79.875, 79.71666666666667, 79.61666666666666, 79.925, 79.61666666666666, 79.31666666666666, 79.275, 79.41666666666667, 79.85, 79.85, 79.95833333333333, 80.33333333333333, 80.71666666666667, 80.70833333333333, 80.025, 80.11666666666666, 79.95833333333333, 80.25, 80.25833333333334, 80.05833333333334, 80.24166666666666, 80.05833333333334, 79.61666666666666, 79.85833333333333, 79.94166666666666, 79.9, 80.0, 80.19166666666666, 80.04166666666667, 80.49166666666666, 80.06666666666666, 80.4, 80.44166666666666, 80.05833333333334, 80.49166666666666, 80.31666666666666, 80.90833333333333, 80.65, 81.24166666666666, 81.06666666666666, 81.16666666666667, 81.21666666666667, 80.85, 80.575, 80.60833333333333, 80.63333333333334, 80.55833333333334, 80.89166666666667, 80.91666666666667, 80.89166666666667, 80.70833333333333, 80.81666666666666, 80.925, 80.65833333333333, 80.73333333333333, 80.95, 81.025, 80.78333333333333, 80.85, 80.70833333333333, 80.76666666666667, 81.16666666666667, 80.74166666666666, 80.49166666666666, 80.79166666666667, 80.85, 80.675, 80.325, 80.29166666666667, 80.70833333333333, 80.68333333333334, 81.25]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.905, Test loss: 1.039, Test accuracy: 44.62 

Round   0, Global train loss: 0.905, Global test loss: 1.122, Global test accuracy: 37.32 

Round   1, Train loss: 0.766, Test loss: 0.976, Test accuracy: 48.84 

Round   1, Global train loss: 0.766, Global test loss: 1.154, Global test accuracy: 35.80 

Round   2, Train loss: 0.717, Test loss: 0.936, Test accuracy: 54.82 

Round   2, Global train loss: 0.717, Global test loss: 1.234, Global test accuracy: 36.07 

Round   3, Train loss: 0.615, Test loss: 0.930, Test accuracy: 56.54 

Round   3, Global train loss: 0.615, Global test loss: 1.280, Global test accuracy: 36.67 

Round   4, Train loss: 0.745, Test loss: 0.773, Test accuracy: 64.29 

Round   4, Global train loss: 0.745, Global test loss: 1.165, Global test accuracy: 39.21 

Round   5, Train loss: 0.605, Test loss: 0.834, Test accuracy: 64.52 

Round   5, Global train loss: 0.605, Global test loss: 1.522, Global test accuracy: 38.67 

Round   6, Train loss: 0.645, Test loss: 0.759, Test accuracy: 67.12 

Round   6, Global train loss: 0.645, Global test loss: 1.360, Global test accuracy: 39.62 

Round   7, Train loss: 0.575, Test loss: 0.647, Test accuracy: 70.29 

Round   7, Global train loss: 0.575, Global test loss: 1.137, Global test accuracy: 39.03 

Round   8, Train loss: 0.553, Test loss: 0.602, Test accuracy: 73.55 

Round   8, Global train loss: 0.553, Global test loss: 1.548, Global test accuracy: 34.89 

Round   9, Train loss: 0.586, Test loss: 0.594, Test accuracy: 74.21 

Round   9, Global train loss: 0.586, Global test loss: 1.352, Global test accuracy: 36.08 

Round  10, Train loss: 0.549, Test loss: 0.598, Test accuracy: 74.01 

Round  10, Global train loss: 0.549, Global test loss: 1.280, Global test accuracy: 34.20 

Round  11, Train loss: 0.616, Test loss: 0.581, Test accuracy: 74.77 

Round  11, Global train loss: 0.616, Global test loss: 1.439, Global test accuracy: 34.88 

Round  12, Train loss: 0.589, Test loss: 0.576, Test accuracy: 75.39 

Round  12, Global train loss: 0.589, Global test loss: 1.133, Global test accuracy: 40.34 

Round  13, Train loss: 0.594, Test loss: 0.566, Test accuracy: 75.90 

Round  13, Global train loss: 0.594, Global test loss: 1.498, Global test accuracy: 39.94 

Round  14, Train loss: 0.552, Test loss: 0.552, Test accuracy: 76.72 

Round  14, Global train loss: 0.552, Global test loss: 1.236, Global test accuracy: 41.00 

Round  15, Train loss: 0.556, Test loss: 0.550, Test accuracy: 77.01 

Round  15, Global train loss: 0.556, Global test loss: 1.284, Global test accuracy: 35.75 

Round  16, Train loss: 0.482, Test loss: 0.542, Test accuracy: 77.53 

Round  16, Global train loss: 0.482, Global test loss: 1.301, Global test accuracy: 36.58 

Round  17, Train loss: 0.523, Test loss: 0.541, Test accuracy: 77.58 

Round  17, Global train loss: 0.523, Global test loss: 1.296, Global test accuracy: 36.88 

Round  18, Train loss: 0.494, Test loss: 0.542, Test accuracy: 77.74 

Round  18, Global train loss: 0.494, Global test loss: 1.561, Global test accuracy: 38.93 

Round  19, Train loss: 0.565, Test loss: 0.547, Test accuracy: 77.72 

Round  19, Global train loss: 0.565, Global test loss: 1.444, Global test accuracy: 38.40 

Round  20, Train loss: 0.477, Test loss: 0.547, Test accuracy: 77.53 

Round  20, Global train loss: 0.477, Global test loss: 1.348, Global test accuracy: 39.73 

Round  21, Train loss: 0.422, Test loss: 0.539, Test accuracy: 77.74 

Round  21, Global train loss: 0.422, Global test loss: 1.514, Global test accuracy: 40.19 

Round  22, Train loss: 0.527, Test loss: 0.555, Test accuracy: 76.97 

Round  22, Global train loss: 0.527, Global test loss: 1.387, Global test accuracy: 37.34 

Round  23, Train loss: 0.449, Test loss: 0.546, Test accuracy: 77.63 

Round  23, Global train loss: 0.449, Global test loss: 1.226, Global test accuracy: 39.74 

Round  24, Train loss: 0.485, Test loss: 0.534, Test accuracy: 78.13 

Round  24, Global train loss: 0.485, Global test loss: 1.340, Global test accuracy: 37.69 

Round  25, Train loss: 0.534, Test loss: 0.527, Test accuracy: 78.73 

Round  25, Global train loss: 0.534, Global test loss: 1.164, Global test accuracy: 40.48 

Round  26, Train loss: 0.491, Test loss: 0.520, Test accuracy: 78.90 

Round  26, Global train loss: 0.491, Global test loss: 1.330, Global test accuracy: 36.86 

Round  27, Train loss: 0.473, Test loss: 0.517, Test accuracy: 79.29 

Round  27, Global train loss: 0.473, Global test loss: 1.228, Global test accuracy: 40.70 

Round  28, Train loss: 0.412, Test loss: 0.502, Test accuracy: 79.77 

Round  28, Global train loss: 0.412, Global test loss: 1.592, Global test accuracy: 38.59 

Round  29, Train loss: 0.444, Test loss: 0.500, Test accuracy: 79.93 

Round  29, Global train loss: 0.444, Global test loss: 1.322, Global test accuracy: 37.16 

Round  30, Train loss: 0.384, Test loss: 0.504, Test accuracy: 79.93 

Round  30, Global train loss: 0.384, Global test loss: 1.590, Global test accuracy: 37.27 

Round  31, Train loss: 0.450, Test loss: 0.492, Test accuracy: 80.33 

Round  31, Global train loss: 0.450, Global test loss: 1.547, Global test accuracy: 41.17 

Round  32, Train loss: 0.421, Test loss: 0.501, Test accuracy: 80.16 

Round  32, Global train loss: 0.421, Global test loss: 1.309, Global test accuracy: 40.38 

Round  33, Train loss: 0.442, Test loss: 0.496, Test accuracy: 80.29 

Round  33, Global train loss: 0.442, Global test loss: 1.429, Global test accuracy: 41.18 

Round  34, Train loss: 0.418, Test loss: 0.504, Test accuracy: 80.15 

Round  34, Global train loss: 0.418, Global test loss: 1.608, Global test accuracy: 38.60 

Round  35, Train loss: 0.422, Test loss: 0.495, Test accuracy: 80.39 

Round  35, Global train loss: 0.422, Global test loss: 1.497, Global test accuracy: 40.12 

Round  36, Train loss: 0.426, Test loss: 0.505, Test accuracy: 80.34 

Round  36, Global train loss: 0.426, Global test loss: 1.478, Global test accuracy: 40.23 

Round  37, Train loss: 0.418, Test loss: 0.501, Test accuracy: 80.34 

Round  37, Global train loss: 0.418, Global test loss: 1.391, Global test accuracy: 37.64 

Round  38, Train loss: 0.394, Test loss: 0.493, Test accuracy: 80.49 

Round  38, Global train loss: 0.394, Global test loss: 1.443, Global test accuracy: 40.67 

Round  39, Train loss: 0.470, Test loss: 0.505, Test accuracy: 80.22 

Round  39, Global train loss: 0.470, Global test loss: 1.812, Global test accuracy: 35.12 

Round  40, Train loss: 0.346, Test loss: 0.507, Test accuracy: 80.33 

Round  40, Global train loss: 0.346, Global test loss: 1.397, Global test accuracy: 40.65 

Round  41, Train loss: 0.404, Test loss: 0.494, Test accuracy: 80.66 

Round  41, Global train loss: 0.404, Global test loss: 1.458, Global test accuracy: 36.28 

Round  42, Train loss: 0.371, Test loss: 0.481, Test accuracy: 81.11 

Round  42, Global train loss: 0.371, Global test loss: 1.459, Global test accuracy: 41.24 

Round  43, Train loss: 0.366, Test loss: 0.486, Test accuracy: 81.37 

Round  43, Global train loss: 0.366, Global test loss: 1.620, Global test accuracy: 39.83 

Round  44, Train loss: 0.352, Test loss: 0.486, Test accuracy: 81.43 

Round  44, Global train loss: 0.352, Global test loss: 1.954, Global test accuracy: 38.23 

Round  45, Train loss: 0.360, Test loss: 0.490, Test accuracy: 81.45 

Round  45, Global train loss: 0.360, Global test loss: 1.356, Global test accuracy: 37.84 

Round  46, Train loss: 0.367, Test loss: 0.487, Test accuracy: 81.31 

Round  46, Global train loss: 0.367, Global test loss: 1.395, Global test accuracy: 41.67 

Round  47, Train loss: 0.367, Test loss: 0.489, Test accuracy: 81.47 

Round  47, Global train loss: 0.367, Global test loss: 1.414, Global test accuracy: 39.86 

Round  48, Train loss: 0.402, Test loss: 0.477, Test accuracy: 81.85 

Round  48, Global train loss: 0.402, Global test loss: 1.385, Global test accuracy: 39.47 

Round  49, Train loss: 0.341, Test loss: 0.475, Test accuracy: 81.88 

Round  49, Global train loss: 0.341, Global test loss: 1.858, Global test accuracy: 39.48 

Round  50, Train loss: 0.356, Test loss: 0.486, Test accuracy: 81.61 

Round  50, Global train loss: 0.356, Global test loss: 1.558, Global test accuracy: 40.37 

Round  51, Train loss: 0.338, Test loss: 0.492, Test accuracy: 81.67 

Round  51, Global train loss: 0.338, Global test loss: 1.758, Global test accuracy: 40.38 

Round  52, Train loss: 0.295, Test loss: 0.493, Test accuracy: 81.57 

Round  52, Global train loss: 0.295, Global test loss: 1.658, Global test accuracy: 41.42 

Round  53, Train loss: 0.362, Test loss: 0.500, Test accuracy: 81.71 

Round  53, Global train loss: 0.362, Global test loss: 1.359, Global test accuracy: 39.48 

Round  54, Train loss: 0.315, Test loss: 0.505, Test accuracy: 81.52 

Round  54, Global train loss: 0.315, Global test loss: 1.453, Global test accuracy: 38.73 

Round  55, Train loss: 0.383, Test loss: 0.491, Test accuracy: 81.72 

Round  55, Global train loss: 0.383, Global test loss: 1.406, Global test accuracy: 38.76 

Round  56, Train loss: 0.289, Test loss: 0.496, Test accuracy: 81.67 

Round  56, Global train loss: 0.289, Global test loss: 1.611, Global test accuracy: 41.57 

Round  57, Train loss: 0.390, Test loss: 0.478, Test accuracy: 82.32 

Round  57, Global train loss: 0.390, Global test loss: 1.522, Global test accuracy: 37.73 

Round  58, Train loss: 0.304, Test loss: 0.482, Test accuracy: 82.38 

Round  58, Global train loss: 0.304, Global test loss: 2.343, Global test accuracy: 41.12 

Round  59, Train loss: 0.312, Test loss: 0.472, Test accuracy: 82.67 

Round  59, Global train loss: 0.312, Global test loss: 1.829, Global test accuracy: 39.16 

Round  60, Train loss: 0.339, Test loss: 0.467, Test accuracy: 82.86 

Round  60, Global train loss: 0.339, Global test loss: 1.464, Global test accuracy: 39.29 

Round  61, Train loss: 0.281, Test loss: 0.487, Test accuracy: 82.21 

Round  61, Global train loss: 0.281, Global test loss: 1.587, Global test accuracy: 39.59 

Round  62, Train loss: 0.267, Test loss: 0.479, Test accuracy: 82.58 

Round  62, Global train loss: 0.267, Global test loss: 1.564, Global test accuracy: 41.14 

Round  63, Train loss: 0.282, Test loss: 0.495, Test accuracy: 82.20 

Round  63, Global train loss: 0.282, Global test loss: 1.927, Global test accuracy: 39.41 

Round  64, Train loss: 0.274, Test loss: 0.494, Test accuracy: 82.09 

Round  64, Global train loss: 0.274, Global test loss: 1.468, Global test accuracy: 39.71 

Round  65, Train loss: 0.292, Test loss: 0.492, Test accuracy: 82.11 

Round  65, Global train loss: 0.292, Global test loss: 1.590, Global test accuracy: 40.41 

Round  66, Train loss: 0.266, Test loss: 0.497, Test accuracy: 82.11 

Round  66, Global train loss: 0.266, Global test loss: 1.486, Global test accuracy: 42.67 

Round  67, Train loss: 0.302, Test loss: 0.504, Test accuracy: 81.83 

Round  67, Global train loss: 0.302, Global test loss: 1.602, Global test accuracy: 42.68 

Round  68, Train loss: 0.323, Test loss: 0.507, Test accuracy: 81.73 

Round  68, Global train loss: 0.323, Global test loss: 1.883, Global test accuracy: 36.71 

Round  69, Train loss: 0.261, Test loss: 0.500, Test accuracy: 82.05 

Round  69, Global train loss: 0.261, Global test loss: 1.917, Global test accuracy: 37.87 

Round  70, Train loss: 0.246, Test loss: 0.501, Test accuracy: 82.18 

Round  70, Global train loss: 0.246, Global test loss: 1.769, Global test accuracy: 39.64 

Round  71, Train loss: 0.328, Test loss: 0.497, Test accuracy: 82.54 

Round  71, Global train loss: 0.328, Global test loss: 1.434, Global test accuracy: 39.48 

Round  72, Train loss: 0.277, Test loss: 0.499, Test accuracy: 82.52 

Round  72, Global train loss: 0.277, Global test loss: 1.727, Global test accuracy: 40.86 

Round  73, Train loss: 0.263, Test loss: 0.494, Test accuracy: 82.67 

Round  73, Global train loss: 0.263, Global test loss: 1.755, Global test accuracy: 38.15 

Round  74, Train loss: 0.220, Test loss: 0.502, Test accuracy: 82.44 

Round  74, Global train loss: 0.220, Global test loss: 1.826, Global test accuracy: 39.30 

Round  75, Train loss: 0.362, Test loss: 0.501, Test accuracy: 82.49 

Round  75, Global train loss: 0.362, Global test loss: 1.501, Global test accuracy: 41.02 

Round  76, Train loss: 0.304, Test loss: 0.490, Test accuracy: 82.83 

Round  76, Global train loss: 0.304, Global test loss: 1.471, Global test accuracy: 38.23 

Round  77, Train loss: 0.235, Test loss: 0.496, Test accuracy: 82.67 

Round  77, Global train loss: 0.235, Global test loss: 1.553, Global test accuracy: 40.80 

Round  78, Train loss: 0.232, Test loss: 0.491, Test accuracy: 82.92 

Round  78, Global train loss: 0.232, Global test loss: 1.555, Global test accuracy: 40.12 

Round  79, Train loss: 0.272, Test loss: 0.500, Test accuracy: 82.58 

Round  79, Global train loss: 0.272, Global test loss: 1.915, Global test accuracy: 42.48 

Round  80, Train loss: 0.205, Test loss: 0.504, Test accuracy: 82.63 

Round  80, Global train loss: 0.205, Global test loss: 2.252, Global test accuracy: 43.14 

Round  81, Train loss: 0.314, Test loss: 0.504, Test accuracy: 82.60 

Round  81, Global train loss: 0.314, Global test loss: 1.528, Global test accuracy: 42.24 

Round  82, Train loss: 0.331, Test loss: 0.508, Test accuracy: 82.67 

Round  82, Global train loss: 0.331, Global test loss: 1.623, Global test accuracy: 40.28 

Round  83, Train loss: 0.240, Test loss: 0.503, Test accuracy: 82.87 

Round  83, Global train loss: 0.240, Global test loss: 1.685, Global test accuracy: 38.65 

Round  84, Train loss: 0.224, Test loss: 0.505, Test accuracy: 82.54 

Round  84, Global train loss: 0.224, Global test loss: 1.623, Global test accuracy: 39.29 

Round  85, Train loss: 0.274, Test loss: 0.498, Test accuracy: 82.44 

Round  85, Global train loss: 0.274, Global test loss: 1.855, Global test accuracy: 40.52 

Round  86, Train loss: 0.333, Test loss: 0.507, Test accuracy: 82.38 

Round  86, Global train loss: 0.333, Global test loss: 1.563, Global test accuracy: 39.14 

Round  87, Train loss: 0.225, Test loss: 0.525, Test accuracy: 82.06 

Round  87, Global train loss: 0.225, Global test loss: 1.624, Global test accuracy: 38.56 

Round  88, Train loss: 0.215, Test loss: 0.546, Test accuracy: 81.85 

Round  88, Global train loss: 0.215, Global test loss: 1.589, Global test accuracy: 40.05 

Round  89, Train loss: 0.226, Test loss: 0.529, Test accuracy: 82.54 

Round  89, Global train loss: 0.226, Global test loss: 1.725, Global test accuracy: 40.25 

Round  90, Train loss: 0.275, Test loss: 0.528, Test accuracy: 82.66 

Round  90, Global train loss: 0.275, Global test loss: 1.820, Global test accuracy: 42.57 

Round  91, Train loss: 0.234, Test loss: 0.523, Test accuracy: 82.00 

Round  91, Global train loss: 0.234, Global test loss: 2.452, Global test accuracy: 41.62 

Round  92, Train loss: 0.282, Test loss: 0.530, Test accuracy: 82.18 

Round  92, Global train loss: 0.282, Global test loss: 1.739, Global test accuracy: 39.36 

Round  93, Train loss: 0.267, Test loss: 0.520, Test accuracy: 82.42 

Round  93, Global train loss: 0.267, Global test loss: 1.683, Global test accuracy: 42.84 

Round  94, Train loss: 0.229, Test loss: 0.517, Test accuracy: 82.77 

Round  94, Global train loss: 0.229, Global test loss: 1.955, Global test accuracy: 43.13 

Round  95, Train loss: 0.207, Test loss: 0.518, Test accuracy: 83.00 

Round  95, Global train loss: 0.207, Global test loss: 1.844, Global test accuracy: 42.37 

Round  96, Train loss: 0.240, Test loss: 0.518, Test accuracy: 82.86 

Round  96, Global train loss: 0.240, Global test loss: 1.508, Global test accuracy: 40.53 

Round  97, Train loss: 0.232, Test loss: 0.512, Test accuracy: 82.66 

Round  97, Global train loss: 0.232, Global test loss: 1.696, Global test accuracy: 42.77 

Round  98, Train loss: 0.260, Test loss: 0.518, Test accuracy: 82.71 

Round  98, Global train loss: 0.260, Global test loss: 1.578, Global test accuracy: 40.35 

Round  99, Train loss: 0.219, Test loss: 0.505, Test accuracy: 82.72 

Round  99, Global train loss: 0.219, Global test loss: 1.788, Global test accuracy: 41.83 

Final Round, Train loss: 0.183, Test loss: 0.585, Test accuracy: 82.83 

Final Round, Global train loss: 0.183, Global test loss: 1.788, Global test accuracy: 41.83 

Average accuracy final 10 rounds: 82.59749999999998 

Average global accuracy final 10 rounds: 41.735833333333325 

1299.0341801643372
[1.4136595726013184, 2.555959701538086, 3.70005202293396, 4.877057313919067, 6.05763053894043, 7.23851203918457, 8.514435052871704, 9.693271160125732, 10.876097679138184, 12.056430101394653, 13.233677625656128, 14.416636228561401, 15.59741497039795, 16.772165536880493, 17.95469617843628, 19.133273363113403, 20.320720911026, 21.506773471832275, 22.68581199645996, 23.860824823379517, 25.024349689483643, 26.19184398651123, 27.36655020713806, 28.540284633636475, 29.710453271865845, 30.885844469070435, 32.05654001235962, 33.226828813552856, 34.40903854370117, 35.583348989486694, 36.76107740402222, 37.93502974510193, 39.11172795295715, 40.290971755981445, 41.465496301651, 42.64035964012146, 43.826205253601074, 45.00919008255005, 46.18731451034546, 47.361982107162476, 48.53847408294678, 49.710753440856934, 50.885703563690186, 52.06529378890991, 53.24968433380127, 54.42208695411682, 55.59842801094055, 56.77740478515625, 57.95222544670105, 59.12928652763367, 60.306135416030884, 61.47935771942139, 62.66123962402344, 63.83714723587036, 65.01719284057617, 66.19469356536865, 67.37672853469849, 68.55839228630066, 69.73547315597534, 70.91760921478271, 72.10105848312378, 73.28565049171448, 74.458016872406, 75.63361763954163, 76.81295895576477, 77.98746752738953, 79.16191840171814, 80.33697438240051, 81.51660585403442, 82.69971060752869, 83.88085699081421, 85.06028628349304, 86.23637819290161, 87.32712435722351, 88.3396360874176, 89.3547215461731, 90.36841773986816, 91.38451957702637, 92.39936351776123, 93.41793298721313, 94.4312093257904, 95.44393491744995, 96.45737266540527, 97.47327089309692, 98.48705530166626, 99.50268888473511, 100.51673436164856, 101.5318169593811, 102.54800653457642, 103.56232953071594, 104.57875156402588, 105.59526038169861, 106.61680316925049, 107.63820910453796, 108.66318798065186, 109.68952560424805, 110.71048259735107, 111.73672556877136, 112.7542839050293, 113.77577924728394, 115.8034155368805]
[44.625, 48.84166666666667, 54.81666666666667, 56.541666666666664, 64.29166666666667, 64.51666666666667, 67.125, 70.29166666666667, 73.55, 74.20833333333333, 74.00833333333334, 74.76666666666667, 75.39166666666667, 75.9, 76.725, 77.00833333333334, 77.525, 77.575, 77.74166666666666, 77.725, 77.525, 77.74166666666666, 76.96666666666667, 77.63333333333334, 78.13333333333334, 78.73333333333333, 78.9, 79.29166666666667, 79.76666666666667, 79.93333333333334, 79.93333333333334, 80.33333333333333, 80.15833333333333, 80.29166666666667, 80.15, 80.39166666666667, 80.34166666666667, 80.34166666666667, 80.49166666666666, 80.225, 80.33333333333333, 80.65833333333333, 81.10833333333333, 81.36666666666666, 81.43333333333334, 81.45, 81.30833333333334, 81.46666666666667, 81.85, 81.88333333333334, 81.60833333333333, 81.66666666666667, 81.56666666666666, 81.70833333333333, 81.51666666666667, 81.71666666666667, 81.66666666666667, 82.31666666666666, 82.38333333333334, 82.675, 82.85833333333333, 82.20833333333333, 82.58333333333333, 82.2, 82.09166666666667, 82.10833333333333, 82.10833333333333, 81.825, 81.73333333333333, 82.05, 82.18333333333334, 82.54166666666667, 82.51666666666667, 82.675, 82.44166666666666, 82.49166666666666, 82.83333333333333, 82.675, 82.91666666666667, 82.575, 82.63333333333334, 82.6, 82.675, 82.86666666666666, 82.54166666666667, 82.44166666666666, 82.38333333333334, 82.05833333333334, 81.85, 82.54166666666667, 82.65833333333333, 82.0, 82.18333333333334, 82.425, 82.76666666666667, 83.0, 82.85833333333333, 82.65833333333333, 82.70833333333333, 82.71666666666667, 82.825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.045, Test loss: 1.102, Test accuracy: 36.81 

Round   1, Train loss: 0.939, Test loss: 1.129, Test accuracy: 38.74 

Round   2, Train loss: 0.881, Test loss: 1.019, Test accuracy: 47.27 

Round   3, Train loss: 0.767, Test loss: 1.006, Test accuracy: 50.67 

Round   4, Train loss: 0.709, Test loss: 0.991, Test accuracy: 51.44 

Round   5, Train loss: 0.759, Test loss: 0.914, Test accuracy: 55.30 

Round   6, Train loss: 0.728, Test loss: 0.800, Test accuracy: 60.47 

Round   7, Train loss: 0.661, Test loss: 0.742, Test accuracy: 65.10 

Round   8, Train loss: 0.619, Test loss: 0.744, Test accuracy: 64.83 

Round   9, Train loss: 0.615, Test loss: 0.678, Test accuracy: 67.30 

Round  10, Train loss: 0.613, Test loss: 0.714, Test accuracy: 66.87 

Round  11, Train loss: 0.639, Test loss: 0.668, Test accuracy: 69.21 

Round  12, Train loss: 0.573, Test loss: 0.661, Test accuracy: 70.31 

Round  13, Train loss: 0.546, Test loss: 0.609, Test accuracy: 72.09 

Round  14, Train loss: 0.530, Test loss: 0.582, Test accuracy: 74.24 

Round  15, Train loss: 0.545, Test loss: 0.566, Test accuracy: 75.26 

Round  16, Train loss: 0.534, Test loss: 0.554, Test accuracy: 75.59 

Round  17, Train loss: 0.578, Test loss: 0.537, Test accuracy: 76.75 

Round  18, Train loss: 0.534, Test loss: 0.523, Test accuracy: 77.67 

Round  19, Train loss: 0.510, Test loss: 0.520, Test accuracy: 77.51 

Round  20, Train loss: 0.527, Test loss: 0.520, Test accuracy: 77.82 

Round  21, Train loss: 0.468, Test loss: 0.513, Test accuracy: 78.39 

Round  22, Train loss: 0.541, Test loss: 0.506, Test accuracy: 78.64 

Round  23, Train loss: 0.515, Test loss: 0.502, Test accuracy: 78.78 

Round  24, Train loss: 0.493, Test loss: 0.492, Test accuracy: 79.12 

Round  25, Train loss: 0.534, Test loss: 0.493, Test accuracy: 79.47 

Round  26, Train loss: 0.438, Test loss: 0.492, Test accuracy: 79.44 

Round  27, Train loss: 0.483, Test loss: 0.480, Test accuracy: 79.72 

Round  28, Train loss: 0.513, Test loss: 0.485, Test accuracy: 79.20 

Round  29, Train loss: 0.473, Test loss: 0.477, Test accuracy: 79.53 

Round  30, Train loss: 0.529, Test loss: 0.475, Test accuracy: 80.09 

Round  31, Train loss: 0.422, Test loss: 0.459, Test accuracy: 80.76 

Round  32, Train loss: 0.463, Test loss: 0.456, Test accuracy: 80.90 

Round  33, Train loss: 0.483, Test loss: 0.459, Test accuracy: 80.60 

Round  34, Train loss: 0.477, Test loss: 0.466, Test accuracy: 80.42 

Round  35, Train loss: 0.451, Test loss: 0.458, Test accuracy: 80.79 

Round  36, Train loss: 0.403, Test loss: 0.444, Test accuracy: 81.34 

Round  37, Train loss: 0.435, Test loss: 0.439, Test accuracy: 81.82 

Round  38, Train loss: 0.378, Test loss: 0.438, Test accuracy: 81.77 

Round  39, Train loss: 0.380, Test loss: 0.439, Test accuracy: 81.50 

Round  40, Train loss: 0.399, Test loss: 0.428, Test accuracy: 82.16 

Round  41, Train loss: 0.455, Test loss: 0.438, Test accuracy: 81.70 

Round  42, Train loss: 0.479, Test loss: 0.435, Test accuracy: 81.98 

Round  43, Train loss: 0.395, Test loss: 0.433, Test accuracy: 82.28 

Round  44, Train loss: 0.446, Test loss: 0.432, Test accuracy: 82.23 

Round  45, Train loss: 0.339, Test loss: 0.422, Test accuracy: 82.50 

Round  46, Train loss: 0.370, Test loss: 0.422, Test accuracy: 82.58 

Round  47, Train loss: 0.385, Test loss: 0.415, Test accuracy: 83.10 

Round  48, Train loss: 0.388, Test loss: 0.421, Test accuracy: 83.00 

Round  49, Train loss: 0.335, Test loss: 0.424, Test accuracy: 82.61 

Round  50, Train loss: 0.418, Test loss: 0.410, Test accuracy: 83.00 

Round  51, Train loss: 0.413, Test loss: 0.404, Test accuracy: 83.14 

Round  52, Train loss: 0.408, Test loss: 0.406, Test accuracy: 83.33 

Round  53, Train loss: 0.295, Test loss: 0.409, Test accuracy: 82.87 

Round  54, Train loss: 0.386, Test loss: 0.405, Test accuracy: 83.34 

Round  55, Train loss: 0.337, Test loss: 0.403, Test accuracy: 83.51 

Round  56, Train loss: 0.314, Test loss: 0.409, Test accuracy: 83.46 

Round  57, Train loss: 0.339, Test loss: 0.399, Test accuracy: 83.70 

Round  58, Train loss: 0.292, Test loss: 0.401, Test accuracy: 83.42 

Round  59, Train loss: 0.352, Test loss: 0.398, Test accuracy: 83.73 

Round  60, Train loss: 0.402, Test loss: 0.393, Test accuracy: 84.17 

Round  61, Train loss: 0.342, Test loss: 0.393, Test accuracy: 84.07 

Round  62, Train loss: 0.361, Test loss: 0.390, Test accuracy: 84.22 

Round  63, Train loss: 0.365, Test loss: 0.387, Test accuracy: 84.25 

Round  64, Train loss: 0.314, Test loss: 0.392, Test accuracy: 83.99 

Round  65, Train loss: 0.330, Test loss: 0.384, Test accuracy: 84.64 

Round  66, Train loss: 0.356, Test loss: 0.389, Test accuracy: 84.06 

Round  67, Train loss: 0.315, Test loss: 0.388, Test accuracy: 84.01 

Round  68, Train loss: 0.363, Test loss: 0.385, Test accuracy: 84.25 

Round  69, Train loss: 0.297, Test loss: 0.386, Test accuracy: 84.43 

Round  70, Train loss: 0.290, Test loss: 0.390, Test accuracy: 84.26 

Round  71, Train loss: 0.270, Test loss: 0.387, Test accuracy: 84.47 

Round  72, Train loss: 0.323, Test loss: 0.381, Test accuracy: 84.53 

Round  73, Train loss: 0.250, Test loss: 0.389, Test accuracy: 84.32 

Round  74, Train loss: 0.292, Test loss: 0.389, Test accuracy: 84.42 

Round  75, Train loss: 0.281, Test loss: 0.383, Test accuracy: 84.31 

Round  76, Train loss: 0.325, Test loss: 0.379, Test accuracy: 84.78 

Round  77, Train loss: 0.217, Test loss: 0.379, Test accuracy: 84.77 

Round  78, Train loss: 0.315, Test loss: 0.375, Test accuracy: 85.02 

Round  79, Train loss: 0.296, Test loss: 0.376, Test accuracy: 85.08 

Round  80, Train loss: 0.276, Test loss: 0.385, Test accuracy: 84.81 

Round  81, Train loss: 0.299, Test loss: 0.375, Test accuracy: 85.12 

Round  82, Train loss: 0.231, Test loss: 0.378, Test accuracy: 85.11 

Round  83, Train loss: 0.281, Test loss: 0.376, Test accuracy: 84.79 

Round  84, Train loss: 0.257, Test loss: 0.370, Test accuracy: 85.28 

Round  85, Train loss: 0.261, Test loss: 0.383, Test accuracy: 84.85 

Round  86, Train loss: 0.229, Test loss: 0.382, Test accuracy: 84.85 

Round  87, Train loss: 0.263, Test loss: 0.375, Test accuracy: 84.86 

Round  88, Train loss: 0.234, Test loss: 0.378, Test accuracy: 85.43 

Round  89, Train loss: 0.319, Test loss: 0.370, Test accuracy: 85.70 

Round  90, Train loss: 0.222, Test loss: 0.376, Test accuracy: 85.33 

Round  91, Train loss: 0.245, Test loss: 0.376, Test accuracy: 85.56 

Round  92, Train loss: 0.205, Test loss: 0.376, Test accuracy: 85.47 

Round  93, Train loss: 0.271, Test loss: 0.374, Test accuracy: 85.65 

Round  94, Train loss: 0.260, Test loss: 0.371, Test accuracy: 85.83 

Round  95, Train loss: 0.222, Test loss: 0.390, Test accuracy: 85.16 

Round  96, Train loss: 0.233, Test loss: 0.385, Test accuracy: 85.48 

Round  97, Train loss: 0.256, Test loss: 0.375, Test accuracy: 85.66 

Round  98, Train loss: 0.216, Test loss: 0.368, Test accuracy: 85.80 

Round  99, Train loss: 0.227, Test loss: 0.373, Test accuracy: 85.33 

Final Round, Train loss: 0.210, Test loss: 0.377, Test accuracy: 85.76 

Average accuracy final 10 rounds: 85.52666666666666 

943.7484905719757
[1.3009612560272217, 2.3456428050994873, 3.394474506378174, 4.442238092422485, 5.489205598831177, 6.53584361076355, 7.581589221954346, 8.627515316009521, 9.677824020385742, 10.724499464035034, 11.777747631072998, 12.825876474380493, 13.872684955596924, 14.917826175689697, 15.966173648834229, 16.98021936416626, 17.995350122451782, 19.010651111602783, 20.031586170196533, 21.04910397529602, 22.065792083740234, 23.081255674362183, 24.118056297302246, 25.13053607940674, 26.157315969467163, 27.165915966033936, 28.183246850967407, 29.195776224136353, 30.21437096595764, 31.22411346435547, 32.241771936416626, 33.25701594352722, 34.27337718009949, 35.287023067474365, 36.327900409698486, 37.37494611740112, 38.41756248474121, 39.46416926383972, 40.50888419151306, 41.55661869049072, 42.60311412811279, 43.65085291862488, 44.696396827697754, 45.742135763168335, 46.79074430465698, 47.838643074035645, 48.88570857048035, 49.931926012039185, 50.98214936256409, 52.02621340751648, 53.03699827194214, 54.05355787277222, 55.0827362537384, 56.125842571258545, 57.16685223579407, 58.20571231842041, 59.252032995224, 60.2942578792572, 61.333401679992676, 62.38113594055176, 63.43076205253601, 64.47386598587036, 65.51765370368958, 66.56971859931946, 67.61247777938843, 68.65978860855103, 69.70179486274719, 70.74428343772888, 71.78972387313843, 72.83803677558899, 73.89569926261902, 74.94767999649048, 75.99455118179321, 77.03728771209717, 78.08531093597412, 79.1257004737854, 80.17172145843506, 81.20858097076416, 82.25031876564026, 83.2919328212738, 84.33361005783081, 85.37891840934753, 86.3921446800232, 87.40263724327087, 88.41282725334167, 89.42702651023865, 90.44108462333679, 91.45847654342651, 92.47209334373474, 93.48292636871338, 94.494713306427, 95.5070948600769, 96.51996803283691, 97.56206607818604, 98.61194586753845, 99.6606957912445, 100.70665645599365, 101.7511854171753, 102.80267763137817, 103.84978413581848, 105.71806406974792]
[36.80833333333333, 38.74166666666667, 47.275, 50.675, 51.44166666666667, 55.3, 60.46666666666667, 65.1, 64.825, 67.3, 66.86666666666666, 69.20833333333333, 70.30833333333334, 72.09166666666667, 74.24166666666666, 75.25833333333334, 75.59166666666667, 76.75, 77.66666666666667, 77.50833333333334, 77.81666666666666, 78.39166666666667, 78.64166666666667, 78.78333333333333, 79.125, 79.475, 79.44166666666666, 79.71666666666667, 79.2, 79.525, 80.09166666666667, 80.75833333333334, 80.9, 80.6, 80.425, 80.79166666666667, 81.34166666666667, 81.81666666666666, 81.76666666666667, 81.5, 82.15833333333333, 81.7, 81.98333333333333, 82.275, 82.23333333333333, 82.5, 82.575, 83.1, 83.0, 82.60833333333333, 83.0, 83.14166666666667, 83.33333333333333, 82.86666666666666, 83.34166666666667, 83.50833333333334, 83.45833333333333, 83.7, 83.425, 83.73333333333333, 84.16666666666667, 84.06666666666666, 84.21666666666667, 84.25, 83.99166666666666, 84.64166666666667, 84.05833333333334, 84.00833333333334, 84.25, 84.43333333333334, 84.25833333333334, 84.46666666666667, 84.525, 84.31666666666666, 84.425, 84.30833333333334, 84.78333333333333, 84.76666666666667, 85.01666666666667, 85.08333333333333, 84.80833333333334, 85.11666666666666, 85.10833333333333, 84.79166666666667, 85.275, 84.85, 84.85, 84.85833333333333, 85.43333333333334, 85.7, 85.33333333333333, 85.55833333333334, 85.475, 85.65, 85.825, 85.15833333333333, 85.48333333333333, 85.65833333333333, 85.8, 85.325, 85.75833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.958, Test loss: 1.115, Test accuracy: 33.85 

Round   1, Train loss: 0.800, Test loss: 1.116, Test accuracy: 40.19 

Round   2, Train loss: 0.723, Test loss: 1.184, Test accuracy: 41.34 

Round   3, Train loss: 0.745, Test loss: 1.039, Test accuracy: 47.38 

Round   4, Train loss: 0.670, Test loss: 1.114, Test accuracy: 42.59 

Round   5, Train loss: 0.618, Test loss: 1.022, Test accuracy: 48.32 

Round   6, Train loss: 0.599, Test loss: 0.951, Test accuracy: 53.12 

Round   7, Train loss: 0.603, Test loss: 0.936, Test accuracy: 54.53 

Round   8, Train loss: 0.503, Test loss: 0.833, Test accuracy: 61.78 

Round   9, Train loss: 0.573, Test loss: 0.817, Test accuracy: 61.05 

Round  10, Train loss: 0.549, Test loss: 0.862, Test accuracy: 60.61 

Round  11, Train loss: 0.543, Test loss: 0.712, Test accuracy: 66.42 

Round  12, Train loss: 0.564, Test loss: 0.737, Test accuracy: 65.43 

Round  13, Train loss: 0.574, Test loss: 0.732, Test accuracy: 65.93 

Round  14, Train loss: 0.491, Test loss: 0.689, Test accuracy: 68.32 

Round  15, Train loss: 0.498, Test loss: 0.659, Test accuracy: 69.95 

Round  16, Train loss: 0.537, Test loss: 0.657, Test accuracy: 69.91 

Round  17, Train loss: 0.554, Test loss: 0.623, Test accuracy: 71.88 

Round  18, Train loss: 0.507, Test loss: 0.610, Test accuracy: 73.09 

Round  19, Train loss: 0.444, Test loss: 0.625, Test accuracy: 72.69 

Round  20, Train loss: 0.527, Test loss: 0.588, Test accuracy: 73.71 

Round  21, Train loss: 0.411, Test loss: 0.613, Test accuracy: 72.99 

Round  22, Train loss: 0.493, Test loss: 0.571, Test accuracy: 75.18 

Round  23, Train loss: 0.426, Test loss: 0.555, Test accuracy: 75.92 

Round  24, Train loss: 0.428, Test loss: 0.558, Test accuracy: 76.08 

Round  25, Train loss: 0.431, Test loss: 0.562, Test accuracy: 75.57 

Round  26, Train loss: 0.368, Test loss: 0.537, Test accuracy: 76.71 

Round  27, Train loss: 0.356, Test loss: 0.527, Test accuracy: 77.31 

Round  28, Train loss: 0.372, Test loss: 0.522, Test accuracy: 77.37 

Round  29, Train loss: 0.375, Test loss: 0.513, Test accuracy: 78.24 

Round  30, Train loss: 0.394, Test loss: 0.507, Test accuracy: 78.37 

Round  31, Train loss: 0.320, Test loss: 0.485, Test accuracy: 79.20 

Round  32, Train loss: 0.373, Test loss: 0.489, Test accuracy: 79.04 

Round  33, Train loss: 0.310, Test loss: 0.484, Test accuracy: 79.33 

Round  34, Train loss: 0.340, Test loss: 0.488, Test accuracy: 79.62 

Round  35, Train loss: 0.332, Test loss: 0.476, Test accuracy: 80.20 

Round  36, Train loss: 0.359, Test loss: 0.478, Test accuracy: 80.11 

Round  37, Train loss: 0.387, Test loss: 0.475, Test accuracy: 80.11 

Round  38, Train loss: 0.315, Test loss: 0.448, Test accuracy: 81.41 

Round  39, Train loss: 0.370, Test loss: 0.463, Test accuracy: 81.12 

Round  40, Train loss: 0.317, Test loss: 0.459, Test accuracy: 81.33 

Round  41, Train loss: 0.301, Test loss: 0.446, Test accuracy: 81.69 

Round  42, Train loss: 0.339, Test loss: 0.461, Test accuracy: 81.30 

Round  43, Train loss: 0.342, Test loss: 0.456, Test accuracy: 81.53 

Round  44, Train loss: 0.294, Test loss: 0.460, Test accuracy: 81.26 

Round  45, Train loss: 0.255, Test loss: 0.447, Test accuracy: 82.24 

Round  46, Train loss: 0.258, Test loss: 0.447, Test accuracy: 81.83 

Round  47, Train loss: 0.262, Test loss: 0.450, Test accuracy: 81.68 

Round  48, Train loss: 0.266, Test loss: 0.438, Test accuracy: 82.33 

Round  49, Train loss: 0.286, Test loss: 0.443, Test accuracy: 82.07 

Round  50, Train loss: 0.250, Test loss: 0.463, Test accuracy: 81.59 

Round  51, Train loss: 0.279, Test loss: 0.437, Test accuracy: 82.38 

Round  52, Train loss: 0.225, Test loss: 0.455, Test accuracy: 82.22 

Round  53, Train loss: 0.244, Test loss: 0.431, Test accuracy: 82.80 

Round  54, Train loss: 0.302, Test loss: 0.443, Test accuracy: 82.23 

Round  55, Train loss: 0.261, Test loss: 0.429, Test accuracy: 82.65 

Round  56, Train loss: 0.292, Test loss: 0.434, Test accuracy: 82.90 

Round  57, Train loss: 0.250, Test loss: 0.425, Test accuracy: 83.52 

Round  58, Train loss: 0.245, Test loss: 0.411, Test accuracy: 83.84 

Round  59, Train loss: 0.254, Test loss: 0.431, Test accuracy: 83.00 

Round  60, Train loss: 0.230, Test loss: 0.427, Test accuracy: 83.21 

Round  61, Train loss: 0.277, Test loss: 0.428, Test accuracy: 82.97 

Round  62, Train loss: 0.189, Test loss: 0.435, Test accuracy: 83.59 

Round  63, Train loss: 0.259, Test loss: 0.428, Test accuracy: 83.28 

Round  64, Train loss: 0.233, Test loss: 0.433, Test accuracy: 83.25 

Round  65, Train loss: 0.250, Test loss: 0.418, Test accuracy: 83.72 

Round  66, Train loss: 0.234, Test loss: 0.415, Test accuracy: 83.90 

Round  67, Train loss: 0.261, Test loss: 0.415, Test accuracy: 84.09 

Round  68, Train loss: 0.211, Test loss: 0.440, Test accuracy: 83.26 

Round  69, Train loss: 0.246, Test loss: 0.427, Test accuracy: 84.18 

Round  70, Train loss: 0.186, Test loss: 0.423, Test accuracy: 84.07 

Round  71, Train loss: 0.198, Test loss: 0.420, Test accuracy: 84.62 

Round  72, Train loss: 0.271, Test loss: 0.425, Test accuracy: 84.02 

Round  73, Train loss: 0.200, Test loss: 0.433, Test accuracy: 84.06 

Round  74, Train loss: 0.180, Test loss: 0.426, Test accuracy: 84.38 

Round  75, Train loss: 0.179, Test loss: 0.420, Test accuracy: 84.56 

Round  76, Train loss: 0.194, Test loss: 0.432, Test accuracy: 84.39 

Round  77, Train loss: 0.222, Test loss: 0.434, Test accuracy: 83.92 

Round  78, Train loss: 0.188, Test loss: 0.435, Test accuracy: 84.47 

Round  79, Train loss: 0.212, Test loss: 0.428, Test accuracy: 84.68 

Round  80, Train loss: 0.228, Test loss: 0.425, Test accuracy: 84.40 

Round  81, Train loss: 0.220, Test loss: 0.430, Test accuracy: 84.16 

Round  82, Train loss: 0.188, Test loss: 0.414, Test accuracy: 84.50 

Round  83, Train loss: 0.141, Test loss: 0.428, Test accuracy: 84.75 

Round  84, Train loss: 0.195, Test loss: 0.417, Test accuracy: 84.52 

Round  85, Train loss: 0.145, Test loss: 0.422, Test accuracy: 84.64 

Round  86, Train loss: 0.168, Test loss: 0.435, Test accuracy: 84.33 

Round  87, Train loss: 0.181, Test loss: 0.421, Test accuracy: 84.92 

Round  88, Train loss: 0.160, Test loss: 0.430, Test accuracy: 84.86 

Round  89, Train loss: 0.237, Test loss: 0.428, Test accuracy: 84.51 

Round  90, Train loss: 0.209, Test loss: 0.431, Test accuracy: 84.89 

Round  91, Train loss: 0.148, Test loss: 0.437, Test accuracy: 84.67 

Round  92, Train loss: 0.164, Test loss: 0.443, Test accuracy: 84.79 

Round  93, Train loss: 0.214, Test loss: 0.442, Test accuracy: 84.65 

Round  94, Train loss: 0.214, Test loss: 0.448, Test accuracy: 84.43 

Round  95, Train loss: 0.222, Test loss: 0.441, Test accuracy: 84.62 

Round  96, Train loss: 0.140, Test loss: 0.449, Test accuracy: 84.57 

Round  97, Train loss: 0.162, Test loss: 0.434, Test accuracy: 84.69 

Round  98, Train loss: 0.169, Test loss: 0.443, Test accuracy: 84.58 

Round  99, Train loss: 0.151, Test loss: 0.462, Test accuracy: 84.39 

Final Round, Train loss: 0.152, Test loss: 0.430, Test accuracy: 85.40 

Average accuracy final 10 rounds: 84.62916666666666 

998.9166803359985
[1.4085090160369873, 2.5901942253112793, 3.7717905044555664, 4.947521924972534, 6.118747711181641, 7.290632009506226, 8.460937738418579, 9.631439924240112, 10.80173134803772, 11.972764015197754, 13.143340349197388, 14.319161176681519, 15.48944902420044, 16.666112661361694, 17.84020686149597, 19.013636112213135, 20.186256408691406, 21.356943130493164, 22.51974129676819, 23.68726873397827, 24.85544514656067, 26.030997276306152, 27.2097384929657, 28.386578798294067, 29.568939924240112, 30.74777579307556, 31.923850774765015, 33.1014986038208, 34.280696868896484, 35.4580352306366, 36.63169550895691, 37.810856342315674, 38.98966145515442, 40.16828417778015, 41.348604679107666, 42.528897523880005, 43.70624852180481, 44.88362789154053, 46.066184520721436, 47.24885034561157, 48.427539587020874, 49.60358738899231, 50.781264305114746, 51.95056676864624, 53.12036156654358, 54.29147005081177, 55.464146852493286, 56.636860370635986, 57.809197425842285, 58.989086866378784, 60.1667594909668, 61.34241342544556, 62.51440668106079, 63.684160232543945, 64.85339522361755, 66.02288722991943, 67.19371128082275, 68.36122226715088, 69.5300886631012, 70.69880676269531, 71.86772060394287, 73.03893041610718, 74.20987057685852, 75.37513065338135, 76.53219056129456, 77.69051885604858, 78.84786319732666, 80.00457167625427, 81.15838146209717, 82.32045388221741, 83.32124352455139, 84.32499241828918, 85.32964396476746, 86.32960605621338, 87.33585214614868, 88.33821392059326, 89.33717966079712, 90.34098935127258, 91.34441900253296, 92.34354186058044, 93.34468626976013, 94.34829211235046, 95.35561943054199, 96.3576648235321, 97.35460209846497, 98.35438799858093, 99.35573053359985, 100.35546255111694, 101.35947227478027, 102.36605167388916, 103.37003636360168, 104.37153267860413, 105.3788423538208, 106.38334035873413, 107.38468432426453, 108.39137482643127, 109.39714431762695, 110.3999547958374, 111.40350580215454, 112.41079640388489, 114.19190549850464]
[33.85, 40.19166666666667, 41.34166666666667, 47.375, 42.59166666666667, 48.31666666666667, 53.125, 54.53333333333333, 61.78333333333333, 61.05, 60.608333333333334, 66.41666666666667, 65.43333333333334, 65.93333333333334, 68.31666666666666, 69.95, 69.90833333333333, 71.875, 73.09166666666667, 72.69166666666666, 73.70833333333333, 72.99166666666666, 75.18333333333334, 75.925, 76.08333333333333, 75.56666666666666, 76.70833333333333, 77.30833333333334, 77.36666666666666, 78.24166666666666, 78.36666666666666, 79.2, 79.04166666666667, 79.33333333333333, 79.61666666666666, 80.2, 80.10833333333333, 80.10833333333333, 81.40833333333333, 81.125, 81.325, 81.69166666666666, 81.3, 81.53333333333333, 81.25833333333334, 82.24166666666666, 81.83333333333333, 81.68333333333334, 82.325, 82.06666666666666, 81.59166666666667, 82.375, 82.21666666666667, 82.8, 82.23333333333333, 82.65, 82.9, 83.51666666666667, 83.84166666666667, 83.0, 83.20833333333333, 82.96666666666667, 83.59166666666667, 83.28333333333333, 83.25, 83.725, 83.9, 84.09166666666667, 83.25833333333334, 84.18333333333334, 84.06666666666666, 84.61666666666666, 84.01666666666667, 84.05833333333334, 84.38333333333334, 84.55833333333334, 84.39166666666667, 83.925, 84.46666666666667, 84.68333333333334, 84.4, 84.15833333333333, 84.5, 84.75, 84.51666666666667, 84.64166666666667, 84.33333333333333, 84.91666666666667, 84.85833333333333, 84.50833333333334, 84.89166666666667, 84.675, 84.79166666666667, 84.65, 84.43333333333334, 84.61666666666666, 84.56666666666666, 84.69166666666666, 84.58333333333333, 84.39166666666667, 85.4]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 7939 (global); Percentage 2.58 (7939/307387 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.897, Test loss: 1.004, Test accuracy: 44.82 

Round   1, Train loss: 0.788, Test loss: 0.998, Test accuracy: 49.98 

Round   2, Train loss: 0.663, Test loss: 0.866, Test accuracy: 56.74 

Round   3, Train loss: 0.684, Test loss: 0.819, Test accuracy: 61.35 

Round   4, Train loss: 0.624, Test loss: 0.712, Test accuracy: 66.10 

Round   5, Train loss: 0.595, Test loss: 0.717, Test accuracy: 68.38 

Round   6, Train loss: 0.646, Test loss: 0.665, Test accuracy: 70.31 

Round   7, Train loss: 0.531, Test loss: 0.604, Test accuracy: 73.97 

Round   8, Train loss: 0.517, Test loss: 0.598, Test accuracy: 74.66 

Round   9, Train loss: 0.476, Test loss: 0.582, Test accuracy: 75.58 

Round  10, Train loss: 0.510, Test loss: 0.591, Test accuracy: 75.45 

Round  11, Train loss: 0.429, Test loss: 0.578, Test accuracy: 75.89 

Round  12, Train loss: 0.453, Test loss: 0.566, Test accuracy: 76.25 

Round  13, Train loss: 0.478, Test loss: 0.557, Test accuracy: 76.60 

Round  14, Train loss: 0.443, Test loss: 0.572, Test accuracy: 76.40 

Round  15, Train loss: 0.445, Test loss: 0.580, Test accuracy: 76.40 

Round  16, Train loss: 0.446, Test loss: 0.562, Test accuracy: 77.18 

Round  17, Train loss: 0.374, Test loss: 0.571, Test accuracy: 77.35 

Round  18, Train loss: 0.350, Test loss: 0.579, Test accuracy: 77.18 

Round  19, Train loss: 0.451, Test loss: 0.572, Test accuracy: 78.09 

Round  20, Train loss: 0.431, Test loss: 0.572, Test accuracy: 78.17 

Round  21, Train loss: 0.364, Test loss: 0.563, Test accuracy: 78.89 

Round  22, Train loss: 0.332, Test loss: 0.557, Test accuracy: 79.35 

Round  23, Train loss: 0.293, Test loss: 0.581, Test accuracy: 79.03 

Round  24, Train loss: 0.268, Test loss: 0.597, Test accuracy: 78.69 

Round  25, Train loss: 0.336, Test loss: 0.603, Test accuracy: 78.65 

Round  26, Train loss: 0.322, Test loss: 0.618, Test accuracy: 78.72 

Round  27, Train loss: 0.256, Test loss: 0.618, Test accuracy: 79.00 

Round  28, Train loss: 0.272, Test loss: 0.610, Test accuracy: 79.43 

Round  29, Train loss: 0.247, Test loss: 0.644, Test accuracy: 78.63 

Round  30, Train loss: 0.250, Test loss: 0.634, Test accuracy: 78.80 

Round  31, Train loss: 0.228, Test loss: 0.630, Test accuracy: 79.47 

Round  32, Train loss: 0.217, Test loss: 0.615, Test accuracy: 79.69 

Round  33, Train loss: 0.208, Test loss: 0.623, Test accuracy: 79.47 

Round  34, Train loss: 0.267, Test loss: 0.646, Test accuracy: 79.83 

Round  35, Train loss: 0.227, Test loss: 0.666, Test accuracy: 79.70 

Round  36, Train loss: 0.276, Test loss: 0.695, Test accuracy: 79.24 

Round  37, Train loss: 0.175, Test loss: 0.687, Test accuracy: 79.59 

Round  38, Train loss: 0.250, Test loss: 0.675, Test accuracy: 79.72 

Round  39, Train loss: 0.202, Test loss: 0.685, Test accuracy: 79.69 

Round  40, Train loss: 0.198, Test loss: 0.700, Test accuracy: 79.79 

Round  41, Train loss: 0.217, Test loss: 0.685, Test accuracy: 80.09 

Round  42, Train loss: 0.163, Test loss: 0.702, Test accuracy: 79.91 

Round  43, Train loss: 0.192, Test loss: 0.721, Test accuracy: 79.40 

Round  44, Train loss: 0.182, Test loss: 0.728, Test accuracy: 78.99 

Round  45, Train loss: 0.158, Test loss: 0.723, Test accuracy: 79.43 

Round  46, Train loss: 0.198, Test loss: 0.722, Test accuracy: 79.84 

Round  47, Train loss: 0.136, Test loss: 0.746, Test accuracy: 79.74 

Round  48, Train loss: 0.152, Test loss: 0.738, Test accuracy: 80.22 

Round  49, Train loss: 0.118, Test loss: 0.748, Test accuracy: 80.22 

Round  50, Train loss: 0.171, Test loss: 0.757, Test accuracy: 80.24 

Round  51, Train loss: 0.114, Test loss: 0.771, Test accuracy: 80.52 

Round  52, Train loss: 0.164, Test loss: 0.767, Test accuracy: 80.55 

Round  53, Train loss: 0.133, Test loss: 0.817, Test accuracy: 80.24 

Round  54, Train loss: 0.092, Test loss: 0.831, Test accuracy: 80.43 

Round  55, Train loss: 0.107, Test loss: 0.852, Test accuracy: 80.51 

Round  56, Train loss: 0.128, Test loss: 0.828, Test accuracy: 80.47 

Round  57, Train loss: 0.147, Test loss: 0.821, Test accuracy: 80.29 

Round  58, Train loss: 0.114, Test loss: 0.845, Test accuracy: 79.82 

Round  59, Train loss: 0.102, Test loss: 0.835, Test accuracy: 80.21 

Round  60, Train loss: 0.115, Test loss: 0.865, Test accuracy: 79.99 

Round  61, Train loss: 0.105, Test loss: 0.868, Test accuracy: 79.46 

Round  62, Train loss: 0.101, Test loss: 0.857, Test accuracy: 80.11 

Round  63, Train loss: 0.079, Test loss: 0.901, Test accuracy: 79.68 

Round  64, Train loss: 0.072, Test loss: 0.936, Test accuracy: 79.32 

Round  65, Train loss: 0.089, Test loss: 0.913, Test accuracy: 79.70 

Round  66, Train loss: 0.097, Test loss: 0.901, Test accuracy: 79.99 

Round  67, Train loss: 0.082, Test loss: 0.918, Test accuracy: 79.83 

Round  68, Train loss: 0.076, Test loss: 0.924, Test accuracy: 79.70 

Round  69, Train loss: 0.091, Test loss: 0.916, Test accuracy: 79.96 

Round  70, Train loss: 0.075, Test loss: 0.926, Test accuracy: 80.17 

Round  71, Train loss: 0.097, Test loss: 0.902, Test accuracy: 80.45 

Round  72, Train loss: 0.078, Test loss: 0.909, Test accuracy: 80.51 

Round  73, Train loss: 0.064, Test loss: 0.940, Test accuracy: 80.55 

Round  74, Train loss: 0.065, Test loss: 0.955, Test accuracy: 80.48 

Round  75, Train loss: 0.075, Test loss: 0.936, Test accuracy: 80.17 

Round  76, Train loss: 0.057, Test loss: 0.937, Test accuracy: 80.15 

Round  77, Train loss: 0.093, Test loss: 0.942, Test accuracy: 80.34 

Round  78, Train loss: 0.082, Test loss: 0.983, Test accuracy: 80.17 

Round  79, Train loss: 0.091, Test loss: 0.972, Test accuracy: 80.32 

Round  80, Train loss: 0.084, Test loss: 0.975, Test accuracy: 80.34 

Round  81, Train loss: 0.057, Test loss: 0.968, Test accuracy: 80.69 

Round  82, Train loss: 0.051, Test loss: 0.987, Test accuracy: 80.51 

Round  83, Train loss: 0.064, Test loss: 1.009, Test accuracy: 80.47 

Round  84, Train loss: 0.056, Test loss: 1.021, Test accuracy: 80.23 

Round  85, Train loss: 0.072, Test loss: 1.038, Test accuracy: 79.89 

Round  86, Train loss: 0.063, Test loss: 1.025, Test accuracy: 80.01 

Round  87, Train loss: 0.067, Test loss: 0.993, Test accuracy: 80.35 

Round  88, Train loss: 0.038, Test loss: 1.023, Test accuracy: 80.22 

Round  89, Train loss: 0.050, Test loss: 1.012, Test accuracy: 80.48 

Round  90, Train loss: 0.056, Test loss: 0.989, Test accuracy: 80.53 

Round  91, Train loss: 0.034, Test loss: 1.020, Test accuracy: 80.45 

Round  92, Train loss: 0.034, Test loss: 1.032, Test accuracy: 80.67 

Round  93, Train loss: 0.073, Test loss: 1.039, Test accuracy: 80.56 

Round  94, Train loss: 0.050, Test loss: 1.023, Test accuracy: 80.67 

Round  95, Train loss: 0.038, Test loss: 1.062, Test accuracy: 80.57 

Round  96, Train loss: 0.057, Test loss: 1.042, Test accuracy: 80.53 

Round  97, Train loss: 0.051, Test loss: 1.048, Test accuracy: 80.86 

Round  98, Train loss: 0.044, Test loss: 1.062, Test accuracy: 80.73 

Round  99, Train loss: 0.040, Test loss: 1.053, Test accuracy: 80.97 

Final Round, Train loss: 0.042, Test loss: 1.089, Test accuracy: 80.88 

Average accuracy final 10 rounds: 80.65416666666665 

937.005669593811
[1.4098539352416992, 2.4158058166503906, 3.4216089248657227, 4.428410291671753, 5.435052156448364, 6.444446563720703, 7.449214220046997, 8.453679084777832, 9.461769104003906, 10.471785545349121, 11.47831654548645, 12.487751007080078, 13.496224880218506, 14.50154709815979, 15.513338565826416, 16.524377584457397, 17.532420873641968, 18.54372811317444, 19.55321502685547, 20.563546657562256, 21.572382926940918, 22.583727836608887, 23.589641571044922, 24.59340190887451, 25.604132413864136, 26.614034175872803, 27.621422052383423, 28.631205081939697, 29.638116359710693, 30.64224410057068, 31.648173570632935, 32.65319585800171, 33.66315054893494, 34.67139720916748, 35.677531003952026, 36.68837881088257, 37.696349143981934, 38.70346188545227, 39.718096017837524, 40.72945237159729, 41.73827648162842, 42.74496626853943, 43.756500482559204, 44.76528453826904, 45.771955490112305, 46.78225326538086, 47.78989362716675, 48.8039186000824, 49.81739830970764, 50.82647705078125, 51.833635091781616, 52.839104652404785, 53.84592628479004, 54.851417541503906, 55.86699151992798, 56.87211847305298, 57.87715530395508, 58.89114189147949, 59.89658546447754, 60.90212035179138, 61.91001081466675, 62.91647124290466, 63.92545127868652, 64.93907070159912, 65.95753574371338, 66.96382594108582, 67.97474455833435, 68.98419380187988, 69.99170589447021, 71.00402092933655, 72.01293110847473, 73.01830720901489, 74.02392864227295, 75.03561449050903, 76.0446400642395, 77.04865407943726, 78.06041860580444, 79.06732559204102, 80.0919041633606, 81.1044397354126, 82.11010408401489, 83.11414766311646, 84.11846542358398, 85.12079882621765, 86.12780928611755, 87.13494729995728, 88.13496279716492, 89.14336657524109, 90.15049648284912, 91.15417838096619, 92.16356301307678, 93.17098498344421, 94.1776933670044, 95.1859622001648, 96.19542694091797, 97.20220470428467, 98.2130823135376, 99.22365021705627, 100.23233580589294, 101.2394073009491, 103.20514512062073]
[44.81666666666667, 49.975, 56.74166666666667, 61.35, 66.1, 68.38333333333334, 70.30833333333334, 73.96666666666667, 74.65833333333333, 75.58333333333333, 75.45, 75.89166666666667, 76.25, 76.6, 76.4, 76.4, 77.18333333333334, 77.35, 77.18333333333334, 78.09166666666667, 78.16666666666667, 78.89166666666667, 79.35, 79.03333333333333, 78.69166666666666, 78.65, 78.725, 79.0, 79.43333333333334, 78.63333333333334, 78.8, 79.46666666666667, 79.69166666666666, 79.475, 79.825, 79.7, 79.24166666666666, 79.59166666666667, 79.71666666666667, 79.69166666666666, 79.79166666666667, 80.09166666666667, 79.90833333333333, 79.4, 78.99166666666666, 79.43333333333334, 79.84166666666667, 79.74166666666666, 80.225, 80.21666666666667, 80.24166666666666, 80.51666666666667, 80.55, 80.24166666666666, 80.43333333333334, 80.50833333333334, 80.475, 80.29166666666667, 79.81666666666666, 80.20833333333333, 79.99166666666666, 79.45833333333333, 80.10833333333333, 79.68333333333334, 79.31666666666666, 79.7, 79.99166666666666, 79.825, 79.7, 79.95833333333333, 80.175, 80.45, 80.50833333333334, 80.55, 80.48333333333333, 80.16666666666667, 80.15, 80.34166666666667, 80.16666666666667, 80.31666666666666, 80.34166666666667, 80.69166666666666, 80.50833333333334, 80.475, 80.23333333333333, 79.89166666666667, 80.00833333333334, 80.35, 80.21666666666667, 80.48333333333333, 80.53333333333333, 80.45, 80.66666666666667, 80.55833333333334, 80.675, 80.56666666666666, 80.53333333333333, 80.85833333333333, 80.73333333333333, 80.96666666666667, 80.875]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 0.635, Test loss: 1.104, Test accuracy: 35.71
Round   1, Train loss: 0.511, Test loss: 1.117, Test accuracy: 44.25
Round   2, Train loss: 0.527, Test loss: 1.125, Test accuracy: 44.76
Round   3, Train loss: 0.478, Test loss: 1.065, Test accuracy: 48.25
Round   4, Train loss: 0.470, Test loss: 1.113, Test accuracy: 49.14
Round   5, Train loss: 0.377, Test loss: 1.126, Test accuracy: 52.38
Round   6, Train loss: 0.438, Test loss: 1.145, Test accuracy: 52.02
Round   7, Train loss: 0.410, Test loss: 1.041, Test accuracy: 53.57
Round   8, Train loss: 0.353, Test loss: 1.046, Test accuracy: 56.03
Round   9, Train loss: 0.366, Test loss: 1.034, Test accuracy: 56.10
Round  10, Train loss: 0.411, Test loss: 0.972, Test accuracy: 58.49
Round  11, Train loss: 0.325, Test loss: 0.962, Test accuracy: 61.03
Round  12, Train loss: 0.317, Test loss: 0.951, Test accuracy: 63.16
Round  13, Train loss: 0.294, Test loss: 0.939, Test accuracy: 63.92
Round  14, Train loss: 0.328, Test loss: 0.937, Test accuracy: 63.89
Round  15, Train loss: 0.319, Test loss: 0.929, Test accuracy: 64.01
Round  16, Train loss: 0.324, Test loss: 0.927, Test accuracy: 63.96
Round  17, Train loss: 0.249, Test loss: 0.913, Test accuracy: 65.23
Round  18, Train loss: 0.296, Test loss: 0.910, Test accuracy: 64.68
Round  19, Train loss: 0.280, Test loss: 0.893, Test accuracy: 67.00
Round  20, Train loss: 0.270, Test loss: 0.888, Test accuracy: 66.84
Round  21, Train loss: 0.199, Test loss: 0.875, Test accuracy: 67.02
Round  22, Train loss: 0.302, Test loss: 0.872, Test accuracy: 67.54
Round  23, Train loss: 0.243, Test loss: 0.862, Test accuracy: 67.94
Round  24, Train loss: 0.251, Test loss: 0.859, Test accuracy: 68.42
Round  25, Train loss: 0.225, Test loss: 0.859, Test accuracy: 68.07
Round  26, Train loss: 0.234, Test loss: 0.858, Test accuracy: 67.62
Round  27, Train loss: 0.243, Test loss: 0.849, Test accuracy: 67.28
Round  28, Train loss: 0.223, Test loss: 0.844, Test accuracy: 67.62
Round  29, Train loss: 0.191, Test loss: 0.829, Test accuracy: 68.51
Round  30, Train loss: 0.228, Test loss: 0.825, Test accuracy: 68.90
Round  31, Train loss: 0.176, Test loss: 0.828, Test accuracy: 68.10
Round  32, Train loss: 0.241, Test loss: 0.815, Test accuracy: 68.26
Round  33, Train loss: 0.152, Test loss: 0.809, Test accuracy: 68.62
Round  34, Train loss: 0.190, Test loss: 0.809, Test accuracy: 68.72
Round  35, Train loss: 0.212, Test loss: 0.805, Test accuracy: 68.59
Round  36, Train loss: 0.129, Test loss: 0.792, Test accuracy: 69.70
Round  37, Train loss: 0.175, Test loss: 0.791, Test accuracy: 69.91
Round  38, Train loss: 0.158, Test loss: 0.785, Test accuracy: 69.91
Round  39, Train loss: 0.141, Test loss: 0.786, Test accuracy: 69.69
Round  40, Train loss: 0.203, Test loss: 0.794, Test accuracy: 68.48
Round  41, Train loss: 0.143, Test loss: 0.790, Test accuracy: 67.78
Round  42, Train loss: 0.156, Test loss: 0.782, Test accuracy: 69.26
Round  43, Train loss: 0.132, Test loss: 0.779, Test accuracy: 68.90
Round  44, Train loss: 0.119, Test loss: 0.773, Test accuracy: 69.18
Round  45, Train loss: 0.153, Test loss: 0.769, Test accuracy: 69.52
Round  46, Train loss: 0.103, Test loss: 0.772, Test accuracy: 69.36
Round  47, Train loss: 0.123, Test loss: 0.774, Test accuracy: 68.20
Round  48, Train loss: 0.122, Test loss: 0.763, Test accuracy: 69.06
Round  49, Train loss: 0.118, Test loss: 0.758, Test accuracy: 69.45
Round  50, Train loss: 0.165, Test loss: 0.751, Test accuracy: 70.67
Round  51, Train loss: 0.132, Test loss: 0.749, Test accuracy: 70.33
Round  52, Train loss: 0.137, Test loss: 0.749, Test accuracy: 69.70
Round  53, Train loss: 0.154, Test loss: 0.745, Test accuracy: 69.93
Round  54, Train loss: 0.087, Test loss: 0.750, Test accuracy: 69.18
Round  55, Train loss: 0.078, Test loss: 0.738, Test accuracy: 69.99
Round  56, Train loss: 0.107, Test loss: 0.747, Test accuracy: 69.02
Round  57, Train loss: 0.080, Test loss: 0.742, Test accuracy: 69.16
Round  58, Train loss: 0.078, Test loss: 0.729, Test accuracy: 70.01
Round  59, Train loss: 0.132, Test loss: 0.733, Test accuracy: 69.98
Round  60, Train loss: 0.128, Test loss: 0.725, Test accuracy: 70.28
Round  61, Train loss: 0.082, Test loss: 0.714, Test accuracy: 71.16
Round  62, Train loss: 0.078, Test loss: 0.713, Test accuracy: 70.97
Round  63, Train loss: 0.126, Test loss: 0.720, Test accuracy: 70.62
Round  64, Train loss: 0.087, Test loss: 0.716, Test accuracy: 70.88
Round  65, Train loss: 0.090, Test loss: 0.714, Test accuracy: 70.43
Round  66, Train loss: 0.084, Test loss: 0.723, Test accuracy: 69.53
Round  67, Train loss: 0.084, Test loss: 0.712, Test accuracy: 70.38
Round  68, Train loss: 0.118, Test loss: 0.714, Test accuracy: 69.92
Round  69, Train loss: 0.123, Test loss: 0.712, Test accuracy: 70.27
Round  70, Train loss: 0.076, Test loss: 0.709, Test accuracy: 70.66
Round  71, Train loss: 0.093, Test loss: 0.727, Test accuracy: 68.92
Round  72, Train loss: 0.097, Test loss: 0.715, Test accuracy: 69.53
Round  73, Train loss: 0.071, Test loss: 0.711, Test accuracy: 69.18
Round  74, Train loss: 0.094, Test loss: 0.699, Test accuracy: 70.00
Round  75, Train loss: 0.079, Test loss: 0.706, Test accuracy: 69.63
Round  76, Train loss: 0.097, Test loss: 0.700, Test accuracy: 69.88
Round  77, Train loss: 0.088, Test loss: 0.703, Test accuracy: 69.67
Round  78, Train loss: 0.077, Test loss: 0.702, Test accuracy: 69.47
Round  79, Train loss: 0.085, Test loss: 0.705, Test accuracy: 69.11
Round  80, Train loss: 0.056, Test loss: 0.699, Test accuracy: 69.28
Round  81, Train loss: 0.071, Test loss: 0.700, Test accuracy: 68.93
Round  82, Train loss: 0.057, Test loss: 0.691, Test accuracy: 69.62
Round  83, Train loss: 0.056, Test loss: 0.693, Test accuracy: 69.50
Round  84, Train loss: 0.090, Test loss: 0.699, Test accuracy: 69.03
Round  85, Train loss: 0.064, Test loss: 0.696, Test accuracy: 69.35
Round  86, Train loss: 0.051, Test loss: 0.694, Test accuracy: 69.68
Round  87, Train loss: 0.056, Test loss: 0.691, Test accuracy: 70.01
Round  88, Train loss: 0.064, Test loss: 0.701, Test accuracy: 68.42
Round  89, Train loss: 0.093, Test loss: 0.707, Test accuracy: 68.42
Round  90, Train loss: 0.088, Test loss: 0.693, Test accuracy: 68.84
Round  91, Train loss: 0.053, Test loss: 0.683, Test accuracy: 69.72
Round  92, Train loss: 0.069, Test loss: 0.686, Test accuracy: 69.08
Round  93, Train loss: 0.068, Test loss: 0.677, Test accuracy: 70.50
Round  94, Train loss: 0.069, Test loss: 0.673, Test accuracy: 70.71
Round  95, Train loss: 0.056, Test loss: 0.668, Test accuracy: 71.21
Round  96, Train loss: 0.064, Test loss: 0.676, Test accuracy: 70.99
Round  97, Train loss: 0.071, Test loss: 0.686, Test accuracy: 70.17
Round  98, Train loss: 0.080, Test loss: 0.677, Test accuracy: 70.97
Round  99, Train loss: 0.076, Test loss: 0.683, Test accuracy: 70.70
Final Round, Train loss: 0.058, Test loss: 0.697, Test accuracy: 69.34
Average accuracy final 10 rounds: 70.29083333333332
1770.3711578845978
[]
[35.708333333333336, 44.25, 44.75833333333333, 48.25, 49.141666666666666, 52.375, 52.016666666666666, 53.56666666666667, 56.03333333333333, 56.1, 58.49166666666667, 61.03333333333333, 63.15833333333333, 63.925, 63.891666666666666, 64.00833333333334, 63.958333333333336, 65.23333333333333, 64.68333333333334, 67.0, 66.84166666666667, 67.01666666666667, 67.54166666666667, 67.94166666666666, 68.41666666666667, 68.06666666666666, 67.625, 67.28333333333333, 67.625, 68.50833333333334, 68.9, 68.1, 68.25833333333334, 68.625, 68.71666666666667, 68.59166666666667, 69.7, 69.90833333333333, 69.90833333333333, 69.69166666666666, 68.48333333333333, 67.775, 69.25833333333334, 68.9, 69.18333333333334, 69.51666666666667, 69.35833333333333, 68.2, 69.05833333333334, 69.45, 70.66666666666667, 70.33333333333333, 69.7, 69.93333333333334, 69.18333333333334, 69.99166666666666, 69.01666666666667, 69.15833333333333, 70.00833333333334, 69.98333333333333, 70.275, 71.15833333333333, 70.975, 70.625, 70.88333333333334, 70.43333333333334, 69.53333333333333, 70.375, 69.91666666666667, 70.26666666666667, 70.65833333333333, 68.91666666666667, 69.525, 69.18333333333334, 70.0, 69.63333333333334, 69.875, 69.66666666666667, 69.46666666666667, 69.10833333333333, 69.275, 68.93333333333334, 69.61666666666666, 69.5, 69.025, 69.35, 69.68333333333334, 70.00833333333334, 68.41666666666667, 68.41666666666667, 68.84166666666667, 69.725, 69.08333333333333, 70.5, 70.70833333333333, 71.20833333333333, 70.99166666666666, 70.175, 70.975, 70.7, 69.34166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
Round   0, Train loss: 0.910, Test loss: 0.999, Test accuracy: 42.65
Round   0: Global train loss: 0.910, Global test loss: 1.099, Global test accuracy: 32.88
Round   1, Train loss: 0.738, Test loss: 0.964, Test accuracy: 46.02
Round   1: Global train loss: 0.738, Global test loss: 1.099, Global test accuracy: 33.17
Round   2, Train loss: 0.680, Test loss: 0.900, Test accuracy: 50.87
Round   2: Global train loss: 0.680, Global test loss: 1.099, Global test accuracy: 32.36
Round   3, Train loss: 0.522, Test loss: 0.844, Test accuracy: 55.77
Round   3: Global train loss: 0.522, Global test loss: 1.099, Global test accuracy: 32.94
Round   4, Train loss: 0.551, Test loss: 0.824, Test accuracy: 57.69
Round   4: Global train loss: 0.551, Global test loss: 1.099, Global test accuracy: 32.63
Round   5, Train loss: 0.027, Test loss: 0.794, Test accuracy: 59.42
Round   5: Global train loss: 0.027, Global test loss: 1.098, Global test accuracy: 32.88
Round   6, Train loss: 0.527, Test loss: 0.766, Test accuracy: 61.67
Round   6: Global train loss: 0.527, Global test loss: 1.098, Global test accuracy: 32.15
Round   7, Train loss: -0.160, Test loss: 0.748, Test accuracy: 62.32
Round   7: Global train loss: -0.160, Global test loss: 1.098, Global test accuracy: 32.70
Round   8, Train loss: 0.067, Test loss: 0.748, Test accuracy: 62.12
Round   8: Global train loss: 0.067, Global test loss: 1.099, Global test accuracy: 32.02
Round   9, Train loss: -0.426, Test loss: 0.707, Test accuracy: 64.42
Round   9: Global train loss: -0.426, Global test loss: 1.098, Global test accuracy: 32.46
Round  10, Train loss: -0.359, Test loss: 0.730, Test accuracy: 63.94
Round  10: Global train loss: -0.359, Global test loss: 1.099, Global test accuracy: 32.29
Round  11, Train loss: -0.697, Test loss: 0.715, Test accuracy: 65.83
Round  11: Global train loss: -0.697, Global test loss: 1.096, Global test accuracy: 33.46
Round  12, Train loss: -0.085, Test loss: 0.681, Test accuracy: 69.01
Round  12: Global train loss: -0.085, Global test loss: 1.095, Global test accuracy: 35.67
Round  13, Train loss: -0.804, Test loss: 0.664, Test accuracy: 70.38
Round  13: Global train loss: -0.804, Global test loss: 1.095, Global test accuracy: 35.85
Round  14, Train loss: -0.311, Test loss: 0.670, Test accuracy: 69.91
Round  14: Global train loss: -0.311, Global test loss: 1.095, Global test accuracy: 35.91
Round  15, Train loss: -1.058, Test loss: 0.664, Test accuracy: 70.44
Round  15: Global train loss: -1.058, Global test loss: 1.094, Global test accuracy: 36.52
Round  16, Train loss: -0.899, Test loss: 0.657, Test accuracy: 70.50
Round  16: Global train loss: -0.899, Global test loss: 1.094, Global test accuracy: 36.83
Round  17, Train loss: -1.202, Test loss: 0.656, Test accuracy: 70.22
Round  17: Global train loss: -1.202, Global test loss: 1.093, Global test accuracy: 37.62
Round  18, Train loss: -1.190, Test loss: 0.662, Test accuracy: 70.29
Round  18: Global train loss: -1.190, Global test loss: 1.093, Global test accuracy: 37.90
Round  19, Train loss: -0.888, Test loss: 0.662, Test accuracy: 70.36
Round  19: Global train loss: -0.888, Global test loss: 1.093, Global test accuracy: 38.17
Round  20, Train loss: -1.027, Test loss: 0.659, Test accuracy: 70.92
Round  20: Global train loss: -1.027, Global test loss: 1.093, Global test accuracy: 38.08
Round  21, Train loss: -1.325, Test loss: 0.652, Test accuracy: 71.81
Round  21: Global train loss: -1.325, Global test loss: 1.092, Global test accuracy: 38.05
Round  22, Train loss: -1.170, Test loss: 0.635, Test accuracy: 72.63
Round  22: Global train loss: -1.170, Global test loss: 1.092, Global test accuracy: 38.18
Round  23, Train loss: -1.767, Test loss: 0.630, Test accuracy: 72.88
Round  23: Global train loss: -1.767, Global test loss: 1.091, Global test accuracy: 38.13
Round  24, Train loss: -1.481, Test loss: 0.639, Test accuracy: 72.48
Round  24: Global train loss: -1.481, Global test loss: 1.091, Global test accuracy: 38.50
Round  25, Train loss: -1.625, Test loss: 0.613, Test accuracy: 74.60
Round  25: Global train loss: -1.625, Global test loss: 1.090, Global test accuracy: 38.67
Round  26, Train loss: -1.522, Test loss: 0.599, Test accuracy: 75.04
Round  26: Global train loss: -1.522, Global test loss: 1.090, Global test accuracy: 38.58
Round  27, Train loss: -2.424, Test loss: 0.606, Test accuracy: 75.32
Round  27: Global train loss: -2.424, Global test loss: 1.089, Global test accuracy: 38.55
Round  28, Train loss: -1.942, Test loss: 0.602, Test accuracy: 75.09
Round  28: Global train loss: -1.942, Global test loss: 1.089, Global test accuracy: 38.37
Round  29, Train loss: -1.533, Test loss: 0.598, Test accuracy: 75.23
Round  29: Global train loss: -1.533, Global test loss: 1.088, Global test accuracy: 39.07
Round  30, Train loss: -2.304, Test loss: 0.594, Test accuracy: 75.58
Round  30: Global train loss: -2.304, Global test loss: 1.088, Global test accuracy: 38.77
Round  31, Train loss: -2.209, Test loss: 0.574, Test accuracy: 76.67
Round  31: Global train loss: -2.209, Global test loss: 1.088, Global test accuracy: 38.42
Round  32, Train loss: -2.393, Test loss: 0.589, Test accuracy: 75.90
Round  32: Global train loss: -2.393, Global test loss: 1.088, Global test accuracy: 38.77
Round  33, Train loss: -2.258, Test loss: 0.591, Test accuracy: 75.96
Round  33: Global train loss: -2.258, Global test loss: 1.087, Global test accuracy: 38.88
Round  34, Train loss: -2.756, Test loss: 0.622, Test accuracy: 75.00
Round  34: Global train loss: -2.756, Global test loss: 1.087, Global test accuracy: 38.90
Round  35, Train loss: -3.007, Test loss: 0.608, Test accuracy: 75.10
Round  35: Global train loss: -3.007, Global test loss: 1.087, Global test accuracy: 39.23
Round  36, Train loss: -2.295, Test loss: 0.610, Test accuracy: 75.77
Round  36: Global train loss: -2.295, Global test loss: 1.086, Global test accuracy: 39.11
Round  37, Train loss: -2.561, Test loss: 0.608, Test accuracy: 76.18
Round  37: Global train loss: -2.561, Global test loss: 1.086, Global test accuracy: 39.13
Round  38, Train loss: -2.488, Test loss: 0.600, Test accuracy: 76.03
Round  38: Global train loss: -2.488, Global test loss: 1.086, Global test accuracy: 39.17
Round  39, Train loss: -2.458, Test loss: 0.575, Test accuracy: 76.92
Round  39: Global train loss: -2.458, Global test loss: 1.086, Global test accuracy: 39.01
Round  40, Train loss: -2.848, Test loss: 0.562, Test accuracy: 77.51
Round  40: Global train loss: -2.848, Global test loss: 1.086, Global test accuracy: 39.23
Round  41, Train loss: -2.566, Test loss: 0.569, Test accuracy: 77.36
Round  41: Global train loss: -2.566, Global test loss: 1.087, Global test accuracy: 39.21
Round  42, Train loss: -3.249, Test loss: 0.574, Test accuracy: 77.02
Round  42: Global train loss: -3.249, Global test loss: 1.086, Global test accuracy: 39.23
Round  43, Train loss: -3.506, Test loss: 0.574, Test accuracy: 77.08
Round  43: Global train loss: -3.506, Global test loss: 1.086, Global test accuracy: 39.19
Round  44, Train loss: -3.341, Test loss: 0.583, Test accuracy: 77.19
Round  44: Global train loss: -3.341, Global test loss: 1.086, Global test accuracy: 39.08
Round  45, Train loss: -3.651, Test loss: 0.589, Test accuracy: 76.93
Round  45: Global train loss: -3.651, Global test loss: 1.086, Global test accuracy: 38.82
Round  46, Train loss: -3.833, Test loss: 0.602, Test accuracy: 76.71
Round  46: Global train loss: -3.833, Global test loss: 1.086, Global test accuracy: 38.87
Round  47, Train loss: -4.500, Test loss: 0.594, Test accuracy: 77.42
Round  47: Global train loss: -4.500, Global test loss: 1.086, Global test accuracy: 38.77
Round  48, Train loss: -2.985, Test loss: 0.587, Test accuracy: 77.49
Round  48: Global train loss: -2.985, Global test loss: 1.086, Global test accuracy: 38.78
Round  49, Train loss: -3.278, Test loss: 0.586, Test accuracy: 77.22
Round  49: Global train loss: -3.278, Global test loss: 1.086, Global test accuracy: 38.82
Round  50, Train loss: -3.466, Test loss: 0.581, Test accuracy: 77.89
Round  50: Global train loss: -3.466, Global test loss: 1.087, Global test accuracy: 38.63
Round  51, Train loss: -3.965, Test loss: 0.573, Test accuracy: 78.12
Round  51: Global train loss: -3.965, Global test loss: 1.087, Global test accuracy: 38.20
Round  52, Train loss: -4.883, Test loss: 0.569, Test accuracy: 78.67
Round  52: Global train loss: -4.883, Global test loss: 1.087, Global test accuracy: 38.14
Round  53, Train loss: -4.278, Test loss: 0.575, Test accuracy: 78.25
Round  53: Global train loss: -4.278, Global test loss: 1.086, Global test accuracy: 38.14
Round  54, Train loss: -4.584, Test loss: 0.574, Test accuracy: 77.87
Round  54: Global train loss: -4.584, Global test loss: 1.086, Global test accuracy: 38.51
Round  55, Train loss: -3.850, Test loss: 0.594, Test accuracy: 77.29
Round  55: Global train loss: -3.850, Global test loss: 1.086, Global test accuracy: 38.07
Round  56, Train loss: -3.762, Test loss: 0.593, Test accuracy: 77.53
Round  56: Global train loss: -3.762, Global test loss: 1.086, Global test accuracy: 38.52
Round  57, Train loss: -3.585, Test loss: 0.606, Test accuracy: 77.03
Round  57: Global train loss: -3.585, Global test loss: 1.084, Global test accuracy: 38.93
Round  58, Train loss: -4.543, Test loss: 0.593, Test accuracy: 77.58
Round  58: Global train loss: -4.543, Global test loss: 1.084, Global test accuracy: 38.67
Round  59, Train loss: -3.263, Test loss: 0.601, Test accuracy: 76.85
Round  59: Global train loss: -3.263, Global test loss: 1.084, Global test accuracy: 38.59
Round  60, Train loss: -4.482, Test loss: 0.622, Test accuracy: 76.89
Round  60: Global train loss: -4.482, Global test loss: 1.085, Global test accuracy: 38.61
Round  61, Train loss: -4.356, Test loss: 0.608, Test accuracy: 77.55
Round  61: Global train loss: -4.356, Global test loss: 1.084, Global test accuracy: 38.83
Round  62, Train loss: -4.062, Test loss: 0.609, Test accuracy: 77.86
Round  62: Global train loss: -4.062, Global test loss: 1.086, Global test accuracy: 38.27
Round  63, Train loss: -4.152, Test loss: 0.633, Test accuracy: 77.44
Round  63: Global train loss: -4.152, Global test loss: 1.085, Global test accuracy: 38.69
Round  64, Train loss: -3.975, Test loss: 0.661, Test accuracy: 77.19
Round  64: Global train loss: -3.975, Global test loss: 1.085, Global test accuracy: 38.67
Round  65, Train loss: -4.937, Test loss: 0.642, Test accuracy: 77.26
Round  65: Global train loss: -4.937, Global test loss: 1.086, Global test accuracy: 38.42
Round  66, Train loss: -3.976, Test loss: 0.651, Test accuracy: 76.56
Round  66: Global train loss: -3.976, Global test loss: 1.086, Global test accuracy: 38.44
Round  67, Train loss: -5.099, Test loss: 0.621, Test accuracy: 77.82
Round  67: Global train loss: -5.099, Global test loss: 1.087, Global test accuracy: 38.62
Round  68, Train loss: -4.643, Test loss: 0.634, Test accuracy: 77.17
Round  68: Global train loss: -4.643, Global test loss: 1.087, Global test accuracy: 38.55
Round  69, Train loss: -5.288, Test loss: 0.621, Test accuracy: 78.19
Round  69: Global train loss: -5.288, Global test loss: 1.087, Global test accuracy: 38.98
Round  70, Train loss: -4.791, Test loss: 0.596, Test accuracy: 78.40
Round  70: Global train loss: -4.791, Global test loss: 1.086, Global test accuracy: 38.91
Round  71, Train loss: -4.223, Test loss: 0.589, Test accuracy: 78.31
Round  71: Global train loss: -4.223, Global test loss: 1.086, Global test accuracy: 38.95
Round  72, Train loss: -5.740, Test loss: 0.621, Test accuracy: 77.61
Round  72: Global train loss: -5.740, Global test loss: 1.086, Global test accuracy: 39.35
Round  73, Train loss: -5.222, Test loss: 0.616, Test accuracy: 77.98
Round  73: Global train loss: -5.222, Global test loss: 1.086, Global test accuracy: 39.09
Round  74, Train loss: -4.521, Test loss: 0.640, Test accuracy: 77.12
Round  74: Global train loss: -4.521, Global test loss: 1.086, Global test accuracy: 39.12
Round  75, Train loss: -5.354, Test loss: 0.665, Test accuracy: 77.11
Round  75: Global train loss: -5.354, Global test loss: 1.085, Global test accuracy: 39.20
Round  76, Train loss: -5.605, Test loss: 0.646, Test accuracy: 77.53
Round  76: Global train loss: -5.605, Global test loss: 1.084, Global test accuracy: 39.40
Round  77, Train loss: -4.721, Test loss: 0.637, Test accuracy: 77.51
Round  77: Global train loss: -4.721, Global test loss: 1.084, Global test accuracy: 39.06
Round  78, Train loss: -4.090, Test loss: 0.644, Test accuracy: 76.98
Round  78: Global train loss: -4.090, Global test loss: 1.083, Global test accuracy: 39.42
Round  79, Train loss: -5.637, Test loss: 0.670, Test accuracy: 76.86
Round  79: Global train loss: -5.637, Global test loss: 1.084, Global test accuracy: 39.46
Round  80, Train loss: -5.588, Test loss: 0.667, Test accuracy: 76.57
Round  80: Global train loss: -5.588, Global test loss: 1.085, Global test accuracy: 39.68
Round  81, Train loss: -6.163, Test loss: 0.644, Test accuracy: 77.50
Round  81: Global train loss: -6.163, Global test loss: 1.088, Global test accuracy: 39.62
Round  82, Train loss: -5.047, Test loss: 0.646, Test accuracy: 77.40
Round  82: Global train loss: -5.047, Global test loss: 1.087, Global test accuracy: 39.39
Round  83, Train loss: -5.328, Test loss: 0.615, Test accuracy: 78.34
Round  83: Global train loss: -5.328, Global test loss: 1.087, Global test accuracy: 39.36
Round  84, Train loss: -5.663, Test loss: 0.609, Test accuracy: 78.79
Round  84: Global train loss: -5.663, Global test loss: 1.089, Global test accuracy: 39.67
Round  85, Train loss: -5.281, Test loss: 0.613, Test accuracy: 78.64
Round  85: Global train loss: -5.281, Global test loss: 1.088, Global test accuracy: 39.63
Round  86, Train loss: -4.716, Test loss: 0.629, Test accuracy: 78.28
Round  86: Global train loss: -4.716, Global test loss: 1.088, Global test accuracy: 39.51
Round  87, Train loss: -5.556, Test loss: 0.639, Test accuracy: 77.58
Round  87: Global train loss: -5.556, Global test loss: 1.087, Global test accuracy: 39.71
Round  88, Train loss: -5.669, Test loss: 0.638, Test accuracy: 78.40
Round  88: Global train loss: -5.669, Global test loss: 1.086, Global test accuracy: 39.80
Round  89, Train loss: -5.928, Test loss: 0.632, Test accuracy: 78.76
Round  89: Global train loss: -5.928, Global test loss: 1.088, Global test accuracy: 39.80
Round  90, Train loss: -5.323, Test loss: 0.651, Test accuracy: 78.26
Round  90: Global train loss: -5.323, Global test loss: 1.090, Global test accuracy: 39.52
Round  91, Train loss: -5.374, Test loss: 0.610, Test accuracy: 78.81
Round  91: Global train loss: -5.374, Global test loss: 1.088, Global test accuracy: 39.48
Round  92, Train loss: -5.802, Test loss: 0.619, Test accuracy: 78.03
Round  92: Global train loss: -5.802, Global test loss: 1.089, Global test accuracy: 39.76
Round  93, Train loss: -5.048, Test loss: 0.626, Test accuracy: 77.74
Round  93: Global train loss: -5.048, Global test loss: 1.088, Global test accuracy: 40.18
Round  94, Train loss: -5.777, Test loss: 0.642, Test accuracy: 77.83
Round  94: Global train loss: -5.777, Global test loss: 1.089, Global test accuracy: 39.85
Round  95, Train loss: -5.663, Test loss: 0.618, Test accuracy: 77.72
Round  95: Global train loss: -5.663, Global test loss: 1.091, Global test accuracy: 39.95
Round  96, Train loss: -5.121, Test loss: 0.626, Test accuracy: 78.08
Round  96: Global train loss: -5.121, Global test loss: 1.093, Global test accuracy: 40.02
Round  97, Train loss: -4.801, Test loss: 0.615, Test accuracy: 78.69
Round  97: Global train loss: -4.801, Global test loss: 1.091, Global test accuracy: 39.98
Round  98, Train loss: -4.720, Test loss: 0.614, Test accuracy: 78.30
Round  98: Global train loss: -4.720, Global test loss: 1.089, Global test accuracy: 39.90
Round  99, Train loss: -5.252, Test loss: 0.625, Test accuracy: 78.11
Round  99: Global train loss: -5.252, Global test loss: 1.088, Global test accuracy: 39.76
Final Round: Train loss: 0.530, Test loss: 0.536, Test accuracy: 78.06
Final Round: Global train loss: 0.530, Global test loss: 1.090, Global test accuracy: 39.87
Average accuracy final 10 rounds: 78.1575
Average global accuracy final 10 rounds: 39.84
1718.725626707077
[]
[42.65, 46.016666666666666, 50.86666666666667, 55.775, 57.69166666666667, 59.416666666666664, 61.675, 62.31666666666667, 62.11666666666667, 64.41666666666667, 63.94166666666667, 65.83333333333333, 69.00833333333334, 70.38333333333334, 69.90833333333333, 70.44166666666666, 70.5, 70.225, 70.29166666666667, 70.35833333333333, 70.925, 71.80833333333334, 72.63333333333334, 72.88333333333334, 72.48333333333333, 74.6, 75.04166666666667, 75.31666666666666, 75.09166666666667, 75.23333333333333, 75.575, 76.675, 75.9, 75.95833333333333, 75.0, 75.1, 75.76666666666667, 76.18333333333334, 76.03333333333333, 76.925, 77.50833333333334, 77.35833333333333, 77.01666666666667, 77.08333333333333, 77.19166666666666, 76.93333333333334, 76.70833333333333, 77.41666666666667, 77.49166666666666, 77.21666666666667, 77.89166666666667, 78.11666666666666, 78.675, 78.25, 77.86666666666666, 77.29166666666667, 77.53333333333333, 77.025, 77.575, 76.85, 76.89166666666667, 77.55, 77.85833333333333, 77.44166666666666, 77.19166666666666, 77.25833333333334, 76.55833333333334, 77.81666666666666, 77.175, 78.19166666666666, 78.4, 78.30833333333334, 77.60833333333333, 77.98333333333333, 77.11666666666666, 77.10833333333333, 77.53333333333333, 77.50833333333334, 76.98333333333333, 76.85833333333333, 76.56666666666666, 77.5, 77.4, 78.34166666666667, 78.79166666666667, 78.64166666666667, 78.275, 77.575, 78.4, 78.75833333333334, 78.25833333333334, 78.80833333333334, 78.03333333333333, 77.74166666666666, 77.825, 77.725, 78.08333333333333, 78.69166666666666, 78.3, 78.10833333333333, 78.05833333333334]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

python: can't open file 'main_fedpac_k.py': [Errno 2] No such file or directory
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307384
307387
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.299, Test accuracy: 16.44 

Round   0, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 16.18 

Round   1, Train loss: 2.296, Test loss: 2.294, Test accuracy: 16.96 

Round   1, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 16.50 

Round   2, Train loss: 2.286, Test loss: 2.281, Test accuracy: 16.57 

Round   2, Global train loss: 2.286, Global test loss: 2.275, Global test accuracy: 13.84 

Round   3, Train loss: 2.258, Test loss: 2.259, Test accuracy: 19.82 

Round   3, Global train loss: 2.258, Global test loss: 2.240, Global test accuracy: 22.30 

Round   4, Train loss: 2.206, Test loss: 2.223, Test accuracy: 28.05 

Round   4, Global train loss: 2.206, Global test loss: 2.183, Global test accuracy: 37.28 

Round   5, Train loss: 2.260, Test loss: 2.232, Test accuracy: 27.75 

Round   5, Global train loss: 2.260, Global test loss: 2.258, Global test accuracy: 33.15 

Round   6, Train loss: 2.137, Test loss: 2.177, Test accuracy: 33.58 

Round   6, Global train loss: 2.137, Global test loss: 2.146, Global test accuracy: 39.84 

Round   7, Train loss: 2.144, Test loss: 2.124, Test accuracy: 41.48 

Round   7, Global train loss: 2.144, Global test loss: 2.099, Global test accuracy: 54.68 

Round   8, Train loss: 1.984, Test loss: 2.075, Test accuracy: 44.19 

Round   8, Global train loss: 1.984, Global test loss: 1.988, Global test accuracy: 60.05 

Round   9, Train loss: 1.964, Test loss: 2.032, Test accuracy: 47.01 

Round   9, Global train loss: 1.964, Global test loss: 1.911, Global test accuracy: 66.80 

Round  10, Train loss: 2.070, Test loss: 2.003, Test accuracy: 48.77 

Round  10, Global train loss: 2.070, Global test loss: 2.088, Global test accuracy: 55.77 

Round  11, Train loss: 1.888, Test loss: 1.974, Test accuracy: 52.00 

Round  11, Global train loss: 1.888, Global test loss: 1.877, Global test accuracy: 65.92 

Round  12, Train loss: 1.977, Test loss: 1.940, Test accuracy: 56.35 

Round  12, Global train loss: 1.977, Global test loss: 1.984, Global test accuracy: 57.78 

Round  13, Train loss: 1.830, Test loss: 1.920, Test accuracy: 57.94 

Round  13, Global train loss: 1.830, Global test loss: 1.858, Global test accuracy: 62.62 

Round  14, Train loss: 1.756, Test loss: 1.901, Test accuracy: 59.93 

Round  14, Global train loss: 1.756, Global test loss: 1.821, Global test accuracy: 67.50 

Round  15, Train loss: 1.760, Test loss: 1.876, Test accuracy: 62.31 

Round  15, Global train loss: 1.760, Global test loss: 1.777, Global test accuracy: 71.73 

Round  16, Train loss: 1.907, Test loss: 1.844, Test accuracy: 65.97 

Round  16, Global train loss: 1.907, Global test loss: 1.890, Global test accuracy: 66.67 

Round  17, Train loss: 1.778, Test loss: 1.821, Test accuracy: 68.03 

Round  17, Global train loss: 1.778, Global test loss: 1.814, Global test accuracy: 65.85 

Round  18, Train loss: 1.703, Test loss: 1.810, Test accuracy: 68.89 

Round  18, Global train loss: 1.703, Global test loss: 1.818, Global test accuracy: 64.95 

Round  19, Train loss: 1.778, Test loss: 1.788, Test accuracy: 70.79 

Round  19, Global train loss: 1.778, Global test loss: 1.834, Global test accuracy: 66.24 

Round  20, Train loss: 1.668, Test loss: 1.778, Test accuracy: 71.72 

Round  20, Global train loss: 1.668, Global test loss: 1.784, Global test accuracy: 67.91 

Round  21, Train loss: 1.695, Test loss: 1.765, Test accuracy: 73.14 

Round  21, Global train loss: 1.695, Global test loss: 1.783, Global test accuracy: 70.23 

Round  22, Train loss: 1.675, Test loss: 1.758, Test accuracy: 73.35 

Round  22, Global train loss: 1.675, Global test loss: 1.788, Global test accuracy: 68.01 

Round  23, Train loss: 1.631, Test loss: 1.745, Test accuracy: 74.23 

Round  23, Global train loss: 1.631, Global test loss: 1.710, Global test accuracy: 77.24 

Round  24, Train loss: 1.639, Test loss: 1.740, Test accuracy: 74.61 

Round  24, Global train loss: 1.639, Global test loss: 1.772, Global test accuracy: 71.49 

Round  25, Train loss: 1.651, Test loss: 1.731, Test accuracy: 75.39 

Round  25, Global train loss: 1.651, Global test loss: 1.753, Global test accuracy: 72.72 

Round  26, Train loss: 1.611, Test loss: 1.728, Test accuracy: 75.47 

Round  26, Global train loss: 1.611, Global test loss: 1.729, Global test accuracy: 75.27 

Round  27, Train loss: 1.595, Test loss: 1.723, Test accuracy: 75.63 

Round  27, Global train loss: 1.595, Global test loss: 1.699, Global test accuracy: 78.11 

Round  28, Train loss: 1.609, Test loss: 1.719, Test accuracy: 76.06 

Round  28, Global train loss: 1.609, Global test loss: 1.701, Global test accuracy: 78.74 

Round  29, Train loss: 1.618, Test loss: 1.715, Test accuracy: 76.31 

Round  29, Global train loss: 1.618, Global test loss: 1.725, Global test accuracy: 76.36 

Round  30, Train loss: 1.592, Test loss: 1.712, Test accuracy: 76.54 

Round  30, Global train loss: 1.592, Global test loss: 1.710, Global test accuracy: 76.82 

Round  31, Train loss: 1.572, Test loss: 1.705, Test accuracy: 77.19 

Round  31, Global train loss: 1.572, Global test loss: 1.672, Global test accuracy: 80.72 

Round  32, Train loss: 1.595, Test loss: 1.703, Test accuracy: 77.42 

Round  32, Global train loss: 1.595, Global test loss: 1.705, Global test accuracy: 78.70 

Round  33, Train loss: 1.542, Test loss: 1.698, Test accuracy: 77.96 

Round  33, Global train loss: 1.542, Global test loss: 1.635, Global test accuracy: 84.82 

Round  34, Train loss: 1.573, Test loss: 1.696, Test accuracy: 78.07 

Round  34, Global train loss: 1.573, Global test loss: 1.692, Global test accuracy: 77.86 

Round  35, Train loss: 1.619, Test loss: 1.693, Test accuracy: 78.29 

Round  35, Global train loss: 1.619, Global test loss: 1.762, Global test accuracy: 70.31 

Round  36, Train loss: 1.576, Test loss: 1.692, Test accuracy: 78.30 

Round  36, Global train loss: 1.576, Global test loss: 1.741, Global test accuracy: 72.09 

Round  37, Train loss: 1.599, Test loss: 1.690, Test accuracy: 78.50 

Round  37, Global train loss: 1.599, Global test loss: 1.752, Global test accuracy: 71.26 

Round  38, Train loss: 1.574, Test loss: 1.687, Test accuracy: 78.78 

Round  38, Global train loss: 1.574, Global test loss: 1.693, Global test accuracy: 78.27 

Round  39, Train loss: 1.525, Test loss: 1.686, Test accuracy: 78.87 

Round  39, Global train loss: 1.525, Global test loss: 1.651, Global test accuracy: 82.30 

Round  40, Train loss: 1.545, Test loss: 1.685, Test accuracy: 78.91 

Round  40, Global train loss: 1.545, Global test loss: 1.676, Global test accuracy: 79.98 

Round  41, Train loss: 1.543, Test loss: 1.685, Test accuracy: 78.87 

Round  41, Global train loss: 1.543, Global test loss: 1.713, Global test accuracy: 74.32 

Round  42, Train loss: 1.533, Test loss: 1.682, Test accuracy: 79.03 

Round  42, Global train loss: 1.533, Global test loss: 1.659, Global test accuracy: 81.90 

Round  43, Train loss: 1.553, Test loss: 1.682, Test accuracy: 79.08 

Round  43, Global train loss: 1.553, Global test loss: 1.687, Global test accuracy: 77.84 

Round  44, Train loss: 1.493, Test loss: 1.680, Test accuracy: 79.18 

Round  44, Global train loss: 1.493, Global test loss: 1.601, Global test accuracy: 87.65 

Round  45, Train loss: 1.547, Test loss: 1.678, Test accuracy: 79.44 

Round  45, Global train loss: 1.547, Global test loss: 1.693, Global test accuracy: 77.24 

Round  46, Train loss: 1.529, Test loss: 1.677, Test accuracy: 79.50 

Round  46, Global train loss: 1.529, Global test loss: 1.663, Global test accuracy: 81.19 

Round  47, Train loss: 1.561, Test loss: 1.676, Test accuracy: 79.61 

Round  47, Global train loss: 1.561, Global test loss: 1.736, Global test accuracy: 72.80 

Round  48, Train loss: 1.568, Test loss: 1.675, Test accuracy: 79.66 

Round  48, Global train loss: 1.568, Global test loss: 1.709, Global test accuracy: 76.97 

Round  49, Train loss: 1.547, Test loss: 1.673, Test accuracy: 79.95 

Round  49, Global train loss: 1.547, Global test loss: 1.683, Global test accuracy: 78.45 

Round  50, Train loss: 1.501, Test loss: 1.672, Test accuracy: 79.89 

Round  50, Global train loss: 1.501, Global test loss: 1.626, Global test accuracy: 84.65 

Round  51, Train loss: 1.564, Test loss: 1.672, Test accuracy: 79.93 

Round  51, Global train loss: 1.564, Global test loss: 1.732, Global test accuracy: 73.94 

Round  52, Train loss: 1.540, Test loss: 1.671, Test accuracy: 79.97 

Round  52, Global train loss: 1.540, Global test loss: 1.685, Global test accuracy: 78.39 

Round  53, Train loss: 1.501, Test loss: 1.671, Test accuracy: 79.96 

Round  53, Global train loss: 1.501, Global test loss: 1.617, Global test accuracy: 85.44 

Round  54, Train loss: 1.527, Test loss: 1.671, Test accuracy: 79.94 

Round  54, Global train loss: 1.527, Global test loss: 1.673, Global test accuracy: 79.30 

Round  55, Train loss: 1.529, Test loss: 1.671, Test accuracy: 79.95 

Round  55, Global train loss: 1.529, Global test loss: 1.647, Global test accuracy: 82.33 

Round  56, Train loss: 1.516, Test loss: 1.670, Test accuracy: 80.01 

Round  56, Global train loss: 1.516, Global test loss: 1.639, Global test accuracy: 83.12 

Round  57, Train loss: 1.554, Test loss: 1.670, Test accuracy: 79.95 

Round  57, Global train loss: 1.554, Global test loss: 1.708, Global test accuracy: 76.01 

Round  58, Train loss: 1.546, Test loss: 1.668, Test accuracy: 80.26 

Round  58, Global train loss: 1.546, Global test loss: 1.706, Global test accuracy: 76.16 

Round  59, Train loss: 1.533, Test loss: 1.668, Test accuracy: 80.28 

Round  59, Global train loss: 1.533, Global test loss: 1.677, Global test accuracy: 79.06 

Round  60, Train loss: 1.525, Test loss: 1.667, Test accuracy: 80.28 

Round  60, Global train loss: 1.525, Global test loss: 1.666, Global test accuracy: 80.06 

Round  61, Train loss: 1.512, Test loss: 1.667, Test accuracy: 80.34 

Round  61, Global train loss: 1.512, Global test loss: 1.637, Global test accuracy: 83.53 

Round  62, Train loss: 1.493, Test loss: 1.666, Test accuracy: 80.35 

Round  62, Global train loss: 1.493, Global test loss: 1.615, Global test accuracy: 85.65 

Round  63, Train loss: 1.549, Test loss: 1.666, Test accuracy: 80.36 

Round  63, Global train loss: 1.549, Global test loss: 1.732, Global test accuracy: 73.28 

Round  64, Train loss: 1.498, Test loss: 1.666, Test accuracy: 80.38 

Round  64, Global train loss: 1.498, Global test loss: 1.617, Global test accuracy: 85.48 

Round  65, Train loss: 1.539, Test loss: 1.666, Test accuracy: 80.39 

Round  65, Global train loss: 1.539, Global test loss: 1.679, Global test accuracy: 79.10 

Round  66, Train loss: 1.535, Test loss: 1.666, Test accuracy: 80.35 

Round  66, Global train loss: 1.535, Global test loss: 1.692, Global test accuracy: 77.31 

Round  67, Train loss: 1.495, Test loss: 1.665, Test accuracy: 80.39 

Round  67, Global train loss: 1.495, Global test loss: 1.620, Global test accuracy: 85.54 

Round  68, Train loss: 1.534, Test loss: 1.665, Test accuracy: 80.39 

Round  68, Global train loss: 1.534, Global test loss: 1.699, Global test accuracy: 76.50 

Round  69, Train loss: 1.501, Test loss: 1.665, Test accuracy: 80.40 

Round  69, Global train loss: 1.501, Global test loss: 1.623, Global test accuracy: 84.83 

Round  70, Train loss: 1.510, Test loss: 1.665, Test accuracy: 80.39 

Round  70, Global train loss: 1.510, Global test loss: 1.638, Global test accuracy: 82.96 

Round  71, Train loss: 1.497, Test loss: 1.665, Test accuracy: 80.39 

Round  71, Global train loss: 1.497, Global test loss: 1.639, Global test accuracy: 82.99 

Round  72, Train loss: 1.559, Test loss: 1.664, Test accuracy: 80.39 

Round  72, Global train loss: 1.559, Global test loss: 1.727, Global test accuracy: 74.33 

Round  73, Train loss: 1.512, Test loss: 1.664, Test accuracy: 80.41 

Round  73, Global train loss: 1.512, Global test loss: 1.638, Global test accuracy: 83.19 

Round  74, Train loss: 1.548, Test loss: 1.664, Test accuracy: 80.42 

Round  74, Global train loss: 1.548, Global test loss: 1.724, Global test accuracy: 73.43 

Round  75, Train loss: 1.525, Test loss: 1.664, Test accuracy: 80.43 

Round  75, Global train loss: 1.525, Global test loss: 1.663, Global test accuracy: 80.91 

Round  76, Train loss: 1.492, Test loss: 1.664, Test accuracy: 80.42 

Round  76, Global train loss: 1.492, Global test loss: 1.622, Global test accuracy: 84.79 

Round  77, Train loss: 1.521, Test loss: 1.663, Test accuracy: 80.42 

Round  77, Global train loss: 1.521, Global test loss: 1.669, Global test accuracy: 80.11 

Round  78, Train loss: 1.576, Test loss: 1.663, Test accuracy: 80.41 

Round  78, Global train loss: 1.576, Global test loss: 1.747, Global test accuracy: 72.16 

Round  79, Train loss: 1.535, Test loss: 1.663, Test accuracy: 80.39 

Round  79, Global train loss: 1.535, Global test loss: 1.702, Global test accuracy: 76.27 

Round  80, Train loss: 1.495, Test loss: 1.663, Test accuracy: 80.41 

Round  80, Global train loss: 1.495, Global test loss: 1.617, Global test accuracy: 85.29 

Round  81, Train loss: 1.484, Test loss: 1.663, Test accuracy: 80.39 

Round  81, Global train loss: 1.484, Global test loss: 1.598, Global test accuracy: 87.40 

Round  82, Train loss: 1.535, Test loss: 1.663, Test accuracy: 80.39 

Round  82, Global train loss: 1.535, Global test loss: 1.707, Global test accuracy: 75.09 

Round  83, Train loss: 1.533, Test loss: 1.663, Test accuracy: 80.36 

Round  83, Global train loss: 1.533, Global test loss: 1.697, Global test accuracy: 76.75 

Round  84, Train loss: 1.520, Test loss: 1.663, Test accuracy: 80.36 

Round  84, Global train loss: 1.520, Global test loss: 1.672, Global test accuracy: 79.35 

Round  85, Train loss: 1.560, Test loss: 1.663, Test accuracy: 80.36 

Round  85, Global train loss: 1.560, Global test loss: 1.702, Global test accuracy: 76.64 

Round  86, Train loss: 1.494, Test loss: 1.663, Test accuracy: 80.37 

Round  86, Global train loss: 1.494, Global test loss: 1.614, Global test accuracy: 85.69 

Round  87, Train loss: 1.522, Test loss: 1.663, Test accuracy: 80.40 

Round  87, Global train loss: 1.522, Global test loss: 1.675, Global test accuracy: 79.03 

Round  88, Train loss: 1.523, Test loss: 1.663, Test accuracy: 80.42 

Round  88, Global train loss: 1.523, Global test loss: 1.655, Global test accuracy: 81.35 

Round  89, Train loss: 1.545, Test loss: 1.663, Test accuracy: 80.41 

Round  89, Global train loss: 1.545, Global test loss: 1.710, Global test accuracy: 75.38 

Round  90, Train loss: 1.521, Test loss: 1.663, Test accuracy: 80.38 

Round  90, Global train loss: 1.521, Global test loss: 1.673, Global test accuracy: 79.03 

Round  91, Train loss: 1.521, Test loss: 1.663, Test accuracy: 80.37 

Round  91, Global train loss: 1.521, Global test loss: 1.665, Global test accuracy: 80.25 

Round  92, Train loss: 1.519, Test loss: 1.663, Test accuracy: 80.34 

Round  92, Global train loss: 1.519, Global test loss: 1.669, Global test accuracy: 79.98 

Round  93, Train loss: 1.520, Test loss: 1.663, Test accuracy: 80.33 

Round  93, Global train loss: 1.520, Global test loss: 1.679, Global test accuracy: 78.36 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.522, Test loss: 1.663, Test accuracy: 80.35 

Round  94, Global train loss: 1.522, Global test loss: 1.663, Global test accuracy: 80.42 

Round  95, Train loss: 1.503, Test loss: 1.663, Test accuracy: 80.37 

Round  95, Global train loss: 1.503, Global test loss: 1.639, Global test accuracy: 83.05 

Round  96, Train loss: 1.517, Test loss: 1.663, Test accuracy: 80.37 

Round  96, Global train loss: 1.517, Global test loss: 1.670, Global test accuracy: 79.58 

Round  97, Train loss: 1.503, Test loss: 1.663, Test accuracy: 80.38 

Round  97, Global train loss: 1.503, Global test loss: 1.632, Global test accuracy: 83.60 

Round  98, Train loss: 1.535, Test loss: 1.663, Test accuracy: 80.38 

Round  98, Global train loss: 1.535, Global test loss: 1.693, Global test accuracy: 76.85 

Round  99, Train loss: 1.486, Test loss: 1.663, Test accuracy: 80.34 

Round  99, Global train loss: 1.486, Global test loss: 1.590, Global test accuracy: 88.06 

Final Round, Train loss: 1.523, Test loss: 1.662, Test accuracy: 80.35 

Final Round, Global train loss: 1.523, Global test loss: 1.590, Global test accuracy: 88.06 

Average accuracy final 10 rounds: 80.36175 

Average global accuracy final 10 rounds: 80.919 

2108.165360212326
[0.8355996608734131, 1.5845181941986084, 2.334195375442505, 3.087692975997925, 3.8513028621673584, 4.6018407344818115, 5.350720643997192, 6.101351737976074, 6.851322889328003, 7.603301763534546, 8.347714185714722, 9.089219093322754, 9.837637186050415, 10.5889413356781, 11.33078384399414, 12.080727815628052, 12.827370882034302, 13.574724197387695, 14.330241203308105, 15.07575511932373, 15.830928564071655, 16.584962844848633, 17.334152698516846, 18.083654165267944, 18.830393075942993, 19.58276081085205, 20.33340835571289, 21.084737539291382, 21.838294506072998, 22.58740997314453, 23.334511756896973, 24.088112592697144, 24.837799072265625, 25.58543372154236, 26.330329656600952, 27.078204870224, 27.829265594482422, 28.581746339797974, 29.328667640686035, 30.080888986587524, 30.83284020423889, 31.588260889053345, 32.35140037536621, 33.09996771812439, 33.851526498794556, 34.60350728034973, 35.3541202545166, 36.10665678977966, 36.857775926589966, 37.60702466964722, 38.35429906845093, 39.08951807022095, 39.87700629234314, 40.638922452926636, 41.442710638046265, 42.24369239807129, 43.04380798339844, 43.84220266342163, 44.649057149887085, 45.456358432769775, 46.25765538215637, 47.06411957740784, 47.86730194091797, 48.66986536979675, 49.47037124633789, 50.26891756057739, 51.07203793525696, 51.868653774261475, 52.67118430137634, 53.473209381103516, 54.27626943588257, 55.074970960617065, 55.873136043548584, 56.6790132522583, 57.48073959350586, 58.275704860687256, 59.07356023788452, 59.86854648590088, 60.67164587974548, 61.47897386550903, 62.27925395965576, 63.07581615447998, 63.88023591041565, 64.68689322471619, 65.48319816589355, 66.27619409561157, 67.08249616622925, 67.88472557067871, 68.68357872962952, 69.48618292808533, 70.28708100318909, 71.0872118473053, 71.8855562210083, 72.684077501297, 73.48223328590393, 74.28362941741943, 75.0856351852417, 75.88178610801697, 76.68109893798828, 77.48627781867981, 79.08658790588379]
[16.44, 16.9575, 16.57, 19.8175, 28.05, 27.7475, 33.5825, 41.475, 44.185, 47.005, 48.765, 51.9975, 56.3525, 57.9375, 59.93, 62.3075, 65.975, 68.0325, 68.8875, 70.7875, 71.72, 73.1425, 73.3525, 74.235, 74.615, 75.39, 75.4725, 75.63, 76.055, 76.305, 76.5425, 77.19, 77.42, 77.9625, 78.07, 78.2875, 78.295, 78.495, 78.785, 78.8725, 78.9125, 78.8725, 79.025, 79.08, 79.1825, 79.4425, 79.5025, 79.605, 79.655, 79.95, 79.89, 79.9325, 79.965, 79.96, 79.9425, 79.955, 80.0125, 79.9525, 80.2575, 80.28, 80.2825, 80.34, 80.3475, 80.3575, 80.3775, 80.385, 80.3525, 80.3875, 80.3925, 80.3975, 80.39, 80.385, 80.385, 80.4125, 80.4225, 80.43, 80.4175, 80.42, 80.4075, 80.3925, 80.4075, 80.39, 80.3875, 80.355, 80.365, 80.36, 80.37, 80.4, 80.415, 80.405, 80.38, 80.3675, 80.3425, 80.335, 80.3475, 80.3675, 80.3725, 80.3825, 80.3775, 80.345, 80.35]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.300, Test accuracy: 17.13 

Round   0, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 16.93 

Round   1, Train loss: 2.298, Test loss: 2.297, Test accuracy: 27.57 

Round   1, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 30.96 

Round   2, Train loss: 2.291, Test loss: 2.291, Test accuracy: 37.55 

Round   2, Global train loss: 2.291, Global test loss: 2.287, Global test accuracy: 48.12 

Round   3, Train loss: 2.274, Test loss: 2.271, Test accuracy: 38.08 

Round   3, Global train loss: 2.274, Global test loss: 2.255, Global test accuracy: 47.82 

Round   4, Train loss: 2.149, Test loss: 2.161, Test accuracy: 43.07 

Round   4, Global train loss: 2.149, Global test loss: 2.039, Global test accuracy: 48.06 

Round   5, Train loss: 1.962, Test loss: 2.058, Test accuracy: 48.75 

Round   5, Global train loss: 1.962, Global test loss: 1.922, Global test accuracy: 56.42 

Round   6, Train loss: 1.892, Test loss: 2.012, Test accuracy: 52.02 

Round   6, Global train loss: 1.892, Global test loss: 1.900, Global test accuracy: 57.02 

Round   7, Train loss: 1.881, Test loss: 2.002, Test accuracy: 52.32 

Round   7, Global train loss: 1.881, Global test loss: 1.892, Global test accuracy: 57.33 

Round   8, Train loss: 1.874, Test loss: 1.963, Test accuracy: 53.69 

Round   8, Global train loss: 1.874, Global test loss: 1.887, Global test accuracy: 57.41 

Round   9, Train loss: 1.851, Test loss: 1.915, Test accuracy: 56.05 

Round   9, Global train loss: 1.851, Global test loss: 1.860, Global test accuracy: 60.17 

Round  10, Train loss: 1.817, Test loss: 1.885, Test accuracy: 59.14 

Round  10, Global train loss: 1.817, Global test loss: 1.804, Global test accuracy: 67.45 

Round  11, Train loss: 1.743, Test loss: 1.843, Test accuracy: 63.74 

Round  11, Global train loss: 1.743, Global test loss: 1.711, Global test accuracy: 78.05 

Round  12, Train loss: 1.668, Test loss: 1.803, Test accuracy: 68.14 

Round  12, Global train loss: 1.668, Global test loss: 1.668, Global test accuracy: 81.39 

Round  13, Train loss: 1.625, Test loss: 1.752, Test accuracy: 73.67 

Round  13, Global train loss: 1.625, Global test loss: 1.628, Global test accuracy: 86.09 

Round  14, Train loss: 1.609, Test loss: 1.715, Test accuracy: 77.32 

Round  14, Global train loss: 1.609, Global test loss: 1.602, Global test accuracy: 87.97 

Round  15, Train loss: 1.577, Test loss: 1.699, Test accuracy: 78.67 

Round  15, Global train loss: 1.577, Global test loss: 1.588, Global test accuracy: 88.62 

Round  16, Train loss: 1.574, Test loss: 1.675, Test accuracy: 80.74 

Round  16, Global train loss: 1.574, Global test loss: 1.579, Global test accuracy: 89.53 

Round  17, Train loss: 1.553, Test loss: 1.668, Test accuracy: 81.26 

Round  17, Global train loss: 1.553, Global test loss: 1.574, Global test accuracy: 89.67 

Round  18, Train loss: 1.546, Test loss: 1.643, Test accuracy: 83.70 

Round  18, Global train loss: 1.546, Global test loss: 1.569, Global test accuracy: 90.14 

Round  19, Train loss: 1.542, Test loss: 1.635, Test accuracy: 84.32 

Round  19, Global train loss: 1.542, Global test loss: 1.564, Global test accuracy: 90.82 

Round  20, Train loss: 1.546, Test loss: 1.629, Test accuracy: 84.74 

Round  20, Global train loss: 1.546, Global test loss: 1.561, Global test accuracy: 90.83 

Round  21, Train loss: 1.544, Test loss: 1.614, Test accuracy: 86.08 

Round  21, Global train loss: 1.544, Global test loss: 1.560, Global test accuracy: 91.02 

Round  22, Train loss: 1.540, Test loss: 1.609, Test accuracy: 86.48 

Round  22, Global train loss: 1.540, Global test loss: 1.557, Global test accuracy: 91.03 

Round  23, Train loss: 1.532, Test loss: 1.569, Test accuracy: 90.03 

Round  23, Global train loss: 1.532, Global test loss: 1.555, Global test accuracy: 91.27 

Round  24, Train loss: 1.524, Test loss: 1.566, Test accuracy: 90.27 

Round  24, Global train loss: 1.524, Global test loss: 1.554, Global test accuracy: 91.52 

Round  25, Train loss: 1.527, Test loss: 1.564, Test accuracy: 90.35 

Round  25, Global train loss: 1.527, Global test loss: 1.552, Global test accuracy: 91.34 

Round  26, Train loss: 1.533, Test loss: 1.563, Test accuracy: 90.49 

Round  26, Global train loss: 1.533, Global test loss: 1.552, Global test accuracy: 91.54 

Round  27, Train loss: 1.525, Test loss: 1.561, Test accuracy: 90.58 

Round  27, Global train loss: 1.525, Global test loss: 1.550, Global test accuracy: 91.51 

Round  28, Train loss: 1.522, Test loss: 1.560, Test accuracy: 90.72 

Round  28, Global train loss: 1.522, Global test loss: 1.549, Global test accuracy: 91.62 

Round  29, Train loss: 1.518, Test loss: 1.559, Test accuracy: 90.83 

Round  29, Global train loss: 1.518, Global test loss: 1.548, Global test accuracy: 91.53 

Round  30, Train loss: 1.517, Test loss: 1.556, Test accuracy: 91.08 

Round  30, Global train loss: 1.517, Global test loss: 1.546, Global test accuracy: 91.78 

Round  31, Train loss: 1.513, Test loss: 1.555, Test accuracy: 91.11 

Round  31, Global train loss: 1.513, Global test loss: 1.546, Global test accuracy: 91.87 

Round  32, Train loss: 1.514, Test loss: 1.553, Test accuracy: 91.12 

Round  32, Global train loss: 1.514, Global test loss: 1.544, Global test accuracy: 92.03 

Round  33, Train loss: 1.515, Test loss: 1.551, Test accuracy: 91.38 

Round  33, Global train loss: 1.515, Global test loss: 1.544, Global test accuracy: 92.13 

Round  34, Train loss: 1.511, Test loss: 1.550, Test accuracy: 91.57 

Round  34, Global train loss: 1.511, Global test loss: 1.542, Global test accuracy: 92.21 

Round  35, Train loss: 1.508, Test loss: 1.548, Test accuracy: 91.70 

Round  35, Global train loss: 1.508, Global test loss: 1.541, Global test accuracy: 92.33 

Round  36, Train loss: 1.512, Test loss: 1.547, Test accuracy: 91.86 

Round  36, Global train loss: 1.512, Global test loss: 1.541, Global test accuracy: 92.38 

Round  37, Train loss: 1.507, Test loss: 1.546, Test accuracy: 91.91 

Round  37, Global train loss: 1.507, Global test loss: 1.540, Global test accuracy: 92.43 

Round  38, Train loss: 1.506, Test loss: 1.546, Test accuracy: 91.83 

Round  38, Global train loss: 1.506, Global test loss: 1.541, Global test accuracy: 92.54 

Round  39, Train loss: 1.506, Test loss: 1.546, Test accuracy: 91.91 

Round  39, Global train loss: 1.506, Global test loss: 1.539, Global test accuracy: 92.51 

Round  40, Train loss: 1.506, Test loss: 1.545, Test accuracy: 91.84 

Round  40, Global train loss: 1.506, Global test loss: 1.538, Global test accuracy: 92.53 

Round  41, Train loss: 1.506, Test loss: 1.545, Test accuracy: 91.87 

Round  41, Global train loss: 1.506, Global test loss: 1.537, Global test accuracy: 92.85 

Round  42, Train loss: 1.503, Test loss: 1.545, Test accuracy: 91.88 

Round  42, Global train loss: 1.503, Global test loss: 1.536, Global test accuracy: 92.95 

Round  43, Train loss: 1.501, Test loss: 1.544, Test accuracy: 92.03 

Round  43, Global train loss: 1.501, Global test loss: 1.536, Global test accuracy: 92.91 

Round  44, Train loss: 1.503, Test loss: 1.544, Test accuracy: 91.99 

Round  44, Global train loss: 1.503, Global test loss: 1.536, Global test accuracy: 92.65 

Round  45, Train loss: 1.497, Test loss: 1.543, Test accuracy: 92.13 

Round  45, Global train loss: 1.497, Global test loss: 1.534, Global test accuracy: 93.08 

Round  46, Train loss: 1.496, Test loss: 1.541, Test accuracy: 92.28 

Round  46, Global train loss: 1.496, Global test loss: 1.535, Global test accuracy: 92.91 

Round  47, Train loss: 1.503, Test loss: 1.540, Test accuracy: 92.51 

Round  47, Global train loss: 1.503, Global test loss: 1.533, Global test accuracy: 93.14 

Round  48, Train loss: 1.496, Test loss: 1.539, Test accuracy: 92.65 

Round  48, Global train loss: 1.496, Global test loss: 1.532, Global test accuracy: 93.33 

Round  49, Train loss: 1.496, Test loss: 1.539, Test accuracy: 92.58 

Round  49, Global train loss: 1.496, Global test loss: 1.532, Global test accuracy: 93.25 

Round  50, Train loss: 1.500, Test loss: 1.538, Test accuracy: 92.65 

Round  50, Global train loss: 1.500, Global test loss: 1.531, Global test accuracy: 93.31 

Round  51, Train loss: 1.499, Test loss: 1.537, Test accuracy: 92.70 

Round  51, Global train loss: 1.499, Global test loss: 1.529, Global test accuracy: 93.46 

Round  52, Train loss: 1.502, Test loss: 1.536, Test accuracy: 92.73 

Round  52, Global train loss: 1.502, Global test loss: 1.530, Global test accuracy: 93.39 

Round  53, Train loss: 1.498, Test loss: 1.536, Test accuracy: 92.78 

Round  53, Global train loss: 1.498, Global test loss: 1.529, Global test accuracy: 93.38 

Round  54, Train loss: 1.493, Test loss: 1.535, Test accuracy: 92.89 

Round  54, Global train loss: 1.493, Global test loss: 1.528, Global test accuracy: 93.66 

Round  55, Train loss: 1.495, Test loss: 1.535, Test accuracy: 92.97 

Round  55, Global train loss: 1.495, Global test loss: 1.529, Global test accuracy: 93.49 

Round  56, Train loss: 1.494, Test loss: 1.534, Test accuracy: 93.06 

Round  56, Global train loss: 1.494, Global test loss: 1.528, Global test accuracy: 93.66 

Round  57, Train loss: 1.494, Test loss: 1.534, Test accuracy: 93.17 

Round  57, Global train loss: 1.494, Global test loss: 1.527, Global test accuracy: 93.80 

Round  58, Train loss: 1.494, Test loss: 1.533, Test accuracy: 93.25 

Round  58, Global train loss: 1.494, Global test loss: 1.526, Global test accuracy: 93.70 

Round  59, Train loss: 1.495, Test loss: 1.533, Test accuracy: 93.21 

Round  59, Global train loss: 1.495, Global test loss: 1.527, Global test accuracy: 93.69 

Round  60, Train loss: 1.494, Test loss: 1.532, Test accuracy: 93.20 

Round  60, Global train loss: 1.494, Global test loss: 1.527, Global test accuracy: 93.69 

Round  61, Train loss: 1.496, Test loss: 1.531, Test accuracy: 93.28 

Round  61, Global train loss: 1.496, Global test loss: 1.527, Global test accuracy: 93.81 

Round  62, Train loss: 1.491, Test loss: 1.532, Test accuracy: 93.18 

Round  62, Global train loss: 1.491, Global test loss: 1.526, Global test accuracy: 93.87 

Round  63, Train loss: 1.493, Test loss: 1.532, Test accuracy: 93.22 

Round  63, Global train loss: 1.493, Global test loss: 1.526, Global test accuracy: 93.69 

Round  64, Train loss: 1.489, Test loss: 1.531, Test accuracy: 93.28 

Round  64, Global train loss: 1.489, Global test loss: 1.527, Global test accuracy: 93.64 

Round  65, Train loss: 1.493, Test loss: 1.531, Test accuracy: 93.31 

Round  65, Global train loss: 1.493, Global test loss: 1.526, Global test accuracy: 93.92 

Round  66, Train loss: 1.492, Test loss: 1.531, Test accuracy: 93.33 

Round  66, Global train loss: 1.492, Global test loss: 1.525, Global test accuracy: 93.95 

Round  67, Train loss: 1.491, Test loss: 1.531, Test accuracy: 93.38 

Round  67, Global train loss: 1.491, Global test loss: 1.526, Global test accuracy: 93.78 

Round  68, Train loss: 1.488, Test loss: 1.530, Test accuracy: 93.47 

Round  68, Global train loss: 1.488, Global test loss: 1.525, Global test accuracy: 93.89 

Round  69, Train loss: 1.490, Test loss: 1.530, Test accuracy: 93.52 

Round  69, Global train loss: 1.490, Global test loss: 1.525, Global test accuracy: 93.79 

Round  70, Train loss: 1.491, Test loss: 1.529, Test accuracy: 93.60 

Round  70, Global train loss: 1.491, Global test loss: 1.524, Global test accuracy: 94.08 

Round  71, Train loss: 1.489, Test loss: 1.529, Test accuracy: 93.62 

Round  71, Global train loss: 1.489, Global test loss: 1.523, Global test accuracy: 94.28 

Round  72, Train loss: 1.484, Test loss: 1.528, Test accuracy: 93.67 

Round  72, Global train loss: 1.484, Global test loss: 1.523, Global test accuracy: 94.20 

Round  73, Train loss: 1.489, Test loss: 1.528, Test accuracy: 93.72 

Round  73, Global train loss: 1.489, Global test loss: 1.523, Global test accuracy: 94.03 

Round  74, Train loss: 1.491, Test loss: 1.528, Test accuracy: 93.71 

Round  74, Global train loss: 1.491, Global test loss: 1.523, Global test accuracy: 94.12 

Round  75, Train loss: 1.489, Test loss: 1.527, Test accuracy: 93.68 

Round  75, Global train loss: 1.489, Global test loss: 1.523, Global test accuracy: 94.00 

Round  76, Train loss: 1.487, Test loss: 1.527, Test accuracy: 93.75 

Round  76, Global train loss: 1.487, Global test loss: 1.522, Global test accuracy: 94.13 

Round  77, Train loss: 1.483, Test loss: 1.526, Test accuracy: 93.83 

Round  77, Global train loss: 1.483, Global test loss: 1.522, Global test accuracy: 94.16 

Round  78, Train loss: 1.485, Test loss: 1.527, Test accuracy: 93.73 

Round  78, Global train loss: 1.485, Global test loss: 1.523, Global test accuracy: 94.17 

Round  79, Train loss: 1.483, Test loss: 1.526, Test accuracy: 93.73 

Round  79, Global train loss: 1.483, Global test loss: 1.522, Global test accuracy: 94.12 

Round  80, Train loss: 1.489, Test loss: 1.526, Test accuracy: 93.72 

Round  80, Global train loss: 1.489, Global test loss: 1.522, Global test accuracy: 94.18 

Round  81, Train loss: 1.484, Test loss: 1.526, Test accuracy: 93.72 

Round  81, Global train loss: 1.484, Global test loss: 1.522, Global test accuracy: 94.22 

Round  82, Train loss: 1.483, Test loss: 1.526, Test accuracy: 93.72 

Round  82, Global train loss: 1.483, Global test loss: 1.521, Global test accuracy: 94.28 

Round  83, Train loss: 1.481, Test loss: 1.526, Test accuracy: 93.73 

Round  83, Global train loss: 1.481, Global test loss: 1.521, Global test accuracy: 94.34 

Round  84, Train loss: 1.484, Test loss: 1.526, Test accuracy: 93.74 

Round  84, Global train loss: 1.484, Global test loss: 1.521, Global test accuracy: 94.22 

Round  85, Train loss: 1.489, Test loss: 1.525, Test accuracy: 93.74 

Round  85, Global train loss: 1.489, Global test loss: 1.521, Global test accuracy: 94.32 

Round  86, Train loss: 1.484, Test loss: 1.525, Test accuracy: 93.75 

Round  86, Global train loss: 1.484, Global test loss: 1.521, Global test accuracy: 94.27 

Round  87, Train loss: 1.482, Test loss: 1.525, Test accuracy: 93.77 

Round  87, Global train loss: 1.482, Global test loss: 1.521, Global test accuracy: 94.21 

Round  88, Train loss: 1.482, Test loss: 1.525, Test accuracy: 93.73 

Round  88, Global train loss: 1.482, Global test loss: 1.520, Global test accuracy: 94.35 

Round  89, Train loss: 1.482, Test loss: 1.525, Test accuracy: 93.76 

Round  89, Global train loss: 1.482, Global test loss: 1.521, Global test accuracy: 94.27 

Round  90, Train loss: 1.483, Test loss: 1.525, Test accuracy: 93.77 

Round  90, Global train loss: 1.483, Global test loss: 1.520, Global test accuracy: 94.45 

Round  91, Train loss: 1.483, Test loss: 1.524, Test accuracy: 93.84 

Round  91, Global train loss: 1.483, Global test loss: 1.520, Global test accuracy: 94.51 

Round  92, Train loss: 1.487, Test loss: 1.524, Test accuracy: 93.90 

Round  92, Global train loss: 1.487, Global test loss: 1.520, Global test accuracy: 94.38 

Round  93, Train loss: 1.483, Test loss: 1.524, Test accuracy: 93.92 

Round  93, Global train loss: 1.483, Global test loss: 1.521, Global test accuracy: 94.34 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.480, Test loss: 1.523, Test accuracy: 93.97 

Round  94, Global train loss: 1.480, Global test loss: 1.520, Global test accuracy: 94.42 

Round  95, Train loss: 1.485, Test loss: 1.523, Test accuracy: 93.97 

Round  95, Global train loss: 1.485, Global test loss: 1.521, Global test accuracy: 94.30 

Round  96, Train loss: 1.480, Test loss: 1.523, Test accuracy: 93.94 

Round  96, Global train loss: 1.480, Global test loss: 1.519, Global test accuracy: 94.58 

Round  97, Train loss: 1.484, Test loss: 1.523, Test accuracy: 93.96 

Round  97, Global train loss: 1.484, Global test loss: 1.520, Global test accuracy: 94.40 

Round  98, Train loss: 1.490, Test loss: 1.524, Test accuracy: 93.88 

Round  98, Global train loss: 1.490, Global test loss: 1.520, Global test accuracy: 94.42 

Round  99, Train loss: 1.482, Test loss: 1.524, Test accuracy: 93.92 

Round  99, Global train loss: 1.482, Global test loss: 1.519, Global test accuracy: 94.66 

Final Round, Train loss: 1.482, Test loss: 1.523, Test accuracy: 94.08 

Final Round, Global train loss: 1.482, Global test loss: 1.519, Global test accuracy: 94.66 

Average accuracy final 10 rounds: 93.90666666666667 

Average global accuracy final 10 rounds: 94.445 

902.440863609314
[0.8706974983215332, 1.6337158679962158, 2.3895342350006104, 3.1409213542938232, 3.898002862930298, 4.653726816177368, 5.4068522453308105, 6.155097246170044, 6.800069093704224, 7.450962781906128, 8.09106731414795, 8.736228227615356, 9.383744716644287, 10.01888370513916, 10.664761066436768, 11.312366485595703, 11.948491334915161, 12.59598708152771, 13.242346286773682, 13.886168241500854, 14.534566640853882, 15.184273481369019, 15.824649333953857, 16.470887899398804, 17.124910354614258, 17.762377500534058, 18.414175033569336, 19.068559169769287, 19.70612120628357, 20.358710289001465, 21.010528087615967, 21.648688077926636, 22.302201509475708, 22.952276706695557, 23.597309112548828, 24.250380992889404, 24.903517484664917, 25.549514293670654, 26.20813822746277, 26.858492374420166, 27.50499653816223, 28.152613878250122, 28.807862997055054, 29.450304746627808, 30.093129634857178, 30.743393182754517, 31.396703720092773, 32.03622913360596, 32.691269397735596, 33.34561562538147, 33.98407459259033, 34.63578128814697, 35.28372931480408, 35.92406439781189, 36.57233762741089, 37.22589564323425, 37.86478304862976, 38.515586614608765, 39.16550254821777, 39.81177520751953, 40.45845818519592, 41.1038293838501, 41.74457049369812, 42.387794733047485, 43.0533971786499, 43.698869466781616, 44.33725166320801, 44.99091935157776, 45.64238905906677, 46.279078006744385, 46.93131375312805, 47.58245015144348, 48.23124885559082, 48.88110280036926, 49.536768436431885, 50.182437896728516, 50.828099727630615, 51.474461793899536, 52.11971831321716, 52.76824641227722, 53.41747188568115, 54.069409132003784, 54.71498417854309, 55.36205291748047, 56.019296407699585, 56.666020154953, 57.31776285171509, 57.97271251678467, 58.61862063407898, 59.26128530502319, 59.91285014152527, 60.56709384918213, 61.20573902130127, 61.85770225524902, 62.501851081848145, 63.14770174026489, 63.797605991363525, 64.44561624526978, 65.10168695449829, 65.75303959846497, 67.05821442604065]
[17.133333333333333, 27.566666666666666, 37.55, 38.083333333333336, 43.06666666666667, 48.75, 52.016666666666666, 52.31666666666667, 53.69166666666667, 56.05, 59.141666666666666, 63.74166666666667, 68.14166666666667, 73.66666666666667, 77.31666666666666, 78.66666666666667, 80.74166666666666, 81.25833333333334, 83.7, 84.31666666666666, 84.74166666666666, 86.075, 86.48333333333333, 90.03333333333333, 90.26666666666667, 90.35, 90.49166666666666, 90.575, 90.725, 90.825, 91.075, 91.10833333333333, 91.11666666666666, 91.38333333333334, 91.56666666666666, 91.7, 91.85833333333333, 91.90833333333333, 91.825, 91.90833333333333, 91.84166666666667, 91.86666666666666, 91.875, 92.025, 91.99166666666666, 92.13333333333334, 92.275, 92.50833333333334, 92.65, 92.575, 92.65, 92.7, 92.73333333333333, 92.78333333333333, 92.89166666666667, 92.975, 93.05833333333334, 93.16666666666667, 93.25, 93.20833333333333, 93.2, 93.28333333333333, 93.18333333333334, 93.21666666666667, 93.28333333333333, 93.30833333333334, 93.325, 93.375, 93.46666666666667, 93.51666666666667, 93.6, 93.61666666666666, 93.66666666666667, 93.71666666666667, 93.70833333333333, 93.68333333333334, 93.75, 93.83333333333333, 93.73333333333333, 93.73333333333333, 93.71666666666667, 93.725, 93.71666666666667, 93.73333333333333, 93.74166666666666, 93.74166666666666, 93.75, 93.76666666666667, 93.73333333333333, 93.75833333333334, 93.76666666666667, 93.84166666666667, 93.9, 93.91666666666667, 93.975, 93.96666666666667, 93.94166666666666, 93.95833333333333, 93.88333333333334, 93.91666666666667, 94.08333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.300, Test accuracy: 16.64 

Round   1, Train loss: 2.300, Test loss: 2.299, Test accuracy: 26.22 

Round   2, Train loss: 2.298, Test loss: 2.296, Test accuracy: 35.90 

Round   3, Train loss: 2.295, Test loss: 2.293, Test accuracy: 40.47 

Round   4, Train loss: 2.291, Test loss: 2.287, Test accuracy: 45.62 

Round   5, Train loss: 2.284, Test loss: 2.278, Test accuracy: 45.69 

Round   6, Train loss: 2.274, Test loss: 2.261, Test accuracy: 44.92 

Round   7, Train loss: 2.239, Test loss: 2.204, Test accuracy: 40.42 

Round   8, Train loss: 2.155, Test loss: 2.099, Test accuracy: 46.72 

Round   9, Train loss: 2.039, Test loss: 1.998, Test accuracy: 59.47 

Round  10, Train loss: 1.924, Test loss: 1.914, Test accuracy: 62.33 

Round  11, Train loss: 1.863, Test loss: 1.854, Test accuracy: 66.15 

Round  12, Train loss: 1.804, Test loss: 1.811, Test accuracy: 69.13 

Round  13, Train loss: 1.751, Test loss: 1.767, Test accuracy: 73.18 

Round  14, Train loss: 1.729, Test loss: 1.736, Test accuracy: 75.72 

Round  15, Train loss: 1.689, Test loss: 1.712, Test accuracy: 77.74 

Round  16, Train loss: 1.685, Test loss: 1.690, Test accuracy: 79.79 

Round  17, Train loss: 1.665, Test loss: 1.682, Test accuracy: 80.21 

Round  18, Train loss: 1.656, Test loss: 1.672, Test accuracy: 80.90 

Round  19, Train loss: 1.644, Test loss: 1.668, Test accuracy: 81.19 

Round  20, Train loss: 1.652, Test loss: 1.665, Test accuracy: 81.28 

Round  21, Train loss: 1.654, Test loss: 1.656, Test accuracy: 82.15 

Round  22, Train loss: 1.645, Test loss: 1.652, Test accuracy: 82.37 

Round  23, Train loss: 1.640, Test loss: 1.650, Test accuracy: 82.49 

Round  24, Train loss: 1.636, Test loss: 1.644, Test accuracy: 83.06 

Round  25, Train loss: 1.623, Test loss: 1.642, Test accuracy: 83.01 

Round  26, Train loss: 1.632, Test loss: 1.639, Test accuracy: 83.35 

Round  27, Train loss: 1.616, Test loss: 1.636, Test accuracy: 83.88 

Round  28, Train loss: 1.605, Test loss: 1.628, Test accuracy: 84.62 

Round  29, Train loss: 1.595, Test loss: 1.621, Test accuracy: 85.42 

Round  30, Train loss: 1.592, Test loss: 1.616, Test accuracy: 85.99 

Round  31, Train loss: 1.578, Test loss: 1.613, Test accuracy: 86.26 

Round  32, Train loss: 1.576, Test loss: 1.611, Test accuracy: 86.25 

Round  33, Train loss: 1.566, Test loss: 1.599, Test accuracy: 87.64 

Round  34, Train loss: 1.564, Test loss: 1.590, Test accuracy: 88.57 

Round  35, Train loss: 1.563, Test loss: 1.587, Test accuracy: 88.66 

Round  36, Train loss: 1.553, Test loss: 1.581, Test accuracy: 89.17 

Round  37, Train loss: 1.548, Test loss: 1.580, Test accuracy: 89.28 

Round  38, Train loss: 1.554, Test loss: 1.579, Test accuracy: 89.31 

Round  39, Train loss: 1.544, Test loss: 1.581, Test accuracy: 88.99 

Round  40, Train loss: 1.537, Test loss: 1.578, Test accuracy: 89.24 

Round  41, Train loss: 1.533, Test loss: 1.577, Test accuracy: 89.31 

Round  42, Train loss: 1.532, Test loss: 1.574, Test accuracy: 89.63 

Round  43, Train loss: 1.541, Test loss: 1.573, Test accuracy: 89.58 

Round  44, Train loss: 1.538, Test loss: 1.575, Test accuracy: 89.50 

Round  45, Train loss: 1.533, Test loss: 1.574, Test accuracy: 89.47 

Round  46, Train loss: 1.532, Test loss: 1.573, Test accuracy: 89.62 

Round  47, Train loss: 1.525, Test loss: 1.570, Test accuracy: 89.88 

Round  48, Train loss: 1.525, Test loss: 1.568, Test accuracy: 90.01 

Round  49, Train loss: 1.531, Test loss: 1.568, Test accuracy: 89.94 

Round  50, Train loss: 1.528, Test loss: 1.568, Test accuracy: 89.93 

Round  51, Train loss: 1.523, Test loss: 1.566, Test accuracy: 90.12 

Round  52, Train loss: 1.527, Test loss: 1.565, Test accuracy: 90.18 

Round  53, Train loss: 1.522, Test loss: 1.565, Test accuracy: 90.33 

Round  54, Train loss: 1.523, Test loss: 1.563, Test accuracy: 90.44 

Round  55, Train loss: 1.516, Test loss: 1.562, Test accuracy: 90.50 

Round  56, Train loss: 1.520, Test loss: 1.563, Test accuracy: 90.42 

Round  57, Train loss: 1.519, Test loss: 1.562, Test accuracy: 90.49 

Round  58, Train loss: 1.517, Test loss: 1.562, Test accuracy: 90.46 

Round  59, Train loss: 1.518, Test loss: 1.561, Test accuracy: 90.49 

Round  60, Train loss: 1.517, Test loss: 1.561, Test accuracy: 90.46 

Round  61, Train loss: 1.513, Test loss: 1.560, Test accuracy: 90.62 

Round  62, Train loss: 1.512, Test loss: 1.561, Test accuracy: 90.47 

Round  63, Train loss: 1.505, Test loss: 1.559, Test accuracy: 90.65 

Round  64, Train loss: 1.514, Test loss: 1.559, Test accuracy: 90.59 

Round  65, Train loss: 1.512, Test loss: 1.559, Test accuracy: 90.56 

Round  66, Train loss: 1.509, Test loss: 1.558, Test accuracy: 90.67 

Round  67, Train loss: 1.512, Test loss: 1.558, Test accuracy: 90.69 

Round  68, Train loss: 1.503, Test loss: 1.557, Test accuracy: 90.82 

Round  69, Train loss: 1.507, Test loss: 1.557, Test accuracy: 90.75 

Round  70, Train loss: 1.512, Test loss: 1.558, Test accuracy: 90.68 

Round  71, Train loss: 1.508, Test loss: 1.557, Test accuracy: 90.67 

Round  72, Train loss: 1.510, Test loss: 1.557, Test accuracy: 90.61 

Round  73, Train loss: 1.510, Test loss: 1.557, Test accuracy: 90.72 

Round  74, Train loss: 1.506, Test loss: 1.556, Test accuracy: 90.84 

Round  75, Train loss: 1.506, Test loss: 1.555, Test accuracy: 90.88 

Round  76, Train loss: 1.509, Test loss: 1.556, Test accuracy: 90.86 

Round  77, Train loss: 1.507, Test loss: 1.556, Test accuracy: 90.91 

Round  78, Train loss: 1.508, Test loss: 1.556, Test accuracy: 90.83 

Round  79, Train loss: 1.505, Test loss: 1.555, Test accuracy: 90.92 

Round  80, Train loss: 1.507, Test loss: 1.556, Test accuracy: 90.65 

Round  81, Train loss: 1.500, Test loss: 1.556, Test accuracy: 90.79 

Round  82, Train loss: 1.505, Test loss: 1.556, Test accuracy: 90.75 

Round  83, Train loss: 1.502, Test loss: 1.556, Test accuracy: 90.90 

Round  84, Train loss: 1.508, Test loss: 1.555, Test accuracy: 90.87 

Round  85, Train loss: 1.504, Test loss: 1.554, Test accuracy: 90.96 

Round  86, Train loss: 1.500, Test loss: 1.554, Test accuracy: 90.88 

Round  87, Train loss: 1.503, Test loss: 1.554, Test accuracy: 91.07 

Round  88, Train loss: 1.506, Test loss: 1.554, Test accuracy: 90.97 

Round  89, Train loss: 1.503, Test loss: 1.555, Test accuracy: 90.92 

Round  90, Train loss: 1.502, Test loss: 1.555, Test accuracy: 90.89 

Round  91, Train loss: 1.502, Test loss: 1.555, Test accuracy: 90.92 

Round  92, Train loss: 1.497, Test loss: 1.554, Test accuracy: 91.04 

Round  93, Train loss: 1.502, Test loss: 1.553, Test accuracy: 90.97 

Round  94, Train loss: 1.500, Test loss: 1.553, Test accuracy: 91.08 

Round  95, Train loss: 1.500, Test loss: 1.553, Test accuracy: 91.16 

Round  96, Train loss: 1.498, Test loss: 1.553, Test accuracy: 90.96 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 1.500, Test loss: 1.553, Test accuracy: 90.95 

Round  98, Train loss: 1.502, Test loss: 1.553, Test accuracy: 91.03 

Round  99, Train loss: 1.500, Test loss: 1.553, Test accuracy: 91.08 

Final Round, Train loss: 1.500, Test loss: 1.554, Test accuracy: 90.91 

Average accuracy final 10 rounds: 91.00666666666667 

671.8655269145966
[0.8284201622009277, 1.5808742046356201, 2.2562928199768066, 2.9397847652435303, 3.6191065311431885, 4.302206039428711, 4.987476348876953, 5.671797752380371, 6.351391315460205, 7.03665566444397, 7.717416763305664, 8.39799427986145, 9.077645301818848, 9.759738206863403, 10.44271731376648, 11.12128758430481, 11.803993463516235, 12.48388934135437, 13.165602445602417, 13.846582412719727, 14.524216413497925, 15.206094741821289, 15.884248495101929, 16.567954301834106, 17.24797010421753, 17.92730212211609, 18.606048107147217, 19.285614728927612, 19.967992544174194, 20.647236108779907, 21.333252906799316, 22.010868549346924, 22.69489288330078, 23.36824917793274, 24.050581455230713, 24.727408170700073, 25.40682101249695, 26.086579084396362, 26.765724182128906, 27.444292783737183, 28.120254278182983, 28.803292989730835, 29.476354360580444, 30.162471771240234, 30.84123706817627, 31.52344560623169, 32.20387816429138, 32.89117169380188, 33.57138419151306, 34.24808740615845, 34.93130302429199, 35.602009534835815, 36.28995728492737, 36.965280532836914, 37.65488910675049, 38.33257532119751, 39.01177763938904, 39.6927330493927, 40.37038969993591, 41.054131269454956, 41.73199510574341, 42.42032861709595, 43.09664511680603, 43.774433612823486, 44.45280051231384, 45.1314058303833, 45.81408381462097, 46.502036333084106, 47.18502712249756, 47.86600708961487, 48.551276206970215, 49.22857451438904, 49.90937685966492, 50.586830377578735, 51.27281308174133, 51.95215702056885, 52.638993978500366, 53.321682929992676, 54.00330424308777, 54.68397092819214, 55.36530613899231, 56.0462327003479, 56.72940945625305, 57.40892052650452, 58.09569525718689, 58.78016400337219, 59.458574295043945, 60.13758063316345, 60.804977893829346, 61.4850697517395, 62.156519651412964, 62.82826900482178, 63.500186920166016, 64.17297697067261, 64.8459644317627, 65.51599311828613, 66.19143486022949, 66.86851000785828, 67.5429892539978, 68.22444200515747, 69.43384718894958]
[16.641666666666666, 26.216666666666665, 35.9, 40.46666666666667, 45.61666666666667, 45.69166666666667, 44.916666666666664, 40.425, 46.71666666666667, 59.46666666666667, 62.333333333333336, 66.15, 69.13333333333334, 73.18333333333334, 75.725, 77.74166666666666, 79.79166666666667, 80.20833333333333, 80.9, 81.19166666666666, 81.275, 82.15, 82.36666666666666, 82.49166666666666, 83.05833333333334, 83.00833333333334, 83.35, 83.875, 84.61666666666666, 85.41666666666667, 85.99166666666666, 86.25833333333334, 86.25, 87.64166666666667, 88.56666666666666, 88.65833333333333, 89.175, 89.275, 89.30833333333334, 88.99166666666666, 89.24166666666666, 89.30833333333334, 89.63333333333334, 89.58333333333333, 89.5, 89.46666666666667, 89.625, 89.875, 90.00833333333334, 89.94166666666666, 89.93333333333334, 90.11666666666666, 90.18333333333334, 90.325, 90.44166666666666, 90.5, 90.41666666666667, 90.49166666666666, 90.45833333333333, 90.49166666666666, 90.45833333333333, 90.625, 90.475, 90.65, 90.59166666666667, 90.55833333333334, 90.675, 90.69166666666666, 90.81666666666666, 90.75, 90.68333333333334, 90.66666666666667, 90.60833333333333, 90.725, 90.84166666666667, 90.875, 90.85833333333333, 90.90833333333333, 90.83333333333333, 90.91666666666667, 90.65, 90.79166666666667, 90.75, 90.9, 90.86666666666666, 90.95833333333333, 90.88333333333334, 91.06666666666666, 90.975, 90.925, 90.89166666666667, 90.91666666666667, 91.04166666666667, 90.975, 91.075, 91.15833333333333, 90.95833333333333, 90.95, 91.025, 91.075, 90.90833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.299, Test accuracy: 12.93 

Round   1, Train loss: 2.295, Test loss: 2.292, Test accuracy: 29.68 

Round   2, Train loss: 2.282, Test loss: 2.271, Test accuracy: 30.24 

Round   3, Train loss: 2.221, Test loss: 2.180, Test accuracy: 41.55 

Round   4, Train loss: 2.081, Test loss: 2.054, Test accuracy: 45.21 

Round   5, Train loss: 1.983, Test loss: 1.970, Test accuracy: 51.17 

Round   6, Train loss: 1.880, Test loss: 1.892, Test accuracy: 58.70 

Round   7, Train loss: 1.779, Test loss: 1.820, Test accuracy: 66.76 

Round   8, Train loss: 1.725, Test loss: 1.762, Test accuracy: 72.41 

Round   9, Train loss: 1.686, Test loss: 1.723, Test accuracy: 75.53 

Round  10, Train loss: 1.672, Test loss: 1.682, Test accuracy: 79.67 

Round  11, Train loss: 1.647, Test loss: 1.663, Test accuracy: 81.44 

Round  12, Train loss: 1.645, Test loss: 1.649, Test accuracy: 82.56 

Round  13, Train loss: 1.626, Test loss: 1.643, Test accuracy: 82.97 

Round  14, Train loss: 1.625, Test loss: 1.638, Test accuracy: 83.39 

Round  15, Train loss: 1.616, Test loss: 1.636, Test accuracy: 83.52 

Round  16, Train loss: 1.625, Test loss: 1.630, Test accuracy: 83.97 

Round  17, Train loss: 1.620, Test loss: 1.628, Test accuracy: 84.05 

Round  18, Train loss: 1.607, Test loss: 1.626, Test accuracy: 84.12 

Round  19, Train loss: 1.610, Test loss: 1.625, Test accuracy: 84.23 

Round  20, Train loss: 1.602, Test loss: 1.624, Test accuracy: 84.44 

Round  21, Train loss: 1.603, Test loss: 1.622, Test accuracy: 84.46 

Round  22, Train loss: 1.608, Test loss: 1.621, Test accuracy: 84.56 

Round  23, Train loss: 1.606, Test loss: 1.620, Test accuracy: 84.51 

Round  24, Train loss: 1.592, Test loss: 1.619, Test accuracy: 84.54 

Round  25, Train loss: 1.597, Test loss: 1.617, Test accuracy: 84.77 

Round  26, Train loss: 1.590, Test loss: 1.617, Test accuracy: 84.84 

Round  27, Train loss: 1.603, Test loss: 1.617, Test accuracy: 84.91 

Round  28, Train loss: 1.599, Test loss: 1.616, Test accuracy: 84.89 

Round  29, Train loss: 1.594, Test loss: 1.616, Test accuracy: 84.89 

Round  30, Train loss: 1.586, Test loss: 1.616, Test accuracy: 84.95 

Round  31, Train loss: 1.589, Test loss: 1.615, Test accuracy: 84.95 

Round  32, Train loss: 1.594, Test loss: 1.615, Test accuracy: 85.00 

Round  33, Train loss: 1.591, Test loss: 1.615, Test accuracy: 84.91 

Round  34, Train loss: 1.600, Test loss: 1.615, Test accuracy: 84.96 

Round  35, Train loss: 1.585, Test loss: 1.615, Test accuracy: 84.96 

Round  36, Train loss: 1.589, Test loss: 1.614, Test accuracy: 85.02 

Round  37, Train loss: 1.589, Test loss: 1.614, Test accuracy: 85.00 

Round  38, Train loss: 1.589, Test loss: 1.614, Test accuracy: 85.10 

Round  39, Train loss: 1.592, Test loss: 1.612, Test accuracy: 85.14 

Round  40, Train loss: 1.587, Test loss: 1.613, Test accuracy: 85.12 

Round  41, Train loss: 1.586, Test loss: 1.613, Test accuracy: 85.13 

Round  42, Train loss: 1.584, Test loss: 1.612, Test accuracy: 85.14 

Round  43, Train loss: 1.583, Test loss: 1.612, Test accuracy: 85.17 

Round  44, Train loss: 1.585, Test loss: 1.612, Test accuracy: 85.19 

Round  45, Train loss: 1.582, Test loss: 1.612, Test accuracy: 85.25 

Round  46, Train loss: 1.580, Test loss: 1.612, Test accuracy: 85.19 

Round  47, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.29 

Round  48, Train loss: 1.590, Test loss: 1.611, Test accuracy: 85.30 

Round  49, Train loss: 1.578, Test loss: 1.611, Test accuracy: 85.28 

Round  50, Train loss: 1.589, Test loss: 1.611, Test accuracy: 85.33 

Round  51, Train loss: 1.584, Test loss: 1.611, Test accuracy: 85.22 

Round  52, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.28 

Round  53, Train loss: 1.586, Test loss: 1.610, Test accuracy: 85.33 

Round  54, Train loss: 1.589, Test loss: 1.610, Test accuracy: 85.30 

Round  55, Train loss: 1.578, Test loss: 1.610, Test accuracy: 85.31 

Round  56, Train loss: 1.584, Test loss: 1.610, Test accuracy: 85.37 

Round  57, Train loss: 1.578, Test loss: 1.610, Test accuracy: 85.42 

Round  58, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.38 

Round  59, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.35 

Round  60, Train loss: 1.575, Test loss: 1.609, Test accuracy: 85.37 

Round  61, Train loss: 1.578, Test loss: 1.609, Test accuracy: 85.42 

Round  62, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.42 

Round  63, Train loss: 1.573, Test loss: 1.609, Test accuracy: 85.45 

Round  64, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.41 

Round  65, Train loss: 1.590, Test loss: 1.609, Test accuracy: 85.31 

Round  66, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.37 

Round  67, Train loss: 1.584, Test loss: 1.609, Test accuracy: 85.37 

Round  68, Train loss: 1.583, Test loss: 1.609, Test accuracy: 85.35 

Round  69, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.45 

Round  70, Train loss: 1.582, Test loss: 1.608, Test accuracy: 85.46 

Round  71, Train loss: 1.580, Test loss: 1.608, Test accuracy: 85.53 

Round  72, Train loss: 1.580, Test loss: 1.607, Test accuracy: 85.57 

Round  73, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.62 

Round  74, Train loss: 1.563, Test loss: 1.606, Test accuracy: 85.73 

Round  75, Train loss: 1.569, Test loss: 1.606, Test accuracy: 85.83 

Round  76, Train loss: 1.567, Test loss: 1.606, Test accuracy: 85.78 

Round  77, Train loss: 1.563, Test loss: 1.604, Test accuracy: 85.88 

Round  78, Train loss: 1.574, Test loss: 1.604, Test accuracy: 85.95 

Round  79, Train loss: 1.562, Test loss: 1.603, Test accuracy: 86.01 

Round  80, Train loss: 1.568, Test loss: 1.603, Test accuracy: 85.96 

Round  81, Train loss: 1.551, Test loss: 1.601, Test accuracy: 86.13 

Round  82, Train loss: 1.571, Test loss: 1.602, Test accuracy: 86.20 

Round  83, Train loss: 1.562, Test loss: 1.602, Test accuracy: 86.15 

Round  84, Train loss: 1.569, Test loss: 1.601, Test accuracy: 86.21 

Round  85, Train loss: 1.577, Test loss: 1.602, Test accuracy: 86.15 

Round  86, Train loss: 1.553, Test loss: 1.601, Test accuracy: 86.19 

Round  87, Train loss: 1.569, Test loss: 1.601, Test accuracy: 86.16 

Round  88, Train loss: 1.538, Test loss: 1.600, Test accuracy: 86.25 

Round  89, Train loss: 1.555, Test loss: 1.600, Test accuracy: 86.19 

Round  90, Train loss: 1.549, Test loss: 1.600, Test accuracy: 86.28 

Round  91, Train loss: 1.573, Test loss: 1.600, Test accuracy: 86.28 

Round  92, Train loss: 1.558, Test loss: 1.599, Test accuracy: 86.35 

Round  93, Train loss: 1.555, Test loss: 1.599, Test accuracy: 86.38 

Round  94, Train loss: 1.565, Test loss: 1.599, Test accuracy: 86.37 

Round  95, Train loss: 1.568, Test loss: 1.598, Test accuracy: 86.49 

Round  96, Train loss: 1.551, Test loss: 1.598, Test accuracy: 86.46 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 1.551, Test loss: 1.598, Test accuracy: 86.51 

Round  98, Train loss: 1.553, Test loss: 1.597, Test accuracy: 86.54 

Round  99, Train loss: 1.559, Test loss: 1.596, Test accuracy: 86.72 

Final Round, Train loss: 1.552, Test loss: 1.592, Test accuracy: 87.15 

Average accuracy final 10 rounds: 86.43750000000001 

688.5101847648621
[0.8061695098876953, 1.5195808410644531, 2.2590489387512207, 3.009612560272217, 3.758556842803955, 4.506368398666382, 5.251147031784058, 6.00141167640686, 6.74948787689209, 7.499538421630859, 8.248693704605103, 8.991097211837769, 9.741246938705444, 10.483635187149048, 11.228221416473389, 11.971707344055176, 12.715337991714478, 13.461381435394287, 14.201412200927734, 14.94640588760376, 15.716569423675537, 16.526384115219116, 17.18184781074524, 17.83115315437317, 18.49340057373047, 19.14484453201294, 19.78812289237976, 20.444938898086548, 21.09439182281494, 21.749844074249268, 22.403055429458618, 23.056220054626465, 23.711101531982422, 24.371881246566772, 25.03167414665222, 25.686737775802612, 26.34176254272461, 27.00040888786316, 27.655211448669434, 28.307394981384277, 28.969502687454224, 29.63071846961975, 30.280019521713257, 30.93987536430359, 31.60033631324768, 32.25731325149536, 32.91072702407837, 33.56332778930664, 34.21190571784973, 34.86906433105469, 35.526167154312134, 36.17361283302307, 36.83070707321167, 37.48408317565918, 38.13920068740845, 38.78706765174866, 39.44002294540405, 40.090794801712036, 40.737573862075806, 41.39773869514465, 42.047592878341675, 42.68906903266907, 43.343236684799194, 43.99772810935974, 44.64776825904846, 45.40807247161865, 46.16866397857666, 46.92659902572632, 47.68542194366455, 48.447818994522095, 49.200361490249634, 49.95525407791138, 50.71217894554138, 51.466997146606445, 52.22749471664429, 52.989712953567505, 53.746742725372314, 54.50406622886658, 55.26313614845276, 56.01879167556763, 56.77999997138977, 57.54423379898071, 58.303027391433716, 59.060178995132446, 59.81424045562744, 60.57519316673279, 61.33608269691467, 62.09181475639343, 62.84661650657654, 63.598387241363525, 64.35198593139648, 65.10608720779419, 65.8605408668518, 66.61901235580444, 67.37372255325317, 68.02795481681824, 68.67596888542175, 69.33012700080872, 69.98802280426025, 70.6348078250885, 71.78760814666748]
[12.933333333333334, 29.675, 30.241666666666667, 41.55, 45.208333333333336, 51.175, 58.7, 66.75833333333334, 72.40833333333333, 75.53333333333333, 79.675, 81.44166666666666, 82.55833333333334, 82.96666666666667, 83.39166666666667, 83.51666666666667, 83.96666666666667, 84.05, 84.125, 84.23333333333333, 84.44166666666666, 84.45833333333333, 84.55833333333334, 84.50833333333334, 84.54166666666667, 84.76666666666667, 84.84166666666667, 84.90833333333333, 84.89166666666667, 84.89166666666667, 84.95, 84.95, 85.0, 84.90833333333333, 84.95833333333333, 84.95833333333333, 85.01666666666667, 85.0, 85.1, 85.14166666666667, 85.125, 85.13333333333334, 85.14166666666667, 85.16666666666667, 85.19166666666666, 85.25, 85.19166666666666, 85.29166666666667, 85.3, 85.28333333333333, 85.33333333333333, 85.225, 85.28333333333333, 85.325, 85.3, 85.30833333333334, 85.36666666666666, 85.41666666666667, 85.38333333333334, 85.35, 85.36666666666666, 85.425, 85.41666666666667, 85.45, 85.40833333333333, 85.30833333333334, 85.36666666666666, 85.36666666666666, 85.35, 85.45, 85.45833333333333, 85.525, 85.56666666666666, 85.61666666666666, 85.73333333333333, 85.825, 85.78333333333333, 85.875, 85.95, 86.00833333333334, 85.95833333333333, 86.13333333333334, 86.2, 86.15, 86.20833333333333, 86.15, 86.19166666666666, 86.15833333333333, 86.25, 86.19166666666666, 86.275, 86.28333333333333, 86.35, 86.38333333333334, 86.36666666666666, 86.49166666666666, 86.45833333333333, 86.50833333333334, 86.54166666666667, 86.71666666666667, 87.15]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.299, Test accuracy: 9.81 

Round   1, Train loss: 2.295, Test loss: 2.293, Test accuracy: 20.72 

Round   2, Train loss: 2.279, Test loss: 2.271, Test accuracy: 21.09 

Round   3, Train loss: 2.249, Test loss: 2.234, Test accuracy: 26.47 

Round   4, Train loss: 2.174, Test loss: 2.166, Test accuracy: 38.14 

Round   5, Train loss: 2.081, Test loss: 2.072, Test accuracy: 47.25 

Round   6, Train loss: 1.969, Test loss: 1.981, Test accuracy: 55.27 

Round   7, Train loss: 1.841, Test loss: 1.912, Test accuracy: 65.04 

Round   8, Train loss: 1.786, Test loss: 1.841, Test accuracy: 69.37 

Round   9, Train loss: 1.745, Test loss: 1.787, Test accuracy: 72.62 

Round  10, Train loss: 1.663, Test loss: 1.773, Test accuracy: 73.08 

Round  11, Train loss: 1.686, Test loss: 1.741, Test accuracy: 75.03 

Round  12, Train loss: 1.656, Test loss: 1.722, Test accuracy: 76.47 

Round  13, Train loss: 1.624, Test loss: 1.714, Test accuracy: 76.95 

Round  14, Train loss: 1.609, Test loss: 1.708, Test accuracy: 77.22 

Round  15, Train loss: 1.629, Test loss: 1.703, Test accuracy: 77.45 

Round  16, Train loss: 1.608, Test loss: 1.699, Test accuracy: 77.69 

Round  17, Train loss: 1.606, Test loss: 1.697, Test accuracy: 77.88 

Round  18, Train loss: 1.595, Test loss: 1.695, Test accuracy: 77.83 

Round  19, Train loss: 1.586, Test loss: 1.693, Test accuracy: 78.00 

Round  20, Train loss: 1.604, Test loss: 1.688, Test accuracy: 78.45 

Round  21, Train loss: 1.577, Test loss: 1.687, Test accuracy: 78.49 

Round  22, Train loss: 1.585, Test loss: 1.686, Test accuracy: 78.47 

Round  23, Train loss: 1.582, Test loss: 1.686, Test accuracy: 78.48 

Round  24, Train loss: 1.593, Test loss: 1.685, Test accuracy: 78.53 

Round  25, Train loss: 1.580, Test loss: 1.685, Test accuracy: 78.46 

Round  26, Train loss: 1.577, Test loss: 1.685, Test accuracy: 78.36 

Round  27, Train loss: 1.575, Test loss: 1.685, Test accuracy: 78.39 

Round  28, Train loss: 1.577, Test loss: 1.684, Test accuracy: 78.46 

Round  29, Train loss: 1.572, Test loss: 1.684, Test accuracy: 78.46 

Round  30, Train loss: 1.572, Test loss: 1.683, Test accuracy: 78.42 

Round  31, Train loss: 1.563, Test loss: 1.673, Test accuracy: 79.54 

Round  32, Train loss: 1.524, Test loss: 1.659, Test accuracy: 81.82 

Round  33, Train loss: 1.523, Test loss: 1.649, Test accuracy: 82.74 

Round  34, Train loss: 1.507, Test loss: 1.642, Test accuracy: 83.40 

Round  35, Train loss: 1.500, Test loss: 1.637, Test accuracy: 83.88 

Round  36, Train loss: 1.511, Test loss: 1.629, Test accuracy: 84.63 

Round  37, Train loss: 1.506, Test loss: 1.624, Test accuracy: 85.38 

Round  38, Train loss: 1.493, Test loss: 1.622, Test accuracy: 85.31 

Round  39, Train loss: 1.485, Test loss: 1.620, Test accuracy: 85.53 

Round  40, Train loss: 1.488, Test loss: 1.619, Test accuracy: 85.44 

Round  41, Train loss: 1.485, Test loss: 1.618, Test accuracy: 85.53 

Round  42, Train loss: 1.487, Test loss: 1.618, Test accuracy: 85.47 

Round  43, Train loss: 1.493, Test loss: 1.616, Test accuracy: 85.52 

Round  44, Train loss: 1.487, Test loss: 1.616, Test accuracy: 85.52 

Round  45, Train loss: 1.484, Test loss: 1.616, Test accuracy: 85.52 

Round  46, Train loss: 1.485, Test loss: 1.615, Test accuracy: 85.61 

Round  47, Train loss: 1.486, Test loss: 1.615, Test accuracy: 85.67 

Round  48, Train loss: 1.477, Test loss: 1.614, Test accuracy: 85.67 

Round  49, Train loss: 1.488, Test loss: 1.614, Test accuracy: 85.58 

Round  50, Train loss: 1.489, Test loss: 1.613, Test accuracy: 85.64 

Round  51, Train loss: 1.481, Test loss: 1.613, Test accuracy: 85.69 

Round  52, Train loss: 1.482, Test loss: 1.612, Test accuracy: 85.65 

Round  53, Train loss: 1.480, Test loss: 1.612, Test accuracy: 85.68 

Round  54, Train loss: 1.485, Test loss: 1.612, Test accuracy: 85.66 

Round  55, Train loss: 1.486, Test loss: 1.612, Test accuracy: 85.72 

Round  56, Train loss: 1.486, Test loss: 1.611, Test accuracy: 85.75 

Round  57, Train loss: 1.483, Test loss: 1.611, Test accuracy: 85.67 

Round  58, Train loss: 1.482, Test loss: 1.611, Test accuracy: 85.78 

Round  59, Train loss: 1.483, Test loss: 1.611, Test accuracy: 85.69 

Round  60, Train loss: 1.482, Test loss: 1.611, Test accuracy: 85.63 

Round  61, Train loss: 1.483, Test loss: 1.611, Test accuracy: 85.78 

Round  62, Train loss: 1.484, Test loss: 1.611, Test accuracy: 85.72 

Round  63, Train loss: 1.484, Test loss: 1.611, Test accuracy: 85.73 

Round  64, Train loss: 1.484, Test loss: 1.611, Test accuracy: 85.73 

Round  65, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.67 

Round  66, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.71 

Round  67, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.69 

Round  68, Train loss: 1.480, Test loss: 1.610, Test accuracy: 85.69 

Round  69, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.72 

Round  70, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.67 

Round  71, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.72 

Round  72, Train loss: 1.480, Test loss: 1.610, Test accuracy: 85.70 

Round  73, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.67 

Round  74, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.65 

Round  75, Train loss: 1.486, Test loss: 1.610, Test accuracy: 85.65 

Round  76, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.67 

Round  77, Train loss: 1.481, Test loss: 1.610, Test accuracy: 85.67 

Round  78, Train loss: 1.481, Test loss: 1.610, Test accuracy: 85.67 

Round  79, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.66 

Round  80, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.64 

Round  81, Train loss: 1.482, Test loss: 1.610, Test accuracy: 85.65 

Round  82, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.72 

Round  83, Train loss: 1.482, Test loss: 1.610, Test accuracy: 85.72 

Round  84, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.70 

Round  85, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.72 

Round  86, Train loss: 1.482, Test loss: 1.610, Test accuracy: 85.69 

Round  87, Train loss: 1.481, Test loss: 1.610, Test accuracy: 85.67 

Round  88, Train loss: 1.481, Test loss: 1.610, Test accuracy: 85.62 

Round  89, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.62 

Round  90, Train loss: 1.484, Test loss: 1.609, Test accuracy: 85.68 

Round  91, Train loss: 1.478, Test loss: 1.609, Test accuracy: 85.71 

Round  92, Train loss: 1.480, Test loss: 1.609, Test accuracy: 85.71 

Round  93, Train loss: 1.480, Test loss: 1.609, Test accuracy: 85.70 

Round  94, Train loss: 1.481, Test loss: 1.609, Test accuracy: 85.67 

Round  95, Train loss: 1.480, Test loss: 1.609, Test accuracy: 85.72 

Round  96, Train loss: 1.481, Test loss: 1.609, Test accuracy: 85.66 

Round  97, Train loss: 1.479, Test loss: 1.609, Test accuracy: 85.72 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 1.484, Test loss: 1.609, Test accuracy: 85.70 

Round  99, Train loss: 1.483, Test loss: 1.609, Test accuracy: 85.70 

Final Round, Train loss: 1.481, Test loss: 1.609, Test accuracy: 85.67 

Average accuracy final 10 rounds: 85.69666666666666 

725.2032697200775
[0.8952627182006836, 1.6957783699035645, 2.5038020610809326, 3.323000907897949, 4.094758749008179, 4.859174013137817, 5.630231857299805, 6.399234771728516, 7.163597345352173, 7.934328079223633, 8.69733190536499, 9.458141326904297, 10.22089958190918, 10.987489938735962, 11.748006582260132, 12.511425495147705, 13.272780418395996, 14.034607410430908, 14.798208713531494, 15.551633358001709, 16.311479806900024, 17.066861152648926, 17.82231068611145, 18.54719090461731, 19.28226590156555, 20.02179718017578, 20.742634534835815, 21.461727142333984, 22.19013738632202, 22.915400743484497, 23.681495189666748, 24.448994159698486, 25.211061239242554, 25.970027446746826, 26.736559867858887, 27.496475219726562, 28.26299548149109, 29.025734186172485, 29.790919065475464, 30.554016590118408, 31.31709051132202, 32.084226846694946, 32.849984645843506, 33.61026382446289, 34.37408113479614, 35.136029958724976, 35.896583557128906, 36.6560845375061, 37.41767477989197, 38.18197441101074, 38.941144943237305, 39.69689083099365, 40.44187355041504, 41.18695688247681, 41.935351848602295, 42.67730450630188, 43.41784381866455, 44.166494369506836, 44.91233992576599, 45.652092933654785, 46.397329568862915, 47.14816474914551, 47.8983850479126, 48.64562726020813, 49.38654446601868, 50.12947201728821, 50.86373448371887, 51.60953116416931, 52.355180740356445, 53.09939646720886, 53.85795283317566, 54.623464584350586, 55.38299870491028, 56.144296407699585, 56.905216693878174, 57.6628315448761, 58.42604160308838, 59.18472099304199, 59.94461464881897, 60.70698380470276, 61.46656608581543, 62.2250337600708, 62.985721826553345, 63.74002289772034, 64.50499486923218, 65.26466274261475, 66.02658677101135, 66.78217220306396, 67.53727173805237, 68.29415726661682, 69.05239868164062, 69.80689764022827, 70.571608543396, 71.33178067207336, 72.09131336212158, 72.85097002983093, 73.61159133911133, 74.37021350860596, 75.13132524490356, 75.88795375823975, 77.30201172828674]
[9.808333333333334, 20.716666666666665, 21.091666666666665, 26.466666666666665, 38.141666666666666, 47.25, 55.266666666666666, 65.04166666666667, 69.36666666666666, 72.625, 73.08333333333333, 75.025, 76.46666666666667, 76.95, 77.21666666666667, 77.45, 77.69166666666666, 77.88333333333334, 77.825, 78.0, 78.45, 78.49166666666666, 78.46666666666667, 78.48333333333333, 78.525, 78.45833333333333, 78.35833333333333, 78.39166666666667, 78.45833333333333, 78.45833333333333, 78.425, 79.54166666666667, 81.81666666666666, 82.74166666666666, 83.4, 83.875, 84.63333333333334, 85.38333333333334, 85.30833333333334, 85.525, 85.44166666666666, 85.525, 85.46666666666667, 85.51666666666667, 85.51666666666667, 85.51666666666667, 85.60833333333333, 85.675, 85.66666666666667, 85.58333333333333, 85.64166666666667, 85.69166666666666, 85.65, 85.68333333333334, 85.65833333333333, 85.71666666666667, 85.75, 85.66666666666667, 85.775, 85.69166666666666, 85.63333333333334, 85.78333333333333, 85.725, 85.73333333333333, 85.73333333333333, 85.675, 85.70833333333333, 85.69166666666666, 85.69166666666666, 85.71666666666667, 85.675, 85.725, 85.7, 85.66666666666667, 85.65, 85.65, 85.66666666666667, 85.675, 85.675, 85.65833333333333, 85.64166666666667, 85.65, 85.71666666666667, 85.725, 85.7, 85.725, 85.69166666666666, 85.675, 85.625, 85.625, 85.68333333333334, 85.70833333333333, 85.70833333333333, 85.7, 85.675, 85.71666666666667, 85.65833333333333, 85.71666666666667, 85.7, 85.7, 85.66666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.703, Test loss: 2.291, Test accuracy: 32.17
Round   1, Train loss: 1.561, Test loss: 2.235, Test accuracy: 52.07
Round   2, Train loss: 1.378, Test loss: 2.102, Test accuracy: 62.74
Round   3, Train loss: 1.270, Test loss: 1.973, Test accuracy: 74.04
Round   4, Train loss: 1.225, Test loss: 1.922, Test accuracy: 77.13
Round   5, Train loss: 1.220, Test loss: 1.902, Test accuracy: 77.92
Round   6, Train loss: 1.216, Test loss: 1.886, Test accuracy: 80.02
Round   7, Train loss: 1.211, Test loss: 1.873, Test accuracy: 81.36
Round   8, Train loss: 1.197, Test loss: 1.864, Test accuracy: 81.38
Round   9, Train loss: 1.201, Test loss: 1.859, Test accuracy: 81.22
Round  10, Train loss: 1.201, Test loss: 1.856, Test accuracy: 81.12
Round  11, Train loss: 1.186, Test loss: 1.853, Test accuracy: 81.13
Round  12, Train loss: 1.187, Test loss: 1.850, Test accuracy: 80.82
Round  13, Train loss: 1.195, Test loss: 1.849, Test accuracy: 80.81
Round  14, Train loss: 1.189, Test loss: 1.848, Test accuracy: 80.73
Round  15, Train loss: 1.184, Test loss: 1.846, Test accuracy: 80.51
Round  16, Train loss: 1.183, Test loss: 1.845, Test accuracy: 80.42
Round  17, Train loss: 1.183, Test loss: 1.844, Test accuracy: 80.41
Round  18, Train loss: 1.190, Test loss: 1.844, Test accuracy: 80.26
Round  19, Train loss: 1.182, Test loss: 1.843, Test accuracy: 80.14
Round  20, Train loss: 1.187, Test loss: 1.842, Test accuracy: 80.05
Round  21, Train loss: 1.178, Test loss: 1.843, Test accuracy: 79.88
Round  22, Train loss: 1.177, Test loss: 1.842, Test accuracy: 79.66
Round  23, Train loss: 1.181, Test loss: 1.841, Test accuracy: 79.45
Round  24, Train loss: 1.183, Test loss: 1.841, Test accuracy: 79.42
Round  25, Train loss: 1.182, Test loss: 1.842, Test accuracy: 79.15
Round  26, Train loss: 1.187, Test loss: 1.843, Test accuracy: 78.93
Round  27, Train loss: 1.183, Test loss: 1.843, Test accuracy: 78.90
Round  28, Train loss: 1.180, Test loss: 1.843, Test accuracy: 78.74
Round  29, Train loss: 1.177, Test loss: 1.844, Test accuracy: 78.53
Round  30, Train loss: 1.185, Test loss: 1.846, Test accuracy: 77.98
Round  31, Train loss: 1.182, Test loss: 1.847, Test accuracy: 77.78
Round  32, Train loss: 1.183, Test loss: 1.848, Test accuracy: 77.53
Round  33, Train loss: 1.178, Test loss: 1.848, Test accuracy: 77.28
Round  34, Train loss: 1.180, Test loss: 1.849, Test accuracy: 77.14
Round  35, Train loss: 1.172, Test loss: 1.850, Test accuracy: 77.00
Round  36, Train loss: 1.175, Test loss: 1.851, Test accuracy: 76.71
Round  37, Train loss: 1.175, Test loss: 1.851, Test accuracy: 76.70
Round  38, Train loss: 1.182, Test loss: 1.852, Test accuracy: 76.62
Round  39, Train loss: 1.174, Test loss: 1.851, Test accuracy: 76.58
Round  40, Train loss: 1.179, Test loss: 1.852, Test accuracy: 76.46
Round  41, Train loss: 1.184, Test loss: 1.853, Test accuracy: 76.29
Round  42, Train loss: 1.180, Test loss: 1.854, Test accuracy: 76.04
Round  43, Train loss: 1.181, Test loss: 1.853, Test accuracy: 76.00
Round  44, Train loss: 1.177, Test loss: 1.854, Test accuracy: 75.81
Round  45, Train loss: 1.182, Test loss: 1.855, Test accuracy: 75.76
Round  46, Train loss: 1.178, Test loss: 1.855, Test accuracy: 75.47
Round  47, Train loss: 1.183, Test loss: 1.856, Test accuracy: 75.46
Round  48, Train loss: 1.183, Test loss: 1.856, Test accuracy: 75.29
Round  49, Train loss: 1.178, Test loss: 1.856, Test accuracy: 75.24
Round  50, Train loss: 1.173, Test loss: 1.856, Test accuracy: 74.90
Round  51, Train loss: 1.182, Test loss: 1.857, Test accuracy: 74.72
Round  52, Train loss: 1.177, Test loss: 1.859, Test accuracy: 74.59
Round  53, Train loss: 1.184, Test loss: 1.860, Test accuracy: 74.52
Round  54, Train loss: 1.175, Test loss: 1.860, Test accuracy: 74.46
Round  55, Train loss: 1.178, Test loss: 1.860, Test accuracy: 74.36
Round  56, Train loss: 1.176, Test loss: 1.860, Test accuracy: 74.39
Round  57, Train loss: 1.177, Test loss: 1.861, Test accuracy: 74.17
Round  58, Train loss: 1.177, Test loss: 1.862, Test accuracy: 74.05
Round  59, Train loss: 1.181, Test loss: 1.862, Test accuracy: 73.92
Round  60, Train loss: 1.172, Test loss: 1.862, Test accuracy: 73.86
Round  61, Train loss: 1.167, Test loss: 1.864, Test accuracy: 73.77
Round  62, Train loss: 1.180, Test loss: 1.864, Test accuracy: 73.58
Round  63, Train loss: 1.182, Test loss: 1.865, Test accuracy: 73.50
Round  64, Train loss: 1.173, Test loss: 1.865, Test accuracy: 73.36
Round  65, Train loss: 1.174, Test loss: 1.865, Test accuracy: 73.38
Round  66, Train loss: 1.171, Test loss: 1.865, Test accuracy: 73.42
Round  67, Train loss: 1.180, Test loss: 1.867, Test accuracy: 73.07
Round  68, Train loss: 1.176, Test loss: 1.867, Test accuracy: 73.11
Round  69, Train loss: 1.180, Test loss: 1.867, Test accuracy: 73.12
Round  70, Train loss: 1.178, Test loss: 1.867, Test accuracy: 72.98
Round  71, Train loss: 1.174, Test loss: 1.867, Test accuracy: 72.85
Round  72, Train loss: 1.173, Test loss: 1.868, Test accuracy: 72.71
Round  73, Train loss: 1.179, Test loss: 1.869, Test accuracy: 72.50
Round  74, Train loss: 1.173, Test loss: 1.869, Test accuracy: 72.52
Round  75, Train loss: 1.176, Test loss: 1.869, Test accuracy: 72.28
Round  76, Train loss: 1.171, Test loss: 1.870, Test accuracy: 72.21
Round  77, Train loss: 1.174, Test loss: 1.870, Test accuracy: 72.03
Round  78, Train loss: 1.175, Test loss: 1.871, Test accuracy: 71.77
Round  79, Train loss: 1.174, Test loss: 1.872, Test accuracy: 71.72
Round  80, Train loss: 1.176, Test loss: 1.872, Test accuracy: 71.79
Round  81, Train loss: 1.178, Test loss: 1.873, Test accuracy: 71.72
Round  82, Train loss: 1.175, Test loss: 1.874, Test accuracy: 71.57
Round  83, Train loss: 1.174, Test loss: 1.874, Test accuracy: 71.60
Round  84, Train loss: 1.173, Test loss: 1.875, Test accuracy: 71.60
Round  85, Train loss: 1.177, Test loss: 1.875, Test accuracy: 71.53
Round  86, Train loss: 1.172, Test loss: 1.875, Test accuracy: 71.43
Round  87, Train loss: 1.175, Test loss: 1.876, Test accuracy: 71.37
Round  88, Train loss: 1.171, Test loss: 1.876, Test accuracy: 71.19
Round  89, Train loss: 1.174, Test loss: 1.876, Test accuracy: 71.34
Round  90, Train loss: 1.177, Test loss: 1.879, Test accuracy: 71.28
Round  91, Train loss: 1.164, Test loss: 1.879, Test accuracy: 71.43
Round  92, Train loss: 1.163, Test loss: 1.875, Test accuracy: 72.47
Round  93, Train loss: 1.134, Test loss: 1.872, Test accuracy: 73.65
Round  94, Train loss: 1.129, Test loss: 1.869, Test accuracy: 74.22
Round  95, Train loss: 1.121, Test loss: 1.868, Test accuracy: 74.47
Round  96, Train loss: 1.125, Test loss: 1.864, Test accuracy: 75.25
Round  97, Train loss: 1.114, Test loss: 1.862, Test accuracy: 75.58
Round  98, Train loss: 1.110, Test loss: 1.862, Test accuracy: 75.62
Round  99, Train loss: 1.112, Test loss: 1.860, Test accuracy: 75.80
Final Round, Train loss: 1.115, Test loss: 1.860, Test accuracy: 76.12
Average accuracy final 10 rounds: 73.97749999999999
1302.0514051914215
[]
[32.175, 52.06666666666667, 62.74166666666667, 74.04166666666667, 77.13333333333334, 77.91666666666667, 80.01666666666667, 81.35833333333333, 81.38333333333334, 81.21666666666667, 81.11666666666666, 81.13333333333334, 80.81666666666666, 80.80833333333334, 80.73333333333333, 80.50833333333334, 80.41666666666667, 80.40833333333333, 80.25833333333334, 80.14166666666667, 80.05, 79.875, 79.65833333333333, 79.45, 79.41666666666667, 79.15, 78.93333333333334, 78.9, 78.74166666666666, 78.525, 77.98333333333333, 77.78333333333333, 77.53333333333333, 77.28333333333333, 77.14166666666667, 77.0, 76.70833333333333, 76.7, 76.61666666666666, 76.575, 76.45833333333333, 76.29166666666667, 76.04166666666667, 76.0, 75.80833333333334, 75.75833333333334, 75.475, 75.45833333333333, 75.29166666666667, 75.24166666666666, 74.9, 74.71666666666667, 74.59166666666667, 74.51666666666667, 74.45833333333333, 74.35833333333333, 74.39166666666667, 74.16666666666667, 74.05, 73.91666666666667, 73.85833333333333, 73.76666666666667, 73.58333333333333, 73.5, 73.35833333333333, 73.38333333333334, 73.41666666666667, 73.06666666666666, 73.10833333333333, 73.11666666666666, 72.98333333333333, 72.85, 72.70833333333333, 72.5, 72.51666666666667, 72.275, 72.20833333333333, 72.025, 71.76666666666667, 71.71666666666667, 71.79166666666667, 71.71666666666667, 71.56666666666666, 71.6, 71.6, 71.53333333333333, 71.43333333333334, 71.36666666666666, 71.19166666666666, 71.34166666666667, 71.275, 71.43333333333334, 72.475, 73.65, 74.225, 74.46666666666667, 75.25, 75.575, 75.625, 75.8, 76.11666666666666]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 12.33
Round   0: Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 8.18
Round   1, Train loss: 2.305, Test loss: 2.300, Test accuracy: 18.49
Round   1: Global train loss: 2.305, Global test loss: 2.302, Global test accuracy: 10.59
Round   2, Train loss: 2.288, Test loss: 2.300, Test accuracy: 18.23
Round   2: Global train loss: 2.288, Global test loss: 2.302, Global test accuracy: 11.46
Round   3, Train loss: 2.302, Test loss: 2.298, Test accuracy: 20.82
Round   3: Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.38
Round   4, Train loss: 2.301, Test loss: 2.298, Test accuracy: 17.38
Round   4: Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.06
Round   5, Train loss: 2.292, Test loss: 2.297, Test accuracy: 17.88
Round   5: Global train loss: 2.292, Global test loss: 2.301, Global test accuracy: 16.43
Round   6, Train loss: 2.272, Test loss: 2.290, Test accuracy: 22.26
Round   6: Global train loss: 2.272, Global test loss: 2.300, Global test accuracy: 20.61
Round   7, Train loss: 2.269, Test loss: 2.288, Test accuracy: 22.28
Round   7: Global train loss: 2.269, Global test loss: 2.300, Global test accuracy: 22.28
Round   8, Train loss: 2.282, Test loss: 2.290, Test accuracy: 19.68
Round   8: Global train loss: 2.282, Global test loss: 2.300, Global test accuracy: 23.77
Round   9, Train loss: 2.251, Test loss: 2.283, Test accuracy: 21.18
Round   9: Global train loss: 2.251, Global test loss: 2.299, Global test accuracy: 27.16
Round  10, Train loss: 2.264, Test loss: 2.278, Test accuracy: 21.79
Round  10: Global train loss: 2.264, Global test loss: 2.299, Global test accuracy: 30.32
Round  11, Train loss: 2.295, Test loss: 2.280, Test accuracy: 21.76
Round  11: Global train loss: 2.295, Global test loss: 2.299, Global test accuracy: 30.60
Round  12, Train loss: 2.193, Test loss: 2.261, Test accuracy: 24.03
Round  12: Global train loss: 2.193, Global test loss: 2.298, Global test accuracy: 33.77
Round  13, Train loss: 2.271, Test loss: 2.278, Test accuracy: 19.82
Round  13: Global train loss: 2.271, Global test loss: 2.299, Global test accuracy: 32.02
Round  14, Train loss: 2.263, Test loss: 2.277, Test accuracy: 18.14
Round  14: Global train loss: 2.263, Global test loss: 2.299, Global test accuracy: 29.07
Round  15, Train loss: 2.284, Test loss: 2.285, Test accuracy: 17.04
Round  15: Global train loss: 2.284, Global test loss: 2.300, Global test accuracy: 23.02
Round  16, Train loss: 2.226, Test loss: 2.287, Test accuracy: 16.78
Round  16: Global train loss: 2.226, Global test loss: 2.301, Global test accuracy: 20.32
Round  17, Train loss: 2.227, Test loss: 2.276, Test accuracy: 16.37
Round  17: Global train loss: 2.227, Global test loss: 2.301, Global test accuracy: 20.68
Round  18, Train loss: 2.168, Test loss: 2.280, Test accuracy: 14.11
Round  18: Global train loss: 2.168, Global test loss: 2.302, Global test accuracy: 13.91
Round  19, Train loss: 2.137, Test loss: 2.258, Test accuracy: 16.70
Round  19: Global train loss: 2.137, Global test loss: 2.301, Global test accuracy: 17.83
Round  20, Train loss: 1.919, Test loss: 2.247, Test accuracy: 18.55
Round  20: Global train loss: 1.919, Global test loss: 2.301, Global test accuracy: 21.68
Round  21, Train loss: 1.806, Test loss: 2.204, Test accuracy: 24.86
Round  21: Global train loss: 1.806, Global test loss: 2.299, Global test accuracy: 30.50
Round  22, Train loss: 2.148, Test loss: 2.200, Test accuracy: 25.98
Round  22: Global train loss: 2.148, Global test loss: 2.299, Global test accuracy: 33.29
Round  23, Train loss: 1.706, Test loss: 2.161, Test accuracy: 32.19
Round  23: Global train loss: 1.706, Global test loss: 2.297, Global test accuracy: 40.07
Round  24, Train loss: 1.537, Test loss: 2.140, Test accuracy: 34.28
Round  24: Global train loss: 1.537, Global test loss: 2.296, Global test accuracy: 46.82
Round  25, Train loss: 1.526, Test loss: 2.119, Test accuracy: 36.64
Round  25: Global train loss: 1.526, Global test loss: 2.294, Global test accuracy: 50.43
Round  26, Train loss: 0.817, Test loss: 2.054, Test accuracy: 43.57
Round  26: Global train loss: 0.817, Global test loss: 2.287, Global test accuracy: 54.35
Round  27, Train loss: 1.507, Test loss: 2.083, Test accuracy: 39.77
Round  27: Global train loss: 1.507, Global test loss: 2.286, Global test accuracy: 54.67
Round  28, Train loss: 1.579, Test loss: 2.180, Test accuracy: 28.88
Round  28: Global train loss: 1.579, Global test loss: 2.293, Global test accuracy: 48.52
Round  29, Train loss: 0.883, Test loss: 2.119, Test accuracy: 36.07
Round  29: Global train loss: 0.883, Global test loss: 2.291, Global test accuracy: 50.47
Round  30, Train loss: 0.860, Test loss: 2.082, Test accuracy: 38.11
Round  30: Global train loss: 0.860, Global test loss: 2.286, Global test accuracy: 55.30
Round  31, Train loss: 1.473, Test loss: 2.117, Test accuracy: 34.02
Round  31: Global train loss: 1.473, Global test loss: 2.290, Global test accuracy: 53.53
Round  32, Train loss: 1.725, Test loss: 2.224, Test accuracy: 22.11
Round  32: Global train loss: 1.725, Global test loss: 2.298, Global test accuracy: 30.76
Round  33, Train loss: 1.103, Test loss: 2.104, Test accuracy: 37.78
Round  33: Global train loss: 1.103, Global test loss: 2.292, Global test accuracy: 49.97
Round  34, Train loss: 0.866, Test loss: 2.110, Test accuracy: 37.60
Round  34: Global train loss: 0.866, Global test loss: 2.292, Global test accuracy: 53.71
Round  35, Train loss: 1.290, Test loss: 2.107, Test accuracy: 37.27
Round  35: Global train loss: 1.290, Global test loss: 2.292, Global test accuracy: 52.42
Round  36, Train loss: 1.336, Test loss: 2.098, Test accuracy: 37.88
Round  36: Global train loss: 1.336, Global test loss: 2.291, Global test accuracy: 54.91
Round  37, Train loss: 0.756, Test loss: 2.059, Test accuracy: 40.47
Round  37: Global train loss: 0.756, Global test loss: 2.289, Global test accuracy: 57.16
Round  38, Train loss: 0.589, Test loss: 2.080, Test accuracy: 38.12
Round  38: Global train loss: 0.589, Global test loss: 2.289, Global test accuracy: 58.37
Round  39, Train loss: -0.705, Test loss: 1.981, Test accuracy: 48.29
Round  39: Global train loss: -0.705, Global test loss: 2.272, Global test accuracy: 66.29
Round  40, Train loss: 1.091, Test loss: 2.034, Test accuracy: 43.07
Round  40: Global train loss: 1.091, Global test loss: 2.277, Global test accuracy: 66.27
Round  41, Train loss: 0.534, Test loss: 2.036, Test accuracy: 44.46
Round  41: Global train loss: 0.534, Global test loss: 2.274, Global test accuracy: 66.74
Round  42, Train loss: 1.604, Test loss: 2.124, Test accuracy: 36.54
Round  42: Global train loss: 1.604, Global test loss: 2.285, Global test accuracy: 56.93
Round  43, Train loss: -0.240, Test loss: 2.052, Test accuracy: 43.10
Round  43: Global train loss: -0.240, Global test loss: 2.274, Global test accuracy: 65.47
Round  44, Train loss: 0.264, Test loss: 2.056, Test accuracy: 40.85
Round  44: Global train loss: 0.264, Global test loss: 2.271, Global test accuracy: 63.00
Round  45, Train loss: -0.097, Test loss: 2.045, Test accuracy: 43.02
Round  45: Global train loss: -0.097, Global test loss: 2.266, Global test accuracy: 63.57
Round  46, Train loss: 0.718, Test loss: 2.063, Test accuracy: 39.54
Round  46: Global train loss: 0.718, Global test loss: 2.272, Global test accuracy: 62.87
Round  47, Train loss: 0.064, Test loss: 2.054, Test accuracy: 40.29
Round  47: Global train loss: 0.064, Global test loss: 2.272, Global test accuracy: 63.77
Round  48, Train loss: 1.234, Test loss: 2.112, Test accuracy: 34.31
Round  48: Global train loss: 1.234, Global test loss: 2.285, Global test accuracy: 59.09
Round  49, Train loss: 0.057, Test loss: 2.078, Test accuracy: 38.14
Round  49: Global train loss: 0.057, Global test loss: 2.285, Global test accuracy: 58.68
Round  50, Train loss: -0.247, Test loss: 2.009, Test accuracy: 45.27
Round  50: Global train loss: -0.247, Global test loss: 2.276, Global test accuracy: 60.04
Round  51, Train loss: -1.780, Test loss: 1.944, Test accuracy: 51.99
Round  51: Global train loss: -1.780, Global test loss: 2.250, Global test accuracy: 61.94
Round  52, Train loss: -2.026, Test loss: 1.932, Test accuracy: 54.21
Round  52: Global train loss: -2.026, Global test loss: 2.209, Global test accuracy: 64.28
Round  53, Train loss: 0.546, Test loss: 1.989, Test accuracy: 49.15
Round  53: Global train loss: 0.546, Global test loss: 2.213, Global test accuracy: 62.24
Round  54, Train loss: -0.675, Test loss: 1.985, Test accuracy: 50.77
Round  54: Global train loss: -0.675, Global test loss: 2.206, Global test accuracy: 63.61
Round  55, Train loss: 0.263, Test loss: 2.018, Test accuracy: 47.20
Round  55: Global train loss: 0.263, Global test loss: 2.208, Global test accuracy: 61.10
Round  56, Train loss: 0.489, Test loss: 2.021, Test accuracy: 45.43
Round  56: Global train loss: 0.489, Global test loss: 2.211, Global test accuracy: 59.38
Round  57, Train loss: 0.454, Test loss: 2.063, Test accuracy: 40.36
Round  57: Global train loss: 0.454, Global test loss: 2.234, Global test accuracy: 57.58
Round  58, Train loss: -0.534, Test loss: 2.044, Test accuracy: 42.61
Round  58: Global train loss: -0.534, Global test loss: 2.237, Global test accuracy: 58.32
Round  59, Train loss: 0.027, Test loss: 2.014, Test accuracy: 45.42
Round  59: Global train loss: 0.027, Global test loss: 2.236, Global test accuracy: 57.52
Round  60, Train loss: -1.098, Test loss: 2.012, Test accuracy: 44.56
Round  60: Global train loss: -1.098, Global test loss: 2.234, Global test accuracy: 56.49
Round  61, Train loss: -0.489, Test loss: 1.993, Test accuracy: 47.98
Round  61: Global train loss: -0.489, Global test loss: 2.236, Global test accuracy: 55.11
Round  62, Train loss: -0.838, Test loss: 1.946, Test accuracy: 52.85
Round  62: Global train loss: -0.838, Global test loss: 2.217, Global test accuracy: 55.94
Round  63, Train loss: -3.267, Test loss: 1.880, Test accuracy: 59.80
Round  63: Global train loss: -3.267, Global test loss: 2.165, Global test accuracy: 60.35
Round  64, Train loss: -1.952, Test loss: 1.911, Test accuracy: 56.08
Round  64: Global train loss: -1.952, Global test loss: 2.154, Global test accuracy: 60.05
Round  65, Train loss: -2.432, Test loss: 1.868, Test accuracy: 60.19
Round  65: Global train loss: -2.432, Global test loss: 2.116, Global test accuracy: 61.12
Round  66, Train loss: 0.056, Test loss: 1.907, Test accuracy: 56.27
Round  66: Global train loss: 0.056, Global test loss: 2.136, Global test accuracy: 59.86
Round  67, Train loss: -2.553, Test loss: 1.838, Test accuracy: 63.41
Round  67: Global train loss: -2.553, Global test loss: 2.071, Global test accuracy: 63.68
Round  68, Train loss: -0.347, Test loss: 1.854, Test accuracy: 62.13
Round  68: Global train loss: -0.347, Global test loss: 2.045, Global test accuracy: 64.08
Round  69, Train loss: -0.427, Test loss: 1.858, Test accuracy: 62.38
Round  69: Global train loss: -0.427, Global test loss: 2.052, Global test accuracy: 62.62
Round  70, Train loss: -0.350, Test loss: 1.881, Test accuracy: 61.54
Round  70: Global train loss: -0.350, Global test loss: 2.071, Global test accuracy: 55.88
Round  71, Train loss: -1.063, Test loss: 1.844, Test accuracy: 64.84
Round  71: Global train loss: -1.063, Global test loss: 2.045, Global test accuracy: 58.44
Round  72, Train loss: -1.961, Test loss: 1.794, Test accuracy: 68.74
Round  72: Global train loss: -1.961, Global test loss: 2.006, Global test accuracy: 62.12
Round  73, Train loss: -0.787, Test loss: 1.809, Test accuracy: 67.14
Round  73: Global train loss: -0.787, Global test loss: 1.998, Global test accuracy: 61.66
Round  74, Train loss: -0.056, Test loss: 1.792, Test accuracy: 68.28
Round  74: Global train loss: -0.056, Global test loss: 1.988, Global test accuracy: 62.52
Round  75, Train loss: -1.261, Test loss: 1.759, Test accuracy: 72.10
Round  75: Global train loss: -1.261, Global test loss: 1.957, Global test accuracy: 63.92
Round  76, Train loss: -0.647, Test loss: 1.770, Test accuracy: 70.83
Round  76: Global train loss: -0.647, Global test loss: 1.950, Global test accuracy: 62.55
Round  77, Train loss: -0.189, Test loss: 1.772, Test accuracy: 70.42
Round  77: Global train loss: -0.189, Global test loss: 1.948, Global test accuracy: 62.94
Round  78, Train loss: -1.924, Test loss: 1.724, Test accuracy: 74.52
Round  78: Global train loss: -1.924, Global test loss: 1.922, Global test accuracy: 66.05
Round  79, Train loss: 0.069, Test loss: 1.736, Test accuracy: 73.54
Round  79: Global train loss: 0.069, Global test loss: 1.923, Global test accuracy: 66.28
Round  80, Train loss: -0.342, Test loss: 1.725, Test accuracy: 74.87
Round  80: Global train loss: -0.342, Global test loss: 1.912, Global test accuracy: 67.64
Round  81, Train loss: -1.893, Test loss: 1.703, Test accuracy: 76.58
Round  81: Global train loss: -1.893, Global test loss: 1.886, Global test accuracy: 69.17
Round  82, Train loss: -0.098, Test loss: 1.702, Test accuracy: 76.45
Round  82: Global train loss: -0.098, Global test loss: 1.883, Global test accuracy: 69.33
Round  83, Train loss: -0.707, Test loss: 1.692, Test accuracy: 77.47
Round  83: Global train loss: -0.707, Global test loss: 1.868, Global test accuracy: 70.53
Round  84, Train loss: -0.432, Test loss: 1.696, Test accuracy: 77.29
Round  84: Global train loss: -0.432, Global test loss: 1.863, Global test accuracy: 71.86
Round  85, Train loss: -2.189, Test loss: 1.682, Test accuracy: 78.44
Round  85: Global train loss: -2.189, Global test loss: 1.842, Global test accuracy: 72.69
Round  86, Train loss: -0.524, Test loss: 1.685, Test accuracy: 78.38
Round  86: Global train loss: -0.524, Global test loss: 1.849, Global test accuracy: 72.94
Round  87, Train loss: -0.887, Test loss: 1.684, Test accuracy: 78.28
Round  87: Global train loss: -0.887, Global test loss: 1.841, Global test accuracy: 73.70
Round  88, Train loss: -0.796, Test loss: 1.684, Test accuracy: 78.36
Round  88: Global train loss: -0.796, Global test loss: 1.827, Global test accuracy: 75.24
Round  89, Train loss: -1.006, Test loss: 1.688, Test accuracy: 77.87
Round  89: Global train loss: -1.006, Global test loss: 1.817, Global test accuracy: 75.95
Round  90, Train loss: 0.071, Test loss: 1.685, Test accuracy: 78.16
Round  90: Global train loss: 0.071, Global test loss: 1.811, Global test accuracy: 75.94
Round  91, Train loss: -1.596, Test loss: 1.676, Test accuracy: 78.85
Round  91: Global train loss: -1.596, Global test loss: 1.797, Global test accuracy: 76.27
Round  92, Train loss: -0.512, Test loss: 1.672, Test accuracy: 79.18
Round  92: Global train loss: -0.512, Global test loss: 1.787, Global test accuracy: 76.68
Round  93, Train loss: -1.120, Test loss: 1.670, Test accuracy: 79.34
Round  93: Global train loss: -1.120, Global test loss: 1.782, Global test accuracy: 77.02
Round  94, Train loss: -1.292, Test loss: 1.671, Test accuracy: 79.25
Round  94: Global train loss: -1.292, Global test loss: 1.771, Global test accuracy: 77.45
Round  95, Train loss: -0.582, Test loss: 1.668, Test accuracy: 79.59
Round  95: Global train loss: -0.582, Global test loss: 1.768, Global test accuracy: 77.67
Round  96, Train loss: -0.877, Test loss: 1.666, Test accuracy: 79.71
Round  96: Global train loss: -0.877, Global test loss: 1.761, Global test accuracy: 77.97
Round  97, Train loss: -0.824, Test loss: 1.668, Test accuracy: 79.58
Round  97: Global train loss: -0.824, Global test loss: 1.756, Global test accuracy: 78.33
Round  98, Train loss: -0.679, Test loss: 1.668, Test accuracy: 79.61
Round  98: Global train loss: -0.679, Global test loss: 1.751, Global test accuracy: 78.63
Round  99, Train loss: -0.451, Test loss: 1.666, Test accuracy: 79.92/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99: Global train loss: -0.451, Global test loss: 1.746, Global test accuracy: 78.99
Final Round: Train loss: 1.662, Test loss: 1.667, Test accuracy: 81.38
Final Round: Global train loss: 1.662, Global test loss: 1.731, Global test accuracy: 79.78
Average accuracy final 10 rounds: 79.31916666666666
Average global accuracy final 10 rounds: 77.49666666666667
1226.409749031067
[]
[12.333333333333334, 18.491666666666667, 18.233333333333334, 20.816666666666666, 17.375, 17.883333333333333, 22.258333333333333, 22.283333333333335, 19.675, 21.175, 21.791666666666668, 21.758333333333333, 24.033333333333335, 19.825, 18.141666666666666, 17.041666666666668, 16.783333333333335, 16.366666666666667, 14.108333333333333, 16.7, 18.55, 24.858333333333334, 25.975, 32.19166666666667, 34.28333333333333, 36.641666666666666, 43.56666666666667, 39.766666666666666, 28.875, 36.06666666666667, 38.108333333333334, 34.016666666666666, 22.108333333333334, 37.78333333333333, 37.6, 37.275, 37.875, 40.46666666666667, 38.11666666666667, 48.291666666666664, 43.06666666666667, 44.458333333333336, 36.541666666666664, 43.1, 40.85, 43.016666666666666, 39.541666666666664, 40.291666666666664, 34.30833333333333, 38.141666666666666, 45.266666666666666, 51.99166666666667, 54.208333333333336, 49.15, 50.766666666666666, 47.2, 45.43333333333333, 40.358333333333334, 42.608333333333334, 45.416666666666664, 44.55833333333333, 47.975, 52.85, 59.8, 56.075, 60.19166666666667, 56.275, 63.40833333333333, 62.13333333333333, 62.375, 61.541666666666664, 64.84166666666667, 68.74166666666666, 67.14166666666667, 68.28333333333333, 72.1, 70.83333333333333, 70.41666666666667, 74.51666666666667, 73.54166666666667, 74.86666666666666, 76.58333333333333, 76.45, 77.475, 77.29166666666667, 78.44166666666666, 78.38333333333334, 78.275, 78.35833333333333, 77.86666666666666, 78.15833333333333, 78.85, 79.18333333333334, 79.34166666666667, 79.25, 79.59166666666667, 79.70833333333333, 79.58333333333333, 79.60833333333333, 79.91666666666667, 81.38333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.88 

Round   0, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.89 

Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.92 

Round   1, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.84 

Round   2, Train loss: 2.303, Test loss: 2.302, Test accuracy: 11.92 

Round   2, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 11.84 

Round   3, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.95 

Round   3, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.92 

Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.99 

Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.03 

Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.06 

Round   5, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.16 

Round   6, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.09 

Round   6, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.21 

Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.15 

Round   7, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.27 

Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.16 

Round   8, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.27 

Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.18 

Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.33 

Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.24 

Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.44 

Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.34 

Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.53 

Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.46 

Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.64 

Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.54 

Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.72 

Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.61 

Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.78 

Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.74 

Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89 

Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.77 

Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.93 

Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88 

Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.93 

Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.91 

Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.03 

Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.96 

Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.05 

Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.99 

Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.12 

Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.09 

Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.33 

Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.15 

Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.35 

Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.21 

Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.43 

Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.44 

Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.63 

Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.51 

Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.66 

Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.58 

Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.66 

Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.62 

Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.72 

Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.62 

Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.66 

Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.62 

Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.62 

Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.62 

Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.53 

Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.65 

Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.63 

Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.67 

Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.65 

Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.69 

Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.84 

Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.73 

Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.77 

Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.77 

Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.86 

Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.79 

Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.88 

Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.81 

Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.89 

Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.82 

Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.91 

Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.85 

Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.93 

Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.87 

Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.96 

Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.89 

Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.97 

Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.86 

Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.96 

Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.95 

Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.07 

Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.98 

Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.07 

Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.98 

Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.08 

Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.99 

Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.07 

Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.09 

Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.10 

Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.07 

Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.17 

Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.12 

Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.28 

Round  50, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.14 

Round  50, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.25 

Round  51, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.18 

Round  51, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.26 

Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.14 

Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.14 

Round  53, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.16 

Round  53, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.16 

Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.18 

Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.29 

Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.18 

Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.32 

Round  56, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.23 

Round  56, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.43 

Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.28 

Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.50 

Round  58, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.38 

Round  58, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.51 

Round  59, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.37 

Round  59, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.49 

Round  60, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.40 

Round  60, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.50 

Round  61, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.40 

Round  61, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.52 

Round  62, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.50 

Round  62, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.57 

Round  63, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.54 

Round  63, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.61 

Round  64, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.61 

Round  64, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.66 

Round  65, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.67 

Round  65, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.73 

Round  66, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.72 

Round  66, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.79 

Round  67, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.75 

Round  67, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 14.87 

Round  68, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.72 

Round  68, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.90 

Round  69, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.78 

Round  69, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.89 

Round  70, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.84 

Round  70, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.97 

Round  71, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.96 

Round  71, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.07 

Round  72, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.02 

Round  72, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.12 

Round  73, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.00 

Round  73, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.12 

Round  74, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.97 

Round  74, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.08 

Round  75, Train loss: 2.301, Test loss: 2.302, Test accuracy: 14.95 

Round  75, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.12 

Round  76, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.01 

Round  76, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.12 

Round  77, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.08 

Round  77, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.18 

Round  78, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.09 

Round  78, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.14 

Round  79, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.12 

Round  79, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.18 

Round  80, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.13 

Round  80, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.16 

Round  81, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.21 

Round  81, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.18 

Round  82, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.14 

Round  82, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.16 

Round  83, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.21 

Round  83, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.41 

Round  84, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.32 

Round  84, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.49 

Round  85, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.40 

Round  85, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.62 

Round  86, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.41 

Round  86, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.62 

Round  87, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.49 

Round  87, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.66 

Round  88, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.52 

Round  88, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.67 

Round  89, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.55 

Round  89, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.70 

Round  90, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.56 

Round  90, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.68 

Round  91, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.62 

Round  91, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.68 

Round  92, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.62 

Round  92, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.78 

Round  93, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.65 

Round  93, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.80 

Round  94, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.71 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.82 

Round  95, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.72 

Round  95, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.82 

Round  96, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.72 

Round  96, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.82 

Round  97, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.76 

Round  97, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 15.82 

Round  98, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.77 

Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.85 

Round  99, Train loss: 2.301, Test loss: 2.302, Test accuracy: 15.85 

Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.88 

Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 15.91 

Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 15.88 

Average accuracy final 10 rounds: 15.696666666666665 

Average global accuracy final 10 rounds: 15.796666666666665 

1069.9331767559052
[0.9948136806488037, 1.9044773578643799, 2.8056466579437256, 3.709818124771118, 4.617498874664307, 5.51695990562439, 6.425658464431763, 7.3310866355896, 8.232603311538696, 9.130309820175171, 10.02309775352478, 10.917522192001343, 11.819297552108765, 12.713798522949219, 13.609523057937622, 14.511911153793335, 15.405327796936035, 16.30492925643921, 17.207752227783203, 18.102900505065918, 18.994044065475464, 19.87519884109497, 20.770816326141357, 21.670939922332764, 22.56002140045166, 23.4436297416687, 24.33533787727356, 25.222499132156372, 26.116400718688965, 27.00336480140686, 27.903281688690186, 28.801918268203735, 29.699143409729004, 30.59444570541382, 31.49195957183838, 32.38708972930908, 33.28420662879944, 34.182488679885864, 35.08455944061279, 35.98224759101868, 36.877622842788696, 37.77047514915466, 38.668336391448975, 39.56378626823425, 40.4574134349823, 41.351677656173706, 42.249478816986084, 43.14258027076721, 44.03402376174927, 44.93423914909363, 45.83326768875122, 46.72789549827576, 47.61736345291138, 48.511921882629395, 49.40498900413513, 50.29779839515686, 51.179293394088745, 52.07563924789429, 52.97710871696472, 53.85505127906799, 54.74189043045044, 55.6303985118866, 56.5206139087677, 57.403796911239624, 58.26931977272034, 59.151588678359985, 60.027730226516724, 60.89597034454346, 61.77942943572998, 62.66111779212952, 63.55032229423523, 64.4267828464508, 65.29331350326538, 66.17112445831299, 67.06339764595032, 67.93639802932739, 68.81863355636597, 69.69332885742188, 70.58620476722717, 71.47169256210327, 72.35263085365295, 73.24271893501282, 74.12374138832092, 75.02087759971619, 75.90391206741333, 76.7930679321289, 77.68400549888611, 78.57566666603088, 79.46834015846252, 80.36047530174255, 81.25004434585571, 82.13960456848145, 83.02544474601746, 83.90783476829529, 84.79281759262085, 85.66969680786133, 86.55340313911438, 87.43167781829834, 88.31696844100952, 89.20176100730896, 90.97389841079712]
[11.883333333333333, 11.916666666666666, 11.916666666666666, 11.95, 11.991666666666667, 12.058333333333334, 12.091666666666667, 12.15, 12.158333333333333, 12.175, 12.241666666666667, 12.341666666666667, 12.458333333333334, 12.541666666666666, 12.608333333333333, 12.741666666666667, 12.766666666666667, 12.875, 12.908333333333333, 12.958333333333334, 12.991666666666667, 13.091666666666667, 13.15, 13.208333333333334, 13.441666666666666, 13.508333333333333, 13.583333333333334, 13.625, 13.625, 13.625, 13.616666666666667, 13.65, 13.666666666666666, 13.691666666666666, 13.733333333333333, 13.766666666666667, 13.791666666666666, 13.808333333333334, 13.816666666666666, 13.85, 13.866666666666667, 13.891666666666667, 13.858333333333333, 13.95, 13.983333333333333, 13.983333333333333, 13.991666666666667, 14.091666666666667, 14.066666666666666, 14.116666666666667, 14.141666666666667, 14.175, 14.141666666666667, 14.158333333333333, 14.175, 14.183333333333334, 14.233333333333333, 14.283333333333333, 14.375, 14.366666666666667, 14.4, 14.4, 14.5, 14.541666666666666, 14.608333333333333, 14.666666666666666, 14.725, 14.75, 14.716666666666667, 14.783333333333333, 14.841666666666667, 14.958333333333334, 15.016666666666667, 15.0, 14.966666666666667, 14.95, 15.008333333333333, 15.083333333333334, 15.091666666666667, 15.116666666666667, 15.133333333333333, 15.208333333333334, 15.141666666666667, 15.208333333333334, 15.316666666666666, 15.4, 15.408333333333333, 15.491666666666667, 15.516666666666667, 15.55, 15.558333333333334, 15.625, 15.616666666666667, 15.65, 15.708333333333334, 15.716666666666667, 15.716666666666667, 15.758333333333333, 15.766666666666667, 15.85, 15.908333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.299, Test accuracy: 11.69
Round   1, Train loss: 2.298, Test loss: 2.294, Test accuracy: 26.81
Round   2, Train loss: 2.293, Test loss: 2.280, Test accuracy: 25.44
Round   3, Train loss: 2.282, Test loss: 2.203, Test accuracy: 29.62
Round   4, Train loss: 2.213, Test loss: 2.037, Test accuracy: 46.44
Round   5, Train loss: 2.031, Test loss: 1.866, Test accuracy: 64.92
Round   6, Train loss: 1.897, Test loss: 1.818, Test accuracy: 66.38
Round   7, Train loss: 1.865, Test loss: 1.762, Test accuracy: 73.38
Round   8, Train loss: 1.784, Test loss: 1.684, Test accuracy: 81.71
Round   9, Train loss: 1.734, Test loss: 1.658, Test accuracy: 82.61
Round  10, Train loss: 1.680, Test loss: 1.649, Test accuracy: 83.10
Round  11, Train loss: 1.684, Test loss: 1.638, Test accuracy: 83.75
Round  12, Train loss: 1.711, Test loss: 1.613, Test accuracy: 86.87
Round  13, Train loss: 1.612, Test loss: 1.594, Test accuracy: 88.33
Round  14, Train loss: 1.593, Test loss: 1.583, Test accuracy: 89.22
Round  15, Train loss: 1.610, Test loss: 1.574, Test accuracy: 89.94
Round  16, Train loss: 1.578, Test loss: 1.569, Test accuracy: 90.13
Round  17, Train loss: 1.565, Test loss: 1.565, Test accuracy: 90.56
Round  18, Train loss: 1.549, Test loss: 1.562, Test accuracy: 90.81
Round  19, Train loss: 1.540, Test loss: 1.560, Test accuracy: 90.97
Round  20, Train loss: 1.546, Test loss: 1.557, Test accuracy: 91.28
Round  21, Train loss: 1.549, Test loss: 1.554, Test accuracy: 91.44
Round  22, Train loss: 1.540, Test loss: 1.554, Test accuracy: 91.45
Round  23, Train loss: 1.531, Test loss: 1.551, Test accuracy: 91.41
Round  24, Train loss: 1.529, Test loss: 1.549, Test accuracy: 91.77
Round  25, Train loss: 1.528, Test loss: 1.548, Test accuracy: 91.77
Round  26, Train loss: 1.525, Test loss: 1.549, Test accuracy: 91.59
Round  27, Train loss: 1.520, Test loss: 1.548, Test accuracy: 91.65
Round  28, Train loss: 1.520, Test loss: 1.546, Test accuracy: 91.91
Round  29, Train loss: 1.527, Test loss: 1.545, Test accuracy: 92.20
Round  30, Train loss: 1.519, Test loss: 1.545, Test accuracy: 92.03
Round  31, Train loss: 1.513, Test loss: 1.542, Test accuracy: 92.35
Round  32, Train loss: 1.518, Test loss: 1.543, Test accuracy: 92.21
Round  33, Train loss: 1.516, Test loss: 1.542, Test accuracy: 92.42
Round  34, Train loss: 1.513, Test loss: 1.542, Test accuracy: 92.30
Round  35, Train loss: 1.510, Test loss: 1.541, Test accuracy: 92.42
Round  36, Train loss: 1.507, Test loss: 1.540, Test accuracy: 92.62
Round  37, Train loss: 1.508, Test loss: 1.538, Test accuracy: 92.74
Round  38, Train loss: 1.506, Test loss: 1.537, Test accuracy: 92.69
Round  39, Train loss: 1.508, Test loss: 1.537, Test accuracy: 92.77
Round  40, Train loss: 1.504, Test loss: 1.537, Test accuracy: 92.83
Round  41, Train loss: 1.507, Test loss: 1.536, Test accuracy: 92.83
Round  42, Train loss: 1.503, Test loss: 1.536, Test accuracy: 92.89
Round  43, Train loss: 1.505, Test loss: 1.536, Test accuracy: 92.99
Round  44, Train loss: 1.500, Test loss: 1.536, Test accuracy: 92.96
Round  45, Train loss: 1.500, Test loss: 1.535, Test accuracy: 92.97
Round  46, Train loss: 1.500, Test loss: 1.534, Test accuracy: 93.28
Round  47, Train loss: 1.499, Test loss: 1.533, Test accuracy: 93.12
Round  48, Train loss: 1.498, Test loss: 1.533, Test accuracy: 93.28
Round  49, Train loss: 1.502, Test loss: 1.533, Test accuracy: 93.28
Round  50, Train loss: 1.496, Test loss: 1.532, Test accuracy: 93.28
Round  51, Train loss: 1.496, Test loss: 1.531, Test accuracy: 93.23
Round  52, Train loss: 1.497, Test loss: 1.531, Test accuracy: 93.37
Round  53, Train loss: 1.494, Test loss: 1.530, Test accuracy: 93.41
Round  54, Train loss: 1.498, Test loss: 1.530, Test accuracy: 93.53
Round  55, Train loss: 1.497, Test loss: 1.531, Test accuracy: 93.38
Round  56, Train loss: 1.496, Test loss: 1.529, Test accuracy: 93.53
Round  57, Train loss: 1.496, Test loss: 1.531, Test accuracy: 93.60
Round  58, Train loss: 1.498, Test loss: 1.530, Test accuracy: 93.46
Round  59, Train loss: 1.491, Test loss: 1.529, Test accuracy: 93.55
Round  60, Train loss: 1.494, Test loss: 1.529, Test accuracy: 93.45
Round  61, Train loss: 1.493, Test loss: 1.528, Test accuracy: 93.61
Round  62, Train loss: 1.493, Test loss: 1.528, Test accuracy: 93.52
Round  63, Train loss: 1.493, Test loss: 1.527, Test accuracy: 93.72
Round  64, Train loss: 1.490, Test loss: 1.528, Test accuracy: 93.82
Round  65, Train loss: 1.490, Test loss: 1.528, Test accuracy: 93.65
Round  66, Train loss: 1.492, Test loss: 1.527, Test accuracy: 93.73
Round  67, Train loss: 1.494, Test loss: 1.527, Test accuracy: 93.79
Round  68, Train loss: 1.492, Test loss: 1.526, Test accuracy: 93.92
Round  69, Train loss: 1.490, Test loss: 1.526, Test accuracy: 94.01
Round  70, Train loss: 1.489, Test loss: 1.525, Test accuracy: 94.05
Round  71, Train loss: 1.490, Test loss: 1.525, Test accuracy: 93.78
Round  72, Train loss: 1.491, Test loss: 1.525, Test accuracy: 94.03
Round  73, Train loss: 1.491, Test loss: 1.525, Test accuracy: 94.01
Round  74, Train loss: 1.489, Test loss: 1.524, Test accuracy: 93.99
Round  75, Train loss: 1.485, Test loss: 1.524, Test accuracy: 94.08
Round  76, Train loss: 1.491, Test loss: 1.524, Test accuracy: 94.11
Round  77, Train loss: 1.487, Test loss: 1.523, Test accuracy: 94.23
Round  78, Train loss: 1.488, Test loss: 1.523, Test accuracy: 94.26
Round  79, Train loss: 1.489, Test loss: 1.524, Test accuracy: 94.12
Round  80, Train loss: 1.487, Test loss: 1.523, Test accuracy: 94.12
Round  81, Train loss: 1.486, Test loss: 1.524, Test accuracy: 94.14
Round  82, Train loss: 1.486, Test loss: 1.523, Test accuracy: 94.08
Round  83, Train loss: 1.486, Test loss: 1.523, Test accuracy: 94.25
Round  84, Train loss: 1.488, Test loss: 1.522, Test accuracy: 94.30
Round  85, Train loss: 1.486, Test loss: 1.522, Test accuracy: 94.29
Round  86, Train loss: 1.483, Test loss: 1.522, Test accuracy: 94.28
Round  87, Train loss: 1.485, Test loss: 1.522, Test accuracy: 94.17
Round  88, Train loss: 1.485, Test loss: 1.522, Test accuracy: 94.23
Round  89, Train loss: 1.483, Test loss: 1.522, Test accuracy: 94.13
Round  90, Train loss: 1.485, Test loss: 1.522, Test accuracy: 94.22
Round  91, Train loss: 1.486, Test loss: 1.522, Test accuracy: 94.21
Round  92, Train loss: 1.483, Test loss: 1.521, Test accuracy: 94.33
Round  93, Train loss: 1.485, Test loss: 1.521, Test accuracy: 94.34
Round  94, Train loss: 1.484, Test loss: 1.522, Test accuracy: 94.24
Round  95, Train loss: 1.483, Test loss: 1.521, Test accuracy: 94.22
Round  96, Train loss: 1.485, Test loss: 1.522, Test accuracy: 94.22
Round  97, Train loss: 1.484, Test loss: 1.520, Test accuracy: 94.33
Round  98, Train loss: 1.484, Test loss: 1.520, Test accuracy: 94.46
Round  99, Train loss: 1.483, Test loss: 1.520, Test accuracy: 94.25
Final Round, Train loss: 1.483, Test loss: 1.520, Test accuracy: 94.33
Average accuracy final 10 rounds: 94.28249999999998
1374.7758951187134
[2.167560577392578, 4.187452554702759, 6.206372261047363, 8.13611626625061, 9.943491220474243, 11.772181987762451, 13.592665433883667, 15.436760902404785, 17.27616286277771, 19.093870401382446, 20.911854028701782, 22.732297897338867, 24.55814242362976, 26.38431692123413, 28.206812620162964, 30.027338981628418, 31.854971170425415, 33.697856187820435, 35.54997277259827, 37.37813901901245, 39.20071816444397, 41.04007935523987, 42.883721113204956, 44.733420610427856, 46.584619998931885, 48.43765997886658, 50.29206037521362, 52.10723614692688, 53.93441128730774, 55.78668999671936, 57.63603591918945, 59.468424558639526, 61.31986904144287, 63.152496337890625, 64.99055433273315, 66.82794094085693, 68.65861868858337, 70.48722243309021, 72.30945110321045, 74.13305735588074, 75.96314215660095, 77.79056596755981, 79.60504674911499, 81.433518409729, 83.26300358772278, 85.11545038223267, 86.96679377555847, 88.82262873649597, 90.67402935028076, 92.52325916290283, 94.3672091960907, 96.2171847820282, 98.04463958740234, 99.86787247657776, 101.69869089126587, 103.55353808403015, 105.38002967834473, 107.20612812042236, 109.03360509872437, 110.86336755752563, 112.68400692939758, 114.49940347671509, 116.33822679519653, 118.19349145889282, 120.04941439628601, 121.878253698349, 123.70870542526245, 125.53497529029846, 127.35242772102356, 129.17893409729004, 131.02926921844482, 132.85492777824402, 134.67797875404358, 136.50734400749207, 138.35346460342407, 140.18091106414795, 142.3080062866211, 144.4326298236847, 146.2587070465088, 148.0845308303833, 149.915531873703, 151.72329950332642, 153.53722476959229, 155.34362626075745, 157.1577010154724, 158.95815896987915, 160.77066588401794, 162.58046746253967, 164.39083075523376, 166.19850039482117, 168.01381993293762, 169.8220202922821, 171.60012221336365, 173.37460494041443, 175.1544268131256, 176.93489265441895, 178.7523193359375, 180.5451443195343, 182.35528755187988, 184.13167524337769, 185.9490625858307]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[11.691666666666666, 26.808333333333334, 25.441666666666666, 29.625, 46.44166666666667, 64.91666666666667, 66.38333333333334, 73.375, 81.70833333333333, 82.60833333333333, 83.1, 83.75, 86.86666666666666, 88.33333333333333, 89.21666666666667, 89.94166666666666, 90.13333333333334, 90.55833333333334, 90.80833333333334, 90.975, 91.28333333333333, 91.44166666666666, 91.45, 91.40833333333333, 91.76666666666667, 91.76666666666667, 91.59166666666667, 91.65, 91.90833333333333, 92.2, 92.025, 92.35, 92.20833333333333, 92.425, 92.3, 92.425, 92.61666666666666, 92.74166666666666, 92.69166666666666, 92.76666666666667, 92.825, 92.83333333333333, 92.89166666666667, 92.99166666666666, 92.95833333333333, 92.975, 93.275, 93.11666666666666, 93.275, 93.275, 93.275, 93.23333333333333, 93.36666666666666, 93.40833333333333, 93.525, 93.38333333333334, 93.525, 93.6, 93.45833333333333, 93.55, 93.45, 93.60833333333333, 93.51666666666667, 93.725, 93.81666666666666, 93.65, 93.73333333333333, 93.79166666666667, 93.91666666666667, 94.00833333333334, 94.05, 93.775, 94.025, 94.00833333333334, 93.99166666666666, 94.075, 94.10833333333333, 94.23333333333333, 94.25833333333334, 94.125, 94.125, 94.14166666666667, 94.075, 94.25, 94.3, 94.29166666666667, 94.275, 94.16666666666667, 94.23333333333333, 94.13333333333334, 94.225, 94.20833333333333, 94.325, 94.34166666666667, 94.24166666666666, 94.21666666666667, 94.225, 94.33333333333333, 94.45833333333333, 94.25, 94.33333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.322, Test loss: 2.302, Test accuracy: 8.77
Round   1, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.67
Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.30
Round   3, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.06
Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.51
Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.42
Round   6, Train loss: 2.303, Test loss: 2.302, Test accuracy: 7.34
Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.85
Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.67
Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.75
Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.73
Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.08
Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.15
Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.23
Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.12
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.87
Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.61
Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 6.71
Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.00
Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.02
Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.01
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.13
Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.26
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.30
Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.35
Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.34
Round  26, Train loss: 2.301, Test loss: 2.302, Test accuracy: 7.69
Round  27, Train loss: 2.301, Test loss: 2.302, Test accuracy: 7.69
Round  28, Train loss: 2.302, Test loss: 2.303, Test accuracy: 7.58
Round  29, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.78
Round  30, Train loss: 2.302, Test loss: 2.303, Test accuracy: 7.41
Round  31, Train loss: 2.302, Test loss: 2.303, Test accuracy: 7.43
Round  32, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.37
Round  33, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.33
Round  34, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.41
Round  35, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.32
Round  36, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.29
Round  37, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.04
Round  38, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.32
Round  39, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.45
Round  40, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.54
Round  41, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.55
Round  42, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.40
Round  43, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.39
Round  44, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.27
Round  45, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.33
Round  46, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.28
Round  47, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.43
Round  48, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.30
Round  49, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.29
Round  50, Train loss: 2.301, Test loss: 2.303, Test accuracy: 7.30
Round  51, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.55
Round  52, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.38
Round  53, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.32
Round  54, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.19
Round  55, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.23
Round  56, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.35
Round  57, Train loss: 2.299, Test loss: 2.303, Test accuracy: 7.36
Round  58, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.45
Round  59, Train loss: 2.300, Test loss: 2.303, Test accuracy: 7.33
Round  60, Train loss: 2.299, Test loss: 2.303, Test accuracy: 7.43
Round  61, Train loss: 2.299, Test loss: 2.303, Test accuracy: 7.64
Round  62, Train loss: 2.299, Test loss: 2.303, Test accuracy: 7.59
Round  63, Train loss: 2.299, Test loss: 2.303, Test accuracy: 7.58
Round  64, Train loss: 2.299, Test loss: 2.304, Test accuracy: 7.62
Round  65, Train loss: 2.300, Test loss: 2.304, Test accuracy: 7.62
Round  66, Train loss: 2.299, Test loss: 2.304, Test accuracy: 7.69
Round  67, Train loss: 2.299, Test loss: 2.304, Test accuracy: 7.83
Round  68, Train loss: 2.298, Test loss: 2.304, Test accuracy: 7.79
Round  69, Train loss: 2.298, Test loss: 2.304, Test accuracy: 7.77
Round  70, Train loss: 2.298, Test loss: 2.304, Test accuracy: 7.80
Round  71, Train loss: 2.298, Test loss: 2.304, Test accuracy: 7.99
Round  72, Train loss: 2.299, Test loss: 2.304, Test accuracy: 8.08
Round  73, Train loss: 2.298, Test loss: 2.304, Test accuracy: 8.13
Round  74, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.21
Round  75, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.35
Round  76, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.37
Round  77, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.42
Round  78, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.59
Round  79, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.57
Round  80, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.61
Round  81, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.68
Round  82, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.74
Round  83, Train loss: 2.298, Test loss: 2.305, Test accuracy: 8.73
Round  84, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.79
Round  85, Train loss: 2.297, Test loss: 2.305, Test accuracy: 8.81
Round  86, Train loss: 2.296, Test loss: 2.305, Test accuracy: 8.87
Round  87, Train loss: 2.297, Test loss: 2.306, Test accuracy: 8.91
Round  88, Train loss: 2.296, Test loss: 2.306, Test accuracy: 8.91
Round  89, Train loss: 2.296, Test loss: 2.306, Test accuracy: 8.95
Round  90, Train loss: 2.296, Test loss: 2.306, Test accuracy: 9.00
Round  91, Train loss: 2.294, Test loss: 2.306, Test accuracy: 8.99
Round  92, Train loss: 2.296, Test loss: 2.306, Test accuracy: 8.95
Round  93, Train loss: 2.297, Test loss: 2.306, Test accuracy: 8.94
Round  94, Train loss: 2.295, Test loss: 2.306, Test accuracy: 8.95
Round  95, Train loss: 2.296, Test loss: 2.307, Test accuracy: 8.98/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 2.296, Test loss: 2.307, Test accuracy: 9.06
Round  97, Train loss: 2.293, Test loss: 2.307, Test accuracy: 9.06
Round  98, Train loss: 2.296, Test loss: 2.307, Test accuracy: 9.06
Round  99, Train loss: 2.294, Test loss: 2.307, Test accuracy: 9.05
Final Round, Train loss: 2.294, Test loss: 2.308, Test accuracy: 9.04
Average accuracy final 10 rounds: 9.0045
1348.3984599113464
[1.0372323989868164, 1.9125072956085205, 2.7745518684387207, 3.6463794708251953, 4.511243581771851, 5.38986873626709, 6.256757974624634, 7.156548976898193, 7.932474374771118, 8.794819355010986, 9.65129566192627, 10.51368761062622, 11.374762296676636, 12.24022364616394, 13.106030225753784, 13.974013805389404, 14.842626333236694, 15.708485126495361, 16.575552463531494, 17.437601804733276, 18.306498527526855, 19.17211604118347, 20.039499521255493, 20.902031898498535, 21.76968026161194, 22.633233308792114, 23.500731945037842, 24.36290168762207, 25.231526374816895, 26.09182596206665, 26.958958625793457, 27.82048773765564, 28.68865990638733, 29.549376487731934, 30.419408321380615, 31.279245615005493, 32.14899182319641, 33.00840640068054, 33.87479281425476, 34.73414659500122, 35.604206562042236, 36.46461892127991, 37.33252811431885, 38.19390630722046, 39.06109595298767, 39.918835163116455, 40.78565979003906, 41.645100116729736, 42.51009678840637, 43.37534999847412, 44.23873043060303, 45.095316648483276, 45.95754909515381, 46.81556510925293, 47.680503606796265, 48.53944659233093, 49.40231657028198, 50.260008573532104, 51.12319493293762, 51.98584032058716, 52.848642110824585, 53.70542311668396, 54.56607532501221, 55.420079708099365, 56.28310036659241, 57.13651132583618, 57.99785614013672, 58.85055112838745, 59.70962429046631, 60.56537342071533, 61.42164969444275, 62.27472805976868, 63.13483953475952, 63.98958230018616, 64.85216999053955, 65.70507311820984, 66.56238341331482, 67.42151498794556, 68.28436779975891, 69.14706325531006, 70.0064845085144, 70.87285852432251, 71.73210263252258, 72.59786462783813, 73.4579131603241, 74.33195090293884, 75.1945686340332, 76.06310057640076, 76.92774200439453, 77.79499506950378, 78.65703582763672, 79.5246012210846, 80.38877606391907, 81.25526309013367, 82.11486768722534, 82.98404717445374, 83.84589171409607, 84.71115970611572, 85.57690834999084, 86.44857406616211, 87.7477195262909]
[8.7725, 8.67, 8.2975, 8.06, 7.51, 7.4225, 7.345, 6.8475, 6.6725, 6.7525, 6.73, 7.0825, 7.1475, 7.2275, 7.125, 6.87, 6.6125, 6.715, 7.0, 7.0175, 7.01, 7.1325, 7.26, 7.305, 7.3475, 7.34, 7.6875, 7.6875, 7.585, 7.7775, 7.405, 7.435, 7.365, 7.3325, 7.405, 7.315, 7.2925, 7.0375, 7.3225, 7.445, 7.535, 7.5475, 7.3975, 7.39, 7.2725, 7.33, 7.275, 7.4325, 7.305, 7.2875, 7.3025, 7.5475, 7.385, 7.315, 7.19, 7.23, 7.3525, 7.3625, 7.45, 7.33, 7.4275, 7.645, 7.5875, 7.585, 7.625, 7.6175, 7.69, 7.835, 7.7925, 7.7675, 7.7975, 7.985, 8.0825, 8.13, 8.2125, 8.345, 8.3725, 8.4225, 8.585, 8.5725, 8.6075, 8.68, 8.745, 8.735, 8.795, 8.815, 8.8725, 8.915, 8.9125, 8.95, 8.9975, 8.9875, 8.9475, 8.9375, 8.9525, 8.9825, 9.0625, 9.0625, 9.06, 9.055, 9.045]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.301, Test accuracy: 12.09
Round   1, Train loss: 2.316, Test loss: 2.298, Test accuracy: 14.86
Round   2, Train loss: 2.308, Test loss: 2.292, Test accuracy: 19.79
Round   3, Train loss: 2.296, Test loss: 2.281, Test accuracy: 25.61
Round   4, Train loss: 2.273, Test loss: 2.250, Test accuracy: 26.62
Round   5, Train loss: 2.235, Test loss: 2.199, Test accuracy: 33.48
Round   6, Train loss: 2.183, Test loss: 2.145, Test accuracy: 43.23
Round   7, Train loss: 2.121, Test loss: 2.074, Test accuracy: 46.75
Round   8, Train loss: 2.047, Test loss: 2.027, Test accuracy: 50.49
Round   9, Train loss: 2.018, Test loss: 1.996, Test accuracy: 55.17
Round  10, Train loss: 2.004, Test loss: 1.971, Test accuracy: 58.96
Round  11, Train loss: 1.987, Test loss: 1.942, Test accuracy: 62.26
Round  12, Train loss: 1.947, Test loss: 1.913, Test accuracy: 63.95
Round  13, Train loss: 1.915, Test loss: 1.887, Test accuracy: 66.59
Round  14, Train loss: 1.893, Test loss: 1.864, Test accuracy: 68.11
Round  15, Train loss: 1.873, Test loss: 1.842, Test accuracy: 70.89
Round  16, Train loss: 1.861, Test loss: 1.821, Test accuracy: 72.85
Round  17, Train loss: 1.827, Test loss: 1.806, Test accuracy: 74.50
Round  18, Train loss: 1.822, Test loss: 1.786, Test accuracy: 76.45
Round  19, Train loss: 1.794, Test loss: 1.773, Test accuracy: 79.27
Round  20, Train loss: 1.778, Test loss: 1.751, Test accuracy: 82.89
Round  21, Train loss: 1.771, Test loss: 1.719, Test accuracy: 86.25
Round  22, Train loss: 1.722, Test loss: 1.700, Test accuracy: 87.71
Round  23, Train loss: 1.710, Test loss: 1.684, Test accuracy: 89.00
Round  24, Train loss: 1.696, Test loss: 1.669, Test accuracy: 89.91
Round  25, Train loss: 1.692, Test loss: 1.653, Test accuracy: 90.57
Round  26, Train loss: 1.685, Test loss: 1.639, Test accuracy: 90.94
Round  27, Train loss: 1.672, Test loss: 1.631, Test accuracy: 91.29
Round  28, Train loss: 1.655, Test loss: 1.626, Test accuracy: 91.48
Round  29, Train loss: 1.650, Test loss: 1.621, Test accuracy: 91.76
Round  30, Train loss: 1.639, Test loss: 1.618, Test accuracy: 91.98
Round  31, Train loss: 1.642, Test loss: 1.613, Test accuracy: 92.16
Round  32, Train loss: 1.624, Test loss: 1.611, Test accuracy: 92.33
Round  33, Train loss: 1.622, Test loss: 1.608, Test accuracy: 92.44
Round  34, Train loss: 1.632, Test loss: 1.600, Test accuracy: 92.64
Round  35, Train loss: 1.616, Test loss: 1.600, Test accuracy: 92.70
Round  36, Train loss: 1.618, Test loss: 1.595, Test accuracy: 92.85
Round  37, Train loss: 1.611, Test loss: 1.591, Test accuracy: 93.07
Round  38, Train loss: 1.604, Test loss: 1.589, Test accuracy: 93.22
Round  39, Train loss: 1.615, Test loss: 1.584, Test accuracy: 93.36
Round  40, Train loss: 1.596, Test loss: 1.585, Test accuracy: 93.42
Round  41, Train loss: 1.598, Test loss: 1.582, Test accuracy: 93.51
Round  42, Train loss: 1.601, Test loss: 1.577, Test accuracy: 93.68
Round  43, Train loss: 1.592, Test loss: 1.577, Test accuracy: 93.85
Round  44, Train loss: 1.577, Test loss: 1.578, Test accuracy: 93.86
Round  45, Train loss: 1.579, Test loss: 1.576, Test accuracy: 93.82
Round  46, Train loss: 1.589, Test loss: 1.572, Test accuracy: 93.98
Round  47, Train loss: 1.583, Test loss: 1.570, Test accuracy: 94.00
Round  48, Train loss: 1.573, Test loss: 1.571, Test accuracy: 94.12
Round  49, Train loss: 1.573, Test loss: 1.569, Test accuracy: 94.15
Round  50, Train loss: 1.581, Test loss: 1.566, Test accuracy: 94.24
Round  51, Train loss: 1.566, Test loss: 1.566, Test accuracy: 94.40
Round  52, Train loss: 1.572, Test loss: 1.564, Test accuracy: 94.38
Round  53, Train loss: 1.578, Test loss: 1.561, Test accuracy: 94.41
Round  54, Train loss: 1.561, Test loss: 1.563, Test accuracy: 94.37
Round  55, Train loss: 1.565, Test loss: 1.562, Test accuracy: 94.40
Round  56, Train loss: 1.562, Test loss: 1.560, Test accuracy: 94.48
Round  57, Train loss: 1.560, Test loss: 1.559, Test accuracy: 94.56
Round  58, Train loss: 1.561, Test loss: 1.557, Test accuracy: 94.63
Round  59, Train loss: 1.553, Test loss: 1.557, Test accuracy: 94.70
Round  60, Train loss: 1.553, Test loss: 1.558, Test accuracy: 94.67
Round  61, Train loss: 1.553, Test loss: 1.556, Test accuracy: 94.72
Round  62, Train loss: 1.548, Test loss: 1.556, Test accuracy: 94.78
Round  63, Train loss: 1.544, Test loss: 1.556, Test accuracy: 94.77
Round  64, Train loss: 1.552, Test loss: 1.553, Test accuracy: 94.79
Round  65, Train loss: 1.544, Test loss: 1.554, Test accuracy: 94.82
Round  66, Train loss: 1.545, Test loss: 1.552, Test accuracy: 94.95
Round  67, Train loss: 1.545, Test loss: 1.552, Test accuracy: 94.93
Round  68, Train loss: 1.546, Test loss: 1.551, Test accuracy: 94.94
Round  69, Train loss: 1.550, Test loss: 1.548, Test accuracy: 94.98
Round  70, Train loss: 1.539, Test loss: 1.550, Test accuracy: 94.98
Round  71, Train loss: 1.541, Test loss: 1.549, Test accuracy: 95.02
Round  72, Train loss: 1.538, Test loss: 1.549, Test accuracy: 95.01
Round  73, Train loss: 1.539, Test loss: 1.548, Test accuracy: 95.09
Round  74, Train loss: 1.543, Test loss: 1.546, Test accuracy: 95.07
Round  75, Train loss: 1.535, Test loss: 1.547, Test accuracy: 95.18
Round  76, Train loss: 1.541, Test loss: 1.544, Test accuracy: 95.17
Round  77, Train loss: 1.539, Test loss: 1.544, Test accuracy: 95.18
Round  78, Train loss: 1.537, Test loss: 1.544, Test accuracy: 95.18
Round  79, Train loss: 1.531, Test loss: 1.546, Test accuracy: 95.28
Round  80, Train loss: 1.538, Test loss: 1.543, Test accuracy: 95.27
Round  81, Train loss: 1.532, Test loss: 1.543, Test accuracy: 95.39
Round  82, Train loss: 1.533, Test loss: 1.542, Test accuracy: 95.43
Round  83, Train loss: 1.534, Test loss: 1.541, Test accuracy: 95.42
Round  84, Train loss: 1.528, Test loss: 1.542, Test accuracy: 95.39
Round  85, Train loss: 1.530, Test loss: 1.541, Test accuracy: 95.42
Round  86, Train loss: 1.527, Test loss: 1.541, Test accuracy: 95.43
Round  87, Train loss: 1.524, Test loss: 1.541, Test accuracy: 95.54
Round  88, Train loss: 1.523, Test loss: 1.541, Test accuracy: 95.55
Round  89, Train loss: 1.523, Test loss: 1.540, Test accuracy: 95.64
Round  90, Train loss: 1.523, Test loss: 1.540, Test accuracy: 95.63
Round  91, Train loss: 1.525, Test loss: 1.539, Test accuracy: 95.68
Round  92, Train loss: 1.522, Test loss: 1.539, Test accuracy: 95.70
Round  93, Train loss: 1.520, Test loss: 1.539, Test accuracy: 95.72/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.521, Test loss: 1.540, Test accuracy: 95.72
Round  95, Train loss: 1.527, Test loss: 1.536, Test accuracy: 95.72
Round  96, Train loss: 1.521, Test loss: 1.537, Test accuracy: 95.73
Round  97, Train loss: 1.520, Test loss: 1.537, Test accuracy: 95.74
Round  98, Train loss: 1.519, Test loss: 1.537, Test accuracy: 95.74
Round  99, Train loss: 1.515, Test loss: 1.538, Test accuracy: 95.78
Final Round, Train loss: 1.495, Test loss: 1.532, Test accuracy: 95.77
Average accuracy final 10 rounds: 95.71475000000001
1321.87154006958
[0.9835460186004639, 1.8451042175292969, 2.7020857334136963, 3.5609114170074463, 4.418253183364868, 5.279950380325317, 6.140278577804565, 6.997110605239868, 7.8564605712890625, 8.724778175354004, 9.581104516983032, 10.447983503341675, 11.311675310134888, 12.177855730056763, 13.044528722763062, 13.905204772949219, 14.777242183685303, 15.638170957565308, 16.505834579467773, 17.366734266281128, 18.224811792373657, 19.08313012123108, 19.949543714523315, 20.808690071105957, 21.67358088493347, 22.52927327156067, 23.392956495285034, 24.247897148132324, 25.110564947128296, 25.962742567062378, 26.825227737426758, 27.67964816093445, 28.543838024139404, 29.39844250679016, 30.2702898979187, 31.12434673309326, 32.00019454956055, 32.85400581359863, 33.731001138687134, 34.58234477043152, 35.4597430229187, 36.31439661979675, 37.1915009021759, 38.04280161857605, 38.91740441322327, 39.77880668640137, 40.65808391571045, 41.51383185386658, 42.39049172401428, 43.24687314033508, 44.12210130691528, 44.97822904586792, 45.84621858596802, 46.70668292045593, 47.60627102851868, 48.394174337387085, 49.19890284538269, 49.98069620132446, 50.78219127655029, 51.56544876098633, 52.35594177246094, 53.14208769798279, 53.930503606796265, 54.71644067764282, 55.50067067146301, 56.2905957698822, 57.0777792930603, 57.860004901885986, 58.64324593544006, 59.42679190635681, 60.21247363090515, 60.997589111328125, 61.78375506401062, 62.58319139480591, 63.36949419975281, 64.16598176956177, 64.94766449928284, 65.73461675643921, 66.51329731941223, 67.30468320846558, 68.08885979652405, 68.87180423736572, 69.65102982521057, 70.43931293487549, 71.2244622707367, 72.0147910118103, 72.81078290939331, 73.59855556488037, 74.38722133636475, 75.17268538475037, 75.96339344978333, 76.74326825141907, 77.53306317329407, 78.31861138343811, 79.10362577438354, 79.88593435287476, 80.6758246421814, 81.46194529533386, 82.24741220474243, 83.03685474395752, 84.35057497024536]
[12.0925, 14.8625, 19.7925, 25.6125, 26.6225, 33.475, 43.2275, 46.7475, 50.4925, 55.175, 58.96, 62.26, 63.955, 66.5925, 68.105, 70.8875, 72.85, 74.4975, 76.4525, 79.265, 82.89, 86.2525, 87.71, 88.9975, 89.9075, 90.5725, 90.94, 91.29, 91.48, 91.76, 91.9775, 92.1625, 92.335, 92.44, 92.635, 92.6975, 92.85, 93.0675, 93.2225, 93.3575, 93.415, 93.51, 93.6775, 93.85, 93.8625, 93.8225, 93.985, 94.0025, 94.125, 94.15, 94.2425, 94.3975, 94.375, 94.405, 94.3725, 94.3975, 94.48, 94.5575, 94.6325, 94.7025, 94.6725, 94.7225, 94.7825, 94.765, 94.7875, 94.8175, 94.9475, 94.9275, 94.945, 94.9775, 94.98, 95.0225, 95.01, 95.095, 95.07, 95.1775, 95.165, 95.18, 95.1825, 95.275, 95.2675, 95.385, 95.43, 95.42, 95.385, 95.42, 95.4325, 95.54, 95.545, 95.635, 95.6275, 95.68, 95.6975, 95.715, 95.72, 95.7175, 95.7275, 95.74, 95.7425, 95.78, 95.7675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.226, Test loss: 2.194, Test accuracy: 35.85 

Round   0, Global train loss: 2.226, Global test loss: 2.283, Global test accuracy: 28.64 

Round   1, Train loss: 1.767, Test loss: 2.051, Test accuracy: 38.91 

Round   1, Global train loss: 1.767, Global test loss: 2.243, Global test accuracy: 17.64 

Round   2, Train loss: 1.867, Test loss: 1.895, Test accuracy: 63.51 

Round   2, Global train loss: 1.867, Global test loss: 2.161, Global test accuracy: 39.24 

Round   3, Train loss: 1.660, Test loss: 1.835, Test accuracy: 65.17 

Round   3, Global train loss: 1.660, Global test loss: 2.202, Global test accuracy: 27.46 

Round   4, Train loss: 1.589, Test loss: 1.699, Test accuracy: 80.38 

Round   4, Global train loss: 1.589, Global test loss: 2.126, Global test accuracy: 37.63 

Round   5, Train loss: 1.712, Test loss: 1.651, Test accuracy: 84.39 

Round   5, Global train loss: 1.712, Global test loss: 2.203, Global test accuracy: 23.41 

Round   6, Train loss: 1.645, Test loss: 1.636, Test accuracy: 84.60 

Round   6, Global train loss: 1.645, Global test loss: 2.196, Global test accuracy: 20.68 

Round   7, Train loss: 1.748, Test loss: 1.634, Test accuracy: 84.65 

Round   7, Global train loss: 1.748, Global test loss: 2.217, Global test accuracy: 25.37 

Round   8, Train loss: 1.638, Test loss: 1.612, Test accuracy: 86.45 

Round   8, Global train loss: 1.638, Global test loss: 2.098, Global test accuracy: 34.37 

Round   9, Train loss: 1.554, Test loss: 1.603, Test accuracy: 86.94 

Round   9, Global train loss: 1.554, Global test loss: 2.243, Global test accuracy: 15.43 

Round  10, Train loss: 1.589, Test loss: 1.599, Test accuracy: 86.98 

Round  10, Global train loss: 1.589, Global test loss: 2.098, Global test accuracy: 34.99 

Round  11, Train loss: 1.579, Test loss: 1.598, Test accuracy: 87.01 

Round  11, Global train loss: 1.579, Global test loss: 2.083, Global test accuracy: 40.40 

Round  12, Train loss: 1.591, Test loss: 1.597, Test accuracy: 87.08 

Round  12, Global train loss: 1.591, Global test loss: 2.187, Global test accuracy: 22.09 

Round  13, Train loss: 1.636, Test loss: 1.597, Test accuracy: 87.08 

Round  13, Global train loss: 1.636, Global test loss: 2.212, Global test accuracy: 19.72 

Round  14, Train loss: 1.683, Test loss: 1.596, Test accuracy: 87.11 

Round  14, Global train loss: 1.683, Global test loss: 2.180, Global test accuracy: 30.12 

Round  15, Train loss: 1.533, Test loss: 1.594, Test accuracy: 87.17 

Round  15, Global train loss: 1.533, Global test loss: 2.110, Global test accuracy: 34.68 

Round  16, Train loss: 1.523, Test loss: 1.594, Test accuracy: 87.12 

Round  16, Global train loss: 1.523, Global test loss: 2.156, Global test accuracy: 25.77 

Round  17, Train loss: 1.553, Test loss: 1.581, Test accuracy: 88.44 

Round  17, Global train loss: 1.553, Global test loss: 2.141, Global test accuracy: 30.95 

Round  18, Train loss: 1.524, Test loss: 1.581, Test accuracy: 88.52 

Round  18, Global train loss: 1.524, Global test loss: 2.080, Global test accuracy: 49.48 

Round  19, Train loss: 1.608, Test loss: 1.568, Test accuracy: 90.02 

Round  19, Global train loss: 1.608, Global test loss: 2.201, Global test accuracy: 28.93 

Round  20, Train loss: 1.526, Test loss: 1.566, Test accuracy: 90.04 

Round  20, Global train loss: 1.526, Global test loss: 2.153, Global test accuracy: 27.99 

Round  21, Train loss: 1.576, Test loss: 1.566, Test accuracy: 90.06 

Round  21, Global train loss: 1.576, Global test loss: 2.115, Global test accuracy: 38.40 

Round  22, Train loss: 1.528, Test loss: 1.565, Test accuracy: 90.08 

Round  22, Global train loss: 1.528, Global test loss: 2.200, Global test accuracy: 23.88 

Round  23, Train loss: 1.467, Test loss: 1.565, Test accuracy: 90.05 

Round  23, Global train loss: 1.467, Global test loss: 2.125, Global test accuracy: 39.06 

Round  24, Train loss: 1.530, Test loss: 1.564, Test accuracy: 90.02 

Round  24, Global train loss: 1.530, Global test loss: 2.100, Global test accuracy: 36.98 

Round  25, Train loss: 1.580, Test loss: 1.564, Test accuracy: 90.06 

Round  25, Global train loss: 1.580, Global test loss: 2.178, Global test accuracy: 28.22 

Round  26, Train loss: 1.578, Test loss: 1.564, Test accuracy: 89.98 

Round  26, Global train loss: 1.578, Global test loss: 2.064, Global test accuracy: 45.32 

Round  27, Train loss: 1.523, Test loss: 1.564, Test accuracy: 89.97 

Round  27, Global train loss: 1.523, Global test loss: 2.154, Global test accuracy: 38.33 

Round  28, Train loss: 1.521, Test loss: 1.564, Test accuracy: 90.03 

Round  28, Global train loss: 1.521, Global test loss: 2.070, Global test accuracy: 40.58 

Round  29, Train loss: 1.518, Test loss: 1.551, Test accuracy: 91.50 

Round  29, Global train loss: 1.518, Global test loss: 2.102, Global test accuracy: 42.62 

Round  30, Train loss: 1.465, Test loss: 1.551, Test accuracy: 91.53 

Round  30, Global train loss: 1.465, Global test loss: 2.115, Global test accuracy: 31.77 

Round  31, Train loss: 1.476, Test loss: 1.547, Test accuracy: 91.66 

Round  31, Global train loss: 1.476, Global test loss: 2.014, Global test accuracy: 45.28 

Round  32, Train loss: 1.604, Test loss: 1.534, Test accuracy: 93.13 

Round  32, Global train loss: 1.604, Global test loss: 2.153, Global test accuracy: 27.53 

Round  33, Train loss: 1.525, Test loss: 1.534, Test accuracy: 93.14 

Round  33, Global train loss: 1.525, Global test loss: 2.155, Global test accuracy: 27.01 

Round  34, Train loss: 1.469, Test loss: 1.532, Test accuracy: 93.17 

Round  34, Global train loss: 1.469, Global test loss: 2.069, Global test accuracy: 38.96 

Round  35, Train loss: 1.467, Test loss: 1.532, Test accuracy: 93.21 

Round  35, Global train loss: 1.467, Global test loss: 2.150, Global test accuracy: 27.62 

Round  36, Train loss: 1.522, Test loss: 1.532, Test accuracy: 93.23 

Round  36, Global train loss: 1.522, Global test loss: 2.213, Global test accuracy: 21.08 

Round  37, Train loss: 1.520, Test loss: 1.532, Test accuracy: 93.22 

Round  37, Global train loss: 1.520, Global test loss: 2.206, Global test accuracy: 24.67 

Round  38, Train loss: 1.468, Test loss: 1.532, Test accuracy: 93.20 

Round  38, Global train loss: 1.468, Global test loss: 2.025, Global test accuracy: 42.92 

Round  39, Train loss: 1.519, Test loss: 1.532, Test accuracy: 93.20 

Round  39, Global train loss: 1.519, Global test loss: 2.038, Global test accuracy: 48.63 

Round  40, Train loss: 1.519, Test loss: 1.532, Test accuracy: 93.21 

Round  40, Global train loss: 1.519, Global test loss: 2.124, Global test accuracy: 34.42 

Round  41, Train loss: 1.518, Test loss: 1.532, Test accuracy: 93.23 

Round  41, Global train loss: 1.518, Global test loss: 2.076, Global test accuracy: 39.93 

Round  42, Train loss: 1.518, Test loss: 1.532, Test accuracy: 93.26 

Round  42, Global train loss: 1.518, Global test loss: 2.160, Global test accuracy: 22.79 

Round  43, Train loss: 1.525, Test loss: 1.531, Test accuracy: 93.30 

Round  43, Global train loss: 1.525, Global test loss: 2.135, Global test accuracy: 34.67 

Round  44, Train loss: 1.521, Test loss: 1.531, Test accuracy: 93.32 

Round  44, Global train loss: 1.521, Global test loss: 2.131, Global test accuracy: 36.33 

Round  45, Train loss: 1.523, Test loss: 1.531, Test accuracy: 93.32 

Round  45, Global train loss: 1.523, Global test loss: 2.088, Global test accuracy: 36.12 

Round  46, Train loss: 1.521, Test loss: 1.531, Test accuracy: 93.32 

Round  46, Global train loss: 1.521, Global test loss: 2.080, Global test accuracy: 51.90 

Round  47, Train loss: 1.467, Test loss: 1.531, Test accuracy: 93.33 

Round  47, Global train loss: 1.467, Global test loss: 2.254, Global test accuracy: 16.47 

Round  48, Train loss: 1.520, Test loss: 1.531, Test accuracy: 93.37 

Round  48, Global train loss: 1.520, Global test loss: 2.188, Global test accuracy: 22.87 

Round  49, Train loss: 1.467, Test loss: 1.531, Test accuracy: 93.34 

Round  49, Global train loss: 1.467, Global test loss: 2.261, Global test accuracy: 13.63 

Round  50, Train loss: 1.465, Test loss: 1.531, Test accuracy: 93.33 

Round  50, Global train loss: 1.465, Global test loss: 2.210, Global test accuracy: 20.98 

Round  51, Train loss: 1.464, Test loss: 1.531, Test accuracy: 93.34 

Round  51, Global train loss: 1.464, Global test loss: 2.152, Global test accuracy: 30.38 

Round  52, Train loss: 1.517, Test loss: 1.531, Test accuracy: 93.33 

Round  52, Global train loss: 1.517, Global test loss: 2.146, Global test accuracy: 31.12 

Round  53, Train loss: 1.521, Test loss: 1.531, Test accuracy: 93.34 

Round  53, Global train loss: 1.521, Global test loss: 1.993, Global test accuracy: 50.54 

Round  54, Train loss: 1.466, Test loss: 1.531, Test accuracy: 93.37 

Round  54, Global train loss: 1.466, Global test loss: 2.080, Global test accuracy: 36.88 

Round  55, Train loss: 1.506, Test loss: 1.519, Test accuracy: 94.66 

Round  55, Global train loss: 1.506, Global test loss: 2.205, Global test accuracy: 24.13 

Round  56, Train loss: 1.523, Test loss: 1.519, Test accuracy: 94.64 

Round  56, Global train loss: 1.523, Global test loss: 2.103, Global test accuracy: 34.14 

Round  57, Train loss: 1.466, Test loss: 1.519, Test accuracy: 94.64 

Round  57, Global train loss: 1.466, Global test loss: 2.143, Global test accuracy: 31.74 

Round  58, Train loss: 1.465, Test loss: 1.519, Test accuracy: 94.65 

Round  58, Global train loss: 1.465, Global test loss: 2.082, Global test accuracy: 41.95 

Round  59, Train loss: 1.522, Test loss: 1.519, Test accuracy: 94.65 

Round  59, Global train loss: 1.522, Global test loss: 2.115, Global test accuracy: 37.10 

Round  60, Train loss: 1.464, Test loss: 1.519, Test accuracy: 94.64 

Round  60, Global train loss: 1.464, Global test loss: 2.037, Global test accuracy: 48.17 

Round  61, Train loss: 1.474, Test loss: 1.517, Test accuracy: 94.70 

Round  61, Global train loss: 1.474, Global test loss: 2.186, Global test accuracy: 20.34 

Round  62, Train loss: 1.467, Test loss: 1.518, Test accuracy: 94.69 

Round  62, Global train loss: 1.467, Global test loss: 2.194, Global test accuracy: 24.22 

Round  63, Train loss: 1.525, Test loss: 1.517, Test accuracy: 94.77 

Round  63, Global train loss: 1.525, Global test loss: 2.152, Global test accuracy: 29.91 

Round  64, Train loss: 1.467, Test loss: 1.517, Test accuracy: 94.77 

Round  64, Global train loss: 1.467, Global test loss: 2.076, Global test accuracy: 34.95 

Round  65, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.77 

Round  65, Global train loss: 1.522, Global test loss: 2.191, Global test accuracy: 24.84 

Round  66, Train loss: 1.469, Test loss: 1.517, Test accuracy: 94.70 

Round  66, Global train loss: 1.469, Global test loss: 2.277, Global test accuracy: 16.18 

Round  67, Train loss: 1.464, Test loss: 1.517, Test accuracy: 94.70 

Round  67, Global train loss: 1.464, Global test loss: 2.062, Global test accuracy: 38.28 

Round  68, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.71 

Round  68, Global train loss: 1.522, Global test loss: 2.109, Global test accuracy: 39.24 

Round  69, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.71 

Round  69, Global train loss: 1.522, Global test loss: 2.092, Global test accuracy: 44.51 

Round  70, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.72 

Round  70, Global train loss: 1.464, Global test loss: 2.058, Global test accuracy: 43.92 

Round  71, Train loss: 1.468, Test loss: 1.516, Test accuracy: 94.72 

Round  71, Global train loss: 1.468, Global test loss: 2.254, Global test accuracy: 17.24 

Round  72, Train loss: 1.523, Test loss: 1.516, Test accuracy: 94.73 

Round  72, Global train loss: 1.523, Global test loss: 2.102, Global test accuracy: 31.70 

Round  73, Train loss: 1.465, Test loss: 1.516, Test accuracy: 94.74 

Round  73, Global train loss: 1.465, Global test loss: 2.162, Global test accuracy: 22.64 

Round  74, Train loss: 1.462, Test loss: 1.516, Test accuracy: 94.74 

Round  74, Global train loss: 1.462, Global test loss: 2.081, Global test accuracy: 40.91 

Round  75, Train loss: 1.466, Test loss: 1.516, Test accuracy: 94.73 

Round  75, Global train loss: 1.466, Global test loss: 2.261, Global test accuracy: 13.91 

Round  76, Train loss: 1.465, Test loss: 1.516, Test accuracy: 94.73 

Round  76, Global train loss: 1.465, Global test loss: 2.144, Global test accuracy: 27.88 

Round  77, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.73 

Round  77, Global train loss: 1.464, Global test loss: 2.076, Global test accuracy: 45.16 

Round  78, Train loss: 1.462, Test loss: 1.516, Test accuracy: 94.73 

Round  78, Global train loss: 1.462, Global test loss: 2.097, Global test accuracy: 41.56 

Round  79, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.74 

Round  79, Global train loss: 1.522, Global test loss: 2.169, Global test accuracy: 28.43 

Round  80, Train loss: 1.521, Test loss: 1.516, Test accuracy: 94.74 

Round  80, Global train loss: 1.521, Global test loss: 2.093, Global test accuracy: 38.70 

Round  81, Train loss: 1.523, Test loss: 1.516, Test accuracy: 94.74 

Round  81, Global train loss: 1.523, Global test loss: 2.115, Global test accuracy: 41.20 

Round  82, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.75 

Round  82, Global train loss: 1.464, Global test loss: 2.108, Global test accuracy: 35.20 

Round  83, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.70 

Round  83, Global train loss: 1.522, Global test loss: 2.259, Global test accuracy: 19.65 

Round  84, Train loss: 1.522, Test loss: 1.517, Test accuracy: 94.67 

Round  84, Global train loss: 1.522, Global test loss: 2.157, Global test accuracy: 29.08 

Round  85, Train loss: 1.466, Test loss: 1.516, Test accuracy: 94.70 

Round  85, Global train loss: 1.466, Global test loss: 2.114, Global test accuracy: 33.76 

Round  86, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.70 

Round  86, Global train loss: 1.464, Global test loss: 2.044, Global test accuracy: 42.33 

Round  87, Train loss: 1.462, Test loss: 1.516, Test accuracy: 94.70 

Round  87, Global train loss: 1.462, Global test loss: 2.038, Global test accuracy: 45.34 

Round  88, Train loss: 1.465, Test loss: 1.516, Test accuracy: 94.70 

Round  88, Global train loss: 1.465, Global test loss: 2.188, Global test accuracy: 28.00 

Round  89, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.72 

Round  89, Global train loss: 1.464, Global test loss: 2.193, Global test accuracy: 21.00 

Round  90, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.72 

Round  90, Global train loss: 1.464, Global test loss: 2.202, Global test accuracy: 25.42 

Round  91, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.71 

Round  91, Global train loss: 1.464, Global test loss: 2.128, Global test accuracy: 32.64 

Round  92, Train loss: 1.521, Test loss: 1.516, Test accuracy: 94.67 

Round  92, Global train loss: 1.521, Global test loss: 2.206, Global test accuracy: 21.62 

Round  93, Train loss: 1.465, Test loss: 1.516, Test accuracy: 94.67 

Round  93, Global train loss: 1.465, Global test loss: 2.161, Global test accuracy: 24.97 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.70 

Round  94, Global train loss: 1.464, Global test loss: 2.032, Global test accuracy: 46.91 

Round  95, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.70 

Round  95, Global train loss: 1.464, Global test loss: 2.103, Global test accuracy: 36.86 

Round  96, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.69 

Round  96, Global train loss: 1.464, Global test loss: 2.036, Global test accuracy: 41.34 

Round  97, Train loss: 1.464, Test loss: 1.516, Test accuracy: 94.69 

Round  97, Global train loss: 1.464, Global test loss: 2.124, Global test accuracy: 29.43 

Round  98, Train loss: 1.466, Test loss: 1.516, Test accuracy: 94.68 

Round  98, Global train loss: 1.466, Global test loss: 2.077, Global test accuracy: 42.17 

Round  99, Train loss: 1.466, Test loss: 1.516, Test accuracy: 94.68 

Round  99, Global train loss: 1.466, Global test loss: 2.183, Global test accuracy: 23.57 

Final Round, Train loss: 1.482, Test loss: 1.516, Test accuracy: 94.71 

Final Round, Global train loss: 1.482, Global test loss: 2.183, Global test accuracy: 23.57 

Average accuracy final 10 rounds: 94.6925 

Average global accuracy final 10 rounds: 32.49333333333334 

949.7276291847229
[0.8246917724609375, 1.5636649131774902, 2.300971031188965, 3.0324037075042725, 3.770650625228882, 4.515578746795654, 5.252824783325195, 5.990726947784424, 6.742670297622681, 7.480889797210693, 8.217970132827759, 8.968200445175171, 9.707202196121216, 10.444408893585205, 11.192769050598145, 11.92981481552124, 12.66389536857605, 13.416408777236938, 14.156615495681763, 14.891223192214966, 15.640198469161987, 16.37914729118347, 17.116074323654175, 17.864531993865967, 18.605958938598633, 19.344679594039917, 20.088707208633423, 20.828411102294922, 21.566513538360596, 22.319811582565308, 23.055299520492554, 23.794392347335815, 24.546466827392578, 25.282646656036377, 26.02553701400757, 26.77276301383972, 27.512327909469604, 28.252593755722046, 29.004347562789917, 29.744654655456543, 30.485228776931763, 31.235632181167603, 31.973567724227905, 32.7116482257843, 33.45187306404114, 34.19294333457947, 34.938727378845215, 35.68393516540527, 36.419413328170776, 37.160619020462036, 37.89783811569214, 38.63385486602783, 39.376805782318115, 40.11631536483765, 40.852635860443115, 41.60102677345276, 42.3389036655426, 43.080830574035645, 43.82691526412964, 44.56602668762207, 45.30404996871948, 46.04948711395264, 46.79249978065491, 47.5294668674469, 48.27772521972656, 49.016003131866455, 49.75576996803284, 50.50100231170654, 51.242483615875244, 51.99053597450256, 52.72710728645325, 53.468756914138794, 54.21456480026245, 54.95151662826538, 55.693074464797974, 56.43904781341553, 57.18124222755432, 57.92129611968994, 58.6706120967865, 59.411147594451904, 60.145620584487915, 60.89630365371704, 61.63397574424744, 62.37221908569336, 63.12007403373718, 63.86166000366211, 64.61085724830627, 65.35291719436646, 66.09031796455383, 66.83171439170837, 67.57030153274536, 68.30934524536133, 69.05807852745056, 69.79678797721863, 70.53563451766968, 71.28788542747498, 72.02949023246765, 72.76687288284302, 73.51042985916138, 74.24630856513977, 75.73575139045715]
[35.85, 38.90833333333333, 63.50833333333333, 65.16666666666667, 80.38333333333334, 84.39166666666667, 84.6, 84.65, 86.45, 86.94166666666666, 86.98333333333333, 87.00833333333334, 87.08333333333333, 87.075, 87.10833333333333, 87.175, 87.125, 88.44166666666666, 88.51666666666667, 90.01666666666667, 90.04166666666667, 90.05833333333334, 90.075, 90.05, 90.01666666666667, 90.05833333333334, 89.98333333333333, 89.975, 90.025, 91.5, 91.525, 91.65833333333333, 93.13333333333334, 93.14166666666667, 93.16666666666667, 93.20833333333333, 93.23333333333333, 93.225, 93.2, 93.2, 93.20833333333333, 93.23333333333333, 93.25833333333334, 93.3, 93.31666666666666, 93.31666666666666, 93.31666666666666, 93.325, 93.36666666666666, 93.34166666666667, 93.33333333333333, 93.34166666666667, 93.33333333333333, 93.34166666666667, 93.36666666666666, 94.65833333333333, 94.64166666666667, 94.64166666666667, 94.65, 94.65, 94.64166666666667, 94.7, 94.69166666666666, 94.76666666666667, 94.76666666666667, 94.76666666666667, 94.7, 94.7, 94.70833333333333, 94.70833333333333, 94.725, 94.71666666666667, 94.73333333333333, 94.74166666666666, 94.74166666666666, 94.73333333333333, 94.73333333333333, 94.73333333333333, 94.73333333333333, 94.74166666666666, 94.74166666666666, 94.74166666666666, 94.75, 94.7, 94.675, 94.7, 94.7, 94.7, 94.7, 94.71666666666667, 94.71666666666667, 94.70833333333333, 94.675, 94.675, 94.7, 94.7, 94.69166666666666, 94.69166666666666, 94.68333333333334, 94.68333333333334, 94.70833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.233, Test loss: 2.223, Test accuracy: 16.96 

Round   0, Global train loss: 2.233, Global test loss: 2.293, Global test accuracy: 8.33 

Round   1, Train loss: 1.959, Test loss: 2.053, Test accuracy: 44.71 

Round   1, Global train loss: 1.959, Global test loss: 2.257, Global test accuracy: 25.62 

Round   2, Train loss: 1.735, Test loss: 1.843, Test accuracy: 64.03 

Round   2, Global train loss: 1.735, Global test loss: 2.156, Global test accuracy: 30.84 

Round   3, Train loss: 1.618, Test loss: 1.792, Test accuracy: 68.63 

Round   3, Global train loss: 1.618, Global test loss: 2.166, Global test accuracy: 28.12 

Round   4, Train loss: 1.584, Test loss: 1.719, Test accuracy: 75.52 

Round   4, Global train loss: 1.584, Global test loss: 2.098, Global test accuracy: 35.96 

Round   5, Train loss: 1.520, Test loss: 1.646, Test accuracy: 83.82 

Round   5, Global train loss: 1.520, Global test loss: 1.934, Global test accuracy: 55.75 

Round   6, Train loss: 1.538, Test loss: 1.611, Test accuracy: 86.28 

Round   6, Global train loss: 1.538, Global test loss: 1.896, Global test accuracy: 57.19 

Round   7, Train loss: 1.555, Test loss: 1.546, Test accuracy: 93.23 

Round   7, Global train loss: 1.555, Global test loss: 1.819, Global test accuracy: 68.10 

Round   8, Train loss: 1.502, Test loss: 1.543, Test accuracy: 93.43 

Round   8, Global train loss: 1.502, Global test loss: 1.833, Global test accuracy: 63.26 

Round   9, Train loss: 1.513, Test loss: 1.542, Test accuracy: 93.49 

Round   9, Global train loss: 1.513, Global test loss: 1.794, Global test accuracy: 72.58 

Round  10, Train loss: 1.504, Test loss: 1.540, Test accuracy: 93.50 

Round  10, Global train loss: 1.504, Global test loss: 1.760, Global test accuracy: 72.84 

Round  11, Train loss: 1.506, Test loss: 1.539, Test accuracy: 93.65 

Round  11, Global train loss: 1.506, Global test loss: 1.705, Global test accuracy: 78.91 

Round  12, Train loss: 1.496, Test loss: 1.506, Test accuracy: 96.37 

Round  12, Global train loss: 1.496, Global test loss: 1.778, Global test accuracy: 69.91 

Round  13, Train loss: 1.506, Test loss: 1.506, Test accuracy: 96.26 

Round  13, Global train loss: 1.506, Global test loss: 1.698, Global test accuracy: 79.10 

Round  14, Train loss: 1.491, Test loss: 1.505, Test accuracy: 96.30 

Round  14, Global train loss: 1.491, Global test loss: 1.663, Global test accuracy: 83.16 

Round  15, Train loss: 1.491, Test loss: 1.506, Test accuracy: 96.14 

Round  15, Global train loss: 1.491, Global test loss: 1.710, Global test accuracy: 77.04 

Round  16, Train loss: 1.490, Test loss: 1.506, Test accuracy: 96.20 

Round  16, Global train loss: 1.490, Global test loss: 1.667, Global test accuracy: 82.10 

Round  17, Train loss: 1.506, Test loss: 1.503, Test accuracy: 96.38 

Round  17, Global train loss: 1.506, Global test loss: 1.649, Global test accuracy: 84.88 

Round  18, Train loss: 1.495, Test loss: 1.501, Test accuracy: 96.64 

Round  18, Global train loss: 1.495, Global test loss: 1.642, Global test accuracy: 85.33 

Round  19, Train loss: 1.488, Test loss: 1.502, Test accuracy: 96.54 

Round  19, Global train loss: 1.488, Global test loss: 1.682, Global test accuracy: 79.93 

Round  20, Train loss: 1.495, Test loss: 1.501, Test accuracy: 96.61 

Round  20, Global train loss: 1.495, Global test loss: 1.696, Global test accuracy: 77.72 

Round  21, Train loss: 1.488, Test loss: 1.500, Test accuracy: 96.70 

Round  21, Global train loss: 1.488, Global test loss: 1.638, Global test accuracy: 84.62 

Round  22, Train loss: 1.488, Test loss: 1.499, Test accuracy: 96.68 

Round  22, Global train loss: 1.488, Global test loss: 1.662, Global test accuracy: 82.33 

Round  23, Train loss: 1.485, Test loss: 1.499, Test accuracy: 96.69 

Round  23, Global train loss: 1.485, Global test loss: 1.652, Global test accuracy: 82.82 

Round  24, Train loss: 1.489, Test loss: 1.500, Test accuracy: 96.66 

Round  24, Global train loss: 1.489, Global test loss: 1.639, Global test accuracy: 84.54 

Round  25, Train loss: 1.485, Test loss: 1.499, Test accuracy: 96.66 

Round  25, Global train loss: 1.485, Global test loss: 1.637, Global test accuracy: 84.20 

Round  26, Train loss: 1.485, Test loss: 1.500, Test accuracy: 96.53 

Round  26, Global train loss: 1.485, Global test loss: 1.629, Global test accuracy: 84.97 

Round  27, Train loss: 1.488, Test loss: 1.499, Test accuracy: 96.63 

Round  27, Global train loss: 1.488, Global test loss: 1.650, Global test accuracy: 82.80 

Round  28, Train loss: 1.483, Test loss: 1.499, Test accuracy: 96.66 

Round  28, Global train loss: 1.483, Global test loss: 1.627, Global test accuracy: 85.16 

Round  29, Train loss: 1.485, Test loss: 1.499, Test accuracy: 96.64 

Round  29, Global train loss: 1.485, Global test loss: 1.604, Global test accuracy: 87.72 

Round  30, Train loss: 1.483, Test loss: 1.498, Test accuracy: 96.67 

Round  30, Global train loss: 1.483, Global test loss: 1.677, Global test accuracy: 79.06 

Round  31, Train loss: 1.487, Test loss: 1.498, Test accuracy: 96.62 

Round  31, Global train loss: 1.487, Global test loss: 1.622, Global test accuracy: 85.77 

Round  32, Train loss: 1.477, Test loss: 1.498, Test accuracy: 96.59 

Round  32, Global train loss: 1.477, Global test loss: 1.645, Global test accuracy: 82.97 

Round  33, Train loss: 1.481, Test loss: 1.498, Test accuracy: 96.69 

Round  33, Global train loss: 1.481, Global test loss: 1.652, Global test accuracy: 82.27 

Round  34, Train loss: 1.485, Test loss: 1.497, Test accuracy: 96.83 

Round  34, Global train loss: 1.485, Global test loss: 1.608, Global test accuracy: 87.38 

Round  35, Train loss: 1.478, Test loss: 1.497, Test accuracy: 96.79 

Round  35, Global train loss: 1.478, Global test loss: 1.593, Global test accuracy: 88.39 

Round  36, Train loss: 1.479, Test loss: 1.498, Test accuracy: 96.62 

Round  36, Global train loss: 1.479, Global test loss: 1.619, Global test accuracy: 85.84 

Round  37, Train loss: 1.478, Test loss: 1.499, Test accuracy: 96.60 

Round  37, Global train loss: 1.478, Global test loss: 1.607, Global test accuracy: 87.33 

Round  38, Train loss: 1.477, Test loss: 1.498, Test accuracy: 96.58 

Round  38, Global train loss: 1.477, Global test loss: 1.610, Global test accuracy: 86.74 

Round  39, Train loss: 1.476, Test loss: 1.498, Test accuracy: 96.56 

Round  39, Global train loss: 1.476, Global test loss: 1.600, Global test accuracy: 87.50 

Round  40, Train loss: 1.479, Test loss: 1.499, Test accuracy: 96.54 

Round  40, Global train loss: 1.479, Global test loss: 1.622, Global test accuracy: 85.47 

Round  41, Train loss: 1.477, Test loss: 1.499, Test accuracy: 96.53 

Round  41, Global train loss: 1.477, Global test loss: 1.612, Global test accuracy: 86.38 

Round  42, Train loss: 1.478, Test loss: 1.499, Test accuracy: 96.47 

Round  42, Global train loss: 1.478, Global test loss: 1.631, Global test accuracy: 84.51 

Round  43, Train loss: 1.478, Test loss: 1.498, Test accuracy: 96.64 

Round  43, Global train loss: 1.478, Global test loss: 1.587, Global test accuracy: 88.92 

Round  44, Train loss: 1.480, Test loss: 1.497, Test accuracy: 96.71 

Round  44, Global train loss: 1.480, Global test loss: 1.703, Global test accuracy: 76.21 

Round  45, Train loss: 1.478, Test loss: 1.496, Test accuracy: 96.73 

Round  45, Global train loss: 1.478, Global test loss: 1.622, Global test accuracy: 84.83 

Round  46, Train loss: 1.478, Test loss: 1.496, Test accuracy: 96.77 

Round  46, Global train loss: 1.478, Global test loss: 1.615, Global test accuracy: 85.82 

Round  47, Train loss: 1.477, Test loss: 1.497, Test accuracy: 96.80 

Round  47, Global train loss: 1.477, Global test loss: 1.608, Global test accuracy: 86.17 

Round  48, Train loss: 1.476, Test loss: 1.497, Test accuracy: 96.78 

Round  48, Global train loss: 1.476, Global test loss: 1.585, Global test accuracy: 89.00 

Round  49, Train loss: 1.477, Test loss: 1.496, Test accuracy: 96.85 

Round  49, Global train loss: 1.477, Global test loss: 1.602, Global test accuracy: 86.95 

Round  50, Train loss: 1.476, Test loss: 1.497, Test accuracy: 96.80 

Round  50, Global train loss: 1.476, Global test loss: 1.602, Global test accuracy: 87.06 

Round  51, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.83 

Round  51, Global train loss: 1.474, Global test loss: 1.620, Global test accuracy: 84.88 

Round  52, Train loss: 1.478, Test loss: 1.496, Test accuracy: 96.84 

Round  52, Global train loss: 1.478, Global test loss: 1.604, Global test accuracy: 86.98 

Round  53, Train loss: 1.476, Test loss: 1.496, Test accuracy: 96.79 

Round  53, Global train loss: 1.476, Global test loss: 1.619, Global test accuracy: 85.45 

Round  54, Train loss: 1.473, Test loss: 1.496, Test accuracy: 96.85 

Round  54, Global train loss: 1.473, Global test loss: 1.626, Global test accuracy: 84.22 

Round  55, Train loss: 1.475, Test loss: 1.496, Test accuracy: 96.85 

Round  55, Global train loss: 1.475, Global test loss: 1.620, Global test accuracy: 84.83 

Round  56, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.82 

Round  56, Global train loss: 1.474, Global test loss: 1.593, Global test accuracy: 87.68 

Round  57, Train loss: 1.474, Test loss: 1.495, Test accuracy: 96.85 

Round  57, Global train loss: 1.474, Global test loss: 1.601, Global test accuracy: 86.98 

Round  58, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.78 

Round  58, Global train loss: 1.473, Global test loss: 1.579, Global test accuracy: 89.50 

Round  59, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.77 

Round  59, Global train loss: 1.473, Global test loss: 1.611, Global test accuracy: 85.92 

Round  60, Train loss: 1.479, Test loss: 1.495, Test accuracy: 96.87 

Round  60, Global train loss: 1.479, Global test loss: 1.569, Global test accuracy: 90.46 

Round  61, Train loss: 1.476, Test loss: 1.494, Test accuracy: 96.93 

Round  61, Global train loss: 1.476, Global test loss: 1.615, Global test accuracy: 85.29 

Round  62, Train loss: 1.472, Test loss: 1.494, Test accuracy: 96.97 

Round  62, Global train loss: 1.472, Global test loss: 1.611, Global test accuracy: 85.97 

Round  63, Train loss: 1.475, Test loss: 1.495, Test accuracy: 96.92 

Round  63, Global train loss: 1.475, Global test loss: 1.600, Global test accuracy: 87.12 

Round  64, Train loss: 1.476, Test loss: 1.495, Test accuracy: 96.85 

Round  64, Global train loss: 1.476, Global test loss: 1.585, Global test accuracy: 88.37 

Round  65, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.88 

Round  65, Global train loss: 1.470, Global test loss: 1.576, Global test accuracy: 89.55 

Round  66, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.92 

Round  66, Global train loss: 1.471, Global test loss: 1.585, Global test accuracy: 88.71 

Round  67, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.88 

Round  67, Global train loss: 1.473, Global test loss: 1.569, Global test accuracy: 90.17 

Round  68, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.88 

Round  68, Global train loss: 1.471, Global test loss: 1.568, Global test accuracy: 90.33 

Round  69, Train loss: 1.476, Test loss: 1.494, Test accuracy: 97.00 

Round  69, Global train loss: 1.476, Global test loss: 1.594, Global test accuracy: 87.47 

Round  70, Train loss: 1.473, Test loss: 1.494, Test accuracy: 96.99 

Round  70, Global train loss: 1.473, Global test loss: 1.577, Global test accuracy: 89.50 

Round  71, Train loss: 1.471, Test loss: 1.494, Test accuracy: 97.01 

Round  71, Global train loss: 1.471, Global test loss: 1.572, Global test accuracy: 89.71 

Round  72, Train loss: 1.474, Test loss: 1.494, Test accuracy: 96.94 

Round  72, Global train loss: 1.474, Global test loss: 1.562, Global test accuracy: 91.08 

Round  73, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.99 

Round  73, Global train loss: 1.470, Global test loss: 1.579, Global test accuracy: 88.97 

Round  74, Train loss: 1.472, Test loss: 1.494, Test accuracy: 96.96 

Round  74, Global train loss: 1.472, Global test loss: 1.601, Global test accuracy: 87.10 

Round  75, Train loss: 1.472, Test loss: 1.494, Test accuracy: 96.98 

Round  75, Global train loss: 1.472, Global test loss: 1.587, Global test accuracy: 88.51 

Round  76, Train loss: 1.474, Test loss: 1.494, Test accuracy: 97.02 

Round  76, Global train loss: 1.474, Global test loss: 1.591, Global test accuracy: 88.03 

Round  77, Train loss: 1.474, Test loss: 1.493, Test accuracy: 97.04 

Round  77, Global train loss: 1.474, Global test loss: 1.567, Global test accuracy: 90.42 

Round  78, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.07 

Round  78, Global train loss: 1.469, Global test loss: 1.598, Global test accuracy: 87.14 

Round  79, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.06 

Round  79, Global train loss: 1.471, Global test loss: 1.591, Global test accuracy: 87.85 

Round  80, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.12 

Round  80, Global train loss: 1.470, Global test loss: 1.605, Global test accuracy: 86.51 

Round  81, Train loss: 1.473, Test loss: 1.493, Test accuracy: 97.11 

Round  81, Global train loss: 1.473, Global test loss: 1.591, Global test accuracy: 87.92 

Round  82, Train loss: 1.473, Test loss: 1.493, Test accuracy: 97.12 

Round  82, Global train loss: 1.473, Global test loss: 1.586, Global test accuracy: 88.64 

Round  83, Train loss: 1.472, Test loss: 1.493, Test accuracy: 97.12 

Round  83, Global train loss: 1.472, Global test loss: 1.575, Global test accuracy: 89.91 

Round  84, Train loss: 1.474, Test loss: 1.493, Test accuracy: 97.16 

Round  84, Global train loss: 1.474, Global test loss: 1.572, Global test accuracy: 89.97 

Round  85, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.16 

Round  85, Global train loss: 1.470, Global test loss: 1.582, Global test accuracy: 88.60 

Round  86, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.13 

Round  86, Global train loss: 1.471, Global test loss: 1.573, Global test accuracy: 89.45 

Round  87, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.17 

Round  87, Global train loss: 1.471, Global test loss: 1.578, Global test accuracy: 89.47 

Round  88, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.13 

Round  88, Global train loss: 1.470, Global test loss: 1.582, Global test accuracy: 88.85 

Round  89, Train loss: 1.471, Test loss: 1.492, Test accuracy: 97.14 

Round  89, Global train loss: 1.471, Global test loss: 1.573, Global test accuracy: 89.60 

Round  90, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.09 

Round  90, Global train loss: 1.470, Global test loss: 1.580, Global test accuracy: 88.97 

Round  91, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.09 

Round  91, Global train loss: 1.471, Global test loss: 1.589, Global test accuracy: 88.13 

Round  92, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.14 

Round  92, Global train loss: 1.469, Global test loss: 1.568, Global test accuracy: 90.17 

Round  93, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.12 

Round  93, Global train loss: 1.470, Global test loss: 1.559, Global test accuracy: 91.08 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.470, Test loss: 1.493, Test accuracy: 97.08 

Round  94, Global train loss: 1.470, Global test loss: 1.579, Global test accuracy: 88.89 

Round  95, Train loss: 1.472, Test loss: 1.493, Test accuracy: 97.09 

Round  95, Global train loss: 1.472, Global test loss: 1.585, Global test accuracy: 88.47 

Round  96, Train loss: 1.470, Test loss: 1.492, Test accuracy: 97.08 

Round  96, Global train loss: 1.470, Global test loss: 1.590, Global test accuracy: 88.09 

Round  97, Train loss: 1.469, Test loss: 1.492, Test accuracy: 97.10 

Round  97, Global train loss: 1.469, Global test loss: 1.604, Global test accuracy: 86.14 

Round  98, Train loss: 1.471, Test loss: 1.492, Test accuracy: 97.12 

Round  98, Global train loss: 1.471, Global test loss: 1.568, Global test accuracy: 90.17 

Round  99, Train loss: 1.472, Test loss: 1.492, Test accuracy: 97.13 

Round  99, Global train loss: 1.472, Global test loss: 1.573, Global test accuracy: 89.78 

Final Round, Train loss: 1.469, Test loss: 1.491, Test accuracy: 97.27 

Final Round, Global train loss: 1.469, Global test loss: 1.573, Global test accuracy: 89.78 

Average accuracy final 10 rounds: 97.10583333333334 

Average global accuracy final 10 rounds: 88.99083333333334 

940.6393694877625
[0.8495545387268066, 1.5989887714385986, 2.353464126586914, 3.1047637462615967, 3.8526265621185303, 4.607360601425171, 5.357455730438232, 6.111208200454712, 6.869455814361572, 7.6169373989105225, 8.368174076080322, 9.12178921699524, 9.873459100723267, 10.624867916107178, 11.380621194839478, 12.128995418548584, 12.881164789199829, 13.632563829421997, 14.379873752593994, 15.129658460617065, 15.872927904129028, 16.614463806152344, 17.369447708129883, 18.10988759994507, 18.852219343185425, 19.59948444366455, 20.336974620819092, 21.08057475090027, 21.83209252357483, 22.578015565872192, 23.328310012817383, 24.07796621322632, 24.822295904159546, 25.57413625717163, 26.329962491989136, 27.07239031791687, 27.820801496505737, 28.56352972984314, 29.30432677268982, 30.05466055870056, 30.797905206680298, 31.50767159461975, 32.23776841163635, 32.941234827041626, 33.654696464538574, 34.37002158164978, 35.089149713516235, 35.8142511844635, 36.56772589683533, 37.31328749656677, 38.0635142326355, 38.81698417663574, 39.56486988067627, 40.30910611152649, 41.064525842666626, 41.81228971481323, 42.55870270729065, 43.31342625617981, 44.05543327331543, 44.80208683013916, 45.560176849365234, 46.30375099182129, 47.046382427215576, 47.80505633354187, 48.54774022102356, 49.3031485080719, 50.06130504608154, 50.80450654029846, 51.46443462371826, 52.12601447105408, 52.76889395713806, 53.41200804710388, 54.05748701095581, 54.69651198387146, 55.333810806274414, 55.99091553688049, 56.635117530822754, 57.27971863746643, 57.93502449989319, 58.58136296272278, 59.224441051483154, 59.87535309791565, 60.52038764953613, 61.15942668914795, 61.81259536743164, 62.451908111572266, 63.094576835632324, 63.7451856136322, 64.38158297538757, 65.01901984214783, 65.67355799674988, 66.31081914901733, 66.95319390296936, 67.60431575775146, 68.26004338264465, 68.90269732475281, 69.5466058254242, 70.19917011260986, 70.83591556549072, 71.48204493522644, 72.78256869316101]
[16.958333333333332, 44.708333333333336, 64.025, 68.63333333333334, 75.51666666666667, 83.81666666666666, 86.275, 93.23333333333333, 93.43333333333334, 93.49166666666666, 93.5, 93.65, 96.36666666666666, 96.25833333333334, 96.3, 96.14166666666667, 96.2, 96.38333333333334, 96.64166666666667, 96.54166666666667, 96.60833333333333, 96.7, 96.68333333333334, 96.69166666666666, 96.65833333333333, 96.65833333333333, 96.53333333333333, 96.63333333333334, 96.65833333333333, 96.64166666666667, 96.675, 96.625, 96.59166666666667, 96.69166666666666, 96.825, 96.79166666666667, 96.625, 96.6, 96.575, 96.55833333333334, 96.54166666666667, 96.525, 96.46666666666667, 96.64166666666667, 96.70833333333333, 96.73333333333333, 96.76666666666667, 96.8, 96.78333333333333, 96.85, 96.8, 96.83333333333333, 96.84166666666667, 96.79166666666667, 96.85, 96.85, 96.81666666666666, 96.85, 96.78333333333333, 96.76666666666667, 96.86666666666666, 96.93333333333334, 96.975, 96.925, 96.85, 96.875, 96.925, 96.875, 96.875, 97.0, 96.99166666666666, 97.00833333333334, 96.94166666666666, 96.99166666666666, 96.95833333333333, 96.98333333333333, 97.01666666666667, 97.04166666666667, 97.06666666666666, 97.05833333333334, 97.11666666666666, 97.10833333333333, 97.11666666666666, 97.11666666666666, 97.15833333333333, 97.15833333333333, 97.13333333333334, 97.16666666666667, 97.13333333333334, 97.14166666666667, 97.09166666666667, 97.09166666666667, 97.14166666666667, 97.11666666666666, 97.08333333333333, 97.09166666666667, 97.08333333333333, 97.1, 97.125, 97.13333333333334, 97.26666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.302, Test accuracy: 7.92 

Round   1, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.13 

Round   2, Train loss: 2.301, Test loss: 2.300, Test accuracy: 15.88 

Round   3, Train loss: 2.299, Test loss: 2.299, Test accuracy: 20.87 

Round   4, Train loss: 2.298, Test loss: 2.297, Test accuracy: 26.80 

Round   5, Train loss: 2.296, Test loss: 2.295, Test accuracy: 31.08 

Round   6, Train loss: 2.294, Test loss: 2.293, Test accuracy: 34.98 

Round   7, Train loss: 2.292, Test loss: 2.290, Test accuracy: 37.00 

Round   8, Train loss: 2.288, Test loss: 2.285, Test accuracy: 37.48 

Round   9, Train loss: 2.281, Test loss: 2.277, Test accuracy: 33.80 

Round  10, Train loss: 2.268, Test loss: 2.261, Test accuracy: 27.70 

Round  11, Train loss: 2.253, Test loss: 2.238, Test accuracy: 30.10 

Round  12, Train loss: 2.215, Test loss: 2.205, Test accuracy: 32.28 

Round  13, Train loss: 2.170, Test loss: 2.159, Test accuracy: 37.42 

Round  14, Train loss: 2.128, Test loss: 2.117, Test accuracy: 42.47 

Round  15, Train loss: 2.091, Test loss: 2.076, Test accuracy: 44.67 

Round  16, Train loss: 2.035, Test loss: 2.035, Test accuracy: 46.80 

Round  17, Train loss: 2.014, Test loss: 1.995, Test accuracy: 50.87 

Round  18, Train loss: 1.963, Test loss: 1.954, Test accuracy: 55.48 

Round  19, Train loss: 1.938, Test loss: 1.918, Test accuracy: 58.63 

Round  20, Train loss: 1.889, Test loss: 1.886, Test accuracy: 62.55 

Round  21, Train loss: 1.851, Test loss: 1.854, Test accuracy: 65.88 

Round  22, Train loss: 1.810, Test loss: 1.821, Test accuracy: 68.70 

Round  23, Train loss: 1.789, Test loss: 1.789, Test accuracy: 71.73 

Round  24, Train loss: 1.761, Test loss: 1.764, Test accuracy: 73.83 

Round  25, Train loss: 1.733, Test loss: 1.742, Test accuracy: 76.12 

Round  26, Train loss: 1.715, Test loss: 1.730, Test accuracy: 76.78 

Round  27, Train loss: 1.704, Test loss: 1.712, Test accuracy: 78.70 

Round  28, Train loss: 1.686, Test loss: 1.700, Test accuracy: 79.37 

Round  29, Train loss: 1.673, Test loss: 1.692, Test accuracy: 79.83 

Round  30, Train loss: 1.685, Test loss: 1.686, Test accuracy: 80.12 

Round  31, Train loss: 1.657, Test loss: 1.681, Test accuracy: 80.73 

Round  32, Train loss: 1.649, Test loss: 1.675, Test accuracy: 80.90 

Round  33, Train loss: 1.658, Test loss: 1.670, Test accuracy: 81.22 

Round  34, Train loss: 1.636, Test loss: 1.667, Test accuracy: 81.32 

Round  35, Train loss: 1.648, Test loss: 1.666, Test accuracy: 81.43 

Round  36, Train loss: 1.635, Test loss: 1.662, Test accuracy: 81.60 

Round  37, Train loss: 1.640, Test loss: 1.663, Test accuracy: 81.43 

Round  38, Train loss: 1.637, Test loss: 1.660, Test accuracy: 81.70 

Round  39, Train loss: 1.639, Test loss: 1.656, Test accuracy: 82.05 

Round  40, Train loss: 1.631, Test loss: 1.652, Test accuracy: 82.37 

Round  41, Train loss: 1.617, Test loss: 1.651, Test accuracy: 82.25 

Round  42, Train loss: 1.637, Test loss: 1.649, Test accuracy: 82.27 

Round  43, Train loss: 1.608, Test loss: 1.648, Test accuracy: 82.28 

Round  44, Train loss: 1.618, Test loss: 1.646, Test accuracy: 82.37 

Round  45, Train loss: 1.604, Test loss: 1.645, Test accuracy: 82.38 

Round  46, Train loss: 1.625, Test loss: 1.645, Test accuracy: 82.47 

Round  47, Train loss: 1.604, Test loss: 1.645, Test accuracy: 82.53 

Round  48, Train loss: 1.620, Test loss: 1.644, Test accuracy: 82.73 

Round  49, Train loss: 1.614, Test loss: 1.643, Test accuracy: 82.77 

Round  50, Train loss: 1.616, Test loss: 1.642, Test accuracy: 82.92 

Round  51, Train loss: 1.607, Test loss: 1.642, Test accuracy: 82.88 

Round  52, Train loss: 1.589, Test loss: 1.641, Test accuracy: 83.07 

Round  53, Train loss: 1.622, Test loss: 1.640, Test accuracy: 83.03 

Round  54, Train loss: 1.601, Test loss: 1.640, Test accuracy: 83.05 

Round  55, Train loss: 1.604, Test loss: 1.640, Test accuracy: 82.80 

Round  56, Train loss: 1.594, Test loss: 1.639, Test accuracy: 83.12 

Round  57, Train loss: 1.622, Test loss: 1.639, Test accuracy: 82.93 

Round  58, Train loss: 1.595, Test loss: 1.638, Test accuracy: 82.95 

Round  59, Train loss: 1.591, Test loss: 1.637, Test accuracy: 82.95 

Round  60, Train loss: 1.600, Test loss: 1.637, Test accuracy: 83.18 

Round  61, Train loss: 1.598, Test loss: 1.637, Test accuracy: 83.00 

Round  62, Train loss: 1.598, Test loss: 1.636, Test accuracy: 83.08 

Round  63, Train loss: 1.588, Test loss: 1.638, Test accuracy: 82.93 

Round  64, Train loss: 1.596, Test loss: 1.638, Test accuracy: 82.92 

Round  65, Train loss: 1.593, Test loss: 1.637, Test accuracy: 82.88 

Round  66, Train loss: 1.600, Test loss: 1.636, Test accuracy: 83.02 

Round  67, Train loss: 1.569, Test loss: 1.636, Test accuracy: 82.97 

Round  68, Train loss: 1.591, Test loss: 1.635, Test accuracy: 83.18 

Round  69, Train loss: 1.578, Test loss: 1.632, Test accuracy: 83.47 

Round  70, Train loss: 1.573, Test loss: 1.631, Test accuracy: 83.52 

Round  71, Train loss: 1.597, Test loss: 1.630, Test accuracy: 83.60 

Round  72, Train loss: 1.587, Test loss: 1.630, Test accuracy: 83.62 

Round  73, Train loss: 1.577, Test loss: 1.629, Test accuracy: 83.77 

Round  74, Train loss: 1.591, Test loss: 1.629, Test accuracy: 83.83 

Round  75, Train loss: 1.585, Test loss: 1.630, Test accuracy: 83.63 

Round  76, Train loss: 1.553, Test loss: 1.630, Test accuracy: 83.63 

Round  77, Train loss: 1.552, Test loss: 1.630, Test accuracy: 83.47 

Round  78, Train loss: 1.589, Test loss: 1.630, Test accuracy: 83.40 

Round  79, Train loss: 1.593, Test loss: 1.629, Test accuracy: 83.47 

Round  80, Train loss: 1.577, Test loss: 1.628, Test accuracy: 83.62 

Round  81, Train loss: 1.578, Test loss: 1.628, Test accuracy: 83.53 

Round  82, Train loss: 1.568, Test loss: 1.627, Test accuracy: 84.00 

Round  83, Train loss: 1.590, Test loss: 1.627, Test accuracy: 84.05 

Round  84, Train loss: 1.590, Test loss: 1.627, Test accuracy: 83.93 

Round  85, Train loss: 1.574, Test loss: 1.627, Test accuracy: 83.88 

Round  86, Train loss: 1.559, Test loss: 1.626, Test accuracy: 83.87 

Round  87, Train loss: 1.589, Test loss: 1.626, Test accuracy: 83.95 

Round  88, Train loss: 1.577, Test loss: 1.626, Test accuracy: 83.80 

Round  89, Train loss: 1.578, Test loss: 1.625, Test accuracy: 84.15 

Round  90, Train loss: 1.598, Test loss: 1.625, Test accuracy: 84.00 

Round  91, Train loss: 1.550, Test loss: 1.625, Test accuracy: 84.10 

Round  92, Train loss: 1.576, Test loss: 1.624, Test accuracy: 84.33 

Round  93, Train loss: 1.579, Test loss: 1.624, Test accuracy: 84.27 

Round  94, Train loss: 1.559, Test loss: 1.623, Test accuracy: 84.27 

Round  95, Train loss: 1.594, Test loss: 1.623, Test accuracy: 84.38 

Round  96, Train loss: 1.573, Test loss: 1.622, Test accuracy: 84.35 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 1.569, Test loss: 1.622, Test accuracy: 84.25 

Round  98, Train loss: 1.550, Test loss: 1.622, Test accuracy: 84.33 

Round  99, Train loss: 1.582, Test loss: 1.622, Test accuracy: 84.22 

Final Round, Train loss: 1.570, Test loss: 1.621, Test accuracy: 84.52 

Average accuracy final 10 rounds: 84.25 

362.12333941459656
[0.487257719039917, 0.8807661533355713, 1.2864437103271484, 1.6951119899749756, 2.1076161861419678, 2.5192348957061768, 2.9374043941497803, 3.3588149547576904, 3.7692418098449707, 4.177155494689941, 4.585704803466797, 4.994160413742065, 5.407101392745972, 5.820838689804077, 6.2397754192352295, 6.646289587020874, 7.059704542160034, 7.464690208435059, 7.838522434234619, 8.205855131149292, 8.581127405166626, 8.959025144577026, 9.335022211074829, 9.705747842788696, 10.072954893112183, 10.439764499664307, 10.806702613830566, 11.179539442062378, 11.552603006362915, 11.928205490112305, 12.300095319747925, 12.666860342025757, 13.038381099700928, 13.4075186252594, 13.775705337524414, 14.14623475074768, 14.517895460128784, 14.892140865325928, 15.2618989944458, 15.632987976074219, 16.00127625465393, 16.370402574539185, 16.73993945121765, 17.115269899368286, 17.490248680114746, 17.865710496902466, 18.23459553718567, 18.601896286010742, 18.973117351531982, 19.342201232910156, 19.7140371799469, 20.088976860046387, 20.46270179748535, 20.834066152572632, 21.204919576644897, 21.57714557647705, 21.945763111114502, 22.315165042877197, 22.685413360595703, 23.04484486579895, 23.415126085281372, 23.781002521514893, 24.15441060066223, 24.522000789642334, 24.889681339263916, 25.25770378112793, 25.62743377685547, 25.997681856155396, 26.369651317596436, 26.738161325454712, 27.106258630752563, 27.475406646728516, 27.846681594848633, 28.217375993728638, 28.58841371536255, 28.960510730743408, 29.327869415283203, 29.696969509124756, 30.069279193878174, 30.438191413879395, 30.805190801620483, 31.17475962638855, 31.543932914733887, 31.914803504943848, 32.28459358215332, 32.65467953681946, 33.02501606941223, 33.39382839202881, 33.76118445396423, 34.12858772277832, 34.50068664550781, 34.8686957359314, 35.24222683906555, 35.609458208084106, 35.98339557647705, 36.35342812538147, 36.72155261039734, 37.095008850097656, 37.46636176109314, 37.84040975570679, 38.55731248855591]
[7.916666666666667, 12.133333333333333, 15.883333333333333, 20.866666666666667, 26.8, 31.083333333333332, 34.983333333333334, 37.0, 37.483333333333334, 33.8, 27.7, 30.1, 32.28333333333333, 37.416666666666664, 42.46666666666667, 44.666666666666664, 46.8, 50.86666666666667, 55.483333333333334, 58.63333333333333, 62.55, 65.88333333333334, 68.7, 71.73333333333333, 73.83333333333333, 76.11666666666666, 76.78333333333333, 78.7, 79.36666666666666, 79.83333333333333, 80.11666666666666, 80.73333333333333, 80.9, 81.21666666666667, 81.31666666666666, 81.43333333333334, 81.6, 81.43333333333334, 81.7, 82.05, 82.36666666666666, 82.25, 82.26666666666667, 82.28333333333333, 82.36666666666666, 82.38333333333334, 82.46666666666667, 82.53333333333333, 82.73333333333333, 82.76666666666667, 82.91666666666667, 82.88333333333334, 83.06666666666666, 83.03333333333333, 83.05, 82.8, 83.11666666666666, 82.93333333333334, 82.95, 82.95, 83.18333333333334, 83.0, 83.08333333333333, 82.93333333333334, 82.91666666666667, 82.88333333333334, 83.01666666666667, 82.96666666666667, 83.18333333333334, 83.46666666666667, 83.51666666666667, 83.6, 83.61666666666666, 83.76666666666667, 83.83333333333333, 83.63333333333334, 83.63333333333334, 83.46666666666667, 83.4, 83.46666666666667, 83.61666666666666, 83.53333333333333, 84.0, 84.05, 83.93333333333334, 83.88333333333334, 83.86666666666666, 83.95, 83.8, 84.15, 84.0, 84.1, 84.33333333333333, 84.26666666666667, 84.26666666666667, 84.38333333333334, 84.35, 84.25, 84.33333333333333, 84.21666666666667, 84.51666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.300, Test accuracy: 18.80 

Round   1, Train loss: 2.299, Test loss: 2.298, Test accuracy: 30.00 

Round   2, Train loss: 2.296, Test loss: 2.294, Test accuracy: 44.45 

Round   3, Train loss: 2.291, Test loss: 2.289, Test accuracy: 50.58 

Round   4, Train loss: 2.283, Test loss: 2.280, Test accuracy: 52.38 

Round   5, Train loss: 2.264, Test loss: 2.251, Test accuracy: 46.17 

Round   6, Train loss: 2.186, Test loss: 2.175, Test accuracy: 46.83 

Round   7, Train loss: 2.093, Test loss: 2.068, Test accuracy: 48.38 

Round   8, Train loss: 1.979, Test loss: 1.982, Test accuracy: 54.12 

Round   9, Train loss: 1.913, Test loss: 1.920, Test accuracy: 59.23 

Round  10, Train loss: 1.856, Test loss: 1.875, Test accuracy: 62.22 

Round  11, Train loss: 1.817, Test loss: 1.829, Test accuracy: 66.78 

Round  12, Train loss: 1.788, Test loss: 1.798, Test accuracy: 69.48 

Round  13, Train loss: 1.759, Test loss: 1.780, Test accuracy: 70.98 

Round  14, Train loss: 1.743, Test loss: 1.761, Test accuracy: 72.47 

Round  15, Train loss: 1.737, Test loss: 1.748, Test accuracy: 73.35 

Round  16, Train loss: 1.717, Test loss: 1.744, Test accuracy: 73.58 

Round  17, Train loss: 1.725, Test loss: 1.737, Test accuracy: 73.95 

Round  18, Train loss: 1.708, Test loss: 1.735, Test accuracy: 73.95 

Round  19, Train loss: 1.707, Test loss: 1.731, Test accuracy: 74.07 

Round  20, Train loss: 1.709, Test loss: 1.726, Test accuracy: 74.43 

Round  21, Train loss: 1.697, Test loss: 1.724, Test accuracy: 74.65 

Round  22, Train loss: 1.689, Test loss: 1.723, Test accuracy: 74.62 

Round  23, Train loss: 1.695, Test loss: 1.721, Test accuracy: 74.92 

Round  24, Train loss: 1.683, Test loss: 1.720, Test accuracy: 74.85 

Round  25, Train loss: 1.680, Test loss: 1.719, Test accuracy: 74.73 

Round  26, Train loss: 1.687, Test loss: 1.718, Test accuracy: 74.80 

Round  27, Train loss: 1.692, Test loss: 1.718, Test accuracy: 74.80 

Round  28, Train loss: 1.699, Test loss: 1.718, Test accuracy: 74.82 

Round  29, Train loss: 1.688, Test loss: 1.716, Test accuracy: 74.97 

Round  30, Train loss: 1.675, Test loss: 1.715, Test accuracy: 74.80 

Round  31, Train loss: 1.675, Test loss: 1.715, Test accuracy: 75.05 

Round  32, Train loss: 1.674, Test loss: 1.716, Test accuracy: 74.90 

Round  33, Train loss: 1.689, Test loss: 1.714, Test accuracy: 75.02 

Round  34, Train loss: 1.671, Test loss: 1.712, Test accuracy: 75.37 

Round  35, Train loss: 1.690, Test loss: 1.712, Test accuracy: 75.40 

Round  36, Train loss: 1.682, Test loss: 1.712, Test accuracy: 75.33 

Round  37, Train loss: 1.687, Test loss: 1.711, Test accuracy: 75.50 

Round  38, Train loss: 1.679, Test loss: 1.711, Test accuracy: 75.45 

Round  39, Train loss: 1.668, Test loss: 1.709, Test accuracy: 75.68 

Round  40, Train loss: 1.675, Test loss: 1.708, Test accuracy: 75.73 

Round  41, Train loss: 1.668, Test loss: 1.707, Test accuracy: 75.82 

Round  42, Train loss: 1.667, Test loss: 1.707, Test accuracy: 75.83 

Round  43, Train loss: 1.652, Test loss: 1.706, Test accuracy: 76.05 

Round  44, Train loss: 1.668, Test loss: 1.705, Test accuracy: 76.15 

Round  45, Train loss: 1.669, Test loss: 1.706, Test accuracy: 75.90 

Round  46, Train loss: 1.657, Test loss: 1.705, Test accuracy: 76.05 

Round  47, Train loss: 1.629, Test loss: 1.704, Test accuracy: 76.13 

Round  48, Train loss: 1.661, Test loss: 1.701, Test accuracy: 76.60 

Round  49, Train loss: 1.637, Test loss: 1.700, Test accuracy: 76.67 

Round  50, Train loss: 1.674, Test loss: 1.700, Test accuracy: 76.70 

Round  51, Train loss: 1.643, Test loss: 1.699, Test accuracy: 76.72 

Round  52, Train loss: 1.653, Test loss: 1.694, Test accuracy: 77.13 

Round  53, Train loss: 1.640, Test loss: 1.692, Test accuracy: 77.42 

Round  54, Train loss: 1.643, Test loss: 1.690, Test accuracy: 77.82 

Round  55, Train loss: 1.628, Test loss: 1.688, Test accuracy: 77.90 

Round  56, Train loss: 1.623, Test loss: 1.684, Test accuracy: 78.27 

Round  57, Train loss: 1.603, Test loss: 1.680, Test accuracy: 78.60 

Round  58, Train loss: 1.647, Test loss: 1.679, Test accuracy: 78.63 

Round  59, Train loss: 1.612, Test loss: 1.675, Test accuracy: 79.15 

Round  60, Train loss: 1.607, Test loss: 1.672, Test accuracy: 79.33 

Round  61, Train loss: 1.619, Test loss: 1.668, Test accuracy: 79.70 

Round  62, Train loss: 1.610, Test loss: 1.664, Test accuracy: 80.27 

Round  63, Train loss: 1.593, Test loss: 1.662, Test accuracy: 80.42 

Round  64, Train loss: 1.602, Test loss: 1.659, Test accuracy: 80.80 

Round  65, Train loss: 1.567, Test loss: 1.655, Test accuracy: 81.10 

Round  66, Train loss: 1.593, Test loss: 1.654, Test accuracy: 81.27 

Round  67, Train loss: 1.581, Test loss: 1.649, Test accuracy: 81.72 

Round  68, Train loss: 1.583, Test loss: 1.646, Test accuracy: 81.97 

Round  69, Train loss: 1.585, Test loss: 1.646, Test accuracy: 81.95 

Round  70, Train loss: 1.600, Test loss: 1.638, Test accuracy: 82.78 

Round  71, Train loss: 1.562, Test loss: 1.637, Test accuracy: 83.13 

Round  72, Train loss: 1.579, Test loss: 1.631, Test accuracy: 83.73 

Round  73, Train loss: 1.566, Test loss: 1.629, Test accuracy: 83.85 

Round  74, Train loss: 1.546, Test loss: 1.627, Test accuracy: 83.97 

Round  75, Train loss: 1.573, Test loss: 1.619, Test accuracy: 84.82 

Round  76, Train loss: 1.567, Test loss: 1.615, Test accuracy: 85.35 

Round  77, Train loss: 1.524, Test loss: 1.611, Test accuracy: 85.63 

Round  78, Train loss: 1.555, Test loss: 1.604, Test accuracy: 86.35 

Round  79, Train loss: 1.524, Test loss: 1.601, Test accuracy: 86.57 

Round  80, Train loss: 1.533, Test loss: 1.598, Test accuracy: 86.87 

Round  81, Train loss: 1.526, Test loss: 1.597, Test accuracy: 86.87 

Round  82, Train loss: 1.529, Test loss: 1.597, Test accuracy: 86.75 

Round  83, Train loss: 1.550, Test loss: 1.590, Test accuracy: 87.70 

Round  84, Train loss: 1.504, Test loss: 1.589, Test accuracy: 87.73 

Round  85, Train loss: 1.531, Test loss: 1.580, Test accuracy: 88.67 

Round  86, Train loss: 1.512, Test loss: 1.576, Test accuracy: 89.38 

Round  87, Train loss: 1.494, Test loss: 1.574, Test accuracy: 89.33 

Round  88, Train loss: 1.516, Test loss: 1.572, Test accuracy: 89.35 

Round  89, Train loss: 1.506, Test loss: 1.570, Test accuracy: 89.58 

Round  90, Train loss: 1.513, Test loss: 1.570, Test accuracy: 89.72 

Round  91, Train loss: 1.524, Test loss: 1.570, Test accuracy: 89.63 

Round  92, Train loss: 1.496, Test loss: 1.569, Test accuracy: 89.72 

Round  93, Train loss: 1.513, Test loss: 1.568, Test accuracy: 89.75 

Round  94, Train loss: 1.510, Test loss: 1.568, Test accuracy: 89.77 

Round  95, Train loss: 1.495, Test loss: 1.566, Test accuracy: 89.98 

Round  96, Train loss: 1.486, Test loss: 1.566, Test accuracy: 90.15 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 1.498, Test loss: 1.565, Test accuracy: 90.08 

Round  98, Train loss: 1.494, Test loss: 1.565, Test accuracy: 90.05 

Round  99, Train loss: 1.494, Test loss: 1.565, Test accuracy: 90.07 

Final Round, Train loss: 1.500, Test loss: 1.566, Test accuracy: 89.92 

Average accuracy final 10 rounds: 89.89166666666665 

407.7642397880554
[0.5409283638000488, 0.9951682090759277, 1.4496374130249023, 1.9033453464508057, 2.3562018871307373, 2.806838274002075, 3.26350736618042, 3.720792055130005, 4.171487808227539, 4.621882200241089, 5.0731751918792725, 5.525545358657837, 5.979192495346069, 6.434047222137451, 6.8924572467803955, 7.34738564491272, 7.79965353012085, 8.25204086303711, 8.703572750091553, 9.159320831298828, 9.615415334701538, 10.067798614501953, 10.520601511001587, 10.97398591041565, 11.42657208442688, 11.878801584243774, 12.335839033126831, 12.790773868560791, 13.244305849075317, 13.696988582611084, 14.150573015213013, 14.605364561080933, 15.05793309211731, 15.519176721572876, 15.976807832717896, 16.428571224212646, 16.880488395690918, 17.333880186080933, 17.78596782684326, 18.24272394180298, 18.698538303375244, 19.151719570159912, 19.60590934753418, 20.05986785888672, 20.51058340072632, 20.963435173034668, 21.421310901641846, 21.88039255142212, 22.32953953742981, 22.782142877578735, 23.232200622558594, 23.681597232818604, 24.13169765472412, 24.587992429733276, 25.042754411697388, 25.494540691375732, 25.944847106933594, 26.398128271102905, 26.8487389087677, 27.302436351776123, 27.760578393936157, 28.218326330184937, 28.67420768737793, 29.127081632614136, 29.577536582946777, 30.030967473983765, 30.485533237457275, 30.943575859069824, 31.400180339813232, 31.851250886917114, 32.306663036346436, 32.761958837509155, 33.21228551864624, 33.668293952941895, 34.126468896865845, 34.58388710021973, 35.034740924835205, 35.48683047294617, 35.93685483932495, 36.388373136520386, 36.84572911262512, 37.30368518829346, 37.75614619255066, 38.206968784332275, 38.658958435058594, 39.11135506629944, 39.56113338470459, 40.01998686790466, 40.48167634010315, 40.934945821762085, 41.388474464416504, 41.83856439590454, 42.29126334190369, 42.74170732498169, 43.197608947753906, 43.65294671058655, 44.10233116149902, 44.55244588851929, 45.00233054161072, 45.451380014419556, 46.1934757232666]
[18.8, 30.0, 44.45, 50.583333333333336, 52.38333333333333, 46.166666666666664, 46.833333333333336, 48.38333333333333, 54.11666666666667, 59.233333333333334, 62.21666666666667, 66.78333333333333, 69.48333333333333, 70.98333333333333, 72.46666666666667, 73.35, 73.58333333333333, 73.95, 73.95, 74.06666666666666, 74.43333333333334, 74.65, 74.61666666666666, 74.91666666666667, 74.85, 74.73333333333333, 74.8, 74.8, 74.81666666666666, 74.96666666666667, 74.8, 75.05, 74.9, 75.01666666666667, 75.36666666666666, 75.4, 75.33333333333333, 75.5, 75.45, 75.68333333333334, 75.73333333333333, 75.81666666666666, 75.83333333333333, 76.05, 76.15, 75.9, 76.05, 76.13333333333334, 76.6, 76.66666666666667, 76.7, 76.71666666666667, 77.13333333333334, 77.41666666666667, 77.81666666666666, 77.9, 78.26666666666667, 78.6, 78.63333333333334, 79.15, 79.33333333333333, 79.7, 80.26666666666667, 80.41666666666667, 80.8, 81.1, 81.26666666666667, 81.71666666666667, 81.96666666666667, 81.95, 82.78333333333333, 83.13333333333334, 83.73333333333333, 83.85, 83.96666666666667, 84.81666666666666, 85.35, 85.63333333333334, 86.35, 86.56666666666666, 86.86666666666666, 86.86666666666666, 86.75, 87.7, 87.73333333333333, 88.66666666666667, 89.38333333333334, 89.33333333333333, 89.35, 89.58333333333333, 89.71666666666667, 89.63333333333334, 89.71666666666667, 89.75, 89.76666666666667, 89.98333333333333, 90.15, 90.08333333333333, 90.05, 90.06666666666666, 89.91666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 17.32 

Round   1, Train loss: 2.300, Test loss: 2.300, Test accuracy: 22.92 

Round   2, Train loss: 2.298, Test loss: 2.298, Test accuracy: 30.78 

Round   3, Train loss: 2.299, Test loss: 2.297, Test accuracy: 31.62 

Round   4, Train loss: 2.296, Test loss: 2.295, Test accuracy: 41.18 

Round   5, Train loss: 2.293, Test loss: 2.293, Test accuracy: 42.57 

Round   6, Train loss: 2.288, Test loss: 2.289, Test accuracy: 43.43 

Round   7, Train loss: 2.285, Test loss: 2.285, Test accuracy: 45.58 

Round   8, Train loss: 2.264, Test loss: 2.271, Test accuracy: 44.37 

Round   9, Train loss: 2.213, Test loss: 2.236, Test accuracy: 42.85 

Round  10, Train loss: 2.215, Test loss: 2.194, Test accuracy: 41.73 

Round  11, Train loss: 2.106, Test loss: 2.132, Test accuracy: 45.35 

Round  12, Train loss: 2.108, Test loss: 2.076, Test accuracy: 48.42 

Round  13, Train loss: 1.983, Test loss: 2.012, Test accuracy: 56.38 

Round  14, Train loss: 1.900, Test loss: 1.960, Test accuracy: 61.77 

Round  15, Train loss: 1.852, Test loss: 1.907, Test accuracy: 64.48 

Round  16, Train loss: 1.834, Test loss: 1.858, Test accuracy: 67.43 

Round  17, Train loss: 1.740, Test loss: 1.814, Test accuracy: 72.08 

Round  18, Train loss: 1.713, Test loss: 1.775, Test accuracy: 74.88 

Round  19, Train loss: 1.665, Test loss: 1.751, Test accuracy: 76.02 

Round  20, Train loss: 1.641, Test loss: 1.734, Test accuracy: 76.67 

Round  21, Train loss: 1.614, Test loss: 1.726, Test accuracy: 76.93 

Round  22, Train loss: 1.627, Test loss: 1.719, Test accuracy: 77.18 

Round  23, Train loss: 1.623, Test loss: 1.704, Test accuracy: 78.32 

Round  24, Train loss: 1.614, Test loss: 1.697, Test accuracy: 78.70 

Round  25, Train loss: 1.597, Test loss: 1.695, Test accuracy: 78.62 

Round  26, Train loss: 1.580, Test loss: 1.693, Test accuracy: 78.62 

Round  27, Train loss: 1.605, Test loss: 1.690, Test accuracy: 78.57 

Round  28, Train loss: 1.584, Test loss: 1.683, Test accuracy: 80.62 

Round  29, Train loss: 1.559, Test loss: 1.675, Test accuracy: 81.78 

Round  30, Train loss: 1.527, Test loss: 1.674, Test accuracy: 81.48 

Round  31, Train loss: 1.529, Test loss: 1.667, Test accuracy: 81.98 

Round  32, Train loss: 1.513, Test loss: 1.660, Test accuracy: 82.58 

Round  33, Train loss: 1.508, Test loss: 1.659, Test accuracy: 82.23 

Round  34, Train loss: 1.515, Test loss: 1.655, Test accuracy: 82.30 

Round  35, Train loss: 1.516, Test loss: 1.652, Test accuracy: 82.55 

Round  36, Train loss: 1.500, Test loss: 1.650, Test accuracy: 82.58 

Round  37, Train loss: 1.505, Test loss: 1.646, Test accuracy: 82.90 

Round  38, Train loss: 1.499, Test loss: 1.646, Test accuracy: 82.95 

Round  39, Train loss: 1.492, Test loss: 1.645, Test accuracy: 82.93 

Round  40, Train loss: 1.498, Test loss: 1.645, Test accuracy: 82.87 

Round  41, Train loss: 1.488, Test loss: 1.644, Test accuracy: 82.92 

Round  42, Train loss: 1.491, Test loss: 1.643, Test accuracy: 82.90 

Round  43, Train loss: 1.494, Test loss: 1.643, Test accuracy: 82.88 

Round  44, Train loss: 1.490, Test loss: 1.642, Test accuracy: 83.02 

Round  45, Train loss: 1.487, Test loss: 1.642, Test accuracy: 83.02 

Round  46, Train loss: 1.486, Test loss: 1.641, Test accuracy: 83.03 

Round  47, Train loss: 1.490, Test loss: 1.641, Test accuracy: 82.90 

Round  48, Train loss: 1.480, Test loss: 1.640, Test accuracy: 83.00 

Round  49, Train loss: 1.490, Test loss: 1.640, Test accuracy: 82.93 

Round  50, Train loss: 1.485, Test loss: 1.639, Test accuracy: 82.92 

Round  51, Train loss: 1.493, Test loss: 1.640, Test accuracy: 82.87 

Round  52, Train loss: 1.485, Test loss: 1.640, Test accuracy: 82.98 

Round  53, Train loss: 1.483, Test loss: 1.639, Test accuracy: 82.95 

Round  54, Train loss: 1.491, Test loss: 1.639, Test accuracy: 83.00 

Round  55, Train loss: 1.480, Test loss: 1.639, Test accuracy: 83.05 

Round  56, Train loss: 1.482, Test loss: 1.638, Test accuracy: 82.97 

Round  57, Train loss: 1.481, Test loss: 1.638, Test accuracy: 83.05 

Round  58, Train loss: 1.481, Test loss: 1.638, Test accuracy: 83.05 

Round  59, Train loss: 1.486, Test loss: 1.638, Test accuracy: 83.13 

Round  60, Train loss: 1.478, Test loss: 1.638, Test accuracy: 83.13 

Round  61, Train loss: 1.492, Test loss: 1.638, Test accuracy: 83.03 

Round  62, Train loss: 1.489, Test loss: 1.638, Test accuracy: 83.03 

Round  63, Train loss: 1.487, Test loss: 1.638, Test accuracy: 83.10 

Round  64, Train loss: 1.482, Test loss: 1.638, Test accuracy: 83.05 

Round  65, Train loss: 1.487, Test loss: 1.638, Test accuracy: 83.00 

Round  66, Train loss: 1.484, Test loss: 1.638, Test accuracy: 82.95 

Round  67, Train loss: 1.476, Test loss: 1.638, Test accuracy: 82.98 

Round  68, Train loss: 1.476, Test loss: 1.637, Test accuracy: 82.98 

Round  69, Train loss: 1.485, Test loss: 1.637, Test accuracy: 83.00 

Round  70, Train loss: 1.479, Test loss: 1.637, Test accuracy: 83.12 

Round  71, Train loss: 1.483, Test loss: 1.637, Test accuracy: 83.00 

Round  72, Train loss: 1.486, Test loss: 1.637, Test accuracy: 83.03 

Round  73, Train loss: 1.485, Test loss: 1.637, Test accuracy: 83.03 

Round  74, Train loss: 1.483, Test loss: 1.637, Test accuracy: 83.12 

Round  75, Train loss: 1.479, Test loss: 1.637, Test accuracy: 83.10 

Round  76, Train loss: 1.482, Test loss: 1.637, Test accuracy: 83.13 

Round  77, Train loss: 1.482, Test loss: 1.636, Test accuracy: 83.12 

Round  78, Train loss: 1.481, Test loss: 1.636, Test accuracy: 83.12 

Round  79, Train loss: 1.483, Test loss: 1.636, Test accuracy: 83.15 

Round  80, Train loss: 1.486, Test loss: 1.636, Test accuracy: 83.20 

Round  81, Train loss: 1.480, Test loss: 1.636, Test accuracy: 83.18 

Round  82, Train loss: 1.481, Test loss: 1.636, Test accuracy: 83.17 

Round  83, Train loss: 1.482, Test loss: 1.636, Test accuracy: 83.20 

Round  84, Train loss: 1.483, Test loss: 1.636, Test accuracy: 83.17 

Round  85, Train loss: 1.488, Test loss: 1.636, Test accuracy: 83.18 

Round  86, Train loss: 1.489, Test loss: 1.636, Test accuracy: 83.17 

Round  87, Train loss: 1.481, Test loss: 1.636, Test accuracy: 83.08 

Round  88, Train loss: 1.484, Test loss: 1.636, Test accuracy: 83.10 

Round  89, Train loss: 1.479, Test loss: 1.636, Test accuracy: 83.08 

Round  90, Train loss: 1.480, Test loss: 1.636, Test accuracy: 83.08 

Round  91, Train loss: 1.482, Test loss: 1.635, Test accuracy: 83.08 

Round  92, Train loss: 1.485, Test loss: 1.635, Test accuracy: 83.15 

Round  93, Train loss: 1.483, Test loss: 1.635, Test accuracy: 83.15 

Round  94, Train loss: 1.477, Test loss: 1.635, Test accuracy: 83.15 

Round  95, Train loss: 1.484, Test loss: 1.635, Test accuracy: 83.12 

Round  96, Train loss: 1.482, Test loss: 1.635, Test accuracy: 83.15 

Round  97, Train loss: 1.481, Test loss: 1.635, Test accuracy: 83.20 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 1.482, Test loss: 1.635, Test accuracy: 83.18 

Round  99, Train loss: 1.480, Test loss: 1.635, Test accuracy: 83.20 

Final Round, Train loss: 1.481, Test loss: 1.635, Test accuracy: 83.20 

Average accuracy final 10 rounds: 83.14666666666665 

410.8892707824707
[0.5448095798492432, 0.9981906414031982, 1.4524812698364258, 1.9109585285186768, 2.3652782440185547, 2.8234245777130127, 3.278489112854004, 3.734861373901367, 4.189744234085083, 4.642908811569214, 5.102741718292236, 5.558742046356201, 6.008871078491211, 6.465338945388794, 6.920605897903442, 7.371825695037842, 7.82998514175415, 8.28705358505249, 8.748699188232422, 9.202770233154297, 9.652282953262329, 10.109473466873169, 10.56213665008545, 11.016321420669556, 11.476469278335571, 11.931370258331299, 12.382124185562134, 12.831827402114868, 13.28219747543335, 13.740391492843628, 14.198841333389282, 14.655132055282593, 15.109740734100342, 15.559993743896484, 16.010509729385376, 16.463931560516357, 16.91633105278015, 17.36921501159668, 17.823713541030884, 18.275415658950806, 18.73323369026184, 19.186420679092407, 19.634690284729004, 20.0886390209198, 20.54794430732727, 21.0012686252594, 21.456902027130127, 21.905627727508545, 22.35375690460205, 22.80568027496338, 23.25927233695984, 23.722382307052612, 24.17821979522705, 24.62627625465393, 25.079612493515015, 25.530165433883667, 25.97908306121826, 26.440272092819214, 26.90222716331482, 27.356069803237915, 27.809022426605225, 28.25953197479248, 28.711256742477417, 29.16358709335327, 29.621794939041138, 30.084471225738525, 30.541677951812744, 30.990527391433716, 31.443198919296265, 31.895172119140625, 32.35231351852417, 32.81340479850769, 33.269262075424194, 33.72020769119263, 34.171326637268066, 34.620113372802734, 35.073755979537964, 35.526028871536255, 35.982627391815186, 36.44149827957153, 36.89650869369507, 37.35051918029785, 37.80409526824951, 38.25378942489624, 38.707932233810425, 39.16657638549805, 39.62257194519043, 40.0737738609314, 40.5228545665741, 40.981436252593994, 41.43133592605591, 41.881524324417114, 42.33999538421631, 42.79662275314331, 43.247201442718506, 43.69836139678955, 44.15268039703369, 44.60392785072327, 45.05818176269531, 45.51664876937866, 46.366199016571045]
[17.316666666666666, 22.916666666666668, 30.783333333333335, 31.616666666666667, 41.18333333333333, 42.56666666666667, 43.43333333333333, 45.583333333333336, 44.36666666666667, 42.85, 41.733333333333334, 45.35, 48.416666666666664, 56.38333333333333, 61.766666666666666, 64.48333333333333, 67.43333333333334, 72.08333333333333, 74.88333333333334, 76.01666666666667, 76.66666666666667, 76.93333333333334, 77.18333333333334, 78.31666666666666, 78.7, 78.61666666666666, 78.61666666666666, 78.56666666666666, 80.61666666666666, 81.78333333333333, 81.48333333333333, 81.98333333333333, 82.58333333333333, 82.23333333333333, 82.3, 82.55, 82.58333333333333, 82.9, 82.95, 82.93333333333334, 82.86666666666666, 82.91666666666667, 82.9, 82.88333333333334, 83.01666666666667, 83.01666666666667, 83.03333333333333, 82.9, 83.0, 82.93333333333334, 82.91666666666667, 82.86666666666666, 82.98333333333333, 82.95, 83.0, 83.05, 82.96666666666667, 83.05, 83.05, 83.13333333333334, 83.13333333333334, 83.03333333333333, 83.03333333333333, 83.1, 83.05, 83.0, 82.95, 82.98333333333333, 82.98333333333333, 83.0, 83.11666666666666, 83.0, 83.03333333333333, 83.03333333333333, 83.11666666666666, 83.1, 83.13333333333334, 83.11666666666666, 83.11666666666666, 83.15, 83.2, 83.18333333333334, 83.16666666666667, 83.2, 83.16666666666667, 83.18333333333334, 83.16666666666667, 83.08333333333333, 83.1, 83.08333333333333, 83.08333333333333, 83.08333333333333, 83.15, 83.15, 83.15, 83.11666666666666, 83.15, 83.2, 83.18333333333334, 83.2, 83.2]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.714, Test loss: 2.297, Test accuracy: 38.18
Round   1, Train loss: 1.652, Test loss: 2.283, Test accuracy: 40.68
Round   2, Train loss: 1.551, Test loss: 2.258, Test accuracy: 45.52
Round   3, Train loss: 1.490, Test loss: 2.214, Test accuracy: 50.80
Round   4, Train loss: 1.393, Test loss: 2.181, Test accuracy: 56.70
Round   5, Train loss: 1.353, Test loss: 2.146, Test accuracy: 60.77
Round   6, Train loss: 1.318, Test loss: 2.120, Test accuracy: 62.62
Round   7, Train loss: 1.308, Test loss: 2.098, Test accuracy: 65.40
Round   8, Train loss: 1.280, Test loss: 2.082, Test accuracy: 67.08
Round   9, Train loss: 1.278, Test loss: 2.068, Test accuracy: 68.38
Round  10, Train loss: 1.285, Test loss: 2.059, Test accuracy: 69.75
Round  11, Train loss: 1.260, Test loss: 2.049, Test accuracy: 70.43
Round  12, Train loss: 1.260, Test loss: 2.045, Test accuracy: 70.30
Round  13, Train loss: 1.267, Test loss: 2.039, Test accuracy: 70.70
Round  14, Train loss: 1.254, Test loss: 2.035, Test accuracy: 70.98
Round  15, Train loss: 1.253, Test loss: 2.034, Test accuracy: 70.63
Round  16, Train loss: 1.257, Test loss: 2.029, Test accuracy: 70.63
Round  17, Train loss: 1.246, Test loss: 2.025, Test accuracy: 70.62
Round  18, Train loss: 1.261, Test loss: 2.024, Test accuracy: 70.43
Round  19, Train loss: 1.251, Test loss: 2.022, Test accuracy: 69.83
Round  20, Train loss: 1.249, Test loss: 2.020, Test accuracy: 69.68
Round  21, Train loss: 1.249, Test loss: 2.019, Test accuracy: 69.67
Round  22, Train loss: 1.255, Test loss: 2.018, Test accuracy: 69.62
Round  23, Train loss: 1.254, Test loss: 2.017, Test accuracy: 69.07
Round  24, Train loss: 1.245, Test loss: 2.016, Test accuracy: 69.03
Round  25, Train loss: 1.254, Test loss: 2.016, Test accuracy: 68.95
Round  26, Train loss: 1.247, Test loss: 2.014, Test accuracy: 68.78
Round  27, Train loss: 1.247, Test loss: 2.014, Test accuracy: 68.55
Round  28, Train loss: 1.245, Test loss: 2.014, Test accuracy: 68.13
Round  29, Train loss: 1.249, Test loss: 2.013, Test accuracy: 67.85
Round  30, Train loss: 1.249, Test loss: 2.013, Test accuracy: 67.70
Round  31, Train loss: 1.248, Test loss: 2.012, Test accuracy: 67.55
Round  32, Train loss: 1.244, Test loss: 2.013, Test accuracy: 67.32
Round  33, Train loss: 1.247, Test loss: 2.012, Test accuracy: 67.27
Round  34, Train loss: 1.245, Test loss: 2.011, Test accuracy: 67.02
Round  35, Train loss: 1.248, Test loss: 2.012, Test accuracy: 66.47
Round  36, Train loss: 1.253, Test loss: 2.012, Test accuracy: 66.13
Round  37, Train loss: 1.245, Test loss: 2.012, Test accuracy: 65.78
Round  38, Train loss: 1.255, Test loss: 2.012, Test accuracy: 66.00
Round  39, Train loss: 1.250, Test loss: 2.013, Test accuracy: 65.43
Round  40, Train loss: 1.249, Test loss: 2.013, Test accuracy: 65.45
Round  41, Train loss: 1.249, Test loss: 2.011, Test accuracy: 65.43
Round  42, Train loss: 1.242, Test loss: 2.011, Test accuracy: 65.28
Round  43, Train loss: 1.247, Test loss: 2.011, Test accuracy: 65.13
Round  44, Train loss: 1.243, Test loss: 2.012, Test accuracy: 64.98
Round  45, Train loss: 1.254, Test loss: 2.011, Test accuracy: 64.80
Round  46, Train loss: 1.251, Test loss: 2.012, Test accuracy: 64.63
Round  47, Train loss: 1.241, Test loss: 2.010, Test accuracy: 64.77
Round  48, Train loss: 1.243, Test loss: 2.011, Test accuracy: 64.43
Round  49, Train loss: 1.242, Test loss: 2.011, Test accuracy: 64.23
Round  50, Train loss: 1.241, Test loss: 2.010, Test accuracy: 64.42
Round  51, Train loss: 1.244, Test loss: 2.010, Test accuracy: 64.22
Round  52, Train loss: 1.241, Test loss: 2.011, Test accuracy: 63.82
Round  53, Train loss: 1.250, Test loss: 2.012, Test accuracy: 63.58
Round  54, Train loss: 1.246, Test loss: 2.012, Test accuracy: 63.32
Round  55, Train loss: 1.241, Test loss: 2.012, Test accuracy: 63.43
Round  56, Train loss: 1.233, Test loss: 2.011, Test accuracy: 63.33
Round  57, Train loss: 1.240, Test loss: 2.012, Test accuracy: 62.88
Round  58, Train loss: 1.247, Test loss: 2.013, Test accuracy: 62.97
Round  59, Train loss: 1.242, Test loss: 2.013, Test accuracy: 62.95
Round  60, Train loss: 1.239, Test loss: 2.014, Test accuracy: 62.20
Round  61, Train loss: 1.242, Test loss: 2.014, Test accuracy: 61.85
Round  62, Train loss: 1.236, Test loss: 2.014, Test accuracy: 61.50
Round  63, Train loss: 1.249, Test loss: 2.014, Test accuracy: 61.32
Round  64, Train loss: 1.242, Test loss: 2.014, Test accuracy: 61.13
Round  65, Train loss: 1.253, Test loss: 2.014, Test accuracy: 61.12
Round  66, Train loss: 1.251, Test loss: 2.015, Test accuracy: 61.10
Round  67, Train loss: 1.247, Test loss: 2.014, Test accuracy: 61.17
Round  68, Train loss: 1.233, Test loss: 2.014, Test accuracy: 60.83
Round  69, Train loss: 1.243, Test loss: 2.015, Test accuracy: 60.55
Round  70, Train loss: 1.248, Test loss: 2.015, Test accuracy: 60.50
Round  71, Train loss: 1.241, Test loss: 2.016, Test accuracy: 60.35
Round  72, Train loss: 1.247, Test loss: 2.016, Test accuracy: 60.05
Round  73, Train loss: 1.247, Test loss: 2.017, Test accuracy: 59.93
Round  74, Train loss: 1.233, Test loss: 2.016, Test accuracy: 59.87
Round  75, Train loss: 1.246, Test loss: 2.016, Test accuracy: 59.97
Round  76, Train loss: 1.245, Test loss: 2.016, Test accuracy: 59.83
Round  77, Train loss: 1.245, Test loss: 2.017, Test accuracy: 59.80
Round  78, Train loss: 1.241, Test loss: 2.017, Test accuracy: 59.58
Round  79, Train loss: 1.246, Test loss: 2.017, Test accuracy: 59.63
Round  80, Train loss: 1.243, Test loss: 2.017, Test accuracy: 59.38
Round  81, Train loss: 1.248, Test loss: 2.018, Test accuracy: 59.25
Round  82, Train loss: 1.247, Test loss: 2.018, Test accuracy: 59.35
Round  83, Train loss: 1.248, Test loss: 2.018, Test accuracy: 59.22
Round  84, Train loss: 1.239, Test loss: 2.018, Test accuracy: 59.12
Round  85, Train loss: 1.235, Test loss: 2.018, Test accuracy: 59.10
Round  86, Train loss: 1.235, Test loss: 2.018, Test accuracy: 59.15
Round  87, Train loss: 1.227, Test loss: 2.019, Test accuracy: 58.98
Round  88, Train loss: 1.244, Test loss: 2.019, Test accuracy: 58.87
Round  89, Train loss: 1.239, Test loss: 2.020, Test accuracy: 58.85
Round  90, Train loss: 1.241, Test loss: 2.020, Test accuracy: 58.83
Round  91, Train loss: 1.242, Test loss: 2.024, Test accuracy: 58.82
Round  92, Train loss: 1.218, Test loss: 2.024, Test accuracy: 60.35
Round  93, Train loss: 1.201, Test loss: 2.018, Test accuracy: 63.18
Round  94, Train loss: 1.191, Test loss: 2.015, Test accuracy: 64.20
Round  95, Train loss: 1.195, Test loss: 2.015, Test accuracy: 64.75
Round  96, Train loss: 1.189, Test loss: 2.013, Test accuracy: 65.63
Round  97, Train loss: 1.188, Test loss: 2.009, Test accuracy: 66.10
Round  98, Train loss: 1.180, Test loss: 2.010, Test accuracy: 66.22
Round  99, Train loss: 1.183, Test loss: 2.008, Test accuracy: 66.33
Final Round, Train loss: 1.182, Test loss: 2.008, Test accuracy: 65.60
Average accuracy final 10 rounds: 63.44166666666667
665.707957983017
[]
[38.18333333333333, 40.68333333333333, 45.516666666666666, 50.8, 56.7, 60.766666666666666, 62.61666666666667, 65.4, 67.08333333333333, 68.38333333333334, 69.75, 70.43333333333334, 70.3, 70.7, 70.98333333333333, 70.63333333333334, 70.63333333333334, 70.61666666666666, 70.43333333333334, 69.83333333333333, 69.68333333333334, 69.66666666666667, 69.61666666666666, 69.06666666666666, 69.03333333333333, 68.95, 68.78333333333333, 68.55, 68.13333333333334, 67.85, 67.7, 67.55, 67.31666666666666, 67.26666666666667, 67.01666666666667, 66.46666666666667, 66.13333333333334, 65.78333333333333, 66.0, 65.43333333333334, 65.45, 65.43333333333334, 65.28333333333333, 65.13333333333334, 64.98333333333333, 64.8, 64.63333333333334, 64.76666666666667, 64.43333333333334, 64.23333333333333, 64.41666666666667, 64.21666666666667, 63.81666666666667, 63.583333333333336, 63.31666666666667, 63.43333333333333, 63.333333333333336, 62.88333333333333, 62.96666666666667, 62.95, 62.2, 61.85, 61.5, 61.31666666666667, 61.13333333333333, 61.11666666666667, 61.1, 61.166666666666664, 60.833333333333336, 60.55, 60.5, 60.35, 60.05, 59.93333333333333, 59.86666666666667, 59.96666666666667, 59.833333333333336, 59.8, 59.583333333333336, 59.63333333333333, 59.38333333333333, 59.25, 59.35, 59.21666666666667, 59.11666666666667, 59.1, 59.15, 58.983333333333334, 58.86666666666667, 58.85, 58.833333333333336, 58.81666666666667, 60.35, 63.18333333333333, 64.2, 64.75, 65.63333333333334, 66.1, 66.21666666666667, 66.33333333333333, 65.6]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.82
Round   0: Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.73
Round   1, Train loss: 2.298, Test loss: 2.302, Test accuracy: 9.70
Round   1: Global train loss: 2.298, Global test loss: 2.303, Global test accuracy: 8.77
Round   2, Train loss: 2.305, Test loss: 2.301, Test accuracy: 11.18
Round   2: Global train loss: 2.305, Global test loss: 2.303, Global test accuracy: 8.77
Round   3, Train loss: 2.303, Test loss: 2.301, Test accuracy: 12.33
Round   3: Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.77
Round   4, Train loss: 2.291, Test loss: 2.301, Test accuracy: 11.60
Round   4: Global train loss: 2.291, Global test loss: 2.302, Global test accuracy: 8.78
Round   5, Train loss: 2.296, Test loss: 2.301, Test accuracy: 11.85
Round   5: Global train loss: 2.296, Global test loss: 2.302, Global test accuracy: 8.78
Round   6, Train loss: 2.305, Test loss: 2.301, Test accuracy: 12.68
Round   6: Global train loss: 2.305, Global test loss: 2.302, Global test accuracy: 8.78
Round   7, Train loss: 2.301, Test loss: 2.299, Test accuracy: 15.38
Round   7: Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 8.80
Round   8, Train loss: 2.291, Test loss: 2.299, Test accuracy: 15.02
Round   8: Global train loss: 2.291, Global test loss: 2.301, Global test accuracy: 8.87
Round   9, Train loss: 2.293, Test loss: 2.299, Test accuracy: 14.52
Round   9: Global train loss: 2.293, Global test loss: 2.301, Global test accuracy: 8.88
Round  10, Train loss: 2.293, Test loss: 2.300, Test accuracy: 13.23
Round  10: Global train loss: 2.293, Global test loss: 2.301, Global test accuracy: 8.90
Round  11, Train loss: 2.295, Test loss: 2.298, Test accuracy: 15.38
Round  11: Global train loss: 2.295, Global test loss: 2.301, Global test accuracy: 9.17
Round  12, Train loss: 2.279, Test loss: 2.296, Test accuracy: 17.43
Round  12: Global train loss: 2.279, Global test loss: 2.300, Global test accuracy: 9.67
Round  13, Train loss: 2.276, Test loss: 2.294, Test accuracy: 18.48
Round  13: Global train loss: 2.276, Global test loss: 2.300, Global test accuracy: 11.00
Round  14, Train loss: 2.276, Test loss: 2.293, Test accuracy: 19.02
Round  14: Global train loss: 2.276, Global test loss: 2.300, Global test accuracy: 13.98
Round  15, Train loss: 2.294, Test loss: 2.294, Test accuracy: 18.02
Round  15: Global train loss: 2.294, Global test loss: 2.299, Global test accuracy: 15.32
Round  16, Train loss: 2.282, Test loss: 2.292, Test accuracy: 17.55
Round  16: Global train loss: 2.282, Global test loss: 2.299, Global test accuracy: 15.60
Round  17, Train loss: 2.240, Test loss: 2.288, Test accuracy: 19.57
Round  17: Global train loss: 2.240, Global test loss: 2.299, Global test accuracy: 17.43
Round  18, Train loss: 2.245, Test loss: 2.288, Test accuracy: 18.12
Round  18: Global train loss: 2.245, Global test loss: 2.298, Global test accuracy: 17.87
Round  19, Train loss: 2.281, Test loss: 2.287, Test accuracy: 19.02
Round  19: Global train loss: 2.281, Global test loss: 2.298, Global test accuracy: 19.30
Round  20, Train loss: 2.228, Test loss: 2.282, Test accuracy: 20.92
Round  20: Global train loss: 2.228, Global test loss: 2.297, Global test accuracy: 20.53
Round  21, Train loss: 2.266, Test loss: 2.281, Test accuracy: 20.13
Round  21: Global train loss: 2.266, Global test loss: 2.297, Global test accuracy: 22.18
Round  22, Train loss: 1.992, Test loss: 2.255, Test accuracy: 24.65
Round  22: Global train loss: 1.992, Global test loss: 2.294, Global test accuracy: 25.37
Round  23, Train loss: 2.250, Test loss: 2.258, Test accuracy: 22.52
Round  23: Global train loss: 2.250, Global test loss: 2.293, Global test accuracy: 26.28
Round  24, Train loss: 2.362, Test loss: 2.273, Test accuracy: 21.20
Round  24: Global train loss: 2.362, Global test loss: 2.294, Global test accuracy: 25.05
Round  25, Train loss: 2.141, Test loss: 2.263, Test accuracy: 22.75
Round  25: Global train loss: 2.141, Global test loss: 2.292, Global test accuracy: 26.87
Round  26, Train loss: 2.335, Test loss: 2.273, Test accuracy: 21.83
Round  26: Global train loss: 2.335, Global test loss: 2.293, Global test accuracy: 26.07
Round  27, Train loss: 2.274, Test loss: 2.280, Test accuracy: 21.00
Round  27: Global train loss: 2.274, Global test loss: 2.293, Global test accuracy: 25.82
Round  28, Train loss: 2.156, Test loss: 2.271, Test accuracy: 19.95
Round  28: Global train loss: 2.156, Global test loss: 2.293, Global test accuracy: 26.42
Round  29, Train loss: 1.929, Test loss: 2.258, Test accuracy: 21.27
Round  29: Global train loss: 1.929, Global test loss: 2.292, Global test accuracy: 27.58
Round  30, Train loss: 2.137, Test loss: 2.252, Test accuracy: 23.32
Round  30: Global train loss: 2.137, Global test loss: 2.291, Global test accuracy: 27.87
Round  31, Train loss: 2.096, Test loss: 2.264, Test accuracy: 21.37
Round  31: Global train loss: 2.096, Global test loss: 2.292, Global test accuracy: 27.08
Round  32, Train loss: 1.846, Test loss: 2.243, Test accuracy: 23.18
Round  32: Global train loss: 1.846, Global test loss: 2.290, Global test accuracy: 27.97
Round  33, Train loss: 1.955, Test loss: 2.244, Test accuracy: 24.32
Round  33: Global train loss: 1.955, Global test loss: 2.291, Global test accuracy: 29.77
Round  34, Train loss: 2.131, Test loss: 2.242, Test accuracy: 26.12
Round  34: Global train loss: 2.131, Global test loss: 2.291, Global test accuracy: 28.07
Round  35, Train loss: 2.226, Test loss: 2.255, Test accuracy: 24.85
Round  35: Global train loss: 2.226, Global test loss: 2.292, Global test accuracy: 24.03
Round  36, Train loss: 1.545, Test loss: 2.218, Test accuracy: 28.72
Round  36: Global train loss: 1.545, Global test loss: 2.289, Global test accuracy: 30.58
Round  37, Train loss: 1.886, Test loss: 2.236, Test accuracy: 25.77
Round  37: Global train loss: 1.886, Global test loss: 2.290, Global test accuracy: 28.05
Round  38, Train loss: 2.046, Test loss: 2.225, Test accuracy: 26.95
Round  38: Global train loss: 2.046, Global test loss: 2.289, Global test accuracy: 28.77
Round  39, Train loss: 1.430, Test loss: 2.211, Test accuracy: 29.67
Round  39: Global train loss: 1.430, Global test loss: 2.288, Global test accuracy: 29.40
Round  40, Train loss: 2.004, Test loss: 2.232, Test accuracy: 27.12
Round  40: Global train loss: 2.004, Global test loss: 2.290, Global test accuracy: 27.47
Round  41, Train loss: 2.368, Test loss: 2.248, Test accuracy: 23.07
Round  41: Global train loss: 2.368, Global test loss: 2.292, Global test accuracy: 23.02
Round  42, Train loss: 1.580, Test loss: 2.224, Test accuracy: 23.50
Round  42: Global train loss: 1.580, Global test loss: 2.291, Global test accuracy: 22.35
Round  43, Train loss: 1.606, Test loss: 2.191, Test accuracy: 30.50
Round  43: Global train loss: 1.606, Global test loss: 2.287, Global test accuracy: 26.88
Round  44, Train loss: 2.312, Test loss: 2.212, Test accuracy: 25.73
Round  44: Global train loss: 2.312, Global test loss: 2.289, Global test accuracy: 26.08
Round  45, Train loss: 1.533, Test loss: 2.205, Test accuracy: 26.63
Round  45: Global train loss: 1.533, Global test loss: 2.289, Global test accuracy: 25.57
Round  46, Train loss: 1.301, Test loss: 2.199, Test accuracy: 25.70
Round  46: Global train loss: 1.301, Global test loss: 2.288, Global test accuracy: 27.58
Round  47, Train loss: 1.717, Test loss: 2.167, Test accuracy: 30.22
Round  47: Global train loss: 1.717, Global test loss: 2.286, Global test accuracy: 30.82
Round  48, Train loss: 1.186, Test loss: 2.174, Test accuracy: 27.32
Round  48: Global train loss: 1.186, Global test loss: 2.286, Global test accuracy: 29.45
Round  49, Train loss: 1.387, Test loss: 2.161, Test accuracy: 28.33
Round  49: Global train loss: 1.387, Global test loss: 2.284, Global test accuracy: 29.92
Round  50, Train loss: 1.367, Test loss: 2.161, Test accuracy: 27.93
Round  50: Global train loss: 1.367, Global test loss: 2.285, Global test accuracy: 29.72
Round  51, Train loss: 1.082, Test loss: 2.117, Test accuracy: 33.53
Round  51: Global train loss: 1.082, Global test loss: 2.281, Global test accuracy: 33.38
Round  52, Train loss: 0.221, Test loss: 2.099, Test accuracy: 35.75
Round  52: Global train loss: 0.221, Global test loss: 2.274, Global test accuracy: 41.57
Round  53, Train loss: 0.447, Test loss: 2.072, Test accuracy: 38.88
Round  53: Global train loss: 0.447, Global test loss: 2.266, Global test accuracy: 44.57
Round  54, Train loss: 1.419, Test loss: 2.080, Test accuracy: 36.97
Round  54: Global train loss: 1.419, Global test loss: 2.262, Global test accuracy: 44.88
Round  55, Train loss: 1.420, Test loss: 2.076, Test accuracy: 37.73
Round  55: Global train loss: 1.420, Global test loss: 2.262, Global test accuracy: 44.75
Round  56, Train loss: 1.268, Test loss: 2.101, Test accuracy: 34.42
Round  56: Global train loss: 1.268, Global test loss: 2.260, Global test accuracy: 43.13
Round  57, Train loss: 2.221, Test loss: 2.177, Test accuracy: 26.40
Round  57: Global train loss: 2.221, Global test loss: 2.273, Global test accuracy: 38.47
Round  58, Train loss: 1.504, Test loss: 2.177, Test accuracy: 26.45
Round  58: Global train loss: 1.504, Global test loss: 2.276, Global test accuracy: 34.93
Round  59, Train loss: 1.110, Test loss: 2.164, Test accuracy: 27.70
Round  59: Global train loss: 1.110, Global test loss: 2.277, Global test accuracy: 36.27
Round  60, Train loss: 1.363, Test loss: 2.118, Test accuracy: 34.77
Round  60: Global train loss: 1.363, Global test loss: 2.274, Global test accuracy: 38.73
Round  61, Train loss: 0.878, Test loss: 2.113, Test accuracy: 34.97
Round  61: Global train loss: 0.878, Global test loss: 2.273, Global test accuracy: 39.52
Round  62, Train loss: 1.636, Test loss: 2.154, Test accuracy: 30.62
Round  62: Global train loss: 1.636, Global test loss: 2.280, Global test accuracy: 36.05
Round  63, Train loss: 0.674, Test loss: 2.114, Test accuracy: 34.58
Round  63: Global train loss: 0.674, Global test loss: 2.277, Global test accuracy: 39.00
Round  64, Train loss: -0.307, Test loss: 2.067, Test accuracy: 40.20
Round  64: Global train loss: -0.307, Global test loss: 2.269, Global test accuracy: 42.97
Round  65, Train loss: 0.801, Test loss: 2.066, Test accuracy: 39.57
Round  65: Global train loss: 0.801, Global test loss: 2.269, Global test accuracy: 43.18
Round  66, Train loss: 1.183, Test loss: 2.116, Test accuracy: 33.83
Round  66: Global train loss: 1.183, Global test loss: 2.274, Global test accuracy: 42.53
Round  67, Train loss: 0.388, Test loss: 2.107, Test accuracy: 34.77
Round  67: Global train loss: 0.388, Global test loss: 2.277, Global test accuracy: 43.03
Round  68, Train loss: -0.299, Test loss: 2.096, Test accuracy: 36.08
Round  68: Global train loss: -0.299, Global test loss: 2.275, Global test accuracy: 43.05
Round  69, Train loss: 1.007, Test loss: 2.115, Test accuracy: 33.60
Round  69: Global train loss: 1.007, Global test loss: 2.279, Global test accuracy: 34.70
Round  70, Train loss: -0.424, Test loss: 2.031, Test accuracy: 43.53
Round  70: Global train loss: -0.424, Global test loss: 2.267, Global test accuracy: 47.20
Round  71, Train loss: -0.375, Test loss: 2.061, Test accuracy: 39.12
Round  71: Global train loss: -0.375, Global test loss: 2.267, Global test accuracy: 49.30
Round  72, Train loss: -1.189, Test loss: 1.971, Test accuracy: 48.25
Round  72: Global train loss: -1.189, Global test loss: 2.244, Global test accuracy: 54.90
Round  73, Train loss: 0.141, Test loss: 2.009, Test accuracy: 44.20
Round  73: Global train loss: 0.141, Global test loss: 2.237, Global test accuracy: 53.78
Round  74, Train loss: 0.766, Test loss: 2.079, Test accuracy: 38.03
Round  74: Global train loss: 0.766, Global test loss: 2.243, Global test accuracy: 51.00
Round  75, Train loss: -0.113, Test loss: 2.028, Test accuracy: 44.40
Round  75: Global train loss: -0.113, Global test loss: 2.230, Global test accuracy: 53.18
Round  76, Train loss: -0.546, Test loss: 1.989, Test accuracy: 49.17
Round  76: Global train loss: -0.546, Global test loss: 2.213, Global test accuracy: 54.42
Round  77, Train loss: -0.725, Test loss: 1.989, Test accuracy: 47.80
Round  77: Global train loss: -0.725, Global test loss: 2.204, Global test accuracy: 54.13
Round  78, Train loss: 0.760, Test loss: 2.021, Test accuracy: 44.73
Round  78: Global train loss: 0.760, Global test loss: 2.205, Global test accuracy: 52.20
Round  79, Train loss: -2.178, Test loss: 1.936, Test accuracy: 54.07
Round  79: Global train loss: -2.178, Global test loss: 2.154, Global test accuracy: 52.53
Round  80, Train loss: -1.358, Test loss: 1.950, Test accuracy: 52.70
Round  80: Global train loss: -1.358, Global test loss: 2.130, Global test accuracy: 50.02
Round  81, Train loss: 0.832, Test loss: 2.004, Test accuracy: 49.42
Round  81: Global train loss: 0.832, Global test loss: 2.144, Global test accuracy: 49.93
Round  82, Train loss: -2.141, Test loss: 1.936, Test accuracy: 55.03
Round  82: Global train loss: -2.141, Global test loss: 2.096, Global test accuracy: 51.98
Round  83, Train loss: 0.631, Test loss: 1.951, Test accuracy: 54.60
Round  83: Global train loss: 0.631, Global test loss: 2.087, Global test accuracy: 51.32
Round  84, Train loss: 0.970, Test loss: 1.985, Test accuracy: 51.05
Round  84: Global train loss: 0.970, Global test loss: 2.091, Global test accuracy: 50.07
Round  85, Train loss: -0.735, Test loss: 1.933, Test accuracy: 54.85
Round  85: Global train loss: -0.735, Global test loss: 2.055, Global test accuracy: 51.03
Round  86, Train loss: 1.715, Test loss: 1.988, Test accuracy: 50.65
Round  86: Global train loss: 1.715, Global test loss: 2.063, Global test accuracy: 50.15
Round  87, Train loss: -0.533, Test loss: 1.952, Test accuracy: 53.63
Round  87: Global train loss: -0.533, Global test loss: 2.053, Global test accuracy: 52.63
Round  88, Train loss: -0.251, Test loss: 1.949, Test accuracy: 54.93
Round  88: Global train loss: -0.251, Global test loss: 2.035, Global test accuracy: 52.17
Round  89, Train loss: -0.826, Test loss: 1.881, Test accuracy: 59.30
Round  89: Global train loss: -0.826, Global test loss: 2.004, Global test accuracy: 53.20
Round  90, Train loss: 1.367, Test loss: 1.925, Test accuracy: 55.83
Round  90: Global train loss: 1.367, Global test loss: 2.010, Global test accuracy: 52.48
Round  91, Train loss: 0.078, Test loss: 1.911, Test accuracy: 56.98
Round  91: Global train loss: 0.078, Global test loss: 2.000, Global test accuracy: 52.87
Round  92, Train loss: 0.576, Test loss: 1.909, Test accuracy: 57.85
Round  92: Global train loss: 0.576, Global test loss: 1.995, Global test accuracy: 53.02
Round  93, Train loss: 1.310, Test loss: 1.944, Test accuracy: 55.03
Round  93: Global train loss: 1.310, Global test loss: 2.003, Global test accuracy: 52.13
Round  94, Train loss: 0.790, Test loss: 1.933, Test accuracy: 56.52
Round  94: Global train loss: 0.790, Global test loss: 1.999, Global test accuracy: 51.73
Round  95, Train loss: 0.099, Test loss: 1.890, Test accuracy: 59.60
Round  95: Global train loss: 0.099, Global test loss: 1.986, Global test accuracy: 52.43
Round  96, Train loss: 0.171, Test loss: 1.874, Test accuracy: 61.13
Round  96: Global train loss: 0.171, Global test loss: 1.979, Global test accuracy: 52.83
Round  97, Train loss: -0.726, Test loss: 1.816, Test accuracy: 66.45
Round  97: Global train loss: -0.726, Global test loss: 1.956, Global test accuracy: 54.10
Round  98, Train loss: -0.309, Test loss: 1.813, Test accuracy: 66.63
Round  98: Global train loss: -0.309, Global test loss: 1.942, Global test accuracy: 55.72
Round  99, Train loss: -0.643, Test loss: 1.776, Test accuracy: 69.77
Round  99: Global train loss: -0.643, Global test loss: 1.925, Global test accuracy: 57.03/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round: Train loss: 1.783, Test loss: 1.731, Test accuracy: 79.15
Final Round: Global train loss: 1.783, Global test loss: 1.907, Global test accuracy: 58.57
Average accuracy final 10 rounds: 60.580000000000005
Average global accuracy final 10 rounds: 53.434999999999995
631.5837082862854
[]
[8.816666666666666, 9.7, 11.183333333333334, 12.333333333333334, 11.6, 11.85, 12.683333333333334, 15.383333333333333, 15.016666666666667, 14.516666666666667, 13.233333333333333, 15.383333333333333, 17.433333333333334, 18.483333333333334, 19.016666666666666, 18.016666666666666, 17.55, 19.566666666666666, 18.116666666666667, 19.016666666666666, 20.916666666666668, 20.133333333333333, 24.65, 22.516666666666666, 21.2, 22.75, 21.833333333333332, 21.0, 19.95, 21.266666666666666, 23.316666666666666, 21.366666666666667, 23.183333333333334, 24.316666666666666, 26.116666666666667, 24.85, 28.716666666666665, 25.766666666666666, 26.95, 29.666666666666668, 27.116666666666667, 23.066666666666666, 23.5, 30.5, 25.733333333333334, 26.633333333333333, 25.7, 30.216666666666665, 27.316666666666666, 28.333333333333332, 27.933333333333334, 33.53333333333333, 35.75, 38.88333333333333, 36.96666666666667, 37.733333333333334, 34.416666666666664, 26.4, 26.45, 27.7, 34.766666666666666, 34.96666666666667, 30.616666666666667, 34.583333333333336, 40.2, 39.56666666666667, 33.833333333333336, 34.766666666666666, 36.083333333333336, 33.6, 43.53333333333333, 39.11666666666667, 48.25, 44.2, 38.03333333333333, 44.4, 49.166666666666664, 47.8, 44.733333333333334, 54.06666666666667, 52.7, 49.416666666666664, 55.03333333333333, 54.6, 51.05, 54.85, 50.65, 53.63333333333333, 54.93333333333333, 59.3, 55.833333333333336, 56.983333333333334, 57.85, 55.03333333333333, 56.516666666666666, 59.6, 61.13333333333333, 66.45, 66.63333333333334, 69.76666666666667, 79.15]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.303, Test accuracy: 13.67 

Round   0, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 13.70 

Round   1, Train loss: 2.302, Test loss: 2.303, Test accuracy: 13.90 

Round   1, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 13.93 

Round   2, Train loss: 2.302, Test loss: 2.303, Test accuracy: 13.92 

Round   2, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.02 

Round   3, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.00 

Round   3, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.10 

Round   4, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.05 

Round   4, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.22 

Round   5, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.17 

Round   5, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.45 

Round   6, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.35 

Round   6, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.62 

Round   7, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.48 

Round   7, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 14.87 

Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.60 

Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 15.03 

Round   9, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.87 

Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.25 

Round  10, Train loss: 2.302, Test loss: 2.303, Test accuracy: 14.95 

Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.35 

Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.20 

Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.53 

Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.22 

Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.43 

Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.33 

Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.67 

Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.57 

Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.73 

Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.57 

Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.82 

Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.72 

Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.92 

Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.83 

Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 15.97 

Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 15.90 

Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.13 

Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.03 

Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.27 

Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.12 

Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.32 

Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.38 

Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.60 

Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.43 

Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.67 

Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.43 

Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.72 

Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.58 

Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.75 

Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.67 

Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.90 

Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.88 

Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.93 

Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.93 

Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.92 

Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.98 

Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 16.95 

Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 16.98 

Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.02 

Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.03 

Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.42 

Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.17 

Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.38 

Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.33 

Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.52 

Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.47 

Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.58 

Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.45 

Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.63 

Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.55 

Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.88 

Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.65 

Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 17.98 

Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.78 

Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.03 

Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 17.92 

Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.17 

Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.07 

Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.25 

Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.10 

Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.30 

Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.40 

Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.32 

Round  42, Train loss: 2.301, Test loss: 2.302, Test accuracy: 18.37 

Round  42, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 18.47 

Round  43, Train loss: 2.301, Test loss: 2.302, Test accuracy: 18.38 

Round  43, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 18.57 

Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.45 

Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.60 

Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.57 

Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.63 

Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.63 

Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 18.70 

Round  47, Train loss: 2.301, Test loss: 2.302, Test accuracy: 18.68 

Round  47, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 18.88 

Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 18.77 

Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.00 

Round  49, Train loss: 2.301, Test loss: 2.302, Test accuracy: 18.88 

Round  49, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.15 

Round  50, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.02 

Round  50, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.33 

Round  51, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.13 

Round  51, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.35 

Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 19.18 

Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.52 

Round  53, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.17 

Round  53, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.55 

Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 19.32 

Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.62 

Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 19.38 

Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.67 

Round  56, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.55 

Round  56, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.72 

Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 19.63 

Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.78 

Round  58, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.65 

Round  58, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.83 

Round  59, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.72 

Round  59, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 19.90 

Round  60, Train loss: 2.302, Test loss: 2.302, Test accuracy: 19.78 

Round  60, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 19.97 

Round  61, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.93 

Round  61, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.03 

Round  62, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.92 

Round  62, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.03 

Round  63, Train loss: 2.301, Test loss: 2.302, Test accuracy: 19.93 

Round  63, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.03 

Round  64, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.05 

Round  64, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.07 

Round  65, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.12 

Round  65, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.18 

Round  66, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.12 

Round  66, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.25 

Round  67, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.15 

Round  67, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.27 

Round  68, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.22 

Round  68, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.35 

Round  69, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.22 

Round  69, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.38 

Round  70, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.27 

Round  70, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 20.37 

Round  71, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.28 

Round  71, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.38 

Round  72, Train loss: 2.301, Test loss: 2.302, Test accuracy: 20.32 

Round  72, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.53 

Round  73, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.37 

Round  73, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.55 

Round  74, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.47 

Round  74, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.58 

Round  75, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.53 

Round  75, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.60 

Round  76, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.62 

Round  76, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.70 

Round  77, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.63 

Round  77, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.70 

Round  78, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.75 

Round  78, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.82 

Round  79, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.75 

Round  79, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.90 

Round  80, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.83 

Round  80, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.95 

Round  81, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.88 

Round  81, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 20.97 

Round  82, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.93 

Round  82, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.07 

Round  83, Train loss: 2.301, Test loss: 2.301, Test accuracy: 20.95 

Round  83, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.05 

Round  84, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.03 

Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.20 

Round  85, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.12 

Round  85, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.23 

Round  86, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.20 

Round  86, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.28 

Round  87, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.23 

Round  87, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.27 

Round  88, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.23 

Round  88, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.25 

Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.23 

Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.35 

Round  90, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.25 

Round  90, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.28 

Round  91, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.30 

Round  91, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.35 

Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.28 

Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.40 

Round  93, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.45 

Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.40 

Round  94, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.45 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.48 

Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.47 

Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.50 

Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.50 

Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.50 

Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.45 

Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.48 

Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.50 

Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.58 

Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.48 

Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.57 

Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 21.58 

Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 21.57 

Average accuracy final 10 rounds: 21.41333333333333 

Average global accuracy final 10 rounds: 21.455 

528.0413286685944
[0.5943806171417236, 1.0897603034973145, 1.5893924236297607, 2.0896108150482178, 2.5932514667510986, 3.097294807434082, 3.598078489303589, 4.0991740226745605, 4.604261636734009, 5.110774278640747, 5.618943691253662, 6.121859312057495, 6.626470327377319, 7.1284263134002686, 7.635884046554565, 8.124279499053955, 8.557582378387451, 8.989379167556763, 9.421461582183838, 9.85852861404419, 10.29589319229126, 10.729526281356812, 11.163991451263428, 11.59464979171753, 12.030524492263794, 12.473021268844604, 12.903381824493408, 13.33982253074646, 13.773521661758423, 14.204024076461792, 14.644250392913818, 15.082537651062012, 15.513116836547852, 15.947884798049927, 16.380833625793457, 16.81848430633545, 17.25814461708069, 17.691359758377075, 18.121618032455444, 18.555020809173584, 18.991342306137085, 19.427594661712646, 19.85945415496826, 20.292837858200073, 20.722110986709595, 21.1558735370636, 21.59581208229065, 22.02640962600708, 22.458704948425293, 22.89294695854187, 23.323036670684814, 23.762162923812866, 24.202548503875732, 24.633850812911987, 25.067757606506348, 25.50174379348755, 25.93862509727478, 26.378540515899658, 26.813234090805054, 27.244619131088257, 27.676883459091187, 28.11208963394165, 28.548036336898804, 28.976233959197998, 29.40804171562195, 29.838714122772217, 30.26572871208191, 30.70605182647705, 31.13690733909607, 31.563472270965576, 31.997053861618042, 32.42821216583252, 32.86057424545288, 33.29964876174927, 33.730785608291626, 34.15605401992798, 34.590505599975586, 35.02643322944641, 35.458688735961914, 35.89207673072815, 36.323726415634155, 36.74872064590454, 37.18273878097534, 37.61933994293213, 38.046550273895264, 38.48129725456238, 38.91575026512146, 39.34010195732117, 39.78080344200134, 40.21946430206299, 40.64443397521973, 41.079018354415894, 41.51331067085266, 41.9419424533844, 42.38102412223816, 42.81586027145386, 43.23918890953064, 43.67309069633484, 44.10743975639343, 44.54224252700806, 45.406097412109375]
[13.666666666666666, 13.9, 13.916666666666666, 14.0, 14.05, 14.166666666666666, 14.35, 14.483333333333333, 14.6, 14.866666666666667, 14.95, 15.2, 15.216666666666667, 15.333333333333334, 15.566666666666666, 15.566666666666666, 15.716666666666667, 15.833333333333334, 15.9, 16.033333333333335, 16.116666666666667, 16.383333333333333, 16.433333333333334, 16.433333333333334, 16.583333333333332, 16.666666666666668, 16.883333333333333, 16.933333333333334, 16.983333333333334, 16.983333333333334, 17.033333333333335, 17.166666666666668, 17.333333333333332, 17.466666666666665, 17.45, 17.55, 17.65, 17.783333333333335, 17.916666666666668, 18.066666666666666, 18.1, 18.4, 18.366666666666667, 18.383333333333333, 18.45, 18.566666666666666, 18.633333333333333, 18.683333333333334, 18.766666666666666, 18.883333333333333, 19.016666666666666, 19.133333333333333, 19.183333333333334, 19.166666666666668, 19.316666666666666, 19.383333333333333, 19.55, 19.633333333333333, 19.65, 19.716666666666665, 19.783333333333335, 19.933333333333334, 19.916666666666668, 19.933333333333334, 20.05, 20.116666666666667, 20.116666666666667, 20.15, 20.216666666666665, 20.216666666666665, 20.266666666666666, 20.283333333333335, 20.316666666666666, 20.366666666666667, 20.466666666666665, 20.533333333333335, 20.616666666666667, 20.633333333333333, 20.75, 20.75, 20.833333333333332, 20.883333333333333, 20.933333333333334, 20.95, 21.033333333333335, 21.116666666666667, 21.2, 21.233333333333334, 21.233333333333334, 21.233333333333334, 21.25, 21.3, 21.283333333333335, 21.45, 21.45, 21.466666666666665, 21.5, 21.45, 21.5, 21.483333333333334, 21.583333333333332]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 16.95
Round   1, Train loss: 2.300, Test loss: 2.299, Test accuracy: 17.83
Round   2, Train loss: 2.298, Test loss: 2.296, Test accuracy: 22.08
Round   3, Train loss: 2.295, Test loss: 2.291, Test accuracy: 29.95
Round   4, Train loss: 2.293, Test loss: 2.281, Test accuracy: 37.17
Round   5, Train loss: 2.283, Test loss: 2.243, Test accuracy: 29.83
Round   6, Train loss: 2.237, Test loss: 2.170, Test accuracy: 30.33
Round   7, Train loss: 2.196, Test loss: 2.064, Test accuracy: 46.43
Round   8, Train loss: 2.105, Test loss: 1.976, Test accuracy: 49.78
Round   9, Train loss: 2.021, Test loss: 1.915, Test accuracy: 57.78
Round  10, Train loss: 2.005, Test loss: 1.870, Test accuracy: 60.60
Round  11, Train loss: 1.926, Test loss: 1.811, Test accuracy: 68.65
Round  12, Train loss: 1.894, Test loss: 1.744, Test accuracy: 77.53
Round  13, Train loss: 1.780, Test loss: 1.702, Test accuracy: 79.93
Round  14, Train loss: 1.710, Test loss: 1.680, Test accuracy: 80.90
Round  15, Train loss: 1.740, Test loss: 1.669, Test accuracy: 81.23
Round  16, Train loss: 1.698, Test loss: 1.656, Test accuracy: 82.25
Round  17, Train loss: 1.686, Test loss: 1.650, Test accuracy: 82.52
Round  18, Train loss: 1.680, Test loss: 1.645, Test accuracy: 82.78
Round  19, Train loss: 1.687, Test loss: 1.644, Test accuracy: 82.80
Round  20, Train loss: 1.661, Test loss: 1.638, Test accuracy: 83.25
Round  21, Train loss: 1.619, Test loss: 1.637, Test accuracy: 83.43
Round  22, Train loss: 1.635, Test loss: 1.634, Test accuracy: 83.67
Round  23, Train loss: 1.604, Test loss: 1.634, Test accuracy: 83.47
Round  24, Train loss: 1.620, Test loss: 1.630, Test accuracy: 83.78
Round  25, Train loss: 1.612, Test loss: 1.629, Test accuracy: 84.02
Round  26, Train loss: 1.593, Test loss: 1.628, Test accuracy: 83.67
Round  27, Train loss: 1.612, Test loss: 1.626, Test accuracy: 83.97
Round  28, Train loss: 1.594, Test loss: 1.625, Test accuracy: 83.95
Round  29, Train loss: 1.603, Test loss: 1.624, Test accuracy: 84.13
Round  30, Train loss: 1.604, Test loss: 1.624, Test accuracy: 84.23
Round  31, Train loss: 1.594, Test loss: 1.624, Test accuracy: 84.17
Round  32, Train loss: 1.590, Test loss: 1.623, Test accuracy: 84.32
Round  33, Train loss: 1.610, Test loss: 1.622, Test accuracy: 84.33
Round  34, Train loss: 1.596, Test loss: 1.621, Test accuracy: 84.38
Round  35, Train loss: 1.601, Test loss: 1.621, Test accuracy: 84.42
Round  36, Train loss: 1.604, Test loss: 1.620, Test accuracy: 84.52
Round  37, Train loss: 1.585, Test loss: 1.621, Test accuracy: 84.37
Round  38, Train loss: 1.589, Test loss: 1.621, Test accuracy: 84.12
Round  39, Train loss: 1.596, Test loss: 1.620, Test accuracy: 84.40
Round  40, Train loss: 1.594, Test loss: 1.618, Test accuracy: 84.50
Round  41, Train loss: 1.590, Test loss: 1.618, Test accuracy: 84.55
Round  42, Train loss: 1.592, Test loss: 1.619, Test accuracy: 84.67
Round  43, Train loss: 1.589, Test loss: 1.617, Test accuracy: 84.72
Round  44, Train loss: 1.599, Test loss: 1.616, Test accuracy: 84.73
Round  45, Train loss: 1.587, Test loss: 1.615, Test accuracy: 84.80
Round  46, Train loss: 1.587, Test loss: 1.615, Test accuracy: 84.95
Round  47, Train loss: 1.596, Test loss: 1.616, Test accuracy: 84.95
Round  48, Train loss: 1.585, Test loss: 1.616, Test accuracy: 84.83
Round  49, Train loss: 1.580, Test loss: 1.615, Test accuracy: 84.83
Round  50, Train loss: 1.593, Test loss: 1.615, Test accuracy: 85.05
Round  51, Train loss: 1.584, Test loss: 1.615, Test accuracy: 84.92
Round  52, Train loss: 1.582, Test loss: 1.614, Test accuracy: 85.00
Round  53, Train loss: 1.590, Test loss: 1.613, Test accuracy: 84.97
Round  54, Train loss: 1.587, Test loss: 1.613, Test accuracy: 85.05
Round  55, Train loss: 1.582, Test loss: 1.613, Test accuracy: 85.17
Round  56, Train loss: 1.584, Test loss: 1.614, Test accuracy: 84.92
Round  57, Train loss: 1.588, Test loss: 1.613, Test accuracy: 85.13
Round  58, Train loss: 1.585, Test loss: 1.613, Test accuracy: 85.20
Round  59, Train loss: 1.585, Test loss: 1.613, Test accuracy: 85.28
Round  60, Train loss: 1.580, Test loss: 1.612, Test accuracy: 85.23
Round  61, Train loss: 1.588, Test loss: 1.612, Test accuracy: 85.25
Round  62, Train loss: 1.580, Test loss: 1.612, Test accuracy: 85.22
Round  63, Train loss: 1.587, Test loss: 1.613, Test accuracy: 85.15
Round  64, Train loss: 1.578, Test loss: 1.612, Test accuracy: 85.17
Round  65, Train loss: 1.591, Test loss: 1.612, Test accuracy: 85.30
Round  66, Train loss: 1.584, Test loss: 1.612, Test accuracy: 85.28
Round  67, Train loss: 1.581, Test loss: 1.612, Test accuracy: 85.03
Round  68, Train loss: 1.582, Test loss: 1.612, Test accuracy: 84.97
Round  69, Train loss: 1.585, Test loss: 1.611, Test accuracy: 85.30
Round  70, Train loss: 1.584, Test loss: 1.612, Test accuracy: 85.08
Round  71, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.28
Round  72, Train loss: 1.577, Test loss: 1.611, Test accuracy: 85.18
Round  73, Train loss: 1.582, Test loss: 1.610, Test accuracy: 85.43
Round  74, Train loss: 1.576, Test loss: 1.611, Test accuracy: 85.38
Round  75, Train loss: 1.584, Test loss: 1.611, Test accuracy: 85.33
Round  76, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.32
Round  77, Train loss: 1.577, Test loss: 1.611, Test accuracy: 85.33
Round  78, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.33
Round  79, Train loss: 1.574, Test loss: 1.610, Test accuracy: 85.40
Round  80, Train loss: 1.583, Test loss: 1.609, Test accuracy: 85.40
Round  81, Train loss: 1.576, Test loss: 1.610, Test accuracy: 85.25
Round  82, Train loss: 1.571, Test loss: 1.609, Test accuracy: 85.30
Round  83, Train loss: 1.580, Test loss: 1.610, Test accuracy: 85.23
Round  84, Train loss: 1.573, Test loss: 1.610, Test accuracy: 85.42
Round  85, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.40
Round  86, Train loss: 1.572, Test loss: 1.609, Test accuracy: 85.40
Round  87, Train loss: 1.572, Test loss: 1.609, Test accuracy: 85.40
Round  88, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.27
Round  89, Train loss: 1.585, Test loss: 1.609, Test accuracy: 85.40
Round  90, Train loss: 1.573, Test loss: 1.609, Test accuracy: 85.40
Round  91, Train loss: 1.573, Test loss: 1.609, Test accuracy: 85.30
Round  92, Train loss: 1.575, Test loss: 1.609, Test accuracy: 85.42
Round  93, Train loss: 1.581, Test loss: 1.610, Test accuracy: 85.37
Round  94, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.30
Round  95, Train loss: 1.582, Test loss: 1.609, Test accuracy: 85.22
Round  96, Train loss: 1.577, Test loss: 1.608, Test accuracy: 85.55
Round  97, Train loss: 1.573, Test loss: 1.608, Test accuracy: 85.47
Round  98, Train loss: 1.572, Test loss: 1.608, Test accuracy: 85.38
Round  99, Train loss: 1.581, Test loss: 1.608, Test accuracy: 85.57
Final Round, Train loss: 1.576, Test loss: 1.608, Test accuracy: 85.55
Average accuracy final 10 rounds: 85.39666666666668
829.7833926677704
[1.3175828456878662, 2.5213565826416016, 3.7290055751800537, 4.935198783874512, 6.142941236495972, 7.352817535400391, 8.559576988220215, 9.766870021820068, 10.970908641815186, 12.18018364906311, 13.386073112487793, 14.596026420593262, 15.8047776222229, 17.019051551818848, 18.238057136535645, 19.45146155357361, 20.66543459892273, 21.881502866744995, 23.101394653320312, 24.30892038345337, 25.519550323486328, 26.732839584350586, 27.94517707824707, 29.16144871711731, 30.37321400642395, 31.587527990341187, 32.79794645309448, 33.99982285499573, 35.203943729400635, 36.401506185531616, 37.60238528251648, 38.808349609375, 40.00529956817627, 41.20328903198242, 42.414450883865356, 43.60931372642517, 44.80514478683472, 46.01205134391785, 47.21974182128906, 48.42436385154724, 49.62986421585083, 50.83704686164856, 52.03833484649658, 53.2457640171051, 54.462522745132446, 55.693543910980225, 56.78769540786743, 57.89002513885498, 58.98585796356201, 60.08248043060303, 61.18365693092346, 62.27333331108093, 63.369062185287476, 64.46041941642761, 65.5516185760498, 66.64344835281372, 67.73383951187134, 68.82485842704773, 69.91894721984863, 71.0078992843628, 72.10449171066284, 73.19590878486633, 74.29359483718872, 75.38107943534851, 76.47196769714355, 77.56156086921692, 78.65332508087158, 79.74778842926025, 80.84054207801819, 81.932373046875, 83.02109742164612, 84.11371088027954, 85.20077300071716, 86.28850412368774, 87.37902402877808, 88.47108674049377, 89.56728315353394, 90.66284847259521, 91.75548195838928, 92.8502471446991, 93.94390630722046, 95.03262257575989, 96.1230206489563, 97.21535181999207, 98.3050045967102, 99.39742636680603, 100.49477005004883, 101.58608675003052, 102.67570781707764, 103.76417469978333, 104.85070943832397, 105.93842220306396, 107.02647089958191, 108.11950922012329, 109.21083617210388, 110.30034923553467, 111.39379453659058, 112.4861650466919, 113.57774186134338, 114.66959691047668, 115.7630786895752]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[16.95, 17.833333333333332, 22.083333333333332, 29.95, 37.166666666666664, 29.833333333333332, 30.333333333333332, 46.43333333333333, 49.78333333333333, 57.78333333333333, 60.6, 68.65, 77.53333333333333, 79.93333333333334, 80.9, 81.23333333333333, 82.25, 82.51666666666667, 82.78333333333333, 82.8, 83.25, 83.43333333333334, 83.66666666666667, 83.46666666666667, 83.78333333333333, 84.01666666666667, 83.66666666666667, 83.96666666666667, 83.95, 84.13333333333334, 84.23333333333333, 84.16666666666667, 84.31666666666666, 84.33333333333333, 84.38333333333334, 84.41666666666667, 84.51666666666667, 84.36666666666666, 84.11666666666666, 84.4, 84.5, 84.55, 84.66666666666667, 84.71666666666667, 84.73333333333333, 84.8, 84.95, 84.95, 84.83333333333333, 84.83333333333333, 85.05, 84.91666666666667, 85.0, 84.96666666666667, 85.05, 85.16666666666667, 84.91666666666667, 85.13333333333334, 85.2, 85.28333333333333, 85.23333333333333, 85.25, 85.21666666666667, 85.15, 85.16666666666667, 85.3, 85.28333333333333, 85.03333333333333, 84.96666666666667, 85.3, 85.08333333333333, 85.28333333333333, 85.18333333333334, 85.43333333333334, 85.38333333333334, 85.33333333333333, 85.31666666666666, 85.33333333333333, 85.33333333333333, 85.4, 85.4, 85.25, 85.3, 85.23333333333333, 85.41666666666667, 85.4, 85.4, 85.4, 85.26666666666667, 85.4, 85.4, 85.3, 85.41666666666667, 85.36666666666666, 85.3, 85.21666666666667, 85.55, 85.46666666666667, 85.38333333333334, 85.56666666666666, 85.55]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.313, Test loss: 2.303, Test accuracy: 9.73
Round   1, Train loss: 2.293, Test loss: 2.303, Test accuracy: 9.22
Round   2, Train loss: 2.286, Test loss: 2.303, Test accuracy: 12.23
Round   3, Train loss: 2.270, Test loss: 2.304, Test accuracy: 11.87
Round   4, Train loss: 2.225, Test loss: 2.306, Test accuracy: 11.22
Round   5, Train loss: 2.187, Test loss: 2.312, Test accuracy: 10.43
Round   6, Train loss: 2.154, Test loss: 2.318, Test accuracy: 10.57
Round   7, Train loss: 2.134, Test loss: 2.322, Test accuracy: 10.33
Round   8, Train loss: 2.116, Test loss: 2.320, Test accuracy: 10.60
Round   9, Train loss: 2.117, Test loss: 2.325, Test accuracy: 10.45
Round  10, Train loss: 2.104, Test loss: 2.324, Test accuracy: 10.35
Round  11, Train loss: 2.101, Test loss: 2.322, Test accuracy: 10.65
Round  12, Train loss: 2.097, Test loss: 2.322, Test accuracy: 11.02
Round  13, Train loss: 2.096, Test loss: 2.323, Test accuracy: 10.88
Round  14, Train loss: 2.104, Test loss: 2.322, Test accuracy: 10.93
Round  15, Train loss: 2.105, Test loss: 2.324, Test accuracy: 10.52
Round  16, Train loss: 2.093, Test loss: 2.324, Test accuracy: 10.48
Round  17, Train loss: 2.103, Test loss: 2.324, Test accuracy: 10.32
Round  18, Train loss: 2.090, Test loss: 2.324, Test accuracy: 10.42
Round  19, Train loss: 2.085, Test loss: 2.324, Test accuracy: 10.88
Round  20, Train loss: 2.082, Test loss: 2.324, Test accuracy: 10.62
Round  21, Train loss: 2.100, Test loss: 2.321, Test accuracy: 10.60
Round  22, Train loss: 2.087, Test loss: 2.320, Test accuracy: 10.70
Round  23, Train loss: 2.089, Test loss: 2.320, Test accuracy: 10.77
Round  24, Train loss: 2.085, Test loss: 2.320, Test accuracy: 10.87
Round  25, Train loss: 2.085, Test loss: 2.320, Test accuracy: 10.70
Round  26, Train loss: 2.073, Test loss: 2.321, Test accuracy: 10.52
Round  27, Train loss: 2.074, Test loss: 2.322, Test accuracy: 10.83
Round  28, Train loss: 2.071, Test loss: 2.322, Test accuracy: 10.60
Round  29, Train loss: 2.086, Test loss: 2.321, Test accuracy: 10.13
Round  30, Train loss: 2.072, Test loss: 2.321, Test accuracy: 10.78
Round  31, Train loss: 2.063, Test loss: 2.322, Test accuracy: 10.97
Round  32, Train loss: 2.080, Test loss: 2.323, Test accuracy: 10.98
Round  33, Train loss: 2.083, Test loss: 2.324, Test accuracy: 11.00
Round  34, Train loss: 2.068, Test loss: 2.324, Test accuracy: 10.23
Round  35, Train loss: 2.077, Test loss: 2.323, Test accuracy: 10.85
Round  36, Train loss: 2.054, Test loss: 2.324, Test accuracy: 10.38
Round  37, Train loss: 2.060, Test loss: 2.325, Test accuracy: 10.50
Round  38, Train loss: 2.062, Test loss: 2.324, Test accuracy: 10.58
Round  39, Train loss: 2.073, Test loss: 2.324, Test accuracy: 10.73
Round  40, Train loss: 2.065, Test loss: 2.323, Test accuracy: 10.98
Round  41, Train loss: 2.063, Test loss: 2.323, Test accuracy: 11.05
Round  42, Train loss: 2.056, Test loss: 2.324, Test accuracy: 11.12
Round  43, Train loss: 2.069, Test loss: 2.323, Test accuracy: 10.85
Round  44, Train loss: 2.045, Test loss: 2.323, Test accuracy: 10.85
Round  45, Train loss: 2.049, Test loss: 2.324, Test accuracy: 10.67
Round  46, Train loss: 2.044, Test loss: 2.324, Test accuracy: 10.35
Round  47, Train loss: 2.023, Test loss: 2.325, Test accuracy: 10.57
Round  48, Train loss: 2.041, Test loss: 2.324, Test accuracy: 10.40
Round  49, Train loss: 2.016, Test loss: 2.325, Test accuracy: 10.18
Round  50, Train loss: 2.020, Test loss: 2.325, Test accuracy: 10.37
Round  51, Train loss: 2.032, Test loss: 2.323, Test accuracy: 10.78
Round  52, Train loss: 2.035, Test loss: 2.323, Test accuracy: 10.80
Round  53, Train loss: 2.006, Test loss: 2.323, Test accuracy: 10.95
Round  54, Train loss: 2.014, Test loss: 2.323, Test accuracy: 10.90
Round  55, Train loss: 2.034, Test loss: 2.322, Test accuracy: 10.95
Round  56, Train loss: 2.000, Test loss: 2.321, Test accuracy: 11.13
Round  57, Train loss: 1.948, Test loss: 2.322, Test accuracy: 11.37
Round  58, Train loss: 2.030, Test loss: 2.322, Test accuracy: 11.22
Round  59, Train loss: 1.978, Test loss: 2.321, Test accuracy: 11.25
Round  60, Train loss: 1.981, Test loss: 2.325, Test accuracy: 10.63
Round  61, Train loss: 1.998, Test loss: 2.324, Test accuracy: 10.90
Round  62, Train loss: 1.956, Test loss: 2.325, Test accuracy: 10.72
Round  63, Train loss: 1.990, Test loss: 2.320, Test accuracy: 11.35
Round  64, Train loss: 1.994, Test loss: 2.318, Test accuracy: 11.72
Round  65, Train loss: 1.987, Test loss: 2.316, Test accuracy: 12.25
Round  66, Train loss: 1.954, Test loss: 2.317, Test accuracy: 11.83
Round  67, Train loss: 1.978, Test loss: 2.317, Test accuracy: 11.87
Round  68, Train loss: 1.932, Test loss: 2.317, Test accuracy: 12.35
Round  69, Train loss: 1.920, Test loss: 2.321, Test accuracy: 11.60
Round  70, Train loss: 1.932, Test loss: 2.317, Test accuracy: 12.02
Round  71, Train loss: 1.914, Test loss: 2.316, Test accuracy: 12.15
Round  72, Train loss: 1.922, Test loss: 2.319, Test accuracy: 11.62
Round  73, Train loss: 1.947, Test loss: 2.322, Test accuracy: 11.55
Round  74, Train loss: 1.921, Test loss: 2.321, Test accuracy: 11.80
Round  75, Train loss: 1.907, Test loss: 2.319, Test accuracy: 11.80
Round  76, Train loss: 1.891, Test loss: 2.319, Test accuracy: 12.18
Round  77, Train loss: 1.917, Test loss: 2.320, Test accuracy: 12.15
Round  78, Train loss: 1.936, Test loss: 2.321, Test accuracy: 11.88
Round  79, Train loss: 1.877, Test loss: 2.321, Test accuracy: 11.90
Round  80, Train loss: 1.882, Test loss: 2.325, Test accuracy: 11.28
Round  81, Train loss: 1.902, Test loss: 2.321, Test accuracy: 11.83
Round  82, Train loss: 1.887, Test loss: 2.320, Test accuracy: 11.45
Round  83, Train loss: 1.893, Test loss: 2.325, Test accuracy: 11.32
Round  84, Train loss: 1.848, Test loss: 2.325, Test accuracy: 11.15
Round  85, Train loss: 1.917, Test loss: 2.323, Test accuracy: 11.68
Round  86, Train loss: 1.885, Test loss: 2.321, Test accuracy: 12.08
Round  87, Train loss: 1.869, Test loss: 2.319, Test accuracy: 12.07
Round  88, Train loss: 1.822, Test loss: 2.320, Test accuracy: 12.15
Round  89, Train loss: 1.839, Test loss: 2.323, Test accuracy: 11.65
Round  90, Train loss: 1.863, Test loss: 2.325, Test accuracy: 11.42
Round  91, Train loss: 1.854, Test loss: 2.331, Test accuracy: 11.17
Round  92, Train loss: 1.805, Test loss: 2.325, Test accuracy: 11.90
Round  93, Train loss: 1.867, Test loss: 2.326, Test accuracy: 11.50
Round  94, Train loss: 1.837, Test loss: 2.324, Test accuracy: 11.90/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.785, Test loss: 2.335, Test accuracy: 10.45
Round  96, Train loss: 1.793, Test loss: 2.328, Test accuracy: 11.47
Round  97, Train loss: 1.798, Test loss: 2.336, Test accuracy: 10.17
Round  98, Train loss: 1.832, Test loss: 2.335, Test accuracy: 10.20
Round  99, Train loss: 1.815, Test loss: 2.327, Test accuracy: 11.42
Final Round, Train loss: 1.813, Test loss: 2.330, Test accuracy: 11.15
Average accuracy final 10 rounds: 11.158333333333331
471.0575635433197
[0.6999070644378662, 1.2360563278198242, 1.7638609409332275, 2.3034448623657227, 2.83735728263855, 3.377251386642456, 3.9183390140533447, 4.448573112487793, 4.990302801132202, 5.5271875858306885, 6.059247732162476, 6.601284742355347, 7.1352698802948, 7.662715911865234, 8.203510284423828, 8.734521389007568, 9.27754807472229, 9.8175630569458, 10.351679563522339, 10.89294695854187, 11.429709434509277, 11.955450773239136, 12.489805698394775, 13.017563819885254, 13.549735069274902, 14.083766222000122, 14.616534233093262, 15.154390811920166, 15.697277545928955, 16.23258876800537, 16.770776748657227, 17.305510997772217, 17.836382389068604, 18.379928827285767, 18.909826040267944, 19.449625730514526, 19.990978002548218, 20.523940801620483, 21.063032627105713, 21.617457628250122, 22.16897678375244, 22.72280478477478, 23.264734745025635, 23.804062604904175, 24.353899717330933, 24.91107177734375, 25.472792387008667, 26.02631449699402, 26.57662343978882, 27.13553810119629, 27.686363220214844, 28.24358320236206, 28.80463218688965, 29.35599446296692, 29.89916491508484, 30.45125937461853, 30.999982357025146, 31.54755187034607, 32.11416506767273, 32.66439437866211, 33.21868371963501, 33.76393532752991, 34.31136655807495, 34.86933183670044, 35.43145251274109, 35.97667956352234, 36.5320029258728, 37.08379054069519, 37.63001275062561, 38.18554663658142, 38.74057960510254, 39.28331542015076, 39.84030294418335, 40.38691759109497, 40.940372705459595, 41.504600524902344, 42.057801485061646, 42.597639083862305, 43.151001930236816, 43.69190168380737, 44.250115633010864, 44.81216073036194, 45.362719774246216, 45.92165756225586, 46.47290802001953, 47.0115020275116, 47.56760311126709, 48.12758493423462, 48.69152331352234, 49.24516248703003, 49.79222130775452, 50.34759020805359, 50.90437412261963, 51.46375250816345, 52.02765607833862, 52.58043456077576, 53.12325620651245, 53.67786931991577, 54.234883546829224, 54.792855978012085, 55.61679530143738]
[9.733333333333333, 9.216666666666667, 12.233333333333333, 11.866666666666667, 11.216666666666667, 10.433333333333334, 10.566666666666666, 10.333333333333334, 10.6, 10.45, 10.35, 10.65, 11.016666666666667, 10.883333333333333, 10.933333333333334, 10.516666666666667, 10.483333333333333, 10.316666666666666, 10.416666666666666, 10.883333333333333, 10.616666666666667, 10.6, 10.7, 10.766666666666667, 10.866666666666667, 10.7, 10.516666666666667, 10.833333333333334, 10.6, 10.133333333333333, 10.783333333333333, 10.966666666666667, 10.983333333333333, 11.0, 10.233333333333333, 10.85, 10.383333333333333, 10.5, 10.583333333333334, 10.733333333333333, 10.983333333333333, 11.05, 11.116666666666667, 10.85, 10.85, 10.666666666666666, 10.35, 10.566666666666666, 10.4, 10.183333333333334, 10.366666666666667, 10.783333333333333, 10.8, 10.95, 10.9, 10.95, 11.133333333333333, 11.366666666666667, 11.216666666666667, 11.25, 10.633333333333333, 10.9, 10.716666666666667, 11.35, 11.716666666666667, 12.25, 11.833333333333334, 11.866666666666667, 12.35, 11.6, 12.016666666666667, 12.15, 11.616666666666667, 11.55, 11.8, 11.8, 12.183333333333334, 12.15, 11.883333333333333, 11.9, 11.283333333333333, 11.833333333333334, 11.45, 11.316666666666666, 11.15, 11.683333333333334, 12.083333333333334, 12.066666666666666, 12.15, 11.65, 11.416666666666666, 11.166666666666666, 11.9, 11.5, 11.9, 10.45, 11.466666666666667, 10.166666666666666, 10.2, 11.416666666666666, 11.15]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.320, Test loss: 2.301, Test accuracy: 25.00
Round   1, Train loss: 2.315, Test loss: 2.298, Test accuracy: 32.26
Round   2, Train loss: 2.307, Test loss: 2.293, Test accuracy: 28.00
Round   3, Train loss: 2.298, Test loss: 2.285, Test accuracy: 25.46
Round   4, Train loss: 2.286, Test loss: 2.272, Test accuracy: 26.93
Round   5, Train loss: 2.265, Test loss: 2.241, Test accuracy: 40.95
Round   6, Train loss: 2.218, Test loss: 2.177, Test accuracy: 46.63
Round   7, Train loss: 2.155, Test loss: 2.105, Test accuracy: 51.16
Round   8, Train loss: 2.079, Test loss: 2.047, Test accuracy: 58.78
Round   9, Train loss: 2.013, Test loss: 1.987, Test accuracy: 65.05
Round  10, Train loss: 1.975, Test loss: 1.927, Test accuracy: 68.66
Round  11, Train loss: 1.919, Test loss: 1.886, Test accuracy: 72.38
Round  12, Train loss: 1.882, Test loss: 1.855, Test accuracy: 74.39
Round  13, Train loss: 1.862, Test loss: 1.823, Test accuracy: 75.50
Round  14, Train loss: 1.841, Test loss: 1.801, Test accuracy: 77.23
Round  15, Train loss: 1.811, Test loss: 1.783, Test accuracy: 79.27
Round  16, Train loss: 1.789, Test loss: 1.769, Test accuracy: 80.83
Round  17, Train loss: 1.785, Test loss: 1.750, Test accuracy: 81.88
Round  18, Train loss: 1.771, Test loss: 1.737, Test accuracy: 82.58
Round  19, Train loss: 1.766, Test loss: 1.726, Test accuracy: 83.22
Round  20, Train loss: 1.756, Test loss: 1.712, Test accuracy: 83.70
Round  21, Train loss: 1.739, Test loss: 1.703, Test accuracy: 84.09
Round  22, Train loss: 1.735, Test loss: 1.696, Test accuracy: 84.56
Round  23, Train loss: 1.717, Test loss: 1.693, Test accuracy: 85.15
Round  24, Train loss: 1.724, Test loss: 1.676, Test accuracy: 85.88
Round  25, Train loss: 1.707, Test loss: 1.669, Test accuracy: 87.00
Round  26, Train loss: 1.687, Test loss: 1.660, Test accuracy: 88.43
Round  27, Train loss: 1.679, Test loss: 1.650, Test accuracy: 89.58
Round  28, Train loss: 1.668, Test loss: 1.644, Test accuracy: 90.36
Round  29, Train loss: 1.672, Test loss: 1.632, Test accuracy: 90.97
Round  30, Train loss: 1.651, Test loss: 1.628, Test accuracy: 91.58
Round  31, Train loss: 1.638, Test loss: 1.621, Test accuracy: 91.90
Round  32, Train loss: 1.640, Test loss: 1.617, Test accuracy: 92.24
Round  33, Train loss: 1.635, Test loss: 1.612, Test accuracy: 92.32
Round  34, Train loss: 1.636, Test loss: 1.603, Test accuracy: 92.65
Round  35, Train loss: 1.620, Test loss: 1.603, Test accuracy: 92.93
Round  36, Train loss: 1.617, Test loss: 1.599, Test accuracy: 93.08
Round  37, Train loss: 1.618, Test loss: 1.593, Test accuracy: 93.20
Round  38, Train loss: 1.611, Test loss: 1.590, Test accuracy: 93.33
Round  39, Train loss: 1.609, Test loss: 1.586, Test accuracy: 93.45
Round  40, Train loss: 1.600, Test loss: 1.586, Test accuracy: 93.58
Round  41, Train loss: 1.614, Test loss: 1.579, Test accuracy: 93.65
Round  42, Train loss: 1.596, Test loss: 1.578, Test accuracy: 93.69
Round  43, Train loss: 1.593, Test loss: 1.577, Test accuracy: 93.82
Round  44, Train loss: 1.586, Test loss: 1.578, Test accuracy: 93.86
Round  45, Train loss: 1.585, Test loss: 1.576, Test accuracy: 93.98
Round  46, Train loss: 1.592, Test loss: 1.571, Test accuracy: 94.08
Round  47, Train loss: 1.580, Test loss: 1.571, Test accuracy: 94.16
Round  48, Train loss: 1.578, Test loss: 1.569, Test accuracy: 94.31
Round  49, Train loss: 1.582, Test loss: 1.566, Test accuracy: 94.43
Round  50, Train loss: 1.574, Test loss: 1.566, Test accuracy: 94.50
Round  51, Train loss: 1.578, Test loss: 1.562, Test accuracy: 94.54
Round  52, Train loss: 1.569, Test loss: 1.564, Test accuracy: 94.59
Round  53, Train loss: 1.573, Test loss: 1.559, Test accuracy: 94.70
Round  54, Train loss: 1.574, Test loss: 1.557, Test accuracy: 94.71
Round  55, Train loss: 1.563, Test loss: 1.558, Test accuracy: 94.77
Round  56, Train loss: 1.559, Test loss: 1.558, Test accuracy: 94.83
Round  57, Train loss: 1.558, Test loss: 1.557, Test accuracy: 94.83
Round  58, Train loss: 1.568, Test loss: 1.553, Test accuracy: 94.94
Round  59, Train loss: 1.557, Test loss: 1.554, Test accuracy: 95.08
Round  60, Train loss: 1.553, Test loss: 1.554, Test accuracy: 95.12
Round  61, Train loss: 1.551, Test loss: 1.553, Test accuracy: 95.03
Round  62, Train loss: 1.559, Test loss: 1.550, Test accuracy: 95.22
Round  63, Train loss: 1.546, Test loss: 1.551, Test accuracy: 95.25
Round  64, Train loss: 1.548, Test loss: 1.550, Test accuracy: 95.26
Round  65, Train loss: 1.545, Test loss: 1.550, Test accuracy: 95.36
Round  66, Train loss: 1.548, Test loss: 1.547, Test accuracy: 95.40
Round  67, Train loss: 1.548, Test loss: 1.547, Test accuracy: 95.39
Round  68, Train loss: 1.551, Test loss: 1.545, Test accuracy: 95.42
Round  69, Train loss: 1.550, Test loss: 1.544, Test accuracy: 95.44
Round  70, Train loss: 1.544, Test loss: 1.544, Test accuracy: 95.59
Round  71, Train loss: 1.542, Test loss: 1.543, Test accuracy: 95.58
Round  72, Train loss: 1.539, Test loss: 1.544, Test accuracy: 95.68
Round  73, Train loss: 1.542, Test loss: 1.543, Test accuracy: 95.56
Round  74, Train loss: 1.536, Test loss: 1.544, Test accuracy: 95.56
Round  75, Train loss: 1.538, Test loss: 1.542, Test accuracy: 95.59
Round  76, Train loss: 1.537, Test loss: 1.541, Test accuracy: 95.60
Round  77, Train loss: 1.534, Test loss: 1.541, Test accuracy: 95.70
Round  78, Train loss: 1.534, Test loss: 1.540, Test accuracy: 95.79
Round  79, Train loss: 1.535, Test loss: 1.540, Test accuracy: 95.82
Round  80, Train loss: 1.535, Test loss: 1.538, Test accuracy: 95.87
Round  81, Train loss: 1.531, Test loss: 1.539, Test accuracy: 95.93
Round  82, Train loss: 1.533, Test loss: 1.538, Test accuracy: 95.90
Round  83, Train loss: 1.527, Test loss: 1.539, Test accuracy: 95.81
Round  84, Train loss: 1.530, Test loss: 1.538, Test accuracy: 95.93
Round  85, Train loss: 1.530, Test loss: 1.537, Test accuracy: 95.95
Round  86, Train loss: 1.527, Test loss: 1.537, Test accuracy: 95.92
Round  87, Train loss: 1.527, Test loss: 1.535, Test accuracy: 96.05
Round  88, Train loss: 1.523, Test loss: 1.536, Test accuracy: 96.05
Round  89, Train loss: 1.523, Test loss: 1.536, Test accuracy: 96.03
Round  90, Train loss: 1.526, Test loss: 1.535, Test accuracy: 96.01
Round  91, Train loss: 1.526, Test loss: 1.534, Test accuracy: 96.09
Round  92, Train loss: 1.522, Test loss: 1.535, Test accuracy: 96.06
Round  93, Train loss: 1.525, Test loss: 1.533, Test accuracy: 96.13/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.524, Test loss: 1.533, Test accuracy: 96.14
Round  95, Train loss: 1.525, Test loss: 1.533, Test accuracy: 96.17
Round  96, Train loss: 1.521, Test loss: 1.533, Test accuracy: 96.20
Round  97, Train loss: 1.518, Test loss: 1.533, Test accuracy: 96.23
Round  98, Train loss: 1.518, Test loss: 1.533, Test accuracy: 96.16
Round  99, Train loss: 1.521, Test loss: 1.532, Test accuracy: 96.21
Final Round, Train loss: 1.492, Test loss: 1.527, Test accuracy: 96.19
Average accuracy final 10 rounds: 96.13950000000001
1471.431144952774
[1.1460802555084229, 2.0705361366271973, 3.0132689476013184, 3.9391472339630127, 4.8765130043029785, 5.792395830154419, 6.7264440059661865, 7.648383140563965, 8.572468996047974, 9.484440565109253, 10.402430772781372, 11.32544732093811, 12.242138862609863, 13.172514200210571, 14.092060565948486, 15.030566453933716, 15.947115182876587, 16.877132177352905, 17.79289221763611, 18.724231004714966, 19.64321780204773, 20.56780457496643, 21.447191953659058, 22.318434238433838, 23.191564083099365, 24.09431219100952, 25.005008935928345, 25.897939682006836, 26.815091371536255, 27.72545289993286, 28.668410778045654, 29.60618305206299, 30.56253218650818, 31.52226686477661, 32.50209164619446, 33.46902680397034, 34.440619468688965, 35.40021085739136, 36.38477921485901, 37.343905448913574, 38.304216146469116, 39.260101556777954, 40.2099027633667, 41.17352247238159, 42.14783692359924, 43.11487030982971, 44.0678927898407, 45.046053647994995, 46.013792753219604, 47.003984212875366, 47.97490119934082, 48.935513496398926, 49.894468784332275, 50.87414836883545, 51.84810400009155, 52.82928204536438, 53.7781023979187, 54.74153733253479, 55.720720529556274, 56.691333532333374, 57.66341805458069, 58.61129927635193, 59.58330702781677, 60.54603338241577, 61.53348398208618, 62.50351071357727, 63.47103404998779, 64.43972945213318, 65.40046381950378, 66.37534070014954, 67.35055351257324, 68.307053565979, 69.26864171028137, 70.25115036964417, 71.21641182899475, 72.20555281639099, 73.1544759273529, 74.13631892204285, 75.10501718521118, 76.0917739868164, 77.05904245376587, 78.01295924186707, 78.983553647995, 79.9528694152832, 80.9199366569519, 81.88628244400024, 82.85202765464783, 83.80795502662659, 84.78981614112854, 85.76802706718445, 86.75260472297668, 87.70956754684448, 88.68230652809143, 89.64753079414368, 90.61155939102173, 91.57351803779602, 92.5350694656372, 93.51421093940735, 94.48152875900269, 95.46036291122437, 96.88942527770996]
[25.0, 32.255, 28.0, 25.465, 26.93, 40.955, 46.635, 51.1625, 58.7775, 65.0525, 68.66, 72.3825, 74.3875, 75.5025, 77.2325, 79.27, 80.83, 81.88, 82.5775, 83.2225, 83.705, 84.09, 84.555, 85.15, 85.8775, 86.995, 88.4325, 89.5825, 90.365, 90.97, 91.585, 91.9025, 92.2375, 92.32, 92.65, 92.9275, 93.085, 93.2, 93.325, 93.455, 93.585, 93.6525, 93.6925, 93.8175, 93.8575, 93.9825, 94.085, 94.1625, 94.315, 94.4325, 94.5, 94.5375, 94.5925, 94.6975, 94.71, 94.77, 94.825, 94.835, 94.945, 95.0825, 95.1225, 95.03, 95.215, 95.25, 95.26, 95.3625, 95.3975, 95.3875, 95.415, 95.435, 95.59, 95.585, 95.68, 95.5625, 95.565, 95.5875, 95.6025, 95.6975, 95.7925, 95.8175, 95.87, 95.9325, 95.9, 95.805, 95.9325, 95.955, 95.92, 96.045, 96.045, 96.0325, 96.0125, 96.09, 96.06, 96.1325, 96.135, 96.1675, 96.2025, 96.23, 96.1575, 96.2075, 96.195]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.149, Test loss: 2.033, Test accuracy: 43.95 

Round   0, Global train loss: 2.149, Global test loss: 2.106, Global test accuracy: 31.91 

Round   1, Train loss: 1.670, Test loss: 1.899, Test accuracy: 56.06 

Round   1, Global train loss: 1.670, Global test loss: 2.068, Global test accuracy: 37.10 

Round   2, Train loss: 1.629, Test loss: 1.826, Test accuracy: 65.72 

Round   2, Global train loss: 1.629, Global test loss: 2.065, Global test accuracy: 38.30 

Round   3, Train loss: 1.547, Test loss: 1.802, Test accuracy: 67.42 

Round   3, Global train loss: 1.547, Global test loss: 2.055, Global test accuracy: 39.39 

Round   4, Train loss: 1.598, Test loss: 1.750, Test accuracy: 73.06 

Round   4, Global train loss: 1.598, Global test loss: 2.064, Global test accuracy: 38.76 

Round   5, Train loss: 1.559, Test loss: 1.722, Test accuracy: 74.79 

Round   5, Global train loss: 1.559, Global test loss: 2.049, Global test accuracy: 39.31 

Round   6, Train loss: 1.594, Test loss: 1.686, Test accuracy: 78.60 

Round   6, Global train loss: 1.594, Global test loss: 2.048, Global test accuracy: 40.17 

Round   7, Train loss: 1.584, Test loss: 1.674, Test accuracy: 80.03 

Round   7, Global train loss: 1.584, Global test loss: 2.022, Global test accuracy: 43.12 

Round   8, Train loss: 1.662, Test loss: 1.609, Test accuracy: 86.61 

Round   8, Global train loss: 1.662, Global test loss: 2.080, Global test accuracy: 36.13 

Round   9, Train loss: 1.590, Test loss: 1.597, Test accuracy: 87.12 

Round   9, Global train loss: 1.590, Global test loss: 2.094, Global test accuracy: 35.27 

Round  10, Train loss: 1.581, Test loss: 1.596, Test accuracy: 87.05 

Round  10, Global train loss: 1.581, Global test loss: 2.059, Global test accuracy: 38.98 

Round  11, Train loss: 1.566, Test loss: 1.582, Test accuracy: 88.43 

Round  11, Global train loss: 1.566, Global test loss: 2.045, Global test accuracy: 40.33 

Round  12, Train loss: 1.578, Test loss: 1.582, Test accuracy: 88.41 

Round  12, Global train loss: 1.578, Global test loss: 2.058, Global test accuracy: 39.44 

Round  13, Train loss: 1.585, Test loss: 1.580, Test accuracy: 88.60 

Round  13, Global train loss: 1.585, Global test loss: 2.071, Global test accuracy: 36.81 

Round  14, Train loss: 1.582, Test loss: 1.579, Test accuracy: 88.64 

Round  14, Global train loss: 1.582, Global test loss: 2.048, Global test accuracy: 39.69 

Round  15, Train loss: 1.581, Test loss: 1.578, Test accuracy: 88.64 

Round  15, Global train loss: 1.581, Global test loss: 2.065, Global test accuracy: 37.40 

Round  16, Train loss: 1.578, Test loss: 1.578, Test accuracy: 88.67 

Round  16, Global train loss: 1.578, Global test loss: 2.061, Global test accuracy: 38.22 

Round  17, Train loss: 1.520, Test loss: 1.578, Test accuracy: 88.65 

Round  17, Global train loss: 1.520, Global test loss: 2.051, Global test accuracy: 39.88 

Round  18, Train loss: 1.469, Test loss: 1.577, Test accuracy: 88.63 

Round  18, Global train loss: 1.469, Global test loss: 2.047, Global test accuracy: 39.96 

Round  19, Train loss: 1.469, Test loss: 1.577, Test accuracy: 88.65 

Round  19, Global train loss: 1.469, Global test loss: 2.076, Global test accuracy: 37.81 

Round  20, Train loss: 1.482, Test loss: 1.562, Test accuracy: 90.18 

Round  20, Global train loss: 1.482, Global test loss: 2.064, Global test accuracy: 38.89 

Round  21, Train loss: 1.468, Test loss: 1.562, Test accuracy: 90.25 

Round  21, Global train loss: 1.468, Global test loss: 2.042, Global test accuracy: 40.85 

Round  22, Train loss: 1.576, Test loss: 1.562, Test accuracy: 90.27 

Round  22, Global train loss: 1.576, Global test loss: 2.048, Global test accuracy: 40.79 

Round  23, Train loss: 1.466, Test loss: 1.562, Test accuracy: 90.27 

Round  23, Global train loss: 1.466, Global test loss: 2.049, Global test accuracy: 40.56 

Round  24, Train loss: 1.576, Test loss: 1.561, Test accuracy: 90.28 

Round  24, Global train loss: 1.576, Global test loss: 2.119, Global test accuracy: 31.99 

Round  25, Train loss: 1.465, Test loss: 1.561, Test accuracy: 90.30 

Round  25, Global train loss: 1.465, Global test loss: 2.060, Global test accuracy: 38.94 

Round  26, Train loss: 1.522, Test loss: 1.561, Test accuracy: 90.30 

Round  26, Global train loss: 1.522, Global test loss: 2.056, Global test accuracy: 39.84 

Round  27, Train loss: 1.466, Test loss: 1.561, Test accuracy: 90.31 

Round  27, Global train loss: 1.466, Global test loss: 2.080, Global test accuracy: 36.01 

Round  28, Train loss: 1.578, Test loss: 1.560, Test accuracy: 90.34 

Round  28, Global train loss: 1.578, Global test loss: 2.052, Global test accuracy: 39.04 

Round  29, Train loss: 1.573, Test loss: 1.560, Test accuracy: 90.33 

Round  29, Global train loss: 1.573, Global test loss: 2.075, Global test accuracy: 37.04 

Round  30, Train loss: 1.575, Test loss: 1.560, Test accuracy: 90.34 

Round  30, Global train loss: 1.575, Global test loss: 2.083, Global test accuracy: 35.98 

Round  31, Train loss: 1.519, Test loss: 1.560, Test accuracy: 90.31 

Round  31, Global train loss: 1.519, Global test loss: 2.061, Global test accuracy: 38.43 

Round  32, Train loss: 1.576, Test loss: 1.560, Test accuracy: 90.33 

Round  32, Global train loss: 1.576, Global test loss: 2.057, Global test accuracy: 39.58 

Round  33, Train loss: 1.572, Test loss: 1.560, Test accuracy: 90.33 

Round  33, Global train loss: 1.572, Global test loss: 2.094, Global test accuracy: 34.92 

Round  34, Train loss: 1.520, Test loss: 1.559, Test accuracy: 90.33 

Round  34, Global train loss: 1.520, Global test loss: 2.091, Global test accuracy: 35.21 

Round  35, Train loss: 1.465, Test loss: 1.559, Test accuracy: 90.33 

Round  35, Global train loss: 1.465, Global test loss: 2.058, Global test accuracy: 38.72 

Round  36, Train loss: 1.519, Test loss: 1.559, Test accuracy: 90.37 

Round  36, Global train loss: 1.519, Global test loss: 2.047, Global test accuracy: 39.71 

Round  37, Train loss: 1.627, Test loss: 1.559, Test accuracy: 90.37 

Round  37, Global train loss: 1.627, Global test loss: 2.081, Global test accuracy: 36.23 

Round  38, Train loss: 1.573, Test loss: 1.559, Test accuracy: 90.38 

Round  38, Global train loss: 1.573, Global test loss: 2.090, Global test accuracy: 36.26 

Round  39, Train loss: 1.518, Test loss: 1.558, Test accuracy: 90.41 

Round  39, Global train loss: 1.518, Global test loss: 2.051, Global test accuracy: 40.18 

Round  40, Train loss: 1.548, Test loss: 1.530, Test accuracy: 93.44 

Round  40, Global train loss: 1.548, Global test loss: 2.081, Global test accuracy: 35.67 

Round  41, Train loss: 1.575, Test loss: 1.529, Test accuracy: 93.49 

Round  41, Global train loss: 1.575, Global test loss: 2.094, Global test accuracy: 34.41 

Round  42, Train loss: 1.518, Test loss: 1.529, Test accuracy: 93.49 

Round  42, Global train loss: 1.518, Global test loss: 2.077, Global test accuracy: 36.85 

Round  43, Train loss: 1.464, Test loss: 1.529, Test accuracy: 93.49 

Round  43, Global train loss: 1.464, Global test loss: 2.075, Global test accuracy: 37.12 

Round  44, Train loss: 1.519, Test loss: 1.529, Test accuracy: 93.44 

Round  44, Global train loss: 1.519, Global test loss: 2.074, Global test accuracy: 37.37 

Round  45, Train loss: 1.518, Test loss: 1.529, Test accuracy: 93.47 

Round  45, Global train loss: 1.518, Global test loss: 2.089, Global test accuracy: 35.24 

Round  46, Train loss: 1.518, Test loss: 1.529, Test accuracy: 93.46 

Round  46, Global train loss: 1.518, Global test loss: 2.093, Global test accuracy: 35.12 

Round  47, Train loss: 1.464, Test loss: 1.529, Test accuracy: 93.45 

Round  47, Global train loss: 1.464, Global test loss: 2.032, Global test accuracy: 41.38 

Round  48, Train loss: 1.464, Test loss: 1.529, Test accuracy: 93.45 

Round  48, Global train loss: 1.464, Global test loss: 2.077, Global test accuracy: 35.94 

Round  49, Train loss: 1.519, Test loss: 1.529, Test accuracy: 93.45 

Round  49, Global train loss: 1.519, Global test loss: 2.058, Global test accuracy: 39.27 

Round  50, Train loss: 1.520, Test loss: 1.528, Test accuracy: 93.49 

Round  50, Global train loss: 1.520, Global test loss: 2.077, Global test accuracy: 36.65 

Round  51, Train loss: 1.520, Test loss: 1.528, Test accuracy: 93.51 

Round  51, Global train loss: 1.520, Global test loss: 2.042, Global test accuracy: 39.98 

Round  52, Train loss: 1.520, Test loss: 1.528, Test accuracy: 93.51 

Round  52, Global train loss: 1.520, Global test loss: 2.049, Global test accuracy: 39.70 

Round  53, Train loss: 1.519, Test loss: 1.528, Test accuracy: 93.51 

Round  53, Global train loss: 1.519, Global test loss: 2.048, Global test accuracy: 39.49 

Round  54, Train loss: 1.463, Test loss: 1.528, Test accuracy: 93.52 

Round  54, Global train loss: 1.463, Global test loss: 2.095, Global test accuracy: 34.75 

Round  55, Train loss: 1.465, Test loss: 1.528, Test accuracy: 93.53 

Round  55, Global train loss: 1.465, Global test loss: 2.046, Global test accuracy: 40.96 

Round  56, Train loss: 1.495, Test loss: 1.512, Test accuracy: 95.14 

Round  56, Global train loss: 1.495, Global test loss: 2.103, Global test accuracy: 34.36 

Round  57, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.13 

Round  57, Global train loss: 1.464, Global test loss: 2.099, Global test accuracy: 35.27 

Round  58, Train loss: 1.465, Test loss: 1.512, Test accuracy: 95.12 

Round  58, Global train loss: 1.465, Global test loss: 2.071, Global test accuracy: 37.68 

Round  59, Train loss: 1.464, Test loss: 1.513, Test accuracy: 95.12 

Round  59, Global train loss: 1.464, Global test loss: 2.023, Global test accuracy: 42.49 

Round  60, Train loss: 1.519, Test loss: 1.513, Test accuracy: 95.12 

Round  60, Global train loss: 1.519, Global test loss: 2.053, Global test accuracy: 39.41 

Round  61, Train loss: 1.465, Test loss: 1.512, Test accuracy: 95.12 

Round  61, Global train loss: 1.465, Global test loss: 2.048, Global test accuracy: 39.64 

Round  62, Train loss: 1.518, Test loss: 1.512, Test accuracy: 95.14 

Round  62, Global train loss: 1.518, Global test loss: 2.068, Global test accuracy: 38.08 

Round  63, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.14 

Round  63, Global train loss: 1.464, Global test loss: 2.085, Global test accuracy: 34.68 

Round  64, Train loss: 1.520, Test loss: 1.512, Test accuracy: 95.16 

Round  64, Global train loss: 1.520, Global test loss: 2.058, Global test accuracy: 39.23 

Round  65, Train loss: 1.465, Test loss: 1.512, Test accuracy: 95.17 

Round  65, Global train loss: 1.465, Global test loss: 2.044, Global test accuracy: 40.53 

Round  66, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.17 

Round  66, Global train loss: 1.464, Global test loss: 2.043, Global test accuracy: 40.92 

Round  67, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.17 

Round  67, Global train loss: 1.464, Global test loss: 2.050, Global test accuracy: 39.62 

Round  68, Train loss: 1.462, Test loss: 1.512, Test accuracy: 95.19 

Round  68, Global train loss: 1.462, Global test loss: 2.075, Global test accuracy: 36.80 

Round  69, Train loss: 1.464, Test loss: 1.512, Test accuracy: 95.22 

Round  69, Global train loss: 1.464, Global test loss: 2.044, Global test accuracy: 40.36 

Round  70, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.22 

Round  70, Global train loss: 1.464, Global test loss: 2.071, Global test accuracy: 37.43 

Round  71, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.22 

Round  71, Global train loss: 1.464, Global test loss: 2.072, Global test accuracy: 37.43 

Round  72, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.22 

Round  72, Global train loss: 1.463, Global test loss: 2.089, Global test accuracy: 35.14 

Round  73, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.22 

Round  73, Global train loss: 1.464, Global test loss: 2.108, Global test accuracy: 33.15 

Round  74, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.23 

Round  74, Global train loss: 1.463, Global test loss: 2.070, Global test accuracy: 37.64 

Round  75, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.23 

Round  75, Global train loss: 1.519, Global test loss: 2.054, Global test accuracy: 39.62 

Round  76, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.20 

Round  76, Global train loss: 1.519, Global test loss: 2.044, Global test accuracy: 40.88 

Round  77, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.20 

Round  77, Global train loss: 1.464, Global test loss: 2.052, Global test accuracy: 40.06 

Round  78, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.20 

Round  78, Global train loss: 1.464, Global test loss: 2.062, Global test accuracy: 39.49 

Round  79, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.20 

Round  79, Global train loss: 1.463, Global test loss: 2.081, Global test accuracy: 36.56 

Round  80, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.21 

Round  80, Global train loss: 1.464, Global test loss: 2.036, Global test accuracy: 41.54 

Round  81, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.22 

Round  81, Global train loss: 1.464, Global test loss: 2.086, Global test accuracy: 34.59 

Round  82, Train loss: 1.462, Test loss: 1.511, Test accuracy: 95.21 

Round  82, Global train loss: 1.462, Global test loss: 2.069, Global test accuracy: 37.70 

Round  83, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.21 

Round  83, Global train loss: 1.464, Global test loss: 2.052, Global test accuracy: 39.73 

Round  84, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.21 

Round  84, Global train loss: 1.463, Global test loss: 2.074, Global test accuracy: 37.37 

Round  85, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.21 

Round  85, Global train loss: 1.519, Global test loss: 2.071, Global test accuracy: 37.80 

Round  86, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.21 

Round  86, Global train loss: 1.464, Global test loss: 2.031, Global test accuracy: 42.39 

Round  87, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.22 

Round  87, Global train loss: 1.519, Global test loss: 2.065, Global test accuracy: 37.78 

Round  88, Train loss: 1.462, Test loss: 1.511, Test accuracy: 95.22 

Round  88, Global train loss: 1.462, Global test loss: 2.077, Global test accuracy: 36.55 

Round  89, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.22 

Round  89, Global train loss: 1.463, Global test loss: 2.040, Global test accuracy: 41.55 

Round  90, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.22 

Round  90, Global train loss: 1.463, Global test loss: 2.088, Global test accuracy: 35.04 

Round  91, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.22 

Round  91, Global train loss: 1.463, Global test loss: 2.035, Global test accuracy: 41.93 

Round  92, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.22 

Round  92, Global train loss: 1.464, Global test loss: 2.062, Global test accuracy: 37.94 

Round  93, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.22 

Round  93, Global train loss: 1.519, Global test loss: 2.066, Global test accuracy: 37.45 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.23 

Round  94, Global train loss: 1.464, Global test loss: 2.078, Global test accuracy: 36.42 

Round  95, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.23 

Round  95, Global train loss: 1.463, Global test loss: 2.032, Global test accuracy: 42.06 

Round  96, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.25 

Round  96, Global train loss: 1.519, Global test loss: 2.054, Global test accuracy: 39.38 

Round  97, Train loss: 1.463, Test loss: 1.511, Test accuracy: 95.24 

Round  97, Global train loss: 1.463, Global test loss: 2.067, Global test accuracy: 37.47 

Round  98, Train loss: 1.464, Test loss: 1.511, Test accuracy: 95.24 

Round  98, Global train loss: 1.464, Global test loss: 2.080, Global test accuracy: 36.57 

Round  99, Train loss: 1.519, Test loss: 1.511, Test accuracy: 95.24 

Round  99, Global train loss: 1.519, Global test loss: 2.048, Global test accuracy: 40.27 

Final Round, Train loss: 1.480, Test loss: 1.511, Test accuracy: 95.25 

Final Round, Global train loss: 1.480, Global test loss: 2.048, Global test accuracy: 40.27 

Average accuracy final 10 rounds: 95.23250000000002 

Average global accuracy final 10 rounds: 38.4525 

991.6302058696747
[0.9271705150604248, 1.7493882179260254, 2.5750880241394043, 3.4018630981445312, 4.2234649658203125, 5.0449700355529785, 5.869471311569214, 6.694937705993652, 7.519896030426025, 8.342526912689209, 9.16695499420166, 9.998734474182129, 10.821998596191406, 11.651015996932983, 12.482836723327637, 13.313130617141724, 14.14797592163086, 14.967515707015991, 15.806807279586792, 16.642619848251343, 17.47615694999695, 18.32229232788086, 19.153258323669434, 19.975351333618164, 20.78970432281494, 21.587895154953003, 22.416446924209595, 23.236096382141113, 24.014195203781128, 24.80537486076355, 25.585399389266968, 26.33113384246826, 27.08626937866211, 27.835434675216675, 28.610586404800415, 29.371957302093506, 30.124377965927124, 30.905012369155884, 31.682422876358032, 32.46120262145996, 33.24022912979126, 34.01630663871765, 34.803426027297974, 35.599507093429565, 36.38121795654297, 37.15511751174927, 37.83132004737854, 38.50787329673767, 39.18127965927124, 39.8427619934082, 40.53286290168762, 41.18438649177551, 41.8563289642334, 42.5582058429718, 43.24188947677612, 43.91457962989807, 44.595630407333374, 45.26536679267883, 45.94000291824341, 46.61456751823425, 47.29008603096008, 47.95714807510376, 48.625736951828, 49.286375522613525, 49.963807106018066, 50.62765574455261, 51.301310539245605, 51.98017621040344, 52.64897656440735, 53.323975563049316, 54.000614404678345, 54.6848464012146, 55.36757564544678, 56.046772956848145, 56.75329089164734, 57.448683738708496, 58.13221001625061, 58.80563569068909, 59.49210453033447, 60.18745470046997, 60.866419076919556, 61.55925130844116, 62.23892879486084, 62.912593364715576, 63.59093618392944, 64.26419425010681, 64.95146822929382, 65.63461375236511, 66.30789041519165, 66.99075818061829, 67.67394757270813, 68.36134362220764, 69.02740168571472, 69.71522355079651, 70.39698648452759, 71.08545207977295, 71.77266550064087, 72.45306253433228, 73.13944935798645, 73.81442141532898, 75.18319249153137]
[43.95, 56.05833333333333, 65.71666666666667, 67.425, 73.05833333333334, 74.79166666666667, 78.6, 80.03333333333333, 86.60833333333333, 87.11666666666666, 87.05, 88.43333333333334, 88.40833333333333, 88.6, 88.64166666666667, 88.64166666666667, 88.675, 88.65, 88.63333333333334, 88.65, 90.18333333333334, 90.25, 90.26666666666667, 90.26666666666667, 90.28333333333333, 90.3, 90.3, 90.30833333333334, 90.34166666666667, 90.325, 90.34166666666667, 90.30833333333334, 90.33333333333333, 90.325, 90.325, 90.33333333333333, 90.36666666666666, 90.36666666666666, 90.38333333333334, 90.40833333333333, 93.44166666666666, 93.49166666666666, 93.49166666666666, 93.49166666666666, 93.44166666666666, 93.46666666666667, 93.45833333333333, 93.45, 93.45, 93.45, 93.49166666666666, 93.50833333333334, 93.50833333333334, 93.50833333333334, 93.51666666666667, 93.525, 95.14166666666667, 95.13333333333334, 95.125, 95.11666666666666, 95.11666666666666, 95.11666666666666, 95.14166666666667, 95.14166666666667, 95.15833333333333, 95.16666666666667, 95.16666666666667, 95.16666666666667, 95.19166666666666, 95.21666666666667, 95.225, 95.21666666666667, 95.21666666666667, 95.21666666666667, 95.23333333333333, 95.23333333333333, 95.2, 95.2, 95.2, 95.2, 95.20833333333333, 95.21666666666667, 95.20833333333333, 95.20833333333333, 95.20833333333333, 95.20833333333333, 95.20833333333333, 95.21666666666667, 95.21666666666667, 95.21666666666667, 95.21666666666667, 95.21666666666667, 95.225, 95.225, 95.23333333333333, 95.23333333333333, 95.25, 95.24166666666666, 95.24166666666666, 95.24166666666666, 95.25]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 275, in <module>
    acc_test, loss_test = test_img_local_all(net_glob, args, dataset_test, dict_users_test,
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 133, in test_img_local_all
    a, b = test_img_local(net_local, dataset_test, args, user_idx=idx, idxs=dict_users_test[idx], concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 97, in test_img_local
    test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.214, Test loss: 2.075, Test accuracy: 40.03 

Round   0, Global train loss: 2.214, Global test loss: 2.122, Global test accuracy: 33.52 

Round   1, Train loss: 1.766, Test loss: 1.901, Test accuracy: 57.38 

Round   1, Global train loss: 1.766, Global test loss: 2.062, Global test accuracy: 38.43 

Round   2, Train loss: 1.616, Test loss: 1.841, Test accuracy: 62.01 

Round   2, Global train loss: 1.616, Global test loss: 2.080, Global test accuracy: 34.88 

Round   3, Train loss: 1.566, Test loss: 1.735, Test accuracy: 73.68 

Round   3, Global train loss: 1.566, Global test loss: 2.046, Global test accuracy: 39.73 

Round   4, Train loss: 1.556, Test loss: 1.624, Test accuracy: 87.39 

Round   4, Global train loss: 1.556, Global test loss: 2.038, Global test accuracy: 42.16 

Round   5, Train loss: 1.535, Test loss: 1.580, Test accuracy: 89.95 

Round   5, Global train loss: 1.535, Global test loss: 2.002, Global test accuracy: 46.35 

Round   6, Train loss: 1.488, Test loss: 1.598, Test accuracy: 86.70 

Round   6, Global train loss: 1.488, Global test loss: 2.047, Global test accuracy: 39.63 

Round   7, Train loss: 1.482, Test loss: 1.582, Test accuracy: 88.96 

Round   7, Global train loss: 1.482, Global test loss: 2.038, Global test accuracy: 41.25 

Round   8, Train loss: 1.485, Test loss: 1.552, Test accuracy: 92.02 

Round   8, Global train loss: 1.485, Global test loss: 2.000, Global test accuracy: 47.37 

Round   9, Train loss: 1.487, Test loss: 1.531, Test accuracy: 93.92 

Round   9, Global train loss: 1.487, Global test loss: 2.025, Global test accuracy: 42.00 

Round  10, Train loss: 1.474, Test loss: 1.530, Test accuracy: 94.01 

Round  10, Global train loss: 1.474, Global test loss: 2.074, Global test accuracy: 36.36 

Round  11, Train loss: 1.478, Test loss: 1.528, Test accuracy: 94.12 

Round  11, Global train loss: 1.478, Global test loss: 2.029, Global test accuracy: 42.30 

Round  12, Train loss: 1.473, Test loss: 1.528, Test accuracy: 94.06 

Round  12, Global train loss: 1.473, Global test loss: 2.042, Global test accuracy: 40.14 

Round  13, Train loss: 1.486, Test loss: 1.512, Test accuracy: 95.62 

Round  13, Global train loss: 1.486, Global test loss: 2.013, Global test accuracy: 44.72 

Round  14, Train loss: 1.470, Test loss: 1.512, Test accuracy: 95.54 

Round  14, Global train loss: 1.470, Global test loss: 2.126, Global test accuracy: 31.25 

Round  15, Train loss: 1.468, Test loss: 1.512, Test accuracy: 95.55 

Round  15, Global train loss: 1.468, Global test loss: 2.003, Global test accuracy: 45.30 

Round  16, Train loss: 1.470, Test loss: 1.511, Test accuracy: 95.60 

Round  16, Global train loss: 1.470, Global test loss: 2.064, Global test accuracy: 37.87 

Round  17, Train loss: 1.470, Test loss: 1.510, Test accuracy: 95.62 

Round  17, Global train loss: 1.470, Global test loss: 2.017, Global test accuracy: 43.64 

Round  18, Train loss: 1.470, Test loss: 1.509, Test accuracy: 95.65 

Round  18, Global train loss: 1.470, Global test loss: 2.019, Global test accuracy: 44.38 

Round  19, Train loss: 1.466, Test loss: 1.509, Test accuracy: 95.64 

Round  19, Global train loss: 1.466, Global test loss: 2.021, Global test accuracy: 43.03 

Round  20, Train loss: 1.470, Test loss: 1.509, Test accuracy: 95.62 

Round  20, Global train loss: 1.470, Global test loss: 2.017, Global test accuracy: 44.83 

Round  21, Train loss: 1.467, Test loss: 1.508, Test accuracy: 95.63 

Round  21, Global train loss: 1.467, Global test loss: 2.016, Global test accuracy: 43.20 

Round  22, Train loss: 1.470, Test loss: 1.508, Test accuracy: 95.67 

Round  22, Global train loss: 1.470, Global test loss: 2.011, Global test accuracy: 44.16 

Round  23, Train loss: 1.469, Test loss: 1.508, Test accuracy: 95.67 

Round  23, Global train loss: 1.469, Global test loss: 2.044, Global test accuracy: 40.02 

Round  24, Train loss: 1.468, Test loss: 1.508, Test accuracy: 95.67 

Round  24, Global train loss: 1.468, Global test loss: 2.057, Global test accuracy: 39.24 

Round  25, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.69 

Round  25, Global train loss: 1.467, Global test loss: 2.071, Global test accuracy: 37.06 

Round  26, Train loss: 1.468, Test loss: 1.507, Test accuracy: 95.69 

Round  26, Global train loss: 1.468, Global test loss: 2.039, Global test accuracy: 42.58 

Round  27, Train loss: 1.468, Test loss: 1.507, Test accuracy: 95.68 

Round  27, Global train loss: 1.468, Global test loss: 2.084, Global test accuracy: 36.28 

Round  28, Train loss: 1.468, Test loss: 1.507, Test accuracy: 95.63 

Round  28, Global train loss: 1.468, Global test loss: 2.026, Global test accuracy: 42.02 

Round  29, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.66 

Round  29, Global train loss: 1.467, Global test loss: 2.019, Global test accuracy: 43.58 

Round  30, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.68 

Round  30, Global train loss: 1.467, Global test loss: 2.016, Global test accuracy: 43.73 

Round  31, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.68 

Round  31, Global train loss: 1.466, Global test loss: 2.033, Global test accuracy: 41.88 

Round  32, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.66 

Round  32, Global train loss: 1.466, Global test loss: 2.066, Global test accuracy: 38.29 

Round  33, Train loss: 1.468, Test loss: 1.507, Test accuracy: 95.64 

Round  33, Global train loss: 1.468, Global test loss: 2.052, Global test accuracy: 40.27 

Round  34, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.67 

Round  34, Global train loss: 1.465, Global test loss: 2.001, Global test accuracy: 46.50 

Round  35, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.68 

Round  35, Global train loss: 1.468, Global test loss: 2.004, Global test accuracy: 45.46 

Round  36, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.68 

Round  36, Global train loss: 1.467, Global test loss: 2.078, Global test accuracy: 36.67 

Round  37, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.68 

Round  37, Global train loss: 1.468, Global test loss: 2.005, Global test accuracy: 45.04 

Round  38, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.70 

Round  38, Global train loss: 1.467, Global test loss: 1.998, Global test accuracy: 45.37 

Round  39, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.68 

Round  39, Global train loss: 1.464, Global test loss: 2.015, Global test accuracy: 43.33 

Round  40, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.69 

Round  40, Global train loss: 1.465, Global test loss: 2.031, Global test accuracy: 42.19 

Round  41, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.68 

Round  41, Global train loss: 1.468, Global test loss: 2.031, Global test accuracy: 41.88 

Round  42, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.69 

Round  42, Global train loss: 1.467, Global test loss: 2.030, Global test accuracy: 41.46 

Round  43, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.68 

Round  43, Global train loss: 1.466, Global test loss: 2.050, Global test accuracy: 40.17 

Round  44, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.68 

Round  44, Global train loss: 1.466, Global test loss: 2.011, Global test accuracy: 44.25 

Round  45, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.68 

Round  45, Global train loss: 1.466, Global test loss: 2.018, Global test accuracy: 43.84 

Round  46, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.68 

Round  46, Global train loss: 1.465, Global test loss: 2.082, Global test accuracy: 34.81 

Round  47, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.68 

Round  47, Global train loss: 1.464, Global test loss: 2.021, Global test accuracy: 43.54 

Round  48, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.67 

Round  48, Global train loss: 1.466, Global test loss: 2.013, Global test accuracy: 43.77 

Round  49, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.68 

Round  49, Global train loss: 1.465, Global test loss: 2.001, Global test accuracy: 45.58 

Round  50, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.68 

Round  50, Global train loss: 1.467, Global test loss: 2.043, Global test accuracy: 41.30 

Round  51, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.69 

Round  51, Global train loss: 1.467, Global test loss: 2.035, Global test accuracy: 40.92 

Round  52, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.68 

Round  52, Global train loss: 1.467, Global test loss: 2.018, Global test accuracy: 44.73 

Round  53, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.67 

Round  53, Global train loss: 1.465, Global test loss: 2.005, Global test accuracy: 44.88 

Round  54, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.67 

Round  54, Global train loss: 1.464, Global test loss: 2.050, Global test accuracy: 39.95 

Round  55, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.68 

Round  55, Global train loss: 1.468, Global test loss: 1.998, Global test accuracy: 46.12 

Round  56, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.66 

Round  56, Global train loss: 1.464, Global test loss: 2.051, Global test accuracy: 38.90 

Round  57, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.66 

Round  57, Global train loss: 1.465, Global test loss: 2.005, Global test accuracy: 44.88 

Round  58, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.65 

Round  58, Global train loss: 1.468, Global test loss: 2.008, Global test accuracy: 44.09 

Round  59, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.64 

Round  59, Global train loss: 1.466, Global test loss: 2.015, Global test accuracy: 44.14 

Round  60, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.64 

Round  60, Global train loss: 1.465, Global test loss: 2.011, Global test accuracy: 44.47 

Round  61, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.65 

Round  61, Global train loss: 1.468, Global test loss: 2.006, Global test accuracy: 44.62 

Round  62, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.66 

Round  62, Global train loss: 1.466, Global test loss: 2.028, Global test accuracy: 42.41 

Round  63, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.66 

Round  63, Global train loss: 1.467, Global test loss: 2.032, Global test accuracy: 40.80 

Round  64, Train loss: 1.467, Test loss: 1.506, Test accuracy: 95.66 

Round  64, Global train loss: 1.467, Global test loss: 2.028, Global test accuracy: 42.14 

Round  65, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.66 

Round  65, Global train loss: 1.466, Global test loss: 2.040, Global test accuracy: 41.23 

Round  66, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.69 

Round  66, Global train loss: 1.468, Global test loss: 2.004, Global test accuracy: 45.61 

Round  67, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.68 

Round  67, Global train loss: 1.465, Global test loss: 2.093, Global test accuracy: 35.50 

Round  68, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.68 

Round  68, Global train loss: 1.465, Global test loss: 2.042, Global test accuracy: 39.82 

Round  69, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.68 

Round  69, Global train loss: 1.465, Global test loss: 2.022, Global test accuracy: 43.98 

Round  70, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.68 

Round  70, Global train loss: 1.466, Global test loss: 2.074, Global test accuracy: 36.83 

Round  71, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.70 

Round  71, Global train loss: 1.465, Global test loss: 2.043, Global test accuracy: 40.20 

Round  72, Train loss: 1.467, Test loss: 1.505, Test accuracy: 95.69 

Round  72, Global train loss: 1.467, Global test loss: 2.033, Global test accuracy: 42.23 

Round  73, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.67 

Round  73, Global train loss: 1.466, Global test loss: 2.044, Global test accuracy: 39.81 

Round  74, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.68 

Round  74, Global train loss: 1.465, Global test loss: 2.046, Global test accuracy: 40.07 

Round  75, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.68 

Round  75, Global train loss: 1.464, Global test loss: 2.050, Global test accuracy: 39.68 

Round  76, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.68 

Round  76, Global train loss: 1.464, Global test loss: 2.014, Global test accuracy: 43.46 

Round  77, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.69 

Round  77, Global train loss: 1.465, Global test loss: 2.005, Global test accuracy: 45.01 

Round  78, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.71 

Round  78, Global train loss: 1.464, Global test loss: 2.005, Global test accuracy: 45.01 

Round  79, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.72 

Round  79, Global train loss: 1.465, Global test loss: 2.025, Global test accuracy: 42.26 

Round  80, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.71 

Round  80, Global train loss: 1.464, Global test loss: 2.021, Global test accuracy: 43.88 

Round  81, Train loss: 1.467, Test loss: 1.505, Test accuracy: 95.70 

Round  81, Global train loss: 1.467, Global test loss: 2.003, Global test accuracy: 46.38 

Round  82, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.70 

Round  82, Global train loss: 1.466, Global test loss: 2.029, Global test accuracy: 41.81 

Round  83, Train loss: 1.468, Test loss: 1.505, Test accuracy: 95.69 

Round  83, Global train loss: 1.468, Global test loss: 1.999, Global test accuracy: 45.92 

Round  84, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.69 

Round  84, Global train loss: 1.465, Global test loss: 2.033, Global test accuracy: 41.93 

Round  85, Train loss: 1.467, Test loss: 1.505, Test accuracy: 95.68 

Round  85, Global train loss: 1.467, Global test loss: 2.008, Global test accuracy: 44.84 

Round  86, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.69 

Round  86, Global train loss: 1.466, Global test loss: 2.061, Global test accuracy: 38.87 

Round  87, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.68 

Round  87, Global train loss: 1.465, Global test loss: 2.013, Global test accuracy: 43.03 

Round  88, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.68 

Round  88, Global train loss: 1.464, Global test loss: 2.016, Global test accuracy: 43.41 

Round  89, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.68 

Round  89, Global train loss: 1.466, Global test loss: 2.030, Global test accuracy: 42.24 

Round  90, Train loss: 1.467, Test loss: 1.505, Test accuracy: 95.70 

Round  90, Global train loss: 1.467, Global test loss: 2.015, Global test accuracy: 44.08 

Round  91, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.70 

Round  91, Global train loss: 1.466, Global test loss: 2.021, Global test accuracy: 43.42 

Round  92, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.70 

Round  92, Global train loss: 1.466, Global test loss: 2.032, Global test accuracy: 41.62 

Round  93, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.70 

Round  93, Global train loss: 1.465, Global test loss: 2.026, Global test accuracy: 42.85 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.464, Test loss: 1.505, Test accuracy: 95.70 

Round  94, Global train loss: 1.464, Global test loss: 2.037, Global test accuracy: 41.61 

Round  95, Train loss: 1.465, Test loss: 1.505, Test accuracy: 95.70 

Round  95, Global train loss: 1.465, Global test loss: 2.028, Global test accuracy: 42.13 

Round  96, Train loss: 1.468, Test loss: 1.505, Test accuracy: 95.70 

Round  96, Global train loss: 1.468, Global test loss: 2.072, Global test accuracy: 36.95 

Round  97, Train loss: 1.468, Test loss: 1.505, Test accuracy: 95.70 

Round  97, Global train loss: 1.468, Global test loss: 2.007, Global test accuracy: 44.68 

Round  98, Train loss: 1.468, Test loss: 1.505, Test accuracy: 95.70 

Round  98, Global train loss: 1.468, Global test loss: 2.016, Global test accuracy: 43.33 

Round  99, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.70 

Round  99, Global train loss: 1.466, Global test loss: 2.001, Global test accuracy: 45.27 

Final Round, Train loss: 1.466, Test loss: 1.505, Test accuracy: 95.71 

Final Round, Global train loss: 1.466, Global test loss: 2.001, Global test accuracy: 45.27 

Average accuracy final 10 rounds: 95.69999999999999 

Average global accuracy final 10 rounds: 42.59583333333333 

1021.9177551269531
[0.8993685245513916, 1.6930105686187744, 2.4915778636932373, 3.2798564434051514, 4.070462226867676, 4.869231700897217, 5.665306568145752, 6.4557061195373535, 7.23860764503479, 8.029424667358398, 8.815176486968994, 9.59731125831604, 10.37497615814209, 11.169540166854858, 11.966145515441895, 12.752490282058716, 13.543281555175781, 14.336907148361206, 15.136670112609863, 15.92072319984436, 16.705268621444702, 17.49579668045044, 18.28331232070923, 19.052167177200317, 19.83941650390625, 20.626559495925903, 21.404985666275024, 22.194485425949097, 22.983049154281616, 23.765376329421997, 24.568039655685425, 25.347493886947632, 26.13352656364441, 26.928847551345825, 27.716259717941284, 28.495057582855225, 29.289212703704834, 30.076797246932983, 30.85640597343445, 31.645941972732544, 32.426127910614014, 33.21953320503235, 34.00045108795166, 34.78326082229614, 35.57741856575012, 36.368053913116455, 37.14796471595764, 37.938342809677124, 38.72996973991394, 39.50357246398926, 40.28333830833435, 41.07811427116394, 41.872254610061646, 42.66605043411255, 43.4509551525116, 44.24251699447632, 45.031652212142944, 45.81217050552368, 46.59431982040405, 47.37577748298645, 48.15853810310364, 48.93843054771423, 49.727303981781006, 50.51279091835022, 51.29575991630554, 52.0768837928772, 52.76130986213684, 53.4447386264801, 54.11384725570679, 54.79802966117859, 55.479944467544556, 56.152928590774536, 56.84139156341553, 57.63282823562622, 58.41997814178467, 59.20552349090576, 59.991477251052856, 60.78019428253174, 61.56824254989624, 62.36038088798523, 63.16170024871826, 63.96671986579895, 64.7462649345398, 65.52310752868652, 66.24978494644165, 66.96752595901489, 67.68332982063293, 68.39751100540161, 69.12295317649841, 69.84834146499634, 70.5697033405304, 71.30922961235046, 72.0574803352356, 72.79069638252258, 73.55415868759155, 74.32537198066711, 75.0543098449707, 75.7774407863617, 76.51287961006165, 77.24255228042603, 78.71541285514832]
[40.03333333333333, 57.375, 62.00833333333333, 73.68333333333334, 87.39166666666667, 89.95, 86.7, 88.95833333333333, 92.01666666666667, 93.925, 94.00833333333334, 94.11666666666666, 94.05833333333334, 95.625, 95.54166666666667, 95.55, 95.6, 95.61666666666666, 95.65, 95.64166666666667, 95.625, 95.63333333333334, 95.675, 95.66666666666667, 95.675, 95.69166666666666, 95.69166666666666, 95.68333333333334, 95.63333333333334, 95.65833333333333, 95.68333333333334, 95.68333333333334, 95.65833333333333, 95.64166666666667, 95.66666666666667, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.7, 95.68333333333334, 95.69166666666666, 95.68333333333334, 95.69166666666666, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.675, 95.68333333333334, 95.68333333333334, 95.69166666666666, 95.68333333333334, 95.675, 95.675, 95.68333333333334, 95.65833333333333, 95.65833333333333, 95.65, 95.64166666666667, 95.64166666666667, 95.65, 95.65833333333333, 95.65833333333333, 95.65833333333333, 95.65833333333333, 95.69166666666666, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.7, 95.69166666666666, 95.675, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.69166666666666, 95.70833333333333, 95.71666666666667, 95.70833333333333, 95.7, 95.7, 95.69166666666666, 95.69166666666666, 95.68333333333334, 95.69166666666666, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.7, 95.7, 95.7, 95.7, 95.7, 95.7, 95.7, 95.7, 95.7, 95.7, 95.70833333333333]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 275, in <module>
    acc_test, loss_test = test_img_local_all(net_glob, args, dataset_test, dict_users_test,
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 133, in test_img_local_all
    a, b = test_img_local(net_local, dataset_test, args, user_idx=idx, idxs=dict_users_test[idx], concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 97, in test_img_local
    test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 44, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 44, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 44, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.225, Test loss: 2.076, Test accuracy: 25.86 

Round   0, Global train loss: 2.225, Global test loss: 2.072, Global test accuracy: 27.25 

Round   1, Train loss: 2.004, Test loss: 1.934, Test accuracy: 28.93 

Round   1, Global train loss: 2.004, Global test loss: 1.873, Global test accuracy: 31.55 

Round   2, Train loss: 1.873, Test loss: 1.824, Test accuracy: 32.31 

Round   2, Global train loss: 1.873, Global test loss: 1.718, Global test accuracy: 37.16 

Round   3, Train loss: 1.739, Test loss: 1.732, Test accuracy: 36.21 

Round   3, Global train loss: 1.739, Global test loss: 1.598, Global test accuracy: 42.63 

Round   4, Train loss: 1.666, Test loss: 1.699, Test accuracy: 37.35 

Round   4, Global train loss: 1.666, Global test loss: 1.536, Global test accuracy: 43.94 

Round   5, Train loss: 1.619, Test loss: 1.695, Test accuracy: 37.47 

Round   5, Global train loss: 1.619, Global test loss: 1.490, Global test accuracy: 45.62 

Round   6, Train loss: 1.589, Test loss: 1.649, Test accuracy: 39.58 

Round   6, Global train loss: 1.589, Global test loss: 1.456, Global test accuracy: 47.09 

Round   7, Train loss: 1.525, Test loss: 1.613, Test accuracy: 41.23 

Round   7, Global train loss: 1.525, Global test loss: 1.421, Global test accuracy: 49.22 

Round   8, Train loss: 1.483, Test loss: 1.608, Test accuracy: 41.49 

Round   8, Global train loss: 1.483, Global test loss: 1.372, Global test accuracy: 50.82 

Round   9, Train loss: 1.451, Test loss: 1.552, Test accuracy: 43.58 

Round   9, Global train loss: 1.451, Global test loss: 1.342, Global test accuracy: 52.06 

Round  10, Train loss: 1.414, Test loss: 1.520, Test accuracy: 45.11 

Round  10, Global train loss: 1.414, Global test loss: 1.317, Global test accuracy: 52.85 

Round  11, Train loss: 1.373, Test loss: 1.501, Test accuracy: 46.01 

Round  11, Global train loss: 1.373, Global test loss: 1.291, Global test accuracy: 53.68 

Round  12, Train loss: 1.329, Test loss: 1.488, Test accuracy: 46.51 

Round  12, Global train loss: 1.329, Global test loss: 1.282, Global test accuracy: 54.11 

Round  13, Train loss: 1.318, Test loss: 1.456, Test accuracy: 47.72 

Round  13, Global train loss: 1.318, Global test loss: 1.254, Global test accuracy: 55.05 

Round  14, Train loss: 1.254, Test loss: 1.455, Test accuracy: 48.30 

Round  14, Global train loss: 1.254, Global test loss: 1.234, Global test accuracy: 55.87 

Round  15, Train loss: 1.261, Test loss: 1.439, Test accuracy: 48.77 

Round  15, Global train loss: 1.261, Global test loss: 1.207, Global test accuracy: 57.45 

Round  16, Train loss: 1.213, Test loss: 1.422, Test accuracy: 49.64 

Round  16, Global train loss: 1.213, Global test loss: 1.196, Global test accuracy: 57.68 

Round  17, Train loss: 1.199, Test loss: 1.417, Test accuracy: 50.20 

Round  17, Global train loss: 1.199, Global test loss: 1.172, Global test accuracy: 58.94 

Round  18, Train loss: 1.207, Test loss: 1.391, Test accuracy: 51.32 

Round  18, Global train loss: 1.207, Global test loss: 1.153, Global test accuracy: 59.06 

Round  19, Train loss: 1.130, Test loss: 1.383, Test accuracy: 51.83 

Round  19, Global train loss: 1.130, Global test loss: 1.134, Global test accuracy: 59.99 

Round  20, Train loss: 1.103, Test loss: 1.373, Test accuracy: 52.35 

Round  20, Global train loss: 1.103, Global test loss: 1.129, Global test accuracy: 60.20 

Round  21, Train loss: 1.073, Test loss: 1.380, Test accuracy: 52.37 

Round  21, Global train loss: 1.073, Global test loss: 1.133, Global test accuracy: 60.27 

Round  22, Train loss: 1.104, Test loss: 1.364, Test accuracy: 52.84 

Round  22, Global train loss: 1.104, Global test loss: 1.111, Global test accuracy: 61.02 

Round  23, Train loss: 1.061, Test loss: 1.360, Test accuracy: 53.02 

Round  23, Global train loss: 1.061, Global test loss: 1.084, Global test accuracy: 61.79 

Round  24, Train loss: 1.026, Test loss: 1.371, Test accuracy: 53.26 

Round  24, Global train loss: 1.026, Global test loss: 1.086, Global test accuracy: 61.80 

Round  25, Train loss: 0.993, Test loss: 1.359, Test accuracy: 53.88 

Round  25, Global train loss: 0.993, Global test loss: 1.091, Global test accuracy: 62.10 

Round  26, Train loss: 0.967, Test loss: 1.353, Test accuracy: 54.30 

Round  26, Global train loss: 0.967, Global test loss: 1.081, Global test accuracy: 62.21 

Round  27, Train loss: 0.965, Test loss: 1.326, Test accuracy: 55.10 

Round  27, Global train loss: 0.965, Global test loss: 1.063, Global test accuracy: 63.15 

Round  28, Train loss: 0.959, Test loss: 1.330, Test accuracy: 55.21 

Round  28, Global train loss: 0.959, Global test loss: 1.052, Global test accuracy: 63.53 

Round  29, Train loss: 0.918, Test loss: 1.321, Test accuracy: 55.70 

Round  29, Global train loss: 0.918, Global test loss: 1.078, Global test accuracy: 62.50 

Round  30, Train loss: 0.927, Test loss: 1.338, Test accuracy: 55.66 

Round  30, Global train loss: 0.927, Global test loss: 1.070, Global test accuracy: 62.60 

Round  31, Train loss: 0.901, Test loss: 1.333, Test accuracy: 55.93 

Round  31, Global train loss: 0.901, Global test loss: 1.050, Global test accuracy: 63.69 

Round  32, Train loss: 0.879, Test loss: 1.333, Test accuracy: 56.20 

Round  32, Global train loss: 0.879, Global test loss: 1.046, Global test accuracy: 63.80 

Round  33, Train loss: 0.856, Test loss: 1.326, Test accuracy: 56.61 

Round  33, Global train loss: 0.856, Global test loss: 1.041, Global test accuracy: 64.00 

Round  34, Train loss: 0.910, Test loss: 1.318, Test accuracy: 57.27 

Round  34, Global train loss: 0.910, Global test loss: 1.015, Global test accuracy: 65.33 

Round  35, Train loss: 0.854, Test loss: 1.318, Test accuracy: 57.72 

Round  35, Global train loss: 0.854, Global test loss: 1.013, Global test accuracy: 65.54 

Round  36, Train loss: 0.820, Test loss: 1.329, Test accuracy: 57.50 

Round  36, Global train loss: 0.820, Global test loss: 1.035, Global test accuracy: 64.95 

Round  37, Train loss: 0.810, Test loss: 1.308, Test accuracy: 58.05 

Round  37, Global train loss: 0.810, Global test loss: 1.012, Global test accuracy: 65.42 

Round  38, Train loss: 0.801, Test loss: 1.329, Test accuracy: 57.88 

Round  38, Global train loss: 0.801, Global test loss: 1.033, Global test accuracy: 65.24 

Round  39, Train loss: 0.770, Test loss: 1.340, Test accuracy: 58.01 

Round  39, Global train loss: 0.770, Global test loss: 1.026, Global test accuracy: 65.21 

Round  40, Train loss: 0.768, Test loss: 1.346, Test accuracy: 57.69 

Round  40, Global train loss: 0.768, Global test loss: 1.061, Global test accuracy: 64.61 

Round  41, Train loss: 0.831, Test loss: 1.364, Test accuracy: 57.66 

Round  41, Global train loss: 0.831, Global test loss: 1.022, Global test accuracy: 65.26 

Round  42, Train loss: 0.739, Test loss: 1.347, Test accuracy: 58.20 

Round  42, Global train loss: 0.739, Global test loss: 1.020, Global test accuracy: 65.98 

Round  43, Train loss: 0.717, Test loss: 1.373, Test accuracy: 57.96 

Round  43, Global train loss: 0.717, Global test loss: 1.024, Global test accuracy: 65.77 

Round  44, Train loss: 0.722, Test loss: 1.356, Test accuracy: 58.27 

Round  44, Global train loss: 0.722, Global test loss: 1.028, Global test accuracy: 65.84 

Round  45, Train loss: 0.730, Test loss: 1.345, Test accuracy: 58.77 

Round  45, Global train loss: 0.730, Global test loss: 0.996, Global test accuracy: 67.00 

Round  46, Train loss: 0.716, Test loss: 1.344, Test accuracy: 58.79 

Round  46, Global train loss: 0.716, Global test loss: 1.020, Global test accuracy: 66.34 

Round  47, Train loss: 0.775, Test loss: 1.357, Test accuracy: 58.97 

Round  47, Global train loss: 0.775, Global test loss: 1.014, Global test accuracy: 66.56 

Round  48, Train loss: 0.659, Test loss: 1.363, Test accuracy: 59.10 

Round  48, Global train loss: 0.659, Global test loss: 1.028, Global test accuracy: 66.67 

Round  49, Train loss: 0.678, Test loss: 1.368, Test accuracy: 59.16 

Round  49, Global train loss: 0.678, Global test loss: 1.023, Global test accuracy: 66.65 

Round  50, Train loss: 0.685, Test loss: 1.380, Test accuracy: 58.87 

Round  50, Global train loss: 0.685, Global test loss: 1.011, Global test accuracy: 67.01 

Round  51, Train loss: 0.712, Test loss: 1.386, Test accuracy: 59.20 

Round  51, Global train loss: 0.712, Global test loss: 1.015, Global test accuracy: 66.96 

Round  52, Train loss: 0.700, Test loss: 1.367, Test accuracy: 59.76 

Round  52, Global train loss: 0.700, Global test loss: 0.996, Global test accuracy: 67.76 

Round  53, Train loss: 0.660, Test loss: 1.356, Test accuracy: 60.08 

Round  53, Global train loss: 0.660, Global test loss: 1.003, Global test accuracy: 67.49 

Round  54, Train loss: 0.651, Test loss: 1.363, Test accuracy: 60.13 

Round  54, Global train loss: 0.651, Global test loss: 1.011, Global test accuracy: 67.54 

Round  55, Train loss: 0.658, Test loss: 1.388, Test accuracy: 59.97 

Round  55, Global train loss: 0.658, Global test loss: 1.017, Global test accuracy: 67.47 

Round  56, Train loss: 0.624, Test loss: 1.394, Test accuracy: 59.96 

Round  56, Global train loss: 0.624, Global test loss: 1.012, Global test accuracy: 67.57 

Round  57, Train loss: 0.643, Test loss: 1.365, Test accuracy: 60.42 

Round  57, Global train loss: 0.643, Global test loss: 1.001, Global test accuracy: 67.73 

Round  58, Train loss: 0.619, Test loss: 1.362, Test accuracy: 60.74 

Round  58, Global train loss: 0.619, Global test loss: 1.010, Global test accuracy: 67.64 

Round  59, Train loss: 0.596, Test loss: 1.364, Test accuracy: 60.95 

Round  59, Global train loss: 0.596, Global test loss: 1.007, Global test accuracy: 68.22 

Round  60, Train loss: 0.621, Test loss: 1.363, Test accuracy: 61.18 

Round  60, Global train loss: 0.621, Global test loss: 1.017, Global test accuracy: 67.55 

Round  61, Train loss: 0.611, Test loss: 1.370, Test accuracy: 61.02 

Round  61, Global train loss: 0.611, Global test loss: 1.005, Global test accuracy: 68.16 

Round  62, Train loss: 0.607, Test loss: 1.364, Test accuracy: 61.10 

Round  62, Global train loss: 0.607, Global test loss: 0.982, Global test accuracy: 68.80 

Round  63, Train loss: 0.553, Test loss: 1.376, Test accuracy: 61.01 

Round  63, Global train loss: 0.553, Global test loss: 1.021, Global test accuracy: 68.19 

Round  64, Train loss: 0.580, Test loss: 1.383, Test accuracy: 61.24 

Round  64, Global train loss: 0.580, Global test loss: 1.029, Global test accuracy: 68.12 

Round  65, Train loss: 0.560, Test loss: 1.396, Test accuracy: 61.21 

Round  65, Global train loss: 0.560, Global test loss: 1.008, Global test accuracy: 68.99 

Round  66, Train loss: 0.573, Test loss: 1.383, Test accuracy: 61.53 

Round  66, Global train loss: 0.573, Global test loss: 1.028, Global test accuracy: 68.69 

Round  67, Train loss: 0.520, Test loss: 1.382, Test accuracy: 61.83 

Round  67, Global train loss: 0.520, Global test loss: 1.021, Global test accuracy: 69.14 

Round  68, Train loss: 0.552, Test loss: 1.394, Test accuracy: 61.84 

Round  68, Global train loss: 0.552, Global test loss: 1.033, Global test accuracy: 68.54 

Round  69, Train loss: 0.548, Test loss: 1.386, Test accuracy: 61.81 

Round  69, Global train loss: 0.548, Global test loss: 1.035, Global test accuracy: 68.31 

Round  70, Train loss: 0.553, Test loss: 1.395, Test accuracy: 61.83 

Round  70, Global train loss: 0.553, Global test loss: 1.023, Global test accuracy: 68.31 

Round  71, Train loss: 0.522, Test loss: 1.408, Test accuracy: 61.78 

Round  71, Global train loss: 0.522, Global test loss: 1.070, Global test accuracy: 67.57 

Round  72, Train loss: 0.519, Test loss: 1.398, Test accuracy: 62.18 

Round  72, Global train loss: 0.519, Global test loss: 1.049, Global test accuracy: 68.69 

Round  73, Train loss: 0.558, Test loss: 1.388, Test accuracy: 62.22 

Round  73, Global train loss: 0.558, Global test loss: 1.023, Global test accuracy: 68.67 

Round  74, Train loss: 0.535, Test loss: 1.388, Test accuracy: 62.38 

Round  74, Global train loss: 0.535, Global test loss: 1.041, Global test accuracy: 68.23 

Round  75, Train loss: 0.515, Test loss: 1.383, Test accuracy: 62.39 

Round  75, Global train loss: 0.515, Global test loss: 1.039, Global test accuracy: 68.57 

Round  76, Train loss: 0.526, Test loss: 1.392, Test accuracy: 62.19 

Round  76, Global train loss: 0.526, Global test loss: 1.033, Global test accuracy: 68.56 

Round  77, Train loss: 0.525, Test loss: 1.385, Test accuracy: 62.43 

Round  77, Global train loss: 0.525, Global test loss: 1.020, Global test accuracy: 68.83 

Round  78, Train loss: 0.466, Test loss: 1.385, Test accuracy: 62.56 

Round  78, Global train loss: 0.466, Global test loss: 1.038, Global test accuracy: 68.60 

Round  79, Train loss: 0.491, Test loss: 1.407, Test accuracy: 62.37 

Round  79, Global train loss: 0.491, Global test loss: 1.057, Global test accuracy: 68.14 

Round  80, Train loss: 0.515, Test loss: 1.399, Test accuracy: 62.70 

Round  80, Global train loss: 0.515, Global test loss: 1.034, Global test accuracy: 68.54 

Round  81, Train loss: 0.478, Test loss: 1.395, Test accuracy: 62.98 

Round  81, Global train loss: 0.478, Global test loss: 1.048, Global test accuracy: 68.93 

Round  82, Train loss: 0.522, Test loss: 1.387, Test accuracy: 62.77 

Round  82, Global train loss: 0.522, Global test loss: 1.046, Global test accuracy: 68.28 

Round  83, Train loss: 0.463, Test loss: 1.419, Test accuracy: 62.79 

Round  83, Global train loss: 0.463, Global test loss: 1.092, Global test accuracy: 67.90 

Round  84, Train loss: 0.518, Test loss: 1.409, Test accuracy: 62.80 

Round  84, Global train loss: 0.518, Global test loss: 1.049, Global test accuracy: 68.51 

Round  85, Train loss: 0.455, Test loss: 1.424, Test accuracy: 62.68 

Round  85, Global train loss: 0.455, Global test loss: 1.083, Global test accuracy: 68.44 

Round  86, Train loss: 0.509, Test loss: 1.437, Test accuracy: 62.46 

Round  86, Global train loss: 0.509, Global test loss: 1.049, Global test accuracy: 68.67 

Round  87, Train loss: 0.470, Test loss: 1.424, Test accuracy: 62.97 

Round  87, Global train loss: 0.470, Global test loss: 1.067, Global test accuracy: 68.89 

Round  88, Train loss: 0.449, Test loss: 1.422, Test accuracy: 63.15 

Round  88, Global train loss: 0.449, Global test loss: 1.081, Global test accuracy: 68.89 

Round  89, Train loss: 0.443, Test loss: 1.439, Test accuracy: 63.16 

Round  89, Global train loss: 0.443, Global test loss: 1.050, Global test accuracy: 69.03 

Round  90, Train loss: 0.439, Test loss: 1.438, Test accuracy: 63.11 

Round  90, Global train loss: 0.439, Global test loss: 1.084, Global test accuracy: 68.54 

Round  91, Train loss: 0.464, Test loss: 1.443, Test accuracy: 63.21 

Round  91, Global train loss: 0.464, Global test loss: 1.080, Global test accuracy: 69.23 

Round  92, Train loss: 0.449, Test loss: 1.433, Test accuracy: 63.38 

Round  92, Global train loss: 0.449, Global test loss: 1.068, Global test accuracy: 69.51 

Round  93, Train loss: 0.426, Test loss: 1.449, Test accuracy: 63.20 

Round  93, Global train loss: 0.426, Global test loss: 1.095, Global test accuracy: 69.05 

Round  94, Train loss: 0.452, Test loss: 1.466, Test accuracy: 62.84 

Round  94, Global train loss: 0.452, Global test loss: 1.077, Global test accuracy: 69.32 

Round  95, Train loss: 0.443, Test loss: 1.460, Test accuracy: 62.78 

Round  95, Global train loss: 0.443, Global test loss: 1.077, Global test accuracy: 68.95 

Round  96, Train loss: 0.434, Test loss: 1.462, Test accuracy: 63.02 

Round  96, Global train loss: 0.434, Global test loss: 1.073, Global test accuracy: 69.18 

Round  97, Train loss: 0.478, Test loss: 1.457, Test accuracy: 62.96 

Round  97, Global train loss: 0.478, Global test loss: 1.080, Global test accuracy: 68.97 

Round  98, Train loss: 0.449, Test loss: 1.451, Test accuracy: 63.23 

Round  98, Global train loss: 0.449, Global test loss: 1.100, Global test accuracy: 68.84 

Round  99, Train loss: 0.429, Test loss: 1.447, Test accuracy: 63.23 

Round  99, Global train loss: 0.429, Global test loss: 1.080, Global test accuracy: 69.74 

Final Round, Train loss: 0.335, Test loss: 1.646, Test accuracy: 62.41 

Final Round, Global train loss: 0.335, Global test loss: 1.080, Global test accuracy: 69.74 

Average accuracy final 10 rounds: 63.09525 

Average global accuracy final 10 rounds: 69.1335 

2750.518965482712
[1.4191102981567383, 2.5369367599487305, 3.648463010787964, 4.7514612674713135, 5.863318920135498, 6.9686572551727295, 8.083221673965454, 9.301065444946289, 10.444710493087769, 11.561556339263916, 12.678393840789795, 13.800625085830688, 14.902364253997803, 16.023310899734497, 17.137409210205078, 18.238019704818726, 19.335100650787354, 20.470505952835083, 21.574443101882935, 22.696884632110596, 23.874844551086426, 25.00705909729004, 26.10233163833618, 27.21260166168213, 28.336180210113525, 29.44087505340576, 30.45210075378418, 31.46401882171631, 32.47675609588623, 33.491370677948, 34.51067781448364, 35.52295899391174, 36.54135704040527, 37.56004214286804, 38.58257818222046, 39.604838848114014, 40.61521601676941, 41.62768340110779, 42.791232109069824, 43.91703820228577, 44.93145775794983, 45.94464826583862, 46.96505856513977, 47.99238181114197, 49.02038025856018, 50.192840337753296, 51.36817741394043, 52.53734350204468, 53.70948123931885, 54.867433309555054, 56.032235860824585, 57.199230909347534, 58.36499619483948, 59.55068063735962, 60.58250713348389, 61.60277271270752, 62.62712001800537, 63.64636516571045, 64.67342162132263, 65.70101952552795, 66.72135019302368, 67.7456841468811, 68.76289916038513, 69.78810405731201, 70.96031427383423, 72.10039472579956, 73.3409173488617, 74.44102478027344, 75.4507360458374, 76.65025806427002, 77.88059568405151, 79.04473185539246, 80.07260537147522, 81.09681558609009, 82.12123560905457, 83.14402031898499, 84.16152572631836, 85.18962550163269, 86.20555448532104, 87.22197556495667, 88.23032426834106, 89.24093866348267, 90.26193761825562, 91.28047776222229, 92.30439758300781, 93.40482354164124, 94.41379714012146, 95.42365288734436, 96.43995714187622, 97.4594190120697, 98.48684644699097, 99.49722528457642, 100.50860548019409, 101.53613495826721, 102.56156897544861, 103.72936248779297, 104.91617274284363, 106.1131055355072, 107.21643304824829, 108.31621313095093, 110.79237413406372]
[25.8575, 28.93, 32.3075, 36.2125, 37.3525, 37.4725, 39.58, 41.225, 41.49, 43.5775, 45.1075, 46.01, 46.51, 47.72, 48.295, 48.775, 49.64, 50.195, 51.3175, 51.825, 52.355, 52.37, 52.8375, 53.025, 53.2625, 53.885, 54.3025, 55.1025, 55.21, 55.6975, 55.6625, 55.9325, 56.2025, 56.6125, 57.2725, 57.72, 57.5025, 58.0525, 57.88, 58.0075, 57.69, 57.66, 58.2025, 57.96, 58.275, 58.775, 58.7925, 58.9725, 59.1025, 59.155, 58.865, 59.205, 59.755, 60.0825, 60.13, 59.965, 59.9625, 60.4175, 60.745, 60.9475, 61.1825, 61.015, 61.105, 61.01, 61.2425, 61.21, 61.53, 61.8275, 61.8375, 61.815, 61.8325, 61.78, 62.1825, 62.2175, 62.385, 62.3875, 62.1875, 62.4325, 62.565, 62.3675, 62.7, 62.98, 62.7675, 62.7875, 62.795, 62.6775, 62.4575, 62.97, 63.15, 63.1575, 63.1075, 63.2075, 63.3775, 63.195, 62.8425, 62.7825, 63.02, 62.96, 63.2325, 63.2275, 62.41]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.482, Test loss: 2.436, Test accuracy: 19.97 

Round   1, Train loss: 1.002, Test loss: 2.471, Test accuracy: 30.63 

Round   2, Train loss: 0.978, Test loss: 1.650, Test accuracy: 40.73 

Round   3, Train loss: 0.931, Test loss: 1.419, Test accuracy: 45.33 

Round   4, Train loss: 0.796, Test loss: 1.340, Test accuracy: 52.62 

Round   5, Train loss: 0.766, Test loss: 1.002, Test accuracy: 59.65 

Round   6, Train loss: 0.696, Test loss: 1.005, Test accuracy: 60.17 

Round   7, Train loss: 0.803, Test loss: 0.999, Test accuracy: 62.60 

Round   8, Train loss: 0.754, Test loss: 0.822, Test accuracy: 65.10 

Round   9, Train loss: 0.748, Test loss: 0.718, Test accuracy: 68.48 

Round  10, Train loss: 0.682, Test loss: 0.699, Test accuracy: 69.50 

Round  11, Train loss: 0.654, Test loss: 0.666, Test accuracy: 71.55 

Round  12, Train loss: 0.613, Test loss: 0.662, Test accuracy: 71.73 

Round  13, Train loss: 0.701, Test loss: 0.656, Test accuracy: 71.71 

Round  14, Train loss: 0.665, Test loss: 0.627, Test accuracy: 72.90 

Round  15, Train loss: 0.717, Test loss: 0.626, Test accuracy: 73.38 

Round  16, Train loss: 0.607, Test loss: 0.626, Test accuracy: 73.85 

Round  17, Train loss: 0.587, Test loss: 0.611, Test accuracy: 74.54 

Round  18, Train loss: 0.567, Test loss: 0.585, Test accuracy: 74.75 

Round  19, Train loss: 0.633, Test loss: 0.586, Test accuracy: 74.70 

Round  20, Train loss: 0.685, Test loss: 0.578, Test accuracy: 75.77 

Round  21, Train loss: 0.612, Test loss: 0.562, Test accuracy: 75.88 

Round  22, Train loss: 0.644, Test loss: 0.553, Test accuracy: 76.75 

Round  23, Train loss: 0.660, Test loss: 0.561, Test accuracy: 76.47 

Round  24, Train loss: 0.555, Test loss: 0.559, Test accuracy: 76.62 

Round  25, Train loss: 0.597, Test loss: 0.544, Test accuracy: 77.30 

Round  26, Train loss: 0.552, Test loss: 0.546, Test accuracy: 77.35 

Round  27, Train loss: 0.478, Test loss: 0.545, Test accuracy: 77.26 

Round  28, Train loss: 0.566, Test loss: 0.536, Test accuracy: 77.48 

Round  29, Train loss: 0.573, Test loss: 0.530, Test accuracy: 77.81 

Round  30, Train loss: 0.510, Test loss: 0.515, Test accuracy: 78.38 

Round  31, Train loss: 0.514, Test loss: 0.525, Test accuracy: 78.15 

Round  32, Train loss: 0.531, Test loss: 0.519, Test accuracy: 78.29 

Round  33, Train loss: 0.544, Test loss: 0.507, Test accuracy: 78.90 

Round  34, Train loss: 0.480, Test loss: 0.495, Test accuracy: 79.50 

Round  35, Train loss: 0.379, Test loss: 0.489, Test accuracy: 79.89 

Round  36, Train loss: 0.496, Test loss: 0.496, Test accuracy: 79.47 

Round  37, Train loss: 0.389, Test loss: 0.484, Test accuracy: 80.13 

Round  38, Train loss: 0.599, Test loss: 0.487, Test accuracy: 79.92 

Round  39, Train loss: 0.506, Test loss: 0.480, Test accuracy: 80.30 

Round  40, Train loss: 0.455, Test loss: 0.481, Test accuracy: 80.20 

Round  41, Train loss: 0.479, Test loss: 0.479, Test accuracy: 80.35 

Round  42, Train loss: 0.543, Test loss: 0.477, Test accuracy: 80.47 

Round  43, Train loss: 0.458, Test loss: 0.467, Test accuracy: 81.08 

Round  44, Train loss: 0.474, Test loss: 0.459, Test accuracy: 81.20 

Round  45, Train loss: 0.522, Test loss: 0.467, Test accuracy: 80.84 

Round  46, Train loss: 0.397, Test loss: 0.459, Test accuracy: 81.30 

Round  47, Train loss: 0.425, Test loss: 0.466, Test accuracy: 81.12 

Round  48, Train loss: 0.420, Test loss: 0.468, Test accuracy: 81.11 

Round  49, Train loss: 0.444, Test loss: 0.475, Test accuracy: 80.89 

Round  50, Train loss: 0.396, Test loss: 0.464, Test accuracy: 81.46 

Round  51, Train loss: 0.451, Test loss: 0.450, Test accuracy: 81.80 

Round  52, Train loss: 0.354, Test loss: 0.441, Test accuracy: 82.10 

Round  53, Train loss: 0.409, Test loss: 0.442, Test accuracy: 82.12 

Round  54, Train loss: 0.383, Test loss: 0.441, Test accuracy: 82.27 

Round  55, Train loss: 0.336, Test loss: 0.447, Test accuracy: 82.12 

Round  56, Train loss: 0.403, Test loss: 0.439, Test accuracy: 82.29 

Round  57, Train loss: 0.402, Test loss: 0.439, Test accuracy: 82.41 

Round  58, Train loss: 0.389, Test loss: 0.442, Test accuracy: 82.67 

Round  59, Train loss: 0.291, Test loss: 0.448, Test accuracy: 82.04 

Round  60, Train loss: 0.307, Test loss: 0.453, Test accuracy: 81.67 

Round  61, Train loss: 0.398, Test loss: 0.448, Test accuracy: 82.13 

Round  62, Train loss: 0.341, Test loss: 0.461, Test accuracy: 81.55 

Round  63, Train loss: 0.315, Test loss: 0.459, Test accuracy: 81.84 

Round  64, Train loss: 0.343, Test loss: 0.448, Test accuracy: 82.36 

Round  65, Train loss: 0.327, Test loss: 0.458, Test accuracy: 81.88 

Round  66, Train loss: 0.350, Test loss: 0.443, Test accuracy: 82.65 

Round  67, Train loss: 0.294, Test loss: 0.457, Test accuracy: 82.02 

Round  68, Train loss: 0.340, Test loss: 0.459, Test accuracy: 82.20 

Round  69, Train loss: 0.367, Test loss: 0.437, Test accuracy: 83.00 

Round  70, Train loss: 0.415, Test loss: 0.442, Test accuracy: 82.84 

Round  71, Train loss: 0.328, Test loss: 0.450, Test accuracy: 82.83 

Round  72, Train loss: 0.366, Test loss: 0.450, Test accuracy: 82.60 

Round  73, Train loss: 0.286, Test loss: 0.445, Test accuracy: 83.15 

Round  74, Train loss: 0.362, Test loss: 0.440, Test accuracy: 83.15 

Round  75, Train loss: 0.246, Test loss: 0.441, Test accuracy: 83.25 

Round  76, Train loss: 0.311, Test loss: 0.440, Test accuracy: 83.14 

Round  77, Train loss: 0.185, Test loss: 0.440, Test accuracy: 83.40 

Round  78, Train loss: 0.251, Test loss: 0.441, Test accuracy: 83.18 

Round  79, Train loss: 0.319, Test loss: 0.439, Test accuracy: 83.15 

Round  80, Train loss: 0.300, Test loss: 0.448, Test accuracy: 82.92 

Round  81, Train loss: 0.347, Test loss: 0.448, Test accuracy: 83.06 

Round  82, Train loss: 0.297, Test loss: 0.433, Test accuracy: 83.57 

Round  83, Train loss: 0.257, Test loss: 0.449, Test accuracy: 83.01 

Round  84, Train loss: 0.292, Test loss: 0.438, Test accuracy: 83.32 

Round  85, Train loss: 0.343, Test loss: 0.434, Test accuracy: 83.72 

Round  86, Train loss: 0.318, Test loss: 0.443, Test accuracy: 83.64 

Round  87, Train loss: 0.327, Test loss: 0.448, Test accuracy: 83.40 

Round  88, Train loss: 0.315, Test loss: 0.437, Test accuracy: 83.69 

Round  89, Train loss: 0.264, Test loss: 0.439, Test accuracy: 83.39 

Round  90, Train loss: 0.244, Test loss: 0.427, Test accuracy: 83.98 

Round  91, Train loss: 0.296, Test loss: 0.445, Test accuracy: 83.52 

Round  92, Train loss: 0.238, Test loss: 0.454, Test accuracy: 83.35 

Round  93, Train loss: 0.371, Test loss: 0.437, Test accuracy: 83.70 

Round  94, Train loss: 0.291, Test loss: 0.433, Test accuracy: 83.72 

Round  95, Train loss: 0.323, Test loss: 0.443, Test accuracy: 83.52 

Round  96, Train loss: 0.297, Test loss: 0.441, Test accuracy: 83.89 

Round  97, Train loss: 0.216, Test loss: 0.451, Test accuracy: 83.26 

Round  98, Train loss: 0.371, Test loss: 0.456, Test accuracy: 83.39 

Round  99, Train loss: 0.269, Test loss: 0.451, Test accuracy: 83.68 

Final Round, Train loss: 0.227, Test loss: 0.450, Test accuracy: 84.08 

Average accuracy final 10 rounds: 83.60041666666666 

1332.674974679947
[1.2975826263427734, 2.364497184753418, 3.4176628589630127, 4.470544338226318, 5.5329906940460205, 6.59775447845459, 7.6614601612091064, 8.719135522842407, 9.77370285987854, 10.832968711853027, 11.899566888809204, 12.963523864746094, 14.081355810165405, 15.1348135471344, 16.18940234184265, 17.28781771659851, 18.394970417022705, 19.488526344299316, 20.619014263153076, 21.678698778152466, 22.738357305526733, 23.78702211380005, 24.83754587173462, 25.89521884918213, 26.945056915283203, 27.999502182006836, 29.05882978439331, 30.10999846458435, 31.169787883758545, 32.23230719566345, 33.29000186920166, 34.3428795337677, 35.404693365097046, 36.46009111404419, 37.516154289245605, 38.575153827667236, 39.63397240638733, 40.692543745040894, 41.7528395652771, 42.81345725059509, 43.93070673942566, 45.07666230201721, 46.13828206062317, 47.196688413619995, 48.25411534309387, 49.309107542037964, 50.40505790710449, 51.55848050117493, 52.709893465042114, 53.86314153671265, 55.107569217681885, 56.259151220321655, 57.4006769657135, 58.613271713256836, 59.781388998031616, 60.889045000076294, 61.97077965736389, 63.09860825538635, 64.2313802242279, 65.36070513725281, 66.50124645233154, 67.64754295349121, 68.78577041625977, 69.92929077148438, 71.06789779663086, 72.20626306533813, 73.34915852546692, 74.48651218414307, 75.63941931724548, 76.78067827224731, 77.92126107215881, 79.05344009399414, 80.19277262687683, 81.33317160606384, 82.46215796470642, 83.59831070899963, 84.7417061328888, 85.88299584388733, 87.0232903957367, 88.16299366950989, 89.29859638214111, 90.4396243095398, 91.58955478668213, 92.72304630279541, 93.86425042152405, 95.0052444934845, 96.14259076118469, 97.28427505493164, 98.42162728309631, 99.55743050575256, 100.60662627220154, 101.7038459777832, 102.7982816696167, 103.90031909942627, 104.9993667602539, 106.0954008102417, 107.22868871688843, 108.33490681648254, 109.43111634254456, 110.53170943260193, 112.51845383644104]
[19.970833333333335, 30.633333333333333, 40.729166666666664, 45.325, 52.62083333333333, 59.645833333333336, 60.175, 62.6, 65.09583333333333, 68.48333333333333, 69.50416666666666, 71.55416666666666, 71.73333333333333, 71.7125, 72.89583333333333, 73.375, 73.85, 74.5375, 74.75, 74.69583333333334, 75.77083333333333, 75.875, 76.75416666666666, 76.47083333333333, 76.61666666666666, 77.29583333333333, 77.35416666666667, 77.2625, 77.47916666666667, 77.80833333333334, 78.38333333333334, 78.15, 78.2875, 78.89583333333333, 79.50416666666666, 79.89166666666667, 79.47083333333333, 80.12916666666666, 79.91666666666667, 80.30416666666666, 80.20416666666667, 80.34583333333333, 80.47083333333333, 81.075, 81.19583333333334, 80.84166666666667, 81.3, 81.12083333333334, 81.10833333333333, 80.89166666666667, 81.45833333333333, 81.79583333333333, 82.09583333333333, 82.125, 82.27083333333333, 82.12083333333334, 82.2875, 82.40833333333333, 82.67083333333333, 82.0375, 81.675, 82.13333333333334, 81.55, 81.84166666666667, 82.3625, 81.88333333333334, 82.65, 82.01666666666667, 82.2, 83.0, 82.84166666666667, 82.82916666666667, 82.59583333333333, 83.14583333333333, 83.15416666666667, 83.25416666666666, 83.14166666666667, 83.40416666666667, 83.18333333333334, 83.15, 82.91666666666667, 83.05833333333334, 83.56666666666666, 83.00833333333334, 83.31666666666666, 83.72083333333333, 83.64166666666667, 83.40416666666667, 83.6875, 83.3875, 83.97916666666667, 83.52083333333333, 83.34583333333333, 83.70416666666667, 83.71666666666667, 83.52083333333333, 83.8875, 83.2625, 83.3875, 83.67916666666666, 84.075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.538, Test loss: 2.035, Test accuracy: 22.84
Round   1, Train loss: 1.070, Test loss: 1.825, Test accuracy: 32.04
Round   2, Train loss: 0.942, Test loss: 1.379, Test accuracy: 43.08
Round   3, Train loss: 0.835, Test loss: 1.458, Test accuracy: 48.42
Round   4, Train loss: 0.909, Test loss: 1.057, Test accuracy: 60.12
Round   5, Train loss: 0.799, Test loss: 0.974, Test accuracy: 64.39
Round   6, Train loss: 0.801, Test loss: 0.824, Test accuracy: 67.01
Round   7, Train loss: 0.721, Test loss: 0.863, Test accuracy: 67.67
Round   8, Train loss: 0.800, Test loss: 0.741, Test accuracy: 70.81
Round   9, Train loss: 0.729, Test loss: 0.705, Test accuracy: 72.08
Round  10, Train loss: 0.674, Test loss: 0.706, Test accuracy: 72.48
Round  11, Train loss: 0.566, Test loss: 0.696, Test accuracy: 71.88
Round  12, Train loss: 0.704, Test loss: 0.673, Test accuracy: 73.11
Round  13, Train loss: 0.756, Test loss: 0.633, Test accuracy: 75.14
Round  14, Train loss: 0.700, Test loss: 0.632, Test accuracy: 75.11
Round  15, Train loss: 0.691, Test loss: 0.623, Test accuracy: 75.33
Round  16, Train loss: 0.666, Test loss: 0.585, Test accuracy: 76.41
Round  17, Train loss: 0.629, Test loss: 0.581, Test accuracy: 76.82
Round  18, Train loss: 0.636, Test loss: 0.574, Test accuracy: 77.29
Round  19, Train loss: 0.605, Test loss: 0.557, Test accuracy: 78.31
Round  20, Train loss: 0.559, Test loss: 0.538, Test accuracy: 78.57
Round  21, Train loss: 0.485, Test loss: 0.536, Test accuracy: 78.77
Round  22, Train loss: 0.636, Test loss: 0.566, Test accuracy: 77.65
Round  23, Train loss: 0.591, Test loss: 0.529, Test accuracy: 79.14
Round  24, Train loss: 0.465, Test loss: 0.523, Test accuracy: 79.41
Round  25, Train loss: 0.574, Test loss: 0.534, Test accuracy: 79.10
Round  26, Train loss: 0.516, Test loss: 0.521, Test accuracy: 79.48
Round  27, Train loss: 0.515, Test loss: 0.521, Test accuracy: 79.92
Round  28, Train loss: 0.567, Test loss: 0.514, Test accuracy: 79.96
Round  29, Train loss: 0.469, Test loss: 0.493, Test accuracy: 80.45
Round  30, Train loss: 0.583, Test loss: 0.500, Test accuracy: 80.46
Round  31, Train loss: 0.571, Test loss: 0.502, Test accuracy: 80.39
Round  32, Train loss: 0.538, Test loss: 0.495, Test accuracy: 80.27
Round  33, Train loss: 0.475, Test loss: 0.475, Test accuracy: 80.54
Round  34, Train loss: 0.480, Test loss: 0.470, Test accuracy: 81.22
Round  35, Train loss: 0.419, Test loss: 0.461, Test accuracy: 81.38
Round  36, Train loss: 0.579, Test loss: 0.469, Test accuracy: 81.51
Round  37, Train loss: 0.460, Test loss: 0.467, Test accuracy: 81.55
Round  38, Train loss: 0.422, Test loss: 0.453, Test accuracy: 82.12
Round  39, Train loss: 0.480, Test loss: 0.450, Test accuracy: 82.07
Round  40, Train loss: 0.426, Test loss: 0.449, Test accuracy: 81.95
Round  41, Train loss: 0.440, Test loss: 0.447, Test accuracy: 82.42
Round  42, Train loss: 0.474, Test loss: 0.438, Test accuracy: 82.59
Round  43, Train loss: 0.534, Test loss: 0.442, Test accuracy: 82.88
Round  44, Train loss: 0.521, Test loss: 0.448, Test accuracy: 82.70
Round  45, Train loss: 0.486, Test loss: 0.438, Test accuracy: 82.58
Round  46, Train loss: 0.438, Test loss: 0.431, Test accuracy: 82.85
Round  47, Train loss: 0.464, Test loss: 0.434, Test accuracy: 82.88
Round  48, Train loss: 0.530, Test loss: 0.439, Test accuracy: 82.59
Round  49, Train loss: 0.425, Test loss: 0.431, Test accuracy: 82.75
Round  50, Train loss: 0.426, Test loss: 0.432, Test accuracy: 82.86
Round  51, Train loss: 0.440, Test loss: 0.427, Test accuracy: 83.09
Round  52, Train loss: 0.481, Test loss: 0.424, Test accuracy: 83.26
Round  53, Train loss: 0.336, Test loss: 0.420, Test accuracy: 83.26
Round  54, Train loss: 0.417, Test loss: 0.425, Test accuracy: 83.17
Round  55, Train loss: 0.359, Test loss: 0.419, Test accuracy: 83.30
Round  56, Train loss: 0.364, Test loss: 0.419, Test accuracy: 83.10
Round  57, Train loss: 0.330, Test loss: 0.410, Test accuracy: 83.45
Round  58, Train loss: 0.311, Test loss: 0.414, Test accuracy: 83.26
Round  59, Train loss: 0.424, Test loss: 0.415, Test accuracy: 83.54
Round  60, Train loss: 0.419, Test loss: 0.415, Test accuracy: 83.62
Round  61, Train loss: 0.325, Test loss: 0.412, Test accuracy: 83.40
Round  62, Train loss: 0.316, Test loss: 0.412, Test accuracy: 83.46
Round  63, Train loss: 0.271, Test loss: 0.418, Test accuracy: 82.94
Round  64, Train loss: 0.320, Test loss: 0.414, Test accuracy: 83.33
Round  65, Train loss: 0.345, Test loss: 0.419, Test accuracy: 83.28
Round  66, Train loss: 0.364, Test loss: 0.414, Test accuracy: 83.57
Round  67, Train loss: 0.388, Test loss: 0.407, Test accuracy: 83.57
Round  68, Train loss: 0.406, Test loss: 0.410, Test accuracy: 83.77
Round  69, Train loss: 0.341, Test loss: 0.410, Test accuracy: 83.95
Round  70, Train loss: 0.336, Test loss: 0.407, Test accuracy: 84.03
Round  71, Train loss: 0.333, Test loss: 0.399, Test accuracy: 84.33
Round  72, Train loss: 0.337, Test loss: 0.406, Test accuracy: 83.95
Round  73, Train loss: 0.311, Test loss: 0.401, Test accuracy: 84.11
Round  74, Train loss: 0.269, Test loss: 0.394, Test accuracy: 84.33
Round  75, Train loss: 0.356, Test loss: 0.407, Test accuracy: 83.80
Round  76, Train loss: 0.253, Test loss: 0.397, Test accuracy: 84.23
Round  77, Train loss: 0.337, Test loss: 0.395, Test accuracy: 84.27
Round  78, Train loss: 0.327, Test loss: 0.402, Test accuracy: 84.07
Round  79, Train loss: 0.257, Test loss: 0.401, Test accuracy: 84.09
Round  80, Train loss: 0.275, Test loss: 0.399, Test accuracy: 84.20
Round  81, Train loss: 0.380, Test loss: 0.401, Test accuracy: 84.12
Round  82, Train loss: 0.243, Test loss: 0.396, Test accuracy: 84.33
Round  83, Train loss: 0.327, Test loss: 0.397, Test accuracy: 84.45
Round  84, Train loss: 0.351, Test loss: 0.395, Test accuracy: 84.30
Round  85, Train loss: 0.278, Test loss: 0.396, Test accuracy: 84.33
Round  86, Train loss: 0.313, Test loss: 0.394, Test accuracy: 84.46
Round  87, Train loss: 0.295, Test loss: 0.395, Test accuracy: 84.46
Round  88, Train loss: 0.345, Test loss: 0.394, Test accuracy: 84.37
Round  89, Train loss: 0.259, Test loss: 0.394, Test accuracy: 84.60
Round  90, Train loss: 0.236, Test loss: 0.391, Test accuracy: 84.58
Round  91, Train loss: 0.223, Test loss: 0.390, Test accuracy: 84.57
Round  92, Train loss: 0.377, Test loss: 0.394, Test accuracy: 84.50
Round  93, Train loss: 0.245, Test loss: 0.391, Test accuracy: 84.62
Round  94, Train loss: 0.264, Test loss: 0.388, Test accuracy: 84.70
Round  95, Train loss: 0.283, Test loss: 0.393, Test accuracy: 84.57
Round  96, Train loss: 0.250, Test loss: 0.389, Test accuracy: 84.73
Round  97, Train loss: 0.244, Test loss: 0.385, Test accuracy: 84.97
Round  98, Train loss: 0.278, Test loss: 0.385, Test accuracy: 85.12
Round  99, Train loss: 0.252, Test loss: 0.393, Test accuracy: 84.55
Final Round, Train loss: 0.224, Test loss: 0.385, Test accuracy: 84.95
Average accuracy final 10 rounds: 84.69041666666666
1397.4424095153809
[1.7772326469421387, 3.0446717739105225, 4.275237798690796, 5.507526874542236, 6.740595817565918, 7.9680681228637695, 9.201441049575806, 10.430102825164795, 11.658099174499512, 12.887461423873901, 14.120133399963379, 15.342477083206177, 16.571966648101807, 17.796764135360718, 19.020524978637695, 20.250532388687134, 21.47943663597107, 22.706296682357788, 23.931199073791504, 25.15928339958191, 26.386040925979614, 27.623882055282593, 28.984041690826416, 30.21211814880371, 31.44769859313965, 32.67979669570923, 33.905088663101196, 35.12991189956665, 36.3573899269104, 37.583107233047485, 38.97013330459595, 40.35189771652222, 41.73415422439575, 43.114174365997314, 44.50488305091858, 45.8801109790802, 47.24172353744507, 48.609496116638184, 49.9787483215332, 51.42153716087341, 52.6567120552063, 53.89135980606079, 55.126370906829834, 56.35935044288635, 57.5913827419281, 58.82577133178711, 60.06865954399109, 61.29329514503479, 62.52996039390564, 63.75947713851929, 64.99725103378296, 66.2253098487854, 67.46423745155334, 68.69416093826294, 69.92899131774902, 71.15790939331055, 72.39369893074036, 73.76167440414429, 74.99588465690613, 76.21587014198303, 77.5196852684021, 78.74634027481079, 79.97131681442261, 81.19571471214294, 82.43902111053467, 83.66621589660645, 84.8841381072998, 86.11442732810974, 87.34083819389343, 88.57213973999023, 89.88190531730652, 91.12094497680664, 92.36096525192261, 93.59269666671753, 94.83268094062805, 96.08331084251404, 97.31765031814575, 98.62552905082703, 99.86347889900208, 101.08914923667908, 102.32041215896606, 103.53269338607788, 104.74631524085999, 105.96287274360657, 107.18358111381531, 108.4026927947998, 109.6277871131897, 110.8503487110138, 112.12997436523438, 113.37963700294495, 114.60326504707336, 115.83391547203064, 117.16229820251465, 118.4002628326416, 119.65108394622803, 120.89033246040344, 122.1304259300232, 123.36815214157104, 124.61512565612793, 125.84623217582703, 128.03400015830994]
[22.8375, 32.0375, 43.075, 48.416666666666664, 60.11666666666667, 64.39166666666667, 67.0125, 67.675, 70.8125, 72.07916666666667, 72.47916666666667, 71.88333333333334, 73.1125, 75.14166666666667, 75.10833333333333, 75.32916666666667, 76.4125, 76.81666666666666, 77.2875, 78.30833333333334, 78.57083333333334, 78.76666666666667, 77.65, 79.14166666666667, 79.4125, 79.1, 79.47916666666667, 79.91666666666667, 79.9625, 80.44583333333334, 80.4625, 80.39166666666667, 80.26666666666667, 80.5375, 81.21666666666667, 81.375, 81.5125, 81.55416666666666, 82.11666666666666, 82.06666666666666, 81.94583333333334, 82.41666666666667, 82.5875, 82.875, 82.7, 82.575, 82.85, 82.875, 82.5875, 82.75416666666666, 82.8625, 83.0875, 83.2625, 83.25833333333334, 83.17083333333333, 83.30416666666666, 83.09583333333333, 83.44583333333334, 83.2625, 83.5375, 83.62083333333334, 83.4, 83.4625, 82.94166666666666, 83.32916666666667, 83.28333333333333, 83.56666666666666, 83.57083333333334, 83.77083333333333, 83.94583333333334, 84.03333333333333, 84.325, 83.95, 84.1125, 84.32916666666667, 83.8, 84.22916666666667, 84.26666666666667, 84.07083333333334, 84.0875, 84.2, 84.12083333333334, 84.32916666666667, 84.45, 84.3, 84.33333333333333, 84.45833333333333, 84.45833333333333, 84.37083333333334, 84.59583333333333, 84.58333333333333, 84.56666666666666, 84.49583333333334, 84.62083333333334, 84.70416666666667, 84.57083333333334, 84.72916666666667, 84.96666666666667, 85.12083333333334, 84.54583333333333, 84.95]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.264, Test loss: 2.091, Test accuracy: 25.27
Round   1, Train loss: 2.073, Test loss: 1.921, Test accuracy: 28.39
Round   2, Train loss: 1.940, Test loss: 1.761, Test accuracy: 34.94
Round   3, Train loss: 1.852, Test loss: 1.647, Test accuracy: 39.88
Round   4, Train loss: 1.755, Test loss: 1.585, Test accuracy: 42.20
Round   5, Train loss: 1.672, Test loss: 1.514, Test accuracy: 45.19
Round   6, Train loss: 1.621, Test loss: 1.462, Test accuracy: 46.85
Round   7, Train loss: 1.576, Test loss: 1.424, Test accuracy: 48.06
Round   8, Train loss: 1.502, Test loss: 1.398, Test accuracy: 49.07
Round   9, Train loss: 1.572, Test loss: 1.371, Test accuracy: 50.24
Round  10, Train loss: 1.473, Test loss: 1.353, Test accuracy: 51.34
Round  11, Train loss: 1.461, Test loss: 1.328, Test accuracy: 52.81
Round  12, Train loss: 1.427, Test loss: 1.291, Test accuracy: 54.38
Round  13, Train loss: 1.312, Test loss: 1.268, Test accuracy: 55.63
Round  14, Train loss: 1.289, Test loss: 1.256, Test accuracy: 55.29
Round  15, Train loss: 1.271, Test loss: 1.238, Test accuracy: 56.18
Round  16, Train loss: 1.305, Test loss: 1.205, Test accuracy: 57.75
Round  17, Train loss: 1.208, Test loss: 1.206, Test accuracy: 57.50
Round  18, Train loss: 1.228, Test loss: 1.188, Test accuracy: 57.90
Round  19, Train loss: 1.126, Test loss: 1.181, Test accuracy: 58.59
Round  20, Train loss: 1.085, Test loss: 1.182, Test accuracy: 59.05
Round  21, Train loss: 1.095, Test loss: 1.152, Test accuracy: 60.27
Round  22, Train loss: 1.164, Test loss: 1.139, Test accuracy: 60.31
Round  23, Train loss: 1.063, Test loss: 1.134, Test accuracy: 60.54
Round  24, Train loss: 1.007, Test loss: 1.152, Test accuracy: 59.88
Round  25, Train loss: 1.036, Test loss: 1.110, Test accuracy: 60.93
Round  26, Train loss: 1.008, Test loss: 1.122, Test accuracy: 61.32
Round  27, Train loss: 0.911, Test loss: 1.121, Test accuracy: 61.02
Round  28, Train loss: 0.971, Test loss: 1.116, Test accuracy: 61.07
Round  29, Train loss: 0.907, Test loss: 1.104, Test accuracy: 61.93
Round  30, Train loss: 0.916, Test loss: 1.094, Test accuracy: 61.91
Round  31, Train loss: 0.909, Test loss: 1.090, Test accuracy: 62.63
Round  32, Train loss: 0.887, Test loss: 1.100, Test accuracy: 62.34
Round  33, Train loss: 0.825, Test loss: 1.099, Test accuracy: 62.51
Round  34, Train loss: 0.864, Test loss: 1.091, Test accuracy: 63.66
Round  35, Train loss: 0.840, Test loss: 1.094, Test accuracy: 63.48
Round  36, Train loss: 0.834, Test loss: 1.085, Test accuracy: 63.64
Round  37, Train loss: 0.831, Test loss: 1.083, Test accuracy: 64.05
Round  38, Train loss: 0.818, Test loss: 1.067, Test accuracy: 64.84
Round  39, Train loss: 0.746, Test loss: 1.089, Test accuracy: 64.47
Round  40, Train loss: 0.762, Test loss: 1.072, Test accuracy: 64.84
Round  41, Train loss: 0.727, Test loss: 1.090, Test accuracy: 64.48
Round  42, Train loss: 0.699, Test loss: 1.087, Test accuracy: 64.50
Round  43, Train loss: 0.709, Test loss: 1.079, Test accuracy: 64.64
Round  44, Train loss: 0.759, Test loss: 1.047, Test accuracy: 65.50
Round  45, Train loss: 0.716, Test loss: 1.079, Test accuracy: 64.94
Round  46, Train loss: 0.726, Test loss: 1.082, Test accuracy: 64.99
Round  47, Train loss: 0.681, Test loss: 1.087, Test accuracy: 65.16
Round  48, Train loss: 0.698, Test loss: 1.080, Test accuracy: 64.80
Round  49, Train loss: 0.651, Test loss: 1.066, Test accuracy: 65.83
Round  50, Train loss: 0.670, Test loss: 1.063, Test accuracy: 65.70
Round  51, Train loss: 0.652, Test loss: 1.067, Test accuracy: 66.17
Round  52, Train loss: 0.608, Test loss: 1.071, Test accuracy: 66.20
Round  53, Train loss: 0.627, Test loss: 1.057, Test accuracy: 65.86
Round  54, Train loss: 0.591, Test loss: 1.071, Test accuracy: 65.96
Round  55, Train loss: 0.617, Test loss: 1.067, Test accuracy: 66.47
Round  56, Train loss: 0.646, Test loss: 1.052, Test accuracy: 67.01
Round  57, Train loss: 0.578, Test loss: 1.083, Test accuracy: 66.83
Round  58, Train loss: 0.571, Test loss: 1.111, Test accuracy: 65.98
Round  59, Train loss: 0.591, Test loss: 1.096, Test accuracy: 66.15
Round  60, Train loss: 0.554, Test loss: 1.113, Test accuracy: 66.20
Round  61, Train loss: 0.560, Test loss: 1.097, Test accuracy: 67.08
Round  62, Train loss: 0.573, Test loss: 1.080, Test accuracy: 67.37
Round  63, Train loss: 0.578, Test loss: 1.084, Test accuracy: 67.17
Round  64, Train loss: 0.539, Test loss: 1.110, Test accuracy: 66.26
Round  65, Train loss: 0.531, Test loss: 1.118, Test accuracy: 66.06
Round  66, Train loss: 0.481, Test loss: 1.142, Test accuracy: 66.27
Round  67, Train loss: 0.540, Test loss: 1.100, Test accuracy: 66.46
Round  68, Train loss: 0.517, Test loss: 1.118, Test accuracy: 67.45
Round  69, Train loss: 0.464, Test loss: 1.130, Test accuracy: 66.67
Round  70, Train loss: 0.491, Test loss: 1.114, Test accuracy: 66.89
Round  71, Train loss: 0.473, Test loss: 1.108, Test accuracy: 66.64
Round  72, Train loss: 0.511, Test loss: 1.082, Test accuracy: 67.48
Round  73, Train loss: 0.453, Test loss: 1.121, Test accuracy: 67.31
Round  74, Train loss: 0.497, Test loss: 1.137, Test accuracy: 67.28
Round  75, Train loss: 0.448, Test loss: 1.123, Test accuracy: 67.76
Round  76, Train loss: 0.479, Test loss: 1.110, Test accuracy: 67.67
Round  77, Train loss: 0.456, Test loss: 1.098, Test accuracy: 67.73
Round  78, Train loss: 0.424, Test loss: 1.131, Test accuracy: 67.53
Round  79, Train loss: 0.453, Test loss: 1.131, Test accuracy: 67.71
Round  80, Train loss: 0.422, Test loss: 1.145, Test accuracy: 67.47
Round  81, Train loss: 0.430, Test loss: 1.151, Test accuracy: 67.20
Round  82, Train loss: 0.462, Test loss: 1.108, Test accuracy: 67.75
Round  83, Train loss: 0.471, Test loss: 1.115, Test accuracy: 67.84
Round  84, Train loss: 0.377, Test loss: 1.154, Test accuracy: 67.60
Round  85, Train loss: 0.445, Test loss: 1.113, Test accuracy: 67.86
Round  86, Train loss: 0.434, Test loss: 1.123, Test accuracy: 68.12
Round  87, Train loss: 0.408, Test loss: 1.097, Test accuracy: 68.70
Round  88, Train loss: 0.390, Test loss: 1.137, Test accuracy: 68.15
Round  89, Train loss: 0.361, Test loss: 1.137, Test accuracy: 68.23
Round  90, Train loss: 0.386, Test loss: 1.147, Test accuracy: 68.06
Round  91, Train loss: 0.370, Test loss: 1.172, Test accuracy: 67.71
Round  92, Train loss: 0.395, Test loss: 1.142, Test accuracy: 67.91
Round  93, Train loss: 0.363, Test loss: 1.168, Test accuracy: 67.98
Round  94, Train loss: 0.355, Test loss: 1.153, Test accuracy: 68.17
Round  95, Train loss: 0.385, Test loss: 1.182, Test accuracy: 67.77
Round  96, Train loss: 0.430, Test loss: 1.132, Test accuracy: 67.89
Round  97, Train loss: 0.403, Test loss: 1.105, Test accuracy: 68.14
Round  98, Train loss: 0.354, Test loss: 1.164, Test accuracy: 68.31
Round  99, Train loss: 0.316, Test loss: 1.177, Test accuracy: 68.30
Final Round, Train loss: 0.363, Test loss: 1.116, Test accuracy: 69.24
Average accuracy final 10 rounds: 68.02324999999999
2750.597479581833
[3.1557068824768066, 6.0750932693481445, 8.996207237243652, 11.99158763885498, 15.036269664764404, 18.06874680519104, 21.08669424057007, 24.09367322921753, 27.08895492553711, 30.08780002593994, 33.08948636054993, 36.08073902130127, 39.07891511917114, 42.08962821960449, 45.09491729736328, 48.10346698760986, 51.11138105392456, 54.099764823913574, 57.09762144088745, 60.100292682647705, 63.098787784576416, 66.10015964508057, 69.11578464508057, 72.1210777759552, 75.1316819190979, 78.1347644329071, 81.1271755695343, 84.15519666671753, 87.19606852531433, 90.240731716156, 93.28538131713867, 96.43024849891663, 99.31505513191223, 102.14180326461792, 104.81158995628357, 107.47821617126465, 110.14270853996277, 112.81224322319031, 115.48661851882935, 118.15968775749207, 120.82984375953674, 123.50207495689392, 126.17742085456848, 128.86440443992615, 131.5370864868164, 134.2036736011505, 136.8791265487671, 139.54517769813538, 142.21532464027405, 144.88571667671204, 147.55779552459717, 150.22298741340637, 152.89497137069702, 155.56438183784485, 158.23599338531494, 160.92771172523499, 163.61914205551147, 166.3031723499298, 168.97677779197693, 171.65084743499756, 174.32348823547363, 176.99632787704468, 179.66754984855652, 182.3434247970581, 185.02039003372192, 187.70417308807373, 190.37341785430908, 193.06173491477966, 195.74580907821655, 198.4249665737152, 201.12700843811035, 203.82907390594482, 206.52038383483887, 209.21572160720825, 211.89382672309875, 214.57170343399048, 217.26639914512634, 219.96077799797058, 222.65288853645325, 225.3289339542389, 227.99943375587463, 230.6821734905243, 233.36728143692017, 236.0953347682953, 238.81860828399658, 241.57199597358704, 244.29305768013, 246.99744153022766, 249.68779373168945, 252.39989471435547, 255.11858439445496, 257.84334683418274, 260.55949997901917, 263.66851568222046, 266.7355635166168, 269.48909735679626, 272.2078559398651, 274.9168972969055, 277.6215670108795, 280.34079480171204, 283.0955419540405]
[25.265, 28.385, 34.94, 39.8775, 42.195, 45.1925, 46.8475, 48.065, 49.0725, 50.2425, 51.335, 52.81, 54.375, 55.6325, 55.29, 56.1825, 57.75, 57.5, 57.9025, 58.595, 59.055, 60.265, 60.3075, 60.5375, 59.885, 60.9325, 61.3175, 61.025, 61.0725, 61.9325, 61.91, 62.6275, 62.345, 62.5075, 63.665, 63.4825, 63.6425, 64.05, 64.8375, 64.4725, 64.8375, 64.485, 64.5025, 64.6375, 65.5, 64.9375, 64.99, 65.16, 64.8, 65.8325, 65.705, 66.175, 66.2, 65.8575, 65.96, 66.4725, 67.0125, 66.835, 65.9825, 66.1475, 66.205, 67.075, 67.37, 67.1725, 66.26, 66.0575, 66.2675, 66.4625, 67.45, 66.665, 66.89, 66.6425, 67.485, 67.31, 67.2775, 67.7625, 67.6675, 67.7275, 67.5275, 67.71, 67.4725, 67.2025, 67.7475, 67.84, 67.6025, 67.8625, 68.12, 68.7025, 68.1525, 68.23, 68.065, 67.7125, 67.905, 67.98, 68.1675, 67.7675, 67.885, 68.1375, 68.315, 68.2975, 69.2375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.306, Test accuracy: 10.31 

Round   0, Global train loss: 2.303, Global test loss: 2.306, Global test accuracy: 10.30 

Round   1, Train loss: 2.305, Test loss: 2.306, Test accuracy: 10.36 

Round   1, Global train loss: 2.305, Global test loss: 2.305, Global test accuracy: 10.33 

Round   2, Train loss: 2.304, Test loss: 2.305, Test accuracy: 10.29 

Round   2, Global train loss: 2.304, Global test loss: 2.305, Global test accuracy: 10.33 

Round   3, Train loss: 2.304, Test loss: 2.305, Test accuracy: 10.29 

Round   3, Global train loss: 2.304, Global test loss: 2.305, Global test accuracy: 10.15 

Round   4, Train loss: 2.302, Test loss: 2.305, Test accuracy: 10.28 

Round   4, Global train loss: 2.302, Global test loss: 2.304, Global test accuracy: 10.07 

Round   5, Train loss: 2.304, Test loss: 2.304, Test accuracy: 10.20 

Round   5, Global train loss: 2.304, Global test loss: 2.304, Global test accuracy: 10.02 

Round   6, Train loss: 2.301, Test loss: 2.304, Test accuracy: 10.13 

Round   6, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 10.03 

Round   7, Train loss: 2.304, Test loss: 2.304, Test accuracy: 10.08 

Round   7, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 10.03 

Round   8, Train loss: 2.300, Test loss: 2.303, Test accuracy: 10.06 

Round   8, Global train loss: 2.300, Global test loss: 2.303, Global test accuracy: 10.03 

Round   9, Train loss: 2.304, Test loss: 2.303, Test accuracy: 10.05 

Round   9, Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 10.02 

Round  10, Train loss: 2.300, Test loss: 2.302, Test accuracy: 10.06 

Round  10, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 10.03 

Round  11, Train loss: 2.300, Test loss: 2.302, Test accuracy: 10.07 

Round  11, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 10.02 

Round  12, Train loss: 2.300, Test loss: 2.301, Test accuracy: 10.08 

Round  12, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 10.06 

Round  13, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.08 

Round  13, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.10 

Round  14, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.09 

Round  14, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.03 

Round  15, Train loss: 2.300, Test loss: 2.300, Test accuracy: 10.08 

Round  15, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 10.03 

Round  16, Train loss: 2.298, Test loss: 2.300, Test accuracy: 10.09 

Round  16, Global train loss: 2.298, Global test loss: 2.299, Global test accuracy: 10.07 

Round  17, Train loss: 2.300, Test loss: 2.299, Test accuracy: 10.09 

Round  17, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 10.06 

Round  18, Train loss: 2.297, Test loss: 2.299, Test accuracy: 10.09 

Round  18, Global train loss: 2.297, Global test loss: 2.298, Global test accuracy: 10.08 

Round  19, Train loss: 2.299, Test loss: 2.299, Test accuracy: 10.15 

Round  19, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.23 

Round  20, Train loss: 2.298, Test loss: 2.298, Test accuracy: 10.17 

Round  20, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 10.16 

Round  21, Train loss: 2.297, Test loss: 2.297, Test accuracy: 10.25 

Round  21, Global train loss: 2.297, Global test loss: 2.297, Global test accuracy: 10.24 

Round  22, Train loss: 2.296, Test loss: 2.297, Test accuracy: 10.27 

Round  22, Global train loss: 2.296, Global test loss: 2.296, Global test accuracy: 10.11 

Round  23, Train loss: 2.298, Test loss: 2.297, Test accuracy: 10.33 

Round  23, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 10.38 

Round  24, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.42 

Round  24, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.67 

Round  25, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.55 

Round  25, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.74 

Round  26, Train loss: 2.295, Test loss: 2.295, Test accuracy: 10.66 

Round  26, Global train loss: 2.295, Global test loss: 2.294, Global test accuracy: 10.54 

Round  27, Train loss: 2.293, Test loss: 2.295, Test accuracy: 10.76 

Round  27, Global train loss: 2.293, Global test loss: 2.294, Global test accuracy: 10.75 

Round  28, Train loss: 2.294, Test loss: 2.295, Test accuracy: 10.96 

Round  28, Global train loss: 2.294, Global test loss: 2.293, Global test accuracy: 11.34 

Round  29, Train loss: 2.296, Test loss: 2.294, Test accuracy: 11.32 

Round  29, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 11.98 

Round  30, Train loss: 2.296, Test loss: 2.293, Test accuracy: 11.50 

Round  30, Global train loss: 2.296, Global test loss: 2.292, Global test accuracy: 11.72 

Round  31, Train loss: 2.293, Test loss: 2.293, Test accuracy: 11.68 

Round  31, Global train loss: 2.293, Global test loss: 2.292, Global test accuracy: 11.75 

Round  32, Train loss: 2.294, Test loss: 2.292, Test accuracy: 11.64 

Round  32, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 11.95 

Round  33, Train loss: 2.291, Test loss: 2.292, Test accuracy: 11.71 

Round  33, Global train loss: 2.291, Global test loss: 2.291, Global test accuracy: 12.07 

Round  34, Train loss: 2.291, Test loss: 2.291, Test accuracy: 11.74 

Round  34, Global train loss: 2.291, Global test loss: 2.290, Global test accuracy: 12.23 

Round  35, Train loss: 2.289, Test loss: 2.291, Test accuracy: 12.00 

Round  35, Global train loss: 2.289, Global test loss: 2.289, Global test accuracy: 12.38 

Round  36, Train loss: 2.290, Test loss: 2.290, Test accuracy: 12.14 

Round  36, Global train loss: 2.290, Global test loss: 2.289, Global test accuracy: 12.56 

Round  37, Train loss: 2.292, Test loss: 2.290, Test accuracy: 12.20 

Round  37, Global train loss: 2.292, Global test loss: 2.288, Global test accuracy: 13.06 

Round  38, Train loss: 2.290, Test loss: 2.289, Test accuracy: 12.45 

Round  38, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 12.73 

Round  39, Train loss: 2.290, Test loss: 2.289, Test accuracy: 12.55 

Round  39, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 12.85 

Round  40, Train loss: 2.288, Test loss: 2.288, Test accuracy: 12.61 

Round  40, Global train loss: 2.288, Global test loss: 2.286, Global test accuracy: 12.68 

Round  41, Train loss: 2.289, Test loss: 2.286, Test accuracy: 12.63 

Round  41, Global train loss: 2.289, Global test loss: 2.285, Global test accuracy: 12.77 

Round  42, Train loss: 2.286, Test loss: 2.286, Test accuracy: 12.59 

Round  42, Global train loss: 2.286, Global test loss: 2.284, Global test accuracy: 12.62 

Round  43, Train loss: 2.287, Test loss: 2.285, Test accuracy: 12.63 

Round  43, Global train loss: 2.287, Global test loss: 2.284, Global test accuracy: 12.90 

Round  44, Train loss: 2.286, Test loss: 2.284, Test accuracy: 12.71 

Round  44, Global train loss: 2.286, Global test loss: 2.283, Global test accuracy: 12.94 

Round  45, Train loss: 2.286, Test loss: 2.284, Test accuracy: 12.95 

Round  45, Global train loss: 2.286, Global test loss: 2.282, Global test accuracy: 13.10 

Round  46, Train loss: 2.286, Test loss: 2.283, Test accuracy: 12.79 

Round  46, Global train loss: 2.286, Global test loss: 2.281, Global test accuracy: 12.96 

Round  47, Train loss: 2.283, Test loss: 2.282, Test accuracy: 12.93 

Round  47, Global train loss: 2.283, Global test loss: 2.280, Global test accuracy: 13.12 

Round  48, Train loss: 2.283, Test loss: 2.281, Test accuracy: 13.02 

Round  48, Global train loss: 2.283, Global test loss: 2.279, Global test accuracy: 13.69 

Round  49, Train loss: 2.283, Test loss: 2.281, Test accuracy: 13.40 

Round  49, Global train loss: 2.283, Global test loss: 2.279, Global test accuracy: 14.32 

Round  50, Train loss: 2.282, Test loss: 2.280, Test accuracy: 13.61 

Round  50, Global train loss: 2.282, Global test loss: 2.278, Global test accuracy: 14.38 

Round  51, Train loss: 2.282, Test loss: 2.279, Test accuracy: 13.94 

Round  51, Global train loss: 2.282, Global test loss: 2.277, Global test accuracy: 14.55 

Round  52, Train loss: 2.282, Test loss: 2.279, Test accuracy: 14.25 

Round  52, Global train loss: 2.282, Global test loss: 2.277, Global test accuracy: 14.79 

Round  53, Train loss: 2.285, Test loss: 2.278, Test accuracy: 14.32 

Round  53, Global train loss: 2.285, Global test loss: 2.276, Global test accuracy: 14.72 

Round  54, Train loss: 2.282, Test loss: 2.277, Test accuracy: 14.55 

Round  54, Global train loss: 2.282, Global test loss: 2.275, Global test accuracy: 15.13 

Round  55, Train loss: 2.281, Test loss: 2.276, Test accuracy: 14.71 

Round  55, Global train loss: 2.281, Global test loss: 2.274, Global test accuracy: 15.02 

Round  56, Train loss: 2.278, Test loss: 2.276, Test accuracy: 14.98 

Round  56, Global train loss: 2.278, Global test loss: 2.274, Global test accuracy: 15.32 

Round  57, Train loss: 2.281, Test loss: 2.275, Test accuracy: 15.15 

Round  57, Global train loss: 2.281, Global test loss: 2.273, Global test accuracy: 15.25 

Round  58, Train loss: 2.277, Test loss: 2.274, Test accuracy: 15.10 

Round  58, Global train loss: 2.277, Global test loss: 2.272, Global test accuracy: 15.30 

Round  59, Train loss: 2.277, Test loss: 2.273, Test accuracy: 15.02 

Round  59, Global train loss: 2.277, Global test loss: 2.270, Global test accuracy: 15.35 

Round  60, Train loss: 2.276, Test loss: 2.272, Test accuracy: 15.06 

Round  60, Global train loss: 2.276, Global test loss: 2.270, Global test accuracy: 15.41 

Round  61, Train loss: 2.273, Test loss: 2.271, Test accuracy: 15.19 

Round  61, Global train loss: 2.273, Global test loss: 2.269, Global test accuracy: 15.14 

Round  62, Train loss: 2.275, Test loss: 2.270, Test accuracy: 15.30 

Round  62, Global train loss: 2.275, Global test loss: 2.267, Global test accuracy: 15.35 

Round  63, Train loss: 2.273, Test loss: 2.270, Test accuracy: 15.30 

Round  63, Global train loss: 2.273, Global test loss: 2.266, Global test accuracy: 14.98 

Round  64, Train loss: 2.274, Test loss: 2.268, Test accuracy: 15.28 

Round  64, Global train loss: 2.274, Global test loss: 2.266, Global test accuracy: 15.42 

Round  65, Train loss: 2.274, Test loss: 2.266, Test accuracy: 15.45 

Round  65, Global train loss: 2.274, Global test loss: 2.264, Global test accuracy: 15.63 

Round  66, Train loss: 2.273, Test loss: 2.266, Test accuracy: 15.51 

Round  66, Global train loss: 2.273, Global test loss: 2.263, Global test accuracy: 16.05 

Round  67, Train loss: 2.269, Test loss: 2.264, Test accuracy: 15.88 

Round  67, Global train loss: 2.269, Global test loss: 2.263, Global test accuracy: 16.43 

Round  68, Train loss: 2.268, Test loss: 2.263, Test accuracy: 16.07 

Round  68, Global train loss: 2.268, Global test loss: 2.261, Global test accuracy: 16.66 

Round  69, Train loss: 2.270, Test loss: 2.262, Test accuracy: 16.35 

Round  69, Global train loss: 2.270, Global test loss: 2.260, Global test accuracy: 16.76 

Round  70, Train loss: 2.266, Test loss: 2.261, Test accuracy: 16.55 

Round  70, Global train loss: 2.266, Global test loss: 2.259, Global test accuracy: 16.89 

Round  71, Train loss: 2.271, Test loss: 2.260, Test accuracy: 16.37 

Round  71, Global train loss: 2.271, Global test loss: 2.257, Global test accuracy: 16.55 

Round  72, Train loss: 2.269, Test loss: 2.259, Test accuracy: 16.38 

Round  72, Global train loss: 2.269, Global test loss: 2.255, Global test accuracy: 16.62 

Round  73, Train loss: 2.266, Test loss: 2.257, Test accuracy: 16.57 

Round  73, Global train loss: 2.266, Global test loss: 2.255, Global test accuracy: 16.75 

Round  74, Train loss: 2.267, Test loss: 2.256, Test accuracy: 16.54 

Round  74, Global train loss: 2.267, Global test loss: 2.253, Global test accuracy: 16.54 

Round  75, Train loss: 2.268, Test loss: 2.255, Test accuracy: 16.68 

Round  75, Global train loss: 2.268, Global test loss: 2.252, Global test accuracy: 17.03 

Round  76, Train loss: 2.265, Test loss: 2.254, Test accuracy: 16.99 

Round  76, Global train loss: 2.265, Global test loss: 2.251, Global test accuracy: 17.09 

Round  77, Train loss: 2.263, Test loss: 2.253, Test accuracy: 17.21 

Round  77, Global train loss: 2.263, Global test loss: 2.249, Global test accuracy: 17.58 

Round  78, Train loss: 2.267, Test loss: 2.251, Test accuracy: 17.26 

Round  78, Global train loss: 2.267, Global test loss: 2.247, Global test accuracy: 17.45 

Round  79, Train loss: 2.263, Test loss: 2.249, Test accuracy: 17.26 

Round  79, Global train loss: 2.263, Global test loss: 2.245, Global test accuracy: 17.21 

Round  80, Train loss: 2.264, Test loss: 2.248, Test accuracy: 17.18 

Round  80, Global train loss: 2.264, Global test loss: 2.244, Global test accuracy: 17.24 

Round  81, Train loss: 2.256, Test loss: 2.247, Test accuracy: 17.08 

Round  81, Global train loss: 2.256, Global test loss: 2.243, Global test accuracy: 17.05 

Round  82, Train loss: 2.268, Test loss: 2.245, Test accuracy: 16.93 

Round  82, Global train loss: 2.268, Global test loss: 2.242, Global test accuracy: 16.61 

Round  83, Train loss: 2.259, Test loss: 2.244, Test accuracy: 17.03 

Round  83, Global train loss: 2.259, Global test loss: 2.241, Global test accuracy: 17.50 

Round  84, Train loss: 2.260, Test loss: 2.243, Test accuracy: 17.10 

Round  84, Global train loss: 2.260, Global test loss: 2.239, Global test accuracy: 17.37 

Round  85, Train loss: 2.258, Test loss: 2.242, Test accuracy: 17.34 

Round  85, Global train loss: 2.258, Global test loss: 2.238, Global test accuracy: 17.57 

Round  86, Train loss: 2.257, Test loss: 2.240, Test accuracy: 17.29 

Round  86, Global train loss: 2.257, Global test loss: 2.237, Global test accuracy: 17.52 

Round  87, Train loss: 2.256, Test loss: 2.239, Test accuracy: 17.25 

Round  87, Global train loss: 2.256, Global test loss: 2.235, Global test accuracy: 17.38 

Round  88, Train loss: 2.259, Test loss: 2.237, Test accuracy: 17.31 

Round  88, Global train loss: 2.259, Global test loss: 2.233, Global test accuracy: 17.71 

Round  89, Train loss: 2.256, Test loss: 2.235, Test accuracy: 17.54 

Round  89, Global train loss: 2.256, Global test loss: 2.231, Global test accuracy: 17.95 

Round  90, Train loss: 2.252, Test loss: 2.234, Test accuracy: 17.88 

Round  90, Global train loss: 2.252, Global test loss: 2.229, Global test accuracy: 18.30 

Round  91, Train loss: 2.259, Test loss: 2.232, Test accuracy: 18.13 

Round  91, Global train loss: 2.259, Global test loss: 2.228, Global test accuracy: 18.36 

Round  92, Train loss: 2.244, Test loss: 2.231, Test accuracy: 18.34 

Round  92, Global train loss: 2.244, Global test loss: 2.226, Global test accuracy: 18.79 

Round  93, Train loss: 2.243, Test loss: 2.229, Test accuracy: 18.55 

Round  93, Global train loss: 2.243, Global test loss: 2.224, Global test accuracy: 18.82 

Round  94, Train loss: 2.252, Test loss: 2.227, Test accuracy: 18.56 

Round  94, Global train loss: 2.252, Global test loss: 2.221, Global test accuracy: 18.77 

Round  95, Train loss: 2.254, Test loss: 2.225, Test accuracy: 18.43 

Round  95, Global train loss: 2.254, Global test loss: 2.221, Global test accuracy: 18.16 

Round  96, Train loss: 2.247, Test loss: 2.224, Test accuracy: 18.48 

Round  96, Global train loss: 2.247, Global test loss: 2.221, Global test accuracy: 18.38 

Round  97, Train loss: 2.248, Test loss: 2.223, Test accuracy: 18.54 

Round  97, Global train loss: 2.248, Global test loss: 2.220, Global test accuracy: 18.86 

Round  98, Train loss: 2.244, Test loss: 2.222, Test accuracy: 18.71 

Round  98, Global train loss: 2.244, Global test loss: 2.219, Global test accuracy: 19.25 

Round  99, Train loss: 2.239, Test loss: 2.221, Test accuracy: 18.87 

Round  99, Global train loss: 2.239, Global test loss: 2.217, Global test accuracy: 19.21 

Round 100, Train loss: 2.238, Test loss: 2.220, Test accuracy: 18.93 

Round 100, Global train loss: 2.238, Global test loss: 2.217, Global test accuracy: 19.16 

Round 101, Train loss: 2.240, Test loss: 2.219, Test accuracy: 19.18 

Round 101, Global train loss: 2.240, Global test loss: 2.218, Global test accuracy: 19.44 

Round 102, Train loss: 2.241, Test loss: 2.218, Test accuracy: 19.26 

Round 102, Global train loss: 2.241, Global test loss: 2.216, Global test accuracy: 19.39 

Round 103, Train loss: 2.243, Test loss: 2.217, Test accuracy: 19.14 

Round 103, Global train loss: 2.243, Global test loss: 2.213, Global test accuracy: 18.89 

Round 104, Train loss: 2.239, Test loss: 2.216, Test accuracy: 19.17 

Round 104, Global train loss: 2.239, Global test loss: 2.214, Global test accuracy: 19.54 

Round 105, Train loss: 2.237, Test loss: 2.215, Test accuracy: 19.03 

Round 105, Global train loss: 2.237, Global test loss: 2.211, Global test accuracy: 19.36 

Round 106, Train loss: 2.231, Test loss: 2.214, Test accuracy: 19.34 

Round 106, Global train loss: 2.231, Global test loss: 2.210, Global test accuracy: 19.38 

Round 107, Train loss: 2.231, Test loss: 2.213, Test accuracy: 19.04 

Round 107, Global train loss: 2.231, Global test loss: 2.209, Global test accuracy: 18.60 

Round 108, Train loss: 2.237, Test loss: 2.212, Test accuracy: 18.93 

Round 108, Global train loss: 2.237, Global test loss: 2.206, Global test accuracy: 18.45 

Round 109, Train loss: 2.234, Test loss: 2.210, Test accuracy: 18.86 

Round 109, Global train loss: 2.234, Global test loss: 2.204, Global test accuracy: 18.91 

Round 110, Train loss: 2.232, Test loss: 2.208, Test accuracy: 19.21 

Round 110, Global train loss: 2.232, Global test loss: 2.202, Global test accuracy: 19.09 

Round 111, Train loss: 2.236, Test loss: 2.206, Test accuracy: 19.29 

Round 111, Global train loss: 2.236, Global test loss: 2.201, Global test accuracy: 19.42 

Round 112, Train loss: 2.232, Test loss: 2.204, Test accuracy: 19.17 

Round 112, Global train loss: 2.232, Global test loss: 2.200, Global test accuracy: 19.91 

Round 113, Train loss: 2.226, Test loss: 2.203, Test accuracy: 19.49 

Round 113, Global train loss: 2.226, Global test loss: 2.197, Global test accuracy: 20.30 

Round 114, Train loss: 2.223, Test loss: 2.200, Test accuracy: 20.00 

Round 114, Global train loss: 2.223, Global test loss: 2.194, Global test accuracy: 21.55 

Round 115, Train loss: 2.229, Test loss: 2.198, Test accuracy: 20.38 

Round 115, Global train loss: 2.229, Global test loss: 2.193, Global test accuracy: 20.86 

Round 116, Train loss: 2.228, Test loss: 2.197, Test accuracy: 20.66 

Round 116, Global train loss: 2.228, Global test loss: 2.193, Global test accuracy: 21.14 

Round 117, Train loss: 2.225, Test loss: 2.196, Test accuracy: 20.77 

Round 117, Global train loss: 2.225, Global test loss: 2.191, Global test accuracy: 20.77 

Round 118, Train loss: 2.223, Test loss: 2.195, Test accuracy: 21.21 

Round 118, Global train loss: 2.223, Global test loss: 2.190, Global test accuracy: 20.72 

Round 119, Train loss: 2.215, Test loss: 2.193, Test accuracy: 21.20 

Round 119, Global train loss: 2.215, Global test loss: 2.190, Global test accuracy: 20.40 

Round 120, Train loss: 2.228, Test loss: 2.193, Test accuracy: 21.02 

Round 120, Global train loss: 2.228, Global test loss: 2.189, Global test accuracy: 21.23 

Round 121, Train loss: 2.224, Test loss: 2.192, Test accuracy: 21.17 

Round 121, Global train loss: 2.224, Global test loss: 2.188, Global test accuracy: 21.60 

Round 122, Train loss: 2.224, Test loss: 2.190, Test accuracy: 21.18 

Round 122, Global train loss: 2.224, Global test loss: 2.185, Global test accuracy: 21.32 

Round 123, Train loss: 2.232, Test loss: 2.188, Test accuracy: 21.41 

Round 123, Global train loss: 2.232, Global test loss: 2.183, Global test accuracy: 21.57 

Round 124, Train loss: 2.223, Test loss: 2.186, Test accuracy: 21.30 

Round 124, Global train loss: 2.223, Global test loss: 2.181, Global test accuracy: 21.08 

Round 125, Train loss: 2.234, Test loss: 2.185, Test accuracy: 21.12 

Round 125, Global train loss: 2.234, Global test loss: 2.181, Global test accuracy: 20.45 

Round 126, Train loss: 2.231, Test loss: 2.183, Test accuracy: 21.09 

Round 126, Global train loss: 2.231, Global test loss: 2.181, Global test accuracy: 21.22 

Round 127, Train loss: 2.218, Test loss: 2.184, Test accuracy: 20.91 

Round 127, Global train loss: 2.218, Global test loss: 2.181, Global test accuracy: 20.77 

Round 128, Train loss: 2.223, Test loss: 2.183, Test accuracy: 20.95 

Round 128, Global train loss: 2.223, Global test loss: 2.181, Global test accuracy: 21.89 

Round 129, Train loss: 2.213, Test loss: 2.182, Test accuracy: 21.39 

Round 129, Global train loss: 2.213, Global test loss: 2.180, Global test accuracy: 22.38 

Round 130, Train loss: 2.208, Test loss: 2.183, Test accuracy: 21.79 

Round 130, Global train loss: 2.208, Global test loss: 2.181, Global test accuracy: 23.12 

Round 131, Train loss: 2.210, Test loss: 2.182, Test accuracy: 22.23 

Round 131, Global train loss: 2.210, Global test loss: 2.177, Global test accuracy: 23.11 

Round 132, Train loss: 2.208, Test loss: 2.180, Test accuracy: 22.23 

Round 132, Global train loss: 2.208, Global test loss: 2.175, Global test accuracy: 23.19 

Round 133, Train loss: 2.219, Test loss: 2.178, Test accuracy: 22.04 

Round 133, Global train loss: 2.219, Global test loss: 2.172, Global test accuracy: 22.33 

Round 134, Train loss: 2.213, Test loss: 2.177, Test accuracy: 22.09 

Round 134, Global train loss: 2.213, Global test loss: 2.172, Global test accuracy: 22.20 

Round 135, Train loss: 2.223, Test loss: 2.176, Test accuracy: 21.78 

Round 135, Global train loss: 2.223, Global test loss: 2.171, Global test accuracy: 22.23 

Round 136, Train loss: 2.215, Test loss: 2.173, Test accuracy: 22.07 

Round 136, Global train loss: 2.215, Global test loss: 2.171, Global test accuracy: 22.15 

Round 137, Train loss: 2.212, Test loss: 2.173, Test accuracy: 22.11 

Round 137, Global train loss: 2.212, Global test loss: 2.171, Global test accuracy: 21.60 

Round 138, Train loss: 2.209, Test loss: 2.172, Test accuracy: 22.34 

Round 138, Global train loss: 2.209, Global test loss: 2.171, Global test accuracy: 23.54 

Round 139, Train loss: 2.208, Test loss: 2.172, Test accuracy: 22.83 

Round 139, Global train loss: 2.208, Global test loss: 2.169, Global test accuracy: 24.27 

Round 140, Train loss: 2.208, Test loss: 2.171, Test accuracy: 22.99 

Round 140, Global train loss: 2.208, Global test loss: 2.168, Global test accuracy: 24.04 

Round 141, Train loss: 2.204, Test loss: 2.169, Test accuracy: 22.98 

Round 141, Global train loss: 2.204, Global test loss: 2.165, Global test accuracy: 24.15 

Round 142, Train loss: 2.205, Test loss: 2.168, Test accuracy: 23.02 

Round 142, Global train loss: 2.205, Global test loss: 2.165, Global test accuracy: 24.29 

Round 143, Train loss: 2.203, Test loss: 2.167, Test accuracy: 23.20 

Round 143, Global train loss: 2.203, Global test loss: 2.163, Global test accuracy: 23.83 

Round 144, Train loss: 2.206, Test loss: 2.165, Test accuracy: 23.22 

Round 144, Global train loss: 2.206, Global test loss: 2.161, Global test accuracy: 23.53 

Round 145, Train loss: 2.207, Test loss: 2.164, Test accuracy: 22.96 

Round 145, Global train loss: 2.207, Global test loss: 2.161, Global test accuracy: 22.95 

Round 146, Train loss: 2.202, Test loss: 2.164, Test accuracy: 22.64 

Round 146, Global train loss: 2.202, Global test loss: 2.160, Global test accuracy: 21.99 

Round 147, Train loss: 2.211, Test loss: 2.164, Test accuracy: 22.57 

Round 147, Global train loss: 2.211, Global test loss: 2.161, Global test accuracy: 21.51 

Round 148, Train loss: 2.204, Test loss: 2.164, Test accuracy: 22.48 

Round 148, Global train loss: 2.204, Global test loss: 2.161, Global test accuracy: 22.23 

Round 149, Train loss: 2.192, Test loss: 2.163, Test accuracy: 22.27 

Round 149, Global train loss: 2.192, Global test loss: 2.160, Global test accuracy: 22.02 

Round 150, Train loss: 2.197, Test loss: 2.161, Test accuracy: 22.11 

Round 150, Global train loss: 2.197, Global test loss: 2.159, Global test accuracy: 22.97 

Round 151, Train loss: 2.202, Test loss: 2.160, Test accuracy: 22.00 

Round 151, Global train loss: 2.202, Global test loss: 2.155, Global test accuracy: 21.98 

Round 152, Train loss: 2.201, Test loss: 2.159, Test accuracy: 21.82 

Round 152, Global train loss: 2.201, Global test loss: 2.154, Global test accuracy: 22.57 

Round 153, Train loss: 2.198, Test loss: 2.158, Test accuracy: 22.14 

Round 153, Global train loss: 2.198, Global test loss: 2.153, Global test accuracy: 23.15 

Round 154, Train loss: 2.195, Test loss: 2.156, Test accuracy: 22.53 

Round 154, Global train loss: 2.195, Global test loss: 2.152, Global test accuracy: 23.43 

Round 155, Train loss: 2.192, Test loss: 2.155, Test accuracy: 22.58 

Round 155, Global train loss: 2.192, Global test loss: 2.153, Global test accuracy: 23.28 

Round 156, Train loss: 2.196, Test loss: 2.155, Test accuracy: 22.66 

Round 156, Global train loss: 2.196, Global test loss: 2.153, Global test accuracy: 23.60 

Round 157, Train loss: 2.189, Test loss: 2.156, Test accuracy: 22.84 

Round 157, Global train loss: 2.189, Global test loss: 2.154, Global test accuracy: 23.88 

Round 158, Train loss: 2.195, Test loss: 2.154, Test accuracy: 23.10 

Round 158, Global train loss: 2.195, Global test loss: 2.152, Global test accuracy: 23.90 

Round 159, Train loss: 2.198, Test loss: 2.153, Test accuracy: 23.32 

Round 159, Global train loss: 2.198, Global test loss: 2.152, Global test accuracy: 23.57 

Round 160, Train loss: 2.195, Test loss: 2.153, Test accuracy: 23.45 

Round 160, Global train loss: 2.195, Global test loss: 2.152, Global test accuracy: 23.66 

Round 161, Train loss: 2.193, Test loss: 2.154, Test accuracy: 23.40 

Round 161, Global train loss: 2.193, Global test loss: 2.152, Global test accuracy: 23.61 

Round 162, Train loss: 2.198, Test loss: 2.152, Test accuracy: 23.16 

Round 162, Global train loss: 2.198, Global test loss: 2.150, Global test accuracy: 22.94 

Round 163, Train loss: 2.190, Test loss: 2.151, Test accuracy: 23.06 

Round 163, Global train loss: 2.190, Global test loss: 2.147, Global test accuracy: 22.71 

Round 164, Train loss: 2.195, Test loss: 2.151, Test accuracy: 22.83 

Round 164, Global train loss: 2.195, Global test loss: 2.147, Global test accuracy: 22.86 

Round 165, Train loss: 2.191, Test loss: 2.149, Test accuracy: 22.53 

Round 165, Global train loss: 2.191, Global test loss: 2.144, Global test accuracy: 22.68 

Round 166, Train loss: 2.201, Test loss: 2.148, Test accuracy: 22.52 

Round 166, Global train loss: 2.201, Global test loss: 2.142, Global test accuracy: 22.59 

Round 167, Train loss: 2.201, Test loss: 2.147, Test accuracy: 22.61 

Round 167, Global train loss: 2.201, Global test loss: 2.141, Global test accuracy: 22.04 

Round 168, Train loss: 2.187, Test loss: 2.145, Test accuracy: 22.54 

Round 168, Global train loss: 2.187, Global test loss: 2.140, Global test accuracy: 22.19 

Round 169, Train loss: 2.200, Test loss: 2.144, Test accuracy: 22.43 

Round 169, Global train loss: 2.200, Global test loss: 2.139, Global test accuracy: 22.42 

Round 170, Train loss: nan, Test loss: nan, Test accuracy: 21.85 

Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 171, Train loss: nan, Test loss: nan, Test accuracy: 18.96 

Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 172, Train loss: nan, Test loss: nan, Test accuracy: 15.33 

Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 173, Train loss: nan, Test loss: nan, Test accuracy: 13.94 

Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 174, Train loss: nan, Test loss: nan, Test accuracy: 12.59 

Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 175, Train loss: nan, Test loss: nan, Test accuracy: 11.83 

Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 176, Train loss: nan, Test loss: nan, Test accuracy: 11.22 

Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 177, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 178, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 179, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 180, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 181, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 182, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 183, Train loss: nan, Test loss: nan, Test accuracy: 10.57 

Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 184, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 185, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 186, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 187, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 188, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 189, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 190, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 191, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 192, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 193, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 194, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 195, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 196, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 197, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 198, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 199, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 200, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 201, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 202, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 203, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 204, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 205, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 206, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 207, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 208, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 209, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 210, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 211, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 212, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 213, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 214, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 215, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 216, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 217, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 218, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 219, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 220, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 221, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 222, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 223, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 224, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 225, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 226, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 227, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 228, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 229, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 230, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 231, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 232, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 233, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 234, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 235, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 236, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 237, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 238, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 239, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 240, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 241, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 242, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 243, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 244, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 245, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 246, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 247, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 248, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 249, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 250, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 251, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 252, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 253, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 254, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 255, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 256, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 257, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 258, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 259, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 260, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 261, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 262, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 263, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 264, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 265, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 266, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 267, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 268, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 269, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 270, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 271, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 272, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 273, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 274, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 275, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 276, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 277, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 278, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 279, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 280, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 281, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 282, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 283, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 284, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 285, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 286, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 287, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 288, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 289, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 290, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 291, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 292, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 293, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 294, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 295, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 296, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 297, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 298, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 299, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

8387.27372455597
[1.5799472332000732, 2.9590110778808594, 4.329233884811401, 5.702244997024536, 7.095094680786133, 8.191386699676514, 9.30330204963684, 10.406426906585693, 11.50325608253479, 12.60305643081665, 13.707261085510254, 14.809475898742676, 16.07423162460327, 17.175675868988037, 18.281556129455566, 19.387863397598267, 20.48917055130005, 21.593072414398193, 22.86562967300415, 24.13024878501892, 25.274988174438477, 26.385865926742554, 27.482696533203125, 28.581605195999146, 29.883564710617065, 31.144898414611816, 32.241501569747925, 33.33795666694641, 34.43055868148804, 35.704681634902954, 36.81843423843384, 37.927507638931274, 39.12134790420532, 40.21552348136902, 41.30908417701721, 42.40582084655762, 43.65571403503418, 44.752843379974365, 45.8497896194458, 46.93910217285156, 48.03821301460266, 49.208263874053955, 50.30362272262573, 51.406296491622925, 52.504862546920776, 53.600750207901, 54.69611859321594, 55.796550273895264, 56.891979694366455, 58.019880294799805, 59.10879898071289, 60.209309577941895, 61.30726671218872, 62.40413498878479, 63.4994957447052, 64.7504472732544, 65.99630761146545, 67.25337076187134, 68.35615491867065, 69.61469888687134, 70.94888663291931, 72.19984221458435, 73.3113842010498, 74.42558789253235, 75.53015899658203, 76.63790345191956, 77.74983382225037, 78.84240651130676, 79.96050071716309, 81.06948900222778, 82.17128992080688, 83.279470205307, 84.53858923912048, 85.80558633804321, 86.9125828742981, 88.01049852371216, 89.29559683799744, 90.39477467536926, 91.48893141746521, 92.59484100341797, 93.70684719085693, 94.81162428855896, 95.93667602539062, 97.38315892219543, 98.53036975860596, 99.7233452796936, 100.8852117061615, 102.00164866447449, 103.11555671691895, 104.22843194007874, 105.33825516700745, 106.48626470565796, 107.74868559837341, 108.85401439666748, 109.9505455493927, 111.0504412651062, 112.14413952827454, 113.23751068115234, 114.33574604988098, 115.44058227539062, 116.55373477935791, 117.6784737110138, 118.7896237373352, 119.89177799224854, 120.99957084655762, 122.10592842102051, 123.21209144592285, 124.35245370864868, 125.46929001808167, 126.6878821849823, 127.95127940177917, 129.05915331840515, 130.3019995689392, 131.49147081375122, 132.61155104637146, 133.7974829673767, 135.14116978645325, 136.33208537101746, 137.53942275047302, 138.71384692192078, 139.90708422660828, 141.02061867713928, 142.27091789245605, 143.555645942688, 144.81692028045654, 145.9783480167389, 147.18527054786682, 148.63617420196533, 150.10281991958618, 151.36549139022827, 152.60983419418335, 153.82149648666382, 155.10188674926758, 156.38360977172852, 157.62005281448364, 158.87263321876526, 160.14959454536438, 161.372873544693, 162.58692932128906, 163.7781937122345, 165.0412402153015, 166.24933767318726, 167.48931169509888, 168.70057606697083, 169.88528561592102, 171.1505320072174, 172.35956120491028, 173.5963830947876, 174.84059596061707, 176.0498025417328, 177.2678689956665, 178.5103771686554, 179.76121282577515, 180.9481074810028, 182.17694234848022, 183.40375781059265, 184.657958984375, 185.9571976661682, 187.17194533348083, 188.45281291007996, 189.6643214225769, 190.91814470291138, 192.2047266960144, 193.5131094455719, 194.76550197601318, 196.06972241401672, 197.2650249004364, 198.4560465812683, 199.7106056213379, 200.95550990104675, 202.1896858215332, 203.3726727962494, 204.62791633605957, 205.88404059410095, 207.14396691322327, 208.40300726890564, 209.66462421417236, 210.88524556159973, 212.143150806427, 213.41259121894836, 214.6151442527771, 215.8622441291809, 217.1504909992218, 218.46747756004333, 219.728040933609, 220.97774195671082, 222.1911060810089, 223.6266360282898, 224.89619302749634, 226.13210153579712, 227.343425989151, 228.53803610801697, 229.82408833503723, 231.16935777664185, 232.40003538131714, 233.7292778491974, 235.02152562141418, 236.22997999191284, 237.52586889266968, 238.79359197616577, 240.04411005973816, 241.28820776939392, 242.51823449134827, 243.78212308883667, 244.98974871635437, 246.18818998336792, 247.3872845172882, 248.59510469436646, 249.7993562221527, 250.99711990356445, 252.2030701637268, 253.3963098526001, 254.5991826057434, 255.79814314842224, 256.9947111606598, 258.18844723701477, 259.48611879348755, 260.74522829055786, 262.0663561820984, 263.321480512619, 264.5747287273407, 265.84517884254456, 267.1193006038666, 268.39543771743774, 269.6754415035248, 270.9434027671814, 272.1414511203766, 273.34518361091614, 274.566561460495, 275.8262302875519, 277.03201389312744, 278.2306697368622, 279.42876982688904, 280.62831926345825, 281.8232841491699, 283.0157153606415, 284.2189588546753, 285.41095495224, 286.6011600494385, 287.80301094055176, 289.0066239833832, 290.1992027759552, 291.41312646865845, 292.6084041595459, 293.8050026893616, 295.0056254863739, 296.2037980556488, 297.40446519851685, 298.6037266254425, 299.86593532562256, 301.1489782333374, 302.39696168899536, 303.63544631004333, 304.8790349960327, 306.1546194553375, 307.39207220077515, 308.62908959388733, 309.88430738449097, 311.12782430648804, 312.38737988471985, 313.58693528175354, 314.778902053833, 315.96882581710815, 317.16157817840576, 318.3641984462738, 319.5651364326477, 320.76385259628296, 322.02557921409607, 323.2284743785858, 324.41429328918457, 325.6084418296814, 326.8076813220978, 327.99371695518494, 329.1818583011627, 330.37515115737915, 331.56720638275146, 332.7613983154297, 333.96247267723083, 335.17190504074097, 336.37161922454834, 337.5724356174469, 338.7684600353241, 339.9443757534027, 341.1297507286072, 342.338502407074, 343.42562222480774, 344.52130341529846, 345.77685546875, 347.001825094223, 348.19354796409607, 349.4117600917816, 350.6605443954468, 351.9149396419525, 353.0375919342041, 354.1265916824341, 355.2125005722046, 356.3266348838806, 357.4161286354065, 358.51169180870056, 359.602285861969, 361.8124873638153]
[10.31, 10.3625, 10.2875, 10.285, 10.2825, 10.2, 10.13, 10.0825, 10.0575, 10.05, 10.06, 10.075, 10.08, 10.08, 10.0925, 10.0775, 10.09, 10.0875, 10.0925, 10.1525, 10.17, 10.2525, 10.2725, 10.3275, 10.4175, 10.5525, 10.6575, 10.76, 10.955, 11.3225, 11.4975, 11.6825, 11.6425, 11.715, 11.7375, 12.0025, 12.145, 12.1975, 12.445, 12.55, 12.605, 12.6275, 12.585, 12.63, 12.705, 12.9475, 12.79, 12.93, 13.0175, 13.3975, 13.61, 13.9425, 14.2475, 14.325, 14.555, 14.705, 14.98, 15.155, 15.1, 15.0225, 15.0625, 15.185, 15.3, 15.3, 15.275, 15.4525, 15.5125, 15.8775, 16.07, 16.3525, 16.545, 16.3675, 16.38, 16.57, 16.5375, 16.675, 16.9875, 17.2125, 17.26, 17.26, 17.185, 17.0775, 16.9275, 17.0325, 17.1025, 17.335, 17.29, 17.2475, 17.31, 17.5425, 17.8775, 18.1325, 18.34, 18.55, 18.56, 18.4325, 18.48, 18.535, 18.715, 18.8675, 18.925, 19.1775, 19.2625, 19.1375, 19.1675, 19.0275, 19.3425, 19.0425, 18.9325, 18.8625, 19.21, 19.2875, 19.17, 19.4875, 19.995, 20.38, 20.66, 20.77, 21.2075, 21.205, 21.025, 21.17, 21.18, 21.41, 21.3, 21.12, 21.0875, 20.9075, 20.95, 21.395, 21.7875, 22.2325, 22.225, 22.0425, 22.095, 21.7825, 22.07, 22.1125, 22.34, 22.8275, 22.9875, 22.9775, 23.0225, 23.2, 23.2175, 22.965, 22.64, 22.575, 22.4825, 22.275, 22.105, 22.005, 21.8175, 22.1425, 22.5275, 22.5775, 22.66, 22.845, 23.1025, 23.3225, 23.445, 23.4025, 23.155, 23.0575, 22.8325, 22.5325, 22.5225, 22.605, 22.535, 22.435, 21.8475, 18.9575, 15.33, 13.9425, 12.59, 11.8325, 11.22, 10.5675, 10.5675, 10.5675, 10.5675, 10.5675, 10.5675, 10.5675, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.226, Test loss: 2.116, Test accuracy: 22.02 

Round   0, Global train loss: 2.226, Global test loss: 2.121, Global test accuracy: 22.45 

Round   1, Train loss: 2.042, Test loss: 2.003, Test accuracy: 25.61 

Round   1, Global train loss: 2.042, Global test loss: 1.953, Global test accuracy: 28.14 

Round   2, Train loss: 1.919, Test loss: 1.925, Test accuracy: 27.69 

Round   2, Global train loss: 1.919, Global test loss: 1.841, Global test accuracy: 30.87 

Round   3, Train loss: 1.805, Test loss: 1.873, Test accuracy: 29.84 

Round   3, Global train loss: 1.805, Global test loss: 1.738, Global test accuracy: 33.76 

Round   4, Train loss: 1.758, Test loss: 1.796, Test accuracy: 33.03 

Round   4, Global train loss: 1.758, Global test loss: 1.665, Global test accuracy: 38.03 

Round   5, Train loss: 1.669, Test loss: 1.718, Test accuracy: 36.33 

Round   5, Global train loss: 1.669, Global test loss: 1.608, Global test accuracy: 39.95 

Round   6, Train loss: 1.630, Test loss: 1.706, Test accuracy: 36.89 

Round   6, Global train loss: 1.630, Global test loss: 1.586, Global test accuracy: 40.62 

Round   7, Train loss: 1.561, Test loss: 1.671, Test accuracy: 38.09 

Round   7, Global train loss: 1.561, Global test loss: 1.527, Global test accuracy: 42.70 

Round   8, Train loss: 1.509, Test loss: 1.637, Test accuracy: 40.10 

Round   8, Global train loss: 1.509, Global test loss: 1.506, Global test accuracy: 43.42 

Round   9, Train loss: 1.481, Test loss: 1.595, Test accuracy: 41.56 

Round   9, Global train loss: 1.481, Global test loss: 1.480, Global test accuracy: 45.17 

Round  10, Train loss: 1.457, Test loss: 1.580, Test accuracy: 42.35 

Round  10, Global train loss: 1.457, Global test loss: 1.468, Global test accuracy: 46.60 

Round  11, Train loss: 1.400, Test loss: 1.569, Test accuracy: 42.90 

Round  11, Global train loss: 1.400, Global test loss: 1.477, Global test accuracy: 47.57 

Round  12, Train loss: 1.407, Test loss: 1.552, Test accuracy: 43.70 

Round  12, Global train loss: 1.407, Global test loss: 1.427, Global test accuracy: 47.28 

Round  13, Train loss: 1.360, Test loss: 1.519, Test accuracy: 45.25 

Round  13, Global train loss: 1.360, Global test loss: 1.404, Global test accuracy: 48.82 

Round  14, Train loss: 1.311, Test loss: 1.499, Test accuracy: 45.76 

Round  14, Global train loss: 1.311, Global test loss: 1.513, Global test accuracy: 48.18 

Round  15, Train loss: 1.286, Test loss: 1.491, Test accuracy: 46.28 

Round  15, Global train loss: 1.286, Global test loss: 1.341, Global test accuracy: 49.89 

Round  16, Train loss: 1.266, Test loss: 1.486, Test accuracy: 46.80 

Round  16, Global train loss: 1.266, Global test loss: 1.352, Global test accuracy: 50.11 

Round  17, Train loss: 1.270, Test loss: 1.453, Test accuracy: 48.72 

Round  17, Global train loss: 1.270, Global test loss: 1.343, Global test accuracy: 51.16 

Round  18, Train loss: 1.205, Test loss: 1.447, Test accuracy: 49.46 

Round  18, Global train loss: 1.205, Global test loss: 1.320, Global test accuracy: 51.28 

Round  19, Train loss: 1.170, Test loss: 1.450, Test accuracy: 49.88 

Round  19, Global train loss: 1.170, Global test loss: 1.317, Global test accuracy: 51.84 

Round  20, Train loss: 1.180, Test loss: 1.430, Test accuracy: 50.74 

Round  20, Global train loss: 1.180, Global test loss: 1.283, Global test accuracy: 52.51 

Round  21, Train loss: 1.154, Test loss: 1.424, Test accuracy: 51.36 

Round  21, Global train loss: 1.154, Global test loss: 1.358, Global test accuracy: 51.54 

Round  22, Train loss: 1.118, Test loss: 1.409, Test accuracy: 52.02 

Round  22, Global train loss: 1.118, Global test loss: 1.330, Global test accuracy: 53.27 

Round  23, Train loss: 1.104, Test loss: 1.410, Test accuracy: 52.23 

Round  23, Global train loss: 1.104, Global test loss: 1.246, Global test accuracy: 54.42 

Round  24, Train loss: 1.097, Test loss: 1.389, Test accuracy: 52.80 

Round  24, Global train loss: 1.097, Global test loss: 1.263, Global test accuracy: 54.50 

Round  25, Train loss: 1.048, Test loss: 1.383, Test accuracy: 53.22 

Round  25, Global train loss: 1.048, Global test loss: 1.231, Global test accuracy: 54.47 

Round  26, Train loss: 1.014, Test loss: 1.375, Test accuracy: 53.56 

Round  26, Global train loss: 1.014, Global test loss: 1.259, Global test accuracy: 55.58 

Round  27, Train loss: 1.046, Test loss: 1.364, Test accuracy: 53.90 

Round  27, Global train loss: 1.046, Global test loss: 1.281, Global test accuracy: 55.68 

Round  28, Train loss: 1.028, Test loss: 1.366, Test accuracy: 54.02 

Round  28, Global train loss: 1.028, Global test loss: 1.246, Global test accuracy: 56.21 

Round  29, Train loss: 1.000, Test loss: 1.373, Test accuracy: 54.16 

Round  29, Global train loss: 1.000, Global test loss: 1.240, Global test accuracy: 56.24 

Round  30, Train loss: 0.989, Test loss: 1.386, Test accuracy: 54.30 

Round  30, Global train loss: 0.989, Global test loss: 1.319, Global test accuracy: 56.79 

Round  31, Train loss: 0.957, Test loss: 1.387, Test accuracy: 54.65 

Round  31, Global train loss: 0.957, Global test loss: 1.246, Global test accuracy: 56.15 

Round  32, Train loss: 0.938, Test loss: 1.368, Test accuracy: 55.31 

Round  32, Global train loss: 0.938, Global test loss: 1.228, Global test accuracy: 56.00 

Round  33, Train loss: 0.922, Test loss: 1.373, Test accuracy: 55.12 

Round  33, Global train loss: 0.922, Global test loss: 1.227, Global test accuracy: 57.14 

Round  34, Train loss: 0.907, Test loss: 1.362, Test accuracy: 55.30 

Round  34, Global train loss: 0.907, Global test loss: 1.199, Global test accuracy: 57.00 

Round  35, Train loss: 0.896, Test loss: 1.350, Test accuracy: 55.85 

Round  35, Global train loss: 0.896, Global test loss: 1.304, Global test accuracy: 57.96 

Round  36, Train loss: 0.870, Test loss: 1.349, Test accuracy: 56.05 

Round  36, Global train loss: 0.870, Global test loss: 1.316, Global test accuracy: 58.07 

Round  37, Train loss: 0.849, Test loss: 1.348, Test accuracy: 56.35 

Round  37, Global train loss: 0.849, Global test loss: 1.205, Global test accuracy: 57.35 

Round  38, Train loss: 0.836, Test loss: 1.366, Test accuracy: 56.33 

Round  38, Global train loss: 0.836, Global test loss: 1.249, Global test accuracy: 57.62 

Round  39, Train loss: 0.841, Test loss: 1.366, Test accuracy: 56.45 

Round  39, Global train loss: 0.841, Global test loss: 1.221, Global test accuracy: 57.40 

Round  40, Train loss: 0.830, Test loss: 1.381, Test accuracy: 56.22 

Round  40, Global train loss: 0.830, Global test loss: 1.271, Global test accuracy: 57.33 

Round  41, Train loss: 0.787, Test loss: 1.383, Test accuracy: 56.36 

Round  41, Global train loss: 0.787, Global test loss: 1.225, Global test accuracy: 57.67 

Round  42, Train loss: 0.781, Test loss: 1.377, Test accuracy: 56.74 

Round  42, Global train loss: 0.781, Global test loss: 1.237, Global test accuracy: 58.91 

Round  43, Train loss: 0.821, Test loss: 1.371, Test accuracy: 57.16 

Round  43, Global train loss: 0.821, Global test loss: 1.236, Global test accuracy: 58.34 

Round  44, Train loss: 0.743, Test loss: 1.387, Test accuracy: 56.90 

Round  44, Global train loss: 0.743, Global test loss: 1.241, Global test accuracy: 59.51 

Round  45, Train loss: 0.747, Test loss: 1.386, Test accuracy: 57.18 

Round  45, Global train loss: 0.747, Global test loss: 1.206, Global test accuracy: 58.56 

Round  46, Train loss: 0.750, Test loss: 1.369, Test accuracy: 57.97 

Round  46, Global train loss: 0.750, Global test loss: 1.244, Global test accuracy: 59.29 

Round  47, Train loss: 0.771, Test loss: 1.378, Test accuracy: 58.23 

Round  47, Global train loss: 0.771, Global test loss: 1.197, Global test accuracy: 58.66 

Round  48, Train loss: 0.721, Test loss: 1.380, Test accuracy: 58.41 

Round  48, Global train loss: 0.721, Global test loss: 1.232, Global test accuracy: 59.04 

Round  49, Train loss: 0.701, Test loss: 1.374, Test accuracy: 58.43 

Round  49, Global train loss: 0.701, Global test loss: 1.203, Global test accuracy: 58.90 

Round  50, Train loss: 0.727, Test loss: 1.380, Test accuracy: 58.33 

Round  50, Global train loss: 0.727, Global test loss: 1.212, Global test accuracy: 58.32 

Round  51, Train loss: 0.732, Test loss: 1.388, Test accuracy: 58.52 

Round  51, Global train loss: 0.732, Global test loss: 1.190, Global test accuracy: 58.60 

Round  52, Train loss: 0.697, Test loss: 1.376, Test accuracy: 58.52 

Round  52, Global train loss: 0.697, Global test loss: 1.374, Global test accuracy: 59.52 

Round  53, Train loss: 0.698, Test loss: 1.377, Test accuracy: 58.80 

Round  53, Global train loss: 0.698, Global test loss: 1.249, Global test accuracy: 59.51 

Round  54, Train loss: 0.649, Test loss: 1.385, Test accuracy: 58.82 

Round  54, Global train loss: 0.649, Global test loss: 1.267, Global test accuracy: 59.91 

Round  55, Train loss: 0.692, Test loss: 1.382, Test accuracy: 59.20 

Round  55, Global train loss: 0.692, Global test loss: 1.219, Global test accuracy: 59.86 

Round  56, Train loss: 0.672, Test loss: 1.380, Test accuracy: 59.68 

Round  56, Global train loss: 0.672, Global test loss: 1.290, Global test accuracy: 59.66 

Round  57, Train loss: 0.685, Test loss: 1.384, Test accuracy: 59.47 

Round  57, Global train loss: 0.685, Global test loss: 1.263, Global test accuracy: 59.71 

Round  58, Train loss: 0.635, Test loss: 1.367, Test accuracy: 59.41 

Round  58, Global train loss: 0.635, Global test loss: 1.318, Global test accuracy: 58.95 

Round  59, Train loss: 0.712, Test loss: 1.378, Test accuracy: 59.42 

Round  59, Global train loss: 0.712, Global test loss: 1.288, Global test accuracy: 58.82 

Round  60, Train loss: 0.642, Test loss: 1.400, Test accuracy: 59.41 

Round  60, Global train loss: 0.642, Global test loss: 1.387, Global test accuracy: 60.54 

Round  61, Train loss: 0.607, Test loss: 1.397, Test accuracy: 59.52 

Round  61, Global train loss: 0.607, Global test loss: 1.264, Global test accuracy: 60.09 

Round  62, Train loss: 0.621, Test loss: 1.409, Test accuracy: 59.80 

Round  62, Global train loss: 0.621, Global test loss: 1.406, Global test accuracy: 60.77 

Round  63, Train loss: 0.573, Test loss: 1.424, Test accuracy: 59.27 

Round  63, Global train loss: 0.573, Global test loss: 1.557, Global test accuracy: 59.82 

Round  64, Train loss: 0.623, Test loss: 1.435, Test accuracy: 59.15 

Round  64, Global train loss: 0.623, Global test loss: 1.260, Global test accuracy: 59.27 

Round  65, Train loss: 0.648, Test loss: 1.430, Test accuracy: 59.59 

Round  65, Global train loss: 0.648, Global test loss: 1.251, Global test accuracy: 60.11 

Round  66, Train loss: 0.637, Test loss: 1.410, Test accuracy: 60.02 

Round  66, Global train loss: 0.637, Global test loss: 1.242, Global test accuracy: 59.77 

Round  67, Train loss: 0.621, Test loss: 1.431, Test accuracy: 59.92 

Round  67, Global train loss: 0.621, Global test loss: 1.308, Global test accuracy: 59.35 

Round  68, Train loss: 0.602, Test loss: 1.442, Test accuracy: 59.70 

Round  68, Global train loss: 0.602, Global test loss: 1.282, Global test accuracy: 60.55 

Round  69, Train loss: 0.588, Test loss: 1.453, Test accuracy: 59.64 

Round  69, Global train loss: 0.588, Global test loss: 1.268, Global test accuracy: 60.28 

Round  70, Train loss: 0.560, Test loss: 1.454, Test accuracy: 60.03 

Round  70, Global train loss: 0.560, Global test loss: 1.308, Global test accuracy: 60.22 

Round  71, Train loss: 0.542, Test loss: 1.461, Test accuracy: 59.80 

Round  71, Global train loss: 0.542, Global test loss: 1.493, Global test accuracy: 60.27 

Round  72, Train loss: 0.558, Test loss: 1.471, Test accuracy: 59.73 

Round  72, Global train loss: 0.558, Global test loss: 1.353, Global test accuracy: 60.13 

Round  73, Train loss: 0.553, Test loss: 1.452, Test accuracy: 60.15 

Round  73, Global train loss: 0.553, Global test loss: 1.288, Global test accuracy: 59.63 

Round  74, Train loss: 0.516, Test loss: 1.437, Test accuracy: 60.68 

Round  74, Global train loss: 0.516, Global test loss: 1.287, Global test accuracy: 59.95 

Round  75, Train loss: 0.557, Test loss: 1.450, Test accuracy: 60.75 

Round  75, Global train loss: 0.557, Global test loss: 1.289, Global test accuracy: 60.69 

Round  76, Train loss: 0.595, Test loss: 1.443, Test accuracy: 61.03 

Round  76, Global train loss: 0.595, Global test loss: 1.330, Global test accuracy: 60.85 

Round  77, Train loss: 0.503, Test loss: 1.433, Test accuracy: 61.27 

Round  77, Global train loss: 0.503, Global test loss: 1.336, Global test accuracy: 61.15 

Round  78, Train loss: 0.564, Test loss: 1.425, Test accuracy: 61.47 

Round  78, Global train loss: 0.564, Global test loss: 1.268, Global test accuracy: 60.20 

Round  79, Train loss: 0.550, Test loss: 1.439, Test accuracy: 61.36 

Round  79, Global train loss: 0.550, Global test loss: 1.243, Global test accuracy: 60.55 

Round  80, Train loss: 0.542, Test loss: 1.447, Test accuracy: 61.27 

Round  80, Global train loss: 0.542, Global test loss: 1.276, Global test accuracy: 60.15 

Round  81, Train loss: 0.516, Test loss: 1.459, Test accuracy: 61.29 

Round  81, Global train loss: 0.516, Global test loss: 1.333, Global test accuracy: 60.62 

Round  82, Train loss: 0.479, Test loss: 1.459, Test accuracy: 61.39 

Round  82, Global train loss: 0.479, Global test loss: 1.315, Global test accuracy: 59.87 

Round  83, Train loss: 0.536, Test loss: 1.466, Test accuracy: 61.39 

Round  83, Global train loss: 0.536, Global test loss: 1.328, Global test accuracy: 61.33 

Round  84, Train loss: 0.512, Test loss: 1.475, Test accuracy: 61.16 

Round  84, Global train loss: 0.512, Global test loss: 1.322, Global test accuracy: 60.83 

Round  85, Train loss: 0.489, Test loss: 1.468, Test accuracy: 61.50 

Round  85, Global train loss: 0.489, Global test loss: 1.342, Global test accuracy: 60.62 

Round  86, Train loss: 0.525, Test loss: 1.468, Test accuracy: 61.62 

Round  86, Global train loss: 0.525, Global test loss: 1.281, Global test accuracy: 60.67 

Round  87, Train loss: 0.498, Test loss: 1.467, Test accuracy: 61.58 

Round  87, Global train loss: 0.498, Global test loss: 1.455, Global test accuracy: 61.72 

Round  88, Train loss: 0.473, Test loss: 1.479, Test accuracy: 61.31 

Round  88, Global train loss: 0.473, Global test loss: 1.305, Global test accuracy: 60.48 

Round  89, Train loss: 0.467, Test loss: 1.490, Test accuracy: 61.34 

Round  89, Global train loss: 0.467, Global test loss: 1.306, Global test accuracy: 60.47 

Round  90, Train loss: 0.506, Test loss: 1.498, Test accuracy: 61.42 

Round  90, Global train loss: 0.506, Global test loss: 1.364, Global test accuracy: 60.40 

Round  91, Train loss: 0.449, Test loss: 1.516, Test accuracy: 61.36 

Round  91, Global train loss: 0.449, Global test loss: 1.559, Global test accuracy: 61.09 

Round  92, Train loss: 0.454, Test loss: 1.519, Test accuracy: 61.23 

Round  92, Global train loss: 0.454, Global test loss: 1.389, Global test accuracy: 61.21 

Round  93, Train loss: 0.445, Test loss: 1.507, Test accuracy: 61.25 

Round  93, Global train loss: 0.445, Global test loss: 1.345, Global test accuracy: 60.90 

Round  94, Train loss: 0.401, Test loss: 1.536, Test accuracy: 61.05 

Round  94, Global train loss: 0.401, Global test loss: 1.729, Global test accuracy: 61.10 

Round  95, Train loss: 0.506, Test loss: 1.523, Test accuracy: 61.16 

Round  95, Global train loss: 0.506, Global test loss: 1.397, Global test accuracy: 60.06 

Round  96, Train loss: 0.502, Test loss: 1.523, Test accuracy: 61.34 

Round  96, Global train loss: 0.502, Global test loss: 1.320, Global test accuracy: 60.39 

Round  97, Train loss: 0.468, Test loss: 1.529, Test accuracy: 61.49 

Round  97, Global train loss: 0.468, Global test loss: 1.389, Global test accuracy: 61.32 

Round  98, Train loss: 0.504, Test loss: 1.517, Test accuracy: 61.52 

Round  98, Global train loss: 0.504, Global test loss: 1.314, Global test accuracy: 61.18 

Round  99, Train loss: 0.457, Test loss: 1.526, Test accuracy: 61.30 

Round  99, Global train loss: 0.457, Global test loss: 1.336, Global test accuracy: 60.45 

Final Round, Train loss: 0.359, Test loss: 1.701, Test accuracy: 60.95 

Final Round, Global train loss: 0.359, Global test loss: 1.336, Global test accuracy: 60.45 

Average accuracy final 10 rounds: 61.313500000000005 

Average global accuracy final 10 rounds: 60.81025000000001 

2814.1803271770477
[1.5261149406433105, 2.7178714275360107, 3.9039902687072754, 5.094900131225586, 6.313772439956665, 7.516180038452148, 8.703750848770142, 9.906837463378906, 11.117218971252441, 12.323936700820923, 13.536064863204956, 14.73347544670105, 15.93257188796997, 17.13411283493042, 18.340087175369263, 19.54059934616089, 20.734293460845947, 21.93627166748047, 23.13929796218872, 24.341482162475586, 25.545634269714355, 26.74432945251465, 27.941843032836914, 29.1423397064209, 30.341749906539917, 31.53486466407776, 32.734214305877686, 33.93934512138367, 35.14544987678528, 36.352182388305664, 37.55443286895752, 38.752690076828, 39.9597647190094, 41.16049146652222, 42.367043018341064, 43.564690351486206, 44.76003432273865, 45.963406801223755, 47.16792964935303, 48.373656272888184, 49.56804919242859, 50.76658844947815, 51.97250771522522, 53.18004775047302, 54.38166379928589, 55.577720403671265, 56.77416944503784, 57.97993063926697, 59.17861533164978, 60.37987971305847, 61.58162879943848, 62.78263330459595, 63.978917598724365, 65.1770761013031, 66.37933659553528, 67.58310198783875, 68.7843828201294, 69.9789354801178, 71.17450404167175, 72.37673163414001, 73.5720534324646, 74.76572442054749, 75.96781063079834, 77.1630609035492, 78.36642527580261, 79.56613826751709, 80.76222920417786, 81.96261382102966, 83.15353608131409, 84.34792232513428, 85.55168437957764, 86.74924802780151, 87.94288969039917, 89.1393072605133, 90.34047746658325, 91.53937077522278, 92.74025058746338, 93.93285465240479, 95.11237382888794, 96.31196856498718, 97.52980923652649, 98.8260452747345, 100.09152674674988, 101.3566677570343, 102.62240839004517, 103.877614736557, 105.13365983963013, 106.39348316192627, 107.65592288970947, 108.83008742332458, 110.08800315856934, 111.34755373001099, 112.60923314094543, 113.86840581893921, 115.12698435783386, 116.39301204681396, 117.65776562690735, 118.9184958934784, 120.17706441879272, 121.43753576278687, 123.95437669754028]
[22.025, 25.6075, 27.6875, 29.84, 33.035, 36.3275, 36.8925, 38.09, 40.105, 41.5575, 42.3475, 42.8975, 43.705, 45.25, 45.76, 46.28, 46.8, 48.72, 49.4625, 49.88, 50.745, 51.3625, 52.02, 52.2275, 52.8025, 53.22, 53.5625, 53.8975, 54.02, 54.16, 54.305, 54.65, 55.31, 55.12, 55.295, 55.8525, 56.0475, 56.3475, 56.33, 56.455, 56.22, 56.36, 56.7425, 57.1625, 56.895, 57.18, 57.9725, 58.2275, 58.415, 58.43, 58.33, 58.5225, 58.5225, 58.7975, 58.8175, 59.2025, 59.6775, 59.4725, 59.4125, 59.42, 59.41, 59.5225, 59.7975, 59.2675, 59.145, 59.595, 60.0225, 59.9225, 59.705, 59.64, 60.0275, 59.8, 59.7325, 60.1475, 60.6825, 60.7475, 61.0325, 61.2675, 61.465, 61.3575, 61.2675, 61.29, 61.3925, 61.3875, 61.165, 61.4975, 61.6175, 61.5775, 61.315, 61.335, 61.4225, 61.36, 61.235, 61.2525, 61.0475, 61.1625, 61.3425, 61.4925, 61.52, 61.3, 60.955]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.292, Test loss: 2.259, Test accuracy: 18.64 

Round   1, Train loss: 2.188, Test loss: 2.057, Test accuracy: 23.83 

Round   2, Train loss: 2.036, Test loss: 1.976, Test accuracy: 26.25 

Round   3, Train loss: 1.934, Test loss: 1.886, Test accuracy: 28.36 

Round   4, Train loss: 1.854, Test loss: 1.846, Test accuracy: 29.75 

Round   5, Train loss: 1.814, Test loss: 1.776, Test accuracy: 32.99 

Round   6, Train loss: 1.775, Test loss: 1.725, Test accuracy: 34.70 

Round   7, Train loss: 1.713, Test loss: 1.689, Test accuracy: 36.70 

Round   8, Train loss: 1.679, Test loss: 1.673, Test accuracy: 37.87 

Round   9, Train loss: 1.636, Test loss: 1.625, Test accuracy: 39.81 

Round  10, Train loss: 1.614, Test loss: 1.611, Test accuracy: 40.19 

Round  11, Train loss: 1.598, Test loss: 1.586, Test accuracy: 40.96 

Round  12, Train loss: 1.599, Test loss: 1.547, Test accuracy: 42.78 

Round  13, Train loss: 1.547, Test loss: 1.549, Test accuracy: 42.76 

Round  14, Train loss: 1.540, Test loss: 1.504, Test accuracy: 44.69 

Round  15, Train loss: 1.498, Test loss: 1.489, Test accuracy: 45.28 

Round  16, Train loss: 1.483, Test loss: 1.481, Test accuracy: 45.52 

Round  17, Train loss: 1.468, Test loss: 1.451, Test accuracy: 46.80 

Round  18, Train loss: 1.431, Test loss: 1.436, Test accuracy: 47.51 

Round  19, Train loss: 1.437, Test loss: 1.413, Test accuracy: 48.28 

Round  20, Train loss: 1.407, Test loss: 1.394, Test accuracy: 48.93 

Round  21, Train loss: 1.390, Test loss: 1.398, Test accuracy: 49.17 

Round  22, Train loss: 1.355, Test loss: 1.374, Test accuracy: 50.38 

Round  23, Train loss: 1.348, Test loss: 1.363, Test accuracy: 50.87 

Round  24, Train loss: 1.341, Test loss: 1.353, Test accuracy: 51.29 

Round  25, Train loss: 1.321, Test loss: 1.335, Test accuracy: 52.32 

Round  26, Train loss: 1.264, Test loss: 1.325, Test accuracy: 52.35 

Round  27, Train loss: 1.291, Test loss: 1.352, Test accuracy: 51.11 

Round  28, Train loss: 1.269, Test loss: 1.335, Test accuracy: 51.85 

Round  29, Train loss: 1.291, Test loss: 1.307, Test accuracy: 52.87 

Round  30, Train loss: 1.217, Test loss: 1.294, Test accuracy: 53.66 

Round  31, Train loss: 1.232, Test loss: 1.286, Test accuracy: 53.51 

Round  32, Train loss: 1.187, Test loss: 1.301, Test accuracy: 53.26 

Round  33, Train loss: 1.186, Test loss: 1.271, Test accuracy: 54.68 

Round  34, Train loss: 1.159, Test loss: 1.273, Test accuracy: 54.51 

Round  35, Train loss: 1.138, Test loss: 1.270, Test accuracy: 54.69 

Round  36, Train loss: 1.177, Test loss: 1.264, Test accuracy: 54.90 

Round  37, Train loss: 1.096, Test loss: 1.258, Test accuracy: 55.45 

Round  38, Train loss: 1.082, Test loss: 1.224, Test accuracy: 56.80 

Round  39, Train loss: 1.063, Test loss: 1.217, Test accuracy: 57.13 

Round  40, Train loss: 1.103, Test loss: 1.212, Test accuracy: 57.28 

Round  41, Train loss: 1.093, Test loss: 1.198, Test accuracy: 57.58 

Round  42, Train loss: 1.025, Test loss: 1.216, Test accuracy: 57.03 

Round  43, Train loss: 1.043, Test loss: 1.206, Test accuracy: 57.45 

Round  44, Train loss: 1.023, Test loss: 1.210, Test accuracy: 57.90 

Round  45, Train loss: 1.002, Test loss: 1.176, Test accuracy: 58.46 

Round  46, Train loss: 1.026, Test loss: 1.188, Test accuracy: 58.35 

Round  47, Train loss: 1.032, Test loss: 1.176, Test accuracy: 58.94 

Round  48, Train loss: 0.984, Test loss: 1.163, Test accuracy: 59.18 

Round  49, Train loss: 0.958, Test loss: 1.170, Test accuracy: 59.01 

Round  50, Train loss: 0.954, Test loss: 1.174, Test accuracy: 58.94 

Round  51, Train loss: 0.950, Test loss: 1.181, Test accuracy: 58.91 

Round  52, Train loss: 0.930, Test loss: 1.198, Test accuracy: 59.17 

Round  53, Train loss: 0.932, Test loss: 1.190, Test accuracy: 59.36 

Round  54, Train loss: 0.919, Test loss: 1.155, Test accuracy: 59.80 

Round  55, Train loss: 0.963, Test loss: 1.151, Test accuracy: 60.39 

Round  56, Train loss: 0.925, Test loss: 1.159, Test accuracy: 60.58 

Round  57, Train loss: 0.891, Test loss: 1.174, Test accuracy: 60.08 

Round  58, Train loss: 0.888, Test loss: 1.164, Test accuracy: 60.60 

Round  59, Train loss: 0.904, Test loss: 1.159, Test accuracy: 60.77 

Round  60, Train loss: 0.854, Test loss: 1.157, Test accuracy: 60.94 

Round  61, Train loss: 0.861, Test loss: 1.160, Test accuracy: 60.97 

Round  62, Train loss: 0.817, Test loss: 1.162, Test accuracy: 61.02 

Round  63, Train loss: 0.837, Test loss: 1.150, Test accuracy: 61.55 

Round  64, Train loss: 0.838, Test loss: 1.143, Test accuracy: 61.65 

Round  65, Train loss: 0.803, Test loss: 1.142, Test accuracy: 61.73 

Round  66, Train loss: 0.779, Test loss: 1.150, Test accuracy: 61.66 

Round  67, Train loss: 0.804, Test loss: 1.146, Test accuracy: 61.73 

Round  68, Train loss: 0.795, Test loss: 1.141, Test accuracy: 61.76 

Round  69, Train loss: 0.816, Test loss: 1.157, Test accuracy: 61.47 

Round  70, Train loss: 0.790, Test loss: 1.146, Test accuracy: 62.02 

Round  71, Train loss: 0.832, Test loss: 1.146, Test accuracy: 62.35 

Round  72, Train loss: 0.737, Test loss: 1.139, Test accuracy: 62.30 

Round  73, Train loss: 0.736, Test loss: 1.157, Test accuracy: 61.78 

Round  74, Train loss: 0.749, Test loss: 1.157, Test accuracy: 62.13 

Round  75, Train loss: 0.796, Test loss: 1.146, Test accuracy: 62.56 

Round  76, Train loss: 0.767, Test loss: 1.153, Test accuracy: 62.15 

Round  77, Train loss: 0.709, Test loss: 1.180, Test accuracy: 61.97 

Round  78, Train loss: 0.659, Test loss: 1.167, Test accuracy: 62.38 

Round  79, Train loss: 0.760, Test loss: 1.161, Test accuracy: 62.46 

Round  80, Train loss: 0.699, Test loss: 1.151, Test accuracy: 62.94 

Round  81, Train loss: 0.707, Test loss: 1.158, Test accuracy: 63.25 

Round  82, Train loss: 0.678, Test loss: 1.146, Test accuracy: 63.14 

Round  83, Train loss: 0.740, Test loss: 1.159, Test accuracy: 63.21 

Round  84, Train loss: 0.670, Test loss: 1.167, Test accuracy: 63.27 

Round  85, Train loss: 0.707, Test loss: 1.178, Test accuracy: 63.28 

Round  86, Train loss: 0.657, Test loss: 1.150, Test accuracy: 63.62 

Round  87, Train loss: 0.704, Test loss: 1.188, Test accuracy: 62.85 

Round  88, Train loss: 0.689, Test loss: 1.164, Test accuracy: 63.46 

Round  89, Train loss: 0.621, Test loss: 1.168, Test accuracy: 63.27 

Round  90, Train loss: 0.660, Test loss: 1.183, Test accuracy: 63.31 

Round  91, Train loss: 0.654, Test loss: 1.178, Test accuracy: 63.46 

Round  92, Train loss: 0.692, Test loss: 1.190, Test accuracy: 63.37 

Round  93, Train loss: 0.653, Test loss: 1.178, Test accuracy: 63.42 

Round  94, Train loss: 0.625, Test loss: 1.185, Test accuracy: 63.46 

Round  95, Train loss: 0.624, Test loss: 1.182, Test accuracy: 63.18 

Round  96, Train loss: 0.671, Test loss: 1.175, Test accuracy: 63.55 

Round  97, Train loss: 0.626, Test loss: 1.191, Test accuracy: 63.83 

Round  98, Train loss: 0.625, Test loss: 1.220, Test accuracy: 63.46 

Round  99, Train loss: 0.603, Test loss: 1.205, Test accuracy: 63.81 

Final Round, Train loss: 0.535, Test loss: 1.217, Test accuracy: 64.03 

Average accuracy final 10 rounds: 63.48475 

1650.7109637260437
[1.3003554344177246, 2.3291378021240234, 3.3587610721588135, 4.385822296142578, 5.416116952896118, 6.4414732456207275, 7.495015382766724, 8.547472476959229, 9.601008892059326, 10.657144784927368, 11.71363878250122, 12.76239275932312, 13.813973903656006, 14.868176937103271, 15.92379379272461, 16.97332763671875, 18.027022123336792, 19.07749056816101, 20.126206636428833, 21.181105852127075, 22.23666477203369, 23.287814617156982, 24.340781688690186, 25.391992807388306, 26.444355010986328, 27.498438596725464, 28.55324149131775, 29.609721183776855, 30.664313316345215, 31.714824438095093, 32.76444435119629, 33.818812131881714, 34.87094163894653, 35.92671203613281, 36.986940145492554, 38.0370147228241, 39.08244490623474, 40.132570028305054, 41.19529676437378, 42.25082230567932, 43.30220103263855, 44.35745644569397, 45.40740084648132, 46.45454668998718, 47.505717515945435, 48.55962157249451, 49.61253643035889, 50.671653747558594, 51.72722601890564, 52.78449034690857, 53.83838391304016, 54.88904094696045, 55.941696643829346, 56.99214148521423, 57.942285776138306, 58.897329330444336, 59.85366988182068, 60.80974745750427, 61.769312143325806, 62.72557044029236, 63.67699956893921, 64.63161540031433, 65.58696293830872, 66.54297041893005, 67.49579405784607, 68.44839811325073, 69.3996353149414, 70.35289192199707, 71.30971050262451, 72.26487517356873, 73.21839499473572, 74.16791343688965, 75.12506055831909, 76.08182525634766, 77.03878021240234, 77.9934070110321, 78.94352746009827, 79.89505529403687, 80.85102558135986, 81.80406141281128, 82.7723605632782, 83.72043967247009, 84.6740951538086, 85.62948513031006, 86.59177279472351, 87.55310773849487, 88.5003023147583, 89.45846390724182, 90.4125235080719, 91.36519145965576, 92.31581807136536, 93.26775932312012, 94.21940302848816, 95.17522168159485, 96.13154911994934, 97.08383536338806, 98.03554034233093, 98.98593068122864, 99.93623733520508, 100.8862247467041, 102.66796684265137]
[18.635, 23.8275, 26.2525, 28.3575, 29.755, 32.9925, 34.705, 36.705, 37.8725, 39.81, 40.1925, 40.96, 42.785, 42.755, 44.685, 45.285, 45.5225, 46.7975, 47.5125, 48.28, 48.9325, 49.175, 50.3775, 50.865, 51.29, 52.3175, 52.3525, 51.1075, 51.8475, 52.865, 53.665, 53.51, 53.255, 54.6775, 54.51, 54.69, 54.895, 55.445, 56.7975, 57.135, 57.2775, 57.5825, 57.0325, 57.445, 57.9, 58.4575, 58.3475, 58.9425, 59.1825, 59.0075, 58.935, 58.905, 59.175, 59.3625, 59.8025, 60.3925, 60.5825, 60.08, 60.6025, 60.77, 60.94, 60.965, 61.015, 61.555, 61.6525, 61.7275, 61.6575, 61.725, 61.7575, 61.4675, 62.02, 62.3525, 62.3025, 61.785, 62.13, 62.565, 62.1475, 61.9675, 62.385, 62.4625, 62.9375, 63.2525, 63.1375, 63.2075, 63.2725, 63.2825, 63.615, 62.855, 63.46, 63.2675, 63.3125, 63.4575, 63.3675, 63.4175, 63.4625, 63.1775, 63.5525, 63.8275, 63.46, 63.8125, 64.0325]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.295, Test loss: 2.206, Test accuracy: 19.82
Round   1, Train loss: 2.156, Test loss: 2.038, Test accuracy: 24.30
Round   2, Train loss: 2.036, Test loss: 1.947, Test accuracy: 28.07
Round   3, Train loss: 1.941, Test loss: 1.871, Test accuracy: 30.93
Round   4, Train loss: 1.869, Test loss: 1.810, Test accuracy: 34.30
Round   5, Train loss: 1.814, Test loss: 1.742, Test accuracy: 35.83
Round   6, Train loss: 1.747, Test loss: 1.684, Test accuracy: 37.81
Round   7, Train loss: 1.736, Test loss: 1.634, Test accuracy: 40.02
Round   8, Train loss: 1.693, Test loss: 1.601, Test accuracy: 41.81
Round   9, Train loss: 1.643, Test loss: 1.570, Test accuracy: 42.30
Round  10, Train loss: 1.618, Test loss: 1.561, Test accuracy: 43.41
Round  11, Train loss: 1.619, Test loss: 1.504, Test accuracy: 45.54
Round  12, Train loss: 1.552, Test loss: 1.491, Test accuracy: 46.74
Round  13, Train loss: 1.537, Test loss: 1.458, Test accuracy: 47.77
Round  14, Train loss: 1.504, Test loss: 1.434, Test accuracy: 48.15
Round  15, Train loss: 1.507, Test loss: 1.410, Test accuracy: 49.01
Round  16, Train loss: 1.477, Test loss: 1.386, Test accuracy: 49.89
Round  17, Train loss: 1.433, Test loss: 1.381, Test accuracy: 50.66
Round  18, Train loss: 1.415, Test loss: 1.353, Test accuracy: 51.82
Round  19, Train loss: 1.395, Test loss: 1.351, Test accuracy: 51.65
Round  20, Train loss: 1.397, Test loss: 1.318, Test accuracy: 52.60
Round  21, Train loss: 1.355, Test loss: 1.326, Test accuracy: 52.68
Round  22, Train loss: 1.350, Test loss: 1.291, Test accuracy: 54.03
Round  23, Train loss: 1.295, Test loss: 1.308, Test accuracy: 53.38
Round  24, Train loss: 1.335, Test loss: 1.273, Test accuracy: 54.89
Round  25, Train loss: 1.274, Test loss: 1.249, Test accuracy: 56.01
Round  26, Train loss: 1.298, Test loss: 1.236, Test accuracy: 56.05
Round  27, Train loss: 1.281, Test loss: 1.228, Test accuracy: 56.28
Round  28, Train loss: 1.258, Test loss: 1.206, Test accuracy: 57.08
Round  29, Train loss: 1.217, Test loss: 1.206, Test accuracy: 57.15
Round  30, Train loss: 1.252, Test loss: 1.191, Test accuracy: 57.59
Round  31, Train loss: 1.180, Test loss: 1.198, Test accuracy: 57.20
Round  32, Train loss: 1.186, Test loss: 1.186, Test accuracy: 57.72
Round  33, Train loss: 1.183, Test loss: 1.169, Test accuracy: 58.10
Round  34, Train loss: 1.156, Test loss: 1.161, Test accuracy: 58.71
Round  35, Train loss: 1.153, Test loss: 1.147, Test accuracy: 59.43
Round  36, Train loss: 1.125, Test loss: 1.144, Test accuracy: 59.90
Round  37, Train loss: 1.116, Test loss: 1.135, Test accuracy: 59.99
Round  38, Train loss: 1.091, Test loss: 1.134, Test accuracy: 60.02
Round  39, Train loss: 1.140, Test loss: 1.104, Test accuracy: 60.97
Round  40, Train loss: 1.073, Test loss: 1.110, Test accuracy: 61.02
Round  41, Train loss: 1.105, Test loss: 1.102, Test accuracy: 61.35
Round  42, Train loss: 1.033, Test loss: 1.095, Test accuracy: 61.36
Round  43, Train loss: 1.042, Test loss: 1.096, Test accuracy: 61.13
Round  44, Train loss: 1.017, Test loss: 1.093, Test accuracy: 61.55
Round  45, Train loss: 1.011, Test loss: 1.090, Test accuracy: 61.71
Round  46, Train loss: 1.002, Test loss: 1.085, Test accuracy: 61.95
Round  47, Train loss: 1.024, Test loss: 1.073, Test accuracy: 62.45
Round  48, Train loss: 0.995, Test loss: 1.067, Test accuracy: 62.48
Round  49, Train loss: 0.974, Test loss: 1.057, Test accuracy: 62.92
Round  50, Train loss: 0.973, Test loss: 1.050, Test accuracy: 63.14
Round  51, Train loss: 0.974, Test loss: 1.051, Test accuracy: 63.01
Round  52, Train loss: 0.955, Test loss: 1.041, Test accuracy: 63.16
Round  53, Train loss: 0.952, Test loss: 1.033, Test accuracy: 63.53
Round  54, Train loss: 0.977, Test loss: 1.027, Test accuracy: 64.12
Round  55, Train loss: 0.933, Test loss: 1.038, Test accuracy: 63.44
Round  56, Train loss: 0.946, Test loss: 1.039, Test accuracy: 63.55
Round  57, Train loss: 0.903, Test loss: 1.027, Test accuracy: 63.80
Round  58, Train loss: 0.901, Test loss: 1.027, Test accuracy: 64.30
Round  59, Train loss: 0.849, Test loss: 1.030, Test accuracy: 64.06
Round  60, Train loss: 0.872, Test loss: 1.038, Test accuracy: 63.96
Round  61, Train loss: 0.881, Test loss: 1.029, Test accuracy: 63.81
Round  62, Train loss: 0.909, Test loss: 1.016, Test accuracy: 64.44
Round  63, Train loss: 0.853, Test loss: 1.012, Test accuracy: 64.58
Round  64, Train loss: 0.832, Test loss: 1.016, Test accuracy: 64.32
Round  65, Train loss: 0.864, Test loss: 1.007, Test accuracy: 64.95
Round  66, Train loss: 0.843, Test loss: 1.022, Test accuracy: 64.50
Round  67, Train loss: 0.795, Test loss: 1.005, Test accuracy: 65.51
Round  68, Train loss: 0.854, Test loss: 0.998, Test accuracy: 65.56
Round  69, Train loss: 0.807, Test loss: 1.004, Test accuracy: 65.61
Round  70, Train loss: 0.821, Test loss: 0.997, Test accuracy: 65.82
Round  71, Train loss: 0.784, Test loss: 0.990, Test accuracy: 65.45
Round  72, Train loss: 0.722, Test loss: 1.002, Test accuracy: 65.63
Round  73, Train loss: 0.771, Test loss: 0.991, Test accuracy: 65.94
Round  74, Train loss: 0.753, Test loss: 1.005, Test accuracy: 65.25
Round  75, Train loss: 0.803, Test loss: 0.995, Test accuracy: 65.91
Round  76, Train loss: 0.764, Test loss: 0.995, Test accuracy: 65.91
Round  77, Train loss: 0.777, Test loss: 0.995, Test accuracy: 65.83
Round  78, Train loss: 0.746, Test loss: 0.997, Test accuracy: 65.86
Round  79, Train loss: 0.758, Test loss: 0.980, Test accuracy: 66.50
Round  80, Train loss: 0.788, Test loss: 0.971, Test accuracy: 66.91
Round  81, Train loss: 0.723, Test loss: 0.977, Test accuracy: 66.38
Round  82, Train loss: 0.745, Test loss: 0.968, Test accuracy: 67.06
Round  83, Train loss: 0.741, Test loss: 0.971, Test accuracy: 66.65
Round  84, Train loss: 0.725, Test loss: 0.976, Test accuracy: 66.33
Round  85, Train loss: 0.767, Test loss: 0.973, Test accuracy: 66.67
Round  86, Train loss: 0.677, Test loss: 0.987, Test accuracy: 66.26
Round  87, Train loss: 0.701, Test loss: 0.983, Test accuracy: 66.53
Round  88, Train loss: 0.676, Test loss: 0.978, Test accuracy: 66.66
Round  89, Train loss: 0.648, Test loss: 0.977, Test accuracy: 66.68
Round  90, Train loss: 0.705, Test loss: 0.970, Test accuracy: 67.17
Round  91, Train loss: 0.674, Test loss: 0.969, Test accuracy: 66.89
Round  92, Train loss: 0.673, Test loss: 0.984, Test accuracy: 66.31
Round  93, Train loss: 0.673, Test loss: 0.985, Test accuracy: 66.58
Round  94, Train loss: 0.689, Test loss: 0.979, Test accuracy: 66.59
Round  95, Train loss: 0.654, Test loss: 0.968, Test accuracy: 66.86
Round  96, Train loss: 0.638, Test loss: 0.969, Test accuracy: 67.03
Round  97, Train loss: 0.670, Test loss: 0.973, Test accuracy: 66.97
Round  98, Train loss: 0.633, Test loss: 0.990, Test accuracy: 66.88
Round  99, Train loss: 0.600, Test loss: 0.977, Test accuracy: 66.98
Final Round, Train loss: 0.561, Test loss: 0.986, Test accuracy: 66.98
Average accuracy final 10 rounds: 66.82624999999999
1845.3841094970703
[1.728520154953003, 3.080855369567871, 4.417928457260132, 5.758871793746948, 7.087554216384888, 8.437259674072266, 9.862974405288696, 11.18232774734497, 12.468670129776001, 13.760276079177856, 15.049176454544067, 16.337173461914062, 17.62404179573059, 18.971008777618408, 20.256219625473022, 21.542320489883423, 22.829643487930298, 24.170535564422607, 25.520429134368896, 26.87145757675171, 28.230546951293945, 29.571030855178833, 30.921404600143433, 32.28109431266785, 33.636051177978516, 34.96775436401367, 36.31261444091797, 37.65805411338806, 38.9946985244751, 40.329474210739136, 41.67476487159729, 43.00808382034302, 44.34389901161194, 45.6920371055603, 47.036651372909546, 48.37123990058899, 49.71704030036926, 51.06159543991089, 52.41388440132141, 53.76203155517578, 55.112236738204956, 56.45098567008972, 57.79218864440918, 59.14431810379028, 60.48948931694031, 61.8194797039032, 63.153045892715454, 64.49359178543091, 65.82098126411438, 67.1529529094696, 68.498202085495, 69.83609175682068, 71.1629204750061, 72.49767565727234, 73.82543349266052, 75.11218047142029, 76.40085053443909, 77.69587635993958, 79.00421643257141, 80.28608584403992, 81.58135890960693, 82.89448142051697, 84.22874999046326, 85.57726764678955, 86.93624114990234, 88.2748670578003, 89.61081409454346, 90.96573376655579, 92.33229947090149, 93.63002514839172, 94.97077512741089, 96.34834313392639, 97.64012217521667, 98.98561120033264, 100.33817315101624, 101.67831254005432, 103.00989508628845, 104.36192274093628, 105.70194053649902, 107.04691672325134, 108.39983248710632, 109.71443605422974, 110.99408721923828, 112.28755211830139, 113.61977577209473, 114.96213102340698, 116.25274705886841, 117.57149267196655, 118.88735508918762, 120.17551779747009, 121.47344946861267, 122.77436232566833, 124.0753858089447, 125.36396861076355, 126.70511388778687, 128.00178861618042, 129.28301286697388, 130.59494829177856, 131.89682531356812, 133.18628096580505, 135.25688314437866]
[19.815, 24.305, 28.0725, 30.925, 34.2975, 35.8325, 37.8075, 40.0225, 41.815, 42.2975, 43.4125, 45.5375, 46.7425, 47.7725, 48.15, 49.005, 49.89, 50.655, 51.82, 51.6475, 52.5975, 52.6825, 54.0275, 53.3825, 54.89, 56.01, 56.045, 56.2825, 57.08, 57.15, 57.5875, 57.195, 57.7225, 58.105, 58.7125, 59.43, 59.9, 59.995, 60.015, 60.965, 61.02, 61.3475, 61.36, 61.13, 61.55, 61.7125, 61.955, 62.4475, 62.4825, 62.9225, 63.14, 63.0075, 63.165, 63.53, 64.1175, 63.4425, 63.545, 63.8, 64.295, 64.06, 63.9625, 63.81, 64.4425, 64.58, 64.3175, 64.955, 64.495, 65.5075, 65.5575, 65.615, 65.8225, 65.455, 65.6325, 65.945, 65.2475, 65.9125, 65.9075, 65.8275, 65.8625, 66.505, 66.91, 66.375, 67.0625, 66.6475, 66.3275, 66.675, 66.2575, 66.53, 66.66, 66.68, 67.1675, 66.89, 66.3075, 66.5825, 66.595, 66.8575, 67.0325, 66.9725, 66.875, 66.9825, 66.9775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.274, Test loss: 2.130, Test accuracy: 22.23
Round   1, Train loss: 2.072, Test loss: 1.928, Test accuracy: 29.49
Round   2, Train loss: 1.970, Test loss: 1.805, Test accuracy: 31.50
Round   3, Train loss: 1.863, Test loss: 1.747, Test accuracy: 34.90
Round   4, Train loss: 1.801, Test loss: 1.655, Test accuracy: 36.84
Round   5, Train loss: 1.700, Test loss: 1.609, Test accuracy: 40.22
Round   6, Train loss: 1.634, Test loss: 1.571, Test accuracy: 43.24
Round   7, Train loss: 1.606, Test loss: 1.560, Test accuracy: 40.51
Round   8, Train loss: 1.571, Test loss: 1.534, Test accuracy: 41.87
Round   9, Train loss: 1.485, Test loss: 1.489, Test accuracy: 46.74
Round  10, Train loss: 1.510, Test loss: 1.525, Test accuracy: 46.85
Round  11, Train loss: 1.510, Test loss: 1.572, Test accuracy: 44.27
Round  12, Train loss: 1.412, Test loss: 1.420, Test accuracy: 47.67
Round  13, Train loss: 1.366, Test loss: 1.414, Test accuracy: 49.74
Round  14, Train loss: 1.428, Test loss: 1.377, Test accuracy: 50.56
Round  15, Train loss: 1.294, Test loss: 1.356, Test accuracy: 51.35
Round  16, Train loss: 1.297, Test loss: 1.350, Test accuracy: 49.85
Round  17, Train loss: 1.286, Test loss: 1.344, Test accuracy: 51.74
Round  18, Train loss: 1.203, Test loss: 1.344, Test accuracy: 50.42
Round  19, Train loss: 1.190, Test loss: 1.336, Test accuracy: 53.74
Round  20, Train loss: 1.135, Test loss: 1.352, Test accuracy: 53.27
Round  21, Train loss: 1.225, Test loss: 1.353, Test accuracy: 50.69
Round  22, Train loss: 1.124, Test loss: 1.265, Test accuracy: 55.12
Round  23, Train loss: 1.096, Test loss: 1.298, Test accuracy: 56.23
Round  24, Train loss: 1.149, Test loss: 1.339, Test accuracy: 55.61
Round  25, Train loss: 1.067, Test loss: 1.260, Test accuracy: 55.79
Round  26, Train loss: 1.075, Test loss: 1.240, Test accuracy: 54.13
Round  27, Train loss: 1.035, Test loss: 1.236, Test accuracy: 54.28
Round  28, Train loss: 0.989, Test loss: 1.214, Test accuracy: 57.18
Round  29, Train loss: 0.965, Test loss: 1.275, Test accuracy: 57.92
Round  30, Train loss: 0.991, Test loss: 1.209, Test accuracy: 57.44
Round  31, Train loss: 0.963, Test loss: 1.204, Test accuracy: 57.58
Round  32, Train loss: 0.926, Test loss: 1.360, Test accuracy: 57.85
Round  33, Train loss: 0.905, Test loss: 1.208, Test accuracy: 58.53
Round  34, Train loss: 0.900, Test loss: 1.282, Test accuracy: 54.99
Round  35, Train loss: 0.887, Test loss: 1.201, Test accuracy: 58.91
Round  36, Train loss: 0.882, Test loss: 1.243, Test accuracy: 59.55
Round  37, Train loss: 0.835, Test loss: 1.187, Test accuracy: 57.76
Round  38, Train loss: 0.838, Test loss: 1.220, Test accuracy: 58.41
Round  39, Train loss: 0.784, Test loss: 1.216, Test accuracy: 59.19
Round  40, Train loss: 0.802, Test loss: 1.289, Test accuracy: 56.02
Round  41, Train loss: 0.802, Test loss: 1.244, Test accuracy: 60.13
Round  42, Train loss: 0.773, Test loss: 1.195, Test accuracy: 59.78
Round  43, Train loss: 0.722, Test loss: 1.221, Test accuracy: 60.09
Round  44, Train loss: 0.724, Test loss: 1.392, Test accuracy: 59.98
Round  45, Train loss: 0.745, Test loss: 1.364, Test accuracy: 60.01
Round  46, Train loss: 0.745, Test loss: 1.232, Test accuracy: 59.70
Round  47, Train loss: 0.714, Test loss: 1.227, Test accuracy: 59.42
Round  48, Train loss: 0.715, Test loss: 1.232, Test accuracy: 57.74
Round  49, Train loss: 0.689, Test loss: 1.228, Test accuracy: 58.27
Round  50, Train loss: 0.654, Test loss: 1.228, Test accuracy: 59.94
Round  51, Train loss: 0.673, Test loss: 1.293, Test accuracy: 61.27
Round  52, Train loss: 0.668, Test loss: 1.319, Test accuracy: 60.73
Round  53, Train loss: 0.608, Test loss: 1.233, Test accuracy: 58.23
Round  54, Train loss: 0.707, Test loss: 1.296, Test accuracy: 61.06
Round  55, Train loss: 0.612, Test loss: 1.310, Test accuracy: 61.41
Round  56, Train loss: 0.602, Test loss: 1.250, Test accuracy: 61.83
Round  57, Train loss: 0.625, Test loss: 1.305, Test accuracy: 61.51
Round  58, Train loss: 0.621, Test loss: 1.244, Test accuracy: 58.77
Round  59, Train loss: 0.568, Test loss: 1.312, Test accuracy: 61.85
Round  60, Train loss: 0.622, Test loss: 1.241, Test accuracy: 58.91
Round  61, Train loss: 0.575, Test loss: 1.337, Test accuracy: 61.81
Round  62, Train loss: 0.552, Test loss: 1.246, Test accuracy: 61.33
Round  63, Train loss: 0.575, Test loss: 1.230, Test accuracy: 59.51
Round  64, Train loss: 0.534, Test loss: 1.271, Test accuracy: 61.33
Round  65, Train loss: 0.583, Test loss: 1.332, Test accuracy: 57.67
Round  66, Train loss: 0.596, Test loss: 1.224, Test accuracy: 59.56
Round  67, Train loss: 0.556, Test loss: 1.232, Test accuracy: 59.79
Round  68, Train loss: 0.518, Test loss: 1.218, Test accuracy: 60.40
Round  69, Train loss: 0.523, Test loss: 1.294, Test accuracy: 61.42
Round  70, Train loss: 0.577, Test loss: 1.391, Test accuracy: 57.72
Round  71, Train loss: 0.501, Test loss: 1.615, Test accuracy: 56.98
Round  72, Train loss: 0.523, Test loss: 1.236, Test accuracy: 59.74
Round  73, Train loss: 0.496, Test loss: 1.365, Test accuracy: 62.26
Round  74, Train loss: 0.499, Test loss: 1.249, Test accuracy: 60.38
Round  75, Train loss: 0.480, Test loss: 1.230, Test accuracy: 60.27
Round  76, Train loss: 0.512, Test loss: 1.295, Test accuracy: 61.88
Round  77, Train loss: 0.456, Test loss: 1.347, Test accuracy: 61.13
Round  78, Train loss: 0.447, Test loss: 1.303, Test accuracy: 61.65
Round  79, Train loss: 0.455, Test loss: 1.318, Test accuracy: 62.42
Round  80, Train loss: 0.472, Test loss: 1.304, Test accuracy: 62.30
Round  81, Train loss: 0.474, Test loss: 1.442, Test accuracy: 62.26
Round  82, Train loss: 0.468, Test loss: 1.461, Test accuracy: 58.38
Round  83, Train loss: 0.460, Test loss: 1.430, Test accuracy: 58.20
Round  84, Train loss: 0.425, Test loss: 1.308, Test accuracy: 60.31
Round  85, Train loss: 0.473, Test loss: 1.319, Test accuracy: 62.13
Round  86, Train loss: 0.422, Test loss: 1.426, Test accuracy: 62.74
Round  87, Train loss: 0.428, Test loss: 1.417, Test accuracy: 63.03
Round  88, Train loss: 0.462, Test loss: 1.285, Test accuracy: 62.55
Round  89, Train loss: 0.403, Test loss: 1.297, Test accuracy: 62.69
Round  90, Train loss: 0.388, Test loss: 1.414, Test accuracy: 59.26
Round  91, Train loss: 0.435, Test loss: 1.575, Test accuracy: 62.97
Round  92, Train loss: 0.383, Test loss: 1.479, Test accuracy: 62.87
Round  93, Train loss: 0.425, Test loss: 1.285, Test accuracy: 61.16
Round  94, Train loss: 0.370, Test loss: 1.459, Test accuracy: 62.95
Round  95, Train loss: 0.434, Test loss: 1.260, Test accuracy: 60.44
Round  96, Train loss: 0.403, Test loss: 1.296, Test accuracy: 61.04
Round  97, Train loss: 0.359, Test loss: 1.459, Test accuracy: 63.05
Round  98, Train loss: 0.448, Test loss: 1.441, Test accuracy: 63.10
Round  99, Train loss: 0.369, Test loss: 1.365, Test accuracy: 60.27
Final Round, Train loss: 0.386, Test loss: 1.297, Test accuracy: 63.12
Average accuracy final 10 rounds: 61.711
2744.9555053710938
[3.263240098953247, 6.242027521133423, 9.22866678237915, 12.21421217918396, 15.20934247970581, 18.22723889350891, 21.243773937225342, 24.238218545913696, 27.235559225082397, 30.237459659576416, 33.24103856086731, 36.24674439430237, 39.23945331573486, 42.241843700408936, 45.23610877990723, 48.22881531715393, 51.21450924873352, 54.2017936706543, 57.20739197731018, 60.201233863830566, 63.15887260437012, 66.11473298072815, 69.08624911308289, 71.96938252449036, 74.93054914474487, 77.90841436386108, 80.91554427146912, 83.89285469055176, 86.86769366264343, 89.87470889091492, 92.90247750282288, 95.9341402053833, 98.94833111763, 102.00098657608032, 105.0184121131897, 108.03647422790527, 111.0763828754425, 114.08436918258667, 117.0988097190857, 119.83531951904297, 122.56092596054077, 125.29561686515808, 128.02351331710815, 130.73790073394775, 133.45431518554688, 136.1857566833496, 138.93084502220154, 141.65531635284424, 144.3733639717102, 147.07602715492249, 149.7743844985962, 152.47281050682068, 155.1787292957306, 157.86657524108887, 160.57773566246033, 163.3105607032776, 166.0291006565094, 168.75678324699402, 171.4687795639038, 174.1828863620758, 176.89643096923828, 179.6093249320984, 182.32577562332153, 185.02183318138123, 187.72178626060486, 190.4342052936554, 193.1649775505066, 195.88747715950012, 198.58345866203308, 201.2999782562256, 204.0094645023346, 206.70716094970703, 209.4079246520996, 212.11520099639893, 214.83325052261353, 217.56013107299805, 220.29109120368958, 223.01408624649048, 225.7233808040619, 228.44534134864807, 231.17122721672058, 233.88971877098083, 236.6157352924347, 239.3444857597351, 242.11374688148499, 244.8527193069458, 247.56012988090515, 250.26007890701294, 252.97513556480408, 255.65195751190186, 258.29340410232544, 260.95833230018616, 263.6014277935028, 266.2496283054352, 268.8960978984833, 271.5598568916321, 274.2014994621277, 276.8551170825958, 279.5174503326416, 282.1660852432251, 284.85024309158325]
[22.235, 29.49, 31.5, 34.9, 36.84, 40.22, 43.245, 40.51, 41.8675, 46.7375, 46.855, 44.2725, 47.675, 49.7375, 50.565, 51.35, 49.8475, 51.7375, 50.42, 53.745, 53.275, 50.6875, 55.12, 56.235, 55.6125, 55.7925, 54.135, 54.2775, 57.18, 57.9175, 57.4375, 57.5825, 57.8475, 58.53, 54.9925, 58.91, 59.5525, 57.7625, 58.405, 59.1925, 56.0175, 60.1275, 59.785, 60.0875, 59.9775, 60.005, 59.7025, 59.425, 57.745, 58.2675, 59.94, 61.27, 60.7325, 58.2275, 61.065, 61.41, 61.825, 61.505, 58.775, 61.8525, 58.9075, 61.8075, 61.325, 59.505, 61.3325, 57.67, 59.5575, 59.7875, 60.4025, 61.4225, 57.715, 56.9775, 59.7425, 62.2625, 60.38, 60.265, 61.88, 61.135, 61.645, 62.4225, 62.2975, 62.255, 58.38, 58.205, 60.315, 62.1275, 62.74, 63.0325, 62.5525, 62.6925, 59.255, 62.9725, 62.87, 61.1575, 62.955, 60.4375, 61.0375, 63.055, 63.105, 60.265, 63.115]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

8577.309713602066
[1.485741376876831, 2.744899272918701, 4.0171897411346436, 5.2887701988220215, 6.5664427280426025, 7.842285633087158, 9.123176574707031, 10.399111270904541, 11.674952983856201, 12.960114479064941, 14.24113917350769, 15.514020204544067, 16.792462587356567, 18.072309494018555, 19.356674671173096, 20.65300703048706, 21.942466497421265, 23.219088315963745, 24.498382329940796, 25.78818941116333, 27.126450777053833, 28.482134103775024, 29.80755925178528, 30.94739294052124, 32.0838668346405, 33.213802099227905, 34.34742331504822, 35.47528147697449, 36.607290506362915, 37.74225068092346, 38.871485233306885, 40.00357151031494, 41.13218140602112, 42.266507625579834, 43.39836025238037, 44.52700734138489, 45.65699863433838, 46.78714346885681, 47.916757106781006, 49.0539767742157, 50.18342041969299, 51.31398677825928, 52.44231700897217, 53.572458267211914, 54.709097385406494, 55.83798146247864, 56.97412467002869, 58.1089301109314, 59.24448776245117, 60.382853507995605, 61.5191969871521, 62.65430188179016, 63.94381809234619, 65.2445125579834, 66.54146647453308, 67.8311996459961, 69.12142157554626, 70.41027426719666, 71.70022082328796, 72.9973840713501, 74.28480553627014, 75.58466672897339, 76.87322044372559, 78.16670989990234, 79.45878577232361, 80.751629114151, 82.04472398757935, 83.34120035171509, 84.63696527481079, 85.92908239364624, 87.21844792366028, 88.51182389259338, 89.8072657585144, 91.09816932678223, 92.38774871826172, 93.68523144721985, 94.9725067615509, 96.2614688873291, 97.5617983341217, 98.85002613067627, 100.14062976837158, 101.42951226234436, 102.72252607345581, 104.01110935211182, 105.29905939102173, 106.58839392662048, 107.87774181365967, 109.15863990783691, 110.44008207321167, 111.73624873161316, 113.03523206710815, 114.32317018508911, 115.61774778366089, 116.91839694976807, 118.21519160270691, 119.51080226898193, 120.80143904685974, 122.10232257843018, 123.39872765541077, 124.68954992294312, 125.98655080795288, 127.14839148521423, 128.27553057670593, 129.40251994132996, 130.5359287261963, 131.66687297821045, 132.80131363868713, 133.93269681930542, 135.07957863807678, 136.21258234977722, 137.34742832183838, 138.4829661846161, 139.61910724639893, 140.7517647743225, 141.8848237991333, 143.02041721343994, 144.15608167648315, 145.29504656791687, 146.43497848510742, 147.5745804309845, 148.71475195884705, 149.86005520820618, 150.99963784217834, 152.1475875377655, 153.2957272529602, 154.4400475025177, 155.58038902282715, 156.72151708602905, 157.85483646392822, 158.98894357681274, 160.13889908790588, 161.2827169895172, 162.42356872558594, 163.5614035129547, 164.70228791236877, 165.8424301147461, 166.97750616073608, 168.27100133895874, 169.5670166015625, 170.86306762695312, 172.16578650474548, 173.46242547035217, 174.76121520996094, 176.05752086639404, 177.3651626110077, 178.66634273529053, 179.9574203491211, 181.2546854019165, 182.55120849609375, 183.85409569740295, 185.15963888168335, 186.45968341827393, 187.75650644302368, 189.05585169792175, 190.3621380329132, 191.66328835487366, 192.95547366142273, 194.2488079071045, 195.5497224330902, 196.8382396697998, 198.1353521347046, 199.4284472465515, 200.73005986213684, 202.01779675483704, 203.31353902816772, 204.6056694984436, 205.90590357780457, 207.20264196395874, 208.49584460258484, 209.78547739982605, 211.0833854675293, 212.37583184242249, 213.6658592224121, 214.96589756011963, 216.2643985748291, 217.56467366218567, 218.86812353134155, 220.16650485992432, 221.45921516418457, 222.76074504852295, 224.0616579055786, 225.3714394569397, 226.66041088104248, 227.9583010673523, 229.2522292137146, 230.56107187271118, 231.85001015663147, 233.14968466758728, 234.44547533988953, 235.7438805103302, 237.03145122528076, 238.32166814804077, 239.61258792877197, 240.91118621826172, 242.20537161827087, 243.50193738937378, 244.7912974357605, 246.08311319351196, 247.37755036354065, 248.67571187019348, 249.9677448272705, 251.2578203678131, 252.55065274238586, 253.8489465713501, 255.14387798309326, 256.4354622364044, 257.72601890563965, 259.0217807292938, 260.31906747817993, 261.61546206474304, 262.9091110229492, 264.1998145580292, 265.4873926639557, 266.7937150001526, 268.07720613479614, 269.3590352535248, 270.6519844532013, 271.95335602760315, 273.25963973999023, 274.56332659721375, 275.8627486228943, 277.16447591781616, 278.46968030929565, 279.777058839798, 281.0788941383362, 282.37751269340515, 283.67891788482666, 284.98358392715454, 286.2817680835724, 287.5815098285675, 288.8834869861603, 290.179536819458, 291.4765086174011, 292.7760624885559, 294.07407903671265, 295.3730366230011, 296.6776113510132, 297.9796071052551, 299.27089953422546, 300.5591583251953, 301.85632848739624, 303.15325927734375, 304.4522032737732, 305.73877120018005, 307.03686022758484, 308.33849477767944, 309.64660906791687, 310.94708156585693, 312.24617290496826, 313.5526101589203, 314.8529198169708, 316.1472885608673, 317.4438180923462, 318.74423813819885, 320.0419547557831, 321.34821462631226, 322.6591022014618, 323.9643428325653, 325.29720878601074, 326.60826110839844, 327.926833152771, 329.23938274383545, 330.54946303367615, 331.8572156429291, 333.16129517555237, 334.46772837638855, 335.77862548828125, 337.0844693183899, 338.40298986434937, 339.72027492523193, 341.03036308288574, 342.33647656440735, 343.64236664772034, 344.95042753219604, 346.2524616718292, 347.56371784210205, 348.8700225353241, 350.17079877853394, 351.4764573574066, 352.7880494594574, 354.09473395347595, 355.4006118774414, 356.7060601711273, 358.02151226997375, 359.3521406650543, 360.6692008972168, 362.01570558547974, 363.32181453704834, 364.62473797798157, 365.9180586338043, 367.06447649002075, 368.2049448490143, 369.3532109260559, 370.48913979530334, 371.6216514110565, 372.75670742988586, 373.8974394798279, 375.0262415409088, 376.1585569381714, 377.28912687301636, 379.5690977573395]
[10.145, 10.1825, 10.175, 10.1725, 10.195, 10.2325, 10.185, 10.2025, 10.3625, 10.47, 10.5175, 10.525, 10.555, 10.6725, 10.9, 10.9625, 11.1025, 11.2775, 11.4825, 11.5775, 11.6475, 11.7925, 11.8, 11.8775, 11.935, 11.975, 12.1975, 12.5225, 12.67, 12.8675, 12.8725, 12.9825, 13.21, 13.455, 13.5625, 13.6175, 14.0475, 14.2175, 14.3375, 14.5425, 14.6825, 14.73, 14.6975, 14.8275, 14.895, 14.92, 14.7775, 14.8325, 14.86, 14.665, 14.46, 14.6225, 14.57, 14.3525, 14.6425, 14.7225, 14.3925, 14.49, 14.2175, 14.2175, 14.2825, 14.3, 14.0925, 14.0375, 14.0975, 14.2775, 14.31, 14.2125, 14.4325, 14.53, 14.46, 14.4, 14.5525, 14.49, 14.7175, 14.97, 15.595, 15.69, 15.905, 16.0025, 15.905, 15.9425, 15.955, 15.995, 16.06, 16.2325, 16.52, 16.51, 16.4675, 16.3525, 16.6875, 17.2175, 17.27, 17.4975, 17.235, 17.4325, 17.3625, 17.57, 17.83, 18.01, 18.19, 18.2175, 18.3975, 18.51, 18.5925, 18.735, 18.695, 18.945, 18.91, 18.93, 18.72, 18.845, 18.955, 19.02, 18.97, 18.8925, 18.76, 18.725, 18.785, 18.9425, 19.21, 19.0575, 18.9625, 19.12, 19.1175, 19.2375, 19.2475, 19.3075, 19.32, 19.4225, 19.5525, 19.4825, 19.5575, 19.795, 19.8975, 19.9525, 19.81, 19.7225, 19.93, 20.16, 20.175, 20.08, 20.085, 20.1975, 20.42, 20.41, 20.5375, 20.6525, 20.325, 17.21, 15.0175, 12.705, 12.1775, 12.1775, 11.6825, 11.6825, 11.6825, 11.1275, 10.5725, 10.5725, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.401, Test loss: 1.788, Test accuracy: 59.47
Final Round, Global train loss: 0.401, Global test loss: 2.005, Global test accuracy: 52.12
Average accuracy final 10 rounds: 59.404999999999994 

Average global accuracy final 10 rounds: 51.954 

3806.202492237091
[1.4103562831878662, 2.8207125663757324, 3.84812331199646, 4.8755340576171875, 5.915058851242065, 6.954583644866943, 7.980641841888428, 9.006700038909912, 10.021193265914917, 11.035686492919922, 12.068202495574951, 13.10071849822998, 14.127964496612549, 15.155210494995117, 16.18198871612549, 17.20876693725586, 18.233819723129272, 19.258872509002686, 20.28331232070923, 21.30775213241577, 22.326284885406494, 23.344817638397217, 24.359041690826416, 25.373265743255615, 26.39162802696228, 27.409990310668945, 28.43360996246338, 29.457229614257812, 30.478211641311646, 31.49919366836548, 32.5236496925354, 33.54810571670532, 34.74046516418457, 35.93282461166382, 37.12054896354675, 38.30827331542969, 39.50035810470581, 40.692442893981934, 41.87731909751892, 43.06219530105591, 44.25386309623718, 45.44553089141846, 46.63901710510254, 47.83250331878662, 49.02420401573181, 50.215904712677, 51.41365671157837, 52.611408710479736, 53.80345797538757, 54.99550724029541, 56.18899345397949, 57.382479667663574, 58.56874942779541, 59.755019187927246, 60.93324041366577, 62.1114616394043, 63.50636672973633, 64.90127182006836, 66.09338927268982, 67.28550672531128, 68.4775779247284, 69.66964912414551, 70.86016345024109, 72.05067777633667, 73.24474787712097, 74.43881797790527, 75.62815642356873, 76.81749486923218, 78.00550436973572, 79.19351387023926, 80.37496829032898, 81.5564227104187, 82.74044060707092, 83.92445850372314, 85.10388922691345, 86.28331995010376, 87.46390080451965, 88.64448165893555, 89.83645987510681, 91.02843809127808, 92.22466683387756, 93.42089557647705, 94.61356902122498, 95.8062424659729, 96.99738430976868, 98.18852615356445, 99.37472581863403, 100.56092548370361, 101.75235080718994, 102.94377613067627, 103.98852300643921, 105.03326988220215, 106.06097555160522, 107.0886812210083, 108.11950039863586, 109.15031957626343, 110.17915034294128, 111.20798110961914, 112.24747848510742, 113.2869758605957, 114.3122808933258, 115.33758592605591, 116.35810875892639, 117.37863159179688, 118.41368818283081, 119.44874477386475, 120.4829568862915, 121.51716899871826, 122.55376386642456, 123.59035873413086, 124.62695288658142, 125.66354703903198, 126.7045967578888, 127.7456464767456, 128.78423738479614, 129.82282829284668, 130.84497380256653, 131.86711931228638, 132.89633297920227, 133.92554664611816, 134.95406770706177, 135.98258876800537, 137.0094497203827, 138.03631067276, 139.0750434398651, 140.11377620697021, 141.147287607193, 142.18079900741577, 143.20634508132935, 144.23189115524292, 145.25620651245117, 146.28052186965942, 147.31233859062195, 148.34415531158447, 149.37252497673035, 150.40089464187622, 151.42366790771484, 152.44644117355347, 153.47789764404297, 154.50935411453247, 155.5418345928192, 156.57431507110596, 157.6096076965332, 158.64490032196045, 159.67160058021545, 160.69830083847046, 161.71725630760193, 162.7362117767334, 163.76321482658386, 164.79021787643433, 165.83077263832092, 166.87132740020752, 167.9028925895691, 168.93445777893066, 169.96851587295532, 171.00257396697998, 172.03388929367065, 173.06520462036133, 174.08740210533142, 175.1095995903015, 176.1280755996704, 177.1465516090393, 178.17060828208923, 179.19466495513916, 180.24916195869446, 181.30365896224976, 182.34293937683105, 183.38221979141235, 184.4109969139099, 185.43977403640747, 186.4700973033905, 187.50042057037354, 188.53010892868042, 189.5597972869873, 190.59654116630554, 191.63328504562378, 192.65882754325867, 193.68437004089355, 194.7184956073761, 195.75262117385864, 196.78460574150085, 197.81659030914307, 198.84740614891052, 199.87822198867798, 200.90804028511047, 201.93785858154297, 202.9671778678894, 203.99649715423584, 205.03166961669922, 206.0668420791626, 207.0929183959961, 208.1189947128296, 209.1379165649414, 210.15683841705322, 211.19127869606018, 212.22571897506714, 213.2516348361969, 214.27755069732666, 215.3056116104126, 216.33367252349854, 218.41060042381287, 220.4875283241272]
[22.855, 22.855, 26.25, 26.25, 29.12, 29.12, 32.0675, 32.0675, 33.8425, 33.8425, 35.5725, 35.5725, 37.395, 37.395, 37.5975, 37.5975, 38.88, 38.88, 40.5225, 40.5225, 41.83, 41.83, 43.4925, 43.4925, 44.06, 44.06, 44.5275, 44.5275, 45.2, 45.2, 45.065, 45.065, 46.1925, 46.1925, 47.635, 47.635, 47.885, 47.885, 48.62, 48.62, 49.105, 49.105, 49.58, 49.58, 50.0875, 50.0875, 49.775, 49.775, 50.37, 50.37, 50.355, 50.355, 51.59, 51.59, 52.115, 52.115, 52.5575, 52.5575, 52.89, 52.89, 53.095, 53.095, 53.1575, 53.1575, 53.4725, 53.4725, 53.34, 53.34, 54.0, 54.0, 53.6575, 53.6575, 54.3125, 54.3125, 54.3025, 54.3025, 54.7125, 54.7125, 54.94, 54.94, 55.29, 55.29, 55.6475, 55.6475, 55.8925, 55.8925, 55.91, 55.91, 55.915, 55.915, 55.9075, 55.9075, 56.0075, 56.0075, 56.295, 56.295, 56.5125, 56.5125, 56.2375, 56.2375, 56.415, 56.415, 56.335, 56.335, 56.7375, 56.7375, 56.445, 56.445, 56.7225, 56.7225, 57.0775, 57.0775, 57.495, 57.495, 57.5625, 57.5625, 57.4175, 57.4175, 57.7325, 57.7325, 57.7625, 57.7625, 57.61, 57.61, 57.935, 57.935, 58.015, 58.015, 57.6625, 57.6625, 58.005, 58.005, 58.08, 58.08, 58.5825, 58.5825, 58.28, 58.28, 58.055, 58.055, 58.11, 58.11, 58.3375, 58.3375, 58.51, 58.51, 58.205, 58.205, 58.2325, 58.2325, 58.3425, 58.3425, 58.225, 58.225, 58.645, 58.645, 58.665, 58.665, 58.8075, 58.8075, 58.765, 58.765, 58.7075, 58.7075, 58.885, 58.885, 59.065, 59.065, 59.2925, 59.2925, 59.3675, 59.3675, 59.335, 59.335, 59.44, 59.44, 59.3275, 59.3275, 59.22, 59.22, 58.8725, 58.8725, 59.2225, 59.2225, 59.28, 59.28, 59.475, 59.475, 59.8175, 59.8175, 59.615, 59.615, 59.3375, 59.3375, 59.635, 59.635, 59.345, 59.345, 59.45, 59.45, 59.465, 59.465]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.564, Test loss: 1.276, Test accuracy: 62.48
Average accuracy final 10 rounds: 61.81399999999999 

2834.7167434692383
[1.303173542022705, 2.60634708404541, 3.697333574295044, 4.788320064544678, 5.880782604217529, 6.973245143890381, 8.063112020492554, 9.152978897094727, 10.250566720962524, 11.348154544830322, 12.451720476150513, 13.555286407470703, 14.658189058303833, 15.761091709136963, 16.871233701705933, 17.981375694274902, 19.08536672592163, 20.18935775756836, 21.292253732681274, 22.39514970779419, 23.50356698036194, 24.611984252929688, 25.72049832344055, 26.829012393951416, 27.920594215393066, 29.012176036834717, 30.11156988143921, 31.2109637260437, 32.320247411727905, 33.42953109741211, 34.53812265396118, 35.646714210510254, 36.7444269657135, 37.84213972091675, 38.94390916824341, 40.04567861557007, 41.14974308013916, 42.25380754470825, 43.35553979873657, 44.45727205276489, 45.559945821762085, 46.66261959075928, 47.76466464996338, 48.86670970916748, 49.97204613685608, 51.07738256454468, 52.182865381240845, 53.28834819793701, 54.39263129234314, 55.49691438674927, 56.59559679031372, 57.694279193878174, 58.79957675933838, 59.904874324798584, 61.003485441207886, 62.10209655761719, 63.20792603492737, 64.31375551223755, 65.413006067276, 66.51225662231445, 67.60877656936646, 68.70529651641846, 69.8093318939209, 70.91336727142334, 72.01797461509705, 73.12258195877075, 74.22619295120239, 75.32980394363403, 76.42993116378784, 77.53005838394165, 78.63365817070007, 79.7372579574585, 80.83457255363464, 81.93188714981079, 83.03059577941895, 84.1293044090271, 85.23208928108215, 86.3348741531372, 87.43859052658081, 88.54230690002441, 89.64873123168945, 90.75515556335449, 91.85815167427063, 92.96114778518677, 94.06618428230286, 95.17122077941895, 96.27441835403442, 97.3776159286499, 98.48252558708191, 99.58743524551392, 100.68532085418701, 101.78320646286011, 102.88624906539917, 103.98929166793823, 105.09053444862366, 106.19177722930908, 107.29398393630981, 108.39619064331055, 109.50513672828674, 110.61408281326294, 111.72143387794495, 112.82878494262695, 113.92883062362671, 115.02887630462646, 116.12373757362366, 117.21859884262085, 118.29504013061523, 119.37148141860962, 120.45957374572754, 121.54766607284546, 122.64174342155457, 123.73582077026367, 124.82341647148132, 125.91101217269897, 127.00205183029175, 128.09309148788452, 129.1905641555786, 130.2880368232727, 131.3716104030609, 132.45518398284912, 133.53407764434814, 134.61297130584717, 135.70780038833618, 136.8026294708252, 137.90061807632446, 138.99860668182373, 140.0915207862854, 141.18443489074707, 142.27764868736267, 143.37086248397827, 144.46412086486816, 145.55737924575806, 146.6420295238495, 147.72667980194092, 148.81221532821655, 149.8977508544922, 150.99053740501404, 152.0833239555359, 153.17531728744507, 154.26731061935425, 155.35667300224304, 156.44603538513184, 157.53682136535645, 158.62760734558105, 159.7237048149109, 160.81980228424072, 161.90821623802185, 162.99663019180298, 164.0750606060028, 165.15349102020264, 166.24876379966736, 167.34403657913208, 168.43208575248718, 169.52013492584229, 170.6099922657013, 171.6998496055603, 172.78053426742554, 173.86121892929077, 174.94454264640808, 176.0278663635254, 177.10611701011658, 178.18436765670776, 179.2678759098053, 180.35138416290283, 181.43557953834534, 182.51977491378784, 183.60711240768433, 184.6944499015808, 185.75211453437805, 186.8097791671753, 187.89998579025269, 188.99019241333008, 190.08377194404602, 191.17735147476196, 192.26120519638062, 193.34505891799927, 194.42711973190308, 195.50918054580688, 196.59417128562927, 197.67916202545166, 198.76909494400024, 199.85902786254883, 200.9480037689209, 202.03697967529297, 203.140465259552, 204.24395084381104, 205.3401038646698, 206.43625688552856, 207.5316460132599, 208.6270351409912, 209.77459001541138, 210.92214488983154, 212.011488199234, 213.10083150863647, 214.19216990470886, 215.28350830078125, 216.37354350090027, 217.4635787010193, 218.55577182769775, 219.64796495437622, 221.61017632484436, 223.5723876953125]
[16.6825, 16.6825, 22.1225, 22.1225, 24.88, 24.88, 28.035, 28.035, 29.8175, 29.8175, 32.4725, 32.4725, 33.4575, 33.4575, 35.35, 35.35, 36.475, 36.475, 38.6175, 38.6175, 39.59, 39.59, 40.535, 40.535, 42.6725, 42.6725, 43.515, 43.515, 45.02, 45.02, 45.525, 45.525, 45.5325, 45.5325, 46.305, 46.305, 46.9475, 46.9475, 47.305, 47.305, 48.72, 48.72, 49.065, 49.065, 49.7125, 49.7125, 49.95, 49.95, 50.035, 50.035, 50.92, 50.92, 51.1425, 51.1425, 51.3325, 51.3325, 52.58, 52.58, 53.2325, 53.2325, 53.8825, 53.8825, 54.09, 54.09, 53.9575, 53.9575, 54.13, 54.13, 54.8375, 54.8375, 55.125, 55.125, 54.4225, 54.4225, 55.215, 55.215, 55.49, 55.49, 56.615, 56.615, 57.0175, 57.0175, 57.565, 57.565, 56.8925, 56.8925, 57.015, 57.015, 57.3175, 57.3175, 57.74, 57.74, 58.1175, 58.1175, 57.9075, 57.9075, 58.795, 58.795, 58.53, 58.53, 58.365, 58.365, 58.2875, 58.2875, 58.575, 58.575, 59.2225, 59.2225, 59.61, 59.61, 59.2425, 59.2425, 60.025, 60.025, 59.855, 59.855, 60.235, 60.235, 59.825, 59.825, 59.8675, 59.8675, 60.19, 60.19, 59.3625, 59.3625, 59.6725, 59.6725, 59.815, 59.815, 60.3125, 60.3125, 60.7625, 60.7625, 60.1675, 60.1675, 60.54, 60.54, 60.6825, 60.6825, 60.9225, 60.9225, 60.785, 60.785, 60.745, 60.745, 61.33, 61.33, 60.8975, 60.8975, 61.0325, 61.0325, 61.48, 61.48, 61.375, 61.375, 60.7925, 60.7925, 61.0125, 61.0125, 60.8925, 60.8925, 60.8375, 60.8375, 61.1375, 61.1375, 61.555, 61.555, 61.78, 61.78, 62.1, 62.1, 62.1075, 62.1075, 61.97, 61.97, 62.2875, 62.2875, 61.915, 61.915, 62.36, 62.36, 61.97, 61.97, 62.0775, 62.0775, 61.825, 61.825, 61.7225, 61.7225, 61.71, 61.71, 61.8575, 61.8575, 61.3825, 61.3825, 61.6325, 61.6325, 61.6025, 61.6025, 62.485, 62.485]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.569, Test loss: 0.940, Test accuracy: 68.32
Average accuracy final 10 rounds: 67.60549999999999
2980.5951182842255
[1.7271485328674316, 3.4542970657348633, 4.8097310066223145, 6.165164947509766, 7.52192497253418, 8.878684997558594, 10.176389455795288, 11.474093914031982, 12.69073224067688, 13.907370567321777, 15.117257118225098, 16.327143669128418, 17.55336856842041, 18.779593467712402, 20.009201526641846, 21.23880958557129, 22.4594247341156, 23.680039882659912, 24.893428087234497, 26.106816291809082, 27.31518006324768, 28.52354383468628, 29.74140453338623, 30.95926523208618, 32.18032217025757, 33.401379108428955, 34.625807762145996, 35.85023641586304, 37.07111668586731, 38.29199695587158, 39.50806474685669, 40.7241325378418, 41.93400692939758, 43.14388132095337, 44.352375984191895, 45.56087064743042, 46.770241498947144, 47.97961235046387, 49.20195508003235, 50.42429780960083, 51.64830017089844, 52.872302532196045, 54.0909206867218, 55.30953884124756, 56.52425003051758, 57.7389612197876, 58.95216774940491, 60.16537427902222, 61.389695167541504, 62.61401605606079, 63.83119058609009, 65.04836511611938, 66.54362964630127, 68.03889417648315, 69.31364679336548, 70.5883994102478, 71.85724830627441, 73.12609720230103, 74.3931155204773, 75.66013383865356, 77.04806399345398, 78.4359941482544, 79.83298182487488, 81.22996950149536, 82.61608529090881, 84.00220108032227, 85.38938164710999, 86.7765622138977, 88.15789246559143, 89.53922271728516, 90.92038941383362, 92.30155611038208, 93.69348573684692, 95.08541536331177, 96.4968810081482, 97.90834665298462, 99.29953169822693, 100.69071674346924, 102.06774234771729, 103.44476795196533, 104.83795118331909, 106.23113441467285, 107.62728071212769, 109.02342700958252, 110.41905927658081, 111.8146915435791, 113.19967031478882, 114.58464908599854, 115.97184443473816, 117.35903978347778, 118.74043130874634, 120.12182283401489, 121.53217911720276, 122.94253540039062, 124.33033490180969, 125.71813440322876, 127.10532116889954, 128.4925079345703, 129.90069794654846, 131.3088879585266, 132.71173000335693, 134.11457204818726, 135.5298614501953, 136.94515085220337, 138.33025693893433, 139.71536302566528, 141.12251353263855, 142.52966403961182, 143.93001341819763, 145.33036279678345, 146.7158544063568, 148.10134601593018, 149.47388911247253, 150.8464322090149, 152.23758935928345, 153.628746509552, 155.01362705230713, 156.39850759506226, 157.7894265651703, 159.18034553527832, 160.57839250564575, 161.97643947601318, 163.3835608959198, 164.79068231582642, 166.18853116035461, 167.5863800048828, 168.98994398117065, 170.3935079574585, 171.77253222465515, 173.1515564918518, 174.5265679359436, 175.9015793800354, 177.3087284564972, 178.71587753295898, 180.11583495140076, 181.51579236984253, 182.89730024337769, 184.27880811691284, 185.65163278579712, 187.0244574546814, 188.41258668899536, 189.80071592330933, 191.19078922271729, 192.58086252212524, 193.97538948059082, 195.3699164390564, 196.76017332077026, 198.15043020248413, 199.54839372634888, 200.94635725021362, 202.34793329238892, 203.7495093345642, 205.145281791687, 206.54105424880981, 207.93421125411987, 209.32736825942993, 210.72432708740234, 212.12128591537476, 213.51959872245789, 214.91791152954102, 216.18254470825195, 217.4471778869629, 218.72277808189392, 219.99837827682495, 221.2564079761505, 222.51443767547607, 223.79884219169617, 225.08324670791626, 226.35573959350586, 227.62823247909546, 228.89521861076355, 230.16220474243164, 231.44617867469788, 232.7301526069641, 234.01997637748718, 235.30980014801025, 236.58406972885132, 237.85833930969238, 239.142893075943, 240.4274468421936, 241.71715021133423, 243.00685358047485, 244.29757809638977, 245.5883026123047, 246.87036681175232, 248.15243101119995, 249.44020700454712, 250.7279829978943, 252.01466059684753, 253.30133819580078, 254.57932710647583, 255.85731601715088, 257.13224482536316, 258.40717363357544, 259.6980812549591, 260.9889888763428, 262.26764965057373, 263.5463104248047, 264.82836961746216, 266.11042881011963, 268.1782636642456, 270.2460985183716]
[17.935, 17.935, 23.4625, 23.4625, 25.7, 25.7, 28.525, 28.525, 30.91, 30.91, 32.76, 32.76, 34.41, 34.41, 36.565, 36.565, 38.245, 38.245, 39.975, 39.975, 41.1025, 41.1025, 42.6925, 42.6925, 43.26, 43.26, 44.5225, 44.5225, 45.7625, 45.7625, 46.2375, 46.2375, 46.6225, 46.6225, 47.905, 47.905, 48.2425, 48.2425, 48.785, 48.785, 49.45, 49.45, 50.5325, 50.5325, 52.105, 52.105, 52.7475, 52.7475, 53.255, 53.255, 54.0575, 54.0575, 54.3875, 54.3875, 54.8, 54.8, 54.52, 54.52, 55.1975, 55.1975, 55.695, 55.695, 56.025, 56.025, 56.72, 56.72, 56.9625, 56.9625, 57.6925, 57.6925, 58.065, 58.065, 58.5325, 58.5325, 58.5725, 58.5725, 59.245, 59.245, 60.11, 60.11, 59.765, 59.765, 60.1425, 60.1425, 60.605, 60.605, 60.1425, 60.1425, 60.7425, 60.7425, 61.165, 61.165, 61.0475, 61.0475, 61.265, 61.265, 61.7725, 61.7725, 62.2725, 62.2725, 62.555, 62.555, 62.785, 62.785, 62.2525, 62.2525, 62.83, 62.83, 63.21, 63.21, 63.67, 63.67, 63.5925, 63.5925, 63.62, 63.62, 63.665, 63.665, 63.4425, 63.4425, 63.37, 63.37, 63.89, 63.89, 64.075, 64.075, 63.525, 63.525, 64.5425, 64.5425, 64.145, 64.145, 65.2525, 65.2525, 65.43, 65.43, 65.185, 65.185, 64.9125, 64.9125, 65.245, 65.245, 65.48, 65.48, 65.4975, 65.4975, 65.78, 65.78, 65.5075, 65.5075, 65.8575, 65.8575, 66.0925, 66.0925, 66.3025, 66.3025, 66.075, 66.075, 66.145, 66.145, 65.8775, 65.8775, 65.485, 65.485, 66.715, 66.715, 66.5825, 66.5825, 66.8175, 66.8175, 66.925, 66.925, 67.4875, 67.4875, 67.3025, 67.3025, 67.4525, 67.4525, 67.72, 67.72, 67.6575, 67.6575, 67.7625, 67.7625, 66.8875, 66.8875, 67.165, 67.165, 67.4675, 67.4675, 67.915, 67.915, 67.7975, 67.7975, 67.6275, 67.6275, 67.825, 67.825, 67.95, 67.95, 68.3175, 68.3175]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.457, Test loss: 1.784, Test accuracy: 51.37
Average accuracy final 10 rounds: 50.74975
3003.1093814373016
[3.3148739337921143, 6.484703302383423, 9.651582717895508, 12.790904760360718, 15.930892705917358, 19.075708389282227, 22.22748112678528, 25.373470067977905, 28.52538537979126, 31.684412479400635, 34.80570316314697, 37.91144013404846, 41.03539276123047, 44.152857542037964, 47.26487588882446, 50.37461876869202, 53.51289486885071, 56.664106130599976, 59.797101974487305, 62.908578634262085, 66.0356330871582, 69.17241406440735, 72.30268096923828, 75.41245007514954, 78.54417705535889, 81.6632342338562, 84.79641771316528, 87.89321660995483, 91.03000044822693, 94.16279006004333, 97.28887009620667, 100.40565037727356, 103.52657675743103, 106.67107152938843, 109.79726362228394, 112.89790844917297, 116.01703381538391, 119.13936138153076, 122.24703335762024, 125.3530170917511, 128.4655203819275, 131.60604619979858, 134.7142345905304, 137.8163092136383, 140.9409523010254, 144.06522369384766, 147.19932889938354, 150.31482553482056, 153.45426869392395, 156.60268115997314, 159.7372908592224, 162.85434556007385, 166.00344228744507, 169.09723782539368, 172.23464608192444, 175.35742473602295, 178.47722744941711, 181.59724044799805, 184.73695921897888, 187.85209345817566, 190.980220079422, 194.0940659046173, 197.21887588500977, 200.34067606925964, 203.46736192703247, 206.58075046539307, 209.7134759426117, 212.8284478187561, 215.95961570739746, 219.05800080299377, 222.15899109840393, 225.2504105567932, 228.37404370307922, 231.45221543312073, 234.5606825351715, 237.66454648971558, 240.73277497291565, 243.80193758010864, 246.87829160690308, 249.95831656455994, 253.05714416503906, 256.1559591293335, 259.25396251678467, 262.36156153678894, 265.4876549243927, 268.5927834510803, 271.6981608867645, 274.8113634586334, 277.9279022216797, 281.035053730011, 284.1610224246979, 287.28430438041687, 290.405611038208, 293.51259779930115, 296.62190890312195, 299.74413084983826, 302.85972356796265, 306.0077986717224, 309.1412687301636, 312.26705169677734, 315.38077664375305]
[22.3225, 26.38, 29.8825, 32.54, 34.19, 35.0275, 37.6375, 38.975, 39.9325, 40.42, 40.2875, 41.225, 41.725, 42.1, 43.045, 43.29, 44.18, 45.47, 45.4, 45.275, 45.5125, 45.5075, 45.955, 46.53, 46.96, 47.24, 47.5925, 47.605, 47.5375, 47.785, 47.9675, 48.7325, 47.84, 48.47, 48.43, 48.25, 49.105, 48.8075, 48.69, 48.96, 48.15, 48.7875, 48.07, 48.835, 48.715, 49.3225, 48.485, 48.7975, 49.575, 49.3, 49.5825, 49.7225, 49.7275, 49.84, 49.585, 48.9825, 49.0475, 49.6975, 49.62, 50.26, 49.825, 49.195, 50.3, 48.9425, 49.29, 50.615, 50.7225, 50.96, 49.935, 50.705, 50.2925, 49.6225, 49.875, 49.9525, 50.235, 50.515, 50.43, 50.3, 50.0925, 50.7525, 50.685, 50.2175, 50.83, 50.725, 50.4325, 50.685, 50.33, 50.9825, 50.9, 50.53, 50.795, 50.905, 51.5275, 50.6275, 50.4925, 50.775, 51.1575, 50.5, 49.835, 50.8825, 51.37]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

8511.822862386703
[1.536494255065918, 2.8441474437713623, 4.147979736328125, 5.4704296588897705, 6.787309646606445, 8.107546329498291, 9.415691137313843, 10.720038175582886, 12.023712158203125, 13.33666181564331, 14.647384643554688, 15.973679065704346, 17.291126012802124, 18.61319661140442, 19.92956829071045, 21.258206129074097, 22.57543420791626, 23.833186626434326, 25.13597273826599, 26.40947699546814, 27.668770790100098, 28.93672204017639, 30.202754497528076, 31.528791427612305, 32.851622104644775, 34.17412090301514, 35.497366189956665, 36.821112394332886, 38.146554946899414, 39.472700357437134, 40.79137587547302, 42.104170083999634, 43.41874957084656, 44.786866664886475, 46.1008095741272, 47.397380352020264, 48.702818155288696, 50.00516104698181, 51.30347204208374, 52.4342520236969, 53.56792950630188, 54.701032400131226, 55.83570170402527, 56.96623349189758, 58.09909248352051, 59.232168674468994, 60.37393116950989, 61.505433082580566, 62.638630867004395, 63.78113532066345, 64.91675043106079, 66.05036807060242, 67.18156456947327, 68.3127646446228, 69.44355344772339, 70.57434630393982, 71.70961236953735, 72.84779810905457, 73.98166275024414, 75.11250638961792, 76.40184426307678, 77.69412469863892, 78.97775268554688, 80.28442049026489, 81.58721876144409, 82.8927891254425, 84.1874418258667, 85.48524975776672, 86.7841866016388, 88.08459854125977, 89.41636681556702, 90.71981430053711, 92.01081562042236, 93.30609083175659, 94.59989786148071, 95.90216565132141, 97.21155285835266, 98.5072283744812, 99.80181407928467, 101.14076900482178, 102.43490433692932, 103.74066758155823, 105.05892825126648, 106.36528658866882, 107.67023968696594, 108.96433448791504, 110.2587947845459, 111.54630708694458, 112.67903470993042, 113.81421208381653, 114.94395089149475, 116.07601881027222, 117.2090539932251, 118.33640909194946, 119.46630764007568, 120.59809756278992, 121.72826910018921, 122.8636429309845, 123.99015951156616, 125.11376905441284, 126.26054906845093, 127.38557076454163, 128.5119662284851, 129.63653111457825, 130.78834581375122, 131.9178283214569, 133.20529508590698, 134.48638367652893, 135.60767364501953, 136.73786568641663, 137.86733627319336, 139.0333697795868, 140.161297082901, 141.29520845413208, 142.42220401763916, 143.72467136383057, 145.02896857261658, 146.32993698120117, 147.63269805908203, 148.93000411987305, 150.23445343971252, 151.53219652175903, 152.8256812095642, 154.12369585037231, 155.42259335517883, 156.72547316551208, 158.02842211723328, 159.33097577095032, 160.63313388824463, 161.94130182266235, 163.24094557762146, 164.5465371608734, 165.84789395332336, 167.1503758430481, 168.46542382240295, 169.75829195976257, 171.06279373168945, 172.36431574821472, 173.67534685134888, 174.98763060569763, 176.30824947357178, 177.62406420707703, 178.92827916145325, 180.25503778457642, 181.57226729393005, 182.8747799396515, 184.18368005752563, 185.49934673309326, 186.8130145072937, 188.10880184173584, 189.41483879089355, 190.73135614395142, 192.0326292514801, 193.3449068069458, 194.66023445129395, 195.96618843078613, 197.29121923446655, 198.59328746795654, 199.91705679893494, 201.23747301101685, 202.54845118522644, 203.86655044555664, 205.1738612651825, 206.49444150924683, 207.80069255828857, 209.09605884552002, 210.42354202270508, 211.72896146774292, 213.04459595680237, 214.35717368125916, 215.66204690933228, 216.96109342575073, 218.27440977096558, 219.60039281845093, 220.90322041511536, 222.0414752960205, 223.18993282318115, 224.32430458068848, 225.45921397209167, 226.59602069854736, 227.7261667251587, 228.87079977989197, 230.0230906009674, 231.1632857322693, 232.2988579273224, 233.4388620853424, 234.58494806289673, 235.72284841537476, 236.85320115089417, 237.9920518398285, 239.13616228103638, 240.27797031402588, 241.41044282913208, 242.5502734184265, 243.69017958641052, 244.8219838142395, 245.94961857795715, 247.0997667312622, 248.3892204761505, 249.69473004341125, 250.9899127483368, 252.28822469711304, 253.59399819374084, 254.89498448371887, 256.19083619117737, 257.33192920684814, 258.4654150009155, 259.60165333747864, 260.7373695373535, 261.87186098098755, 263.00863909721375, 264.14071321487427, 265.2751851081848, 266.4121518135071, 267.54679799079895, 268.67920303344727, 269.81427359580994, 270.94811511039734, 272.0872781276703, 273.2264106273651, 274.36075043678284, 275.49741315841675, 276.6380081176758, 277.77421259880066, 278.90766072273254, 280.04980754852295, 281.1882691383362, 282.3223898410797, 283.45340633392334, 284.587917804718, 285.7181398868561, 286.8506135940552, 287.980295419693, 289.11872243881226, 290.2596778869629, 291.3973169326782, 292.5236828327179, 293.6481120586395, 294.76851892471313, 295.89084029197693, 297.0169789791107, 298.1330554485321, 299.2570834159851, 300.3743143081665, 301.49648427963257, 302.61800622940063, 303.7306156158447, 304.8444654941559, 305.9600462913513, 307.08204650878906, 308.20448994636536, 309.3265264034271, 310.4427251815796, 311.56107568740845, 312.6801509857178, 313.7992043495178, 314.92015957832336, 316.0358533859253, 317.15184450149536, 318.2709276676178, 319.3886013031006, 320.50452709198, 321.6209990978241, 322.7368485927582, 323.8539671897888, 324.9701852798462, 326.08923149108887, 327.36311197280884, 328.6432797908783, 329.9097955226898, 331.181125164032, 332.4512197971344, 333.7268989086151, 335.0079264640808, 336.29453778266907, 337.5779049396515, 338.85915446281433, 340.1420478820801, 341.42553782463074, 342.7071120738983, 343.9915544986725, 345.27061128616333, 346.3851537704468, 347.497695684433, 348.6100718975067, 349.72536611557007, 350.8357219696045, 351.9496409893036, 353.06003046035767, 354.16905546188354, 355.27776551246643, 356.38718581199646, 357.49633622169495, 358.6079351902008, 359.72220635414124, 360.8317084312439, 361.94117736816406, 363.04803371429443, 364.1602509021759, 365.26891803741455, 367.4866783618927]
[10.0325, 10.0825, 10.165, 10.18, 10.3075, 10.35, 10.465, 10.5225, 10.5075, 10.45, 10.48, 10.405, 10.535, 10.5925, 10.6725, 10.6775, 10.5975, 10.61, 10.6175, 10.6575, 10.8075, 10.8425, 10.9175, 10.95, 10.965, 11.165, 11.5, 11.56, 11.475, 11.3475, 11.32, 11.3325, 11.5375, 11.66, 11.725, 11.765, 11.745, 11.98, 12.085, 12.0275, 12.19, 12.2425, 12.39, 12.535, 12.53, 12.5675, 12.68, 12.9175, 13.2025, 13.4925, 13.7825, 14.165, 14.3075, 14.4475, 14.525, 14.615, 14.7025, 15.03, 15.4525, 15.2575, 15.3575, 15.64, 15.92, 16.105, 16.035, 16.2325, 16.32, 16.1725, 16.23, 16.3275, 16.3175, 16.44, 16.5525, 16.58, 16.6, 16.6025, 16.8625, 17.08, 16.8375, 16.83, 16.975, 16.8775, 16.8025, 17.005, 17.045, 17.1975, 17.195, 17.2525, 17.6425, 17.6725, 17.91, 18.025, 18.065, 17.96, 17.9275, 17.8475, 17.745, 17.485, 17.3775, 17.1575, 16.8975, 16.61, 16.4975, 16.2925, 16.6275, 16.915, 17.355, 17.55, 17.74, 17.8125, 17.7, 18.2225, 18.4425, 18.6575, 18.8025, 19.065, 19.075, 19.16, 19.315, 19.45, 19.67, 19.8225, 19.975, 20.1675, 20.43, 20.475, 20.5075, 20.5675, 20.67, 20.5025, 20.59, 20.43, 20.5075, 20.3925, 20.3675, 20.595, 20.5925, 20.7975, 20.8275, 20.7725, 20.815, 20.775, 20.74, 21.075, 21.04, 21.1325, 21.07, 21.1975, 21.09, 21.1, 21.2175, 21.56, 21.4275, 21.3575, 21.32, 21.22, 21.235, 21.215, 21.2025, 21.2475, 21.3425, 21.2275, 21.2525, 21.21, 21.1625, 21.05, 21.1475, 21.3775, 21.32, 21.42, 21.56, 21.5525, 21.62, 21.81, 21.935, 22.1325, 22.0975, 21.965, 21.8775, 21.805, 21.8075, 21.835, 21.9175, 21.7575, 21.93, 22.0575, 22.01, 22.0, 21.9025, 21.885, 21.8525, 21.925, 21.815, 21.835, 21.9375, 21.9325, 21.915, 21.855, 21.8725, 21.75, 21.7225, 21.705, 21.68, 21.6025, 21.5775, 21.52, 21.6375, 21.605, 21.7625, 21.8675, 22.0525, 22.25, 22.2525, 22.1375, 21.34, 17.71, 14.1925, 12.3625, 11.1475, 11.1475, 11.1475, 11.1475, 11.1475, 11.1475, 11.1475, 11.1475, 10.57, 10.57, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.431, Test loss: 1.926, Test accuracy: 57.06
Final Round, Global train loss: 0.431, Global test loss: 2.361, Global test accuracy: 41.89
Average accuracy final 10 rounds: 56.869499999999995 

Average global accuracy final 10 rounds: 41.857 

3766.8201570510864
[1.4019052982330322, 2.8038105964660645, 3.970139980316162, 5.13646936416626, 6.30764365196228, 7.478817939758301, 8.646282434463501, 9.813746929168701, 10.977843999862671, 12.14194107055664, 13.309732675552368, 14.477524280548096, 15.646687030792236, 16.815849781036377, 17.98593235015869, 19.156014919281006, 20.327718496322632, 21.499422073364258, 22.673827409744263, 23.848232746124268, 25.020301342010498, 26.19236993789673, 27.35956859588623, 28.526767253875732, 29.69344925880432, 30.86013126373291, 32.03506779670715, 33.2100043296814, 34.39239311218262, 35.57478189468384, 36.75466465950012, 37.934547424316406, 39.1150963306427, 40.295645236968994, 41.474524974823, 42.653404712677, 43.832003355026245, 45.01060199737549, 46.19065260887146, 47.37070322036743, 48.54936456680298, 49.728025913238525, 50.909175872802734, 52.09032583236694, 53.27357053756714, 54.456815242767334, 55.63516926765442, 56.813523292541504, 57.996837854385376, 59.18015241622925, 60.36315941810608, 61.54616641998291, 62.728527307510376, 63.91088819503784, 65.0885763168335, 66.26626443862915, 67.44935750961304, 68.63245058059692, 69.81478428840637, 70.99711799621582, 72.18615746498108, 73.37519693374634, 74.56477189064026, 75.75434684753418, 76.93809342384338, 78.12184000015259, 79.29902243614197, 80.47620487213135, 81.65851926803589, 82.84083366394043, 84.02674007415771, 85.212646484375, 86.39305996894836, 87.57347345352173, 88.75289225578308, 89.93231105804443, 91.11184811592102, 92.29138517379761, 93.46289300918579, 94.63440084457397, 95.81652975082397, 96.99865865707397, 98.1781256198883, 99.35759258270264, 100.53804993629456, 101.71850728988647, 102.90224385261536, 104.08598041534424, 105.26310086250305, 106.44022130966187, 107.59697580337524, 108.75373029708862, 109.91065526008606, 111.0675802230835, 112.2179024219513, 113.36822462081909, 114.53412508964539, 115.70002555847168, 116.8684630393982, 118.0369005203247, 119.03935766220093, 120.04181480407715, 121.04736065864563, 122.05290651321411, 123.05761694908142, 124.06232738494873, 125.06459355354309, 126.06685972213745, 127.0677056312561, 128.06855154037476, 129.07013034820557, 130.07170915603638, 131.07140851020813, 132.07110786437988, 133.06854391098022, 134.06597995758057, 135.23131227493286, 136.39664459228516, 137.56314873695374, 138.72965288162231, 139.8931884765625, 141.05672407150269, 142.22286987304688, 143.38901567459106, 144.48677372932434, 145.58453178405762, 146.74049592018127, 147.89646005630493, 149.06929421424866, 150.24212837219238, 151.40902185440063, 152.5759153366089, 153.74713730812073, 154.91835927963257, 156.09872221946716, 157.27908515930176, 158.44917511940002, 159.6192650794983, 160.78360843658447, 161.94795179367065, 163.11377668380737, 164.2796015739441, 165.4502673149109, 166.62093305587769, 167.79247403144836, 168.96401500701904, 170.14279055595398, 171.32156610488892, 172.49180722236633, 173.66204833984375, 174.8366732597351, 176.01129817962646, 177.1877794265747, 178.36426067352295, 179.54783940315247, 180.73141813278198, 181.89787483215332, 183.06433153152466, 184.23714995384216, 185.40996837615967, 186.58384704589844, 187.7577257156372, 188.92227506637573, 190.08682441711426, 191.2573311328888, 192.42783784866333, 193.59588384628296, 194.7639298439026, 195.9305477142334, 197.0971655845642, 198.2670440673828, 199.43692255020142, 200.6020872592926, 201.7672519683838, 202.7765760421753, 203.7859001159668, 204.79416394233704, 205.80242776870728, 206.81278681755066, 207.82314586639404, 208.8311686515808, 209.83919143676758, 210.84681296348572, 211.85443449020386, 212.86363887786865, 213.87284326553345, 214.88204979896545, 215.89125633239746, 216.898832321167, 217.90640830993652, 218.91602730751038, 219.92564630508423, 220.9358446598053, 221.94604301452637, 222.95564818382263, 223.9652533531189, 224.97288250923157, 225.98051166534424, 226.98892974853516, 227.99734783172607, 230.01526045799255, 232.03317308425903]
[22.8175, 22.8175, 25.07, 25.07, 28.3, 28.3, 30.4525, 30.4525, 31.66, 31.66, 34.22, 34.22, 36.0875, 36.0875, 37.265, 37.265, 38.585, 38.585, 39.6875, 39.6875, 40.89, 40.89, 41.33, 41.33, 41.885, 41.885, 42.9975, 42.9975, 44.1325, 44.1325, 44.8475, 44.8475, 45.2475, 45.2475, 46.175, 46.175, 46.71, 46.71, 47.01, 47.01, 47.305, 47.305, 48.345, 48.345, 49.1425, 49.1425, 49.3925, 49.3925, 49.445, 49.445, 49.545, 49.545, 49.72, 49.72, 50.3825, 50.3825, 50.5825, 50.5825, 50.705, 50.705, 50.99, 50.99, 50.9975, 50.9975, 51.7625, 51.7625, 51.625, 51.625, 51.895, 51.895, 51.955, 51.955, 52.0, 52.0, 52.1975, 52.1975, 52.57, 52.57, 53.215, 53.215, 53.1675, 53.1675, 53.08, 53.08, 53.135, 53.135, 53.2625, 53.2625, 53.165, 53.165, 53.455, 53.455, 53.645, 53.645, 53.695, 53.695, 53.775, 53.775, 53.9875, 53.9875, 54.42, 54.42, 54.795, 54.795, 55.05, 55.05, 54.59, 54.59, 54.405, 54.405, 54.5525, 54.5525, 54.16, 54.16, 54.63, 54.63, 54.7375, 54.7375, 54.695, 54.695, 54.6525, 54.6525, 54.625, 54.625, 54.5675, 54.5675, 54.76, 54.76, 55.0775, 55.0775, 55.4275, 55.4275, 55.475, 55.475, 55.355, 55.355, 55.47, 55.47, 55.4025, 55.4025, 55.2825, 55.2825, 55.3925, 55.3925, 55.3825, 55.3825, 55.62, 55.62, 55.86, 55.86, 55.795, 55.795, 55.8225, 55.8225, 55.8725, 55.8725, 55.695, 55.695, 56.0525, 56.0525, 56.59, 56.59, 56.42, 56.42, 56.5625, 56.5625, 56.4675, 56.4675, 56.3925, 56.3925, 56.415, 56.415, 56.675, 56.675, 56.545, 56.545, 56.7375, 56.7375, 56.6825, 56.6825, 56.7275, 56.7275, 56.96, 56.96, 56.835, 56.835, 56.6775, 56.6775, 56.9, 56.9, 56.75, 56.75, 57.055, 57.055, 56.96, 56.96, 56.9075, 56.9075, 56.9225, 56.9225, 57.0575, 57.0575]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.516, Test loss: 1.328, Test accuracy: 61.53
Average accuracy final 10 rounds: 61.2435 

2751.312468767166
[1.3693809509277344, 2.7387619018554688, 3.861083745956421, 4.983405590057373, 6.103696584701538, 7.223987579345703, 8.346612453460693, 9.469237327575684, 10.59262752532959, 11.716017723083496, 12.835236072540283, 13.95445442199707, 15.065553426742554, 16.176652431488037, 17.286686182022095, 18.396719932556152, 19.520000457763672, 20.64328098297119, 21.770265340805054, 22.897249698638916, 24.02309823036194, 25.14894676208496, 26.276208639144897, 27.403470516204834, 28.52935028076172, 29.655230045318604, 30.784271001815796, 31.91331195831299, 33.03894376754761, 34.16457557678223, 35.2861442565918, 36.40771293640137, 37.53316831588745, 38.658623695373535, 39.78567934036255, 40.91273498535156, 42.039294719696045, 43.16585445404053, 44.289292097091675, 45.41272974014282, 46.54316568374634, 47.67360162734985, 48.80189847946167, 49.930195331573486, 51.053510904312134, 52.17682647705078, 53.303152084350586, 54.42947769165039, 55.54971218109131, 56.66994667053223, 57.78954839706421, 58.90915012359619, 60.03199553489685, 61.15484094619751, 62.279417753219604, 63.4039945602417, 64.52370953559875, 65.64342451095581, 66.77133178710938, 67.89923906326294, 69.02299332618713, 70.14674758911133, 71.27809882164001, 72.4094500541687, 73.53190851211548, 74.65436697006226, 75.78024888038635, 76.90613079071045, 78.06695890426636, 79.22778701782227, 80.35265016555786, 81.47751331329346, 82.60080623626709, 83.72409915924072, 84.84626126289368, 85.96842336654663, 87.09343910217285, 88.21845483779907, 89.341468334198, 90.46448183059692, 91.59832501411438, 92.73216819763184, 93.85410737991333, 94.97604656219482, 96.10624742507935, 97.23644828796387, 98.35179710388184, 99.4671459197998, 100.58773708343506, 101.70832824707031, 102.78697562217712, 103.86562299728394, 104.99563074111938, 106.12563848495483, 107.24572777748108, 108.36581707000732, 109.50073075294495, 110.63564443588257, 111.7625641822815, 112.88948392868042, 114.0185604095459, 115.14763689041138, 116.27587103843689, 117.4041051864624, 118.52773427963257, 119.65136337280273, 120.78077459335327, 121.91018581390381, 123.03982210159302, 124.16945838928223, 125.30128049850464, 126.43310260772705, 127.56044435501099, 128.68778610229492, 129.81283855438232, 130.93789100646973, 132.0658824443817, 133.1938738822937, 134.3177149295807, 135.44155597686768, 136.56636810302734, 137.691180229187, 138.8152985572815, 139.93941688537598, 141.07053065299988, 142.20164442062378, 143.32813334465027, 144.45462226867676, 145.58382940292358, 146.7130365371704, 147.8377640247345, 148.96249151229858, 150.0923445224762, 151.2221975326538, 152.35643553733826, 153.4906735420227, 154.61616849899292, 155.74166345596313, 156.86325335502625, 157.98484325408936, 159.11335468292236, 160.24186611175537, 161.37293124198914, 162.5039963722229, 163.6382372379303, 164.7724781036377, 165.8995463848114, 167.0266146659851, 168.15380930900574, 169.28100395202637, 170.4119963645935, 171.54298877716064, 172.67660689353943, 173.8102250099182, 174.89442896842957, 175.97863292694092, 177.06748032569885, 178.1563277244568, 179.2337532043457, 180.31117868423462, 181.44199538230896, 182.5728120803833, 183.69657969474792, 184.82034730911255, 185.94413805007935, 187.06792879104614, 188.19066739082336, 189.3134059906006, 190.43685483932495, 191.56030368804932, 192.69890213012695, 193.8375005722046, 194.9604151248932, 196.0833296775818, 197.2149519920349, 198.34657430648804, 199.47162127494812, 200.5966682434082, 201.7151234149933, 202.83357858657837, 203.96152067184448, 205.0894627571106, 206.21288967132568, 207.33631658554077, 208.46200561523438, 209.58769464492798, 210.71477389335632, 211.84185314178467, 212.97240042686462, 214.10294771194458, 215.22785639762878, 216.352765083313, 217.47605276107788, 218.59934043884277, 219.72442293167114, 220.8495054244995, 221.97963500022888, 223.10976457595825, 224.23639726638794, 225.36302995681763, 227.32978463172913, 229.29653930664062]
[17.16, 17.16, 21.17, 21.17, 23.8825, 23.8825, 25.635, 25.635, 28.4025, 28.4025, 30.3625, 30.3625, 32.895, 32.895, 33.9825, 33.9825, 35.5775, 35.5775, 37.0825, 37.0825, 38.7, 38.7, 40.1775, 40.1775, 41.68, 41.68, 43.555, 43.555, 44.0125, 44.0125, 44.945, 44.945, 45.7175, 45.7175, 46.2, 46.2, 46.73, 46.73, 47.005, 47.005, 48.09, 48.09, 48.5775, 48.5775, 49.505, 49.505, 49.5675, 49.5675, 50.6025, 50.6025, 51.4325, 51.4325, 51.7675, 51.7675, 51.675, 51.675, 51.9575, 51.9575, 51.875, 51.875, 52.8825, 52.8825, 52.4575, 52.4575, 53.51, 53.51, 53.36, 53.36, 53.6275, 53.6275, 54.505, 54.505, 54.9975, 54.9975, 55.4175, 55.4175, 55.29, 55.29, 55.7975, 55.7975, 55.0625, 55.0625, 56.0425, 56.0425, 55.7925, 55.7925, 56.5375, 56.5375, 56.6775, 56.6775, 56.5375, 56.5375, 56.87, 56.87, 57.42, 57.42, 58.105, 58.105, 58.12, 58.12, 58.33, 58.33, 58.3725, 58.3725, 58.4675, 58.4675, 58.595, 58.595, 58.545, 58.545, 58.37, 58.37, 58.56, 58.56, 58.57, 58.57, 58.305, 58.305, 58.8525, 58.8525, 59.06, 59.06, 58.835, 58.835, 58.4075, 58.4075, 59.0175, 59.0175, 58.92, 58.92, 59.5825, 59.5825, 59.08, 59.08, 59.89, 59.89, 60.24, 60.24, 60.595, 60.595, 59.8375, 59.8375, 60.145, 60.145, 60.19, 60.19, 59.78, 59.78, 60.295, 60.295, 60.265, 60.265, 60.0125, 60.0125, 60.5975, 60.5975, 60.6475, 60.6475, 60.655, 60.655, 60.9, 60.9, 60.9275, 60.9275, 60.7675, 60.7675, 60.86, 60.86, 61.08, 61.08, 61.165, 61.165, 60.9325, 60.9325, 60.9175, 60.9175, 61.1025, 61.1025, 60.875, 60.875, 61.1725, 61.1725, 61.22, 61.22, 60.8925, 60.8925, 61.2575, 61.2575, 61.205, 61.205, 60.86, 60.86, 61.4725, 61.4725, 61.7875, 61.7875, 61.2, 61.2, 61.3675, 61.3675, 61.53, 61.53]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.576, Test loss: 0.985, Test accuracy: 67.58
Average accuracy final 10 rounds: 66.87
2890.6669466495514
[1.6954216957092285, 3.390843391418457, 4.748420476913452, 6.105997562408447, 7.449983596801758, 8.793969631195068, 10.134435653686523, 11.474901676177979, 12.767045974731445, 14.059190273284912, 15.350731134414673, 16.642271995544434, 17.93394947052002, 19.225626945495605, 20.5541250705719, 21.882623195648193, 23.19308114051819, 24.503539085388184, 25.814303636550903, 27.125068187713623, 28.427993059158325, 29.730917930603027, 31.0334050655365, 32.33589220046997, 33.67974352836609, 35.02359485626221, 36.402496337890625, 37.78139781951904, 39.11125588417053, 40.44111394882202, 41.771294593811035, 43.10147523880005, 44.43796896934509, 45.77446269989014, 47.120556354522705, 48.46665000915527, 49.821486949920654, 51.176323890686035, 52.51737380027771, 53.858423709869385, 55.19658088684082, 56.534738063812256, 57.87108087539673, 59.2074236869812, 60.549336671829224, 61.891249656677246, 63.23366641998291, 64.57608318328857, 65.91502165794373, 67.25396013259888, 68.59502696990967, 69.93609380722046, 71.2757339477539, 72.61537408828735, 73.95817756652832, 75.30098104476929, 76.63903665542603, 77.97709226608276, 79.32956671714783, 80.68204116821289, 82.03960657119751, 83.39717197418213, 84.74254631996155, 86.08792066574097, 87.4309229850769, 88.77392530441284, 90.1245949268341, 91.47526454925537, 92.82697749137878, 94.1786904335022, 95.46858835220337, 96.75848627090454, 98.08613181114197, 99.4137773513794, 100.71320247650146, 102.01262760162354, 103.30858492851257, 104.60454225540161, 105.95408153533936, 107.3036208152771, 108.5918915271759, 109.8801622390747, 111.17917728424072, 112.47819232940674, 113.76779842376709, 115.05740451812744, 116.39177370071411, 117.72614288330078, 119.01223301887512, 120.29832315444946, 121.65419483184814, 123.01006650924683, 124.36092829704285, 125.71179008483887, 127.06172895431519, 128.4116678237915, 129.75673294067383, 131.10179805755615, 132.45914888381958, 133.816499710083, 135.1803777217865, 136.54425573349, 137.89700841903687, 139.24976110458374, 140.6140012741089, 141.97824144363403, 143.33124136924744, 144.68424129486084, 146.03119611740112, 147.3781509399414, 148.72270441055298, 150.06725788116455, 151.4061577320099, 152.74505758285522, 154.07911467552185, 155.41317176818848, 156.7548315525055, 158.0964913368225, 159.43363070487976, 160.770770072937, 162.11395001411438, 163.45712995529175, 164.79125452041626, 166.12537908554077, 167.47193908691406, 168.81849908828735, 170.16558361053467, 171.51266813278198, 172.85867953300476, 174.20469093322754, 175.57440185546875, 176.94411277770996, 178.30219721794128, 179.6602816581726, 181.0375702381134, 182.4148588180542, 183.76139211654663, 185.10792541503906, 186.47495555877686, 187.84198570251465, 189.21151423454285, 190.58104276657104, 191.9572069644928, 193.33337116241455, 194.6958041191101, 196.05823707580566, 197.41468143463135, 198.77112579345703, 200.12734484672546, 201.4835638999939, 202.84864377975464, 204.21372365951538, 205.58043837547302, 206.94715309143066, 208.2986342906952, 209.65011548995972, 211.00434684753418, 212.35857820510864, 213.71430492401123, 215.07003164291382, 216.43990063667297, 217.80976963043213, 219.16043376922607, 220.51109790802002, 221.8587236404419, 223.20634937286377, 224.56656527519226, 225.92678117752075, 227.28093957901, 228.63509798049927, 229.99600958824158, 231.3569211959839, 232.71940398216248, 234.08188676834106, 235.44072461128235, 236.79956245422363, 238.1680850982666, 239.53660774230957, 240.90608072280884, 242.2755537033081, 243.63560271263123, 244.99565172195435, 246.36515498161316, 247.73465824127197, 249.10239434242249, 250.470130443573, 251.8318099975586, 253.1934895515442, 254.5462749004364, 255.8990602493286, 257.25253677368164, 258.60601329803467, 259.9577534198761, 261.30949354171753, 262.6767683029175, 264.04404306411743, 265.3985493183136, 266.75305557250977, 268.1073715686798, 269.46168756484985, 271.5380177497864, 273.6143479347229]
[14.8925, 14.8925, 21.7875, 21.7875, 23.7825, 23.7825, 25.8575, 25.8575, 28.7125, 28.7125, 30.4825, 30.4825, 32.86, 32.86, 34.6, 34.6, 36.2675, 36.2675, 37.4825, 37.4825, 38.9425, 38.9425, 40.5275, 40.5275, 42.535, 42.535, 43.275, 43.275, 44.4775, 44.4775, 45.0275, 45.0275, 47.0275, 47.0275, 47.815, 47.815, 49.0675, 49.0675, 48.9025, 48.9025, 49.905, 49.905, 50.4475, 50.4475, 51.2875, 51.2875, 52.2025, 52.2025, 52.435, 52.435, 53.1925, 53.1925, 53.42, 53.42, 54.405, 54.405, 54.33, 54.33, 55.0125, 55.0125, 55.56, 55.56, 55.5975, 55.5975, 56.3125, 56.3125, 56.4875, 56.4875, 56.225, 56.225, 57.195, 57.195, 57.205, 57.205, 57.575, 57.575, 58.0975, 58.0975, 58.3425, 58.3425, 58.9925, 58.9925, 59.32, 59.32, 59.3925, 59.3925, 59.8075, 59.8075, 60.035, 60.035, 59.92, 59.92, 60.8125, 60.8125, 60.6025, 60.6025, 61.535, 61.535, 61.075, 61.075, 61.185, 61.185, 61.095, 61.095, 61.9325, 61.9325, 62.3225, 62.3225, 63.015, 63.015, 62.5975, 62.5975, 62.41, 62.41, 62.7925, 62.7925, 63.4575, 63.4575, 63.85, 63.85, 63.555, 63.555, 63.66, 63.66, 63.745, 63.745, 63.49, 63.49, 63.5475, 63.5475, 64.145, 64.145, 64.825, 64.825, 64.5425, 64.5425, 64.7, 64.7, 64.7625, 64.7625, 65.17, 65.17, 65.37, 65.37, 65.1575, 65.1575, 65.4375, 65.4375, 65.655, 65.655, 66.0025, 66.0025, 65.7675, 65.7675, 65.8275, 65.8275, 66.2125, 66.2125, 66.32, 66.32, 66.12, 66.12, 66.5175, 66.5175, 66.605, 66.605, 66.0025, 66.0025, 66.1825, 66.1825, 66.31, 66.31, 65.8325, 65.8325, 66.34, 66.34, 66.2225, 66.2225, 65.9575, 65.9575, 66.45, 66.45, 67.085, 67.085, 66.7875, 66.7875, 66.5425, 66.5425, 66.275, 66.275, 66.955, 66.955, 67.425, 67.425, 67.12, 67.12, 66.9325, 66.9325, 67.1275, 67.1275, 67.58, 67.58]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.467, Test loss: 2.128, Test accuracy: 42.92
Average accuracy final 10 rounds: 41.994249999999994
2747.1748592853546
[3.118896961212158, 6.047323703765869, 8.962646007537842, 11.93749737739563, 14.908758640289307, 17.888784885406494, 20.8774516582489, 23.531768321990967, 26.203502655029297, 29.174450874328613, 32.154229402542114, 35.13469171524048, 38.125659465789795, 41.09543013572693, 44.066123247146606, 47.04986810684204, 50.03478813171387, 53.012311697006226, 55.966267585754395, 58.929336071014404, 61.895408153533936, 64.86822009086609, 67.83535861968994, 70.80949449539185, 73.7786602973938, 76.73951029777527, 79.71956372261047, 82.69202184677124, 85.66664028167725, 88.6363251209259, 91.60839056968689, 94.57943844795227, 97.56076407432556, 100.53503489494324, 103.50625443458557, 106.47484421730042, 109.30116295814514, 111.9809558391571, 114.66895723342896, 117.36960029602051, 120.05492949485779, 122.734783411026, 125.42882657051086, 128.16635870933533, 130.88910841941833, 133.6281726360321, 136.35394430160522, 139.0862283706665, 141.80748963356018, 144.56067848205566, 147.29523181915283, 150.02285766601562, 152.75558805465698, 155.48269963264465, 158.21542525291443, 160.9481828212738, 163.67360711097717, 166.41310167312622, 169.14411330223083, 171.87492609024048, 174.60412669181824, 177.34423518180847, 180.07791018486023, 182.80571007728577, 185.54059386253357, 188.27841687202454, 191.0124225616455, 193.74718475341797, 196.47612476348877, 199.20952987670898, 201.93980741500854, 204.6764621734619, 207.40421652793884, 210.12612915039062, 212.86228966712952, 215.55484175682068, 218.29606771469116, 220.99696946144104, 223.73098945617676, 226.46436214447021, 229.1920735836029, 231.91092443466187, 234.64681673049927, 237.37052273750305, 240.09626626968384, 242.82514262199402, 245.55311608314514, 248.28569221496582, 251.01371574401855, 253.74834084510803, 256.48142671585083, 259.2078380584717, 261.93917536735535, 264.6800434589386, 267.40667057037354, 270.1291706562042, 272.86636900901794, 275.5981066226959, 278.325884103775, 281.06196784973145, 283.798953294754]
[20.9425, 24.1875, 26.1425, 28.37, 30.56, 32.0025, 32.625, 34.3075, 33.715, 35.16, 34.65, 34.965, 36.2475, 35.9725, 36.455, 36.94, 37.7575, 38.01, 37.6225, 38.66, 37.975, 39.07, 37.8225, 38.4375, 39.0525, 38.9475, 39.5, 39.86, 39.6025, 40.4275, 40.3775, 40.805, 39.715, 39.47, 40.0575, 40.0725, 40.0175, 41.4425, 39.9725, 39.7825, 40.4325, 40.5225, 40.45, 42.0575, 41.2475, 40.1725, 40.9575, 40.7875, 40.77, 41.0675, 41.5625, 41.7225, 41.165, 40.4625, 41.535, 41.095, 41.205, 41.2175, 41.795, 41.2675, 41.9375, 41.035, 42.4075, 41.7775, 42.765, 41.6575, 41.23, 41.1125, 41.79, 40.945, 42.91, 42.3425, 42.8175, 41.8825, 41.2075, 42.05, 42.135, 42.9875, 40.9275, 41.75, 42.49, 41.485, 41.635, 42.44, 40.4675, 41.8475, 41.245, 41.8575, 41.31, 41.265, 42.03, 42.5825, 41.5275, 41.615, 42.7475, 42.905, 41.5, 41.6475, 41.175, 42.2125, 42.925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

8265.518300294876
[1.520085096359253, 2.836699962615967, 4.151113510131836, 5.4602086544036865, 6.794298410415649, 8.12129521369934, 9.458529472351074, 10.788928747177124, 12.119317293167114, 13.445854663848877, 14.778692483901978, 16.111433744430542, 17.442452430725098, 18.770874500274658, 20.092725038528442, 21.25323462486267, 22.408291816711426, 23.56707787513733, 24.722899198532104, 25.877843379974365, 27.034082889556885, 28.190402269363403, 29.34356927871704, 30.49909210205078, 31.651632070541382, 32.80010390281677, 33.947200775146484, 35.09439468383789, 36.23727989196777, 37.387213945388794, 38.52877593040466, 39.67848181724548, 40.81857752799988, 41.9735062122345, 43.11172080039978, 44.26024270057678, 45.39847493171692, 46.54915928840637, 47.69601559638977, 48.84306716918945, 49.99011588096619, 51.140387296676636, 52.288633823394775, 53.43347096443176, 54.5828001499176, 55.72958302497864, 56.87835717201233, 58.01650333404541, 59.16241669654846, 60.308658838272095, 61.45161962509155, 62.5966215133667, 63.743969202041626, 64.88819193840027, 66.03173780441284, 67.17373132705688, 68.31118369102478, 69.45331072807312, 70.5999686717987, 71.7427237033844, 72.89055609703064, 74.03457450866699, 75.17633104324341, 76.31767535209656, 77.46242427825928, 78.59548807144165, 79.7369019985199, 80.87300133705139, 82.01254606246948, 83.14551568031311, 84.2871561050415, 85.4258074760437, 86.56791996955872, 87.70518612861633, 88.84873223304749, 89.9964029788971, 91.14238667488098, 92.29031682014465, 93.43446445465088, 94.58063125610352, 95.72549271583557, 96.86719822883606, 98.01953840255737, 99.15744733810425, 100.3039071559906, 101.44952917098999, 102.5949501991272, 103.73904085159302, 104.88599610328674, 106.03110933303833, 107.17935538291931, 108.32423543930054, 109.46807670593262, 110.6050055027008, 111.7485740184784, 112.89556813240051, 114.03900074958801, 115.18397879600525, 116.3279938697815, 117.4718930721283, 118.61535835266113, 119.76056170463562, 120.89391899108887, 122.02831935882568, 123.16240906715393, 124.29885172843933, 125.42721962928772, 126.56416893005371, 127.69931483268738, 128.83517146110535, 129.97009778022766, 131.10356426239014, 132.23671674728394, 133.3699550628662, 134.502836227417, 135.64107942581177, 136.77890348434448, 137.91526794433594, 139.04827117919922, 140.1901412010193, 141.33179187774658, 142.47703099250793, 143.6126959323883, 144.75742292404175, 145.90082955360413, 147.04804849624634, 148.1923599243164, 149.33547377586365, 150.48764944076538, 151.62607216835022, 152.76893043518066, 153.9082851409912, 155.05346131324768, 156.18362975120544, 157.33107829093933, 158.46882104873657, 159.6178216934204, 160.75239539146423, 161.89469027519226, 163.02894186973572, 164.16430616378784, 165.29629755020142, 166.43356466293335, 167.5638289451599, 168.69814085960388, 169.83834791183472, 170.9846224784851, 172.12193512916565, 173.2759828567505, 174.41755747795105, 175.55382990837097, 176.70390915870667, 177.84854006767273, 178.98851442337036, 180.12522554397583, 181.26704716682434, 182.40160250663757, 183.5415279865265, 184.6855034828186, 185.8218696117401, 186.9513533115387, 188.0744149684906, 189.20764541625977, 190.33551406860352, 191.4638922214508, 192.59555101394653, 193.7223436832428, 194.85236501693726, 195.98293590545654, 197.11373257637024, 198.24572014808655, 199.36749958992004, 200.49504733085632, 201.62616610527039, 202.7482831478119, 203.88051056861877, 205.01687693595886, 206.13914370536804, 207.2724244594574, 208.4029357433319, 209.56977152824402, 210.7006676197052, 211.82893800735474, 212.9551751613617, 214.07719016075134, 215.1989471912384, 216.3205029964447, 217.4501039981842, 218.57439517974854, 219.69768524169922, 220.8245656490326, 221.955717086792, 223.083988904953, 224.2178931236267, 225.34697222709656, 226.47070908546448, 227.5979516506195, 228.72780108451843, 229.85911178588867, 230.99283599853516, 232.11291766166687, 233.23627281188965, 234.35878992080688, 235.47861003875732, 236.61183285713196, 237.74533772468567, 238.8683750629425, 240.00202107429504, 241.12902927398682, 242.2551975250244, 243.38706970214844, 244.5245397090912, 245.65569496154785, 246.78018355369568, 247.91126108169556, 249.03689765930176, 250.16856956481934, 251.30135321617126, 252.43363428115845, 253.55950212478638, 254.69138026237488, 255.8230757713318, 256.9546024799347, 258.0819401741028, 259.2070028781891, 260.3377182483673, 261.4617350101471, 262.5913381576538, 263.7281754016876, 264.85189962387085, 265.975643157959, 267.09824323654175, 268.2259955406189, 269.3544590473175, 270.49076652526855, 271.6151854991913, 272.73552942276, 273.8564145565033, 274.98448395729065, 276.11223888397217, 277.24464893341064, 278.3690814971924, 279.49518179893494, 280.61703395843506, 281.7399435043335, 282.8717563152313, 284.001930475235, 285.12770438194275, 286.249144077301, 287.3696377277374, 288.49374294281006, 289.6223945617676, 290.7512352466583, 291.87170600891113, 292.9956171512604, 294.1189558506012, 295.2467200756073, 296.37330055236816, 297.5018594264984, 298.6296670436859, 299.7621591091156, 300.8906123638153, 302.02207469940186, 303.15619587898254, 304.2998011112213, 305.4432945251465, 306.58644580841064, 307.7188153266907, 308.85812425613403, 310.0004131793976, 311.13833236694336, 312.2726345062256, 313.40932631492615, 314.5535886287689, 315.6859521865845, 316.813547372818, 317.9482808113098, 319.09100222587585, 320.2341969013214, 321.3732559680939, 322.5090026855469, 323.64805030822754, 324.7918438911438, 325.93137550354004, 327.0643587112427, 328.20554065704346, 329.3463022708893, 330.48565912246704, 331.61818528175354, 332.757107257843, 333.8981235027313, 335.04249477386475, 336.17947268486023, 337.31883454322815, 338.4653298854828, 339.6031770706177, 340.743750333786, 341.8815448284149, 343.02504897117615, 344.16671204566956, 346.45104479789734]
[10.6025, 10.7075, 10.7875, 10.86, 10.9975, 11.1825, 11.29, 11.475, 11.5025, 11.62, 11.6525, 11.8075, 12.08, 12.1375, 12.1875, 12.1225, 12.335, 12.32, 12.395, 12.495, 12.52, 12.53, 12.5525, 12.675, 12.6875, 12.7625, 12.72, 12.8475, 12.9625, 12.9125, 12.7975, 12.6675, 12.7075, 12.7025, 12.6775, 12.465, 12.61, 12.365, 12.3625, 12.5325, 12.5925, 12.705, 12.9375, 13.19, 13.34, 13.2375, 13.3925, 13.41, 13.4425, 13.3275, 13.4125, 13.39, 13.365, 13.465, 13.425, 13.48, 13.525, 13.6, 13.7, 13.5925, 13.5375, 13.4875, 13.35, 13.29, 13.3425, 13.2525, 13.3075, 13.25, 13.3175, 13.4075, 13.625, 13.815, 13.925, 14.05, 14.24, 14.3225, 14.475, 14.4425, 14.395, 14.525, 14.495, 14.58, 14.6675, 14.96, 15.1725, 15.395, 15.7, 15.7775, 15.8175, 15.8875, 15.9675, 15.975, 16.0775, 16.04, 16.0875, 16.1425, 16.165, 16.23, 16.2225, 16.3025, 16.275, 16.2675, 16.43, 16.6025, 16.79, 17.0275, 17.43, 17.4625, 17.605, 17.5375, 17.8175, 18.0775, 18.275, 18.515, 18.485, 18.9, 18.9025, 19.2025, 19.1825, 19.1425, 19.13, 19.015, 19.09, 18.9875, 18.9575, 18.85, 18.96, 18.9875, 19.05, 19.185, 19.195, 19.1975, 19.13, 18.9325, 18.8825, 18.8475, 18.935, 18.8725, 18.6225, 18.5425, 18.445, 18.4425, 18.4525, 18.4175, 18.455, 18.34, 18.345, 18.5625, 18.6375, 18.6425, 18.61, 18.755, 18.8875, 18.8475, 18.835, 18.925, 18.9175, 19.015, 19.13, 19.2175, 19.3575, 19.3625, 19.275, 19.24, 19.0425, 18.9275, 19.005, 19.175, 19.13, 19.1575, 19.225, 19.225, 19.2275, 19.3925, 19.355, 19.3375, 19.4275, 19.635, 19.625, 19.595, 19.71, 19.6375, 19.7025, 19.815, 19.8, 19.735, 19.8375, 19.81, 19.7575, 19.7625, 19.78, 19.73, 19.8325, 19.7375, 19.805, 19.865, 19.8925, 20.04, 20.0, 19.865, 19.3625, 16.4525, 14.9, 12.87, 11.9625, 10.9275, 10.9275, 10.9275, 10.9275, 10.4675, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.478, Test loss: 2.007, Test accuracy: 55.20
Final Round, Global train loss: 0.478, Global test loss: 2.797, Global test accuracy: 29.68
Average accuracy final 10 rounds: 54.876999999999995 

Average global accuracy final 10 rounds: 28.704749999999997 

3843.357641220093
[1.4760682582855225, 2.952136516571045, 4.210308313369751, 5.468480110168457, 6.7303924560546875, 7.992304801940918, 9.252727270126343, 10.513149738311768, 11.766606092453003, 13.020062446594238, 14.283827781677246, 15.547593116760254, 16.808379888534546, 18.069166660308838, 19.341209650039673, 20.613252639770508, 21.870585203170776, 23.127917766571045, 24.387206554412842, 25.64649534225464, 26.90003204345703, 28.153568744659424, 29.405746698379517, 30.65792465209961, 31.915905237197876, 33.17388582229614, 34.43118953704834, 35.68849325180054, 36.9406464099884, 38.19279956817627, 39.44203519821167, 40.69127082824707, 41.9458384513855, 43.200406074523926, 44.45091438293457, 45.701422691345215, 46.943214893341064, 48.185007095336914, 49.43272376060486, 50.6804404258728, 51.932488441467285, 53.18453645706177, 54.43576741218567, 55.68699836730957, 56.93623685836792, 58.18547534942627, 59.43746590614319, 60.68945646286011, 61.939634799957275, 63.18981313705444, 64.44006991386414, 65.69032669067383, 66.93862724304199, 68.18692779541016, 69.43781232833862, 70.68869686126709, 71.94011974334717, 73.19154262542725, 74.44820952415466, 75.70487642288208, 76.95879626274109, 78.2127161026001, 79.46321105957031, 80.71370601654053, 81.96554446220398, 83.21738290786743, 84.47061491012573, 85.72384691238403, 86.97438859939575, 88.22493028640747, 89.47974634170532, 90.73456239700317, 91.98017168045044, 93.2257809638977, 94.476069688797, 95.72635841369629, 96.9756965637207, 98.22503471374512, 99.47606635093689, 100.72709798812866, 101.97327995300293, 103.2194619178772, 104.45350241661072, 105.68754291534424, 106.92648696899414, 108.16543102264404, 109.40877795219421, 110.65212488174438, 111.89798426628113, 113.14384365081787, 114.38813304901123, 115.63242244720459, 116.87993836402893, 118.12745428085327, 119.37461447715759, 120.62177467346191, 121.86399579048157, 123.10621690750122, 124.35350108146667, 125.60078525543213, 126.85216331481934, 128.10354137420654, 129.34751200675964, 130.59148263931274, 131.83769822120667, 133.0839138031006, 134.3276722431183, 135.571430683136, 136.81792497634888, 138.06441926956177, 139.3028450012207, 140.54127073287964, 141.7732253074646, 143.00517988204956, 144.23744130134583, 145.4697027206421, 146.70670866966248, 147.94371461868286, 149.1817924976349, 150.4198703765869, 151.6612937450409, 152.90271711349487, 154.13947939872742, 155.37624168395996, 156.61630964279175, 157.85637760162354, 159.0958161354065, 160.33525466918945, 161.57529973983765, 162.81534481048584, 164.05645966529846, 165.29757452011108, 166.53306913375854, 167.768563747406, 169.0059928894043, 170.2434220314026, 171.3636510372162, 172.48388004302979, 173.66846632957458, 174.85305261611938, 175.98708319664001, 177.12111377716064, 178.29784035682678, 179.47456693649292, 180.66868019104004, 181.86279344558716, 183.05911374092102, 184.25543403625488, 185.4569730758667, 186.65851211547852, 187.83772587776184, 189.01693964004517, 190.1975634098053, 191.37818717956543, 192.5641803741455, 193.7501735687256, 194.93038058280945, 196.1105875968933, 197.30550074577332, 198.50041389465332, 199.69312858581543, 200.88584327697754, 202.0756196975708, 203.26539611816406, 204.45397758483887, 205.64255905151367, 206.82478380203247, 208.00700855255127, 209.19097018241882, 210.37493181228638, 211.57441568374634, 212.7738995552063, 213.96444439888, 215.1549892425537, 216.34408712387085, 217.533185005188, 218.73649311065674, 219.9398012161255, 221.12307810783386, 222.30635499954224, 223.3446707725525, 224.38298654556274, 225.41338181495667, 226.4437770843506, 227.46979069709778, 228.49580430984497, 229.52993750572205, 230.56407070159912, 231.58948016166687, 232.61488962173462, 233.64856576919556, 234.6822419166565, 235.71352076530457, 236.74479961395264, 237.76789593696594, 238.79099225997925, 239.81972002983093, 240.84844779968262, 241.8753912448883, 242.902334690094, 244.95160102844238, 247.00086736679077]
[18.7975, 18.7975, 22.0525, 22.0525, 23.9075, 23.9075, 26.725, 26.725, 27.865, 27.865, 29.6525, 29.6525, 32.2825, 32.2825, 33.83, 33.83, 36.5075, 36.5075, 38.8825, 38.8825, 39.115, 39.115, 39.4325, 39.4325, 40.73, 40.73, 41.36, 41.36, 42.2925, 42.2925, 42.9, 42.9, 43.7025, 43.7025, 44.3575, 44.3575, 44.62, 44.62, 45.08, 45.08, 46.285, 46.285, 46.0775, 46.0775, 46.705, 46.705, 47.45, 47.45, 48.165, 48.165, 47.87, 47.87, 48.26, 48.26, 48.6525, 48.6525, 48.5875, 48.5875, 49.0475, 49.0475, 49.615, 49.615, 49.8475, 49.8475, 49.8125, 49.8125, 50.465, 50.465, 50.105, 50.105, 50.39, 50.39, 51.115, 51.115, 51.0325, 51.0325, 51.3625, 51.3625, 51.235, 51.235, 51.015, 51.015, 50.98, 50.98, 51.2325, 51.2325, 51.47, 51.47, 51.3575, 51.3575, 52.0375, 52.0375, 52.1725, 52.1725, 52.495, 52.495, 52.65, 52.65, 52.7675, 52.7675, 52.8, 52.8, 52.6625, 52.6625, 53.01, 53.01, 52.9225, 52.9225, 52.88, 52.88, 53.02, 53.02, 52.7725, 52.7725, 52.9475, 52.9475, 53.23, 53.23, 53.4675, 53.4675, 53.53, 53.53, 53.4175, 53.4175, 53.5975, 53.5975, 53.7875, 53.7875, 53.7775, 53.7775, 53.4925, 53.4925, 53.67, 53.67, 53.82, 53.82, 54.1525, 54.1525, 54.0975, 54.0975, 54.0775, 54.0775, 54.075, 54.075, 53.775, 53.775, 53.76, 53.76, 53.93, 53.93, 54.18, 54.18, 54.3, 54.3, 54.2275, 54.2275, 54.2325, 54.2325, 54.345, 54.345, 54.4775, 54.4775, 54.7675, 54.7675, 54.93, 54.93, 54.7125, 54.7125, 54.91, 54.91, 55.2175, 55.2175, 54.9075, 54.9075, 54.8575, 54.8575, 55.0475, 55.0475, 54.67, 54.67, 54.825, 54.825, 54.61, 54.61, 54.97, 54.97, 54.78, 54.78, 55.155, 55.155, 54.8075, 54.8075, 55.2475, 55.2475, 54.91, 54.91, 54.6425, 54.6425, 54.8225, 54.8225, 55.205, 55.205]
