nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.152, Test loss: 2.827, Test accuracy: 55.94
Final Round, Global train loss: 0.152, Global test loss: 1.226, Global test accuracy: 58.26
Average accuracy final 10 rounds: 55.550250000000005 

Average global accuracy final 10 rounds: 57.344249999999995 

6412.171003580093
[4.04476523399353, 8.08953046798706, 11.96475100517273, 15.839971542358398, 19.684996128082275, 23.530020713806152, 27.974517107009888, 32.41901350021362, 36.8659188747406, 41.31282424926758, 45.209726333618164, 49.10662841796875, 53.36417078971863, 57.621713161468506, 61.7765851020813, 65.93145704269409, 70.0639340877533, 74.1964111328125, 78.52257895469666, 82.84874677658081, 87.1592321395874, 91.469717502594, 95.61953663825989, 99.76935577392578, 104.3779125213623, 108.98646926879883, 113.52644634246826, 118.0664234161377, 122.63046860694885, 127.19451379776001, 131.7405183315277, 136.2865228652954, 140.8913803100586, 145.49623775482178, 150.16422247886658, 154.83220720291138, 159.56492805480957, 164.29764890670776, 168.96071004867554, 173.6237711906433, 178.2826647758484, 182.94155836105347, 187.6654498577118, 192.38934135437012, 197.15751385688782, 201.92568635940552, 206.78892064094543, 211.65215492248535, 216.5109236240387, 221.36969232559204, 226.1367015838623, 230.90371084213257, 235.79563999176025, 240.68756914138794, 245.62522625923157, 250.5628833770752, 255.29135465621948, 260.01982593536377, 264.8451406955719, 269.67045545578003, 274.49128699302673, 279.31211853027344, 284.19774103164673, 289.08336353302, 293.9992275238037, 298.9150915145874, 303.6445415019989, 308.3739914894104, 313.1896710395813, 318.0053505897522, 322.8791403770447, 327.75293016433716, 332.58516550064087, 337.4174008369446, 342.23021626472473, 347.0430316925049, 351.79855847358704, 356.5540852546692, 361.52270007133484, 366.4913148880005, 371.5529363155365, 376.6145577430725, 381.4571535587311, 386.29974937438965, 391.24170088768005, 396.18365240097046, 401.1315248012543, 406.0793972015381, 410.9787459373474, 415.87809467315674, 420.7121880054474, 425.54628133773804, 430.0550105571747, 434.5637397766113, 439.1091740131378, 443.6546082496643, 448.2104353904724, 452.7662625312805, 457.2698264122009, 461.77339029312134, 466.5606732368469, 471.3479561805725, 476.2396466732025, 481.1313371658325, 485.6228811740875, 490.11442518234253, 494.8964467048645, 499.6784682273865, 504.4604938030243, 509.2425193786621, 513.6362447738647, 518.0299701690674, 522.3230729103088, 526.6161756515503, 530.8227806091309, 535.0293855667114, 539.2793383598328, 543.5292911529541, 547.8582150936127, 552.1871390342712, 556.651086807251, 561.1150345802307, 565.6711273193359, 570.2272200584412, 575.0717132091522, 579.9162063598633, 584.7567949295044, 589.5973834991455, 594.2566196918488, 598.915855884552, 603.6291947364807, 608.3425335884094, 612.7640907764435, 617.1856479644775, 621.6166844367981, 626.0477209091187, 630.4819760322571, 634.9162311553955, 639.7324604988098, 644.5486898422241, 649.5926375389099, 654.6365852355957, 659.650018453598, 664.6634516716003, 669.767422914505, 674.8713941574097, 679.9181332588196, 684.9648723602295, 689.6835486888885, 694.4022250175476, 699.1726088523865, 703.9429926872253, 708.6925539970398, 713.4421153068542, 718.4205596446991, 723.399003982544, 728.4556448459625, 733.5122857093811, 738.6286916732788, 743.7450976371765, 748.9709026813507, 754.1967077255249, 759.159193277359, 764.1216788291931, 769.2569041252136, 774.3921294212341, 779.4223375320435, 784.4525456428528, 789.3042345046997, 794.1559233665466, 799.0370757579803, 803.9182281494141, 808.8984589576721, 813.8786897659302, 818.8112380504608, 823.7437863349915, 828.7581024169922, 833.7724184989929, 838.7386741638184, 843.7049298286438, 848.7070569992065, 853.7091841697693, 858.7011349201202, 863.6930856704712, 869.0630683898926, 874.433051109314, 879.2844371795654, 884.1358232498169, 889.2920272350311, 894.4482312202454, 899.8458104133606, 905.2433896064758, 910.1644718647003, 915.0855541229248, 920.0921015739441, 925.0986490249634, 930.122302532196, 935.1459560394287, 940.0300951004028, 944.914234161377, 947.4395678043365, 949.9649014472961]
[38.12, 38.12, 42.8525, 42.8525, 45.2925, 45.2925, 46.3125, 46.3125, 47.2025, 47.2025, 48.02, 48.02, 47.695, 47.695, 48.665, 48.665, 48.5675, 48.5675, 49.2625, 49.2625, 49.61, 49.61, 50.2025, 50.2025, 50.9725, 50.9725, 51.49, 51.49, 52.3675, 52.3675, 52.555, 52.555, 53.07, 53.07, 53.105, 53.105, 53.3275, 53.3275, 53.425, 53.425, 53.02, 53.02, 53.2625, 53.2625, 53.2975, 53.2975, 54.0, 54.0, 54.1975, 54.1975, 54.29, 54.29, 53.855, 53.855, 54.0575, 54.0575, 53.7825, 53.7825, 54.5225, 54.5225, 54.5325, 54.5325, 54.345, 54.345, 54.3775, 54.3775, 54.3175, 54.3175, 54.3825, 54.3825, 54.665, 54.665, 54.5325, 54.5325, 54.6375, 54.6375, 54.96, 54.96, 55.2025, 55.2025, 54.78, 54.78, 54.81, 54.81, 54.805, 54.805, 54.8525, 54.8525, 54.885, 54.885, 54.87, 54.87, 54.95, 54.95, 55.075, 55.075, 54.9425, 54.9425, 55.23, 55.23, 55.04, 55.04, 55.04, 55.04, 55.0475, 55.0475, 55.0025, 55.0025, 55.135, 55.135, 55.2125, 55.2125, 54.785, 54.785, 54.8175, 54.8175, 55.07, 55.07, 55.2725, 55.2725, 55.4425, 55.4425, 55.225, 55.225, 55.22, 55.22, 55.305, 55.305, 55.1325, 55.1325, 55.13, 55.13, 55.1925, 55.1925, 55.3525, 55.3525, 55.2625, 55.2625, 55.18, 55.18, 55.4375, 55.4375, 55.175, 55.175, 55.2825, 55.2825, 55.485, 55.485, 55.6225, 55.6225, 55.6075, 55.6075, 55.4475, 55.4475, 55.965, 55.965, 55.75, 55.75, 55.4375, 55.4375, 55.53, 55.53, 55.305, 55.305, 55.4475, 55.4475, 55.295, 55.295, 55.415, 55.415, 55.52, 55.52, 55.285, 55.285, 55.215, 55.215, 55.6725, 55.6725, 55.635, 55.635, 55.65, 55.65, 55.5275, 55.5275, 55.4675, 55.4675, 55.3875, 55.3875, 55.5575, 55.5575, 55.5875, 55.5875, 55.455, 55.455, 55.4675, 55.4675, 55.64, 55.64, 55.7625, 55.7625, 55.94, 55.94]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.166, Test loss: 0.580, Test accuracy: 83.63
Final Round, Global train loss: 0.166, Global test loss: 1.090, Global test accuracy: 64.89
Average accuracy final 10 rounds: 83.78666666666666 

Average global accuracy final 10 rounds: 59.13083333333333 

2243.439014673233
[1.6994259357452393, 3.3988518714904785, 4.903605699539185, 6.408359527587891, 7.864601135253906, 9.320842742919922, 10.811562061309814, 12.302281379699707, 13.819536924362183, 15.336792469024658, 16.761521577835083, 18.186250686645508, 19.756696224212646, 21.327141761779785, 22.706772089004517, 24.086402416229248, 25.582922220230103, 27.079442024230957, 28.529970407485962, 29.980498790740967, 31.43440341949463, 32.88830804824829, 34.42324876785278, 35.958189487457275, 37.33408212661743, 38.70997476577759, 40.23009157180786, 41.750208377838135, 43.170666217803955, 44.591124057769775, 46.06492257118225, 47.53872108459473, 49.06890678405762, 50.59909248352051, 51.970723152160645, 53.34235382080078, 54.83101224899292, 56.31967067718506, 57.73452067375183, 59.1493706703186, 60.57636070251465, 62.00335073471069, 63.467751264572144, 64.9321517944336, 66.35442543029785, 67.77669906616211, 69.24166226387024, 70.70662546157837, 72.11832976341248, 73.53003406524658, 75.0590705871582, 76.58810710906982, 78.10849404335022, 79.62888097763062, 81.07118034362793, 82.51347970962524, 84.05251789093018, 85.59155607223511, 87.07094144821167, 88.55032682418823, 90.03951048851013, 91.52869415283203, 92.95653057098389, 94.38436698913574, 95.80009198188782, 97.21581697463989, 98.70759773254395, 100.199378490448, 101.70402693748474, 103.20867538452148, 104.69886255264282, 106.18904972076416, 107.65317130088806, 109.11729288101196, 110.70365786552429, 112.29002285003662, 113.86577200889587, 115.44152116775513, 117.05882573127747, 118.6761302947998, 120.26691842079163, 121.85770654678345, 123.44466781616211, 125.03162908554077, 126.60926818847656, 128.18690729141235, 129.72867965698242, 131.2704520225525, 132.8400354385376, 134.4096188545227, 136.02119326591492, 137.63276767730713, 139.19007778167725, 140.74738788604736, 142.34202003479004, 143.93665218353271, 145.49088048934937, 147.04510879516602, 148.59598565101624, 150.14686250686646, 151.66132402420044, 153.17578554153442, 154.60809111595154, 156.04039669036865, 157.5176658630371, 158.99493503570557, 160.46869564056396, 161.94245624542236, 163.37133479118347, 164.80021333694458, 166.27770471572876, 167.75519609451294, 169.19147872924805, 170.62776136398315, 172.08923077583313, 173.5507001876831, 175.0360255241394, 176.5213508605957, 177.96051168441772, 179.39967250823975, 180.87541604042053, 182.35115957260132, 183.807963848114, 185.2647681236267, 186.72815227508545, 188.1915364265442, 189.66500115394592, 191.13846588134766, 192.61397123336792, 194.08947658538818, 195.7211081981659, 197.3527398109436, 198.9675736427307, 200.58240747451782, 202.24913477897644, 203.91586208343506, 205.56985330581665, 207.22384452819824, 208.83010935783386, 210.43637418746948, 212.04328608512878, 213.6501979827881, 215.3055374622345, 216.9608769416809, 218.55196595191956, 220.1430549621582, 221.73736357688904, 223.33167219161987, 224.93205785751343, 226.53244352340698, 228.10007286071777, 229.66770219802856, 231.25102066993713, 232.8343391418457, 234.43580603599548, 236.03727293014526, 237.63337564468384, 239.2294783592224, 240.82670426368713, 242.42393016815186, 244.05739545822144, 245.69086074829102, 247.32901453971863, 248.96716833114624, 250.56953048706055, 252.17189264297485, 253.77345204353333, 255.3750114440918, 256.9821443557739, 258.58927726745605, 260.1728732585907, 261.75646924972534, 263.34979462623596, 264.9431200027466, 266.50543546676636, 268.06775093078613, 269.69225883483887, 271.3167667388916, 272.8531174659729, 274.3894681930542, 275.9445676803589, 277.4996671676636, 279.0701575279236, 280.6406478881836, 282.1975028514862, 283.7543578147888, 285.3055486679077, 286.8567395210266, 288.386061668396, 289.9153838157654, 291.450790643692, 292.98619747161865, 294.44960355758667, 295.9130096435547, 297.5217328071594, 299.13045597076416, 300.5947651863098, 302.05907440185547, 303.50443410873413, 304.9497938156128, 307.3980464935303, 309.84629917144775]
[24.683333333333334, 24.683333333333334, 32.266666666666666, 32.266666666666666, 42.35, 42.35, 55.025, 55.025, 63.43333333333333, 63.43333333333333, 66.93333333333334, 66.93333333333334, 69.98333333333333, 69.98333333333333, 71.95833333333333, 71.95833333333333, 73.36666666666666, 73.36666666666666, 73.26666666666667, 73.26666666666667, 75.35, 75.35, 75.93333333333334, 75.93333333333334, 76.19166666666666, 76.19166666666666, 76.775, 76.775, 77.275, 77.275, 77.68333333333334, 77.68333333333334, 77.46666666666667, 77.46666666666667, 77.90833333333333, 77.90833333333333, 78.38333333333334, 78.38333333333334, 79.35833333333333, 79.35833333333333, 79.425, 79.425, 79.75, 79.75, 79.44166666666666, 79.44166666666666, 79.64166666666667, 79.64166666666667, 79.725, 79.725, 79.89166666666667, 79.89166666666667, 80.08333333333333, 80.08333333333333, 80.0, 80.0, 80.21666666666667, 80.21666666666667, 80.51666666666667, 80.51666666666667, 80.84166666666667, 80.84166666666667, 81.24166666666666, 81.24166666666666, 81.63333333333334, 81.63333333333334, 81.78333333333333, 81.78333333333333, 82.13333333333334, 82.13333333333334, 81.775, 81.775, 81.6, 81.6, 81.525, 81.525, 81.025, 81.025, 81.4, 81.4, 81.49166666666666, 81.49166666666666, 81.65, 81.65, 81.675, 81.675, 82.08333333333333, 82.08333333333333, 82.06666666666666, 82.06666666666666, 81.91666666666667, 81.91666666666667, 81.925, 81.925, 81.25, 81.25, 81.2, 81.2, 81.03333333333333, 81.03333333333333, 82.0, 82.0, 82.01666666666667, 82.01666666666667, 82.51666666666667, 82.51666666666667, 82.78333333333333, 82.78333333333333, 83.14166666666667, 83.14166666666667, 83.1, 83.1, 83.26666666666667, 83.26666666666667, 83.65, 83.65, 83.3, 83.3, 83.05833333333334, 83.05833333333334, 82.95833333333333, 82.95833333333333, 82.8, 82.8, 82.86666666666666, 82.86666666666666, 82.71666666666667, 82.71666666666667, 83.13333333333334, 83.13333333333334, 82.925, 82.925, 83.11666666666666, 83.11666666666666, 82.91666666666667, 82.91666666666667, 82.85, 82.85, 82.73333333333333, 82.73333333333333, 82.84166666666667, 82.84166666666667, 83.05833333333334, 83.05833333333334, 82.7, 82.7, 82.85, 82.85, 82.80833333333334, 82.80833333333334, 83.16666666666667, 83.16666666666667, 83.21666666666667, 83.21666666666667, 83.24166666666666, 83.24166666666666, 83.35833333333333, 83.35833333333333, 83.13333333333334, 83.13333333333334, 82.95833333333333, 82.95833333333333, 83.46666666666667, 83.46666666666667, 83.80833333333334, 83.80833333333334, 83.53333333333333, 83.53333333333333, 83.525, 83.525, 83.31666666666666, 83.31666666666666, 83.74166666666666, 83.74166666666666, 84.025, 84.025, 83.79166666666667, 83.79166666666667, 83.50833333333334, 83.50833333333334, 83.75, 83.75, 83.69166666666666, 83.69166666666666, 83.75833333333334, 83.75833333333334, 84.13333333333334, 84.13333333333334, 83.625, 83.625, 83.45, 83.45, 83.68333333333334, 83.68333333333334, 83.85833333333333, 83.85833333333333, 84.0, 84.0, 83.91666666666667, 83.91666666666667, 83.63333333333334, 83.63333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.161, Test loss: 0.461, Test accuracy: 86.20
Final Round, Global train loss: 0.161, Global test loss: 1.252, Global test accuracy: 64.24
Average accuracy final 10 rounds: 85.71722222222222 

Average global accuracy final 10 rounds: 64.77 

3372.7148683071136
[2.7580742835998535, 5.516148567199707, 7.992208957672119, 10.468269348144531, 13.076659440994263, 15.685049533843994, 18.226130962371826, 20.767212390899658, 23.4279146194458, 26.088616847991943, 28.402318239212036, 30.71601963043213, 33.03825545310974, 35.36049127578735, 37.7324333190918, 40.10437536239624, 42.52597761154175, 44.947579860687256, 47.23747158050537, 49.527363300323486, 51.898192167282104, 54.26902103424072, 56.57389807701111, 58.878775119781494, 61.194103717803955, 63.509432315826416, 65.8125410079956, 68.1156497001648, 70.42818403244019, 72.74071836471558, 75.05167984962463, 77.36264133453369, 79.65636372566223, 81.95008611679077, 84.27028822898865, 86.59049034118652, 88.87780117988586, 91.1651120185852, 93.51415634155273, 95.86320066452026, 98.15578699111938, 100.4483733177185, 102.96380925178528, 105.47924518585205, 108.01233577728271, 110.54542636871338, 113.05348944664001, 115.56155252456665, 118.10250782966614, 120.64346313476562, 123.27086329460144, 125.89826345443726, 128.2553095817566, 130.61235570907593, 133.11069989204407, 135.6090440750122, 137.93910026550293, 140.26915645599365, 142.5595531463623, 144.84994983673096, 147.1649522781372, 149.47995471954346, 151.79090023040771, 154.10184574127197, 156.4019694328308, 158.70209312438965, 160.95226192474365, 163.20243072509766, 165.46068286895752, 167.71893501281738, 169.98552298545837, 172.25211095809937, 174.53900814056396, 176.82590532302856, 179.08942317962646, 181.35294103622437, 183.63654732704163, 185.9201536178589, 188.20736479759216, 190.49457597732544, 192.77235507965088, 195.05013418197632, 197.31866335868835, 199.5871925354004, 201.87894749641418, 204.17070245742798, 206.45202541351318, 208.7333483695984, 211.01885652542114, 213.3043646812439, 215.5840516090393, 217.86373853683472, 220.14791011810303, 222.43208169937134, 224.83723640441895, 227.24239110946655, 229.7636821269989, 232.28497314453125, 234.8065242767334, 237.32807540893555, 239.6317183971405, 241.93536138534546, 244.24627566337585, 246.55718994140625, 248.9188368320465, 251.28048372268677, 253.58043432235718, 255.8803849220276, 258.17062735557556, 260.46086978912354, 262.77492904663086, 265.0889883041382, 267.39689779281616, 269.70480728149414, 272.04770064353943, 274.3905940055847, 276.70159578323364, 279.01259756088257, 281.30173897743225, 283.59088039398193, 285.8739230632782, 288.15696573257446, 290.4109296798706, 292.66489362716675, 294.96112036705017, 297.2573471069336, 299.5168921947479, 301.77643728256226, 304.0558648109436, 306.33529233932495, 308.64253401756287, 310.9497756958008, 313.2448408603668, 315.53990602493286, 317.82856917381287, 320.11723232269287, 322.38455748558044, 324.651882648468, 326.9540927410126, 329.25630283355713, 331.5639057159424, 333.87150859832764, 336.2023046016693, 338.533100605011, 340.8685231208801, 343.20394563674927, 345.5651352405548, 347.92632484436035, 350.25852513313293, 352.5907254219055, 354.92343187332153, 357.25613832473755, 359.6215763092041, 361.98701429367065, 364.3255763053894, 366.66413831710815, 368.959894657135, 371.25565099716187, 373.5834057331085, 375.9111604690552, 378.30931305885315, 380.7074656486511, 383.0486271381378, 385.3897886276245, 387.68485045433044, 389.9799122810364, 392.280775308609, 394.58163833618164, 396.9491083621979, 399.3165783882141, 401.614462852478, 403.91234731674194, 406.22995686531067, 408.5475664138794, 410.8308267593384, 413.11408710479736, 415.41844391822815, 417.72280073165894, 420.0061767101288, 422.28955268859863, 424.5777654647827, 426.8659782409668, 429.15430545806885, 431.4426326751709, 433.75930428504944, 436.075975894928, 438.4217162132263, 440.76745653152466, 443.12286949157715, 445.47828245162964, 447.87996912002563, 450.28165578842163, 452.5759696960449, 454.8702836036682, 457.2095983028412, 459.54891300201416, 461.9343545436859, 464.31979608535767, 466.850403547287, 469.3810110092163, 472.07718086242676, 474.7733507156372]
[37.43333333333333, 37.43333333333333, 46.05, 46.05, 54.51111111111111, 54.51111111111111, 59.88333333333333, 59.88333333333333, 64.79444444444445, 64.79444444444445, 66.29444444444445, 66.29444444444445, 68.99444444444444, 68.99444444444444, 68.95555555555555, 68.95555555555555, 75.2611111111111, 75.2611111111111, 75.63888888888889, 75.63888888888889, 75.71111111111111, 75.71111111111111, 75.02777777777777, 75.02777777777777, 75.41666666666667, 75.41666666666667, 76.77222222222223, 76.77222222222223, 79.66111111111111, 79.66111111111111, 79.47777777777777, 79.47777777777777, 79.96111111111111, 79.96111111111111, 79.89444444444445, 79.89444444444445, 79.80555555555556, 79.80555555555556, 80.31666666666666, 80.31666666666666, 80.49444444444444, 80.49444444444444, 81.02777777777777, 81.02777777777777, 81.25, 81.25, 81.82222222222222, 81.82222222222222, 81.16111111111111, 81.16111111111111, 81.06111111111112, 81.06111111111112, 81.40555555555555, 81.40555555555555, 81.85, 81.85, 81.62222222222222, 81.62222222222222, 82.09444444444445, 82.09444444444445, 81.62777777777778, 81.62777777777778, 81.62222222222222, 81.62222222222222, 82.42777777777778, 82.42777777777778, 82.96111111111111, 82.96111111111111, 82.70555555555555, 82.70555555555555, 82.93333333333334, 82.93333333333334, 83.32222222222222, 83.32222222222222, 83.25555555555556, 83.25555555555556, 83.68333333333334, 83.68333333333334, 84.29444444444445, 84.29444444444445, 84.31666666666666, 84.31666666666666, 83.68888888888888, 83.68888888888888, 83.89444444444445, 83.89444444444445, 83.97222222222223, 83.97222222222223, 84.3, 84.3, 83.82222222222222, 83.82222222222222, 84.13333333333334, 84.13333333333334, 84.28888888888889, 84.28888888888889, 84.33333333333333, 84.33333333333333, 83.99444444444444, 83.99444444444444, 84.09444444444445, 84.09444444444445, 84.31666666666666, 84.31666666666666, 84.43888888888888, 84.43888888888888, 84.71111111111111, 84.71111111111111, 84.60555555555555, 84.60555555555555, 84.35555555555555, 84.35555555555555, 84.58888888888889, 84.58888888888889, 84.96666666666667, 84.96666666666667, 84.77777777777777, 84.77777777777777, 84.68333333333334, 84.68333333333334, 84.96111111111111, 84.96111111111111, 84.96666666666667, 84.96666666666667, 85.47222222222223, 85.47222222222223, 85.08888888888889, 85.08888888888889, 85.65, 85.65, 85.85, 85.85, 85.56111111111112, 85.56111111111112, 85.22222222222223, 85.22222222222223, 84.7388888888889, 84.7388888888889, 84.95555555555555, 84.95555555555555, 85.58888888888889, 85.58888888888889, 85.46111111111111, 85.46111111111111, 85.86111111111111, 85.86111111111111, 85.5111111111111, 85.5111111111111, 85.42222222222222, 85.42222222222222, 85.62777777777778, 85.62777777777778, 85.65, 85.65, 85.79444444444445, 85.79444444444445, 85.53333333333333, 85.53333333333333, 85.75, 85.75, 86.16666666666667, 86.16666666666667, 85.90555555555555, 85.90555555555555, 85.67777777777778, 85.67777777777778, 85.85, 85.85, 86.2611111111111, 86.2611111111111, 86.05555555555556, 86.05555555555556, 85.75555555555556, 85.75555555555556, 85.79444444444445, 85.79444444444445, 85.97222222222223, 85.97222222222223, 86.2388888888889, 86.2388888888889, 86.01666666666667, 86.01666666666667, 85.93333333333334, 85.93333333333334, 85.63888888888889, 85.63888888888889, 85.27777777777777, 85.27777777777777, 85.40555555555555, 85.40555555555555, 85.52777777777777, 85.52777777777777, 85.6, 85.6, 86.09444444444445, 86.09444444444445, 85.93888888888888, 85.93888888888888, 85.7388888888889, 85.7388888888889, 86.2, 86.2]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.365, Test loss: 1.941, Test accuracy: 28.70
Round   1, Train loss: 0.834, Test loss: 2.178, Test accuracy: 36.41
Round   2, Train loss: 0.875, Test loss: 1.423, Test accuracy: 46.91
Round   3, Train loss: 0.740, Test loss: 1.124, Test accuracy: 57.74
Round   4, Train loss: 0.675, Test loss: 1.021, Test accuracy: 58.13
Round   5, Train loss: 0.655, Test loss: 0.989, Test accuracy: 62.84
Round   6, Train loss: 0.642, Test loss: 0.839, Test accuracy: 68.42
Round   7, Train loss: 0.631, Test loss: 0.952, Test accuracy: 66.94
Round   8, Train loss: 0.559, Test loss: 0.632, Test accuracy: 74.06
Round   9, Train loss: 0.684, Test loss: 0.619, Test accuracy: 75.27
Round  10, Train loss: 0.597, Test loss: 0.644, Test accuracy: 74.41
Round  11, Train loss: 0.520, Test loss: 0.592, Test accuracy: 76.70
Round  12, Train loss: 0.520, Test loss: 0.511, Test accuracy: 79.46
Round  13, Train loss: 0.511, Test loss: 0.509, Test accuracy: 80.05
Round  14, Train loss: 0.485, Test loss: 0.498, Test accuracy: 80.44
Round  15, Train loss: 0.474, Test loss: 0.477, Test accuracy: 80.88
Round  16, Train loss: 0.488, Test loss: 0.468, Test accuracy: 81.57
Round  17, Train loss: 0.541, Test loss: 0.452, Test accuracy: 81.68
Round  18, Train loss: 0.479, Test loss: 0.449, Test accuracy: 81.85
Round  19, Train loss: 0.510, Test loss: 0.465, Test accuracy: 81.14
Round  20, Train loss: 0.379, Test loss: 0.460, Test accuracy: 81.11
Round  21, Train loss: 0.362, Test loss: 0.453, Test accuracy: 81.38
Round  22, Train loss: 0.417, Test loss: 0.440, Test accuracy: 82.22
Round  23, Train loss: 0.384, Test loss: 0.435, Test accuracy: 82.55
Round  24, Train loss: 0.425, Test loss: 0.420, Test accuracy: 83.35
Round  25, Train loss: 0.395, Test loss: 0.419, Test accuracy: 83.26
Round  26, Train loss: 0.430, Test loss: 0.410, Test accuracy: 83.87
Round  27, Train loss: 0.431, Test loss: 0.406, Test accuracy: 84.03
Round  28, Train loss: 0.414, Test loss: 0.407, Test accuracy: 84.07
Round  29, Train loss: 0.358, Test loss: 0.404, Test accuracy: 83.98
Round  30, Train loss: 0.355, Test loss: 0.397, Test accuracy: 84.21
Round  31, Train loss: 0.404, Test loss: 0.400, Test accuracy: 84.24
Round  32, Train loss: 0.360, Test loss: 0.391, Test accuracy: 84.71
Round  33, Train loss: 0.373, Test loss: 0.387, Test accuracy: 84.59
Round  34, Train loss: 0.382, Test loss: 0.391, Test accuracy: 84.71
Round  35, Train loss: 0.336, Test loss: 0.395, Test accuracy: 84.34
Round  36, Train loss: 0.368, Test loss: 0.384, Test accuracy: 84.97
Round  37, Train loss: 0.323, Test loss: 0.381, Test accuracy: 85.11
Round  38, Train loss: 0.317, Test loss: 0.372, Test accuracy: 85.49
Round  39, Train loss: 0.334, Test loss: 0.368, Test accuracy: 85.57
Round  40, Train loss: 0.296, Test loss: 0.374, Test accuracy: 85.22
Round  41, Train loss: 0.344, Test loss: 0.374, Test accuracy: 85.29
Round  42, Train loss: 0.336, Test loss: 0.373, Test accuracy: 85.15
Round  43, Train loss: 0.287, Test loss: 0.368, Test accuracy: 85.39
Round  44, Train loss: 0.290, Test loss: 0.368, Test accuracy: 85.39
Round  45, Train loss: 0.379, Test loss: 0.369, Test accuracy: 85.42
Round  46, Train loss: 0.330, Test loss: 0.369, Test accuracy: 85.29
Round  47, Train loss: 0.268, Test loss: 0.360, Test accuracy: 86.01
Round  48, Train loss: 0.335, Test loss: 0.358, Test accuracy: 86.13
Round  49, Train loss: 0.302, Test loss: 0.354, Test accuracy: 86.08
Round  50, Train loss: 0.273, Test loss: 0.354, Test accuracy: 86.32
Round  51, Train loss: 0.358, Test loss: 0.345, Test accuracy: 86.61
Round  52, Train loss: 0.301, Test loss: 0.349, Test accuracy: 86.05
Round  53, Train loss: 0.321, Test loss: 0.344, Test accuracy: 86.65
Round  54, Train loss: 0.291, Test loss: 0.342, Test accuracy: 86.47
Round  55, Train loss: 0.286, Test loss: 0.347, Test accuracy: 86.40
Round  56, Train loss: 0.233, Test loss: 0.345, Test accuracy: 86.59
Round  57, Train loss: 0.291, Test loss: 0.340, Test accuracy: 86.88
Round  58, Train loss: 0.310, Test loss: 0.337, Test accuracy: 86.74
Round  59, Train loss: 0.236, Test loss: 0.337, Test accuracy: 86.84
Round  60, Train loss: 0.265, Test loss: 0.340, Test accuracy: 86.76
Round  61, Train loss: 0.302, Test loss: 0.338, Test accuracy: 86.83
Round  62, Train loss: 0.281, Test loss: 0.336, Test accuracy: 86.87
Round  63, Train loss: 0.255, Test loss: 0.340, Test accuracy: 86.77
Round  64, Train loss: 0.295, Test loss: 0.336, Test accuracy: 86.77
Round  65, Train loss: 0.285, Test loss: 0.331, Test accuracy: 87.01
Round  66, Train loss: 0.245, Test loss: 0.333, Test accuracy: 86.91
Round  67, Train loss: 0.206, Test loss: 0.331, Test accuracy: 86.91
Round  68, Train loss: 0.237, Test loss: 0.327, Test accuracy: 87.18
Round  69, Train loss: 0.319, Test loss: 0.329, Test accuracy: 87.17
Round  70, Train loss: 0.251, Test loss: 0.335, Test accuracy: 86.83
Round  71, Train loss: 0.198, Test loss: 0.330, Test accuracy: 87.16
Round  72, Train loss: 0.328, Test loss: 0.336, Test accuracy: 86.92
Round  73, Train loss: 0.256, Test loss: 0.323, Test accuracy: 87.73
Round  74, Train loss: 0.249, Test loss: 0.326, Test accuracy: 87.33
Round  75, Train loss: 0.239, Test loss: 0.325, Test accuracy: 87.38
Round  76, Train loss: 0.254, Test loss: 0.329, Test accuracy: 87.46
Round  77, Train loss: 0.198, Test loss: 0.334, Test accuracy: 87.35
Round  78, Train loss: 0.238, Test loss: 0.336, Test accuracy: 87.12
Round  79, Train loss: 0.211, Test loss: 0.333, Test accuracy: 87.09
Round  80, Train loss: 0.208, Test loss: 0.333, Test accuracy: 87.05
Round  81, Train loss: 0.231, Test loss: 0.331, Test accuracy: 87.27
Round  82, Train loss: 0.175, Test loss: 0.329, Test accuracy: 87.57
Round  83, Train loss: 0.166, Test loss: 0.325, Test accuracy: 87.71
Round  84, Train loss: 0.235, Test loss: 0.326, Test accuracy: 87.57
Round  85, Train loss: 0.166, Test loss: 0.325, Test accuracy: 87.66
Round  86, Train loss: 0.214, Test loss: 0.325, Test accuracy: 87.57
Round  87, Train loss: 0.218, Test loss: 0.332, Test accuracy: 87.42
Round  88, Train loss: 0.246, Test loss: 0.330, Test accuracy: 87.34
Round  89, Train loss: 0.203, Test loss: 0.329, Test accuracy: 87.47
Round  90, Train loss: 0.164, Test loss: 0.324, Test accuracy: 87.57
Round  91, Train loss: 0.227, Test loss: 0.325, Test accuracy: 87.65
Round  92, Train loss: 0.217, Test loss: 0.322, Test accuracy: 87.66
Round  93, Train loss: 0.217, Test loss: 0.325, Test accuracy: 87.71
Round  94, Train loss: 0.205, Test loss: 0.328, Test accuracy: 87.70
Round  95, Train loss: 0.175, Test loss: 0.329, Test accuracy: 87.58
Round  96, Train loss: 0.184, Test loss: 0.326, Test accuracy: 88.00
Round  97, Train loss: 0.200, Test loss: 0.323, Test accuracy: 87.88
Round  98, Train loss: 0.219, Test loss: 0.327, Test accuracy: 87.83
Round  99, Train loss: 0.177, Test loss: 0.322, Test accuracy: 87.97
Final Round, Train loss: 0.161, Test loss: 0.323, Test accuracy: 87.85
Average accuracy final 10 rounds: 87.75444444444443
2463.5731897354126
[3.2145018577575684, 6.296590566635132, 9.182297229766846, 12.235690832138062, 15.18464970588684, 18.189073085784912, 21.266648292541504, 24.307943105697632, 27.35022282600403, 30.39087224006653, 33.38397526741028, 36.37247848510742, 39.30241870880127, 42.279956340789795, 45.15795564651489, 48.03669452667236, 50.91350960731506, 53.95019030570984, 56.995161294937134, 60.10730314254761, 63.1973819732666, 66.21590757369995, 69.20774006843567, 72.22082138061523, 75.30100560188293, 78.43061184883118, 81.59376072883606, 84.67868900299072, 87.68439865112305, 90.67329120635986, 93.76052045822144, 96.7577257156372, 99.79961967468262, 102.88470721244812, 106.0030825138092, 108.99268794059753, 112.06230330467224, 115.08459258079529, 118.11687850952148, 121.09549355506897, 124.239586353302, 127.28968524932861, 130.26652026176453, 133.35244941711426, 136.40422916412354, 139.37413001060486, 142.47055006027222, 145.50669813156128, 148.46351742744446, 151.34398746490479, 154.29598474502563, 157.29098105430603, 160.30760288238525, 163.36698818206787, 166.42337226867676, 169.4263186454773, 172.54273176193237, 175.52387928962708, 178.497145652771, 181.57198977470398, 184.64525747299194, 187.7299473285675, 190.74115824699402, 193.77666354179382, 196.7746138572693, 199.7485671043396, 202.80923628807068, 205.88242053985596, 208.83875584602356, 211.81348395347595, 214.9247431755066, 217.95289063453674, 220.90325236320496, 224.0566520690918, 227.11780881881714, 230.06625604629517, 233.11036443710327, 236.22234845161438, 239.14158844947815, 242.13969159126282, 245.2171869277954, 248.22048377990723, 251.14633965492249, 254.2111041545868, 257.1579144001007, 260.07877492904663, 263.22275614738464, 266.2899360656738, 269.2518515586853, 272.26315331459045, 275.2825663089752, 278.2942590713501, 281.33524680137634, 284.37455344200134, 287.3235719203949, 290.39977860450745, 293.39793586730957, 296.370178937912, 299.4046289920807, 302.48772644996643, 304.94802808761597]
[28.7, 36.40555555555556, 46.90555555555556, 57.74444444444445, 58.12777777777778, 62.84444444444444, 68.41666666666667, 66.94444444444444, 74.05555555555556, 75.27222222222223, 74.41111111111111, 76.7, 79.46111111111111, 80.05, 80.43888888888888, 80.88333333333334, 81.57222222222222, 81.67777777777778, 81.85, 81.14444444444445, 81.10555555555555, 81.38333333333334, 82.22222222222223, 82.55, 83.35, 83.2611111111111, 83.87222222222222, 84.02777777777777, 84.07222222222222, 83.97777777777777, 84.21111111111111, 84.2388888888889, 84.71111111111111, 84.58888888888889, 84.70555555555555, 84.33888888888889, 84.97222222222223, 85.11111111111111, 85.4888888888889, 85.56666666666666, 85.22222222222223, 85.28888888888889, 85.15, 85.39444444444445, 85.38888888888889, 85.42222222222222, 85.28888888888889, 86.00555555555556, 86.13333333333334, 86.08333333333333, 86.31666666666666, 86.61111111111111, 86.05, 86.65, 86.47222222222223, 86.4, 86.58888888888889, 86.87777777777778, 86.7388888888889, 86.83888888888889, 86.75555555555556, 86.82777777777778, 86.86666666666666, 86.76666666666667, 86.77222222222223, 87.00555555555556, 86.90555555555555, 86.90555555555555, 87.17777777777778, 87.17222222222222, 86.82777777777778, 87.16111111111111, 86.92222222222222, 87.72777777777777, 87.33333333333333, 87.38333333333334, 87.45555555555555, 87.35, 87.11666666666666, 87.08888888888889, 87.05, 87.27222222222223, 87.57222222222222, 87.71111111111111, 87.57222222222222, 87.66111111111111, 87.56666666666666, 87.41666666666667, 87.34444444444445, 87.47222222222223, 87.56666666666666, 87.65, 87.65555555555555, 87.71111111111111, 87.7, 87.57777777777778, 88.0, 87.87777777777778, 87.83333333333333, 87.97222222222223, 87.85]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  18.9100
Round 1 global test acc  20.5900
Round 2 global test acc  17.1100
Round 3 global test acc  24.9600
Round 4 global test acc  21.5200
Round 5 global test acc  18.6800
Round 6 global test acc  26.3400
Round 7 global test acc  25.5300
Round 8 global test acc  21.9000
Round 9 global test acc  27.0100
Round 10 global test acc  26.5500
Round 11 global test acc  33.8800
Round 12 global test acc  25.7900
Round 13 global test acc  32.5300
Round 14 global test acc  32.7800
Round 15 global test acc  32.9100
Round 16 global test acc  32.2800
Round 17 global test acc  33.3200
Round 18 global test acc  24.5800
Round 19 global test acc  26.9800
Round 20 global test acc  28.5100
Round 21 global test acc  27.4400
Round 22 global test acc  28.8800
Round 23 global test acc  28.5600
Round 24 global test acc  27.8400
Round 25 global test acc  29.7600
Round 26 global test acc  25.0300
Round 27 global test acc  29.2300
Round 28 global test acc  27.9000
Round 29 global test acc  29.5500
Round 30 global test acc  27.2600
Round 31 global test acc  27.5600
Round 32 global test acc  36.9400
Round 33 global test acc  27.4300
Round 34 global test acc  31.2100
Round 35 global test acc  37.5500
Round 36 global test acc  28.4100
Round 37 global test acc  36.3600
Round 38 global test acc  37.8500
Round 39 global test acc  36.3600
Round 40 global test acc  35.4300
Round 41 global test acc  32.0000
Round 42 global test acc  33.2000
Round 43 global test acc  31.4700
Round 44 global test acc  39.8900
Round 45 global test acc  32.7300
Round 46 global test acc  30.9900
Round 47 global test acc  40.4200
Round 48 global test acc  26.1900
Round 49 global test acc  31.9500
Round 50 global test acc  37.0000
Round 51 global test acc  41.0200
Round 52 global test acc  32.3900
Round 53 global test acc  36.8000
Round 54 global test acc  38.5300
Round 55 global test acc  38.4400
Round 56 global test acc  26.5400
Round 57 global test acc  32.5900
Round 58 global test acc  26.6800
Round 59 global test acc  32.5600
Round 60 global test acc  31.7300
Round 61 global test acc  29.9400
Round 62 global test acc  39.4700
Round 63 global test acc  34.7400
Round 64 global test acc  28.2200
Round 65 global test acc  32.6100
Round 66 global test acc  40.2600
Round 67 global test acc  42.0100
Round 68 global test acc  40.8700
Round 69 global test acc  41.1700
Round 70 global test acc  30.7800
Round 71 global test acc  34.3600
Round 72 global test acc  31.8200
Round 73 global test acc  41.1400
Round 74 global test acc  31.2300
Round 75 global test acc  32.1700
Round 76 global test acc  33.3700
Round 77 global test acc  34.5900
Round 78 global test acc  39.3200
Round 79 global test acc  37.8300
Round 80 global test acc  36.1500
Round 81 global test acc  35.1400
Round 82 global test acc  33.3000
Round 83 global test acc  30.9200
Round 84 global test acc  29.4400
Round 85 global test acc  28.3200
Round 86 global test acc  26.8100
Round 87 global test acc  24.8400
Round 88 global test acc  24.0700
Round 89 global test acc  23.3300
Round 90 global test acc  23.5800
Round 91 global test acc  22.2300
Round 92 global test acc  21.6000
Round 93 global test acc  20.5000
Round 94 global test acc  19.2400
Round 95 global test acc  17.8400
Round 96 global test acc  17.8600
Round 97 global test acc  16.9300
Round 98 global test acc  16.0000
Round 99 global test acc  15.2200
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.420, Test loss: 0.716, Test accuracy: 76.79
Average accuracy final 10 rounds: 76.84125
4957.462503194809
[7.090769052505493, 13.861989736557007, 20.53506302833557, 27.244653463363647, 33.96758723258972, 40.66926598548889, 47.221503257751465, 53.78197145462036, 60.34034776687622, 66.84604501724243, 73.45021033287048, 79.93066191673279, 86.52363276481628, 93.12043642997742, 99.74033260345459, 106.4320456981659, 113.1442940235138, 119.74608707427979, 126.37427043914795, 132.96344137191772, 139.6453835964203, 146.255704164505, 152.69653916358948, 159.335928440094, 165.7642686367035, 172.13606238365173, 178.44342803955078, 184.63437938690186, 190.83737969398499, 197.00690841674805, 203.19002485275269, 209.330970287323, 215.51084208488464, 221.70223259925842, 227.90454602241516, 234.03009128570557, 240.11869311332703, 246.22958827018738, 252.3631501197815, 258.551988363266, 264.7141592502594, 270.85020542144775, 277.08416056632996, 283.298721075058, 289.5151104927063, 295.75256299972534, 301.9413661956787, 308.0181076526642, 314.1254017353058, 320.2799005508423, 326.45732617378235, 332.68714451789856, 338.8985981941223, 345.1223785877228, 351.3240647315979, 357.4760193824768, 363.60925006866455, 369.8098850250244, 376.02562737464905, 382.2134132385254, 388.3661468029022, 394.5180492401123, 400.7085077762604, 406.8969581127167, 413.10059785842896, 419.2178018093109, 425.3933711051941, 431.5660126209259, 437.77347803115845, 443.8772075176239, 449.9944896697998, 456.24535942077637, 462.4655170440674, 468.65100479125977, 474.7774360179901, 480.9621651172638, 487.13740038871765, 493.3201892375946, 499.52261209487915, 505.7084128856659, 511.8416681289673, 517.9829347133636, 524.136804819107, 530.2680861949921, 536.4260718822479, 542.5873599052429, 548.7296469211578, 554.8533992767334, 560.9800159931183, 567.1099474430084, 573.2512927055359, 579.3970754146576, 585.4712197780609, 590.8103404045105, 596.1191799640656, 601.465423822403, 606.8139445781708, 612.1266753673553, 617.4476580619812, 622.804783821106, 624.9219777584076]
[32.8825, 40.795, 44.8625, 48.9525, 52.49, 55.0125, 56.8625, 58.24, 59.555, 61.0925, 61.2475, 63.2425, 63.4725, 64.91, 65.555, 66.6175, 67.3525, 67.0325, 67.06, 68.445, 67.8725, 69.1825, 70.07, 69.9975, 71.0025, 70.8875, 71.5575, 71.2425, 71.7125, 72.0025, 72.2675, 72.8625, 73.005, 72.8875, 73.22, 73.25, 73.3075, 73.6325, 74.1825, 74.2775, 73.935, 74.12, 74.3325, 74.29, 74.5475, 74.44, 74.2725, 73.995, 74.1125, 74.4925, 74.955, 75.2075, 75.045, 75.255, 75.1725, 75.1175, 75.2325, 75.4225, 75.4, 75.5, 75.4475, 75.3875, 75.6825, 75.775, 75.955, 75.985, 76.4475, 76.105, 75.7, 76.105, 76.2475, 76.1825, 76.3075, 76.0525, 76.265, 76.0725, 76.525, 76.92, 76.3525, 76.7625, 76.6625, 76.845, 76.6825, 76.885, 77.23, 76.815, 76.095, 76.5475, 76.6425, 76.725, 76.6075, 76.3925, 76.99, 77.02, 77.1325, 77.185, 77.01, 76.86, 76.7925, 76.4225, 76.79]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.200, Test loss: 1.896, Test accuracy: 33.79
Round   1, Train loss: 1.843, Test loss: 1.644, Test accuracy: 41.52
Round   2, Train loss: 1.654, Test loss: 1.520, Test accuracy: 46.45
Round   3, Train loss: 1.566, Test loss: 1.441, Test accuracy: 49.94
Round   4, Train loss: 1.491, Test loss: 1.361, Test accuracy: 53.48
Round   5, Train loss: 1.426, Test loss: 1.283, Test accuracy: 55.46
Round   6, Train loss: 1.343, Test loss: 1.243, Test accuracy: 56.88
Round   7, Train loss: 1.293, Test loss: 1.208, Test accuracy: 58.42
Round   8, Train loss: 1.253, Test loss: 1.167, Test accuracy: 59.99
Round   9, Train loss: 1.208, Test loss: 1.153, Test accuracy: 60.10
Round  10, Train loss: 1.153, Test loss: 1.159, Test accuracy: 60.46
Round  11, Train loss: 1.176, Test loss: 1.080, Test accuracy: 62.51
Round  12, Train loss: 1.103, Test loss: 1.047, Test accuracy: 64.09
Round  13, Train loss: 1.049, Test loss: 1.040, Test accuracy: 64.70
Round  14, Train loss: 1.062, Test loss: 1.015, Test accuracy: 65.42
Round  15, Train loss: 1.029, Test loss: 0.984, Test accuracy: 66.77
Round  16, Train loss: 1.003, Test loss: 0.983, Test accuracy: 66.55
Round  17, Train loss: 0.976, Test loss: 0.977, Test accuracy: 66.89
Round  18, Train loss: 0.975, Test loss: 0.933, Test accuracy: 68.55
Round  19, Train loss: 0.931, Test loss: 0.911, Test accuracy: 69.44
Round  20, Train loss: 0.912, Test loss: 0.893, Test accuracy: 69.67
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1966, in train
    self.filter_by_loss2(net=net, concept_matrix_local=concept_matrix_local, iter_num=iter2,
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1809, in filter_by_loss2
    loss = self.loss_func(log_probs, lable_tmp)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8615
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2855
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8175
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0305
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6075
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7890
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1395
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6715
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5230
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8465
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.350, Test loss: 4.618, Test accuracy: 34.51
Final Round, Global train loss: 0.350, Global test loss: 1.932, Global test accuracy: 31.02
Average accuracy final 10 rounds: 33.91575 

Average global accuracy final 10 rounds: 33.2485 

5967.901341199875
[4.957217454910278, 9.914434909820557, 14.298394918441772, 18.68235492706299, 22.975258588790894, 27.2681622505188, 31.615724086761475, 35.96328592300415, 40.28782391548157, 44.612361907958984, 49.055423736572266, 53.49848556518555, 58.39583992958069, 63.29319429397583, 67.5833375453949, 71.87348079681396, 76.15504598617554, 80.43661117553711, 84.93547439575195, 89.4343376159668, 94.81144738197327, 100.18855714797974, 104.49135088920593, 108.79414463043213, 113.12665963172913, 117.45917463302612, 122.06816983222961, 126.6771650314331, 130.95098519325256, 135.22480535507202, 139.51701664924622, 143.8092279434204, 148.12322354316711, 152.43721914291382, 156.74275708198547, 161.04829502105713, 165.35476517677307, 169.661235332489, 173.98499131202698, 178.30874729156494, 182.63852167129517, 186.9682960510254, 191.26138520240784, 195.55447435379028, 199.8715295791626, 204.1885848045349, 208.48054575920105, 212.7725067138672, 217.04895544052124, 221.3254041671753, 225.58789801597595, 229.8503918647766, 234.14562225341797, 238.44085264205933, 242.75551176071167, 247.070170879364, 251.30723762512207, 255.54430437088013, 259.7561812400818, 263.96805810928345, 268.3406286239624, 272.71319913864136, 276.9534010887146, 281.19360303878784, 285.47859597206116, 289.7635889053345, 295.3128459453583, 300.8621029853821, 305.12755131721497, 309.39299964904785, 313.7544355392456, 318.11587142944336, 322.45871591567993, 326.8015604019165, 331.1022322177887, 335.4029040336609, 339.68264627456665, 343.9623885154724, 348.22276186943054, 352.4831352233887, 356.69006967544556, 360.89700412750244, 365.1298716068268, 369.3627390861511, 373.64744114875793, 377.93214321136475, 382.22901344299316, 386.5258836746216, 390.788143157959, 395.0504026412964, 399.2891936302185, 403.5279846191406, 407.74259519577026, 411.9572057723999, 416.17178750038147, 420.38636922836304, 424.6562442779541, 428.92611932754517, 433.20784759521484, 437.4895758628845, 441.7878110408783, 446.08604621887207, 450.38243985176086, 454.67883348464966, 458.96597599983215, 463.25311851501465, 467.5058846473694, 471.7586507797241, 476.0687334537506, 480.3788161277771, 484.65878891944885, 488.9387617111206, 493.19394874572754, 497.4491357803345, 501.6649305820465, 505.88072538375854, 510.18167996406555, 514.4826345443726, 518.7662830352783, 523.0499315261841, 527.343775510788, 531.6376194953918, 535.892728805542, 540.1478381156921, 544.3772747516632, 548.6067113876343, 552.8880743980408, 557.1694374084473, 561.4456541538239, 565.7218708992004, 569.9753277301788, 574.2287845611572, 578.5153963565826, 582.802008152008, 587.0653154850006, 591.3286228179932, 595.5512681007385, 599.7739133834839, 604.0411925315857, 608.3084716796875, 612.5623905658722, 616.8163094520569, 621.1109821796417, 625.4056549072266, 629.6561801433563, 633.9067053794861, 638.1378591060638, 642.3690128326416, 646.6282448768616, 650.8874769210815, 655.138200044632, 659.3889231681824, 663.6872835159302, 667.985643863678, 672.3504333496094, 676.7152228355408, 680.9868819713593, 685.2585411071777, 689.5636339187622, 693.8687267303467, 698.1172318458557, 702.3657369613647, 706.6900870800018, 711.0144371986389, 715.3002808094025, 719.586124420166, 723.8955228328705, 728.204921245575, 732.4940738677979, 736.7832264900208, 741.0115587711334, 745.2398910522461, 749.4836530685425, 753.7274150848389, 757.9884128570557, 762.2494106292725, 766.5366768836975, 770.8239431381226, 775.140864610672, 779.4577860832214, 783.742201089859, 788.0266160964966, 792.3261988162994, 796.6257815361023, 801.0568151473999, 805.4878487586975, 809.7715137004852, 814.055178642273, 818.5676040649414, 823.0800294876099, 827.3645622730255, 831.6490950584412, 835.9159083366394, 840.1827216148376, 844.5030105113983, 848.823299407959, 853.2064230442047, 857.5895466804504, 861.8898234367371, 866.1901001930237, 868.3506467342377, 870.5111932754517]
[24.8025, 24.8025, 33.435, 33.435, 33.305, 33.305, 33.795, 33.795, 34.68, 34.68, 35.245, 35.245, 36.1475, 36.1475, 35.595, 35.595, 36.6025, 36.6025, 35.7225, 35.7225, 36.275, 36.275, 36.5125, 36.5125, 35.86, 35.86, 36.1375, 36.1375, 36.23, 36.23, 36.545, 36.545, 36.47, 36.47, 36.7675, 36.7675, 37.375, 37.375, 37.3825, 37.3825, 37.3, 37.3, 36.7525, 36.7525, 36.81, 36.81, 36.9775, 36.9775, 36.7975, 36.7975, 36.64, 36.64, 36.8825, 36.8825, 36.855, 36.855, 36.5575, 36.5575, 36.5725, 36.5725, 36.5475, 36.5475, 36.3425, 36.3425, 35.9925, 35.9925, 35.7, 35.7, 35.495, 35.495, 35.2575, 35.2575, 35.2575, 35.2575, 35.145, 35.145, 34.8425, 34.8425, 35.1925, 35.1925, 35.2925, 35.2925, 35.44, 35.44, 35.4525, 35.4525, 35.0825, 35.0825, 35.13, 35.13, 34.6075, 34.6075, 34.3625, 34.3625, 34.4125, 34.4125, 34.44, 34.44, 34.6175, 34.6175, 34.8, 34.8, 34.4925, 34.4925, 34.5225, 34.5225, 34.67, 34.67, 34.48, 34.48, 34.6525, 34.6525, 34.4825, 34.4825, 34.185, 34.185, 33.9975, 33.9975, 33.7875, 33.7875, 33.97, 33.97, 34.0475, 34.0475, 33.9425, 33.9425, 33.8425, 33.8425, 33.8625, 33.8625, 33.995, 33.995, 34.0525, 34.0525, 33.9225, 33.9225, 33.9075, 33.9075, 33.9875, 33.9875, 34.0375, 34.0375, 34.2575, 34.2575, 34.2725, 34.2725, 34.3075, 34.3075, 34.1625, 34.1625, 33.995, 33.995, 33.6775, 33.6775, 33.89, 33.89, 33.965, 33.965, 33.5875, 33.5875, 33.8625, 33.8625, 33.93, 33.93, 33.6575, 33.6575, 33.3725, 33.3725, 33.6125, 33.6125, 33.53, 33.53, 33.7125, 33.7125, 34.3075, 34.3075, 33.905, 33.905, 33.79, 33.79, 33.9975, 33.9975, 33.9825, 33.9825, 33.9425, 33.9425, 33.93, 33.93, 33.9175, 33.9175, 33.8175, 33.8175, 33.9925, 33.9925, 33.87, 33.87, 33.8775, 33.8775, 33.83, 33.83, 34.5075, 34.5075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8645
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2890
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8270
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1135
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5680
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8130
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.2520
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6795
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5125
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8630
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.544, Test loss: 1.715, Test accuracy: 59.05
Final Round, Global train loss: 0.544, Global test loss: 1.914, Global test accuracy: 42.00
Average accuracy final 10 rounds: 59.74999999999999 

Average global accuracy final 10 rounds: 41.43083333333333 

1843.8794839382172
[1.7739672660827637, 3.5479345321655273, 4.841006755828857, 6.1340789794921875, 7.427464485168457, 8.720849990844727, 9.999529600143433, 11.278209209442139, 12.577208757400513, 13.876208305358887, 15.16386866569519, 16.451529026031494, 17.739340782165527, 19.02715253829956, 20.309136629104614, 21.591120719909668, 22.864623069763184, 24.1381254196167, 25.420076608657837, 26.702027797698975, 27.98301386833191, 29.263999938964844, 30.56075096130371, 31.857501983642578, 33.14634084701538, 34.435179710388184, 35.722148180007935, 37.009116649627686, 38.29380917549133, 39.57850170135498, 40.91188645362854, 42.2452712059021, 43.556766510009766, 44.86826181411743, 46.15498161315918, 47.44170141220093, 48.74035930633545, 50.03901720046997, 51.31052827835083, 52.58203935623169, 53.87947344779968, 55.176907539367676, 56.457173109054565, 57.737438678741455, 59.01852464675903, 60.29961061477661, 61.58886361122131, 62.878116607666016, 64.17482852935791, 65.4715404510498, 66.74623346328735, 68.0209264755249, 69.3279116153717, 70.6348967552185, 71.92657160758972, 73.21824645996094, 74.49672174453735, 75.77519702911377, 77.06828832626343, 78.36137962341309, 79.63754177093506, 80.91370391845703, 82.21468782424927, 83.5156717300415, 84.7970016002655, 86.0783314704895, 87.35974025726318, 88.64114904403687, 89.92366671562195, 91.20618438720703, 92.50060486793518, 93.79502534866333, 95.0735228061676, 96.35202026367188, 97.6495418548584, 98.94706344604492, 100.23753833770752, 101.52801322937012, 102.7998297214508, 104.0716462135315, 105.37916421890259, 106.68668222427368, 107.98011898994446, 109.27355575561523, 110.56059169769287, 111.84762763977051, 113.13351655006409, 114.41940546035767, 115.72079277038574, 117.02218008041382, 118.29867053031921, 119.57516098022461, 120.86861634254456, 122.1620717048645, 123.72271370887756, 125.28335571289062, 126.58585095405579, 127.88834619522095, 129.1856610774994, 130.48297595977783, 131.75953221321106, 133.0360884666443, 134.4767026901245, 135.91731691360474, 137.20019698143005, 138.48307704925537, 139.76748180389404, 141.05188655853271, 142.32979822158813, 143.60770988464355, 144.89737844467163, 146.1870470046997, 147.4600965976715, 148.7331461906433, 150.0385980606079, 151.3440499305725, 152.6241579055786, 153.90426588058472, 155.19721174240112, 156.49015760421753, 157.77761268615723, 159.06506776809692, 160.3411841392517, 161.6173005104065, 162.92975163459778, 164.24220275878906, 165.56565594673157, 166.88910913467407, 168.17866110801697, 169.46821308135986, 170.75503492355347, 172.04185676574707, 173.3283760547638, 174.61489534378052, 175.8953399658203, 177.1757845878601, 178.46636319160461, 179.75694179534912, 181.03997325897217, 182.32300472259521, 183.60060286521912, 184.87820100784302, 186.23751187324524, 187.59682273864746, 188.87592148780823, 190.155020236969, 191.45369672775269, 192.75237321853638, 194.03903794288635, 195.32570266723633, 196.60637664794922, 197.8870506286621, 199.23285055160522, 200.57865047454834, 201.8760633468628, 203.17347621917725, 204.43942785263062, 205.70537948608398, 206.9985625743866, 208.2917456626892, 209.5811893939972, 210.87063312530518, 212.13750767707825, 213.40438222885132, 214.69924068450928, 215.99409914016724, 217.2844693660736, 218.57483959197998, 219.85172367095947, 221.12860774993896, 222.40275382995605, 223.67689990997314, 224.95410704612732, 226.2313141822815, 227.50532722473145, 228.7793402671814, 230.1766061782837, 231.573872089386, 232.84360241889954, 234.1133327484131, 235.39187812805176, 236.67042350769043, 237.99916625022888, 239.32790899276733, 240.6209499835968, 241.91399097442627, 243.19285774230957, 244.47172451019287, 245.8723428249359, 247.27296113967896, 248.64492630958557, 250.0168914794922, 251.4533088207245, 252.8897261619568, 254.32286262512207, 255.75599908828735, 257.1962101459503, 258.6364212036133, 259.92068672180176, 261.20495223999023, 267.6239757537842, 274.0429992675781]
[27.733333333333334, 27.733333333333334, 32.525, 32.525, 42.30833333333333, 42.30833333333333, 46.425, 46.425, 48.28333333333333, 48.28333333333333, 49.59166666666667, 49.59166666666667, 49.291666666666664, 49.291666666666664, 50.05, 50.05, 52.275, 52.275, 54.24166666666667, 54.24166666666667, 53.125, 53.125, 54.608333333333334, 54.608333333333334, 54.5, 54.5, 58.275, 58.275, 58.266666666666666, 58.266666666666666, 59.93333333333333, 59.93333333333333, 59.75833333333333, 59.75833333333333, 60.2, 60.2, 60.80833333333333, 60.80833333333333, 61.1, 61.1, 61.49166666666667, 61.49166666666667, 61.225, 61.225, 61.28333333333333, 61.28333333333333, 62.125, 62.125, 62.766666666666666, 62.766666666666666, 62.35, 62.35, 61.96666666666667, 61.96666666666667, 61.975, 61.975, 61.06666666666667, 61.06666666666667, 61.65833333333333, 61.65833333333333, 61.766666666666666, 61.766666666666666, 62.99166666666667, 62.99166666666667, 62.95, 62.95, 63.44166666666667, 63.44166666666667, 63.141666666666666, 63.141666666666666, 63.13333333333333, 63.13333333333333, 61.49166666666667, 61.49166666666667, 61.05, 61.05, 61.25, 61.25, 61.833333333333336, 61.833333333333336, 62.05, 62.05, 61.95, 61.95, 62.0, 62.0, 61.56666666666667, 61.56666666666667, 60.858333333333334, 60.858333333333334, 60.775, 60.775, 60.825, 60.825, 60.31666666666667, 60.31666666666667, 60.575, 60.575, 61.325, 61.325, 61.416666666666664, 61.416666666666664, 61.725, 61.725, 61.666666666666664, 61.666666666666664, 61.7, 61.7, 60.425, 60.425, 60.541666666666664, 60.541666666666664, 61.016666666666666, 61.016666666666666, 61.40833333333333, 61.40833333333333, 61.075, 61.075, 60.975, 60.975, 61.108333333333334, 61.108333333333334, 60.93333333333333, 60.93333333333333, 60.06666666666667, 60.06666666666667, 60.43333333333333, 60.43333333333333, 60.09166666666667, 60.09166666666667, 60.30833333333333, 60.30833333333333, 60.25, 60.25, 60.75, 60.75, 60.71666666666667, 60.71666666666667, 60.84166666666667, 60.84166666666667, 59.74166666666667, 59.74166666666667, 59.483333333333334, 59.483333333333334, 59.5, 59.5, 60.3, 60.3, 60.875, 60.875, 59.81666666666667, 59.81666666666667, 59.63333333333333, 59.63333333333333, 59.85, 59.85, 59.775, 59.775, 60.18333333333333, 60.18333333333333, 60.016666666666666, 60.016666666666666, 60.125, 60.125, 60.208333333333336, 60.208333333333336, 59.8, 59.8, 59.46666666666667, 59.46666666666667, 58.96666666666667, 58.96666666666667, 59.00833333333333, 59.00833333333333, 59.38333333333333, 59.38333333333333, 59.7, 59.7, 59.63333333333333, 59.63333333333333, 60.06666666666667, 60.06666666666667, 60.05833333333333, 60.05833333333333, 60.13333333333333, 60.13333333333333, 59.975, 59.975, 59.80833333333333, 59.80833333333333, 59.99166666666667, 59.99166666666667, 58.833333333333336, 58.833333333333336, 59.125, 59.125, 59.55833333333333, 59.55833333333333, 59.95, 59.95, 59.05, 59.05]
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8560
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2850
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8265
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0575
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5600
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8015
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0105
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7290
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5240
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8455
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.701, Test loss: 1.440, Test accuracy: 59.36
Final Round, Global train loss: 0.701, Global test loss: 0.889, Global test accuracy: 71.75
Average accuracy final 10 rounds: 61.251000000000005 

Average global accuracy final 10 rounds: 71.45700000000001 

6440.742843389511
[5.458752632141113, 10.917505264282227, 16.14120888710022, 21.364912509918213, 26.589845418930054, 31.814778327941895, 37.09364032745361, 42.37250232696533, 47.65316343307495, 52.93382453918457, 58.197919607162476, 63.46201467514038, 68.74339556694031, 74.02477645874023, 78.63837194442749, 83.25196743011475, 87.9805600643158, 92.70915269851685, 97.90790104866028, 103.10664939880371, 108.30064630508423, 113.49464321136475, 118.67735934257507, 123.8600754737854, 129.07334184646606, 134.28660821914673, 139.56797289848328, 144.84933757781982, 149.99112844467163, 155.13291931152344, 160.29337239265442, 165.4538254737854, 170.61492371559143, 175.77602195739746, 180.47083520889282, 185.16564846038818, 189.81414318084717, 194.46263790130615, 199.6566355228424, 204.85063314437866, 210.18669843673706, 215.52276372909546, 220.75814175605774, 225.99351978302002, 231.16848039627075, 236.34344100952148, 241.56075882911682, 246.77807664871216, 252.055668592453, 257.33326053619385, 262.6233687400818, 267.9134769439697, 273.10631799697876, 278.2991590499878, 283.52485036849976, 288.7505416870117, 294.02302718162537, 299.295512676239, 304.5276792049408, 309.7598457336426, 314.9734396934509, 320.1870336532593, 325.3721272945404, 330.55722093582153, 335.730672121048, 340.9041233062744, 346.0782518386841, 351.25238037109375, 356.4439437389374, 361.635507106781, 366.83992052078247, 372.04433393478394, 377.2601134777069, 382.4758930206299, 387.7240614891052, 392.97222995758057, 398.27864480018616, 403.58505964279175, 408.90719652175903, 414.2293334007263, 419.4986991882324, 424.7680649757385, 430.0015637874603, 435.23506259918213, 440.496390581131, 445.75771856307983, 450.9442241191864, 456.13072967529297, 461.3268506526947, 466.52297163009644, 471.69888854026794, 476.87480545043945, 482.0886278152466, 487.3024501800537, 491.97360014915466, 496.6447501182556, 501.3385510444641, 506.0323519706726, 510.70496892929077, 515.3775858879089, 520.0245118141174, 524.6714377403259, 529.3507339954376, 534.0300302505493, 538.7737004756927, 543.5173707008362, 548.1595692634583, 552.8017678260803, 557.4937653541565, 562.1857628822327, 566.8186094760895, 571.4514560699463, 576.1210517883301, 580.7906475067139, 585.4769020080566, 590.1631565093994, 594.8260283470154, 599.4889001846313, 604.1477041244507, 608.80650806427, 613.4596636295319, 618.1128191947937, 622.7816412448883, 627.4504632949829, 632.1216554641724, 636.7928476333618, 641.4667189121246, 646.1405901908875, 650.8035242557526, 655.4664583206177, 660.1483619213104, 664.8302655220032, 669.4733538627625, 674.1164422035217, 678.786655664444, 683.4568691253662, 688.1177432537079, 692.7786173820496, 697.44864153862, 702.1186656951904, 706.7822542190552, 711.4458427429199, 716.0927155017853, 720.7395882606506, 725.3864274024963, 730.033266544342, 734.6934552192688, 739.3536438941956, 744.0289452075958, 748.7042465209961, 753.4077236652374, 758.1112008094788, 762.7417778968811, 767.3723549842834, 772.0864098072052, 776.800464630127, 781.4666903018951, 786.1329159736633, 790.8126873970032, 795.492458820343, 800.2138376235962, 804.9352164268494, 810.0876154899597, 815.2400145530701, 820.3162829875946, 825.3925514221191, 830.6194138526917, 835.8462762832642, 841.0518176555634, 846.2573590278625, 850.918075799942, 855.5787925720215, 860.3170573711395, 865.0553221702576, 869.7621054649353, 874.468888759613, 879.1090459823608, 883.7492032051086, 888.4021286964417, 893.0550541877747, 897.6739783287048, 902.292902469635, 906.9685523509979, 911.6442022323608, 916.3082160949707, 920.9722299575806, 925.6466240882874, 930.3210182189941, 935.0207824707031, 939.7205467224121, 944.4263184070587, 949.1320900917053, 953.8450818061829, 958.5580735206604, 963.198320388794, 967.8385672569275, 972.4885005950928, 977.138433933258, 981.7635691165924, 986.3887042999268, 988.7268588542938, 991.0650134086609]
[25.5725, 25.5725, 30.9175, 30.9175, 34.26, 34.26, 35.5775, 35.5775, 38.1175, 38.1175, 39.2825, 39.2825, 41.5525, 41.5525, 43.6, 43.6, 44.3775, 44.3775, 45.04, 45.04, 46.345, 46.345, 47.4625, 47.4625, 47.515, 47.515, 48.08, 48.08, 49.7775, 49.7775, 50.3275, 50.3275, 50.825, 50.825, 51.8375, 51.8375, 52.2825, 52.2825, 52.68, 52.68, 53.1875, 53.1875, 53.845, 53.845, 54.2275, 54.2275, 54.1675, 54.1675, 54.585, 54.585, 55.3125, 55.3125, 55.56, 55.56, 56.45, 56.45, 56.375, 56.375, 56.4175, 56.4175, 56.61, 56.61, 56.555, 56.555, 56.875, 56.875, 56.8125, 56.8125, 57.16, 57.16, 57.3475, 57.3475, 57.805, 57.805, 57.91, 57.91, 58.6325, 58.6325, 57.9825, 57.9825, 58.0875, 58.0875, 58.74, 58.74, 58.645, 58.645, 58.9975, 58.9975, 59.4875, 59.4875, 59.36, 59.36, 59.3775, 59.3775, 59.6225, 59.6225, 60.1325, 60.1325, 59.8825, 59.8825, 60.0, 60.0, 60.045, 60.045, 60.2925, 60.2925, 59.9675, 59.9675, 59.795, 59.795, 59.5975, 59.5975, 59.53, 59.53, 59.575, 59.575, 59.975, 59.975, 60.05, 60.05, 60.145, 60.145, 60.63, 60.63, 60.8, 60.8, 60.465, 60.465, 60.44, 60.44, 60.6525, 60.6525, 60.73, 60.73, 60.485, 60.485, 60.7175, 60.7175, 60.81, 60.81, 60.9125, 60.9125, 60.7925, 60.7925, 60.9225, 60.9225, 61.0, 61.0, 61.1925, 61.1925, 61.1675, 61.1675, 61.2575, 61.2575, 60.97, 60.97, 61.1125, 61.1125, 61.63, 61.63, 61.3825, 61.3825, 61.6325, 61.6325, 61.72, 61.72, 61.735, 61.735, 61.215, 61.215, 61.15, 61.15, 61.4525, 61.4525, 61.2775, 61.2775, 61.23, 61.23, 61.125, 61.125, 60.88, 60.88, 61.165, 61.165, 61.4775, 61.4775, 61.5375, 61.5375, 61.665, 61.665, 61.475, 61.475, 61.15, 61.15, 61.045, 61.045, 61.225, 61.225, 60.89, 60.89, 59.3625, 59.3625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8630
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3495
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8275
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1165
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5900
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8145
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1190
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7475
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4915
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8435
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.498, Test loss: 1.914, Test accuracy: 35.02
Round   1, Train loss: 0.921, Test loss: 1.914, Test accuracy: 33.60
Round   2, Train loss: 0.907, Test loss: 1.294, Test accuracy: 55.29
Round   3, Train loss: 0.795, Test loss: 1.146, Test accuracy: 55.95
Round   4, Train loss: 0.732, Test loss: 1.132, Test accuracy: 60.63
Round   5, Train loss: 0.749, Test loss: 1.161, Test accuracy: 58.99
Round   6, Train loss: 0.689, Test loss: 1.054, Test accuracy: 59.83
Round   7, Train loss: 0.594, Test loss: 1.036, Test accuracy: 62.41
Round   8, Train loss: 0.698, Test loss: 0.862, Test accuracy: 67.78
Round   9, Train loss: 0.700, Test loss: 0.721, Test accuracy: 71.98
Round  10, Train loss: 0.589, Test loss: 0.702, Test accuracy: 73.81
Round  11, Train loss: 0.593, Test loss: 0.686, Test accuracy: 74.91
Round  12, Train loss: 0.522, Test loss: 0.697, Test accuracy: 75.29
Round  13, Train loss: 0.592, Test loss: 0.548, Test accuracy: 77.03
Round  14, Train loss: 0.491, Test loss: 0.536, Test accuracy: 77.42
Round  15, Train loss: 0.434, Test loss: 0.531, Test accuracy: 77.99
Round  16, Train loss: 0.513, Test loss: 0.523, Test accuracy: 78.30
Round  17, Train loss: 0.482, Test loss: 0.520, Test accuracy: 78.14
Round  18, Train loss: 0.460, Test loss: 0.525, Test accuracy: 78.38
Round  19, Train loss: 0.556, Test loss: 0.496, Test accuracy: 79.48
Round  20, Train loss: 0.584, Test loss: 0.484, Test accuracy: 80.28
Round  21, Train loss: 0.457, Test loss: 0.487, Test accuracy: 80.42
Round  22, Train loss: 0.462, Test loss: 0.464, Test accuracy: 81.20
Round  23, Train loss: 0.456, Test loss: 0.453, Test accuracy: 81.65
Round  24, Train loss: 0.519, Test loss: 0.461, Test accuracy: 81.23
Round  25, Train loss: 0.404, Test loss: 0.465, Test accuracy: 81.08
Round  26, Train loss: 0.371, Test loss: 0.460, Test accuracy: 80.94
Round  27, Train loss: 0.453, Test loss: 0.451, Test accuracy: 81.46
Round  28, Train loss: 0.482, Test loss: 0.443, Test accuracy: 81.92
Round  29, Train loss: 0.457, Test loss: 0.436, Test accuracy: 82.43
Round  30, Train loss: 0.489, Test loss: 0.435, Test accuracy: 82.15
Round  31, Train loss: 0.420, Test loss: 0.426, Test accuracy: 82.73
Round  32, Train loss: 0.410, Test loss: 0.427, Test accuracy: 82.79
Round  33, Train loss: 0.395, Test loss: 0.426, Test accuracy: 82.77
Round  34, Train loss: 0.356, Test loss: 0.423, Test accuracy: 82.74
Round  35, Train loss: 0.452, Test loss: 0.416, Test accuracy: 83.32
Round  36, Train loss: 0.364, Test loss: 0.422, Test accuracy: 82.87
Round  37, Train loss: 0.475, Test loss: 0.417, Test accuracy: 83.28
Round  38, Train loss: 0.435, Test loss: 0.414, Test accuracy: 83.26
Round  39, Train loss: 0.423, Test loss: 0.411, Test accuracy: 83.58
Round  40, Train loss: 0.372, Test loss: 0.398, Test accuracy: 83.97
Round  41, Train loss: 0.382, Test loss: 0.409, Test accuracy: 83.84
Round  42, Train loss: 0.336, Test loss: 0.404, Test accuracy: 84.12
Round  43, Train loss: 0.414, Test loss: 0.401, Test accuracy: 84.09
Round  44, Train loss: 0.365, Test loss: 0.400, Test accuracy: 84.33
Round  45, Train loss: 0.353, Test loss: 0.400, Test accuracy: 83.94
Round  46, Train loss: 0.348, Test loss: 0.394, Test accuracy: 84.26
Round  47, Train loss: 0.418, Test loss: 0.396, Test accuracy: 84.37
Round  48, Train loss: 0.381, Test loss: 0.394, Test accuracy: 84.39
Round  49, Train loss: 0.361, Test loss: 0.383, Test accuracy: 84.70
Round  50, Train loss: 0.284, Test loss: 0.382, Test accuracy: 84.71
Round  51, Train loss: 0.238, Test loss: 0.379, Test accuracy: 84.52
Round  52, Train loss: 0.257, Test loss: 0.386, Test accuracy: 84.08
Round  53, Train loss: 0.277, Test loss: 0.380, Test accuracy: 84.75
Round  54, Train loss: 0.401, Test loss: 0.380, Test accuracy: 85.12
Round  55, Train loss: 0.325, Test loss: 0.379, Test accuracy: 85.08
Round  56, Train loss: 0.337, Test loss: 0.373, Test accuracy: 85.34
Round  57, Train loss: 0.351, Test loss: 0.371, Test accuracy: 85.43
Round  58, Train loss: 0.313, Test loss: 0.371, Test accuracy: 85.31
Round  59, Train loss: 0.305, Test loss: 0.372, Test accuracy: 85.12
Round  60, Train loss: 0.375, Test loss: 0.362, Test accuracy: 85.59
Round  61, Train loss: 0.308, Test loss: 0.370, Test accuracy: 85.21
Round  62, Train loss: 0.246, Test loss: 0.373, Test accuracy: 85.33
Round  63, Train loss: 0.258, Test loss: 0.367, Test accuracy: 85.50
Round  64, Train loss: 0.390, Test loss: 0.371, Test accuracy: 85.47
Round  65, Train loss: 0.328, Test loss: 0.366, Test accuracy: 85.40
Round  66, Train loss: 0.283, Test loss: 0.365, Test accuracy: 85.53
Round  67, Train loss: 0.302, Test loss: 0.369, Test accuracy: 85.18
Round  68, Train loss: 0.249, Test loss: 0.365, Test accuracy: 85.66
Round  69, Train loss: 0.265, Test loss: 0.364, Test accuracy: 85.47
Round  70, Train loss: 0.283, Test loss: 0.360, Test accuracy: 85.84
Round  71, Train loss: 0.294, Test loss: 0.359, Test accuracy: 85.92
Round  72, Train loss: 0.263, Test loss: 0.356, Test accuracy: 85.90
Round  73, Train loss: 0.364, Test loss: 0.356, Test accuracy: 86.08
Round  74, Train loss: 0.274, Test loss: 0.351, Test accuracy: 86.24
Round  75, Train loss: 0.306, Test loss: 0.355, Test accuracy: 86.18
Round  76, Train loss: 0.206, Test loss: 0.358, Test accuracy: 85.56
Round  77, Train loss: 0.279, Test loss: 0.351, Test accuracy: 86.25
Round  78, Train loss: 0.279, Test loss: 0.361, Test accuracy: 85.88
Round  79, Train loss: 0.280, Test loss: 0.356, Test accuracy: 86.17
Round  80, Train loss: 0.280, Test loss: 0.357, Test accuracy: 86.07
Round  81, Train loss: 0.254, Test loss: 0.358, Test accuracy: 85.98
Round  82, Train loss: 0.199, Test loss: 0.355, Test accuracy: 86.05
Round  83, Train loss: 0.212, Test loss: 0.351, Test accuracy: 85.99
Round  84, Train loss: 0.252, Test loss: 0.366, Test accuracy: 85.54
Round  85, Train loss: 0.247, Test loss: 0.364, Test accuracy: 85.60
Round  86, Train loss: 0.291, Test loss: 0.350, Test accuracy: 86.15
Round  87, Train loss: 0.263, Test loss: 0.345, Test accuracy: 86.57
Round  88, Train loss: 0.215, Test loss: 0.345, Test accuracy: 86.58
Round  89, Train loss: 0.277, Test loss: 0.347, Test accuracy: 86.46
Round  90, Train loss: 0.281, Test loss: 0.346, Test accuracy: 86.44
Round  91, Train loss: 0.206, Test loss: 0.347, Test accuracy: 86.38
Round  92, Train loss: 0.275, Test loss: 0.341, Test accuracy: 86.59
Round  93, Train loss: 0.244, Test loss: 0.343, Test accuracy: 86.53
Round  94, Train loss: 0.269, Test loss: 0.350, Test accuracy: 86.46
Round  95, Train loss: 0.221, Test loss: 0.347, Test accuracy: 86.33
Round  96, Train loss: 0.199, Test loss: 0.351, Test accuracy: 86.38
Round  97, Train loss: 0.227, Test loss: 0.342, Test accuracy: 86.77
Round  98, Train loss: 0.218, Test loss: 0.345, Test accuracy: 86.64
Round  99, Train loss: 0.234, Test loss: 0.347, Test accuracy: 86.86
Final Round, Train loss: 0.199, Test loss: 0.343, Test accuracy: 86.79
Average accuracy final 10 rounds: 86.5375
1364.438722372055
[2.1608939170837402, 3.7899274826049805, 5.4246602058410645, 7.043451547622681, 8.66253137588501, 10.261699438095093, 11.866528511047363, 13.472406148910522, 15.088599920272827, 16.690345525741577, 18.29374122619629, 19.91004252433777, 21.522472858428955, 23.14066457748413, 24.751400470733643, 26.36506724357605, 27.982184886932373, 29.59224224090576, 31.211416244506836, 32.825541257858276, 34.43643307685852, 36.05538749694824, 37.67349815368652, 39.285032510757446, 40.914844274520874, 42.535773277282715, 44.14699625968933, 45.768237352371216, 47.393524169921875, 49.00848913192749, 50.61326026916504, 52.241923809051514, 53.87722110748291, 55.50297999382019, 57.1222882270813, 58.73563838005066, 60.34760403633118, 61.95656776428223, 63.57386755943298, 65.19244766235352, 66.8099992275238, 68.43545913696289, 70.05989789962769, 71.66717171669006, 73.2967677116394, 74.91713356971741, 76.53071451187134, 78.14935898780823, 79.77792763710022, 81.391108751297, 82.99208283424377, 84.60321855545044, 86.2136697769165, 87.98635745048523, 89.75769877433777, 91.52518391609192, 93.2968978881836, 95.07447028160095, 96.86608338356018, 98.64686012268066, 100.4165301322937, 102.02696084976196, 103.63904476165771, 105.24564170837402, 106.85760164260864, 108.47000813484192, 110.08465909957886, 111.69284749031067, 113.30593848228455, 114.91426825523376, 116.52821183204651, 118.14948987960815, 119.76583480834961, 121.37668347358704, 122.98879861831665, 124.60939836502075, 126.21456050872803, 127.82542324066162, 129.436594247818, 131.04393601417542, 132.67217350006104, 134.2960181236267, 135.90122079849243, 137.5022735595703, 139.11175203323364, 140.73375606536865, 142.3541123867035, 143.96850395202637, 145.59362173080444, 147.17863607406616, 148.7843382358551, 150.38115572929382, 151.98765301704407, 153.59158086776733, 155.21160316467285, 156.84341549873352, 158.46509766578674, 160.0865604877472, 161.6987018585205, 163.33093070983887, 165.47790002822876]
[35.016666666666666, 33.6, 55.291666666666664, 55.95, 60.63333333333333, 58.99166666666667, 59.825, 62.40833333333333, 67.78333333333333, 71.98333333333333, 73.80833333333334, 74.90833333333333, 75.29166666666667, 77.03333333333333, 77.425, 77.99166666666666, 78.3, 78.14166666666667, 78.375, 79.48333333333333, 80.275, 80.425, 81.2, 81.65, 81.23333333333333, 81.075, 80.94166666666666, 81.45833333333333, 81.925, 82.43333333333334, 82.15, 82.73333333333333, 82.79166666666667, 82.76666666666667, 82.74166666666666, 83.31666666666666, 82.86666666666666, 83.275, 83.25833333333334, 83.58333333333333, 83.96666666666667, 83.84166666666667, 84.125, 84.09166666666667, 84.325, 83.94166666666666, 84.25833333333334, 84.36666666666666, 84.39166666666667, 84.7, 84.70833333333333, 84.51666666666667, 84.075, 84.75, 85.125, 85.08333333333333, 85.34166666666667, 85.43333333333334, 85.30833333333334, 85.125, 85.59166666666667, 85.20833333333333, 85.325, 85.5, 85.46666666666667, 85.4, 85.53333333333333, 85.18333333333334, 85.65833333333333, 85.475, 85.84166666666667, 85.91666666666667, 85.9, 86.08333333333333, 86.24166666666666, 86.18333333333334, 85.55833333333334, 86.25, 85.88333333333334, 86.175, 86.06666666666666, 85.98333333333333, 86.05, 85.99166666666666, 85.54166666666667, 85.6, 86.15, 86.56666666666666, 86.575, 86.45833333333333, 86.44166666666666, 86.375, 86.59166666666667, 86.525, 86.45833333333333, 86.33333333333333, 86.38333333333334, 86.76666666666667, 86.64166666666667, 86.85833333333333, 86.79166666666667]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8505
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2900
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8295
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0310
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5670
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8125
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0930
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6925
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4870
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8630
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  14.9900
Round 1 global test acc  10.1200
Round 2 global test acc  10.4200
Round 3 global test acc  18.0300
Round 4 global test acc  20.2400
Round 5 global test acc  21.6000
Round 6 global test acc  18.0100
Round 7 global test acc  24.4800
Round 8 global test acc  20.2300
Round 9 global test acc  22.8600
Round 10 global test acc  28.7400
Round 11 global test acc  26.3500
Round 12 global test acc  25.6300
Round 13 global test acc  23.0500
Round 14 global test acc  21.2000
Round 15 global test acc  22.0700
Round 16 global test acc  25.5400
Round 17 global test acc  21.4500
Round 18 global test acc  24.3400
Round 19 global test acc  30.7900
Round 20 global test acc  32.5700
Round 21 global test acc  23.4400
Round 22 global test acc  30.3600
Round 23 global test acc  29.7500
Round 24 global test acc  35.7000
Round 25 global test acc  34.5800
Round 26 global test acc  28.2400
Round 27 global test acc  29.6100
Round 28 global test acc  28.1300
Round 29 global test acc  29.5400
Round 30 global test acc  30.0700
Round 31 global test acc  24.6500
Round 32 global test acc  29.4900
Round 33 global test acc  31.6300
Round 34 global test acc  29.1800
Round 35 global test acc  35.2700
Round 36 global test acc  27.6500
Round 37 global test acc  33.9800
Round 38 global test acc  31.3200
Round 39 global test acc  31.7200
Round 40 global test acc  31.7800
Round 41 global test acc  20.2200
Round 42 global test acc  32.3300
Round 43 global test acc  29.6900
Round 44 global test acc  28.5600
Round 45 global test acc  38.1800
Round 46 global test acc  31.6500
Round 47 global test acc  26.0200
Round 48 global test acc  31.3700
Round 49 global test acc  29.1400
Round 50 global test acc  32.2500
Round 51 global test acc  37.9900
Round 52 global test acc  28.1100
Round 53 global test acc  31.5400
Round 54 global test acc  27.2700
Round 55 global test acc  37.7700
Round 56 global test acc  22.6700
Round 57 global test acc  30.9300
Round 58 global test acc  28.9500
Round 59 global test acc  31.6000
Round 60 global test acc  31.2000
Round 61 global test acc  37.9700
Round 62 global test acc  38.7700
Round 63 global test acc  29.2900
Round 64 global test acc  27.2500
Round 65 global test acc  26.2200
Round 66 global test acc  39.5400
Round 67 global test acc  32.3400
Round 68 global test acc  36.2800
Round 69 global test acc  27.5200
Round 70 global test acc  29.9200
Round 71 global test acc  32.7100
Round 72 global test acc  41.2800
Round 73 global test acc  30.3400
Round 74 global test acc  38.1600
Round 75 global test acc  30.7600
Round 76 global test acc  31.1000
Round 77 global test acc  35.9300
Round 78 global test acc  22.7700
Round 79 global test acc  25.5700
Round 80 global test acc  28.1000
Round 81 global test acc  26.5700
Round 82 global test acc  30.9000
Round 83 global test acc  27.4400
Round 84 global test acc  27.4300
Round 85 global test acc  26.0000
Round 86 global test acc  26.0600
Round 87 global test acc  24.3400
Round 88 global test acc  26.6800
Round 89 global test acc  27.5800
Round 90 global test acc  25.2100
Round 91 global test acc  26.1000
Round 92 global test acc  26.2500
Round 93 global test acc  24.7600
Round 94 global test acc  27.0300
Round 95 global test acc  25.7300
Round 96 global test acc  25.8300
Round 97 global test acc  23.1000
Round 98 global test acc  22.6400
Round 99 global test acc  24.7400
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8680
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3985
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8240
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1080
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5665
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8395
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0650
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7270
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4785
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8770
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.770, Test loss: 0.938, Test accuracy: 69.92
Average accuracy final 10 rounds: 70.14
1465.8215646743774
[2.124946355819702, 3.9565553665161133, 5.803889036178589, 7.6225550174713135, 9.44469690322876, 11.261621475219727, 13.087005853652954, 14.895664930343628, 16.73668646812439, 18.562499523162842, 20.3912992477417, 22.222918272018433, 24.03431725502014, 25.856970071792603, 27.67081332206726, 29.483270168304443, 31.293302059173584, 33.09309720993042, 34.88681507110596, 36.689146280288696, 38.508385181427, 40.33459186553955, 42.156137228012085, 43.967405796051025, 45.77261447906494, 47.581050634384155, 49.38667058944702, 51.19540238380432, 53.00749111175537, 54.82282590866089, 56.62218451499939, 58.42572736740112, 60.24907445907593, 62.07365417480469, 63.87187719345093, 65.6659095287323, 67.50618577003479, 69.30302572250366, 71.10145807266235, 72.92450284957886, 74.74864482879639, 76.55494737625122, 78.36140871047974, 80.18194699287415, 81.99513864517212, 83.79161643981934, 85.60331463813782, 87.42325115203857, 89.23557567596436, 91.04320001602173, 92.86252784729004, 94.68849349021912, 96.49929285049438, 98.29844951629639, 100.10271716117859, 101.8980667591095, 103.7152693271637, 105.52629566192627, 107.35377264022827, 109.16153764724731, 110.95614504814148, 112.73919820785522, 114.52332758903503, 116.30986952781677, 118.07674026489258, 119.82083654403687, 121.61591792106628, 123.41658473014832, 125.1577696800232, 126.9535744190216, 128.7244622707367, 130.49944138526917, 132.27881050109863, 134.04985690116882, 135.82449078559875, 137.59166932106018, 139.3673734664917, 141.1503701210022, 142.91366505622864, 144.7082531452179, 146.4878866672516, 148.26396417617798, 149.9676558971405, 151.71145963668823, 153.44633889198303, 155.16383862495422, 156.92624521255493, 158.70336151123047, 160.49676370620728, 162.28362011909485, 164.06757760047913, 165.84761571884155, 167.59993433952332, 169.37071204185486, 171.13258576393127, 172.90384078025818, 174.6915671825409, 176.44904851913452, 178.2310287952423, 179.98361086845398, 182.22906827926636]
[21.825, 35.68333333333333, 42.75833333333333, 49.44166666666667, 49.075, 48.825, 51.958333333333336, 53.30833333333333, 55.375, 61.075, 61.13333333333333, 59.55833333333333, 60.46666666666667, 64.36666666666666, 64.725, 64.41666666666667, 65.44166666666666, 64.325, 66.73333333333333, 66.89166666666667, 66.975, 67.14166666666667, 67.25833333333334, 67.875, 68.43333333333334, 68.04166666666667, 69.15833333333333, 68.71666666666667, 68.75833333333334, 68.76666666666667, 68.08333333333333, 68.90833333333333, 68.76666666666667, 69.20833333333333, 69.24166666666666, 68.91666666666667, 69.38333333333334, 69.51666666666667, 69.975, 69.80833333333334, 69.9, 68.98333333333333, 69.96666666666667, 69.34166666666667, 70.03333333333333, 70.74166666666666, 70.54166666666667, 69.775, 70.55, 69.9, 69.80833333333334, 70.83333333333333, 69.95, 70.39166666666667, 70.34166666666667, 71.01666666666667, 70.65833333333333, 70.44166666666666, 71.05, 70.66666666666667, 70.39166666666667, 71.25833333333334, 70.89166666666667, 69.84166666666667, 69.825, 69.89166666666667, 70.475, 70.125, 69.58333333333333, 69.74166666666666, 69.66666666666667, 70.05, 69.50833333333334, 70.14166666666667, 70.5, 70.675, 70.86666666666666, 70.44166666666666, 70.03333333333333, 70.825, 70.69166666666666, 70.83333333333333, 70.83333333333333, 70.24166666666666, 70.3, 69.775, 70.23333333333333, 70.46666666666667, 70.75, 70.23333333333333, 70.475, 70.55833333333334, 69.99166666666666, 70.05, 69.68333333333334, 69.70833333333333, 70.25, 70.45833333333333, 70.125, 70.1, 69.925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8495
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2910
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8320
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1450
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6030
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8160
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0845
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7095
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5750
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8540
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.661, Test loss: 2.129, Test accuracy: 24.18
Round   1, Train loss: 1.163, Test loss: 1.834, Test accuracy: 36.72
Round   2, Train loss: 1.197, Test loss: 1.694, Test accuracy: 40.48
Round   3, Train loss: 1.173, Test loss: 1.343, Test accuracy: 47.95
Round   4, Train loss: 1.098, Test loss: 1.483, Test accuracy: 47.61
Round   5, Train loss: 1.083, Test loss: 1.389, Test accuracy: 49.25
Round   6, Train loss: 1.090, Test loss: 1.259, Test accuracy: 46.89
Round   7, Train loss: 1.052, Test loss: 1.314, Test accuracy: 49.44
Round   8, Train loss: 1.107, Test loss: 1.167, Test accuracy: 55.44
Round   9, Train loss: 0.988, Test loss: 0.968, Test accuracy: 61.34
Round  10, Train loss: 1.037, Test loss: 1.014, Test accuracy: 59.31
Round  11, Train loss: 1.075, Test loss: 1.017, Test accuracy: 59.88
Round  12, Train loss: 1.049, Test loss: 0.994, Test accuracy: 58.83
Round  13, Train loss: 1.058, Test loss: 0.892, Test accuracy: 63.71
Round  14, Train loss: 0.905, Test loss: 0.889, Test accuracy: 63.98
Round  15, Train loss: 0.970, Test loss: 0.872, Test accuracy: 63.58
Round  16, Train loss: 0.983, Test loss: 0.859, Test accuracy: 64.72
Round  17, Train loss: 0.846, Test loss: 0.835, Test accuracy: 65.01
Round  18, Train loss: 0.995, Test loss: 0.849, Test accuracy: 65.12
Round  19, Train loss: 0.922, Test loss: 0.836, Test accuracy: 66.67
Round  20, Train loss: 0.857, Test loss: 0.824, Test accuracy: 66.44
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1966, in train
    self.filter_by_loss2(net=net, concept_matrix_local=concept_matrix_local, iter_num=iter2,
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1809, in filter_by_loss2
    loss = self.loss_func(log_probs, lable_tmp)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8720
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5935
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8660
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.4575
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7630
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8515
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5875
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8050
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7415
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8875
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.416, Test loss: 5.033, Test accuracy: 28.24
Final Round, Global train loss: 0.416, Global test loss: 2.077, Global test accuracy: 26.12
Average accuracy final 10 rounds: 27.994500000000002 

Average global accuracy final 10 rounds: 30.155250000000002 

6133.197684288025
[5.549697399139404, 11.099394798278809, 16.39387607574463, 21.68835735321045, 26.97116446495056, 32.253971576690674, 37.55569243431091, 42.85741329193115, 48.12186026573181, 53.38630723953247, 58.680140018463135, 63.9739727973938, 69.23735451698303, 74.50073623657227, 79.74955773353577, 84.99837923049927, 90.2705225944519, 95.54266595840454, 100.87550234794617, 106.2083387374878, 111.55151891708374, 116.89469909667969, 122.07650017738342, 127.25830125808716, 131.59408903121948, 135.9298768043518, 140.27238965034485, 144.6149024963379, 148.97492671012878, 153.33495092391968, 157.66663002967834, 161.998309135437, 166.41957783699036, 170.8408465385437, 175.18789911270142, 179.53495168685913, 183.89338159561157, 188.251811504364, 192.6213583946228, 196.9909052848816, 201.4093418121338, 205.827778339386, 210.42485404014587, 215.02192974090576, 219.37911677360535, 223.73630380630493, 228.08756113052368, 232.43881845474243, 236.85036253929138, 241.26190662384033, 245.65296268463135, 250.04401874542236, 254.64914202690125, 259.2542653083801, 263.58955240249634, 267.92483949661255, 272.28427267074585, 276.64370584487915, 280.988516330719, 285.33332681655884, 289.69699931144714, 294.06067180633545, 298.4306654930115, 302.8006591796875, 307.13945388793945, 311.4782485961914, 315.85643887519836, 320.2346291542053, 324.64289927482605, 329.0511693954468, 333.419064283371, 337.78695917129517, 342.0957794189453, 346.40459966659546, 351.4487166404724, 356.49283361434937, 360.94669699668884, 365.4005603790283, 369.83961296081543, 374.27866554260254, 378.55795669555664, 382.83724784851074, 387.1372449398041, 391.4372420310974, 395.73996019363403, 400.04267835617065, 404.455792427063, 408.8689064979553, 413.1965925693512, 417.52427864074707, 421.87862181663513, 426.2329649925232, 430.509414434433, 434.7858638763428, 439.1095669269562, 443.4332699775696, 447.7542898654938, 452.07530975341797, 456.37691855430603, 460.6785273551941, 464.94641160964966, 469.2142958641052, 473.5206937789917, 477.8270916938782, 482.13061356544495, 486.4341354370117, 490.75200510025024, 495.06987476348877, 499.3556787967682, 503.6414828300476, 507.94589042663574, 512.2502980232239, 516.5401785373688, 520.8300590515137, 525.12704205513, 529.4240250587463, 533.9741687774658, 538.5243124961853, 542.8502726554871, 547.1762328147888, 551.4537916183472, 555.7313504219055, 560.0222840309143, 564.3132176399231, 568.688462972641, 573.0637083053589, 577.3577826023102, 581.6518568992615, 585.9632680416107, 590.27467918396, 595.5016613006592, 600.7286434173584, 605.0776603221893, 609.4266772270203, 613.7652552127838, 618.1038331985474, 622.5998239517212, 627.095814704895, 631.4658000469208, 635.8357853889465, 640.2533838748932, 644.6709823608398, 649.1928296089172, 653.7146768569946, 658.050121307373, 662.3855657577515, 666.7083897590637, 671.031213760376, 675.4767606258392, 679.9223074913025, 684.2989003658295, 688.6754932403564, 693.0428359508514, 697.4101786613464, 701.6808979511261, 705.9516172409058, 710.3256165981293, 714.6996159553528, 719.104585647583, 723.5095553398132, 727.8814053535461, 732.253255367279, 736.5341141223907, 740.8149728775024, 745.0767555236816, 749.3385381698608, 753.8908035755157, 758.4430689811707, 762.8442223072052, 767.2453756332397, 771.6216607093811, 775.9979457855225, 780.3275637626648, 784.6571817398071, 788.9587576389313, 793.2603335380554, 797.633376121521, 802.0064187049866, 806.3786466121674, 810.7508745193481, 815.1592009067535, 819.5675272941589, 823.8845944404602, 828.2016615867615, 832.4759142398834, 836.7501668930054, 841.0301196575165, 845.3100724220276, 849.6020033359528, 853.8939342498779, 858.1799354553223, 862.4659366607666, 866.7349882125854, 871.0040397644043, 875.234611749649, 879.4651837348938, 883.7131516933441, 887.9611196517944, 892.2417047023773, 896.5222897529602, 898.6773176193237, 900.8323454856873]
[23.4075, 23.4075, 25.8, 25.8, 30.8525, 30.8525, 30.435, 30.435, 31.3625, 31.3625, 29.7725, 29.7725, 30.79, 30.79, 31.225, 31.225, 31.57, 31.57, 31.6425, 31.6425, 32.06, 32.06, 32.4725, 32.4725, 32.405, 32.405, 32.2825, 32.2825, 32.3475, 32.3475, 32.07, 32.07, 31.7325, 31.7325, 31.4325, 31.4325, 32.045, 32.045, 31.7925, 31.7925, 31.785, 31.785, 31.4925, 31.4925, 31.2775, 31.2775, 30.7775, 30.7775, 30.9775, 30.9775, 30.9875, 30.9875, 31.025, 31.025, 31.2725, 31.2725, 31.1525, 31.1525, 31.0725, 31.0725, 30.6725, 30.6725, 30.6125, 30.6125, 30.61, 30.61, 30.73, 30.73, 30.485, 30.485, 30.12, 30.12, 30.27, 30.27, 29.96, 29.96, 29.5775, 29.5775, 29.4075, 29.4075, 29.4875, 29.4875, 29.545, 29.545, 29.19, 29.19, 29.23, 29.23, 29.34, 29.34, 29.335, 29.335, 29.27, 29.27, 28.9875, 28.9875, 28.7775, 28.7775, 28.7925, 28.7925, 28.895, 28.895, 28.5975, 28.5975, 28.56, 28.56, 28.675, 28.675, 28.605, 28.605, 28.935, 28.935, 28.6225, 28.6225, 28.5275, 28.5275, 28.7325, 28.7325, 28.2875, 28.2875, 28.0475, 28.0475, 28.085, 28.085, 28.1725, 28.1725, 27.895, 27.895, 28.37, 28.37, 28.41, 28.41, 28.3475, 28.3475, 28.2875, 28.2875, 28.095, 28.095, 28.2625, 28.2625, 28.285, 28.285, 28.0025, 28.0025, 28.235, 28.235, 28.3175, 28.3175, 27.925, 27.925, 28.1225, 28.1225, 27.6775, 27.6775, 27.565, 27.565, 27.415, 27.415, 27.7325, 27.7325, 27.82, 27.82, 27.7825, 27.7825, 27.855, 27.855, 27.86, 27.86, 27.8075, 27.8075, 27.755, 27.755, 27.765, 27.765, 27.975, 27.975, 28.065, 28.065, 28.115, 28.115, 28.255, 28.255, 28.1875, 28.1875, 28.2, 28.2, 28.165, 28.165, 28.1925, 28.1925, 27.9375, 27.9375, 27.82, 27.82, 27.65, 27.65, 27.7475, 27.7475, 27.79, 27.79, 28.24, 28.24]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8855
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.6470
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8660
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5260
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7505
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8495
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5750
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7765
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7330
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8820
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.628, Test loss: 1.916, Test accuracy: 53.72
Final Round, Global train loss: 0.628, Global test loss: 1.006, Global test accuracy: 69.85
Average accuracy final 10 rounds: 53.768 

Average global accuracy final 10 rounds: 67.7945 

6125.270520210266
[5.000871181488037, 10.001742362976074, 14.902879238128662, 19.80401611328125, 24.78536581993103, 29.76671552658081, 34.10544013977051, 38.444164752960205, 42.79743409156799, 47.15070343017578, 51.46006155014038, 55.76941967010498, 60.07404112815857, 64.37866258621216, 68.73556280136108, 73.09246301651001, 77.42198133468628, 81.75149965286255, 86.06219553947449, 90.37289142608643, 94.71608281135559, 99.05927419662476, 103.4263014793396, 107.79332876205444, 112.13661193847656, 116.47989511489868, 120.86327004432678, 125.24664497375488, 129.67173027992249, 134.0968155860901, 138.41802716255188, 142.73923873901367, 147.0395188331604, 151.33979892730713, 155.71317386627197, 160.08654880523682, 164.3966827392578, 168.7068166732788, 173.02632403373718, 177.34583139419556, 181.7338583469391, 186.12188529968262, 190.42168378829956, 194.7214822769165, 199.03108382225037, 203.34068536758423, 207.6508274078369, 211.9609694480896, 216.28738927841187, 220.61380910873413, 224.94262719154358, 229.27144527435303, 233.62885999679565, 237.98627471923828, 242.2811152935028, 246.57595586776733, 250.84656643867493, 255.11717700958252, 259.38466334342957, 263.6521496772766, 267.9297773838043, 272.20740509033203, 276.5309760570526, 280.8545470237732, 285.20573329925537, 289.55691957473755, 293.88022089004517, 298.2035222053528, 302.52198028564453, 306.8404383659363, 311.14103722572327, 315.44163608551025, 319.77064299583435, 324.09964990615845, 328.4164116382599, 332.7331733703613, 337.0502462387085, 341.36731910705566, 345.6922607421875, 350.01720237731934, 354.34887051582336, 358.6805386543274, 363.04066467285156, 367.40079069137573, 371.79554772377014, 376.19030475616455, 380.5745539665222, 384.9588031768799, 389.32608461380005, 393.6933660507202, 398.03730177879333, 402.38123750686646, 406.73243594169617, 411.0836343765259, 415.4715003967285, 419.85936641693115, 424.2273418903351, 428.595317363739, 432.9142680168152, 437.23321866989136, 441.546911239624, 445.8606038093567, 450.220636844635, 454.58066987991333, 458.8988838195801, 463.2170977592468, 467.5679566860199, 471.91881561279297, 476.2661101818085, 480.613404750824, 484.98395228385925, 489.35449981689453, 493.73429703712463, 498.11409425735474, 502.5022666454315, 506.8904390335083, 511.2619254589081, 515.6334118843079, 519.9459664821625, 524.2585210800171, 528.5781185626984, 532.8977160453796, 537.2332897186279, 541.5688633918762, 545.9009532928467, 550.2330431938171, 554.566978931427, 558.9009146690369, 563.225038766861, 567.5491628646851, 571.8548662662506, 576.1605696678162, 580.4721710681915, 584.7837724685669, 589.1217105388641, 593.4596486091614, 597.746994972229, 602.0343413352966, 606.3551392555237, 610.6759371757507, 614.9897055625916, 619.3034739494324, 623.6376307010651, 627.9717874526978, 632.3243074417114, 636.6768274307251, 641.0313868522644, 645.3859462738037, 649.6806869506836, 653.9754276275635, 658.2628238201141, 662.5502200126648, 666.9479687213898, 671.3457174301147, 675.714047908783, 680.0823783874512, 684.4560174942017, 688.8296566009521, 693.2118473052979, 697.5940380096436, 701.9412014484406, 706.2883648872375, 710.6462042331696, 715.0040435791016, 719.3742783069611, 723.7445130348206, 728.1154465675354, 732.4863801002502, 736.809353351593, 741.1323266029358, 745.4373557567596, 749.7423849105835, 754.0600860118866, 758.3777871131897, 762.6840169429779, 766.9902467727661, 771.2780177593231, 775.5657887458801, 779.8565864562988, 784.1473841667175, 788.4224381446838, 792.6974921226501, 796.9810259342194, 801.2645597457886, 805.5548453330994, 809.8451309204102, 814.3429985046387, 818.8408660888672, 823.1454277038574, 827.4499893188477, 831.829892873764, 836.2097964286804, 840.5557940006256, 844.9017915725708, 849.2315649986267, 853.5613384246826, 857.8846006393433, 862.2078628540039, 866.5224807262421, 870.8370985984802, 873.0775420665741, 875.317985534668]
[26.2075, 26.2075, 32.8175, 32.8175, 34.325, 34.325, 36.89, 36.89, 37.695, 37.695, 40.2025, 40.2025, 41.105, 41.105, 42.8225, 42.8225, 43.9325, 43.9325, 45.5275, 45.5275, 46.14, 46.14, 47.21, 47.21, 47.9225, 47.9225, 49.115, 49.115, 49.775, 49.775, 49.68, 49.68, 50.1825, 50.1825, 50.81, 50.81, 51.28, 51.28, 51.5175, 51.5175, 51.875, 51.875, 52.7325, 52.7325, 53.1075, 53.1075, 53.5975, 53.5975, 53.515, 53.515, 53.14, 53.14, 53.46, 53.46, 52.9075, 52.9075, 53.1425, 53.1425, 53.0, 53.0, 53.255, 53.255, 53.2225, 53.2225, 52.7125, 52.7125, 52.7875, 52.7875, 53.355, 53.355, 53.63, 53.63, 53.675, 53.675, 53.5975, 53.5975, 53.475, 53.475, 53.45, 53.45, 53.8775, 53.8775, 54.105, 54.105, 54.035, 54.035, 54.335, 54.335, 54.435, 54.435, 54.335, 54.335, 53.9375, 53.9375, 53.88, 53.88, 53.7425, 53.7425, 54.1, 54.1, 53.82, 53.82, 53.87, 53.87, 54.175, 54.175, 54.375, 54.375, 54.1025, 54.1025, 54.0475, 54.0475, 54.21, 54.21, 54.4, 54.4, 54.1175, 54.1175, 54.215, 54.215, 54.01, 54.01, 54.035, 54.035, 54.04, 54.04, 53.7175, 53.7175, 53.9475, 53.9475, 53.745, 53.745, 53.8875, 53.8875, 54.0, 54.0, 53.4125, 53.4125, 53.63, 53.63, 53.6425, 53.6425, 54.045, 54.045, 54.295, 54.295, 54.13, 54.13, 54.2125, 54.2125, 54.0675, 54.0675, 54.0225, 54.0225, 53.8575, 53.8575, 54.005, 54.005, 53.945, 53.945, 53.6175, 53.6175, 53.7875, 53.7875, 53.68, 53.68, 53.975, 53.975, 54.0825, 54.0825, 53.8775, 53.8775, 54.1325, 54.1325, 53.925, 53.925, 53.8575, 53.8575, 53.825, 53.825, 53.405, 53.405, 53.3075, 53.3075, 53.78, 53.78, 53.83, 53.83, 53.8025, 53.8025, 53.7825, 53.7825, 53.73, 53.73, 53.9025, 53.9025, 53.9975, 53.9975, 54.1425, 54.1425, 53.72, 53.72]
