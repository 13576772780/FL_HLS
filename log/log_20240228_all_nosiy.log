nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.152, Test loss: 2.827, Test accuracy: 55.94
Final Round, Global train loss: 0.152, Global test loss: 1.226, Global test accuracy: 58.26
Average accuracy final 10 rounds: 55.550250000000005 

Average global accuracy final 10 rounds: 57.344249999999995 

6412.171003580093
[4.04476523399353, 8.08953046798706, 11.96475100517273, 15.839971542358398, 19.684996128082275, 23.530020713806152, 27.974517107009888, 32.41901350021362, 36.8659188747406, 41.31282424926758, 45.209726333618164, 49.10662841796875, 53.36417078971863, 57.621713161468506, 61.7765851020813, 65.93145704269409, 70.0639340877533, 74.1964111328125, 78.52257895469666, 82.84874677658081, 87.1592321395874, 91.469717502594, 95.61953663825989, 99.76935577392578, 104.3779125213623, 108.98646926879883, 113.52644634246826, 118.0664234161377, 122.63046860694885, 127.19451379776001, 131.7405183315277, 136.2865228652954, 140.8913803100586, 145.49623775482178, 150.16422247886658, 154.83220720291138, 159.56492805480957, 164.29764890670776, 168.96071004867554, 173.6237711906433, 178.2826647758484, 182.94155836105347, 187.6654498577118, 192.38934135437012, 197.15751385688782, 201.92568635940552, 206.78892064094543, 211.65215492248535, 216.5109236240387, 221.36969232559204, 226.1367015838623, 230.90371084213257, 235.79563999176025, 240.68756914138794, 245.62522625923157, 250.5628833770752, 255.29135465621948, 260.01982593536377, 264.8451406955719, 269.67045545578003, 274.49128699302673, 279.31211853027344, 284.19774103164673, 289.08336353302, 293.9992275238037, 298.9150915145874, 303.6445415019989, 308.3739914894104, 313.1896710395813, 318.0053505897522, 322.8791403770447, 327.75293016433716, 332.58516550064087, 337.4174008369446, 342.23021626472473, 347.0430316925049, 351.79855847358704, 356.5540852546692, 361.52270007133484, 366.4913148880005, 371.5529363155365, 376.6145577430725, 381.4571535587311, 386.29974937438965, 391.24170088768005, 396.18365240097046, 401.1315248012543, 406.0793972015381, 410.9787459373474, 415.87809467315674, 420.7121880054474, 425.54628133773804, 430.0550105571747, 434.5637397766113, 439.1091740131378, 443.6546082496643, 448.2104353904724, 452.7662625312805, 457.2698264122009, 461.77339029312134, 466.5606732368469, 471.3479561805725, 476.2396466732025, 481.1313371658325, 485.6228811740875, 490.11442518234253, 494.8964467048645, 499.6784682273865, 504.4604938030243, 509.2425193786621, 513.6362447738647, 518.0299701690674, 522.3230729103088, 526.6161756515503, 530.8227806091309, 535.0293855667114, 539.2793383598328, 543.5292911529541, 547.8582150936127, 552.1871390342712, 556.651086807251, 561.1150345802307, 565.6711273193359, 570.2272200584412, 575.0717132091522, 579.9162063598633, 584.7567949295044, 589.5973834991455, 594.2566196918488, 598.915855884552, 603.6291947364807, 608.3425335884094, 612.7640907764435, 617.1856479644775, 621.6166844367981, 626.0477209091187, 630.4819760322571, 634.9162311553955, 639.7324604988098, 644.5486898422241, 649.5926375389099, 654.6365852355957, 659.650018453598, 664.6634516716003, 669.767422914505, 674.8713941574097, 679.9181332588196, 684.9648723602295, 689.6835486888885, 694.4022250175476, 699.1726088523865, 703.9429926872253, 708.6925539970398, 713.4421153068542, 718.4205596446991, 723.399003982544, 728.4556448459625, 733.5122857093811, 738.6286916732788, 743.7450976371765, 748.9709026813507, 754.1967077255249, 759.159193277359, 764.1216788291931, 769.2569041252136, 774.3921294212341, 779.4223375320435, 784.4525456428528, 789.3042345046997, 794.1559233665466, 799.0370757579803, 803.9182281494141, 808.8984589576721, 813.8786897659302, 818.8112380504608, 823.7437863349915, 828.7581024169922, 833.7724184989929, 838.7386741638184, 843.7049298286438, 848.7070569992065, 853.7091841697693, 858.7011349201202, 863.6930856704712, 869.0630683898926, 874.433051109314, 879.2844371795654, 884.1358232498169, 889.2920272350311, 894.4482312202454, 899.8458104133606, 905.2433896064758, 910.1644718647003, 915.0855541229248, 920.0921015739441, 925.0986490249634, 930.122302532196, 935.1459560394287, 940.0300951004028, 944.914234161377, 947.4395678043365, 949.9649014472961]
[38.12, 38.12, 42.8525, 42.8525, 45.2925, 45.2925, 46.3125, 46.3125, 47.2025, 47.2025, 48.02, 48.02, 47.695, 47.695, 48.665, 48.665, 48.5675, 48.5675, 49.2625, 49.2625, 49.61, 49.61, 50.2025, 50.2025, 50.9725, 50.9725, 51.49, 51.49, 52.3675, 52.3675, 52.555, 52.555, 53.07, 53.07, 53.105, 53.105, 53.3275, 53.3275, 53.425, 53.425, 53.02, 53.02, 53.2625, 53.2625, 53.2975, 53.2975, 54.0, 54.0, 54.1975, 54.1975, 54.29, 54.29, 53.855, 53.855, 54.0575, 54.0575, 53.7825, 53.7825, 54.5225, 54.5225, 54.5325, 54.5325, 54.345, 54.345, 54.3775, 54.3775, 54.3175, 54.3175, 54.3825, 54.3825, 54.665, 54.665, 54.5325, 54.5325, 54.6375, 54.6375, 54.96, 54.96, 55.2025, 55.2025, 54.78, 54.78, 54.81, 54.81, 54.805, 54.805, 54.8525, 54.8525, 54.885, 54.885, 54.87, 54.87, 54.95, 54.95, 55.075, 55.075, 54.9425, 54.9425, 55.23, 55.23, 55.04, 55.04, 55.04, 55.04, 55.0475, 55.0475, 55.0025, 55.0025, 55.135, 55.135, 55.2125, 55.2125, 54.785, 54.785, 54.8175, 54.8175, 55.07, 55.07, 55.2725, 55.2725, 55.4425, 55.4425, 55.225, 55.225, 55.22, 55.22, 55.305, 55.305, 55.1325, 55.1325, 55.13, 55.13, 55.1925, 55.1925, 55.3525, 55.3525, 55.2625, 55.2625, 55.18, 55.18, 55.4375, 55.4375, 55.175, 55.175, 55.2825, 55.2825, 55.485, 55.485, 55.6225, 55.6225, 55.6075, 55.6075, 55.4475, 55.4475, 55.965, 55.965, 55.75, 55.75, 55.4375, 55.4375, 55.53, 55.53, 55.305, 55.305, 55.4475, 55.4475, 55.295, 55.295, 55.415, 55.415, 55.52, 55.52, 55.285, 55.285, 55.215, 55.215, 55.6725, 55.6725, 55.635, 55.635, 55.65, 55.65, 55.5275, 55.5275, 55.4675, 55.4675, 55.3875, 55.3875, 55.5575, 55.5575, 55.5875, 55.5875, 55.455, 55.455, 55.4675, 55.4675, 55.64, 55.64, 55.7625, 55.7625, 55.94, 55.94]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.166, Test loss: 0.580, Test accuracy: 83.63
Final Round, Global train loss: 0.166, Global test loss: 1.090, Global test accuracy: 64.89
Average accuracy final 10 rounds: 83.78666666666666 

Average global accuracy final 10 rounds: 59.13083333333333 

2243.439014673233
[1.6994259357452393, 3.3988518714904785, 4.903605699539185, 6.408359527587891, 7.864601135253906, 9.320842742919922, 10.811562061309814, 12.302281379699707, 13.819536924362183, 15.336792469024658, 16.761521577835083, 18.186250686645508, 19.756696224212646, 21.327141761779785, 22.706772089004517, 24.086402416229248, 25.582922220230103, 27.079442024230957, 28.529970407485962, 29.980498790740967, 31.43440341949463, 32.88830804824829, 34.42324876785278, 35.958189487457275, 37.33408212661743, 38.70997476577759, 40.23009157180786, 41.750208377838135, 43.170666217803955, 44.591124057769775, 46.06492257118225, 47.53872108459473, 49.06890678405762, 50.59909248352051, 51.970723152160645, 53.34235382080078, 54.83101224899292, 56.31967067718506, 57.73452067375183, 59.1493706703186, 60.57636070251465, 62.00335073471069, 63.467751264572144, 64.9321517944336, 66.35442543029785, 67.77669906616211, 69.24166226387024, 70.70662546157837, 72.11832976341248, 73.53003406524658, 75.0590705871582, 76.58810710906982, 78.10849404335022, 79.62888097763062, 81.07118034362793, 82.51347970962524, 84.05251789093018, 85.59155607223511, 87.07094144821167, 88.55032682418823, 90.03951048851013, 91.52869415283203, 92.95653057098389, 94.38436698913574, 95.80009198188782, 97.21581697463989, 98.70759773254395, 100.199378490448, 101.70402693748474, 103.20867538452148, 104.69886255264282, 106.18904972076416, 107.65317130088806, 109.11729288101196, 110.70365786552429, 112.29002285003662, 113.86577200889587, 115.44152116775513, 117.05882573127747, 118.6761302947998, 120.26691842079163, 121.85770654678345, 123.44466781616211, 125.03162908554077, 126.60926818847656, 128.18690729141235, 129.72867965698242, 131.2704520225525, 132.8400354385376, 134.4096188545227, 136.02119326591492, 137.63276767730713, 139.19007778167725, 140.74738788604736, 142.34202003479004, 143.93665218353271, 145.49088048934937, 147.04510879516602, 148.59598565101624, 150.14686250686646, 151.66132402420044, 153.17578554153442, 154.60809111595154, 156.04039669036865, 157.5176658630371, 158.99493503570557, 160.46869564056396, 161.94245624542236, 163.37133479118347, 164.80021333694458, 166.27770471572876, 167.75519609451294, 169.19147872924805, 170.62776136398315, 172.08923077583313, 173.5507001876831, 175.0360255241394, 176.5213508605957, 177.96051168441772, 179.39967250823975, 180.87541604042053, 182.35115957260132, 183.807963848114, 185.2647681236267, 186.72815227508545, 188.1915364265442, 189.66500115394592, 191.13846588134766, 192.61397123336792, 194.08947658538818, 195.7211081981659, 197.3527398109436, 198.9675736427307, 200.58240747451782, 202.24913477897644, 203.91586208343506, 205.56985330581665, 207.22384452819824, 208.83010935783386, 210.43637418746948, 212.04328608512878, 213.6501979827881, 215.3055374622345, 216.9608769416809, 218.55196595191956, 220.1430549621582, 221.73736357688904, 223.33167219161987, 224.93205785751343, 226.53244352340698, 228.10007286071777, 229.66770219802856, 231.25102066993713, 232.8343391418457, 234.43580603599548, 236.03727293014526, 237.63337564468384, 239.2294783592224, 240.82670426368713, 242.42393016815186, 244.05739545822144, 245.69086074829102, 247.32901453971863, 248.96716833114624, 250.56953048706055, 252.17189264297485, 253.77345204353333, 255.3750114440918, 256.9821443557739, 258.58927726745605, 260.1728732585907, 261.75646924972534, 263.34979462623596, 264.9431200027466, 266.50543546676636, 268.06775093078613, 269.69225883483887, 271.3167667388916, 272.8531174659729, 274.3894681930542, 275.9445676803589, 277.4996671676636, 279.0701575279236, 280.6406478881836, 282.1975028514862, 283.7543578147888, 285.3055486679077, 286.8567395210266, 288.386061668396, 289.9153838157654, 291.450790643692, 292.98619747161865, 294.44960355758667, 295.9130096435547, 297.5217328071594, 299.13045597076416, 300.5947651863098, 302.05907440185547, 303.50443410873413, 304.9497938156128, 307.3980464935303, 309.84629917144775]
[24.683333333333334, 24.683333333333334, 32.266666666666666, 32.266666666666666, 42.35, 42.35, 55.025, 55.025, 63.43333333333333, 63.43333333333333, 66.93333333333334, 66.93333333333334, 69.98333333333333, 69.98333333333333, 71.95833333333333, 71.95833333333333, 73.36666666666666, 73.36666666666666, 73.26666666666667, 73.26666666666667, 75.35, 75.35, 75.93333333333334, 75.93333333333334, 76.19166666666666, 76.19166666666666, 76.775, 76.775, 77.275, 77.275, 77.68333333333334, 77.68333333333334, 77.46666666666667, 77.46666666666667, 77.90833333333333, 77.90833333333333, 78.38333333333334, 78.38333333333334, 79.35833333333333, 79.35833333333333, 79.425, 79.425, 79.75, 79.75, 79.44166666666666, 79.44166666666666, 79.64166666666667, 79.64166666666667, 79.725, 79.725, 79.89166666666667, 79.89166666666667, 80.08333333333333, 80.08333333333333, 80.0, 80.0, 80.21666666666667, 80.21666666666667, 80.51666666666667, 80.51666666666667, 80.84166666666667, 80.84166666666667, 81.24166666666666, 81.24166666666666, 81.63333333333334, 81.63333333333334, 81.78333333333333, 81.78333333333333, 82.13333333333334, 82.13333333333334, 81.775, 81.775, 81.6, 81.6, 81.525, 81.525, 81.025, 81.025, 81.4, 81.4, 81.49166666666666, 81.49166666666666, 81.65, 81.65, 81.675, 81.675, 82.08333333333333, 82.08333333333333, 82.06666666666666, 82.06666666666666, 81.91666666666667, 81.91666666666667, 81.925, 81.925, 81.25, 81.25, 81.2, 81.2, 81.03333333333333, 81.03333333333333, 82.0, 82.0, 82.01666666666667, 82.01666666666667, 82.51666666666667, 82.51666666666667, 82.78333333333333, 82.78333333333333, 83.14166666666667, 83.14166666666667, 83.1, 83.1, 83.26666666666667, 83.26666666666667, 83.65, 83.65, 83.3, 83.3, 83.05833333333334, 83.05833333333334, 82.95833333333333, 82.95833333333333, 82.8, 82.8, 82.86666666666666, 82.86666666666666, 82.71666666666667, 82.71666666666667, 83.13333333333334, 83.13333333333334, 82.925, 82.925, 83.11666666666666, 83.11666666666666, 82.91666666666667, 82.91666666666667, 82.85, 82.85, 82.73333333333333, 82.73333333333333, 82.84166666666667, 82.84166666666667, 83.05833333333334, 83.05833333333334, 82.7, 82.7, 82.85, 82.85, 82.80833333333334, 82.80833333333334, 83.16666666666667, 83.16666666666667, 83.21666666666667, 83.21666666666667, 83.24166666666666, 83.24166666666666, 83.35833333333333, 83.35833333333333, 83.13333333333334, 83.13333333333334, 82.95833333333333, 82.95833333333333, 83.46666666666667, 83.46666666666667, 83.80833333333334, 83.80833333333334, 83.53333333333333, 83.53333333333333, 83.525, 83.525, 83.31666666666666, 83.31666666666666, 83.74166666666666, 83.74166666666666, 84.025, 84.025, 83.79166666666667, 83.79166666666667, 83.50833333333334, 83.50833333333334, 83.75, 83.75, 83.69166666666666, 83.69166666666666, 83.75833333333334, 83.75833333333334, 84.13333333333334, 84.13333333333334, 83.625, 83.625, 83.45, 83.45, 83.68333333333334, 83.68333333333334, 83.85833333333333, 83.85833333333333, 84.0, 84.0, 83.91666666666667, 83.91666666666667, 83.63333333333334, 83.63333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.161, Test loss: 0.461, Test accuracy: 86.20
Final Round, Global train loss: 0.161, Global test loss: 1.252, Global test accuracy: 64.24
Average accuracy final 10 rounds: 85.71722222222222 

Average global accuracy final 10 rounds: 64.77 

3372.7148683071136
[2.7580742835998535, 5.516148567199707, 7.992208957672119, 10.468269348144531, 13.076659440994263, 15.685049533843994, 18.226130962371826, 20.767212390899658, 23.4279146194458, 26.088616847991943, 28.402318239212036, 30.71601963043213, 33.03825545310974, 35.36049127578735, 37.7324333190918, 40.10437536239624, 42.52597761154175, 44.947579860687256, 47.23747158050537, 49.527363300323486, 51.898192167282104, 54.26902103424072, 56.57389807701111, 58.878775119781494, 61.194103717803955, 63.509432315826416, 65.8125410079956, 68.1156497001648, 70.42818403244019, 72.74071836471558, 75.05167984962463, 77.36264133453369, 79.65636372566223, 81.95008611679077, 84.27028822898865, 86.59049034118652, 88.87780117988586, 91.1651120185852, 93.51415634155273, 95.86320066452026, 98.15578699111938, 100.4483733177185, 102.96380925178528, 105.47924518585205, 108.01233577728271, 110.54542636871338, 113.05348944664001, 115.56155252456665, 118.10250782966614, 120.64346313476562, 123.27086329460144, 125.89826345443726, 128.2553095817566, 130.61235570907593, 133.11069989204407, 135.6090440750122, 137.93910026550293, 140.26915645599365, 142.5595531463623, 144.84994983673096, 147.1649522781372, 149.47995471954346, 151.79090023040771, 154.10184574127197, 156.4019694328308, 158.70209312438965, 160.95226192474365, 163.20243072509766, 165.46068286895752, 167.71893501281738, 169.98552298545837, 172.25211095809937, 174.53900814056396, 176.82590532302856, 179.08942317962646, 181.35294103622437, 183.63654732704163, 185.9201536178589, 188.20736479759216, 190.49457597732544, 192.77235507965088, 195.05013418197632, 197.31866335868835, 199.5871925354004, 201.87894749641418, 204.17070245742798, 206.45202541351318, 208.7333483695984, 211.01885652542114, 213.3043646812439, 215.5840516090393, 217.86373853683472, 220.14791011810303, 222.43208169937134, 224.83723640441895, 227.24239110946655, 229.7636821269989, 232.28497314453125, 234.8065242767334, 237.32807540893555, 239.6317183971405, 241.93536138534546, 244.24627566337585, 246.55718994140625, 248.9188368320465, 251.28048372268677, 253.58043432235718, 255.8803849220276, 258.17062735557556, 260.46086978912354, 262.77492904663086, 265.0889883041382, 267.39689779281616, 269.70480728149414, 272.04770064353943, 274.3905940055847, 276.70159578323364, 279.01259756088257, 281.30173897743225, 283.59088039398193, 285.8739230632782, 288.15696573257446, 290.4109296798706, 292.66489362716675, 294.96112036705017, 297.2573471069336, 299.5168921947479, 301.77643728256226, 304.0558648109436, 306.33529233932495, 308.64253401756287, 310.9497756958008, 313.2448408603668, 315.53990602493286, 317.82856917381287, 320.11723232269287, 322.38455748558044, 324.651882648468, 326.9540927410126, 329.25630283355713, 331.5639057159424, 333.87150859832764, 336.2023046016693, 338.533100605011, 340.8685231208801, 343.20394563674927, 345.5651352405548, 347.92632484436035, 350.25852513313293, 352.5907254219055, 354.92343187332153, 357.25613832473755, 359.6215763092041, 361.98701429367065, 364.3255763053894, 366.66413831710815, 368.959894657135, 371.25565099716187, 373.5834057331085, 375.9111604690552, 378.30931305885315, 380.7074656486511, 383.0486271381378, 385.3897886276245, 387.68485045433044, 389.9799122810364, 392.280775308609, 394.58163833618164, 396.9491083621979, 399.3165783882141, 401.614462852478, 403.91234731674194, 406.22995686531067, 408.5475664138794, 410.8308267593384, 413.11408710479736, 415.41844391822815, 417.72280073165894, 420.0061767101288, 422.28955268859863, 424.5777654647827, 426.8659782409668, 429.15430545806885, 431.4426326751709, 433.75930428504944, 436.075975894928, 438.4217162132263, 440.76745653152466, 443.12286949157715, 445.47828245162964, 447.87996912002563, 450.28165578842163, 452.5759696960449, 454.8702836036682, 457.2095983028412, 459.54891300201416, 461.9343545436859, 464.31979608535767, 466.850403547287, 469.3810110092163, 472.07718086242676, 474.7733507156372]
[37.43333333333333, 37.43333333333333, 46.05, 46.05, 54.51111111111111, 54.51111111111111, 59.88333333333333, 59.88333333333333, 64.79444444444445, 64.79444444444445, 66.29444444444445, 66.29444444444445, 68.99444444444444, 68.99444444444444, 68.95555555555555, 68.95555555555555, 75.2611111111111, 75.2611111111111, 75.63888888888889, 75.63888888888889, 75.71111111111111, 75.71111111111111, 75.02777777777777, 75.02777777777777, 75.41666666666667, 75.41666666666667, 76.77222222222223, 76.77222222222223, 79.66111111111111, 79.66111111111111, 79.47777777777777, 79.47777777777777, 79.96111111111111, 79.96111111111111, 79.89444444444445, 79.89444444444445, 79.80555555555556, 79.80555555555556, 80.31666666666666, 80.31666666666666, 80.49444444444444, 80.49444444444444, 81.02777777777777, 81.02777777777777, 81.25, 81.25, 81.82222222222222, 81.82222222222222, 81.16111111111111, 81.16111111111111, 81.06111111111112, 81.06111111111112, 81.40555555555555, 81.40555555555555, 81.85, 81.85, 81.62222222222222, 81.62222222222222, 82.09444444444445, 82.09444444444445, 81.62777777777778, 81.62777777777778, 81.62222222222222, 81.62222222222222, 82.42777777777778, 82.42777777777778, 82.96111111111111, 82.96111111111111, 82.70555555555555, 82.70555555555555, 82.93333333333334, 82.93333333333334, 83.32222222222222, 83.32222222222222, 83.25555555555556, 83.25555555555556, 83.68333333333334, 83.68333333333334, 84.29444444444445, 84.29444444444445, 84.31666666666666, 84.31666666666666, 83.68888888888888, 83.68888888888888, 83.89444444444445, 83.89444444444445, 83.97222222222223, 83.97222222222223, 84.3, 84.3, 83.82222222222222, 83.82222222222222, 84.13333333333334, 84.13333333333334, 84.28888888888889, 84.28888888888889, 84.33333333333333, 84.33333333333333, 83.99444444444444, 83.99444444444444, 84.09444444444445, 84.09444444444445, 84.31666666666666, 84.31666666666666, 84.43888888888888, 84.43888888888888, 84.71111111111111, 84.71111111111111, 84.60555555555555, 84.60555555555555, 84.35555555555555, 84.35555555555555, 84.58888888888889, 84.58888888888889, 84.96666666666667, 84.96666666666667, 84.77777777777777, 84.77777777777777, 84.68333333333334, 84.68333333333334, 84.96111111111111, 84.96111111111111, 84.96666666666667, 84.96666666666667, 85.47222222222223, 85.47222222222223, 85.08888888888889, 85.08888888888889, 85.65, 85.65, 85.85, 85.85, 85.56111111111112, 85.56111111111112, 85.22222222222223, 85.22222222222223, 84.7388888888889, 84.7388888888889, 84.95555555555555, 84.95555555555555, 85.58888888888889, 85.58888888888889, 85.46111111111111, 85.46111111111111, 85.86111111111111, 85.86111111111111, 85.5111111111111, 85.5111111111111, 85.42222222222222, 85.42222222222222, 85.62777777777778, 85.62777777777778, 85.65, 85.65, 85.79444444444445, 85.79444444444445, 85.53333333333333, 85.53333333333333, 85.75, 85.75, 86.16666666666667, 86.16666666666667, 85.90555555555555, 85.90555555555555, 85.67777777777778, 85.67777777777778, 85.85, 85.85, 86.2611111111111, 86.2611111111111, 86.05555555555556, 86.05555555555556, 85.75555555555556, 85.75555555555556, 85.79444444444445, 85.79444444444445, 85.97222222222223, 85.97222222222223, 86.2388888888889, 86.2388888888889, 86.01666666666667, 86.01666666666667, 85.93333333333334, 85.93333333333334, 85.63888888888889, 85.63888888888889, 85.27777777777777, 85.27777777777777, 85.40555555555555, 85.40555555555555, 85.52777777777777, 85.52777777777777, 85.6, 85.6, 86.09444444444445, 86.09444444444445, 85.93888888888888, 85.93888888888888, 85.7388888888889, 85.7388888888889, 86.2, 86.2]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.365, Test loss: 1.941, Test accuracy: 28.70
Round   1, Train loss: 0.834, Test loss: 2.178, Test accuracy: 36.41
Round   2, Train loss: 0.875, Test loss: 1.423, Test accuracy: 46.91
Round   3, Train loss: 0.740, Test loss: 1.124, Test accuracy: 57.74
Round   4, Train loss: 0.675, Test loss: 1.021, Test accuracy: 58.13
Round   5, Train loss: 0.655, Test loss: 0.989, Test accuracy: 62.84
Round   6, Train loss: 0.642, Test loss: 0.839, Test accuracy: 68.42
Round   7, Train loss: 0.631, Test loss: 0.952, Test accuracy: 66.94
Round   8, Train loss: 0.559, Test loss: 0.632, Test accuracy: 74.06
Round   9, Train loss: 0.684, Test loss: 0.619, Test accuracy: 75.27
Round  10, Train loss: 0.597, Test loss: 0.644, Test accuracy: 74.41
Round  11, Train loss: 0.520, Test loss: 0.592, Test accuracy: 76.70
Round  12, Train loss: 0.520, Test loss: 0.511, Test accuracy: 79.46
Round  13, Train loss: 0.511, Test loss: 0.509, Test accuracy: 80.05
Round  14, Train loss: 0.485, Test loss: 0.498, Test accuracy: 80.44
Round  15, Train loss: 0.474, Test loss: 0.477, Test accuracy: 80.88
Round  16, Train loss: 0.488, Test loss: 0.468, Test accuracy: 81.57
Round  17, Train loss: 0.541, Test loss: 0.452, Test accuracy: 81.68
Round  18, Train loss: 0.479, Test loss: 0.449, Test accuracy: 81.85
Round  19, Train loss: 0.510, Test loss: 0.465, Test accuracy: 81.14
Round  20, Train loss: 0.379, Test loss: 0.460, Test accuracy: 81.11
Round  21, Train loss: 0.362, Test loss: 0.453, Test accuracy: 81.38
Round  22, Train loss: 0.417, Test loss: 0.440, Test accuracy: 82.22
Round  23, Train loss: 0.384, Test loss: 0.435, Test accuracy: 82.55
Round  24, Train loss: 0.425, Test loss: 0.420, Test accuracy: 83.35
Round  25, Train loss: 0.395, Test loss: 0.419, Test accuracy: 83.26
Round  26, Train loss: 0.430, Test loss: 0.410, Test accuracy: 83.87
Round  27, Train loss: 0.431, Test loss: 0.406, Test accuracy: 84.03
Round  28, Train loss: 0.414, Test loss: 0.407, Test accuracy: 84.07
Round  29, Train loss: 0.358, Test loss: 0.404, Test accuracy: 83.98
Round  30, Train loss: 0.355, Test loss: 0.397, Test accuracy: 84.21
Round  31, Train loss: 0.404, Test loss: 0.400, Test accuracy: 84.24
Round  32, Train loss: 0.360, Test loss: 0.391, Test accuracy: 84.71
Round  33, Train loss: 0.373, Test loss: 0.387, Test accuracy: 84.59
Round  34, Train loss: 0.382, Test loss: 0.391, Test accuracy: 84.71
Round  35, Train loss: 0.336, Test loss: 0.395, Test accuracy: 84.34
Round  36, Train loss: 0.368, Test loss: 0.384, Test accuracy: 84.97
Round  37, Train loss: 0.323, Test loss: 0.381, Test accuracy: 85.11
Round  38, Train loss: 0.317, Test loss: 0.372, Test accuracy: 85.49
Round  39, Train loss: 0.334, Test loss: 0.368, Test accuracy: 85.57
Round  40, Train loss: 0.296, Test loss: 0.374, Test accuracy: 85.22
Round  41, Train loss: 0.344, Test loss: 0.374, Test accuracy: 85.29
Round  42, Train loss: 0.336, Test loss: 0.373, Test accuracy: 85.15
Round  43, Train loss: 0.287, Test loss: 0.368, Test accuracy: 85.39
Round  44, Train loss: 0.290, Test loss: 0.368, Test accuracy: 85.39
Round  45, Train loss: 0.379, Test loss: 0.369, Test accuracy: 85.42
Round  46, Train loss: 0.330, Test loss: 0.369, Test accuracy: 85.29
Round  47, Train loss: 0.268, Test loss: 0.360, Test accuracy: 86.01
Round  48, Train loss: 0.335, Test loss: 0.358, Test accuracy: 86.13
Round  49, Train loss: 0.302, Test loss: 0.354, Test accuracy: 86.08
Round  50, Train loss: 0.273, Test loss: 0.354, Test accuracy: 86.32
Round  51, Train loss: 0.358, Test loss: 0.345, Test accuracy: 86.61
Round  52, Train loss: 0.301, Test loss: 0.349, Test accuracy: 86.05
Round  53, Train loss: 0.321, Test loss: 0.344, Test accuracy: 86.65
Round  54, Train loss: 0.291, Test loss: 0.342, Test accuracy: 86.47
Round  55, Train loss: 0.286, Test loss: 0.347, Test accuracy: 86.40
Round  56, Train loss: 0.233, Test loss: 0.345, Test accuracy: 86.59
Round  57, Train loss: 0.291, Test loss: 0.340, Test accuracy: 86.88
Round  58, Train loss: 0.310, Test loss: 0.337, Test accuracy: 86.74
Round  59, Train loss: 0.236, Test loss: 0.337, Test accuracy: 86.84
Round  60, Train loss: 0.265, Test loss: 0.340, Test accuracy: 86.76
Round  61, Train loss: 0.302, Test loss: 0.338, Test accuracy: 86.83
Round  62, Train loss: 0.281, Test loss: 0.336, Test accuracy: 86.87
Round  63, Train loss: 0.255, Test loss: 0.340, Test accuracy: 86.77
Round  64, Train loss: 0.295, Test loss: 0.336, Test accuracy: 86.77
Round  65, Train loss: 0.285, Test loss: 0.331, Test accuracy: 87.01
Round  66, Train loss: 0.245, Test loss: 0.333, Test accuracy: 86.91
Round  67, Train loss: 0.206, Test loss: 0.331, Test accuracy: 86.91
Round  68, Train loss: 0.237, Test loss: 0.327, Test accuracy: 87.18
Round  69, Train loss: 0.319, Test loss: 0.329, Test accuracy: 87.17
Round  70, Train loss: 0.251, Test loss: 0.335, Test accuracy: 86.83
Round  71, Train loss: 0.198, Test loss: 0.330, Test accuracy: 87.16
Round  72, Train loss: 0.328, Test loss: 0.336, Test accuracy: 86.92
Round  73, Train loss: 0.256, Test loss: 0.323, Test accuracy: 87.73
Round  74, Train loss: 0.249, Test loss: 0.326, Test accuracy: 87.33
Round  75, Train loss: 0.239, Test loss: 0.325, Test accuracy: 87.38
Round  76, Train loss: 0.254, Test loss: 0.329, Test accuracy: 87.46
Round  77, Train loss: 0.198, Test loss: 0.334, Test accuracy: 87.35
Round  78, Train loss: 0.238, Test loss: 0.336, Test accuracy: 87.12
Round  79, Train loss: 0.211, Test loss: 0.333, Test accuracy: 87.09
Round  80, Train loss: 0.208, Test loss: 0.333, Test accuracy: 87.05
Round  81, Train loss: 0.231, Test loss: 0.331, Test accuracy: 87.27
Round  82, Train loss: 0.175, Test loss: 0.329, Test accuracy: 87.57
Round  83, Train loss: 0.166, Test loss: 0.325, Test accuracy: 87.71
Round  84, Train loss: 0.235, Test loss: 0.326, Test accuracy: 87.57
Round  85, Train loss: 0.166, Test loss: 0.325, Test accuracy: 87.66
Round  86, Train loss: 0.214, Test loss: 0.325, Test accuracy: 87.57
Round  87, Train loss: 0.218, Test loss: 0.332, Test accuracy: 87.42
Round  88, Train loss: 0.246, Test loss: 0.330, Test accuracy: 87.34
Round  89, Train loss: 0.203, Test loss: 0.329, Test accuracy: 87.47
Round  90, Train loss: 0.164, Test loss: 0.324, Test accuracy: 87.57
Round  91, Train loss: 0.227, Test loss: 0.325, Test accuracy: 87.65
Round  92, Train loss: 0.217, Test loss: 0.322, Test accuracy: 87.66
Round  93, Train loss: 0.217, Test loss: 0.325, Test accuracy: 87.71
Round  94, Train loss: 0.205, Test loss: 0.328, Test accuracy: 87.70
Round  95, Train loss: 0.175, Test loss: 0.329, Test accuracy: 87.58
Round  96, Train loss: 0.184, Test loss: 0.326, Test accuracy: 88.00
Round  97, Train loss: 0.200, Test loss: 0.323, Test accuracy: 87.88
Round  98, Train loss: 0.219, Test loss: 0.327, Test accuracy: 87.83
Round  99, Train loss: 0.177, Test loss: 0.322, Test accuracy: 87.97
Final Round, Train loss: 0.161, Test loss: 0.323, Test accuracy: 87.85
Average accuracy final 10 rounds: 87.75444444444443
2463.5731897354126
[3.2145018577575684, 6.296590566635132, 9.182297229766846, 12.235690832138062, 15.18464970588684, 18.189073085784912, 21.266648292541504, 24.307943105697632, 27.35022282600403, 30.39087224006653, 33.38397526741028, 36.37247848510742, 39.30241870880127, 42.279956340789795, 45.15795564651489, 48.03669452667236, 50.91350960731506, 53.95019030570984, 56.995161294937134, 60.10730314254761, 63.1973819732666, 66.21590757369995, 69.20774006843567, 72.22082138061523, 75.30100560188293, 78.43061184883118, 81.59376072883606, 84.67868900299072, 87.68439865112305, 90.67329120635986, 93.76052045822144, 96.7577257156372, 99.79961967468262, 102.88470721244812, 106.0030825138092, 108.99268794059753, 112.06230330467224, 115.08459258079529, 118.11687850952148, 121.09549355506897, 124.239586353302, 127.28968524932861, 130.26652026176453, 133.35244941711426, 136.40422916412354, 139.37413001060486, 142.47055006027222, 145.50669813156128, 148.46351742744446, 151.34398746490479, 154.29598474502563, 157.29098105430603, 160.30760288238525, 163.36698818206787, 166.42337226867676, 169.4263186454773, 172.54273176193237, 175.52387928962708, 178.497145652771, 181.57198977470398, 184.64525747299194, 187.7299473285675, 190.74115824699402, 193.77666354179382, 196.7746138572693, 199.7485671043396, 202.80923628807068, 205.88242053985596, 208.83875584602356, 211.81348395347595, 214.9247431755066, 217.95289063453674, 220.90325236320496, 224.0566520690918, 227.11780881881714, 230.06625604629517, 233.11036443710327, 236.22234845161438, 239.14158844947815, 242.13969159126282, 245.2171869277954, 248.22048377990723, 251.14633965492249, 254.2111041545868, 257.1579144001007, 260.07877492904663, 263.22275614738464, 266.2899360656738, 269.2518515586853, 272.26315331459045, 275.2825663089752, 278.2942590713501, 281.33524680137634, 284.37455344200134, 287.3235719203949, 290.39977860450745, 293.39793586730957, 296.370178937912, 299.4046289920807, 302.48772644996643, 304.94802808761597]
[28.7, 36.40555555555556, 46.90555555555556, 57.74444444444445, 58.12777777777778, 62.84444444444444, 68.41666666666667, 66.94444444444444, 74.05555555555556, 75.27222222222223, 74.41111111111111, 76.7, 79.46111111111111, 80.05, 80.43888888888888, 80.88333333333334, 81.57222222222222, 81.67777777777778, 81.85, 81.14444444444445, 81.10555555555555, 81.38333333333334, 82.22222222222223, 82.55, 83.35, 83.2611111111111, 83.87222222222222, 84.02777777777777, 84.07222222222222, 83.97777777777777, 84.21111111111111, 84.2388888888889, 84.71111111111111, 84.58888888888889, 84.70555555555555, 84.33888888888889, 84.97222222222223, 85.11111111111111, 85.4888888888889, 85.56666666666666, 85.22222222222223, 85.28888888888889, 85.15, 85.39444444444445, 85.38888888888889, 85.42222222222222, 85.28888888888889, 86.00555555555556, 86.13333333333334, 86.08333333333333, 86.31666666666666, 86.61111111111111, 86.05, 86.65, 86.47222222222223, 86.4, 86.58888888888889, 86.87777777777778, 86.7388888888889, 86.83888888888889, 86.75555555555556, 86.82777777777778, 86.86666666666666, 86.76666666666667, 86.77222222222223, 87.00555555555556, 86.90555555555555, 86.90555555555555, 87.17777777777778, 87.17222222222222, 86.82777777777778, 87.16111111111111, 86.92222222222222, 87.72777777777777, 87.33333333333333, 87.38333333333334, 87.45555555555555, 87.35, 87.11666666666666, 87.08888888888889, 87.05, 87.27222222222223, 87.57222222222222, 87.71111111111111, 87.57222222222222, 87.66111111111111, 87.56666666666666, 87.41666666666667, 87.34444444444445, 87.47222222222223, 87.56666666666666, 87.65, 87.65555555555555, 87.71111111111111, 87.7, 87.57777777777778, 88.0, 87.87777777777778, 87.83333333333333, 87.97222222222223, 87.85]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  18.9100
Round 1 global test acc  20.5900
Round 2 global test acc  17.1100
Round 3 global test acc  24.9600
Round 4 global test acc  21.5200
Round 5 global test acc  18.6800
Round 6 global test acc  26.3400
Round 7 global test acc  25.5300
Round 8 global test acc  21.9000
Round 9 global test acc  27.0100
Round 10 global test acc  26.5500
Round 11 global test acc  33.8800
Round 12 global test acc  25.7900
Round 13 global test acc  32.5300
Round 14 global test acc  32.7800
Round 15 global test acc  32.9100
Round 16 global test acc  32.2800
Round 17 global test acc  33.3200
Round 18 global test acc  24.5800
Round 19 global test acc  26.9800
Round 20 global test acc  28.5100
Round 21 global test acc  27.4400
Round 22 global test acc  28.8800
Round 23 global test acc  28.5600
Round 24 global test acc  27.8400
Round 25 global test acc  29.7600
Round 26 global test acc  25.0300
Round 27 global test acc  29.2300
Round 28 global test acc  27.9000
Round 29 global test acc  29.5500
Round 30 global test acc  27.2600
Round 31 global test acc  27.5600
Round 32 global test acc  36.9400
Round 33 global test acc  27.4300
Round 34 global test acc  31.2100
Round 35 global test acc  37.5500
Round 36 global test acc  28.4100
Round 37 global test acc  36.3600
Round 38 global test acc  37.8500
Round 39 global test acc  36.3600
Round 40 global test acc  35.4300
Round 41 global test acc  32.0000
Round 42 global test acc  33.2000
Round 43 global test acc  31.4700
Round 44 global test acc  39.8900
Round 45 global test acc  32.7300
Round 46 global test acc  30.9900
Round 47 global test acc  40.4200
Round 48 global test acc  26.1900
Round 49 global test acc  31.9500
Round 50 global test acc  37.0000
Round 51 global test acc  41.0200
Round 52 global test acc  32.3900
Round 53 global test acc  36.8000
Round 54 global test acc  38.5300
Round 55 global test acc  38.4400
Round 56 global test acc  26.5400
Round 57 global test acc  32.5900
Round 58 global test acc  26.6800
Round 59 global test acc  32.5600
Round 60 global test acc  31.7300
Round 61 global test acc  29.9400
Round 62 global test acc  39.4700
Round 63 global test acc  34.7400
Round 64 global test acc  28.2200
Round 65 global test acc  32.6100
Round 66 global test acc  40.2600
Round 67 global test acc  42.0100
Round 68 global test acc  40.8700
Round 69 global test acc  41.1700
Round 70 global test acc  30.7800
Round 71 global test acc  34.3600
Round 72 global test acc  31.8200
Round 73 global test acc  41.1400
Round 74 global test acc  31.2300
Round 75 global test acc  32.1700
Round 76 global test acc  33.3700
Round 77 global test acc  34.5900
Round 78 global test acc  39.3200
Round 79 global test acc  37.8300
Round 80 global test acc  36.1500
Round 81 global test acc  35.1400
Round 82 global test acc  33.3000
Round 83 global test acc  30.9200
Round 84 global test acc  29.4400
Round 85 global test acc  28.3200
Round 86 global test acc  26.8100
Round 87 global test acc  24.8400
Round 88 global test acc  24.0700
Round 89 global test acc  23.3300
Round 90 global test acc  23.5800
Round 91 global test acc  22.2300
Round 92 global test acc  21.6000
Round 93 global test acc  20.5000
Round 94 global test acc  19.2400
Round 95 global test acc  17.8400
Round 96 global test acc  17.8600
Round 97 global test acc  16.9300
Round 98 global test acc  16.0000
Round 99 global test acc  15.2200
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.420, Test loss: 0.716, Test accuracy: 76.79
Average accuracy final 10 rounds: 76.84125
4957.462503194809
[7.090769052505493, 13.861989736557007, 20.53506302833557, 27.244653463363647, 33.96758723258972, 40.66926598548889, 47.221503257751465, 53.78197145462036, 60.34034776687622, 66.84604501724243, 73.45021033287048, 79.93066191673279, 86.52363276481628, 93.12043642997742, 99.74033260345459, 106.4320456981659, 113.1442940235138, 119.74608707427979, 126.37427043914795, 132.96344137191772, 139.6453835964203, 146.255704164505, 152.69653916358948, 159.335928440094, 165.7642686367035, 172.13606238365173, 178.44342803955078, 184.63437938690186, 190.83737969398499, 197.00690841674805, 203.19002485275269, 209.330970287323, 215.51084208488464, 221.70223259925842, 227.90454602241516, 234.03009128570557, 240.11869311332703, 246.22958827018738, 252.3631501197815, 258.551988363266, 264.7141592502594, 270.85020542144775, 277.08416056632996, 283.298721075058, 289.5151104927063, 295.75256299972534, 301.9413661956787, 308.0181076526642, 314.1254017353058, 320.2799005508423, 326.45732617378235, 332.68714451789856, 338.8985981941223, 345.1223785877228, 351.3240647315979, 357.4760193824768, 363.60925006866455, 369.8098850250244, 376.02562737464905, 382.2134132385254, 388.3661468029022, 394.5180492401123, 400.7085077762604, 406.8969581127167, 413.10059785842896, 419.2178018093109, 425.3933711051941, 431.5660126209259, 437.77347803115845, 443.8772075176239, 449.9944896697998, 456.24535942077637, 462.4655170440674, 468.65100479125977, 474.7774360179901, 480.9621651172638, 487.13740038871765, 493.3201892375946, 499.52261209487915, 505.7084128856659, 511.8416681289673, 517.9829347133636, 524.136804819107, 530.2680861949921, 536.4260718822479, 542.5873599052429, 548.7296469211578, 554.8533992767334, 560.9800159931183, 567.1099474430084, 573.2512927055359, 579.3970754146576, 585.4712197780609, 590.8103404045105, 596.1191799640656, 601.465423822403, 606.8139445781708, 612.1266753673553, 617.4476580619812, 622.804783821106, 624.9219777584076]
[32.8825, 40.795, 44.8625, 48.9525, 52.49, 55.0125, 56.8625, 58.24, 59.555, 61.0925, 61.2475, 63.2425, 63.4725, 64.91, 65.555, 66.6175, 67.3525, 67.0325, 67.06, 68.445, 67.8725, 69.1825, 70.07, 69.9975, 71.0025, 70.8875, 71.5575, 71.2425, 71.7125, 72.0025, 72.2675, 72.8625, 73.005, 72.8875, 73.22, 73.25, 73.3075, 73.6325, 74.1825, 74.2775, 73.935, 74.12, 74.3325, 74.29, 74.5475, 74.44, 74.2725, 73.995, 74.1125, 74.4925, 74.955, 75.2075, 75.045, 75.255, 75.1725, 75.1175, 75.2325, 75.4225, 75.4, 75.5, 75.4475, 75.3875, 75.6825, 75.775, 75.955, 75.985, 76.4475, 76.105, 75.7, 76.105, 76.2475, 76.1825, 76.3075, 76.0525, 76.265, 76.0725, 76.525, 76.92, 76.3525, 76.7625, 76.6625, 76.845, 76.6825, 76.885, 77.23, 76.815, 76.095, 76.5475, 76.6425, 76.725, 76.6075, 76.3925, 76.99, 77.02, 77.1325, 77.185, 77.01, 76.86, 76.7925, 76.4225, 76.79]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.200, Test loss: 1.896, Test accuracy: 33.79
Round   1, Train loss: 1.843, Test loss: 1.644, Test accuracy: 41.52
Round   2, Train loss: 1.654, Test loss: 1.520, Test accuracy: 46.45
Round   3, Train loss: 1.566, Test loss: 1.441, Test accuracy: 49.94
Round   4, Train loss: 1.491, Test loss: 1.361, Test accuracy: 53.48
Round   5, Train loss: 1.426, Test loss: 1.283, Test accuracy: 55.46
Round   6, Train loss: 1.343, Test loss: 1.243, Test accuracy: 56.88
Round   7, Train loss: 1.293, Test loss: 1.208, Test accuracy: 58.42
Round   8, Train loss: 1.253, Test loss: 1.167, Test accuracy: 59.99
Round   9, Train loss: 1.208, Test loss: 1.153, Test accuracy: 60.10
Round  10, Train loss: 1.153, Test loss: 1.159, Test accuracy: 60.46
Round  11, Train loss: 1.176, Test loss: 1.080, Test accuracy: 62.51
Round  12, Train loss: 1.103, Test loss: 1.047, Test accuracy: 64.09
Round  13, Train loss: 1.049, Test loss: 1.040, Test accuracy: 64.70
Round  14, Train loss: 1.062, Test loss: 1.015, Test accuracy: 65.42
Round  15, Train loss: 1.029, Test loss: 0.984, Test accuracy: 66.77
Round  16, Train loss: 1.003, Test loss: 0.983, Test accuracy: 66.55
Round  17, Train loss: 0.976, Test loss: 0.977, Test accuracy: 66.89
Round  18, Train loss: 0.975, Test loss: 0.933, Test accuracy: 68.55
Round  19, Train loss: 0.931, Test loss: 0.911, Test accuracy: 69.44
Round  20, Train loss: 0.912, Test loss: 0.893, Test accuracy: 69.67
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1966, in train
    self.filter_by_loss2(net=net, concept_matrix_local=concept_matrix_local, iter_num=iter2,
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1809, in filter_by_loss2
    loss = self.loss_func(log_probs, lable_tmp)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8615
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2855
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8175
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0305
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6075
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7890
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1395
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6715
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5230
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8465
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.350, Test loss: 4.618, Test accuracy: 34.51
Final Round, Global train loss: 0.350, Global test loss: 1.932, Global test accuracy: 31.02
Average accuracy final 10 rounds: 33.91575 

Average global accuracy final 10 rounds: 33.2485 

5967.901341199875
[4.957217454910278, 9.914434909820557, 14.298394918441772, 18.68235492706299, 22.975258588790894, 27.2681622505188, 31.615724086761475, 35.96328592300415, 40.28782391548157, 44.612361907958984, 49.055423736572266, 53.49848556518555, 58.39583992958069, 63.29319429397583, 67.5833375453949, 71.87348079681396, 76.15504598617554, 80.43661117553711, 84.93547439575195, 89.4343376159668, 94.81144738197327, 100.18855714797974, 104.49135088920593, 108.79414463043213, 113.12665963172913, 117.45917463302612, 122.06816983222961, 126.6771650314331, 130.95098519325256, 135.22480535507202, 139.51701664924622, 143.8092279434204, 148.12322354316711, 152.43721914291382, 156.74275708198547, 161.04829502105713, 165.35476517677307, 169.661235332489, 173.98499131202698, 178.30874729156494, 182.63852167129517, 186.9682960510254, 191.26138520240784, 195.55447435379028, 199.8715295791626, 204.1885848045349, 208.48054575920105, 212.7725067138672, 217.04895544052124, 221.3254041671753, 225.58789801597595, 229.8503918647766, 234.14562225341797, 238.44085264205933, 242.75551176071167, 247.070170879364, 251.30723762512207, 255.54430437088013, 259.7561812400818, 263.96805810928345, 268.3406286239624, 272.71319913864136, 276.9534010887146, 281.19360303878784, 285.47859597206116, 289.7635889053345, 295.3128459453583, 300.8621029853821, 305.12755131721497, 309.39299964904785, 313.7544355392456, 318.11587142944336, 322.45871591567993, 326.8015604019165, 331.1022322177887, 335.4029040336609, 339.68264627456665, 343.9623885154724, 348.22276186943054, 352.4831352233887, 356.69006967544556, 360.89700412750244, 365.1298716068268, 369.3627390861511, 373.64744114875793, 377.93214321136475, 382.22901344299316, 386.5258836746216, 390.788143157959, 395.0504026412964, 399.2891936302185, 403.5279846191406, 407.74259519577026, 411.9572057723999, 416.17178750038147, 420.38636922836304, 424.6562442779541, 428.92611932754517, 433.20784759521484, 437.4895758628845, 441.7878110408783, 446.08604621887207, 450.38243985176086, 454.67883348464966, 458.96597599983215, 463.25311851501465, 467.5058846473694, 471.7586507797241, 476.0687334537506, 480.3788161277771, 484.65878891944885, 488.9387617111206, 493.19394874572754, 497.4491357803345, 501.6649305820465, 505.88072538375854, 510.18167996406555, 514.4826345443726, 518.7662830352783, 523.0499315261841, 527.343775510788, 531.6376194953918, 535.892728805542, 540.1478381156921, 544.3772747516632, 548.6067113876343, 552.8880743980408, 557.1694374084473, 561.4456541538239, 565.7218708992004, 569.9753277301788, 574.2287845611572, 578.5153963565826, 582.802008152008, 587.0653154850006, 591.3286228179932, 595.5512681007385, 599.7739133834839, 604.0411925315857, 608.3084716796875, 612.5623905658722, 616.8163094520569, 621.1109821796417, 625.4056549072266, 629.6561801433563, 633.9067053794861, 638.1378591060638, 642.3690128326416, 646.6282448768616, 650.8874769210815, 655.138200044632, 659.3889231681824, 663.6872835159302, 667.985643863678, 672.3504333496094, 676.7152228355408, 680.9868819713593, 685.2585411071777, 689.5636339187622, 693.8687267303467, 698.1172318458557, 702.3657369613647, 706.6900870800018, 711.0144371986389, 715.3002808094025, 719.586124420166, 723.8955228328705, 728.204921245575, 732.4940738677979, 736.7832264900208, 741.0115587711334, 745.2398910522461, 749.4836530685425, 753.7274150848389, 757.9884128570557, 762.2494106292725, 766.5366768836975, 770.8239431381226, 775.140864610672, 779.4577860832214, 783.742201089859, 788.0266160964966, 792.3261988162994, 796.6257815361023, 801.0568151473999, 805.4878487586975, 809.7715137004852, 814.055178642273, 818.5676040649414, 823.0800294876099, 827.3645622730255, 831.6490950584412, 835.9159083366394, 840.1827216148376, 844.5030105113983, 848.823299407959, 853.2064230442047, 857.5895466804504, 861.8898234367371, 866.1901001930237, 868.3506467342377, 870.5111932754517]
[24.8025, 24.8025, 33.435, 33.435, 33.305, 33.305, 33.795, 33.795, 34.68, 34.68, 35.245, 35.245, 36.1475, 36.1475, 35.595, 35.595, 36.6025, 36.6025, 35.7225, 35.7225, 36.275, 36.275, 36.5125, 36.5125, 35.86, 35.86, 36.1375, 36.1375, 36.23, 36.23, 36.545, 36.545, 36.47, 36.47, 36.7675, 36.7675, 37.375, 37.375, 37.3825, 37.3825, 37.3, 37.3, 36.7525, 36.7525, 36.81, 36.81, 36.9775, 36.9775, 36.7975, 36.7975, 36.64, 36.64, 36.8825, 36.8825, 36.855, 36.855, 36.5575, 36.5575, 36.5725, 36.5725, 36.5475, 36.5475, 36.3425, 36.3425, 35.9925, 35.9925, 35.7, 35.7, 35.495, 35.495, 35.2575, 35.2575, 35.2575, 35.2575, 35.145, 35.145, 34.8425, 34.8425, 35.1925, 35.1925, 35.2925, 35.2925, 35.44, 35.44, 35.4525, 35.4525, 35.0825, 35.0825, 35.13, 35.13, 34.6075, 34.6075, 34.3625, 34.3625, 34.4125, 34.4125, 34.44, 34.44, 34.6175, 34.6175, 34.8, 34.8, 34.4925, 34.4925, 34.5225, 34.5225, 34.67, 34.67, 34.48, 34.48, 34.6525, 34.6525, 34.4825, 34.4825, 34.185, 34.185, 33.9975, 33.9975, 33.7875, 33.7875, 33.97, 33.97, 34.0475, 34.0475, 33.9425, 33.9425, 33.8425, 33.8425, 33.8625, 33.8625, 33.995, 33.995, 34.0525, 34.0525, 33.9225, 33.9225, 33.9075, 33.9075, 33.9875, 33.9875, 34.0375, 34.0375, 34.2575, 34.2575, 34.2725, 34.2725, 34.3075, 34.3075, 34.1625, 34.1625, 33.995, 33.995, 33.6775, 33.6775, 33.89, 33.89, 33.965, 33.965, 33.5875, 33.5875, 33.8625, 33.8625, 33.93, 33.93, 33.6575, 33.6575, 33.3725, 33.3725, 33.6125, 33.6125, 33.53, 33.53, 33.7125, 33.7125, 34.3075, 34.3075, 33.905, 33.905, 33.79, 33.79, 33.9975, 33.9975, 33.9825, 33.9825, 33.9425, 33.9425, 33.93, 33.93, 33.9175, 33.9175, 33.8175, 33.8175, 33.9925, 33.9925, 33.87, 33.87, 33.8775, 33.8775, 33.83, 33.83, 34.5075, 34.5075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8645
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2890
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8270
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1135
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5680
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8130
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.2520
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6795
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5125
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8630
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.544, Test loss: 1.715, Test accuracy: 59.05
Final Round, Global train loss: 0.544, Global test loss: 1.914, Global test accuracy: 42.00
Average accuracy final 10 rounds: 59.74999999999999 

Average global accuracy final 10 rounds: 41.43083333333333 

1843.8794839382172
[1.7739672660827637, 3.5479345321655273, 4.841006755828857, 6.1340789794921875, 7.427464485168457, 8.720849990844727, 9.999529600143433, 11.278209209442139, 12.577208757400513, 13.876208305358887, 15.16386866569519, 16.451529026031494, 17.739340782165527, 19.02715253829956, 20.309136629104614, 21.591120719909668, 22.864623069763184, 24.1381254196167, 25.420076608657837, 26.702027797698975, 27.98301386833191, 29.263999938964844, 30.56075096130371, 31.857501983642578, 33.14634084701538, 34.435179710388184, 35.722148180007935, 37.009116649627686, 38.29380917549133, 39.57850170135498, 40.91188645362854, 42.2452712059021, 43.556766510009766, 44.86826181411743, 46.15498161315918, 47.44170141220093, 48.74035930633545, 50.03901720046997, 51.31052827835083, 52.58203935623169, 53.87947344779968, 55.176907539367676, 56.457173109054565, 57.737438678741455, 59.01852464675903, 60.29961061477661, 61.58886361122131, 62.878116607666016, 64.17482852935791, 65.4715404510498, 66.74623346328735, 68.0209264755249, 69.3279116153717, 70.6348967552185, 71.92657160758972, 73.21824645996094, 74.49672174453735, 75.77519702911377, 77.06828832626343, 78.36137962341309, 79.63754177093506, 80.91370391845703, 82.21468782424927, 83.5156717300415, 84.7970016002655, 86.0783314704895, 87.35974025726318, 88.64114904403687, 89.92366671562195, 91.20618438720703, 92.50060486793518, 93.79502534866333, 95.0735228061676, 96.35202026367188, 97.6495418548584, 98.94706344604492, 100.23753833770752, 101.52801322937012, 102.7998297214508, 104.0716462135315, 105.37916421890259, 106.68668222427368, 107.98011898994446, 109.27355575561523, 110.56059169769287, 111.84762763977051, 113.13351655006409, 114.41940546035767, 115.72079277038574, 117.02218008041382, 118.29867053031921, 119.57516098022461, 120.86861634254456, 122.1620717048645, 123.72271370887756, 125.28335571289062, 126.58585095405579, 127.88834619522095, 129.1856610774994, 130.48297595977783, 131.75953221321106, 133.0360884666443, 134.4767026901245, 135.91731691360474, 137.20019698143005, 138.48307704925537, 139.76748180389404, 141.05188655853271, 142.32979822158813, 143.60770988464355, 144.89737844467163, 146.1870470046997, 147.4600965976715, 148.7331461906433, 150.0385980606079, 151.3440499305725, 152.6241579055786, 153.90426588058472, 155.19721174240112, 156.49015760421753, 157.77761268615723, 159.06506776809692, 160.3411841392517, 161.6173005104065, 162.92975163459778, 164.24220275878906, 165.56565594673157, 166.88910913467407, 168.17866110801697, 169.46821308135986, 170.75503492355347, 172.04185676574707, 173.3283760547638, 174.61489534378052, 175.8953399658203, 177.1757845878601, 178.46636319160461, 179.75694179534912, 181.03997325897217, 182.32300472259521, 183.60060286521912, 184.87820100784302, 186.23751187324524, 187.59682273864746, 188.87592148780823, 190.155020236969, 191.45369672775269, 192.75237321853638, 194.03903794288635, 195.32570266723633, 196.60637664794922, 197.8870506286621, 199.23285055160522, 200.57865047454834, 201.8760633468628, 203.17347621917725, 204.43942785263062, 205.70537948608398, 206.9985625743866, 208.2917456626892, 209.5811893939972, 210.87063312530518, 212.13750767707825, 213.40438222885132, 214.69924068450928, 215.99409914016724, 217.2844693660736, 218.57483959197998, 219.85172367095947, 221.12860774993896, 222.40275382995605, 223.67689990997314, 224.95410704612732, 226.2313141822815, 227.50532722473145, 228.7793402671814, 230.1766061782837, 231.573872089386, 232.84360241889954, 234.1133327484131, 235.39187812805176, 236.67042350769043, 237.99916625022888, 239.32790899276733, 240.6209499835968, 241.91399097442627, 243.19285774230957, 244.47172451019287, 245.8723428249359, 247.27296113967896, 248.64492630958557, 250.0168914794922, 251.4533088207245, 252.8897261619568, 254.32286262512207, 255.75599908828735, 257.1962101459503, 258.6364212036133, 259.92068672180176, 261.20495223999023, 267.6239757537842, 274.0429992675781]
[27.733333333333334, 27.733333333333334, 32.525, 32.525, 42.30833333333333, 42.30833333333333, 46.425, 46.425, 48.28333333333333, 48.28333333333333, 49.59166666666667, 49.59166666666667, 49.291666666666664, 49.291666666666664, 50.05, 50.05, 52.275, 52.275, 54.24166666666667, 54.24166666666667, 53.125, 53.125, 54.608333333333334, 54.608333333333334, 54.5, 54.5, 58.275, 58.275, 58.266666666666666, 58.266666666666666, 59.93333333333333, 59.93333333333333, 59.75833333333333, 59.75833333333333, 60.2, 60.2, 60.80833333333333, 60.80833333333333, 61.1, 61.1, 61.49166666666667, 61.49166666666667, 61.225, 61.225, 61.28333333333333, 61.28333333333333, 62.125, 62.125, 62.766666666666666, 62.766666666666666, 62.35, 62.35, 61.96666666666667, 61.96666666666667, 61.975, 61.975, 61.06666666666667, 61.06666666666667, 61.65833333333333, 61.65833333333333, 61.766666666666666, 61.766666666666666, 62.99166666666667, 62.99166666666667, 62.95, 62.95, 63.44166666666667, 63.44166666666667, 63.141666666666666, 63.141666666666666, 63.13333333333333, 63.13333333333333, 61.49166666666667, 61.49166666666667, 61.05, 61.05, 61.25, 61.25, 61.833333333333336, 61.833333333333336, 62.05, 62.05, 61.95, 61.95, 62.0, 62.0, 61.56666666666667, 61.56666666666667, 60.858333333333334, 60.858333333333334, 60.775, 60.775, 60.825, 60.825, 60.31666666666667, 60.31666666666667, 60.575, 60.575, 61.325, 61.325, 61.416666666666664, 61.416666666666664, 61.725, 61.725, 61.666666666666664, 61.666666666666664, 61.7, 61.7, 60.425, 60.425, 60.541666666666664, 60.541666666666664, 61.016666666666666, 61.016666666666666, 61.40833333333333, 61.40833333333333, 61.075, 61.075, 60.975, 60.975, 61.108333333333334, 61.108333333333334, 60.93333333333333, 60.93333333333333, 60.06666666666667, 60.06666666666667, 60.43333333333333, 60.43333333333333, 60.09166666666667, 60.09166666666667, 60.30833333333333, 60.30833333333333, 60.25, 60.25, 60.75, 60.75, 60.71666666666667, 60.71666666666667, 60.84166666666667, 60.84166666666667, 59.74166666666667, 59.74166666666667, 59.483333333333334, 59.483333333333334, 59.5, 59.5, 60.3, 60.3, 60.875, 60.875, 59.81666666666667, 59.81666666666667, 59.63333333333333, 59.63333333333333, 59.85, 59.85, 59.775, 59.775, 60.18333333333333, 60.18333333333333, 60.016666666666666, 60.016666666666666, 60.125, 60.125, 60.208333333333336, 60.208333333333336, 59.8, 59.8, 59.46666666666667, 59.46666666666667, 58.96666666666667, 58.96666666666667, 59.00833333333333, 59.00833333333333, 59.38333333333333, 59.38333333333333, 59.7, 59.7, 59.63333333333333, 59.63333333333333, 60.06666666666667, 60.06666666666667, 60.05833333333333, 60.05833333333333, 60.13333333333333, 60.13333333333333, 59.975, 59.975, 59.80833333333333, 59.80833333333333, 59.99166666666667, 59.99166666666667, 58.833333333333336, 58.833333333333336, 59.125, 59.125, 59.55833333333333, 59.55833333333333, 59.95, 59.95, 59.05, 59.05]
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8560
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2850
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8265
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0575
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5600
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8015
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0105
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7290
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5240
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8455
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.701, Test loss: 1.440, Test accuracy: 59.36
Final Round, Global train loss: 0.701, Global test loss: 0.889, Global test accuracy: 71.75
Average accuracy final 10 rounds: 61.251000000000005 

Average global accuracy final 10 rounds: 71.45700000000001 

6440.742843389511
[5.458752632141113, 10.917505264282227, 16.14120888710022, 21.364912509918213, 26.589845418930054, 31.814778327941895, 37.09364032745361, 42.37250232696533, 47.65316343307495, 52.93382453918457, 58.197919607162476, 63.46201467514038, 68.74339556694031, 74.02477645874023, 78.63837194442749, 83.25196743011475, 87.9805600643158, 92.70915269851685, 97.90790104866028, 103.10664939880371, 108.30064630508423, 113.49464321136475, 118.67735934257507, 123.8600754737854, 129.07334184646606, 134.28660821914673, 139.56797289848328, 144.84933757781982, 149.99112844467163, 155.13291931152344, 160.29337239265442, 165.4538254737854, 170.61492371559143, 175.77602195739746, 180.47083520889282, 185.16564846038818, 189.81414318084717, 194.46263790130615, 199.6566355228424, 204.85063314437866, 210.18669843673706, 215.52276372909546, 220.75814175605774, 225.99351978302002, 231.16848039627075, 236.34344100952148, 241.56075882911682, 246.77807664871216, 252.055668592453, 257.33326053619385, 262.6233687400818, 267.9134769439697, 273.10631799697876, 278.2991590499878, 283.52485036849976, 288.7505416870117, 294.02302718162537, 299.295512676239, 304.5276792049408, 309.7598457336426, 314.9734396934509, 320.1870336532593, 325.3721272945404, 330.55722093582153, 335.730672121048, 340.9041233062744, 346.0782518386841, 351.25238037109375, 356.4439437389374, 361.635507106781, 366.83992052078247, 372.04433393478394, 377.2601134777069, 382.4758930206299, 387.7240614891052, 392.97222995758057, 398.27864480018616, 403.58505964279175, 408.90719652175903, 414.2293334007263, 419.4986991882324, 424.7680649757385, 430.0015637874603, 435.23506259918213, 440.496390581131, 445.75771856307983, 450.9442241191864, 456.13072967529297, 461.3268506526947, 466.52297163009644, 471.69888854026794, 476.87480545043945, 482.0886278152466, 487.3024501800537, 491.97360014915466, 496.6447501182556, 501.3385510444641, 506.0323519706726, 510.70496892929077, 515.3775858879089, 520.0245118141174, 524.6714377403259, 529.3507339954376, 534.0300302505493, 538.7737004756927, 543.5173707008362, 548.1595692634583, 552.8017678260803, 557.4937653541565, 562.1857628822327, 566.8186094760895, 571.4514560699463, 576.1210517883301, 580.7906475067139, 585.4769020080566, 590.1631565093994, 594.8260283470154, 599.4889001846313, 604.1477041244507, 608.80650806427, 613.4596636295319, 618.1128191947937, 622.7816412448883, 627.4504632949829, 632.1216554641724, 636.7928476333618, 641.4667189121246, 646.1405901908875, 650.8035242557526, 655.4664583206177, 660.1483619213104, 664.8302655220032, 669.4733538627625, 674.1164422035217, 678.786655664444, 683.4568691253662, 688.1177432537079, 692.7786173820496, 697.44864153862, 702.1186656951904, 706.7822542190552, 711.4458427429199, 716.0927155017853, 720.7395882606506, 725.3864274024963, 730.033266544342, 734.6934552192688, 739.3536438941956, 744.0289452075958, 748.7042465209961, 753.4077236652374, 758.1112008094788, 762.7417778968811, 767.3723549842834, 772.0864098072052, 776.800464630127, 781.4666903018951, 786.1329159736633, 790.8126873970032, 795.492458820343, 800.2138376235962, 804.9352164268494, 810.0876154899597, 815.2400145530701, 820.3162829875946, 825.3925514221191, 830.6194138526917, 835.8462762832642, 841.0518176555634, 846.2573590278625, 850.918075799942, 855.5787925720215, 860.3170573711395, 865.0553221702576, 869.7621054649353, 874.468888759613, 879.1090459823608, 883.7492032051086, 888.4021286964417, 893.0550541877747, 897.6739783287048, 902.292902469635, 906.9685523509979, 911.6442022323608, 916.3082160949707, 920.9722299575806, 925.6466240882874, 930.3210182189941, 935.0207824707031, 939.7205467224121, 944.4263184070587, 949.1320900917053, 953.8450818061829, 958.5580735206604, 963.198320388794, 967.8385672569275, 972.4885005950928, 977.138433933258, 981.7635691165924, 986.3887042999268, 988.7268588542938, 991.0650134086609]
[25.5725, 25.5725, 30.9175, 30.9175, 34.26, 34.26, 35.5775, 35.5775, 38.1175, 38.1175, 39.2825, 39.2825, 41.5525, 41.5525, 43.6, 43.6, 44.3775, 44.3775, 45.04, 45.04, 46.345, 46.345, 47.4625, 47.4625, 47.515, 47.515, 48.08, 48.08, 49.7775, 49.7775, 50.3275, 50.3275, 50.825, 50.825, 51.8375, 51.8375, 52.2825, 52.2825, 52.68, 52.68, 53.1875, 53.1875, 53.845, 53.845, 54.2275, 54.2275, 54.1675, 54.1675, 54.585, 54.585, 55.3125, 55.3125, 55.56, 55.56, 56.45, 56.45, 56.375, 56.375, 56.4175, 56.4175, 56.61, 56.61, 56.555, 56.555, 56.875, 56.875, 56.8125, 56.8125, 57.16, 57.16, 57.3475, 57.3475, 57.805, 57.805, 57.91, 57.91, 58.6325, 58.6325, 57.9825, 57.9825, 58.0875, 58.0875, 58.74, 58.74, 58.645, 58.645, 58.9975, 58.9975, 59.4875, 59.4875, 59.36, 59.36, 59.3775, 59.3775, 59.6225, 59.6225, 60.1325, 60.1325, 59.8825, 59.8825, 60.0, 60.0, 60.045, 60.045, 60.2925, 60.2925, 59.9675, 59.9675, 59.795, 59.795, 59.5975, 59.5975, 59.53, 59.53, 59.575, 59.575, 59.975, 59.975, 60.05, 60.05, 60.145, 60.145, 60.63, 60.63, 60.8, 60.8, 60.465, 60.465, 60.44, 60.44, 60.6525, 60.6525, 60.73, 60.73, 60.485, 60.485, 60.7175, 60.7175, 60.81, 60.81, 60.9125, 60.9125, 60.7925, 60.7925, 60.9225, 60.9225, 61.0, 61.0, 61.1925, 61.1925, 61.1675, 61.1675, 61.2575, 61.2575, 60.97, 60.97, 61.1125, 61.1125, 61.63, 61.63, 61.3825, 61.3825, 61.6325, 61.6325, 61.72, 61.72, 61.735, 61.735, 61.215, 61.215, 61.15, 61.15, 61.4525, 61.4525, 61.2775, 61.2775, 61.23, 61.23, 61.125, 61.125, 60.88, 60.88, 61.165, 61.165, 61.4775, 61.4775, 61.5375, 61.5375, 61.665, 61.665, 61.475, 61.475, 61.15, 61.15, 61.045, 61.045, 61.225, 61.225, 60.89, 60.89, 59.3625, 59.3625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8630
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3495
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8275
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1165
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5900
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8145
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1190
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7475
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4915
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8435
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.498, Test loss: 1.914, Test accuracy: 35.02
Round   1, Train loss: 0.921, Test loss: 1.914, Test accuracy: 33.60
Round   2, Train loss: 0.907, Test loss: 1.294, Test accuracy: 55.29
Round   3, Train loss: 0.795, Test loss: 1.146, Test accuracy: 55.95
Round   4, Train loss: 0.732, Test loss: 1.132, Test accuracy: 60.63
Round   5, Train loss: 0.749, Test loss: 1.161, Test accuracy: 58.99
Round   6, Train loss: 0.689, Test loss: 1.054, Test accuracy: 59.83
Round   7, Train loss: 0.594, Test loss: 1.036, Test accuracy: 62.41
Round   8, Train loss: 0.698, Test loss: 0.862, Test accuracy: 67.78
Round   9, Train loss: 0.700, Test loss: 0.721, Test accuracy: 71.98
Round  10, Train loss: 0.589, Test loss: 0.702, Test accuracy: 73.81
Round  11, Train loss: 0.593, Test loss: 0.686, Test accuracy: 74.91
Round  12, Train loss: 0.522, Test loss: 0.697, Test accuracy: 75.29
Round  13, Train loss: 0.592, Test loss: 0.548, Test accuracy: 77.03
Round  14, Train loss: 0.491, Test loss: 0.536, Test accuracy: 77.42
Round  15, Train loss: 0.434, Test loss: 0.531, Test accuracy: 77.99
Round  16, Train loss: 0.513, Test loss: 0.523, Test accuracy: 78.30
Round  17, Train loss: 0.482, Test loss: 0.520, Test accuracy: 78.14
Round  18, Train loss: 0.460, Test loss: 0.525, Test accuracy: 78.38
Round  19, Train loss: 0.556, Test loss: 0.496, Test accuracy: 79.48
Round  20, Train loss: 0.584, Test loss: 0.484, Test accuracy: 80.28
Round  21, Train loss: 0.457, Test loss: 0.487, Test accuracy: 80.42
Round  22, Train loss: 0.462, Test loss: 0.464, Test accuracy: 81.20
Round  23, Train loss: 0.456, Test loss: 0.453, Test accuracy: 81.65
Round  24, Train loss: 0.519, Test loss: 0.461, Test accuracy: 81.23
Round  25, Train loss: 0.404, Test loss: 0.465, Test accuracy: 81.08
Round  26, Train loss: 0.371, Test loss: 0.460, Test accuracy: 80.94
Round  27, Train loss: 0.453, Test loss: 0.451, Test accuracy: 81.46
Round  28, Train loss: 0.482, Test loss: 0.443, Test accuracy: 81.92
Round  29, Train loss: 0.457, Test loss: 0.436, Test accuracy: 82.43
Round  30, Train loss: 0.489, Test loss: 0.435, Test accuracy: 82.15
Round  31, Train loss: 0.420, Test loss: 0.426, Test accuracy: 82.73
Round  32, Train loss: 0.410, Test loss: 0.427, Test accuracy: 82.79
Round  33, Train loss: 0.395, Test loss: 0.426, Test accuracy: 82.77
Round  34, Train loss: 0.356, Test loss: 0.423, Test accuracy: 82.74
Round  35, Train loss: 0.452, Test loss: 0.416, Test accuracy: 83.32
Round  36, Train loss: 0.364, Test loss: 0.422, Test accuracy: 82.87
Round  37, Train loss: 0.475, Test loss: 0.417, Test accuracy: 83.28
Round  38, Train loss: 0.435, Test loss: 0.414, Test accuracy: 83.26
Round  39, Train loss: 0.423, Test loss: 0.411, Test accuracy: 83.58
Round  40, Train loss: 0.372, Test loss: 0.398, Test accuracy: 83.97
Round  41, Train loss: 0.382, Test loss: 0.409, Test accuracy: 83.84
Round  42, Train loss: 0.336, Test loss: 0.404, Test accuracy: 84.12
Round  43, Train loss: 0.414, Test loss: 0.401, Test accuracy: 84.09
Round  44, Train loss: 0.365, Test loss: 0.400, Test accuracy: 84.33
Round  45, Train loss: 0.353, Test loss: 0.400, Test accuracy: 83.94
Round  46, Train loss: 0.348, Test loss: 0.394, Test accuracy: 84.26
Round  47, Train loss: 0.418, Test loss: 0.396, Test accuracy: 84.37
Round  48, Train loss: 0.381, Test loss: 0.394, Test accuracy: 84.39
Round  49, Train loss: 0.361, Test loss: 0.383, Test accuracy: 84.70
Round  50, Train loss: 0.284, Test loss: 0.382, Test accuracy: 84.71
Round  51, Train loss: 0.238, Test loss: 0.379, Test accuracy: 84.52
Round  52, Train loss: 0.257, Test loss: 0.386, Test accuracy: 84.08
Round  53, Train loss: 0.277, Test loss: 0.380, Test accuracy: 84.75
Round  54, Train loss: 0.401, Test loss: 0.380, Test accuracy: 85.12
Round  55, Train loss: 0.325, Test loss: 0.379, Test accuracy: 85.08
Round  56, Train loss: 0.337, Test loss: 0.373, Test accuracy: 85.34
Round  57, Train loss: 0.351, Test loss: 0.371, Test accuracy: 85.43
Round  58, Train loss: 0.313, Test loss: 0.371, Test accuracy: 85.31
Round  59, Train loss: 0.305, Test loss: 0.372, Test accuracy: 85.12
Round  60, Train loss: 0.375, Test loss: 0.362, Test accuracy: 85.59
Round  61, Train loss: 0.308, Test loss: 0.370, Test accuracy: 85.21
Round  62, Train loss: 0.246, Test loss: 0.373, Test accuracy: 85.33
Round  63, Train loss: 0.258, Test loss: 0.367, Test accuracy: 85.50
Round  64, Train loss: 0.390, Test loss: 0.371, Test accuracy: 85.47
Round  65, Train loss: 0.328, Test loss: 0.366, Test accuracy: 85.40
Round  66, Train loss: 0.283, Test loss: 0.365, Test accuracy: 85.53
Round  67, Train loss: 0.302, Test loss: 0.369, Test accuracy: 85.18
Round  68, Train loss: 0.249, Test loss: 0.365, Test accuracy: 85.66
Round  69, Train loss: 0.265, Test loss: 0.364, Test accuracy: 85.47
Round  70, Train loss: 0.283, Test loss: 0.360, Test accuracy: 85.84
Round  71, Train loss: 0.294, Test loss: 0.359, Test accuracy: 85.92
Round  72, Train loss: 0.263, Test loss: 0.356, Test accuracy: 85.90
Round  73, Train loss: 0.364, Test loss: 0.356, Test accuracy: 86.08
Round  74, Train loss: 0.274, Test loss: 0.351, Test accuracy: 86.24
Round  75, Train loss: 0.306, Test loss: 0.355, Test accuracy: 86.18
Round  76, Train loss: 0.206, Test loss: 0.358, Test accuracy: 85.56
Round  77, Train loss: 0.279, Test loss: 0.351, Test accuracy: 86.25
Round  78, Train loss: 0.279, Test loss: 0.361, Test accuracy: 85.88
Round  79, Train loss: 0.280, Test loss: 0.356, Test accuracy: 86.17
Round  80, Train loss: 0.280, Test loss: 0.357, Test accuracy: 86.07
Round  81, Train loss: 0.254, Test loss: 0.358, Test accuracy: 85.98
Round  82, Train loss: 0.199, Test loss: 0.355, Test accuracy: 86.05
Round  83, Train loss: 0.212, Test loss: 0.351, Test accuracy: 85.99
Round  84, Train loss: 0.252, Test loss: 0.366, Test accuracy: 85.54
Round  85, Train loss: 0.247, Test loss: 0.364, Test accuracy: 85.60
Round  86, Train loss: 0.291, Test loss: 0.350, Test accuracy: 86.15
Round  87, Train loss: 0.263, Test loss: 0.345, Test accuracy: 86.57
Round  88, Train loss: 0.215, Test loss: 0.345, Test accuracy: 86.58
Round  89, Train loss: 0.277, Test loss: 0.347, Test accuracy: 86.46
Round  90, Train loss: 0.281, Test loss: 0.346, Test accuracy: 86.44
Round  91, Train loss: 0.206, Test loss: 0.347, Test accuracy: 86.38
Round  92, Train loss: 0.275, Test loss: 0.341, Test accuracy: 86.59
Round  93, Train loss: 0.244, Test loss: 0.343, Test accuracy: 86.53
Round  94, Train loss: 0.269, Test loss: 0.350, Test accuracy: 86.46
Round  95, Train loss: 0.221, Test loss: 0.347, Test accuracy: 86.33
Round  96, Train loss: 0.199, Test loss: 0.351, Test accuracy: 86.38
Round  97, Train loss: 0.227, Test loss: 0.342, Test accuracy: 86.77
Round  98, Train loss: 0.218, Test loss: 0.345, Test accuracy: 86.64
Round  99, Train loss: 0.234, Test loss: 0.347, Test accuracy: 86.86
Final Round, Train loss: 0.199, Test loss: 0.343, Test accuracy: 86.79
Average accuracy final 10 rounds: 86.5375
1364.438722372055
[2.1608939170837402, 3.7899274826049805, 5.4246602058410645, 7.043451547622681, 8.66253137588501, 10.261699438095093, 11.866528511047363, 13.472406148910522, 15.088599920272827, 16.690345525741577, 18.29374122619629, 19.91004252433777, 21.522472858428955, 23.14066457748413, 24.751400470733643, 26.36506724357605, 27.982184886932373, 29.59224224090576, 31.211416244506836, 32.825541257858276, 34.43643307685852, 36.05538749694824, 37.67349815368652, 39.285032510757446, 40.914844274520874, 42.535773277282715, 44.14699625968933, 45.768237352371216, 47.393524169921875, 49.00848913192749, 50.61326026916504, 52.241923809051514, 53.87722110748291, 55.50297999382019, 57.1222882270813, 58.73563838005066, 60.34760403633118, 61.95656776428223, 63.57386755943298, 65.19244766235352, 66.8099992275238, 68.43545913696289, 70.05989789962769, 71.66717171669006, 73.2967677116394, 74.91713356971741, 76.53071451187134, 78.14935898780823, 79.77792763710022, 81.391108751297, 82.99208283424377, 84.60321855545044, 86.2136697769165, 87.98635745048523, 89.75769877433777, 91.52518391609192, 93.2968978881836, 95.07447028160095, 96.86608338356018, 98.64686012268066, 100.4165301322937, 102.02696084976196, 103.63904476165771, 105.24564170837402, 106.85760164260864, 108.47000813484192, 110.08465909957886, 111.69284749031067, 113.30593848228455, 114.91426825523376, 116.52821183204651, 118.14948987960815, 119.76583480834961, 121.37668347358704, 122.98879861831665, 124.60939836502075, 126.21456050872803, 127.82542324066162, 129.436594247818, 131.04393601417542, 132.67217350006104, 134.2960181236267, 135.90122079849243, 137.5022735595703, 139.11175203323364, 140.73375606536865, 142.3541123867035, 143.96850395202637, 145.59362173080444, 147.17863607406616, 148.7843382358551, 150.38115572929382, 151.98765301704407, 153.59158086776733, 155.21160316467285, 156.84341549873352, 158.46509766578674, 160.0865604877472, 161.6987018585205, 163.33093070983887, 165.47790002822876]
[35.016666666666666, 33.6, 55.291666666666664, 55.95, 60.63333333333333, 58.99166666666667, 59.825, 62.40833333333333, 67.78333333333333, 71.98333333333333, 73.80833333333334, 74.90833333333333, 75.29166666666667, 77.03333333333333, 77.425, 77.99166666666666, 78.3, 78.14166666666667, 78.375, 79.48333333333333, 80.275, 80.425, 81.2, 81.65, 81.23333333333333, 81.075, 80.94166666666666, 81.45833333333333, 81.925, 82.43333333333334, 82.15, 82.73333333333333, 82.79166666666667, 82.76666666666667, 82.74166666666666, 83.31666666666666, 82.86666666666666, 83.275, 83.25833333333334, 83.58333333333333, 83.96666666666667, 83.84166666666667, 84.125, 84.09166666666667, 84.325, 83.94166666666666, 84.25833333333334, 84.36666666666666, 84.39166666666667, 84.7, 84.70833333333333, 84.51666666666667, 84.075, 84.75, 85.125, 85.08333333333333, 85.34166666666667, 85.43333333333334, 85.30833333333334, 85.125, 85.59166666666667, 85.20833333333333, 85.325, 85.5, 85.46666666666667, 85.4, 85.53333333333333, 85.18333333333334, 85.65833333333333, 85.475, 85.84166666666667, 85.91666666666667, 85.9, 86.08333333333333, 86.24166666666666, 86.18333333333334, 85.55833333333334, 86.25, 85.88333333333334, 86.175, 86.06666666666666, 85.98333333333333, 86.05, 85.99166666666666, 85.54166666666667, 85.6, 86.15, 86.56666666666666, 86.575, 86.45833333333333, 86.44166666666666, 86.375, 86.59166666666667, 86.525, 86.45833333333333, 86.33333333333333, 86.38333333333334, 86.76666666666667, 86.64166666666667, 86.85833333333333, 86.79166666666667]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8505
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2900
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8295
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0310
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5670
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8125
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0930
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6925
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4870
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8630
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  14.9900
Round 1 global test acc  10.1200
Round 2 global test acc  10.4200
Round 3 global test acc  18.0300
Round 4 global test acc  20.2400
Round 5 global test acc  21.6000
Round 6 global test acc  18.0100
Round 7 global test acc  24.4800
Round 8 global test acc  20.2300
Round 9 global test acc  22.8600
Round 10 global test acc  28.7400
Round 11 global test acc  26.3500
Round 12 global test acc  25.6300
Round 13 global test acc  23.0500
Round 14 global test acc  21.2000
Round 15 global test acc  22.0700
Round 16 global test acc  25.5400
Round 17 global test acc  21.4500
Round 18 global test acc  24.3400
Round 19 global test acc  30.7900
Round 20 global test acc  32.5700
Round 21 global test acc  23.4400
Round 22 global test acc  30.3600
Round 23 global test acc  29.7500
Round 24 global test acc  35.7000
Round 25 global test acc  34.5800
Round 26 global test acc  28.2400
Round 27 global test acc  29.6100
Round 28 global test acc  28.1300
Round 29 global test acc  29.5400
Round 30 global test acc  30.0700
Round 31 global test acc  24.6500
Round 32 global test acc  29.4900
Round 33 global test acc  31.6300
Round 34 global test acc  29.1800
Round 35 global test acc  35.2700
Round 36 global test acc  27.6500
Round 37 global test acc  33.9800
Round 38 global test acc  31.3200
Round 39 global test acc  31.7200
Round 40 global test acc  31.7800
Round 41 global test acc  20.2200
Round 42 global test acc  32.3300
Round 43 global test acc  29.6900
Round 44 global test acc  28.5600
Round 45 global test acc  38.1800
Round 46 global test acc  31.6500
Round 47 global test acc  26.0200
Round 48 global test acc  31.3700
Round 49 global test acc  29.1400
Round 50 global test acc  32.2500
Round 51 global test acc  37.9900
Round 52 global test acc  28.1100
Round 53 global test acc  31.5400
Round 54 global test acc  27.2700
Round 55 global test acc  37.7700
Round 56 global test acc  22.6700
Round 57 global test acc  30.9300
Round 58 global test acc  28.9500
Round 59 global test acc  31.6000
Round 60 global test acc  31.2000
Round 61 global test acc  37.9700
Round 62 global test acc  38.7700
Round 63 global test acc  29.2900
Round 64 global test acc  27.2500
Round 65 global test acc  26.2200
Round 66 global test acc  39.5400
Round 67 global test acc  32.3400
Round 68 global test acc  36.2800
Round 69 global test acc  27.5200
Round 70 global test acc  29.9200
Round 71 global test acc  32.7100
Round 72 global test acc  41.2800
Round 73 global test acc  30.3400
Round 74 global test acc  38.1600
Round 75 global test acc  30.7600
Round 76 global test acc  31.1000
Round 77 global test acc  35.9300
Round 78 global test acc  22.7700
Round 79 global test acc  25.5700
Round 80 global test acc  28.1000
Round 81 global test acc  26.5700
Round 82 global test acc  30.9000
Round 83 global test acc  27.4400
Round 84 global test acc  27.4300
Round 85 global test acc  26.0000
Round 86 global test acc  26.0600
Round 87 global test acc  24.3400
Round 88 global test acc  26.6800
Round 89 global test acc  27.5800
Round 90 global test acc  25.2100
Round 91 global test acc  26.1000
Round 92 global test acc  26.2500
Round 93 global test acc  24.7600
Round 94 global test acc  27.0300
Round 95 global test acc  25.7300
Round 96 global test acc  25.8300
Round 97 global test acc  23.1000
Round 98 global test acc  22.6400
Round 99 global test acc  24.7400
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8680
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3985
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8240
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1080
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5665
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8395
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0650
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7270
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4785
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8770
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.770, Test loss: 0.938, Test accuracy: 69.92
Average accuracy final 10 rounds: 70.14
1465.8215646743774
[2.124946355819702, 3.9565553665161133, 5.803889036178589, 7.6225550174713135, 9.44469690322876, 11.261621475219727, 13.087005853652954, 14.895664930343628, 16.73668646812439, 18.562499523162842, 20.3912992477417, 22.222918272018433, 24.03431725502014, 25.856970071792603, 27.67081332206726, 29.483270168304443, 31.293302059173584, 33.09309720993042, 34.88681507110596, 36.689146280288696, 38.508385181427, 40.33459186553955, 42.156137228012085, 43.967405796051025, 45.77261447906494, 47.581050634384155, 49.38667058944702, 51.19540238380432, 53.00749111175537, 54.82282590866089, 56.62218451499939, 58.42572736740112, 60.24907445907593, 62.07365417480469, 63.87187719345093, 65.6659095287323, 67.50618577003479, 69.30302572250366, 71.10145807266235, 72.92450284957886, 74.74864482879639, 76.55494737625122, 78.36140871047974, 80.18194699287415, 81.99513864517212, 83.79161643981934, 85.60331463813782, 87.42325115203857, 89.23557567596436, 91.04320001602173, 92.86252784729004, 94.68849349021912, 96.49929285049438, 98.29844951629639, 100.10271716117859, 101.8980667591095, 103.7152693271637, 105.52629566192627, 107.35377264022827, 109.16153764724731, 110.95614504814148, 112.73919820785522, 114.52332758903503, 116.30986952781677, 118.07674026489258, 119.82083654403687, 121.61591792106628, 123.41658473014832, 125.1577696800232, 126.9535744190216, 128.7244622707367, 130.49944138526917, 132.27881050109863, 134.04985690116882, 135.82449078559875, 137.59166932106018, 139.3673734664917, 141.1503701210022, 142.91366505622864, 144.7082531452179, 146.4878866672516, 148.26396417617798, 149.9676558971405, 151.71145963668823, 153.44633889198303, 155.16383862495422, 156.92624521255493, 158.70336151123047, 160.49676370620728, 162.28362011909485, 164.06757760047913, 165.84761571884155, 167.59993433952332, 169.37071204185486, 171.13258576393127, 172.90384078025818, 174.6915671825409, 176.44904851913452, 178.2310287952423, 179.98361086845398, 182.22906827926636]
[21.825, 35.68333333333333, 42.75833333333333, 49.44166666666667, 49.075, 48.825, 51.958333333333336, 53.30833333333333, 55.375, 61.075, 61.13333333333333, 59.55833333333333, 60.46666666666667, 64.36666666666666, 64.725, 64.41666666666667, 65.44166666666666, 64.325, 66.73333333333333, 66.89166666666667, 66.975, 67.14166666666667, 67.25833333333334, 67.875, 68.43333333333334, 68.04166666666667, 69.15833333333333, 68.71666666666667, 68.75833333333334, 68.76666666666667, 68.08333333333333, 68.90833333333333, 68.76666666666667, 69.20833333333333, 69.24166666666666, 68.91666666666667, 69.38333333333334, 69.51666666666667, 69.975, 69.80833333333334, 69.9, 68.98333333333333, 69.96666666666667, 69.34166666666667, 70.03333333333333, 70.74166666666666, 70.54166666666667, 69.775, 70.55, 69.9, 69.80833333333334, 70.83333333333333, 69.95, 70.39166666666667, 70.34166666666667, 71.01666666666667, 70.65833333333333, 70.44166666666666, 71.05, 70.66666666666667, 70.39166666666667, 71.25833333333334, 70.89166666666667, 69.84166666666667, 69.825, 69.89166666666667, 70.475, 70.125, 69.58333333333333, 69.74166666666666, 69.66666666666667, 70.05, 69.50833333333334, 70.14166666666667, 70.5, 70.675, 70.86666666666666, 70.44166666666666, 70.03333333333333, 70.825, 70.69166666666666, 70.83333333333333, 70.83333333333333, 70.24166666666666, 70.3, 69.775, 70.23333333333333, 70.46666666666667, 70.75, 70.23333333333333, 70.475, 70.55833333333334, 69.99166666666666, 70.05, 69.68333333333334, 69.70833333333333, 70.25, 70.45833333333333, 70.125, 70.1, 69.925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8495
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2910
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8320
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1450
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6030
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8160
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0845
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7095
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5750
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8540
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.661, Test loss: 2.129, Test accuracy: 24.18
Round   1, Train loss: 1.163, Test loss: 1.834, Test accuracy: 36.72
Round   2, Train loss: 1.197, Test loss: 1.694, Test accuracy: 40.48
Round   3, Train loss: 1.173, Test loss: 1.343, Test accuracy: 47.95
Round   4, Train loss: 1.098, Test loss: 1.483, Test accuracy: 47.61
Round   5, Train loss: 1.083, Test loss: 1.389, Test accuracy: 49.25
Round   6, Train loss: 1.090, Test loss: 1.259, Test accuracy: 46.89
Round   7, Train loss: 1.052, Test loss: 1.314, Test accuracy: 49.44
Round   8, Train loss: 1.107, Test loss: 1.167, Test accuracy: 55.44
Round   9, Train loss: 0.988, Test loss: 0.968, Test accuracy: 61.34
Round  10, Train loss: 1.037, Test loss: 1.014, Test accuracy: 59.31
Round  11, Train loss: 1.075, Test loss: 1.017, Test accuracy: 59.88
Round  12, Train loss: 1.049, Test loss: 0.994, Test accuracy: 58.83
Round  13, Train loss: 1.058, Test loss: 0.892, Test accuracy: 63.71
Round  14, Train loss: 0.905, Test loss: 0.889, Test accuracy: 63.98
Round  15, Train loss: 0.970, Test loss: 0.872, Test accuracy: 63.58
Round  16, Train loss: 0.983, Test loss: 0.859, Test accuracy: 64.72
Round  17, Train loss: 0.846, Test loss: 0.835, Test accuracy: 65.01
Round  18, Train loss: 0.995, Test loss: 0.849, Test accuracy: 65.12
Round  19, Train loss: 0.922, Test loss: 0.836, Test accuracy: 66.67
Round  20, Train loss: 0.857, Test loss: 0.824, Test accuracy: 66.44
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1966, in train
    self.filter_by_loss2(net=net, concept_matrix_local=concept_matrix_local, iter_num=iter2,
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1809, in filter_by_loss2
    loss = self.loss_func(log_probs, lable_tmp)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8720
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5935
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8660
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.4575
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7630
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8515
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5875
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8050
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7415
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8875
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.416, Test loss: 5.033, Test accuracy: 28.24
Final Round, Global train loss: 0.416, Global test loss: 2.077, Global test accuracy: 26.12
Average accuracy final 10 rounds: 27.994500000000002 

Average global accuracy final 10 rounds: 30.155250000000002 

6133.197684288025
[5.549697399139404, 11.099394798278809, 16.39387607574463, 21.68835735321045, 26.97116446495056, 32.253971576690674, 37.55569243431091, 42.85741329193115, 48.12186026573181, 53.38630723953247, 58.680140018463135, 63.9739727973938, 69.23735451698303, 74.50073623657227, 79.74955773353577, 84.99837923049927, 90.2705225944519, 95.54266595840454, 100.87550234794617, 106.2083387374878, 111.55151891708374, 116.89469909667969, 122.07650017738342, 127.25830125808716, 131.59408903121948, 135.9298768043518, 140.27238965034485, 144.6149024963379, 148.97492671012878, 153.33495092391968, 157.66663002967834, 161.998309135437, 166.41957783699036, 170.8408465385437, 175.18789911270142, 179.53495168685913, 183.89338159561157, 188.251811504364, 192.6213583946228, 196.9909052848816, 201.4093418121338, 205.827778339386, 210.42485404014587, 215.02192974090576, 219.37911677360535, 223.73630380630493, 228.08756113052368, 232.43881845474243, 236.85036253929138, 241.26190662384033, 245.65296268463135, 250.04401874542236, 254.64914202690125, 259.2542653083801, 263.58955240249634, 267.92483949661255, 272.28427267074585, 276.64370584487915, 280.988516330719, 285.33332681655884, 289.69699931144714, 294.06067180633545, 298.4306654930115, 302.8006591796875, 307.13945388793945, 311.4782485961914, 315.85643887519836, 320.2346291542053, 324.64289927482605, 329.0511693954468, 333.419064283371, 337.78695917129517, 342.0957794189453, 346.40459966659546, 351.4487166404724, 356.49283361434937, 360.94669699668884, 365.4005603790283, 369.83961296081543, 374.27866554260254, 378.55795669555664, 382.83724784851074, 387.1372449398041, 391.4372420310974, 395.73996019363403, 400.04267835617065, 404.455792427063, 408.8689064979553, 413.1965925693512, 417.52427864074707, 421.87862181663513, 426.2329649925232, 430.509414434433, 434.7858638763428, 439.1095669269562, 443.4332699775696, 447.7542898654938, 452.07530975341797, 456.37691855430603, 460.6785273551941, 464.94641160964966, 469.2142958641052, 473.5206937789917, 477.8270916938782, 482.13061356544495, 486.4341354370117, 490.75200510025024, 495.06987476348877, 499.3556787967682, 503.6414828300476, 507.94589042663574, 512.2502980232239, 516.5401785373688, 520.8300590515137, 525.12704205513, 529.4240250587463, 533.9741687774658, 538.5243124961853, 542.8502726554871, 547.1762328147888, 551.4537916183472, 555.7313504219055, 560.0222840309143, 564.3132176399231, 568.688462972641, 573.0637083053589, 577.3577826023102, 581.6518568992615, 585.9632680416107, 590.27467918396, 595.5016613006592, 600.7286434173584, 605.0776603221893, 609.4266772270203, 613.7652552127838, 618.1038331985474, 622.5998239517212, 627.095814704895, 631.4658000469208, 635.8357853889465, 640.2533838748932, 644.6709823608398, 649.1928296089172, 653.7146768569946, 658.050121307373, 662.3855657577515, 666.7083897590637, 671.031213760376, 675.4767606258392, 679.9223074913025, 684.2989003658295, 688.6754932403564, 693.0428359508514, 697.4101786613464, 701.6808979511261, 705.9516172409058, 710.3256165981293, 714.6996159553528, 719.104585647583, 723.5095553398132, 727.8814053535461, 732.253255367279, 736.5341141223907, 740.8149728775024, 745.0767555236816, 749.3385381698608, 753.8908035755157, 758.4430689811707, 762.8442223072052, 767.2453756332397, 771.6216607093811, 775.9979457855225, 780.3275637626648, 784.6571817398071, 788.9587576389313, 793.2603335380554, 797.633376121521, 802.0064187049866, 806.3786466121674, 810.7508745193481, 815.1592009067535, 819.5675272941589, 823.8845944404602, 828.2016615867615, 832.4759142398834, 836.7501668930054, 841.0301196575165, 845.3100724220276, 849.6020033359528, 853.8939342498779, 858.1799354553223, 862.4659366607666, 866.7349882125854, 871.0040397644043, 875.234611749649, 879.4651837348938, 883.7131516933441, 887.9611196517944, 892.2417047023773, 896.5222897529602, 898.6773176193237, 900.8323454856873]
[23.4075, 23.4075, 25.8, 25.8, 30.8525, 30.8525, 30.435, 30.435, 31.3625, 31.3625, 29.7725, 29.7725, 30.79, 30.79, 31.225, 31.225, 31.57, 31.57, 31.6425, 31.6425, 32.06, 32.06, 32.4725, 32.4725, 32.405, 32.405, 32.2825, 32.2825, 32.3475, 32.3475, 32.07, 32.07, 31.7325, 31.7325, 31.4325, 31.4325, 32.045, 32.045, 31.7925, 31.7925, 31.785, 31.785, 31.4925, 31.4925, 31.2775, 31.2775, 30.7775, 30.7775, 30.9775, 30.9775, 30.9875, 30.9875, 31.025, 31.025, 31.2725, 31.2725, 31.1525, 31.1525, 31.0725, 31.0725, 30.6725, 30.6725, 30.6125, 30.6125, 30.61, 30.61, 30.73, 30.73, 30.485, 30.485, 30.12, 30.12, 30.27, 30.27, 29.96, 29.96, 29.5775, 29.5775, 29.4075, 29.4075, 29.4875, 29.4875, 29.545, 29.545, 29.19, 29.19, 29.23, 29.23, 29.34, 29.34, 29.335, 29.335, 29.27, 29.27, 28.9875, 28.9875, 28.7775, 28.7775, 28.7925, 28.7925, 28.895, 28.895, 28.5975, 28.5975, 28.56, 28.56, 28.675, 28.675, 28.605, 28.605, 28.935, 28.935, 28.6225, 28.6225, 28.5275, 28.5275, 28.7325, 28.7325, 28.2875, 28.2875, 28.0475, 28.0475, 28.085, 28.085, 28.1725, 28.1725, 27.895, 27.895, 28.37, 28.37, 28.41, 28.41, 28.3475, 28.3475, 28.2875, 28.2875, 28.095, 28.095, 28.2625, 28.2625, 28.285, 28.285, 28.0025, 28.0025, 28.235, 28.235, 28.3175, 28.3175, 27.925, 27.925, 28.1225, 28.1225, 27.6775, 27.6775, 27.565, 27.565, 27.415, 27.415, 27.7325, 27.7325, 27.82, 27.82, 27.7825, 27.7825, 27.855, 27.855, 27.86, 27.86, 27.8075, 27.8075, 27.755, 27.755, 27.765, 27.765, 27.975, 27.975, 28.065, 28.065, 28.115, 28.115, 28.255, 28.255, 28.1875, 28.1875, 28.2, 28.2, 28.165, 28.165, 28.1925, 28.1925, 27.9375, 27.9375, 27.82, 27.82, 27.65, 27.65, 27.7475, 27.7475, 27.79, 27.79, 28.24, 28.24]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8855
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.6470
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8660
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5260
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7505
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8495
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5750
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7765
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7330
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8820
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.628, Test loss: 1.916, Test accuracy: 53.72
Final Round, Global train loss: 0.628, Global test loss: 1.006, Global test accuracy: 69.85
Average accuracy final 10 rounds: 53.768 

Average global accuracy final 10 rounds: 67.7945 

6125.270520210266
[5.000871181488037, 10.001742362976074, 14.902879238128662, 19.80401611328125, 24.78536581993103, 29.76671552658081, 34.10544013977051, 38.444164752960205, 42.79743409156799, 47.15070343017578, 51.46006155014038, 55.76941967010498, 60.07404112815857, 64.37866258621216, 68.73556280136108, 73.09246301651001, 77.42198133468628, 81.75149965286255, 86.06219553947449, 90.37289142608643, 94.71608281135559, 99.05927419662476, 103.4263014793396, 107.79332876205444, 112.13661193847656, 116.47989511489868, 120.86327004432678, 125.24664497375488, 129.67173027992249, 134.0968155860901, 138.41802716255188, 142.73923873901367, 147.0395188331604, 151.33979892730713, 155.71317386627197, 160.08654880523682, 164.3966827392578, 168.7068166732788, 173.02632403373718, 177.34583139419556, 181.7338583469391, 186.12188529968262, 190.42168378829956, 194.7214822769165, 199.03108382225037, 203.34068536758423, 207.6508274078369, 211.9609694480896, 216.28738927841187, 220.61380910873413, 224.94262719154358, 229.27144527435303, 233.62885999679565, 237.98627471923828, 242.2811152935028, 246.57595586776733, 250.84656643867493, 255.11717700958252, 259.38466334342957, 263.6521496772766, 267.9297773838043, 272.20740509033203, 276.5309760570526, 280.8545470237732, 285.20573329925537, 289.55691957473755, 293.88022089004517, 298.2035222053528, 302.52198028564453, 306.8404383659363, 311.14103722572327, 315.44163608551025, 319.77064299583435, 324.09964990615845, 328.4164116382599, 332.7331733703613, 337.0502462387085, 341.36731910705566, 345.6922607421875, 350.01720237731934, 354.34887051582336, 358.6805386543274, 363.04066467285156, 367.40079069137573, 371.79554772377014, 376.19030475616455, 380.5745539665222, 384.9588031768799, 389.32608461380005, 393.6933660507202, 398.03730177879333, 402.38123750686646, 406.73243594169617, 411.0836343765259, 415.4715003967285, 419.85936641693115, 424.2273418903351, 428.595317363739, 432.9142680168152, 437.23321866989136, 441.546911239624, 445.8606038093567, 450.220636844635, 454.58066987991333, 458.8988838195801, 463.2170977592468, 467.5679566860199, 471.91881561279297, 476.2661101818085, 480.613404750824, 484.98395228385925, 489.35449981689453, 493.73429703712463, 498.11409425735474, 502.5022666454315, 506.8904390335083, 511.2619254589081, 515.6334118843079, 519.9459664821625, 524.2585210800171, 528.5781185626984, 532.8977160453796, 537.2332897186279, 541.5688633918762, 545.9009532928467, 550.2330431938171, 554.566978931427, 558.9009146690369, 563.225038766861, 567.5491628646851, 571.8548662662506, 576.1605696678162, 580.4721710681915, 584.7837724685669, 589.1217105388641, 593.4596486091614, 597.746994972229, 602.0343413352966, 606.3551392555237, 610.6759371757507, 614.9897055625916, 619.3034739494324, 623.6376307010651, 627.9717874526978, 632.3243074417114, 636.6768274307251, 641.0313868522644, 645.3859462738037, 649.6806869506836, 653.9754276275635, 658.2628238201141, 662.5502200126648, 666.9479687213898, 671.3457174301147, 675.714047908783, 680.0823783874512, 684.4560174942017, 688.8296566009521, 693.2118473052979, 697.5940380096436, 701.9412014484406, 706.2883648872375, 710.6462042331696, 715.0040435791016, 719.3742783069611, 723.7445130348206, 728.1154465675354, 732.4863801002502, 736.809353351593, 741.1323266029358, 745.4373557567596, 749.7423849105835, 754.0600860118866, 758.3777871131897, 762.6840169429779, 766.9902467727661, 771.2780177593231, 775.5657887458801, 779.8565864562988, 784.1473841667175, 788.4224381446838, 792.6974921226501, 796.9810259342194, 801.2645597457886, 805.5548453330994, 809.8451309204102, 814.3429985046387, 818.8408660888672, 823.1454277038574, 827.4499893188477, 831.829892873764, 836.2097964286804, 840.5557940006256, 844.9017915725708, 849.2315649986267, 853.5613384246826, 857.8846006393433, 862.2078628540039, 866.5224807262421, 870.8370985984802, 873.0775420665741, 875.317985534668]
[26.2075, 26.2075, 32.8175, 32.8175, 34.325, 34.325, 36.89, 36.89, 37.695, 37.695, 40.2025, 40.2025, 41.105, 41.105, 42.8225, 42.8225, 43.9325, 43.9325, 45.5275, 45.5275, 46.14, 46.14, 47.21, 47.21, 47.9225, 47.9225, 49.115, 49.115, 49.775, 49.775, 49.68, 49.68, 50.1825, 50.1825, 50.81, 50.81, 51.28, 51.28, 51.5175, 51.5175, 51.875, 51.875, 52.7325, 52.7325, 53.1075, 53.1075, 53.5975, 53.5975, 53.515, 53.515, 53.14, 53.14, 53.46, 53.46, 52.9075, 52.9075, 53.1425, 53.1425, 53.0, 53.0, 53.255, 53.255, 53.2225, 53.2225, 52.7125, 52.7125, 52.7875, 52.7875, 53.355, 53.355, 53.63, 53.63, 53.675, 53.675, 53.5975, 53.5975, 53.475, 53.475, 53.45, 53.45, 53.8775, 53.8775, 54.105, 54.105, 54.035, 54.035, 54.335, 54.335, 54.435, 54.435, 54.335, 54.335, 53.9375, 53.9375, 53.88, 53.88, 53.7425, 53.7425, 54.1, 54.1, 53.82, 53.82, 53.87, 53.87, 54.175, 54.175, 54.375, 54.375, 54.1025, 54.1025, 54.0475, 54.0475, 54.21, 54.21, 54.4, 54.4, 54.1175, 54.1175, 54.215, 54.215, 54.01, 54.01, 54.035, 54.035, 54.04, 54.04, 53.7175, 53.7175, 53.9475, 53.9475, 53.745, 53.745, 53.8875, 53.8875, 54.0, 54.0, 53.4125, 53.4125, 53.63, 53.63, 53.6425, 53.6425, 54.045, 54.045, 54.295, 54.295, 54.13, 54.13, 54.2125, 54.2125, 54.0675, 54.0675, 54.0225, 54.0225, 53.8575, 53.8575, 54.005, 54.005, 53.945, 53.945, 53.6175, 53.6175, 53.7875, 53.7875, 53.68, 53.68, 53.975, 53.975, 54.0825, 54.0825, 53.8775, 53.8775, 54.1325, 54.1325, 53.925, 53.925, 53.8575, 53.8575, 53.825, 53.825, 53.405, 53.405, 53.3075, 53.3075, 53.78, 53.78, 53.83, 53.83, 53.8025, 53.8025, 53.7825, 53.7825, 53.73, 53.73, 53.9025, 53.9025, 53.9975, 53.9975, 54.1425, 54.1425, 53.72, 53.72]
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8860
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5835
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8720
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5335
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7815
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8630
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5595
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7855
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7450
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8900
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.718, Test loss: 1.391, Test accuracy: 59.69
Final Round, Global train loss: 0.718, Global test loss: 0.947, Global test accuracy: 69.97
Average accuracy final 10 rounds: 60.9435 

Average global accuracy final 10 rounds: 71.26574999999998 

6274.735829114914
[5.540134906768799, 11.080269813537598, 15.745734930038452, 20.411200046539307, 25.048530340194702, 29.685860633850098, 34.32183647155762, 38.95781230926514, 43.58457851409912, 48.211344718933105, 52.82910752296448, 57.44687032699585, 62.13012766838074, 66.81338500976562, 71.46106147766113, 76.10873794555664, 80.79503297805786, 85.48132801055908, 90.11300802230835, 94.74468803405762, 99.40195274353027, 104.05921745300293, 108.7131290435791, 113.36704063415527, 118.02674603462219, 122.68645143508911, 127.32078289985657, 131.95511436462402, 136.58384370803833, 141.21257305145264, 145.86726665496826, 150.5219602584839, 155.19574570655823, 159.86953115463257, 164.5440559387207, 169.21858072280884, 173.91921710968018, 178.6198534965515, 183.25795483589172, 187.89605617523193, 192.565505027771, 197.23495388031006, 202.4898705482483, 207.74478721618652, 212.92594742774963, 218.10710763931274, 223.35307931900024, 228.59905099868774, 233.22109127044678, 237.8431315422058, 242.47541666030884, 247.10770177841187, 251.71174454689026, 256.31578731536865, 260.9291331768036, 265.5424790382385, 270.1334578990936, 274.72443675994873, 279.3581802845001, 283.9919238090515, 288.61375308036804, 293.23558235168457, 297.8541433811188, 302.472704410553, 307.0542311668396, 311.6357579231262, 316.31638741493225, 320.9970169067383, 325.6036813259125, 330.21034574508667, 334.82311272621155, 339.4358797073364, 344.06027340888977, 348.6846671104431, 353.3054258823395, 357.92618465423584, 362.5513288974762, 367.17647314071655, 371.81631731987, 376.45616149902344, 381.1021685600281, 385.7481756210327, 390.3153579235077, 394.88254022598267, 399.4804766178131, 404.07841300964355, 408.64399909973145, 413.20958518981934, 417.78842520713806, 422.3672652244568, 426.9846177101135, 431.60197019577026, 436.2428767681122, 440.8837833404541, 445.55066084861755, 450.217538356781, 454.8352675437927, 459.45299673080444, 464.1175513267517, 468.782105922699, 473.3983705043793, 478.01463508605957, 482.6462595462799, 487.27788400650024, 491.9092755317688, 496.54066705703735, 501.1380503177643, 505.7354335784912, 510.33696842193604, 514.9385032653809, 519.5333709716797, 524.1282386779785, 528.7114880084991, 533.2947373390198, 537.8878939151764, 542.481050491333, 547.0898458957672, 551.6986413002014, 556.3414576053619, 560.9842739105225, 565.641416311264, 570.2985587120056, 574.9071733951569, 579.5157880783081, 584.1426148414612, 588.7694416046143, 593.3609073162079, 597.9523730278015, 602.6390297412872, 607.325686454773, 612.5179209709167, 617.7101554870605, 622.9032146930695, 628.0962738990784, 633.2774953842163, 638.4587168693542, 643.6303691864014, 648.8020215034485, 653.9273107051849, 659.0525999069214, 664.2647275924683, 669.4768552780151, 674.5859289169312, 679.6950025558472, 684.8826942443848, 690.0703859329224, 695.2652933597565, 700.4602007865906, 705.6135425567627, 710.7668843269348, 716.0081877708435, 721.2494912147522, 725.9387784004211, 730.6280655860901, 735.2504012584686, 739.8727369308472, 744.5149693489075, 749.1572017669678, 753.7962648868561, 758.4353280067444, 763.0468509197235, 767.6583738327026, 772.3167915344238, 776.975209236145, 781.6191158294678, 786.2630224227905, 790.9281096458435, 795.5931968688965, 800.2297961711884, 804.8663954734802, 809.5238943099976, 814.1813931465149, 818.8270921707153, 823.4727911949158, 828.165177822113, 832.8575644493103, 837.4979519844055, 842.1383395195007, 846.8010132312775, 851.4636869430542, 856.1188161373138, 860.7739453315735, 865.43123960495, 870.0885338783264, 874.7453107833862, 879.402087688446, 884.048290014267, 888.6944923400879, 893.4138295650482, 898.1331667900085, 902.8893632888794, 907.6455597877502, 912.3917610645294, 917.1379623413086, 921.9138739109039, 926.6897854804993, 931.4104738235474, 936.1311621665955, 940.8477113246918, 945.5642604827881, 947.9555041790009, 950.3467478752136]
[27.1975, 27.1975, 33.3775, 33.3775, 35.3675, 35.3675, 35.405, 35.405, 36.7275, 36.7275, 37.69, 37.69, 38.6525, 38.6525, 40.4225, 40.4225, 42.235, 42.235, 42.94, 42.94, 43.8525, 43.8525, 45.3875, 45.3875, 46.4475, 46.4475, 47.1575, 47.1575, 48.095, 48.095, 49.0525, 49.0525, 50.1975, 50.1975, 51.01, 51.01, 51.6025, 51.6025, 52.895, 52.895, 52.58, 52.58, 52.9625, 52.9625, 53.25, 53.25, 54.175, 54.175, 54.1125, 54.1125, 54.675, 54.675, 55.0425, 55.0425, 55.2775, 55.2775, 55.815, 55.815, 55.9875, 55.9875, 56.235, 56.235, 56.33, 56.33, 55.96, 55.96, 56.1725, 56.1725, 56.9675, 56.9675, 56.83, 56.83, 56.7375, 56.7375, 56.825, 56.825, 56.7575, 56.7575, 56.8675, 56.8675, 56.9925, 56.9925, 56.8625, 56.8625, 57.3075, 57.3075, 57.6425, 57.6425, 58.0425, 58.0425, 58.6425, 58.6425, 58.705, 58.705, 58.9475, 58.9475, 59.325, 59.325, 59.32, 59.32, 59.405, 59.405, 59.6275, 59.6275, 59.535, 59.535, 59.64, 59.64, 59.75, 59.75, 59.7975, 59.7975, 59.815, 59.815, 60.0475, 60.0475, 59.7125, 59.7125, 59.8175, 59.8175, 60.215, 60.215, 59.91, 59.91, 59.76, 59.76, 60.235, 60.235, 60.2525, 60.2525, 60.005, 60.005, 60.39, 60.39, 60.015, 60.015, 60.01, 60.01, 59.9725, 59.9725, 59.88, 59.88, 59.865, 59.865, 60.0025, 60.0025, 60.1, 60.1, 60.7125, 60.7125, 60.41, 60.41, 60.85, 60.85, 60.8, 60.8, 61.105, 61.105, 61.4325, 61.4325, 61.0125, 61.0125, 60.92, 60.92, 60.745, 60.745, 60.4075, 60.4075, 60.3225, 60.3225, 60.1825, 60.1825, 60.0625, 60.0625, 60.45, 60.45, 60.8925, 60.8925, 60.75, 60.75, 61.01, 61.01, 60.9475, 60.9475, 61.17, 61.17, 60.965, 60.965, 60.705, 60.705, 61.2075, 61.2075, 60.69, 60.69, 60.9425, 60.9425, 60.9325, 60.9325, 60.865, 60.865, 59.6925, 59.6925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8795
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.6855
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8560
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5670
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7265
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8475
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5950
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7860
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.6995
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8905
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.497, Test loss: 2.178, Test accuracy: 26.96
Round   1, Train loss: 0.956, Test loss: 1.846, Test accuracy: 37.92
Round   2, Train loss: 0.891, Test loss: 1.412, Test accuracy: 46.84
Round   3, Train loss: 0.808, Test loss: 1.110, Test accuracy: 56.76
Round   4, Train loss: 0.718, Test loss: 0.949, Test accuracy: 63.52
Round   5, Train loss: 0.677, Test loss: 0.958, Test accuracy: 63.08
Round   6, Train loss: 0.653, Test loss: 0.890, Test accuracy: 68.26
Round   7, Train loss: 0.654, Test loss: 0.920, Test accuracy: 68.15
Round   8, Train loss: 0.746, Test loss: 0.681, Test accuracy: 70.97
Round   9, Train loss: 0.675, Test loss: 0.673, Test accuracy: 70.90
Round  10, Train loss: 0.642, Test loss: 0.665, Test accuracy: 73.64
Round  11, Train loss: 0.541, Test loss: 0.570, Test accuracy: 75.25
Round  12, Train loss: 0.628, Test loss: 0.557, Test accuracy: 76.16
Round  13, Train loss: 0.617, Test loss: 0.552, Test accuracy: 77.47
Round  14, Train loss: 0.534, Test loss: 0.530, Test accuracy: 77.39
Round  15, Train loss: 0.513, Test loss: 0.530, Test accuracy: 77.89
Round  16, Train loss: 0.584, Test loss: 0.537, Test accuracy: 77.66
Round  17, Train loss: 0.543, Test loss: 0.519, Test accuracy: 78.06
Round  18, Train loss: 0.441, Test loss: 0.495, Test accuracy: 79.27
Round  19, Train loss: 0.431, Test loss: 0.496, Test accuracy: 79.41
Round  20, Train loss: 0.517, Test loss: 0.497, Test accuracy: 79.99
Round  21, Train loss: 0.531, Test loss: 0.489, Test accuracy: 79.85
Round  22, Train loss: 0.405, Test loss: 0.474, Test accuracy: 80.30
Round  23, Train loss: 0.541, Test loss: 0.462, Test accuracy: 81.08
Round  24, Train loss: 0.502, Test loss: 0.468, Test accuracy: 80.91
Round  25, Train loss: 0.448, Test loss: 0.447, Test accuracy: 82.03
Round  26, Train loss: 0.529, Test loss: 0.441, Test accuracy: 82.38
Round  27, Train loss: 0.408, Test loss: 0.445, Test accuracy: 82.68
Round  28, Train loss: 0.358, Test loss: 0.440, Test accuracy: 82.78
Round  29, Train loss: 0.441, Test loss: 0.421, Test accuracy: 83.33
Round  30, Train loss: 0.527, Test loss: 0.415, Test accuracy: 83.30
Round  31, Train loss: 0.389, Test loss: 0.411, Test accuracy: 83.42
Round  32, Train loss: 0.387, Test loss: 0.410, Test accuracy: 83.47
Round  33, Train loss: 0.424, Test loss: 0.406, Test accuracy: 83.86
Round  34, Train loss: 0.373, Test loss: 0.401, Test accuracy: 83.83
Round  35, Train loss: 0.363, Test loss: 0.397, Test accuracy: 84.25
Round  36, Train loss: 0.395, Test loss: 0.392, Test accuracy: 84.33
Round  37, Train loss: 0.446, Test loss: 0.386, Test accuracy: 84.58
Round  38, Train loss: 0.329, Test loss: 0.394, Test accuracy: 84.38
Round  39, Train loss: 0.345, Test loss: 0.388, Test accuracy: 84.05
Round  40, Train loss: 0.458, Test loss: 0.391, Test accuracy: 84.19
Round  41, Train loss: 0.386, Test loss: 0.381, Test accuracy: 84.94
Round  42, Train loss: 0.306, Test loss: 0.386, Test accuracy: 84.67
Round  43, Train loss: 0.329, Test loss: 0.380, Test accuracy: 84.62
Round  44, Train loss: 0.343, Test loss: 0.378, Test accuracy: 84.80
Round  45, Train loss: 0.301, Test loss: 0.378, Test accuracy: 85.00
Round  46, Train loss: 0.377, Test loss: 0.378, Test accuracy: 84.82
Round  47, Train loss: 0.353, Test loss: 0.376, Test accuracy: 85.01
Round  48, Train loss: 0.432, Test loss: 0.365, Test accuracy: 85.46
Round  49, Train loss: 0.327, Test loss: 0.357, Test accuracy: 86.06
Round  50, Train loss: 0.286, Test loss: 0.356, Test accuracy: 86.14
Round  51, Train loss: 0.249, Test loss: 0.358, Test accuracy: 85.98
Round  52, Train loss: 0.391, Test loss: 0.359, Test accuracy: 85.56
Round  53, Train loss: 0.410, Test loss: 0.359, Test accuracy: 85.98
Round  54, Train loss: 0.260, Test loss: 0.355, Test accuracy: 86.24
Round  55, Train loss: 0.249, Test loss: 0.346, Test accuracy: 86.26
Round  56, Train loss: 0.298, Test loss: 0.349, Test accuracy: 86.40
Round  57, Train loss: 0.296, Test loss: 0.342, Test accuracy: 86.84
Round  58, Train loss: 0.324, Test loss: 0.342, Test accuracy: 86.58
Round  59, Train loss: 0.270, Test loss: 0.344, Test accuracy: 86.44
Round  60, Train loss: 0.309, Test loss: 0.345, Test accuracy: 86.32
Round  61, Train loss: 0.306, Test loss: 0.347, Test accuracy: 86.22
Round  62, Train loss: 0.364, Test loss: 0.344, Test accuracy: 86.47
Round  63, Train loss: 0.334, Test loss: 0.345, Test accuracy: 86.48
Round  64, Train loss: 0.325, Test loss: 0.343, Test accuracy: 86.42
Round  65, Train loss: 0.230, Test loss: 0.342, Test accuracy: 86.32
Round  66, Train loss: 0.286, Test loss: 0.340, Test accuracy: 86.59
Round  67, Train loss: 0.290, Test loss: 0.340, Test accuracy: 86.40
Round  68, Train loss: 0.248, Test loss: 0.347, Test accuracy: 85.97
Round  69, Train loss: 0.270, Test loss: 0.341, Test accuracy: 86.60
Round  70, Train loss: 0.251, Test loss: 0.342, Test accuracy: 86.24
Round  71, Train loss: 0.326, Test loss: 0.332, Test accuracy: 86.83
Round  72, Train loss: 0.232, Test loss: 0.334, Test accuracy: 86.72
Round  73, Train loss: 0.235, Test loss: 0.332, Test accuracy: 86.90
Round  74, Train loss: 0.250, Test loss: 0.333, Test accuracy: 87.12
Round  75, Train loss: 0.281, Test loss: 0.328, Test accuracy: 87.04
Round  76, Train loss: 0.226, Test loss: 0.331, Test accuracy: 87.02
Round  77, Train loss: 0.293, Test loss: 0.328, Test accuracy: 87.03
Round  78, Train loss: 0.361, Test loss: 0.327, Test accuracy: 87.09
Round  79, Train loss: 0.219, Test loss: 0.327, Test accuracy: 87.08
Round  80, Train loss: 0.225, Test loss: 0.328, Test accuracy: 87.23
Round  81, Train loss: 0.211, Test loss: 0.332, Test accuracy: 87.02
Round  82, Train loss: 0.298, Test loss: 0.341, Test accuracy: 86.71
Round  83, Train loss: 0.287, Test loss: 0.330, Test accuracy: 87.16
Round  84, Train loss: 0.242, Test loss: 0.335, Test accuracy: 86.88
Round  85, Train loss: 0.229, Test loss: 0.334, Test accuracy: 87.08
Round  86, Train loss: 0.229, Test loss: 0.334, Test accuracy: 86.92
Round  87, Train loss: 0.205, Test loss: 0.333, Test accuracy: 86.86
Round  88, Train loss: 0.188, Test loss: 0.331, Test accuracy: 87.17
Round  89, Train loss: 0.175, Test loss: 0.334, Test accuracy: 86.98
Round  90, Train loss: 0.276, Test loss: 0.333, Test accuracy: 87.00
Round  91, Train loss: 0.182, Test loss: 0.336, Test accuracy: 86.88
Round  92, Train loss: 0.236, Test loss: 0.333, Test accuracy: 87.03
Round  93, Train loss: 0.180, Test loss: 0.333, Test accuracy: 86.88
Round  94, Train loss: 0.240, Test loss: 0.330, Test accuracy: 87.12
Round  95, Train loss: 0.200, Test loss: 0.329, Test accuracy: 87.31
Round  96, Train loss: 0.199, Test loss: 0.327, Test accuracy: 87.38
Round  97, Train loss: 0.288, Test loss: 0.329, Test accuracy: 87.18
Round  98, Train loss: 0.201, Test loss: 0.330, Test accuracy: 87.28
Round  99, Train loss: 0.246, Test loss: 0.333, Test accuracy: 87.18
Final Round, Train loss: 0.177, Test loss: 0.327, Test accuracy: 87.30
Average accuracy final 10 rounds: 87.125
1374.5260705947876
[1.9831268787384033, 3.5629026889801025, 5.193915367126465, 6.792093276977539, 8.41343355178833, 10.040374755859375, 11.665899753570557, 13.250824928283691, 14.840265035629272, 16.497611045837402, 18.15128779411316, 19.76372528076172, 21.371906280517578, 22.989282846450806, 24.60581064224243, 26.221768856048584, 27.822493314743042, 29.426615953445435, 31.062978982925415, 32.66297650337219, 34.24882936477661, 35.83826422691345, 37.48449468612671, 39.19895267486572, 40.91127586364746, 42.52973794937134, 44.16859483718872, 45.852763175964355, 47.53362202644348, 49.18915295600891, 50.8021285533905, 52.524155378341675, 54.17156767845154, 55.90124464035034, 57.506497859954834, 59.157350301742554, 60.879608154296875, 62.60502481460571, 64.21376872062683, 65.82700800895691, 67.4867434501648, 69.2018895149231, 70.93501377105713, 72.5409631729126, 74.15650010108948, 75.88396978378296, 77.5467791557312, 79.15767002105713, 80.77356839179993, 82.50633716583252, 84.24104499816895, 85.84875440597534, 87.46035504341125, 89.10048770904541, 90.71970653533936, 92.34417724609375, 93.98715829849243, 95.55343794822693, 97.17748236656189, 98.80210208892822, 100.43010973930359, 102.03798317909241, 103.64889979362488, 105.26013207435608, 106.88180685043335, 108.49650144577026, 110.11638927459717, 111.71430897712708, 113.32164478302002, 114.93241834640503, 116.50464916229248, 118.12195134162903, 119.72856187820435, 121.35049200057983, 122.95643615722656, 124.55863666534424, 126.15800023078918, 127.89621257781982, 129.62576603889465, 131.23700785636902, 132.8153145313263, 134.45817708969116, 136.17845273017883, 137.7539575099945, 139.51835083961487, 141.33298301696777, 143.13486003875732, 144.89731073379517, 146.7021985054016, 148.47605991363525, 150.23956394195557, 151.99909734725952, 153.80366897583008, 155.60108542442322, 157.3851273059845, 159.16050148010254, 160.9194257259369, 162.6498363018036, 164.27455592155457, 165.8988175392151, 168.031494140625]
[26.958333333333332, 37.916666666666664, 46.84166666666667, 56.75833333333333, 63.516666666666666, 63.075, 68.25833333333334, 68.15, 70.96666666666667, 70.9, 73.64166666666667, 75.25, 76.15833333333333, 77.475, 77.39166666666667, 77.89166666666667, 77.65833333333333, 78.05833333333334, 79.26666666666667, 79.40833333333333, 79.99166666666666, 79.85, 80.3, 81.08333333333333, 80.90833333333333, 82.025, 82.38333333333334, 82.68333333333334, 82.775, 83.33333333333333, 83.3, 83.41666666666667, 83.46666666666667, 83.85833333333333, 83.825, 84.25, 84.33333333333333, 84.58333333333333, 84.375, 84.05, 84.19166666666666, 84.94166666666666, 84.66666666666667, 84.61666666666666, 84.8, 85.0, 84.81666666666666, 85.00833333333334, 85.45833333333333, 86.05833333333334, 86.14166666666667, 85.98333333333333, 85.55833333333334, 85.98333333333333, 86.24166666666666, 86.25833333333334, 86.4, 86.84166666666667, 86.575, 86.44166666666666, 86.31666666666666, 86.225, 86.475, 86.48333333333333, 86.41666666666667, 86.31666666666666, 86.59166666666667, 86.4, 85.975, 86.6, 86.24166666666666, 86.825, 86.725, 86.9, 87.11666666666666, 87.04166666666667, 87.01666666666667, 87.025, 87.09166666666667, 87.08333333333333, 87.23333333333333, 87.01666666666667, 86.70833333333333, 87.15833333333333, 86.875, 87.075, 86.91666666666667, 86.85833333333333, 87.175, 86.98333333333333, 87.0, 86.88333333333334, 87.03333333333333, 86.88333333333334, 87.11666666666666, 87.30833333333334, 87.38333333333334, 87.18333333333334, 87.275, 87.18333333333334, 87.3]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8840
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5840
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8625
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5155
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7585
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8405
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5015
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7880
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7425
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8990
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "RFL.py", line 126, in <module>
    w_local, loss_local, f_k = local.train(copy.deepcopy(net_glob).to(args.device), copy.deepcopy(f_G).to(args.device),
  File "/home/ChenSM/code/FL_HLS/util/local_training.py", line 257, in train
    for batch_idx, (images, labels, idxs) in enumerate(self.ldr_train_tmp):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/util/local_training.py", line 48, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53334 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8915
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5915
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8685
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.4680
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7445
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8525
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.6030
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7890
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7260
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8940
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52236 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8800
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5815
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8750
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5490
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7470
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8680
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4660
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7635
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.6950
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8925
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1977, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train_local):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 56740 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2025
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0730
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7360
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0665
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5790
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4555
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0060
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2925
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6815
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3825
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.178, Test loss: 2.124, Test accuracy: 25.71
Round   0, Global train loss: 2.178, Global test loss: 2.136, Global test accuracy: 26.35
Round   1, Train loss: 1.936, Test loss: 1.842, Test accuracy: 35.86
Round   1, Global train loss: 1.936, Global test loss: 1.733, Global test accuracy: 41.00
Round   2, Train loss: 1.862, Test loss: 1.807, Test accuracy: 38.35
Round   2, Global train loss: 1.862, Global test loss: 1.728, Global test accuracy: 45.54
Round   3, Train loss: 1.720, Test loss: 1.729, Test accuracy: 39.22
Round   3, Global train loss: 1.720, Global test loss: 1.464, Global test accuracy: 49.52
Round   4, Train loss: 1.757, Test loss: 1.733, Test accuracy: 39.60
Round   4, Global train loss: 1.757, Global test loss: 1.590, Global test accuracy: 51.01
Round   5, Train loss: 1.704, Test loss: 1.727, Test accuracy: 39.87
Round   5, Global train loss: 1.704, Global test loss: 1.579, Global test accuracy: 50.87
Round   6, Train loss: 1.745, Test loss: 1.713, Test accuracy: 40.51
Round   6, Global train loss: 1.745, Global test loss: 1.719, Global test accuracy: 47.67
Round   7, Train loss: 1.749, Test loss: 1.699, Test accuracy: 41.10
Round   7, Global train loss: 1.749, Global test loss: 1.647, Global test accuracy: 48.02
Round   8, Train loss: 1.910, Test loss: 1.681, Test accuracy: 41.09
Round   8, Global train loss: 1.910, Global test loss: 1.741, Global test accuracy: 48.31
Round   9, Train loss: 1.743, Test loss: 1.669, Test accuracy: 41.66
Round   9, Global train loss: 1.743, Global test loss: 1.743, Global test accuracy: 47.69
Round  10, Train loss: 1.807, Test loss: 1.660, Test accuracy: 42.34
Round  10, Global train loss: 1.807, Global test loss: 1.708, Global test accuracy: 47.31
Round  11, Train loss: 1.620, Test loss: 1.650, Test accuracy: 42.49
Round  11, Global train loss: 1.620, Global test loss: 1.506, Global test accuracy: 52.57
Round  12, Train loss: 1.797, Test loss: 1.648, Test accuracy: 42.95
Round  12, Global train loss: 1.797, Global test loss: 1.915, Global test accuracy: 37.30
Round  13, Train loss: 1.577, Test loss: 1.636, Test accuracy: 43.45
Round  13, Global train loss: 1.577, Global test loss: 1.531, Global test accuracy: 52.26
Round  14, Train loss: 1.360, Test loss: 1.638, Test accuracy: 43.76
Round  14, Global train loss: 1.360, Global test loss: 1.331, Global test accuracy: 55.27
Round  15, Train loss: 1.595, Test loss: 1.636, Test accuracy: 44.09
Round  15, Global train loss: 1.595, Global test loss: 1.643, Global test accuracy: 49.24
Round  16, Train loss: 1.340, Test loss: 1.634, Test accuracy: 44.49
Round  16, Global train loss: 1.340, Global test loss: 1.421, Global test accuracy: 53.79
Round  17, Train loss: 1.656, Test loss: 1.636, Test accuracy: 44.71
Round  17, Global train loss: 1.656, Global test loss: 1.781, Global test accuracy: 44.31
Round  18, Train loss: 1.471, Test loss: 1.642, Test accuracy: 44.55
Round  18, Global train loss: 1.471, Global test loss: 1.566, Global test accuracy: 48.68
Round  19, Train loss: 1.438, Test loss: 1.652, Test accuracy: 44.58
Round  19, Global train loss: 1.438, Global test loss: 1.490, Global test accuracy: 51.79
Round  20, Train loss: 1.263, Test loss: 1.658, Test accuracy: 44.55
Round  20, Global train loss: 1.263, Global test loss: 1.342, Global test accuracy: 55.21
Round  21, Train loss: 1.224, Test loss: 1.685, Test accuracy: 44.10
Round  21, Global train loss: 1.224, Global test loss: 1.407, Global test accuracy: 52.35
Round  22, Train loss: 1.010, Test loss: 1.699, Test accuracy: 44.23
Round  22, Global train loss: 1.010, Global test loss: 1.264, Global test accuracy: 56.34
Round  23, Train loss: 1.365, Test loss: 1.720, Test accuracy: 43.96
Round  23, Global train loss: 1.365, Global test loss: 1.708, Global test accuracy: 44.50
Round  24, Train loss: 1.252, Test loss: 1.738, Test accuracy: 43.78
Round  24, Global train loss: 1.252, Global test loss: 1.607, Global test accuracy: 46.74
Round  25, Train loss: 1.004, Test loss: 1.751, Test accuracy: 43.92
Round  25, Global train loss: 1.004, Global test loss: 1.262, Global test accuracy: 56.33
Round  26, Train loss: 0.964, Test loss: 1.792, Test accuracy: 43.64
Round  26, Global train loss: 0.964, Global test loss: 1.271, Global test accuracy: 56.49
Round  27, Train loss: 1.655, Test loss: 1.811, Test accuracy: 43.52
Round  27, Global train loss: 1.655, Global test loss: 1.881, Global test accuracy: 38.49
Round  28, Train loss: 1.180, Test loss: 1.812, Test accuracy: 43.65
Round  28, Global train loss: 1.180, Global test loss: 1.620, Global test accuracy: 46.56
Round  29, Train loss: 1.322, Test loss: 1.831, Test accuracy: 43.23
Round  29, Global train loss: 1.322, Global test loss: 1.770, Global test accuracy: 40.03
Round  30, Train loss: 1.055, Test loss: 1.844, Test accuracy: 43.12
Round  30, Global train loss: 1.055, Global test loss: 1.576, Global test accuracy: 44.57
Round  31, Train loss: 1.237, Test loss: 1.844, Test accuracy: 43.55
Round  31, Global train loss: 1.237, Global test loss: 1.718, Global test accuracy: 43.31
Round  32, Train loss: 1.199, Test loss: 1.854, Test accuracy: 43.62
Round  32, Global train loss: 1.199, Global test loss: 1.524, Global test accuracy: 51.27
Round  33, Train loss: 0.890, Test loss: 1.873, Test accuracy: 43.91
Round  33, Global train loss: 0.890, Global test loss: 1.311, Global test accuracy: 54.73
Round  34, Train loss: 1.020, Test loss: 1.914, Test accuracy: 43.90
Round  34, Global train loss: 1.020, Global test loss: 1.411, Global test accuracy: 53.02
Round  35, Train loss: 1.317, Test loss: 1.931, Test accuracy: 43.66
Round  35, Global train loss: 1.317, Global test loss: 1.786, Global test accuracy: 40.62
Round  36, Train loss: 1.011, Test loss: 1.963, Test accuracy: 43.21
Round  36, Global train loss: 1.011, Global test loss: 1.484, Global test accuracy: 50.23
Round  37, Train loss: 1.034, Test loss: 1.960, Test accuracy: 43.30
Round  37, Global train loss: 1.034, Global test loss: 1.428, Global test accuracy: 53.01
Round  38, Train loss: 0.907, Test loss: 2.018, Test accuracy: 42.60
Round  38, Global train loss: 0.907, Global test loss: 1.486, Global test accuracy: 48.71
Round  39, Train loss: 0.694, Test loss: 2.035, Test accuracy: 42.62
Round  39, Global train loss: 0.694, Global test loss: 1.355, Global test accuracy: 53.09
Round  40, Train loss: 0.871, Test loss: 2.081, Test accuracy: 42.44
Round  40, Global train loss: 0.871, Global test loss: 1.498, Global test accuracy: 48.70
Round  41, Train loss: 0.738, Test loss: 2.104, Test accuracy: 42.36
Round  41, Global train loss: 0.738, Global test loss: 1.315, Global test accuracy: 54.96
Round  42, Train loss: 0.832, Test loss: 2.125, Test accuracy: 42.33
Round  42, Global train loss: 0.832, Global test loss: 1.499, Global test accuracy: 49.69
Round  43, Train loss: 0.990, Test loss: 2.148, Test accuracy: 42.59
Round  43, Global train loss: 0.990, Global test loss: 1.669, Global test accuracy: 42.59
Round  44, Train loss: 0.990, Test loss: 2.196, Test accuracy: 42.35
Round  44, Global train loss: 0.990, Global test loss: 1.666, Global test accuracy: 42.60
Round  45, Train loss: 1.020, Test loss: 2.199, Test accuracy: 42.36
Round  45, Global train loss: 1.020, Global test loss: 1.626, Global test accuracy: 44.27
Round  46, Train loss: 0.608, Test loss: 2.246, Test accuracy: 42.39
Round  46, Global train loss: 0.608, Global test loss: 1.352, Global test accuracy: 53.59
Round  47, Train loss: 0.635, Test loss: 2.284, Test accuracy: 42.19
Round  47, Global train loss: 0.635, Global test loss: 1.426, Global test accuracy: 50.18
Round  48, Train loss: 0.898, Test loss: 2.276, Test accuracy: 42.33
Round  48, Global train loss: 0.898, Global test loss: 1.605, Global test accuracy: 43.71
Round  49, Train loss: 0.627, Test loss: 2.288, Test accuracy: 42.27
Round  49, Global train loss: 0.627, Global test loss: 1.438, Global test accuracy: 50.41
Round  50, Train loss: 0.809, Test loss: 2.304, Test accuracy: 42.62
Round  50, Global train loss: 0.809, Global test loss: 1.503, Global test accuracy: 50.47
Round  51, Train loss: 0.603, Test loss: 2.321, Test accuracy: 42.58
Round  51, Global train loss: 0.603, Global test loss: 1.382, Global test accuracy: 52.97
Round  52, Train loss: 0.835, Test loss: 2.348, Test accuracy: 42.56
Round  52, Global train loss: 0.835, Global test loss: 1.550, Global test accuracy: 44.41
Round  53, Train loss: 0.771, Test loss: 2.419, Test accuracy: 41.99
Round  53, Global train loss: 0.771, Global test loss: 1.412, Global test accuracy: 51.67
Round  54, Train loss: 0.761, Test loss: 2.465, Test accuracy: 42.01
Round  54, Global train loss: 0.761, Global test loss: 1.576, Global test accuracy: 44.95
Round  55, Train loss: 0.926, Test loss: 2.491, Test accuracy: 41.87
Round  55, Global train loss: 0.926, Global test loss: 1.754, Global test accuracy: 36.91
Round  56, Train loss: 0.939, Test loss: 2.497, Test accuracy: 41.90
Round  56, Global train loss: 0.939, Global test loss: 1.606, Global test accuracy: 44.33
Round  57, Train loss: 0.521, Test loss: 2.504, Test accuracy: 41.91
Round  57, Global train loss: 0.521, Global test loss: 1.437, Global test accuracy: 50.62
Round  58, Train loss: 0.550, Test loss: 2.558, Test accuracy: 41.79
Round  58, Global train loss: 0.550, Global test loss: 1.470, Global test accuracy: 49.58
Round  59, Train loss: 0.665, Test loss: 2.593, Test accuracy: 42.05
Round  59, Global train loss: 0.665, Global test loss: 1.492, Global test accuracy: 47.96
Round  60, Train loss: 0.628, Test loss: 2.602, Test accuracy: 42.09
Round  60, Global train loss: 0.628, Global test loss: 1.421, Global test accuracy: 51.58
Round  61, Train loss: 0.739, Test loss: 2.612, Test accuracy: 41.99
Round  61, Global train loss: 0.739, Global test loss: 1.470, Global test accuracy: 50.48
Round  62, Train loss: 0.708, Test loss: 2.615, Test accuracy: 42.20
Round  62, Global train loss: 0.708, Global test loss: 1.636, Global test accuracy: 42.77
Round  63, Train loss: 0.808, Test loss: 2.644, Test accuracy: 42.03
Round  63, Global train loss: 0.808, Global test loss: 1.834, Global test accuracy: 33.59
Round  64, Train loss: 0.734, Test loss: 2.692, Test accuracy: 41.55
Round  64, Global train loss: 0.734, Global test loss: 1.662, Global test accuracy: 40.69
Round  65, Train loss: 0.632, Test loss: 2.733, Test accuracy: 41.84
Round  65, Global train loss: 0.632, Global test loss: 1.436, Global test accuracy: 49.49
Round  66, Train loss: 0.623, Test loss: 2.740, Test accuracy: 41.77
Round  66, Global train loss: 0.623, Global test loss: 1.546, Global test accuracy: 46.13
Round  67, Train loss: 0.537, Test loss: 2.747, Test accuracy: 41.83
Round  67, Global train loss: 0.537, Global test loss: 1.507, Global test accuracy: 47.91
Round  68, Train loss: 0.432, Test loss: 2.760, Test accuracy: 41.90
Round  68, Global train loss: 0.432, Global test loss: 1.501, Global test accuracy: 47.34
Round  69, Train loss: 0.620, Test loss: 2.754, Test accuracy: 41.86
Round  69, Global train loss: 0.620, Global test loss: 1.535, Global test accuracy: 47.59
Round  70, Train loss: 0.705, Test loss: 2.805, Test accuracy: 41.68
Round  70, Global train loss: 0.705, Global test loss: 1.829, Global test accuracy: 33.30
Round  71, Train loss: 0.602, Test loss: 2.839, Test accuracy: 41.57
Round  71, Global train loss: 0.602, Global test loss: 1.447, Global test accuracy: 49.86
Round  72, Train loss: 0.843, Test loss: 2.855, Test accuracy: 41.58
Round  72, Global train loss: 0.843, Global test loss: 1.910, Global test accuracy: 30.88
Round  73, Train loss: 0.684, Test loss: 2.915, Test accuracy: 41.42
Round  73, Global train loss: 0.684, Global test loss: 1.703, Global test accuracy: 40.77
Round  74, Train loss: 0.511, Test loss: 2.916, Test accuracy: 41.53
Round  74, Global train loss: 0.511, Global test loss: 1.530, Global test accuracy: 47.41
Round  75, Train loss: 0.510, Test loss: 2.908, Test accuracy: 41.62
Round  75, Global train loss: 0.510, Global test loss: 1.533, Global test accuracy: 46.52
Round  76, Train loss: 0.438, Test loss: 2.973, Test accuracy: 41.39
Round  76, Global train loss: 0.438, Global test loss: 1.552, Global test accuracy: 45.84
Round  77, Train loss: 0.678, Test loss: 3.013, Test accuracy: 41.25
Round  77, Global train loss: 0.678, Global test loss: 1.755, Global test accuracy: 38.85
Round  78, Train loss: 0.596, Test loss: 3.066, Test accuracy: 41.52
Round  78, Global train loss: 0.596, Global test loss: 1.562, Global test accuracy: 46.69
Round  79, Train loss: 0.700, Test loss: 3.026, Test accuracy: 41.37
Round  79, Global train loss: 0.700, Global test loss: 1.630, Global test accuracy: 43.77
Round  80, Train loss: 0.426, Test loss: 3.068, Test accuracy: 41.30
Round  80, Global train loss: 0.426, Global test loss: 1.355, Global test accuracy: 52.87
Round  81, Train loss: 0.617, Test loss: 3.116, Test accuracy: 41.16
Round  81, Global train loss: 0.617, Global test loss: 1.666, Global test accuracy: 43.12
Round  82, Train loss: 0.380, Test loss: 3.125, Test accuracy: 41.35
Round  82, Global train loss: 0.380, Global test loss: 1.425, Global test accuracy: 51.41
Round  83, Train loss: 0.350, Test loss: 3.130, Test accuracy: 41.15
Round  83, Global train loss: 0.350, Global test loss: 1.442, Global test accuracy: 51.30
Round  84, Train loss: 0.637, Test loss: 3.154, Test accuracy: 41.13
Round  84, Global train loss: 0.637, Global test loss: 1.807, Global test accuracy: 34.23
Round  85, Train loss: 0.718, Test loss: 3.135, Test accuracy: 41.20
Round  85, Global train loss: 0.718, Global test loss: 1.696, Global test accuracy: 43.30
Round  86, Train loss: 0.356, Test loss: 3.162, Test accuracy: 41.25
Round  86, Global train loss: 0.356, Global test loss: 1.453, Global test accuracy: 50.72
Round  87, Train loss: 0.371, Test loss: 3.188, Test accuracy: 41.02
Round  87, Global train loss: 0.371, Global test loss: 1.473, Global test accuracy: 48.44
Round  88, Train loss: 0.476, Test loss: 3.203, Test accuracy: 40.95
Round  88, Global train loss: 0.476, Global test loss: 1.681, Global test accuracy: 39.74
Round  89, Train loss: 0.488, Test loss: 3.213, Test accuracy: 40.97
Round  89, Global train loss: 0.488, Global test loss: 1.648, Global test accuracy: 41.68
Round  90, Train loss: 0.641, Test loss: 3.202, Test accuracy: 41.13
Round  90, Global train loss: 0.641, Global test loss: 1.925, Global test accuracy: 31.87
Round  91, Train loss: 0.775, Test loss: 3.202, Test accuracy: 41.01
Round  91, Global train loss: 0.775, Global test loss: 1.995, Global test accuracy: 27.22
Round  92, Train loss: 0.444, Test loss: 3.222, Test accuracy: 41.10
Round  92, Global train loss: 0.444, Global test loss: 1.674, Global test accuracy: 39.94
Round  93, Train loss: 0.502, Test loss: 3.251, Test accuracy: 41.27
Round  93, Global train loss: 0.502, Global test loss: 1.674, Global test accuracy: 39.55
Round  94, Train loss: 0.417, Test loss: 3.303, Test accuracy: 41.39
Round  94, Global train loss: 0.417, Global test loss: 1.580, Global test accuracy: 44.01
Round  95, Train loss: 0.613, Test loss: 3.390, Test accuracy: 40.94
Round  95, Global train loss: 0.613, Global test loss: 1.924, Global test accuracy: 29.26
Round  96, Train loss: 0.449, Test loss: 3.411, Test accuracy: 41.12
Round  96, Global train loss: 0.449, Global test loss: 1.536, Global test accuracy: 45.92
Round  97, Train loss: 0.569, Test loss: 3.427, Test accuracy: 40.80
Round  97, Global train loss: 0.569, Global test loss: 1.925, Global test accuracy: 30.55
Round  98, Train loss: 0.512, Test loss: 3.440, Test accuracy: 41.12
Round  98, Global train loss: 0.512, Global test loss: 1.700, Global test accuracy: 40.38
Round  99, Train loss: 0.307, Test loss: 3.448, Test accuracy: 41.19
Round  99, Global train loss: 0.307, Global test loss: 1.377, Global test accuracy: 54.57
Final Round, Train loss: 0.312, Test loss: 3.883, Test accuracy: 41.37
Final Round, Global train loss: 0.312, Global test loss: 1.377, Global test accuracy: 54.57
Average accuracy final 10 rounds: 41.10725 

Average global accuracy final 10 rounds: 38.328 

6336.560024499893
[5.433935165405273, 10.867870330810547, 15.535231590270996, 20.202592849731445, 24.510946035385132, 28.81929922103882, 33.142112731933594, 37.46492624282837, 41.763620376586914, 46.06231451034546, 50.376636266708374, 54.69095802307129, 59.02130603790283, 63.351654052734375, 67.64563250541687, 71.93961095809937, 76.2564001083374, 80.57318925857544, 84.88937258720398, 89.20555591583252, 93.5259337425232, 97.84631156921387, 102.17420315742493, 106.50209474563599, 110.80413365364075, 115.10617256164551, 119.40888500213623, 123.71159744262695, 128.07888317108154, 132.44616889953613, 136.76052284240723, 141.07487678527832, 145.39901852607727, 149.72316026687622, 154.0521068572998, 158.3810534477234, 162.68521547317505, 166.9893774986267, 171.33082962036133, 175.67228174209595, 179.94835758209229, 184.22443342208862, 188.53068947792053, 192.83694553375244, 197.2599835395813, 201.68302154541016, 206.01804065704346, 210.35305976867676, 214.68275451660156, 219.01244926452637, 223.35502171516418, 227.697594165802, 231.97439193725586, 236.25118970870972, 240.56670260429382, 244.88221549987793, 249.24164748191833, 253.60107946395874, 257.9569835662842, 262.3128876686096, 266.73078322410583, 271.14867877960205, 275.57583260536194, 280.0029864311218, 284.39409923553467, 288.7852120399475, 293.14394330978394, 297.50267457962036, 301.8697488307953, 306.2368230819702, 310.57997369766235, 314.9231243133545, 319.3488872051239, 323.7746500968933, 328.84448742866516, 333.914324760437, 338.9341514110565, 343.953978061676, 348.955792427063, 353.95760679244995, 358.95324754714966, 363.94888830184937, 368.96558952331543, 373.9822907447815, 378.998202085495, 384.0141134262085, 389.0627453327179, 394.1113772392273, 399.14700841903687, 404.18263959884644, 409.2268497943878, 414.2710599899292, 419.2827982902527, 424.2945365905762, 429.3249475955963, 434.35535860061646, 439.36860847473145, 444.38185834884644, 448.88140535354614, 453.38095235824585, 457.8110029697418, 462.2410535812378, 467.2584538459778, 472.2758541107178, 477.257199048996, 482.23854398727417, 487.16146969795227, 492.08439540863037, 497.0255534648895, 501.9667115211487, 506.3998191356659, 510.8329267501831, 515.37184715271, 519.9107675552368, 524.3448905944824, 528.779013633728, 533.1835741996765, 537.588134765625, 541.9987943172455, 546.409453868866, 550.8235061168671, 555.2375583648682, 559.6715173721313, 564.1054763793945, 568.5066485404968, 572.9078207015991, 577.2930274009705, 581.6782341003418, 586.0843393802643, 590.4904446601868, 594.8899066448212, 599.2893686294556, 603.6668183803558, 608.0442681312561, 612.4436221122742, 616.8429760932922, 621.2069752216339, 625.5709743499756, 629.9665443897247, 634.3621144294739, 638.7910039424896, 643.2198934555054, 647.6313805580139, 652.0428676605225, 656.6332490444183, 661.2236304283142, 665.6852021217346, 670.146773815155, 674.6283142566681, 679.1098546981812, 683.5731148719788, 688.0363750457764, 693.0498766899109, 698.0633783340454, 702.4407775402069, 706.8181767463684, 711.1714358329773, 715.5246949195862, 719.8781158924103, 724.2315368652344, 728.719108581543, 733.2066802978516, 737.7256569862366, 742.2446336746216, 746.7469215393066, 751.2492094039917, 755.71977186203, 760.1903343200684, 764.9476463794708, 769.7049584388733, 774.1162655353546, 778.5275726318359, 782.9719026088715, 787.416232585907, 791.8730666637421, 796.3299007415771, 801.3429288864136, 806.35595703125, 811.3648738861084, 816.3737907409668, 821.2720775604248, 826.1703643798828, 831.0909078121185, 836.0114512443542, 840.9096894264221, 845.80792760849, 850.7888572216034, 855.7697868347168, 860.938175201416, 866.1065635681152, 870.9488005638123, 875.7910375595093, 880.7632269859314, 885.7354164123535, 890.1797246932983, 894.6240329742432, 899.0383574962616, 903.45268201828, 907.8171238899231, 912.1815657615662, 914.4356398582458, 916.6897139549255]
[25.715, 25.715, 35.86, 35.86, 38.355, 38.355, 39.2175, 39.2175, 39.6025, 39.6025, 39.8725, 39.8725, 40.5125, 40.5125, 41.1, 41.1, 41.095, 41.095, 41.6625, 41.6625, 42.34, 42.34, 42.495, 42.495, 42.955, 42.955, 43.455, 43.455, 43.76, 43.76, 44.0925, 44.0925, 44.49, 44.49, 44.7075, 44.7075, 44.555, 44.555, 44.5775, 44.5775, 44.5525, 44.5525, 44.0975, 44.0975, 44.235, 44.235, 43.9575, 43.9575, 43.785, 43.785, 43.92, 43.92, 43.6375, 43.6375, 43.5225, 43.5225, 43.65, 43.65, 43.225, 43.225, 43.12, 43.12, 43.555, 43.555, 43.6175, 43.6175, 43.9125, 43.9125, 43.895, 43.895, 43.66, 43.66, 43.2125, 43.2125, 43.305, 43.305, 42.605, 42.605, 42.6225, 42.6225, 42.44, 42.44, 42.36, 42.36, 42.33, 42.33, 42.585, 42.585, 42.3475, 42.3475, 42.3625, 42.3625, 42.3925, 42.3925, 42.1875, 42.1875, 42.3325, 42.3325, 42.2675, 42.2675, 42.6225, 42.6225, 42.575, 42.575, 42.565, 42.565, 41.995, 41.995, 42.0075, 42.0075, 41.87, 41.87, 41.9, 41.9, 41.9125, 41.9125, 41.7875, 41.7875, 42.0525, 42.0525, 42.0875, 42.0875, 41.9925, 41.9925, 42.205, 42.205, 42.03, 42.03, 41.545, 41.545, 41.84, 41.84, 41.765, 41.765, 41.8325, 41.8325, 41.895, 41.895, 41.8625, 41.8625, 41.68, 41.68, 41.5675, 41.5675, 41.575, 41.575, 41.4175, 41.4175, 41.535, 41.535, 41.6175, 41.6175, 41.3925, 41.3925, 41.2475, 41.2475, 41.52, 41.52, 41.365, 41.365, 41.3, 41.3, 41.155, 41.155, 41.3475, 41.3475, 41.15, 41.15, 41.135, 41.135, 41.195, 41.195, 41.2475, 41.2475, 41.0175, 41.0175, 40.945, 40.945, 40.965, 40.965, 41.13, 41.13, 41.01, 41.01, 41.1025, 41.1025, 41.2675, 41.2675, 41.39, 41.39, 40.9425, 40.9425, 41.125, 41.125, 40.8, 40.8, 41.115, 41.115, 41.19, 41.19, 41.37, 41.37]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2030
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0550
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7320
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0675
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5835
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4595
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1095
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2710
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6490
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3105
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.059, Test loss: 1.907, Test accuracy: 32.92
Round   0, Global train loss: 2.059, Global test loss: 1.902, Global test accuracy: 33.67
Round   1, Train loss: 1.878, Test loss: 1.702, Test accuracy: 39.15
Round   1, Global train loss: 1.878, Global test loss: 1.616, Global test accuracy: 43.00
Round   2, Train loss: 1.775, Test loss: 1.621, Test accuracy: 42.08
Round   2, Global train loss: 1.775, Global test loss: 1.498, Global test accuracy: 48.02
Round   3, Train loss: 1.653, Test loss: 1.587, Test accuracy: 42.84
Round   3, Global train loss: 1.653, Global test loss: 1.369, Global test accuracy: 53.09
Round   4, Train loss: 1.572, Test loss: 1.555, Test accuracy: 44.38
Round   4, Global train loss: 1.572, Global test loss: 1.268, Global test accuracy: 56.78
Round   5, Train loss: 1.525, Test loss: 1.507, Test accuracy: 47.12
Round   5, Global train loss: 1.525, Global test loss: 1.245, Global test accuracy: 58.41
Round   6, Train loss: 1.459, Test loss: 1.470, Test accuracy: 48.74
Round   6, Global train loss: 1.459, Global test loss: 1.204, Global test accuracy: 60.16
Round   7, Train loss: 1.506, Test loss: 1.425, Test accuracy: 50.88
Round   7, Global train loss: 1.506, Global test loss: 1.193, Global test accuracy: 61.41
Round   8, Train loss: 1.363, Test loss: 1.386, Test accuracy: 52.04
Round   8, Global train loss: 1.363, Global test loss: 1.102, Global test accuracy: 62.67
Round   9, Train loss: 1.372, Test loss: 1.363, Test accuracy: 53.16
Round   9, Global train loss: 1.372, Global test loss: 1.085, Global test accuracy: 64.30
Round  10, Train loss: 1.249, Test loss: 1.326, Test accuracy: 54.82
Round  10, Global train loss: 1.249, Global test loss: 1.029, Global test accuracy: 65.28
Round  11, Train loss: 1.342, Test loss: 1.318, Test accuracy: 55.14
Round  11, Global train loss: 1.342, Global test loss: 1.053, Global test accuracy: 65.96
Round  12, Train loss: 1.291, Test loss: 1.300, Test accuracy: 55.94
Round  12, Global train loss: 1.291, Global test loss: 1.015, Global test accuracy: 66.45
Round  13, Train loss: 1.137, Test loss: 1.277, Test accuracy: 56.88
Round  13, Global train loss: 1.137, Global test loss: 0.941, Global test accuracy: 68.57
Round  14, Train loss: 1.279, Test loss: 1.235, Test accuracy: 58.66
Round  14, Global train loss: 1.279, Global test loss: 0.990, Global test accuracy: 68.19
Round  15, Train loss: 1.184, Test loss: 1.213, Test accuracy: 59.53
Round  15, Global train loss: 1.184, Global test loss: 0.939, Global test accuracy: 69.08
Round  16, Train loss: 1.258, Test loss: 1.207, Test accuracy: 59.83
Round  16, Global train loss: 1.258, Global test loss: 0.969, Global test accuracy: 69.49
Round  17, Train loss: 1.214, Test loss: 1.203, Test accuracy: 60.02
Round  17, Global train loss: 1.214, Global test loss: 0.950, Global test accuracy: 69.55
Round  18, Train loss: 1.126, Test loss: 1.177, Test accuracy: 61.14
Round  18, Global train loss: 1.126, Global test loss: 0.892, Global test accuracy: 70.44
Round  19, Train loss: 1.004, Test loss: 1.171, Test accuracy: 61.45
Round  19, Global train loss: 1.004, Global test loss: 0.873, Global test accuracy: 70.55
Round  20, Train loss: 1.095, Test loss: 1.165, Test accuracy: 61.77
Round  20, Global train loss: 1.095, Global test loss: 0.903, Global test accuracy: 71.36
Round  21, Train loss: 1.142, Test loss: 1.158, Test accuracy: 62.36
Round  21, Global train loss: 1.142, Global test loss: 0.897, Global test accuracy: 71.32
Round  22, Train loss: 1.072, Test loss: 1.137, Test accuracy: 63.35
Round  22, Global train loss: 1.072, Global test loss: 0.866, Global test accuracy: 71.97
Round  23, Train loss: 1.077, Test loss: 1.132, Test accuracy: 63.57
Round  23, Global train loss: 1.077, Global test loss: 0.855, Global test accuracy: 72.43
Round  24, Train loss: 1.038, Test loss: 1.132, Test accuracy: 63.44
Round  24, Global train loss: 1.038, Global test loss: 0.857, Global test accuracy: 72.44
Round  25, Train loss: 0.948, Test loss: 1.137, Test accuracy: 63.11
Round  25, Global train loss: 0.948, Global test loss: 0.839, Global test accuracy: 72.08
Round  26, Train loss: 1.052, Test loss: 1.134, Test accuracy: 63.32
Round  26, Global train loss: 1.052, Global test loss: 0.851, Global test accuracy: 72.72
Round  27, Train loss: 0.932, Test loss: 1.119, Test accuracy: 64.16
Round  27, Global train loss: 0.932, Global test loss: 0.803, Global test accuracy: 73.42
Round  28, Train loss: 1.097, Test loss: 1.112, Test accuracy: 64.35
Round  28, Global train loss: 1.097, Global test loss: 0.866, Global test accuracy: 72.80
Round  29, Train loss: 0.961, Test loss: 1.114, Test accuracy: 64.43
Round  29, Global train loss: 0.961, Global test loss: 0.817, Global test accuracy: 73.06
Round  30, Train loss: 1.074, Test loss: 1.114, Test accuracy: 64.82
Round  30, Global train loss: 1.074, Global test loss: 0.868, Global test accuracy: 72.96
Round  31, Train loss: 1.038, Test loss: 1.106, Test accuracy: 64.99
Round  31, Global train loss: 1.038, Global test loss: 0.861, Global test accuracy: 72.43
Round  32, Train loss: 1.013, Test loss: 1.108, Test accuracy: 64.93
Round  32, Global train loss: 1.013, Global test loss: 0.835, Global test accuracy: 73.59
Round  33, Train loss: 0.936, Test loss: 1.108, Test accuracy: 64.95
Round  33, Global train loss: 0.936, Global test loss: 0.817, Global test accuracy: 72.87
Round  34, Train loss: 0.947, Test loss: 1.103, Test accuracy: 65.14
Round  34, Global train loss: 0.947, Global test loss: 0.795, Global test accuracy: 74.39
Round  35, Train loss: 0.945, Test loss: 1.114, Test accuracy: 64.86
Round  35, Global train loss: 0.945, Global test loss: 0.815, Global test accuracy: 73.74
Round  36, Train loss: 0.870, Test loss: 1.110, Test accuracy: 64.99
Round  36, Global train loss: 0.870, Global test loss: 0.788, Global test accuracy: 74.29
Round  37, Train loss: 0.936, Test loss: 1.109, Test accuracy: 65.00
Round  37, Global train loss: 0.936, Global test loss: 0.798, Global test accuracy: 74.31
Round  38, Train loss: 0.980, Test loss: 1.103, Test accuracy: 65.24
Round  38, Global train loss: 0.980, Global test loss: 0.819, Global test accuracy: 74.15
Round  39, Train loss: 1.034, Test loss: 1.089, Test accuracy: 65.80
Round  39, Global train loss: 1.034, Global test loss: 0.855, Global test accuracy: 73.09
Round  40, Train loss: 0.909, Test loss: 1.084, Test accuracy: 66.00
Round  40, Global train loss: 0.909, Global test loss: 0.807, Global test accuracy: 73.99
Round  41, Train loss: 0.888, Test loss: 1.088, Test accuracy: 65.80
Round  41, Global train loss: 0.888, Global test loss: 0.798, Global test accuracy: 73.96
Round  42, Train loss: 0.855, Test loss: 1.094, Test accuracy: 65.71
Round  42, Global train loss: 0.855, Global test loss: 0.794, Global test accuracy: 73.96
Round  43, Train loss: 0.945, Test loss: 1.096, Test accuracy: 65.75
Round  43, Global train loss: 0.945, Global test loss: 0.813, Global test accuracy: 73.89
Round  44, Train loss: 0.889, Test loss: 1.106, Test accuracy: 65.55
Round  44, Global train loss: 0.889, Global test loss: 0.813, Global test accuracy: 73.84
Round  45, Train loss: 0.794, Test loss: 1.115, Test accuracy: 65.48
Round  45, Global train loss: 0.794, Global test loss: 0.770, Global test accuracy: 74.99
Round  46, Train loss: 0.834, Test loss: 1.113, Test accuracy: 65.70
Round  46, Global train loss: 0.834, Global test loss: 0.795, Global test accuracy: 74.18
Round  47, Train loss: 0.870, Test loss: 1.125, Test accuracy: 65.36
Round  47, Global train loss: 0.870, Global test loss: 0.801, Global test accuracy: 74.34
Round  48, Train loss: 0.860, Test loss: 1.122, Test accuracy: 65.38
Round  48, Global train loss: 0.860, Global test loss: 0.797, Global test accuracy: 74.13
Round  49, Train loss: 0.791, Test loss: 1.112, Test accuracy: 65.68
Round  49, Global train loss: 0.791, Global test loss: 0.807, Global test accuracy: 74.41
Round  50, Train loss: 0.883, Test loss: 1.131, Test accuracy: 65.45
Round  50, Global train loss: 0.883, Global test loss: 0.807, Global test accuracy: 74.75
Round  51, Train loss: 0.932, Test loss: 1.133, Test accuracy: 65.62
Round  51, Global train loss: 0.932, Global test loss: 0.823, Global test accuracy: 74.04
Round  52, Train loss: 0.872, Test loss: 1.125, Test accuracy: 65.94
Round  52, Global train loss: 0.872, Global test loss: 0.791, Global test accuracy: 74.69
Round  53, Train loss: 0.737, Test loss: 1.117, Test accuracy: 66.13
Round  53, Global train loss: 0.737, Global test loss: 0.788, Global test accuracy: 74.66
Round  54, Train loss: 0.865, Test loss: 1.120, Test accuracy: 65.89
Round  54, Global train loss: 0.865, Global test loss: 0.814, Global test accuracy: 74.01
Round  55, Train loss: 0.734, Test loss: 1.123, Test accuracy: 65.95
Round  55, Global train loss: 0.734, Global test loss: 0.768, Global test accuracy: 75.10
Round  56, Train loss: 0.832, Test loss: 1.112, Test accuracy: 66.30
Round  56, Global train loss: 0.832, Global test loss: 0.811, Global test accuracy: 74.14
Round  57, Train loss: 0.848, Test loss: 1.119, Test accuracy: 66.18
Round  57, Global train loss: 0.848, Global test loss: 0.811, Global test accuracy: 74.31
Round  58, Train loss: 0.915, Test loss: 1.112, Test accuracy: 66.13
Round  58, Global train loss: 0.915, Global test loss: 0.826, Global test accuracy: 73.92
Round  59, Train loss: 0.824, Test loss: 1.108, Test accuracy: 66.44
Round  59, Global train loss: 0.824, Global test loss: 0.795, Global test accuracy: 74.49
Round  60, Train loss: 0.812, Test loss: 1.112, Test accuracy: 66.50
Round  60, Global train loss: 0.812, Global test loss: 0.795, Global test accuracy: 74.58
Round  61, Train loss: 0.801, Test loss: 1.109, Test accuracy: 66.47
Round  61, Global train loss: 0.801, Global test loss: 0.815, Global test accuracy: 74.23
Round  62, Train loss: 0.840, Test loss: 1.119, Test accuracy: 66.32
Round  62, Global train loss: 0.840, Global test loss: 0.796, Global test accuracy: 74.53
Round  63, Train loss: 0.776, Test loss: 1.126, Test accuracy: 66.13
Round  63, Global train loss: 0.776, Global test loss: 0.799, Global test accuracy: 74.63
Round  64, Train loss: 0.856, Test loss: 1.127, Test accuracy: 66.20
Round  64, Global train loss: 0.856, Global test loss: 0.791, Global test accuracy: 74.95
Round  65, Train loss: 0.798, Test loss: 1.152, Test accuracy: 65.74
Round  65, Global train loss: 0.798, Global test loss: 0.805, Global test accuracy: 74.93
Round  66, Train loss: 0.845, Test loss: 1.143, Test accuracy: 65.84
Round  66, Global train loss: 0.845, Global test loss: 0.780, Global test accuracy: 74.97
Round  67, Train loss: 0.747, Test loss: 1.134, Test accuracy: 65.91
Round  67, Global train loss: 0.747, Global test loss: 0.781, Global test accuracy: 75.09
Round  68, Train loss: 0.850, Test loss: 1.136, Test accuracy: 66.02
Round  68, Global train loss: 0.850, Global test loss: 0.829, Global test accuracy: 74.21
Round  69, Train loss: 0.791, Test loss: 1.137, Test accuracy: 65.98
Round  69, Global train loss: 0.791, Global test loss: 0.790, Global test accuracy: 74.94
Round  70, Train loss: 0.835, Test loss: 1.135, Test accuracy: 65.91
Round  70, Global train loss: 0.835, Global test loss: 0.800, Global test accuracy: 74.77
Round  71, Train loss: 0.766, Test loss: 1.128, Test accuracy: 66.18
Round  71, Global train loss: 0.766, Global test loss: 0.835, Global test accuracy: 74.10
Round  72, Train loss: 0.723, Test loss: 1.140, Test accuracy: 66.31
Round  72, Global train loss: 0.723, Global test loss: 0.773, Global test accuracy: 75.58
Round  73, Train loss: 0.698, Test loss: 1.144, Test accuracy: 66.44
Round  73, Global train loss: 0.698, Global test loss: 0.781, Global test accuracy: 75.75
Round  74, Train loss: 0.726, Test loss: 1.148, Test accuracy: 66.46
Round  74, Global train loss: 0.726, Global test loss: 0.782, Global test accuracy: 75.47
Round  75, Train loss: 0.816, Test loss: 1.133, Test accuracy: 66.81
Round  75, Global train loss: 0.816, Global test loss: 0.767, Global test accuracy: 75.28
Round  76, Train loss: 0.746, Test loss: 1.134, Test accuracy: 66.89
Round  76, Global train loss: 0.746, Global test loss: 0.801, Global test accuracy: 74.71
Round  77, Train loss: 0.763, Test loss: 1.131, Test accuracy: 67.07
Round  77, Global train loss: 0.763, Global test loss: 0.807, Global test accuracy: 75.03
Round  78, Train loss: 0.712, Test loss: 1.129, Test accuracy: 66.95
Round  78, Global train loss: 0.712, Global test loss: 0.800, Global test accuracy: 74.96
Round  79, Train loss: 0.685, Test loss: 1.134, Test accuracy: 66.88
Round  79, Global train loss: 0.685, Global test loss: 0.779, Global test accuracy: 75.49
Round  80, Train loss: 0.775, Test loss: 1.141, Test accuracy: 66.55
Round  80, Global train loss: 0.775, Global test loss: 0.824, Global test accuracy: 74.20
Round  81, Train loss: 0.674, Test loss: 1.136, Test accuracy: 66.65
Round  81, Global train loss: 0.674, Global test loss: 0.785, Global test accuracy: 75.36
Round  82, Train loss: 0.820, Test loss: 1.135, Test accuracy: 66.67
Round  82, Global train loss: 0.820, Global test loss: 0.826, Global test accuracy: 74.41
Round  83, Train loss: 0.786, Test loss: 1.146, Test accuracy: 66.23
Round  83, Global train loss: 0.786, Global test loss: 0.828, Global test accuracy: 74.28
Round  84, Train loss: 0.667, Test loss: 1.134, Test accuracy: 66.58
Round  84, Global train loss: 0.667, Global test loss: 0.779, Global test accuracy: 75.69
Round  85, Train loss: 0.735, Test loss: 1.128, Test accuracy: 67.12
Round  85, Global train loss: 0.735, Global test loss: 0.789, Global test accuracy: 75.38
Round  86, Train loss: 0.748, Test loss: 1.134, Test accuracy: 67.21
Round  86, Global train loss: 0.748, Global test loss: 0.761, Global test accuracy: 76.16
Round  87, Train loss: 0.767, Test loss: 1.137, Test accuracy: 67.10
Round  87, Global train loss: 0.767, Global test loss: 0.803, Global test accuracy: 75.12
Round  88, Train loss: 0.676, Test loss: 1.137, Test accuracy: 67.11
Round  88, Global train loss: 0.676, Global test loss: 0.781, Global test accuracy: 75.69
Round  89, Train loss: 0.681, Test loss: 1.145, Test accuracy: 66.88
Round  89, Global train loss: 0.681, Global test loss: 0.794, Global test accuracy: 75.17
Round  90, Train loss: 0.742, Test loss: 1.149, Test accuracy: 66.78
Round  90, Global train loss: 0.742, Global test loss: 0.829, Global test accuracy: 74.71
Round  91, Train loss: 0.734, Test loss: 1.153, Test accuracy: 66.81
Round  91, Global train loss: 0.734, Global test loss: 0.795, Global test accuracy: 75.09
Round  92, Train loss: 0.769, Test loss: 1.148, Test accuracy: 66.67
Round  92, Global train loss: 0.769, Global test loss: 0.840, Global test accuracy: 73.95
Round  93, Train loss: 0.775, Test loss: 1.153, Test accuracy: 66.47
Round  93, Global train loss: 0.775, Global test loss: 0.786, Global test accuracy: 75.57
Round  94, Train loss: 0.673, Test loss: 1.145, Test accuracy: 66.56
Round  94, Global train loss: 0.673, Global test loss: 0.795, Global test accuracy: 75.11
Round  95, Train loss: 0.717, Test loss: 1.146, Test accuracy: 66.53
Round  95, Global train loss: 0.717, Global test loss: 0.812, Global test accuracy: 74.96
Round  96, Train loss: 0.765, Test loss: 1.154, Test accuracy: 66.15
Round  96, Global train loss: 0.765, Global test loss: 0.844, Global test accuracy: 74.19
Round  97, Train loss: 0.757, Test loss: 1.159, Test accuracy: 66.23
Round  97, Global train loss: 0.757, Global test loss: 0.812, Global test accuracy: 74.59
Round  98, Train loss: 0.626, Test loss: 1.177, Test accuracy: 66.09
Round  98, Global train loss: 0.626, Global test loss: 0.782, Global test accuracy: 76.00
Round  99, Train loss: 0.648, Test loss: 1.156, Test accuracy: 66.62
Round  99, Global train loss: 0.648, Global test loss: 0.782, Global test accuracy: 75.93
Final Round, Train loss: 0.464, Test loss: 1.288, Test accuracy: 66.39
Final Round, Global train loss: 0.464, Global test loss: 0.782, Global test accuracy: 75.93
Average accuracy final 10 rounds: 66.49125000000001 

Average global accuracy final 10 rounds: 75.0105 

5964.301768064499
[5.254067420959473, 10.508134841918945, 15.283394575119019, 20.058654308319092, 24.847365617752075, 29.63607692718506, 34.510512351989746, 39.384947776794434, 43.84851431846619, 48.31208086013794, 53.05142855644226, 57.79077625274658, 62.43645644187927, 67.08213663101196, 72.06933641433716, 77.05653619766235, 81.76196670532227, 86.46739721298218, 90.799320936203, 95.13124465942383, 99.54560780525208, 103.95997095108032, 108.2714786529541, 112.58298635482788, 116.93151640892029, 121.2800464630127, 125.67639780044556, 130.07274913787842, 134.43007946014404, 138.78740978240967, 143.13978719711304, 147.4921646118164, 151.80991172790527, 156.12765884399414, 160.76647567749023, 165.40529251098633, 169.694189786911, 173.9830870628357, 178.67686676979065, 183.3706464767456, 188.1149480342865, 192.8592495918274, 197.47328400611877, 202.08731842041016, 206.31308317184448, 210.5388479232788, 214.80770468711853, 219.07656145095825, 223.3104019165039, 227.54424238204956, 231.78971648216248, 236.0351905822754, 240.36132645606995, 244.6874623298645, 249.07540941238403, 253.46335649490356, 258.1380181312561, 262.81267976760864, 267.4596817493439, 272.1066837310791, 276.42740631103516, 280.7481288909912, 285.01639461517334, 289.28466033935547, 293.64000940322876, 297.99535846710205, 302.84546184539795, 307.69556522369385, 312.61376428604126, 317.5319633483887, 322.39614057540894, 327.2603178024292, 332.0969657897949, 336.93361377716064, 341.4825556278229, 346.0314974784851, 350.7372844219208, 355.44307136535645, 360.24652004241943, 365.0499687194824, 369.9440424442291, 374.83811616897583, 380.0471806526184, 385.256245136261, 389.85991621017456, 394.46358728408813, 398.8523654937744, 403.2411437034607, 407.6087067127228, 411.97626972198486, 416.2692618370056, 420.56225395202637, 424.93141198158264, 429.3005700111389, 433.64993166923523, 437.99929332733154, 442.26006746292114, 446.52084159851074, 450.8243246078491, 455.1278076171875, 459.4838969707489, 463.8399863243103, 468.22542333602905, 472.6108603477478, 476.8851532936096, 481.15944623947144, 485.46544003486633, 489.77143383026123, 494.10439705848694, 498.43736028671265, 502.80757570266724, 507.1777911186218, 511.96397376060486, 516.7501564025879, 521.410480260849, 526.0708041191101, 530.3934359550476, 534.7160677909851, 538.9013087749481, 543.0865497589111, 547.2965281009674, 551.5065064430237, 556.0830578804016, 560.6596093177795, 565.0817148685455, 569.5038204193115, 573.8786408901215, 578.2534613609314, 582.7408883571625, 587.2283153533936, 591.7478613853455, 596.2674074172974, 600.7638993263245, 605.2603912353516, 609.7830214500427, 614.3056516647339, 618.757904291153, 623.210156917572, 627.6488854885101, 632.0876140594482, 636.5629432201385, 641.0382723808289, 645.2876539230347, 649.5370354652405, 653.9471969604492, 658.357358455658, 662.6835391521454, 667.0097198486328, 671.2803606987, 675.5510015487671, 679.7932243347168, 684.0354471206665, 688.3271245956421, 692.6188020706177, 696.8724467754364, 701.1260914802551, 705.3170800209045, 709.508068561554, 713.72771525383, 717.947361946106, 722.202689409256, 726.458016872406, 730.6921408176422, 734.9262647628784, 739.227597951889, 743.5289311408997, 748.1063799858093, 752.683828830719, 756.9726135730743, 761.2613983154297, 765.4582133293152, 769.6550283432007, 773.8683331012726, 778.0816378593445, 782.2876937389374, 786.4937496185303, 790.7309668064117, 794.9681839942932, 799.1631510257721, 803.358118057251, 807.5663778781891, 811.7746376991272, 816.0183029174805, 820.2619681358337, 824.4601187705994, 828.658269405365, 832.9592852592468, 837.2603011131287, 841.8409719467163, 846.421642780304, 851.2494120597839, 856.0771813392639, 860.4783637523651, 864.8795461654663, 869.2457928657532, 873.61203956604, 877.9064979553223, 882.2009563446045, 886.9027352333069, 891.6045141220093, 893.8905320167542, 896.176549911499]
[32.92, 32.92, 39.1475, 39.1475, 42.075, 42.075, 42.8375, 42.8375, 44.385, 44.385, 47.1225, 47.1225, 48.745, 48.745, 50.88, 50.88, 52.04, 52.04, 53.1625, 53.1625, 54.8225, 54.8225, 55.1375, 55.1375, 55.9375, 55.9375, 56.8825, 56.8825, 58.665, 58.665, 59.5275, 59.5275, 59.8325, 59.8325, 60.015, 60.015, 61.14, 61.14, 61.455, 61.455, 61.765, 61.765, 62.3625, 62.3625, 63.355, 63.355, 63.5725, 63.5725, 63.44, 63.44, 63.1075, 63.1075, 63.3225, 63.3225, 64.1625, 64.1625, 64.3525, 64.3525, 64.4275, 64.4275, 64.8225, 64.8225, 64.9925, 64.9925, 64.93, 64.93, 64.955, 64.955, 65.135, 65.135, 64.8625, 64.8625, 64.9925, 64.9925, 65.005, 65.005, 65.2375, 65.2375, 65.8025, 65.8025, 66.0025, 66.0025, 65.7975, 65.7975, 65.7075, 65.7075, 65.75, 65.75, 65.55, 65.55, 65.485, 65.485, 65.6975, 65.6975, 65.355, 65.355, 65.375, 65.375, 65.6825, 65.6825, 65.45, 65.45, 65.62, 65.62, 65.9375, 65.9375, 66.1325, 66.1325, 65.895, 65.895, 65.955, 65.955, 66.3, 66.3, 66.1775, 66.1775, 66.1275, 66.1275, 66.4425, 66.4425, 66.5025, 66.5025, 66.465, 66.465, 66.32, 66.32, 66.1325, 66.1325, 66.1975, 66.1975, 65.7425, 65.7425, 65.84, 65.84, 65.9125, 65.9125, 66.02, 66.02, 65.98, 65.98, 65.905, 65.905, 66.18, 66.18, 66.3125, 66.3125, 66.44, 66.44, 66.4575, 66.4575, 66.8125, 66.8125, 66.8875, 66.8875, 67.07, 67.07, 66.9475, 66.9475, 66.875, 66.875, 66.55, 66.55, 66.65, 66.65, 66.6725, 66.6725, 66.2325, 66.2325, 66.585, 66.585, 67.125, 67.125, 67.2125, 67.2125, 67.1, 67.1, 67.115, 67.115, 66.88, 66.88, 66.785, 66.785, 66.8125, 66.8125, 66.665, 66.665, 66.47, 66.47, 66.5625, 66.5625, 66.5275, 66.5275, 66.1475, 66.1475, 66.235, 66.235, 66.0875, 66.0875, 66.62, 66.62, 66.3925, 66.3925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2050
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0970
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7130
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0655
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5855
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4625
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0510
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2475
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6810
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4295
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.526, Test loss: 1.999, Test accuracy: 23.17
Round   0, Global train loss: 1.526, Global test loss: 2.278, Global test accuracy: 15.21
Round   1, Train loss: 1.205, Test loss: 1.530, Test accuracy: 45.02
Round   1, Global train loss: 1.205, Global test loss: 2.097, Global test accuracy: 29.57
Round   2, Train loss: 1.145, Test loss: 1.433, Test accuracy: 47.96
Round   2, Global train loss: 1.145, Global test loss: 2.127, Global test accuracy: 23.80
Round   3, Train loss: 1.121, Test loss: 1.321, Test accuracy: 50.09
Round   3, Global train loss: 1.121, Global test loss: 2.035, Global test accuracy: 25.57
Round   4, Train loss: 1.101, Test loss: 1.156, Test accuracy: 57.95
Round   4, Global train loss: 1.101, Global test loss: 2.089, Global test accuracy: 25.07
Round   5, Train loss: 1.296, Test loss: 0.958, Test accuracy: 67.84
Round   5, Global train loss: 1.296, Global test loss: 1.873, Global test accuracy: 36.37
Round   6, Train loss: 1.066, Test loss: 0.874, Test accuracy: 69.92
Round   6, Global train loss: 1.066, Global test loss: 2.141, Global test accuracy: 29.27
Round   7, Train loss: 0.864, Test loss: 0.882, Test accuracy: 69.30
Round   7, Global train loss: 0.864, Global test loss: 1.827, Global test accuracy: 35.37
Round   8, Train loss: 0.962, Test loss: 0.832, Test accuracy: 70.25
Round   8, Global train loss: 0.962, Global test loss: 1.755, Global test accuracy: 35.70
Round   9, Train loss: 1.021, Test loss: 0.849, Test accuracy: 70.29
Round   9, Global train loss: 1.021, Global test loss: 1.724, Global test accuracy: 38.92
Round  10, Train loss: 1.039, Test loss: 0.822, Test accuracy: 71.54
Round  10, Global train loss: 1.039, Global test loss: 1.769, Global test accuracy: 34.29
Round  11, Train loss: 1.101, Test loss: 0.831, Test accuracy: 71.53
Round  11, Global train loss: 1.101, Global test loss: 1.784, Global test accuracy: 34.91
Round  12, Train loss: 0.999, Test loss: 0.834, Test accuracy: 71.38
Round  12, Global train loss: 0.999, Global test loss: 1.644, Global test accuracy: 43.55
Round  13, Train loss: 0.961, Test loss: 0.817, Test accuracy: 72.58
Round  13, Global train loss: 0.961, Global test loss: 1.768, Global test accuracy: 38.68
Round  14, Train loss: 0.942, Test loss: 0.821, Test accuracy: 72.92
Round  14, Global train loss: 0.942, Global test loss: 1.758, Global test accuracy: 39.64
Round  15, Train loss: 1.008, Test loss: 0.817, Test accuracy: 73.33
Round  15, Global train loss: 1.008, Global test loss: 1.616, Global test accuracy: 43.02
Round  16, Train loss: 1.230, Test loss: 0.786, Test accuracy: 74.22
Round  16, Global train loss: 1.230, Global test loss: 1.685, Global test accuracy: 43.63
Round  17, Train loss: 0.795, Test loss: 0.761, Test accuracy: 74.72
Round  17, Global train loss: 0.795, Global test loss: 1.678, Global test accuracy: 43.74
Round  18, Train loss: 0.894, Test loss: 0.781, Test accuracy: 74.23
Round  18, Global train loss: 0.894, Global test loss: 1.570, Global test accuracy: 44.72
Round  19, Train loss: 0.946, Test loss: 0.794, Test accuracy: 74.17
Round  19, Global train loss: 0.946, Global test loss: 1.702, Global test accuracy: 44.12
Round  20, Train loss: 0.853, Test loss: 0.751, Test accuracy: 75.86
Round  20, Global train loss: 0.853, Global test loss: 1.684, Global test accuracy: 41.71
Round  21, Train loss: 0.978, Test loss: 0.748, Test accuracy: 76.03
Round  21, Global train loss: 0.978, Global test loss: 1.546, Global test accuracy: 49.23
Round  22, Train loss: 0.816, Test loss: 0.738, Test accuracy: 76.87
Round  22, Global train loss: 0.816, Global test loss: 1.561, Global test accuracy: 48.35
Round  23, Train loss: 1.059, Test loss: 0.707, Test accuracy: 78.10
Round  23, Global train loss: 1.059, Global test loss: 1.532, Global test accuracy: 51.06
Round  24, Train loss: 0.960, Test loss: 0.732, Test accuracy: 77.52
Round  24, Global train loss: 0.960, Global test loss: 1.537, Global test accuracy: 49.66
Round  25, Train loss: 0.992, Test loss: 0.720, Test accuracy: 77.61
Round  25, Global train loss: 0.992, Global test loss: 1.515, Global test accuracy: 52.17
Round  26, Train loss: 0.717, Test loss: 0.718, Test accuracy: 77.54
Round  26, Global train loss: 0.717, Global test loss: 1.569, Global test accuracy: 45.90
Round  27, Train loss: 0.964, Test loss: 0.701, Test accuracy: 77.76
Round  27, Global train loss: 0.964, Global test loss: 1.456, Global test accuracy: 54.00
Round  28, Train loss: 0.797, Test loss: 0.701, Test accuracy: 77.54
Round  28, Global train loss: 0.797, Global test loss: 1.480, Global test accuracy: 52.02
Round  29, Train loss: 1.059, Test loss: 0.715, Test accuracy: 77.21
Round  29, Global train loss: 1.059, Global test loss: 1.517, Global test accuracy: 49.23
Round  30, Train loss: 1.163, Test loss: 0.729, Test accuracy: 76.87
Round  30, Global train loss: 1.163, Global test loss: 1.539, Global test accuracy: 50.88
Round  31, Train loss: 0.897, Test loss: 0.729, Test accuracy: 77.38
Round  31, Global train loss: 0.897, Global test loss: 1.450, Global test accuracy: 54.02
Round  32, Train loss: 0.856, Test loss: 0.724, Test accuracy: 77.17
Round  32, Global train loss: 0.856, Global test loss: 1.643, Global test accuracy: 45.93
Round  33, Train loss: 1.089, Test loss: 0.736, Test accuracy: 76.71
Round  33, Global train loss: 1.089, Global test loss: 1.560, Global test accuracy: 50.73
Round  34, Train loss: 0.944, Test loss: 0.736, Test accuracy: 76.57
Round  34, Global train loss: 0.944, Global test loss: 1.564, Global test accuracy: 50.07
Round  35, Train loss: 0.782, Test loss: 0.721, Test accuracy: 77.37
Round  35, Global train loss: 0.782, Global test loss: 1.486, Global test accuracy: 51.08
Round  36, Train loss: 0.803, Test loss: 0.721, Test accuracy: 77.29
Round  36, Global train loss: 0.803, Global test loss: 1.416, Global test accuracy: 52.31
Round  37, Train loss: 0.778, Test loss: 0.720, Test accuracy: 77.15
Round  37, Global train loss: 0.778, Global test loss: 1.526, Global test accuracy: 47.08
Round  38, Train loss: 1.048, Test loss: 0.734, Test accuracy: 76.68
Round  38, Global train loss: 1.048, Global test loss: 1.469, Global test accuracy: 53.49
Round  39, Train loss: 1.002, Test loss: 0.722, Test accuracy: 77.01
Round  39, Global train loss: 1.002, Global test loss: 1.578, Global test accuracy: 50.52
Round  40, Train loss: 0.886, Test loss: 0.734, Test accuracy: 76.31
Round  40, Global train loss: 0.886, Global test loss: 1.459, Global test accuracy: 51.16
Round  41, Train loss: 0.937, Test loss: 0.706, Test accuracy: 77.12
Round  41, Global train loss: 0.937, Global test loss: 1.637, Global test accuracy: 48.68
Round  42, Train loss: 0.792, Test loss: 0.721, Test accuracy: 77.11
Round  42, Global train loss: 0.792, Global test loss: 1.682, Global test accuracy: 48.75
Round  43, Train loss: 0.808, Test loss: 0.735, Test accuracy: 76.45
Round  43, Global train loss: 0.808, Global test loss: 1.721, Global test accuracy: 46.13
Round  44, Train loss: 0.829, Test loss: 0.734, Test accuracy: 76.61
Round  44, Global train loss: 0.829, Global test loss: 1.571, Global test accuracy: 48.47
Round  45, Train loss: 0.648, Test loss: 0.736, Test accuracy: 76.67
Round  45, Global train loss: 0.648, Global test loss: 1.447, Global test accuracy: 52.30
Round  46, Train loss: 0.822, Test loss: 0.736, Test accuracy: 76.41
Round  46, Global train loss: 0.822, Global test loss: 1.484, Global test accuracy: 50.65
Round  47, Train loss: 0.832, Test loss: 0.717, Test accuracy: 76.88
Round  47, Global train loss: 0.832, Global test loss: 1.481, Global test accuracy: 50.02
Round  48, Train loss: 1.075, Test loss: 0.712, Test accuracy: 77.16
Round  48, Global train loss: 1.075, Global test loss: 1.526, Global test accuracy: 52.38
Round  49, Train loss: 0.852, Test loss: 0.727, Test accuracy: 76.09
Round  49, Global train loss: 0.852, Global test loss: 1.451, Global test accuracy: 53.00
Round  50, Train loss: 0.699, Test loss: 0.722, Test accuracy: 76.41
Round  50, Global train loss: 0.699, Global test loss: 1.470, Global test accuracy: 51.34
Round  51, Train loss: 0.912, Test loss: 0.732, Test accuracy: 75.92
Round  51, Global train loss: 0.912, Global test loss: 1.528, Global test accuracy: 51.59
Round  52, Train loss: 1.061, Test loss: 0.710, Test accuracy: 76.47
Round  52, Global train loss: 1.061, Global test loss: 1.603, Global test accuracy: 48.41
Round  53, Train loss: 0.946, Test loss: 0.700, Test accuracy: 77.04
Round  53, Global train loss: 0.946, Global test loss: 1.604, Global test accuracy: 48.54
Round  54, Train loss: 0.789, Test loss: 0.686, Test accuracy: 77.46
Round  54, Global train loss: 0.789, Global test loss: 1.654, Global test accuracy: 43.79
Round  55, Train loss: 0.784, Test loss: 0.683, Test accuracy: 77.67
Round  55, Global train loss: 0.784, Global test loss: 1.545, Global test accuracy: 50.27
Round  56, Train loss: 0.582, Test loss: 0.679, Test accuracy: 77.67
Round  56, Global train loss: 0.582, Global test loss: 1.448, Global test accuracy: 49.86
Round  57, Train loss: 0.765, Test loss: 0.692, Test accuracy: 77.26
Round  57, Global train loss: 0.765, Global test loss: 1.485, Global test accuracy: 50.15
Round  58, Train loss: 0.790, Test loss: 0.714, Test accuracy: 76.64
Round  58, Global train loss: 0.790, Global test loss: 1.672, Global test accuracy: 47.52
Round  59, Train loss: 0.703, Test loss: 0.715, Test accuracy: 76.67
Round  59, Global train loss: 0.703, Global test loss: 1.571, Global test accuracy: 47.68
Round  60, Train loss: 0.798, Test loss: 0.732, Test accuracy: 76.22
Round  60, Global train loss: 0.798, Global test loss: 1.462, Global test accuracy: 52.08
Round  61, Train loss: 0.747, Test loss: 0.734, Test accuracy: 76.22
Round  61, Global train loss: 0.747, Global test loss: 1.647, Global test accuracy: 47.58
Round  62, Train loss: 0.697, Test loss: 0.721, Test accuracy: 76.33
Round  62, Global train loss: 0.697, Global test loss: 1.528, Global test accuracy: 51.20
Round  63, Train loss: 0.626, Test loss: 0.726, Test accuracy: 75.93
Round  63, Global train loss: 0.626, Global test loss: 1.521, Global test accuracy: 52.21
Round  64, Train loss: 0.710, Test loss: 0.727, Test accuracy: 76.32
Round  64, Global train loss: 0.710, Global test loss: 1.671, Global test accuracy: 49.03
Round  65, Train loss: 0.622, Test loss: 0.759, Test accuracy: 75.58
Round  65, Global train loss: 0.622, Global test loss: 1.889, Global test accuracy: 45.45
Round  66, Train loss: 0.918, Test loss: 0.748, Test accuracy: 76.12
Round  66, Global train loss: 0.918, Global test loss: 1.685, Global test accuracy: 47.11
Round  67, Train loss: 0.895, Test loss: 0.743, Test accuracy: 76.05
Round  67, Global train loss: 0.895, Global test loss: 1.707, Global test accuracy: 46.24
Round  68, Train loss: 0.686, Test loss: 0.761, Test accuracy: 75.23
Round  68, Global train loss: 0.686, Global test loss: 1.689, Global test accuracy: 47.03
Round  69, Train loss: 0.572, Test loss: 0.776, Test accuracy: 74.80
Round  69, Global train loss: 0.572, Global test loss: 1.880, Global test accuracy: 43.40
Round  70, Train loss: 0.529, Test loss: 0.773, Test accuracy: 74.44
Round  70, Global train loss: 0.529, Global test loss: 1.526, Global test accuracy: 50.88
Round  71, Train loss: 0.943, Test loss: 0.794, Test accuracy: 74.47
Round  71, Global train loss: 0.943, Global test loss: 1.547, Global test accuracy: 49.02
Round  72, Train loss: 0.600, Test loss: 0.790, Test accuracy: 74.73
Round  72, Global train loss: 0.600, Global test loss: 1.556, Global test accuracy: 49.98
Round  73, Train loss: 0.637, Test loss: 0.790, Test accuracy: 75.02
Round  73, Global train loss: 0.637, Global test loss: 1.547, Global test accuracy: 49.27
Round  74, Train loss: 0.611, Test loss: 0.792, Test accuracy: 74.86
Round  74, Global train loss: 0.611, Global test loss: 1.954, Global test accuracy: 45.10
Round  75, Train loss: 0.684, Test loss: 0.822, Test accuracy: 73.78
Round  75, Global train loss: 0.684, Global test loss: 1.885, Global test accuracy: 45.88
Round  76, Train loss: 0.522, Test loss: 0.816, Test accuracy: 74.05
Round  76, Global train loss: 0.522, Global test loss: 1.647, Global test accuracy: 50.23
Round  77, Train loss: 0.510, Test loss: 0.782, Test accuracy: 74.71
Round  77, Global train loss: 0.510, Global test loss: 1.652, Global test accuracy: 49.05
Round  78, Train loss: 0.868, Test loss: 0.784, Test accuracy: 74.62
Round  78, Global train loss: 0.868, Global test loss: 1.537, Global test accuracy: 50.24
Round  79, Train loss: 0.675, Test loss: 0.763, Test accuracy: 75.33
Round  79, Global train loss: 0.675, Global test loss: 1.632, Global test accuracy: 49.06
Round  80, Train loss: 0.671, Test loss: 0.782, Test accuracy: 74.87
Round  80, Global train loss: 0.671, Global test loss: 1.680, Global test accuracy: 47.46
Round  81, Train loss: 0.610, Test loss: 0.769, Test accuracy: 75.42
Round  81, Global train loss: 0.610, Global test loss: 1.733, Global test accuracy: 46.59
Round  82, Train loss: 0.838, Test loss: 0.770, Test accuracy: 75.21
Round  82, Global train loss: 0.838, Global test loss: 1.621, Global test accuracy: 49.77
Round  83, Train loss: 0.612, Test loss: 0.776, Test accuracy: 75.03
Round  83, Global train loss: 0.612, Global test loss: 1.750, Global test accuracy: 44.17
Round  84, Train loss: 0.491, Test loss: 0.783, Test accuracy: 74.78
Round  84, Global train loss: 0.491, Global test loss: 1.786, Global test accuracy: 46.95
Round  85, Train loss: 0.694, Test loss: 0.780, Test accuracy: 75.26
Round  85, Global train loss: 0.694, Global test loss: 1.815, Global test accuracy: 44.92
Round  86, Train loss: 0.585, Test loss: 0.789, Test accuracy: 75.08
Round  86, Global train loss: 0.585, Global test loss: 1.587, Global test accuracy: 50.58
Round  87, Train loss: 0.605, Test loss: 0.798, Test accuracy: 74.77
Round  87, Global train loss: 0.605, Global test loss: 1.669, Global test accuracy: 50.53
Round  88, Train loss: 0.787, Test loss: 0.787, Test accuracy: 74.98
Round  88, Global train loss: 0.787, Global test loss: 1.574, Global test accuracy: 49.05
Round  89, Train loss: 0.747, Test loss: 0.779, Test accuracy: 75.77
Round  89, Global train loss: 0.747, Global test loss: 1.608, Global test accuracy: 51.27
Round  90, Train loss: 0.518, Test loss: 0.760, Test accuracy: 76.04
Round  90, Global train loss: 0.518, Global test loss: 1.664, Global test accuracy: 52.23
Round  91, Train loss: 0.562, Test loss: 0.787, Test accuracy: 75.34
Round  91, Global train loss: 0.562, Global test loss: 1.722, Global test accuracy: 50.33
Round  92, Train loss: 0.640, Test loss: 0.781, Test accuracy: 75.53
Round  92, Global train loss: 0.640, Global test loss: 1.583, Global test accuracy: 49.63
Round  93, Train loss: 0.535, Test loss: 0.812, Test accuracy: 74.76
Round  93, Global train loss: 0.535, Global test loss: 1.723, Global test accuracy: 45.84
Round  94, Train loss: 0.624, Test loss: 0.800, Test accuracy: 75.13
Round  94, Global train loss: 0.624, Global test loss: 1.610, Global test accuracy: 50.19
Round  95, Train loss: 0.524, Test loss: 0.798, Test accuracy: 75.35
Round  95, Global train loss: 0.524, Global test loss: 1.844, Global test accuracy: 45.28
Round  96, Train loss: 0.846, Test loss: 0.830, Test accuracy: 74.47
Round  96, Global train loss: 0.846, Global test loss: 1.648, Global test accuracy: 45.13
Round  97, Train loss: 0.747, Test loss: 0.840, Test accuracy: 74.08
Round  97, Global train loss: 0.747, Global test loss: 1.689, Global test accuracy: 47.15
Round  98, Train loss: 0.479, Test loss: 0.862, Test accuracy: 73.39
Round  98, Global train loss: 0.479, Global test loss: 1.736, Global test accuracy: 46.63
Round  99, Train loss: 0.513, Test loss: 0.850, Test accuracy: 73.99
Round  99, Global train loss: 0.513, Global test loss: 1.728, Global test accuracy: 48.20
Final Round, Train loss: 0.482, Test loss: 0.887, Test accuracy: 73.35
Final Round, Global train loss: 0.482, Global test loss: 1.728, Global test accuracy: 48.20
Average accuracy final 10 rounds: 74.8075 

Average global accuracy final 10 rounds: 48.06166666666667 

1983.1186141967773
[1.7000820636749268, 3.4001641273498535, 4.874224424362183, 6.348284721374512, 7.851323843002319, 9.354362964630127, 10.806211709976196, 12.258060455322266, 13.739437818527222, 15.220815181732178, 16.68017554283142, 18.139535903930664, 19.71677255630493, 21.2940092086792, 22.782806634902954, 24.27160406112671, 25.819613695144653, 27.367623329162598, 28.825631856918335, 30.283640384674072, 31.71799921989441, 33.152358055114746, 34.62630009651184, 36.100242137908936, 37.5456280708313, 38.99101400375366, 40.4116427898407, 41.832271575927734, 43.29815220832825, 44.76403284072876, 46.24089431762695, 47.71775579452515, 49.12373399734497, 50.529712200164795, 52.024030685424805, 53.518349170684814, 55.01017665863037, 56.50200414657593, 57.9564642906189, 59.410924434661865, 60.783461809158325, 62.155999183654785, 63.579667806625366, 65.00333642959595, 66.39837002754211, 67.79340362548828, 69.31123542785645, 70.82906723022461, 72.30706000328064, 73.78505277633667, 75.28916549682617, 76.79327821731567, 78.30881929397583, 79.82436037063599, 81.24066996574402, 82.65697956085205, 84.21351861953735, 85.77005767822266, 87.3569016456604, 88.94374561309814, 90.51711678504944, 92.09048795700073, 93.64706873893738, 95.20364952087402, 96.82584118843079, 98.44803285598755, 99.97089838981628, 101.49376392364502, 103.07580375671387, 104.65784358978271, 106.1696240901947, 107.68140459060669, 109.11067247390747, 110.53994035720825, 112.04511642456055, 113.55029249191284, 115.14687919616699, 116.74346590042114, 118.32044982910156, 119.89743375778198, 121.37421941757202, 122.85100507736206, 124.32406449317932, 125.79712390899658, 127.2689688205719, 128.74081373214722, 130.24087619781494, 131.74093866348267, 133.2721881866455, 134.80343770980835, 136.35873985290527, 137.9140419960022, 139.4263253211975, 140.93860864639282, 142.4039580821991, 143.86930751800537, 145.34638261795044, 146.8234577178955, 148.28244256973267, 149.74142742156982, 151.2124433517456, 152.6834592819214, 154.17595195770264, 155.6684446334839, 157.06365203857422, 158.45885944366455, 159.8622977733612, 161.26573610305786, 162.763822555542, 164.26190900802612, 165.73432159423828, 167.20673418045044, 168.68644380569458, 170.16615343093872, 171.64917731285095, 173.13220119476318, 174.62205481529236, 176.11190843582153, 177.58373618125916, 179.05556392669678, 180.53617572784424, 182.0167875289917, 183.47378039360046, 184.93077325820923, 186.4079306125641, 187.88508796691895, 189.52595496177673, 191.16682195663452, 192.81140327453613, 194.45598459243774, 196.05143570899963, 197.64688682556152, 199.28366613388062, 200.9204454421997, 202.49381709098816, 204.0671887397766, 205.54368948936462, 207.02019023895264, 208.45513224601746, 209.89007425308228, 211.36047840118408, 212.8308825492859, 214.24940824508667, 215.66793394088745, 217.05657744407654, 218.44522094726562, 219.92036199569702, 221.39550304412842, 222.8256130218506, 224.25572299957275, 225.72456789016724, 227.19341278076172, 228.70684933662415, 230.22028589248657, 231.71119737625122, 233.20210886001587, 234.79919290542603, 236.39627695083618, 237.9983265399933, 239.6003761291504, 241.14520072937012, 242.69002532958984, 244.24544835090637, 245.8008713722229, 247.30699944496155, 248.8131275177002, 250.32176184654236, 251.83039617538452, 253.35873675346375, 254.88707733154297, 256.33685636520386, 257.78663539886475, 259.2883005142212, 260.78996562957764, 262.2727961540222, 263.7556266784668, 265.25115752220154, 266.7466883659363, 268.33061599731445, 269.9145436286926, 271.50070881843567, 273.0868740081787, 274.64442801475525, 276.2019820213318, 277.8220405578613, 279.44209909439087, 280.9166986942291, 282.3912982940674, 283.8524570465088, 285.3136157989502, 286.7658669948578, 288.2181181907654, 289.6737742424011, 291.12943029403687, 292.5138957500458, 293.8983612060547, 295.3669002056122, 296.8354392051697, 298.279230594635, 299.72302198410034, 302.1173324584961, 304.51164293289185]
[23.166666666666668, 23.166666666666668, 45.025, 45.025, 47.958333333333336, 47.958333333333336, 50.09166666666667, 50.09166666666667, 57.95, 57.95, 67.84166666666667, 67.84166666666667, 69.925, 69.925, 69.3, 69.3, 70.25, 70.25, 70.29166666666667, 70.29166666666667, 71.54166666666667, 71.54166666666667, 71.53333333333333, 71.53333333333333, 71.38333333333334, 71.38333333333334, 72.58333333333333, 72.58333333333333, 72.925, 72.925, 73.33333333333333, 73.33333333333333, 74.21666666666667, 74.21666666666667, 74.725, 74.725, 74.23333333333333, 74.23333333333333, 74.175, 74.175, 75.85833333333333, 75.85833333333333, 76.025, 76.025, 76.86666666666666, 76.86666666666666, 78.1, 78.1, 77.51666666666667, 77.51666666666667, 77.60833333333333, 77.60833333333333, 77.54166666666667, 77.54166666666667, 77.75833333333334, 77.75833333333334, 77.54166666666667, 77.54166666666667, 77.20833333333333, 77.20833333333333, 76.86666666666666, 76.86666666666666, 77.375, 77.375, 77.16666666666667, 77.16666666666667, 76.70833333333333, 76.70833333333333, 76.56666666666666, 76.56666666666666, 77.36666666666666, 77.36666666666666, 77.29166666666667, 77.29166666666667, 77.15, 77.15, 76.68333333333334, 76.68333333333334, 77.00833333333334, 77.00833333333334, 76.30833333333334, 76.30833333333334, 77.125, 77.125, 77.10833333333333, 77.10833333333333, 76.45, 76.45, 76.60833333333333, 76.60833333333333, 76.66666666666667, 76.66666666666667, 76.40833333333333, 76.40833333333333, 76.88333333333334, 76.88333333333334, 77.15833333333333, 77.15833333333333, 76.09166666666667, 76.09166666666667, 76.40833333333333, 76.40833333333333, 75.91666666666667, 75.91666666666667, 76.475, 76.475, 77.04166666666667, 77.04166666666667, 77.45833333333333, 77.45833333333333, 77.675, 77.675, 77.675, 77.675, 77.25833333333334, 77.25833333333334, 76.64166666666667, 76.64166666666667, 76.66666666666667, 76.66666666666667, 76.225, 76.225, 76.225, 76.225, 76.33333333333333, 76.33333333333333, 75.93333333333334, 75.93333333333334, 76.31666666666666, 76.31666666666666, 75.58333333333333, 75.58333333333333, 76.11666666666666, 76.11666666666666, 76.05, 76.05, 75.23333333333333, 75.23333333333333, 74.8, 74.8, 74.44166666666666, 74.44166666666666, 74.46666666666667, 74.46666666666667, 74.73333333333333, 74.73333333333333, 75.01666666666667, 75.01666666666667, 74.85833333333333, 74.85833333333333, 73.78333333333333, 73.78333333333333, 74.05, 74.05, 74.70833333333333, 74.70833333333333, 74.625, 74.625, 75.33333333333333, 75.33333333333333, 74.86666666666666, 74.86666666666666, 75.425, 75.425, 75.20833333333333, 75.20833333333333, 75.025, 75.025, 74.78333333333333, 74.78333333333333, 75.25833333333334, 75.25833333333334, 75.075, 75.075, 74.76666666666667, 74.76666666666667, 74.98333333333333, 74.98333333333333, 75.76666666666667, 75.76666666666667, 76.04166666666667, 76.04166666666667, 75.34166666666667, 75.34166666666667, 75.525, 75.525, 74.75833333333334, 74.75833333333334, 75.13333333333334, 75.13333333333334, 75.35, 75.35, 74.46666666666667, 74.46666666666667, 74.075, 74.075, 73.39166666666667, 73.39166666666667, 73.99166666666666, 73.99166666666666, 73.35, 73.35]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2065
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0805
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7315
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0725
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5925
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4270
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0050
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2515
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6915
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4240
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.669, Test loss: 2.170, Test accuracy: 26.63
Round   1, Train loss: 1.254, Test loss: 1.820, Test accuracy: 29.05
Round   2, Train loss: 1.137, Test loss: 1.758, Test accuracy: 31.15
Round   3, Train loss: 1.247, Test loss: 1.616, Test accuracy: 39.08
Round   4, Train loss: 1.146, Test loss: 1.330, Test accuracy: 48.58
Round   5, Train loss: 1.144, Test loss: 1.079, Test accuracy: 51.63
Round   6, Train loss: 1.176, Test loss: 1.039, Test accuracy: 54.27
Round   7, Train loss: 1.121, Test loss: 1.039, Test accuracy: 55.37
Round   8, Train loss: 1.151, Test loss: 1.011, Test accuracy: 57.17
Round   9, Train loss: 1.179, Test loss: 1.025, Test accuracy: 52.85
Round  10, Train loss: 1.087, Test loss: 0.965, Test accuracy: 57.12
Round  11, Train loss: 1.004, Test loss: 0.972, Test accuracy: 56.56
Round  12, Train loss: 1.206, Test loss: 0.968, Test accuracy: 57.67
Round  13, Train loss: 1.115, Test loss: 0.980, Test accuracy: 58.51
Round  14, Train loss: 1.089, Test loss: 0.972, Test accuracy: 59.54
Round  15, Train loss: 1.171, Test loss: 0.965, Test accuracy: 61.40
Round  16, Train loss: 1.107, Test loss: 0.958, Test accuracy: 61.23
Round  17, Train loss: 1.002, Test loss: 0.916, Test accuracy: 63.75
Round  18, Train loss: 1.126, Test loss: 0.924, Test accuracy: 63.17
Round  19, Train loss: 1.093, Test loss: 0.914, Test accuracy: 61.93
Round  20, Train loss: 1.005, Test loss: 0.913, Test accuracy: 63.23
Round  21, Train loss: 1.016, Test loss: 0.917, Test accuracy: 62.88
Round  22, Train loss: 1.020, Test loss: 0.922, Test accuracy: 61.74
Round  23, Train loss: 0.911, Test loss: 0.902, Test accuracy: 62.74
Round  24, Train loss: 1.130, Test loss: 0.905, Test accuracy: 62.72
Round  25, Train loss: 1.159, Test loss: 0.911, Test accuracy: 62.08
Round  26, Train loss: 0.926, Test loss: 0.919, Test accuracy: 62.26
Round  27, Train loss: 0.844, Test loss: 0.887, Test accuracy: 62.28
Round  28, Train loss: 0.920, Test loss: 0.885, Test accuracy: 62.20
Round  29, Train loss: 1.220, Test loss: 0.874, Test accuracy: 65.48
Round  30, Train loss: 1.106, Test loss: 0.880, Test accuracy: 62.79
Round  31, Train loss: 1.094, Test loss: 0.884, Test accuracy: 62.58
Round  32, Train loss: 1.028, Test loss: 0.901, Test accuracy: 62.91
Round  33, Train loss: 0.799, Test loss: 0.895, Test accuracy: 62.67
Round  34, Train loss: 0.850, Test loss: 0.880, Test accuracy: 62.87
Round  35, Train loss: 1.018, Test loss: 0.870, Test accuracy: 65.12
Round  36, Train loss: 0.907, Test loss: 0.877, Test accuracy: 64.25
Round  37, Train loss: 1.105, Test loss: 0.860, Test accuracy: 63.59
Round  38, Train loss: 0.947, Test loss: 0.862, Test accuracy: 63.60
Round  39, Train loss: 0.988, Test loss: 0.863, Test accuracy: 64.32
Round  40, Train loss: 0.941, Test loss: 0.863, Test accuracy: 64.33
Round  41, Train loss: 1.033, Test loss: 0.866, Test accuracy: 64.23
Round  42, Train loss: 1.018, Test loss: 0.872, Test accuracy: 64.43
Round  43, Train loss: 0.975, Test loss: 0.871, Test accuracy: 63.33
Round  44, Train loss: 0.998, Test loss: 0.869, Test accuracy: 63.29
Round  45, Train loss: 1.025, Test loss: 0.859, Test accuracy: 63.48
Round  46, Train loss: 1.195, Test loss: 0.867, Test accuracy: 61.88
Round  47, Train loss: 0.982, Test loss: 0.869, Test accuracy: 63.24
Round  48, Train loss: 1.145, Test loss: 0.873, Test accuracy: 64.53
Round  49, Train loss: 0.740, Test loss: 0.864, Test accuracy: 65.72
Round  50, Train loss: 1.011, Test loss: 0.844, Test accuracy: 65.43
Round  51, Train loss: 1.028, Test loss: 0.851, Test accuracy: 65.04
Round  52, Train loss: 1.008, Test loss: 0.846, Test accuracy: 65.18
Round  53, Train loss: 0.862, Test loss: 0.840, Test accuracy: 64.72
Round  54, Train loss: 1.075, Test loss: 0.843, Test accuracy: 64.38
Round  55, Train loss: 1.094, Test loss: 0.847, Test accuracy: 64.46
Round  56, Train loss: 1.059, Test loss: 0.858, Test accuracy: 63.17
Round  57, Train loss: 1.219, Test loss: 0.860, Test accuracy: 62.79
Round  58, Train loss: 0.859, Test loss: 0.858, Test accuracy: 63.93
Round  59, Train loss: 1.090, Test loss: 0.861, Test accuracy: 63.11
Round  60, Train loss: 0.902, Test loss: 0.851, Test accuracy: 63.89
Round  61, Train loss: 1.045, Test loss: 0.853, Test accuracy: 64.02
Round  62, Train loss: 0.955, Test loss: 0.849, Test accuracy: 63.27
Round  63, Train loss: 1.067, Test loss: 0.857, Test accuracy: 62.46
Round  64, Train loss: 1.042, Test loss: 0.850, Test accuracy: 63.55
Round  65, Train loss: 0.877, Test loss: 0.845, Test accuracy: 63.53
Round  66, Train loss: 1.049, Test loss: 0.850, Test accuracy: 62.93
Round  67, Train loss: 1.046, Test loss: 0.849, Test accuracy: 63.27
Round  68, Train loss: 0.976, Test loss: 0.843, Test accuracy: 64.56
Round  69, Train loss: 0.864, Test loss: 0.853, Test accuracy: 63.74
Round  70, Train loss: 0.877, Test loss: 0.848, Test accuracy: 64.17
Round  71, Train loss: 0.970, Test loss: 0.846, Test accuracy: 64.27
Round  72, Train loss: 0.739, Test loss: 0.843, Test accuracy: 64.79
Round  73, Train loss: 1.075, Test loss: 0.852, Test accuracy: 64.10
Round  74, Train loss: 0.821, Test loss: 0.846, Test accuracy: 63.65
Round  75, Train loss: 1.066, Test loss: 0.855, Test accuracy: 63.38
Round  76, Train loss: 0.719, Test loss: 0.851, Test accuracy: 63.30
Round  77, Train loss: 0.950, Test loss: 0.848, Test accuracy: 63.41
Round  78, Train loss: 0.917, Test loss: 0.836, Test accuracy: 64.37
Round  79, Train loss: 0.733, Test loss: 0.836, Test accuracy: 64.21
Round  80, Train loss: 0.844, Test loss: 0.849, Test accuracy: 63.51
Round  81, Train loss: 0.915, Test loss: 0.859, Test accuracy: 62.79
Round  82, Train loss: 0.702, Test loss: 0.852, Test accuracy: 62.79
Round  83, Train loss: 1.061, Test loss: 0.856, Test accuracy: 61.83
Round  84, Train loss: 0.677, Test loss: 0.864, Test accuracy: 60.98
Round  85, Train loss: 0.966, Test loss: 0.856, Test accuracy: 61.65
Round  86, Train loss: 0.966, Test loss: 0.841, Test accuracy: 62.60
Round  87, Train loss: 0.887, Test loss: 0.858, Test accuracy: 62.70
Round  88, Train loss: 0.918, Test loss: 0.859, Test accuracy: 62.37
Round  89, Train loss: 0.904, Test loss: 0.850, Test accuracy: 62.25
Round  90, Train loss: 0.925, Test loss: 0.845, Test accuracy: 62.99
Round  91, Train loss: 0.771, Test loss: 0.841, Test accuracy: 63.33
Round  92, Train loss: 0.931, Test loss: 0.844, Test accuracy: 62.62
Round  93, Train loss: 0.814, Test loss: 0.838, Test accuracy: 62.74
Round  94, Train loss: 0.975, Test loss: 0.850, Test accuracy: 62.77
Round  95, Train loss: 0.846, Test loss: 0.860, Test accuracy: 61.35
Round  96, Train loss: 0.988, Test loss: 0.857, Test accuracy: 61.77
Round  97, Train loss: 0.970, Test loss: 0.857, Test accuracy: 61.59
Round  98, Train loss: 0.802, Test loss: 0.867, Test accuracy: 61.55
Round  99, Train loss: 0.884, Test loss: 0.857, Test accuracy: 62.11
Final Round, Train loss: 0.870, Test loss: 0.861, Test accuracy: 61.58
Average accuracy final 10 rounds: 62.2825
1469.765988111496
[2.1263887882232666, 3.9494333267211914, 5.758362054824829, 7.569533824920654, 9.38998007774353, 11.216404914855957, 13.033430337905884, 14.856091976165771, 16.694272994995117, 18.52254319190979, 20.365580320358276, 22.10478663444519, 23.945026397705078, 25.76553249359131, 27.566023588180542, 29.28229856491089, 31.02794075012207, 32.81117296218872, 34.62804365158081, 36.46499729156494, 38.383487939834595, 40.21195411682129, 41.874106645584106, 43.69341468811035, 45.52663278579712, 47.37388610839844, 49.177730560302734, 51.016629457473755, 52.89505314826965, 54.75877332687378, 56.563066720962524, 58.412761211395264, 60.296003580093384, 62.14671611785889, 63.98432159423828, 65.89986658096313, 67.84185194969177, 69.7048110961914, 71.52444052696228, 73.37465119361877, 75.23781728744507, 77.07411527633667, 78.86462593078613, 80.73869705200195, 82.66468501091003, 84.56682801246643, 86.43806743621826, 88.15225005149841, 89.90475463867188, 91.67212510108948, 93.43721914291382, 95.3276698589325, 97.15158557891846, 98.95444536209106, 100.79759764671326, 102.58385920524597, 104.40933918952942, 106.21643376350403, 108.06692051887512, 109.95899081230164, 111.76760506629944, 113.56255316734314, 115.42598700523376, 117.33648610115051, 119.17101716995239, 121.0332088470459, 122.78788757324219, 124.55405879020691, 126.37434315681458, 128.21458387374878, 129.994975566864, 131.84079718589783, 133.74670505523682, 135.58173036575317, 137.44920253753662, 139.38074111938477, 141.29437565803528, 143.00584840774536, 144.804461479187, 146.67850995063782, 148.6235203742981, 150.54608869552612, 152.41613745689392, 154.3093295097351, 156.1895158290863, 158.1116304397583, 159.99422693252563, 161.8386299610138, 163.70386028289795, 165.43539929389954, 167.2652940750122, 168.9071409702301, 170.65677738189697, 172.47791028022766, 174.2968294620514, 176.02650117874146, 177.64798998832703, 179.3405454158783, 181.20126032829285, 182.7989809513092, 185.07813477516174]
[26.633333333333333, 29.05, 31.15, 39.075, 48.575, 51.63333333333333, 54.275, 55.36666666666667, 57.175, 52.85, 57.125, 56.55833333333333, 57.675, 58.50833333333333, 59.541666666666664, 61.4, 61.225, 63.75, 63.166666666666664, 61.93333333333333, 63.233333333333334, 62.88333333333333, 61.74166666666667, 62.74166666666667, 62.71666666666667, 62.075, 62.25833333333333, 62.28333333333333, 62.2, 65.48333333333333, 62.791666666666664, 62.575, 62.90833333333333, 62.675, 62.86666666666667, 65.125, 64.25, 63.59166666666667, 63.6, 64.31666666666666, 64.33333333333333, 64.23333333333333, 64.43333333333334, 63.333333333333336, 63.291666666666664, 63.475, 61.88333333333333, 63.24166666666667, 64.525, 65.71666666666667, 65.43333333333334, 65.04166666666667, 65.18333333333334, 64.725, 64.38333333333334, 64.45833333333333, 63.175, 62.791666666666664, 63.93333333333333, 63.108333333333334, 63.891666666666666, 64.01666666666667, 63.275, 62.458333333333336, 63.55, 63.53333333333333, 62.93333333333333, 63.266666666666666, 64.55833333333334, 63.74166666666667, 64.16666666666667, 64.26666666666667, 64.79166666666667, 64.1, 63.65, 63.38333333333333, 63.3, 63.40833333333333, 64.36666666666666, 64.20833333333333, 63.50833333333333, 62.791666666666664, 62.791666666666664, 61.825, 60.983333333333334, 61.65, 62.6, 62.7, 62.36666666666667, 62.25, 62.99166666666667, 63.325, 62.625, 62.74166666666667, 62.775, 61.35, 61.766666666666666, 61.59166666666667, 61.55, 62.108333333333334, 61.583333333333336]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2090
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0755
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7225
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1555
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5695
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4285
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0685
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3095
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6605
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3245
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  27.3300
Round 1 global test acc  37.9100
Round 2 global test acc  43.0300
Round 3 global test acc  47.7000
Round 4 global test acc  48.0200
Round 5 global test acc  48.3800
Round 6 global test acc  49.5300
Round 7 global test acc  50.1100
Round 8 global test acc  50.9100
Round 9 global test acc  52.2600
Round 10 global test acc  53.0900
Round 11 global test acc  54.8400
Round 12 global test acc  55.9100
Round 13 global test acc  56.5900
Round 14 global test acc  55.4400
Round 15 global test acc  55.9100
Round 16 global test acc  57.2900
Round 17 global test acc  57.6900
Round 18 global test acc  58.2300
Round 19 global test acc  57.1500
Round 20 global test acc  58.2500
Round 21 global test acc  56.6400
Round 22 global test acc  59.1300
Round 23 global test acc  58.0500
Round 24 global test acc  59.0400
Round 25 global test acc  60.3600
Round 26 global test acc  61.0400
Round 27 global test acc  59.8400
Round 28 global test acc  61.3700
Round 29 global test acc  59.4700
Round 30 global test acc  60.9800
Round 31 global test acc  61.1900
Round 32 global test acc  61.4900
Round 33 global test acc  61.3300
Round 34 global test acc  62.3600
Round 35 global test acc  62.0800
Round 36 global test acc  62.1800
Round 37 global test acc  62.2000
Round 38 global test acc  61.8200
Round 39 global test acc  61.5400
Round 40 global test acc  62.1100
Round 41 global test acc  63.9200
Round 42 global test acc  61.8900
Round 43 global test acc  62.5500
Round 44 global test acc  61.6000
Round 45 global test acc  63.5900
Round 46 global test acc  63.0800
Round 47 global test acc  63.3100
Round 48 global test acc  61.8900
Round 49 global test acc  63.9400
Round 50 global test acc  64.2500
Round 51 global test acc  63.2900
Round 52 global test acc  64.2700
Round 53 global test acc  63.0200
Round 54 global test acc  64.5100
Round 55 global test acc  64.7800
Round 56 global test acc  63.8700
Round 57 global test acc  63.7800
Round 58 global test acc  64.5500
Round 59 global test acc  64.7000
Round 60 global test acc  65.0400
Round 61 global test acc  64.3400
Round 62 global test acc  62.9100
Round 63 global test acc  64.0000
Round 64 global test acc  64.7000
Round 65 global test acc  64.1500
Round 66 global test acc  65.6800
Round 67 global test acc  64.6500
Round 68 global test acc  64.9600
Round 69 global test acc  63.3900
Round 70 global test acc  64.2900
Round 71 global test acc  63.0700
Round 72 global test acc  65.5700
Round 73 global test acc  65.1100
Round 74 global test acc  65.2400
Round 75 global test acc  63.7200
Round 76 global test acc  64.8300
Round 77 global test acc  65.9900
Round 78 global test acc  65.9100
Round 79 global test acc  66.6800
Round 80 global test acc  65.5400
Round 81 global test acc  63.6300
Round 82 global test acc  62.7700
Round 83 global test acc  62.7600
Round 84 global test acc  60.8400
Round 85 global test acc  60.7700
Round 86 global test acc  60.0500
Round 87 global test acc  60.1800
Round 88 global test acc  59.8800
Round 89 global test acc  59.7500
Round 90 global test acc  59.3300
Round 91 global test acc  59.0200
Round 92 global test acc  58.6900
Round 93 global test acc  59.0200
Round 94 global test acc  59.1800
Round 95 global test acc  59.0400
Round 96 global test acc  58.6000
Round 97 global test acc  58.8000
Round 98 global test acc  58.1600
Round 99 global test acc  58.7200
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2085
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0550
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7390
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.2205
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5870
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4510
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1895
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2260
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6450
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3900
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.582, Test loss: 1.904, Test accuracy: 27.79
Round   1, Train loss: 1.361, Test loss: 1.589, Test accuracy: 41.62
Round   2, Train loss: 1.328, Test loss: 1.310, Test accuracy: 54.51
Round   3, Train loss: 1.251, Test loss: 1.312, Test accuracy: 56.08
Round   4, Train loss: 1.158, Test loss: 1.183, Test accuracy: 59.60
Round   5, Train loss: 1.204, Test loss: 1.128, Test accuracy: 61.59
Round   6, Train loss: 1.113, Test loss: 1.097, Test accuracy: 63.11
Round   7, Train loss: 1.153, Test loss: 1.014, Test accuracy: 69.45
Round   8, Train loss: 1.195, Test loss: 0.983, Test accuracy: 70.72
Round   9, Train loss: 1.208, Test loss: 0.928, Test accuracy: 70.26
Round  10, Train loss: 1.169, Test loss: 0.927, Test accuracy: 71.20
Round  11, Train loss: 1.133, Test loss: 0.855, Test accuracy: 75.50
Round  12, Train loss: 0.995, Test loss: 0.813, Test accuracy: 76.57
Round  13, Train loss: 1.129, Test loss: 0.777, Test accuracy: 77.21
Round  14, Train loss: 1.064, Test loss: 0.664, Test accuracy: 80.00
Round  15, Train loss: 1.018, Test loss: 0.642, Test accuracy: 81.42
Round  16, Train loss: 1.012, Test loss: 0.649, Test accuracy: 81.27
Round  17, Train loss: 1.053, Test loss: 0.640, Test accuracy: 81.55
Round  18, Train loss: 1.035, Test loss: 0.630, Test accuracy: 81.88
Round  19, Train loss: 0.973, Test loss: 0.596, Test accuracy: 82.94
Round  20, Train loss: 0.960, Test loss: 0.604, Test accuracy: 82.87
Round  21, Train loss: 0.950, Test loss: 0.601, Test accuracy: 83.15
Round  22, Train loss: 0.963, Test loss: 0.589, Test accuracy: 83.32
Round  23, Train loss: 0.918, Test loss: 0.590, Test accuracy: 83.40
Round  24, Train loss: 0.969, Test loss: 0.565, Test accuracy: 83.90
Round  25, Train loss: 0.985, Test loss: 0.570, Test accuracy: 84.31
Round  26, Train loss: 0.982, Test loss: 0.563, Test accuracy: 84.64
Round  27, Train loss: 1.053, Test loss: 0.569, Test accuracy: 84.15
Round  28, Train loss: 1.043, Test loss: 0.561, Test accuracy: 84.18
Round  29, Train loss: 0.971, Test loss: 0.541, Test accuracy: 84.53
Round  30, Train loss: 0.947, Test loss: 0.548, Test accuracy: 84.38
Round  31, Train loss: 0.862, Test loss: 0.529, Test accuracy: 84.83
Round  32, Train loss: 0.844, Test loss: 0.526, Test accuracy: 84.72
Round  33, Train loss: 0.965, Test loss: 0.533, Test accuracy: 84.95
Round  34, Train loss: 0.906, Test loss: 0.532, Test accuracy: 84.80
Round  35, Train loss: 0.939, Test loss: 0.541, Test accuracy: 84.97
Round  36, Train loss: 0.953, Test loss: 0.535, Test accuracy: 85.44
Round  37, Train loss: 1.000, Test loss: 0.544, Test accuracy: 85.04
Round  38, Train loss: 0.857, Test loss: 0.532, Test accuracy: 85.15
Round  39, Train loss: 0.942, Test loss: 0.522, Test accuracy: 85.27
Round  40, Train loss: 0.985, Test loss: 0.519, Test accuracy: 85.64
Round  41, Train loss: 0.897, Test loss: 0.518, Test accuracy: 85.68
Round  42, Train loss: 0.930, Test loss: 0.511, Test accuracy: 85.59
Round  43, Train loss: 0.900, Test loss: 0.504, Test accuracy: 85.99
Round  44, Train loss: 0.869, Test loss: 0.505, Test accuracy: 85.66
Round  45, Train loss: 0.947, Test loss: 0.508, Test accuracy: 85.75
Round  46, Train loss: 0.809, Test loss: 0.503, Test accuracy: 85.55
Round  47, Train loss: 0.902, Test loss: 0.499, Test accuracy: 85.89
Round  48, Train loss: 0.890, Test loss: 0.503, Test accuracy: 85.82
Round  49, Train loss: 0.870, Test loss: 0.494, Test accuracy: 85.65
Round  50, Train loss: 0.798, Test loss: 0.498, Test accuracy: 85.88
Round  51, Train loss: 0.913, Test loss: 0.495, Test accuracy: 85.84
Round  52, Train loss: 0.717, Test loss: 0.491, Test accuracy: 86.02
Round  53, Train loss: 0.955, Test loss: 0.495, Test accuracy: 86.16
Round  54, Train loss: 0.772, Test loss: 0.496, Test accuracy: 86.23
Round  55, Train loss: 0.790, Test loss: 0.494, Test accuracy: 85.99
Round  56, Train loss: 0.882, Test loss: 0.502, Test accuracy: 85.70
Round  57, Train loss: 0.826, Test loss: 0.487, Test accuracy: 86.18
Round  58, Train loss: 0.837, Test loss: 0.494, Test accuracy: 85.92
Round  59, Train loss: 0.725, Test loss: 0.478, Test accuracy: 86.41
Round  60, Train loss: 0.814, Test loss: 0.481, Test accuracy: 86.31
Round  61, Train loss: 0.911, Test loss: 0.483, Test accuracy: 86.70
Round  62, Train loss: 0.811, Test loss: 0.492, Test accuracy: 86.38
Round  63, Train loss: 0.907, Test loss: 0.497, Test accuracy: 86.28
Round  64, Train loss: 0.853, Test loss: 0.500, Test accuracy: 86.38
Round  65, Train loss: 0.861, Test loss: 0.491, Test accuracy: 86.74
Round  66, Train loss: 0.773, Test loss: 0.486, Test accuracy: 86.33
Round  67, Train loss: 0.755, Test loss: 0.494, Test accuracy: 86.18
Round  68, Train loss: 0.798, Test loss: 0.492, Test accuracy: 86.59
Round  69, Train loss: 0.854, Test loss: 0.505, Test accuracy: 86.32
Round  70, Train loss: 0.844, Test loss: 0.509, Test accuracy: 86.30
Round  71, Train loss: 0.715, Test loss: 0.498, Test accuracy: 86.08
Round  72, Train loss: 0.719, Test loss: 0.486, Test accuracy: 86.22
Round  73, Train loss: 0.915, Test loss: 0.485, Test accuracy: 86.38
Round  74, Train loss: 0.755, Test loss: 0.492, Test accuracy: 86.17
Round  75, Train loss: 0.848, Test loss: 0.499, Test accuracy: 85.85
Round  76, Train loss: 0.769, Test loss: 0.488, Test accuracy: 86.19
Round  77, Train loss: 0.833, Test loss: 0.483, Test accuracy: 86.12
Round  78, Train loss: 0.738, Test loss: 0.492, Test accuracy: 85.99
Round  79, Train loss: 0.743, Test loss: 0.489, Test accuracy: 86.35
Round  80, Train loss: 0.797, Test loss: 0.494, Test accuracy: 86.14
Round  81, Train loss: 0.803, Test loss: 0.484, Test accuracy: 86.41
Round  82, Train loss: 0.803, Test loss: 0.503, Test accuracy: 86.06
Round  83, Train loss: 0.771, Test loss: 0.499, Test accuracy: 86.02
Round  84, Train loss: 0.710, Test loss: 0.487, Test accuracy: 86.29
Round  85, Train loss: 0.846, Test loss: 0.499, Test accuracy: 85.92
Round  86, Train loss: 0.815, Test loss: 0.509, Test accuracy: 85.72
Round  87, Train loss: 0.846, Test loss: 0.504, Test accuracy: 85.99
Round  88, Train loss: 0.765, Test loss: 0.506, Test accuracy: 85.88
Round  89, Train loss: 0.677, Test loss: 0.511, Test accuracy: 85.69
Round  90, Train loss: 0.728, Test loss: 0.508, Test accuracy: 85.81
Round  91, Train loss: 0.745, Test loss: 0.507, Test accuracy: 85.62
Round  92, Train loss: 0.844, Test loss: 0.520, Test accuracy: 85.27
Round  93, Train loss: 0.780, Test loss: 0.512, Test accuracy: 85.74
Round  94, Train loss: 0.679, Test loss: 0.521, Test accuracy: 85.13
Round  95, Train loss: 0.732, Test loss: 0.520, Test accuracy: 85.08
Round  96, Train loss: 0.791, Test loss: 0.518, Test accuracy: 85.38
Round  97, Train loss: 0.761, Test loss: 0.517, Test accuracy: 85.24
Round  98, Train loss: 0.685, Test loss: 0.505, Test accuracy: 85.72
Round  99, Train loss: 0.765, Test loss: 0.510, Test accuracy: 85.44
Final Round, Train loss: 0.712, Test loss: 0.509, Test accuracy: 85.60
Average accuracy final 10 rounds: 85.44222222222221
4414.142669200897
[5.688763618469238, 11.16743278503418, 16.748286485671997, 22.473387241363525, 27.834421396255493, 33.24396014213562, 38.60872006416321, 44.01322054862976, 49.35547065734863, 54.78274893760681, 60.342495918273926, 65.71666479110718, 71.126149892807, 76.48130178451538, 81.76433753967285, 87.27657580375671, 92.73291563987732, 98.10271286964417, 103.61749792098999, 109.11933422088623, 114.56542491912842, 120.04039454460144, 125.26016616821289, 130.6805591583252, 136.2557396888733, 141.93572092056274, 147.45116901397705, 153.0192847251892, 158.43714499473572, 163.95903253555298, 169.44693422317505, 174.67766666412354, 180.31069493293762, 185.73740792274475, 191.21153807640076, 196.64603567123413, 202.05396819114685, 207.50804805755615, 212.8642406463623, 218.1927752494812, 223.96835827827454, 229.77215838432312, 235.23744940757751, 240.6742181777954, 246.00104689598083, 251.30933260917664, 256.81154465675354, 262.21789836883545, 267.50455951690674, 273.00017952919006, 278.3259325027466, 283.5711100101471, 288.9924249649048, 294.23120641708374, 299.6535565853119, 305.03862142562866, 310.398540019989, 315.88499426841736, 321.352735042572, 326.7539381980896, 332.08847284317017, 337.32985401153564, 342.69683361053467, 348.17000675201416, 354.0903830528259, 359.9595594406128, 365.5730950832367, 371.0575132369995, 376.7756757736206, 382.5476553440094, 388.0405685901642, 393.49368953704834, 398.97196793556213, 404.46357917785645, 409.8437867164612, 415.13915061950684, 420.3800823688507, 425.8339202404022, 431.27675437927246, 436.6718089580536, 442.14414715766907, 447.47111320495605, 452.7615089416504, 458.03336453437805, 463.4117224216461, 468.8197383880615, 474.17735147476196, 479.50333547592163, 484.9516713619232, 490.26692748069763, 495.5969877243042, 501.10330963134766, 506.39332461357117, 511.7363018989563, 517.0787842273712, 522.4693911075592, 528.1630220413208, 533.8274710178375, 539.3294975757599, 544.6342346668243, 546.9251198768616]
[27.794444444444444, 41.625, 54.51111111111111, 56.083333333333336, 59.6, 61.58611111111111, 63.111111111111114, 69.45277777777778, 70.72222222222223, 70.2611111111111, 71.19722222222222, 75.50277777777778, 76.56944444444444, 77.20833333333333, 80.00277777777778, 81.425, 81.26666666666667, 81.54722222222222, 81.88333333333334, 82.94166666666666, 82.86944444444444, 83.15277777777777, 83.31666666666666, 83.4, 83.89722222222223, 84.31111111111112, 84.64166666666667, 84.15277777777777, 84.17777777777778, 84.525, 84.38055555555556, 84.82777777777778, 84.72222222222223, 84.95277777777778, 84.80277777777778, 84.975, 85.43611111111112, 85.03888888888889, 85.15, 85.26666666666667, 85.64444444444445, 85.68055555555556, 85.58611111111111, 85.98611111111111, 85.65555555555555, 85.74722222222222, 85.55, 85.88888888888889, 85.81666666666666, 85.64722222222223, 85.88055555555556, 85.84166666666667, 86.02222222222223, 86.15833333333333, 86.23055555555555, 85.99166666666666, 85.69722222222222, 86.17777777777778, 85.92222222222222, 86.40555555555555, 86.30555555555556, 86.70277777777778, 86.38333333333334, 86.27777777777777, 86.375, 86.74444444444444, 86.33055555555555, 86.18333333333334, 86.59444444444445, 86.31944444444444, 86.29722222222222, 86.08333333333333, 86.21666666666667, 86.375, 86.16944444444445, 85.85277777777777, 86.19166666666666, 86.12222222222222, 85.99444444444444, 86.35, 86.1361111111111, 86.41111111111111, 86.06388888888888, 86.01666666666667, 86.28611111111111, 85.92222222222222, 85.71944444444445, 85.98611111111111, 85.875, 85.69444444444444, 85.81111111111112, 85.61944444444444, 85.26666666666667, 85.73611111111111, 85.13055555555556, 85.08055555555555, 85.37777777777778, 85.24166666666666, 85.71944444444445, 85.43888888888888, 85.60277777777777]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2095
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0580
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7245
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1595
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5965
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4690
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0405
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2280
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6525
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2990
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1977, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train_local):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53676 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5575
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4685
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8265
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4990
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7445
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.6850
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.4690
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6275
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7720
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7080
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.194, Test loss: 2.176, Test accuracy: 24.34
Round   0, Global train loss: 2.194, Global test loss: 2.192, Global test accuracy: 24.33
Round   1, Train loss: 2.085, Test loss: 2.091, Test accuracy: 32.19
Round   1, Global train loss: 2.085, Global test loss: 2.110, Global test accuracy: 35.37
Round   2, Train loss: 2.067, Test loss: 1.950, Test accuracy: 34.09
Round   2, Global train loss: 2.067, Global test loss: 1.896, Global test accuracy: 39.40
Round   3, Train loss: 2.086, Test loss: 1.999, Test accuracy: 34.46
Round   3, Global train loss: 2.086, Global test loss: 2.013, Global test accuracy: 41.13
Round   4, Train loss: 2.167, Test loss: 1.989, Test accuracy: 34.05
Round   4, Global train loss: 2.167, Global test loss: 2.020, Global test accuracy: 39.40
Round   5, Train loss: 2.003, Test loss: 1.948, Test accuracy: 34.49
Round   5, Global train loss: 2.003, Global test loss: 1.812, Global test accuracy: 46.55
Round   6, Train loss: 2.046, Test loss: 1.948, Test accuracy: 34.28
Round   6, Global train loss: 2.046, Global test loss: 1.879, Global test accuracy: 45.61
Round   7, Train loss: 1.992, Test loss: 1.943, Test accuracy: 34.47
Round   7, Global train loss: 1.992, Global test loss: 1.974, Global test accuracy: 42.27
Round   8, Train loss: 2.191, Test loss: 1.946, Test accuracy: 33.76
Round   8, Global train loss: 2.191, Global test loss: 2.125, Global test accuracy: 36.73
Round   9, Train loss: 1.759, Test loss: 1.915, Test accuracy: 34.77
Round   9, Global train loss: 1.759, Global test loss: 1.680, Global test accuracy: 48.38
Round  10, Train loss: 1.828, Test loss: 1.926, Test accuracy: 34.37
Round  10, Global train loss: 1.828, Global test loss: 1.901, Global test accuracy: 40.14
Round  11, Train loss: 1.882, Test loss: 1.925, Test accuracy: 35.08
Round  11, Global train loss: 1.882, Global test loss: 1.896, Global test accuracy: 40.92
Round  12, Train loss: 1.963, Test loss: 1.933, Test accuracy: 34.96
Round  12, Global train loss: 1.963, Global test loss: 1.965, Global test accuracy: 44.39
Round  13, Train loss: 1.930, Test loss: 1.904, Test accuracy: 35.34
Round  13, Global train loss: 1.930, Global test loss: 1.879, Global test accuracy: 42.13
Round  14, Train loss: 1.894, Test loss: 1.899, Test accuracy: 35.16
Round  14, Global train loss: 1.894, Global test loss: 1.977, Global test accuracy: 37.25
Round  15, Train loss: 1.515, Test loss: 1.894, Test accuracy: 35.25
Round  15, Global train loss: 1.515, Global test loss: 1.653, Global test accuracy: 50.28
Round  16, Train loss: 1.803, Test loss: 1.899, Test accuracy: 35.42
Round  16, Global train loss: 1.803, Global test loss: 1.866, Global test accuracy: 40.17
Round  17, Train loss: 1.800, Test loss: 1.902, Test accuracy: 35.39
Round  17, Global train loss: 1.800, Global test loss: 1.818, Global test accuracy: 45.60
Round  18, Train loss: 1.682, Test loss: 1.917, Test accuracy: 35.42
Round  18, Global train loss: 1.682, Global test loss: 1.849, Global test accuracy: 45.34
Round  19, Train loss: 1.651, Test loss: 1.906, Test accuracy: 36.06
Round  19, Global train loss: 1.651, Global test loss: 1.746, Global test accuracy: 48.56
Round  20, Train loss: 1.940, Test loss: 1.907, Test accuracy: 35.67
Round  20, Global train loss: 1.940, Global test loss: 1.924, Global test accuracy: 42.20
Round  21, Train loss: 1.910, Test loss: 1.900, Test accuracy: 36.13
Round  21, Global train loss: 1.910, Global test loss: 1.878, Global test accuracy: 44.39
Round  22, Train loss: 1.688, Test loss: 1.917, Test accuracy: 35.87
Round  22, Global train loss: 1.688, Global test loss: 1.862, Global test accuracy: 42.77
Round  23, Train loss: 1.660, Test loss: 1.944, Test accuracy: 35.79
Round  23, Global train loss: 1.660, Global test loss: 1.888, Global test accuracy: 39.10
Round  24, Train loss: 1.891, Test loss: 1.959, Test accuracy: 35.37
Round  24, Global train loss: 1.891, Global test loss: 2.009, Global test accuracy: 40.17
Round  25, Train loss: 1.717, Test loss: 1.970, Test accuracy: 34.63
Round  25, Global train loss: 1.717, Global test loss: 1.954, Global test accuracy: 36.51
Round  26, Train loss: 1.568, Test loss: 1.981, Test accuracy: 34.61
Round  26, Global train loss: 1.568, Global test loss: 1.770, Global test accuracy: 45.61
Round  27, Train loss: 1.705, Test loss: 2.007, Test accuracy: 34.31
Round  27, Global train loss: 1.705, Global test loss: 1.867, Global test accuracy: 43.65
Round  28, Train loss: 1.529, Test loss: 2.033, Test accuracy: 34.52
Round  28, Global train loss: 1.529, Global test loss: 1.794, Global test accuracy: 42.92
Round  29, Train loss: 1.458, Test loss: 2.071, Test accuracy: 33.93
Round  29, Global train loss: 1.458, Global test loss: 1.816, Global test accuracy: 44.88
Round  30, Train loss: 1.546, Test loss: 2.087, Test accuracy: 33.86
Round  30, Global train loss: 1.546, Global test loss: 1.811, Global test accuracy: 44.89
Round  31, Train loss: 1.436, Test loss: 2.097, Test accuracy: 33.95
Round  31, Global train loss: 1.436, Global test loss: 1.817, Global test accuracy: 43.99
Round  32, Train loss: 1.597, Test loss: 2.120, Test accuracy: 34.02
Round  32, Global train loss: 1.597, Global test loss: 2.115, Global test accuracy: 25.10
Round  33, Train loss: 1.246, Test loss: 2.132, Test accuracy: 34.05
Round  33, Global train loss: 1.246, Global test loss: 1.623, Global test accuracy: 47.04
Round  34, Train loss: 1.766, Test loss: 2.169, Test accuracy: 33.45
Round  34, Global train loss: 1.766, Global test loss: 2.069, Global test accuracy: 33.07
Round  35, Train loss: 1.355, Test loss: 2.211, Test accuracy: 32.84
Round  35, Global train loss: 1.355, Global test loss: 1.850, Global test accuracy: 43.06
Round  36, Train loss: 1.319, Test loss: 2.247, Test accuracy: 33.05
Round  36, Global train loss: 1.319, Global test loss: 1.884, Global test accuracy: 39.58
Round  37, Train loss: 1.446, Test loss: 2.269, Test accuracy: 33.05
Round  37, Global train loss: 1.446, Global test loss: 1.925, Global test accuracy: 39.66
Round  38, Train loss: 1.370, Test loss: 2.308, Test accuracy: 32.86
Round  38, Global train loss: 1.370, Global test loss: 1.991, Global test accuracy: 35.35
Round  39, Train loss: 1.222, Test loss: 2.327, Test accuracy: 32.36
Round  39, Global train loss: 1.222, Global test loss: 1.876, Global test accuracy: 40.43
Round  40, Train loss: 1.070, Test loss: 2.364, Test accuracy: 32.44
Round  40, Global train loss: 1.070, Global test loss: 1.706, Global test accuracy: 42.30
Round  41, Train loss: 1.540, Test loss: 2.381, Test accuracy: 32.16
Round  41, Global train loss: 1.540, Global test loss: 2.100, Global test accuracy: 27.64
Round  42, Train loss: 1.209, Test loss: 2.428, Test accuracy: 31.96
Round  42, Global train loss: 1.209, Global test loss: 2.046, Global test accuracy: 31.95
Round  43, Train loss: 0.984, Test loss: 2.470, Test accuracy: 31.48
Round  43, Global train loss: 0.984, Global test loss: 1.715, Global test accuracy: 43.50
Round  44, Train loss: 1.071, Test loss: 2.547, Test accuracy: 31.63
Round  44, Global train loss: 1.071, Global test loss: 2.008, Global test accuracy: 34.15
Round  45, Train loss: 1.147, Test loss: 2.558, Test accuracy: 31.84
Round  45, Global train loss: 1.147, Global test loss: 1.861, Global test accuracy: 39.81
Round  46, Train loss: 0.923, Test loss: 2.587, Test accuracy: 31.98
Round  46, Global train loss: 0.923, Global test loss: 1.735, Global test accuracy: 41.38
Round  47, Train loss: 1.158, Test loss: 2.628, Test accuracy: 31.70
Round  47, Global train loss: 1.158, Global test loss: 1.882, Global test accuracy: 39.87
Round  48, Train loss: 1.225, Test loss: 2.671, Test accuracy: 31.38
Round  48, Global train loss: 1.225, Global test loss: 1.940, Global test accuracy: 35.13
Round  49, Train loss: 0.931, Test loss: 2.697, Test accuracy: 31.04
Round  49, Global train loss: 0.931, Global test loss: 1.832, Global test accuracy: 41.78
Round  50, Train loss: 0.975, Test loss: 2.706, Test accuracy: 31.07
Round  50, Global train loss: 0.975, Global test loss: 2.100, Global test accuracy: 28.83
Round  51, Train loss: 0.970, Test loss: 2.770, Test accuracy: 30.69
Round  51, Global train loss: 0.970, Global test loss: 1.858, Global test accuracy: 40.16
Round  52, Train loss: 1.006, Test loss: 2.780, Test accuracy: 31.05
Round  52, Global train loss: 1.006, Global test loss: 1.938, Global test accuracy: 35.53
Round  53, Train loss: 0.989, Test loss: 2.827, Test accuracy: 31.21
Round  53, Global train loss: 0.989, Global test loss: 1.931, Global test accuracy: 30.88
Round  54, Train loss: 1.011, Test loss: 2.872, Test accuracy: 31.26
Round  54, Global train loss: 1.011, Global test loss: 1.693, Global test accuracy: 43.77
Round  55, Train loss: 0.807, Test loss: 2.902, Test accuracy: 31.12
Round  55, Global train loss: 0.807, Global test loss: 1.678, Global test accuracy: 44.41
Round  56, Train loss: 0.842, Test loss: 2.908, Test accuracy: 31.10
Round  56, Global train loss: 0.842, Global test loss: 1.842, Global test accuracy: 39.04
Round  57, Train loss: 0.874, Test loss: 2.944, Test accuracy: 31.18
Round  57, Global train loss: 0.874, Global test loss: 1.876, Global test accuracy: 38.60
Round  58, Train loss: 0.991, Test loss: 2.961, Test accuracy: 30.86
Round  58, Global train loss: 0.991, Global test loss: 1.942, Global test accuracy: 31.28
Round  59, Train loss: 0.962, Test loss: 3.008, Test accuracy: 30.69
Round  59, Global train loss: 0.962, Global test loss: 1.935, Global test accuracy: 31.83
Round  60, Train loss: 1.017, Test loss: 3.087, Test accuracy: 30.50
Round  60, Global train loss: 1.017, Global test loss: 1.887, Global test accuracy: 37.03
Round  61, Train loss: 0.913, Test loss: 3.103, Test accuracy: 30.45
Round  61, Global train loss: 0.913, Global test loss: 2.033, Global test accuracy: 27.19
Round  62, Train loss: 0.694, Test loss: 3.129, Test accuracy: 30.30
Round  62, Global train loss: 0.694, Global test loss: 1.684, Global test accuracy: 42.15
Round  63, Train loss: 0.720, Test loss: 3.145, Test accuracy: 30.31
Round  63, Global train loss: 0.720, Global test loss: 1.713, Global test accuracy: 44.10
Round  64, Train loss: 0.827, Test loss: 3.193, Test accuracy: 30.55
Round  64, Global train loss: 0.827, Global test loss: 1.742, Global test accuracy: 41.43
Round  65, Train loss: 0.743, Test loss: 3.239, Test accuracy: 30.60
Round  65, Global train loss: 0.743, Global test loss: 1.783, Global test accuracy: 40.16
Round  66, Train loss: 0.771, Test loss: 3.253, Test accuracy: 30.49
Round  66, Global train loss: 0.771, Global test loss: 1.901, Global test accuracy: 36.13
Round  67, Train loss: 0.834, Test loss: 3.274, Test accuracy: 30.67
Round  67, Global train loss: 0.834, Global test loss: 1.944, Global test accuracy: 35.82
Round  68, Train loss: 0.790, Test loss: 3.278, Test accuracy: 30.70
Round  68, Global train loss: 0.790, Global test loss: 1.942, Global test accuracy: 35.04
Round  69, Train loss: 0.774, Test loss: 3.324, Test accuracy: 30.39
Round  69, Global train loss: 0.774, Global test loss: 1.869, Global test accuracy: 37.47
Round  70, Train loss: 0.828, Test loss: 3.353, Test accuracy: 30.17
Round  70, Global train loss: 0.828, Global test loss: 2.072, Global test accuracy: 27.86
Round  71, Train loss: 0.924, Test loss: 3.396, Test accuracy: 30.21
Round  71, Global train loss: 0.924, Global test loss: 1.890, Global test accuracy: 35.73
Round  72, Train loss: 0.824, Test loss: 3.430, Test accuracy: 30.44
Round  72, Global train loss: 0.824, Global test loss: 1.968, Global test accuracy: 32.15
Round  73, Train loss: 0.765, Test loss: 3.418, Test accuracy: 30.76
Round  73, Global train loss: 0.765, Global test loss: 2.062, Global test accuracy: 26.68
Round  74, Train loss: 0.808, Test loss: 3.484, Test accuracy: 30.70
Round  74, Global train loss: 0.808, Global test loss: 2.042, Global test accuracy: 29.29
Round  75, Train loss: 0.838, Test loss: 3.553, Test accuracy: 30.41
Round  75, Global train loss: 0.838, Global test loss: 1.929, Global test accuracy: 36.53
Round  76, Train loss: 0.711, Test loss: 3.627, Test accuracy: 30.27
Round  76, Global train loss: 0.711, Global test loss: 1.963, Global test accuracy: 34.49
Round  77, Train loss: 0.688, Test loss: 3.621, Test accuracy: 30.37
Round  77, Global train loss: 0.688, Global test loss: 1.948, Global test accuracy: 28.56
Round  78, Train loss: 0.799, Test loss: 3.676, Test accuracy: 29.95
Round  78, Global train loss: 0.799, Global test loss: 1.913, Global test accuracy: 36.15
Round  79, Train loss: 0.725, Test loss: 3.695, Test accuracy: 30.01
Round  79, Global train loss: 0.725, Global test loss: 1.992, Global test accuracy: 32.80
Round  80, Train loss: 0.806, Test loss: 3.743, Test accuracy: 30.01
Round  80, Global train loss: 0.806, Global test loss: 2.085, Global test accuracy: 24.19
Round  81, Train loss: 0.728, Test loss: 3.757, Test accuracy: 30.19
Round  81, Global train loss: 0.728, Global test loss: 1.771, Global test accuracy: 41.06
Round  82, Train loss: 0.685, Test loss: 3.756, Test accuracy: 30.28
Round  82, Global train loss: 0.685, Global test loss: 1.757, Global test accuracy: 41.48
Round  83, Train loss: 0.780, Test loss: 3.782, Test accuracy: 30.57
Round  83, Global train loss: 0.780, Global test loss: 1.991, Global test accuracy: 34.70
Round  84, Train loss: 0.593, Test loss: 3.755, Test accuracy: 30.68
Round  84, Global train loss: 0.593, Global test loss: 1.685, Global test accuracy: 43.06
Round  85, Train loss: 0.678, Test loss: 3.821, Test accuracy: 30.45
Round  85, Global train loss: 0.678, Global test loss: 1.900, Global test accuracy: 34.05
Round  86, Train loss: 0.723, Test loss: 3.844, Test accuracy: 30.57
Round  86, Global train loss: 0.723, Global test loss: 1.987, Global test accuracy: 30.07
Round  87, Train loss: 0.597, Test loss: 3.892, Test accuracy: 30.37
Round  87, Global train loss: 0.597, Global test loss: 1.689, Global test accuracy: 43.89
Round  88, Train loss: 0.695, Test loss: 3.875, Test accuracy: 30.16
Round  88, Global train loss: 0.695, Global test loss: 2.019, Global test accuracy: 33.10
Round  89, Train loss: 0.666, Test loss: 3.907, Test accuracy: 30.34
Round  89, Global train loss: 0.666, Global test loss: 1.960, Global test accuracy: 33.91
Round  90, Train loss: 0.521, Test loss: 3.950, Test accuracy: 30.14
Round  90, Global train loss: 0.521, Global test loss: 1.877, Global test accuracy: 38.53
Round  91, Train loss: 0.683, Test loss: 4.005, Test accuracy: 30.11
Round  91, Global train loss: 0.683, Global test loss: 1.984, Global test accuracy: 31.09
Round  92, Train loss: 0.576, Test loss: 4.013, Test accuracy: 30.01
Round  92, Global train loss: 0.576, Global test loss: 1.878, Global test accuracy: 36.43
Round  93, Train loss: 0.537, Test loss: 4.023, Test accuracy: 30.04
Round  93, Global train loss: 0.537, Global test loss: 1.932, Global test accuracy: 34.12
Round  94, Train loss: 0.612, Test loss: 4.048, Test accuracy: 30.22
Round  94, Global train loss: 0.612, Global test loss: 1.850, Global test accuracy: 36.48
Round  95, Train loss: 0.612, Test loss: 4.088, Test accuracy: 30.03
Round  95, Global train loss: 0.612, Global test loss: 1.832, Global test accuracy: 36.79
Round  96, Train loss: 0.551, Test loss: 4.104, Test accuracy: 29.90
Round  96, Global train loss: 0.551, Global test loss: 1.981, Global test accuracy: 31.45
Round  97, Train loss: 0.579, Test loss: 4.131, Test accuracy: 30.16
Round  97, Global train loss: 0.579, Global test loss: 1.649, Global test accuracy: 43.99
Round  98, Train loss: 0.483, Test loss: 4.169, Test accuracy: 30.08
Round  98, Global train loss: 0.483, Global test loss: 1.693, Global test accuracy: 41.35
Round  99, Train loss: 0.557, Test loss: 4.205, Test accuracy: 30.33
Round  99, Global train loss: 0.557, Global test loss: 1.960, Global test accuracy: 32.45
Final Round, Train loss: 0.378, Test loss: 4.856, Test accuracy: 30.19
Final Round, Global train loss: 0.378, Global test loss: 1.960, Global test accuracy: 32.45
Average accuracy final 10 rounds: 30.10175 

Average global accuracy final 10 rounds: 36.2695 

6663.445144176483
[5.175581216812134, 10.351162433624268, 15.265282392501831, 20.179402351379395, 24.88177490234375, 29.584147453308105, 34.37627720832825, 39.16840696334839, 43.794416666030884, 48.42042636871338, 53.049458026885986, 57.678489685058594, 62.218334436416626, 66.75817918777466, 71.50453996658325, 76.25090074539185, 80.80927395820618, 85.36764717102051, 89.8505334854126, 94.33341979980469, 98.83328175544739, 103.33314371109009, 107.91411924362183, 112.49509477615356, 117.16073036193848, 121.82636594772339, 126.29019141197205, 130.7540168762207, 135.33119225502014, 139.90836763381958, 144.4230670928955, 148.93776655197144, 153.61786723136902, 158.2979679107666, 162.85993695259094, 167.42190599441528, 172.04402351379395, 176.6661410331726, 181.1963140964508, 185.726487159729, 190.27419567108154, 194.82190418243408, 199.62837266921997, 204.43484115600586, 209.38637709617615, 214.33791303634644, 219.286869764328, 224.23582649230957, 228.9481418132782, 233.66045713424683, 238.26898837089539, 242.87751960754395, 247.53991508483887, 252.2023105621338, 257.1248571872711, 262.04740381240845, 267.2001144886017, 272.3528251647949, 277.4715802669525, 282.5903353691101, 287.7404115200043, 292.89048767089844, 297.9609172344208, 303.0313467979431, 307.7071785926819, 312.38301038742065, 317.19677805900574, 322.0105457305908, 326.74379324913025, 331.4770407676697, 336.14054012298584, 340.804039478302, 345.6201729774475, 350.436306476593, 355.2701277732849, 360.1039490699768, 364.9157831668854, 369.72761726379395, 374.5166220664978, 379.30562686920166, 384.0282437801361, 388.75086069107056, 393.57086300849915, 398.39086532592773, 403.20529222488403, 408.01971912384033, 412.819623708725, 417.6195282936096, 422.4518332481384, 427.28413820266724, 431.9954299926758, 436.7067217826843, 441.421790599823, 446.13685941696167, 450.85673093795776, 455.57660245895386, 460.4081733226776, 465.23974418640137, 469.86913299560547, 474.49852180480957, 479.28971552848816, 484.08090925216675, 488.8760998249054, 493.67129039764404, 498.42521023750305, 503.17913007736206, 507.9396677017212, 512.7002053260803, 517.4714155197144, 522.2426257133484, 526.8269610404968, 531.4112963676453, 536.2068076133728, 541.0023188591003, 545.8363296985626, 550.6703405380249, 555.4611530303955, 560.2519655227661, 565.0210316181183, 569.7900977134705, 574.4964082241058, 579.2027187347412, 583.8937129974365, 588.5847072601318, 593.2720565795898, 597.9594058990479, 602.7216458320618, 607.4838857650757, 612.1801340579987, 616.8763823509216, 621.4093165397644, 625.9422507286072, 630.5138432979584, 635.0854358673096, 639.6861627101898, 644.2868895530701, 649.0549509525299, 653.8230123519897, 658.6828806400299, 663.5427489280701, 668.2315599918365, 672.920371055603, 677.5958917140961, 682.2714123725891, 687.0721580982208, 691.8729038238525, 696.6109163761139, 701.3489289283752, 705.9762239456177, 710.6035189628601, 715.2810509204865, 719.9585828781128, 724.6866276264191, 729.4146723747253, 734.1267211437225, 738.8387699127197, 743.5333545207977, 748.2279391288757, 752.9779694080353, 757.7279996871948, 762.5210886001587, 767.3141775131226, 772.119526386261, 776.9248752593994, 781.4817152023315, 786.0385551452637, 790.8064646720886, 795.5743741989136, 800.3604202270508, 805.146466255188, 809.7454731464386, 814.3444800376892, 819.0425946712494, 823.7407093048096, 828.4779295921326, 833.2151498794556, 837.9574775695801, 842.6998052597046, 847.3369193077087, 851.9740333557129, 856.545672416687, 861.1173114776611, 865.6695370674133, 870.2217626571655, 874.752035856247, 879.2823090553284, 883.7513461112976, 888.2203831672668, 892.6846990585327, 897.1490149497986, 901.6553936004639, 906.1617722511292, 910.6482031345367, 915.1346340179443, 919.5982866287231, 924.061939239502, 928.4623119831085, 932.8626847267151, 937.3008301258087, 941.7389755249023, 943.9983701705933, 946.2577648162842]
[24.335, 24.335, 32.1875, 32.1875, 34.0875, 34.0875, 34.4625, 34.4625, 34.0475, 34.0475, 34.4875, 34.4875, 34.28, 34.28, 34.4725, 34.4725, 33.7625, 33.7625, 34.7725, 34.7725, 34.3675, 34.3675, 35.075, 35.075, 34.96, 34.96, 35.34, 35.34, 35.155, 35.155, 35.2525, 35.2525, 35.42, 35.42, 35.3925, 35.3925, 35.42, 35.42, 36.06, 36.06, 35.67, 35.67, 36.1275, 36.1275, 35.8725, 35.8725, 35.7925, 35.7925, 35.365, 35.365, 34.63, 34.63, 34.6125, 34.6125, 34.3075, 34.3075, 34.5175, 34.5175, 33.9275, 33.9275, 33.8625, 33.8625, 33.955, 33.955, 34.0225, 34.0225, 34.045, 34.045, 33.4525, 33.4525, 32.835, 32.835, 33.05, 33.05, 33.0525, 33.0525, 32.86, 32.86, 32.3575, 32.3575, 32.44, 32.44, 32.155, 32.155, 31.9575, 31.9575, 31.4775, 31.4775, 31.63, 31.63, 31.8425, 31.8425, 31.985, 31.985, 31.695, 31.695, 31.3775, 31.3775, 31.04, 31.04, 31.0675, 31.0675, 30.6875, 30.6875, 31.0525, 31.0525, 31.2075, 31.2075, 31.26, 31.26, 31.125, 31.125, 31.0975, 31.0975, 31.185, 31.185, 30.8625, 30.8625, 30.69, 30.69, 30.4975, 30.4975, 30.4525, 30.4525, 30.3, 30.3, 30.3125, 30.3125, 30.555, 30.555, 30.5975, 30.5975, 30.4925, 30.4925, 30.6725, 30.6725, 30.705, 30.705, 30.3925, 30.3925, 30.1725, 30.1725, 30.2075, 30.2075, 30.4425, 30.4425, 30.7625, 30.7625, 30.695, 30.695, 30.405, 30.405, 30.275, 30.275, 30.3725, 30.3725, 29.9475, 29.9475, 30.0075, 30.0075, 30.0075, 30.0075, 30.1925, 30.1925, 30.28, 30.28, 30.5725, 30.5725, 30.6775, 30.6775, 30.45, 30.45, 30.5725, 30.5725, 30.3675, 30.3675, 30.16, 30.16, 30.3375, 30.3375, 30.1375, 30.1375, 30.1125, 30.1125, 30.0125, 30.0125, 30.035, 30.035, 30.22, 30.22, 30.03, 30.03, 29.9, 29.9, 30.16, 30.16, 30.0775, 30.0775, 30.3325, 30.3325, 30.1925, 30.1925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5500
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4940
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8210
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5830
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7720
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7115
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5440
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5930
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7725
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6365
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.039, Test loss: 1.870, Test accuracy: 32.44
Round   0, Global train loss: 2.039, Global test loss: 1.853, Global test accuracy: 32.97
Round   1, Train loss: 1.795, Test loss: 1.646, Test accuracy: 42.08
Round   1, Global train loss: 1.795, Global test loss: 1.562, Global test accuracy: 45.12
Round   2, Train loss: 1.756, Test loss: 1.649, Test accuracy: 42.19
Round   2, Global train loss: 1.756, Global test loss: 1.503, Global test accuracy: 48.34
Round   3, Train loss: 1.733, Test loss: 1.613, Test accuracy: 44.35
Round   3, Global train loss: 1.733, Global test loss: 1.442, Global test accuracy: 52.67
Round   4, Train loss: 1.663, Test loss: 1.613, Test accuracy: 43.94
Round   4, Global train loss: 1.663, Global test loss: 1.382, Global test accuracy: 53.37
Round   5, Train loss: 1.631, Test loss: 1.580, Test accuracy: 45.60
Round   5, Global train loss: 1.631, Global test loss: 1.321, Global test accuracy: 57.97
Round   6, Train loss: 1.582, Test loss: 1.567, Test accuracy: 46.13
Round   6, Global train loss: 1.582, Global test loss: 1.292, Global test accuracy: 58.58
Round   7, Train loss: 1.535, Test loss: 1.517, Test accuracy: 48.16
Round   7, Global train loss: 1.535, Global test loss: 1.261, Global test accuracy: 60.06
Round   8, Train loss: 1.491, Test loss: 1.488, Test accuracy: 49.48
Round   8, Global train loss: 1.491, Global test loss: 1.190, Global test accuracy: 61.07
Round   9, Train loss: 1.414, Test loss: 1.455, Test accuracy: 50.72
Round   9, Global train loss: 1.414, Global test loss: 1.160, Global test accuracy: 62.31
Round  10, Train loss: 1.451, Test loss: 1.430, Test accuracy: 52.24
Round  10, Global train loss: 1.451, Global test loss: 1.164, Global test accuracy: 63.71
Round  11, Train loss: 1.434, Test loss: 1.415, Test accuracy: 52.50
Round  11, Global train loss: 1.434, Global test loss: 1.100, Global test accuracy: 65.01
Round  12, Train loss: 1.412, Test loss: 1.385, Test accuracy: 53.65
Round  12, Global train loss: 1.412, Global test loss: 1.115, Global test accuracy: 65.32
Round  13, Train loss: 1.364, Test loss: 1.383, Test accuracy: 53.91
Round  13, Global train loss: 1.364, Global test loss: 1.069, Global test accuracy: 65.91
Round  14, Train loss: 1.349, Test loss: 1.369, Test accuracy: 54.77
Round  14, Global train loss: 1.349, Global test loss: 1.035, Global test accuracy: 67.62
Round  15, Train loss: 1.200, Test loss: 1.358, Test accuracy: 55.19
Round  15, Global train loss: 1.200, Global test loss: 0.985, Global test accuracy: 68.33
Round  16, Train loss: 1.338, Test loss: 1.334, Test accuracy: 56.34
Round  16, Global train loss: 1.338, Global test loss: 1.038, Global test accuracy: 68.68
Round  17, Train loss: 1.264, Test loss: 1.313, Test accuracy: 57.24
Round  17, Global train loss: 1.264, Global test loss: 0.995, Global test accuracy: 69.27
Round  18, Train loss: 1.237, Test loss: 1.306, Test accuracy: 57.57
Round  18, Global train loss: 1.237, Global test loss: 0.986, Global test accuracy: 68.77
Round  19, Train loss: 1.237, Test loss: 1.281, Test accuracy: 58.62
Round  19, Global train loss: 1.237, Global test loss: 0.991, Global test accuracy: 69.46
Round  20, Train loss: 1.243, Test loss: 1.291, Test accuracy: 58.26
Round  20, Global train loss: 1.243, Global test loss: 0.967, Global test accuracy: 69.94
Round  21, Train loss: 1.302, Test loss: 1.281, Test accuracy: 58.90
Round  21, Global train loss: 1.302, Global test loss: 1.071, Global test accuracy: 66.41
Round  22, Train loss: 1.271, Test loss: 1.290, Test accuracy: 58.54
Round  22, Global train loss: 1.271, Global test loss: 1.000, Global test accuracy: 69.02
Round  23, Train loss: 1.260, Test loss: 1.286, Test accuracy: 58.60
Round  23, Global train loss: 1.260, Global test loss: 0.972, Global test accuracy: 70.22
Round  24, Train loss: 1.231, Test loss: 1.275, Test accuracy: 59.22
Round  24, Global train loss: 1.231, Global test loss: 0.989, Global test accuracy: 69.65
Round  25, Train loss: 1.241, Test loss: 1.263, Test accuracy: 59.55
Round  25, Global train loss: 1.241, Global test loss: 1.000, Global test accuracy: 69.41
Round  26, Train loss: 1.098, Test loss: 1.245, Test accuracy: 60.09
Round  26, Global train loss: 1.098, Global test loss: 0.882, Global test accuracy: 71.47
Round  27, Train loss: 1.188, Test loss: 1.255, Test accuracy: 60.10
Round  27, Global train loss: 1.188, Global test loss: 0.954, Global test accuracy: 70.48
Round  28, Train loss: 1.189, Test loss: 1.245, Test accuracy: 60.49
Round  28, Global train loss: 1.189, Global test loss: 0.935, Global test accuracy: 70.62
Round  29, Train loss: 1.141, Test loss: 1.265, Test accuracy: 59.96
Round  29, Global train loss: 1.141, Global test loss: 0.959, Global test accuracy: 70.71
Round  30, Train loss: 1.136, Test loss: 1.264, Test accuracy: 59.93
Round  30, Global train loss: 1.136, Global test loss: 0.917, Global test accuracy: 70.77
Round  31, Train loss: 1.132, Test loss: 1.264, Test accuracy: 60.23
Round  31, Global train loss: 1.132, Global test loss: 0.944, Global test accuracy: 70.21
Round  32, Train loss: 1.135, Test loss: 1.265, Test accuracy: 60.23
Round  32, Global train loss: 1.135, Global test loss: 0.905, Global test accuracy: 71.83
Round  33, Train loss: 1.099, Test loss: 1.257, Test accuracy: 60.44
Round  33, Global train loss: 1.099, Global test loss: 0.887, Global test accuracy: 71.70
Round  34, Train loss: 1.147, Test loss: 1.252, Test accuracy: 60.85
Round  34, Global train loss: 1.147, Global test loss: 0.917, Global test accuracy: 71.82
Round  35, Train loss: 1.129, Test loss: 1.271, Test accuracy: 60.64
Round  35, Global train loss: 1.129, Global test loss: 0.956, Global test accuracy: 70.99
Round  36, Train loss: 1.088, Test loss: 1.268, Test accuracy: 60.44
Round  36, Global train loss: 1.088, Global test loss: 0.882, Global test accuracy: 72.21
Round  37, Train loss: 1.116, Test loss: 1.269, Test accuracy: 60.38
Round  37, Global train loss: 1.116, Global test loss: 0.966, Global test accuracy: 70.83
Round  38, Train loss: 1.105, Test loss: 1.267, Test accuracy: 60.66
Round  38, Global train loss: 1.105, Global test loss: 0.931, Global test accuracy: 72.04
Round  39, Train loss: 0.999, Test loss: 1.260, Test accuracy: 60.91
Round  39, Global train loss: 0.999, Global test loss: 0.868, Global test accuracy: 72.41
Round  40, Train loss: 1.001, Test loss: 1.252, Test accuracy: 61.30
Round  40, Global train loss: 1.001, Global test loss: 0.874, Global test accuracy: 72.31
Round  41, Train loss: 1.012, Test loss: 1.251, Test accuracy: 61.22
Round  41, Global train loss: 1.012, Global test loss: 0.869, Global test accuracy: 72.09
Round  42, Train loss: 1.045, Test loss: 1.239, Test accuracy: 61.53
Round  42, Global train loss: 1.045, Global test loss: 0.925, Global test accuracy: 71.58
Round  43, Train loss: 0.964, Test loss: 1.256, Test accuracy: 61.18
Round  43, Global train loss: 0.964, Global test loss: 0.859, Global test accuracy: 72.48
Round  44, Train loss: 1.025, Test loss: 1.246, Test accuracy: 61.38
Round  44, Global train loss: 1.025, Global test loss: 0.925, Global test accuracy: 71.07
Round  45, Train loss: 1.017, Test loss: 1.257, Test accuracy: 61.25
Round  45, Global train loss: 1.017, Global test loss: 0.898, Global test accuracy: 71.78
Round  46, Train loss: 1.022, Test loss: 1.273, Test accuracy: 60.95
Round  46, Global train loss: 1.022, Global test loss: 0.906, Global test accuracy: 71.72
Round  47, Train loss: 1.018, Test loss: 1.267, Test accuracy: 61.02
Round  47, Global train loss: 1.018, Global test loss: 0.866, Global test accuracy: 72.82
Round  48, Train loss: 1.021, Test loss: 1.266, Test accuracy: 61.08
Round  48, Global train loss: 1.021, Global test loss: 0.857, Global test accuracy: 73.40
Round  49, Train loss: 0.891, Test loss: 1.266, Test accuracy: 61.09
Round  49, Global train loss: 0.891, Global test loss: 0.847, Global test accuracy: 72.77
Round  50, Train loss: 0.960, Test loss: 1.268, Test accuracy: 61.32
Round  50, Global train loss: 0.960, Global test loss: 0.908, Global test accuracy: 71.48
Round  51, Train loss: 0.936, Test loss: 1.264, Test accuracy: 61.48
Round  51, Global train loss: 0.936, Global test loss: 0.857, Global test accuracy: 73.04
Round  52, Train loss: 1.007, Test loss: 1.273, Test accuracy: 61.32
Round  52, Global train loss: 1.007, Global test loss: 0.905, Global test accuracy: 71.96
Round  53, Train loss: 0.936, Test loss: 1.260, Test accuracy: 61.54
Round  53, Global train loss: 0.936, Global test loss: 0.843, Global test accuracy: 73.61
Round  54, Train loss: 1.073, Test loss: 1.263, Test accuracy: 61.55
Round  54, Global train loss: 1.073, Global test loss: 0.920, Global test accuracy: 71.83
Round  55, Train loss: 0.938, Test loss: 1.261, Test accuracy: 61.77
Round  55, Global train loss: 0.938, Global test loss: 0.850, Global test accuracy: 73.25
Round  56, Train loss: 0.989, Test loss: 1.277, Test accuracy: 61.36
Round  56, Global train loss: 0.989, Global test loss: 0.880, Global test accuracy: 72.37
Round  57, Train loss: 0.917, Test loss: 1.268, Test accuracy: 61.64
Round  57, Global train loss: 0.917, Global test loss: 0.899, Global test accuracy: 72.44
Round  58, Train loss: 1.108, Test loss: 1.252, Test accuracy: 62.10
Round  58, Global train loss: 1.108, Global test loss: 0.965, Global test accuracy: 71.44
Round  59, Train loss: 0.972, Test loss: 1.267, Test accuracy: 61.80
Round  59, Global train loss: 0.972, Global test loss: 0.877, Global test accuracy: 72.55
Round  60, Train loss: 1.012, Test loss: 1.285, Test accuracy: 61.41
Round  60, Global train loss: 1.012, Global test loss: 0.908, Global test accuracy: 72.27
Round  61, Train loss: 0.941, Test loss: 1.293, Test accuracy: 61.17
Round  61, Global train loss: 0.941, Global test loss: 0.876, Global test accuracy: 73.07
Round  62, Train loss: 0.832, Test loss: 1.296, Test accuracy: 61.07
Round  62, Global train loss: 0.832, Global test loss: 0.858, Global test accuracy: 72.47
Round  63, Train loss: 0.934, Test loss: 1.313, Test accuracy: 61.08
Round  63, Global train loss: 0.934, Global test loss: 0.901, Global test accuracy: 71.94
Round  64, Train loss: 0.899, Test loss: 1.320, Test accuracy: 60.85
Round  64, Global train loss: 0.899, Global test loss: 0.915, Global test accuracy: 71.59
Round  65, Train loss: 0.910, Test loss: 1.305, Test accuracy: 61.00
Round  65, Global train loss: 0.910, Global test loss: 0.879, Global test accuracy: 72.57
Round  66, Train loss: 0.823, Test loss: 1.290, Test accuracy: 61.24
Round  66, Global train loss: 0.823, Global test loss: 0.884, Global test accuracy: 72.55
Round  67, Train loss: 0.976, Test loss: 1.285, Test accuracy: 61.25
Round  67, Global train loss: 0.976, Global test loss: 0.923, Global test accuracy: 71.71
Round  68, Train loss: 0.833, Test loss: 1.285, Test accuracy: 61.14
Round  68, Global train loss: 0.833, Global test loss: 0.880, Global test accuracy: 72.30
Round  69, Train loss: 0.916, Test loss: 1.292, Test accuracy: 61.18
Round  69, Global train loss: 0.916, Global test loss: 0.876, Global test accuracy: 72.38
Round  70, Train loss: 0.879, Test loss: 1.308, Test accuracy: 61.01
Round  70, Global train loss: 0.879, Global test loss: 0.826, Global test accuracy: 73.67
Round  71, Train loss: 1.045, Test loss: 1.314, Test accuracy: 60.88
Round  71, Global train loss: 1.045, Global test loss: 0.953, Global test accuracy: 71.31
Round  72, Train loss: 0.924, Test loss: 1.319, Test accuracy: 60.76
Round  72, Global train loss: 0.924, Global test loss: 0.893, Global test accuracy: 72.30
Round  73, Train loss: 0.857, Test loss: 1.315, Test accuracy: 60.91
Round  73, Global train loss: 0.857, Global test loss: 0.891, Global test accuracy: 72.15
Round  74, Train loss: 0.974, Test loss: 1.303, Test accuracy: 61.52
Round  74, Global train loss: 0.974, Global test loss: 0.925, Global test accuracy: 71.38
Round  75, Train loss: 0.914, Test loss: 1.309, Test accuracy: 61.33
Round  75, Global train loss: 0.914, Global test loss: 0.882, Global test accuracy: 72.83
Round  76, Train loss: 0.854, Test loss: 1.288, Test accuracy: 61.91
Round  76, Global train loss: 0.854, Global test loss: 0.934, Global test accuracy: 71.13
Round  77, Train loss: 0.995, Test loss: 1.282, Test accuracy: 62.08
Round  77, Global train loss: 0.995, Global test loss: 0.911, Global test accuracy: 71.54
Round  78, Train loss: 0.884, Test loss: 1.289, Test accuracy: 61.89
Round  78, Global train loss: 0.884, Global test loss: 0.871, Global test accuracy: 73.11
Round  79, Train loss: 0.989, Test loss: 1.289, Test accuracy: 61.70
Round  79, Global train loss: 0.989, Global test loss: 0.906, Global test accuracy: 72.97
Round  80, Train loss: 0.964, Test loss: 1.294, Test accuracy: 61.52
Round  80, Global train loss: 0.964, Global test loss: 0.901, Global test accuracy: 72.44
Round  81, Train loss: 0.933, Test loss: 1.300, Test accuracy: 61.56
Round  81, Global train loss: 0.933, Global test loss: 0.931, Global test accuracy: 71.83
Round  82, Train loss: 0.879, Test loss: 1.303, Test accuracy: 61.55
Round  82, Global train loss: 0.879, Global test loss: 0.879, Global test accuracy: 72.94
Round  83, Train loss: 0.897, Test loss: 1.300, Test accuracy: 61.67
Round  83, Global train loss: 0.897, Global test loss: 0.916, Global test accuracy: 71.49
Round  84, Train loss: 0.829, Test loss: 1.311, Test accuracy: 61.42
Round  84, Global train loss: 0.829, Global test loss: 0.869, Global test accuracy: 73.10
Round  85, Train loss: 0.900, Test loss: 1.307, Test accuracy: 61.58
Round  85, Global train loss: 0.900, Global test loss: 0.915, Global test accuracy: 71.66
Round  86, Train loss: 0.861, Test loss: 1.313, Test accuracy: 61.43
Round  86, Global train loss: 0.861, Global test loss: 0.870, Global test accuracy: 72.97
Round  87, Train loss: 0.856, Test loss: 1.305, Test accuracy: 61.52
Round  87, Global train loss: 0.856, Global test loss: 0.908, Global test accuracy: 72.03
Round  88, Train loss: 0.901, Test loss: 1.306, Test accuracy: 61.59
Round  88, Global train loss: 0.901, Global test loss: 0.962, Global test accuracy: 70.20
Round  89, Train loss: 0.870, Test loss: 1.324, Test accuracy: 61.58
Round  89, Global train loss: 0.870, Global test loss: 0.908, Global test accuracy: 71.73
Round  90, Train loss: 0.808, Test loss: 1.328, Test accuracy: 61.55
Round  90, Global train loss: 0.808, Global test loss: 0.868, Global test accuracy: 73.11
Round  91, Train loss: 0.955, Test loss: 1.324, Test accuracy: 61.79
Round  91, Global train loss: 0.955, Global test loss: 0.937, Global test accuracy: 71.56
Round  92, Train loss: 0.882, Test loss: 1.324, Test accuracy: 61.65
Round  92, Global train loss: 0.882, Global test loss: 0.957, Global test accuracy: 70.12
Round  93, Train loss: 0.825, Test loss: 1.337, Test accuracy: 61.29
Round  93, Global train loss: 0.825, Global test loss: 0.909, Global test accuracy: 71.94
Round  94, Train loss: 0.919, Test loss: 1.347, Test accuracy: 61.20
Round  94, Global train loss: 0.919, Global test loss: 0.983, Global test accuracy: 69.37
Round  95, Train loss: 0.868, Test loss: 1.350, Test accuracy: 61.30
Round  95, Global train loss: 0.868, Global test loss: 0.962, Global test accuracy: 70.25
Round  96, Train loss: 0.850, Test loss: 1.360, Test accuracy: 61.27
Round  96, Global train loss: 0.850, Global test loss: 0.919, Global test accuracy: 71.73
Round  97, Train loss: 0.837, Test loss: 1.369, Test accuracy: 61.12
Round  97, Global train loss: 0.837, Global test loss: 0.888, Global test accuracy: 72.46
Round  98, Train loss: 0.775, Test loss: 1.375, Test accuracy: 60.88
Round  98, Global train loss: 0.775, Global test loss: 0.903, Global test accuracy: 72.16
Round  99, Train loss: 0.780, Test loss: 1.376, Test accuracy: 60.79
Round  99, Global train loss: 0.780, Global test loss: 0.868, Global test accuracy: 72.73
Final Round, Train loss: 0.576, Test loss: 1.547, Test accuracy: 60.61
Final Round, Global train loss: 0.576, Global test loss: 0.868, Global test accuracy: 72.73
Average accuracy final 10 rounds: 61.2845 

Average global accuracy final 10 rounds: 71.5435 

6823.84149646759
[5.138046741485596, 10.276093482971191, 14.985734224319458, 19.695374965667725, 24.407379627227783, 29.119384288787842, 33.84743332862854, 38.57548236846924, 43.337820529937744, 48.10015869140625, 52.85333299636841, 57.606507301330566, 62.15018653869629, 66.69386577606201, 71.24395942687988, 75.79405307769775, 80.31202030181885, 84.82998752593994, 89.36338925361633, 93.89679098129272, 98.57785248756409, 103.25891399383545, 107.73503184318542, 112.2111496925354, 116.70745587348938, 121.20376205444336, 125.74109101295471, 130.27841997146606, 134.80724167823792, 139.33606338500977, 143.74400901794434, 148.1519546508789, 152.87481331825256, 157.59767198562622, 162.28128576278687, 166.9648995399475, 171.4027214050293, 175.84054327011108, 180.43903827667236, 185.03753328323364, 189.65858507156372, 194.2796368598938, 198.76097559928894, 203.24231433868408, 207.68071055412292, 212.11910676956177, 216.57359290122986, 221.02807903289795, 225.47856640815735, 229.92905378341675, 234.33558082580566, 238.74210786819458, 243.2042841911316, 247.6664605140686, 252.18903136253357, 256.71160221099854, 261.16764402389526, 265.623685836792, 270.0914194583893, 274.5591530799866, 279.19883966445923, 283.8385262489319, 288.2581481933594, 292.67777013778687, 297.0097961425781, 301.3418221473694, 305.74226927757263, 310.1427164077759, 314.62778425216675, 319.1128520965576, 323.6827630996704, 328.2526741027832, 332.95334815979004, 337.6540222167969, 342.35093235969543, 347.047842502594, 351.7664020061493, 356.4849615097046, 361.27632117271423, 366.0676808357239, 370.8503201007843, 375.6329593658447, 380.48079538345337, 385.328631401062, 390.0555844306946, 394.78253746032715, 399.56461668014526, 404.3466958999634, 408.984028339386, 413.6213607788086, 418.3361599445343, 423.05095911026, 427.77428817749023, 432.49761724472046, 437.25499272346497, 442.0123682022095, 446.67991948127747, 451.34747076034546, 456.1523585319519, 460.95724630355835, 465.6236503124237, 470.29005432128906, 474.86381220817566, 479.43757009506226, 484.16148686408997, 488.8854036331177, 493.6127655506134, 498.34012746810913, 503.11889481544495, 507.89766216278076, 512.7208206653595, 517.5439791679382, 522.266636133194, 526.9892930984497, 531.6659536361694, 536.3426141738892, 541.0903079509735, 545.8380017280579, 550.3669440746307, 554.8958864212036, 559.5623960494995, 564.2289056777954, 568.8376407623291, 573.4463758468628, 578.1904189586639, 582.9344620704651, 587.6937363147736, 592.453010559082, 597.1862134933472, 601.9194164276123, 606.6686851978302, 611.4179539680481, 616.2188038825989, 621.0196537971497, 625.8328886032104, 630.6461234092712, 635.421427488327, 640.1967315673828, 645.0321707725525, 649.8676099777222, 654.6815423965454, 659.4954748153687, 664.3059394359589, 669.1164040565491, 673.913726568222, 678.711049079895, 683.5193428993225, 688.32763671875, 693.1214046478271, 697.9151725769043, 702.7106323242188, 707.5060920715332, 712.2980070114136, 717.089921951294, 721.9359810352325, 726.7820401191711, 731.5860342979431, 736.3900284767151, 741.1108009815216, 745.8315734863281, 750.6016693115234, 755.3717651367188, 760.1394643783569, 764.9071636199951, 769.7264025211334, 774.5456414222717, 779.2038862705231, 783.8621311187744, 788.5934402942657, 793.3247494697571, 797.8622875213623, 802.3998255729675, 806.9157745838165, 811.4317235946655, 816.1551265716553, 820.878529548645, 825.4375531673431, 829.9965767860413, 834.6842744350433, 839.3719720840454, 844.1134359836578, 848.8548998832703, 853.6519064903259, 858.4489130973816, 863.3414475917816, 868.2339820861816, 873.0981912612915, 877.9624004364014, 882.7796428203583, 887.5968852043152, 892.2554001808167, 896.9139151573181, 901.7414858341217, 906.5690565109253, 911.2714626789093, 915.9738688468933, 920.8262753486633, 925.6786818504333, 930.3673739433289, 935.0560660362244, 937.4717507362366, 939.8874354362488]
[32.44, 32.44, 42.0775, 42.0775, 42.1875, 42.1875, 44.3475, 44.3475, 43.9375, 43.9375, 45.605, 45.605, 46.135, 46.135, 48.1575, 48.1575, 49.485, 49.485, 50.72, 50.72, 52.2375, 52.2375, 52.5, 52.5, 53.6475, 53.6475, 53.915, 53.915, 54.77, 54.77, 55.19, 55.19, 56.3425, 56.3425, 57.245, 57.245, 57.5725, 57.5725, 58.625, 58.625, 58.255, 58.255, 58.895, 58.895, 58.5375, 58.5375, 58.605, 58.605, 59.215, 59.215, 59.555, 59.555, 60.085, 60.085, 60.1, 60.1, 60.49, 60.49, 59.9625, 59.9625, 59.9275, 59.9275, 60.23, 60.23, 60.225, 60.225, 60.44, 60.44, 60.855, 60.855, 60.6375, 60.6375, 60.4425, 60.4425, 60.3825, 60.3825, 60.66, 60.66, 60.9125, 60.9125, 61.305, 61.305, 61.2225, 61.2225, 61.53, 61.53, 61.1825, 61.1825, 61.38, 61.38, 61.2475, 61.2475, 60.955, 60.955, 61.02, 61.02, 61.0825, 61.0825, 61.085, 61.085, 61.3225, 61.3225, 61.4825, 61.4825, 61.32, 61.32, 61.5425, 61.5425, 61.545, 61.545, 61.7725, 61.7725, 61.36, 61.36, 61.64, 61.64, 62.0975, 62.0975, 61.8, 61.8, 61.41, 61.41, 61.175, 61.175, 61.07, 61.07, 61.075, 61.075, 60.8475, 60.8475, 60.9975, 60.9975, 61.2425, 61.2425, 61.25, 61.25, 61.14, 61.14, 61.1775, 61.1775, 61.01, 61.01, 60.875, 60.875, 60.7575, 60.7575, 60.915, 60.915, 61.5175, 61.5175, 61.325, 61.325, 61.9075, 61.9075, 62.075, 62.075, 61.89, 61.89, 61.705, 61.705, 61.5175, 61.5175, 61.5575, 61.5575, 61.55, 61.55, 61.675, 61.675, 61.425, 61.425, 61.58, 61.58, 61.4325, 61.4325, 61.52, 61.52, 61.5925, 61.5925, 61.5825, 61.5825, 61.555, 61.555, 61.79, 61.79, 61.6475, 61.6475, 61.2875, 61.2875, 61.2025, 61.2025, 61.3, 61.3, 61.2725, 61.2725, 61.1175, 61.1175, 60.88, 60.88, 60.7925, 60.7925, 60.6125, 60.6125]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5565
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4890
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8200
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4685
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7545
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7040
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5225
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5615
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8030
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6395
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.576, Test loss: 2.014, Test accuracy: 19.75
Round   0, Global train loss: 1.576, Global test loss: 2.293, Global test accuracy: 10.62
Round   1, Train loss: 1.521, Test loss: 1.854, Test accuracy: 34.18
Round   1, Global train loss: 1.521, Global test loss: 2.204, Global test accuracy: 22.35
Round   2, Train loss: 1.378, Test loss: 1.513, Test accuracy: 43.52
Round   2, Global train loss: 1.378, Global test loss: 2.102, Global test accuracy: 23.03
Round   3, Train loss: 1.416, Test loss: 1.410, Test accuracy: 46.77
Round   3, Global train loss: 1.416, Global test loss: 2.126, Global test accuracy: 22.78
Round   4, Train loss: 1.347, Test loss: 1.253, Test accuracy: 52.18
Round   4, Global train loss: 1.347, Global test loss: 2.053, Global test accuracy: 23.60
Round   5, Train loss: 1.342, Test loss: 1.227, Test accuracy: 53.58
Round   5, Global train loss: 1.342, Global test loss: 2.019, Global test accuracy: 28.68
Round   6, Train loss: 1.376, Test loss: 1.223, Test accuracy: 54.77
Round   6, Global train loss: 1.376, Global test loss: 2.075, Global test accuracy: 23.68
Round   7, Train loss: 1.220, Test loss: 1.188, Test accuracy: 57.27
Round   7, Global train loss: 1.220, Global test loss: 2.036, Global test accuracy: 29.17
Round   8, Train loss: 1.231, Test loss: 1.188, Test accuracy: 57.62
Round   8, Global train loss: 1.231, Global test loss: 1.998, Global test accuracy: 26.92
Round   9, Train loss: 1.136, Test loss: 1.182, Test accuracy: 58.15
Round   9, Global train loss: 1.136, Global test loss: 1.969, Global test accuracy: 27.47
Round  10, Train loss: 1.152, Test loss: 1.142, Test accuracy: 60.58
Round  10, Global train loss: 1.152, Global test loss: 1.901, Global test accuracy: 31.68
Round  11, Train loss: 1.105, Test loss: 1.162, Test accuracy: 59.78
Round  11, Global train loss: 1.105, Global test loss: 1.918, Global test accuracy: 33.13
Round  12, Train loss: 1.186, Test loss: 1.093, Test accuracy: 62.17
Round  12, Global train loss: 1.186, Global test loss: 1.855, Global test accuracy: 37.62
Round  13, Train loss: 1.208, Test loss: 1.098, Test accuracy: 61.78
Round  13, Global train loss: 1.208, Global test loss: 2.044, Global test accuracy: 27.18
Round  14, Train loss: 1.300, Test loss: 1.065, Test accuracy: 63.07
Round  14, Global train loss: 1.300, Global test loss: 1.830, Global test accuracy: 38.13
Round  15, Train loss: 1.170, Test loss: 1.039, Test accuracy: 64.10
Round  15, Global train loss: 1.170, Global test loss: 1.845, Global test accuracy: 35.15
Round  16, Train loss: 1.115, Test loss: 1.013, Test accuracy: 65.68
Round  16, Global train loss: 1.115, Global test loss: 1.904, Global test accuracy: 33.07
Round  17, Train loss: 1.106, Test loss: 0.997, Test accuracy: 66.55
Round  17, Global train loss: 1.106, Global test loss: 1.842, Global test accuracy: 35.77
Round  18, Train loss: 1.132, Test loss: 1.002, Test accuracy: 66.07
Round  18, Global train loss: 1.132, Global test loss: 1.789, Global test accuracy: 38.28
Round  19, Train loss: 1.198, Test loss: 1.003, Test accuracy: 65.97
Round  19, Global train loss: 1.198, Global test loss: 1.778, Global test accuracy: 38.65
Round  20, Train loss: 1.196, Test loss: 0.983, Test accuracy: 66.98
Round  20, Global train loss: 1.196, Global test loss: 1.848, Global test accuracy: 33.70
Round  21, Train loss: 1.121, Test loss: 1.006, Test accuracy: 67.02
Round  21, Global train loss: 1.121, Global test loss: 1.995, Global test accuracy: 31.62
Round  22, Train loss: 1.026, Test loss: 1.004, Test accuracy: 66.93
Round  22, Global train loss: 1.026, Global test loss: 1.772, Global test accuracy: 38.73
Round  23, Train loss: 1.088, Test loss: 1.007, Test accuracy: 67.08
Round  23, Global train loss: 1.088, Global test loss: 1.773, Global test accuracy: 37.53
Round  24, Train loss: 0.997, Test loss: 0.998, Test accuracy: 67.15
Round  24, Global train loss: 0.997, Global test loss: 1.793, Global test accuracy: 39.77
Round  25, Train loss: 1.106, Test loss: 1.007, Test accuracy: 67.43
Round  25, Global train loss: 1.106, Global test loss: 1.648, Global test accuracy: 43.13
Round  26, Train loss: 1.041, Test loss: 1.005, Test accuracy: 67.58
Round  26, Global train loss: 1.041, Global test loss: 1.675, Global test accuracy: 42.30
Round  27, Train loss: 1.285, Test loss: 0.998, Test accuracy: 67.60
Round  27, Global train loss: 1.285, Global test loss: 1.702, Global test accuracy: 41.72
Round  28, Train loss: 1.255, Test loss: 0.992, Test accuracy: 67.67
Round  28, Global train loss: 1.255, Global test loss: 1.806, Global test accuracy: 37.88
Round  29, Train loss: 1.001, Test loss: 0.999, Test accuracy: 67.12
Round  29, Global train loss: 1.001, Global test loss: 1.852, Global test accuracy: 34.75
Round  30, Train loss: 1.088, Test loss: 0.990, Test accuracy: 67.50
Round  30, Global train loss: 1.088, Global test loss: 1.740, Global test accuracy: 36.70
Round  31, Train loss: 0.786, Test loss: 0.986, Test accuracy: 67.17
Round  31, Global train loss: 0.786, Global test loss: 1.805, Global test accuracy: 38.20
Round  32, Train loss: 1.060, Test loss: 0.980, Test accuracy: 67.20
Round  32, Global train loss: 1.060, Global test loss: 1.694, Global test accuracy: 41.97
Round  33, Train loss: 0.778, Test loss: 0.972, Test accuracy: 68.02
Round  33, Global train loss: 0.778, Global test loss: 1.756, Global test accuracy: 38.82
Round  34, Train loss: 0.902, Test loss: 0.975, Test accuracy: 68.22
Round  34, Global train loss: 0.902, Global test loss: 1.713, Global test accuracy: 40.97
Round  35, Train loss: 0.842, Test loss: 0.976, Test accuracy: 67.95
Round  35, Global train loss: 0.842, Global test loss: 1.837, Global test accuracy: 38.72
Round  36, Train loss: 0.926, Test loss: 0.989, Test accuracy: 67.10
Round  36, Global train loss: 0.926, Global test loss: 1.637, Global test accuracy: 43.28
Round  37, Train loss: 1.222, Test loss: 0.986, Test accuracy: 66.98
Round  37, Global train loss: 1.222, Global test loss: 1.749, Global test accuracy: 39.08
Round  38, Train loss: 1.008, Test loss: 0.985, Test accuracy: 67.37
Round  38, Global train loss: 1.008, Global test loss: 1.695, Global test accuracy: 40.13
Round  39, Train loss: 1.049, Test loss: 0.993, Test accuracy: 66.72
Round  39, Global train loss: 1.049, Global test loss: 1.861, Global test accuracy: 33.53
Round  40, Train loss: 0.931, Test loss: 0.987, Test accuracy: 66.78
Round  40, Global train loss: 0.931, Global test loss: 1.753, Global test accuracy: 40.07
Round  41, Train loss: 0.760, Test loss: 0.997, Test accuracy: 67.08
Round  41, Global train loss: 0.760, Global test loss: 1.872, Global test accuracy: 38.55
Round  42, Train loss: 1.135, Test loss: 0.994, Test accuracy: 67.25
Round  42, Global train loss: 1.135, Global test loss: 1.649, Global test accuracy: 43.85
Round  43, Train loss: 0.942, Test loss: 0.996, Test accuracy: 67.40
Round  43, Global train loss: 0.942, Global test loss: 1.616, Global test accuracy: 44.43
Round  44, Train loss: 0.945, Test loss: 1.001, Test accuracy: 67.55
Round  44, Global train loss: 0.945, Global test loss: 1.674, Global test accuracy: 43.90
Round  45, Train loss: 0.676, Test loss: 0.986, Test accuracy: 67.50
Round  45, Global train loss: 0.676, Global test loss: 1.771, Global test accuracy: 41.32
Round  46, Train loss: 0.982, Test loss: 0.999, Test accuracy: 66.92
Round  46, Global train loss: 0.982, Global test loss: 1.660, Global test accuracy: 43.47
Round  47, Train loss: 0.666, Test loss: 1.006, Test accuracy: 66.10
Round  47, Global train loss: 0.666, Global test loss: 1.707, Global test accuracy: 43.53
Round  48, Train loss: 0.819, Test loss: 1.012, Test accuracy: 66.00
Round  48, Global train loss: 0.819, Global test loss: 1.716, Global test accuracy: 43.08
Round  49, Train loss: 1.050, Test loss: 1.030, Test accuracy: 66.25
Round  49, Global train loss: 1.050, Global test loss: 1.659, Global test accuracy: 45.07
Round  50, Train loss: 0.754, Test loss: 1.053, Test accuracy: 64.92
Round  50, Global train loss: 0.754, Global test loss: 1.720, Global test accuracy: 41.53
Round  51, Train loss: 0.775, Test loss: 1.047, Test accuracy: 65.53
Round  51, Global train loss: 0.775, Global test loss: 1.739, Global test accuracy: 41.15
Round  52, Train loss: 0.895, Test loss: 1.046, Test accuracy: 65.35
Round  52, Global train loss: 0.895, Global test loss: 1.712, Global test accuracy: 42.12
Round  53, Train loss: 0.793, Test loss: 1.042, Test accuracy: 65.50
Round  53, Global train loss: 0.793, Global test loss: 1.683, Global test accuracy: 43.53
Round  54, Train loss: 0.876, Test loss: 1.047, Test accuracy: 65.72
Round  54, Global train loss: 0.876, Global test loss: 1.847, Global test accuracy: 38.75
Round  55, Train loss: 0.773, Test loss: 1.035, Test accuracy: 66.48
Round  55, Global train loss: 0.773, Global test loss: 1.661, Global test accuracy: 43.83
Round  56, Train loss: 0.742, Test loss: 1.028, Test accuracy: 66.80
Round  56, Global train loss: 0.742, Global test loss: 1.654, Global test accuracy: 44.57
Round  57, Train loss: 0.887, Test loss: 1.003, Test accuracy: 67.28
Round  57, Global train loss: 0.887, Global test loss: 1.771, Global test accuracy: 40.65
Round  58, Train loss: 0.636, Test loss: 0.998, Test accuracy: 67.42
Round  58, Global train loss: 0.636, Global test loss: 1.817, Global test accuracy: 40.98
Round  59, Train loss: 0.733, Test loss: 1.008, Test accuracy: 67.07
Round  59, Global train loss: 0.733, Global test loss: 1.636, Global test accuracy: 44.53
Round  60, Train loss: 0.902, Test loss: 1.039, Test accuracy: 66.12
Round  60, Global train loss: 0.902, Global test loss: 1.723, Global test accuracy: 41.82
Round  61, Train loss: 0.978, Test loss: 1.033, Test accuracy: 66.23
Round  61, Global train loss: 0.978, Global test loss: 1.707, Global test accuracy: 44.62
Round  62, Train loss: 0.817, Test loss: 1.015, Test accuracy: 67.45
Round  62, Global train loss: 0.817, Global test loss: 1.726, Global test accuracy: 40.58
Round  63, Train loss: 0.752, Test loss: 1.030, Test accuracy: 67.25
Round  63, Global train loss: 0.752, Global test loss: 1.696, Global test accuracy: 44.38
Round  64, Train loss: 0.910, Test loss: 1.047, Test accuracy: 67.17
Round  64, Global train loss: 0.910, Global test loss: 1.754, Global test accuracy: 43.60
Round  65, Train loss: 0.794, Test loss: 1.016, Test accuracy: 68.18
Round  65, Global train loss: 0.794, Global test loss: 1.833, Global test accuracy: 41.95
Round  66, Train loss: 0.757, Test loss: 1.050, Test accuracy: 67.65
Round  66, Global train loss: 0.757, Global test loss: 1.772, Global test accuracy: 43.75
Round  67, Train loss: 0.850, Test loss: 1.065, Test accuracy: 66.80
Round  67, Global train loss: 0.850, Global test loss: 1.803, Global test accuracy: 41.23
Round  68, Train loss: 1.027, Test loss: 1.080, Test accuracy: 66.73
Round  68, Global train loss: 1.027, Global test loss: 1.881, Global test accuracy: 37.78
Round  69, Train loss: 0.557, Test loss: 1.061, Test accuracy: 67.33
Round  69, Global train loss: 0.557, Global test loss: 1.933, Global test accuracy: 39.58
Round  70, Train loss: 0.831, Test loss: 1.071, Test accuracy: 67.30
Round  70, Global train loss: 0.831, Global test loss: 1.705, Global test accuracy: 43.85
Round  71, Train loss: 0.815, Test loss: 1.093, Test accuracy: 66.38
Round  71, Global train loss: 0.815, Global test loss: 1.847, Global test accuracy: 38.07
Round  72, Train loss: 0.562, Test loss: 1.091, Test accuracy: 66.33
Round  72, Global train loss: 0.562, Global test loss: 1.969, Global test accuracy: 38.67
Round  73, Train loss: 0.861, Test loss: 1.110, Test accuracy: 66.17
Round  73, Global train loss: 0.861, Global test loss: 1.770, Global test accuracy: 42.57
Round  74, Train loss: 0.775, Test loss: 1.117, Test accuracy: 65.62
Round  74, Global train loss: 0.775, Global test loss: 1.898, Global test accuracy: 43.83
Round  75, Train loss: 0.872, Test loss: 1.147, Test accuracy: 64.90
Round  75, Global train loss: 0.872, Global test loss: 1.782, Global test accuracy: 42.10
Round  76, Train loss: 0.846, Test loss: 1.123, Test accuracy: 65.97
Round  76, Global train loss: 0.846, Global test loss: 1.808, Global test accuracy: 41.45
Round  77, Train loss: 0.700, Test loss: 1.119, Test accuracy: 66.20
Round  77, Global train loss: 0.700, Global test loss: 2.020, Global test accuracy: 37.70
Round  78, Train loss: 0.491, Test loss: 1.110, Test accuracy: 66.58
Round  78, Global train loss: 0.491, Global test loss: 2.009, Global test accuracy: 41.07
Round  79, Train loss: 0.684, Test loss: 1.107, Test accuracy: 66.50
Round  79, Global train loss: 0.684, Global test loss: 1.844, Global test accuracy: 44.23
Round  80, Train loss: 0.693, Test loss: 1.072, Test accuracy: 67.08
Round  80, Global train loss: 0.693, Global test loss: 1.769, Global test accuracy: 43.42
Round  81, Train loss: 0.624, Test loss: 1.111, Test accuracy: 66.40
Round  81, Global train loss: 0.624, Global test loss: 1.793, Global test accuracy: 41.38
Round  82, Train loss: 0.680, Test loss: 1.141, Test accuracy: 65.55
Round  82, Global train loss: 0.680, Global test loss: 1.745, Global test accuracy: 44.65
Round  83, Train loss: 0.688, Test loss: 1.104, Test accuracy: 65.97
Round  83, Global train loss: 0.688, Global test loss: 1.865, Global test accuracy: 41.80
Round  84, Train loss: 0.620, Test loss: 1.139, Test accuracy: 64.65
Round  84, Global train loss: 0.620, Global test loss: 1.907, Global test accuracy: 44.88
Round  85, Train loss: 0.653, Test loss: 1.193, Test accuracy: 64.08
Round  85, Global train loss: 0.653, Global test loss: 1.821, Global test accuracy: 44.57
Round  86, Train loss: 0.705, Test loss: 1.177, Test accuracy: 65.47
Round  86, Global train loss: 0.705, Global test loss: 1.853, Global test accuracy: 43.33
Round  87, Train loss: 0.581, Test loss: 1.198, Test accuracy: 64.53
Round  87, Global train loss: 0.581, Global test loss: 2.042, Global test accuracy: 38.73
Round  88, Train loss: 0.841, Test loss: 1.211, Test accuracy: 64.75
Round  88, Global train loss: 0.841, Global test loss: 1.969, Global test accuracy: 38.22
Round  89, Train loss: 0.747, Test loss: 1.219, Test accuracy: 64.50
Round  89, Global train loss: 0.747, Global test loss: 1.938, Global test accuracy: 41.38
Round  90, Train loss: 0.762, Test loss: 1.211, Test accuracy: 64.57
Round  90, Global train loss: 0.762, Global test loss: 1.968, Global test accuracy: 38.47
Round  91, Train loss: 0.711, Test loss: 1.287, Test accuracy: 63.73
Round  91, Global train loss: 0.711, Global test loss: 2.026, Global test accuracy: 38.48
Round  92, Train loss: 0.590, Test loss: 1.270, Test accuracy: 64.32
Round  92, Global train loss: 0.590, Global test loss: 2.126, Global test accuracy: 38.10
Round  93, Train loss: 0.699, Test loss: 1.314, Test accuracy: 63.78
Round  93, Global train loss: 0.699, Global test loss: 1.926, Global test accuracy: 40.15
Round  94, Train loss: 0.652, Test loss: 1.312, Test accuracy: 64.10
Round  94, Global train loss: 0.652, Global test loss: 2.047, Global test accuracy: 38.20
Round  95, Train loss: 0.691, Test loss: 1.305, Test accuracy: 64.22
Round  95, Global train loss: 0.691, Global test loss: 2.174, Global test accuracy: 40.05
Round  96, Train loss: 0.569, Test loss: 1.254, Test accuracy: 63.73
Round  96, Global train loss: 0.569, Global test loss: 1.930, Global test accuracy: 43.50
Round  97, Train loss: 0.468, Test loss: 1.285, Test accuracy: 63.42
Round  97, Global train loss: 0.468, Global test loss: 2.020, Global test accuracy: 43.68
Round  98, Train loss: 0.661, Test loss: 1.266, Test accuracy: 64.97
Round  98, Global train loss: 0.661, Global test loss: 2.051, Global test accuracy: 39.27
Round  99, Train loss: 0.594, Test loss: 1.269, Test accuracy: 64.95
Round  99, Global train loss: 0.594, Global test loss: 1.822, Global test accuracy: 45.53
Final Round, Train loss: 0.488, Test loss: 1.421, Test accuracy: 63.25
Final Round, Global train loss: 0.488, Global test loss: 1.822, Global test accuracy: 45.53
Average accuracy final 10 rounds: 64.17833333333334 

Average global accuracy final 10 rounds: 40.54333333333334 

1098.7061495780945
[1.175147294998169, 2.350294589996338, 3.1854498386383057, 4.020605087280273, 4.857990741729736, 5.695376396179199, 6.5716025829315186, 7.447828769683838, 8.318365097045898, 9.188901424407959, 9.964630842208862, 10.740360260009766, 11.511870384216309, 12.283380508422852, 13.047555208206177, 13.811729907989502, 14.570045709609985, 15.328361511230469, 16.102173566818237, 16.875985622406006, 17.621904850006104, 18.3678240776062, 19.146950483322144, 19.926076889038086, 20.6744544506073, 21.422832012176514, 22.170398712158203, 22.917965412139893, 23.683775424957275, 24.449585437774658, 25.192991971969604, 25.93639850616455, 26.703319311141968, 27.470240116119385, 28.270538568496704, 29.070837020874023, 29.863869428634644, 30.656901836395264, 31.402698278427124, 32.148494720458984, 32.88610100746155, 33.62370729446411, 34.37124848365784, 35.11878967285156, 35.85739731788635, 36.59600496292114, 37.39130735397339, 38.186609745025635, 38.972668409347534, 39.758727073669434, 40.5012309551239, 41.24373483657837, 42.01029992103577, 42.776865005493164, 43.5335214138031, 44.29017782211304, 45.03242111206055, 45.77466440200806, 46.55322265625, 47.33178091049194, 48.12868809700012, 48.9255952835083, 49.70891356468201, 50.49223184585571, 51.24887681007385, 52.00552177429199, 52.75315713882446, 53.500792503356934, 54.25335764884949, 55.00592279434204, 55.75377154350281, 56.501620292663574, 57.23875570297241, 57.97589111328125, 58.71569561958313, 59.45550012588501, 60.191508769989014, 60.92751741409302, 61.66220283508301, 62.396888256073, 63.13209080696106, 63.86729335784912, 64.60298705101013, 65.33868074417114, 66.07984018325806, 66.82099962234497, 67.5597870349884, 68.29857444763184, 69.04167938232422, 69.7847843170166, 70.52642941474915, 71.26807451248169, 72.01526474952698, 72.76245498657227, 73.4998927116394, 74.23733043670654, 75.01666975021362, 75.7960090637207, 76.57991886138916, 77.36382865905762, 78.15305709838867, 78.94228553771973, 79.72407388687134, 80.50586223602295, 81.29109406471252, 82.0763258934021, 82.85650420188904, 83.63668251037598, 84.41026425361633, 85.18384599685669, 85.96410584449768, 86.74436569213867, 87.5281081199646, 88.31185054779053, 89.10393071174622, 89.8960108757019, 90.67099118232727, 91.44597148895264, 92.22294688224792, 92.99992227554321, 93.78428626060486, 94.5686502456665, 95.35661578178406, 96.14458131790161, 96.93258237838745, 97.72058343887329, 98.50801396369934, 99.29544448852539, 100.04556179046631, 100.79567909240723, 101.5781729221344, 102.36066675186157, 103.15623664855957, 103.95180654525757, 104.69855737686157, 105.44530820846558, 106.23785090446472, 107.03039360046387, 107.80715370178223, 108.58391380310059, 109.38679027557373, 110.18966674804688, 110.95689725875854, 111.72412776947021, 112.50142216682434, 113.27871656417847, 114.00788593292236, 114.73705530166626, 115.47384643554688, 116.21063756942749, 116.95144486427307, 117.69225215911865, 118.48275017738342, 119.2732481956482, 120.02266883850098, 120.77208948135376, 121.50879907608032, 122.24550867080688, 122.96557116508484, 123.6856336593628, 124.4167947769165, 125.14795589447021, 125.8915421962738, 126.63512849807739, 127.35766744613647, 128.08020639419556, 128.82261037826538, 129.5650143623352, 130.30223298072815, 131.0394515991211, 131.774831533432, 132.51021146774292, 133.24379539489746, 133.977379322052, 134.70813608169556, 135.4388928413391, 136.18050122261047, 136.92210960388184, 137.66665744781494, 138.41120529174805, 139.14517951011658, 139.8791537284851, 140.60108137130737, 141.32300901412964, 142.0520725250244, 142.7811360359192, 143.503000497818, 144.2248649597168, 144.9694321155548, 145.71399927139282, 146.459641456604, 147.20528364181519, 147.96742272377014, 148.7295618057251, 149.46332430839539, 150.19708681106567, 150.92792463302612, 151.65876245498657, 152.4004933834076, 153.1422243118286, 154.681538105011, 156.22085189819336]
[19.75, 19.75, 34.18333333333333, 34.18333333333333, 43.516666666666666, 43.516666666666666, 46.766666666666666, 46.766666666666666, 52.18333333333333, 52.18333333333333, 53.583333333333336, 53.583333333333336, 54.766666666666666, 54.766666666666666, 57.266666666666666, 57.266666666666666, 57.61666666666667, 57.61666666666667, 58.15, 58.15, 60.583333333333336, 60.583333333333336, 59.78333333333333, 59.78333333333333, 62.166666666666664, 62.166666666666664, 61.78333333333333, 61.78333333333333, 63.06666666666667, 63.06666666666667, 64.1, 64.1, 65.68333333333334, 65.68333333333334, 66.55, 66.55, 66.06666666666666, 66.06666666666666, 65.96666666666667, 65.96666666666667, 66.98333333333333, 66.98333333333333, 67.01666666666667, 67.01666666666667, 66.93333333333334, 66.93333333333334, 67.08333333333333, 67.08333333333333, 67.15, 67.15, 67.43333333333334, 67.43333333333334, 67.58333333333333, 67.58333333333333, 67.6, 67.6, 67.66666666666667, 67.66666666666667, 67.11666666666666, 67.11666666666666, 67.5, 67.5, 67.16666666666667, 67.16666666666667, 67.2, 67.2, 68.01666666666667, 68.01666666666667, 68.21666666666667, 68.21666666666667, 67.95, 67.95, 67.1, 67.1, 66.98333333333333, 66.98333333333333, 67.36666666666666, 67.36666666666666, 66.71666666666667, 66.71666666666667, 66.78333333333333, 66.78333333333333, 67.08333333333333, 67.08333333333333, 67.25, 67.25, 67.4, 67.4, 67.55, 67.55, 67.5, 67.5, 66.91666666666667, 66.91666666666667, 66.1, 66.1, 66.0, 66.0, 66.25, 66.25, 64.91666666666667, 64.91666666666667, 65.53333333333333, 65.53333333333333, 65.35, 65.35, 65.5, 65.5, 65.71666666666667, 65.71666666666667, 66.48333333333333, 66.48333333333333, 66.8, 66.8, 67.28333333333333, 67.28333333333333, 67.41666666666667, 67.41666666666667, 67.06666666666666, 67.06666666666666, 66.11666666666666, 66.11666666666666, 66.23333333333333, 66.23333333333333, 67.45, 67.45, 67.25, 67.25, 67.16666666666667, 67.16666666666667, 68.18333333333334, 68.18333333333334, 67.65, 67.65, 66.8, 66.8, 66.73333333333333, 66.73333333333333, 67.33333333333333, 67.33333333333333, 67.3, 67.3, 66.38333333333334, 66.38333333333334, 66.33333333333333, 66.33333333333333, 66.16666666666667, 66.16666666666667, 65.61666666666666, 65.61666666666666, 64.9, 64.9, 65.96666666666667, 65.96666666666667, 66.2, 66.2, 66.58333333333333, 66.58333333333333, 66.5, 66.5, 67.08333333333333, 67.08333333333333, 66.4, 66.4, 65.55, 65.55, 65.96666666666667, 65.96666666666667, 64.65, 64.65, 64.08333333333333, 64.08333333333333, 65.46666666666667, 65.46666666666667, 64.53333333333333, 64.53333333333333, 64.75, 64.75, 64.5, 64.5, 64.56666666666666, 64.56666666666666, 63.733333333333334, 63.733333333333334, 64.31666666666666, 64.31666666666666, 63.78333333333333, 63.78333333333333, 64.1, 64.1, 64.21666666666667, 64.21666666666667, 63.733333333333334, 63.733333333333334, 63.416666666666664, 63.416666666666664, 64.96666666666667, 64.96666666666667, 64.95, 64.95, 63.25, 63.25]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5505
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.5310
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8225
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4700
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7675
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7230
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5500
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6015
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8025
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6270
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.716, Test loss: 2.127, Test accuracy: 17.88
Round   1, Train loss: 1.106, Test loss: 1.871, Test accuracy: 33.62
Round   2, Train loss: 1.014, Test loss: 1.486, Test accuracy: 44.07
Round   3, Train loss: 0.850, Test loss: 1.317, Test accuracy: 48.28
Round   4, Train loss: 0.914, Test loss: 1.035, Test accuracy: 54.05
Round   5, Train loss: 0.894, Test loss: 0.910, Test accuracy: 59.13
Round   6, Train loss: 0.878, Test loss: 0.931, Test accuracy: 58.05
Round   7, Train loss: 0.805, Test loss: 0.876, Test accuracy: 59.48
Round   8, Train loss: 0.732, Test loss: 0.837, Test accuracy: 63.58
Round   9, Train loss: 0.776, Test loss: 0.887, Test accuracy: 61.18
Round  10, Train loss: 0.777, Test loss: 0.785, Test accuracy: 65.20
Round  11, Train loss: 0.693, Test loss: 0.815, Test accuracy: 65.20
Round  12, Train loss: 0.761, Test loss: 0.711, Test accuracy: 68.58
Round  13, Train loss: 0.768, Test loss: 0.720, Test accuracy: 68.50
Round  14, Train loss: 0.710, Test loss: 0.700, Test accuracy: 68.95
Round  15, Train loss: 0.664, Test loss: 0.704, Test accuracy: 69.25
Round  16, Train loss: 0.675, Test loss: 0.686, Test accuracy: 69.85
Round  17, Train loss: 0.754, Test loss: 0.668, Test accuracy: 71.23
Round  18, Train loss: 0.678, Test loss: 0.669, Test accuracy: 70.88
Round  19, Train loss: 0.671, Test loss: 0.656, Test accuracy: 71.48
Round  20, Train loss: 0.691, Test loss: 0.654, Test accuracy: 71.57
Round  21, Train loss: 0.699, Test loss: 0.659, Test accuracy: 71.47
Round  22, Train loss: 0.544, Test loss: 0.630, Test accuracy: 72.55
Round  23, Train loss: 0.523, Test loss: 0.638, Test accuracy: 72.50
Round  24, Train loss: 0.606, Test loss: 0.631, Test accuracy: 72.80
Round  25, Train loss: 0.637, Test loss: 0.629, Test accuracy: 72.88
Round  26, Train loss: 0.575, Test loss: 0.622, Test accuracy: 72.72
Round  27, Train loss: 0.662, Test loss: 0.613, Test accuracy: 73.47
Round  28, Train loss: 0.680, Test loss: 0.607, Test accuracy: 74.23
Round  29, Train loss: 0.600, Test loss: 0.602, Test accuracy: 74.37
Round  30, Train loss: 0.634, Test loss: 0.590, Test accuracy: 75.05
Round  31, Train loss: 0.491, Test loss: 0.580, Test accuracy: 75.40
Round  32, Train loss: 0.599, Test loss: 0.583, Test accuracy: 75.35
Round  33, Train loss: 0.513, Test loss: 0.567, Test accuracy: 75.90
Round  34, Train loss: 0.523, Test loss: 0.572, Test accuracy: 76.00
Round  35, Train loss: 0.538, Test loss: 0.567, Test accuracy: 75.95
Round  36, Train loss: 0.543, Test loss: 0.564, Test accuracy: 75.88
Round  37, Train loss: 0.599, Test loss: 0.563, Test accuracy: 76.25
Round  38, Train loss: 0.548, Test loss: 0.556, Test accuracy: 76.98
Round  39, Train loss: 0.544, Test loss: 0.554, Test accuracy: 76.62
Round  40, Train loss: 0.468, Test loss: 0.546, Test accuracy: 77.43
Round  41, Train loss: 0.417, Test loss: 0.546, Test accuracy: 77.65
Round  42, Train loss: 0.536, Test loss: 0.542, Test accuracy: 77.52
Round  43, Train loss: 0.555, Test loss: 0.540, Test accuracy: 77.35
Round  44, Train loss: 0.516, Test loss: 0.528, Test accuracy: 78.13
Round  45, Train loss: 0.432, Test loss: 0.530, Test accuracy: 78.15
Round  46, Train loss: 0.522, Test loss: 0.536, Test accuracy: 78.25
Round  47, Train loss: 0.425, Test loss: 0.533, Test accuracy: 78.47
Round  48, Train loss: 0.431, Test loss: 0.520, Test accuracy: 78.32
Round  49, Train loss: 0.504, Test loss: 0.523, Test accuracy: 78.30
Round  50, Train loss: 0.460, Test loss: 0.517, Test accuracy: 78.68
Round  51, Train loss: 0.492, Test loss: 0.514, Test accuracy: 78.75
Round  52, Train loss: 0.482, Test loss: 0.514, Test accuracy: 78.53
Round  53, Train loss: 0.428, Test loss: 0.518, Test accuracy: 78.35
Round  54, Train loss: 0.508, Test loss: 0.504, Test accuracy: 79.12
Round  55, Train loss: 0.490, Test loss: 0.509, Test accuracy: 79.25
Round  56, Train loss: 0.410, Test loss: 0.506, Test accuracy: 79.02
Round  57, Train loss: 0.484, Test loss: 0.505, Test accuracy: 78.58
Round  58, Train loss: 0.437, Test loss: 0.502, Test accuracy: 78.90
Round  59, Train loss: 0.403, Test loss: 0.489, Test accuracy: 79.45
Round  60, Train loss: 0.415, Test loss: 0.496, Test accuracy: 79.40
Round  61, Train loss: 0.495, Test loss: 0.490, Test accuracy: 79.83
Round  62, Train loss: 0.441, Test loss: 0.484, Test accuracy: 79.77
Round  63, Train loss: 0.394, Test loss: 0.483, Test accuracy: 79.60
Round  64, Train loss: 0.472, Test loss: 0.484, Test accuracy: 79.55
Round  65, Train loss: 0.440, Test loss: 0.487, Test accuracy: 79.73
Round  66, Train loss: 0.407, Test loss: 0.478, Test accuracy: 80.25
Round  67, Train loss: 0.436, Test loss: 0.483, Test accuracy: 80.17
Round  68, Train loss: 0.520, Test loss: 0.488, Test accuracy: 80.35
Round  69, Train loss: 0.340, Test loss: 0.473, Test accuracy: 80.72
Round  70, Train loss: 0.483, Test loss: 0.471, Test accuracy: 80.48
Round  71, Train loss: 0.450, Test loss: 0.481, Test accuracy: 80.48
Round  72, Train loss: 0.399, Test loss: 0.472, Test accuracy: 80.80
Round  73, Train loss: 0.457, Test loss: 0.470, Test accuracy: 80.72
Round  74, Train loss: 0.411, Test loss: 0.465, Test accuracy: 80.57
Round  75, Train loss: 0.431, Test loss: 0.467, Test accuracy: 80.78
Round  76, Train loss: 0.405, Test loss: 0.466, Test accuracy: 81.13
Round  77, Train loss: 0.296, Test loss: 0.463, Test accuracy: 81.10
Round  78, Train loss: 0.369, Test loss: 0.462, Test accuracy: 81.25
Round  79, Train loss: 0.355, Test loss: 0.466, Test accuracy: 81.02
Round  80, Train loss: 0.394, Test loss: 0.462, Test accuracy: 81.12
Round  81, Train loss: 0.316, Test loss: 0.462, Test accuracy: 81.15
Round  82, Train loss: 0.300, Test loss: 0.467, Test accuracy: 81.05
Round  83, Train loss: 0.297, Test loss: 0.465, Test accuracy: 81.02
Round  84, Train loss: 0.388, Test loss: 0.456, Test accuracy: 81.52
Round  85, Train loss: 0.287, Test loss: 0.461, Test accuracy: 81.58
Round  86, Train loss: 0.358, Test loss: 0.452, Test accuracy: 82.08
Round  87, Train loss: 0.301, Test loss: 0.455, Test accuracy: 81.23
Round  88, Train loss: 0.341, Test loss: 0.452, Test accuracy: 81.50
Round  89, Train loss: 0.343, Test loss: 0.453, Test accuracy: 81.65
Round  90, Train loss: 0.367, Test loss: 0.451, Test accuracy: 82.07
Round  91, Train loss: 0.388, Test loss: 0.447, Test accuracy: 82.15
Round  92, Train loss: 0.332, Test loss: 0.447, Test accuracy: 82.47
Round  93, Train loss: 0.332, Test loss: 0.446, Test accuracy: 82.13
Round  94, Train loss: 0.420, Test loss: 0.442, Test accuracy: 82.32
Round  95, Train loss: 0.328, Test loss: 0.451, Test accuracy: 82.22
Round  96, Train loss: 0.249, Test loss: 0.452, Test accuracy: 82.10
Round  97, Train loss: 0.364, Test loss: 0.441, Test accuracy: 82.57
Round  98, Train loss: 0.350, Test loss: 0.443, Test accuracy: 82.42
Round  99, Train loss: 0.300, Test loss: 0.447, Test accuracy: 82.28
Final Round, Train loss: 0.287, Test loss: 0.442, Test accuracy: 82.50
Average accuracy final 10 rounds: 82.27166666666666
798.3182058334351
[1.3282065391540527, 2.2796757221221924, 3.2290215492248535, 4.172840118408203, 5.159203767776489, 6.136688232421875, 7.105791807174683, 8.059263229370117, 9.006221294403076, 9.956183910369873, 10.90135645866394, 11.850647211074829, 12.788178205490112, 13.786042928695679, 14.765363931655884, 15.770712852478027, 16.74690318107605, 17.69068145751953, 18.613378286361694, 19.561736822128296, 20.524861812591553, 21.493364572525024, 22.470208883285522, 23.445278644561768, 24.413312435150146, 25.367987632751465, 26.29259705543518, 27.23258662223816, 28.195425987243652, 29.15355372428894, 30.113948822021484, 31.092017889022827, 32.06783080101013, 33.050410747528076, 34.001497983932495, 34.944002628326416, 35.86990427970886, 36.806232929229736, 37.746723651885986, 38.73136115074158, 39.71813440322876, 40.684178829193115, 41.65701484680176, 42.60056972503662, 43.54779314994812, 44.48753046989441, 45.43665552139282, 46.39760446548462, 47.38514018058777, 48.38033628463745, 49.338847637176514, 50.315781116485596, 51.277554750442505, 52.235342502593994, 53.17757797241211, 54.13510036468506, 55.08857011795044, 56.06150245666504, 57.0255024433136, 58.00055265426636, 58.98325848579407, 59.94879341125488, 60.90599751472473, 61.849345684051514, 62.772064447402954, 63.72280287742615, 64.70815825462341, 65.68720769882202, 66.65076446533203, 67.64110612869263, 68.59118938446045, 69.54125618934631, 70.46802139282227, 71.43902325630188, 72.41389513015747, 73.37832069396973, 74.32967472076416, 75.30245232582092, 76.25936889648438, 77.19618153572083, 78.15048837661743, 79.09519290924072, 80.04829335212708, 81.0114095211029, 82.0068109035492, 82.9604983329773, 83.91619372367859, 84.83807802200317, 85.75643467903137, 86.70027470588684, 87.64690923690796, 88.56747913360596, 89.53964567184448, 90.50461745262146, 91.48181343078613, 92.44500589370728, 93.37156438827515, 94.25175762176514, 95.10956883430481, 95.96862864494324, 97.42266416549683]
[17.883333333333333, 33.61666666666667, 44.06666666666667, 48.28333333333333, 54.05, 59.13333333333333, 58.05, 59.483333333333334, 63.583333333333336, 61.18333333333333, 65.2, 65.2, 68.58333333333333, 68.5, 68.95, 69.25, 69.85, 71.23333333333333, 70.88333333333334, 71.48333333333333, 71.56666666666666, 71.46666666666667, 72.55, 72.5, 72.8, 72.88333333333334, 72.71666666666667, 73.46666666666667, 74.23333333333333, 74.36666666666666, 75.05, 75.4, 75.35, 75.9, 76.0, 75.95, 75.88333333333334, 76.25, 76.98333333333333, 76.61666666666666, 77.43333333333334, 77.65, 77.51666666666667, 77.35, 78.13333333333334, 78.15, 78.25, 78.46666666666667, 78.31666666666666, 78.3, 78.68333333333334, 78.75, 78.53333333333333, 78.35, 79.11666666666666, 79.25, 79.01666666666667, 78.58333333333333, 78.9, 79.45, 79.4, 79.83333333333333, 79.76666666666667, 79.6, 79.55, 79.73333333333333, 80.25, 80.16666666666667, 80.35, 80.71666666666667, 80.48333333333333, 80.48333333333333, 80.8, 80.71666666666667, 80.56666666666666, 80.78333333333333, 81.13333333333334, 81.1, 81.25, 81.01666666666667, 81.11666666666666, 81.15, 81.05, 81.01666666666667, 81.51666666666667, 81.58333333333333, 82.08333333333333, 81.23333333333333, 81.5, 81.65, 82.06666666666666, 82.15, 82.46666666666667, 82.13333333333334, 82.31666666666666, 82.21666666666667, 82.1, 82.56666666666666, 82.41666666666667, 82.28333333333333, 82.5]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5515
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4870
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8190
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5370
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7490
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.6875
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5165
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6180
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7845
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6125
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  10.0000
Round 1 global test acc  18.8600
Round 2 global test acc  13.0100
Round 3 global test acc  18.8800
Round 4 global test acc  18.1500
Round 5 global test acc  19.4300
Round 6 global test acc  19.8000
Round 7 global test acc  18.0100
Round 8 global test acc  11.4600
Round 9 global test acc  21.8900
Round 10 global test acc  23.1600
Round 11 global test acc  20.5600
Round 12 global test acc  18.6600
Round 13 global test acc  25.5100
Round 14 global test acc  22.5100
Round 15 global test acc  15.1500
Round 16 global test acc  24.1600
Round 17 global test acc  25.8800
Round 18 global test acc  21.5600
Round 19 global test acc  23.8800
Round 20 global test acc  16.6900
Round 21 global test acc  17.3900
Round 22 global test acc  28.7900
Round 23 global test acc  19.9600
Round 24 global test acc  25.2300
Round 25 global test acc  20.0700
Round 26 global test acc  22.3200
Round 27 global test acc  26.2800
Round 28 global test acc  22.7300
Round 29 global test acc  29.7800
Round 30 global test acc  25.9000
Round 31 global test acc  28.3800
Round 32 global test acc  25.1700
Round 33 global test acc  30.0900
Round 34 global test acc  28.2900
Round 35 global test acc  26.1200
Round 36 global test acc  31.3200
Round 37 global test acc  22.2000
Round 38 global test acc  17.4100
Round 39 global test acc  23.8500
Round 40 global test acc  26.4100
Round 41 global test acc  25.6900
Round 42 global test acc  32.2400
Round 43 global test acc  27.5100
Round 44 global test acc  28.8600
Round 45 global test acc  22.2000
Round 46 global test acc  31.6700
Round 47 global test acc  31.2500
Round 48 global test acc  25.5900
Round 49 global test acc  34.1400
Round 50 global test acc  24.6000
Round 51 global test acc  20.9000
Round 52 global test acc  34.9800
Round 53 global test acc  28.8900
Round 54 global test acc  34.5900
Round 55 global test acc  28.7800
Round 56 global test acc  30.4900
Round 57 global test acc  36.9800
Round 58 global test acc  25.9500
Round 59 global test acc  32.1700
Round 60 global test acc  30.8300
Round 61 global test acc  25.3700
Round 62 global test acc  31.7600
Round 63 global test acc  28.2400
Round 64 global test acc  26.3000
Round 65 global test acc  29.8000
Round 66 global test acc  29.1000
Round 67 global test acc  27.7000
Round 68 global test acc  31.8600
Round 69 global test acc  20.6600
Round 70 global test acc  26.0900
Round 71 global test acc  28.4600
Round 72 global test acc  35.1700
Round 73 global test acc  35.5900
Round 74 global test acc  30.5100
Round 75 global test acc  22.8700
Round 76 global test acc  28.5200
Round 77 global test acc  34.4400
Round 78 global test acc  29.9500
Round 79 global test acc  30.3300
Round 80 global test acc  30.1300
Round 81 global test acc  26.2400
Round 82 global test acc  26.2000
Round 83 global test acc  24.6500
Round 84 global test acc  22.9500
Round 85 global test acc  25.3000
Round 86 global test acc  22.8000
Round 87 global test acc  23.2900
Round 88 global test acc  22.7800
Round 89 global test acc  23.5100
Round 90 global test acc  24.5500
Round 91 global test acc  24.3400
Round 92 global test acc  24.7800
Round 93 global test acc  25.4000
Round 94 global test acc  25.8600
Round 95 global test acc  23.6900
Round 96 global test acc  23.9300
Round 97 global test acc  24.6400
Round 98 global test acc  22.7500
Round 99 global test acc  22.2900
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5540
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.5455
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8130
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4995
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7605
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.6925
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5075
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5960
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8035
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6505
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.035, Test loss: 2.256, Test accuracy: 14.85
Round   1, Train loss: 1.713, Test loss: 2.033, Test accuracy: 28.10
Round   2, Train loss: 1.642, Test loss: 1.733, Test accuracy: 37.90
Round   3, Train loss: 1.417, Test loss: 1.591, Test accuracy: 40.82
Round   4, Train loss: 1.555, Test loss: 1.463, Test accuracy: 43.27
Round   5, Train loss: 1.372, Test loss: 1.333, Test accuracy: 48.90
Round   6, Train loss: 1.406, Test loss: 1.340, Test accuracy: 46.85
Round   7, Train loss: 1.446, Test loss: 1.295, Test accuracy: 48.80
Round   8, Train loss: 1.266, Test loss: 1.284, Test accuracy: 49.23
Round   9, Train loss: 1.355, Test loss: 1.299, Test accuracy: 48.12
Round  10, Train loss: 1.364, Test loss: 1.247, Test accuracy: 53.90
Round  11, Train loss: 1.340, Test loss: 1.219, Test accuracy: 52.80
Round  12, Train loss: 1.347, Test loss: 1.216, Test accuracy: 56.37
Round  13, Train loss: 1.198, Test loss: 1.193, Test accuracy: 57.07
Round  14, Train loss: 1.392, Test loss: 1.213, Test accuracy: 58.52
Round  15, Train loss: 1.750, Test loss: 1.201, Test accuracy: 60.33
Round  16, Train loss: 1.446, Test loss: 1.191, Test accuracy: 59.85
Round  17, Train loss: 1.272, Test loss: 1.144, Test accuracy: 60.37
Round  18, Train loss: 1.184, Test loss: 1.140, Test accuracy: 60.65
Round  19, Train loss: 1.195, Test loss: 1.136, Test accuracy: 61.50
Round  20, Train loss: 1.326, Test loss: 1.138, Test accuracy: 62.33
Round  21, Train loss: 1.429, Test loss: 1.175, Test accuracy: 61.32
Round  22, Train loss: 1.408, Test loss: 1.120, Test accuracy: 63.13
Round  23, Train loss: 1.498, Test loss: 1.117, Test accuracy: 63.67
Round  24, Train loss: 1.507, Test loss: 1.102, Test accuracy: 63.43
Round  25, Train loss: 1.422, Test loss: 1.118, Test accuracy: 63.82
Round  26, Train loss: 1.167, Test loss: 1.091, Test accuracy: 63.85
Round  27, Train loss: 1.230, Test loss: 1.088, Test accuracy: 63.30
Round  28, Train loss: 1.201, Test loss: 1.094, Test accuracy: 63.45
Round  29, Train loss: 1.303, Test loss: 1.090, Test accuracy: 64.28
Round  30, Train loss: 1.298, Test loss: 1.083, Test accuracy: 65.15
Round  31, Train loss: 1.397, Test loss: 1.084, Test accuracy: 65.53
Round  32, Train loss: 1.245, Test loss: 1.075, Test accuracy: 65.35
Round  33, Train loss: 1.298, Test loss: 1.050, Test accuracy: 65.98
Round  34, Train loss: 1.137, Test loss: 1.047, Test accuracy: 65.97
Round  35, Train loss: 1.527, Test loss: 1.059, Test accuracy: 66.23
Round  36, Train loss: 0.984, Test loss: 1.034, Test accuracy: 65.98
Round  37, Train loss: 1.350, Test loss: 1.044, Test accuracy: 65.90
Round  38, Train loss: 1.305, Test loss: 1.053, Test accuracy: 66.25
Round  39, Train loss: 1.174, Test loss: 1.052, Test accuracy: 65.72
Round  40, Train loss: 1.082, Test loss: 1.040, Test accuracy: 66.72
Round  41, Train loss: 1.215, Test loss: 1.022, Test accuracy: 67.93
Round  42, Train loss: 1.287, Test loss: 1.029, Test accuracy: 67.63
Round  43, Train loss: 1.207, Test loss: 1.037, Test accuracy: 67.93
Round  44, Train loss: 1.229, Test loss: 1.005, Test accuracy: 68.15
Round  45, Train loss: 1.286, Test loss: 1.012, Test accuracy: 68.78
Round  46, Train loss: 1.105, Test loss: 1.013, Test accuracy: 68.17
Round  47, Train loss: 1.077, Test loss: 1.005, Test accuracy: 68.57
Round  48, Train loss: 1.212, Test loss: 1.012, Test accuracy: 68.25
Round  49, Train loss: 1.067, Test loss: 1.018, Test accuracy: 67.87
Round  50, Train loss: 1.221, Test loss: 1.016, Test accuracy: 68.70
Round  51, Train loss: 1.414, Test loss: 1.006, Test accuracy: 69.03
Round  52, Train loss: 1.026, Test loss: 1.004, Test accuracy: 68.80
Round  53, Train loss: 1.195, Test loss: 0.997, Test accuracy: 69.63
Round  54, Train loss: 1.255, Test loss: 0.986, Test accuracy: 69.65
Round  55, Train loss: 1.315, Test loss: 0.995, Test accuracy: 68.93
Round  56, Train loss: 1.360, Test loss: 0.996, Test accuracy: 69.50
Round  57, Train loss: 1.299, Test loss: 1.002, Test accuracy: 69.02
Round  58, Train loss: 1.256, Test loss: 1.005, Test accuracy: 68.92
Round  59, Train loss: 1.106, Test loss: 0.997, Test accuracy: 69.12
Round  60, Train loss: 1.152, Test loss: 1.004, Test accuracy: 69.20
Round  61, Train loss: 0.901, Test loss: 0.992, Test accuracy: 68.70
Round  62, Train loss: 1.064, Test loss: 0.976, Test accuracy: 69.25
Round  63, Train loss: 0.985, Test loss: 0.983, Test accuracy: 69.35
Round  64, Train loss: 1.042, Test loss: 1.000, Test accuracy: 68.18
Round  65, Train loss: 1.222, Test loss: 0.983, Test accuracy: 69.53
Round  66, Train loss: 1.419, Test loss: 0.984, Test accuracy: 69.25
Round  67, Train loss: 1.475, Test loss: 0.988, Test accuracy: 69.03
Round  68, Train loss: 0.916, Test loss: 0.990, Test accuracy: 68.87
Round  69, Train loss: 1.198, Test loss: 0.968, Test accuracy: 69.70
Round  70, Train loss: 1.237, Test loss: 0.977, Test accuracy: 69.72
Round  71, Train loss: 1.041, Test loss: 0.977, Test accuracy: 69.67
Round  72, Train loss: 1.072, Test loss: 0.979, Test accuracy: 69.63
Round  73, Train loss: 1.367, Test loss: 0.972, Test accuracy: 69.80
Round  74, Train loss: 1.218, Test loss: 0.962, Test accuracy: 69.42
Round  75, Train loss: 1.172, Test loss: 0.976, Test accuracy: 69.57
Round  76, Train loss: 1.013, Test loss: 0.968, Test accuracy: 69.32
Round  77, Train loss: 1.067, Test loss: 0.977, Test accuracy: 69.57
Round  78, Train loss: 1.262, Test loss: 0.978, Test accuracy: 69.53
Round  79, Train loss: 0.941, Test loss: 0.979, Test accuracy: 69.62
Round  80, Train loss: 1.192, Test loss: 0.984, Test accuracy: 69.48
Round  81, Train loss: 0.869, Test loss: 0.986, Test accuracy: 69.77
Round  82, Train loss: 0.931, Test loss: 0.973, Test accuracy: 70.20
Round  83, Train loss: 0.999, Test loss: 0.967, Test accuracy: 69.35
Round  84, Train loss: 1.025, Test loss: 0.969, Test accuracy: 69.00
Round  85, Train loss: 1.004, Test loss: 0.981, Test accuracy: 68.60
Round  86, Train loss: 0.934, Test loss: 0.972, Test accuracy: 69.40
Round  87, Train loss: 1.045, Test loss: 0.965, Test accuracy: 69.58
Round  88, Train loss: 0.976, Test loss: 0.970, Test accuracy: 69.68
Round  89, Train loss: 1.062, Test loss: 0.970, Test accuracy: 69.42
Round  90, Train loss: 0.798, Test loss: 0.989, Test accuracy: 68.65
Round  91, Train loss: 0.860, Test loss: 0.974, Test accuracy: 69.10
Round  92, Train loss: 1.403, Test loss: 0.974, Test accuracy: 68.97
Round  93, Train loss: 0.629, Test loss: 0.974, Test accuracy: 68.85
Round  94, Train loss: 1.199, Test loss: 0.967, Test accuracy: 69.43
Round  95, Train loss: 0.730, Test loss: 0.967, Test accuracy: 68.90
Round  96, Train loss: 0.905, Test loss: 0.970, Test accuracy: 69.28
Round  97, Train loss: 1.158, Test loss: 0.963, Test accuracy: 69.37
Round  98, Train loss: 0.817, Test loss: 0.973, Test accuracy: 69.15
Round  99, Train loss: 0.956, Test loss: 0.972, Test accuracy: 69.23
Final Round, Train loss: 0.967, Test loss: 0.982, Test accuracy: 68.43
Average accuracy final 10 rounds: 69.09333333333333
789.2146670818329
[1.3174960613250732, 2.2734034061431885, 3.23453950881958, 4.208058834075928, 5.177813291549683, 6.1281046867370605, 7.077493190765381, 8.029691457748413, 9.00497317314148, 9.953505754470825, 10.904214143753052, 11.861464977264404, 12.825316905975342, 13.807652950286865, 14.77819275856018, 15.754477500915527, 16.711191177368164, 17.654732942581177, 18.603968143463135, 19.57141613960266, 20.546648263931274, 21.51143980026245, 22.488855361938477, 23.431909799575806, 24.390011310577393, 25.343698501586914, 26.30313229560852, 27.269269466400146, 28.23130965232849, 29.776744842529297, 30.72459578514099, 31.692909479141235, 32.666444301605225, 33.6264374256134, 34.57519054412842, 35.5248019695282, 36.47032856941223, 37.42594313621521, 38.340689182281494, 39.29448962211609, 40.21682691574097, 41.16400861740112, 42.10483908653259, 43.06597685813904, 44.01931357383728, 44.98395395278931, 45.938186168670654, 46.889312982559204, 47.84656357765198, 48.810585498809814, 49.802072286605835, 50.75879788398743, 51.744060754776, 52.68924689292908, 53.63324809074402, 54.60514283180237, 55.52396583557129, 56.45869779586792, 57.411553144454956, 58.327205657958984, 59.2787811756134, 60.22784185409546, 61.20001697540283, 62.17015194892883, 63.118839502334595, 64.06725573539734, 65.01120686531067, 65.97322130203247, 66.9310941696167, 67.91252732276917, 68.87188267707825, 69.82982611656189, 70.79609942436218, 71.7445719242096, 72.71782684326172, 73.67186379432678, 74.63955402374268, 75.59090256690979, 76.56229043006897, 77.52804064750671, 78.49148893356323, 79.45555567741394, 80.41788458824158, 81.39179944992065, 82.35367751121521, 83.30247211456299, 84.27308320999146, 85.26370358467102, 86.22070097923279, 87.15804815292358, 88.13448739051819, 89.11437678337097, 90.09469556808472, 91.04202723503113, 91.9936535358429, 92.95683550834656, 93.92628121376038, 94.87955665588379, 95.83508324623108, 96.80485987663269, 98.28005027770996]
[14.85, 28.1, 37.9, 40.81666666666667, 43.266666666666666, 48.9, 46.85, 48.8, 49.233333333333334, 48.11666666666667, 53.9, 52.8, 56.36666666666667, 57.06666666666667, 58.516666666666666, 60.333333333333336, 59.85, 60.36666666666667, 60.65, 61.5, 62.333333333333336, 61.31666666666667, 63.13333333333333, 63.666666666666664, 63.43333333333333, 63.81666666666667, 63.85, 63.3, 63.45, 64.28333333333333, 65.15, 65.53333333333333, 65.35, 65.98333333333333, 65.96666666666667, 66.23333333333333, 65.98333333333333, 65.9, 66.25, 65.71666666666667, 66.71666666666667, 67.93333333333334, 67.63333333333334, 67.93333333333334, 68.15, 68.78333333333333, 68.16666666666667, 68.56666666666666, 68.25, 67.86666666666666, 68.7, 69.03333333333333, 68.8, 69.63333333333334, 69.65, 68.93333333333334, 69.5, 69.01666666666667, 68.91666666666667, 69.11666666666666, 69.2, 68.7, 69.25, 69.35, 68.18333333333334, 69.53333333333333, 69.25, 69.03333333333333, 68.86666666666666, 69.7, 69.71666666666667, 69.66666666666667, 69.63333333333334, 69.8, 69.41666666666667, 69.56666666666666, 69.31666666666666, 69.56666666666666, 69.53333333333333, 69.61666666666666, 69.48333333333333, 69.76666666666667, 70.2, 69.35, 69.0, 68.6, 69.4, 69.58333333333333, 69.68333333333334, 69.41666666666667, 68.65, 69.1, 68.96666666666667, 68.85, 69.43333333333334, 68.9, 69.28333333333333, 69.36666666666666, 69.15, 69.23333333333333, 68.43333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5545
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4810
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8190
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4940
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7645
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7455
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5285
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5950
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7905
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6950
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.653, Test loss: 2.086, Test accuracy: 19.72
Round   1, Train loss: 1.041, Test loss: 1.816, Test accuracy: 30.97
Round   2, Train loss: 0.997, Test loss: 1.466, Test accuracy: 45.67
Round   3, Train loss: 0.869, Test loss: 1.243, Test accuracy: 49.97
Round   4, Train loss: 0.846, Test loss: 0.895, Test accuracy: 62.15
Round   5, Train loss: 0.801, Test loss: 0.801, Test accuracy: 64.22
Round   6, Train loss: 0.715, Test loss: 0.798, Test accuracy: 64.80
Round   7, Train loss: 0.681, Test loss: 0.822, Test accuracy: 64.97
Round   8, Train loss: 0.691, Test loss: 0.707, Test accuracy: 69.75
Round   9, Train loss: 0.698, Test loss: 0.717, Test accuracy: 69.73
Round  10, Train loss: 0.651, Test loss: 0.708, Test accuracy: 70.05
Round  11, Train loss: 0.723, Test loss: 0.657, Test accuracy: 71.75
Round  12, Train loss: 0.670, Test loss: 0.614, Test accuracy: 73.27
Round  13, Train loss: 0.614, Test loss: 0.607, Test accuracy: 74.05
Round  14, Train loss: 0.599, Test loss: 0.586, Test accuracy: 74.25
Round  15, Train loss: 0.604, Test loss: 0.578, Test accuracy: 75.35
Round  16, Train loss: 0.610, Test loss: 0.587, Test accuracy: 74.85
Round  17, Train loss: 0.583, Test loss: 0.564, Test accuracy: 75.68
Round  18, Train loss: 0.598, Test loss: 0.565, Test accuracy: 76.02
Round  19, Train loss: 0.652, Test loss: 0.547, Test accuracy: 76.95
Round  20, Train loss: 0.530, Test loss: 0.531, Test accuracy: 77.37
Round  21, Train loss: 0.552, Test loss: 0.537, Test accuracy: 77.55
Round  22, Train loss: 0.626, Test loss: 0.534, Test accuracy: 77.30
Round  23, Train loss: 0.509, Test loss: 0.530, Test accuracy: 77.47
Round  24, Train loss: 0.537, Test loss: 0.533, Test accuracy: 77.05
Round  25, Train loss: 0.527, Test loss: 0.518, Test accuracy: 78.30
Round  26, Train loss: 0.530, Test loss: 0.509, Test accuracy: 78.78
Round  27, Train loss: 0.492, Test loss: 0.496, Test accuracy: 79.23
Round  28, Train loss: 0.485, Test loss: 0.497, Test accuracy: 79.45
Round  29, Train loss: 0.578, Test loss: 0.491, Test accuracy: 79.22
Round  30, Train loss: 0.501, Test loss: 0.484, Test accuracy: 79.80
Round  31, Train loss: 0.565, Test loss: 0.486, Test accuracy: 79.72
Round  32, Train loss: 0.503, Test loss: 0.472, Test accuracy: 80.17
Round  33, Train loss: 0.523, Test loss: 0.464, Test accuracy: 80.30
Round  34, Train loss: 0.529, Test loss: 0.474, Test accuracy: 80.72
Round  35, Train loss: 0.505, Test loss: 0.470, Test accuracy: 81.07
Round  36, Train loss: 0.465, Test loss: 0.468, Test accuracy: 80.88
Round  37, Train loss: 0.451, Test loss: 0.462, Test accuracy: 80.43
Round  38, Train loss: 0.441, Test loss: 0.462, Test accuracy: 80.43
Round  39, Train loss: 0.454, Test loss: 0.449, Test accuracy: 81.48
Round  40, Train loss: 0.509, Test loss: 0.445, Test accuracy: 81.28
Round  41, Train loss: 0.502, Test loss: 0.449, Test accuracy: 81.50
Round  42, Train loss: 0.435, Test loss: 0.442, Test accuracy: 81.93
Round  43, Train loss: 0.463, Test loss: 0.434, Test accuracy: 81.85
Round  44, Train loss: 0.525, Test loss: 0.423, Test accuracy: 83.00
Round  45, Train loss: 0.496, Test loss: 0.425, Test accuracy: 82.60
Round  46, Train loss: 0.466, Test loss: 0.420, Test accuracy: 82.45
Round  47, Train loss: 0.468, Test loss: 0.422, Test accuracy: 82.97
Round  48, Train loss: 0.490, Test loss: 0.423, Test accuracy: 82.68
Round  49, Train loss: 0.429, Test loss: 0.413, Test accuracy: 83.18
Round  50, Train loss: 0.455, Test loss: 0.404, Test accuracy: 83.75
Round  51, Train loss: 0.444, Test loss: 0.400, Test accuracy: 83.45
Round  52, Train loss: 0.371, Test loss: 0.393, Test accuracy: 83.97
Round  53, Train loss: 0.373, Test loss: 0.388, Test accuracy: 84.32
Round  54, Train loss: 0.391, Test loss: 0.389, Test accuracy: 84.17
Round  55, Train loss: 0.435, Test loss: 0.395, Test accuracy: 84.22
Round  56, Train loss: 0.410, Test loss: 0.387, Test accuracy: 84.42
Round  57, Train loss: 0.348, Test loss: 0.387, Test accuracy: 84.33
Round  58, Train loss: 0.416, Test loss: 0.392, Test accuracy: 84.12
Round  59, Train loss: 0.417, Test loss: 0.378, Test accuracy: 84.52
Round  60, Train loss: 0.357, Test loss: 0.374, Test accuracy: 84.92
Round  61, Train loss: 0.368, Test loss: 0.376, Test accuracy: 84.83
Round  62, Train loss: 0.347, Test loss: 0.379, Test accuracy: 84.67
Round  63, Train loss: 0.337, Test loss: 0.377, Test accuracy: 84.50
Round  64, Train loss: 0.353, Test loss: 0.376, Test accuracy: 84.45
Round  65, Train loss: 0.337, Test loss: 0.370, Test accuracy: 85.13
Round  66, Train loss: 0.392, Test loss: 0.367, Test accuracy: 85.08
Round  67, Train loss: 0.336, Test loss: 0.371, Test accuracy: 85.07
Round  68, Train loss: 0.348, Test loss: 0.360, Test accuracy: 85.22
Round  69, Train loss: 0.390, Test loss: 0.365, Test accuracy: 85.00
Round  70, Train loss: 0.328, Test loss: 0.365, Test accuracy: 85.27
Round  71, Train loss: 0.325, Test loss: 0.366, Test accuracy: 85.52
Round  72, Train loss: 0.346, Test loss: 0.361, Test accuracy: 85.82
Round  73, Train loss: 0.338, Test loss: 0.360, Test accuracy: 85.70
Round  74, Train loss: 0.311, Test loss: 0.361, Test accuracy: 85.62
Round  75, Train loss: 0.292, Test loss: 0.353, Test accuracy: 85.50
Round  76, Train loss: 0.295, Test loss: 0.356, Test accuracy: 85.80
Round  77, Train loss: 0.372, Test loss: 0.351, Test accuracy: 85.78
Round  78, Train loss: 0.367, Test loss: 0.355, Test accuracy: 85.75
Round  79, Train loss: 0.324, Test loss: 0.348, Test accuracy: 86.03
Round  80, Train loss: 0.318, Test loss: 0.353, Test accuracy: 85.63
Round  81, Train loss: 0.365, Test loss: 0.350, Test accuracy: 86.00
Round  82, Train loss: 0.339, Test loss: 0.351, Test accuracy: 85.77
Round  83, Train loss: 0.325, Test loss: 0.353, Test accuracy: 85.95
Round  84, Train loss: 0.307, Test loss: 0.348, Test accuracy: 86.40
Round  85, Train loss: 0.314, Test loss: 0.356, Test accuracy: 85.78
Round  86, Train loss: 0.316, Test loss: 0.348, Test accuracy: 86.20
Round  87, Train loss: 0.290, Test loss: 0.342, Test accuracy: 86.07
Round  88, Train loss: 0.287, Test loss: 0.337, Test accuracy: 86.40
Round  89, Train loss: 0.275, Test loss: 0.346, Test accuracy: 85.88
Round  90, Train loss: 0.299, Test loss: 0.343, Test accuracy: 86.23
Round  91, Train loss: 0.264, Test loss: 0.340, Test accuracy: 86.32
Round  92, Train loss: 0.291, Test loss: 0.342, Test accuracy: 86.15
Round  93, Train loss: 0.276, Test loss: 0.343, Test accuracy: 86.28
Round  94, Train loss: 0.285, Test loss: 0.343, Test accuracy: 86.38
Round  95, Train loss: 0.244, Test loss: 0.348, Test accuracy: 86.07
Round  96, Train loss: 0.248, Test loss: 0.349, Test accuracy: 85.63
Round  97, Train loss: 0.311, Test loss: 0.337, Test accuracy: 86.08
Round  98, Train loss: 0.262, Test loss: 0.339, Test accuracy: 86.58
Round  99, Train loss: 0.252, Test loss: 0.342, Test accuracy: 86.10
Final Round, Train loss: 0.230, Test loss: 0.337, Test accuracy: 86.43
Average accuracy final 10 rounds: 86.18333333333334
1448.1237106323242
[1.2544257640838623, 2.2090651988983154, 3.1760611534118652, 4.135675430297852, 5.0983641147613525, 6.057103157043457, 7.0173022747039795, 7.9869606494903564, 8.936071157455444, 9.91378378868103, 10.889486074447632, 11.85798954963684, 12.814422845840454, 13.778089761734009, 14.741387367248535, 15.688997983932495, 16.662882566452026, 17.617570638656616, 18.585938453674316, 19.53774356842041, 20.504687070846558, 22.926247119903564, 25.425833463668823, 27.808189868927002, 30.207660675048828, 32.51988434791565, 34.87999248504639, 37.31355428695679, 39.59761428833008, 41.9651083946228, 44.31911277770996, 46.67752528190613, 49.04276514053345, 51.45431995391846, 53.7832453250885, 56.21299171447754, 58.54508900642395, 60.861034631729126, 63.24063539505005, 65.66761994361877, 68.02003145217896, 70.34943270683289, 72.65607690811157, 75.0570969581604, 77.52918457984924, 79.99734449386597, 82.40959095954895, 84.95387506484985, 87.57740521430969, 90.09796595573425, 92.72091245651245, 95.32676815986633, 97.8419144153595, 100.32919907569885, 102.87819790840149, 105.33577084541321, 107.81474590301514, 110.28862261772156, 112.78163838386536, 115.31947946548462, 117.81127405166626, 120.24107217788696, 122.63982057571411, 125.18273115158081, 127.73082756996155, 130.17298078536987, 132.69285464286804, 135.23888659477234, 137.6948311328888, 140.160297870636, 142.6901979446411, 145.2062873840332, 147.66088557243347, 150.08323454856873, 152.58428239822388, 155.1101040840149, 157.65283346176147, 160.10692882537842, 162.61727452278137, 165.07808995246887, 167.53784132003784, 169.96147799491882, 172.38574075698853, 174.84265995025635, 177.27643132209778, 179.70648074150085, 182.19801545143127, 184.7751498222351, 187.28157353401184, 189.6472291946411, 192.0810422897339, 194.51300477981567, 196.99596047401428, 199.50246405601501, 201.90826177597046, 204.31188488006592, 206.73373651504517, 209.25331354141235, 211.74490976333618, 214.22985100746155, 215.55450129508972]
[19.716666666666665, 30.966666666666665, 45.666666666666664, 49.96666666666667, 62.15, 64.21666666666667, 64.8, 64.96666666666667, 69.75, 69.73333333333333, 70.05, 71.75, 73.26666666666667, 74.05, 74.25, 75.35, 74.85, 75.68333333333334, 76.01666666666667, 76.95, 77.36666666666666, 77.55, 77.3, 77.46666666666667, 77.05, 78.3, 78.78333333333333, 79.23333333333333, 79.45, 79.21666666666667, 79.8, 79.71666666666667, 80.16666666666667, 80.3, 80.71666666666667, 81.06666666666666, 80.88333333333334, 80.43333333333334, 80.43333333333334, 81.48333333333333, 81.28333333333333, 81.5, 81.93333333333334, 81.85, 83.0, 82.6, 82.45, 82.96666666666667, 82.68333333333334, 83.18333333333334, 83.75, 83.45, 83.96666666666667, 84.31666666666666, 84.16666666666667, 84.21666666666667, 84.41666666666667, 84.33333333333333, 84.11666666666666, 84.51666666666667, 84.91666666666667, 84.83333333333333, 84.66666666666667, 84.5, 84.45, 85.13333333333334, 85.08333333333333, 85.06666666666666, 85.21666666666667, 85.0, 85.26666666666667, 85.51666666666667, 85.81666666666666, 85.7, 85.61666666666666, 85.5, 85.8, 85.78333333333333, 85.75, 86.03333333333333, 85.63333333333334, 86.0, 85.76666666666667, 85.95, 86.4, 85.78333333333333, 86.2, 86.06666666666666, 86.4, 85.88333333333334, 86.23333333333333, 86.31666666666666, 86.15, 86.28333333333333, 86.38333333333334, 86.06666666666666, 85.63333333333334, 86.08333333333333, 86.58333333333333, 86.1, 86.43333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8545
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1960
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0640
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5685
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8060
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1270
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6960
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7450
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.3340
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6465
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5250
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8705
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1655
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.4215
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6985
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.5360
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.183, Test loss: 2.141, Test accuracy: 25.92
Round   0, Global train loss: 2.183, Global test loss: 2.162, Global test accuracy: 26.54
Round   1, Train loss: 2.128, Test loss: 2.085, Test accuracy: 30.44
Round   1, Global train loss: 2.128, Global test loss: 2.096, Global test accuracy: 34.11
Round   2, Train loss: 2.096, Test loss: 2.055, Test accuracy: 30.86
Round   2, Global train loss: 2.096, Global test loss: 2.077, Global test accuracy: 35.69
Round   3, Train loss: 2.082, Test loss: 2.041, Test accuracy: 32.80
Round   3, Global train loss: 2.082, Global test loss: 2.101, Global test accuracy: 36.66
Round   4, Train loss: 2.097, Test loss: 1.984, Test accuracy: 33.68
Round   4, Global train loss: 2.097, Global test loss: 1.993, Global test accuracy: 37.89
Round   5, Train loss: 1.965, Test loss: 1.960, Test accuracy: 34.53
Round   5, Global train loss: 1.965, Global test loss: 1.937, Global test accuracy: 43.90
Round   6, Train loss: 1.991, Test loss: 1.937, Test accuracy: 34.61
Round   6, Global train loss: 1.991, Global test loss: 1.890, Global test accuracy: 44.77
Round   7, Train loss: 1.950, Test loss: 1.908, Test accuracy: 35.43
Round   7, Global train loss: 1.950, Global test loss: 1.823, Global test accuracy: 46.09
Round   8, Train loss: 2.098, Test loss: 1.924, Test accuracy: 34.00
Round   8, Global train loss: 2.098, Global test loss: 1.998, Global test accuracy: 37.87
Round   9, Train loss: 2.021, Test loss: 1.928, Test accuracy: 34.13
Round   9, Global train loss: 2.021, Global test loss: 1.972, Global test accuracy: 42.08
Round  10, Train loss: 1.883, Test loss: 1.936, Test accuracy: 33.24
Round  10, Global train loss: 1.883, Global test loss: 1.948, Global test accuracy: 43.01
Round  11, Train loss: 1.944, Test loss: 1.930, Test accuracy: 33.39
Round  11, Global train loss: 1.944, Global test loss: 1.883, Global test accuracy: 45.72
Round  12, Train loss: 1.987, Test loss: 1.923, Test accuracy: 33.77
Round  12, Global train loss: 1.987, Global test loss: 1.952, Global test accuracy: 40.49
Round  13, Train loss: 1.939, Test loss: 1.922, Test accuracy: 33.65
Round  13, Global train loss: 1.939, Global test loss: 2.042, Global test accuracy: 38.16
Round  14, Train loss: 1.960, Test loss: 1.925, Test accuracy: 33.70
Round  14, Global train loss: 1.960, Global test loss: 1.973, Global test accuracy: 40.74
Round  15, Train loss: 1.649, Test loss: 1.923, Test accuracy: 34.07
Round  15, Global train loss: 1.649, Global test loss: 1.725, Global test accuracy: 48.04
Round  16, Train loss: 1.826, Test loss: 1.928, Test accuracy: 33.84
Round  16, Global train loss: 1.826, Global test loss: 1.957, Global test accuracy: 42.91
Round  17, Train loss: 1.781, Test loss: 1.960, Test accuracy: 33.30
Round  17, Global train loss: 1.781, Global test loss: 1.935, Global test accuracy: 40.91
Round  18, Train loss: 2.029, Test loss: 1.968, Test accuracy: 33.09
Round  18, Global train loss: 2.029, Global test loss: 1.988, Global test accuracy: 38.94
Round  19, Train loss: 1.822, Test loss: 1.960, Test accuracy: 33.15
Round  19, Global train loss: 1.822, Global test loss: 1.979, Global test accuracy: 39.35
Round  20, Train loss: 1.946, Test loss: 1.962, Test accuracy: 33.36
Round  20, Global train loss: 1.946, Global test loss: 1.972, Global test accuracy: 42.63
Round  21, Train loss: 1.707, Test loss: 1.963, Test accuracy: 33.75
Round  21, Global train loss: 1.707, Global test loss: 1.919, Global test accuracy: 42.33
Round  22, Train loss: 1.875, Test loss: 1.965, Test accuracy: 33.50
Round  22, Global train loss: 1.875, Global test loss: 1.974, Global test accuracy: 40.59
Round  23, Train loss: 1.546, Test loss: 1.993, Test accuracy: 33.24
Round  23, Global train loss: 1.546, Global test loss: 1.779, Global test accuracy: 48.47
Round  24, Train loss: 1.736, Test loss: 2.006, Test accuracy: 32.73
Round  24, Global train loss: 1.736, Global test loss: 2.033, Global test accuracy: 35.63
Round  25, Train loss: 1.657, Test loss: 2.015, Test accuracy: 32.59
Round  25, Global train loss: 1.657, Global test loss: 1.959, Global test accuracy: 40.14
Round  26, Train loss: 1.722, Test loss: 2.048, Test accuracy: 32.02
Round  26, Global train loss: 1.722, Global test loss: 1.891, Global test accuracy: 39.28
Round  27, Train loss: 1.724, Test loss: 2.073, Test accuracy: 31.59
Round  27, Global train loss: 1.724, Global test loss: 1.887, Global test accuracy: 42.56
Round  28, Train loss: 1.420, Test loss: 2.077, Test accuracy: 31.83
Round  28, Global train loss: 1.420, Global test loss: 1.720, Global test accuracy: 45.01
Round  29, Train loss: 1.497, Test loss: 2.091, Test accuracy: 31.83
Round  29, Global train loss: 1.497, Global test loss: 1.869, Global test accuracy: 42.09
Round  30, Train loss: 1.412, Test loss: 2.109, Test accuracy: 31.59
Round  30, Global train loss: 1.412, Global test loss: 1.862, Global test accuracy: 42.77
Round  31, Train loss: 1.641, Test loss: 2.135, Test accuracy: 31.35
Round  31, Global train loss: 1.641, Global test loss: 2.030, Global test accuracy: 33.40
Round  32, Train loss: 1.505, Test loss: 2.160, Test accuracy: 31.50
Round  32, Global train loss: 1.505, Global test loss: 2.004, Global test accuracy: 31.73
Round  33, Train loss: 1.360, Test loss: 2.191, Test accuracy: 31.44
Round  33, Global train loss: 1.360, Global test loss: 1.840, Global test accuracy: 40.64
Round  34, Train loss: 1.411, Test loss: 2.206, Test accuracy: 31.43
Round  34, Global train loss: 1.411, Global test loss: 1.928, Global test accuracy: 38.62
Round  35, Train loss: 1.665, Test loss: 2.239, Test accuracy: 31.39
Round  35, Global train loss: 1.665, Global test loss: 1.952, Global test accuracy: 37.43
Round  36, Train loss: 1.567, Test loss: 2.262, Test accuracy: 30.93
Round  36, Global train loss: 1.567, Global test loss: 2.059, Global test accuracy: 29.48
Round  37, Train loss: 1.513, Test loss: 2.294, Test accuracy: 30.89
Round  37, Global train loss: 1.513, Global test loss: 2.052, Global test accuracy: 34.63
Round  38, Train loss: 1.373, Test loss: 2.344, Test accuracy: 30.43
Round  38, Global train loss: 1.373, Global test loss: 1.937, Global test accuracy: 34.69
Round  39, Train loss: 1.517, Test loss: 2.381, Test accuracy: 30.25
Round  39, Global train loss: 1.517, Global test loss: 2.110, Global test accuracy: 29.48
Round  40, Train loss: 1.610, Test loss: 2.391, Test accuracy: 30.38
Round  40, Global train loss: 1.610, Global test loss: 2.095, Global test accuracy: 26.18
Round  41, Train loss: 1.612, Test loss: 2.421, Test accuracy: 29.95
Round  41, Global train loss: 1.612, Global test loss: 2.117, Global test accuracy: 25.04
Round  42, Train loss: 1.278, Test loss: 2.461, Test accuracy: 29.91
Round  42, Global train loss: 1.278, Global test loss: 1.812, Global test accuracy: 39.53
Round  43, Train loss: 1.440, Test loss: 2.495, Test accuracy: 29.60
Round  43, Global train loss: 1.440, Global test loss: 1.973, Global test accuracy: 33.15
Round  44, Train loss: 1.587, Test loss: 2.530, Test accuracy: 29.72
Round  44, Global train loss: 1.587, Global test loss: 2.146, Global test accuracy: 27.17
Round  45, Train loss: 1.115, Test loss: 2.551, Test accuracy: 29.73
Round  45, Global train loss: 1.115, Global test loss: 1.788, Global test accuracy: 41.80
Round  46, Train loss: 1.459, Test loss: 2.612, Test accuracy: 29.64
Round  46, Global train loss: 1.459, Global test loss: 2.068, Global test accuracy: 29.11
Round  47, Train loss: 1.197, Test loss: 2.647, Test accuracy: 29.29
Round  47, Global train loss: 1.197, Global test loss: 2.042, Global test accuracy: 26.31
Round  48, Train loss: 1.053, Test loss: 2.710, Test accuracy: 29.36
Round  48, Global train loss: 1.053, Global test loss: 2.035, Global test accuracy: 24.38
Round  49, Train loss: 1.185, Test loss: 2.738, Test accuracy: 29.06
Round  49, Global train loss: 1.185, Global test loss: 2.036, Global test accuracy: 27.89
Round  50, Train loss: 0.968, Test loss: 2.772, Test accuracy: 28.76
Round  50, Global train loss: 0.968, Global test loss: 1.744, Global test accuracy: 43.26
Round  51, Train loss: 1.223, Test loss: 2.793, Test accuracy: 28.91
Round  51, Global train loss: 1.223, Global test loss: 1.970, Global test accuracy: 33.55
Round  52, Train loss: 1.211, Test loss: 2.828, Test accuracy: 28.84
Round  52, Global train loss: 1.211, Global test loss: 2.018, Global test accuracy: 32.39
Round  53, Train loss: 1.231, Test loss: 2.861, Test accuracy: 28.72
Round  53, Global train loss: 1.231, Global test loss: 2.033, Global test accuracy: 27.75
Round  54, Train loss: 1.127, Test loss: 2.885, Test accuracy: 28.80
Round  54, Global train loss: 1.127, Global test loss: 1.889, Global test accuracy: 34.34
Round  55, Train loss: 0.951, Test loss: 2.908, Test accuracy: 28.86
Round  55, Global train loss: 0.951, Global test loss: 1.912, Global test accuracy: 35.30
Round  56, Train loss: 0.979, Test loss: 2.985, Test accuracy: 28.68
Round  56, Global train loss: 0.979, Global test loss: 1.954, Global test accuracy: 32.17
Round  57, Train loss: 0.912, Test loss: 2.988, Test accuracy: 28.46
Round  57, Global train loss: 0.912, Global test loss: 1.985, Global test accuracy: 30.95
Round  58, Train loss: 1.087, Test loss: 3.037, Test accuracy: 28.43
Round  58, Global train loss: 1.087, Global test loss: 1.815, Global test accuracy: 39.95
Round  59, Train loss: 0.744, Test loss: 3.078, Test accuracy: 28.24
Round  59, Global train loss: 0.744, Global test loss: 1.886, Global test accuracy: 33.69
Round  60, Train loss: 1.130, Test loss: 3.103, Test accuracy: 28.21
Round  60, Global train loss: 1.130, Global test loss: 2.024, Global test accuracy: 29.53
Round  61, Train loss: 1.020, Test loss: 3.136, Test accuracy: 28.43
Round  61, Global train loss: 1.020, Global test loss: 2.024, Global test accuracy: 26.46
Round  62, Train loss: 0.968, Test loss: 3.173, Test accuracy: 28.38
Round  62, Global train loss: 0.968, Global test loss: 2.075, Global test accuracy: 27.42
Round  63, Train loss: 0.939, Test loss: 3.202, Test accuracy: 28.35
Round  63, Global train loss: 0.939, Global test loss: 2.177, Global test accuracy: 17.32
Round  64, Train loss: 0.911, Test loss: 3.228, Test accuracy: 28.46
Round  64, Global train loss: 0.911, Global test loss: 1.893, Global test accuracy: 36.08
Round  65, Train loss: 0.993, Test loss: 3.260, Test accuracy: 28.48
Round  65, Global train loss: 0.993, Global test loss: 2.154, Global test accuracy: 17.68
Round  66, Train loss: 0.848, Test loss: 3.302, Test accuracy: 28.23
Round  66, Global train loss: 0.848, Global test loss: 1.864, Global test accuracy: 38.88
Round  67, Train loss: 0.926, Test loss: 3.353, Test accuracy: 28.45
Round  67, Global train loss: 0.926, Global test loss: 1.840, Global test accuracy: 36.66
Round  68, Train loss: 0.792, Test loss: 3.405, Test accuracy: 28.25
Round  68, Global train loss: 0.792, Global test loss: 2.075, Global test accuracy: 22.93
Round  69, Train loss: 0.916, Test loss: 3.454, Test accuracy: 28.14
Round  69, Global train loss: 0.916, Global test loss: 2.001, Global test accuracy: 27.96
Round  70, Train loss: 0.848, Test loss: 3.474, Test accuracy: 28.09
Round  70, Global train loss: 0.848, Global test loss: 2.037, Global test accuracy: 25.73
Round  71, Train loss: 0.767, Test loss: 3.478, Test accuracy: 28.23
Round  71, Global train loss: 0.767, Global test loss: 1.848, Global test accuracy: 37.41
Round  72, Train loss: 0.853, Test loss: 3.500, Test accuracy: 28.35
Round  72, Global train loss: 0.853, Global test loss: 2.028, Global test accuracy: 28.17
Round  73, Train loss: 0.833, Test loss: 3.521, Test accuracy: 28.16
Round  73, Global train loss: 0.833, Global test loss: 1.991, Global test accuracy: 30.07
Round  74, Train loss: 1.042, Test loss: 3.564, Test accuracy: 28.13
Round  74, Global train loss: 1.042, Global test loss: 2.173, Global test accuracy: 22.30
Round  75, Train loss: 0.915, Test loss: 3.590, Test accuracy: 28.18
Round  75, Global train loss: 0.915, Global test loss: 1.972, Global test accuracy: 32.00
Round  76, Train loss: 0.952, Test loss: 3.633, Test accuracy: 27.72
Round  76, Global train loss: 0.952, Global test loss: 2.122, Global test accuracy: 24.63
Round  77, Train loss: 0.773, Test loss: 3.679, Test accuracy: 27.95
Round  77, Global train loss: 0.773, Global test loss: 1.915, Global test accuracy: 34.58
Round  78, Train loss: 0.852, Test loss: 3.715, Test accuracy: 28.11
Round  78, Global train loss: 0.852, Global test loss: 2.021, Global test accuracy: 28.37
Round  79, Train loss: 0.810, Test loss: 3.764, Test accuracy: 28.12
Round  79, Global train loss: 0.810, Global test loss: 1.974, Global test accuracy: 31.52
Round  80, Train loss: 0.794, Test loss: 3.803, Test accuracy: 28.07
Round  80, Global train loss: 0.794, Global test loss: 2.059, Global test accuracy: 27.61
Round  81, Train loss: 0.803, Test loss: 3.863, Test accuracy: 27.92
Round  81, Global train loss: 0.803, Global test loss: 2.126, Global test accuracy: 21.41
Round  82, Train loss: 0.826, Test loss: 3.875, Test accuracy: 27.93
Round  82, Global train loss: 0.826, Global test loss: 2.087, Global test accuracy: 27.79
Round  83, Train loss: 0.826, Test loss: 3.915, Test accuracy: 27.98
Round  83, Global train loss: 0.826, Global test loss: 2.109, Global test accuracy: 23.36
Round  84, Train loss: 0.811, Test loss: 3.955, Test accuracy: 27.74
Round  84, Global train loss: 0.811, Global test loss: 2.071, Global test accuracy: 27.16
Round  85, Train loss: 0.675, Test loss: 3.951, Test accuracy: 27.82
Round  85, Global train loss: 0.675, Global test loss: 1.830, Global test accuracy: 38.05
Round  86, Train loss: 0.817, Test loss: 4.055, Test accuracy: 27.52
Round  86, Global train loss: 0.817, Global test loss: 2.196, Global test accuracy: 16.67
Round  87, Train loss: 0.738, Test loss: 4.053, Test accuracy: 27.66
Round  87, Global train loss: 0.738, Global test loss: 2.022, Global test accuracy: 28.29
Round  88, Train loss: 0.742, Test loss: 4.093, Test accuracy: 27.55
Round  88, Global train loss: 0.742, Global test loss: 2.055, Global test accuracy: 29.08
Round  89, Train loss: 0.894, Test loss: 4.090, Test accuracy: 27.64
Round  89, Global train loss: 0.894, Global test loss: 2.198, Global test accuracy: 20.03
Round  90, Train loss: 0.689, Test loss: 4.113, Test accuracy: 27.68
Round  90, Global train loss: 0.689, Global test loss: 2.009, Global test accuracy: 28.89
Round  91, Train loss: 0.628, Test loss: 4.159, Test accuracy: 27.53
Round  91, Global train loss: 0.628, Global test loss: 1.953, Global test accuracy: 30.22
Round  92, Train loss: 0.649, Test loss: 4.150, Test accuracy: 27.82
Round  92, Global train loss: 0.649, Global test loss: 1.891, Global test accuracy: 35.47
Round  93, Train loss: 0.682, Test loss: 4.184, Test accuracy: 27.80
Round  93, Global train loss: 0.682, Global test loss: 2.009, Global test accuracy: 29.43
Round  94, Train loss: 0.719, Test loss: 4.247, Test accuracy: 27.45
Round  94, Global train loss: 0.719, Global test loss: 2.109, Global test accuracy: 21.79
Round  95, Train loss: 0.673, Test loss: 4.238, Test accuracy: 27.51
Round  95, Global train loss: 0.673, Global test loss: 2.081, Global test accuracy: 22.73
Round  96, Train loss: 0.500, Test loss: 4.251, Test accuracy: 27.64
Round  96, Global train loss: 0.500, Global test loss: 1.817, Global test accuracy: 36.99
Round  97, Train loss: 0.698, Test loss: 4.296, Test accuracy: 27.43
Round  97, Global train loss: 0.698, Global test loss: 2.185, Global test accuracy: 17.83
Round  98, Train loss: 0.720, Test loss: 4.333, Test accuracy: 27.31
Round  98, Global train loss: 0.720, Global test loss: 2.075, Global test accuracy: 26.70
Round  99, Train loss: 0.602, Test loss: 4.318, Test accuracy: 27.22
Round  99, Global train loss: 0.602, Global test loss: 2.132, Global test accuracy: 18.77
Final Round, Train loss: 0.419, Test loss: 5.119, Test accuracy: 27.61
Final Round, Global train loss: 0.419, Global test loss: 2.132, Global test accuracy: 18.77
Average accuracy final 10 rounds: 27.539749999999998 

Average global accuracy final 10 rounds: 26.88325 

6765.681962490082
[5.403868913650513, 10.807737827301025, 16.022246599197388, 21.23675537109375, 26.448389768600464, 31.660024166107178, 36.98959302902222, 42.319161891937256, 47.664671182632446, 53.01018047332764, 58.214881896972656, 63.419583320617676, 68.62662649154663, 73.83366966247559, 79.06451392173767, 84.29535818099976, 89.55500602722168, 94.8146538734436, 100.1009910106659, 105.38732814788818, 110.66128087043762, 115.93523359298706, 121.20793914794922, 126.48064470291138, 131.67301988601685, 136.86539506912231, 142.09781289100647, 147.33023071289062, 152.62560892105103, 157.92098712921143, 163.1957709789276, 168.4705548286438, 173.82049345970154, 179.17043209075928, 184.4921989440918, 189.81396579742432, 194.76585030555725, 199.71773481369019, 204.936261177063, 210.1547875404358, 215.4082670211792, 220.6617465019226, 225.90194272994995, 231.1421389579773, 236.50160717964172, 241.86107540130615, 247.05275177955627, 252.2444281578064, 257.4892055988312, 262.73398303985596, 267.94790053367615, 273.16181802749634, 278.36417269706726, 283.5665273666382, 288.69444966316223, 293.8223719596863, 299.0444014072418, 304.26643085479736, 309.4767153263092, 314.68699979782104, 319.91650199890137, 325.1460041999817, 330.36107206344604, 335.5761399269104, 340.8145709037781, 346.05300188064575, 351.26279187202454, 356.4725818634033, 361.6991853713989, 366.92578887939453, 372.12437653541565, 377.32296419143677, 382.4263987541199, 387.529833316803, 392.71921968460083, 397.9086060523987, 403.0903034210205, 408.27200078964233, 413.4425389766693, 418.6130771636963, 423.8140287399292, 429.0149803161621, 434.15521931648254, 439.295458316803, 444.4789307117462, 449.66240310668945, 454.46209359169006, 459.2617840766907, 463.7956418991089, 468.3294997215271, 472.85324239730835, 477.3769850730896, 481.8794367313385, 486.3818883895874, 490.90207529067993, 495.42226219177246, 499.9718883037567, 504.52151441574097, 509.0364406108856, 513.5513668060303, 518.0725405216217, 522.5937142372131, 527.1487255096436, 531.703736782074, 536.2821629047394, 540.8605890274048, 545.4335906505585, 550.0065922737122, 554.5381078720093, 559.0696234703064, 563.5875244140625, 568.1054253578186, 572.6616847515106, 577.2179441452026, 581.8024547100067, 586.3869652748108, 590.9315686225891, 595.4761719703674, 600.203355550766, 604.9305391311646, 609.5224123001099, 614.1142854690552, 618.751387834549, 623.3884902000427, 627.947812795639, 632.5071353912354, 637.0693800449371, 641.6316246986389, 646.2175776958466, 650.8035306930542, 655.3279361724854, 659.8523416519165, 664.3996574878693, 668.946973323822, 673.4921023845673, 678.0372314453125, 682.5911724567413, 687.1451134681702, 691.7683844566345, 696.3916554450989, 700.9563817977905, 705.5211081504822, 710.019936800003, 714.5187654495239, 719.1312739849091, 723.7437825202942, 728.3708102703094, 732.9978380203247, 737.7077629566193, 742.4176878929138, 747.017386674881, 751.6170854568481, 756.1134686470032, 760.6098518371582, 765.2566721439362, 769.9034924507141, 774.4418210983276, 778.9801497459412, 783.5025882720947, 788.0250267982483, 792.5733962059021, 797.1217656135559, 801.6593701839447, 806.1969747543335, 810.7572090625763, 815.3174433708191, 819.8614385128021, 824.4054336547852, 828.9657785892487, 833.5261235237122, 838.1076881885529, 842.6892528533936, 847.3292579650879, 851.9692630767822, 856.5499429702759, 861.1306228637695, 865.758152961731, 870.3856830596924, 874.9901888370514, 879.5946946144104, 884.226247549057, 888.8578004837036, 893.4864954948425, 898.1151905059814, 902.7874443531036, 907.4596982002258, 912.0296201705933, 916.5995421409607, 921.1793103218079, 925.759078502655, 930.3734622001648, 934.9878458976746, 939.5790376663208, 944.170229434967, 948.7707283496857, 953.3712272644043, 957.976934671402, 962.5826420783997, 967.1845257282257, 971.7864093780518, 974.1167500019073, 976.4470906257629]
[25.9175, 25.9175, 30.44, 30.44, 30.86, 30.86, 32.805, 32.805, 33.6825, 33.6825, 34.53, 34.53, 34.6125, 34.6125, 35.4275, 35.4275, 33.9975, 33.9975, 34.13, 34.13, 33.2425, 33.2425, 33.3875, 33.3875, 33.775, 33.775, 33.65, 33.65, 33.695, 33.695, 34.07, 34.07, 33.8425, 33.8425, 33.2975, 33.2975, 33.09, 33.09, 33.1475, 33.1475, 33.36, 33.36, 33.7475, 33.7475, 33.5, 33.5, 33.2425, 33.2425, 32.7275, 32.7275, 32.595, 32.595, 32.02, 32.02, 31.5875, 31.5875, 31.8325, 31.8325, 31.8275, 31.8275, 31.585, 31.585, 31.3525, 31.3525, 31.495, 31.495, 31.4425, 31.4425, 31.4325, 31.4325, 31.385, 31.385, 30.925, 30.925, 30.885, 30.885, 30.4275, 30.4275, 30.245, 30.245, 30.3775, 30.3775, 29.9525, 29.9525, 29.9075, 29.9075, 29.5975, 29.5975, 29.72, 29.72, 29.7325, 29.7325, 29.645, 29.645, 29.2925, 29.2925, 29.355, 29.355, 29.06, 29.06, 28.7575, 28.7575, 28.91, 28.91, 28.84, 28.84, 28.7225, 28.7225, 28.805, 28.805, 28.855, 28.855, 28.6775, 28.6775, 28.4575, 28.4575, 28.43, 28.43, 28.24, 28.24, 28.2125, 28.2125, 28.425, 28.425, 28.3775, 28.3775, 28.3525, 28.3525, 28.4575, 28.4575, 28.48, 28.48, 28.23, 28.23, 28.4475, 28.4475, 28.245, 28.245, 28.145, 28.145, 28.095, 28.095, 28.2325, 28.2325, 28.3475, 28.3475, 28.1575, 28.1575, 28.13, 28.13, 28.18, 28.18, 27.7225, 27.7225, 27.95, 27.95, 28.115, 28.115, 28.1225, 28.1225, 28.07, 28.07, 27.9175, 27.9175, 27.925, 27.925, 27.98, 27.98, 27.7375, 27.7375, 27.815, 27.815, 27.515, 27.515, 27.655, 27.655, 27.555, 27.555, 27.635, 27.635, 27.6825, 27.6825, 27.5275, 27.5275, 27.825, 27.825, 27.795, 27.795, 27.4475, 27.4475, 27.51, 27.51, 27.64, 27.64, 27.435, 27.435, 27.3125, 27.3125, 27.2225, 27.2225, 27.605, 27.605]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8630
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1910
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0810
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5750
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8065
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0155
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6970
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7660
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0685
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5920
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4900
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8655
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.4370
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3840
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7300
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.5890
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.125, Test loss: 2.064, Test accuracy: 30.55
Round   0, Global train loss: 2.125, Global test loss: 2.080, Global test accuracy: 31.95
Round   1, Train loss: 1.951, Test loss: 1.827, Test accuracy: 37.15
Round   1, Global train loss: 1.951, Global test loss: 1.747, Global test accuracy: 41.77
Round   2, Train loss: 1.813, Test loss: 1.717, Test accuracy: 39.96
Round   2, Global train loss: 1.813, Global test loss: 1.566, Global test accuracy: 46.15
Round   3, Train loss: 1.792, Test loss: 1.664, Test accuracy: 43.19
Round   3, Global train loss: 1.792, Global test loss: 1.577, Global test accuracy: 47.64
Round   4, Train loss: 1.727, Test loss: 1.627, Test accuracy: 44.38
Round   4, Global train loss: 1.727, Global test loss: 1.481, Global test accuracy: 50.50
Round   5, Train loss: 1.777, Test loss: 1.651, Test accuracy: 44.25
Round   5, Global train loss: 1.777, Global test loss: 1.514, Global test accuracy: 50.98
Round   6, Train loss: 1.756, Test loss: 1.624, Test accuracy: 45.15
Round   6, Global train loss: 1.756, Global test loss: 1.487, Global test accuracy: 53.24
Round   7, Train loss: 1.692, Test loss: 1.591, Test accuracy: 46.33
Round   7, Global train loss: 1.692, Global test loss: 1.425, Global test accuracy: 53.59
Round   8, Train loss: 1.485, Test loss: 1.560, Test accuracy: 47.43
Round   8, Global train loss: 1.485, Global test loss: 1.265, Global test accuracy: 57.04
Round   9, Train loss: 1.506, Test loss: 1.572, Test accuracy: 46.98
Round   9, Global train loss: 1.506, Global test loss: 1.256, Global test accuracy: 57.99
Round  10, Train loss: 1.620, Test loss: 1.562, Test accuracy: 47.82
Round  10, Global train loss: 1.620, Global test loss: 1.330, Global test accuracy: 60.07
Round  11, Train loss: 1.646, Test loss: 1.540, Test accuracy: 48.56
Round  11, Global train loss: 1.646, Global test loss: 1.313, Global test accuracy: 61.17
Round  12, Train loss: 1.592, Test loss: 1.503, Test accuracy: 50.26
Round  12, Global train loss: 1.592, Global test loss: 1.246, Global test accuracy: 63.10
Round  13, Train loss: 1.426, Test loss: 1.479, Test accuracy: 51.42
Round  13, Global train loss: 1.426, Global test loss: 1.159, Global test accuracy: 62.85
Round  14, Train loss: 1.389, Test loss: 1.459, Test accuracy: 52.15
Round  14, Global train loss: 1.389, Global test loss: 1.146, Global test accuracy: 63.42
Round  15, Train loss: 1.455, Test loss: 1.444, Test accuracy: 53.12
Round  15, Global train loss: 1.455, Global test loss: 1.156, Global test accuracy: 64.86
Round  16, Train loss: 1.475, Test loss: 1.437, Test accuracy: 53.58
Round  16, Global train loss: 1.475, Global test loss: 1.176, Global test accuracy: 65.00
Round  17, Train loss: 1.449, Test loss: 1.429, Test accuracy: 53.81
Round  17, Global train loss: 1.449, Global test loss: 1.179, Global test accuracy: 63.59
Round  18, Train loss: 1.489, Test loss: 1.416, Test accuracy: 54.29
Round  18, Global train loss: 1.489, Global test loss: 1.165, Global test accuracy: 65.88
Round  19, Train loss: 1.479, Test loss: 1.420, Test accuracy: 53.85
Round  19, Global train loss: 1.479, Global test loss: 1.187, Global test accuracy: 64.55
Round  20, Train loss: 1.338, Test loss: 1.425, Test accuracy: 53.79
Round  20, Global train loss: 1.338, Global test loss: 1.105, Global test accuracy: 66.15
Round  21, Train loss: 1.352, Test loss: 1.406, Test accuracy: 54.38
Round  21, Global train loss: 1.352, Global test loss: 1.078, Global test accuracy: 66.56
Round  22, Train loss: 1.411, Test loss: 1.390, Test accuracy: 55.10
Round  22, Global train loss: 1.411, Global test loss: 1.115, Global test accuracy: 67.31
Round  23, Train loss: 1.381, Test loss: 1.378, Test accuracy: 55.44
Round  23, Global train loss: 1.381, Global test loss: 1.091, Global test accuracy: 66.55
Round  24, Train loss: 1.213, Test loss: 1.383, Test accuracy: 55.35
Round  24, Global train loss: 1.213, Global test loss: 0.993, Global test accuracy: 68.58
Round  25, Train loss: 1.197, Test loss: 1.375, Test accuracy: 55.89
Round  25, Global train loss: 1.197, Global test loss: 1.005, Global test accuracy: 68.33
Round  26, Train loss: 1.236, Test loss: 1.373, Test accuracy: 56.01
Round  26, Global train loss: 1.236, Global test loss: 0.985, Global test accuracy: 69.41
Round  27, Train loss: 1.243, Test loss: 1.375, Test accuracy: 56.24
Round  27, Global train loss: 1.243, Global test loss: 1.011, Global test accuracy: 68.92
Round  28, Train loss: 1.334, Test loss: 1.371, Test accuracy: 56.31
Round  28, Global train loss: 1.334, Global test loss: 1.111, Global test accuracy: 67.43
Round  29, Train loss: 1.207, Test loss: 1.370, Test accuracy: 56.36
Round  29, Global train loss: 1.207, Global test loss: 0.997, Global test accuracy: 68.67
Round  30, Train loss: 1.260, Test loss: 1.379, Test accuracy: 56.30
Round  30, Global train loss: 1.260, Global test loss: 1.047, Global test accuracy: 68.62
Round  31, Train loss: 1.291, Test loss: 1.385, Test accuracy: 56.01
Round  31, Global train loss: 1.291, Global test loss: 1.058, Global test accuracy: 68.05
Round  32, Train loss: 1.358, Test loss: 1.391, Test accuracy: 55.97
Round  32, Global train loss: 1.358, Global test loss: 1.109, Global test accuracy: 67.36
Round  33, Train loss: 1.252, Test loss: 1.381, Test accuracy: 56.29
Round  33, Global train loss: 1.252, Global test loss: 1.028, Global test accuracy: 69.18
Round  34, Train loss: 1.301, Test loss: 1.387, Test accuracy: 56.27
Round  34, Global train loss: 1.301, Global test loss: 1.068, Global test accuracy: 67.89
Round  35, Train loss: 1.128, Test loss: 1.388, Test accuracy: 56.05
Round  35, Global train loss: 1.128, Global test loss: 0.998, Global test accuracy: 67.85
Round  36, Train loss: 1.050, Test loss: 1.383, Test accuracy: 56.42
Round  36, Global train loss: 1.050, Global test loss: 0.976, Global test accuracy: 68.76
Round  37, Train loss: 1.212, Test loss: 1.385, Test accuracy: 56.44
Round  37, Global train loss: 1.212, Global test loss: 1.052, Global test accuracy: 67.99
Round  38, Train loss: 1.250, Test loss: 1.397, Test accuracy: 56.35
Round  38, Global train loss: 1.250, Global test loss: 1.011, Global test accuracy: 69.51
Round  39, Train loss: 1.085, Test loss: 1.380, Test accuracy: 56.84
Round  39, Global train loss: 1.085, Global test loss: 0.953, Global test accuracy: 70.32
Round  40, Train loss: 1.125, Test loss: 1.372, Test accuracy: 57.13
Round  40, Global train loss: 1.125, Global test loss: 0.935, Global test accuracy: 70.96
Round  41, Train loss: 1.181, Test loss: 1.370, Test accuracy: 57.24
Round  41, Global train loss: 1.181, Global test loss: 0.981, Global test accuracy: 70.70
Round  42, Train loss: 1.139, Test loss: 1.374, Test accuracy: 57.41
Round  42, Global train loss: 1.139, Global test loss: 0.951, Global test accuracy: 70.21
Round  43, Train loss: 0.965, Test loss: 1.390, Test accuracy: 56.93
Round  43, Global train loss: 0.965, Global test loss: 0.900, Global test accuracy: 70.79
Round  44, Train loss: 0.988, Test loss: 1.388, Test accuracy: 56.90
Round  44, Global train loss: 0.988, Global test loss: 0.926, Global test accuracy: 70.84
Round  45, Train loss: 1.211, Test loss: 1.375, Test accuracy: 57.23
Round  45, Global train loss: 1.211, Global test loss: 0.980, Global test accuracy: 70.20
Round  46, Train loss: 1.233, Test loss: 1.378, Test accuracy: 57.13
Round  46, Global train loss: 1.233, Global test loss: 1.016, Global test accuracy: 70.22
Round  47, Train loss: 1.155, Test loss: 1.370, Test accuracy: 57.34
Round  47, Global train loss: 1.155, Global test loss: 0.973, Global test accuracy: 70.42
Round  48, Train loss: 1.097, Test loss: 1.368, Test accuracy: 57.48
Round  48, Global train loss: 1.097, Global test loss: 0.939, Global test accuracy: 70.56
Round  49, Train loss: 1.171, Test loss: 1.395, Test accuracy: 57.20
Round  49, Global train loss: 1.171, Global test loss: 1.015, Global test accuracy: 68.86
Round  50, Train loss: 1.277, Test loss: 1.395, Test accuracy: 57.04
Round  50, Global train loss: 1.277, Global test loss: 1.104, Global test accuracy: 68.11
Round  51, Train loss: 1.057, Test loss: 1.393, Test accuracy: 57.04
Round  51, Global train loss: 1.057, Global test loss: 0.974, Global test accuracy: 69.81
Round  52, Train loss: 1.216, Test loss: 1.404, Test accuracy: 56.93
Round  52, Global train loss: 1.216, Global test loss: 1.039, Global test accuracy: 68.36
Round  53, Train loss: 1.047, Test loss: 1.402, Test accuracy: 56.80
Round  53, Global train loss: 1.047, Global test loss: 0.962, Global test accuracy: 70.06
Round  54, Train loss: 0.988, Test loss: 1.406, Test accuracy: 57.02
Round  54, Global train loss: 0.988, Global test loss: 0.952, Global test accuracy: 70.13
Round  55, Train loss: 1.132, Test loss: 1.398, Test accuracy: 57.30
Round  55, Global train loss: 1.132, Global test loss: 1.017, Global test accuracy: 68.16
Round  56, Train loss: 1.006, Test loss: 1.412, Test accuracy: 57.52
Round  56, Global train loss: 1.006, Global test loss: 0.959, Global test accuracy: 69.62
Round  57, Train loss: 1.230, Test loss: 1.426, Test accuracy: 56.98
Round  57, Global train loss: 1.230, Global test loss: 1.103, Global test accuracy: 67.52
Round  58, Train loss: 1.013, Test loss: 1.429, Test accuracy: 56.88
Round  58, Global train loss: 1.013, Global test loss: 0.955, Global test accuracy: 70.42
Round  59, Train loss: 1.166, Test loss: 1.427, Test accuracy: 56.92
Round  59, Global train loss: 1.166, Global test loss: 1.097, Global test accuracy: 67.38
Round  60, Train loss: 1.127, Test loss: 1.423, Test accuracy: 56.93
Round  60, Global train loss: 1.127, Global test loss: 0.998, Global test accuracy: 69.53
Round  61, Train loss: 0.939, Test loss: 1.432, Test accuracy: 57.08
Round  61, Global train loss: 0.939, Global test loss: 0.959, Global test accuracy: 70.04
Round  62, Train loss: 0.949, Test loss: 1.429, Test accuracy: 56.94
Round  62, Global train loss: 0.949, Global test loss: 0.950, Global test accuracy: 70.70
Round  63, Train loss: 1.033, Test loss: 1.431, Test accuracy: 57.03
Round  63, Global train loss: 1.033, Global test loss: 0.964, Global test accuracy: 70.20
Round  64, Train loss: 1.034, Test loss: 1.425, Test accuracy: 57.20
Round  64, Global train loss: 1.034, Global test loss: 1.034, Global test accuracy: 67.68
Round  65, Train loss: 1.035, Test loss: 1.431, Test accuracy: 57.23
Round  65, Global train loss: 1.035, Global test loss: 0.951, Global test accuracy: 70.53
Round  66, Train loss: 1.069, Test loss: 1.442, Test accuracy: 57.29
Round  66, Global train loss: 1.069, Global test loss: 1.022, Global test accuracy: 69.02
Round  67, Train loss: 0.971, Test loss: 1.439, Test accuracy: 57.11
Round  67, Global train loss: 0.971, Global test loss: 0.927, Global test accuracy: 71.37
Round  68, Train loss: 0.944, Test loss: 1.440, Test accuracy: 57.23
Round  68, Global train loss: 0.944, Global test loss: 0.983, Global test accuracy: 69.39
Round  69, Train loss: 0.931, Test loss: 1.465, Test accuracy: 56.81
Round  69, Global train loss: 0.931, Global test loss: 0.949, Global test accuracy: 70.24
Round  70, Train loss: 1.044, Test loss: 1.468, Test accuracy: 56.77
Round  70, Global train loss: 1.044, Global test loss: 1.117, Global test accuracy: 65.22
Round  71, Train loss: 0.991, Test loss: 1.491, Test accuracy: 56.76
Round  71, Global train loss: 0.991, Global test loss: 1.101, Global test accuracy: 65.84
Round  72, Train loss: 1.010, Test loss: 1.485, Test accuracy: 57.04
Round  72, Global train loss: 1.010, Global test loss: 0.954, Global test accuracy: 70.62
Round  73, Train loss: 1.017, Test loss: 1.497, Test accuracy: 56.72
Round  73, Global train loss: 1.017, Global test loss: 0.977, Global test accuracy: 70.12
Round  74, Train loss: 1.063, Test loss: 1.504, Test accuracy: 56.52
Round  74, Global train loss: 1.063, Global test loss: 1.017, Global test accuracy: 68.52
Round  75, Train loss: 0.946, Test loss: 1.498, Test accuracy: 56.85
Round  75, Global train loss: 0.946, Global test loss: 0.981, Global test accuracy: 70.25
Round  76, Train loss: 1.062, Test loss: 1.509, Test accuracy: 56.62
Round  76, Global train loss: 1.062, Global test loss: 1.013, Global test accuracy: 69.47
Round  77, Train loss: 0.997, Test loss: 1.488, Test accuracy: 56.71
Round  77, Global train loss: 0.997, Global test loss: 1.069, Global test accuracy: 67.22
Round  78, Train loss: 0.995, Test loss: 1.487, Test accuracy: 56.83
Round  78, Global train loss: 0.995, Global test loss: 0.964, Global test accuracy: 70.53
Round  79, Train loss: 0.959, Test loss: 1.510, Test accuracy: 56.73
Round  79, Global train loss: 0.959, Global test loss: 1.004, Global test accuracy: 69.26
Round  80, Train loss: 0.947, Test loss: 1.500, Test accuracy: 57.08
Round  80, Global train loss: 0.947, Global test loss: 1.070, Global test accuracy: 67.17
Round  81, Train loss: 0.974, Test loss: 1.487, Test accuracy: 57.30
Round  81, Global train loss: 0.974, Global test loss: 0.997, Global test accuracy: 69.11
Round  82, Train loss: 0.940, Test loss: 1.490, Test accuracy: 57.01
Round  82, Global train loss: 0.940, Global test loss: 0.917, Global test accuracy: 71.64
Round  83, Train loss: 0.858, Test loss: 1.498, Test accuracy: 57.05
Round  83, Global train loss: 0.858, Global test loss: 0.969, Global test accuracy: 70.11
Round  84, Train loss: 0.950, Test loss: 1.496, Test accuracy: 56.84
Round  84, Global train loss: 0.950, Global test loss: 1.000, Global test accuracy: 69.33
Round  85, Train loss: 1.099, Test loss: 1.492, Test accuracy: 56.89
Round  85, Global train loss: 1.099, Global test loss: 1.077, Global test accuracy: 67.93
Round  86, Train loss: 0.944, Test loss: 1.472, Test accuracy: 56.93
Round  86, Global train loss: 0.944, Global test loss: 1.046, Global test accuracy: 67.94
Round  87, Train loss: 0.910, Test loss: 1.498, Test accuracy: 56.80
Round  87, Global train loss: 0.910, Global test loss: 1.028, Global test accuracy: 68.36
Round  88, Train loss: 0.995, Test loss: 1.499, Test accuracy: 56.91
Round  88, Global train loss: 0.995, Global test loss: 1.012, Global test accuracy: 68.34
Round  89, Train loss: 0.844, Test loss: 1.501, Test accuracy: 56.85
Round  89, Global train loss: 0.844, Global test loss: 0.971, Global test accuracy: 70.01
Round  90, Train loss: 0.891, Test loss: 1.502, Test accuracy: 56.94
Round  90, Global train loss: 0.891, Global test loss: 0.966, Global test accuracy: 69.53
Round  91, Train loss: 1.095, Test loss: 1.498, Test accuracy: 57.02
Round  91, Global train loss: 1.095, Global test loss: 1.010, Global test accuracy: 69.23
Round  92, Train loss: 0.867, Test loss: 1.505, Test accuracy: 57.05
Round  92, Global train loss: 0.867, Global test loss: 0.986, Global test accuracy: 69.73
Round  93, Train loss: 0.858, Test loss: 1.512, Test accuracy: 56.87
Round  93, Global train loss: 0.858, Global test loss: 1.017, Global test accuracy: 68.45
Round  94, Train loss: 0.780, Test loss: 1.508, Test accuracy: 57.20
Round  94, Global train loss: 0.780, Global test loss: 0.916, Global test accuracy: 71.54
Round  95, Train loss: 0.753, Test loss: 1.538, Test accuracy: 56.59
Round  95, Global train loss: 0.753, Global test loss: 0.923, Global test accuracy: 71.35
Round  96, Train loss: 0.989, Test loss: 1.523, Test accuracy: 56.66
Round  96, Global train loss: 0.989, Global test loss: 1.008, Global test accuracy: 68.15
Round  97, Train loss: 0.830, Test loss: 1.536, Test accuracy: 56.59
Round  97, Global train loss: 0.830, Global test loss: 0.987, Global test accuracy: 69.88
Round  98, Train loss: 0.986, Test loss: 1.514, Test accuracy: 56.99
Round  98, Global train loss: 0.986, Global test loss: 1.030, Global test accuracy: 68.91
Round  99, Train loss: 0.907, Test loss: 1.511, Test accuracy: 57.16
Round  99, Global train loss: 0.907, Global test loss: 0.959, Global test accuracy: 70.67
Final Round, Train loss: 0.638, Test loss: 1.715, Test accuracy: 57.37
Final Round, Global train loss: 0.638, Global test loss: 0.959, Global test accuracy: 70.67
Average accuracy final 10 rounds: 56.907 

Average global accuracy final 10 rounds: 69.744 

6342.206496477127
[4.608664035797119, 9.217328071594238, 13.75466799736023, 18.29200792312622, 22.824837684631348, 27.357667446136475, 31.898497104644775, 36.439326763153076, 41.005024671554565, 45.570722579956055, 50.06106615066528, 54.55140972137451, 59.00589418411255, 63.460378646850586, 67.94861936569214, 72.43686008453369, 76.91774320602417, 81.39862632751465, 85.89226508140564, 90.38590383529663, 94.85616540908813, 99.32642698287964, 103.79174709320068, 108.25706720352173, 112.71426939964294, 117.17147159576416, 121.67004227638245, 126.16861295700073, 130.46627187728882, 134.7639307975769, 139.13623213768005, 143.5085334777832, 147.87874245643616, 152.2489514350891, 156.59047675132751, 160.93200206756592, 165.47830939292908, 170.02461671829224, 174.5182077884674, 179.01179885864258, 183.35937190055847, 187.70694494247437, 192.12552905082703, 196.5441131591797, 200.89669609069824, 205.2492790222168, 209.64145851135254, 214.03363800048828, 218.4004201889038, 222.76720237731934, 227.2342245578766, 231.70124673843384, 236.0355703830719, 240.36989402770996, 244.80451488494873, 249.2391357421875, 253.60797381401062, 257.97681188583374, 262.44151735305786, 266.906222820282, 271.2866816520691, 275.6671404838562, 280.1405334472656, 284.61392641067505, 289.0216372013092, 293.42934799194336, 297.8236017227173, 302.2178554534912, 306.62331366539, 311.0287718772888, 315.4958279132843, 319.9628839492798, 324.3422770500183, 328.72167015075684, 333.11951661109924, 337.51736307144165, 341.92941308021545, 346.34146308898926, 350.7336325645447, 355.1258020401001, 359.5453853607178, 363.96496868133545, 368.32000970840454, 372.67505073547363, 377.05774903297424, 381.44044733047485, 385.8016676902771, 390.16288805007935, 394.68532395362854, 399.20775985717773, 403.4908356666565, 407.77391147613525, 412.0635643005371, 416.35321712493896, 420.67509484291077, 424.99697256088257, 429.3070363998413, 433.61710023880005, 437.9182095527649, 442.21931886672974, 446.59404397010803, 450.9687690734863, 455.2604789733887, 459.552188873291, 464.12056970596313, 468.68895053863525, 473.05953431129456, 477.43011808395386, 481.7569169998169, 486.08371591567993, 490.4338095188141, 494.78390312194824, 499.2343020439148, 503.68470096588135, 508.04677510261536, 512.4088492393494, 516.8807592391968, 521.3526692390442, 525.6893751621246, 530.0260810852051, 534.3773517608643, 538.7286224365234, 543.0661480426788, 547.4036736488342, 551.8130812644958, 556.2224888801575, 560.55277967453, 564.8830704689026, 569.4270668029785, 573.9710631370544, 578.2968554496765, 582.6226477622986, 586.9820685386658, 591.341489315033, 595.7191960811615, 600.09690284729, 604.6000957489014, 609.1032886505127, 613.4514517784119, 617.799614906311, 622.1464140415192, 626.4932131767273, 630.8234565258026, 635.1536998748779, 639.6646416187286, 644.1755833625793, 648.7206447124481, 653.2657060623169, 657.5937707424164, 661.9218354225159, 666.3556880950928, 670.7895407676697, 675.0599257946014, 679.3303108215332, 683.655957698822, 687.9816045761108, 692.294956445694, 696.6083083152771, 700.9027225971222, 705.1971368789673, 709.4861934185028, 713.7752499580383, 718.2781875133514, 722.7811250686646, 727.3187534809113, 731.856381893158, 736.2426464557648, 740.6289110183716, 745.1624500751495, 749.6959891319275, 754.1712136268616, 758.6464381217957, 763.6592698097229, 768.6721014976501, 773.7747104167938, 778.8773193359375, 783.9999635219574, 789.1226077079773, 794.2885603904724, 799.4545130729675, 804.560489654541, 809.6664662361145, 814.7013518810272, 819.7362375259399, 824.7404744625092, 829.7447113990784, 834.7895793914795, 839.8344473838806, 844.5261073112488, 849.217767238617, 854.2093572616577, 859.2009472846985, 864.2084414958954, 869.2159357070923, 874.2603094577789, 879.3046832084656, 884.2775509357452, 889.2504186630249, 894.1464612483978, 899.0425038337708, 901.611278295517, 904.1800527572632]
[30.545, 30.545, 37.15, 37.15, 39.9625, 39.9625, 43.19, 43.19, 44.38, 44.38, 44.25, 44.25, 45.15, 45.15, 46.325, 46.325, 47.4275, 47.4275, 46.985, 46.985, 47.8175, 47.8175, 48.56, 48.56, 50.255, 50.255, 51.42, 51.42, 52.145, 52.145, 53.1175, 53.1175, 53.575, 53.575, 53.815, 53.815, 54.29, 54.29, 53.85, 53.85, 53.7925, 53.7925, 54.38, 54.38, 55.1025, 55.1025, 55.4375, 55.4375, 55.3475, 55.3475, 55.8875, 55.8875, 56.005, 56.005, 56.24, 56.24, 56.3125, 56.3125, 56.36, 56.36, 56.2975, 56.2975, 56.005, 56.005, 55.97, 55.97, 56.2875, 56.2875, 56.275, 56.275, 56.0475, 56.0475, 56.4225, 56.4225, 56.44, 56.44, 56.3525, 56.3525, 56.845, 56.845, 57.1325, 57.1325, 57.2425, 57.2425, 57.4125, 57.4125, 56.93, 56.93, 56.9025, 56.9025, 57.235, 57.235, 57.1275, 57.1275, 57.34, 57.34, 57.485, 57.485, 57.2025, 57.2025, 57.04, 57.04, 57.04, 57.04, 56.93, 56.93, 56.8025, 56.8025, 57.015, 57.015, 57.2975, 57.2975, 57.52, 57.52, 56.9775, 56.9775, 56.875, 56.875, 56.925, 56.925, 56.9275, 56.9275, 57.0775, 57.0775, 56.9425, 56.9425, 57.035, 57.035, 57.1975, 57.1975, 57.225, 57.225, 57.2925, 57.2925, 57.11, 57.11, 57.2325, 57.2325, 56.81, 56.81, 56.775, 56.775, 56.7575, 56.7575, 57.0425, 57.0425, 56.7175, 56.7175, 56.525, 56.525, 56.8475, 56.8475, 56.615, 56.615, 56.7075, 56.7075, 56.83, 56.83, 56.725, 56.725, 57.08, 57.08, 57.305, 57.305, 57.0125, 57.0125, 57.045, 57.045, 56.84, 56.84, 56.8925, 56.8925, 56.93, 56.93, 56.8025, 56.8025, 56.9125, 56.9125, 56.855, 56.855, 56.9425, 56.9425, 57.025, 57.025, 57.045, 57.045, 56.865, 56.865, 57.1975, 57.1975, 56.59, 56.59, 56.665, 56.665, 56.595, 56.595, 56.99, 56.99, 57.155, 57.155, 57.3725, 57.3725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8615
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2680
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0645
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5840
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7870
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1280
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6880
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7685
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1905
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6375
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5435
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8675
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.3275
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.5485
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7110
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3605
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.701, Test loss: 2.000, Test accuracy: 23.21
Round   0, Global train loss: 1.701, Global test loss: 2.223, Global test accuracy: 16.57
Round   1, Train loss: 1.131, Test loss: 1.716, Test accuracy: 36.86
Round   1, Global train loss: 1.131, Global test loss: 2.256, Global test accuracy: 19.18
Round   2, Train loss: 1.432, Test loss: 1.450, Test accuracy: 47.48
Round   2, Global train loss: 1.432, Global test loss: 2.061, Global test accuracy: 26.11
Round   3, Train loss: 1.287, Test loss: 1.321, Test accuracy: 53.58
Round   3, Global train loss: 1.287, Global test loss: 1.891, Global test accuracy: 35.53
Round   4, Train loss: 1.309, Test loss: 1.283, Test accuracy: 55.10
Round   4, Global train loss: 1.309, Global test loss: 1.939, Global test accuracy: 36.49
Round   5, Train loss: 1.418, Test loss: 1.209, Test accuracy: 57.18
Round   5, Global train loss: 1.418, Global test loss: 1.894, Global test accuracy: 37.37
Round   6, Train loss: 1.339, Test loss: 1.181, Test accuracy: 60.02
Round   6, Global train loss: 1.339, Global test loss: 1.876, Global test accuracy: 37.92
Round   7, Train loss: 1.360, Test loss: 1.159, Test accuracy: 61.50
Round   7, Global train loss: 1.360, Global test loss: 1.826, Global test accuracy: 39.97
Round   8, Train loss: 1.086, Test loss: 1.104, Test accuracy: 63.51
Round   8, Global train loss: 1.086, Global test loss: 1.746, Global test accuracy: 40.76
Round   9, Train loss: 1.140, Test loss: 1.087, Test accuracy: 64.98
Round   9, Global train loss: 1.140, Global test loss: 1.783, Global test accuracy: 44.63
Round  10, Train loss: 1.071, Test loss: 1.084, Test accuracy: 65.70
Round  10, Global train loss: 1.071, Global test loss: 1.738, Global test accuracy: 44.52
Round  11, Train loss: 1.268, Test loss: 1.096, Test accuracy: 63.67
Round  11, Global train loss: 1.268, Global test loss: 1.762, Global test accuracy: 39.49
Round  12, Train loss: 1.145, Test loss: 1.098, Test accuracy: 64.34
Round  12, Global train loss: 1.145, Global test loss: 1.804, Global test accuracy: 40.38
Round  13, Train loss: 1.017, Test loss: 1.073, Test accuracy: 64.93
Round  13, Global train loss: 1.017, Global test loss: 1.753, Global test accuracy: 42.17
Round  14, Train loss: 1.177, Test loss: 1.030, Test accuracy: 66.65
Round  14, Global train loss: 1.177, Global test loss: 1.733, Global test accuracy: 44.52
Round  15, Train loss: 1.245, Test loss: 1.036, Test accuracy: 66.30
Round  15, Global train loss: 1.245, Global test loss: 1.711, Global test accuracy: 45.02
Round  16, Train loss: 0.921, Test loss: 1.024, Test accuracy: 66.12
Round  16, Global train loss: 0.921, Global test loss: 1.606, Global test accuracy: 46.43
Round  17, Train loss: 1.035, Test loss: 1.024, Test accuracy: 66.35
Round  17, Global train loss: 1.035, Global test loss: 1.653, Global test accuracy: 43.68
Round  18, Train loss: 1.254, Test loss: 1.035, Test accuracy: 66.28
Round  18, Global train loss: 1.254, Global test loss: 1.803, Global test accuracy: 38.56
Round  19, Train loss: 1.029, Test loss: 1.053, Test accuracy: 66.36
Round  19, Global train loss: 1.029, Global test loss: 1.665, Global test accuracy: 44.25
Round  20, Train loss: 1.118, Test loss: 1.017, Test accuracy: 67.65
Round  20, Global train loss: 1.118, Global test loss: 1.665, Global test accuracy: 44.77
Round  21, Train loss: 1.106, Test loss: 1.018, Test accuracy: 67.73
Round  21, Global train loss: 1.106, Global test loss: 1.666, Global test accuracy: 43.73
Round  22, Train loss: 1.259, Test loss: 1.022, Test accuracy: 67.83
Round  22, Global train loss: 1.259, Global test loss: 1.678, Global test accuracy: 47.13
Round  23, Train loss: 1.124, Test loss: 1.011, Test accuracy: 67.59
Round  23, Global train loss: 1.124, Global test loss: 1.684, Global test accuracy: 43.94
Round  24, Train loss: 1.170, Test loss: 1.002, Test accuracy: 67.82
Round  24, Global train loss: 1.170, Global test loss: 1.779, Global test accuracy: 39.27
Round  25, Train loss: 1.073, Test loss: 0.967, Test accuracy: 69.00
Round  25, Global train loss: 1.073, Global test loss: 1.623, Global test accuracy: 46.98
Round  26, Train loss: 0.874, Test loss: 0.968, Test accuracy: 69.28
Round  26, Global train loss: 0.874, Global test loss: 1.752, Global test accuracy: 39.39
Round  27, Train loss: 1.138, Test loss: 0.973, Test accuracy: 69.32
Round  27, Global train loss: 1.138, Global test loss: 1.578, Global test accuracy: 49.08
Round  28, Train loss: 0.734, Test loss: 0.973, Test accuracy: 68.89
Round  28, Global train loss: 0.734, Global test loss: 1.593, Global test accuracy: 46.06
Round  29, Train loss: 0.906, Test loss: 0.960, Test accuracy: 69.10
Round  29, Global train loss: 0.906, Global test loss: 1.670, Global test accuracy: 48.17
Round  30, Train loss: 1.286, Test loss: 0.991, Test accuracy: 68.24
Round  30, Global train loss: 1.286, Global test loss: 1.619, Global test accuracy: 49.90
Round  31, Train loss: 0.976, Test loss: 0.968, Test accuracy: 68.86
Round  31, Global train loss: 0.976, Global test loss: 1.577, Global test accuracy: 49.33
Round  32, Train loss: 1.084, Test loss: 0.985, Test accuracy: 68.75
Round  32, Global train loss: 1.084, Global test loss: 1.652, Global test accuracy: 43.78
Round  33, Train loss: 1.132, Test loss: 0.985, Test accuracy: 68.35
Round  33, Global train loss: 1.132, Global test loss: 1.597, Global test accuracy: 47.33
Round  34, Train loss: 0.859, Test loss: 0.981, Test accuracy: 68.53
Round  34, Global train loss: 0.859, Global test loss: 1.567, Global test accuracy: 48.86
Round  35, Train loss: 1.134, Test loss: 0.973, Test accuracy: 68.64
Round  35, Global train loss: 1.134, Global test loss: 1.624, Global test accuracy: 45.11
Round  36, Train loss: 0.943, Test loss: 0.976, Test accuracy: 68.56
Round  36, Global train loss: 0.943, Global test loss: 1.689, Global test accuracy: 43.55
Round  37, Train loss: 0.945, Test loss: 0.951, Test accuracy: 69.50
Round  37, Global train loss: 0.945, Global test loss: 1.602, Global test accuracy: 45.65
Round  38, Train loss: 1.132, Test loss: 0.972, Test accuracy: 68.62
Round  38, Global train loss: 1.132, Global test loss: 1.671, Global test accuracy: 46.07
Round  39, Train loss: 0.944, Test loss: 0.977, Test accuracy: 68.25
Round  39, Global train loss: 0.944, Global test loss: 1.615, Global test accuracy: 46.78
Round  40, Train loss: 1.075, Test loss: 0.945, Test accuracy: 70.02
Round  40, Global train loss: 1.075, Global test loss: 1.671, Global test accuracy: 43.83
Round  41, Train loss: 1.193, Test loss: 0.965, Test accuracy: 70.07
Round  41, Global train loss: 1.193, Global test loss: 1.664, Global test accuracy: 46.47
Round  42, Train loss: 1.272, Test loss: 0.990, Test accuracy: 68.62
Round  42, Global train loss: 1.272, Global test loss: 1.844, Global test accuracy: 38.15
Round  43, Train loss: 1.253, Test loss: 1.004, Test accuracy: 68.43
Round  43, Global train loss: 1.253, Global test loss: 1.802, Global test accuracy: 39.72
Round  44, Train loss: 1.291, Test loss: 0.996, Test accuracy: 68.47
Round  44, Global train loss: 1.291, Global test loss: 1.963, Global test accuracy: 33.72
Round  45, Train loss: 1.050, Test loss: 0.984, Test accuracy: 68.71
Round  45, Global train loss: 1.050, Global test loss: 1.654, Global test accuracy: 44.75
Round  46, Train loss: 0.735, Test loss: 0.989, Test accuracy: 68.37
Round  46, Global train loss: 0.735, Global test loss: 1.587, Global test accuracy: 48.98
Round  47, Train loss: 0.785, Test loss: 0.998, Test accuracy: 68.12
Round  47, Global train loss: 0.785, Global test loss: 1.715, Global test accuracy: 44.73
Round  48, Train loss: 1.029, Test loss: 0.988, Test accuracy: 68.79
Round  48, Global train loss: 1.029, Global test loss: 1.604, Global test accuracy: 46.84
Round  49, Train loss: 0.893, Test loss: 0.976, Test accuracy: 69.11
Round  49, Global train loss: 0.893, Global test loss: 1.744, Global test accuracy: 43.88
Round  50, Train loss: 1.038, Test loss: 0.996, Test accuracy: 68.28
Round  50, Global train loss: 1.038, Global test loss: 1.707, Global test accuracy: 42.69
Round  51, Train loss: 0.739, Test loss: 0.987, Test accuracy: 68.83
Round  51, Global train loss: 0.739, Global test loss: 1.573, Global test accuracy: 47.33
Round  52, Train loss: 0.748, Test loss: 0.999, Test accuracy: 68.88
Round  52, Global train loss: 0.748, Global test loss: 1.537, Global test accuracy: 48.44
Round  53, Train loss: 1.035, Test loss: 0.980, Test accuracy: 69.47
Round  53, Global train loss: 1.035, Global test loss: 1.625, Global test accuracy: 46.33
Round  54, Train loss: 0.786, Test loss: 1.004, Test accuracy: 69.16
Round  54, Global train loss: 0.786, Global test loss: 1.677, Global test accuracy: 44.67
Round  55, Train loss: 0.982, Test loss: 1.009, Test accuracy: 68.53
Round  55, Global train loss: 0.982, Global test loss: 1.721, Global test accuracy: 42.74
Round  56, Train loss: 0.876, Test loss: 0.995, Test accuracy: 69.62
Round  56, Global train loss: 0.876, Global test loss: 1.865, Global test accuracy: 40.18
Round  57, Train loss: 0.914, Test loss: 1.005, Test accuracy: 68.94
Round  57, Global train loss: 0.914, Global test loss: 1.582, Global test accuracy: 48.69
Round  58, Train loss: 0.718, Test loss: 1.006, Test accuracy: 68.93
Round  58, Global train loss: 0.718, Global test loss: 1.533, Global test accuracy: 49.17
Round  59, Train loss: 1.090, Test loss: 0.984, Test accuracy: 69.79
Round  59, Global train loss: 1.090, Global test loss: 1.650, Global test accuracy: 45.71
Round  60, Train loss: 0.929, Test loss: 0.969, Test accuracy: 69.78
Round  60, Global train loss: 0.929, Global test loss: 1.635, Global test accuracy: 45.73
Round  61, Train loss: 0.707, Test loss: 0.982, Test accuracy: 69.49
Round  61, Global train loss: 0.707, Global test loss: 1.718, Global test accuracy: 46.43
Round  62, Train loss: 0.981, Test loss: 0.990, Test accuracy: 69.21
Round  62, Global train loss: 0.981, Global test loss: 1.699, Global test accuracy: 45.48
Round  63, Train loss: 0.845, Test loss: 0.995, Test accuracy: 69.42
Round  63, Global train loss: 0.845, Global test loss: 1.757, Global test accuracy: 44.71
Round  64, Train loss: 0.917, Test loss: 1.022, Test accuracy: 68.23
Round  64, Global train loss: 0.917, Global test loss: 1.699, Global test accuracy: 44.64
Round  65, Train loss: 0.762, Test loss: 1.032, Test accuracy: 67.92
Round  65, Global train loss: 0.762, Global test loss: 1.759, Global test accuracy: 45.12
Round  66, Train loss: 0.991, Test loss: 1.053, Test accuracy: 67.16
Round  66, Global train loss: 0.991, Global test loss: 1.731, Global test accuracy: 43.18
Round  67, Train loss: 0.924, Test loss: 1.041, Test accuracy: 67.67
Round  67, Global train loss: 0.924, Global test loss: 1.805, Global test accuracy: 40.67
Round  68, Train loss: 0.816, Test loss: 1.038, Test accuracy: 68.17
Round  68, Global train loss: 0.816, Global test loss: 1.774, Global test accuracy: 44.52
Round  69, Train loss: 0.693, Test loss: 1.023, Test accuracy: 67.92
Round  69, Global train loss: 0.693, Global test loss: 1.786, Global test accuracy: 46.92
Round  70, Train loss: 0.728, Test loss: 1.020, Test accuracy: 67.90
Round  70, Global train loss: 0.728, Global test loss: 1.701, Global test accuracy: 45.64
Round  71, Train loss: 0.727, Test loss: 1.049, Test accuracy: 68.87
Round  71, Global train loss: 0.727, Global test loss: 1.592, Global test accuracy: 47.67
Round  72, Train loss: 0.825, Test loss: 1.053, Test accuracy: 68.79
Round  72, Global train loss: 0.825, Global test loss: 1.640, Global test accuracy: 47.15
Round  73, Train loss: 0.968, Test loss: 1.082, Test accuracy: 67.69
Round  73, Global train loss: 0.968, Global test loss: 1.703, Global test accuracy: 44.07
Round  74, Train loss: 0.849, Test loss: 1.112, Test accuracy: 67.18
Round  74, Global train loss: 0.849, Global test loss: 1.738, Global test accuracy: 41.72
Round  75, Train loss: 0.740, Test loss: 1.104, Test accuracy: 67.11
Round  75, Global train loss: 0.740, Global test loss: 1.695, Global test accuracy: 46.77
Round  76, Train loss: 0.950, Test loss: 1.100, Test accuracy: 66.58
Round  76, Global train loss: 0.950, Global test loss: 1.713, Global test accuracy: 44.33
Round  77, Train loss: 0.750, Test loss: 1.089, Test accuracy: 66.72
Round  77, Global train loss: 0.750, Global test loss: 1.771, Global test accuracy: 44.31
Round  78, Train loss: 0.504, Test loss: 1.086, Test accuracy: 66.66
Round  78, Global train loss: 0.504, Global test loss: 1.812, Global test accuracy: 46.32
Round  79, Train loss: 0.860, Test loss: 1.095, Test accuracy: 66.41
Round  79, Global train loss: 0.860, Global test loss: 1.726, Global test accuracy: 45.23
Round  80, Train loss: 0.774, Test loss: 1.111, Test accuracy: 66.26
Round  80, Global train loss: 0.774, Global test loss: 1.925, Global test accuracy: 42.62
Round  81, Train loss: 0.743, Test loss: 1.112, Test accuracy: 66.26
Round  81, Global train loss: 0.743, Global test loss: 1.750, Global test accuracy: 45.41
Round  82, Train loss: 0.792, Test loss: 1.141, Test accuracy: 65.77
Round  82, Global train loss: 0.792, Global test loss: 1.780, Global test accuracy: 45.60
Round  83, Train loss: 0.772, Test loss: 1.150, Test accuracy: 65.38
Round  83, Global train loss: 0.772, Global test loss: 1.785, Global test accuracy: 43.79
Round  84, Train loss: 0.793, Test loss: 1.142, Test accuracy: 64.96
Round  84, Global train loss: 0.793, Global test loss: 1.793, Global test accuracy: 44.14
Round  85, Train loss: 0.745, Test loss: 1.145, Test accuracy: 65.83
Round  85, Global train loss: 0.745, Global test loss: 2.001, Global test accuracy: 40.67
Round  86, Train loss: 0.743, Test loss: 1.148, Test accuracy: 66.86
Round  86, Global train loss: 0.743, Global test loss: 1.952, Global test accuracy: 41.16
Round  87, Train loss: 0.769, Test loss: 1.166, Test accuracy: 66.40
Round  87, Global train loss: 0.769, Global test loss: 1.843, Global test accuracy: 42.06
Round  88, Train loss: 0.823, Test loss: 1.193, Test accuracy: 66.13
Round  88, Global train loss: 0.823, Global test loss: 1.921, Global test accuracy: 42.61
Round  89, Train loss: 0.717, Test loss: 1.182, Test accuracy: 65.90
Round  89, Global train loss: 0.717, Global test loss: 1.980, Global test accuracy: 41.37
Round  90, Train loss: 0.725, Test loss: 1.177, Test accuracy: 66.02
Round  90, Global train loss: 0.725, Global test loss: 1.766, Global test accuracy: 45.19
Round  91, Train loss: 0.695, Test loss: 1.157, Test accuracy: 66.03
Round  91, Global train loss: 0.695, Global test loss: 1.857, Global test accuracy: 42.52
Round  92, Train loss: 0.589, Test loss: 1.261, Test accuracy: 65.09
Round  92, Global train loss: 0.589, Global test loss: 2.196, Global test accuracy: 40.31
Round  93, Train loss: 0.623, Test loss: 1.232, Test accuracy: 65.28
Round  93, Global train loss: 0.623, Global test loss: 1.991, Global test accuracy: 41.77
Round  94, Train loss: 0.777, Test loss: 1.241, Test accuracy: 65.88
Round  94, Global train loss: 0.777, Global test loss: 1.898, Global test accuracy: 43.07
Round  95, Train loss: 0.519, Test loss: 1.293, Test accuracy: 65.57
Round  95, Global train loss: 0.519, Global test loss: 2.073, Global test accuracy: 44.79
Round  96, Train loss: 0.648, Test loss: 1.266, Test accuracy: 65.74
Round  96, Global train loss: 0.648, Global test loss: 2.532, Global test accuracy: 39.52
Round  97, Train loss: 0.592, Test loss: 1.237, Test accuracy: 65.96
Round  97, Global train loss: 0.592, Global test loss: 1.873, Global test accuracy: 44.86
Round  98, Train loss: 0.747, Test loss: 1.213, Test accuracy: 66.11
Round  98, Global train loss: 0.747, Global test loss: 1.889, Global test accuracy: 42.28
Round  99, Train loss: 0.464, Test loss: 1.264, Test accuracy: 65.58
Round  99, Global train loss: 0.464, Global test loss: 2.264, Global test accuracy: 40.76
Final Round, Train loss: 0.559, Test loss: 1.346, Test accuracy: 65.86
Final Round, Global train loss: 0.559, Global test loss: 2.264, Global test accuracy: 40.76
Average accuracy final 10 rounds: 65.725 

Average global accuracy final 10 rounds: 42.50583333333333 

2076.5718116760254
[1.754720687866211, 3.509441375732422, 4.968732595443726, 6.428023815155029, 7.9898905754089355, 9.551757335662842, 11.122189998626709, 12.692622661590576, 14.309816837310791, 15.927011013031006, 17.536943435668945, 19.146875858306885, 20.7718665599823, 22.396857261657715, 24.064053773880005, 25.731250286102295, 27.36817240715027, 29.005094528198242, 30.67083430290222, 32.3365740776062, 34.00917840003967, 35.681782722473145, 37.32829976081848, 38.97481679916382, 40.621431827545166, 42.268046855926514, 43.8519811630249, 45.43591547012329, 46.84022331237793, 48.24453115463257, 49.65630078315735, 51.06807041168213, 52.47150707244873, 53.87494373321533, 55.332801818847656, 56.79065990447998, 58.203487396240234, 59.61631488800049, 61.06412124633789, 62.51192760467529, 64.02784657478333, 65.54376554489136, 67.00593209266663, 68.4680986404419, 69.89124131202698, 71.31438398361206, 72.77893018722534, 74.24347639083862, 75.68006324768066, 77.1166501045227, 78.62734460830688, 80.13803911209106, 81.57942748069763, 83.0208158493042, 84.53950142860413, 86.05818700790405, 87.53006863594055, 89.00195026397705, 90.47265005111694, 91.94334983825684, 93.47260046005249, 95.00185108184814, 96.48211717605591, 97.96238327026367, 99.47975826263428, 100.99713325500488, 102.49923729896545, 104.00134134292603, 105.43378949165344, 106.86623764038086, 108.27631211280823, 109.6863865852356, 111.13277840614319, 112.57917022705078, 114.09056854248047, 115.60196685791016, 117.10832905769348, 118.6146912574768, 120.07301306724548, 121.53133487701416, 122.99709129333496, 124.46284770965576, 125.93912672996521, 127.41540575027466, 128.92915725708008, 130.4429087638855, 131.88683199882507, 133.33075523376465, 134.74626088142395, 136.16176652908325, 137.5799150466919, 138.99806356430054, 140.4004786014557, 141.80289363861084, 143.22788882255554, 144.65288400650024, 146.06141567230225, 147.46994733810425, 148.90274024009705, 150.33553314208984, 151.76616024971008, 153.19678735733032, 154.62862610816956, 156.0604648590088, 157.55155110359192, 159.04263734817505, 160.64927887916565, 162.25592041015625, 163.8882257938385, 165.52053117752075, 167.22094988822937, 168.921368598938, 170.5502574443817, 172.17914628982544, 173.80618357658386, 175.43322086334229, 177.0509626865387, 178.6687045097351, 180.28932452201843, 181.90994453430176, 183.5173647403717, 185.12478494644165, 186.74045586585999, 188.35612678527832, 189.9590961933136, 191.56206560134888, 193.1880600452423, 194.81405448913574, 196.4082944393158, 198.00253438949585, 199.60974764823914, 201.21696090698242, 202.83726525306702, 204.4575695991516, 206.0744013786316, 207.69123315811157, 209.29651618003845, 210.90179920196533, 212.5180745124817, 214.13434982299805, 215.7430076599121, 217.35166549682617, 218.95482683181763, 220.55798816680908, 222.18575382232666, 223.81351947784424, 225.41404247283936, 227.01456546783447, 228.61891531944275, 230.22326517105103, 231.8434157371521, 233.46356630325317, 235.06013178825378, 236.6566972732544, 238.28975200653076, 239.92280673980713, 241.53594970703125, 243.14909267425537, 244.76489925384521, 246.38070583343506, 247.9699091911316, 249.55911254882812, 251.18095803260803, 252.80280351638794, 254.4077708721161, 256.01273822784424, 257.62341833114624, 259.23409843444824, 260.8437910079956, 262.45348358154297, 264.04163932800293, 265.6297950744629, 267.31602478027344, 269.002254486084, 270.64182925224304, 272.2814040184021, 273.97272515296936, 275.6640462875366, 277.2847421169281, 278.9054379463196, 280.53658533096313, 282.1677327156067, 283.8625752925873, 285.55741786956787, 287.2119255065918, 288.8664331436157, 290.5557949542999, 292.24515676498413, 293.907461643219, 295.56976652145386, 297.1823351383209, 298.794903755188, 300.4294011592865, 302.063898563385, 303.73161458969116, 305.3993306159973, 307.0276663303375, 308.65600204467773, 310.2565176486969, 311.85703325271606, 314.5775637626648, 317.2980942726135]
[23.208333333333332, 23.208333333333332, 36.858333333333334, 36.858333333333334, 47.475, 47.475, 53.583333333333336, 53.583333333333336, 55.1, 55.1, 57.18333333333333, 57.18333333333333, 60.025, 60.025, 61.5, 61.5, 63.50833333333333, 63.50833333333333, 64.98333333333333, 64.98333333333333, 65.7, 65.7, 63.675, 63.675, 64.34166666666667, 64.34166666666667, 64.93333333333334, 64.93333333333334, 66.65, 66.65, 66.3, 66.3, 66.125, 66.125, 66.35, 66.35, 66.28333333333333, 66.28333333333333, 66.35833333333333, 66.35833333333333, 67.65, 67.65, 67.73333333333333, 67.73333333333333, 67.825, 67.825, 67.59166666666667, 67.59166666666667, 67.81666666666666, 67.81666666666666, 69.0, 69.0, 69.275, 69.275, 69.31666666666666, 69.31666666666666, 68.89166666666667, 68.89166666666667, 69.1, 69.1, 68.24166666666666, 68.24166666666666, 68.85833333333333, 68.85833333333333, 68.75, 68.75, 68.35, 68.35, 68.53333333333333, 68.53333333333333, 68.64166666666667, 68.64166666666667, 68.55833333333334, 68.55833333333334, 69.5, 69.5, 68.625, 68.625, 68.25, 68.25, 70.01666666666667, 70.01666666666667, 70.06666666666666, 70.06666666666666, 68.625, 68.625, 68.43333333333334, 68.43333333333334, 68.475, 68.475, 68.70833333333333, 68.70833333333333, 68.36666666666666, 68.36666666666666, 68.125, 68.125, 68.79166666666667, 68.79166666666667, 69.10833333333333, 69.10833333333333, 68.275, 68.275, 68.825, 68.825, 68.875, 68.875, 69.475, 69.475, 69.15833333333333, 69.15833333333333, 68.53333333333333, 68.53333333333333, 69.625, 69.625, 68.94166666666666, 68.94166666666666, 68.93333333333334, 68.93333333333334, 69.79166666666667, 69.79166666666667, 69.78333333333333, 69.78333333333333, 69.49166666666666, 69.49166666666666, 69.20833333333333, 69.20833333333333, 69.425, 69.425, 68.23333333333333, 68.23333333333333, 67.925, 67.925, 67.15833333333333, 67.15833333333333, 67.675, 67.675, 68.16666666666667, 68.16666666666667, 67.925, 67.925, 67.9, 67.9, 68.86666666666666, 68.86666666666666, 68.79166666666667, 68.79166666666667, 67.69166666666666, 67.69166666666666, 67.18333333333334, 67.18333333333334, 67.10833333333333, 67.10833333333333, 66.58333333333333, 66.58333333333333, 66.725, 66.725, 66.65833333333333, 66.65833333333333, 66.40833333333333, 66.40833333333333, 66.25833333333334, 66.25833333333334, 66.25833333333334, 66.25833333333334, 65.76666666666667, 65.76666666666667, 65.38333333333334, 65.38333333333334, 64.95833333333333, 64.95833333333333, 65.83333333333333, 65.83333333333333, 66.85833333333333, 66.85833333333333, 66.4, 66.4, 66.13333333333334, 66.13333333333334, 65.9, 65.9, 66.01666666666667, 66.01666666666667, 66.025, 66.025, 65.09166666666667, 65.09166666666667, 65.28333333333333, 65.28333333333333, 65.88333333333334, 65.88333333333334, 65.56666666666666, 65.56666666666666, 65.74166666666666, 65.74166666666666, 65.95833333333333, 65.95833333333333, 66.10833333333333, 66.10833333333333, 65.575, 65.575, 65.85833333333333, 65.85833333333333]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8595
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1920
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0820
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6020
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8035
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0285
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7170
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7610
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.2435
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6565
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5125
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8670
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0825
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3780
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6535
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4250
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.535, Test loss: 1.996, Test accuracy: 19.96
Round   1, Train loss: 0.969, Test loss: 1.941, Test accuracy: 32.45
Round   2, Train loss: 0.873, Test loss: 1.412, Test accuracy: 46.12
Round   3, Train loss: 0.783, Test loss: 1.145, Test accuracy: 55.71
Round   4, Train loss: 0.705, Test loss: 1.198, Test accuracy: 54.66
Round   5, Train loss: 0.760, Test loss: 0.947, Test accuracy: 59.90
Round   6, Train loss: 0.766, Test loss: 0.857, Test accuracy: 63.29
Round   7, Train loss: 0.642, Test loss: 0.796, Test accuracy: 65.16
Round   8, Train loss: 0.733, Test loss: 0.777, Test accuracy: 66.24
Round   9, Train loss: 0.684, Test loss: 0.715, Test accuracy: 68.53
Round  10, Train loss: 0.642, Test loss: 0.697, Test accuracy: 69.56
Round  11, Train loss: 0.755, Test loss: 0.745, Test accuracy: 70.52
Round  12, Train loss: 0.572, Test loss: 0.715, Test accuracy: 69.75
Round  13, Train loss: 0.595, Test loss: 0.707, Test accuracy: 70.42
Round  14, Train loss: 0.523, Test loss: 0.597, Test accuracy: 74.44
Round  15, Train loss: 0.570, Test loss: 0.591, Test accuracy: 74.72
Round  16, Train loss: 0.578, Test loss: 0.569, Test accuracy: 75.63
Round  17, Train loss: 0.507, Test loss: 0.570, Test accuracy: 75.97
Round  18, Train loss: 0.648, Test loss: 0.543, Test accuracy: 76.92
Round  19, Train loss: 0.575, Test loss: 0.543, Test accuracy: 76.84
Round  20, Train loss: 0.573, Test loss: 0.540, Test accuracy: 77.13
Round  21, Train loss: 0.531, Test loss: 0.528, Test accuracy: 77.76
Round  22, Train loss: 0.528, Test loss: 0.524, Test accuracy: 78.00
Round  23, Train loss: 0.522, Test loss: 0.514, Test accuracy: 78.12
Round  24, Train loss: 0.498, Test loss: 0.509, Test accuracy: 78.44
Round  25, Train loss: 0.459, Test loss: 0.503, Test accuracy: 78.91
Round  26, Train loss: 0.493, Test loss: 0.500, Test accuracy: 78.78
Round  27, Train loss: 0.441, Test loss: 0.494, Test accuracy: 79.13
Round  28, Train loss: 0.430, Test loss: 0.493, Test accuracy: 79.15
Round  29, Train loss: 0.405, Test loss: 0.489, Test accuracy: 79.28
Round  30, Train loss: 0.461, Test loss: 0.484, Test accuracy: 79.60
Round  31, Train loss: 0.385, Test loss: 0.475, Test accuracy: 80.11
Round  32, Train loss: 0.506, Test loss: 0.482, Test accuracy: 80.03
Round  33, Train loss: 0.418, Test loss: 0.467, Test accuracy: 80.88
Round  34, Train loss: 0.419, Test loss: 0.467, Test accuracy: 80.86
Round  35, Train loss: 0.466, Test loss: 0.458, Test accuracy: 81.08
Round  36, Train loss: 0.408, Test loss: 0.462, Test accuracy: 81.09
Round  37, Train loss: 0.416, Test loss: 0.450, Test accuracy: 81.17
Round  38, Train loss: 0.471, Test loss: 0.452, Test accuracy: 81.27
Round  39, Train loss: 0.353, Test loss: 0.457, Test accuracy: 81.22
Round  40, Train loss: 0.470, Test loss: 0.447, Test accuracy: 81.52
Round  41, Train loss: 0.344, Test loss: 0.445, Test accuracy: 81.50
Round  42, Train loss: 0.533, Test loss: 0.442, Test accuracy: 82.16
Round  43, Train loss: 0.456, Test loss: 0.439, Test accuracy: 82.16
Round  44, Train loss: 0.499, Test loss: 0.440, Test accuracy: 81.92
Round  45, Train loss: 0.350, Test loss: 0.431, Test accuracy: 82.26
Round  46, Train loss: 0.318, Test loss: 0.433, Test accuracy: 82.28
Round  47, Train loss: 0.308, Test loss: 0.427, Test accuracy: 82.36
Round  48, Train loss: 0.358, Test loss: 0.424, Test accuracy: 82.53
Round  49, Train loss: 0.344, Test loss: 0.422, Test accuracy: 82.54
Round  50, Train loss: 0.419, Test loss: 0.424, Test accuracy: 82.70
Round  51, Train loss: 0.424, Test loss: 0.420, Test accuracy: 83.16
Round  52, Train loss: 0.465, Test loss: 0.427, Test accuracy: 82.40
Round  53, Train loss: 0.359, Test loss: 0.423, Test accuracy: 82.62
Round  54, Train loss: 0.348, Test loss: 0.423, Test accuracy: 82.70
Round  55, Train loss: 0.398, Test loss: 0.418, Test accuracy: 83.20
Round  56, Train loss: 0.298, Test loss: 0.419, Test accuracy: 82.96
Round  57, Train loss: 0.288, Test loss: 0.411, Test accuracy: 83.75
Round  58, Train loss: 0.406, Test loss: 0.407, Test accuracy: 83.47
Round  59, Train loss: 0.337, Test loss: 0.411, Test accuracy: 83.47
Round  60, Train loss: 0.364, Test loss: 0.412, Test accuracy: 83.28
Round  61, Train loss: 0.294, Test loss: 0.408, Test accuracy: 83.38
Round  62, Train loss: 0.339, Test loss: 0.415, Test accuracy: 83.07
Round  63, Train loss: 0.357, Test loss: 0.414, Test accuracy: 82.98
Round  64, Train loss: 0.266, Test loss: 0.419, Test accuracy: 83.11
Round  65, Train loss: 0.273, Test loss: 0.408, Test accuracy: 83.42
Round  66, Train loss: 0.304, Test loss: 0.403, Test accuracy: 83.57
Round  67, Train loss: 0.265, Test loss: 0.406, Test accuracy: 83.58
Round  68, Train loss: 0.256, Test loss: 0.406, Test accuracy: 83.50
Round  69, Train loss: 0.265, Test loss: 0.403, Test accuracy: 83.36
Round  70, Train loss: 0.352, Test loss: 0.397, Test accuracy: 83.93
Round  71, Train loss: 0.305, Test loss: 0.398, Test accuracy: 83.71
Round  72, Train loss: 0.306, Test loss: 0.397, Test accuracy: 83.88
Round  73, Train loss: 0.307, Test loss: 0.403, Test accuracy: 83.83
Round  74, Train loss: 0.349, Test loss: 0.398, Test accuracy: 84.12
Round  75, Train loss: 0.234, Test loss: 0.393, Test accuracy: 84.32
Round  76, Train loss: 0.285, Test loss: 0.393, Test accuracy: 84.43
Round  77, Train loss: 0.308, Test loss: 0.397, Test accuracy: 84.33
Round  78, Train loss: 0.274, Test loss: 0.399, Test accuracy: 84.29
Round  79, Train loss: 0.294, Test loss: 0.396, Test accuracy: 84.13
Round  80, Train loss: 0.282, Test loss: 0.405, Test accuracy: 84.10
Round  81, Train loss: 0.282, Test loss: 0.393, Test accuracy: 84.38
Round  82, Train loss: 0.214, Test loss: 0.392, Test accuracy: 84.56
Round  83, Train loss: 0.344, Test loss: 0.398, Test accuracy: 83.82
Round  84, Train loss: 0.240, Test loss: 0.386, Test accuracy: 84.54
Round  85, Train loss: 0.268, Test loss: 0.397, Test accuracy: 84.05
Round  86, Train loss: 0.286, Test loss: 0.395, Test accuracy: 84.39
Round  87, Train loss: 0.272, Test loss: 0.391, Test accuracy: 84.43
Round  88, Train loss: 0.281, Test loss: 0.399, Test accuracy: 84.12
Round  89, Train loss: 0.196, Test loss: 0.394, Test accuracy: 84.60
Round  90, Train loss: 0.284, Test loss: 0.388, Test accuracy: 84.38
Round  91, Train loss: 0.319, Test loss: 0.385, Test accuracy: 84.92
Round  92, Train loss: 0.253, Test loss: 0.393, Test accuracy: 84.70
Round  93, Train loss: 0.229, Test loss: 0.394, Test accuracy: 84.73
Round  94, Train loss: 0.182, Test loss: 0.390, Test accuracy: 84.81
Round  95, Train loss: 0.242, Test loss: 0.383, Test accuracy: 84.96
Round  96, Train loss: 0.191, Test loss: 0.384, Test accuracy: 85.09
Round  97, Train loss: 0.205, Test loss: 0.387, Test accuracy: 85.07
Round  98, Train loss: 0.315, Test loss: 0.397, Test accuracy: 84.33
Round  99, Train loss: 0.226, Test loss: 0.390, Test accuracy: 84.62
Final Round, Train loss: 0.213, Test loss: 0.393, Test accuracy: 84.78
Average accuracy final 10 rounds: 84.76166666666668
1416.1538140773773
[2.074504852294922, 3.862966299057007, 5.644784927368164, 7.459676504135132, 9.237759590148926, 11.016538143157959, 12.812543869018555, 14.63842511177063, 16.45313286781311, 18.225175857543945, 20.03494930267334, 21.864405870437622, 23.640934228897095, 25.492850303649902, 27.26100254058838, 29.032653093338013, 30.802919149398804, 32.621422290802, 34.417248487472534, 36.194844484329224, 37.98004198074341, 39.793362855911255, 41.61233735084534, 43.431479930877686, 45.187411069869995, 46.87763738632202, 48.6091468334198, 50.32039737701416, 52.00169825553894, 53.674530029296875, 55.33415079116821, 57.0342276096344, 58.77153205871582, 60.38373255729675, 62.01728868484497, 63.757256507873535, 65.49071645736694, 67.13042187690735, 68.8386378288269, 70.56471276283264, 72.31052708625793, 73.96037817001343, 75.72325277328491, 77.36163449287415, 79.09319615364075, 80.80704951286316, 82.56291317939758, 84.2028968334198, 85.93609714508057, 87.68287634849548, 89.34484028816223, 91.09637975692749, 92.78965830802917, 94.54861545562744, 96.31677484512329, 98.05752110481262, 99.73184490203857, 101.50240087509155, 103.26485991477966, 105.04912543296814, 106.68612051010132, 108.46128726005554, 110.16090822219849, 111.84829807281494, 113.57803630828857, 115.28242683410645, 117.02402544021606, 118.77028131484985, 120.50626230239868, 122.19293165206909, 123.97023940086365, 125.75506973266602, 127.4561915397644, 129.15130305290222, 130.90765047073364, 132.6451199054718, 134.37228918075562, 136.1203329563141, 137.85025906562805, 139.55670142173767, 141.31746697425842, 143.01592350006104, 144.72288966178894, 146.43657517433167, 148.1463918685913, 149.86458468437195, 151.51548171043396, 153.2588346004486, 154.95725512504578, 156.7018687725067, 158.46723651885986, 160.0894739627838, 161.84852385520935, 163.61933374404907, 165.27826952934265, 166.99700045585632, 168.73330307006836, 170.40141606330872, 172.15951371192932, 173.82036709785461, 176.15678548812866]
[19.958333333333332, 32.45, 46.11666666666667, 55.708333333333336, 54.65833333333333, 59.9, 63.291666666666664, 65.15833333333333, 66.24166666666666, 68.53333333333333, 69.55833333333334, 70.51666666666667, 69.75, 70.41666666666667, 74.44166666666666, 74.725, 75.63333333333334, 75.975, 76.91666666666667, 76.84166666666667, 77.13333333333334, 77.75833333333334, 78.0, 78.125, 78.44166666666666, 78.90833333333333, 78.775, 79.13333333333334, 79.15, 79.28333333333333, 79.6, 80.10833333333333, 80.03333333333333, 80.875, 80.85833333333333, 81.075, 81.09166666666667, 81.16666666666667, 81.26666666666667, 81.21666666666667, 81.51666666666667, 81.5, 82.15833333333333, 82.15833333333333, 81.91666666666667, 82.25833333333334, 82.275, 82.35833333333333, 82.53333333333333, 82.54166666666667, 82.7, 83.15833333333333, 82.4, 82.625, 82.7, 83.2, 82.95833333333333, 83.75, 83.475, 83.46666666666667, 83.28333333333333, 83.375, 83.06666666666666, 82.98333333333333, 83.10833333333333, 83.425, 83.56666666666666, 83.575, 83.5, 83.35833333333333, 83.93333333333334, 83.70833333333333, 83.88333333333334, 83.83333333333333, 84.11666666666666, 84.31666666666666, 84.43333333333334, 84.325, 84.29166666666667, 84.13333333333334, 84.1, 84.38333333333334, 84.55833333333334, 83.81666666666666, 84.54166666666667, 84.05, 84.39166666666667, 84.43333333333334, 84.11666666666666, 84.6, 84.38333333333334, 84.91666666666667, 84.7, 84.73333333333333, 84.80833333333334, 84.95833333333333, 85.09166666666667, 85.06666666666666, 84.33333333333333, 84.625, 84.775]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8605
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1985
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0870
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5815
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8025
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0890
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6980
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.8135
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.2030
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6555
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5050
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8685
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.3020
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3820
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6705
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4265
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  20.5600
Round 1 global test acc  10.0300
Round 2 global test acc  14.9100
Round 3 global test acc  14.0200
Round 4 global test acc  19.8200
Round 5 global test acc  21.7200
Round 6 global test acc  24.0000
Round 7 global test acc  22.3900
Round 8 global test acc  15.3600
Round 9 global test acc  22.4600
Round 10 global test acc  23.4500
Round 11 global test acc  25.3600
Round 12 global test acc  25.2900
Round 13 global test acc  24.8300
Round 14 global test acc  25.5400
Round 15 global test acc  16.5000
Round 16 global test acc  24.5300
Round 17 global test acc  23.4100
Round 18 global test acc  21.5800
Round 19 global test acc  23.1200
Round 20 global test acc  20.5400
Round 21 global test acc  27.4200
Round 22 global test acc  19.5500
Round 23 global test acc  23.9100
Round 24 global test acc  24.4900
Round 25 global test acc  24.1600
Round 26 global test acc  20.5700
Round 27 global test acc  25.5600
Round 28 global test acc  29.6000
Round 29 global test acc  20.3200
Round 30 global test acc  27.4500
Round 31 global test acc  28.3200
Round 32 global test acc  18.6700
Round 33 global test acc  24.5400
Round 34 global test acc  27.2100
Round 35 global test acc  28.7200
Round 36 global test acc  18.1500
Round 37 global test acc  20.6100
Round 38 global test acc  33.1400
Round 39 global test acc  29.0700
Round 40 global test acc  28.7900
Round 41 global test acc  28.8100
Round 42 global test acc  21.4400
Round 43 global test acc  31.7600
Round 44 global test acc  31.3400
Round 45 global test acc  26.6700
Round 46 global test acc  30.4000
Round 47 global test acc  30.0500
Round 48 global test acc  25.3200
Round 49 global test acc  28.6500
Round 50 global test acc  34.9200
Round 51 global test acc  24.0500
Round 52 global test acc  33.5800
Round 53 global test acc  28.7900
Round 54 global test acc  33.5600
Round 55 global test acc  35.9300
Round 56 global test acc  27.8000
Round 57 global test acc  25.8200
Round 58 global test acc  24.4500
Round 59 global test acc  25.6200
Round 60 global test acc  32.3900
Round 61 global test acc  37.4000
Round 62 global test acc  36.4800
Round 63 global test acc  29.9200
Round 64 global test acc  36.5300
Round 65 global test acc  29.6100
Round 66 global test acc  29.7700
Round 67 global test acc  30.8200
Round 68 global test acc  23.2200
Round 69 global test acc  29.8300
Round 70 global test acc  23.3800
Round 71 global test acc  27.9400
Round 72 global test acc  24.6400
Round 73 global test acc  31.3400
Round 74 global test acc  34.3600
Round 75 global test acc  34.8300
Round 76 global test acc  33.7600
Round 77 global test acc  35.4900
Round 78 global test acc  30.3800
Round 79 global test acc  30.0100
Round 80 global test acc  24.8300
Round 81 global test acc  18.7800
Round 82 global test acc  18.7000
Round 83 global test acc  18.5500
Round 84 global test acc  19.4100
Round 85 global test acc  19.2600
Round 86 global test acc  17.2600
Round 87 global test acc  20.9800
Round 88 global test acc  23.3700
Round 89 global test acc  22.8900
Round 90 global test acc  24.8200
Round 91 global test acc  24.0800
Round 92 global test acc  21.4300
Round 93 global test acc  21.3100
Round 94 global test acc  25.5700
Round 95 global test acc  22.8500
Round 96 global test acc  21.7900
Round 97 global test acc  22.9300
Round 98 global test acc  22.7300
Round 99 global test acc  22.0500
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8470
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2615
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.1445
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5725
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8100
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1765
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7110
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7405
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1375
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6155
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.6245
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8640
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.2955
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3355
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7185
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4265
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53541 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8505
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2645
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0600
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5855
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7875
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0080
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7240
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7610
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.3045
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6755
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5100
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8685
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1490
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.4090
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6925
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.5130
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1977, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train_local):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52495 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8770
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5430
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4590
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7160
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8430
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5490
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7975
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8165
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.6510
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7915
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.6930
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8910
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5245
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6845
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7975
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6805
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.222, Test loss: 2.231, Test accuracy: 22.98
Round   0, Global train loss: 2.222, Global test loss: 2.233, Global test accuracy: 24.30
Round   1, Train loss: 2.238, Test loss: 2.182, Test accuracy: 25.36
Round   1, Global train loss: 2.238, Global test loss: 2.170, Global test accuracy: 30.27
Round   2, Train loss: 2.098, Test loss: 2.122, Test accuracy: 27.16
Round   2, Global train loss: 2.098, Global test loss: 2.092, Global test accuracy: 31.20
Round   3, Train loss: 2.140, Test loss: 2.139, Test accuracy: 27.55
Round   3, Global train loss: 2.140, Global test loss: 2.121, Global test accuracy: 33.53
Round   4, Train loss: 2.114, Test loss: 2.132, Test accuracy: 27.71
Round   4, Global train loss: 2.114, Global test loss: 2.130, Global test accuracy: 34.71
Round   5, Train loss: 2.174, Test loss: 2.128, Test accuracy: 26.88
Round   5, Global train loss: 2.174, Global test loss: 2.121, Global test accuracy: 34.51
Round   6, Train loss: 2.175, Test loss: 2.137, Test accuracy: 25.82
Round   6, Global train loss: 2.175, Global test loss: 2.167, Global test accuracy: 30.04
Round   7, Train loss: 2.258, Test loss: 2.133, Test accuracy: 25.52
Round   7, Global train loss: 2.258, Global test loss: 2.203, Global test accuracy: 28.82
Round   8, Train loss: 2.132, Test loss: 2.118, Test accuracy: 25.90
Round   8, Global train loss: 2.132, Global test loss: 2.117, Global test accuracy: 30.80
Round   9, Train loss: 2.112, Test loss: 2.126, Test accuracy: 25.75
Round   9, Global train loss: 2.112, Global test loss: 2.155, Global test accuracy: 31.99
Round  10, Train loss: 2.168, Test loss: 2.121, Test accuracy: 25.51
Round  10, Global train loss: 2.168, Global test loss: 2.140, Global test accuracy: 32.07
Round  11, Train loss: 2.053, Test loss: 2.101, Test accuracy: 26.36
Round  11, Global train loss: 2.053, Global test loss: 2.057, Global test accuracy: 35.90
Round  12, Train loss: 2.213, Test loss: 2.105, Test accuracy: 26.00
Round  12, Global train loss: 2.213, Global test loss: 2.145, Global test accuracy: 30.73
Round  13, Train loss: 2.125, Test loss: 2.107, Test accuracy: 26.18
Round  13, Global train loss: 2.125, Global test loss: 2.141, Global test accuracy: 32.33
Round  14, Train loss: 2.195, Test loss: 2.103, Test accuracy: 26.02
Round  14, Global train loss: 2.195, Global test loss: 2.172, Global test accuracy: 30.63
Round  15, Train loss: 2.175, Test loss: 2.104, Test accuracy: 26.17
Round  15, Global train loss: 2.175, Global test loss: 2.130, Global test accuracy: 34.05
Round  16, Train loss: 1.964, Test loss: 2.094, Test accuracy: 26.32
Round  16, Global train loss: 1.964, Global test loss: 1.979, Global test accuracy: 40.88
Round  17, Train loss: 2.141, Test loss: 2.098, Test accuracy: 25.91
Round  17, Global train loss: 2.141, Global test loss: 2.104, Global test accuracy: 32.03
Round  18, Train loss: 1.911, Test loss: 2.114, Test accuracy: 25.61
Round  18, Global train loss: 1.911, Global test loss: 1.901, Global test accuracy: 44.02
Round  19, Train loss: 2.095, Test loss: 2.116, Test accuracy: 25.39
Round  19, Global train loss: 2.095, Global test loss: 2.035, Global test accuracy: 39.63
Round  20, Train loss: 1.817, Test loss: 2.113, Test accuracy: 25.59
Round  20, Global train loss: 1.817, Global test loss: 1.868, Global test accuracy: 43.11
Round  21, Train loss: 1.930, Test loss: 2.121, Test accuracy: 25.27
Round  21, Global train loss: 1.930, Global test loss: 2.029, Global test accuracy: 37.77
Round  22, Train loss: 1.904, Test loss: 2.136, Test accuracy: 24.87
Round  22, Global train loss: 1.904, Global test loss: 1.987, Global test accuracy: 35.37
Round  23, Train loss: 2.041, Test loss: 2.152, Test accuracy: 24.46
Round  23, Global train loss: 2.041, Global test loss: 2.133, Global test accuracy: 32.48
Round  24, Train loss: 2.006, Test loss: 2.155, Test accuracy: 24.58
Round  24, Global train loss: 2.006, Global test loss: 2.089, Global test accuracy: 32.88
Round  25, Train loss: 1.987, Test loss: 2.160, Test accuracy: 24.52
Round  25, Global train loss: 1.987, Global test loss: 2.101, Global test accuracy: 35.16
Round  26, Train loss: 1.759, Test loss: 2.190, Test accuracy: 23.88
Round  26, Global train loss: 1.759, Global test loss: 1.924, Global test accuracy: 38.36
Round  27, Train loss: 1.786, Test loss: 2.212, Test accuracy: 24.11
Round  27, Global train loss: 1.786, Global test loss: 2.043, Global test accuracy: 35.16
Round  28, Train loss: 1.736, Test loss: 2.246, Test accuracy: 23.86
Round  28, Global train loss: 1.736, Global test loss: 2.031, Global test accuracy: 36.87
Round  29, Train loss: 1.743, Test loss: 2.249, Test accuracy: 23.73
Round  29, Global train loss: 1.743, Global test loss: 2.160, Global test accuracy: 28.02
Round  30, Train loss: 2.075, Test loss: 2.258, Test accuracy: 23.90
Round  30, Global train loss: 2.075, Global test loss: 2.118, Global test accuracy: 31.01
Round  31, Train loss: 1.803, Test loss: 2.289, Test accuracy: 23.97
Round  31, Global train loss: 1.803, Global test loss: 2.159, Global test accuracy: 27.22
Round  32, Train loss: 1.672, Test loss: 2.323, Test accuracy: 23.35
Round  32, Global train loss: 1.672, Global test loss: 2.067, Global test accuracy: 35.02
Round  33, Train loss: 1.547, Test loss: 2.346, Test accuracy: 23.32
Round  33, Global train loss: 1.547, Global test loss: 1.993, Global test accuracy: 38.09
Round  34, Train loss: 1.527, Test loss: 2.394, Test accuracy: 23.28
Round  34, Global train loss: 1.527, Global test loss: 2.026, Global test accuracy: 33.38
Round  35, Train loss: 1.591, Test loss: 2.427, Test accuracy: 23.28
Round  35, Global train loss: 1.591, Global test loss: 1.974, Global test accuracy: 38.41
Round  36, Train loss: 1.842, Test loss: 2.476, Test accuracy: 22.68
Round  36, Global train loss: 1.842, Global test loss: 2.157, Global test accuracy: 28.05
Round  37, Train loss: 1.472, Test loss: 2.523, Test accuracy: 22.77
Round  37, Global train loss: 1.472, Global test loss: 2.053, Global test accuracy: 30.89
Round  38, Train loss: 1.731, Test loss: 2.543, Test accuracy: 22.69
Round  38, Global train loss: 1.731, Global test loss: 2.095, Global test accuracy: 32.73
Round  39, Train loss: 1.482, Test loss: 2.570, Test accuracy: 22.81
Round  39, Global train loss: 1.482, Global test loss: 2.113, Global test accuracy: 30.64
Round  40, Train loss: 1.784, Test loss: 2.590, Test accuracy: 22.73
Round  40, Global train loss: 1.784, Global test loss: 2.097, Global test accuracy: 31.28
Round  41, Train loss: 1.660, Test loss: 2.633, Test accuracy: 22.74
Round  41, Global train loss: 1.660, Global test loss: 2.159, Global test accuracy: 27.26
Round  42, Train loss: 1.187, Test loss: 2.666, Test accuracy: 21.89
Round  42, Global train loss: 1.187, Global test loss: 1.791, Global test accuracy: 42.62
Round  43, Train loss: 1.429, Test loss: 2.714, Test accuracy: 21.71
Round  43, Global train loss: 1.429, Global test loss: 2.073, Global test accuracy: 31.86
Round  44, Train loss: 1.658, Test loss: 2.741, Test accuracy: 21.50
Round  44, Global train loss: 1.658, Global test loss: 2.066, Global test accuracy: 33.23
Round  45, Train loss: 1.638, Test loss: 2.769, Test accuracy: 21.60
Round  45, Global train loss: 1.638, Global test loss: 2.177, Global test accuracy: 24.58
Round  46, Train loss: 1.364, Test loss: 2.803, Test accuracy: 21.59
Round  46, Global train loss: 1.364, Global test loss: 2.013, Global test accuracy: 34.69
Round  47, Train loss: 1.410, Test loss: 2.836, Test accuracy: 21.55
Round  47, Global train loss: 1.410, Global test loss: 2.100, Global test accuracy: 31.32
Round  48, Train loss: 1.290, Test loss: 2.903, Test accuracy: 21.47
Round  48, Global train loss: 1.290, Global test loss: 2.170, Global test accuracy: 24.22
Round  49, Train loss: 1.291, Test loss: 2.927, Test accuracy: 21.59
Round  49, Global train loss: 1.291, Global test loss: 2.130, Global test accuracy: 26.20
Round  50, Train loss: 1.403, Test loss: 2.960, Test accuracy: 21.32
Round  50, Global train loss: 1.403, Global test loss: 2.062, Global test accuracy: 31.41
Round  51, Train loss: 1.264, Test loss: 3.019, Test accuracy: 21.07
Round  51, Global train loss: 1.264, Global test loss: 1.926, Global test accuracy: 36.61
Round  52, Train loss: 1.485, Test loss: 3.039, Test accuracy: 21.32
Round  52, Global train loss: 1.485, Global test loss: 2.128, Global test accuracy: 27.01
Round  53, Train loss: 1.394, Test loss: 3.056, Test accuracy: 21.48
Round  53, Global train loss: 1.394, Global test loss: 2.167, Global test accuracy: 23.94
Round  54, Train loss: 1.409, Test loss: 3.092, Test accuracy: 21.25
Round  54, Global train loss: 1.409, Global test loss: 2.093, Global test accuracy: 26.84
Round  55, Train loss: 1.244, Test loss: 3.135, Test accuracy: 21.13
Round  55, Global train loss: 1.244, Global test loss: 2.038, Global test accuracy: 31.64
Round  56, Train loss: 1.184, Test loss: 3.188, Test accuracy: 21.09
Round  56, Global train loss: 1.184, Global test loss: 2.043, Global test accuracy: 31.29
Round  57, Train loss: 1.362, Test loss: 3.234, Test accuracy: 21.28
Round  57, Global train loss: 1.362, Global test loss: 2.130, Global test accuracy: 27.37
Round  58, Train loss: 1.294, Test loss: 3.281, Test accuracy: 21.20
Round  58, Global train loss: 1.294, Global test loss: 1.999, Global test accuracy: 33.92
Round  59, Train loss: 1.071, Test loss: 3.326, Test accuracy: 20.80
Round  59, Global train loss: 1.071, Global test loss: 1.941, Global test accuracy: 35.67
Round  60, Train loss: 1.138, Test loss: 3.350, Test accuracy: 21.02
Round  60, Global train loss: 1.138, Global test loss: 2.122, Global test accuracy: 27.34
Round  61, Train loss: 0.962, Test loss: 3.401, Test accuracy: 20.99
Round  61, Global train loss: 0.962, Global test loss: 2.000, Global test accuracy: 32.15
Round  62, Train loss: 1.041, Test loss: 3.446, Test accuracy: 21.00
Round  62, Global train loss: 1.041, Global test loss: 1.990, Global test accuracy: 33.24
Round  63, Train loss: 1.210, Test loss: 3.520, Test accuracy: 20.81
Round  63, Global train loss: 1.210, Global test loss: 2.229, Global test accuracy: 18.50
Round  64, Train loss: 0.974, Test loss: 3.520, Test accuracy: 20.98
Round  64, Global train loss: 0.974, Global test loss: 2.102, Global test accuracy: 26.33
Round  65, Train loss: 1.048, Test loss: 3.568, Test accuracy: 20.91
Round  65, Global train loss: 1.048, Global test loss: 2.034, Global test accuracy: 30.98
Round  66, Train loss: 1.123, Test loss: 3.626, Test accuracy: 20.84
Round  66, Global train loss: 1.123, Global test loss: 2.187, Global test accuracy: 21.44
Round  67, Train loss: 1.164, Test loss: 3.638, Test accuracy: 20.94
Round  67, Global train loss: 1.164, Global test loss: 2.197, Global test accuracy: 20.63
Round  68, Train loss: 1.004, Test loss: 3.620, Test accuracy: 21.17
Round  68, Global train loss: 1.004, Global test loss: 2.139, Global test accuracy: 24.95
Round  69, Train loss: 0.951, Test loss: 3.695, Test accuracy: 21.06
Round  69, Global train loss: 0.951, Global test loss: 1.996, Global test accuracy: 33.36
Round  70, Train loss: 0.925, Test loss: 3.674, Test accuracy: 21.21
Round  70, Global train loss: 0.925, Global test loss: 2.157, Global test accuracy: 24.58
Round  71, Train loss: 0.911, Test loss: 3.704, Test accuracy: 21.29
Round  71, Global train loss: 0.911, Global test loss: 2.106, Global test accuracy: 26.00
Round  72, Train loss: 0.796, Test loss: 3.766, Test accuracy: 20.81
Round  72, Global train loss: 0.796, Global test loss: 2.165, Global test accuracy: 24.07
Round  73, Train loss: 0.795, Test loss: 3.822, Test accuracy: 20.64
Round  73, Global train loss: 0.795, Global test loss: 2.033, Global test accuracy: 30.11
Round  74, Train loss: 0.797, Test loss: 3.892, Test accuracy: 20.51
Round  74, Global train loss: 0.797, Global test loss: 2.061, Global test accuracy: 29.82
Round  75, Train loss: 0.830, Test loss: 3.959, Test accuracy: 20.66
Round  75, Global train loss: 0.830, Global test loss: 2.043, Global test accuracy: 29.52
Round  76, Train loss: 0.706, Test loss: 4.025, Test accuracy: 20.45
Round  76, Global train loss: 0.706, Global test loss: 1.964, Global test accuracy: 32.00
Round  77, Train loss: 1.027, Test loss: 4.061, Test accuracy: 20.39
Round  77, Global train loss: 1.027, Global test loss: 2.102, Global test accuracy: 27.57
Round  78, Train loss: 0.981, Test loss: 4.087, Test accuracy: 20.41
Round  78, Global train loss: 0.981, Global test loss: 2.071, Global test accuracy: 27.34
Round  79, Train loss: 0.919, Test loss: 4.071, Test accuracy: 20.72
Round  79, Global train loss: 0.919, Global test loss: 2.143, Global test accuracy: 25.38
Round  80, Train loss: 0.908, Test loss: 4.136, Test accuracy: 20.75
Round  80, Global train loss: 0.908, Global test loss: 2.122, Global test accuracy: 26.27
Round  81, Train loss: 0.907, Test loss: 4.134, Test accuracy: 20.61
Round  81, Global train loss: 0.907, Global test loss: 2.156, Global test accuracy: 24.50
Round  82, Train loss: 0.942, Test loss: 4.186, Test accuracy: 20.41
Round  82, Global train loss: 0.942, Global test loss: 2.134, Global test accuracy: 27.85
Round  83, Train loss: 0.754, Test loss: 4.218, Test accuracy: 20.38
Round  83, Global train loss: 0.754, Global test loss: 2.023, Global test accuracy: 30.52
Round  84, Train loss: 1.102, Test loss: 4.227, Test accuracy: 20.39
Round  84, Global train loss: 1.102, Global test loss: 2.235, Global test accuracy: 17.28
Round  85, Train loss: 0.661, Test loss: 4.308, Test accuracy: 20.18
Round  85, Global train loss: 0.661, Global test loss: 2.101, Global test accuracy: 27.57
Round  86, Train loss: 0.884, Test loss: 4.325, Test accuracy: 20.38
Round  86, Global train loss: 0.884, Global test loss: 2.072, Global test accuracy: 28.66
Round  87, Train loss: 0.966, Test loss: 4.339, Test accuracy: 20.59
Round  87, Global train loss: 0.966, Global test loss: 2.142, Global test accuracy: 24.67
Round  88, Train loss: 0.880, Test loss: 4.383, Test accuracy: 20.67
Round  88, Global train loss: 0.880, Global test loss: 2.151, Global test accuracy: 23.07
Round  89, Train loss: 0.704, Test loss: 4.388, Test accuracy: 20.62
Round  89, Global train loss: 0.704, Global test loss: 2.134, Global test accuracy: 24.45
Round  90, Train loss: 0.862, Test loss: 4.453, Test accuracy: 20.27
Round  90, Global train loss: 0.862, Global test loss: 2.118, Global test accuracy: 25.30
Round  91, Train loss: 0.802, Test loss: 4.442, Test accuracy: 20.38
Round  91, Global train loss: 0.802, Global test loss: 2.144, Global test accuracy: 24.32
Round  92, Train loss: 0.926, Test loss: 4.527, Test accuracy: 20.31
Round  92, Global train loss: 0.926, Global test loss: 2.140, Global test accuracy: 24.68
Round  93, Train loss: 0.848, Test loss: 4.541, Test accuracy: 20.30
Round  93, Global train loss: 0.848, Global test loss: 2.159, Global test accuracy: 22.85
Round  94, Train loss: 0.649, Test loss: 4.560, Test accuracy: 20.49
Round  94, Global train loss: 0.649, Global test loss: 2.093, Global test accuracy: 28.80
Round  95, Train loss: 0.909, Test loss: 4.581, Test accuracy: 20.57
Round  95, Global train loss: 0.909, Global test loss: 2.204, Global test accuracy: 20.89
Round  96, Train loss: 0.860, Test loss: 4.623, Test accuracy: 20.31
Round  96, Global train loss: 0.860, Global test loss: 2.136, Global test accuracy: 24.38
Round  97, Train loss: 0.684, Test loss: 4.620, Test accuracy: 20.41
Round  97, Global train loss: 0.684, Global test loss: 2.048, Global test accuracy: 30.43
Round  98, Train loss: 0.860, Test loss: 4.638, Test accuracy: 20.31
Round  98, Global train loss: 0.860, Global test loss: 2.122, Global test accuracy: 24.59
Round  99, Train loss: 0.669, Test loss: 4.672, Test accuracy: 20.17
Round  99, Global train loss: 0.669, Global test loss: 1.924, Global test accuracy: 34.24
Final Round, Train loss: 0.494, Test loss: 5.558, Test accuracy: 20.66
Final Round, Global train loss: 0.494, Global test loss: 1.924, Global test accuracy: 34.24
Average accuracy final 10 rounds: 20.35125 

Average global accuracy final 10 rounds: 26.047749999999997 

6245.787036180496
[5.136620044708252, 10.273240089416504, 15.225827693939209, 20.178415298461914, 25.145225763320923, 30.11203622817993, 35.0900981426239, 40.06816005706787, 45.07184362411499, 50.07552719116211, 55.05920934677124, 60.04289150238037, 65.05539870262146, 70.06790590286255, 75.06093192100525, 80.05395793914795, 85.05779075622559, 90.06162357330322, 95.02034997940063, 99.97907638549805, 105.00287628173828, 110.02667617797852, 115.03225755691528, 120.03783893585205, 125.03768110275269, 130.03752326965332, 134.96628332138062, 139.8950433731079, 144.8424530029297, 149.78986263275146, 154.69974613189697, 159.60962963104248, 164.55850267410278, 169.5073757171631, 174.4709334373474, 179.43449115753174, 184.40365743637085, 189.37282371520996, 194.0425145626068, 198.71220541000366, 203.64461636543274, 208.57702732086182, 213.4780158996582, 218.3790044784546, 223.28334045410156, 228.18767642974854, 233.12756943702698, 238.06746244430542, 242.9643840789795, 247.86130571365356, 252.75806331634521, 257.65482091903687, 262.5341169834137, 267.4134130477905, 271.79025530815125, 276.16709756851196, 280.57931900024414, 284.9915404319763, 289.93326687812805, 294.8749933242798, 299.80812096595764, 304.7412486076355, 309.6445953845978, 314.54794216156006, 319.4733347892761, 324.3987274169922, 329.326149225235, 334.2535710334778, 339.157185792923, 344.06080055236816, 348.99610137939453, 353.9314022064209, 358.8184447288513, 363.70548725128174, 368.61707520484924, 373.52866315841675, 378.42801427841187, 383.327365398407, 388.24433732032776, 393.16130924224854, 398.0663306713104, 402.9713521003723, 407.8862464427948, 412.8011407852173, 417.69878792762756, 422.59643507003784, 427.50294828414917, 432.4094614982605, 437.35854411125183, 442.30762672424316, 447.1988673210144, 452.09010791778564, 457.02544927597046, 461.9607906341553, 466.88444352149963, 471.808096408844, 476.7359411716461, 481.66378593444824, 486.5840938091278, 491.5044016838074, 496.4653751850128, 501.42634868621826, 506.3914942741394, 511.35663986206055, 516.2890889644623, 521.221538066864, 526.147088766098, 531.072639465332, 535.9545063972473, 540.8363733291626, 545.745968580246, 550.6555638313293, 555.4725432395935, 560.2895226478577, 565.2151556015015, 570.1407885551453, 574.677894115448, 579.2149996757507, 583.4304685592651, 587.6459374427795, 591.8961794376373, 596.1464214324951, 600.392181634903, 604.6379418373108, 608.8518280982971, 613.0657143592834, 617.3361179828644, 621.6065216064453, 625.8180990219116, 630.0296764373779, 634.2271347045898, 638.4245929718018, 642.6388432979584, 646.853093624115, 651.0701515674591, 655.2872095108032, 659.4699454307556, 663.652681350708, 667.8775918483734, 672.1025023460388, 676.3342399597168, 680.5659775733948, 684.7620480060577, 688.9581184387207, 693.1575713157654, 697.3570241928101, 701.5785365104675, 705.800048828125, 710.0022740364075, 714.2044992446899, 718.4114406108856, 722.6183819770813, 726.8637671470642, 731.1091523170471, 735.3593513965607, 739.6095504760742, 743.875159740448, 748.1407690048218, 752.3643612861633, 756.5879535675049, 760.804431438446, 765.0209093093872, 769.240975856781, 773.4610424041748, 777.6751272678375, 781.8892121315002, 786.1375818252563, 790.3859515190125, 794.5970029830933, 798.8080544471741, 803.0340192317963, 807.2599840164185, 811.4940257072449, 815.7280673980713, 819.9720010757446, 824.215934753418, 828.4682466983795, 832.7205586433411, 836.9595818519592, 841.1986050605774, 845.4438652992249, 849.6891255378723, 853.9609220027924, 858.2327184677124, 862.5171394348145, 866.8015604019165, 871.087568283081, 875.3735761642456, 879.657363653183, 883.9411511421204, 888.2061476707458, 892.4711441993713, 896.7476034164429, 901.0240626335144, 905.2748758792877, 909.525689125061, 913.8009116649628, 918.0761342048645, 922.4324374198914, 926.7887406349182, 928.9246599674225, 931.0605792999268]
[22.985, 22.985, 25.3625, 25.3625, 27.1625, 27.1625, 27.545, 27.545, 27.7075, 27.7075, 26.88, 26.88, 25.8175, 25.8175, 25.525, 25.525, 25.9025, 25.9025, 25.745, 25.745, 25.5075, 25.5075, 26.3625, 26.3625, 26.005, 26.005, 26.185, 26.185, 26.0175, 26.0175, 26.1725, 26.1725, 26.315, 26.315, 25.91, 25.91, 25.6125, 25.6125, 25.3925, 25.3925, 25.5875, 25.5875, 25.27, 25.27, 24.8675, 24.8675, 24.4575, 24.4575, 24.58, 24.58, 24.5175, 24.5175, 23.8775, 23.8775, 24.115, 24.115, 23.8575, 23.8575, 23.725, 23.725, 23.8975, 23.8975, 23.9725, 23.9725, 23.3475, 23.3475, 23.325, 23.325, 23.2775, 23.2775, 23.2825, 23.2825, 22.68, 22.68, 22.775, 22.775, 22.69, 22.69, 22.8125, 22.8125, 22.735, 22.735, 22.7375, 22.7375, 21.895, 21.895, 21.7125, 21.7125, 21.5025, 21.5025, 21.6025, 21.6025, 21.59, 21.59, 21.555, 21.555, 21.4675, 21.4675, 21.595, 21.595, 21.32, 21.32, 21.0675, 21.0675, 21.3175, 21.3175, 21.485, 21.485, 21.245, 21.245, 21.13, 21.13, 21.09, 21.09, 21.28, 21.28, 21.2025, 21.2025, 20.8, 20.8, 21.015, 21.015, 20.9875, 20.9875, 20.995, 20.995, 20.81, 20.81, 20.9775, 20.9775, 20.9125, 20.9125, 20.845, 20.845, 20.94, 20.94, 21.1725, 21.1725, 21.0625, 21.0625, 21.21, 21.21, 21.285, 21.285, 20.8075, 20.8075, 20.64, 20.64, 20.5075, 20.5075, 20.66, 20.66, 20.45, 20.45, 20.3925, 20.3925, 20.415, 20.415, 20.7225, 20.7225, 20.7475, 20.7475, 20.6125, 20.6125, 20.41, 20.41, 20.3775, 20.3775, 20.3925, 20.3925, 20.18, 20.18, 20.38, 20.38, 20.595, 20.595, 20.67, 20.67, 20.625, 20.625, 20.2725, 20.2725, 20.3775, 20.3775, 20.31, 20.31, 20.305, 20.305, 20.4875, 20.4875, 20.5675, 20.5675, 20.3075, 20.3075, 20.405, 20.405, 20.31, 20.31, 20.17, 20.17, 20.6575, 20.6575]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8730
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5365
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4620
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7250
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8470
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4880
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7995
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8420
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.6470
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7680
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7405
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8875
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.6065
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6475
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8125
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7165
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.711, Test loss: 1.984, Test accuracy: 25.77
Round   0, Global train loss: 1.711, Global test loss: 2.259, Global test accuracy: 18.29
Round   1, Train loss: 1.577, Test loss: 1.686, Test accuracy: 43.67
Round   1, Global train loss: 1.577, Global test loss: 2.085, Global test accuracy: 33.94
Round   2, Train loss: 1.680, Test loss: 1.395, Test accuracy: 51.37
Round   2, Global train loss: 1.680, Global test loss: 2.032, Global test accuracy: 25.78
Round   3, Train loss: 1.384, Test loss: 1.187, Test accuracy: 61.25
Round   3, Global train loss: 1.384, Global test loss: 1.940, Global test accuracy: 33.10
Round   4, Train loss: 1.556, Test loss: 1.153, Test accuracy: 63.22
Round   4, Global train loss: 1.556, Global test loss: 1.948, Global test accuracy: 33.07
Round   5, Train loss: 1.358, Test loss: 1.131, Test accuracy: 65.82
Round   5, Global train loss: 1.358, Global test loss: 1.906, Global test accuracy: 37.28
Round   6, Train loss: 1.500, Test loss: 1.115, Test accuracy: 66.47
Round   6, Global train loss: 1.500, Global test loss: 1.919, Global test accuracy: 33.80
Round   7, Train loss: 1.538, Test loss: 1.109, Test accuracy: 67.17
Round   7, Global train loss: 1.538, Global test loss: 1.854, Global test accuracy: 37.92
Round   8, Train loss: 1.320, Test loss: 1.070, Test accuracy: 70.02
Round   8, Global train loss: 1.320, Global test loss: 1.794, Global test accuracy: 43.39
Round   9, Train loss: 1.477, Test loss: 1.061, Test accuracy: 70.59
Round   9, Global train loss: 1.477, Global test loss: 1.890, Global test accuracy: 29.83
Round  10, Train loss: 1.785, Test loss: 1.049, Test accuracy: 70.84
Round  10, Global train loss: 1.785, Global test loss: 1.947, Global test accuracy: 32.38
Round  11, Train loss: 1.685, Test loss: 1.040, Test accuracy: 70.66
Round  11, Global train loss: 1.685, Global test loss: 1.896, Global test accuracy: 39.78
Round  12, Train loss: 1.348, Test loss: 1.032, Test accuracy: 70.57
Round  12, Global train loss: 1.348, Global test loss: 1.765, Global test accuracy: 41.90
Round  13, Train loss: 1.568, Test loss: 1.042, Test accuracy: 70.18
Round  13, Global train loss: 1.568, Global test loss: 1.849, Global test accuracy: 36.77
Round  14, Train loss: 1.450, Test loss: 1.042, Test accuracy: 70.24
Round  14, Global train loss: 1.450, Global test loss: 1.799, Global test accuracy: 47.33
Round  15, Train loss: 1.385, Test loss: 1.020, Test accuracy: 70.86
Round  15, Global train loss: 1.385, Global test loss: 1.731, Global test accuracy: 46.97
Round  16, Train loss: 1.365, Test loss: 0.992, Test accuracy: 72.27
Round  16, Global train loss: 1.365, Global test loss: 1.705, Global test accuracy: 48.11
Round  17, Train loss: 1.617, Test loss: 0.985, Test accuracy: 72.17
Round  17, Global train loss: 1.617, Global test loss: 1.813, Global test accuracy: 42.81
Round  18, Train loss: 1.492, Test loss: 0.976, Test accuracy: 72.41
Round  18, Global train loss: 1.492, Global test loss: 1.737, Global test accuracy: 48.42
Round  19, Train loss: 1.301, Test loss: 0.971, Test accuracy: 72.71
Round  19, Global train loss: 1.301, Global test loss: 1.712, Global test accuracy: 45.83
Round  20, Train loss: 1.438, Test loss: 0.971, Test accuracy: 72.74
Round  20, Global train loss: 1.438, Global test loss: 1.765, Global test accuracy: 42.59
Round  21, Train loss: 1.330, Test loss: 0.971, Test accuracy: 73.03
Round  21, Global train loss: 1.330, Global test loss: 1.706, Global test accuracy: 47.88
Round  22, Train loss: 1.443, Test loss: 0.984, Test accuracy: 72.42
Round  22, Global train loss: 1.443, Global test loss: 1.756, Global test accuracy: 48.47
Round  23, Train loss: 1.304, Test loss: 0.992, Test accuracy: 72.16
Round  23, Global train loss: 1.304, Global test loss: 1.719, Global test accuracy: 47.64
Round  24, Train loss: 1.476, Test loss: 0.988, Test accuracy: 72.17
Round  24, Global train loss: 1.476, Global test loss: 1.700, Global test accuracy: 48.46
Round  25, Train loss: 1.404, Test loss: 0.964, Test accuracy: 72.38
Round  25, Global train loss: 1.404, Global test loss: 1.673, Global test accuracy: 48.72
Round  26, Train loss: 1.205, Test loss: 0.997, Test accuracy: 71.42
Round  26, Global train loss: 1.205, Global test loss: 1.682, Global test accuracy: 46.47
Round  27, Train loss: 1.429, Test loss: 0.988, Test accuracy: 71.44
Round  27, Global train loss: 1.429, Global test loss: 1.678, Global test accuracy: 47.27
Round  28, Train loss: 1.320, Test loss: 0.972, Test accuracy: 71.71
Round  28, Global train loss: 1.320, Global test loss: 1.734, Global test accuracy: 43.24
Round  29, Train loss: 1.196, Test loss: 0.983, Test accuracy: 71.69
Round  29, Global train loss: 1.196, Global test loss: 1.677, Global test accuracy: 45.63
Round  30, Train loss: 1.255, Test loss: 0.959, Test accuracy: 72.02
Round  30, Global train loss: 1.255, Global test loss: 1.674, Global test accuracy: 46.29
Round  31, Train loss: 1.065, Test loss: 0.968, Test accuracy: 71.48
Round  31, Global train loss: 1.065, Global test loss: 1.655, Global test accuracy: 46.42
Round  32, Train loss: 1.481, Test loss: 0.977, Test accuracy: 71.40
Round  32, Global train loss: 1.481, Global test loss: 1.750, Global test accuracy: 41.93
Round  33, Train loss: 1.328, Test loss: 0.987, Test accuracy: 70.30
Round  33, Global train loss: 1.328, Global test loss: 1.767, Global test accuracy: 43.65
Round  34, Train loss: 1.305, Test loss: 0.979, Test accuracy: 70.31
Round  34, Global train loss: 1.305, Global test loss: 1.682, Global test accuracy: 46.73
Round  35, Train loss: 1.104, Test loss: 0.969, Test accuracy: 70.48
Round  35, Global train loss: 1.104, Global test loss: 1.620, Global test accuracy: 50.84
Round  36, Train loss: 1.129, Test loss: 0.969, Test accuracy: 70.64
Round  36, Global train loss: 1.129, Global test loss: 1.624, Global test accuracy: 50.47
Round  37, Train loss: 1.213, Test loss: 0.980, Test accuracy: 70.63
Round  37, Global train loss: 1.213, Global test loss: 1.709, Global test accuracy: 43.73
Round  38, Train loss: 1.422, Test loss: 0.978, Test accuracy: 70.41
Round  38, Global train loss: 1.422, Global test loss: 1.794, Global test accuracy: 40.52
Round  39, Train loss: 1.242, Test loss: 0.974, Test accuracy: 71.17
Round  39, Global train loss: 1.242, Global test loss: 1.766, Global test accuracy: 41.92
Round  40, Train loss: 1.389, Test loss: 0.980, Test accuracy: 70.57
Round  40, Global train loss: 1.389, Global test loss: 1.755, Global test accuracy: 42.76
Round  41, Train loss: 1.100, Test loss: 0.967, Test accuracy: 70.87
Round  41, Global train loss: 1.100, Global test loss: 1.816, Global test accuracy: 39.34
Round  42, Train loss: 1.120, Test loss: 0.976, Test accuracy: 70.63
Round  42, Global train loss: 1.120, Global test loss: 1.793, Global test accuracy: 40.55
Round  43, Train loss: 1.168, Test loss: 0.987, Test accuracy: 69.99
Round  43, Global train loss: 1.168, Global test loss: 1.763, Global test accuracy: 42.64
Round  44, Train loss: 0.909, Test loss: 0.982, Test accuracy: 69.98
Round  44, Global train loss: 0.909, Global test loss: 1.602, Global test accuracy: 47.27
Round  45, Train loss: 1.235, Test loss: 0.989, Test accuracy: 69.48
Round  45, Global train loss: 1.235, Global test loss: 1.681, Global test accuracy: 44.33
Round  46, Train loss: 1.169, Test loss: 0.989, Test accuracy: 69.44
Round  46, Global train loss: 1.169, Global test loss: 1.718, Global test accuracy: 45.06
Round  47, Train loss: 0.972, Test loss: 0.977, Test accuracy: 69.88
Round  47, Global train loss: 0.972, Global test loss: 1.658, Global test accuracy: 45.98
Round  48, Train loss: 1.107, Test loss: 0.980, Test accuracy: 69.46
Round  48, Global train loss: 1.107, Global test loss: 1.733, Global test accuracy: 42.23
Round  49, Train loss: 1.240, Test loss: 0.995, Test accuracy: 69.12
Round  49, Global train loss: 1.240, Global test loss: 1.809, Global test accuracy: 38.97
Round  50, Train loss: 0.830, Test loss: 1.015, Test accuracy: 67.88
Round  50, Global train loss: 0.830, Global test loss: 1.734, Global test accuracy: 44.46
Round  51, Train loss: 1.097, Test loss: 1.023, Test accuracy: 67.89
Round  51, Global train loss: 1.097, Global test loss: 1.749, Global test accuracy: 41.37
Round  52, Train loss: 1.217, Test loss: 1.043, Test accuracy: 67.26
Round  52, Global train loss: 1.217, Global test loss: 1.711, Global test accuracy: 43.74
Round  53, Train loss: 1.171, Test loss: 1.042, Test accuracy: 66.88
Round  53, Global train loss: 1.171, Global test loss: 1.738, Global test accuracy: 43.90
Round  54, Train loss: 1.167, Test loss: 1.040, Test accuracy: 67.06
Round  54, Global train loss: 1.167, Global test loss: 1.922, Global test accuracy: 38.90
Round  55, Train loss: 1.106, Test loss: 1.045, Test accuracy: 67.09
Round  55, Global train loss: 1.106, Global test loss: 1.877, Global test accuracy: 38.45
Round  56, Train loss: 1.119, Test loss: 1.051, Test accuracy: 66.71
Round  56, Global train loss: 1.119, Global test loss: 1.994, Global test accuracy: 35.35
Round  57, Train loss: 1.203, Test loss: 1.051, Test accuracy: 66.87
Round  57, Global train loss: 1.203, Global test loss: 1.765, Global test accuracy: 41.93
Round  58, Train loss: 1.012, Test loss: 1.044, Test accuracy: 67.08
Round  58, Global train loss: 1.012, Global test loss: 1.751, Global test accuracy: 43.54
Round  59, Train loss: 1.154, Test loss: 1.087, Test accuracy: 65.84
Round  59, Global train loss: 1.154, Global test loss: 1.810, Global test accuracy: 41.11
Round  60, Train loss: 0.989, Test loss: 1.093, Test accuracy: 65.73
Round  60, Global train loss: 0.989, Global test loss: 1.787, Global test accuracy: 39.76
Round  61, Train loss: 1.137, Test loss: 1.086, Test accuracy: 65.89
Round  61, Global train loss: 1.137, Global test loss: 1.853, Global test accuracy: 38.12
Round  62, Train loss: 1.215, Test loss: 1.070, Test accuracy: 66.36
Round  62, Global train loss: 1.215, Global test loss: 1.795, Global test accuracy: 42.11
Round  63, Train loss: 1.127, Test loss: 1.063, Test accuracy: 66.97
Round  63, Global train loss: 1.127, Global test loss: 1.783, Global test accuracy: 41.84
Round  64, Train loss: 0.896, Test loss: 1.046, Test accuracy: 67.32
Round  64, Global train loss: 0.896, Global test loss: 1.820, Global test accuracy: 40.29
Round  65, Train loss: 0.863, Test loss: 1.077, Test accuracy: 66.63
Round  65, Global train loss: 0.863, Global test loss: 1.803, Global test accuracy: 40.54
Round  66, Train loss: 1.044, Test loss: 1.100, Test accuracy: 66.08
Round  66, Global train loss: 1.044, Global test loss: 1.895, Global test accuracy: 37.94
Round  67, Train loss: 1.253, Test loss: 1.096, Test accuracy: 65.82
Round  67, Global train loss: 1.253, Global test loss: 1.886, Global test accuracy: 36.67
Round  68, Train loss: 1.202, Test loss: 1.101, Test accuracy: 65.34
Round  68, Global train loss: 1.202, Global test loss: 1.975, Global test accuracy: 33.34
Round  69, Train loss: 0.981, Test loss: 1.103, Test accuracy: 65.52
Round  69, Global train loss: 0.981, Global test loss: 1.846, Global test accuracy: 40.00
Round  70, Train loss: 0.878, Test loss: 1.100, Test accuracy: 65.14
Round  70, Global train loss: 0.878, Global test loss: 1.785, Global test accuracy: 42.43
Round  71, Train loss: 0.937, Test loss: 1.099, Test accuracy: 65.24
Round  71, Global train loss: 0.937, Global test loss: 1.822, Global test accuracy: 41.76
Round  72, Train loss: 1.104, Test loss: 1.086, Test accuracy: 66.06
Round  72, Global train loss: 1.104, Global test loss: 1.906, Global test accuracy: 39.29
Round  73, Train loss: 1.134, Test loss: 1.097, Test accuracy: 66.23
Round  73, Global train loss: 1.134, Global test loss: 1.937, Global test accuracy: 40.32
Round  74, Train loss: 0.962, Test loss: 1.129, Test accuracy: 65.32
Round  74, Global train loss: 0.962, Global test loss: 1.815, Global test accuracy: 40.51
Round  75, Train loss: 1.086, Test loss: 1.125, Test accuracy: 65.33
Round  75, Global train loss: 1.086, Global test loss: 1.813, Global test accuracy: 41.93
Round  76, Train loss: 1.101, Test loss: 1.139, Test accuracy: 64.82
Round  76, Global train loss: 1.101, Global test loss: 1.931, Global test accuracy: 36.52
Round  77, Train loss: 0.863, Test loss: 1.162, Test accuracy: 64.35
Round  77, Global train loss: 0.863, Global test loss: 1.851, Global test accuracy: 41.05
Round  78, Train loss: 0.857, Test loss: 1.170, Test accuracy: 63.99
Round  78, Global train loss: 0.857, Global test loss: 1.909, Global test accuracy: 39.09
Round  79, Train loss: 1.064, Test loss: 1.140, Test accuracy: 64.76
Round  79, Global train loss: 1.064, Global test loss: 2.011, Global test accuracy: 34.89
Round  80, Train loss: 1.066, Test loss: 1.127, Test accuracy: 65.09
Round  80, Global train loss: 1.066, Global test loss: 1.903, Global test accuracy: 37.81
Round  81, Train loss: 0.912, Test loss: 1.146, Test accuracy: 64.96
Round  81, Global train loss: 0.912, Global test loss: 2.140, Global test accuracy: 33.37
Round  82, Train loss: 0.777, Test loss: 1.164, Test accuracy: 64.68
Round  82, Global train loss: 0.777, Global test loss: 1.946, Global test accuracy: 38.96
Round  83, Train loss: 1.024, Test loss: 1.165, Test accuracy: 64.99
Round  83, Global train loss: 1.024, Global test loss: 1.826, Global test accuracy: 39.49
Round  84, Train loss: 1.031, Test loss: 1.165, Test accuracy: 64.91
Round  84, Global train loss: 1.031, Global test loss: 1.885, Global test accuracy: 39.11
Round  85, Train loss: 0.857, Test loss: 1.183, Test accuracy: 64.51
Round  85, Global train loss: 0.857, Global test loss: 2.040, Global test accuracy: 37.60
Round  86, Train loss: 1.058, Test loss: 1.192, Test accuracy: 64.08
Round  86, Global train loss: 1.058, Global test loss: 1.943, Global test accuracy: 36.51
Round  87, Train loss: 0.994, Test loss: 1.186, Test accuracy: 64.52
Round  87, Global train loss: 0.994, Global test loss: 1.906, Global test accuracy: 38.96
Round  88, Train loss: 0.834, Test loss: 1.177, Test accuracy: 64.86
Round  88, Global train loss: 0.834, Global test loss: 1.940, Global test accuracy: 40.04
Round  89, Train loss: 0.866, Test loss: 1.188, Test accuracy: 64.28
Round  89, Global train loss: 0.866, Global test loss: 2.061, Global test accuracy: 39.24
Round  90, Train loss: 0.962, Test loss: 1.196, Test accuracy: 63.99
Round  90, Global train loss: 0.962, Global test loss: 2.121, Global test accuracy: 34.52
Round  91, Train loss: 0.923, Test loss: 1.204, Test accuracy: 64.21
Round  91, Global train loss: 0.923, Global test loss: 1.927, Global test accuracy: 36.46
Round  92, Train loss: 0.932, Test loss: 1.234, Test accuracy: 63.76
Round  92, Global train loss: 0.932, Global test loss: 1.951, Global test accuracy: 36.96
Round  93, Train loss: 0.849, Test loss: 1.248, Test accuracy: 63.01
Round  93, Global train loss: 0.849, Global test loss: 1.947, Global test accuracy: 38.54
Round  94, Train loss: 0.691, Test loss: 1.235, Test accuracy: 63.56
Round  94, Global train loss: 0.691, Global test loss: 1.927, Global test accuracy: 39.16
Round  95, Train loss: 0.806, Test loss: 1.243, Test accuracy: 63.29
Round  95, Global train loss: 0.806, Global test loss: 2.087, Global test accuracy: 35.17
Round  96, Train loss: 0.988, Test loss: 1.222, Test accuracy: 63.77
Round  96, Global train loss: 0.988, Global test loss: 2.104, Global test accuracy: 33.01
Round  97, Train loss: 0.826, Test loss: 1.229, Test accuracy: 64.12
Round  97, Global train loss: 0.826, Global test loss: 2.000, Global test accuracy: 38.31
Round  98, Train loss: 0.874, Test loss: 1.241, Test accuracy: 63.60
Round  98, Global train loss: 0.874, Global test loss: 2.097, Global test accuracy: 37.20
Round  99, Train loss: 0.941, Test loss: 1.246, Test accuracy: 62.94
Round  99, Global train loss: 0.941, Global test loss: 1.912, Global test accuracy: 39.39
Final Round, Train loss: 0.608, Test loss: 1.498, Test accuracy: 62.09
Final Round, Global train loss: 0.608, Global test loss: 1.912, Global test accuracy: 39.39
Average accuracy final 10 rounds: 63.625 

Average global accuracy final 10 rounds: 36.87111111111111 

2837.0736911296844
[2.2270941734313965, 4.454188346862793, 6.3773932456970215, 8.30059814453125, 10.21780014038086, 12.135002136230469, 14.150128841400146, 16.165255546569824, 18.124648571014404, 20.084041595458984, 22.060105085372925, 24.036168575286865, 25.976911067962646, 27.917653560638428, 29.87056612968445, 31.82347869873047, 33.79962706565857, 35.77577543258667, 37.7334668636322, 39.691158294677734, 41.68780255317688, 43.684446811676025, 45.62448215484619, 47.56451749801636, 49.51629161834717, 51.46806573867798, 53.401127338409424, 55.33418893814087, 57.267011404037476, 59.19983386993408, 61.12212157249451, 63.04440927505493, 64.9691116809845, 66.89381408691406, 68.81834888458252, 70.74288368225098, 72.68122148513794, 74.6195592880249, 76.5851366519928, 78.5507140159607, 80.47352194786072, 82.39632987976074, 84.34127902984619, 86.28622817993164, 88.24826192855835, 90.21029567718506, 92.16654515266418, 94.12279462814331, 96.09154510498047, 98.06029558181763, 99.99474430084229, 101.92919301986694, 103.86885619163513, 105.80851936340332, 107.74508309364319, 109.68164682388306, 111.62206220626831, 113.56247758865356, 115.52054452896118, 117.4786114692688, 119.39685869216919, 121.31510591506958, 123.23853158950806, 125.16195726394653, 127.08921575546265, 129.01647424697876, 130.93779492378235, 132.85911560058594, 134.79239559173584, 136.72567558288574, 138.65070295333862, 140.5757303237915, 142.53591513633728, 144.49609994888306, 146.41920256614685, 148.34230518341064, 150.2972731590271, 152.25224113464355, 154.18023490905762, 156.10822868347168, 158.05593848228455, 160.0036482810974, 161.9530861377716, 163.9025239944458, 165.8437271118164, 167.784930229187, 169.74928903579712, 171.71364784240723, 173.67363357543945, 175.63361930847168, 177.5728087425232, 179.5119981765747, 181.44303369522095, 183.3740692138672, 185.2991805076599, 187.22429180145264, 189.16480445861816, 191.1053171157837, 193.03638529777527, 194.96745347976685, 196.8690037727356, 198.77055406570435, 200.70150518417358, 202.63245630264282, 204.55261898040771, 206.4727816581726, 208.40765976905823, 210.34253787994385, 212.26544857025146, 214.18835926055908, 216.1196048259735, 218.05085039138794, 219.95752835273743, 221.8642063140869, 223.8043611049652, 225.7445158958435, 227.66183066368103, 229.57914543151855, 231.51603984832764, 233.45293426513672, 235.39626479148865, 237.33959531784058, 239.27863025665283, 241.2176651954651, 243.26248168945312, 245.30729818344116, 247.27752232551575, 249.24774646759033, 251.2383360862732, 253.22892570495605, 255.20067954063416, 257.17243337631226, 259.19533801078796, 261.2182426452637, 263.16733145713806, 265.11642026901245, 267.0429322719574, 268.96944427490234, 270.8911519050598, 272.8128595352173, 274.7396032810211, 276.66634702682495, 278.85667395591736, 281.04700088500977, 283.27314257621765, 285.49928426742554, 287.72285890579224, 289.94643354415894, 292.1939389705658, 294.44144439697266, 296.6509232521057, 298.86040210723877, 301.06904006004333, 303.2776780128479, 305.50235986709595, 307.727041721344, 310.103866815567, 312.48069190979004, 314.8443477153778, 317.2080035209656, 319.57945132255554, 321.9508991241455, 324.2926185131073, 326.6343379020691, 328.9632935523987, 331.29224920272827, 333.62760829925537, 335.96296739578247, 338.2961959838867, 340.62942457199097, 342.96134877204895, 345.29327297210693, 347.6637270450592, 350.0341811180115, 352.3778383731842, 354.72149562835693, 356.84315490722656, 358.9648141860962, 361.18842911720276, 363.4120440483093, 365.65352392196655, 367.8950037956238, 370.1259582042694, 372.35691261291504, 374.6069450378418, 376.85697746276855, 379.1410014629364, 381.42502546310425, 383.75391364097595, 386.08280181884766, 388.4387400150299, 390.79467821121216, 393.14029574394226, 395.48591327667236, 397.82567620277405, 400.16543912887573, 402.512953042984, 404.8604669570923, 407.2133467197418, 409.56622648239136, 412.181494474411, 414.79676246643066]
[25.772222222222222, 25.772222222222222, 43.672222222222224, 43.672222222222224, 51.37222222222222, 51.37222222222222, 61.25, 61.25, 63.22222222222222, 63.22222222222222, 65.82222222222222, 65.82222222222222, 66.47222222222223, 66.47222222222223, 67.17222222222222, 67.17222222222222, 70.01666666666667, 70.01666666666667, 70.58888888888889, 70.58888888888889, 70.84444444444445, 70.84444444444445, 70.65555555555555, 70.65555555555555, 70.57222222222222, 70.57222222222222, 70.17777777777778, 70.17777777777778, 70.24444444444444, 70.24444444444444, 70.86111111111111, 70.86111111111111, 72.27222222222223, 72.27222222222223, 72.16666666666667, 72.16666666666667, 72.40555555555555, 72.40555555555555, 72.70555555555555, 72.70555555555555, 72.74444444444444, 72.74444444444444, 73.02777777777777, 73.02777777777777, 72.41666666666667, 72.41666666666667, 72.16111111111111, 72.16111111111111, 72.17222222222222, 72.17222222222222, 72.37777777777778, 72.37777777777778, 71.41666666666667, 71.41666666666667, 71.43888888888888, 71.43888888888888, 71.71111111111111, 71.71111111111111, 71.69444444444444, 71.69444444444444, 72.02222222222223, 72.02222222222223, 71.47777777777777, 71.47777777777777, 71.4, 71.4, 70.3, 70.3, 70.31111111111112, 70.31111111111112, 70.47777777777777, 70.47777777777777, 70.64444444444445, 70.64444444444445, 70.62777777777778, 70.62777777777778, 70.40555555555555, 70.40555555555555, 71.16666666666667, 71.16666666666667, 70.57222222222222, 70.57222222222222, 70.86666666666666, 70.86666666666666, 70.62777777777778, 70.62777777777778, 69.99444444444444, 69.99444444444444, 69.98333333333333, 69.98333333333333, 69.48333333333333, 69.48333333333333, 69.44444444444444, 69.44444444444444, 69.87777777777778, 69.87777777777778, 69.46111111111111, 69.46111111111111, 69.11666666666666, 69.11666666666666, 67.88333333333334, 67.88333333333334, 67.88888888888889, 67.88888888888889, 67.25555555555556, 67.25555555555556, 66.87777777777778, 66.87777777777778, 67.05555555555556, 67.05555555555556, 67.08888888888889, 67.08888888888889, 66.70555555555555, 66.70555555555555, 66.87222222222222, 66.87222222222222, 67.08333333333333, 67.08333333333333, 65.84444444444445, 65.84444444444445, 65.73333333333333, 65.73333333333333, 65.88888888888889, 65.88888888888889, 66.35555555555555, 66.35555555555555, 66.97222222222223, 66.97222222222223, 67.32222222222222, 67.32222222222222, 66.62777777777778, 66.62777777777778, 66.08333333333333, 66.08333333333333, 65.82222222222222, 65.82222222222222, 65.33888888888889, 65.33888888888889, 65.52222222222223, 65.52222222222223, 65.14444444444445, 65.14444444444445, 65.24444444444444, 65.24444444444444, 66.06111111111112, 66.06111111111112, 66.22777777777777, 66.22777777777777, 65.31666666666666, 65.31666666666666, 65.33333333333333, 65.33333333333333, 64.81666666666666, 64.81666666666666, 64.35, 64.35, 63.98888888888889, 63.98888888888889, 64.75555555555556, 64.75555555555556, 65.08888888888889, 65.08888888888889, 64.96111111111111, 64.96111111111111, 64.67777777777778, 64.67777777777778, 64.9888888888889, 64.9888888888889, 64.91111111111111, 64.91111111111111, 64.50555555555556, 64.50555555555556, 64.07777777777778, 64.07777777777778, 64.51666666666667, 64.51666666666667, 64.85555555555555, 64.85555555555555, 64.27777777777777, 64.27777777777777, 63.98888888888889, 63.98888888888889, 64.21111111111111, 64.21111111111111, 63.75555555555555, 63.75555555555555, 63.01111111111111, 63.01111111111111, 63.56111111111111, 63.56111111111111, 63.294444444444444, 63.294444444444444, 63.772222222222226, 63.772222222222226, 64.11666666666666, 64.11666666666666, 63.6, 63.6, 62.93888888888889, 62.93888888888889, 62.09444444444444, 62.09444444444444]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8820
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5395
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.5090
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7360
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8540
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5160
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8145
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8260
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5790
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7730
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7025
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8985
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.6255
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6265
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7965
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6880
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.559, Test loss: 2.015, Test accuracy: 34.23
Round   0, Global train loss: 1.559, Global test loss: 2.199, Global test accuracy: 31.96
Round   1, Train loss: 1.301, Test loss: 1.864, Test accuracy: 40.17
Round   1, Global train loss: 1.301, Global test loss: 2.109, Global test accuracy: 34.73
Round   2, Train loss: 1.603, Test loss: 1.523, Test accuracy: 47.32
Round   2, Global train loss: 1.603, Global test loss: 1.996, Global test accuracy: 32.79
Round   3, Train loss: 1.613, Test loss: 1.390, Test accuracy: 53.68
Round   3, Global train loss: 1.613, Global test loss: 1.939, Global test accuracy: 35.85
Round   4, Train loss: 1.621, Test loss: 1.372, Test accuracy: 54.21
Round   4, Global train loss: 1.621, Global test loss: 1.924, Global test accuracy: 40.73
Round   5, Train loss: 1.196, Test loss: 1.363, Test accuracy: 54.47
Round   5, Global train loss: 1.196, Global test loss: 1.880, Global test accuracy: 34.71
Round   6, Train loss: 1.327, Test loss: 1.276, Test accuracy: 58.26
Round   6, Global train loss: 1.327, Global test loss: 1.901, Global test accuracy: 34.20
Round   7, Train loss: 1.281, Test loss: 1.247, Test accuracy: 59.24
Round   7, Global train loss: 1.281, Global test loss: 1.838, Global test accuracy: 35.67
Round   8, Train loss: 1.294, Test loss: 1.246, Test accuracy: 60.02
Round   8, Global train loss: 1.294, Global test loss: 1.862, Global test accuracy: 38.75
Round   9, Train loss: 1.209, Test loss: 1.190, Test accuracy: 61.15
Round   9, Global train loss: 1.209, Global test loss: 1.776, Global test accuracy: 40.72
Round  10, Train loss: 1.495, Test loss: 1.207, Test accuracy: 61.02
Round  10, Global train loss: 1.495, Global test loss: 1.909, Global test accuracy: 32.66
Round  11, Train loss: 1.596, Test loss: 1.168, Test accuracy: 62.98
Round  11, Global train loss: 1.596, Global test loss: 1.803, Global test accuracy: 45.21
Round  12, Train loss: 1.409, Test loss: 1.139, Test accuracy: 63.42
Round  12, Global train loss: 1.409, Global test loss: 1.727, Global test accuracy: 42.45
Round  13, Train loss: 1.528, Test loss: 1.149, Test accuracy: 64.23
Round  13, Global train loss: 1.528, Global test loss: 1.730, Global test accuracy: 46.34
Round  14, Train loss: 1.404, Test loss: 1.141, Test accuracy: 64.11
Round  14, Global train loss: 1.404, Global test loss: 1.790, Global test accuracy: 45.16
Round  15, Train loss: 1.210, Test loss: 1.143, Test accuracy: 64.28
Round  15, Global train loss: 1.210, Global test loss: 1.685, Global test accuracy: 45.12
Round  16, Train loss: 1.312, Test loss: 1.135, Test accuracy: 64.74
Round  16, Global train loss: 1.312, Global test loss: 1.723, Global test accuracy: 43.13
Round  17, Train loss: 1.053, Test loss: 1.126, Test accuracy: 65.82
Round  17, Global train loss: 1.053, Global test loss: 1.663, Global test accuracy: 46.04
Round  18, Train loss: 1.284, Test loss: 1.124, Test accuracy: 65.99
Round  18, Global train loss: 1.284, Global test loss: 1.728, Global test accuracy: 41.64
Round  19, Train loss: 0.943, Test loss: 1.122, Test accuracy: 65.79
Round  19, Global train loss: 0.943, Global test loss: 1.716, Global test accuracy: 43.87
Round  20, Train loss: 1.505, Test loss: 1.116, Test accuracy: 65.36
Round  20, Global train loss: 1.505, Global test loss: 1.703, Global test accuracy: 45.29
Round  21, Train loss: 1.246, Test loss: 1.110, Test accuracy: 65.70
Round  21, Global train loss: 1.246, Global test loss: 1.737, Global test accuracy: 43.98
Round  22, Train loss: 1.274, Test loss: 1.113, Test accuracy: 65.63
Round  22, Global train loss: 1.274, Global test loss: 1.625, Global test accuracy: 47.93
Round  23, Train loss: 1.330, Test loss: 1.112, Test accuracy: 66.13
Round  23, Global train loss: 1.330, Global test loss: 1.626, Global test accuracy: 49.98
Round  24, Train loss: 1.242, Test loss: 1.122, Test accuracy: 65.46
Round  24, Global train loss: 1.242, Global test loss: 1.597, Global test accuracy: 48.90
Round  25, Train loss: 1.371, Test loss: 1.111, Test accuracy: 65.74
Round  25, Global train loss: 1.371, Global test loss: 1.658, Global test accuracy: 49.12
Round  26, Train loss: 1.103, Test loss: 1.085, Test accuracy: 66.70
Round  26, Global train loss: 1.103, Global test loss: 1.582, Global test accuracy: 48.65
Round  27, Train loss: 1.198, Test loss: 1.110, Test accuracy: 65.83
Round  27, Global train loss: 1.198, Global test loss: 1.625, Global test accuracy: 48.75
Round  28, Train loss: 0.915, Test loss: 1.101, Test accuracy: 66.27
Round  28, Global train loss: 0.915, Global test loss: 1.631, Global test accuracy: 46.12
Round  29, Train loss: 1.349, Test loss: 1.078, Test accuracy: 66.62
Round  29, Global train loss: 1.349, Global test loss: 1.691, Global test accuracy: 47.30
Round  30, Train loss: 1.172, Test loss: 1.076, Test accuracy: 66.69
Round  30, Global train loss: 1.172, Global test loss: 1.656, Global test accuracy: 46.96
Round  31, Train loss: 1.183, Test loss: 1.098, Test accuracy: 65.36
Round  31, Global train loss: 1.183, Global test loss: 1.606, Global test accuracy: 49.11
Round  32, Train loss: 1.306, Test loss: 1.094, Test accuracy: 66.43
Round  32, Global train loss: 1.306, Global test loss: 1.718, Global test accuracy: 45.61
Round  33, Train loss: 1.268, Test loss: 1.110, Test accuracy: 65.21
Round  33, Global train loss: 1.268, Global test loss: 1.730, Global test accuracy: 43.61
Round  34, Train loss: 1.088, Test loss: 1.108, Test accuracy: 64.55
Round  34, Global train loss: 1.088, Global test loss: 1.633, Global test accuracy: 47.85
Round  35, Train loss: 1.050, Test loss: 1.099, Test accuracy: 65.43
Round  35, Global train loss: 1.050, Global test loss: 1.575, Global test accuracy: 47.93
Round  36, Train loss: 0.991, Test loss: 1.097, Test accuracy: 65.03
Round  36, Global train loss: 0.991, Global test loss: 1.637, Global test accuracy: 47.11
Round  37, Train loss: 1.305, Test loss: 1.107, Test accuracy: 64.86
Round  37, Global train loss: 1.305, Global test loss: 1.653, Global test accuracy: 48.29
Round  38, Train loss: 1.054, Test loss: 1.098, Test accuracy: 65.34
Round  38, Global train loss: 1.054, Global test loss: 1.689, Global test accuracy: 45.40
Round  39, Train loss: 0.878, Test loss: 1.085, Test accuracy: 65.36
Round  39, Global train loss: 0.878, Global test loss: 1.503, Global test accuracy: 50.36
Round  40, Train loss: 0.953, Test loss: 1.085, Test accuracy: 65.55
Round  40, Global train loss: 0.953, Global test loss: 1.504, Global test accuracy: 50.15
Round  41, Train loss: 1.124, Test loss: 1.083, Test accuracy: 66.19
Round  41, Global train loss: 1.124, Global test loss: 1.558, Global test accuracy: 50.71
Round  42, Train loss: 1.244, Test loss: 1.135, Test accuracy: 64.75
Round  42, Global train loss: 1.244, Global test loss: 1.750, Global test accuracy: 42.61
Round  43, Train loss: 1.065, Test loss: 1.121, Test accuracy: 64.60
Round  43, Global train loss: 1.065, Global test loss: 1.646, Global test accuracy: 48.11
Round  44, Train loss: 1.104, Test loss: 1.089, Test accuracy: 64.62
Round  44, Global train loss: 1.104, Global test loss: 1.720, Global test accuracy: 44.42
Round  45, Train loss: 0.988, Test loss: 1.113, Test accuracy: 63.60
Round  45, Global train loss: 0.988, Global test loss: 1.579, Global test accuracy: 48.26
Round  46, Train loss: 1.215, Test loss: 1.136, Test accuracy: 63.18
Round  46, Global train loss: 1.215, Global test loss: 1.662, Global test accuracy: 48.29
Round  47, Train loss: 1.106, Test loss: 1.140, Test accuracy: 63.26
Round  47, Global train loss: 1.106, Global test loss: 1.723, Global test accuracy: 45.54
Round  48, Train loss: 1.113, Test loss: 1.178, Test accuracy: 61.94
Round  48, Global train loss: 1.113, Global test loss: 1.726, Global test accuracy: 42.10
Round  49, Train loss: 0.743, Test loss: 1.198, Test accuracy: 61.23
Round  49, Global train loss: 0.743, Global test loss: 1.544, Global test accuracy: 50.39
Round  50, Train loss: 0.842, Test loss: 1.166, Test accuracy: 62.43
Round  50, Global train loss: 0.842, Global test loss: 1.570, Global test accuracy: 47.58
Round  51, Train loss: 1.078, Test loss: 1.148, Test accuracy: 63.73
Round  51, Global train loss: 1.078, Global test loss: 1.609, Global test accuracy: 49.43
Round  52, Train loss: 1.010, Test loss: 1.111, Test accuracy: 64.83
Round  52, Global train loss: 1.010, Global test loss: 1.666, Global test accuracy: 46.23
Round  53, Train loss: 1.042, Test loss: 1.139, Test accuracy: 64.01
Round  53, Global train loss: 1.042, Global test loss: 1.704, Global test accuracy: 46.71
Round  54, Train loss: 1.019, Test loss: 1.167, Test accuracy: 63.08
Round  54, Global train loss: 1.019, Global test loss: 1.677, Global test accuracy: 45.79
Round  55, Train loss: 1.083, Test loss: 1.147, Test accuracy: 63.51
Round  55, Global train loss: 1.083, Global test loss: 1.654, Global test accuracy: 46.52
Round  56, Train loss: 1.040, Test loss: 1.154, Test accuracy: 63.62
Round  56, Global train loss: 1.040, Global test loss: 1.758, Global test accuracy: 42.39
Round  57, Train loss: 0.863, Test loss: 1.154, Test accuracy: 63.92
Round  57, Global train loss: 0.863, Global test loss: 1.694, Global test accuracy: 47.97
Round  58, Train loss: 1.006, Test loss: 1.174, Test accuracy: 63.45
Round  58, Global train loss: 1.006, Global test loss: 1.698, Global test accuracy: 46.18
Round  59, Train loss: 1.138, Test loss: 1.195, Test accuracy: 62.55
Round  59, Global train loss: 1.138, Global test loss: 1.730, Global test accuracy: 43.12
Round  60, Train loss: 0.795, Test loss: 1.162, Test accuracy: 62.98
Round  60, Global train loss: 0.795, Global test loss: 1.674, Global test accuracy: 48.27
Round  61, Train loss: 0.726, Test loss: 1.172, Test accuracy: 62.83
Round  61, Global train loss: 0.726, Global test loss: 1.620, Global test accuracy: 49.05
Round  62, Train loss: 1.084, Test loss: 1.199, Test accuracy: 62.13
Round  62, Global train loss: 1.084, Global test loss: 1.759, Global test accuracy: 43.41
Round  63, Train loss: 1.118, Test loss: 1.155, Test accuracy: 63.47
Round  63, Global train loss: 1.118, Global test loss: 1.776, Global test accuracy: 42.72
Round  64, Train loss: 0.967, Test loss: 1.162, Test accuracy: 63.23
Round  64, Global train loss: 0.967, Global test loss: 1.819, Global test accuracy: 43.16
Round  65, Train loss: 0.808, Test loss: 1.190, Test accuracy: 62.29
Round  65, Global train loss: 0.808, Global test loss: 1.972, Global test accuracy: 42.87
Round  66, Train loss: 0.933, Test loss: 1.209, Test accuracy: 61.55
Round  66, Global train loss: 0.933, Global test loss: 1.832, Global test accuracy: 42.82
Round  67, Train loss: 0.909, Test loss: 1.192, Test accuracy: 62.32
Round  67, Global train loss: 0.909, Global test loss: 1.804, Global test accuracy: 43.70
Round  68, Train loss: 1.122, Test loss: 1.222, Test accuracy: 62.16
Round  68, Global train loss: 1.122, Global test loss: 1.783, Global test accuracy: 43.26
Round  69, Train loss: 0.723, Test loss: 1.222, Test accuracy: 62.14
Round  69, Global train loss: 0.723, Global test loss: 1.757, Global test accuracy: 46.02
Round  70, Train loss: 0.970, Test loss: 1.226, Test accuracy: 62.32
Round  70, Global train loss: 0.970, Global test loss: 1.784, Global test accuracy: 42.87
Round  71, Train loss: 0.912, Test loss: 1.254, Test accuracy: 61.89
Round  71, Global train loss: 0.912, Global test loss: 1.825, Global test accuracy: 43.83
Round  72, Train loss: 0.883, Test loss: 1.304, Test accuracy: 61.52
Round  72, Global train loss: 0.883, Global test loss: 1.740, Global test accuracy: 44.41
Round  73, Train loss: 0.877, Test loss: 1.288, Test accuracy: 61.93
Round  73, Global train loss: 0.877, Global test loss: 1.782, Global test accuracy: 43.77
Round  74, Train loss: 1.025, Test loss: 1.277, Test accuracy: 61.67
Round  74, Global train loss: 1.025, Global test loss: 1.714, Global test accuracy: 45.33
Round  75, Train loss: 0.838, Test loss: 1.314, Test accuracy: 61.06
Round  75, Global train loss: 0.838, Global test loss: 1.863, Global test accuracy: 43.22
Round  76, Train loss: 0.570, Test loss: 1.300, Test accuracy: 61.17
Round  76, Global train loss: 0.570, Global test loss: 1.817, Global test accuracy: 45.07
Round  77, Train loss: 0.780, Test loss: 1.298, Test accuracy: 61.29
Round  77, Global train loss: 0.780, Global test loss: 1.851, Global test accuracy: 44.36
Round  78, Train loss: 0.875, Test loss: 1.330, Test accuracy: 60.95
Round  78, Global train loss: 0.875, Global test loss: 1.861, Global test accuracy: 41.89
Round  79, Train loss: 0.964, Test loss: 1.305, Test accuracy: 61.84
Round  79, Global train loss: 0.964, Global test loss: 1.790, Global test accuracy: 42.81
Round  80, Train loss: 0.532, Test loss: 1.290, Test accuracy: 62.20
Round  80, Global train loss: 0.532, Global test loss: 1.904, Global test accuracy: 45.12
Round  81, Train loss: 0.750, Test loss: 1.315, Test accuracy: 61.42
Round  81, Global train loss: 0.750, Global test loss: 1.891, Global test accuracy: 43.33
Round  82, Train loss: 0.847, Test loss: 1.330, Test accuracy: 61.13
Round  82, Global train loss: 0.847, Global test loss: 1.954, Global test accuracy: 42.83
Round  83, Train loss: 0.747, Test loss: 1.357, Test accuracy: 61.17
Round  83, Global train loss: 0.747, Global test loss: 1.948, Global test accuracy: 43.12
Round  84, Train loss: 1.037, Test loss: 1.346, Test accuracy: 61.67
Round  84, Global train loss: 1.037, Global test loss: 2.034, Global test accuracy: 38.34
Round  85, Train loss: 0.616, Test loss: 1.364, Test accuracy: 60.95
Round  85, Global train loss: 0.616, Global test loss: 2.061, Global test accuracy: 43.34
Round  86, Train loss: 0.697, Test loss: 1.407, Test accuracy: 59.84
Round  86, Global train loss: 0.697, Global test loss: 1.905, Global test accuracy: 44.95
Round  87, Train loss: 0.837, Test loss: 1.372, Test accuracy: 60.77
Round  87, Global train loss: 0.837, Global test loss: 1.877, Global test accuracy: 43.49
Round  88, Train loss: 0.774, Test loss: 1.386, Test accuracy: 61.08
Round  88, Global train loss: 0.774, Global test loss: 1.994, Global test accuracy: 41.51
Round  89, Train loss: 0.827, Test loss: 1.424, Test accuracy: 60.65
Round  89, Global train loss: 0.827, Global test loss: 2.008, Global test accuracy: 39.34
Round  90, Train loss: 0.609, Test loss: 1.364, Test accuracy: 60.88
Round  90, Global train loss: 0.609, Global test loss: 1.954, Global test accuracy: 40.42
Round  91, Train loss: 0.570, Test loss: 1.349, Test accuracy: 60.98
Round  91, Global train loss: 0.570, Global test loss: 2.013, Global test accuracy: 43.62
Round  92, Train loss: 0.809, Test loss: 1.376, Test accuracy: 60.74
Round  92, Global train loss: 0.809, Global test loss: 2.014, Global test accuracy: 40.46
Round  93, Train loss: 0.584, Test loss: 1.370, Test accuracy: 61.40
Round  93, Global train loss: 0.584, Global test loss: 1.886, Global test accuracy: 43.46
Round  94, Train loss: 1.009, Test loss: 1.413, Test accuracy: 61.54
Round  94, Global train loss: 1.009, Global test loss: 1.974, Global test accuracy: 41.04
Round  95, Train loss: 0.896, Test loss: 1.420, Test accuracy: 61.33
Round  95, Global train loss: 0.896, Global test loss: 2.264, Global test accuracy: 32.27
Round  96, Train loss: 0.798, Test loss: 1.357, Test accuracy: 62.77
Round  96, Global train loss: 0.798, Global test loss: 2.014, Global test accuracy: 40.82
Round  97, Train loss: 0.719, Test loss: 1.395, Test accuracy: 61.83
Round  97, Global train loss: 0.719, Global test loss: 2.147, Global test accuracy: 41.81
Round  98, Train loss: 0.827, Test loss: 1.411, Test accuracy: 61.17
Round  98, Global train loss: 0.827, Global test loss: 1.916, Global test accuracy: 41.92
Round  99, Train loss: 0.755, Test loss: 1.428, Test accuracy: 60.86
Round  99, Global train loss: 0.755, Global test loss: 1.960, Global test accuracy: 40.90
Final Round, Train loss: 0.597, Test loss: 1.589, Test accuracy: 60.10
Final Round, Global train loss: 0.597, Global test loss: 1.960, Global test accuracy: 40.90
Average accuracy final 10 rounds: 61.34916666666666 

Average global accuracy final 10 rounds: 40.670833333333334 

1953.5274722576141
[1.8082952499389648, 3.6165904998779297, 5.137432813644409, 6.658275127410889, 8.18984842300415, 9.721421718597412, 11.305721044540405, 12.890020370483398, 14.480929851531982, 16.071839332580566, 17.659459352493286, 19.247079372406006, 20.85555601119995, 22.464032649993896, 23.86139440536499, 25.258756160736084, 26.658519744873047, 28.05828332901001, 29.46810483932495, 30.877926349639893, 32.28289604187012, 33.68786573410034, 35.08864212036133, 36.489418506622314, 37.89429473876953, 39.29917097091675, 40.72118067741394, 42.14319038391113, 43.55525851249695, 44.967326641082764, 46.3836510181427, 47.79997539520264, 49.224164724349976, 50.648354053497314, 52.082934617996216, 53.51751518249512, 54.923792600631714, 56.33007001876831, 57.74379277229309, 59.15751552581787, 60.6117148399353, 62.065914154052734, 63.477823972702026, 64.88973379135132, 66.32054138183594, 67.75134897232056, 69.1762535572052, 70.60115814208984, 72.08948969841003, 73.57782125473022, 75.0507264137268, 76.52363157272339, 77.94331002235413, 79.36298847198486, 80.79676675796509, 82.23054504394531, 83.6517276763916, 85.07291030883789, 86.48675632476807, 87.90060234069824, 89.30837917327881, 90.71615600585938, 92.13216519355774, 93.5481743812561, 94.95979404449463, 96.37141370773315, 97.77065134048462, 99.16988897323608, 100.60004019737244, 102.03019142150879, 103.44080972671509, 104.85142803192139, 106.26972913742065, 107.68803024291992, 109.09445476531982, 110.50087928771973, 111.90957713127136, 113.318274974823, 114.73751139640808, 116.15674781799316, 117.57770204544067, 118.99865627288818, 120.40961599349976, 121.82057571411133, 123.23507809638977, 124.64958047866821, 126.06811237335205, 127.48664426803589, 128.91059923171997, 130.33455419540405, 131.75422954559326, 133.17390489578247, 134.59853434562683, 136.0231637954712, 137.43466925621033, 138.84617471694946, 140.27367997169495, 141.70118522644043, 143.13470888137817, 144.56823253631592, 145.99515223503113, 147.42207193374634, 148.84365463256836, 150.26523733139038, 151.69558429718018, 153.12593126296997, 154.55185437202454, 155.9777774810791, 157.4006862640381, 158.82359504699707, 160.23541283607483, 161.6472306251526, 163.05885767936707, 164.47048473358154, 165.88183045387268, 167.29317617416382, 168.70430612564087, 170.11543607711792, 171.52623748779297, 172.93703889846802, 174.36179971694946, 175.7865605354309, 177.20247197151184, 178.61838340759277, 180.03428483009338, 181.450186252594, 182.86321687698364, 184.2762475013733, 185.6811146736145, 187.0859818458557, 188.51771521568298, 189.94944858551025, 191.38691902160645, 192.82438945770264, 194.23791408538818, 195.65143871307373, 197.04062271118164, 198.42980670928955, 199.82391333580017, 201.2180199623108, 202.61360621452332, 204.00919246673584, 205.3922140598297, 206.77523565292358, 208.16471219062805, 209.55418872833252, 210.94714450836182, 212.3401002883911, 213.72878503799438, 215.11746978759766, 216.50530099868774, 217.89313220977783, 219.28215909004211, 220.6711859703064, 222.05546641349792, 223.43974685668945, 224.82463121414185, 226.20951557159424, 227.5973780155182, 228.98524045944214, 230.3779354095459, 231.77063035964966, 233.1642050743103, 234.55777978897095, 235.93899583816528, 237.32021188735962, 238.70634984970093, 240.09248781204224, 241.47593116760254, 242.85937452316284, 244.2385733127594, 245.61777210235596, 246.99836421012878, 248.3789563179016, 249.75825238227844, 251.13754844665527, 252.51258754730225, 253.88762664794922, 255.26441621780396, 256.6412057876587, 258.025750875473, 259.41029596328735, 260.79746747016907, 262.1846389770508, 263.5691430568695, 264.95364713668823, 266.33358359336853, 267.7135200500488, 269.09673285484314, 270.47994565963745, 271.8535180091858, 273.22709035873413, 274.60971117019653, 275.99233198165894, 277.3758580684662, 278.75938415527344, 280.1437666416168, 281.5281491279602, 282.8926455974579, 284.25714206695557, 286.560551404953, 288.86396074295044]
[34.233333333333334, 34.233333333333334, 40.166666666666664, 40.166666666666664, 47.31666666666667, 47.31666666666667, 53.68333333333333, 53.68333333333333, 54.208333333333336, 54.208333333333336, 54.46666666666667, 54.46666666666667, 58.25833333333333, 58.25833333333333, 59.24166666666667, 59.24166666666667, 60.025, 60.025, 61.15, 61.15, 61.025, 61.025, 62.983333333333334, 62.983333333333334, 63.425, 63.425, 64.23333333333333, 64.23333333333333, 64.10833333333333, 64.10833333333333, 64.28333333333333, 64.28333333333333, 64.74166666666666, 64.74166666666666, 65.81666666666666, 65.81666666666666, 65.99166666666666, 65.99166666666666, 65.79166666666667, 65.79166666666667, 65.35833333333333, 65.35833333333333, 65.7, 65.7, 65.63333333333334, 65.63333333333334, 66.13333333333334, 66.13333333333334, 65.45833333333333, 65.45833333333333, 65.74166666666666, 65.74166666666666, 66.7, 66.7, 65.83333333333333, 65.83333333333333, 66.26666666666667, 66.26666666666667, 66.61666666666666, 66.61666666666666, 66.69166666666666, 66.69166666666666, 65.35833333333333, 65.35833333333333, 66.43333333333334, 66.43333333333334, 65.20833333333333, 65.20833333333333, 64.55, 64.55, 65.43333333333334, 65.43333333333334, 65.03333333333333, 65.03333333333333, 64.85833333333333, 64.85833333333333, 65.34166666666667, 65.34166666666667, 65.35833333333333, 65.35833333333333, 65.55, 65.55, 66.19166666666666, 66.19166666666666, 64.75, 64.75, 64.6, 64.6, 64.625, 64.625, 63.6, 63.6, 63.18333333333333, 63.18333333333333, 63.25833333333333, 63.25833333333333, 61.94166666666667, 61.94166666666667, 61.233333333333334, 61.233333333333334, 62.43333333333333, 62.43333333333333, 63.733333333333334, 63.733333333333334, 64.825, 64.825, 64.00833333333334, 64.00833333333334, 63.083333333333336, 63.083333333333336, 63.50833333333333, 63.50833333333333, 63.61666666666667, 63.61666666666667, 63.925, 63.925, 63.45, 63.45, 62.55, 62.55, 62.983333333333334, 62.983333333333334, 62.825, 62.825, 62.13333333333333, 62.13333333333333, 63.46666666666667, 63.46666666666667, 63.225, 63.225, 62.291666666666664, 62.291666666666664, 61.55, 61.55, 62.31666666666667, 62.31666666666667, 62.15833333333333, 62.15833333333333, 62.141666666666666, 62.141666666666666, 62.31666666666667, 62.31666666666667, 61.891666666666666, 61.891666666666666, 61.525, 61.525, 61.93333333333333, 61.93333333333333, 61.666666666666664, 61.666666666666664, 61.05833333333333, 61.05833333333333, 61.175, 61.175, 61.291666666666664, 61.291666666666664, 60.95, 60.95, 61.84166666666667, 61.84166666666667, 62.2, 62.2, 61.416666666666664, 61.416666666666664, 61.13333333333333, 61.13333333333333, 61.166666666666664, 61.166666666666664, 61.666666666666664, 61.666666666666664, 60.95, 60.95, 59.84166666666667, 59.84166666666667, 60.775, 60.775, 61.083333333333336, 61.083333333333336, 60.65, 60.65, 60.88333333333333, 60.88333333333333, 60.975, 60.975, 60.74166666666667, 60.74166666666667, 61.4, 61.4, 61.541666666666664, 61.541666666666664, 61.333333333333336, 61.333333333333336, 62.766666666666666, 62.766666666666666, 61.825, 61.825, 61.166666666666664, 61.166666666666664, 60.858333333333334, 60.858333333333334, 60.1, 60.1]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8855
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5375
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4635
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7635
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8550
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4715
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8185
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8225
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5240
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7630
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7615
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8900
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.6205
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.7200
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8105
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7965
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.251, Test loss: 2.111, Test accuracy: 27.46
Round   1, Train loss: 2.125, Test loss: 1.927, Test accuracy: 34.51
Round   2, Train loss: 1.887, Test loss: 1.714, Test accuracy: 40.89
Round   3, Train loss: 1.822, Test loss: 1.673, Test accuracy: 43.18
Round   4, Train loss: 1.749, Test loss: 1.621, Test accuracy: 47.30
Round   5, Train loss: 1.919, Test loss: 1.624, Test accuracy: 49.02
Round   6, Train loss: 1.746, Test loss: 1.547, Test accuracy: 50.76
Round   7, Train loss: 1.927, Test loss: 1.575, Test accuracy: 51.68
Round   8, Train loss: 1.692, Test loss: 1.532, Test accuracy: 52.73
Round   9, Train loss: 1.758, Test loss: 1.507, Test accuracy: 53.91
Round  10, Train loss: 1.802, Test loss: 1.472, Test accuracy: 54.73
Round  11, Train loss: 1.769, Test loss: 1.436, Test accuracy: 55.69
Round  12, Train loss: 1.734, Test loss: 1.414, Test accuracy: 56.56
Round  13, Train loss: 1.546, Test loss: 1.397, Test accuracy: 56.95
Round  14, Train loss: 1.841, Test loss: 1.404, Test accuracy: 57.74
Round  15, Train loss: 1.616, Test loss: 1.399, Test accuracy: 58.16
Round  16, Train loss: 1.423, Test loss: 1.356, Test accuracy: 58.55
Round  17, Train loss: 1.744, Test loss: 1.353, Test accuracy: 58.86
Round  18, Train loss: 1.586, Test loss: 1.334, Test accuracy: 59.90
Round  19, Train loss: 1.538, Test loss: 1.293, Test accuracy: 61.44
Round  20, Train loss: 1.419, Test loss: 1.268, Test accuracy: 62.06
Round  21, Train loss: 1.549, Test loss: 1.243, Test accuracy: 62.91
Round  22, Train loss: 1.415, Test loss: 1.230, Test accuracy: 63.73
Round  23, Train loss: 1.356, Test loss: 1.228, Test accuracy: 64.29
Round  24, Train loss: 1.750, Test loss: 1.239, Test accuracy: 63.77
Round  25, Train loss: 1.535, Test loss: 1.237, Test accuracy: 64.42
Round  26, Train loss: 1.345, Test loss: 1.203, Test accuracy: 64.45
Round  27, Train loss: 1.541, Test loss: 1.202, Test accuracy: 64.75
Round  28, Train loss: 1.274, Test loss: 1.185, Test accuracy: 65.18
Round  29, Train loss: 1.510, Test loss: 1.191, Test accuracy: 64.83
Round  30, Train loss: 1.602, Test loss: 1.181, Test accuracy: 65.47
Round  31, Train loss: 1.410, Test loss: 1.170, Test accuracy: 66.16
Round  32, Train loss: 1.482, Test loss: 1.179, Test accuracy: 65.66
Round  33, Train loss: 1.321, Test loss: 1.190, Test accuracy: 65.38
Round  34, Train loss: 1.319, Test loss: 1.178, Test accuracy: 65.49
Round  35, Train loss: 1.400, Test loss: 1.154, Test accuracy: 66.05
Round  36, Train loss: 1.461, Test loss: 1.138, Test accuracy: 66.82
Round  37, Train loss: 1.320, Test loss: 1.136, Test accuracy: 66.82
Round  38, Train loss: 1.223, Test loss: 1.114, Test accuracy: 67.67
Round  39, Train loss: 1.238, Test loss: 1.104, Test accuracy: 67.61
Round  40, Train loss: 1.365, Test loss: 1.087, Test accuracy: 68.00
Round  41, Train loss: 1.345, Test loss: 1.109, Test accuracy: 67.42
Round  42, Train loss: 1.159, Test loss: 1.098, Test accuracy: 68.34
Round  43, Train loss: 1.352, Test loss: 1.126, Test accuracy: 67.16
Round  44, Train loss: 1.535, Test loss: 1.136, Test accuracy: 66.75
Round  45, Train loss: 1.323, Test loss: 1.124, Test accuracy: 66.91
Round  46, Train loss: 1.188, Test loss: 1.104, Test accuracy: 67.23
Round  47, Train loss: 1.225, Test loss: 1.106, Test accuracy: 67.19
Round  48, Train loss: 1.386, Test loss: 1.111, Test accuracy: 67.33
Round  49, Train loss: 1.139, Test loss: 1.094, Test accuracy: 67.76
Round  50, Train loss: 1.206, Test loss: 1.082, Test accuracy: 68.69
Round  51, Train loss: 1.247, Test loss: 1.086, Test accuracy: 68.59
Round  52, Train loss: 1.451, Test loss: 1.105, Test accuracy: 67.93
Round  53, Train loss: 1.425, Test loss: 1.111, Test accuracy: 67.67
Round  54, Train loss: 1.325, Test loss: 1.092, Test accuracy: 68.03
Round  55, Train loss: 1.344, Test loss: 1.097, Test accuracy: 68.01
Round  56, Train loss: 1.311, Test loss: 1.102, Test accuracy: 67.50
Round  57, Train loss: 1.188, Test loss: 1.083, Test accuracy: 68.55
Round  58, Train loss: 1.427, Test loss: 1.081, Test accuracy: 68.42
Round  59, Train loss: 1.114, Test loss: 1.079, Test accuracy: 68.41
Round  60, Train loss: 1.140, Test loss: 1.072, Test accuracy: 68.45
Round  61, Train loss: 1.088, Test loss: 1.067, Test accuracy: 68.82
Round  62, Train loss: 1.241, Test loss: 1.079, Test accuracy: 68.14
Round  63, Train loss: 1.489, Test loss: 1.091, Test accuracy: 68.08
Round  64, Train loss: 1.120, Test loss: 1.077, Test accuracy: 68.05
Round  65, Train loss: 1.353, Test loss: 1.082, Test accuracy: 67.79
Round  66, Train loss: 1.198, Test loss: 1.092, Test accuracy: 67.70
Round  67, Train loss: 1.373, Test loss: 1.084, Test accuracy: 68.04
Round  68, Train loss: 1.277, Test loss: 1.086, Test accuracy: 68.16
Round  69, Train loss: 1.298, Test loss: 1.094, Test accuracy: 68.09
Round  70, Train loss: 1.268, Test loss: 1.093, Test accuracy: 68.06
Round  71, Train loss: 1.364, Test loss: 1.104, Test accuracy: 67.48
Round  72, Train loss: 1.072, Test loss: 1.084, Test accuracy: 68.11
Round  73, Train loss: 1.192, Test loss: 1.085, Test accuracy: 67.78
Round  74, Train loss: 0.997, Test loss: 1.075, Test accuracy: 67.99
Round  75, Train loss: 1.113, Test loss: 1.059, Test accuracy: 68.54
Round  76, Train loss: 1.270, Test loss: 1.081, Test accuracy: 68.06
Round  77, Train loss: 1.312, Test loss: 1.077, Test accuracy: 68.12
Round  78, Train loss: 1.033, Test loss: 1.056, Test accuracy: 68.77
Round  79, Train loss: 1.266, Test loss: 1.069, Test accuracy: 68.27
Round  80, Train loss: 1.126, Test loss: 1.061, Test accuracy: 68.53
Round  81, Train loss: 1.351, Test loss: 1.066, Test accuracy: 68.55
Round  82, Train loss: 1.258, Test loss: 1.069, Test accuracy: 68.47
Round  83, Train loss: 0.993, Test loss: 1.059, Test accuracy: 68.91
Round  84, Train loss: 1.084, Test loss: 1.046, Test accuracy: 69.38
Round  85, Train loss: 0.849, Test loss: 1.049, Test accuracy: 69.18
Round  86, Train loss: 1.369, Test loss: 1.072, Test accuracy: 68.33
Round  87, Train loss: 1.384, Test loss: 1.083, Test accuracy: 67.92
Round  88, Train loss: 1.226, Test loss: 1.074, Test accuracy: 68.38
Round  89, Train loss: 1.056, Test loss: 1.074, Test accuracy: 68.06
Round  90, Train loss: 1.160, Test loss: 1.090, Test accuracy: 67.55
Round  91, Train loss: 1.258, Test loss: 1.112, Test accuracy: 66.94
Round  92, Train loss: 1.119, Test loss: 1.077, Test accuracy: 68.14
Round  93, Train loss: 1.266, Test loss: 1.091, Test accuracy: 67.83
Round  94, Train loss: 1.050, Test loss: 1.076, Test accuracy: 68.05
Round  95, Train loss: 1.302, Test loss: 1.095, Test accuracy: 67.42
Round  96, Train loss: 1.344, Test loss: 1.089, Test accuracy: 67.36
Round  97, Train loss: 1.384, Test loss: 1.099, Test accuracy: 67.28
Round  98, Train loss: 1.160, Test loss: 1.078, Test accuracy: 68.02
Round  99, Train loss: 1.004, Test loss: 1.061, Test accuracy: 68.60
Final Round, Train loss: 1.080, Test loss: 1.072, Test accuracy: 67.91
Average accuracy final 10 rounds: 67.71775
4335.844111204147
[6.083775043487549, 11.859652042388916, 17.6608784198761, 23.516088247299194, 29.403225421905518, 35.27543783187866, 41.10017704963684, 46.894455671310425, 52.71211576461792, 58.52200794219971, 64.3249819278717, 70.13485932350159, 75.88749241828918, 81.17445278167725, 86.39155292510986, 91.64062881469727, 96.89461827278137, 102.17567014694214, 107.52893137931824, 112.83781361579895, 118.08401846885681, 123.39793109893799, 128.70605516433716, 133.9552481174469, 139.21401572227478, 144.4685995578766, 149.8260133266449, 155.13861632347107, 160.4305911064148, 165.67628741264343, 170.90452408790588, 176.2126955986023, 181.53597593307495, 186.87793016433716, 192.17316007614136, 197.41022658348083, 202.7098846435547, 208.0542562007904, 213.32084727287292, 218.6082203388214, 223.94294691085815, 229.22116994857788, 234.5228235721588, 239.80760979652405, 245.13963532447815, 250.30168294906616, 255.4977867603302, 260.86725783348083, 266.0995862483978, 271.34958815574646, 276.6545400619507, 282.0283498764038, 287.27537989616394, 292.56193375587463, 297.8818345069885, 303.16030192375183, 308.4496262073517, 313.6946017742157, 318.94278216362, 324.15554308891296, 329.444885969162, 334.75825357437134, 340.045325756073, 345.3043804168701, 350.70301485061646, 355.96343660354614, 361.23118591308594, 366.6003384590149, 371.878933429718, 377.1741826534271, 382.48673129081726, 387.7543272972107, 392.97121000289917, 398.3392844200134, 403.6460177898407, 408.99388313293457, 414.31053137779236, 419.56464529037476, 424.8464205265045, 430.1550281047821, 435.44730043411255, 440.75415086746216, 446.0270049571991, 451.3111789226532, 456.5405435562134, 461.8207643032074, 467.09916973114014, 472.26713156700134, 477.51940298080444, 482.77656841278076, 488.0573604106903, 493.4187078475952, 498.688903093338, 503.9159996509552, 509.16079449653625, 514.4120316505432, 519.7217085361481, 525.0328388214111, 530.2306427955627, 535.4946029186249, 537.5574972629547]
[27.4575, 34.5125, 40.8925, 43.18, 47.295, 49.0225, 50.755, 51.68, 52.73, 53.915, 54.7325, 55.6875, 56.565, 56.95, 57.74, 58.16, 58.5525, 58.8625, 59.9025, 61.435, 62.0575, 62.9075, 63.7275, 64.2875, 63.7725, 64.4175, 64.455, 64.755, 65.1825, 64.835, 65.465, 66.16, 65.66, 65.38, 65.49, 66.045, 66.8225, 66.82, 67.6725, 67.6075, 67.9975, 67.415, 68.3375, 67.16, 66.7525, 66.9125, 67.235, 67.1925, 67.3275, 67.76, 68.6875, 68.5925, 67.9275, 67.67, 68.0275, 68.0075, 67.505, 68.545, 68.425, 68.41, 68.45, 68.8175, 68.1375, 68.075, 68.045, 67.7925, 67.705, 68.0425, 68.1625, 68.095, 68.06, 67.4775, 68.1075, 67.785, 67.99, 68.5375, 68.06, 68.12, 68.7675, 68.27, 68.53, 68.55, 68.4725, 68.9075, 69.3775, 69.1825, 68.335, 67.9175, 68.3775, 68.06, 67.545, 66.935, 68.14, 67.825, 68.0525, 67.425, 67.36, 67.275, 68.0225, 68.5975, 67.905]
